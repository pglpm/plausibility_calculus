THE ADVANCED 
THEORY OF STATISTICS 
by 
MAURICE G. KENDALL, M.A. 
Fellow and Member of the Council of the Royal 
Statistical Society ; Statistician to the Chamber of 
Shipping of the United Kingdom ; formerly Head 
of the Economics Intelligence Branch, Ministry of 
Agriculture and Fisheries 
VOLUME I 
With 16 Illustrations and 79 Tables 
SECOND EDITION 
REVISED 
it 
LONDON 
CHARLES GRIFFIN & COMPANY LIMITED 
42 DRURY LANE 
1945 
[All Rights Reserved] 
IIA l_lb 
Printed in Great Britain 
by Butler & Tanner Limited* Frome 
DEDICATED 
TO MY MOTHER. 
i 
i m>^ ■ 
\ 
i 
" Let us sit on this log at the roadside," says I, " and 
forget the inhumanity and ribaldry of the poets. It is in 
the glorious columns of ascertained facts and legalised 
measures that beauty is to be found. In this very log 
we sit upon, Mrs. Sampson," says I, c< is statistics more 
wonderful than any poem. The rings show it was sixty 
years old. At the depth of two thousand feet it would 
become coal in three thousand years. The deepest coal 
mine in the world is at Killingworth, near Newcastle. 
A box four feet long, three feet wide, and two feet eight 
inches deep will hold one ton of coal. If an artery is cut, 
compress it above the wound. A man's leg contains thirty 
bones. The Tower of London was burned in 1841." 
" Go on, Mr. Pratt," says Mrs. Sampson. " Them ideas 
is so original and soothing. 1 think statistics are just as 
lovely as they can be." 
0. Henry, The Handbook of Hymen. 
\ 
V 
PREFACE 
The need for a thorough exposition of the theory of statistics has been repeatedly 
emphasised in recent years. The object of this book is to develop a systematic treatment 
of that theory as it exists at the present time. Originally my intention was to complete 
the work in one volume, but the war has made such a course impossible. . Nevertheless, 
this first volume is largely complete in itself and can, I hope, be profitably read in advance 
of the publication of its successor. 
In 1938 Dr. M. S. Bartlett, Dr. J. 0. Irwin, Professor E. S. Pearson, Dr. John Wishart 
and I discussed the possibility of writing a treatise on the theory of statistics in co-operation, 
and even got as far as sketching a synopsis. This proposal, however, had to be abandoned 
after the outbreak of war, and with some misgivings I decided to proceed alone. My present 
treatment differs very considerably from the one then agreed upon, since a number of 
sacrifices of viewpoint made for the purpose of reaching unanimity are no longer necessary. 
I must accordingly assume sole responsibility for the form and content of the present book, 
but acknowledgment is due to my colleagues for the helpful discussions which took place 
while the synopsis of the original proposal was being drafted. 
Apart from the usual problems arising in writing any book with pretensions to 
comprehensiveness—emphasis, rejection of unimportant material, sequence of presentation, 
and so forth—there were two main questions to be decided in regard to this book: the 
amount of mathematics admitted, and the point of introduction of the theory of probability. 
Statistical theory is essentially mathematical, and I have not hesitated—in fact I have 
been compelled—to adopt a rather advanced mathematical treatment in order to achieve 
rigour where it is attainable in the present state of our knowledge. Nevertheless I have 
tried (in places, perhaps, with indifferent success) to keep the mathematics to heel. 
This is intended to be a book on statistics, not on statistical mathematics. 
As to the place of the theory of probability, I have felt it preferable to deal with the 
descriptive properties of frequency-distributions before introducing the probability concept. 
This is justified both by the historical development of the subject and by the necessities 
of a logical presentation. Some readers may feel that the whole theory of modern statistics 
is so permeated with the sampling conception that an earlier introduction of probability 
would more than offset the loss in logical sequence by the gain in didactic force. This 
view I myself hold to be fundamentally wrong, but if the reader feels keenly on the subject 
he has, after all, merely to read Chapters 7 and 8 immediately after Chapter 1 and the 
difficulty is to a great extent resolved. 
The subjects covered by the present volume may be considered under three main 
heads. Chapters 1 to 6 deal with Frequency-Distributions and their properties. Chapters 
7 to 11 deal with the Theories of Probability and Sampling and with the Sampling 
Distributions to which they lead. Broadly, this section comprises the theory of those 
distributions which are derived from parent populations for special purposes such as inferences 
in probability, and may be termed the Theory of Derived Distributions. Chapters 13 to 16 
deal with the Theory of Correlation, considered as a measure of relationship, the general 
theory of regression analysis being left to the second volume. Chapter 12, on the 
^-distribution, is perhaps something of an intrusion in the development, but in view of the 
• ■ 
Vll 
viii PREFACE 
widespread applications of %% in testing agreement between theory and observation I felt 
that it should be introduced at an early stage. 
The second volume will deal with the Theory of Estimation, Regression, Analysis 
of Variance, Tests of Significance, Multivariate Analysis, Theories of Statistical Inference, 
and Time Series. In the first volume it has been possible to.avoid a detailed examination 
of controversial topics connected with the logic of inference in probability; the subject 
will be taken up more systematically in the second volume. 
On the invaluable principle that example is better^than precept, a special effort has 
been made to exemplify the theory at every stage and to provide exercises for the reader 
to work out for himself. Some of the latter are rather difficult, but have nevertheless been 
included to illustrate the scope of application of the theory and to refer to results for which 
no place could conveniently be found in the text. In assembling this material I have 
drawn freely on the wealth of research work in statistical periodicals, particularly Biometrika, 
and am glad to make acknowledgment to the authors from whose papers examples have 
been taken. 
Foremost among my more specific indebtedness is that to Dr. Leon Isserlis, who 
read the whole book at the galley proof stage and to whose careful scrutiny I owe a great 
deal. • I have also to thank Dr. J. O. Irwin, who allowed me to consult his draft of a chapter 
originally intended for the co-operative treatise (this forms the basis of Chapter 10).; 
Professor R. A. Fisher and Messrs. Oliver and Boyd, for permission to reproduce Appendix 
Tables 4 and 5 from the former's Statistical Methods for Research Workers; and the 
publishers, Mess,rs. Charles Griffin and Co., and the printers, Messrs. Butler & Tanner Ltd., 
who have taken great pains with some very difficult manuscript. 
I shall be grateful to any reader who notifies me of any error, omission or ambiguity, 
from which, I fear, no book of this kind can be entirely free at its first appearance. 
M. G. KENDALL. 
London, 
February 1st, 1943* 
PREFACE TO SECOND EDITION 
In this edition no alterations of substance have been made, but a number of misprints 
have been removed. I am indebted to several correspondents for calling my attention 
to them. 
One of the consequences of the rapid exhaustion of the first edition is that this second 
edition appears before Volume 2. The latter is now passing through the press, and I hope 
it will be published before the end of the year. 
M. G. K. 
', March, 1945. 
TABLE OF CONTENTS 
CHAP. 
1. 
2. 
3. 
4. 
5. 
6. 
7. 
8. 
<). 
10. 
11. 
12. 
13. 
14. 
IT). 
Hi. 
Introductory Note ... 
Frequency-distributions 
Measures of location and dispersion ... 
Moments and cumulants 
Characteristic functions 
Standard distributions—(1) 
Standard distributions—(2) 
Probability and likelihood 
Random sampling 
Standard errors 
Exact sampling distributions 
Approximations to sampling distributions 
The £--distribution 
Association and eontingencv 
Product, -moment correlation 
Partial and multiple correlation 
Rank correlation ... 
Aitknmx Taklks :— 
1. Frequency function of the normal distribution 
2. Distribution function of tho normal distribution 
3. Distribution function of tho ^-distribution 
4. 5 per, cent, points of the ^-distribution 
;"). I per cent, points of the ^-distribution 
0, Distribution function of %* for one degree of freedom, %% 
7. Distribution function of ^- for one degree of freedom, %* 
AePHNDix Diagram : Omtour lines of the £2, v surface 
Index to YotiUAiw I 
0 to 1 
1 to 10 
PAGES 
xi-xii 
1-28 
29-48 
49-89 
90-115 
116-136 
137^163 
164-185 
186-203 
204-230 
231-253 
254-289 
290-307 
308-323 
324-367 
368-387 
388-437 
438 
439 
440-441 
442 
443 
444 
TCTcO 
446 
447-457 
IX 
INTRODUCTORY NOTE 
0.1. The chapter-sections in this book are numbered serially. The serial numbers 
are prefixed by the number of the chapter in which they occur and are separated therefrom 
by a period, e.g. 14.13 refers to the thirteenth section of Chapter 14. A similar procedure) 
is followed for tables, equations and exercises, e.g. (7.15) refers to the fifteenth equation 
of Chapter 7. In cross-references, chapter-sections are demoted by clarendon typo, tho others 
by ordinary type. 
0.2. References to printed work are given by author's narao and date of publication. 
In the list of references at the end of the chapter authors are arrangod alphabetically. 
Where articles from publications are referred to, the number of the volume is given in 
clarendon type and the number of the first page of the article in ordinary type, e.g. Ann. 
Math. Statist., 10, 275, refers to the article beginning on page 275 of volume 10 of the Annalft 
of Mathematical Statistics. Where an exercise is followed by an author's name and a date, 
the result given in the exercise appears in the article listed in the references to tho chapter 
concerned under these particulars. Where the result is from an article not previously 
referred to a full reference is given. 
0.3. The mathematical notation is that in current use, but a fow symbols may bo 
explained. 
(1) The exclamation mark ! writton after an integer moans the factorial of that intogor. 
Some writers give the symbol a more extended use for non-integral numbers by writing 
x\ =l\x -f- 1) = f e~n*tU. 
Jo 
This, of course, accords with the factorial notation, but will not bo used in this hook. 
(n\ n\ 
) ~ (, _1 ^ will be UHod in place of the older nOr. 
j « 
(3) The summation sign will be written as Z", e.g. yj-^ -• xr -\* a:a -|- . . - -[- xn. 
j i 
The symbol } can as a rule bo shortened to } and in many cases to y or merely to 
E, the extent of the summation being clear from tho context. 
(4) The ordinary notation for tho Afunetion (given above), tho ft-function, and tho 
hypergeomettric function will bo used, i.e. 
*ip, u) - fV1 (i --xr*tu - 'lp)'Xql 
Jo l[p H- <l) 
and 
"""• ^ l.y T 1.2.y(y + l) ^ 1.2. \\.V(y + ])(y | 2) '' "» ' ■ " 
(5) Where the exponent is concise, tho exponential function will bo writton as a power 
of e, for example eiir\ But where it is lengthy wo shall use tho notation exemplified by 
exp {- h(%2 - %pxy + «/2)} instead of e-^-'^+^K 
xi 
xii INTRODUCTORY NOTE 
0.4. In some fields it is useful to preserve a distinction between a statistical parameter 
in a population and the estimate of that parameter from a sample. Where possible, the 
former will be denoted by a Greek letter and the latter by a Roman letter, e.g. the product- 
moment correlation coefficient of a population is denoted by p and that of a sample by r. 
It is not, however, always possible to preserve this distinction, as for instance with the 
multiple correlation coefficient i2, in which case a Greek capital would be confused with the 
Roman P. Complete notational consistence can only be achieved at the expense of 
jettisoning a great deal of accepted statistical UBage, and even then would probably 
result in some cumbrous symbols. 
0.5. In order to enable the reader to follow the worked examples and illustrative 
material, a few tables of functions commonly required are given at the end of this volume. 
TheBe tables are in no way a substitute for the comprehensive sets which have been 
published and which are a necessary adjunct to most practical and a good deal of theoretical, 
work. Frequent reference will be made to the following :— 
Tables for Statisticians and Biometricians, edited by Karl Pearson, Parts I and II, Biometrika 
Office, University College, London, W.C.I. 
Statistical Tables for use in Biological, Agricultural and Medical Research, by R. A. Fisher 
and F. Yates, Oliver and Boyd, Edinburgh. 
The following are also useful:— 
Tables of the Incomplete r-function, edited by Karl Pearson, Biometrika Office, University 
College, London, W.C.I. . 
Tables of the Incomplete B-function, edited by Karl Pearson, Biometrika Office, University 
College, London, W.C.I. 
The Kelley Statistical Tables, by T. L. Kelley, Macmillan, London and New York. 
" Tables of Pearson's Type III Function," by L. R. Salvosa, Ann. Math. Statist., 1930, 
1, 191. 
Tables of the Higher Mathematical Functions, edited by H. T. Davis, Parts I and.II, Principia 
Press, Bloomington, Indiana. 
Tables of Random, Sampling Numbers, by L. H. C. Tippett, Tracts for Computers, No. lfi, 
Cambridge University Press. 
Tables of Random Sampling Numbers, by M. G. Kendall and B. Babington Smith, Tracts 
for Computers, No. 24, Cambridge University Press. 
Tables of the Correlation Coefficient, by F. N. David, Biometrika Office, University College, 
London, W.C.I. 
Tables of tan~xx and log {I -f- a;2), by L. J. Comrie, Tracts for Computers, No. 23. 
Tables of the Probability Integral, by W. F. Sheppard, British Association Mathematical 
Tables, Vol. 7, Cambridge University Press. 
0.6. The references given at the end of the chapters are mainly intended to guide 
further reading and are not exhaustive. A more complete bibliography will be found in 
Mr. Yule's arid my Introduction to the Theory of Statistics, which contains about 700 
references to.work appearing up to about 1932, and in the valuable periodic reviews of recent 
advances in theoretical statistics appearing in the Journal of the Royal Statistical Society 
arid the Journal of the American Statistical Association. A recently-begun monthly 
publication by the American Mathematical Society, Mathematical Reviews, also contains material 
of interest in this connection. 
CHAPTER 1 
FREQUENCY-DISTRIBUTIONS 
Statistics as the Science of Populations 
1.1. Among the many subjects about which statisticians disagree is tho definition 
of their science. In the Revue de VInstitut International de StatiMtique for 1935 (vol. 3, 
page 388) Dr. W. F. Willcox listed well over a hundred definitions of statistics, and tho 
list was far from exhaustive. Even when we exclude those definitions which were formulated 
before the subject reached its present extent we are left with a. variety of choices, and 
there is no definitive description of the scope of the science of statistics with which we 
can begin this book. 
1.2. The fundamental notion'in statistical theory is that of tho group or aggregate, 
a concept for which statisticians use a special word—" population ". This term will 1ms 
generally employed to denote any collection of objects under consideration, whether 
animate or inanimate ; for example, we shall consider populations of men, of plants, of 
mistakes in reading a scale, of barometric heights on different flays, and oven populations 
of ideas, such as that of the possible ways in which a hand of curds might bo dealt. The 
notion common to all these things is that of aggregation. 
It is with the properties of populations that statistics is mainly concerned. In 
considering a population of men we are not interested, statistically speaking, in whether some 
particular individual has brown eyes or in a forgor, but rather in how many of the individuals 
have brown eyes or are forgers, and whether the poNsession of brown eyes goes with a 
propensity to forgery in the population. We aro, ho to speak, concerned with tho properties 
• of the population itself. Such a standpoint can occur in physics an well as in demographic 
sciences. For instance, in discussing the behaviour of a gas we aro not ho much interested 
in the behaviour of particular molecules, as in that of the aggregate of molecules which 
go to compose the gas. The statistician, like Nature, is mainly concerned with the species 
and is careless of the individual. 
* 
1.3. We may therefore begin an approach to a definition of our subject by the 
following : statistics is the branch of scientific method which deals with the properties 
of populations. This, however, is rather too general. Statistics deals only with tint 
numerical properties. A dictionary, for example, sots out a population of words, and 
among the properties of that population which arc a suitable subject for scientific inquiry 
is that of word-derivation. It is not of statistical concern, however, to know that some 
words are derived from Latin, some from Anglo-Saxon and Home from Hindustani. Tho 
subject would only assume a statistical aspect if we were to impure how many words were 
derived from the different sources. 
1.4. As a second approximation to our definition we may then try tho following : 
statistics is the branch of,scientific method which deals with tho data obtained by counting 
or measuring the properties of populations. 
This again is a little too general. A sot of logarithm tables ih a population of numerals, 
but it is hardly a subjeot for statistical inquiry, for every numeral is determined according 
• 
2 FREQUENCY-DISTRIBUTIONS 
to mathematical laws. The statistician is rather concerned with populations which occur 
in Nature and are thus subject to the multitudinous influences at work in the world at 
large. His populations rarely, if ever, obnform exactly to simple mathematical rules, 
and in fact it is in the departure from such rules that he often finds topics of the greatest 
statistical interest. To allow for this factor we may tlien formulate our definition as 
follows:— 
Statistics is the branch of scientific method which deals with the data obtained by 
counting or measuring the properties of populations of 'natural phenomena. In this 
definition " natural phenomena " includes all the happenings of the external world, whether 
human or not. 
This is as far as we need pursue the matter. The reader who is interested enough to 
look through the definitions listed by Dr. Willcox in the article referred to above will 
find, I think, that in the light of this definition there is a perceptible thread of continuity 
running through them. 
1.5. Eor the avoidance of misunderstandings in the interpretation of this definition 
it may be as well to point out that " statistics," the name of the scientific method, is 
a collective noun and takes the singular. The same word " statistics " is also applied to 
the numerical material with which the method operates, and in such a case takes the 
plural. Later in this book we shall meet the singular form " statistic," which is not, 
as might be supposed, an individual item of information which in the aggregate would 
compose " statistics," but is the name given to an'estimate of certain unknown measures 
of a population. • .- ■ 
Frequency-Distributions 
1.6. Consider a population of members each of which bears some numerical value 
of a variable, e.g. of men measured according to height or of flowers classified according 
to numbers of petals. This variable we shall call a variate. If it can assume only 
a number of isolated values it will be called discontinuous, and if it can assume any value 
of a continuous' range, continuous. The population of members will then correspond to 
a population of variate-values, and it is the properties of this latter population which 
we have to consider. 
If the population consists of only a few members we can without much difficulty 
consider the population of variate-values exhibited by them ; but if, as usually happens, 
the aggregate is large (or, in a sense defined later, infinite), the set of variate-values has 
to be reduced in some way before the mind can grasp their significance. This is done 
by classification of the individuals into ranges of the variate. So far as possible the ranges 
should be equal, so that the numbers falling into different ranges are comparable. The 
interval is called the class-interval (or simply the interval) and the number of members 
bearing a variate-value falling into a given class-interval is the class-frequency (or simply 
the frequency). The manner in which the class-frequencies are distributed over the 
class-intervals is called the frequency-distribution (or simply the distribution). 
1.7. Tables 1.1 and 1.2 give some frequency-distributions of observed populations 
classified according to a single variate. Table 1.1 shows the 1567 Local Government 
Areas of England and Wales distributed according to the variate " birth-rate." Here, 
for example, there were 7 districts with a birth-rate of between 5-5 and 6-6 per thousand, 
and 271 with a birth-rate between 13-5 and 14-5 per thousand. The general nature of 
FREQUENCY-DISTRIBUTIONS 3 
TABLE 1.1 
Showing the Number of Local Government Areas in England with Specified Birth-rates per 
Thousand of Population. 
(Material from the Registrar-General's Statistical Review of England and Wales for 1933.) 
Birth-rate. 
1 -5 and not exceeding 
2-5 
3-5 
4-5 
5-5 
6-5 
7-5 
8-5 
9-5 
10-5 
11-5 
12-5 
ji >i 
>> j> 
»» »» 
>> »> 
j> »> 
sj »> 
it »J 
>> JJ 
JJ JJ 
JJ JJ 
JJ JJ 
2-5 
3-5 
4-5 
5-5 
6-5 
7-5 
8-5 
9-5 
10-5 
11-5 
12-5 
13-5 
Number of 
Districts with 
Birth-rato in 
Specified Range. 
1 
2 
2 
3 
7 
9 
14 
41 
83 
131 
192 
242 
Birth-rate. 
13-5 and not exceeding 14-5 
14-5 
■ 15-5 
10-5 
17-5 
18-5 
19-5 
20-5 
21-5 
22-5 
23-5 
»j » 
JJ 9 
!> J 
>> J 
If J 
>! 1 
JJ J 
JJ t 
JJ > 
JJ J 
15-5 
16-5 
17-5 
18-5 
105 
20-5 
21 -5 
22-5 
23-5 
24 -6 
Totatj 
Number of 
Districts ■with 
Birtii-rate in 
Specified Range. 
271 
190 
127 
89 
78 
37 
21 
17 
4 
4 
2 
. 
15(57 
i 
TABLE 1.2 
Showing the Numbers of Persons in the United Kingdom liable to Sur-tatc and Super-tax in 
the Year beginning 5th April 1931, classified according to the Magnitude of their 
Annual Income. 
(From the Statistical Abstract for tho United Kingdom for the Yoai's 1913 and 1919-32, 
Cmd. 4489.) 
Annual Tncomo 
(11000). 
2 and noL exceeding 2-5 
2-5 „ „ 3 
3 u ii 4 
4 „ „ 5 
5 „ ,, 0 
0 „ '„ 7 
7 ,, .j 8 
8 „ „ 10 
10 „ „ 15 
15 „ „ 20 
20 „ „ 25 
25 „ „ 30 
30 „ „ 40 
40 „ „ 50 
50 „ „ 75 
75 „ „ 100 
100 and over 
Total numbor of pomona 
Number of 
Povhoiw. 
23,988 
15,781 
17,979 
9,755 
5,921 
3,729 
2,54(5 
3,193 
3,<>KS 
1,328 
(179 
378 
372 
192 
182 
57 
M, 
89,790 
Estimated 
I^riMiiiwioy per 
£500 Interval. 
23,988 
15,781 
8,089 
4,877 
2,9150 
1,8(54 
1,273 
798 
3(52 
133 
(18 
38 
19 
10 
4 
1 
? 
, — 
FREQUENCY-DISTRIBUTIONS 
300 
5 
&200 
I 
o 
8 
^ too 
£ 50 
5 ,0 , , ls 20 
Birth-rate (perthous&Ttdaf population) 
Fig. 1.1. Frequency Polygon of the Data of Table 1.1. 
25 
the distribution is shown in this table in a way which would be quite impossible if each 
of the 1567 districts were shown separately. The greatest number of distriots fall within 
the range 13-5-14-5 per thousand and the frequencies tail off on either side of this value. 
Table 1.2 shows the number of persons subject to sur-tax and super-tax in the United 
Kingdom in 1931 classified according to the variate " income." The class-intervals here 
are unequal—a typical defect of official figures—and in the last column 
of the table is a reduction of the class-frequencies to comparability, namely, 
to frequency per £500 within the class-interval concerned. Looking at this 
column we see that the maximum frequency per £500 in this caso is at the 
beginning of the frequency-distribution. 
25 
20 
*• 15 
O 
i 
3 
=5 
1.8. The frequency-distribution may be represented graphically. 
Measuring the variate-value along the #-axis and frequency per class- 
interval along the y-axis, we erect at the abscissa corresponding to the 
centre of each class-interval an ordinate equal to the frequency per unit 
interval in that interval. The ends of these ordinates are joined by 
straight lines, one to the next. The diagram so obtained is called a 
Frequency Polygon. Fig 1.1. shows the frequency polygon for the data 
of Table 1.1. 
As a variant of this procedure we may erect on the abscissa 
range corresponding to each class-interval a rectangle whose area 
is proportional to the frequency in that interval. A diagram 
constructed in this way is called a Histogram. Fig. 1.2 shows 
such a histogram for the data of Table 1.2. It is evident that 
the histogram is a 
more suitableform 
of representation 
when the class- 
intervals are 
unequal. 
/5 20 
Annual Income (jEooo) 
Fig. 1.2. Histogram of the Data of Table 1.2. 
25 
30 
FREQUENCY-DISTRIBUTIONS 5 
1.9. A few practical points in, the tabulation of observed frequency-distributions 
may be noted. 
(1) It has been remarked that wherever possible the class-intervals should be equal. 
The importance of this will be more appreciated in subsequent chapters ; but it is already 
evident that comparability is difficult to carry out by inspection when there exist inequalities 
in class-intervals. On running the eye down the second column of Table 1.2, for example, 
we note that the frequencies in intervals 3-4 and 8-10 are greater than in the immediately 
preceding intervals ; but tins is merely due to a change in the width of the intervals 
at those points and, as is seen from the third column, the frequency per unit interval 
decreases steadily. 
(2) It is important to specify the class-interval with precision. We not infrequently 
meet with such classifications as " 0-10, 10-20, 20-30," etc. To which interval is a member 
with variate-value 10 assigned ? Obviously the classification is ambiguous if such values 
can in fact arise. We must either take the intervals " greater than or equal to 0 and less 
than 10, greater than or equal to 10 and less than 20," or make it clear what convention 
we use to allot a variate-value falling on the border between two neighbouring intervals, 
e.g. it might be decided to allot one-half of the member to each. There are various ways 
of indicating the class-interval in practical tables, e.g. " 10-, 20-, 30-" means " greater 
than or equal to\10 and less than 20," and so forth. Sometimes, where a continuous 
variate is concarned, there is an element of imprecision in the specification of the finenoss 
to which the measurements are made ; for example, if we are measuring lengths in 
centimetres to the nearest centimetre, an interval shown as " graator than 15 and less than 
18" means an interval of "greater than 14.5 and less than 18.5." When the precision 
of the measurements is known wo can specify an interval by its middle point, for example, 
in this case, 16.5. 
TABLE 1.3 
Showing the Number of Deailis from Scarlet Fever at Different Ages in England and Wales 
in 1033. 
(Data from Registrar-General's Statistical Review of England and Wales for 1033, Tables 
Part I, Medical, supplemented by information supplied by him in correspondence.) 
Ago in Years. 
0- 
1- 
2- 
3- 
4- 
5- 
10- 
15- 
20- 
26- 
30- 
35- 
Numbor of 
DotltllS.y 
/ 
nr 
00 
so 
74 
74 
^322 
213, 
70 
27 
2(1 
17 
12 
11 
Numbor pnr 
Your. 
1(J 
00 
80 
74 
74 
42-ti 
14-0 
5-4 
5-2 
3-4 
2-4 
2-2 
Ago in YiuirH. 
40- 
45- 
50- 
55- 
00- 
05- 
70- 
75- 
80- 
Total 
Numbor of 
10 
(J 
7 
5 
— 
1 
1 
1 
— 
729 
-i 
Numbor por 
Yoar. 
2-0 
1-2 
1-4 
1-0 
— 
0-2 
0-2 
0-2 
— 
* 
k 
\ 
q jTBEQUENCY-DISTRrBXJTIONS 
(3) Remark (1) about the importance of equality of cto-intervals should not bo hold 
to preclude the specification of frequencies in finer intervals where the frequency is changing 
t™Sy Table 1.3, for instance, shows the number of deaths fromsoart fever m 
Endand and Wales in 1933 according to the variate " age at death » If the frequency 
in the interval » 0 and less than 5 " were not subdivided and were thus shown as a total 
322 for the interval, we might draw the conclusion from the uniformly decreasing number 
of deaths as the variate increases that the greatest number of deaths occurred m the 
first year of life. This is not so, as is shown by the individual frequencies in the first 
V6 (4?Perhaps it is hardly necessary to add that the histogram is not a suitable method 
of representing data classified according to discontinuous variates. It shows the clans- 
frequency uniformly dispersed over the whole interval, whereas if the variate is 
discontinuous, frequencies must necessarily be concentrated at certain points. 
Frequency-Distribv<tions: Discontinuous Variates 
1.10. It will be useful at this stage to give some examples of the 
frequency-distributions which occur in practice. 
Table 1.4 shows the distribution of digits in numbers taken from a four-figuro telephone 
directory. The numbers were chosen by opening the directory haphazardly and taking 
the last two digits of all the numbers on the page except those in heavy type. The 
distribution is irregular, but from a cursory inspection of the table we are inclined to .suppo.su 
that the digits occur approximately equally frequently in the larger population from 
which these 10,000 members were chosen. We shall see later (p. 11)3) that the divergences 
from the average frequency per digit, 1000, are not accidental sampling offocts ; but at this 
stage it is sufficient to note that the data suggest for consideration a population of equal 
Infrequent members. 
TABLE 1.4 
Showing Number of Different Digits chosen Imphazardly from the London Telephone. 
Directory. 
(M. G. Kendall and B. Babington Smith (1938), Jour. Roy. Statist. Sac, 101, 147.) 
Digit . 
Frequency 
0 
1026 
1 
1107 
2 
997 
3 
966 
4 
1075 
5 
933 
a 
1107 
7 
972 
8 
904 
9 
H/53 
TOTA I, 
10,000 
Table 1.6 shows the distribution of a number of seed capsules of Shirley poppies 
according to the variate " number of stigmatic rays." The distribution in this case ia 
DISCONTINUOUS VABIATES 7 
more regular, there being a maximum frequency at 13 and a steady deorease on either 
side. 
TABLE 1.5 
Showing the Frequencies of Seed Capsules on certain Shirley Poppies with Different Numbers 
of Stigmatic Rays. 
(Cited from G. Udny Yule (1902), Biometrika, 2, 80.) 
Number of 
Stigmatic Rays. 
C 
7 
8 
9 
10 
11 
12 
13 
Numbor of Capsules 
with said Numbor 
of Stigmatic Rays. 
3 
11 
38 
10<i 
152 
238 
305 
315 
Numbor of 
Stigmatic Rays. 
14 
15 
1(5 
17 
IH 
19 
20 
Total 
Number of Capsules 
with Haul Numbor 
of Stigmatic Rays. 
302 
234 
12H 
50 
19 
3 
1 
1005 
In Table 1.6, on the other hand, showing suicides among women in some CJorman 
states in certain years according to the variato " number of suicides nor year," the 
distribution reaches its maximum frequency in the region 1-3 suicides and then tailH ofl'j 
rather slowly. 
TABLE 1.0 
Showing Suicides of Women in Eight German States in Fourteen years. 
(Von Bortkiowif"/., Das Qesetz dcr Idaincn Za/ilen, 18{)H.) 
Numbor of Suicides 
per year 
Froquoncy . 
0 
9 
1 
1» 
2 
17 
3 
20 
4 
Jii 
5 
11 
a 
X 
7 
2 
H 
3 
9 
5 
10 and ovor 
Total. 
112 
Frequency-Distributions; Continuous Variates 
1.11. Table 1.7 shows a numbor of adult males in the United Kingdom (including, 
at the timo of the collection of the data, the whole of Ireland), distributed according to 
the. variate "height in inches." Tho frequency polygon'is shown in Fig. 1,8. It will be 
seen that the distribution is almost symmetrical, there being a maximum ordinate at 
67- inches and a steady decrease in frequency on either side of the maximum. 
8 
FREQUENCY-DISTRIBUTIONS 
TABLE 1.7 
Showing the Frequency-distributions of Statures for AduU Males born in the United Kingdom 
{including the whole of Ireland). 
(Final Report of the Anthropometric Committee to the British Association, 1883, p. 256.) 
As Measurements are stated to have been taken to the nearest Jth of an inch, the class-intervals are here 
presumably 56$-57$, 57$-58$, and so on. 
Height -without 
Shoes (inches). 
67- 
58- 
59- 
60- 
61- 
62- 
63- 
64- 
65- 
66- 
67- 
68- 
Number of Men 
within said Limits 
of Height. 
2 
4 
14 
41 
83 
169 
394 
669 
990 
1223 
1329 
1230 
Height without 
Shoes (inches). 
69- 
70- 
71- 
72- 
73- 
74- 
75- 
76- 
77- 
TOTAL 
Number of Men 
within said Limits 
of Height. 
1063 
646 
392 
202 
79 
32 
16 
5 
2 
8585 
1 
5 
E 90° 
ber 
E 
J* A/V) 
^ 
S 
3 
«1 
/ 
1 
/ 
f 
1 
1 
1 
/ 
/ 
' 
i 
1 
' 
/ 
V 
\ 
\ 
V 
\ 
\ 
\ 
\ 
\ 
\ 
\ 
A 
50 
60 
62 
64 
66 
68 
StaTure In Inches 
70 
72 
74 
76 
78 
BO 
Fig. 1.3. 
Frequency-distribution of theData of Table 1.7. Values of the abscissa correspond 
to the beginning of class intervals. 
This more-or-less uniform " tailing off" of frequencies is very common in observed 
distributions, but the symmetrical property is comparatively rare. Table 1.1 is roughly 
symmetrical, but Tables 1.8 and 1.9, showing respectively a number of Australian marriages 
distributed according to bridegroom's age, and a number of dairy farms distributed 
according to costs of production of milk, illustrate that various degrees of asymmetry 
can occur. An extreme form is shown in Table 1.3. 
CONTINUOUS VARIATES 
9 
TABLE 1.8 
Showing Numbers of Marriages contracted in Australia, 1907-14, arranged according to the 
Age of Bridegroom in 3~Year Groups. 
(From S. J. Pretorius (1930), Biomeirika, 22, 210.) 
Age of Bridegroom 
(Central Value of 3-year 
Bange, in years). • 
16-5 
19-5 
22-5 
25-5 
28-5 
31-5 
. 34-5 
37-5 
40-5 
43-5 
46-5 
49-5 
52-5 
Number of 
Marriagoa. 
204 
10,905 
81,001 
73,054 
56,501 
33,478 
20,569 
14,281 
9,320 
6,230 
4,770 
3,(520 
2,190 
Ago of Bridogroom 
(Central Value of 3-year 
Range, hi yours). 
55-5 
08-5 
01-5 
04-5 
(57-5 
70-5 
73-5 
7(5-5 
79-5 
82-5 
85-5 
88-5 
TOTAL 
Numbor of 
MiirriugoH. 
1,055 
1,100 
810 
(549 
487 
32(5 
211 
110 
73 
27 
14 
5 
301,785. 
TABLE 1.9 
Showing Numbers of Dairy Farms in England and Wales according to Cost of Production 
of Milk in 1935-6. 
(Data from Costs of Milk Production in England and Wales, Interim Itcport No. 2t 
Agricultural Economics Research Institutes Oxford.) 
No. of KannH. 
Cost of Production 
(pence per gallon). 
4- 
5- 
6- 
7- 
8- 
9- 
No. of Farms. 
4 
9 
34 
77 
94 
88 
CohI:. of Production 
(ponoo por gallon). 
10- 
Jl- 
12- 
13- 
14- 
15- 
ToTAfi 
«r» 
40 
15 
4 
5 
a 
437 
In this connection Table 1.10, showing a number of men distributed according to weight, 
is of interest for comparison with the height data of Table 1.7. The latter is symmetrical 
but the former is not. 
10 
FREQUENCY-DISTRIBUTIONS 
TABLE 1.10 
Frequency-distribution of Weights for Adult Males born in the United Kingdom. 
(Loc. cit., Table .1.7. Weights were taken to the nearest pound, consequently tho true 
class-intervals are 89-6-99-5, 99-5-109-5, etc.) 
Weight in lbs. 
90- 
100- 
110- 
120- 
130- 
140- 
150- 
160- 
170- 
180- 
Frequency. 
2 
34 
162 
390 
867 
1623 
1669 
1326 
787 
476 
Weight in lbs. 
190- 
200- 
210- 
220- 
230- 
240- 
250- 
260- 
270- 
280- 
TOTAL 
Frequency. 
2(53 
107 
85 
41 
If! 
11 
8 
1 
— 
1 
77 HI 
1.12. When the asymmetry of a distribution such as that of Tablo 1.3 becomes 
extreme we may be unable to determine whether, near the maximum ordinate, there 
is a fall on either side, or whether the maximum occurs right at tho start of the (list ribution. 
This would have been the case in Table 1.3 if we had not tho finer grouping fur the first 
five years of life ; and it is the case in Table 1.2, in which the maximum frequency apparent \y 
occurs at or very close to an income of £2,000 per annum. Asymmetrical distributions are 
sometimes called "skew" ; and those such as Table 1.2 are called " J-shuped." 
1.13. In rare oases the distribution may have maxima at both cikIh, as in Table 1.11, 
TABLE 1.11 
Showing the Frequencies of Estimated Intensities of CloudMess at Greenwich during the 
Years 1890-1904 (excluding 1901) for the Month of July. 
(Data from Gertrude E. Pearse (1928), JBiometrika, 20A, 33(1.) 
Degrees of 
Cloudiness. 
10 
9 
8 
7 
6 
5 
Frequency. 
676 
148 
90 
66 
66 
46 
Dogroes of 
Cloudiness. 
4 
3 
2 
1 
0 
Total 
Froquoney. 
45 
68 
74 
12!) 
320 
171C 
ruxTLNror.s vaiuatks 
11 
.showing a number of days distributed neettrditig to degree of cloudiness. Thin is known 
as a l"-shaped distribution. 
1.14. Distribution* also occur which in general appearance resemble sections of tin* 
tyjies already mentioned. A A shaped ili**1rihtiEit»it. for example, reHcmbles the " tail "" 
of the symmetrical distribution of Table 1.7. The suicide data of Table I.H may bo regarded 
as n symmetrical distribution truncated just below the maximum ordinate by tin* 
impossibility of tla* occurrence of iM'^jntivt* value* of the variate. This Hort of conception is 
.sometimes useful in fitting curves to observed data a given analytical curve may tit the 
data quite well in a certain variate rang**, hut may also extend into regions where tin* 
data cannot, so to s{H*uk, follow it. 
1.15. Tin* distributions considered up to this point Imvi* nue thing in common — 
thcv have onlv une maximum or. in the ease of the \' shaiied curve, onlv one minimum. 
Distributions also occur .showing several luaviiua. Tables LIU and l.l.M being instances in 
point. The first, showing a number of deaths according to age at death, is Hpieal of 
death distributions. Near the Mart of tlie distribution there is a maximum and a rapid 
fall in the fret|ueney ; there is an indication of another maximum about the age l!n Ho ; and 
a pronottneed maximum about the age 7ft 7.i, the friMjueiu-ies bevond that point tailing 
otf to zero, ft in natural to wonder whether such a distribution eatt be useful! \ considered 
as three superposed distributions, a J shaped distribution indicative ol iutautile mortality, 
a more or less syniiuetrieal single humped di tlrihiitiou with a uta\iuiuiu at «u L*.\ indicative 
of death** at the adventurous age. and a skew distribution with a maximum at 7u 7ot the 
ordinary death curve of senescence. 
TAIU.K I 11* 
Showintj thr XumIht of Mult- fit nth* in /•,'io//«/m«/ *in>t HW« <■ f*n JU.'IU tVJ, *'!»i *m/i^/ /+;/ Atjt > 
*// Ihuth 
(Data from Registrar (leneral's Statistical IIi-m.-w ot Ktit'luud and Wales, UKV,\, text. 
Aer <»i tu-aili 
o 
In 
t.'i 
lies 
:m 
:*., 
to 
Niuiiti'i <«t I»« if )t 
A."*1 i»* t i<'i»»Ii 
Hi -HI i 
NmioI+i i ..I I». ,itit 
'.('.'.I'ltll 
ii..a/ 
v.:«o;» 
i.'i.m.:.' 
H..VU 
lU.lltii 
i...»;*;3 
k:ii."» 
2M.7 7 s 
.TJ.K'.s 
ki.sui 
.'■« 
I.M 
«i-> 
Vo 
i >* 
so 
"*'"♦ 
*IO 
1*:» 
t*H4 ultel Mti-r 
.i.,«.:t!» 
i.s. io;i 
sii,i.!I(> 
M.nU 
vyjHo 
(.•.una 
i t».r»i a 
^..14:. 
7H7 
»H 
TmIVJ, 
"Wl.lll* 
12 
FREQUENCE-DISTRIBUTIONS 
TABLE 1.13 
STwmng Number of Trypanosomes from Glossina morsitans classified according to Length 
in Microns. 
(From K. Pearson (1914-15), Biometrika, 10,112. Length presumably to nearest micron.) 
Length 
(microns). 
16 
16 
17 
18 
19 
20 
21 
22 
23 
24 
26 
Frequency. 
7 
31 
148 
230 
326 
262 
• 237 
184 
143 
115 
130 
Length 
(microns). 
26 
27 
28 
29 
30 
31 
32 
33 
34 
35 
Total 
Frequency. 
110 
127 
133 
113 
96 
54 
44 
11 
7 
2 ■ 
2500 
A similar dissection of a complex distribution could be undertaken for the data of 
Table 1.13, showing a number of trypanosomes from the tsetse fly, Olossina morsitans, 
classified according to length. We are led to suspect here that the distribution is composed 
of the addition of several others (and this, by the way, has led to a suggestion that the 
trypanosomes are a mixture of distinct types). 
Frequency Functions and Distribution Functions ' 
1.16. The examples given above illustrate the remarkable fact that the majority 
of the frequency-distributions encountered in practice possess a high degree of regularity. 
The form of'the frequency polygons and histograms above suggests, almost inevitably, 
that our data are approximations to distributions which can be specified by smooth curves 
and simple mathematical expressions. This approach to the concept of the frequency 
function, however, requires some care, particularly for continuous distributions. 
Consider in the first place a discontinuous distribution such as that of Table 1.4. Let 
us represent our variate by x. Then we may say that x can take any of the ten values 
0, 1, ... 9 and that the frequency of x, say /(#), is given by the table, that is to say, 
/(0) = 1026, /(l) = 1107, /(2) = 997, and so on. The frequency table, in fact, defines-the 
frequency function. Furthermore, most of the frequencies in the table are approximately 
1,000, and we may then consider the observed distribution as approximating to that 
defined by 
f(x) = 1000, x = 0, 1, ... 9 . . . . (1.1) 
or, more generally, to the distribution 
f{x) = fc, x = 0, 1, . . . 9 . . . . (1.2) 
This is perhaps the simplest case of a discontinuous frequency function, f(x) being 
a constant for all permissible values of x. 
FREQUENCY FUNCTIONS AND DISTRIBUTION FUNCTIONS 13 
In Table 1.5 we have a discontinuous variate which can, theoretically, take an infinites 
number of values, namely, any one of the positive integers. In practice, of course, there 
must be a limit to the number of stigmatic rays which a poppy can possosw, but since wo 
do not know that limit we may imagine our variate as infinite in range. Tho frequency 
•function for the table itself is again simply defined by the frequencies therein ; but if wo 
wish to proceed to a conceptual generalisation of suoh a table we must admit a discontinuous 
function f(x) denned for all positive integral values of x. This occasionn no difficulty 
provided that we are able to attach some meaning to the total frequency, i.e. that 
00 
f(Xj) converges. 
/=i 
1.17. Consider now the case of a continuous variate. In the ordinary data of 
experience our distributions are invariably discontinuous, because our measurements can 
only attain a certain degree of aocuraoy. For instance, wo aro accustomed to suppose 
that the height of a man may in reality be any real numbor of inches in a certain range, 
say 50 to 80, such as 20jr. In fact, we can measure heights only to a certain accuracy, 
say to the nearest thousandth of an inch. Our measurements thus consist of whole numbers 
(of thousandths) from 50,000 to 80,000, and such a numbor as 02,831*85 (-- 20,000jr 
approximately) cannot appear. All physical measurements are subject to this limitation, 
but we accept it and nevertheless speak of our variables as " continuous," tho 
underlying supposition being that tho measurements are approximations to numbers which can 
fall anywhere in the arithmetic continuum. 
1.18. With this understanding wo can consider the distribution of grouped frequencies 
as leading to the concept of a frequency function for a continuous variate. if, in one 
of the distributions above, say that of Table 1.7, we were to subdivide the intervals, wo 
should probably find that up to a point the resulting frequencies were* smoother and smoother. 
The reader can verify the appearance of this efleet for himself by grouping tho data of 
Table 1.7 in intervals of 8, 4, and 2 inches. Wo cannot, however, take* tho proeesH loo 
far, because, with a finite population, continued subdivision of the interval would sooner 
or later result in irregular frequencies, there being only a few mcmboi's in each interval. 
But we may suppose that for ranges Ax, not too small, the distribution may be speeilied 
by a function/(jc) Ax, expressing that in the range -± \Ax centred at & the frequency is 
f(x) Ax, wherever x may be in the permi/itrible range of tho. mrirtfe. We may suppose further 
that as Ax tends to zero the population is perpetually replenished so as to prevent, the 
occurrence of small and irregular frequencies ; and in this way we arrive at the concept 
of the frequency function for a continuous variable. We write 
dF=f(x)dx (1.3) 
expressing that the element of frequency dF between x ~ Idx and x -|- hLr is /(.k) ti\\ for 
all x and for dx, however small. 
1.19. This admittedly somewhat intuitive approach to the concept of the continuous 
frequency-distribution appears to be tho best for statistical purposes, and is certainly 
the way in which the concept was originally roached,. In formulating tho axioms and 
postulates of a rigorous mathematical theory, however, the mathematician considers a 
rather more general function. There is as yet no thorough formulation of the theory 
required in this connection, and it would be alien to the primary purpose of this book to 
Z 
14 
FREQUENCY-DISTRIBUTIONS 
attempt one, even if the space were available. We will merely indicate in broad outline 
the general approach. 
1.20. We consider1 a function F which is defined at every point in a continuous 
range and is continuous, except perhaps at a denumerable number of points. We require 
that F shall be zero at the lower point of the range (which may be — oo) and a constant N 
at the upper point (which may be + oo) and that it shall not decrease at any point. Such 
a function is called a Distribution Function. It corresponds to the cumulated frequency 
of a frequency-distribution, N being the total frequency; for example, in Table 1.4, 
F(x) = 0 for x < 0, F(x) = 1026 for x = 1 and x < 2, F(x) = 2133 (= 1026 + 1107) 
for x = 2 and x < 3, and so on. Here there are ten points of discontinuity for F(x). 
These points are called " saltuses " (jumps) and F(x) in this case is called a Step Function. 
If there is no saltus in the range, F(x) is continuous and monotonically increasing. 
If it possesses a derivative we have the equation in differentials 
dF = F'{x) dx 
= f(x) dx . . . . . . (1.4) 
corresponding to (1.3). f(x) is called the Frequency Function. The mathematics of this 
branch of the subject is then that of the study of functions of the class F(x) and /(,*:). 
1.21. The functions as thus defined are more general than those arrived at from the 
statistical approach in two ways : (i) F(x) can increase monotonically in part of tho 
range and then possess a saltus, i.e. the frequency may be continuous for a time and then 
suddenly discontinuous—in statistical practice a variate is either continuous or 
discontinuous, never both in different parts of the range ; (ii) where no saltus exists F(x) can 
exist without there existing a frequency function, just as a continuous function need not 
necessarily possess a derivative. In all the cases we shall consider, the existence of a 
continuous variate will be accompanied by the existence of a frequency function. 
The function F(x) is sometimes called a Probability Function, for reasons which will 
become evident in Chapter 7 when we consider the theory of probability. Essentially, 
however, it has nothing to do with probability and we shall use the term " distribution 
function " only. 
1.22. If the discontinuous frequency function is f(x), and F(x) is taken to bo the 
total frequency less than or equal to x, we have 
r 
F{xr) = £f{xs) (1,5) 
In the continuous case 
F{x) = dF 
J a 
= f(x)dx (1.6) 
J a 
where the range is a to b. We now introduce two conventions which simplify these 
expressions to some extent. We shall suppose, unless the contrary is specified, that in these 
mathematical expressions our frequencies are always expressed as proportions of the total 
frequencies, so that the total frequency is unity and the sum or integral over the whole 
range of the frequency function is also unity, i.e. F(b) = 1. Secondly, to avoid the 
NTIKLT.1KK IXTMJRAUS 
15 
».ii»t«iii^ymh, «!,.,„ „f t»„. tiimtH « and h *<• may, witho„1 Iohh of generality, mippora that 
tin ntrt fit) an- /«„, fur urn j- lf.N» thiui «. mid tlmt *V) -~- 1 and fix) « 0 for any a? 
urtnttr lh«u *«, With thi* eomentioti wi< may writ** 
»«d 
V 
no 
V 
/K) 
*'t * ) F{ 
. (L7) 
•<) 1 
f 
/U)t// Ft O f'( *) , 1 
> * 
(1.8) 
\\ hrrr ti i«* iinr^nry to trtk*' wrtiimt of the total frtsjueney A' we may do ho hy multiplying 
h\ A IrKjui-nt u* jfivni hy the fnujucury funrtiou. In our convention F(x) in always 
<i>nfmuoii* i»it i|h' lilt, 
t,£A. Mil' definition ht'twM-n di*»i*untimtim« mid mntinuouN dint rihut ions, though 
n-ul i»»j»I iiii}*Mi!un« jur M<thMind juiipMM"*, is mitiH'thinn of ii numanee in mathematical 
jm*MiiMfi.iu*. mid to avoid the ii«u-pj*hit\ nt f^ttitiii^ nil our theorems twice wo Hhall uho 
<t !\|*' ••! iitii'^tul diir t4i Shrltjt'H. In rttrri. thi-* integral sulwunu'H under one Hummatory 
j.t.M.tih (),»- hull*- •ttiiumatinii diuidti'd hy 2.* mitl the ordinary integral denoted hy . 
,stii*|H»H4-, in hut, tluit Fir) i« ti ihsttihtttioit function an we have defined it. Let yi(.r) 
!*■ «» .MiiitnuiojH huirtiitu in thi« nuttfe of F(.r), uhit-It s\i* will tnk»' in the iirHt itiHtance to 
}«• limti*. «i to t>. \h\it\r I he tantfe into n intervuls ut points tt .»'„, xu ,r,, , . . .i*rt„l7 
• „ J* *l"»»Kr j, in the i«!»«♦' it to jtl 5, io tin- niuni' .ri to j'b, and ho on. ta't 
* *j.?,):A'i«,i *Vn! ■ vusi!/'V.) *V«)| 
It itm\ l»« *ln»wn that a" »he M/i* of thr iuf«*rviiif4 ./; , xr tends to zero uniformly, 
> t«-n»I% to ii limit whirh ih utdrjH't»d»'i»t of the liieiitioii of the points k or of the houndary 
l«'ititi o| th«< ml«*»\<d* \\ «• tlu-n wute thi*. limit 
.■ (i.ii) 
J .1 
\*)'IF 
. (1.10) 
met d« tiiu it a- tin- >tiflt|«-i iu««-r»id o| »,»'") «»'li M'vjMM't to F(.t'). 
,\t, »..r th» »*»-«' ol ntiiui»i\ inti-Kinl'*' Nu' »»»>"Il,,Vk eoiinidiTM and 6 an (ending to infinity 
<l)ld W»|t«\ lot i'\.IUI)ilf 
yU)*tFt 
I' 
pT»«vid*^l tl»f»t the liutit ♦•^i^tH. 
In j«aifii'iil»r, if yi') I, we \\n\r \)u- iliHtrilutthm funetion 
*'l») 
j' 
r/^'. 
16 FREQUENCY-DISTRIBUTIONS 
1.24. If F(x) is the distribution function of a distribution possessing a continuous 
frequency function, the Stieltjes integral becomes the ordinary integral 
f 
J a 
y>(x)f(x)dx, 
and thus includes ordinary integration as a'particular case. If F(x) is the distribution 
function of a discontinuous distribution, that is to say, is a step function, a term such as 
F(xr+1) — F(xr) will vanish unless there is a saltus in the range xr to xr+1. The sum S of 
(1.9) must then tend to the limit (sinoe it does tend to a limit) Eip{xf) f(xr), i.e. to the 
ordinary summation of a series. The Stieltjes integral thus also includes such summation 
as a particular case. 
1.25. Many of the theorems of ordinary integration are true of the Stieltjes integral. 
We shall frequently require the following: 
I rb I rb 
\\ y)dF\< \ \y\dF (1.11) 
IJa I Ja 
< m\ dF 
Ja 
< M (1.12) 
where M is the upper bound of ip(x) in the range {a, b). 
f ydF =y(£)f dF (1.13) 
J a J a 
where £ is a value of a; in the range (a, b). 
If a and b are finite 
\byux)dF = Z\J> wdF> • - • • (li4) 
provided that Efj{x) converges uniformly in the range. The theorem is not necessarily 
true if a and b, or one of them,- are infinite. 
The ordinary rules of partial integration are also applicable to Stieltjes integrals. 
Variate Transformations 
1.26. Suppose we have a new variate £ related to x by some functional equation 
a =#(£), (1.15) 
£ being continuous and differentiable in x throughout the range of x, and vice-versa. We 
have then the equation in differentials 
dx = x'dg =-T-dg (1.16) 
Consequently, for a continuous distribution 
F{x) =r dF = I"* f(x)dx 
J — 00 » J —00 
• -J>»5* 
VARIATE TRANSFORMATIONS 17 
null eoiwetpiently wo may write the dintribution as 
rf*~/W)}^rf$ (1.17) 
i*%|itv**itiK that nn element of frequency between £ - \il£ and £ + |d£ »/{&(£)}~. 
Tin* i<t)tmtititi determining the frequency function may then be transformed as if it were 
*» equation in differentmIk. Such tratiHformationH are important in the theory of con- 
fmutiu* di«!ribiititniH. By their meant* many mathematically specified distributions may 
In* rMticetl to known fornix, either exactly or approximately. 
For example, h dintribution which we hImII have to Htudy in the theory of sampling is 
». •'■(;) 
It in rraulity verified by integration that F(*xj) -.- I. 
tty the triuwformiition * t we reduce thin to 
tiF ! v r ' ('* l (It 0 < f < oo 
'•Q 
ik well known form in ntmlyMH, the dint ribut ion funetion being the incomplete T-funotion 
- ''/:V'i; 
Again, the dmtribution 
(l » 7- 
(.'/• being chown «o that F{ *) 1), a nymmetrical peaked distribution of infinite range 
rutin* like that of Kig. 1.3. may, by the mibntitulion of t ^ y/v tan 0t be transformed 
'/** . ?/V .. '" ~ oo < « < oo 
•j 
tutu 
tlF !l*\r H.r« Of/0 _7r<0<71 
"' Nee"lfl a 2 
■«- (/p\'vCUH* 'flf/0, 
u distribution of tinite range * to | *. but Htill Hymmetrioal. Putting now sin 0 « £, 
we Jmvo 
dF « y,\Vl - $■/* f/$ - 1< f < 1, 
and again I* «* z, ^ 
18 FREQUENCY-DISTRIBUTIONS 
The effect on the range of this last substitution is to be noted. £ ranges from — 1 to + 1, 
and as it does so x ranges from + 1 to 0 and back to + 1. The distribution function of 
the ^-distribution, F(x) from 0 to a, is thus that of the ^-distribution from — £2 to £2. 
Whenever substitutions are made under which there is not a (1, 1) continuous relation 
between the variates, points such as this require some watching. 
1.27. There is 
the distribution ' 
put 
Then 
so that the distribution is transformed into the very simple " rectangular " form in which 
all values of the variate from 0 to 1 are equally frequent. Any continuous distribution 
can be transformed into the rectangular form; and it follows that there exists at least 
one transformation which will transform any continuous frequency-distribution into any 
other continuous frequency-distribution, viz. the transformation which transforms one 
into the rectangular form coupled with the reverse of that which transforms the other into 
the rectangular form. 
The Genesis of Frequency-Distributions 
1.28. Up to this point we have not inquired into the origin of the various observed 
frequency-distributions which have been adduced in illustration. Certain of them may 
be considered apart from any question of origination from a larger population. The death 
distribution of Table 1.12. is an example ; if we are interested only in the distribution of 
male deaths in England and Wales in 1930-32 the whole of the population under 
consideration is before us. 
But in the great majority of cases the population which we are able to examine is only 
part of a larger population on which our main interest is centred. The height distribution 
of Table 1.7 is only a part of the population of men in the United Kingdom living at the 
time of the inquiry, and it is mainly of importance in the light of the information which 
it gives us about that population. Similarly the distribution of farms of Table 1.9 is largely 
of interest in the information it gives about costs of milk production for the whole country. 
1.29. In the two cases just mentioned, height and costs of milk production, we 
have information about a certain sample of individuals chosen from an existing population. 
Only lack of time and opportunity prevents us from examining the whole population. 
It sometimes happens, however, that we have data which do not emanate from a finite 
existent population in this way. Table 1.14 is an example. It shows the distribution of 
throws with dice. 
one variate transformation which is worth special attention. In 
I 
dF = f(x)dx' 
J —00 
/Jot 
dF=f(x)^d£ 
= df 0 < f < 1 . .". . , (1.18) 
MULTIVARIATE DISTRIBUTIONS 19 
TABLE 1.14 
Showing the Number of Successes {throws of 4, 5 or 6) with Throws of 12 Dice. 
(Weldon's data, cited by F. Y. Edgeworth, Encyclopaedia Britannica, 11th ed., 22, 39.) 
Number of 
Successes. 
0 
1 
2 
3 
4 
5 
6 
Frequency. 
• o 
7 
60 
198 
430 
731 
948 
Number of 
Successes. 
7 
8 
9 
10 . 
11 
12 
Total 
Frequency. 
847 
536 
257 
71 
11 
0 
4096 
Now it is clear that, in a sense, we have not in these data got a complete population, 
for we can add to them by further casting of the dice. But these further throws do not 
exist in the sense that the unexamined men of the United Kingdom or the unexamined 
dairy farms of England and Wales exist. They have a kind of hypothetical existence 
conferred on them by our notion of the throwing of the dice. 
Even distributions which appear at first sight to be existent may be considered in 
this light. The trypanosome'distribution of Table 1.13, for instance, was obtained from 
certain tsetse flies. We may consider it as a sample of all the tsstse flies in existence, 
whether harbouring trypanosomes or not—an existent population; but we may also 
consider it as a sample of what the distribution would be if all the tsetse flies were infected 
with trypanosomes—a hypothetical population. 
The population conceived of as parental to an observed distribution is fundamental 
to statistical inference. We shall take up this matter again in later chapters when we 
consider the sampling problem. The point is mentioned here because it will occasionally 
arise before we reach that chapter. It must be emphasised that the distinction between 
existent and hypothetical universes is not merely a matter of ontological speculation—if 
it were we could safely ignore it—but one of practical Importance when inferences are 
drawn about a population from a sample generated from it. 
Multivariate Distributions 
1.30. In the foregoing sections we have considered the members of a population 
according to a-single variate, and the frequency-distributions may thus be called univariate. 
The Work may be readily generalised to include populations of members considered 
according to two or more variates, yielding bivariate, trivariate . . . multivariate frequency 
distributions. Table 1.15, for example, shows the distribution of a number of beans 
according to both length and breadth. The border frequencies show the univariate 
distributions of the beans according to length and breadth separately, -and the body of the 
tajble shows how the two qualities vary together. 
20 
FREQUENCY-DISTEIBUTIONS 
TABLE 1.15 
Showing Frequencies of Beans with specified Lengths and Breadths. 
(Johannsen's data, cited by S. J. Pretorius (1930), Biometrika, 22, 110.) 
Lengths in millimetres (central values). 
1 
rtfVi in -millfmHt,rfts (nftTitral V 
m 
0-126 
8-876 
8-626 
8-375 
8-125 
7-875 
7-626 
7-376 
7-126 
6-875 
6-625 
6-375 
Totals 
17 
4 
2 
6 
16-6 
2 
8 
23 
18 
4 
56 
16 
17 
101 
105 
44 
7 
1 
276 
16-5 
19 
156 
494 
375 
81 
4 
1129 
16 
•3 
03 
674 
956 
386 
66 
6 
2082 
14-6 
23 
227 
913 
871 
236 
23 
1 
2204 
14 
2 
56 
362 
794 
469 
91 
13 
1787 
13-5 
0 
73 
330 
361 
137 
18 
1 
020 
13 
12 
89 
175 
124 
28 
0 
437 
12-6 
3 
10 
66 
78 
35 
8 
1 
100 
12 
3 
27 
37 
25 
21 
2 
115 
11-5 
4 
22 
32 
12 
11 
11 
11 
13 
1 
70 | 36 
1 
10-6 
6 
7 
4 
1 
18 
10 
1 
1 
1 
3 
1 ■ 
7 
9-6 
1 
1 
Totals. 
6 
48 
400 
1483 
2742 
2579 
1397 
630 
170 
72 
10 
4 
9440 
As for the univariate case, the variates may be discontinuous or continuous and we 
sometimes meet cases in which one variate is of one kind and one of the other. 
1.31. In generalisation of the frequency polygon and the iistogram we may construct 
3-dimensional figures to represent the bivariate distribution. Imagine a horizontal plane 
containing a pair of perpendicular axes and ruled like a chessboard into oells, the rulgd 
lines being drawn at points corresponding to the terminal points of olass-intervals. At 
Length of beans ("«* 
Fig. 1.4. Bivariate Histogram of the Data of Table 1.15. 
INDEPENDENCE 
21 
the centre of each interval we erect a vertical line proportional in length to the frequency 
in that interval. The summits of these verticals are joined, eaoh to the four summits of 
verticals in the neighbouring cells possessing the same values of one or the other variate. 
The resulting figure is the bivariate frequency polygon or Stereogram. 
Similarly we may ereot on each cell a pillar proportional in volume to the frequency 
in that cell and thus obtain a bivariate histogram. Pig. 1.4 shows such a figure for the 
bean data of Table 1.15. 
1.32. We may write the bivariate distribution with variates xX) x%} as 
dF =f(xlt xa)dxxdxa . . ... . (1.19) 
With the usual conventions we shall' then have for the bivariate distribution function 
pi pi 
F(xlt x2) = f(x1} xa) dxx dxt 
J — 00 J — 00 
(1.20} 
this integral also'being understood in the Stieltjes sense, reducing to ordinary integration 
if f(xi, xt) is continuous and to ordinary summation if it is discontinuous. 
/ Independence 
1.33. If there are two distribution functions Flf F2, such that 
F(xu x%) = Ftfa) F2(x2) (1.21) 
then xx and x2 are said to be independent. Where frequency functions exist we have 
/(*i. «■) fax dx» = M%i)dxA Mx%)dxz 
J—ooj—oo J—oo J —w 
giving 
/(«i, *0 = fiM /.(»■) (1.22) 
It is readily seen that this definition of statistical independence conforms to the colloquial 
use of the word and also to its mathematical use. The distribution of xt for any fixed xx 
(e.g. the distribution in a row or column of the bivariate frequency table) is tljp same 
whatever the fixed value of xl} that is to say, the distribution of a:s is independent of xx. 
Two variates which are not independent are said to be dependent. Evidently those 
of Table 1.15 are dependent, for the distributions in rows or in columns are far from similar. 
Generally, n variates are independent if 
F(xx . . . xn) = Fl(xl) . . . Fn(xH). 
1.34. Transformations of the variate for hi- and multivariate distributions follow 
the ordinary laws for the transformation of differentials. For example, if 
dF = f(xlt x2) dxx dx2 
Xx = *i(li, Sa) %a = 3?i(fi, £a) 
we have dF=f{xx{$lt £,)> x%{£u |,)}Jrfli,d£ (1.23) 
where J is the Jacobian 
dxx dxa 
d(xx, x%) 
3(fi,"fi)' 
3£i 
Hi 
dx a 
d£ 
<ai 
Ht 
and is to be taken with a positive sign in (1.23). 
22. FREQUENCY-DISTRIBUTIONS 
Consider, for example, the distribution 
dF = z0 exp |- i^ - 2££p + ^)W*Bi - oo < »i, *. < oo . (1.24) 
20) as usual, being chosen so that the total frequency is unity. The variates are evidently 
dependent. . 
Put 
We have 
3(fi, « _ 
d(xlt xs) 
Sx 
it 
1 
OX 
■0 
_ 5L1 _ E^} 
Ox at 
* 
= (1 ~ p*)*-8 
(1 - p*)*.± 
o* 
(1 - ,i)» 
ffj^g 
and 
^ _2p^! + ^==f2 
The distribution then becomes 
dF = 
ZqO'iO'b 
rerp {-*(*} +*■)}<&,#, 
(1 - p")» 
(1 - p2)* 
The transformed variates £t and £, are thus independent. 
(1.25) 
(1.26) 
NOTES AND REFERENCES 
The colleotion of definitions of statistics by Willcox (1935) has already been referred 
to in the text. 
Examples of practical frequency-distributions will be found in most statistical journals, 
particularly Biometrika. 
As to the mathematical basis of the theory of frequency-distributions, there appears 
to be no account in English. The reader who is interested should, however, make a point 
of reading two French works, that by L6vy and those by Er6chet in the Borel Traiti. Both 
these are written from the standpoint of the theory of probability, but the basic ideas of 
the theory of frequency-distributions are the same whether probability is concerned or not. 
Borel, E., Traiti du Galcul des Probabilites, Gauthier-Villars, Paris. A series of brochures 
written under the general editorship of M. Borel. See particularly the two by 
M. Er6chet called " Nouveaux Recherches." 
L6vy, P., Galcul des Probabilities, Gauthier-Villars, Paris. 
Shohat, J. (1929), " Stieltjes Integrals in Mathematical Statistics," Ann. Math. Statist., 
1, 73. 
Willcox, W. E. (1935), " Definitions of Statistics;" Revue de VInst. Int. de Statistique, 3, 388. 
EXERCISES 
23 
EXERCISES 
1.1. Draw frequency polygons or histograms of the following distributions:— 
TABLE 1.16' 
Frequency-Distribution of Successes in Twelve Dice thrown 4096 Times, a Throw of 6 Points 
reckoned as a Success. 
(Weldon's data; loc. cit., Table 1114.) 
Number of Successes . . 
Number of Throws . . 
0 
447 
1 
1145 
2 
1181 
3 
796 
4 
380 
5 
115 
6 
24 
7 and over 
8 
Total. 
4096 
TABLE 1.17 
Frequency-Distribution of Size of Firms in the Food, Drink and Tobacco Trades of Great 
Britain. 
(Final Report of the Fourth Census of Production, 1930, Part III. The table shows the 
number of firms employing, on an average, oertain numbers of persons.) 
Size of Firm 
(Average Numbers 
Employed). 
Number of Firms .. 
11-24 
2245 
25-49 
1449 
50-99 
771 
100- 
199 
439 
TA 
200- 
209 
164 
BLE 
300- 
399 
75 
1.18 
400- 
499 
36 
500- 
749 
54 
750- 
999 
31 
1000- 
1499 
23 
1500 
and over 
29 
Total. 
5316 
Frequency-Distribution of Plots according to Yield of Grain in Pounds from Plots of -5^-0^ 
Acre in a Wheat Field. 
(Meroer and Hall (1911), Jour. Agr. Science, 4, 107.) 
Yield of Grain in pounds 
por ^th Acre. (Central 
value of range). 
Number of Plots 
2-8 
3^ 
15 
3-2 
20 
3-4 
47 
3-6 
63 
3-8 
78 
4-0 
88 
4-2 
69 
4-4 
59 
4-6 
35 
4-8 
10 
6-0 
5-2 
Total. 
600 
24 
FREQUENCY-DISTRIBUTIONS 
TABLE 1.19 
The Percentages of Deaf-mutes among Children of Parents One of whom at hast was a Deaf- 
mute, for Marriages producing Five Children or More. 
(Compiled by G. Udny Yule from material in Marriages of the Deaf in America, ed. E. A. 
Fay, Volta Bureau, Washington, 1898. Where a family fell on the border line between 
two class-intervals one-half was assigned to each.) 
Percentage of 
Deaf-mutes. 
0-20 
20-40 
40-60 
Number of 
Families. 
220 
20-5 
12 
Percentage of 
Deaf-mutes. 
60-80 
80-100 
Total 
Number of 
Familioa. 
G-G 
15 
273 
TABLE 1.20 
Showing the Frequency-Distribution of Fecundity, i.e. the Ratio of the Number of Yearling 
Foals produced to the Number of Coverings, for Brood-mares (Racehorses) covered flight Tim as 
at least. 
(Pearson, Lee and Moore (1899), Phil. Trans., A, 192, 303. Where a case fell on tho border 
between two intervals, one-half was assigned to each.) 
Fecundity. 
1/30- 3/30 
3/30- 5/30 
5/30- 7/30 
' 7/30- 9/30 
9/30-11/30 
11/30-13/30 
13/30-15/30 
15/30-17/30 
Number of Mares 
with Fecundity 
between the 
Given Limits. 
2 
7-5 
115 
21-5 
55 
104-5 
182 
271-5 
Fecundity. 
17/30-19/30 
19/30-21/30 
21/30-23/30 
23/30-25/30 
25/30-27/30 
27/30-29/30 
29/30-1 
Total 
Number of Mnron 
with Foc'.uiHlHy 
bot.woont.lm 
Givon LitfiitH. 
315 
337 
293-5 
204 
127 
49 
19 
2000-0 
\ 
EXERCISES 25 
TABLE 1.21 
ng Numbers of Sentences of given Lengths in Passages from Macaulay's Essays on 
Bacon and on Chatham. 
(From G. Udny Yule (1939), Biomeirika, 30, 363.) 
Length of Sentenoe 
in Words. 
1-5 
6- 
11- ' 
16- 
21- 
26- 
31- 
36- 
41- 
46- 
51- 
56- 
61- 
Number of 
Sentences. 
46 
204 
252 
200 
,186 
'108 
61 
68 
38 
24 
20 
12 
8 
Length of Sentence 
in<Words. 
66- 
71- 
76- 
Sl- 
86- 
91- 
96- 
101- 
106- 
111- 
116- 
121- 
TOTAL 
Number of 
Sentences. 
2 
4 
8 
2 
2 
1251 
I 
TABLE 1,22 
ng the Numbers of Old Egyptian SkuUs with Specified Lengths of the Left Occipital 
Bone in millimetres. 
(From T. L. Woo (1930), Biometrika, 22, 324.) 
Length 
(central values). 
84-5 
86-5 
88-5 
90-5 
92-5 
94-5 
96-5 
985 
100-5 
Frequonoy. 
12 
12 
32 
48 
79 
y lie 
104 
126 
123 
Length 
(control valuos). 
102-5 
104-5 
106-5 
108-5 
110-6 
112-6 
114-6 
116-5 
118-5 
Total 
Frequency. 
74 
68 
36 
18 
7 
4 
4 
1 
864 
I 
20 
FREQUENCY-DISTRIBUTIONS 
TABLE 1.23 
Showing the Number of Women Aborting at Specified Term in Weeks. 
(Erbm T. V. Pearce (1930), Biometrika, 22, 250.) 
Term 
(weeks). 
4 
5 
6 
7 
* ■ 8 
9 
10 
11 
12 
13 
14 
15 
16 
Frequency. 
3 
7 
10' 
13 
14 
29 
22 
21 
18 ■ 
28 
16 
19 
10 
Term 
(weeks). 
17 
18 
10 
2a 
21 
. 22 
23 
24 
25 
26 
27 
28 
TOTAi 
Frequency. 
13 
14 
8 
4 
2 
10 
4 
4 
3 
4 
6 
1 
283 
1.2. Sketch the following curves and, compare their shapes with those of the 
distributions in the previous exercise :— 
_5: 
y = y0e 2 
y = y0e~x 
y = y<&~y 
2/o 
y (l + x*)n 
y = 2/0(l - xfrf> 
y = y0e~xxy 
2/=2/0(1— x*)a 
— OO < X < 00 
1 < X < 00 
y > 1, 0 <#<oo 
n> 0, — oo < # < oo 
a, 6>1, 0<jb<1 
y > 1, 0 < a: < oo 
a < 0, — 1 < x < 1. 
1.3. Show that the following distributions can all be transformed into the type 
dF = y0(l - xf^afl-1 dx 0 < x < 1 
and find the transformations : 
r„(l — r")-*- dr 
dF 
dF « 
-/ Vr,^ 
/ y2\»»+l 
-1<r<1 
— 00 < t < 00 
AF«- 
g«e,'lB 
•Y+v, 
$3 
— 00 < 2 < 00 
(rxe2* +.ra)~2- 
(All these distributions are important in statistical theory. The distribution to which 
they are reduced is oalled the Type I or B-distribution.) 
EXERCISES 27 
1.4. Sketch the stereograms or bivariate histograms of the following distributions: 
,' TABLE 1.24 
( 
Number of Families deficient in Boom Space in 95 crowded London Wards. 
(Census of 1931, Housing Report, p. xxxif.) 
• 
Families deficient by 
1 room 
2 rooms 
3 rooms , 
4 rooms 
Totals 
. Standard Room Requirement (Rooms). 
2 
12,999 
• • 
12,999 
3 
18,198 
3,054 
• • 
21,252 
4 
7,724 
4,479 
310 
•12,513 
6 
2,170 
1,448 
508 
10 
4,136 * 
6 
164 
221 
106 
21 
512 
7 
19 
15 
4 
4 
42 
8 
1 
1 
• • 
2 
• 
Totals. 
41,274 
9,218 
929 
36 
51,456 
TABLE 1.25 
umber of Cows Distributed according to (1) Age in Years and (2) Yield of Milk per Week 
in 4912 Ayrshire Cows. 
(Data from J. F. Tocher (1928), Biometrika, 20B, 106.) 
(1) Ago in Years. 
■ 
) 
1 
) 
■>* 
f 
i 
5 
i 
) 
) 
3 
H 
3 
5 
4 
8 
9 
10 
11 
12 
13 
14 
15 
16 
17 
1* 
1$) 
20 
21 
22 
23 
24 
25 
26 
27 
28 . 
29 
30 
31 
32 
33 
34 
Totals 
3 
3 
2 
2 
9 
11 
11 
15 
16 
11 
10 
8 
3 
6 
1 
2 
3 
112 
4 
2 
5 
10 
26 
76 
76 
116 
149 
148 
146 
117 
97 
63 
42 
19 
20 
10 
7 
2 
1129 
5 
2 
1 
8 
17 
20 
57 
<79 
119 
131 
132 
112 
107 
93 
03 
33 
23 
. 16 
13 
7 
2 
2 
2 
1047 
6 
1 
7 
9 
18 
38 
43 
74 
94 
83 
113 
79 
88 
49 
38 
34 
22 
7 
9 
1 
2 
1 
2 
812 
7 | 8 
1 
3 
1 
5 
9 
23 
34 
59 
58 
73 
87 
69 
70 
45 
38 
27 
17 
4 
5 
4 
4 
636 
4 
2 
9 
24 
23 
34 
49 
61 
51 
40 
32 
27 
19 
20 
15 
5 
2 
1 
2 
419 
9 
1 
4 
4 
7 
11 
23 
32 
39 
35 
25 
31 
14 
13 
8 
2 
4 
1 
3 
2 
276 
10 
2 
1 
6 
8 
16 
15 
22 
33 
30 
29 
181 
17 
9 
10 
4 
2 
1 
223 
11 
2 
1 
1 
4 
4 
9 
12 
17 
11 
13 
9 
10 
12 
3 
3 
2 
2 
3 
2 
122 
12 
1 
1 
1 
2 
6 
7 
6 
6 
10 
10 
7 
3 
7 
2 
4 
3 
75 
13 
3 
1 
4 
6 
5 
0 
m 
8 
4 
1 
1 
1 
1 
32 
14 
1 
1 
2 
1 
3 
3 
2 
2 
15 
15 
1 
1 
1 
1 
1 
2 
7 
16 
1 
1 
2 
17 
1 
1 
1 
1 
4 
18 
1 
1 
Totals. 
1 
5 
13 
33 
71 
151 
236 
339 
499 
652 , 
685 
586 
496 
448 
284 
214 
153 
112 
68 
35 
13 
15 
4 
2 
1 
1 
4912 
28 
1 
, FREQUENCY-DISTRIBUTIONS 
TABLE 1.26 
Distribution of Weekly Returns accqrddng to (1) Call Discount Bates and (2^ Percentage of 
Reserves on Deposits in New York Associated Banks. 
(From Statistical Studies in the New York Money Market, by J. P. Norton. Publications 
of the Department of the Social Sciences, Yale University; The Macmillan Co., 1902.) Note 
that, after the column headed 8 per cent., blank columns have been omitted to save Bpace, 
(1) Call Discount Rates. 
(2) Percentage Ratio of Reserves to Deposits. 
21 
22 
23 
24 
25 
26 
27 
28 
29 
30 
31 
32 
33 • 
34 
36 
36 
37 
38 
39 
40 
41 
42 
43 
44 
46 
Totals 
1 
3 
1 
8 
16 
16 
2 
8 
7 
8 
9 
19 
7 
7 
8~ 
1 
i 
121 
1-5 | 2 
1 
5 
9 
12 
10 
14 
8 
11 
6 
2 
.1 
2 
8 
3 
2 
93 
10 
30 
48 
12 
6 
10 
4 
125 
2-5 
9 
23 
17 
10 
2 
8 
1 
70 
3 
2 
14 
20 
16 
8 
4 
5 
69 
3-5 
1 
6 
12 
11 
3 
4 
2 
1 
40 
4 
2 
W 
15 
7. 
6 
4 
2 
2 
52 
4-5 
6 
12 
17 
3 
3 
1 
1 
46 
5 
4 
16 
19 
7 
1 
■2 
1 
52 
5-5 
4 
6 
9 
1 
20 
6 
11 
11 
9 
2 
35 
6-5 
1 
4 
3 
2 
10 
7 
2 
7 
4 
3 
2 
18 
7-5 | 8 
— 
— 
3 
fl 
1 
• 
10 
0 
l 
2 
4 
10 
2 
2 
■1 
7 
12 
1 
15 
1 
1 
1 
!•!» 
20 
1 
25 |TOTAIxS 
3 
o 
1 
1 | 4 
2 
1 
1 
i) 
42 
K5 
124 
nr> 
KM) 
58 
.1(1 
G3 
32 
14 
14 
10 
0 
11 
21 
15 
10 
10 
1 
1 
22 
7HO 
1.5. Show that the conditions that the function 
f(xl} x%) = 20 exp {Ax\ + mx&% + Bx\}, -co*- Xl, xa < oo 
may represent a frequency function are 
(a) A < 0 
(6) B < 0 
(c) AB - H8 >0. 
Show further that if these conditions are satisfied and the integral of f(xu x%) between 
— oo and oo for both variates is unity, then 
1 
71 
-A H 
H -5 
# 
CHAPTER 2 
MEASURES OF*LOCATION AND DISPERSION 
2.1. It has been seen in Chapter 1 that the frequency-distributions occurring in 
statistical practice vary considerably in general nature. Some are finite in range and 
some are not. Some are symmetrical and some markedly skew. Some present only 
a single maximum and others present several. Amid this variety we may, however, 
discern four general types : (a) the symmetrical distribution with a single maximum, 
such as that of Table 1.7 ; (b) the asymmetrical distribution, or skew distribution, with 
a single maximum, such as those of Tables 1.8 and 1.9 ; (c) the extremely skew, or J-shaped, 
distribution, such as that of Table 1.2; and (d) the U-shaped distribution, such as that 
of Table 1.11. To make this classification comprehensive we should have to add a fifth 
class comprising the miscellaneous distributions not falling into the other four. 
The distributions with a single maximum will .hereafter be called " unimodal.'' The 
synonymous terms " cocked-hat," " single-humped " and one or two others also occur in 
statistical literature. 
2.2. It frequently happens in statistical work that we have to compare two 
distributions. If one is unimodal and the other J-shaped or multimodal a concise comparison 
is clearly difficult to make, and in such a case it would probably be necessary to specify 
both distributions completely. But if both are of the same type (and it is in such cases 
that comparisons most frequently arise) we may be able to make a satisfactory comparison 
merely by oxamining their principal characteristics ; e.g. if both are unimodal it might 
bo Hiifficient to oompare (a) the whereabouts of some central value, such as the maximum 
—this, as it were, locates the distributions; (b) the degree of scatter about this value 
—the dispersion; and (e) the extent to which the distributions deviate from the 
symmetrical—the skewness. 
The same point emerges when our distributions are specified by some mathematical 
function. If, for example, wo have two distributions of the type 
(j—m)' 
dF —y*e~ «» dx, 
symmetrical about x = m, a complete comparison can be made try comparing the value 
of the constants ra and v in the distributions. Such constants are called parameters of 
the distribution. This chaptor is devoted to a discussion of parameters of location and 
dispersion. 
Meaauren of Location: the Arithmetic Mean 
2.3. There are throe groups of measures of location in common use : the means 
(arithmetic, geometric and harmonic), the median and the mode. We consider them in turn. 
The arithmetic mean is perhaps the most generally used statistical measure, and in 
fact is far older than the scienoe of statistics itself. If the proportional frequency of the 
values a; of a distribution is f(x), the arithmetic mean fa ajbout the point x = a is defined by 
/•OO 
ju,\ = (a; — a)f(x) dx 
J — 00 
= f" (x -a)dF (2.1) 
J —00 
29 
30 MEASURES OF LOCATION 
This integral is to be understood in the Stieltjes sense and hence includes summation in 
the discontinuous case -, e.g. the arithmetic mean of a set of discrete values x is their sum - 
divided by the number of values. In formula (2.1) Ijhe frequency, in accordance with 
our usual convention, is expressed as a proportion of the total frequency. If the actwil 
frequencies are g(x), totalling N, we have 
in the continuous case, and 
If00 
1\ j oo § 
in the discontinuous case. The value of the arithmetic mean thus defends on the valno 
of a, the (point from which it is measured. For a mathematically specified distribution 
the integral (2.1) need not necessarily converge, in which case no arithmetic mean exists. 
2.4. The calculation of the arithmetic mean of a numerically specified distribution 
(i.e. one whose frequency-distribution is given in the form of a numerical table like those 
of Chapter 1) is a simple* process. If there are relatively few values in the population 
we merely sum them and divide by their total number N. If they are given in the form 
of a frequency table a more formal procedure is desirable, but the principle is exactly the 
same. The following example will make the process clear. 
Example 2.1 
To calculate the arithmetic mean of the population of males distributed according to 
height of Table 1.7. 
Let us note first of all that if b is some other arbitrary variate-value, 
f" 
Hi (about a) = (x — a) dF 
J — 00 
= f (x - b) dF + f (b - a) (IF 
J —00 J —00 
= fi\ (about b) + b — a . . . . (2.2) 
In other words, we can find the mean about any point very simply when wo know 
the mean about any other. In calculating the arithmetic mean we can then take an 
arbitrary point as origin and transfer to any other desired point afterwards. It ia generally 
convenient to choose this arbitrary point somewhere near the maximum frequency. 
One further point arises in grouped data such as this. We do not know exactly the 
. variate-values of the individuals wjthin a certain class range. We therefore assume them 
concentrated at the centre of the interval. Corrections for any distortion thus introduced 
will be considered in Chapter 3. In fact, no correction is required for the arithmotio 
mean in the case when the frequency " tails off " at both ends of the distribution. 
Li the particular case before us we take an arbitrary origin at the centre of the interval 
67- inches, i.e. at the point 67-j2g- inches, and measure f (= x — a) from that point. Column 2 
in Table 2.1 shows the frequency, column 3 the value of $ and column 4 the value of £/. 
We find, having due regard to sign. 
£(tf) = 8763 - 8584 = 179. 
179 
Hence the mean, about x = 0 is 67-^ + —— = 67-46 inches. 
8585 
\ 
THE ARITHMETIC MEAN 
SI 
TAHLK 2.1 
Calculation of the Arithmetic Mean for the. Diatrihutian of Table 1.7. 
\\\ I 
; iii'whi, 
1 tlH'lHW, 
i 
1 
R7 
ft* 
A» 
IHI- 
nt- 
m 
iW- 
fl4 
«wv 
1 
tVT 
1 «H 
«» 
* 7»> 
1 -i 
1 '* 
72 
73 
7ft 
1 7rt 
1 77 
(2) 
KnHjumu'y 
'• 
2 
4 
U 
41 
Hit 
Iiitl 
3114 
IMfl* 
WW 
1223 
laau 
1230 
Kiti:i 
tuti 
ana 
2**2 
7» 
:*2 
in 
n 
<3) 
from Arbitrary 
Vuluo 
10 
- y 
*- H 
- 7 
- 11 
- ft 
_. 4 
- 3 
.- 2 
- 1 
0 
J 1 
i o 
* ii 
i 4 
» ft 
« It 
i 7 
< H 
■ it 
» n> 
Product 
v. 
20 
36 
112 
2H7 
4 OK 
H4fl 
1«7» 
2007 
WHO 
1223 
NftH4 
T«tTAt* 
MftHft 
12.10 
212(1 
103* 
1RUH 
KltO 
474 
224 
12H 
4ft 
20 
f« H7H3 
Kimnple '3.2 
K«»r n, iliMtriloHltm H|K«Htii'*i lty n. mitlhemitticnl function, th<* determination of the* 
tiiMiit »•* a iitnff«*r of i«vnhm(iti)J! tin* int**^rnl (2.1), when it oxtHtn. For inHtanuo, to find 
th«' tticiut tif tin* <tt*trilmtiim 
1 
w*i hnvo 
1 f1 
k (1 4-)" 'u^r/j: 
0 s. jt ^ I 
1» * q 
32 MTOARTTRER OF LOCATION 
2.5. Apart from its relative simplicity and ease of calculation, qualities which ensure 
it a firm place in the elementary theory of statistics, the arithmetic mean has a number 
of properties which, make it equally important in advanced theory. For instance :— 
(a) If in (2.1) we take a equal to /*[ itself the mean vanishes and consequently the 
sum over the population of deviations from the arithmetic mean is zero. 
(6) The mean of a sum is the sum of the means ; i.e. if flt ft . . . /„ are the frequency 
functions of n distributions with means fi[, v\ . . . p{, and if the sum of the frequency 
functions is g with mean 0', then 
-00 
0' = (x — a) g(x) dx 
J — 00 
= r (* -a){Jx{x) +/,(*) + . ... +£(*)}& 
J —00 
-oo /"» poo 
= (x — a)fx{x) dx + \ (x - a)f9(x) dx + . . . + (x — a)fn(x) dx 
J _oo J —oo J —oo 
= A + v'l + • • • + Pi' 
(c) We shall see later that mean values are important in the theory of sampling, 
mainly in virtue of their mathematical tractability, but also because in a certain soniso 
the mean is the best measure of location of some distributions. 
The Geometric Mean and the Harmonic Mean 
2.6. Two other types of mean are in use in elementary statistics, though they are 
not of importance in advanced theory. 
The geometric mean of N variate-values is the 2Vth root of their product and is not usod 
if any of the variate-values are negative. For proportional frequencies f(x) we have 
0= fl (xfi) \ 
or log# = E frlogxA 
/=— 00 / 
and for actual frequencies g(x), totalling N, 
Q = II{xfi)N 
log# =-Zgj\ogxj 
(2.4) 
N 
The harmonic mean of N variate-values is the reciprocal of the arithmetic mean of 
their reciprocals. In the usual notation 
1 f dF f00 f(x)dx 
or, for actual frequencies, 
£_JLp g(x)dx 
S~N}_^ x ... 
Example 2.3 
To find the geometric and harmonic means of the distribution 
dF = B&q){1 ~X)P~l ^~ldx ° < * < 1 
we ^ve log G = 5j^jJo(l - *)p_1 a8-1 log x dx. 
(2.6) 
*, 
THS U ISOMETRIC MEAN AND HARMONIC MEAN 33 
Now, nln«» by definition 
f (I J-)'1 l jt» lth> - tt(p, q) 
Ju 
we have, differentiating iwth Hide* with rwpect to $r, an oporation which is 
legitimate iti vtriuo of tho uniform convergent of tho integral and the oxiatonco of the resulting 
«*x}>re««Hm*, 
£<I - jt)" l j* ' log .rife « */*(?, ry). 
Thtw **« 'ml,)**** 
~^{N/'<</) In«rOM 7)}. 
The harmonic mean i« given by 
1 * f (I r V l J"*' ,Jr/r 
„ /*(/'.</ 1) /'(</ 1) I\P 1-7) 
/'(/>.</) Hi' t tf - 1)' iV/) 
</ 1 
** that // H l . 
We mitv note that the nrithmelie mean, ' , in ureal it than the* harmonic moan, for 
/' ' 7 
V ! /' '/ l J 1> 
p i q p * </' /' • V I /i | 7 - 1 
mid ther>'fura p\ * // 
7' ■ y i /' * y 
whieh i* elearly w>. 
i.7. tit general it may 1m* ftlumu that fur liistrihutiotiM in whieh the variate-valuoH 
Mi" I»*«t Iti'lJuttVl* 
// - . ti . /i, (2.7) 
<'*ijiH|ihr tu hut the ijimnlity 
/■'(') 1.(^1 ! •'' J • ■ • J".V) ' 
where the r'» are j>o*itive tiiiiulierrt. We shall »hitw that this in an increaHing function 
ol t. i*\ Kiit) - A'(/g) if/» ■ /,. Ah a trivial raw them* inequalities may bo replaced by 
equalities namely if all the Sh are etmal. We havo 
J.'"«iviy + )'''"« iV- 
A.*. 
34 MEASURES OE LOCATION 
Hence, for the function 
~d 
at 
we have 
At it & dl{dt j 
. =41 log ^ 
- *J2p log* x)XW-{2{(if tog *)}*] • • • <2-fi) 
(Ux1)2 
Now in virtue of Schwarz's inequalityZ{a*)Z{W) > {Z(ab)}* the expression in brackets is 
not negative. Hence ^ has the sign of I and F thus has a minimum at t = 0. But 
ft 
when < = 0,.F = 0 and thus F must be non-negative. Therefore -^ log -E7 is non-nogativo, 
and since E is positive ^- is non-negative and thus E is a non-decreasing function. 
Now in E(t), when t = 1 we have the arithmetic mean; when t = — 1 we have tho 
harmonic mean; and when t —> 0 we have the geometric moan, for 
log^) 
lim log J0 = lim 
i—>-0 
=f lim -^Saj' log x 
= irioga:. 
Hence the inequality (2.7) follows. 
Eor simplicity we have stated these results for the discontinuous variato. Tho analysis, 
however, is easily seen to remain true for Stieltjes integrals and honcci in generally valid. 
Hereafter when the " mean " is mentioned without qualification, the arithmetic mean 
ris to be understood. 
The Median 
2.8. The median value is that value of the variate which divides tho total frequency 
into two equal halves, i.e. is the value /ie such that 
P 'f{x) dx = f f{x) dx = \ (2.i>) 
There is some small mdeteiminacy in this definition when the distribution is discontinuous 
which may be removed by convention. If there are (2N + 1) members of tho population, 
we take the median to be the value of the (# + l)th member. If there aro 2AT wo take it 
to be halfway between the values of the Nth. and the (N + l)th, When tho distribution 
is numerically specified in class-intervals there is the usual indeterminacy duo to grouping, 
which may be dealt with in the manner of the following example. 
MKMAK AND MODE 
35 
KxnmpU 2 4 
To find the mod inn value of the dint rihut ion of height* considered in Example 2.1. 
Half the lot*! frequency of HAM observation* in 421»2-n, 
There are, up to nnd including the interval, fiUjJ inche* JWhD 
leaving ^lilt'fl 
The frequency in the next interval i* 1320 
Hence we taJkc the median to Ik* 
m\l * Tl?»o "W"*? inche*. 
The mean (Ksampte 2,1) i* 07'4<l inelim, practically the «nmo, 
A graphical method of determining the meiiinn in given Inter in thin chapter (2.13). 
2.9. The mode or modal value in that value of the variute exhibited by the greatest 
mimltcr »f memficr* «f the dist rihut ion. if the frequency function in continuous and 
different iahle it in the solution of 
f'U) *J\*\ «»./%«•) (£VW':n • * .(2-l») 
H/V) vanishes and /'V) i* greater than zero we have it. Minimum, and «meh a point lit 
sometime* called an Antimode. 
!u uumt'rit'ully *pecittcd ilintrihutintiM and discontinuous distrihution* generally the 
mode if* sometimes difficult to determine exactly. It in essentially a concept related to 
the routiuuou* frequency function. Fur example, if the distribution merely consist* of 
an isolate*! numln'r ttf value*, ciich of winch occur* only once, there in no m»«le in the 
sense defined iilmve. Where, however, the huiuIht is large enough to jiermit grouping, 
then- ttill usually 1*» an interval containing u maximum frequency, anil we may regard 
the mode at lying in that interval. More generally there may he several maxima, in 
which i'iM' the dint rihut ion in multimolal, In the height distribution of Table 1.7, for 
instance, the mihIc may !m« considered as J>ing somewhere in the interval 117 inches. 
To estimate ttn |h wit ton more accurate!) it i* necessary to tit it continuous curve to the 
distribution and determine the mode of the curve. The process of fitting will 1h« considered 
in Chapter «. 
2.10. In a symmetrical diMiihufiou the mean, the median mid the mode (or in 
rases Hiich a* the r shajicd distribution the notinc'di-i coincide, For «*Kev\ distribution* 
they differ. There i* an iuleivfiug euipuical relationship f«<twecn the three quantities 
which apjs'ar* to hold fur tiuuuodu] hiiwh of moderate asymmetry, namely 
Mean Mode It (Mean Mediiiu). . . . (2.11) 
A mathematical explanation of thin relationship ha* l»cen given ley latodson (1IU7), 
It ih a useful mnemonic to ohscne that the mean, median and mode occur in the same 
order (or the reverse order) an in the dietionary : and that the median is nearer to the mean 
than to the mode, just an the con en |u aiding word* are nearer together in the dictionary. 
In elementary theory the median and the mode have considerable claim* to use an 
measure* of locution. They are readily interpretiiblc in term* of ordinary idea* the 
median i* the middle value and the mode i* the most popular value and the median 
in usually more easily determined than the mean in numerically imeiuttcd drntribtittomt. 
38 MEASURES OF LOCATION 
What gives the arithmetic mean the greater importance in advanced theory is its superior 
mathematical tractability and certain sampling properties; but the median has 
compensating advantages—it is, for instance, less dependent on the scale and the form of tho 
frequency-distribution than the mean—and it seems to deserve more consideration in tho 
advanced theory than it has received. 
= \ dF (2.12) 
J —00 
2.11. The concept of median value can be easily extended to locate' the curve more 
accurately by the use of several parameters. We may, for example, find the three variate- 
values which divide the total frequency into four equal parts. The middle one of these 
will be the median itself; the other two are called the lower and upper quartiles respectively. 
Similarly, we may find the nine variate-values which divide the total frequency into ten 
equal parts—the deciles. Generally we may find the (n — 1) varjate-values whioh divide 
the total frequency into n equal parts—the quantiles. Evidently the knowledge of tho 
quantiles for some fairly high n, such as 10, gives a very good idea of the general form 
of the frequency-distribution. Even the quartiles and the median are valuablo general 
guides. 
2.12. The determination of the quantiles of a numerically specified distribution 
proceeds as for the median, indeterminacies being resolved by the usual conventions. 
That of tjhe quantiles of a mathematically specified distribution, say the j/'th quantilo, 
is a matter of solving the equation 
i 
n 
which can be done without difficulty by interpolation when the integral of dF has boon 
tabulated. 
Example 2.5 
To find the quartiles of the height distribution considered in Example 2.1. 
One-quarter of the total frequency is' 8585/4 = 2146-25 
Up to the interval 65- there are 1370 members 
leaving 770-25 members 
In the next interval there are 990 members 
77fl-9fl 
Thus the lower quartile is 64f£ + -^- = 65-71 inches 
The upper quartile will be found to be 69-21 inches 
We have already found (Example 2.4) that the median is 67-47 inches 
Denoting the quartiles by Qx and Q3 we see that 
Qi — Ve = ~ l*76 inches 
' & — /"„ = 1-74 inches 
so that the median is almost half-way between the quartiles, an indication of the symmetry 
of the distribution. «. 
The Distribution Curve or Ogive of Oalton 
2.13. The Quantiles may also be determined graphically. Suppose we plot x the 
vanate, along a horizontal axis and 2f(x), the cumulated frequency up to and including 
THE DISTRIBUTION CURVE 
37 
to 
80 
70 
-8 eo 
50 
& 
c 
•8 30 
J 
20 
(0 
—- > 
mm 
10 12 14 16 18 
Annual Income (£000) 
20 
22 
24 
26 
28 
30 
8585 
Fia. 2.1. Distribution Curve of the Data of Table 1.2. 
x, along the perpendicular y-axis. We then get a series of points through which, in general, 
a smooth curve may be drawn. This curve, as is evident from its definition, is 
y = F(x), 
i.e. the graph of the distribution function. It is sometimes oalled the graduation curve, 
or Galton's ogive (though 
it is only shaped like an 
ogive in certain cases 
such as that of a uni- 
modal symmetrical 
curve). We shall use 
the expression 
"distribution curve." 
Kg. 2.1 illustrates 
the distribution curve 
for the J-shaped 
distribution of Table 1.2, and 
Fig. 2.2 that for the 
unimodal symmetrical 
distribution of Table 1.7. 
A freehand curve has 
been drawn in both 
cases. 
Curves of this kind 
can be used to 
determine the quantiles. In 
fact, to find the median, 
we merely have to find 
■the abscissa 
corresponding to the ordinate N/2, 
1 
Frequei 
Frequei 
Frequei 
icy- 643 
7cy429 
icy.zm 
B-75 
15 
■zs 
1 
1 
/ 
1 
Q, 
t 
/ 
/ 
j 
1 
i 
M« 
/ 
/ 
Q, 
t 
/ 
\ 
58 
60 
6* 
70 
72 
74 
64 06 68 
Heiqht (inches) 
Fig. 2.2. Distribution Curve of the Data of Table 1.7. 
(Heights shown to correspond to entries in the Table, e.g. cumulated 
frequenoy at 64 inohes is the frequency up to and including the range 
64- and therefore up to 64-JJJ- inches.) 
76 
38 MEASURES OP DISPERSION 
and so on The positions of the quartiles and the median are shown in Fig. 2.2, and 
the reader may care to compare the values obtained by reading the graph by eye with 
those given in Example 2.5. 
Measures of Dispersion. 
2.14. We 'now proceed to consider the quantities which have been proponed to 
measure the dispersion of a distribution. They fall into three groups :— 
(a) Measures of the distance (in terms of the variate) between certain representative 
values,'such as the range, the interdecile range or the interquartile range. 
(b) Measures compiled from the deviations of every member of the population from 
some central value, such as the mean deviation from the mean, the mean deviation from 
the median, and the standard deviation. 
(c) Measures compiled from the deviations of all the members of the population among 
themselves, such as the mean "difference. 
In advanced theory the outstandingly important measure is the standard deviation; 
but they all require some mention. 
» 
Range and Interguantile Differences 
2.15. The range of a distribution is the difference of the greatest and least variate- 
values borne, by its members. As a descriptive parameter of a population it has very little* 
use. A knowledge of the whereabouts of the end values obviously tells little about the 
way the bulk of the distribution is condensed inside the range; and for dintributionH of 
infinite range it is obviously wholly inappropriate. 
More useful rough-and-ready.measures may be obtained from the quantiles, and there 
are two such in general use. The interquartile range is the distance between the upjxn' 
and lower quartiles, and is thus a range which contains one-half the total frequency. 
The interdecile range (or perhaps, more accurately the l-9th interdecile range) in the distance 
between the first and the ninth decile. Both these measures evidently give Home 
approximate idea of the " spread " of a distribution, and are easily calculable. For thw reason 
they are fairly generally used in elementary descriptive statistics. In advanced theory 
they suffer from the disadvantage of being difficult to handle mathematically in the 
theory of sampling. 
Mean Deviations 
2.16. The amount of scatter in a population is evidently measured to some extent 
by the totality of deviations from the mean. We have seen (2.5) that the sum of these 
deviations taken with appropriate sign is zero. We may however write 
/•OO 
<5i = \x-ju\\dF 
J —00 
(2.1») 
where the deviations are now taken absolutely, and define dx to bo a coefficient of dinnerNion 
We shall call it the mean deviation about the mean. 
Similarly for the median jue we may write 
^ * J _ J * - **a I **" (2.14) 
and call <32 the mean deviation about the median. ' 
In future the words "mean deviation " alone will be taken to refer to the mean 
deviation about the mean. 
STANDARD DEVIATION' 39 
Both these measures have merits in elementary work, being fairly easily calculable. 
Once again, however, they are practically excluded from advanced work* by their 
intractability in the theory of sampling. 
Standard Deviation 
2.17. We have seenythat the mean about an arbitrary point a is given by 
<"O0 
A*i = 1 (a — a) dF. 
J — 00 
We may, by analogy with the terminology of Statics, oall this the first moment, and define 
the second moment by 
n'2 = f (x-a)*dF (2.15) 
J — 00 
The second moment about the mean is written without the prime, thus: 
poo 
/"«= (x- ti[)*dF (2.16) 
J — 00 
and is called the Variance. The positive square root of the variance is called the standard 
deviation, and usually denoted by.a, so that we have 
o => + -v^,, . . . . ■. (2.17) 
The variance is thus the mean of the squares of deviations from the mean. The device 
of squaring and then taking the square root of the resultant sum in order to obtain the 
standard deviation may appear a little artificial, but it makes the mathematics of the 
sampling theory very much simpler than is the case, for example, with the mean deviation. 
The calculation of the variance and the standard deviation proceeds by an easy 
extension of the methods used for the mean. In particular, if b is some arbitrary value 
/4 (about a) = \ (x — a)2dF 
{(x - 6)2 + 2(6 - a){x -&) + (&- a)*}dF 
= /«:2 (about 6) + 2(6 - a)fi[ (about 6) + (6 - a)2* . (2.18) 
If now b is the mean we have 
fh = IH + (/»i — aV 
ot < fia = /*i — (a*i — a)a • ■ • * • (2-l9) 
Thus the variance can easily be found from the second moment about an arbitrary point, 
which can bo selected to simplify the calculations. 
TSxamjile 2.6 
To find the mean deviation and the standard deviation for the distribution of men 
according to height considered in Example 2.1 (Table 1.7). 
In the case of the mean deviation for a grouped distribution, the sum of deviations 
should first be calculated from the centre of the class-interval in which the mean lies and 
then reduced to the mean as origin. It so happens that in Table 2.1 the mean fell in the 
interval taken as origin, so. that the preliminary arithmetic already exists in the Table. 
The sum of positive deviations is 8763 and that of negative deviations — 8584. 
Hence the sum of deviations regardless of sign is 17.347, the unit being the class-interval 
and the origin the centre of the interval. 
-r 
40 
MEASURES OE DISPERSION 
To reduce to the mean as origin, we note that if the number of observations below 
the mean is Ni and the number above the mean is Ni} and d = fi\ — a, we have to add Nxd 
to the sum of deviations about the centre of the interval and subtract Nad. In this case 
d = 0-02 (Example 2.1), Nt = 4918, iVB = 3667. Hence we add (4918 - 3667)0-02 = 25. 
Hence the mean deviation 
. 17,347 + 25 0 AO . , 
#i = — 0 ' = 2-02 niches. 
8585 
For the standard deviation some further calculation is required, as shown in Table 2.2. 
TABLE 2.2 
Calculation of the /Standard Deviation for the Distribution of Table 1.7* 
(Some preliminary calculation already carried out in Table 2.1.) 
(1) 
Height, 
inches. 
57- 
58- 
59- 
60- 
61- 
62- 
63- 
64- 
66- 
- 
67- 
68- 
69- 
70- 
71- 
72- 
73- 
74- 
76- 
- 
77- 
TOTALS 
(2) 
Frequency 
/• 
2 
4 
14 
41 * 
83 
169 
394 
669 
990 
1223 
1329 
1230 
1063 
646 
392 
202 
79 
32 
10 
5 
2 
8585 
<.3>. 
Deviation 
£.. 
- 10 
- 9 
— 8 
- 7 
- 6 
- 5 
- 4 
- 3 
- 2 
-' 1 
0 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
— 
(4) 
ey. 
200 
324 
896 
2,009 
2,988 
4,225 
6,304 
6,021 
3,960 
1,223 
0 
1,230 
4,252 
6,814 
6,272 
5,050 
2,844 
1,568 
1,024 
406 
200 
56,809 
Column (4) shows the sum E£zf, where / is the actual frequency. We then have, for 
the second moment about the arbitrary origin 
56,809 
fz2 = 
8585 
6-6172. 
We have already found in Example 2.1 that 
179 
th 
a = 
8585 
= 0-0209. 
SHEPPARD'S CORRECTIONS 41 
Hence, in virtue of (2.19) 
fzt = 6-6172 - (0-0209)3 
= 6-6168, 
a = VPs — 2-57 inches. 
It may be noted that the mean deviation is about 80 per cent, of the standard deviation. 
This relationship often holds approximately for unimodal curves approaching symmetry. 
The reason will become apparent when we study the so-called " normal" distribution in 
Chapter 5. 
Example 2.7 
To find the variance of the distribution * 
dF = -st^—,(1 - a?)""1 afl-1 dx, 0 < x < 1. 
We have, about the origin, 
= B(p, q + 2) = (g + l)g 
B(p, q) (p + q + l)(p + q)' 
We have already found (Example 2.2) that 
q 
F p + q 
Thus t ^ = jn'2 — f.i'i2 
(q + i)q q* 
{p + q + l)(p + q) (p + qy 
M 
. (p + q + i)(p + q) 
r 
Sheppard's Corrections 
2.18. The treatment of the values of a grouped frequency-distribution as if they 
were concentrated at the mid-points of intervals is an approximation, and in certain 
circumstances it is possible to make corrections for any distortion introduced thereby. 
These so-called " Sheppard's corrections " will £e discussed at length in the next chapter, 
but at this stage we may indicate without proof the appropriate correction for the second 
moment. 
If the distribution is continuous and has high order contact with the.variate-axis 
at its extremities, i.e. if it " tails off " slowly, the crude second moment calculated fi'om 
grouped frequencies should be corrected by subtracting from it A2/12, where h is the width 
of the interval. For example, in the height data of Example 2.6, we have h = 1, and the 
corrected second moment is 
6-6168 - 0-0833 =* 6-5335. 
The corrected value of a is -y/Q'5335 = 2-56, as against an uncorrected value of 2-57. 
42 MEASURES OF DISPERSION 
Mean Difference 
2.19. -The coefficient of mean difference (not to be confused with mean deviation) 
is defined by 
^-f f \x - y | dFfr) dF(y) 
J —bo J —oo 
-f r I*^y!/(*).%)**«fc • • • .(2.20) 
J —-co J —oo 
In the discontinuous case two different formulae arise. We have either 
A*=mN-V\ £ £ \*i-*k\f(*t)f(*k)> 'i** - ■ (2-21) 
^ ^ ' ^— — oo k—— oo ' 
the mean difference without repetition, or 
*l~T* ^ £ l*t-*k\f(to)f(*k), • • • (2-22) 
jv — OO Aj—— 00 
the mean difference with repetition. The difference lies only in the divisor and is 
unimportant if N is large. 
The mean difference is the average of the differences of all the possible pairs of variate- 
values, taken regardless of sign. In the coefficient with repetition each value is taken 
with itself, adding of course nothing to the sum of deviations, but resulting in the total 
number of pairs being iVa. In the coefficient without repetition only different values are 
taken, so that the number of pairs is N(N — 1). Hence the divisors in (2.21) and (2.22). 
2.20. The mean difference, which is due to Gini (1912), has a certain theoretical 
attraction, being dependent on the spread of the variate-values among themselves and 
not on the deviations from some central value. It is, however, more difficult to compute 
than the standard deviation, and the appearance of the absolute values in the defining 
equations indicates, as for the mean deviation, the appearance of difficulties in the theory 
of sampling. It might be thought, that this inconvenience could be overcome by the 
definition of a coefficient, 
E*= f f (x-y)*dF(x)dF(y). 
J —00 J —00 
This, however, is nothing but twice the variance. 
For E* = p f dF(x) dF(y) {<ra - 2xy + y*} 
J —ooJ —oo 
= r x* dF{x) \ dF(y) - 2 f x dF{x) C y dF{y) 
J —00 J —00 J —00 J —00 
+ p dF(x)C y*dF(y) 
J —00 J —00 
= 2,4 - 2/*;> 
— 2/u* , . (2.23) 
This interesting relation shows that the variancp may in fact be defined as half the 
mean square of all possible variate differences, that is to say, without reference to deviations 
from a central value, the mean. 
CONCENTRATION 43 
> 
Coefficients of Variation: Standard Measwre 
2.21. The foregoing measures of dispersion have all been expressed in terms of units 
of the variate. It is thus diffioult to compare dispersions in different populations unless 
the units happen to be identical; and this has led to a searoh for measures which shall 
be independent of the variate,scale, that is to say, shall be pure .numbers. 
Several coefficients of this kind may be constructed, such as the mean—em& on or 
mean 
mean deviation /-.,,,, , , . 
ivied"'" ~~"' ^^y *w0 "ave Deen used at all extensively in praotice, Karl Pearson's 
coefficient of variation, defined by 
a 
v = 100-r (2.24) 
H ' 
and Gpii's coefficient of concentration, defined by 
0 = f^l (2-26) 
Both these coefficients suffer from the disadvantage of being affected very much by filt 
the value of the mean measured from some arbitrary origin, and are hardly suitable for 
advanced work. 
2.22. For our purposes, comparability may be attained in a somewhat different way. 
Let us take a itself as a new unit and express the frequenoy function in terms of a new 
variable f rolated to x by 
£ = ILZJL (2.26) 
a 
Any distribution expressed in this way has zero mean and unit variance. It is then said 
to bo expressed in standard measure. Two distributions in standard measure can be readily 
compared in regard to form, skewness, and other qualities, though not of course in regard 
to mean and variance. 
Concentration 
2.23. Gini's coefficient of concentration arises in a natural way from the following 
approach : — 
Writing, as usual 
F(x) = \X f(x) dx (2.27) 
J —00 
let us define 
0(x)±K\X xf(x)dx (2.28) 
i"lJ-00 
0(x) exists, of course, only if /u[ exists. Just as F{x) varies from 0 to 1, &(x) varies from 
0 to 1 provided that the origin is taken to the left of the start of the frequency-distribution, 
which we shall assume to be so. <P{x) m"ay be called the incomplete first moment. 
Now (2.27) and (2.28) may be regarded as defining a relationship between the 
variables F and (P in terms of parametric equations in x.* The curve whose ordinate 
* Tho definition of curves by parametric equations will be found treated in most textbooks of 
differential calculus. The term " parameter " in this connection is usual in mathematics, but is not 
to be confused with the more special statistical parameter as defined in 2.2. 
44 MEASURES OF DISPERSION 
and abscissa are 0 and F is called the curve of concentration. Such a ourve is shown 
in Fig. 2.3. 
/ 
Fig. 2.3.—Curve of Concentration. 
The curve of concentration must be convex to the J^axis, for we have 
, d& _ xf(x) 
^ dF f(x) 
which is positive since our origin is taken to the left of the start of the distribution. 
, d*& dx 1 ... 
Thus the tangent to the curve makes a positive acute angle with the J^-axis, and the angle 
increases as F increases ; in other words, the curve is convex to the J^-axis. 
The area between the concentration curve and the line F = 0 is called the area 
of concentration. We proceed to show that it is equal to one-half the coefficient of 
concentration. 
In fact, we have from Fig. 2.3 
2 (area of concentration) = \ F d0 — \ 0 <£F 
and thus ° f0o ° 
2^ (area) = F(x)x dF(x) - p\ '0{x) dF(x) 
J—oo J-oo 
■ = r x dF(x) r dF(y) - f dF(x) r y dF(y) 
J—oo J—oo J—oo J—oo 
= r r (x-y) dF(x) dF{y). 
r J —00 J —00 
. I (x — y) dF{x) dF(y) = 0, and hence 
00 J —00 
2^ (area) = |f f° f (x - y) dF(x) dF{y) + [ | (y - x) dF(x) dF(y)\ 
LJ-ooJ-oo J-ao J x -I 
= *r r \*-V\ dF(x)dF(y) 
J —00 J —00 
2' 
CONCENTRATION 45 
ThuB 
area of concentration = - —^- = -G, the coefficient of concentration. 
2 2^! 2 
• 2.24. Various methods have been given for calculating the mean difference. The 
following is probably the simplest, particularly for distributions specified in equal group- 
intervals. 
Let us, without loss of generality, take an origin at the start of the distribution. We 
, may then write 
X £ I x1 - x* I = 2Z"(a, - xk) ■. 
the summation 27' being taken over values such that j > Jfc. We have also 
%1 -•»* = (X1 - x]-i) + 0»/-i - »/_g)'-f- • ■ • -f- (»fc+i - **)• 
N-l 
Thus ' F(x, - xk) = £ Ch(H+i ~ %h) 
where Ch is the number of terms of type (x} — xk) in S' containing xh+1 — xh. Since h is 
the number of values of j less than or equal to h (the origin being at the start of the 
distribution) and N — h the number greater than or equal to h ■+■ 1, we have Ch = h(N — h), 
and thus 
2 
2 JV~1 
= 2f2 £ h(N ~ h)(xh+i ~ xh) (2-29) 
« 
This form is particularly useful if all the intervals are equal. Fh being the distribution 
function of xh we then have 
JV-1 
-2^Jfc(l -Fh) (2.30) 
If the actual cumulated frequency for xh is Oh we have 
2 ^ ■ 
^-F«2JG'*(Jvr "^ • ' ' ' ' (2'31) 
the most convenient form in practice. 
Example 2.8 
Returning once more to the height distribution oonsidered in previous examples, we 
may calculate SGh(N — Gh) as in the Table overleaf. 
I 
46 MEASURES OF DISPERSION 
' i 
TABLE 2.3 
Calculation of t?ie Mean Difference for the Height Distribution of Table 1.7. 
Height, 
inches. 
, 57- 
58- 
,59- 
60- 
61- 
62- 
63- 
64- 
65- 
66- 
67- 
68- 
69- 
70- 
71- 
72- 
73- 
74- 
75- 
'76- 
77- 
T0TAL8 
i 
Frequency. 
,2 
4 
14 
41 
83 
169 
394 
669 
990 
1223 
1329 
1230 
1063 
646 
392 
202 
79 
32 
16 
5 
2 
8585 
0,. ' 
2 
6 
20 
61 
144 
313 
707 
1376 
2366, 
3589 
4918 
6148 
7211 
7857 
8249 
8451 
8530 
8562 
8578 
8583 
8585 
N-&. 
8583 
8579 
8565 
8524 
8441 
8272' 
7878 
7209 
6219 
4996 
3667 
2437 
1374 
728 
336 
134 
55 
23 
7 
2 
^™" 
17,166 
61,474 
171,300 
519,964 
1,216,604 
2,589,136 
5,569,746 
.9,919,584 
14,714,154 
17,930,644 
18,034,306 
14,982,676 
9,907,914 
' 6,719,896 
2,771,664 
1,132,434 
469,160 
196,926 
60,046 
17,166 
— 
106,990,850 
We have, from (2.31), for the mean difference with repetition, 
A - 2 x 1°6>990>850 
1 8585a 
= 2-88 inches 
as against a mean deviation of 2*02 inches and a standard deviation of 2-57 inches (Example 
2.6). There is, of course, nothing inconsistent in the difference between these valueB. 
The coefficients are different in nature, and there is no reason why their numerical values 
in any particular case should approach equality. 
NOTES AND REFERENCES 
The relationship between mean, median and mode expressed in equation (2.11) was 
discussed from the mathematical point of view by Doodson (1917); who showed that it 
holds as a first approximation for continuous distributions deviating only moderately from 
symmetry. 
It was shown by Dunham Jackson (1921) that the indeterminacy in the definition of 
the median can be removed by a more sophisticated mathematical approach. He showed 
jV ' 
that for N values the sum £ \ £ — xf j p, considered as a function of f, has 
?-i 
i 
k 
EXERCISES 47 
a minimum for some unique £p if p > 1 ; and further that as p —>- 1, fp tends to some 
unique value, which may be defined as the median. 
The proof of the increasing character of the function F(t) of2.7 is due to Norris (1935), 
who gives references to earlier proofs. 
The work of the Italian school on concentration does not appear to have been treated 
in English books. The fundamental memoir is that of Gini (1912), who has returned to 
the subject in subsequent papers, many of them in Metran. For methods of calculating 
t"he mean difference, see de Finetti and Paciello (1930). 
de Finetti, B., and Paciello, U, (1930), " Sui metodi proposti per il calcolo della differenza 
media," Metron, 8, part 3, 89. 
Doodson, A. T. (1917), " Relation of Mode, Median and Mean in Frequency Curves,'* 
Biometrika, 11, 425. 
Gini, C. (1912), " Variabilita e Mutabilita," Studi Fconomico-Cliwidici della B. Universita 
di Cagliari, Anno 3, part 2,- p. 80. 
Gini, C, and Galvani, L. (1929), '( Di taluni estensioni dei concetti di media ai caratteri 
qualitativi," Metron, 8, parts 1-2, 3. 
Jackson, Dunham (1921), " Note on the median of a set of numbers," BuU. Amer. Math. 
Soc, 27, 160. 
Norris, Nilan (1935), " Inequalities among averages," Ann. Matfi. Stats., 6, 27. 
EXERCISES 
2.1. Show that t.he mean deviation about an arbitrary point is least when that point 
is the median. 
2.2. Show that the mean (about the origin) of the discontinuous distribution whose 
frequencies at 0, 1, 2, . . . r, . . . are • ■ 
mm „,ma „.mr 
' 1! 21' * rl' * * ' 
is m. and that the variance is also m. 
2.3. Show that, if deviations are small compared with the value of the mean, we ■ 
have approximately, for the Geometric and Harmonic means, 
and hence that 
/*; - 2G + H = 0. 
2.4. Show that the mean deviation about the mean is not greater than the standard 
deviation. 
2.5. Show that for the "rectangular" population 
dF =dx, 0 < x < 1 
/ij (about the origin) = \ 
0i =tV 
mean deviation = £ 
i 
48 MEASURES OF LOCATION AND DISPERSION 
2.6. Show that for the distribution 
« dF = y9e ° dx, 0 < x <; oo 
the mean, standard deviation and mean difference are all equal to a; and that the 
interquartile range is a loge 3. 
* 
2.7. Show that for the distribution 
dF = y0e trf" #X> ° < X < °° 
& (about the origin) = r(^±l\/rf^\ 
2.8. Show that if a range of six times the standard deviation contains at least 
18 class-intervals, Sheppard's correction will make a difference of less than 0-5 per cent. 
in the uncorrected value of the standard deviation. 
2.9. Show that for a continuous distribution 
/•OO 
A1 = 2 F(x){\ -F(x)}dx. 
J — 00 
2.10. If the variate-values of a distribution are xt . . . xN in ascending order of 
magnitude* and 
r 
sr = y xt u = ? sr 
= 2> v=Z 
1=1 r~l 
r N 
r 
7^1 ~r=i 
then Ax = ^V - U) 
^%{N{N + 1)m'x - 2U}. 
CHAPTER 3 
MOMENTS AND CUMULANTS 
Definition of Moments 
3.1. In the previous chapter we defined the first moment (arithmetic mean) about 
an arbitrary point a by the Stieltjes integral 
/*i = I (* — a)dF • ... • . - . (3.1) 
and the second moment about the point by 
P* = \ (« -<*)2dF (3.2) 
J -00 
In generalisation of these equations we may define a series of coefficients jiri r — 1, 2 , . ., 
by the relation ' 
fir = [ (x - a)rdF (3.3) 
J —00 
jj,'r is called the moment of order r about the point a. When a is the mean fix we write 
the moment without the prime, 
liT = f {x-^dF (3.4) 
J -00 
In particular 
/<i = 0, 
and we may also define a moment of zero.order 
A*o = A*o = I dF = 1. 
J —00 
It is assumed that when reference is made to the rth moment of a particular distribution, 
the appropriate integral (3.3) converges for that distribution. As will be seen later, some 
of the theoretical distributions encountered in statistics do not possess moments of all 
orders; some, in fact, possess only a few moments of low order, and one or two do not 
possess any, except of course the moment of order zero. 
3.2. If a and b are two variate-values, let b — a = c and denote the moments about 
a and b by p,'(a) and //(&) respectively. Then we have, by the binomial theorem, 
(x — a)r = (x - b + b — a)r = (x - b + c)r 
Hence 
/*» = f (x - af dF 
J —00 
\li-lV>) C (3.5) 
£(;•> 
A.3. 49 E 
60 * MOMENTS AND CUMOLANTS 
This equation gives the rtih. mpment about a in terms of the rth and lower moments about 
b. It may be written in a symbolic form which will be found to provide a useful mnemonic, 
namely ♦ 
with the convention that the expression on the right lis to be expanded binomially and 
the form {fi'{b)Y replaced by fi){b). 
The equation (3.5) is of particular importance if one of the values a or b is the mean 
of the distribution. In this case we have 
Mr = J^L-V-f Mi1 (3*6* 
Mr = £ijjMr-i {~ Ml)' (:*'7) 
In particular 
and 
Mi = M* + Mi* 
Mi = Mi + $MiM* + Mi* r * ' • ' ^'H) 
Mi— Mi + Vi/*a + *Mi*M* + Mi*. 
Mi = Mi - M\* 
Mi = Mz — %M\Mi + 2/*'i8 / f • • • ■ ('*■**) 
Mi = Mi — Vi/*3 + 6/*i Va - 3/"i4. 
Calculation of Moments 
3.3. For a distribution specified numerically in a frequency table the calculation 
of moments of third and higher orders is akin to that of the first and second moments. 
For grouped data (high order moments are hardly ever required for ungrouped data) the 
observations are regarded as concentrated at the mid-points of intervals ; a convenient 
arbitrary origin a is chosen, the moments about a calculated, and then if necosHaiy the 
moments about the mean are ascertained from (3.6) or (3.7). The effect of grouping may 
be corrected for in certain cases. 
In practice numerical moments of order higher than the fourth aro rarely required, 
being so sensitive to sampling fluctuations that values computed from moderate num.born 
of observations are subject to a large margin of error. 
There are two methods in general use for arriving at the moments about an arbitrary 
origin. The first is an immediate generalisation of the methods used in Chapter 2 for tho 
first two moments. The second will be considered in 3.10 in connection with factorial 
moments. . 
Example 3.1 
To find the first four moments about the mean of the distribution of Australian 
marriages of Table 1.8. 
Until the last stage we work in units of three years, the variate interval. A working 
mean is taken at 28-5 years. To check the arithmetic we use an identity of type 
(x + l)**=x*+&c* + Sx + 1 
(a + l)4 = x* + 4«» + 6x* + 4x + 1. 
CALCULATION OF MOMENTS 51 
Thus, for instance, the value of g(x)(x + l)r is found in addition to that of g(x)xr and the 
two checked by identities such as 
Zg(x)(x + l)3 = Zg{x)x* + 3£g(x)x* + 3Zg(x)x + Zg(x), 
g(x) being the actual frequencies. The arithmetic work is shown in Table 3.1. 
TABLE 3.1 _ 
Calculation of the First Four Moments of the Distribution of Marriages of Table 1.8. 
\ 
Mid- 
value 
of 
Intervals, 
Yeais. 
10-5 
19-5 
22-5 
25-5 
28-5 
31-6 
34-5 
37-5 
40-5 
43-5 
40-5 
49-5 
52'5 
65-5 
58-5 
01-5 
04-5 
07-5 
70-5 
73-5 
76-5 
79-5 
82-5 
85-5 
88-5 
Totals 
OP +VB 
THUMB 
fir. 
294 
10,995 
01,001 
73,054 
50,501 
33,478 
20,509 
14,281 
9,320 
6,230 
4,770 
3,(120 
2,190 
1,055 
1,100 
810 
049 
487 
320 
211 
119 
73 
27 
14 
5 
301,785 
X. 
-4 
-3 
-2 
-1 
0 
1 
2 
3 
4 
5 
0 
7 
8 
9 
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
— 
teg. 
- 1,176 
- 32,985 
-122,002 
- 73,054 
-229,217 
33,478 
41,138 
42,843 
37,280 
31,180 
28,020 
25,340 
17,620 
14,895 
11,000 
8,910 
7,788 
0,331 
4,504 
3,165 
1,904 
1,241 
486 
200 
100 
318,049 
(3+1)0. 
- 882 
-21,990 
-01,001 
-83,873 
50,501 
06,956 
01,707 
57,124 
40,000 
37,410 
33,390 
28,900 
19,710 
10,650 
12,100 
0,720 
8,437 
0,818 
4,800 
3,370 
2,023 
1,314 
513 
280 
105 
474,490 
xlg. 
4,704 
98,956 
244,004 
73,064 
33,478 
82,270 
128,520 
149,120 
155,000 
171,720 
177,380 
140,100 
134,055 
110,000 
98,010 
93,460 
82,303 
03,896 
47,475 
30,4(14 
21,097 
8,748 
5,064 
2,000 
2,165,838 
(a + l)V 
2,046 
43,980 
01,001 
50,501 
133,012 
185,121 
228,490 
233,000 
224,490 
233,730 
231,080 
177,300 
105,500 
133,100 
11(1,640 
100,081 
95,452 
73,350 
54,010 
34,391 
23,052 
9,747 
5,000 
2,205 
2,035,287 
x*g. 
- 18,816 
-290,805 
-.488,008 
- 73,054 
-876,743 
33,478 
164,552 
385,587 
590,480 
779,600 
1,030,320 
1,241,000 
1,121,280 
1,200,495 
1,100,000 
1,078,110 
1,121,472 
1,009,039 
894,544 
712,125 
487,424 
358,049 
167,404 
9(1,020 
■ 40,000 
13,075,105 
(«+l)'ff. 
- 7,938 
- 87,900 
- 01,001 
-150,809 
56,601 
267,824 
555,303 
913,984 
1,165,000 
1,340,970 
1,030,110 
1,853,440 
1,590,610 
1,055,000 
1,404,100 
l,30'9,08O 
1,425,863 
1,330,328 
1,100,250 
804,250 
584,047 
425,730 
185,193 
112,000 
40,305 
19,901,050 
i 
x*g. 
75,204 
890,595 
970,010 
73,054 
33,478 
329,104 
1,150,701 
2,385,920 
3,897,500 
0,181,920 
8,091,020 
8,970,240 
10,858,456 
11,000,000 
11,850,210 
13,467,004 
13,909,207 
12,623,016 
10,081,875 
7,798,784 
0,097,033 
2,834,352 
1,824,494 
800,000 
137,300,1(12 
(0+i) V- 
23,814 
175,920 
01,001 
60,601 
535,048 
1,000,081) 
3,065,93(1 
5,825,000 
8,081,850 
11,452,770 
14,827,520 
14,308,590 
10,650,000 
1(1,106,100 
10,790,100 
18,630,089 
18,708,592 
10,503,750 
13,828,090 
0,938,909 
7,003,248 
3,618,007 
2,240.000 
972,405 
202,091,751 
From this table we find 
£(xg) = 88,832 
Z(xzg) = 2,155,838 
Z{x*g) = 12,798,302 t 
Z{x*g) = 137,306,162. 
The values, will be found to check and we have, about the working mean, on dividing by 
the total frequency 301,785, . 
jli\ = 0-294,355,253 
fi'2 = 7-143,622,115 
/*'8 = 42-408,873,867 
//4 = 454-980,075,219. 
52 MOMENTS AND CDMDLANTS 
For the moments about the mean, substitution in equations (3.9) gives 
H% = 7-056,977 
#, = 36-151,595 
/*4 = 408-738,210. 
These are expressed in class-intervals, which are units of three years. To express the 
results in units of one year we multiply the rth moment by 3r, e.g. 
/*, = 7-056,977 X 9 = 63-512,79. 
3.4. If a distribution is specified mathematically the determination of moments is 
equivalent to the evaluation of certain sums or integrals. It is usually necessary to 
consider whether the moments exist. Some examples will illustrate the general principles 
involved. 
Example 3.2 
Consider the so-called binomial distribution (q + p)n in which the frequencies of 
values 0,h, 2h, . . . are the successive terms in the expansion of the distribution, i.e. are 
Qn> U V1-1^ (oV'V • • ■ Taking an origin at the first term and working in units 
of h, we have 
*-£{G>-'"} 
which may be written 
1-0 
3 
= np{q+p)n~1 
= np. 
n f/ \ 1 
= (4)2(2+*)n 
= np(q +PY'1 + n(n - l)p*{q -f p)n~2 
= n^p* + npq. 
Hence [it = npq. 
Similarly //3 = (p—\ (q + p)n 
etc., and it will be found that 
< « 
[MZ = npq(q - p) 
fa = 3p*q*n* + pqn(l - QM)m 
Example 3.3 
Consider the distribution 
dF = - * dx -oo<a;<oo 
{l+x*)m m>l. 
CALCULATION OF MOMENTS 63 
This is a unimodal distribution symmetrical about x = 0. All existent moments of odd 
order about the origin therefore vanish. The constant h is given by the equation 
dx 
= *L(T 
_ ,m)Am - *) 
P(m) 
The moment about the mean of order 2r, if it exists, is given by 
f00 a;2r 
and this integral converges if and only if 
2m > 2r f 1. 
Thus the moments about the origin of order < (2m — 1) exist and those of higher order 
do not. 
> _ J*00 "fey. fig. 
If m — 1 it may be noted that the integral ■=——2 ^ no^ completely convergent, 
rkx dx 
——■—ti does not exist, although the principal value 
. . n (i + x ) 
he dx 
lim T 
n —►■« J —n ' 
(1+Z2) 
does exist and is equal to zero. It is a matter of convention whether we regard the 
distribution as possessing a mean in this case. For m > 1 the mean exists and is located at 
the origin. 
Making the substitution z = ?—-—$ in the formula for iu2f> we ^^ 
i"2r 
= k\1(i-zy 
Jo 
-i zm-r-l dz 
= iAr + Wm -r-\) 
~r(m) 
and on substituting for h, 
r(r + \)r(m - r - i) .£ . ^ _ , , 
Example 3.4 
Consider the " normal " distribution 
1 _^- 
dF = —y=- e 'to* dx — 00 < x < 00. 
This is symmetrical about the origin. All moments exist, those of odd order vanishing. 
Thus 
00 x* 
1 f _ X* 
<7V(2^)J -00 
This may be evaluated by partial integration, but a more direct method is as follows: 
Consider the integral 
1 f00 x* 
M(t)=—77^ etee 2a» dx 
e\M 
54 MOMENTS AND CXJMULANTS 
We have, for all real values of t 
,.-o \ ' 
The series on the right is uniformly convergent in a and may be integrated term by term 
if the resulting series is uniformly convergent. We then have 
M(t) 
r-0 N 
M- 
In other words, y.r is the coefficient of - in ewt% and hence 
'_ o*(2r)l 
Moment-generating Functions and Characteristic Functions 
3.5* The previous example shows that in some cases we can derive from the 
distribution or the frequency function a function M(t) which, when expanded in powers of 
t, will yield the moments of the distribution as the coefficients of those powers. Such 
a function is accordingly called a Moment-generating Function. It will be discuHHcd more 
fully in the next chapter. 
/•OO 
For many frequency functions the integral e/j" dF or the sum 2,1{«/j7/(u,/)} may 
J —OO 
not exist for real values oft. This is, for example, true of the function dF — k(l -\- •»•'-)" "l dx 
for finite positive values of m. A more serviceable auxiliary function is 
<p(t)=*\ eitx dF (t veoX) (3.10) 
J —OO 
This is known as the Characteristic Function and is of great theorotical importance. It 
will be seen in Chapter 4 that under certain general conditions the characteristic function 
determines and is completely determined by the distribution function. It also yields 
many valuable results in the theory of sampling. 
Since by the nature of the distribution function the integral dF converges, 
J — tt 
I <p(t) I < f \eiU\dF <\ 
J —00 
and hence the Stieltjes integral (3.10) converges absolutely and uniformly in /. Tt may 
therefore be integrated under the summation signs with respect to /. and may he 
differentiated provided that the resulting expressions exist and are uniformly convergent. We 
have, for example, writing D, for =-, 
at 
D\<p{t) «^r eitxxrdF, 
J —00 , 
and hence, putting t = 0, 
• A*i = (-*y[Dfo(*)]/-o (3.11) 
provided that f/r exists. If <p(t) be expanded in powers of t, u' must thus be equal .to the 
dty 
coefficient of -^- in the expansion. Thus the characteristic function is also a moment- 
generating function. / 
MOMKN'IMiKNKKATtNG FUNCTIONS AND CHAKACTERlSTIC FUNCTIONS 55 
CuiwuitT ngrtin the binomial (q \ p)n. Taking h hh unit, we have 
*■ np f~ n(» — l)p*, 
nijtl 
*n nn. 
Rjrnmplr 3 d 
t'unwMi'r lho dint rilmt ion 
whirl* i- kii"iHn n* t'tmrxnu'K Tyj** III (of. <'htt|»t**r <l). The diHtrilmtinn may havo a variety 
of »hit|K<«, tli'jH'Uiiinu «m tho vnluo of ;*, hut moniontH of nil ortlorn oxiMt in virtue of tho 
nmvrtxotioo of tho iutounil I rTr "' »/.*\ 11n* /-function integral. Wo havo thu|ia for the 
vimttutrtfliv fuut't tun, 
,. ft It 
ll\ tin* «uW»ututtou i "» r(« »/) thin horomcH 
* t\y) (n i/)1 Jo 
1 
(■ 3' 
I* 
nun *' 
J r V ' i/: - /"{)*) whothor c is rrnl or cimiplox. 
it y(v ■ Dfity , 
nn.i thug 
y 
• .y(y M)(y ! 
■ 2) 
56 MOMENTS AND CUMULANTS 
and so on. In particular, 
[l% 
fa 
y 
a* 
"a8 
Absolute Moments 
3.6. The quantity 
vr= f \x-a\rdF (3.12) 
is called the* absolute moment of order r about a. The absolute moments about tho moan 
are written without primes. 
If r is even, the absolute moment is clearly equal to the ordinary moment, and if the 
range of the distribution is positive the absolute moments about any point to tho left of 
the start of the distribution are equal to the ordinary moments of corresponding order. 
There are some interesting inequalities concerning the absolute moments. Referring 
to the function E(t) of 2.7 and remembering that it is a non-decreasing function of /, wo 
find, on putting t = 0, 1, 2, . . ., that 
«)*<(»4)*< «)i .. . <ti)7 (3.13) 
A more general inequality, due to Liapounoff, is 
vba-c < v'ca-b v'ah-\ a>b>c>0 (3.14) 
A proof of this result is sketched in Exercise 3.14. In particular, putting b = \{a -)- r) 
we find 
V2r+s < v'rv'a (3.1/5) 
2 
Further, putting a = 0 in (3.14) we find 
,h ,1 
or v'^ <v'bi>, c < b 
which is equivalent to (3.13). 
Factorial Moments 
3.7, The factorial expression 
x(x — h)(x — 2h)...(x — r — lh) 
may conveniently be written x[r\ a notation which brings out an analogy with tho power 
xr. Taking first differences with respect to x and with unit h, wo have 
Ax** = (x + h)[r] - s[r] 
= (x + h)x(x - h) ... (x - r - 2h) - x(x - h) . . . (x - r -~1A) 
= rx[r~1]h, 
which may be compared with the differential equation 
dx* = rxT~x dx. 
n 
Conversely ^M = ,x + A)[r+ii 
ex 1 
corresponding to I x* dx = ———xr+1 
Jo r + l 
FACTORIAL MOMENTS 
57 
The rth factorial moment about an arbitrary origin may then be defined by the equation 
00 
Pm = £(xi - a)lr]f(xj) • • ■ ; 
(3.16) 
^•■ — 00 
and hence 
whore we have chosen the summation sign Z rather than the Stieltjes integral because 
it is almost entirely for discontinuous distributions, or continuous distributions grouped 
in intervals of width h, that the factorial moments are used. In statistical theory they 
are not very prominent, but in the theory of interpolation and of curve fitting they are 
sufficiently important to justify some mention of 'their properties. 
As usual, when it is necessary to distinguish between factorial moments about the 
mean and those about an arbitrary point we may write the former without the prime. 
3.8. TtyB factorial moments obey laws of transformation similar to those of equation 
(3.6) governing ordinary moments. In fact we have the expansion * 
(a + 6)M = J^Y'V-* 6UI 
(x — a)[r] = (x — b + c)[r] where c = b — a 
PlM = Z,[j)^r~n 6W 
which may be written symbolically 
fz[r](a) = (p'(b) + c)M. 
3.9, By direct expansion of (3.16) it is seen that 
Au = p\ 
Pm = Pi - Vi 
Pw = Pi - QhPs + ll^Vi - 6AVi , 
and hence 
(3.17) 
> . • • • yt>. 
18) 
and conversely that 
/*2 = P[2] + */«[ll 
/*» = Pm + 3V[2i + * Vin 
/*i ■= /%] + 6V[3] -f- 7*Vial + *V[i]J 
.. (3.19) 
Sinco the first moments are equal the equations remain true when the primes are 
dropped and terms in first moments omitted. 
* It is clear that (a + 6)M will bo a polynomial of degree r in a, and may therefore be equated 
r 
T\ afr-fl, whore the fc's are polynomials in 6 and h but do not contain a. Putting a = 0 we 
obtain &M - kr. Taking first differences with respect to a and putting a = 0 we obtain 
rft[r-i] = kr-i. Successive differences give the Zg's" and the above result follows. 
to 
60 MOMENTS AND CTJMULANTS 
TABLE 3.3 
Calculation of the Factorial Moments of a Distribution of Men according to Height in Inches 
(Table 1.7). 
Height. 
57- 
58- 
59- 
60- 
61- 
62- 
63- 
64- 
65- 
66- 
67- 
68- 
69- 
70- 
71- 
72- 
73- 
74- 
76- 
- 
77- 
TOTAM 
Frequency. 
2 
4 
14 
41 
83 
169 
394 
669 
990 
1223 
1329 
1230 
s 1063 
646 
392 
202 
79 
32 
16 
5 
2 
8585 
First Sum. 
8,585 
8,583 
8,579 
8,565 
8,524 
8,441 
8,272 
7,878 
7,209 . 
6,219 
4,996 
3,667 
2,437 
1,374 
728 
336 
134 
55 
23 
7 
2 
94,614 
Second Sum. 
(94,614) 
86,029 
77,446 
68,867 
60,302 
61,778 
43,337 
35,065 
27,187 
19,978 
13,769 
8,763 
5,096 
2,659 
1,285 
657 
221 
87 
32 
9 
2 
602,459 
Third Sum. 
_ 
(502,459) 
416,430 
338,984 
. 270,117 
' 209,815 
158,037 
114,700 
79,635 
62,448 
32,470 
18,711 
9,948 
-4,852 
2,193 
908 
351 
130 
43 
11 
2 
1,709,785 
Fourth Sum. 
_ 
— 
(1,709,785) 
1,293,355 
954,371 
684,254 
474,439 
316,402 
201,702 
122,067 
69,619 
37,419 
18,438 
8,490 
3,638 
1,445 
537 
186 
56 
13 
2 
4,180,103 
Cumulants 
3.11. The moments are a set of parameters of a distribution which are useful for 
measuring its properties and, in certain circumstances, for specifying it. Their use in these 
connections will be considered in later chapters. They are not, however, the only set of 
parameters for the purpose, or even the best set. Another series of parameters, the so- 
called cumulants, have properties which are more useful from the theoretical standpoint. 
Formally, the cumulants kx, jc9) . . . Kr are defined by the identity in t 
exp |k,jH- _+...+ -JL. + .. .J. 
= 1 +/*[* + ^p + • • • + & + (3.22) 
It is sometimes more convenient to write the same equation with it for t, thus: 
exp|ic1(*<)+*«^- + . . . Acr^- +...!■ 
_ 1 _i_ „'<*0 + _i_ J1? - 
= *■ + Pi-jt ~r • • • -r Pr-jr + • ■ • 
r 
= eiixdF 
J —00 
= <p(t) (3.23) 
RELATIONS BKTWKKN MOMENTS AND CUMULANTS 61 
(MY 
Thiw. whrmw /*', ix the coefficient of '- in ?((), the characteristic function, Kr is the ooeffioient 
(»0* 
of , in ttift 9(f). if an pxpannicm in {tower wries exists. 
A. 12. If in equation (3.23) the origin w changed from a to 6, where as usual 6 — a = c, 
fhr rffppl on 9(0 U to multiply it by r "«\ for fa'"^ bocomos (V«*-fltyte&S\ Henoe the 
rlfei-i on log v(0 ** merely to mid the term ite, and consequently the coefficients in log <p(t) 
nw timhftngi*d> except the* Hrnt, which in decreased by c. 
Hence the cumulanta art* invariant under change of origin, oxoopt the first. In this 
they *tand in »harp contra**, to the momenta about an arbitrary point. 
ifoth rumuimitH nut! momenth have another property of an invariantivo kind, namely, 
that if the vnriate-value* are multiplied by a eoiwtant «, fir and Kr are multiplied by ar, 
*l*hi«. i* at once evident from their deflnitioiiH. Thiw any linoar tranaformation of the kind 
£ « Ijt A m (3.24) 
leave* the cumuhint* unchanged ho far aw tho constant m is concerned and multiplies 
k, hy /'. The mile exception i« the limt cumuhint which ih equal to the mean. In par- 
tu'ulur, if we trunnform a distribution to Htandard meamire, the only effect is to multiply 
*, by n ', »« Mng the standard deviation and, aH we hIuiII hco in a moment, being equal 
In *,*. 
The invnriimtive proj>ertieH of the cutntilantH waH the origin of their original name 
of wmi invariant*. neminviiriitntM or half invarianth (Thiele, 1HH0). It haw, however, 
rverntly Iwen nhuu-ft that there are neveral other classes of parameters with the same 
Hr»»l*er1y, and it wwii* l*e*t 1o reserve the word " Hcminvariant" for any parameter Ar whioh, 
under tin* trnitHfontiiilitiit {H.JM). in multiplied l>y P. Tho oumulantH and tho moments 
nt**ui thr mrtin art* I hint particular eiwen of Ht'ininvnrianta. 
Jtrliihon* ftrlwrm Mttmrnfa uml (UtmuUinh 
Jt.Lt. Nuhjeet t4» condition* of existence, we have, from (IJ.22) 
- «*M' \h*v ' KU\ ' • • • %t • • •(■ 
•■■{■■ ft)'itf)'" ■■•}■■• 
Picking <»ut the teruH in the fxpouential expansions which, when multiplied together, 
give u power of C, we have 
VNYk"'YY*'"Y' (""A** r! -. • . (3-25) 
62 
MOMENTS AND GUMULANTS 
where the second summation extends over all non-negative values of the n'a such that 
JPi»i + Pa^a + 
PmF1- 
'm 
= r. 
(3.26) 
It is worth noting that the rather tedious process of writing down the explicit relations- 
for particular values of r may be shortened considerably. In fact, differentiating (3.22) 
by *y we have \ 
U + ftt + 
+ rl + 
and hence, identifying powers of t, 
In particular 
•■•>- 
3/4 / / 
dfi\ 
dKj 
h+ 
"r r\dKj _r 
(3.27) 
(3.28) 
and thus, given any fir in terms of the *'s we can write down successively those of lower 
orders by a differentiation. 
The first ten of these expressions are, for moments about an arbitrary point:— 
ft = *i, ■ 
f/2 = *2 + k[, 
ft - *3 + 3*2*i + *i> 
III = *4 + 4*3*i + 3/c| + 6*2* 1+4, 
ft = *5 + 5*4*1 + 10*3*2 + lO/CgAC? + 15*l*i + lOKg*3. + /cf, 
ft = K6 + 6*5*i + 15*4*2 + 15*4*i + 10*3 + 60*3*2*i + 20*3*f 
+ 15*1 +.45k|k? + 15/c2fcf + *°, 
ft = *7 + 7*6*i + 21*5*2 + 21*6*i + 35*4*3 + 105*4*2*i 
+ 35*4*? + 70*f*i + 105#c3#c| + 210*3*2*? + 35*3*J 
+ 105*l*i + 105fcl^ + 21*2*2 + *L 
ft = *8 + 8*7*! + 28*6*2 + 28*6*? -f 56*6*3 + 168*5*2*i + 56*6/<? 
+ 35*f + 280*4*3*i + 210*4*|+ 420*4*2*? + 70#c4/cl 
• + 280*1*2 + 280*1*? + 840*3*2*! + 560*3*2*J + 56*3*5 
+ 105*| + 420*1*? + 210*1** + 28*2*5 + *i, 
ft = *9 + 9*8*! .+ 36*7*2 + 36*7*f + 84*6*3 + 252*„*2*1 
+ 84*6*3 + 126*6*4 + 504*5*3*! + 378*6*1 + 756*6*2*? 
+ 126*5*1 + 315*l*i + 1260*4*3*2 + 1260*4*3*2 + 1890*4*1*, 
+ 1260*4*2*? + 126*4*f + 280*1 + 2520*2*2*! + 840*!j*J 
+ 1260*3*1 + 3780*3*1*2 + 1260*3*2*£ + 84*g*« + 945***, 
+ 1260*1*? + 378*1*« + 36*2*1 + *?, 
ft* =*io + 10*9*l + 45*8*2 + 45*8*2 + 120*7*3 + 360*7*2*i 
+ 120*7*1 + 210*6*4 + 840*0*3*! + 630*0*1 + 1260*0*2*2 
+ 210*6*t + 126*§ + 1260*6*4*! + 2520*5*3*2 + 2520*5*3*?- 
+ 3780*5*2*i + 25207f5*2*3i + 252*5*« + 1575*|*2 + 1575*!*2 
+ 2100*4*2 + 12600*4*3*2*1 + 4200*4*3^ + 3150*4*1 
+ 9450*4*1*2 + 3150*4*2*f + 210*4*5 + 2800*1*. 
+ 6300*32*2 + 12600*1*2*2 + 2100*1*1 + 12600*3*1*1 ' 
+ 12600*3*l*i3 + 2520*3*2*« + 120*3*J + 945*1 + 4725***? 
+ 3150*1*1 + 630*22ac« + 45*2*f + *1° ; * l 
(3.29) 
RELATIONS BETWEEN MOMENTS AND CUMDLANTS 
or, for moments about the mean (acj. = 0), 
jUt = AC,, 
/*B == ^3* 
/*! == Ki "T »K2i 
/*5 = * 5 + 10AC3ACa, 
^8 = AC6 + 15ac4AC8 -f IOac2 + 15AC3, 
/t7 = ac 7 + 21ac5ac2 -f 35ac4ac8 -j- 106ac8ac2, 
^b = ac8 + 28ac,ac2 + 56ac5ac8 + 35ac| + 210ac4ac1 + 280*1*:,, + 105k}, 
fi, = ac, + 36ac7ac, + 84ac6ac8 + 126k5ac4 + 378ac8ac| + 1260ac4ac3ac, -f 280acJJ 
• + 1260ac8a4 
flu = *io + 45ac8ac, + 120ac7ac3 + 210ac6ac4 + 630ac6k| + 126ac§ , 
+ 2520ac„ac8ac2 + 1575acJac, + 2100ac4ac| + 3160ac4ac1 
+ 6300ac§ac1 + 945*4 
Conversely we have 
ACi< 
63 
(3.30) 
1! 
+ 
+ TT + 
. . . =log f: 
i + § + 
r\ 
+ 
•)• 
.'2 
Expanding the logarithm and picking out powers of f as before, we have 
Kr _ rl y y /&)-... (&y» (-7-^ -;)' . 
the second summation extending over all non-negative jr's and p's, subject to 
the further condition 
The first ten formulae are, in terms of moments about an arbitrary point 
Kl = fly, 
k* = /4 — Pi 
Kt = ri ~ 3/4/4 + 2/43, 
ac4 = fa — 4/4/4 — 3/42 + 12/4/4 2 -- 6/4*, 
ac6 = /4 — 5/4/4 — 10/4/4 + 20/4/4* + 30/4 Vi — 60/4/43 + 24/45, ^ 
AC. = /4 — 6/4/4 — 15/4/4 "H 30ri/*l2 — 10/«7 + 120/4/4/4 — 120^'g^i8 
+ 30/43 - 270/4 Vi2 + 360/4/4* - 120/4°, 
ac7 = /4 - 7/4/4 — 21/4/4 + 42/4/42 — 35/4/4 + 210/4/4/4 
- 210/4/43 + 140^Vi + 210/4/42 ~ 1260/4/4/4* + 840/4/4* 
- 630/4'Vi + 2520/4Vi3 - 2520/4/45 + 720/47, 
AC8 = /i'g — 8/4/4 — 28/4/4 + 56/4/42 — 56/4/4 + 336/4/4/4 
- 336/4/43 - 35/42 + 560/4/4/4 + 420/4/4a - 2520/4/4/4* 
+ 1680/4/4* + 560/4V2 - 1680/42/42 - 5040/4/42/4 
+ 13440/4/4/43 - 6720/4/45 - 630/44 + 10080/43/42 
- 25200/^2Vi* + 20160/4/48 - 5040/48, 
*» = /4 — 9/4/4 — 36/4/4 + ?2/4/42 — 84/4/4 + 504/4/4/4 
-r 504/fo^3 - 126/4/4 + 1008/«i/«s/«i + 756/4/42 - 4536/4/a'2/a'12 
+ 3024^'/ + 630/42/4 + 2520/<»2 - 7560/4/4/42 
- 11340/a'4/a'22/ai + 30240^/4/43 - 15120^^ + 560/*'33 
- l5120/42/4/4 + 20160/432/*'!3 - 7560/4/42 + 90720/i3/M22/Mi2 
~ 181200/«s/«a/«i* + 60480/4/46 + 22680/i'aV; - l51200/43/43 
+ 272160/M'22/M'15 - 181'440/*2>'17 + 40320^°, 
. (3.31). 
(3.26) and 
. (3.32) 
. (3.33) 
64 MOMENTS AND CUMULANTS 
Kio _ ^ lo^ri - 45/4/4 + 90/4/42 - 120/4/4 + 72WW, 
- 720 /4/43 - 210/4/4 + 1680^^3^1 + 1260^^ 
- 7560/4/4/4* + 5040/4/4* - 126^2 +,25f/Wi 
+ 5040/4/4/4 ~ 15120/wi2 ~ 22680^2Vi + 60480 /*flWl3 
- 30240/4/45 + 3150/42/4 ~ 9450/M;V12 + 4200/^32 
- 75600/4/4/4/4 + 100800/4/4/43 - 18900/y*23 
+ 226800/4/42/42 ~ 378000/4/4/4* +151200^^- 16800/^Vi 
- 37800/4V+302400/4V2/42- 252000/42/44 + 302400^^Vi 
- 1512000/4/4Vi3 + 1814400/4/4/45 - 604800/4/47 
+ 22680/45 - 567000/M'2V'i2 + 2268000piVi* - 317620(W/4° 
+ 181440O/4/48 - 362880/410. 
or, for moments about the mean, 
K2 == /**> 
** = /** — 3/4, - 
*s = /*s — lO^s/ia, 
*e = /"e - 15/"4/Wa ~ 10/4 + 30^|, 
ac7 = p, — 21/*B/*a — 35/*4/*8 + 210^8/wf, 
k8 = /*8 - 28^.^, - 56/^, - 35/4 + 420^ + 560/4/*, - 630^, 
k9 = /j, -36/*7/*s — 84/*fl/*8 — 126/*B/*4 + 756/iB/4 + 2620/*4/*8/*2 
+ 560/4 — 7560/*8/4, 
ic10 = /*10- 45/^ - 120/*7/a8 - 210/*fl/a4 + 1260/*fl/4 - 126/4 
+ 5040/^8/J* + 3l50/4/*a + 4200/^/4 — 18900^4iw| 
- 37800/4/4 + 22680/4. 
. (3.33) 
. (3.34) 
Existence of Cumulants 
3.14. The formal expression (3.22) may be regarded as defining tho eumulants in 
terms of the moments, and it is thus evident that the cumulant of order r exists if tho 
moments of orders r and lower exist. If, however, we look to the equation 
exp (a^) = M 
as defining the cumulants, it is not quite so easy to show that kf exists if //r and lower 
//b exist. It may, however, be shown that kt exists if vr, tho absolute moment, exists, 
and this is sufficient for all ordinary purposes. Some care is necessary with the proof 
because the variable t in the characteristic function is real, but there also appears tho 
complex quantity i. 
We have 
<f>(t) = p eiix dF. 
J —no 
Expanding the exponential we have, if the moments up to fir exist, 
/-0 ' 
CALCULATION OF CXJMULANTS 65 
where 
- C dFlcob xt + i sin arf - J^j^V 
Considering the real and imaginary terms separately, we have, if r is even— I 
* - j>(- - -1<-»" g?) + ']>(* - ~ l( -1)M(^) 
The real term in the integrand consists of (~ 1)2 i—f- plus cos atf minus the first -£ terms of 
' rl r 2 
the Maclaurin expansion of cos xt, and is thus equal to ^-j-\ —r Q0B xt ) where 
\<fa# / xt - o, 
I xt \r 
0 < 0 < 1. The modulus of the term is thus not greater than 2 '—r-k Similarly for the 
imaginary terms. Hence 
J -00 »"l 
A Himilar result follows if r is odd. Now if ju,'r exists, it does not necessarily follow 
that v'r exists. But if the latter exists we have 
w> = i^jr +0(^ 
Wo may thon, for some small t, take logarithms and expand, obtaining 
(ity 
log ^(o = 2^K^l + °^r) • • ' • • • (3-36) 
j-aQ 
the eoeffuuents Kj being the cumulants by definition. Hence if vr exists, Kr and cumulants 
of lower orders exist. 
Calculation of (hmmlantft 
3.15. The cumulants are not, like the moments, determinable directly by summatory 
or integrative processes, and to find them it is necessary either to ascertain the moments 
and then apply equations (3.33), or to derive them from the characteristic function. For 
the latter case wo have, from (3.35) 
^-(-t^Djlog^)] (3. 
36) 
The following examples will illustrate the processes involved. 
a.s. 
66 MOMENTS AND CUMULANTS 
Example 3.8 
In Example 3.7 we found the following values for the moments about the mean of 
the height data of Table 1.7 :— 
fz\ = 11-020,850 
fi% = 0-616,805 
fz3 = - 0-207,840 ■ 
fit = 137-689,185, 
whence, from (3.34) k9 and k8 have the same values as /*a and fiz and 
* i = [ti — fy 
a 
ra 
= 6-342,86. 
*! is the same as fi'1} in this case measured from the centre of the interval 66- inches. 
The same results would, of course, have been obtained if we had used equations (3.33) 
and moments about the origin. 
Example 3.9 
. Consider the discontinuous distribution whose frequencies at the values 0, 1, . . . j . . . 
are e"m( 1, —.,... — ...). The characteristic function is given by 
«•>-.-£=< 
= e~m exp (meu) 
= exp m(eit — 1). 
Since for any r the absolute moment is the same as the ordinary moment, we have 
mijr 
7-0 J 
and since this converges * cumulants of all orders exist. They are therefore given by 
the expansion of log <f>(t) as a power series in t. But 
log <f>(t) = m(eu - 1) 
fi (it)* 
and hence kt = m 
for all r. Thus all cumulants of the distribution are equal to m. 
* For the ratio of the (n + l)th term of the series to the nth is 
mn+i(n + iy /mm ,m / l\f _ fm\ 
(n + ljT I ~nT " nTl^1 *Hj ~ °(j»J' 
and thus the series converges for all finite values of m. 
i 
CALCULATION OF CLOIilULANTS . • 67 
Example 3.10 ■ 
In Example 3.4 we found, in effect, for the characteristic function of the nonnal 
distribution 
1 je1 . 
dF = ———- e~2a» dx 
^-Wvdlf1*'™** 
e 2 
1 J/,\ *2<72 
log <j>(t) = - —. 
It is easily seen that the absolute moments and hence cumulants of all orders exist. Thus 
(it)r 
Kr is the coefficient of —— in log <j>(t)} i.e. for the normal distribution all cumulants of order 
higher than the Becond are zero. The second cumulant is equal to a1. 
Example 3.11 
In Example 3,6 it was found that for the distribution / 
dF-~xy-H-™ a>0' y>0 
~ W) ° < * < °° 
the characteristic function is given by 
<f>(t) = 
(>-r 
It is readily verified that cumulants of all orders exist and hence 
Kr = coeff. of £i- in — y log (\ — -J 
= y(r - 1)! a-.r. 
Example 3.12 
Consider again the distribution of Example 3.3. 
k 
d]p ss -,— dx m > 1, — oo < x < oo. 
(1 + x2)m 
The characteristic function is given by 
/•oo gixl 
whioh, since sin xt is an odd function, reduces to 
7 f °° cos xt j 
k\ —■ — dx. 
This integral may be evaluated by complex integration round a contour consisting of the 
68 , MOMENTS AND CUMULANTS 
z-axis, the innnite semicircle ,above the a-axis and the infinitely small oircle round the 
point x = i. It is found * 
W - 2^(t-l)i'!"l"{(2 ' '' r_1 + ^ " m ' '' )m"8 ' 
(m + l)(»)(m - !)(« - 2) . j )m_8 + . . . + SHr *>'l 
^ 2! v«i' (w — 1)1 J 
If r < 2m — 1 the absolute moment of order r 
x\r dx 
vr = j£ 
.00 (1 + »T 
exists and hence so does the cumulant of order r. But in this case we cannot expand 
log $0 in an infinite series of powers of t, though this might perhaps be thought possible 
from the form of $(t). In fact, we can only expand log <f>(t) in powers of t up to the point 
at which the differential coefficients of <j>(t) exist, for t — 0. 
To simplify the discussion, consider the case when m = 2. We have then, since 
k = 2/n in this case, 
#)=e-(< I{|*|+1} 
log#0«-|*| +log{l + M|} 
If t is positive this equals 
2 + 3 
but if t is negative it equals 
t* P 
~~2~3 ' ' " 
the two expressions differing in the sign of the»term in t3 and every second term thereafter. 
There is thus no unique expansion of log <f>(t) in powers of t about the point t = 0. There 
are two forms of the function expressing log <f>(t) according as t is positive or negative. 
. However, these expressions coincide as far as their terms in t and t2, and the first 
and second differential coefficients of log <j>(t) are uniquely defined when t = 0. Thus 
the first and second cumulants exist and are given by 
ACX =0 
ACa = 1. 
Cumulants of higher orders do not exist. 
Corrections for Grouping 
3.16. When moments are calculated from a numerically specified distribution which 
is grouped, there is present a certain amount of approximation owing to tho fact that 
* Besults of this kind axe given in several text-books of analysis, sometimes incorrectly, e.g. it 
is sometimes stated that 
I 
00 cos tx , . 
- dx = jre-t. 
1 +x* * 
whioh is only true when t > 0. The appearance of the modulus in the expression above is crucial 
for the purposes of the example. A correct proof is given in J. Edwards, Integral Calculus, vol. 2, 
article 1326. 
CORRECTIONS FOR GROUPING 69 
the frequencies are assumed to be concentrated at the mid-points of intervals. It is possible 
to oorreot for this effect under certain conditions. 
Suppose the frequency function f(x) to be continuous. If the range is divided into 
intervals of width h, we are given, not the values of f(x) at all points but the frequencies 
in those intervals, e.g. the frequency in the jth. interval, centred at xp will be 
~2 
We will denote the moments calculated from grouped frequencies^-the " raw" 
moments-^-with a bar, so that we have 
y —— oo 
(3.37) 
The true moment, if it exists, is given by 
poo 
J -co 
and it is required to investigate the relationship between the jS's and the jh'b. 
Now we have, in virtue of the Euler-Maclaurin sum formula, for an arbitrary function 
k(x) which has derivatives of the mth order, 
2 pa+nfc 
T k{x) dx = {lK{a) -f k{cl -f h) + k(o, -f 2h) -f ■. . . -f *(a -f n — lh) -|- \k{(l + nh)} 
nja 
- £ "iTBi K'H1)W - Sm (3.38) 
j^l J' ' L Ja 
where S,n is a remainder term which may be expressed as 
Sm = - -TBmKW(a + Onh) 0 < 0 < 1 
ml 
m even, 
and Sm = =^B$+lQ)KM(a + Onh), 0 < 0 < 1 
m odd.* 
Suppose now that f(x) is of finite range, from a to b, derivable up to the with order, 
* Cf. Milne Thomson, The Calculus of Finite Differences, ejection 7.5, for tho general Euler-Maclaurin 
expansion. Tho form of Sm when m is even is given in section 7.5 of that book, and the above form 
when m is odd may be derived similarly. 
In our convention the Bernouilli number B} is defined as tho coofliciont of t}/jl in t/ft — 1). The 
Bernouilli polynomial has already boon defined in 3.9. Explicitly B0 = 1, J?x = £, J5a = \, Bt = BK 
fcR n„_JLB_JLR 1r_ 5b=691r 7 
* -ff2j+l - U, Bi 30» *« - 42» *" 30' ^10 6(j> -°i« °= 2730' \* = "~ 6' 
70 MOMENTS AND CUMULANTS 
and that at the end of the range f(x) and its first m derivatives vanish. Then f{%) and 
the first m derivatives are continuous throughout the range — oo to +00 and the function 
ft 
k{x)=xt?J{x+£)&£ (3.39) 
together with its first m + 1 derivatives, will also be continuous throughout that" range. 
If a is infinite (and similarly for 6) it is assumed that 
lim xrf®(x)-+0 
X ►'—00 
for all values of j up to and including m, in which case k(x) and its first m + 1 derivatives 
will also tend to zero. Thus in either case the Euler-Maclaurin expansion (3.38) i« valid 
for k(x) given by (3.39) and we may write 
p%)l =0 ,j<m + l. 
Substituting in (3-.38) we have, since «(— 00) = *(+ 00) = 0, 
a oo r - 
= tfr - Sm+1 (3.40) 
The integral on the left of this expression is equal to 
h 
' )y_J h^x + ^d^dx (3.41; 
provided that the multiple integral exists. If, in addition, it is absolutely convergent wo 
may substitute x for x + £ and integrate with respect to f. We shall then have 
7l 
foo r5- 
.£ - Sm+1 = ^ J\ (* ~ W(«) * 
dx 
»J -00 r + 1 
where 11 I " the ktegral part of ^. 
CORRECTIONS FOR GROUPING 
71 
Thus if #m+1 may be neglected, (3.42) gives the raw moments in terms of the actual 
moments. In practice we require the latter in terms of the former and it is easy to find 
from (3.42) the following expressions :— 
fx\ = & 
ft* = & - Y2h* 
A = H ~ Qpifc* + 4y/*i^* 
/«« - Mo - ^ + ft**4 - ^4 
A8 
(3.43) 
Tho general expression for these formulae is 
when* /J is the iWnouilli number of order j. (Cf. Wold, 1934a.) 
(3.44) 
3.17. These art* the corrections known as Shoppard's. It is important to realise 
the conditions un<ler which they were obtained. 
{(i) It is assumed that/(.r) is bounded and tends monotonically to zero in the directions 
in which the range is infinite. 
{b) It is mummed that tho multiple integral (3.41) iR absolutely convergent. This is 
equivalent to Happening that the absolute moment of order r exists. If f(x) is finite in 
range anil hounded, the multiple integral in certainly absolutely convergent. If the range 
iH not finite, since f(x) kinds to zero monotonically in the direction or directions of infinite 
range, 
iLC-J-*-^ 
(13 dx 
will converge or diverge with 
lp° 
h 
i.e. with 
n 
""a 
r \x-\f(x)dx 
j—00 
which is the absolute moment of order r. 
(0) It in assumed that f(x) and its first m derivatives vanish at the tenninal points of 
the range when the range is finite, or that 
lim xrfU)(x) -> 0 
for all j up to and including r when the range is infinite. 
(d) It is assumed that Sm+l is negligible. 
1, 
* 
/ 
72 
Now both 
4nhm+1 
Hi 
and 
m *~ m 
MOMENTS AND CUMULANTS 
fflffi ^ i^g than -4 m magnitude * and hence Sm+1 Is of order 
6* 
multiplied by some value of f»\x) in the range. Thus if h is small, the range is 
finite m&fm\x) is small; Sm+1 will be small and may be neglected. In particular, if 
2["|1 < m . •■ (3-45) 
the Sheppard corrections will give the moments accurate to order W\ i.e. to the order 
of the terms applied in making the corrections. 
3.18. The foregoing discussion is rigorous, but the corrections may be applied in 
practice with considerable confidence whenever there is high-order terminal-contaot. 
Example 3.13 
Consider the distribution 
dF = 
5(12, 6) 
xu(l - «)6 dx 
0 < x < 1, 
a case of the so-called Type I distribution. The exact frequencies for intervals of 0-1 
may be obtained from the Tables of the Incomplete 5-Function, and are as follows :— 
Centre of Interval 
0-05 
0-15 
0-25 
0-35 
0-45 
0-55 
0-65 
0-75 
0-85 
0-95 
Total 
Frequency. 
0-000,000,0 
0,000,009,2 
0-000,046,8 
0-009,938,2 
0-061,137,4 
0-192,199,6 
0-332,887,7 
0-297,479,9 
0-101,033,7 
0-004,667,5 
1-000,000,0 
The raw moments about x = 0 are shown in the following table :— 
Moment. 
Pi 
/*■! 
t*3 
A*4 
tt> 
A*fl 
Raw. 
0-666,662,8 
0-456,965,5 
0-320,952,3 
0-230,335,1 
• 0-163,512,9 
0-125,433,2 
Exact. 
0-666,666,7 
0-456,140,4 
0-319,298,2 
0-228,070,2 
0-165,869,2 
0-122,599,0 
Corrootod. 
. 0-666,662,8 
0-450,132,2 
0-319,285,7 
0-228,053,2 
0-165,848,0 
0-122,574,0 
♦For 
B2j+i = 
(of. Milne Thomson, loc. tit.) 
and further 
, B2j 2(-ltf- 
0,i>0 and ^—^ 
4( -1)1-1 
. (2rc)2/ 
'JJ?W 
CORRECTIONS FOR GROUPING 
73 
The exact values of the moments are calculable by evaluating integrals of the type 
aj11+r(l — xf dx and are shown in the third column. The final column shows the 
J o 
results obtained by applying equations (3.43), e.g. 
1^2 = ^2 — &2/12 
= 0-456,965,5 - 0-000,833,3 
= 0-456,132,2. 
At the terminal x — 1, f(x) and its derivatives up to the fourth vanish. At the other end, 
derivatives up to the tenth vanish. The function is bounded, of finite range, and the 
derivatives remain finite throughout the range. In virtue of (3.45) it is to be expected that 
corrected moments of third and lower orders will be accurate to the order of the terms 
in the corrections, i.e. fa is accurate to order h% (0-001) and jh'3 to order ha (0-0001). Actually 
they are considerably more accurate than this. The oorrected fourth moment is in error 
by a term of order 2 x 10~8, and this is of the same magnitude as the correcting term 
a I „ft* used in arriving at it. Similarly the corrected fifth moments are in error by a term 
of order 10-8, of the same order as one of the correcting terms to the fifth moment, 
7 , 
n\h*, and of the same order as or greater order than two correcting terms to the sixth 
7 . 31 
moment, — fuji* and — 7^7 ^"- 
It) lo1*1* 
Thus the corrected moments are in all cases a substantial improvement on the raw 
momcntH ; but in applying the corrections it is necessary to guard against being misled 
about the accuracy of the final result by the apparent precision of some of the small 
corrective terms. 
Example, S.14 
As an illustration of the way in which Sheppard's corrections break down when the 
condition for high-order contact is violated, an example is taken from a paper by Pairman 
and Pearson (15)18). The following table shows the frequencies in a certain range of the 
normal distribution 
1 _*! 
dF = ——= e 2 dx 
V2n 
with intervals of width 0-5. 
Interval uontrod at 
ir> 
20 
2 5 
3 0 
3-r> 
4-0 
4-5 
5-0 
T0TA*L 
Frequency. 
0-855,91 
0-278,34 
0-092,45 
0-024,02 
0-004,89 
0000,78 
0 000,10 
0 000,01 
1-056,50 
The distribution has high-order contact at one end but not at the start of the curve, 
being in fact J-shaped and very abrupt at that point. 
The following table shows the raw moments about the mean up to the fourth order, 
74 
MOMENTS AND CUMULANTS 
the moments with Sheppard's corrections and the true moments calculated from th© 
continuous' normal distribution :— 
Moment. 
A*3 
A*4 
Baw. 
0-158,524 
0-104,228 
0-149,090 
Exaot. 
0172,222 
0098,812 
0-158,405 
Corrected. 
0-137,691 
0-104,226 
0-131,097 
It will be noted that in the two cases where the corrections are made they operate in th© 
wrong direction. For the fourth moment they increase the difference between calculated 
and true values from about 4 per cent, to about 16 per cent. It is clear that, at least for 
the fairly coarse grouping of this example, Sheppard's corrections may fail completely. 
3.19. Equations (3.43) were written in terms of moments about an arbitrary point. 
This point can, in particular, be the mean of the distribution, and accordingly we may- 
drop the dashes and put fa equal to zero in (3.43), to get the corrections appropriate for 
moments about the mean. 
3.20. The discussion of the Sheppard corrections up to this point, and Examples 
3.13 and 3.14, have,supposed that the given frequencies were those of a distribution 
which was exactly specified by a continuous mathematical function. In practice this 
case very rarely occurs, the most common necessity for grouping corrections arising when 
moments are calculated from tables 'such as those of Chapter 1. For such tables it is 
not possible to state categorically that the corrections will result in an improvement ; 
but there are usually strong presumptions to that effect. Consider, for example, th© 
height data of Table 1.7 (Example 3.7). There can be no doubt that the histogram provided 
by this material can be graduated by a smooth curve and that such a ourve will give better 
values of the moments than the histogram. Moreover, the tailing-off at the extremes of 
the distribution supports the assumption that the conditions for terminal contact are 
' satisfied. It may therefore be confidently assumed that Sheppard's corrections as appliod 
to the grouped data will give improved values for the exact values of the moments which 
would have been' derived from the ungrouped data had they been available. 
Average Corrections 
3.21.- There is a distinct type of problem which also leads to the Sheppard corrections. 
Suppose there is given a distribution of unknown range and the frequencies falling into 
specified intervals, one may ask what are the corrections to be applied to the raw moments 
so as to bring them on the average into closer relation with the real moments. In other 
words, supposing that the interval-mesh is located at random on the distribution, what 
are the average values of the raw moments ? 
Let X] be a fixed set of values of x, j varying from — oo to oo oy integral values. As 
xt varies from X k_ to X, j±, xk varies from X. n to X, /u 
By definition 
jist — 00 
X] + f ) # 
AVERAGE CORRECTIONS 
75 
"Denoting by E{p!r) the average as xf varies from XJ h to X h, we have 
7-5- J+5" 
J 5" ~ H" 
- da;^ 
«>_ ,.*!+£■ 
1 vH f ' 5" f2 
2 
1 f fa 
^aL ^J fc/(* + *)d^^ • • . (3-46) 
which is the samo as equation (3.40) with the omission of 8m+1 and the substitution of 
E{p!r) for p!r. Thus the Sheppard corrections apply for the average group-moments 
whatever the nature of the terminal contact. 
They cannot, however, be applied indiscriminately on that ground. In place of the 
conditions about terminal contaot, which ensure the applicability of Sheppard's corrections 
to any particular distribution, there is the condition that the grouping intervals are located 
at random on the range, which implies that although the corrections may be wrong in any 
given instance, the average effect in a large number of cases will be oorrect. In actual 
fact the condition about the random location of grouping does not operate very frequently 
ior J- and U-shaped distributions, where the Sheppard corrections would not ordinarily 
apply ; for instance, in a distribution of incomes or deaths at given ages it is almost inevitable 
to begin the grouping at zero. ' 
3.22. It is also illegitimate to drop the dashes in order to obtain corrections for 
moments about the moan. If the moan of the grouped distribution is denoted by y, the 
average value of the rth moment about the mean is given by 
ft 
E{fir)" jr.{x ~~y)r r * *+^ ^ ** 
where y is a function of x and the transformation of the integral which has been used earlier 
in this chapter is no longer legitimate. Explicit expressions for average corrections to 
moments about the mean have not yet been obtained. From a consideration of some 
particular distributions, however, Kendall (1938) concluded that for all ordinary purposes 
it is sufficient to use equations (3.43) as if the mean were a fixed point. 
3.23. The Sheppard corrections have also been considered from a slightly different 
point of view (Fisher, 1921). As the centres of the intervals move along the variate axis, 
the raw moments vary according to the different groupings which result; and this variation 
is evidently periodic of period h. We may thus write, 
&= j£rj 2hMdx, 
1- -go C-2" 
76 MOMENTS AND CUMULANTS 
where C - (5 + ^f> 
and may put this equal to 
^o + AsinG + .4a sin 20 + . . . 
-f JBl cos 0 + -Ba cos 20 + . . . 
Then, multiplying by sin sd or cos sd and integrating from 0 to 2n} we have 
A8^- V sm*0d0 F/(a:)<fe 
31 iS^oJo Jc-r 
J — —O0 
A=i V cosS0d0 r/(a)*i 
n <£-» Jo J.* 
and in particular 
1 °° /-2J. ffc-ij 
Since d$ = -=- ^t> we nave 
^°- "(N-Dfc r:+2" 
l vS rw+iw rT2" 
1 f °° f c+2" 
-To 
sJ>),bf>* 
*-a 
2. 
which is the same as (3.41) and (3.46), and thus leads to the Sheppard corrections. 
For the periodic terms we have 
^-C.*T«f[ww*. 
«+*• 
1 2 
For some mathematically specified distributions we are able to consider the magnitude 
8HEPPARDVS CORRECTIONS TO FACTORIAL MOMENTS 
77 
of theae periodic terms. For inBtance, for the normal curve referred to the true mean 
wo have, Mince 
2f"a > . 2; 
tm£ ... & 2^,9cc 
f/£ s= COS ,- COS 7C3 
h 
Ttft h 
2jtMX 1 -*' 
cos - . — e a«" ttj? 
whore* ,.4, and t/f, refer to the coefficients for the corrections to the mean. The grouping 
error of the mean in thtiH 
V* A' mn 0 ~ Je" &'sin 20-f . . . etc/. 
Fur n grouping in which a -- h (a very coarse grouping) thin is, approximately, — e~a*' sin 0 
jr 
rr 
and thus cannot In* greater than a in\ 
3.24. Average correctioiiH may iiIho bo applied to discrete data which have been 
ground in wider interval hut arc different from Uioho of the continuous oase. Cf. Exercise 
:U3 ftud V. ('. Craig (1U3«). 
Shrp]nml\i CnrtretUm* tv Factorial Mammt* 
3.2,5. It liitn been shown by Wold (ll)34a) that for factorial moments the Sheppard 
correction* are as follows : - 
p'm 
fu ■ 
/*!.ii 
/'!*i 
/'';.] h 
/M, 
/Ml 
/Mi " 
/Ml ■ 
/v-.i ■ 
/Ml 
:1 
12 
V1"'1 4 
2/*iai 1 >' /Ml - w/'4 
- I^a- -» J ;.,*■ - i^i.*4 i 
4/lLlj/t 44 S 
■ 
■ 
31 *. 
8 
ri 
and in general are given by 
"w- §G)iaJ',",(^*,rtr"'I• * 
(3.47) 
(3.48) 
78 MOMENTS AND CUMULANTS 
where the Bernoulli polynomial 5}/+2)(f) is equal to 
(- 1)' 
'+i. 
(2j)l 
i)i(* + * 
+ 
+ 
and 
22>(j + l)\\a ' ° % 
42)(f) = l, -Bi3)(l)=0f 
y- 
i>i , 
Sheppard*a Corrections for Cumvkmta 
3.26. As in seotion 3.16, and under the same conditions, we have, writing 6 for it, 
J— — O0 
A 
AJ_A J_oo 
Gh 
2 
J —00 
dx. 
(3.49) 
The expression on the left gives the characteristic function for the grouped data, and the 
integral on the right the true oharacteristio runotion. Taking logarithms of both sideB 
and noting * that 
log 
P 
sinhY_ ^£r(6hy 
6h 
2 
Zu r\r 
r-2 
we have, for the coefficient in ., Kr = Kr — -zzL r > 1 
. (3.50) 
an attractively simple result for the Sheppard corrections to cumulants. Since all i?'s 
of odd order are zero except' Bx and the first cumulant is equal to the mean, no oumulant 
of odd order needs any correction. For the others we have 
K' = Ki + uo 
K6 = K6 <— 
252 
. (3.51) 
* By definition 
and hence 
_6 yiBrQr 
e6 — 1 ™ 2-i r! 
r-0 
ee_l ' 9'+rZ r\ '' 
r-2 
integrating from 0 to 0 -vee have the above result. 
MULTIVARIATE MOMKXTS AND CUMLULANTB 70 
wiping Correction* trhm the Itintrilmtinn in Abrupt 
AJ7* V»riouH writer* have eonHidered the correction*! to be applied when one or 
•til terminal* of the dintrihution do not obey the Nheppard conditiom* for terminal contact. 
I'fiTMHTw are given at the end of thia chapter. 
'ulliiitrmtr Moment* and CumuUwU 
A.28. Tin* foregoing reattlta in thin chapter may be readily generalised to the multi- 
iriate cane* Tt» i4rv«» complicating tht* algebraic exprcMauuiM, we Hhall deal with 
r« variate* tt and jr9\ but the reader will have Htttc* difficulty in carrying out any 
<nerali*ationji for more variatcM. 
The bivariate moment ftft about an origin «t for a?x and nt for *, i« defined by 
/'„ - P r (^i -«i)Vi - •*#*? . - ♦ . (3.«2) 
J * J *> 
T one of r,* in jc.ero the moment IweumeH the cmlimiry univariate moment of the row or 
tttuitut Imnler dt»trihuttou of the bivariate population. In the contrary oaac w» meet 
nru ty|*e of moment theprtKluct moment. The Hint product moment /**n in of particular 
nftttrtaiw in the theory of correlation. The tlr»t product-moment about tha variate 
,teaii», ,**,,, i* known a* the t'ovarianee, 
A» in the univariate eoae, bivariate momenta about certain (aunt a can be Pxprcmrod 
ii trrum »f thiwe alHmt other {Kiiittn. If the j>x origin i« transferred from a% to bx whew 
, /*, «i|. and the jt origin from r/» to btt when* rt - bt - «», we have 
**>!«•) - </'' r-'iJV M.)* .... (3.n») 
*liere the |iriHttiet /i*V* OM tm* rWn1 in to l*e replaced by ft]k{bt fct). Thin correaponda to 
he nvmlHiltt' equation 
or the univariate ca*e. 
Mh)u*U of calculating the product momenta for iiumerienlly ajK*eified tUatrihutuma 
Ail) Ik- rutmidi'rril in Chapter 14. The determination of bivariate moment* for a mathe- 
niHirully HjHHitie«t population m a matter of evaluating double huhih or double iutegrala, 
tnd no tirw HtAtiMtieal pointH call for comment. 
fCxttmjiU .7 /*i 
The hmiriate distribution 
a.irt^,{i p')1 I -(i />8)\"» *i<*i «i/J 
1,1't u* evaluate :^— 
Making the nuhntitution, 
80 MOMENTS AND OUMULANTS 
we find 
M{tu tt) = exp{i(*?or? + 2t1tta1aaP + t\a\)} X 
h*LL-i-*h&-3+$h+ 
2jro,lo,a(] 
= «Pft(*M -f 2po1<rtt1ti + t\a\)}. 
V ft 
Now fin is the coefficient of - -^ in M(tlt ta) and thus we find, for instance, 
Mio = °i> Mu = Pfft0i, Mot = 02 
M*o = Mn = Mti = Mo* = 0 
Mio = 3crf, pn = 3por?ora, ,uM = (1 4- 2/»l)o»for|, 
Mia ■ = %P0102> Mo* = 3ff2. 
3.29. The bivariate. analogue of equation (3.22) may be written 
exp nor + onr + * • -^2 + - • t 
— 14. &*1 4- ^01*' 4- 4- ^W« 4. ('A 541 
or symbolically, 
exploit! + *,)* j = Zyiih + j52)p .... (3.66) 
where ^ «,, + *,)» = pl{«|§ + "f1^ + " " j ' ' " <3"56> 
In terms of characteristic functions we may define 
#!,«.)= ei^+^UdF . . . .(3.57) 
J —00 J —00 
and, as before, write 
<f>(tu U) = £ M> 
°°- . («,>■ (**«)' 
r, 8«=0 
00 
= exp< 
ra r! a\ 
(iti)r(ita)a\ 
Z^'m.. . . .(3.«, 
Lr, s=l J 
subject to conditions of existence. 
From these equations the bivariate moments can be expressed in terms of bivariate 
oumulants and vice-versa. It is also possible to derive bivariate equations from the univariate 
equations by symbolic processes, (of. Kendall, 1941). 
3.30. Wold (19346) has given the following expressions for Sheppard corrections to 
bivariate moments and oumulants, the variates being grouped in intervals hti A„. 
Y~i vH ■Llyrr/r\/s 
i-Q fc-0 
Mr*=2j z, ml Ju)(2W " 1)(21~*" 1)B*Bk &-*«-« • • (3-69) 
» 
MEASURES OF SKKWNESS 
81 
In particular 
/%> = P-'w — VaA?, /Hi ~ £n, /<ia — /4i - iV**; 
/'an ^ f*ao "~ /''mji /<« — /*« ~ Mtu,!,* *umI two symmetrical equations 
» Aa 7 A2 
/'*u ~ /*iu - /"4i0l 4- ;l4l.Af. /<ai ~ Mai - Ma *, and two Hymmotrical equations 
-. AS - A? , A*Ajj 
''»» - **» ~ *"'l2 ~ **12 + 144 
For cumulanta wo havo 
Kn — Kn% T, S 
crll 
lrl) 
wl)« -" K'i»i 
nrk\ 
m 
> o 
r > 2 
* >2 
. (3.60) 
. (3,61 
Mmmtrra of flkwnicm \ 
3.31. Wo have eonsidered measures of location and dispersion in Chapter 2. With 
the aid of the momenta we ean now proceed to eonHitler measures of other qualities* of the 
population, and in particular its departure from Nymmetry. 
In a Mymmetrical population, mean, median and mode coincide. It in thus natural 
to take the deviation mean to mode or mean to median as measuring the skownesH of the 
distribution. K. Pearson prupowd the measure 
Nit- 
Mean tnoile 
which is mihjcot to the inconvenience of determining the mode. For a wide cIuhm of fre* 
quency-diHtrihu firms known as Pearson's (cf. Compter II), Hum measure may, however, be 
expressed exactly in terms of the first four momentm. We ilelino 
Pi 
/U 
/<i 
Then it may be .shown that for Pearson eurvcH 
^(H//, Cl/i, !)) 
(.1.U2) 
(a.nii) 
vUH) 
and thiH equation may be taken an defining a meaHure of skewness applicable to all 
distributions whose moments up to and including the fourth exist. 
The coefficient /J, itaelf in also a measure of skewness. Clearly if the distribution iri 
Hymmelrical it vanishes since //„ vanishes, and the size of//3 relative to ft[ (or \'fix) will 
indicate the extent of tho departuro>£rom nymmetry. 
a.s. ' o 
82 . MOMENTS AND CUMULANTS 
i 
Generally we may define 
D /M3/M2»+8 
P2n+l "S+T 
^ \ (3.66) 
O ^2»+a 
Pa. - -7+r 
quantities which are not in general use but will be found to occur occasionally in statistical 
literature. 
More convenient quantities than fix and ^2 for certain purposes are 
yx =£}--! -•'.,. . . . (3.66) 
y8=£i_3=^ (3.67) 
If the distribution is expressed in standard measure, yx and yt are its third and fourth 
cumulants. 
Kurtosis 
3.32. In the so-called " normal " distribution 
1 -1 i' 
dF = —1=& a <*• dx, — co < x < oo 
oV2n 
/Sa attains the value 3 and y2 is zero. Curves for which yt = 0 are called Mesokurtic. 
Those for which y2 > 0 are called Leptokurtic and will, relative to the normal curve, be 
sharply peaked. Those for whioh yt < 0 are called. Platykurtic and will be flat-topped. 
Example, 3.16 
For the distribution of Australian marriages considered in Example 3.1 we found, 
for the raw moments about the mean in units of three years, 
/Z2 = 7-066,977, /Zs = 36-151,596, /Z« = 408-738,210. 
With Sheppard's corrections these become 
/Lit = 6-973,644, jua = 36-161,596, ^ = 406-238,888. 
From these values we find 
/St = 3-854, yx = 1-963 
/3, = 8-333, yz = 5-333 
indicating considerable skewness and leptokurtosis. 
Example, 3.17 
* From the formulae for the moments of the binomial distribution considered in Example 
3.2 we find 
q —p 
-\/?ipq 
1 — 6»<7 
y% = — 
npg 
so that, as n —>■ oo, yt and yt —> 0. This is in acoordance with a result" we shall prove later, 
that the binomial tends to the normal form as n tends to infinity. 
MOMENTS AS CHARACTERISTICS OF A DISTRIBUTION 83 
Moments as Characteristics of a Distribution 
3.33. The use of moments and cumulants in determining the nature of a frequency- 
distribution will be abundantly illustrated in later chapters, but some general remarks may 
be made at this stage. 
It has been noted that the characteristic function- determines the moments when 
they exist, and it will be proved in Chapter 4 that the characteristic function also 
determines the distribution function. It does not, however, follow that the moments completely 
determine the distribution, even when moments of all orders exist. Only under certain 
conditions will a set of moments determine a distribution uniquely, but, fortunately for 
statisticians, those conditions areobeyed by all the distributions arising in statistical practice. 
For all ordinary purposes, therefore, a knowledge of the moments, when they all exist, is 
equivalent to a knowledge of the distribution function : equivalent, that is, in the sense that 
it should be possible theoretically to exhibit all the properties of the distribution in terms 
of the moments. 
3.34. In particular we expect that if two distributions have a certain number of 
moments in common they will bear some resemblance to each other. If, say, moments 
up to those of order n are identical we know that as n tends to infinity the distributions 
approach identity, and consequently we expect that by identifying the lower moments 
of two distributions we bring them to approximate equality. ., Some mathematical support 
for this so-called Principle of Moments may be derived from the following approach : 
It is known that a function which is continuous in a finite range a to & can be repre- 
f 
J a 
sentcd in that range by a uniformly convergent series of polynomials in x, say y tPnix) 
where Pn(x) is of degree n. Suppose we wish to represent such a function approximately 
n „ 
by the finite series of powers y^anxn. The coefficients an may be determined by the 
principle of least squares, i.e. so as to make 
(f-£anxn)*dx (3.08) 
a minimum. Differentiating by a^ we have 
2f {f - Zanxn)x! dx = 0 
J a 
rb rl> 
or \ fx1 dx = fi'j = \ £aHxn+J dx (3.69) 
J a J a 
If now two distributions have moments up to order' n equal they must have the same 
least-squares approximation, for the coefficients an are determined by the moments in 
virtue of (3.69). Furthermore, if in the range the distribution /x differs from £anxn by 
£i and f2 by e2, then /x differs from /„ by not moro than bx -J- s2. 
A similar line of approach may be adopted when the range .is infinite, the distributions 
in such cases being, under certain general conditions, capable of representation by a series 
of terms suoh as e~x'Pn(x). (Cf. Chapter 6.) The same conclusion is reached. 
Thus distributions which have a finite number of the lower moments in common will, 
in a sense, be approximations one to another. We shall enoounter many cases where, 
although we oannot determine a distribution function explicitly, we may ascertain its 
* 
84 MOMENTS-AND CUMULANTS 
moments at least up to some order; and hence we shall be able to approximate to the 
distribution by finding another distribution of known form which has the same lower 
moments. In practice, approximations of this kind often turn out to be remarkably good, 
even when only the first three or four moments are equated. 
Mean Values 
3.35. To conclude this chapter we may note that the moments are particular cases 
of a general class of functions known as Mean Values. If we have a function ip(x) defined 
in the range of a distribution, then 
r 
y)(x) dF, . . , . < . . (3.70) 
if it exists, is called the mean value of ip(x) for that distribution; it is sometimes written 
as E{y)(x)}} a notation we shall often find useful. The moment'of order r is thus the mean 
value of xr and the characteristic function is the mean value of eitx. The letter E in this 
connection is the first of the word " expectation," and mean values as we have defined 
them are sometimes known as " expected " values, particularly in the theory of probability. 
The objection to this practice is that only rarely is it to be expected that we shall meet with 
the " expected " value in sampling. 
, 3.36. Two important properties of mean values are to be noted. In the first placo, 
if we have two functions ip^x). and ipt(x), 
JVi dF + Jy2 dF = Jfo + y.) dF 
and thus 
F(Vi + ?■) = F(Vi) + #M (3-71) 
i.e. the mean value of a sum is the sum of the mean values. 
Secondly, if we have two independent variates xx and x2 distributed with functions 
Fit Fs; and if yx is a function of xx and tp9 of xi} then 
| Ljy,dFxdF2 = \y)xdFx L2dF2 
or, 
j%iVi) = jE(y>i)jE(y>,) (3.72) 
so that the mean value of the product is the product of the mean values. This iR in gonornl 
only true if the variates are independent, whereas (3.71) is subject to no such restriction. 
NOTES AND REFERENCES 
In most of the literature what have here been called " cumulants " are referred to as 
semi-invariants or seminvariants. They were introduced by Thiele (1889), who, however, 
failed to draw a clear distinction between the parameters of a population and estimates 
of those parameters from a sample, with the result that for some years there was a confusion 
between semiiinvariant parameters and semi-invariant statistics. (This is in no way to 
be interpreted as a criticism of Thiele, who could hardly have been expected to write fifty 
years ahead of his time.) Some recent work by Dressel (1940) has shown the desirability 
of reserving the name " seminvariant" for the more general class of parameters which 
NOTES AND REFERENCES 
86 
are, except for powers of k2, invariant under transformations of the origin. Dressel points 
out the analogy between such parameters and the functions of the coefficients of the binary 
form. 
aQxn + r\a1xn-*ly + . . . + r\arxn~ryr -f . . . a^, 
which are invariant under transformations of type 
£ = he + m, y = ?;. 
The word " seminvariant " has been in use for many years in the theory of algebraic 
invariants to denote such functions. The word " cumulant " is due to Fisher and Wishart. 
A comprehensive account of the mathematical relations between moments, factorial 
moments and cumulants is given by Frisch (1926). 
There is an extensive literature on corrections for grouping. Kendall (1938) gave 
a bibliography which appears to be complete except for the omission of a paper by Fisher 
(1921) and one by ^Elderton (19386). For corrections in the case when the Sheppard 
conditions are violated, see Pairman and Pearson (1918), Sandon (1924), Martin (1934), 
Pearse (1928) and Elderton (1938a). For Sheppard's corrections for a disorete variable 
(which appear to be due to H. C. Carver) see Craig (1936) ; and. for the corrections in the 
multivariate case see Wold (19346). 
References to the problem of moments (i.e. the conditions under which a set of constants 
can form the moments of a distribution) are given at the end of Chapter 4. As to the 
mathematical basis of the principle of moments, seeMerzrath(1933) and Romanovsky (1936). 
Craig, C. C. (1936), " Sheppard's corrections for a discrete variable," Ann. Math. Statist. 
7, 55. 
Dressel, P. L. (1940), " Statistical seminvariants and their estimates with particular 
emphasis on their relation to algebraic invariants," Ann. Math. Statist., 11, 33. 
Elderton, Sir W. P. (1938a), Frequency Curves and Correlation, Cambridge University Press. 
(19386), " Correzioni dei momenti quando la curva e simmetrica," Oiorn. delV 1st. 
Ital. Att., 16, 145. 
Fisher, R. A. (1921), " On the mathematical foundations of theoretical statistics," Phil. 
Trans., A, 222, 309. 
Frisch, R. (1926), " Sur les semi-invariants et moments employes dans l'etude don 
distributions statistiques," Oslo ; Skrifter af det Norske Videnskaps-Akademie, II, 
Hist.-Fihs. Klasse, No. 3. 
Kendall, M. G. (1938), " The conditions under which Sheppard's corrections are valid," 
J. Boy. Statist. Soc, 101, 592. 
(1941), " The derivation of multivariate sampling formulae from univariate formulae 
by symbolic operation," Ann. Eugeu. Lond., 10, 392. 
Martin, E. S. (1934), " On the correction for moment coefficients of frequency distributions 
when the start of the frequency is one of the characteristics to be determined," 
Biometrika, 26, 12. 
Merzrath, E. (1933), " Anpassung vonFlachen an zweidimensionale Kollektivgegenstande 
und ihre Auswertung fiir die Korrelationstheorie," Metron, 11, No. 2, 103. 
Pairman, E., and Pearson, K. (1918), " On corrections for the moment-ooefficionts of 
limited range frequency-distributions when there are finite or infinite ordinates 
and any slopes at the terminals of the range," Biometrika; 12, 231. 
' 86 MOMENTS AND CUMULANTS 
Pearse, G. E. (1928), " On corrections for the moment-coeffioients of frequency-distributions 
when there are infinite ordinates at one or both terminals of the range," Bwmelrika, 
20A, 314. ' 
Romanovsky, V. (1936), "Note on the method of momenta," Biometrika, 28, 18H. 
Sandon, F. (1924), " Note on the simplification of the calculation of the abruptness coefficients! 
to correct crude moments," Biometrika, 16, 193. 
Shohat, J. (1929), " Inequalities for moments of frequency funotions and for various 
statistical constants," Biometrika, 21, 361. 
Thiele, T. N (1889), Theory of Observations (reprinted in English in Ann. Math. Statist., 
1931, 2, 165). 
Wold, H (1934a). "Sulla correzione di Sheppard," Qiorn. delV 1st. Ital. Att., 4, 304. 
'(19346), "Sheppard's correction formulae in several variables," Skand. Aktitar.f 
17, 248. 
EXERCISES 
3.1. Show that the rth moment about the origin of the distribution 
dF = kx-t>e~y/x dx 0 < a; < co, y > 0 
i8 lL _rrr(p -r-i) 
if r <p — 1, and does not exist in the contrary case. 
3.2. In the distribution 
(oj2\ - m _ i J" ' 
1 + ^J e-'to* a dx - oo < x < oo 
show that, about the origin, 
[ir = kaT+1 (* cos *»- r~2 0 sin' 6 e~"° dO 
and hence that 
^ = 2m ~r - 1{(r " 1)a^~2 ~ *&-&' 
the vafues^T, *?* ^ *S^tinU0US distributi™ whose frequencies corresponding to 
■ [l'Tl""J'-j 
has, for the moments about the mean, 
^ = m, ^ = m, ^ = m(l + 3m), Ms = w(1 + 10m), ^ = w(1 + 2rm + lfiwt)< 
3.4. Show that for the distribution whose frequencies for variate-values 0, 1, .../,.. . 
are the successive terms in (i + l)\ [ e (1\* |~1 (n\ (n\ 1 i. ' ' ' ' * 
order except the first vanish. WW J' 
EXERCISES 
87 
3.5. Show generally that the oumulants of odd order vanish for any symmetrical 
distribution, except the first. 
3.6. Show that eita} may be expanded in an infinite series, vahd in — oo < x < oo, 
1 + (e«_i) sW + (e«_l)a ^ -f . . . (e«-l)' ^ir+ . . . 
the factorials being taken with unit interval; and hence that 
* 
d 
where 
d = 
tf(e«) 
w 
Henoe show that, for the binomial (q -f p)n about the origin, 
/"w = nlrlPr> 
3.7. Show tfiat the distribution whose frequency at the variate-value ± 2r (f integral) is 
r*4* 
+ 
a 
2r+2 
+ 
a 
2r+<t 
\0!(2r)! l!(2r + 1)1 ' 2!(2r + 2)1 
+ 
...} 
and at ± (2r + 1) is 
1(> + 1)1 ^ 1! 
a2r+3 a2r+5 
■+■ st/st.—r—^Ti + 
••■} 
.!(2r + 3)1 r 2!(2r -f 5)1 
has odd-order oumulants equal to zero and even-order oumulants equal to 2a, 
3.8. Show that for the distribution 
1 ~a 
dF = - e dz, 0 < x < oo 
a 
Kr = (f(r - 1)1 
3.9. Show that 
and hence that 
«r = (- I)-' 
i«3 
* - iG - !)*-« 
(:>■ 
h"2 
0 
1 
Pr \ Q . jPr-1 ( j )pr-i 
0 
0 
0 
0 
(r - J)*1 
8S MOMENTS AND CUMULANTS 
3.10. Show that for the distribution 
dF = dx, 0 < x < 1 
grouped into an integral number of intervals of equal width h, the corrections to the second 
and fourth moments about the mean are 
(Cf. Mderton, 19386. Note that the first is exactly, and the second approximately, the 
Sheppard correction wiik sign reversed.) 
3.11. If dp stands for the operator such that 
dpp'r = r[pyr-p> r>p 
= 0 r <p 
and dp is distributive when applied to products, e.g. 
Bp(AB) = B(dpA) + A(dpB), 
show that dp annihilates every cumulant (considered as a function of the moments) except 
Kpi and that 
(Gf. Kendall, 1941.) 
3.12. If f(x) is an odd function of x of period \, show that 
f xrx-lo«xf{\ogx)dx = 0 
Jo 
for all integral values of r. Hence show that the distributions 
dF = x-los*{\ — I sin (2n log x)}dx 0 < x < oo 
0 < X < 1 
have the same moments whatever the value of X. (Stieltjes. See refs. to Chapter 4.) 
3.13. Show that if the frequencies' of a discontinuous distribution are distributed 
at equal intervals —, m in each grouping interval h, the average grouping corrections to 
the cumulants are given by 
K -r -£5-7i- M 
(Cf. Craig, 1936.) 
3.14/ LiapounofFs inequality for moments. Beginning with the inequality 
{Zab)* < (£a*){Zb*) 
show that for positive values 
ai+ai\2 
\Ex 2 ) < [Zxt){Zx?). 
EXERCISES 89 
Hence that 
(2Si.aw/P)P < (2VXl)(i7a^)( . . . )(2sf») 
is true when p is of form 2m. Hence show that it is true for any integral p by noting that 
if 2m is the smallest power of 2 greater than p we may take 
oti-f- . . . Op 
aj3 + l — a23 + 2 — • • • 1*2771 — ~ * 
Hence, putting p = a — c, «.,, = . . . == aa_& = c, att_6_1 = . . . aa_c = a, show that v 
(The inequality remains true for a continuous variate, as may be seen by considering limiting 
processes.) , 
3.15. Show that for the bivariate distribution 
dF = 5 r, «i exP - 571 lA^i - -^-8 + ^i\ dx* dx» — oo < xlt xt < oo 
2jror1or1!(l - p2)* r . 2(1 -/02)[orf 0^ or|J 
all cumulants Krg} r, s > 2, vanish ; and further, if 
i f^rs 
°1°2 v 
^ = (r + 3 - 1)^,, ._x + (r - )(* - 1)(1 - p2)^, a_2 
_ (2r)l(2a)l ^i (2P)* 
(r-j)!(4-i)I(2j)I 
(2r + l)!(2s + 1)! vr (2p)»' 
A*+l. 2*+I ~ 2r+a P 2^ (f _"j)!(5 _ j)!(2;. + 1}j 
^2r, 2«+l = *2r+l, 2a — ^> 
where < is the smaller of r and a. In particular, 
^■u = P> ^si = 3p, Abi = 15p, hx =405p, A91 = 945p ; 
Att = (1 + 2p«), Aa4 = 3(1 + 4p2), AH = 15(1 + 6p2) 
A28 « 105(1 + 8p2), X2> lfl = 945(1 + lOp*); 
A38 = 3p(3 + 2p*), A38 = 16p(3 + 4p2), 
A.. = 3(3 + 24p* + 8p*). 
CHAPTER 4 
CHARACTERISTIC FUNCTIONS 
Moment- and Cumulant-Generating Functions 
4.1. In the previous chapter we considered the characteristic function 
<f>(t) = r &dlT-2/j£$! . . . . (4.1) 
as a moment-generating function. We have also 
y(t) = log <f>(t) = £*&£, ..... (4.2) 
y)(f) being known as the Cumulative Function. It generates the cumulants in the same 
way that the characteristic function generates the moments. If the moment of order r 
exists, <f>(t) can be expanded in powers of t at least as far as the term in (it)r, and so can y)(t). 
Other functions can be constructed which generate the moments. For example, 
since for | to \ < 1 
- >>)' 
1 —tx ^Li 
we have the formal expansion 
f00 dF ' f-r „ , 
Generally if a function £(t) can be expanded as a power series in t, Zap we have, subject 
to existence (and convergence when the series is infinite), 
P f(to) dF = EafiiAj (4.4) 
Since 
vS x™ 
v + 'r-Zjr 
1-0 J 
we have , 
co(t) = I (1 + t)*dF = Y^/t'm .... (4.G) 
J700 1=0 °' 
and thus co(t) may be regarded as a factorial moment-generating function. We may also 
define a factorial cumulant-generating function 
t 
though this function has not come into general use. 
log CO(t) = U -ACyj . . . . . . . (4.6) 
4.2. The generation of moments is by no means the most important property of the 
characteristic) function, and in this chapter we discuss some of the theorems which give 
it a fundamental place in statistical theory. 
♦ 90 
THE INVERSION THEOREM' 91 
We recall, in the first instance, that <f>(t) always exists, since 
I f°° eUxdF\ < p \eilx\dF 
| J — 00 1 J — 00 
dF = 1 . . . . (4.7) 
J —00 
so that the defining integral converges absolutely. Further, <f>(t) is uniformly continuous 
in t and differentiable j times under the integral sign if the resulting expressions exist 
and are uniformly convergent, for which it is sufficient that vi exists. For then 
| <j>V\t) | « I f" a>V* dF | 
j J —00 
< f I x11 dF = v, (4.8) 
J —oo 
The Inversion Theorem 
4.3. We now prove the fundamental theorem of the theory of characteristic functions, 
which will be called the Inversion Theorem, namely that the characteristic function uniquely 
determines the distribution function; more precisely, if $(t) is given by (4.1) then 
F(x)-F(0)=±^J(t)±^-f?dt . . . .(4.9) 
the integral being understood as a principal value, i.e. as 
1 re i p—txt 
lim ^ Mt)-—t-—dt. 
e—^ooZttj -c 
y>m 
Further, if F(x) is continuous everywhere and dF = f(x) dx 
f{X) = Ml ^)e"to' * <4-10) 
the integral, as before, being a principal value if there is not separate convergence at the 
limits. Equation (4.10) may be compared with the form 
/•OO 
<f>(i) = fWe^dx, (4.11) 
J —00 
the comparison exhibiting the kind of reciprocal relationship which exists between f(x) 
and <f>(t). 
As a preliminary we require an integral due to Dirichlet. It is easy to show that 
= 1 
00 sin x j n 
ax 
Putting 
• J rut X 
we have 
J = u0 — ux + uz . . . 4- (— \)rur + . . . 
in which the terms decrease monotonically to zero ,in absolute value. Now let H(x) be 
a positive decreasing function. Consider 
"'" ' "~ sin x 
'-IXD^-C"'-""© 
X 
dx. . .. (4.12) 
92 CHARACTERISTIC FUNCTIONS 
Writing H( + 0) as the limit of ^(e) as e—> 0 (e positive), we have, in virtue of the decreasing 
property of H, that any term in the series on the right in (4.12) is not greater than unH( + 0). 
^ "^ Cnn /x\ sin x 
Further, as the series alternates in sign, the difference between /„ and H[ -) dx 
P Jo \V) * 
is less than unH(-\- 0), which tends to zero uniformly in n. Consequently Ip is uniformly 
convergent and we have 
Similarly we have 
,?zS--affir*"fBi-0) ■ • ■ -(i- 
14) 
By a simple change of sign the results are seen to be true if H(x) is a negative increasing 
function. It is therefore true of any function which can be expressed as the sum or 
difference of a positive decreasing function and a negative increasing function, and in particular 
of a frequency function or distribution function. 
Adding (4.13) and (4.14) and writing #'(0) for 1{H{ + °) +#(- 0)} we have 
lim f h(-)B— dx = 7tH(0) .... (4.15) 
and putting px for x in this expression, 
lim f" H(x)B-^^dx=7iH(0), .... (4.16) 
p . > oo J —oo •*' 
the so-oalled Dirichlet integral. If H(x) is continuous at x = 0 the value 
i{jff(+0)+JH(-0)} 
is of course the usual value H(x = 0). 
Now consider 
Jo 
= r t(t)dt[ e-**d£ (4.17) 
J-c J 0 
Putting 4>(t) = f eiUdF(x) 
J —oo 
we have Je = f dtlf eitj dF(x)i e~^dA. 
The product in ourly brackets may be equated to the double integral 
f* f e^x-^dF(x)d^ 
JO J-oo 
which is evidently uniformly convergent. Making the transformation 
y = x - £ 
we have 
(Zf e^dFJi(y+z)dz 
JoJ-00 
THE INVERSION THEOREM ' 93 
and integrating with respect to z, 
= f ^{F{y + X)-F{y)}dy. 
J —00 
This also is uniformly convergent in t and hence, integrating under the integral sign with " 
respect to t, we have 
j°=r r^T ^+*> -F^} ^ 
J — oo L. *2/ J —c 
= f ^~-^y + x) - *w<fc .-.. (4.18) 
j —oo y 
since cos cy is an even function. 
Now (4.18) is a Dirichlet integral and we have, therefore, 
lim Jc = 2n{F(X) - F{0)} (4.19) 
c—>- oo 
Referring again to (4.17) we thus have, writing now x for X, 
F{x) - F{0) = -1 lim f <f>{t) dt[Xe-M d£ 
and integrating with respect to £, 
* 1 re i fi-ixf 
F(x)-F(0) = ± lim <f>(t) £—dt, 
£.71 r—^oo J _,« U 
which is the result stated in (4.9). It is to be remembered that in virtue of our convention 
in arriving at (4.16), F(x) at a saltus is l{F(x +) -f F(x —)}. 
4.4. This expression may be thrown into an alternative form. From the definition 
of <f>(t) it is seen that <j>(l) and <f>(— t) are conjugate quantities, and we thus have 
M(t) = ${<f>(t) + <K~ *)) ' 
R and / being the real and imaginary parts of <f>(t). Thus 
and by a change of sign in t, 
1 r°° 1 — eto' 
_ if* •#(*) sin ^ + /(*)(! - cos a:i) 
" 2^J -oo ~~ * 
This integral is, of course, real. 
dt (4.20) 
4.5. If now F(x) has a derivative f(x) we have 
f(x) = — —- lim <A(f) : .. 
JK ' 2ndx a-**, )-o it 
04 CHARACTERISTIC FUNCTIONS 
The integral being uniformly convergent in x, the differentiation can be carried out on 
the integrand, and we have 
f(x)±±f_J(t)e-™dt, 
the integral being a .principal value. 
4.6. Consider now the expression 
i= U'j®^**'dt ' ' * ' • (4"21) 
If the distribution function F has a derivative /, this is equal in the limit to 
lim i = \im~f(x) =0 
and consequently —c tends to zero everywhere where F(x) is continuous and differentiate, 
i.e. if the frequenoy-distributdon is continuous. 
If, however, the distribution is discontinuous, consider one point of discontinuity, 
say the frequency ff at x$. The contribution of this part of the frequency to <f>(t) will be 
f^e^, and thus the contribution to -^ will be 
ZC 
7 &%(*,-*)J-. " 
If x t6 xi this clearly tends to zero ; but if x ~ Xj it becomes 
Thus the function ~ tends to fia,tx=xi. 
ZC 
Hence, if —c tends to zero at a point x, there is no discontinuity in the distribution 
Zc 
function at that point; but if it tends to a positive number fi} the distribution function 
is discontinuous at that point and the frequency is ft. This gives us a criterion whether 
a given characteristic function represents a continuous distribution or not. 
Example 4.1 
We found in Example 3.10 that the characteristic function of the normal distribution 
1 __£! 
dF = -—p= e 2°* dx — oo < x < oo 
avtn 
is <f>{t) = e-*''a\ 
Suppose we are given such a function and require to find the distribution, if any, of whioh 
it is the characteristic function. 
In the first place we note that the distribution, if any, is continuous. For 
J I Co t%a* 
2c 2cJ _, 
THE INVERSION THEOREM 95 
The integral is less in modulus than I e 2 dt which is less than e 2 dt 
J. — C J—00 
Thus n >■ 0, everywhere. We have then for the frequency function, if any, 
1 r°° -t*oi 
-00 
= —-— I exp — Mta A 1 dt. 
2rcJ_00 • "\ a) 
This may be regarded as an integral in the complex plane along the line parallel to the 
ix 
real axis. Taking ta + — as the, new variable in place of t, we find that the integral is. 
1 f °° 
in fact -I e~*f'dt = 
ffj-oo 
a 
y/2m 
Thus 
Oy All- 
This is everywhere positive and dF converges. Hence it is in fact a frequenoy function 
J —00 
with the given expression as a characteristic function. 
Example 4.2 ' 
To find the frequency funotion, if any, for which 
^(«)=e_l". . 
We note that -^ tends to zero and that the distribution, if any, is continuous. We then 
have for f(x), if it exists, 
f(x) = ^\-\^e-^dt 
-00 
• 00 
= k-[ e~Keitx + e~iix)dt 
InJ 0 
— if 
This may be evaluated by two partial integrations. We find 
fix) = - — e~l cos tx\ — - e~l sin tx dt 
= —+ — e"-"'sinte —'—I e~l cob txdt 
71 nL Jo ^Jo 
•00 
e-' cos txdt. 
0 
Thus 
= - - *2/(*). 
71 
f{x)" aTT^)' -»<■<»• 
96 CHARACTERISTIC FUNCTIONS 
As before, this function can represent a frequency function, and it is readily verified that 
f(x) has, in fact, the required characteristic function. 
Example 4.3 
Does there exist a frequency function for which 
${t) = e* ? ' 
We have 
*L = A fc e*~ito di =-[° e«<i-aOdt. 
2c 2cJ_c 2cJ_c 
If 1 — x is not zero the integral is 
re 
[cos {(1 — x)t} + i sin {(1 — x)t}]dt. 
f 
-c 
Since -sin t is an odd function this is equal to 
j*_!.««i-^}*-[-!*JIfi^\ 
This does not converge, but it is bounded and hence -^ —>• 0. 
If, however, x = 1, the integral is simply 
re 
i: 
<& 
— 0 
and thus -5 = 1. 
2c 
Thus there is unit frequency at x = 1 and it is seen at once that this accounts for 
the whole of the frequency, so that there is no frequenoy elsewhere. The distribution 
thus consists of a unit at x = 1. This is otherwise evident from the consideration that 
log <f)(t) = it, so that the second cumulant is zero and there is no dispersion. 
Example 4.4 
For what distribution, if any, are the cumulants given by #cf = (r — 1)! ? 
The series 
converges absolutely for 11 | < 1 and is thus equal to y(t) if such a function exists! We have 
VQ) = Z{S = - log (1 - it) 
J 
and thus <j>{t) = 
(1 - it)' 
If the frequency function exists we have 
f{x)=^-\ ~—rdt. 
2n}-«> 1 -it 
This integral may be evaluated by integrating the complex function - round a contour 
consisting of the real axis and the infinite semicircle below that axis. The first part reduces 
THE INVERSION THEOREM 97 
to the integral we are seeking. On the semicircle of radius R we have z = i2(cos 0 + *' sin 0) 
and the integrand becomes 
exp (— ixB cos 6 + xR sin 0) 
1 — iR cos 6 + iR sin 0 
0 here lies between n and %t and hence sin 0 is negative. Hence ifxis positive the expression 
- is less in modulus than 
e-xB 
~R~> 
i.e. tends to zero as R—>■ oo. 
g—iXB 
Now the function - has a pole within the domain of integration at z = — i and 
■i ~~■ tz t 
the residue there is e~x. Hence 
= e~x 0 < x < oo. 
More generally, if /cr = p(r — 1)!, # > 0, it will be found that the residue of j= ^ 
is —>y~V~> so *na* ^le distribution is 
/(*) = p/ v , 0<ar<oo, p>0. 
Example, 4.5 
For what distribution, if any, are all cumulants of odd order zero and those of even 
order a constant, say 2a % 
We have 
vW = 2a{if + f +.. j. 
This sorieR converges and 
y)(t) = 2a(cos £ — 1) 
Honco <f>{t) = e2R(coa '_l). 
/•oo 
If wo try to integrate l e8«(oos*-n e-*tr^^ in the ordinary way we fail. Let us 
J —00 
then look into the question of continuity of the distribution function. 
Wo have 
J0 = e~-"[ ein cm l q-mat 
J —c 
= e-2«fc Y^~ cob He-™ <1L ■ ' 
Tho series is uniformly convergent and hence 
jo = e-2« Jr P ^~cos '*e_fr' ^ 
= e"2« y7 T i?^ cos H cos arf rft, 
since sinatf is an odd function. 
A.S. H 
I 
98 CHARACTERISTIC FUNCTIONS 
/•OO * 
Consider now the integral I 2' cos H cos xt dt. By a well-known expansion 
J —00 
2> cos H cos xt = i(e* + e-*)^' + e_iarf) 
The only part of this expression of present interest is the constant term, the others not 
contributing more than a finite amount to Jc. The coefficient of e° is zero unless x is integral 
in absolute value, and in that case is 
3 \ (5 Vj / i, 
J -« J + I j_±_x J > = ( 3 -x 
Thus -^ tends to zero unless x is integral in absolute value, and in the latter case 
2c-*"6 MZji\\L 
i^r*r; ,•- 
Thus, if a; is even, say 2r, the frequency at x = ± 2r is 
fi-2«J_^l i J^T^/2^ + 2\ g»+« (2r + 4\ "1 
\(2r)l ^ (2r + 2)lV 1 / ^ (2r + 4)!\ 2 j"1" ' ' 'J 
= e-*4^L 4 a2r+2 + fl2r+4 ■ + ' I 
\(2r)\ x (2r + 1)111 ^ (2r 4 2)121 ^ * ' "J' 
and if a; is odd the frequency at x = 2r 4 1 is 
c—f "*+-1 + ^+3 + «2r+g , 
\(2r 4 1)! (2r 4 2)111 (2r 4 3)12! n 
We may now verify that these frequencies account for the whole of the characteristic 
function and hence that all frequencies have been found. 
Conditions for a Function to be a Characteristic Function 
4.7. Any function whioh is not negative in its range of definition and which is 
integrable in the Stieltjes sense oan be a frequency funotion; and any non-decreaaing 
funotion which increases from 0 to 1 in its range of definition oan be a distribution function. 
There are muoh more restrictive conditions to be obeyed before a given function can be 
a characteristic function. 
In the first plaoe, let us note that it is a necessary and sufficient condition for a function 
<f>(t) to be a characteristic function that , 
£■71J _oo W 
shall (except for an additive constant F{0)) be a distribution function. This, however, 
is not a very helpful criterion in practice. 
DISTRIBUTION AND CHARACTERISTIC FUNCTIONS 99 
♦ /"CO 
Looking to the definition of <j>(t) as eitx dF, we see that necessary conditions for 
J —CO 
${t) to be a characteristic function are 
(a) that <f>(t) must be continuous in i, 
(6) that <j>(t) is defined in every finite t interval, 
(c) that <£(0) = 1, 
(d) that ^>(£) and $(— £) shall be conjugate quantities, 
(e) that | <f>(t) | < T Jeite | dF < 1 = <£(0). 
J—CO ' 
These criteria enable us to reject certain functions as possible characteristic functions, 
but there- do not appear to be any readily applicable sufficient conditions which enable 
us to determine at sight whether a given function can be a characteristic function. 
Limiting Properties of Distribution and Characteristic Functions 
4.8. Suppose there is given a sequence of distribution functions Fn{x) depending 
on a parameter n which can increase indefinitely. To each Fn there will correspond 
a characteristic function ^n. The question to be discussed is this : if Fn tends to a limit 
F, will (f>n tend to a limit <f> and is <j> th& characteristic function of F ? Conversely, if <f>n 
tends to a limit <j>, does Fn tend to a limit F and is F a distribution function having <f> for 
its characteristic function ? The answers to these questions, as will be seen below, are 
affirmative under certain general conditions. 
It is to be noted what is meant by a distribution function tending to another. If 
both are continuous, Fjx) is said to tend to F(x) if, given any e, there is an n0 such that 
| Fn(x) — F(x) | < e for all n > w0. If there are discontinuities present, Fn will be said 
to tend to F if it does so in every point of continuity of F. Since by definition ourfunctions 
are taken to be continuous on the left at saltuses, this evidently conforms to the definition 
for the continuous case and to the oommon-sense requirements of the situation. 
4.9. We require two preHminary theorems for later work. The first is that if Fn 
tends to F it does so uniformly. 
For the range can be divided into a finite number of parts, say at f ls £„ . . . £7t, such 
that F{£i+J) — F(i-j) < - for all j. Then as n increases there will come a time when 
I -^n(£j) ~~ F{£j) < - for all j. Thus there exists an n0 such that for n> n0 
| Fn(^) - Fft) | < |. 
It is sufficient to show that this implies that for any x 
^ | F„(x) - F{x) | < e, n> n„. 
In fact, if x lies between £; and fy+1 
Fft) < F(x) < F{*i+1) < F{$) +1 
and 
Ftf,) - \ < Fn{£t) < Fn(x) < Fntf,+ l) < F($j,,) + | < Fft) + e 
and thus — £ < Fn{x) — F{x) < e, 
which is the required result* 
100 CHARACTERISTIC FUNCTIONS 
4.10. The second theorem we require (the Montel-Helly theorem) is that if the 
sequence Fn(x) is monotonic and bounded for all x (which is so for distribution functions) 
then we can pick out a subsequenoe Fn, (x) which converges to some monotonic increasing 
function F (not necessarily a distribution function itself, for it may not vary from 0 to 1). 
Consider first of /all a series of values Xi, x%, • • . It is known that every bounded 
set of numbers contains a convergent sequence. Hence we can pick out from the sequence 
Fn{xx) a convergent sequence, say, Fni{xi). Then from the subsequence FUi(x2) we can 
pick out a subsequence F^fa) and F^x) is thus convergent at both xx and x9. Continuing 
in this way we may, by picking out the first function in Fni(x), the second in Fn%{x), and 
so on, arrive at a sequence of functions G\{x), O^x) . . . which converges at each of the 
values x1} x2, . . ., etc. This is the so-called Weierstrassian diagonal process. 
It follows that the sequenoe On is convergent at every rational point x. Since 
On(a) < Gn(x) < On(b) for every a; between a and 6, we see that if On(a) and On(b) converge, 
the limiting values of On(x) lie between those limits, say 0(a) and 0(b). 
Then the function u(x) = upper bound of On(x) (x not necessarily rational) is 
well defined, and non-decreasing and so has no more than an enumerable number of points 
of discontinuity. If u is continuous at x, we take y and z such that y < x < z and 
u{z) — u(y) < e. Then if a and b are rational points such that y <a <x < b <z it 
follows that u(y) < 0(a) < 0(b) < u(z). Moreover, as all the limiting values of On(x) 
are between 0(a) and 0(b), they are between u(y) and u(z). Hence, as e can be arbitrarily 
small, we see that 0(x) tends to u(x) at every point of continuity of u. Finally, by the. 
diagonal process, we can select a sequence which will also be convergent at the points of 
discontinuity of u(x). The theorem is established. 
The First Limit Theorem 
4.11. We now prove the theorem : if a sequence of distribution functions Fu tends 
to a distribution function F, then the corresponding sequence of characteristic functions 
<j>n tends to <j> uniformly in any finite tf-interval, where ^ is the characteristic function of F. 
It is required to prove that, given e\ there is an n0 independent of t such that 
i wo - u*) i=i r e^dF - w*) \<£> n> »•• 
J —no 
Select two points of continuity of F, X and — X. We can make X as largo as we 
please. We then split the integral 
i 
eilx(dF - dFn) (4.22) 
into two parts, that in the range — X to + X and that in the remaining portion of the 
range. Now 
I \r>X eitxdF\ < p"'"* dF < 1 - F(X) -F(- X), 
\Jjr<-X I Jx<-A' 
and by taking Z large enough we can make this quantity less than ^. 
Similarly 
•x>X 
eilx dFv 
* x<X 
i: 
<l-.Fn(X)-Fn(-X), 
THE FIRST LIMIT THEOREM 101 
and since Jn*lfends to F (and that uniformly) this, for some large X, will be less than 
e 
£. Henoe for some n0 the portion of (4.22) outside the range — X to + X will be less in 
£ £ £ 
modulus than »+« = «• Consider now the other part 
O o £ 
J. 
eP*(dF - dFn) (4.23) 
-A' 
This expression is the limit of the sum 
£& £j+i being the boundaries of the interval into whioh the range is subdivided and x^ 
a value in that interval. The difference between this sum and the limiting value can be 
made less than - if the intervals are small enough ; for if they are less than v\ in width the 
difference of eilxl and eitsi is less in modulus than i] \ t |, by the mean value theorem, and 
thus in any tf-range ± T the difference of (4.23) and (4.24) is less in modulus than 
nT | 2[{F[€,+1) - Fft)} - {J*ft(f/+i) ~ **»(«}] I < W> 
E E 
which is less than -. if ri < -7=. 
4 nl 
Now the sum (4.24) will itself be less than - for some n > n^, for it is the sum of a finite 
4 
number of terms eaoh of whioh tends to zero. 
Consequently (4.23) is less than - and henoe 
I ^ (0 - 4>JJ) I < e» n> ri0. 
Converse of the First Limit Theorem 
4.12. The converse result is even more important: 
Lot <(>n be a sequence of characteristic funotions corresponding to the sequence of 
distribution functions Fn. Then if </}n(t) tends to <f>(t) uniformly in some finite ^-interval, 
Fn tends to a distribution function F and ^ is the characteristic function of F. 
As a preliminary lemma, let us prove that if J?1 is a distribution function with 
characteristic function ^, then for all real f and all h > 0 
1 r*+;* 1 Ch If00 /sin A 2 SMfl? A)/\ 
If, '<•> du - iL'M du=JI-(tv e_1 *u)dh - • <*-26> 
In fact, put 
0(x) = A F{u) du. 
This is a continuous distribution function and its characteristic function is 
c«* dO 
^J -c 
■= i f e**» {^(a + A) - Jfa)} da;, 
102 
CHARACTERISTIC FUNCTIONS 
whioh by a partial integration becomes 
ir{f(, + »-Jw>*r _ if" ^mx + h)_dm} 
h\_ it J_oo ithj-to 
= -=i f {e^-V dF{x) - eitx dF(x)} 
1 — e~m 
= <f>(t)- 
ith 
Substituting for G(x) in (4.9) we get 
1 r£+2h l rS+h 
F(u)du-±\ 
whenoe, writing £ for £ + h, we find 
.irS+h If* 1 f00 /I _e-^\a- ., 
J, '« ** - sL'M dtt ~ S*L (—5-) «-*-"*W * 
the result announced in (4.25). , 
Reverting now to the theorem required to be proved, note that it is sufficient to establish 
that if <j>n —> (f> uniformly in some interval \t\ < a, then Fn tends to some distribution 
function F in every point of continuity of F. When this is established it follows from the 
First Limit Theorem that <£ is the characteristic function of F and that $n converges to 
$ uniformly in every finite ^-interval. 
As shown in 4.10, given a sequence Fn we may always choose from it a subsequence 
Fn, suoh that Fn,, converges to a non-decreasing function F in every continuity point of F. 
Let us then choose such a sequenoe. We have of necessity 0 < F < 1, and F may 
be supposed everywhere continuous on the left. It is then a distribution function if 
F(-{- oo) — F(— oo) = 1, and this we proceed to prove.* From (4.25) with £ = 0 we have 
By hypothesis $n tends uniformly to $ for | t | < a and hence <{>n> does so, and it is easily 
seen that the- integral on the right is uniformly convergent. Thus, given e, we can find 
h(, such that for h > h0 
* It is not obvious that if the functions Fn all vary from 0 to 1, then their limit must do so. In 
fact, if Fn{x) = 0, x < — n, Fn{x) = £, — n< x < n, Fn{x) => 1, x > n then lim Fn(x) = $■ for 
n—>m» 
— oo < x < oo. 
CONVERSE OF THE FIRST LIMIT THEOREM 103 
where | r\ | < e. Now let h tend to infinity. As F is a non-decreasing function the left- 
hand side tends to F(-\- oo) — F( — oo). The right-hand side tends, in virtue of the 
uniformity of 0n,and the consequent continuity of <f> near t = 0, to 
Or)'* 
which is equal to unity. 
Hence F, the limit of the subsequenoe Fn,, is a distribution function whose characteristic 
function is (f>. 
■ But any subsequence of <f>n tends to <£, in virtue of the uniformity of the convergence, 
and henoe any convergent subsequenoe of Fn tends to F. Consequently Fn tends to J^ in 
every point of continuity of F and the theorem follows. 
Example 4.6 
The binomial distribution (q + p)n considered in Example S.2 has the oharacteristio 
function 
(q + peil)n. 
Now the frequency at x = j is ( . jqn~'ipi. This is greater than the ordinate at 
x = j + 1 if 
(*y-y > (j I ^-'-y+i 
or j > pn — q. 
For large n the maximum frequency will then be in the neighbourhood of j =? #n, and 
is then 
\pnj 
In virtue of Stirling's approximation to the factorial this approximates to 
nne~n V27tn qWpvn 1 
(jm)^e-Pn-y/{27cpn)(qn)«ne-<^n\/(^mn) ~ Vi^P^Y 
and therefore tends to zero. 
Thus every frequency in the binomial tends to zero and the distribution does not tend 
to any limiting distribution. 
Suppose, however, that we express the distribution in standard measure. Putting 
£ = —— we have 
a 
<f>M = e*te dF(x) = ' d"e*+"i'> dF(£) 
J — 00 , J —00 
= eitfiS<f>s((jt). 
■U"i' (t\ 
Hence <f>s(t) = e « <f>x[-\. 
The eflfeot on <£(£) of transferring to standard measure is then to replace t by - and 
nth.' 
to multiply by e <* . 
% 
104 CHARACTERISTIC FUNCTIONS 
For the binomial ^ = np, fit = npq, and thus the characteristic function of the 
binomial expressed in standard measure is 
Thus 
/ itnp \/ i ** \n 
log </, = t-^ + it log 11 + Wexp 
(«W)1 l\ («OT)* /J 
"^ + nlDg{1+*e^»~£ + 8S>} 0<|,|<1 
[ 2^^ 2 npq J 
= - |i2 + 0(i%-*). 
Thus for any finite t log <f> tends uniformly to — |Z2 and hence ^ 
Thus the distribution {q + jp)71 expressed in standard measure tends to the distribution 
whose characteristic function is e-4'', i.e. to the form 
dF = ,,n xe~ix*dx, — oo < x < oo. 
V(2w) 
Multivariate Characteristic Functions 
4.13. The characteristic function of a bivariate distribution F(xu x%) is defined as 
fl*i, *i) = «***+«*. rfi^, *,) ... (4.26) 
J —ooj —oo 
and generally, that of a multivariate distribution F(xlf xit . . . xn) as 
<jWi, *., . . . tn) = f f ■ . . . f fl**i+«.*.+ • • • «-* dJf"^, *,,... zn) (4.27) 
J —OO J — 00 J —00 
If independent we have 
/■OO /•OO /•OO 
J —oo J —oo J—oo 
= M1W2) ■ ■ ■ <A(U (4.28) 
Similarly 
>i 
xp{tu J, . . . tn) = JT loS W .... (4.29) 
Thus the characteristic function of the joint distribution of a number of independent 
variables is the product of their characteristic functions ; and the cumulative function 
is the sum of their cumulative functions. This is a fundamentally important result in the 
theory of sampling. 
4.14. In generalisation of (4.9) we have 
i r r00 
F(x1} xs, . . . ztt)=_j^ . . . j_ 
%tx ttn 
<f>(ti, tt, . . . tn) dA-L . . . dtn. . . . (4.30) 
MULTIVARIATE CHARACTERISTIC FUNCTIONS 105 
The multiple integrals are to be interpreted as principal values 
C—>°°J_c J_c 
The proof is similar to that for the univariate case. We have 
Jo Jo «i *n \2/ 
lim r . . . r #fe . . . 3?) ^i . . . iiBi ^ . . . efa, = n"H(+0 . . . +0) 
p—>■ oo J—oo J—oo \P P/ %i Xn 
lim f . . . r H{xu . . . xn)^-^ . . . Si^^^1 . . . dxn = H*ff(0, ... 0). 
J) >- 00 J — 00 J — 00 "&1 #jj, 
Considering now 
* 
Jc = ... 0(«:, . . . i„)^! . . . dtA ... exp(—itx^i ... — itjn)d^ . . . d£n (4.31) 
J-c J-c J0 Jo 
we find that 
J—oo J—oo *i ^n 
■*■' (#i> • ■ ■ *^») /flW'i • ■ ■ u%n' 
lim Jc = (27r)»{J,(*1, . . . xn) - F(0, ... 0)} 
C >• 00 
and by considering the integration of (4.31) with respect to the £'s the result (4.30) follows. 
4.15. If we have a distribution F(x) and some function of the variate such as £{x) 
we may consider the characteristic function of f 
<f,((t) = f eP*dF(x) (4.32) 
J —00 
The distribution of f will then be given by (4.9) or (4.10), e.g. the distribution function of 
£, say #(£), is 
<?(£) = i-f exp {-itg)<f>e(f) dt (4.33) 
The Problem of Moments 
4.16. We can now consider in more detail a problem which suggested itself in Chapter 
3. Do the moments determine the distribution uniquely, and if not, under what conditions 
do they do so ? To give some point to this question let us note that in some circumstances 
it is possible for two different distributions to have the same set of moments. 
Consider in fact the integral 
f" p-h-* dt = ^, p > 0, q > 0. 
Jo Q 
(ll _L 1) 
Put p = -—2—- n a non-negative integer 
0<A< I 
q = a + fc/9 
1 - = tan ?&i 
a 
x* = t. 
j: 
106 CHARACTERISTIC FUNCTIONS 
We find on substitution that 
a^e_oa,*{ooB (/to* tan fot) + i sin {fiat- tan hi)} Xdx . . . (4.34) 
a^L+J(l + t- tan ybr)B4-l 
j • /i i ■+ i \?Ljr-i cos (to + l)?r + isin(w + l)?r 
and smoe (1 + a tan Ate) * = *—■—-—' . , —!—- 
(oos x&rp£-= 
— a real quantity, 
the imaginary part of (4.34) is zero. Thus the distributions 
fix) = Ae-«*{l-+« Erin (j9a^ tan Are)} . . . .(4.35) 
0<a;<co, oc>0, 0 < X < \, \ e |\< 1 
have moments independent of e, and (4.35) defines a whole family of distributions having 
the same moments. 
Similarly, if we substitute 
p = - i—', g — a -f ifi, *- — tan £_, of = t, p — As a positive integer) 
p a 2 8 -{- 1 
w6 find that the family 
f(x) = ^e-«l^p|l +■ e cos ^a[a|" tan ^|'. . . . (4.36) 
2s 
— oo < a; < oo, <x>0, 0<<o = —— <1, |e|<l 
8 4" 1 
all have the same moments, the range in this case being infinite in both directions. 
*■- 
4.17. In full generality the problem of moments may be formulated as follows: 
Given a sequence of constants c„, clt . . . Cj . . ., 
(i) Does there exist a distribution function F such that 
-ft 
x*dF =cr ? . . . . ' . . (4.37) 
(ii) If so, is the distribution function unique ? 
(iii) What are the functions, if any ? 
We have not the space here to enter on a full discussion of these questions, which have 
stimulated some beautiful mathematics, particularly by Stieltjes (1918). Our treatment 
will be oonfined to the results of statistical interest, but we may indicate the principal 
results of Stieltjes. 
If we express the series 
r 
Ja 
JT(- 1)'| • • . • . . . (4-38) 
z1 
1-0 
as a continued fraotion of the form 
1 11 1 
axz 4- a2 4- a3z 4- a4 4- • a2n_xz 4- «2n + 
then, if the limits in (4.37) are 0 to oo, it is a necessary and sufficient condition for the 
THE PROBLEM OF MOMENTS 107 
existence of at least one F that all the a'a be positive ; and F is unique or not according 
00 
as y af diverges or converges. 
1" l 
The case when the limits in (4.37) are ± oo has been treated by Hamburger (1920), 
who showed that an F exists if the expression of (4.38) as a continued fraction of the form 
b0 b-L bt 
a0 +. z + fl^ + z + a>s + 2 + 
. (4.40) 
gives positive values of the fe's. In order that F may be unique it is necessary and sufficient 
that the continued fraction be completely convergent in a sense defined by Hamburger. 
We shall see presently that for finite limits in the integral of (4.37) the funotion F is 
always unique. 
4.18. Unfortunately the Stieltjes-Hamburger criteria are not of muoh praotioal use 
beoause, as a rule, it is too difficult to express the a'a and b'a of (4.39) and (4.40) explicitly 
■• enough in terms of the given c's to enable questions of sign or convergence to be deoided. 
We ma^, however, derive some criteria of statistical importance by considering the more 
restricted problem : given the moments of a distribution, oan any other distribution also 
have the moments ? In other words, we are given the existence of one F and require 
to know whether F is unique. 
Note in the first instance that this problem need only be considered when absolute 
moments of all orders exist. It is evident that more than one distribution oan exist having 
a limited number of moments finite and the remainder infinite. Furthermore, if any 
moment of even order exists, those of lower order must exist. In particular, if ju,2r exists 
poo pO r°° 
,x2rdF and x2r dF exist separately, and hence so do | a;2'"-1 | dF and 
J0 J -oo JO 
| a;2r_11 dF, and so also does I | x2r_1 \ dF, the absolute moment of order 2r — 1. 
J —00 J —«> 
Thus we consider only the oase when all absolute moments exist. 
4.19. We will prove in the first place the theorem that a set of moments dotennines 
a distribution uniquely if the series } -4p converges for some real non-zero t. 
/=■() •>' 
The oharaoteristic funotion is oontinuous in t and its derivatives exist if the, moments 
exist. We have then in the neighbourhood of t = 0 
+® = Zir* + ^ • • • • • <4-41> 
where JSr is less in absolute value than —~ (3.14). 
Thus if —~- converges, 4r- tends to zero and hence <j>{t) is equal to the sum of the 
infinite series // !, ■ if it exists. Moreover, this series is majorated by —i— and hence 
fe-0 3 3 
108 CHARACTERISTIC FUNCTIONS 
is absolutely convergent if the latter is convergent. Hence we have 
00 / -AJ ' 
m _ £&M (4.42) 
and thus <f>(t) is uniquely detennined in the neighbourhood of t = 0. In the neighbourhood 
of t = t0 we have 
m = ^WL^^jzWo'dF} + R'r 
and the modulus of the coefficient of -——- is not greater than vs. Consequently ${t) 
3- 
can be expanded everywhere aa a convergent Taylor series and is equal to the sum of that 
series. Hence <f>(t) may be extended from the neighbourhood t = t0 by analytic continuation 
through any finite (-interval. Hence <f>(t) is everywhere uniquely defined. 
But <f>(t) determines the distribution function and hence the latter is uniquely 
determined. 
4.20. As a corollary of this theorem we have the result that a set of moments Uniquely 
determines a distribution if 
l 
^ *£ is finite (4.43) 
7l->00 % 
V t"1 
For the series Z-~- is convergent if 
that is to say, in virtue of the Stirling approximation to the factorial, if 
I i 
hm -^—-] s= hm-5-6 <1. 
i_ 
If h is the upper limit of — this inequality will be satisfied for t 
It is also a sufficient condition for uniqueness that Hm ^-'^ should bo finite, a form 
of the criterion which enables us to disregard the absolute moments. In fact 
so that 
1 „ k~ ^ 2n 1. J. 2w + 1 l i 
Taking upper limits throughout we have 
1 1_ I 1_ 
and thus Km-V and nm-^sn are finjte or infinite together. 
THE PROBLEM OF MOMENTS 109 
4.21. As a further corollary we have the result that a set of moments uniquely 
determines a distribution if the range is finite. For suppose the range is a to 6. Taking an 
origin at x = a and letting b — a = c, we have 
a4 = ■' 
J a 
xPdF <cn. 
Thus * /u'nn < c = v* 
i_ 
and hence lim — = 0. 
n 
4.22. Two further^ criteria may be mentioned. The first is due to Carleman (1925). 
A set of moments determines a distributibn uniquely if (in the case of limits — oo to + oo) 
Y —^ (4.44) 
■diverges. For the limits 0 to oo the corresponding series is 
oo 1 
V r (4.45) 
Secondly, if there exists a frequency function, the moments determine it uniquely 
if, for limits — oo to + oo 
f(x) <M\x\ 0-ie-a|*|A for | x \ > x0 (M, p, a, > 0 X < 1) . . (4.46) 
and for limits 0 to oo 
f(x)<M\x\ P-ie-*x]* for | x | > x0 (M, 0, a, > 0 X < \) . . (4.47) 
This result is due ultimately to Stieltjes. It follows without difficulty from the Carleman 
criterion. 
It is interesting to note that if for some xa 
f(x) > e-«^X (a > 0) . . . . (4.48) 
then the problem of moments is necessarily indeterminate (as usual, X < \ for the range 
0 to oo and X < 1 for the range — oo to + oo). This follows from the examples in 
equations (4.35) and (4.36), for we can add to (4.48), without rendering any frequency 
negative, a function all of whose moments are zero. 
Example 4.7 
The moments of the distribution 
1 _£l 
dF — —7= e 2a« dx, — oo < x < oo 
aV2n 
are given by f*2r+i = 0 
110 
CHARACTERISTIC FUNCTIONS 
n nv2[ n\ J 
a_ r(e-*n(2n)*nV(4nn)\^ 
\/2j\ e-nnnV(27m) J 
ny/2 
a 2n 
n\/(2e) w* 
a 
~ \/{2e)n* 
and thus the upper limit is zero and the distribution is unique. 
4.23. If the moment of order r exists it must be given by 
Thus if (j>(t) can be expanded in an infinite Taylor series, that series must be E ^—~ 
fori 
Further, if this series does not converge, <j)(t) cannot be expanded as an infinite Taylor 
series. But it oan always be expanded in the finite form with remainder 
/ = () •* 
Thus, when the series does not converge, <f>(t) oan be expanded in powers of t only 
asymptotically. 
This illustrates the source of the ambiguity in the definition of <j>(t) when the infinite 
series Z~ (jls does not converge, for it is known that there exist an infinite number of 
funotions which have a given set of coefficients in an asymptotic expansion. For instance, 
if a.(t) has an asymptotic expansion in t the functions cn{t) ~\~ki~k)St all have the same 
v i? (it)r 
expansion. It is therefore hardly surprising that when JC-^j- or H——fi'r fail to oonverge, 
there may be more than one frequency or distribution function with the same set of 
moments. 
But it does not follow from what has been said that there miist be more than one 
frequency-distribution. There must be more than one function, but those functions may 
not qualify as frequency-distributions, e.g. they may be negative in part of the range. 
In the example just given, t~loet cannot be a characteristic function, for it does not obey 
the well-known oondition that <f>{t) and 0(— t) should be conjugate. So far as I am aware, 
it is not known whether the condition that 2J^—-/ij should converge is necessary as 
well as sufficient for uniqueness. 
i 
The Second Limit Theorem 
4.24. We are now in a position to prove a further theorem on the limits of 
distribution functions. If a sequence of functions Fn(x) has all moments existing and for all 
j ju,](n)—>- fa, then the ^'s are the moments of a distribution function F which is the 
limit of the sequence Fn, provided that F is completely determined by its moments. 
r 
THE SECOND LIMIT THEOREM 111 
We will first prove the rather more general theorem: If there is given a sequence of 
distribution functions Fn such that all moments of Fn exist, and for any j the sequenoo 
Hj(n) lies between fixed limits independent of n, then a subsequence F^, can be selected 
from Fn such that 
(1) lim I xj dFn, exists, = fi} say. 
(2) The subsequence Fn> converges to some distribution function F. 
i 
x1 dF exists, and is equal to yiy 
(3) f 
J —c 
The existence of ^ may be proved by the diagonal method exactly as- for the Montel- 
Helly theorem of 4.10. By hypothesis, fi'^n) is uniformly bounded and the rest of the 
proof follows that of 4.10. 
The existence of J1 follows also from the Montel-Helly theorem. We apply the theorem 
to the subsequence derived by satisfying condition (1) and hence arrive at a subsequenoe 
obeying both (1) and (2). It must however be shown that F is a distribution functionj 
i.e. varies effectively from 0 to 1. This follows because 
i 
b ° 
, (4.49) 
and hence, for the subsequence, with r = 0 and letting n' tend to infinity, 
0 < 1 - F(b) < i±A 
bA 
0 < F(a) < l±& 
so that, as a, 6 tend to infinity the equations F{co) = 1, F(— oo) = 0 are seen to hold. 
We also require for later parts of the proof two results : the first that the convergence 
of 
Urn PW dFn to f rf dFn . 
a—>-— oo, b—>-«o J a J — oo 
(4.50) 
is uniform with respect to n. This follows from the hypothesis that ju,[2rf 2)(n) is bounded 
and from the equations (4.49). The second is that 
lim Xs{1 - Fn(x)} =0, Urn | x8 \ Fn(x) = 0 . . (4.51) 
X—>-oo x—>■— oo 
for s > 0 and all integral n > 0. The first limit follows from 
and hence from 
6i{l-JP-n(6)}<^L, 6>o, 0<s<2j. 
We now have to complete the proof by showing that I x1 dF exists and is equal 
J —00 
w 
112 CHARACTERISTIC FUNCTIONS 
to fif. For this we use the theorem (an extension by Fr6chet and Shohat (1931) of one 
due to Helly) that if a sequence vn(x), defined in the interval — oo to 4- 00, is such that 
(1) vn(x) is of bounded variation in any finite interval, 
(2) all vn(x) and their total variations are bounded in any finite interval, 
(3) lim vn(x) = v(x) exists for all x, except perhaps at a denumerable number 
n—>*» 
of points, 
(4) f(x) dvn(x) converges uniformly with respect to % to I f(x) dv(x) if f(x) is 
everywhere continuous, 
/•OO /"OO 
then f(x) dv(x) exists and = lim f(x) dvn(x). 
J—oo n—>-ooJ—00 
This result may be applied to our sequence Fn(x), which obeys conditions (1), (2) 
and (3). It also obeys (4) when f(x) = x1 in virtue of (4.50) and (4.51). Further F(x) 
is of bounded variation and hence I x1 dF(x) exists and equals, say, /li'j. 
J —00 
Finally 
I /*; - ^(n) I = I f AdF - dFn) 
I j —00 
< x7 dF + x1 dFn + \\ x1 dF 
| J —00 | | J —oo | Jb 
+ f x1dFn + f x' (dF - dFn) \ a < 0, &•> 0. . (4.52) 
\ Jb | , | Ja | 
By taking — a and 6 sufficiently large we can make the first four terms on the right as 
small as we please, for — a < — a0,b > b0 and some n > n0. Then by taking n sufficiently 
large we can make the fifth term as small as we please (without affecting the smallness 
of the other terms). Hence | /u'f — fj,](n) \ may be made as small as we please. 
This establishes the more general result. The theorem enunciated at tho beginning 
of the section follows as a corollary. In fact, if jn'j(n) tends to a limit /a), then the 
subsequence Fn, can always be selected and tends to a distribution function F with tho 
moments ft], AH we have to prove is that if the ^ are such that they uniquely determine 
F, the sequence Fn itself converges to F. 
Suppose that there exists a point of continuity x0 such that ^(^0) does not converge 
to F(x0). Then a subsequence Fn>(x) can be selected which converges to some other value 
at xQ. But from this Fn, we can select a subsequence Fn„ converging say to F,.(x), having 
the same moments as F(x). Since by hypothesis these moments uniquely determine 
F, Fv must be the same as F in all points of continuity, i.e. 
lim Fn.,(xQ) = F(x0). 
n" 
This is impossible, for ^"(^o) is .a subsequence of Fn,(x0) which converges, but not to 
4.25. The above proof can hardly be described as easy, though it depends only on 
simple notions such as continuity and convergence, but the Second Limit Theorem is so 
important that it has seemed worth while reproducing the proof in full. Many examples 
of its application will occur in the sequel. The chapter may be concluded with an 
illustration of its use in determining the limiting forms of distributions. 
THE SECOND LIMIT THEOREM 113 
Exainple 4.8 
W? 
The discontinuous distribution whose frequency at x = j (j = 0, 1, . . .) is e~n-^ 
has a characteristic function 
<j>(t) = exp m{eit — 1), 
and hence all cumulants equal to m. 
« (it)f (it)i 
The distribution is evidently the only one with such cumulants, for E *£-— = mZ^-4- 
i 3]- J]- 
is convergent and equals m(eit — 1), so that the cumulative function and the characteristic 
function are uniquely determined. 
Now as m tends to iniinity the frequency at x}, e m-rr, tends to zero and thus the 
distribution does not tend to a limit. This is consistent with the behaviour of the 
cumulants, which increase without limit. 
Suppose, however, we express the distribution in standard measure. Then 
_ m __ 1 
Kr 7 ^2- 
kb2 m 2 
Hence as m —> oo all cumulants higher than the second tend to zero, and hence the 
cumulants of the distribution tend to those of the normal distribution 
dF = -—!—^ e-M'-Vm)' dx, — oo < x < oo 
V(27r) 
with the mean ra*. 
Now we know that this distribution is completely determined by its moments 
(Example 4.7). We also know that the cumulants determine the moments and vice-versa, 
so that if the cumulants of the discontinuous distribution tend to those of the normal 
distribution, the moments will tend to the moments of that distribution. Hence the 
Second Limit Theorem is applicable, and the discontinuous distribution does in fact tend 
to the normal form when expressed in standard measure. 
NOTES AND REFERENCES 
The idea' of the characteristic function can be traced back as far as Laplace, but its 
introduction into the theory of statistics, through the theory of probability, is mainly due 
to Poincare and Levy (1925), whose book provides the most readable and complete account 
of the function. More recent researches are outlined by Cramer (1937). The proof of the 
First Limit Theorem is substantially that given by Levy. The converse, given originally 
in a somewhat less general form by L6vy, was proved simultaneously by him and Cramer, 
the proof in 4.12 following the latter's. 
The Second Limit Theorem seems to have been first proved by Markoff for the case 
when the limiting form is the normal distribution dF = —^- e~iJ,'dx. It was subsequently 
considered and extended by several writers, the general form of 4.24 being due to Frechet 
and Shohat (1931), whose proof has been closely followed. Some references to prior work 
are given by these authors. 
The problem of moments appears to have been first considered and solved by 
A.S. I 
114 
CHARACTERISTIC FUNCTIONS 
Tchebycheff. The memoir by Stieltjes (1918—the memoir being first published in 1894) 
is olassical. For some subsequent work see Hamburger (1920) and Carleman (1925). 
Carleman, T. (1925), Les fonctions quasi-analytiques, Gauthier-Villars, Paris. 
Cramer, H. (1937), Random Vanables and Probability Distributions, Cambridge University 
Press. 
Frechet, M., and Shohat, J. (1931), "A Proof of the Generalised Second-Limit Theorem," 
Trans. Am. Math. Soc, 33, 533. 
Hamburger, H. (1920, 1921), " Uber eine Erweiterong des Stieltjesschen Momentproblems," 
Math. Annalen, 81, 235, and 82, 120 and 168. 
L6vy, P. (1925), Calcul des probability, Gauthier-Villars, Paris.. 
Stieltjes, J. (1918), Recherches sur les fractions continues, CEuvres, Groningen. 
EXERCISES 
4.1. Show that if a frequency function f(x) is symmetrical the characteristic function 
is an even function, i,e, <f>(t) = <£(— t), and that therefore <j>(t) is real; and conversely, 
if <^(t) is real the frequency function, if any, is symmetrical. 
4.2. Show that the function 
<f>{t) — I—: 1 > n a positive integer, 
is the characteristic function of a distribution function 
F(x) = l-j^ - (fy(x - l)n + (£)(x - 2)* 
■ }• 
J 
4.3. Show that the factorial moment-generating function co(t) of the binomial (q -f- p)n 
is (1 -\-pt)n, and hence that 
Piri = Vr w[r]. 
4.4. If for a certain distribution 
Kr = bar, 
a and 6 being positive constants, show that the distribution is discontinuous with variate- 
e~bbr 
values 0, a, . . ., ra, . . . and the frequency at ra equal to " 
r\ 
4.5. Show that the function e~l cannot be a characteristic function unless a = 2. 
4.6. Show that there is only one distribution with moments given by 
, _ i> + r) 
^~ P(v) 
and that it is 
dF = ___e-*aj»-i dx 0 < x < oo. 
Z» 
EXERCISES 115 
4.7.- A theorem due to Weierstrass states that any function continuous in the range 
(a, b) can be represented by a uniformly convergent series of polynomials yPn(x), Pn(x) 
n=0 
being of degree n in x. Deduce that if two continuous frequency functions, fx and ft, 
have the same moments of all orders, 
P (/i-/a)2^ = 0, 
Ja 
and hence that the moments determine a distribution uniquely if it is continuous and of 
finite range. 
4.8. If 6 is a non-negative function of the variate x and 
a(*)=f 0*dF(x), ] 
J — 00 
show that the frequency function of '0, if any, is given by 
4.9. Show that if a characteristic function <f>(t) possesses derivatives up to and 
including the second order, then 
and generalise this result. 
4.10. A theorem of Denjoy (Comptes rendus, 1921,173, 1399) states that if a function 
f(x) defined in a range (a, b) possesses derivatives of all orders, if Mn is the maximum of 
l/ln ix) I m the range and if-27—j- is divergent, then f(x) is completely determined by 
its value and that of its derivatives at a single point. Use this result to show that a set 
of moments determines a distribution uniquely if 17—j diverges. 
CHAPTER 5 
STANDARD DISTRIBUTIONS—(1) 
5.1. There are certain distribution and frequency functions which, for both theoretical 
and practical reasons, occupy a central position in statistical theory. In this and tho next 
chapter we shall consider their properties, leaving their statistical uses to bo dovoloped 
and illustrated later in the book. We shall, however, indicate briefly some of tho way.s 
in which they arise, even at the expense of anticipating ideas introduced at a subsoquont 
stage. This will not impair the logical continuity of our development and will givo con- 
creteness to a treatment which might otherwise appear somewhat abstract. 
The Binomial Distribution 
5.2. Suppose we have a large population of members each of which exhibits oithor 
some quality P or a complementary quality Q (= not-P), for oxample, a population of 
men who are either blue-eyed or not-blue-eyed. Suppose that the proportion of individiuvlM 
with quality P is p and that with quality Q is q, where of course <p -\- q = I. If we take* 
a random sample of N members from the population we expect that on tho average j)N 
members will exhibit P and Nq will exhibit Q. We may thus array the members according 
to the quality as 
N(p + q). 
Now suppose we choose N pairs of individuals. There will be pairs PP, pairs PQ, pairs QP 
and pairs QQ. Of the Np pairs for which the first member is P there will, on the avorujn<\ 
be a proportion p for which the second member is P and q for which it is Q. Similarly 
for the Nq exhibiting Q in the first member. Thus the pairs may be arrayed as 
Np(p +q)+ Nq(p + q) = N(p + q)'1. 
Generally if we choose N sets of n the array will be N(p -|- </)«. That is to say, 1 ho 
proportion of cases containing j P's and [n — j) Q'a will be ( . Wr/7l-/> the term in pit/'1 "/ 
m (P + <l)n- We are then led to consider the binomial distribution 
f=*(P+q)n (-i.l) 
as a discontinuous frequency-distribution, the variato being the number of P'h in tho set 
of n, which may vary from n to 0. If, as is frequently more convenient, wo wish to 
tuinsider the variate as increasing from 0 to n, the distribution is inverted, i.e. becomes 
/=(? +P)n (~>.2) 
in 
5.3. Distributions very close to the binomial form occur in practice, partieularl\ 
artificial experiments with coin-tossing or dice-throwing. Some data by Weltlon' uiv 
shown in Table 5.1. Weldon threw 12 dice 26,306 times and noted the values at cut-h 
throw. This is equivalent to the drawing of samples of 12 from a large population. Tho 
occurrence of a 5 or a 6 on any die was regarded as the exhibition of tho quality /* a 
" success " as we may call it. ' 
116 
THE BINOMIAL DISTRIBUTION 
117 
TABLE 5.1 
Frequency-distribution of 26,306 Throws of 12 Dice, the Occurrence of a <5 or 6 being 
counted a Success. 
No. of 
Successes. 
0 
1 
2 
3 
4 
5 
Observed 
Frequency. 
185 
1,149 
3,265 * 
5,475 
6,114 
5,194 
Theoretical Frequency 
from the Binomial 
26,306 
(0-6623 + 0-3377)" 
187 
1,146 
3,215 
5,465 
6,269 
5,115 
I 
No. of 
Successes. 
6 
7 
8 
9 
10 and over 
Total 
Observed 
Frequency. 
3,067 
1,331 
403 
105 
18 
26,306 
Theoretical Frequency 
from the Binomial 
26,306 
(0-6623 + 0-3377)1' 
3,043 
1,330 
424 
96 
16 
26,306 
If the dice were perfect (a condition- rarely realised in practice) the proportion p of 
successes would be -i; and the appropriate binomial would be, in the form (5.2), (f + £)12. 
In this particular case the dice were not quite perfect, the proportion of cases exhibiting 
a 5 or a 6 being 0-3377. Taldng this as the value of p, we get the frequency function 
"(0-6623 + 0-3377)12, which when multiplied by the total frequenoy 26,306 gives the 
theoretical frequencies shown in the third column of Table 5.1. The agreement with 
observation is evidently fairly good. 
5.4. Taking our variate to be inoreasing, we have, from (5.2), that the frequency at 
x = j is ( . jqn'lpl. The characteristic function of the distribution is then 
*»=Z{(t)qn~¥em) 
— (q + peil)n .... 
We then have for the moment of order j about the origin, from (3.11), 
and hence 
and so on. We find 
«- £[!<*+**">i. 
fi2 = np + n(n — l)p -J 
^a = npq .... 
fz3 = npq(q -p) 
fj,t = 3nzp2q2 + pqn(l — 6pq) 
and hence 
fi»i (npq)* 
= /?i* 
y% = 
fit — 3/iaz _ 1 — 6pq 
Pi 
npq 
Pa — 3. 
(5.3) ' 
(5.4) 
(5.5) 
(5.6) 
(5.7) 
(5.8) 
(5.9) 
118 STANDARD DISTRIBUTIONS 
5.5. Further formulae are not often required, but when they are can be derived 
from some interesting reourrence relations connecting the moments of the binomial. 
Writing 0 = it we have, for the characteristic function referred to the mean as origin, 
<f>(t) =■ e-»»"»(gr + pe°)n (5.10) 
Differentiating with respect to 0 we find 
yj±r— = _ npe-nvd(q + peP)n + ne-nt>6(q -f- pe6)"'1 pe6 
^j(i — 1)! 
fcO-Di 
n flrf a n 
and hence, after a little re-arrangement,- 
<*+HZif^i ~npq{e? ~ 1}§f 
Identifying coefficients in 6r l we get 
r-2 . ,. r-2 
fir=npqj£^ , p~P2^\ j 1)^+l5 * " " (5-H) 
giving the moment of order r about the mean in terms of those of lower orders. 
Furthermore, writing the moment about the mean as 
we have, differentiating with respeot to p, 
pr. _ _ mZti-npr-ifyr-ttf - z{j - »i»)r^)sry-1(»-i)jp'+ zti - np^V-'ip'-1. 
The first term on the right is — mp ±. The sum of the other two will be found to be 
—m - npY+1(n\*-irf = — zl.+1. Hence we find 
i"r+l =^TO_! +^)- ■' ■ • .(5.12) 
For example, fix = 0, //B = wp<7 = np{\ — #) and hence 
= wp<L\9. - P) 
as stated in (5.6). 
For factorial moments the expressions assume a particularly simple form. In fact, 
differentiating (q + p)n r times partially with respect to p and multiplying by pr we have 
nw 
(q + pr-*? = 2^) j[rv~¥ 
and henoe since £ -J- jp = 1 
/ 
THE BINOMIAL DISTRIBUTION 
119 
5.6. If p = q the binomial distribution is obviously symmetrical. If p ^ q the 
distribution is skew. But in both oases it will be unimodal unless pn is small. For the 
frequency of the (r + l)th term is greater than that of the rth so long as 
or 
or 
(n 
-r-lpr+l ■■■>. 
{n — r)\r\ 
l)!(r + 1)! 
which is equivalent to 
r + 1 p 
n — r q' 
<v 
r + 1 
<P- 
n + 1 
Hence the frequency increases until the point when .(r + 1) > p(;n, + 1) and then declines 
again*. Some typical distributions are shown in Table 5.2. 
TABLE 6.2 
Terms of the Binomial Distribution 10,000 (q + p)*° for Values of p from 0-1 to 0-5. 
(Figures given to the nearest unit.) 
Number of 
Successes. 
0 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
p = 0-1 
q = 0-9 
1216 
2702 
2852 
1901 
898 
319 
89 
20 
4 
1 
— 
— 
— 
— 
— 
— 
— 
— 
\ 
p = 0-2 
q = 0-8 
115 
576 
1369 
2054 
2182 
1746 
1091 
545 
222 
74 
20 
5 
1 
— 
— 
— 
— 
— 
— 
p = 0-3 
q = 0-7 
8 
68 
278 
716 
1304 
1789 
1916 
1643 
1144 
654 
308 
120 
39 
10 
2 
— 
— 
— 
— 
~ 
p = 04 
q = 0-6 
5 
31 ' 
123 
350 
746 
1244 
1659 
1797 
1597 
1171 
710 
355 
146 
49 
13 
3 
— 
—. 
~^~ 
p = 0-5 
q = 0-5 
— 
2 
11 
46 
' 148 
370 
739 
1201 
1602 
1762 
1602 
1201 
739 
370 
148 
46 » 
11 
2 
' 
5.7. The ordinates of the binomial are most directly calculated from the formula 
( . jg™-^ ; for low values of n the calculation is straightforward and for high values 
assistance can be derived from tables of log n\ The calculation of the distribution function, 
120 STANDARD DISTRIBUTIONS 
which is equivalent to the summation of terms of the binomial, is tedious to perform directly, 
but use may be made of the tables of the incomplete ^-function. We have, for Taylor's 
series with the integral form of remainder 
na + h) = y K-jim + fhr{1 ~ yrW + *)dt- • • <5-13> 
^ r- Jo (*■ - 1)1 
Putting a = q, h = p and f(a + h) = (q + p)n we have 
r—1 
(q + 3,). = £(n\<?-W + ^ 
where Br is the remainder after r terms and equals 
= p ^1 - y-' »! ( + ^ * . m M) 
J0 (r — 1)3 (n — r)! v ' 
In (5.14) put * = 1 - -. We find 
(r - 1)!(» -r)!J„ 
7\n + 1) _ 
r(r)/>-r + i) ^ r + 1) 
_jg,(r,n -r + 1) 
B(r,n —r + 1) 
= i^(r, n — r + 1) (5.15) 
in the usual notation. This is also equal to 
1 -Ig(n -r + l,r) (5.16) 
by a well-known property of the ZJ-function. 
The remainder after r — 1 terms is, similarly, lp(r — 1, n — r + 2), and hence the 
rth term is 
- Ip(r, n - r + 1) + Ip(r - 1, n - r + 2) 
= 7fl(w - r + 1, r) - Iq(n - r + 2, r - 1). . . . (/>.17) 
Example 5.1 
When n — 20, r = 11, jo =* 0-4 we have for the remainder after 11 terms 1^(11, 10) 
which from the tables is found to be 0-127,521,2. The value given by summing the last six 
terms in the appropriate column of Table 5.2 is 0-1276, the error in the last place being due to 
rounding up. The remainder after 12 terms is I0.i{l^, 9) = 0-056,526,4. The 11th term 
(10 "successes ") is then the difference of these two remainders = 0-0710, as shown in 
Table 5.2 for the frequency per 10,000 of 11 successes. 
The Poisson Distribution ,, 
5.8. Cases sometimes occur in which the proportion p of " successes " in the 
population is very small. We may suppose our number n large enough to render np itself appreci- 
THE POISSON DISTRIBUTION 121 
able though J? is small; and we are thus led to consider the limiting form of the binomial 
(5.2) as p —>- 0 subject to the condition that np remains finite, and equal to X, say. 
Under these conditions the term 
\rpq (n — r)\r\ n\ n) 
• —r 6 
■v/(2rc)(» — r)n~r+l e~n+r nr t\ 
1 & x X x 
—e~ *■ r-j — e . 
( 
r\n „ r! rl 
1 er 
n) 
*~X[l*Ti> ■ ■ ■ C • • .^ (5.18) 
Thus the terms of the binomial become the successive terms 
V* ir ■ ■ ■ fi 
This is called the Poisson distribution, having been given first by Poisson in-1837. It 
has since been discovered independently by several other writers. 
From the point of view of characteristic functions we have 
4(t) = lim (q + peil)n 
= lim {l + -(e« - 1)1" 
= expJt(e« - 1) (5.19) 
which is readily verified to result in the distribution (5.18). 
Thus 
0 
and hence all cumulants of the Poisson distribution are equal to X. We thus find 
Pi =^ 
t*>% = % 
jia = X 
^ = X + 3A2J 
If we let n—>■ ooin (5.11) and (5.12) we find 
r-2 
y,(t) = A(e« - 1) = AZ^ 
. (5.20) 
Vr = AjFr • jftj (5.21) 
0 7 
and fjbr+i = rX/ji^x -\—^-p ...... (5.22) 
Xr 
5.9. Tables of the function e~x-r for various values of X and r are given in Tables 
for Statisticians and Biometricians, Part I. The frequency polygon's are very skew, almost 
J-shaped for low values of X, but become nearer to unimodal symmetry as X increases. 
Xr Xr+1 
A comparison of the successive terms -: and -— - - ■ shows that the frequency increases 
up to the point for which r + 1 < X and then deoreases again. 
122 STANDARD DISTRIBUTIONS 
The summation of r terms in the Poisson distribution may be carried out in a manner 
similar to that of 5.7. The remainder after r terms of the distribution is found to be, 
from (6.13) 
_ rx(r) 
J\r) 
<*>'-> 
. (5.23) 
in the notation of Pearson's tables of the Incomplete /"-function. The argument used 
in these tables is a diffioult one to work with in the present case and, though' formula (5.23) 
may be used for summing a number of terms in the Poisson distribution, it is easier to 
caloulate e~*—. directly rather than to use an analogous expression to (5.17) in the form 
"* *™ = iv^irry ' ~ 2) - '(£-•' - J> 
5.10. We now consider a generalisation of the binomial and the Poisson distributions. 
In 5.2 our approach was based on the drawing*of sets of n from the same population. 
Suppose, however, we draw them from n different populations with proportions 
(2>t,2i) (PuQ*) • • ■ (2>n»?J- 
Then our proportional frequencies will be arrayed by the form 
n 
(Pi + ffi)CP» + 2a) ■ ■ • (pn + ff») = n (Pj + qt) . . . (6.24) 
which of course reduces to the binomial if all the p'a are equal. 
The characteristic function of this distribution is 
<f>(t) = i%, + Pje") 
from which we have 
y>(t)=i:iog(qi+pjeit) 
= iPPPt + ^(P, ~ Pi*) +, ^c, 
(5.25) 
giving >! = Epj \ 
k2 = fa = Epfa) 
Writing now f for the mean of the p'a in the different populations, we have 
tii = nf 
i"B = £pq = £p — Zp% 
= Ep- l{Zp)* - J2?p« - i(2fr)-J 
= nf — nfz — n varp 
(where vaxp is written for the variance of p) 
= nfq — n var p ....... (5.26) 
THE POISSON DISTRIBUTION 123 
A comparison of these results with those for the binomial shows that the variance 
of the distribution (5.24) is less than that of the binomial with the same average p by an 
amount equal to n times the variance of p. 
Similarly we see that, for the Poisson distribution in such a case 
[ix = X, the mean of the A's .... (5.27) 
and ^a = \ var (np) 
= X to order ra-1. 
The Poisson form thus holds for (5.24) notwithstanding the inequality of the p'a, provided 
that the variance of A is small compared with n, which will be so if all the p'b are small. 
5.11. Consider now the case when successive sets of n are drawn from different 
populations characterised by pl3 p% . . . pk. In the previous case we supposed any set of n obtained 
by taking one from each of n populations. 
We now suppose that any set is drawn from one population only, but that different 
sets come from different populations. Our array of frequencies will now be 
and evidently the moments of this array are the sums of the moments of the (q + p)n, 
that is to say, from (5.4), 
fi\ = ^Snp 
Writing § for the mean of the p'b as before, we have 
i"i = nf 
' ju2 — nf + -rZn{n — l)pz — n2^8 
= nfq + T^>{n — 1)#2 — n(n — l)j>2 
= nfq + n{n - l)\^P2 — f*Y 
= nfq + n(n — 1) var p. (5.29) 
In this case the variance is greater than what it would be if the distribution were of the 
ordinary binomial type by an amount n(n — 1) v&rp. 
For the Poisson distribution we have, on taking limits, 
^B=A + var;i} (5'3°) 
and here also the variance of the distribution is affected. » 
5.12. The results of the two preceding sections enable us to discuss the occurrence 
of the binomial and the Poisson distributions in practice. An example has already been 
124 STANDARD DISTRIBUTIONS 
given in Table 5.1 of a distribution conforming to the simple binomial type. It is not 
easy to find material compiled outside the laboratory which does so. 
For example, suppose we regard the possession of blue eyes' as a success, and take 
a number of samples of to from the population of the United Kingdom in different localities. 
We should probably find that the proportions in these samples did not conform to the 
simple binomial form. The variance npq calculated from the known n and observed p would 
probably turn out to be too small. If so we should conclude from (5.26) that the proportion 
p varied from place to place in the population, the deficiency in the variance of the 
proportions observed being due to the variance of p itself in the sections of the population from 
which the samples were chosen. We are assuming for the time being that these differences 
are not explicable on the basis of sampling fluctuation alone; but a full discussion will 
have to wait until later chapters. 
5.13. The same effect is found in distributions which at first sight might be expected 
to be of the Poisson type. For example, suicide is a rare event and it might be supposed 
that if we took a series of large samples, say the population of the United Kingdom in 
successive years, the frequencies of suicides would follow the Poisson distribution. This, 
however, is not neeessarily so, for all members of the population are not equally exposed 
to risk and the temptation to suicide may vary from year to year, e.g. being greater in 
years of trade depression. This inequality of risk is typical of one field in which the Poisson 
distribution has been freely applied, namely, industrial accidents. Tatyle 5.3 shows, in 
the second column, the frequency of accidents occurring to women working on the 
manufacture of shells. The Poisson frequencies shown in the third column provide a very 
poor fit. The reason is that the liability of individuals to accident varies: 
TABLE 5.3 
Accidents to 647 women working on H.E. shells in 5 weeks. 
(Greenwood and Yule (1920), J. Roy. Statist. Soc., 83, 255.) 
Number of 
Accidents. 
0 
1 
2 
3 
4 
5 
Totals 
Observed 
Frequency. 
447 
132 
42 
21 
3 
2 
647 
Poisson Distribution D 
with same Mean. 
406 
189 
45 
7 
i—i 
{ 9-1 
648 
istribution j 
by (5.33) 
442 
140 
45 
14 
5 
{ 2 
048 
As a working hypothesis (cf. Greenwood and Yule, 1920) suppose that the population 
is composed of individuals with different degrees of accident proneness, represented by 
different values of X in a Poisson distribution; and suppose that in the population the 
distribution of X is given by 
dF = -^—JTa X*-1 dX 0 < X < co . f . .(5.31) 
There are theoretical reasons justifying this supposition. 
/ 
THE POISSON DISTRIBUTION , 125 
The frequenoy of j successes is then 
Jc 
or the coefficient of tf in 
=^-v e-^W-ie-WdX, 
which, on the substitution of (c + 1 — t)X = u, becomes 
l-t)» \c + l)\ c+l) 
(c + 
The frequency of 0, 1, 2, . . . successes is therefore 
tew 
P P(P + l) 
The mean is thus 
(5.32) 
(5.33) 
~\T+l) 'c + l\ c ) 
1 
c 
(5.34) 
Similarly //% =P(PJL—-—}, so that ^ 
_ p{c + 1) _ 2> , p rfi ofix 
^2 = 5 = 1 a • - • • • » • [0.65) 
C2 C Ca 
If we now put the observed mean and variance of Table 5.3 equal to the values of (5.34) 
and (5.35) we have two equations which can be solved for p and c. The distribution (5.33) 
can then be found. The frequencies are given in the fourth column of Table 5.3 and evidently 
give a much better agreement with the facts. 
5.14. The interesting feature of the distribution (5.32) is that it is a binomial with 
negative index. In the approach adopted in 5.2 the index is necessarily positive; but it 
is often found that observational materials are represented by negatively indexed binomials. 
Yule (1910) * has given an illustration of this effect which does not depend on any arbitrary 
assumption about distributions such as that embodied in (5.31). Suppose, in fact, that 
we have a population subjected to recurring attacks of a disease, that r attacks are fatal 
and that on the average one attack is fatal to a proportion p of individuals at risk, the 
actual numbers succumbing varying as if the population were chosen at random from a 
larger population in which the proportion of survivors is p. Consider the proportion of 
individuals surviving 0, 1, . . . attacks at the nth. exposure. Evidently this is the 
proportion of successes in samples of n when the chance of success is p, i.e. (q + p)n. The proportion 
of survivors at the end of n exposures will be the sum of the first r terms in this series. 
* J. Boy. Statist. Soo., 73, 26. 
126 STANDARD DISTRIBUTIONS 
The proportion of survivors at the end of (n — 1) exposures will be the sum of the first 
r terms in (g + 2>)n_1. Consequently the proportion dying during the rath exposure is 
the difference, ' 
-|{C;lK-C71>^w} 
Thus, since death does not commence till the rth exposure, for the values of n from 
r onwards we have the proportion of deaths 
f r(r + 1) _ . "1 
P{ ' Tq' 21 g' + • • •[ 
. (5.36) 
i.e. successive terms in £>r(l — <z)~r, a binomial with negative index. A law of this kind 
has been found to operate in experiments on the killing of bacteria by disinfectants. 
( 
The Hypergeometric Distribution 
5.15. Consider now the generalisation of the approach of 5.2 when samples of n are 
drawn from a poptdation of N individuals, where N is not necessarily large. If we take 
a sample which contains r P's and n — r Q'a, it can arise in 
/n\Np{Np - 1) . . . (Np - r + l)Nq{Nq - 1) . . . (Nq - n + r - 1) 
\r) N{N - 1) . . . {N - n + 1) 
~\r) NW ^ ' 
ways. For there are ( 1 ways of selecting the sample, and the r P's can be chosen in 
' [ J ways, and the n — r Q'a in I J ways, the expression given in (5.37) being 
■ Hence we are led to consider the discontinuous distribution 
f = S^ij{G)^B1^)1"-'1} • • • • (5-38) 
a form in which the analogy with the binomial (5.1) is evident. As N —> oo the form 
(5.38) approaches the binomial. 
THE HYPERGEOMETRIO DISTRIBUTION 127 
The series 
is equal to 
Xfln] (Nq-n + j)lfl j \ 
that, is to say, to the hypergeometrio function 
r)^ Pi*.?'.?,*) 
if a = _ n, ft = - Np, y = Nq — n + 1. . . . (5.40) 
The distribution (6.38) is therefore called hypergeometrio. We have 
>**> " > ^ y ii ^ y(y + 1) 21 ^ 
and it is well known that this function satisfies the differential equation 
x{1 ~ x)w*+ {v ~(a + ^+ l)x}% ~a/?i?T = °- • ■ (5>41) 
a fact which may be readily verified from the equation itself. 
If in (5.39) we put x = eB (0 = it) we evidently have the characteristic function of 
the distribution. On making this substitution in (5.41) we find, after some reduction and 
replacement of the values of a, /?, y by those of (5.40), 
(1 " e°]{w* ~{n + Np)-Je + nN^} ~ Nnp* + NU = ° " ' (5-42) 
$\ 
— Nnp + Nfii\ = 0 
Mi =np . . . „ . . (5.43) 
the same result as for the binomial. The mean of the hypergeometrio series is independent 
of N. 
Taking now the distribution about its mean, and hence substituting enp6<f> for <f> in 
(5.42), we find 
(1" e0)\w* + M{nip ""q) ~ Np] + {N ~ n^ni] +Nm'=0 • (5,44) 
whence, identifying coefficients in 0, 02, 03 we find 
_ npq{N - n) ' . ' 
f*2 tTt j ••...On... ,. ^0.40^ 
_ npqjq - p){N - n){N - 2n) 
^ (N - 1)(N - 2) <5-46> 
^ = (#-^^^ +Qn(N-n)}] (5.47) 
and generally, if E denotes the operation of raising the order of a.moment by unity, i.e. 
Eju,r = jUr+i, we have 
flX+i = {(1 + ®)r - Er}[H>*-{Np + n{q - p) }fi, + {npq(N - n)/i.}] . (5.48) 
As we expect, when N—► oo these values tend to those of'the binomial. 
Since cf> = E ~- we find, from the coefficient of 0° in this expression, 
128 
STANDARD DISTRIBUTIONS 
5.16. An example of the occurrence of the hypergeometric series in practice is given 
in Table 5.4, giving the frequency of occurrence of cards of a certain suit in hands of whist - 
Here N is the number of cards in the pack, 52, and n = 13, p = £. The appropriate 
series is thus 
_i—z/13>W3M39[n-fl 
52ri3:r\j J 
giving the frequencies shown in the third column. This agreement appears to be reasonably 
good. 
TABLE 5.4 
Distribution, of 3400 First Hands at Whist according to Number of Trumps in the Hand. 
(K. Pearson, 1924, Biometrika, 16, 172.) 
Number of 
Cards in 
the Hand. 
0 
1 
2 
3 
4 
Observed 
Frequency. 
35 
280 
696 
937 
851 
Frequency of 
Hypergeometric 
Distribution. 
43-5 
272-2 
700-0 
973-5 
811-3 
Numbor of 
Cards in 
the Hand. 
5 
6 
7 
8 
9 and ovor 
Totals 
Observod 
Froquoncy. 
444 
115 
21 
11 
0 
3400 
Froquoncy of 
Hy pi M'jjoon wtrio 
Distribution. 
424-0 
141-3 
300 
4-0 
0-2 
3400 
5.17. The calculation of frequencies and the summing of scries of lYoquonoio.s in not- 
so simple a matter as for the binomial, but the incomplete B-iunction may bo usod to 
give a fairly good approximation. The method consists of fitting a fl-curvo of typo 
dF = —-—- (1 — x)r>-! afl-1 dx 0 < x < 1 
to the distribution and obtaining areas of that curve from the i*-tal>k\s. Details of tin* 
method and an example are given in the preface to the Tables of tho Incomplete /Muuulion. 
The Normal Distribution 
5.18. We have already noted in Examples 4.6 and 4.8 that tho binomial distribution 
and the Poisson distribution both tend, when expressed in standard measure, to tho form 
1 -£l 
dF » —- e 2 dx 
V2n 
The slightly more general form 
dF 1 -efc-A*!)8 
V2na6 
— 00 <£E < 00 
dx — oo < x 
oo 
. (H.4U) 
. (r>.r>o) 
is known as the normal distribution. It is the most important theoretical distribution in 
statistics. The expression (5.49) is of course the normal distribution in standard measuro. 
THE NORMAL DISTRIBUTION 129 
We have for the characteristic function of (5,50) 
_£- 
<f>{t) = e 2a ea*\ (5.51) 
^ ^ " W ^ I (5.52) 
i"2r+l = ° J 
and y(t) = ~ g . (5.53) 
so that 
;':r*\>2} .... .(5.«, 
We also have /3a = 3, ya = 0, which accounts for the standard adopted for mesokurtosis 
in 3.32. 
5.19. The distribution function of the normal distribution expressed in standard 
measure is the integral 
1 ex x* 
F(x) = e~I dx (5.55) 
V\*n) J-oo 
The integrand' may be expanded and we have 
1 IF _£ 
=i+vt)Hi-Y+§!ffy-- • ■}** 
1,1/ as" , 1 a;6 . \ 
This converges too slowly to be of use for other than small values of a;. 
If a is large an asymptotic series may be employed. We have 
1 - F{x) = -7^— e~ T dx 
_ 1 r°e 2a; da; 
~ v(27t)L * 
r- a-1 —i oo rr* 
= 1 _ !H _ * fe'i , 
V(27t)L * J, V(2*0 J* *a * 
and on repeating the partial integration, 
_ e-7 f, _1 , 3 3.5 "I 
~ V(2ji)x \ xz x* «• * ' ' nJ * 
where iL is less in absolute value than the last term taken into account. 
(5.56) 
(5.57) 
The most useful formula, however, is a continued fraction due to Laplace. Put 
A.S. 
130 STANDARD DISTRIBUTIONS 
so that a(0) is the expression in curly brackets in (5,57). We then have 
1_ da _ 1 3 
a2 dt ~~ x\l - t)* a4(l - f)4 ' ' 
--{(l-l)a(O-l} 
Hence if <x(t) = Ey$ we have, identifying coefficients in f, 
Hence 
Thus 
.we have, from (5.57), 
Vr _ 1 
2/r-i 1 _|_ r + 1 «/r+1 
^2 Vr 
yx _ a; _ a; 
a: 2 3 
a; -J- a; + a; + 
" 2/r-l ^ 0. 
2 3 2/a 
x + xz yt 
n 
* • • • • 
a; + 
58 to y0, and we also see from (5.58) that as 
I FM - ^ { X 
I{X} V.(2*)* I* + 
1 _*!f 
_ a 'J! J 
1 2 
a; + a; + 
1 1 2 
. (5.58) 
°o. 2/i = 2/o- Hence 
+ z + 
. (5.59) 
The continued fraction thus gives the ratio of the frequency of the normal distribution 
to the right of the point x (the " tail area ") to the ordinate of the frequency-distribution 
at that point. 
This expression was in fact used by Sheppard (1939, posthumous) in calculating his 
superb tables of the normal function. These tables give, among other things :— 
(a) The ratio of the tail area to the bounding ordinate, to, 12 places of decimals, for 
intervals 0-01. 
(6) The same to 24 places for intervals 0-1. 
(c) The negative natural logarithm of the tail area, to sixteen places, by intervals 0-1. 
Tables which are sufficient for all ordinary purposes will be found in Tables for Statisticians 
and Biometricians, Parts I and II. At the end of this volume we give some tables which 
will suffice to illustrate the theory and examples given herein. 
5.20. The shape of the normal curve 
1 _?! 
is illustrated in Fig. 5.1. It is symmetrical and unlimited in range, falling off to zero 
very rapidly as the variate increases. There are points of inflection at unit distance on 
either side of the mean. 
THE NORMAL DISTRIBUTION 131 
For the mean deviation we have 
_L_r 
x e 
a dx 
00 _#• 
xe a dx 
o 
0-79788 
(5.60) 
The variance is of course unity, because the distribution is expressed in standard measure 
The quartiles are distant 0-674,489,75 from the mean, as may be found from the Tables 
04 
- 
u 
- 
3 
• 
, 
/ 
I 
j 0-3 
0-2 
Ol 
1 ( 
7 
f 1 
' 
K 
I ; 
■ 
J 4. 
Fig. 5.1.-—The Normal Curve y 
1 e-**. 
V(2tt) 
5.21. As an illustration of the occurrence in practice of a distribution which is very 
close to the normal, the height data of Table 1.7 may be taken. Table 5.6 shows the 
actual frequencies and those given by the normal curve with the same mean and standard 
deviation (67-46 and 2-56 inches respectively). 
The correspondence is evidently fairly good. It must, however, be noted that whereas 
the theoretical distribution has infinite range, the practical distribution has not, since it 
is impossible to have a negative height. In this particular case the relative frequency 
of the normal distribution outside the range 57-77 inches is so small that the point is 
unimportant; but when distributions of finite range are represented by those of infinite 
range it is as well to remember that the fit near the tails may not be very close. 
5.22. The normal distribution has had a curious history. It was first discovered 
by De Moivre in 1753 as the limiting form of the binomial, but was apparently forgotten 
and rediscovered later in the eighteenth century by workers engaged in investigating the 
theory of probability and the theory of errors. The discovery that errors of observation 
132 STANDARD DISTRIBUTIONS 
TABLE 5.5 
Frequency-Distribution of 8585 Men according to Height (Table 1.7) compared with Theoretical 
Frequencies of a Normal Distribution with the Same Mean and Variance. 
Height 
(inches). 
57- 
58- 
59- 
60- 
61- 
62- 
63- 
64- 
65- 
.66- 
07- 
Observed 
Frequency. 
2 
4 
14 
41 
83 
169 
394 
669 
990 
1223 
1329 
Theoretioal 
Frequency. 
1 
3 
11 
33 
88 
200 
395 
669 
976 
1227 
1326 
Height 
(inches). 
68- 
69- 
70- 
71- 
72- 
73- 
74- 
75- 
76- 
77- 
TOTALS 
Observed 
Frequency. 
1230 
1063 
646 
392 
202 
79 
32 
16 
5 
2 
8585 
1 
Theoretical 
Frequency. 
1234 
989 
682 
405 
207 
91 
34 
11 
3 
1 
8586 
ought, on certain plausible hypotheses, to be distributed normally led to a general belief 
that they were so distributed. The belief extended itself to distributions such as those 
of height, in which the variate-value of an individual may be regarded as the cumulation 
of a large number of small effects. Vestiges of this dogma are still found in textbooks. 
It was found in the latter half of the nineteenth century that the frequency-distributions 
occurring in practice are rarely of the normal type and it seemed that the normal 
distribution was due to be discarded as a representation of natural phenomena. But a,s tho 
importance of the distribution'declined in the observational sphere it grew in the theoretical, 
particularly in the theory of sampling. It is in fact found that many of the distributions 
arising in that theory are either normal or sufficiently close to normality to permit 
satisfactory approximations by the use of the normal distribution. Furthermore,- by a fortunate 
accident (if one may speak of accidents in mathematics) it happens that the analytic form 
of the normal distribution is particularly well adapted to the requirements of sampling 
theory. For these and other reasons which will be amply illustrated in the sequel, the 
normal distribution is pre-eminent among the distributions of statistical theory. 
5.23. Since the normal distribution may be considered as the limit of the binomial 
it is natural to inquire into the limiting forms, if any, of the hypergeometric distribution. 
From (5.38) we see that the difference between two successive terms in the distribution is 
#[»] r\[n - r - 1)! ^ P) K q) Vr + 1 n-r J 
NM r\{n-r)\ K F) K *' \ (r + l){Nq - n - r) J" 
The ratio of this difference to the (r -f l)th term is then 
Ayr _ A -f Br 
~y^ ~~ C + Dr + Er2' 
THE BIVARIATE BINOMIAL DISTRIBUTION 133 
where the quantities A . . . E are constants. In the limit when the distribution is 
expressed in standard measure, Ayr is the increment when r increases by a small quantity, 
and we are thus led t6 consider the differential equation defining a frequency function 
df A + Bx , 
J= O + Dec + E^ <6-61) 
This is the equation of a family of functions—the Pearson distributions—which will be 
considered from a slightly different standpoint in the next chapter. 
The Bivariate Binomial Distribution 
5.24. In generalisation of the results of 5.2, consider the drawing of samples of n 
from a population the individuals of which may or may not have two attributes, P and 
not-P (= Q) and R and not-R (= S). Suppose that the proportions of the individuals 
with attributes PR, QR, P$ and QS are a, b, c and d respectively, where a + 6 -\- c -\- d = 1. 
In exactly the same way as for the binomial case it is seen that the proportion of samples 
with * PR's j QR% k PS's and I QS's is . . .. tiltyftd1 and the distribution of samples is 
given by the multinomial form 
/=(a+6+c+ d)n. . (5.62) 
The distribution given by this form is bivariate, one variate being the number of P's and 
the other the number of R's. The characteristic function of the distribution is 
4> = (aeWl+tt» + be*' + ce^ + d)n. . . . . (5.63) 
We have then 
hog <j> = log/a + b + c + d + a»(*i + ta) + bit, + cit, - |(^ + (,)" ~ 3 -| l\ + • • ■} 
= logjl + i(a + b)U + i[a + c% - -+-1\ - a-±^ q-atj, + . . .1 
(5.64) 
From this it is seen that the mean of the variate corresponding to the occurrence of P's 
is n(a + c), and that of the variate corresponding to the occurrence of the R% n(a -f 6). 
From the terms in t\ and t\ in the expansion of (5.64) we find also that the variances are 
n(a + c)(l — a + c) and n(a + 6)(1 — a + b). If we now transfer the origin to the mean 
of the variates we have 
n 
log <f> =--{i\{a + e)(l - a + c) + t\(a + &)(1 - a + 6) + U^{a- a + ca + b)} + 0(n). 
Thus when the distribution is expressed in standard measure and n allowed to tend 
to infinity the characteristic function tends to the form 
log <f> = - \{t\ + t\ + 2Ptlti) .... (5.65 
a — (a 4- c)(a + 6) 
where p = =L=±— — ,—, 
{(a + e)(l - a + c)(a + 6)(1 - a + &)}* 
This, as was seen in Example 3.15, is the characteristic function of the bivariate form 
dF = z~r, iTi exP \ ~ H7i » (^ — Spa^a, + ^i) [■ daji *Ci — oo < &!, za < co (5.66) 
2tt(1 -p2)* [ 2(1 — p2) J 
Thus the multinomial form (5.62) tends to the form (5.66), which may be regarded as the 
bivariate analogue of the normal distribution. 
134 STANDARD DISTRIBUTIONS 
If the two attributes P and B are independent in the population, that is to say, the 
proportion of P's among It's is the same as among the not-J2's, we have 
a _ c 
a + b c + d 
and hence 
a + c _ a _a + c 
a -f- b + c + d- a + b 1 
so that a — (a + b)(a + c) = 0. Thus p in equation (5.65) vanishes. In this case and 
only in this case the distribution (5.66) becomes 
dF = -——r e-i'i dxx —— e-*xldx2, 
V(27r) v^Zx) 
i.e. Xi and x2 are independent variables. This is what we should expect and, indeed, is 
necessary if our use of the word " independent " in relation to attributes and frequency- 
distributions is to be consistent. 
NOTES AND REFERENCES 
For further formulae about the constants of the binomial distribution, including the 
incomplete moments \~( . ta71-^ see Frisch (1926) and Romanovsky (1925). Some of 
the results are given as exercises below. See also Haldane (19139). For the formulae of 
the hypergeometric distribution see K. Pearson (1895 and 1924). On the distribution 
functions of the binomial and the hypergeometric, see Camp (1924 and 1925). 
On Poisson's distribution reference may be made to Whitaker (1914), " Student " 
(1907 and 1919) and Morant (1921). 
Camp, B. H. (r924), " Probability integrals for the point binomial," Biometrika, 16, 163. 
(1925), " Probability integrals for the hypergeometrical series," Biometrika, 17, 61. 
Frisch, R. (1926), see refs. to Chapter 3. 
Haldane, J. B. S. (1939), " The oumulants and moments of the binomial," Biometrika, 
31, 392. 
Morant, G. (1921), " On random occurrences in space and time when followed by a closed 
interval," Biometrika, 13, 309,. 
Pearson, K. (1895), £< Skew variation in homogeneous material," Phil. Trans. A., 186, 343. 
(1924&), " On the moments of the hypergeometrical series," Biometrika, 16, 157. 
(19246), " On a certain double hypergeometrical series and its representation by 
continuous frequency surfaces," Biometrika, 16, 172. 
Romanovsky, V. (1925), " On the moments of the hypergeometrical series," Biometrika, 
17, 57. 
Sheppard, W. F. (1939), The Probability Integral, British Association Mathematical Tables, 
vol. 7, Cambridge University Press. 
Soper, H. E. (1922), Frequency Arrays, Cambridge University Press. 
" Student " (1907), " On the error of counting with a haemacytometer," Biometrika, 5, 351. 
(1919), " An explanation of deviations from Poisson's law in practice," Biometrika, 
12, 211. 
Whitaker, L. (1914), " On Poisson's law of small numbers," Biometrika, 10, 36. 
i 
EXERCISES 135 
' EXERCISES , 
5.1. Show that for the binomial distribution (q -f- p)n 
^=pqs^' r>1- 
Hence, writing c = npq, g = q — p, that— 
k% = c ; ks = eg ; /c4 = c — 6c2; kb = g (c — 12c2); *„ = c — 30ca + 120c3; 
K, = g (C - 60c2 + 360c3) ; Ka = c — 126c2 -f- 1680c3 — 5040c4. 
(Cf. Haldane (1939), who gives formulae up to ic18.) 
5.2. Show that for the incomplete moments about the mean of the binomial 
n 
equation (5.12) holds, i.e. 
(Romanovsky, 1925.) 
5.3. Writing Tt = ( . j p^q71^, show that the incomplete moments of the binomial 
are- given by 
n 
^ = pqTp 
P* = P<lTp{p ~{n + \)p] + npqp0 
fr = pqTP[{p ~{n + l)pY + pq (2n - 1) + npq (q -p) /*J, 
and generally 
r-2 , _.> r-2 , _ .> 
ih = pgTp(P - wpy-1 + npq JT r u, -p £ r w+1. 
(Frisch, 1926. This is the generalisation of equation (5.11) to incomplete moments.) 
5.4. Show that about the origin of the hypergeometric distribution 
nm(Np)M 
/"[r] = 
JSflr] 
5.5. Fromequation (5.48)derive the recurrence formula for the moments of the binomial 
{(1 + JZf)' - Er}(npq/,0 - pfxx) = t*r+1 
and that for the Poisson distribution 
{(1 + Ef - M*-}Xt*0 =jirJrl. 
(K. Pearson, 1924.) 
136 STANDARD DISTRIBUTIONS 
1 3^1 
5.6. Show that if y = —jz~ e~w 
oy2n 
i 
y* dec = 
Hence, if a normal distribution is grouped in intervals with total frequency Nu and Nt is 
the sum of the squares of frequencies, an estimate of a is 
Nz TV2 
_P— = 0-282.096& 
For the height data of Table 1.7 show that this gives an estimate of a equal to 2-553, an 
error of about 1 per cent. 
(Yule (1938), Biomeirika, 30, 1.) 
5.7. If a distribution of type (5.24) is represented approximately by a binomial 
(Q + PY, show that 
vP = np 
vPQ = npq — n- var p 
var p 
so that P = p -\ =-=- and hence is positive ; consequently that v is, positive. 
If, however, the distribution is of type (5.28), then 
P = v - (* ~ 1) var P 
§ 
so that P, and hence v, may be negative. 
("Student," 1919.T 
5.8. The bivariate Poisson series. Show that when a, b and c in equation (5.62) 
are small but na{= A3), nb(= Ax — k3) and nc{= Aa — A3) are finite, the distribution tends 
to the form whose general term is 
A3*(>li — Aa^/U — X3)k j, ;1+^ 
•i -i 7 i —— e » 
5.9. Show that if the frequencies of two symmetrical binomial forms of degree n 
are superposed so that the rth term of one is added to the (r + 1) term of the other, the 
resultant frequencies are those of a symmetrical binomial of degree (n + 1). Deduce that 
if two normal distributions with the same variance and means differing by a small part 
of the variance are added together, the resultant distribution is nearly normal. 
CHAPTER 6 
STANDARD DISTRIBUTIONS—(2) 
6.1. In this chapter we continue the aocount, begun in the last, of the standard 
distributions of statistical theory. From the variety of forms assumed by the frequency- 
flistributions of experience, as exemplified in Chapter 1, it is evident that an elastic system 
would be required to describe them all in mathematical terms. Three approaches will 
be considered herein : the first, due to Karl Pearson, seeks to ascertain a family of curves 
which will satisfactorily represent practical distributions ; the second, due to Bnins, Gram 
and Charlier, seeks to represent a given frequenoy function as a series of derivatives1 of 
the normal frequency function ; the third, due to Edgeworth, seeks1 for a transformation 
of the variate which will throw the distribution at least approximately into the normal form. 
Pearson Distributions 
6.2. It was noted in 5.23 that in the limiting case the hypergeometric series can 
be expressed in the form 
df _ {x- a)f 
dx &0 + btx + btxz 
This equation may be considered from a slightly different standpoint. The unimodal 
distributions of Chapter 1 suggest that it might be worth while examining the class of 
frequency functions which (a) have a single mode, so that ~- vanishes at some point x = a ; 
(b) have smooth contact with the #-axis at the extremities, so that —■ vanishes when / = 0. 
Evidently these conditions are in general obeyed by any distribution of the family (6.1). 
In actual fact, as will be seen below, there are also solutions of (6.1) in particular cases 
which are J- or U-shaped. 
The family of frequency functions defined by (6.1) are known as Pearson distributions. 
Before obtaining explicit solutions of the equation, we consider certain general results 
which are true of all members of Hhe system. We have immediately 
(b0 + b^x + b2x2) df = (x — a)fdx 
or #"(&<> ■+■ bxx + b&*) ~fdx = x^(x — a)fdx. 
Integrating the left-hand side by parts over the range of the distribution, we find, assuming 
that the integrals exist, 
["a!»(&o + b& + btx*)/] - ( {nb^-1 + (n + l)bxxn + (n + 2)b,xn+1}fdx 
1*00 y.00 
= xn+1fdx-a\ xnfdx. . (6.2) 
J —00 J—00 
Let us assume that the expression in square brackets vanishes at the extremities of the 
137 
138 STANDARD DISTRIBUTIONS 
distribution, i.e. that lim xn+2f—> 0 if the range is infinite. We then have, sub- 
x—>-±co 
stituting moments for integrals in (6.2) :— 
- ^o^-i - (n + 1)6^ - (» + 2)6B/a;+1 = fin+1 - a^ 
or »6a/S_i + {(n + l)6i - «)K + {(n + 2)62 + lfci = 0 . . (6.3) 
This equation permits of the determination of any moment from those of lower orders. 
In fact, all moments can be expressed in terms of a, b0, bt and 6a and the moments fi0(= 1) 
and^j. Conversely we can express these four constants in terms of the moments ^ to fa, 
or the three moments about the mean fi2 to fit. Putting n = 0, 1, 2, 3, successively in 
(6.3), we find equations for a, b0, b1} ba which result in 
„ _ Mp* + fyi) _ Vt*tVPi(P% + 3) 
a~ I _ I7 
, P*(4p*P 4 — 3^1) _ . ,aa(4/?a — 3ft) 
bo ~ I " " A' 
. _ n,{fi, + 3^1) _ v7Wft(ft + 3) 
ftt - -j - - T 
. _ {2fi.fi, - Zfil - 6jul) _ (2/9. - 3ft - 6) 
■ ' _ A A' 
where A = lO^^a — 18/*| — 12^1 
A' = 10/?2 - 18 - 12ft J 
It follows that a curve of the family (6.1) is completely determined by its first four 
moments, fi\ to ji^. 
(6.4) 
(6.5) 
6.3. In equation (6.1) the mode is evidently at the point x = a. We have then 
for the Pearson measure of skewness (3.31) 
ov _ — a _ Vfti(fta + 3) 
VS ~ 10/5, - 12/9, - 18 " " .' " {b'b) 
the form given in 3.31. 
Further, if we take an origin at the mode so that a = 0 we find 
dzf d xf f 
dx~* = dxb0 + blx + b2x* = (bo + b^ + btx*)^1 ~ b%)X% + M " (6"7) 
Thus any points of inflection in the frequency curve are given by 
xz = r-^j- (6.8) 
Hence there cannot be more than two of them, and if they exist, they are equidistant from 
the mode. It is not to be inferred that a ctfrve of the family cannot have a single point 
of inflection, for one point corresponding to the solution of (6.8) may be outside the 
permissible range of x. 
6.4. By a simple transformation of the origin to the mode, (6.1) may be written 
d (log/) = X~a 
dxx OJ/ BQ + Bx(x - a) -f- Bz(x — a)ai 
dX (1°g/) = BT+BX+BJ* 
THE PBAKSONIAN SYSTEM 139 
* 
The explicit expression of the frequency function /is thus a matter of integrating the right- 
hand side of (6.9). 
Following Pearson, we may distinguish three main types according as the denominator 
on the right in (6.9) has real roots of opposite sign, real roots of the same sign, or imaginary 
roots. Pearson also distinguished ten other types, some entirely trivial, when the B'a 
take particular values.* 
Type I 
6.5. Let 
Bo 4- B1X + BtX* *= Bt(X + ai)(X - a8), a1( aa > 0 
Then ± (log/) = - X 
dXx °" B*{X + olx){X - aa) 
«i A ■ aa 
J?i(«i + «i)' {X + ax) ^ ^.(a! + a„)' (X - a,). ' 
giving / = fc(X + ai)B»(«i+«»)(X - a1)*(«i+»J . ♦ . .(6.10) 
This is generally written in the form 
where , — = —. 
ax aa' 
' The range of the curve is from — at to a% and by integrating between these values we find 
J-aA 01/ \ «h/ 
which, on putting x = (at 4- #»)2/ — «i reduces to 
■ l=k\ 3T>(1 -y)«0?1+a,J <fr 
Jo ■ a1™'a,'* ^ 
h(ax +al)™i+m-+1 n, . . , ,, 
= aW» 5(Wl + l> m* + X)- 
This determines h and we have 
f a^a^ / * Wi _ ^\m" /« i9\ 
7 " (<h + a,)^+-+1 B{mx + 1, ma + 1)1/ + a J ^ «J * " ^j 
The origin here is the mode. Taking an origin at the start of the curve we have 
(ax 4. o,)«,i+m«+1 £(mx + 1, »», + 1) V aa / 
or again, measuring in units {ax 4- az) times the original, 
/=r7 TT —r-.a^l - *)"■ ,' . . .(6.13) 
jB(mi 4 1, w, 4 1) 
6.6. In these expressions the a's are necessarily positive, but the m's may have any 
value greater than — 1. They cannot be less because the distribution function of (6.12) 
or (6.13) would not then oonverge. 
* The numbering of the types followed herein is that of Elderton (1938). Some variations occur in 
earlier literature and the reader must not be surprised to find the normal curve referred to occasionally 
as Type VII. 
140 STANDARD DISTRIBUTIONS 
If ml3 ma > 0 the 'distribution is evidently unimodal and zero at its extremities. If 
one of the m's is between 0 and 1 the corresponding terminal frequency is atill zero, but 
the frequency curve makes a sharp angle with the a>axis, for ^ is not zero at the terminal. 
If one and only one m is less than zero the curve has an infinite ordinate and is thus 
J-shaped. If both m's are less than zero the curve is U.-shaped. 
The condition that B0 + BXX + BZXZ shall have real roots of opposite sign is that 
B0 and B2 are of opposite sign, which is equivalent to 
R2 
■°l <0 
4JB0B2 
or, in terms of /?x and /?2, from (6.4), 
4(20, - 3& - 6)(4& - m < (6'14) 
The quantity on the left was denoted by Pearson by the letter k and provides a criterion 
which will occur again below. 
The frequency function of the Type I curve is calculable directly from its equation. 
The distribution function, as may be seen from (6.13), is expressible in terms of incompleto 
5-functions. 
Type VI 
6.7. If the roots of jB0 + BtX + B2X2 are real and of the same sign it is easy to see, 
in the manner of the preceding sections, that the frequency functions may bo written in 
the form 
0<7i-?«-l 
f = W7, a 1 „ >ux~qi(x ~ a)Qx where qx ></a - 1 . ((Uf>) 
where the range lies from a to ao if a is positive and from — oo to a if a is negative. By 
the simple transformation y = - tins reduces to the Type I form (6.13). 
It will readily be verified that if q2 > 0 the curves are unimodal with zero frequencies 
at the terminals. If q2 < 0 the start is J-shaped and the distribution falls away to zero 
at infinity. The distribution function may be expressed in terms of incompleto ^-functions, 
and in this case the quantity k of (6.14) is greater than unity. 
Type IV 
6.8. If the roots of B0 + BtX + B2XZ are imaginary wo have 
_ x 
*.{(X+y)«+«»}» "^ 
giving log/ - log h + -L log {(X + yf + J»} - y tan-i ^L+V 
%*** J Btd S 
f = HX + y)« + 6*}k exp {.- ^ tan- *+rj . . . (6>16) 
SPECIAL TYPES OF THE PEARSON DISTRIBUTION 141 
This js Pearson's Type IV and is usually written in the form 
f = lc(\ +^rWV*tan"1£ (6.17) 
The distribution has unlimited range in both directions, tends to zero at infinity and is 
unimodal. The calculation of its ordinates may be assisted by some tables by Comrie 
(1939). The distribution funotion has to be found either by quadratures from the frequency 
function or by the use of some tabulated integrals .given in Tables for Statisticians and 
Biometricians, Part I. For instance, for the constant k in (6.17) we have 
K.('+5) 
—m — 1 x j 
a—v tan — CLX 
= af8 cos2"1"2 0 e~* dd 
1 
= aF(2m — 2, v) in Pearson's notation. 
In this case the quantity k of (6.14) lies between 0 and 1. 
The above are the three main types in the Pearsonian system. The remaining types 
are described briefly below. A number of results which the reader can easily verify for 
himself are given without proof. 
The Normal Distribution 
6.9. If, in equation (6.9), a = Bt = Bt = 0, we have 
log/-J£fa-|£.+log* 
/ = lce*B,. 
If this frequency function is to have a convergent distribution function, jB„ must be negative, 
= — a2 say, and we get the familiar form 
Thus the normal distribution itself is one of Pearson's types. 
Type II 
6.10. If in equation (6-9) Bt = 0 and B&, B0 are of opposite signs, the distribution, 
a particular case of Type I, becomes of the character 
f= ~_ fi_ ) -a<x<a . . .(6.18) 
J aJ3(£,m + l)\ a*) v } 
(a here being different from the a of (6.9) ). 
In this case the criterion k of equation (6.14) is zero. The distribution is symmetrical 
about the origin and ranges from — a to -J- a. If m > 0 it is unimodal with contact at 
142 STANDARD DISTRIBUTIONS 
the terminals of the range ; if m < 0 it is U-shaped. If m = 0 the distribution beoomes 
/ = —, — a <cc <a . . . . . (6.19) 
the so-called " reotangular " distribution. 
Type VII 
6.11. If in (6.9) B1 — 0 and Ba, B0 are of the same sign, we find 
1 / -*.2\-m 
f=^7i-Z, TU1+-2) -oo<z<oo . ■. (6.20) 
. The range is now unlimited in both directions. Here also the criterion #c of (6.14) vanishes, 
but the difference between this case and that of Type II lies in the fact that here ^B > 3, 
whereas in the Type II case £2 < 3. 
Type, III • 
6.12. If in (6.9) B2 = 0 we obtain the distribution 
f = ^WTr{l + t)^ • • • -<6-21 
this being the form with the origin at the mode. The ourve is unlimited in one direotion 
(positive or negative as - is positive or negative). It is unimodal if p > 0, J-shaped if 
p < 0. The oondition Bz = 0, from (6.4), is equivalent to 2£a — 3& — 6 = 0, i.e. #c of 
(6.14) is infinite. 
Type V ' 
6.13. If the roots of B0 -f- BiX -f- B^X2 are equal, i.e. #c = 1, we arrive at the 
distribution 
f = rC- lfVe~' 0<x<co . . . (6.22) 
which ranges from 0 to oo and is unimodal. 
Types VIII, IX, X, XI and, XII 
6.14. The remaining types are of a more special character still. 
If in (6.9) B0 = 0, Bi> 0 we have 
Type VIII :/ = ——(1+a) ' 0<m<l, -a<x<0 .(6.23) 
If B0 = 0, Bt < 0 we have 
Type IX: / =—^-(l +-) » -a <x <0 . . . . (6.24) 
If B0 = B2 = 0 we have 
Type X: f = -e ^ 0 < a; < oo ' (6,25) 
If B0 = jBx = 0 we have 
Type XI: f = bm-\xn> - l)x~m b < x < oo . . . . (6.26) 
■SPECIAL TYPES OF THE PEARSON DISTRIBUTION 143 
Finally, as a particular case of Type I when 5/3a *— 6ft — 9 = 0, equations (6.4) become 
indeterminate. In this case we have 
Type XII: f = (g) 
m 
(ax 4- a2)jB(l + m, 1 — m) 
<1, 
. (6.27) 
6.15. Pearson ourves of Types I and III, and to a somewhat smaller extent, those 
of Types V and VII, arise in the theory of sampling and would in any oase have to be 
studied in that theory. Apart from this, the principal use that has been made of the 
distributions in the theory of statistics is in fitting them to observed distributions such as 
those of Chapter 1. It has been found that in many cases the Pearson distributions provide 
a remarkably good fit to observation. 
A systematic account of the technique of fitting will be found in Elderton's Frequency 
Curves and Correlation (1938). We will here merely indicate the general principles and 
give one example of fitting in what is, perhaps, the most difficult case. 
6.16. All the Pearson distributions are determined by the first four moments, [i[ 
to fa inclusive, except some of the degenerate types which are determined by fewer than 
four moments. Pearson's method of fitting consists of 
(1) determining the numerical values of the first* four moments of the observed 
distribution ; 
(2) calculating the numerical values of ft, ft, k (equation 6.14) and hence determining 
the type to which the distribution belongs ; 
(3) equating the observed moments to the moments of the appropriate distribution 
expressed in terms of its parameters ; and 
(4) solving the resulting equations for those parameters, whereupon the distribution 
is determined. 
The following example will illustrate the process :— 
Example 6.1 
In Table 1.15 there are shown, in the column totals, a distribution of 9440 beans 
according to length. The figures are repeated in Table 6.1 on page 150. Required to fit a 
Pearson distribution to these data. 
For the moments it is found that, with Sheppard's corrections, 
ju,! (centre at 14-5) = — 0-190,783,898 
^B ' = 3-238,424,951 
^3 = — 5-306,566,352 
fiA = 50-999,624,044 
ft = 0-829,135,838, - Vft = — 0-910,569 
ft = 4-862,944,362 
First of all, as to type. For the oriterion #c (6,14) iwe have 
ft(ft + 3)2 
K = 
4(4ft - 3ft)(2ft - 3ft - 6) 
51-262 
84-040' 
144 STANDARD DISTRIBUTIONS 
This lies between 0 and 1 and hence the appropriate ourve is Type IV. We have to 
determine a, m, v in 
J aF(2m-2,v)\ a'J 
Writing tan <f> — - and 2m - 2 = r we find 
//n = Tc \ an+1 cosr-" 6 sin" 6 e—° dd, 
J — 00 
whence, integrating by parts with cosr-n 6 sin 6 as one part, 
r — n -\- l 
* 
a particular oase of (6.3). Henoe, in terms of moments about the mean, 
av 
A*i — 
» - pj^tF + ^ 
_ _ 4ah>(r2 + v*) 
P* ~ r8(r-l)(r—2) 
_ 3aA(r2 + v2){r + 6(r2 + f2) - 8v2} 
i"4 r4(r - l)(r - 2J(r^"3) '' 
whence it is found that 
6Qg. -/?!-!) 
202 - 3ft - 6 
r 
r(r _ 2)V/?t 
V{16(r-l)-/?1(r-2)2} 
a = N/[^{16(r~1)'"/3l(r~2)2}]- 
Substituting for fiit 0X and /*a we find 
r = 14-697,72, m = 8-348,86 
v = 18-380,43 a = 4-159,49 
The signs, here want a little watching, r atid m present no difficulty ; but a is to be taken 
positive and v positive since \Z/9X is to be considered negative. 
From the tables of F(r, v) we evaluate the constant term k and finally arrive at 
(r2 \-8-348,86 ,«,.»on\io* -1 x 
1 + 17.3*01 34j e-^80,43 tan j—.^ 
The frequencies given by this curve are shown in Table 6.1 on page 150. 
6.17. The following points are worth noting in connection with the fitting of Pearson 
ourves to observational data:— 
(1) Although the various types have dissimilar analytical equations they merge into 
one another in geometrical shape. For instance, Type V may be regarded as transitional 
TCHEBYCHEFF-HERMITE POLYNOMIALS 145 
between Types IV and VI and is very gimilar to the shape assumed by those curves 
near #c = 1. 
(2) It is tacitly assumed that the data can be represented by a curve with finite moments 
up to the fourth order at least. Curves for which higher moments do not exist were 
called by Pearson heterotypic ; but there is nothing sinister about them except that they 
do not fall within the Pearsonian - system. 
(3) In calculating moments, Sheppard's corrections are usually to be employed when 
there are contacts of sufficiently high order at the terminals. In the case of J- or U-distri- 
butions the other corrections mentioned in 3.27 may be employed. This case sometimes 
raises difficulties in that the resultant curve does not start in the right place. In such 
circumstances there is no golden rule. The most satisfactory course is to try several curves 
(or the same curve translated to several points) and to judge by the results whioh of 
them gives the best fit. 
(4) The quadrature of Pearson curves, as indicated in the foregoing, may in some 
cases be effected by tabulated integrals ; -but the more generally applicable procedure 
appears to be to calculate ordinates direct from the equation of the ourve and then to 
find areas in ranges by Simpson's rule, Weddle's rule, or some similar process of quadrature. 
6.18. The mathematical description of an observed distribution by a Pearson curve 
may be regarded from two rather different standpoints. If our object (for instance in 
actuarial work) is to obtain a mathematical expression which will satisfactorily represent 
observation and allow of accurate graduation and interpolation, fitting by moments is 
generally satisfactory. The method has, however, been critioised when the observed data 
are regarded as samples from a population, and it is desired to find a mathematical 
representation of that 'population. In such cases the'moments oalculated from observation are 
only estimates of population-moments. It has been objected that they may be inefficient 
estimates, and alternative methods have been proposed. ' We shall have to defer a full 
discussion of this point until the seoond volume. 
6.19. Other systems of ourves have been studied, mainly by Scandinavian writers, 
with a view to representing frequency functions by expansions in series. It is well known 
in mathematical and physical work that functions oan often be usefully expressed as a 
series of terms such as powers of the variable (Taylor's series) or trigonometrical funotions 
(Fourier's series). Neither of these forms is very suitable for frequency funotions, but 
we proceed to consider another set of functions with more promising possibilities. 
Tchebycheff-Hermite Polynomials 
6.20. Writing 
1 _-L' 
and D = -r- 
dx 
consider successive derivatives of <x.(x) with respect to x. We have 
Da.(x) = — xx(x) 
D2x(x) = (xz - 1)ol(x) 
D*k(x) = (3x - a3)a(aO, 
A.S. 
i i 
\, 
L 
146 
STANDARD DISTRIBUTIONS 
/ 
and so on. The result will obviously be, in general, a polynomial in x multiplied by cc(x). 
We then define the Tchebycheff-Hermite polynomial Hr(x) by the equation 
(— Dy<x(x) = Hr(x)cx.(x) . 
Evidently Hr(x) is of degree r in x and the coefficient of af is unity. By convention H 
We have 
1 / xz tz\ ( t2\ 
<x~l) = vpS)exp v"»+ to ~ *) ~*{x) exp r ~ v 
and also, by Taylor's theorem 
(6.28) 
B = l. 
a(* - *) = JT<i>Wa(a;) - j^Z7,(a;)a(a;). 
Consequently Hr(x) is the coefficient of -r in exp 
(<•-9 
— ). It follows that 
Hr(x) = 
The first ten polynomials are 
#o = 1 
Ht =x 
x* 
x7 
2~1! 
-.x 
,r-2 
+ 
r[4] 
222! 
x 
•r-4 _ 
2131 
af"6 
a;a 
a;< 
a;6 
1- 
3a: 
6x2 + 3 
10a:3 + 15a; 
#B 
#3 
#4 - 
■IT. = 
jya. = a;0 — 15a;4 4- 45a;2 - 15 
H7 = x7 - 21a;5 4- 105a;3 - 105a; 
#8 = a* - 28a;8 4- 210a;4 - 420a;2 + 105 
Ho 
■#io 
a;9 - 36a;7 4- 378a;5 - 1260a;3 4- 945a; 
x 
.10 
45a* + 630a;0 - 3150a;4 4- 4725a;2 — 945 
(6.29) 
(6.30) 
6.21. The polynomials have a number of interesting properties. Differentiating the 
identity 
exp (fa; - - 
(x) 
with respect to x and identifying coefficients in tr we have 
d 
. (6.31) 
faEr(x) = rH^x) .... 
and generally 2Xffr(a;) = r^H^x) (6.32) 
Differentiating the identity with respect to t and identifying coefficients in f-1 we have 
Hr(x) - xH^x) 4- (r - l)#r_2(a;) = 0. . . . (6,33) 
Jfaom (6.31) and (6.33) together we find 
*3P-J%*+*w-o. 
. (6.34) 
THE GRAM-CHARLIER SERIES OF TYPE A 
147 
It is also known that the equation in x, Hr(x) = 0, has r real roots, each not greater 
fftf _ i) 
in absolute value than /—-—-. (Cf. Charlier, 1931.) 
, Tables of the values of the first six polynomials to 10 decimal places proceeding by 
. x — 0 (0-01) 4 have been given by J0rgensen (1916). 
6.22. The polynomials have an important orthogonal property, namely, that 
Hm(x)Hn(x)tx.(x) dx = 0 m =^n 
I 
n\ 
m = n 
. (6.35) 
In fact, integrating by parts, we have, if m < n, 
/•OO pOO 
HmHnoL dx = (- 1H HmDnv. dx . 
J —00 J —00 
=(- l^pff^-^T + (-1)"-1 r ^n*'1*dx- 
The term in square brackets vanishes and, in virtue of (6.31), the integral becomes 
m( _ _i)«-1 f Hm_ 1jp-ia dx. 
J —00 
Continuing the process, we find either zero, if m is not equal to n, or ml if m = n. 
The Oram-Charlier Series of Type A 
6.23. Suppose now that a frequency function oan be expanded formally in series 
of derivatives of x(x). (We shall discuss the conditions under which such an expansion 
is valid below.) We have then 
/(«) = ^/i#/(z)a(a). 
;=o 
Multiplying by Hr(x) and integrating from — oo to oo we have, in virtue of the orthogonal 
relationship (6.35), 
If00 
cr = -A f(x)Hr(x) dx . • . i . . (6.36) 
The reader familiar with harmonic analysis will recognise the resemblance between this 
procedure and the evaluation of constants in a Fourier series. * • 
Substituting in (6.36) the explicit value of Hr{x) given in (6.29) we find, 
<v = r1^ 
T/"r-2 + rfr-i 
2 U"-" ' 222! 
In particular, for moments about the mean, 
Co = 1 
cx =* 0 
ca = i(A*2 — 1) 
Cs = nf^s 
c4 = -fabti - 6^2 + 3) 
c6~= ^hifis - 10^3) 
Co = 7io(/"o — IS/** + 45^ — 15) 
C7 = TnrVffC"* - 21Mb + lOS^s) 
cB = -roiao^ - 28/*<> + 210^4 - 420^B + 105) 
(6.37) 
(6.38) 
148 STANDARD DISTRIBUTIONS 
Thus we find the formal expansion 
f(x) = a(*){l + Ufh - 1)#* + iV*H3 + MP* - 6<"2 + 3)^4 + . . .} . (6.39) 
If f(x) is in standard measure the series becomes 
f(x) = <z(x)Jl + if*JS» + A(A«t ~ 3)#4 +...}. . - (6.40) 
This is the so-called Gram-Charlier series of Type A. 
Edgeworth's Form of the Type A Series 
6.24. Consider the characteristic function of a term Hr(x)tx.(x). 
Sinoe V{2n) a(^) = e~^ — eitx /,n \e~* ^x 
we have ' \/(2n)~a(t) = (- l)W(^)Sr(t)a.(t) = far-^—- e~2dx 
dP J _«, v (2tt) 
and thus the characteristic function of xrcc(x) is ir\/(27i;)Hr(t)<x(t). Conversely, by the 
Inversion Theorem of 4.3, we have 
afa(») = ~ f° e~itx ir^/{2n)Hr{t)a.{t) dt. 
2ttJ _m 
Interchanging a; and t, we find 
V(2w)(-»')rfa(0 = | e~to'^r(a;)a(a;) dx 
J — 00 
and hence, changing the sign of t, that the characteristic function of Hr(x)tt.(x)is -\/(27i)irlra.(t). 
Consider now the expression 
exp (jCjJFJafa:) (0.41) 
Its characteristic function is "" 
f°° e*te exp (K^a^)^ = f ^s(p^!\%(x) dx 
= l£z f e*to( _ i)tf#d(a;)a(a:) ^ 
J' J —00 
= V(2^) «(0 exp - (jcrtt)r . . . (6.42) 
In a similar way it will be seen that the characteristic function of 
exp {- KJ^D + K-±^D* - £p + ^D* . . .}«(*) . . (6.43) 
is equal to 
Vfrtofi) exp {^4p* + ^-W + g^tt)8 + g(ft)4 + • • •} • (6-44) 
More generally, if 
_, (TV(27r) 
EDGEWORTH'S EORM OF THE TYPE A SERIES 
149 
the characteristic function of 
Kt — a 
exp 
11 
B + *'„ bD* 
2! 
3! 
K, 
D3 4. Up* 
4! 
►fl«) 
is equal to 
*/{2n)a.{to)#mi exp -fe 
-** H—hn—(*0 
IC 
/c* 
+ FW + jj(«)* 
(6.45) 
(6.46) 
1! "' ' 2! 
as may be seen by the same line of argument. 
Now suppose that (6.45) represents a frequency function. Its cumulative function 
is then the logarithm of (6.46),"i.e. is equal to 
(/cx — a + m)it k2 — b + <** 
n 1 
*3 
K, 
(«)«.+ 2-(ft)» + lf(»0* + 
2! 
and hence its cumulants are kx — a + m, «r2 — b + c3, «r3, #c4 . . ., «rr . . ., etc. We may 
take a = m and 6 = c2 and thus we obtain a distribution whose cumulants are ku #ca, . 
etc. Now if these are in fact the cumulants of a distribution the series (6.45) must be 
equal to that distribution, provided that (1) the series converges to a frequency function, 
and (2) it is uniquely determined by its moments. 
If we take the frequency function to be expressed in standard measure, then kx= 0, /ca= 1 
and (6.45) becomes 
CD3 D4 1 
exp j - k3— + /c4— . . .U(tf) =f(x) . . . . (6.47) 
where we have written ct(x) for fi(x) because now m vanishes and c2 = 1. 
A series of this kind was derived by Edgeworth (1904), though from an entirely different 
approach through the theory of elementary errors. Equation (6.47) is formally identical 
.with (6.40), and the reader who consults the original memoirs on this subject may be 
puzzled by the fact that Edgeworth claimed his series to be different from the Type A 
series and better as a representation of frequency functions. The explanation is that 
for practical purposes it is necessary to take only a finite number of terms in the series and 
to neglect the remainder. If we take the first k terms in (6.40) the result is in general 
different from that obtained by taking the first (k — 1) terms of the operator in the 
exponential of (6.47). The argument centred on the fact (cf. Example 6.3 below) that the 
terms in (6.40) do not tend regularly to zero from the point of view of elementary errors, so 
that in general no term is negligible compared with ■ a preceding term. 
6.25. In standard measure the relations.(6.38) become, in terms of cumulants, 
Cq = 1, Ci ^= C% ^ U 
c -*' 
24 
120 
1 
720 
1 
c, = 
cK. = 
Ce = 
C = 
5040 
(«• + IOjcJ) 
(ic7 + 35#c4#c3) 
Co — 
1 
40320 
(jc8 + 56«6ic3 + 35*2) 
(6.48)' 
150 
STANDARD DISTRIBUTIONS 
6.26. In the practical representation of frequency .functions by the Type A series 
only the first few terms can be taken into account. The term in Hr(x) has a coefficient 
dependent on ju,r and for r >, 4 this is unreliable owing to sampling fluctuations. When 
sampling effects are not in question the series may be taken: to more terms, usually not 
higher than the term in H0. We should then have to investigate how far the observed 
distribution can be represented by the series 
a(*)(l + S». + £ff. + iff. + *±£*B.) . . . (6.49) 
in the hope that the remainder after these terms could be neglected in comparison. 
It may be noted in passing that the distribution function of such a series is easy to 
obtain. If 
f(x) = £arHr(x)a.{x), 
rx rx 
then f(x) = Ear Hr(x)x(x) dx 
J — 00 J —00 
= -Zar#P_1(j;ja(a:) (6.50) 
TABLE 6.1 
Fitting of Pearson Type IV Distribution and Qram-Cha/rlier Type A Series to. the Data of 
Length of Beans (Table 1.15). 
(Prom Pretorius, 1930.) 
Length of 
Beans 
(nun.) 
— 
17-0 
16-5 
16-0 
15-5 
150 
14-5 
14-0 
13-5 
13-0 
12-5 
12 0 
11-5 
11-0 
10-5 
10-0 
9-5 
'■ — 
Totals 
Observed 
Frequency. 
— 
t; 
55 
275 
1129 
2082 
2294 
1787 
929 
437 
199 
115 
70 
36 
18 
7 
1 
— 
9440 
Type TV. 
;{w 
1 28-5 
I 299-3 
1181-6 
2132-6 
2229-8 
1638-9 
968-9 
503-6 
243-7 
113-8 
52-5 
24-2 
rll-3 
I 5-4 
] 2-6 
Wl-9 
9440 
Typo A. 
(1) 
r{10-3 
I 12-8 
} 25-6 
L 241-7 
1012-7 
2155-4 
2593-0 
1788-4 
713-4 
280-7 
258-7 
206-2 
98-7 
29-6 
r 5-9 
' {{- 
I 
9440 
Typo A. 
(2) 
,-..., 
J 13-7 
) 116-6 
^ 370-4 ■ 
926-2 
1833-0 
2506-4 
2082-6 
921-3 
199-0 
132-1 
178-1 
117-0 
43-5 
t 10-0 
{it 
I 
" 
9440 
Typo A. 
(3) 
A '2'° 
J - 35-3 
) 22-3 
l> 438-1 
12L4-0 
1866-9 
2112-8 
1916-7 
1L83-4 
371-2 
66-9 
101-2 
107-1 
54-0 
r 15-4 
{3, 
I 
~ 
9440 
(The brackets* mean that the frequencies shown are rounded up and include some small frequency 
in blank rows covered by the brackets.) 
I 
TETRACHORIC FUNCTIONS 151 
Example, 6.2 
Consider the fitting of a Type A series to the.bean data of Example 6.1. 
We have already found the first four moments. In standard measure we have 
fz3 = — 0-910,569 
jit = 4-862,944 
and we also find ju& = — 12-574,125 
ju0 = 53-221,083. 
Hence the series is 
9440a(a){l — 0-151,762 Ez + 0-077,622,7 Hi — 0-028,903,6 Hs +0-014,273,5 #6}. 
Table 6.1, on page 150, due to Pretorius (1930), shows the frequencies given by taking 
the first three, the first four and the first five terms of this series (columns headed Type A(l), 
Type A(2) and Type A(3) respectively). A glance at the figures will show that the four- 
and five-term series is no better than the three-term and, if anything, rather worse. 
Furthermore, the five-term series gives negative frequencies at one terminal and a mode at 12 mm., 
which is contrary to the data. The representation is clearly not very satisfactory and no 
better than that given by the Pearson Type IV curve. 
Tetrachoric Functions 
6.27. The terms Hr(x)cc(x) may be obtained from J0rgensen's tables combined with 
those of the exponential e 2. Some related functions have also been tabulated in Tables 
for Statisticians and Biometricians, Parts I and II. The funotion 
_ (- ir ijfr-Ms) _ H^ixHx) 
is known as the Tetrachoric Function of order r, and tables are available to seven places 
of decimals for r = 0 (1) 30 and x = 0 (0-1) 4. In the notation of these functions, series 
(6.49) would become 
f{x) = xx{x) + ^-/c3T4(tf) + ^24~K *rs(x) + • • • ' 
and the particular series of Example 6.2 would be 
f(x) = 9440{r1(a;) - 0-743,477 rt(x) + 0-850,313 rt(x) -0-775,565 r0{x) + 1-013,318 r,(x)}. 
The reason for the definition and the name of the function will appear in Chapter 14. 
6.28. Up to this point it has been assumed that a frequency function possesHos a 
convergent Type A series. We shall not here enter into a discussion of the conditions under 
which this is so, except to warn the reader that a great many mistakes have been made on 
the subject and to quote some theorems without proof. 
(1) Cram6r (1925). If f(x) is a function which has a continuous derivative suoh that 
as)'-5 
dx 
converges and if f(x) tends to zero as | x | tends to infinity, then f{x) may be developod in 
the series 
00 
f(x) = J£!%J)laL{x)t (6.52) 
i-o' 
152 STANDARD DISTRIBUTIONS 
where 0$ is given by 
J — CO 
This series-is absolutely and uniformly convergent for — go < x < oo, 
(2) A theorem by Cramer (1925) based on one by Galbrun. If f(x) is of bounded 
variation in every finite interval and if 
I 
\f(x)\e*dx 
exists, then the expansion of f(x) in the series (6.52) converges everywhere to the sum 
ifffc + °) + /(* ~ °) }• ^^e convergence is uniform in every finite interval of continuity. 
Cramer has also shown that this last theorem cannot be substantially improved upon 
as regards the behaviour of f(x) at infinity. Consider in fact the function f(x) = el*"". 
We have, in virtue of (6.33) and (6.31), 
dx 
/•CO 
dx 
f" 6-***Hr(x)dx = f e-^\xHr_x dx - (r - l)f e"**'. Hr_ 
J — CO J —CO J —00 
[o-te1 "I00 r _ i r00 /•» 
=(r ~ ^i ~ i)He"lrlfrr"2(a:) *"■ 
If r is odd the integral vanishes because Hr is an odd function of x. If r is even, say 2r, 
the integral becomes 
(Sr-•.!,(».-8,... 1(^-1)'.-^ 
1 "(2v)!/l 
V(2A) 2' 
iY- - lV 
The appropriate coefficient of H2r in the Type A series is then ^ L. Now when 
2"vl 
2M 
" " 2M 
a; = 0. #«„ = ——JA—-". The series then becomes 
2M , 
f> l (2v)l A IV 
Z^V(2A)22"(v!)2\ 2aJ" 
In virtue of the Stirling approximation to the factorial, the vth term of this, say w„, becomes 
in the limit 
/, IV 1 
10 11 . ■'. . > Uv r+t I 1 — I - 
' \ 2XJ <s/(2fa 
V(2Ajiv) 
I i 
so that v^l- -^7- 
Hence,' for "'fl'< J thei' series is divergent. ■■' 
f $.29. Prom the statistical viewpoint, however, the important question is not whether 
an infinite series can represent a frequency function, but whether a finite number of terms 
TETRACHORIC FUNCTIONS 153 
can do so to a satisfactory approximation. It is possible that even when the infinite series 
diverges its first few terms will give an approximation of an asymptotic character. 
This subject has not yet been fully explored and there has been some controversy 
about the value of the finite Type A series. Two things seem clear:— 
(a) The sum of a finite number of terms of the series may give negative frequencies, 
particularly near the tails (as, for instance, in Example 6.2). 
(b) The series in the CHarlier form (6.40) may. behave irregularly in the sense that 
the sum of k terms may give a worse fit than the sum of (k — 1) terms. 
How serious these disadvantages may be depends on the purposes in view. So far 
as practical graduation is concerned it would appear that the finite Type A series is successful 
only in cases of moderate skewness and in many such cases a Pearson distribution is just as 
good. In many statistical inquiries we are more interested in the tails of a distribution 
than its behaviour in the neighbourhood of the mode, and it is here that the Type A series 
appears particularly inadequate. 
But this is not by any means a unanimous view. Arne Fisher (1922) has considered 
a modified form of the series which he claims to meet most of these criticisms. He considers 
the series ^ 
/ = (c0 + c1Hl + . . . cr#r)a(z) .... (6.53) 
but determines the c's, not from the observed moments and the relations (6.38) but by the 
method of least squares, i.e. so that 
^{/-(Co + C:# . . . +crHr)*(x)}* 
shall be a minimum. The method involves some laborious arithmetic, but Fisher has 
successfully graduated a number of actuarial experiences by using it. 
Two other actuarial statisticians have pointed out the difficulties of the Type A series, 
Steffensen (1930) adducing some theoretical objections and Elderton (1938) summing up 
in favour of Pearson distributions. 
Example 6.3 
As an illustration of the irregular behaviour of terms in the Type A series, consider 
the distribution 
dF = ——tf>-H-x dx 0 < x < oo. 
Its - characteristic function is 
W) 
(1 - it)P 
and thus Kr = p(r — 1)1 
or, in standard measure, 
(r - 1)! 
f r 
P'2 1 
From the manner of the formation of terms in (6.48) it is "evident that the coefficient 
cr is the sum of terms Kr, Kr_3 k3, . . . (#cffi kQi : . . KgJ, where (qt . . . qm) is a partition 
of r such that no q is lass than 3. It will then be clear that, since kq is of order #1_a, the 
term of greatest order in p is that with the greatest number of parts in («■ ...«:_ ). 
For example, if r = 9 it is (33), if r = 8 it is (42), and so on. 
i 
154 STANDARD DISTRIBUTIONS 
From these considerations we can find the order in p of the terms in the Type A series. 
They are 
Term 
Order in p 
• 
* 
• 
• 
• Cq 
. 0 
c3 
-i 
c4 
-1 
cs 
-1* 
c8 
-1 
c7 
-1* 
c8 
-2 
cB 
-H 
CI0 
-2 
cu 
-2i 
cia 
-2 
The terms decrease in order of p, but not at all regularly, and it is clear that in general no 
-coefficient will be negligible compared with a preceding one if p is large. The asymptotic 
qualities of such series obviously require careful investigation in particular cases. 
The Type B Series 
6.30. Just as the Type A is derived from the normal integral, a Type B series has 
been derived by Charlier from the Poisson distribution. Writing 
y(m, x) = ,— „ (6.54) 
x\ N 
for integral values of x, put 
y(m, x) = em00B< cos (m sin t — xt) dt . . . (6.55) 
n Jo 
for all x., When x is integral this reduces to (6.54).* In other cases 
y(m,x) = sinyra; > (— 1)'— -. , . . (6.56) 
Write Vy = y(m>x) — y(m> x — \) 
and fix) = ^brfy (6.57) 
y=o 
This is the Type B. . Charlier recommends it in cases of skewness when Type A is 
inapplicable (though the dividing-line is not clear). In theory it may be used for continuous 
variates, but in practice has only been applied to discontinuous variates proceeding by 
•equal intervals. In fact, the objections to Type A apply a fortiori to the continuous form 
of Type B and various other complications appear (cf. StefFensen, 1930). 
6.31. Defining polynomials Or by the relation 
dr 
y(m, x)Gr(m, x\ = ^ y(m, x) , ■ . . (6.58) 
we find that 
V / t\x 
Or = coefficient of — in e~'[ 1 -\ ] 
r\ \ mj 
r! r 
7=0 
f2rt qTTIS 
* The integral I em eit~ixtdt, by the substitution e" = z, is 2n times the residue of -—77 in the 
Jo ^ 
, . , , m* 
unit circle and is thus equal to 2n —r. 
xl 
THE GRAM-CHARLlfiR SERIES OF TYPE B 
155 
In a similar manner to that used for the Tchebycheff-Hermite polynomials we have 
v0r = ^r-i , • (e-eo; 
which may be compared with (6.31). It may also be shown that 
^-(-irv'rfo*"') (6.61) 
y(m, x) 
and thus Or may be calculated from the r£h differences of the Poisson function y(m, x) in 
the same way that Hj. may be derived from the rfch differential coefficient of the normal 
distribution. 
The O'a also obey the orthogonal law 
£(GrGay) = 0, r^s 
x~0 
r\ 
= —;> r = * 
mr 
. (6.62) 
Thus if 
i 
. (6.63) 
x = 0 
TABLE 6.2 
Type B Series fitted to a Discontinuous Distribution of Particles emitted by a Radioactive 
Element in Units of Time. 
X 
0 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
14 
Totals 
Frequency. 
57 
203 
383 
525 
532 
408 
273 
139 
45 
27 
10 
4 
0 
1 
1 
2608 
TypeB 
(2 tonns). 
49-5 
201-3 
403-4 
632-3 
520-6 
402-6 
254-8 
137-1 
64-0 
26-1 
9-4 
. 3-0 
0-9 
0-2 
0-0 
, 2605-2 
TypeB 
(3 terms). 
49-0 
201-0 
404-3 
533-8 
521-6 
402-6 
254-4 
136-7 
63-9 
26-2 
9-6 
31 
0-9 
0-2 
0-0 
2607-1 
TypeB 
(4 torrns). 
58-2 
199-8 
386-1 
523-9 
6321 
418-2 
260-2 
134-0 
56-7 
229 
8-6 
3-6 
1-6 
0-8 
0-3 
2609-0 
156 STANDARD DISTRIBUTIONS 
In the same manner as for Type A we have, choosing the constant m equal to filt 
b0 = 1 
1 I (6.64) 
b3 = 3j(/"s - 3^ + 2m) 
64 = 4j{/«4 - 6^8 + ^,(11 - 6m) + 3m(m - 2)} 
etc. • 
Example 6.4 
Table 6.2 shows the frequency of the number of alpha-particles (a;) emitted by a bar 
of polonium in intervals of £th of a minute in some experiments by Rutherford and Geiger, 
together with the frequencies given by the Type B series with two, three and four terms. 
The calculations are due partly to A. Fisher and partly to Aroian (1937). 
The Normalisation of Frequency Functions 
N 6.32. Several of the important theoretical distributions occurring in statistics depend 
on some parameter n in such a way that as n tends to infinity the distribution tends to 
normality. For large n it is often a sufficient approximation to assume the distribution 
normal, but for small or moderate n this may be hardly exact enough. In such a case 
we are nevertheless able to use the normal integral by seeking for a variate transformation 
£ = a0 + axx + a%x2 + azx3 + . (6.65) 
where the a's are of order n~* or smaller. By choosing the a's appropriately we can bring 
the distribution of f much nearer to normality than that of x and hence find the distribution 
function of x from that of £, assumed normal. 
Consider in fact the Edgeworth form of the Type A expansion (6.45) 
exp {- =4-* + ^ -§* + .. ]^e.-*~ • (0-06) 
We have retained the terms in D and D2 because the approximation may perhaps be slightly 
improved by taking m and a2 in the ^-distribution not quite equal to the mean and 
variance of x. 
We now assume that the cumulant #cr is of order n1~r, a case of fairly common 
occurrence ; that kx — m is, by choice of m, of order n~1 ; and that #c8 — a2, by choioe of a2, 
is of order n~2, so that we may write 
Kl — m = lxa lj = 0(w~*) * 
Ki - a2 = l2a2 lz = 0(ra_1) 
Then a2 is of order 1. and thus 
5 = 0(n1"*r). 
Thus (6.66) may be written 
exp {- lxoD + \1&2D2 - \ho*D* + ^kaW - ^^ 
, i +TbJla"JD»-. . .}-j^b-V#*-** , (6.67) 
/ 
THE NORMALISATION OF FREQUENCY FUNCTIONS 157 
where lt and Z3 are 0(w~*) 
Z2 and 14 are 0(w-1) 
Z8 is O(ra-J) 
Z6 is 0(w~2), etc. 
Expanding the operator and retaining only terms up to and including Q(n~*) we find 
for the operator 
1 - hoD + \lto*D* - iko*D* + ^Z4<r4£4 - Th>ho*iy + y^o^-D6 + £(Z?<r2£a 
+ Ws°*D* - %l,ho*D* + -hU^D* - ^hl^Di + ^Zso8!)8) + £ ( - %<t*Dz 
~*i(A9£9 + fFMW - ll\Uo*& - \llho*D* + ^t§liaioDio _ ^^7^7 
+ &l$o*B* + ^Z^o*!)* + ^Z^cW) + i&(Zfr4£4 + TaWy^13- 
+ f Z&tr«Z>fl + ^^.D8 + M^10^10) (6.68) 
The result of this operation is a similar expression, which we will not bother to write 
— J and multiplied by —jr—- 
The distribution function is given by integrating this expression, and we then have 
for the frequency less than or equal to m + ax (arranging the terms in order of , 
magnitude in n) 
I 
1 e-*"1 dx -f -rj-^e-^t- (h + iZ8ffa) - (K^ + \WBX + ^.H," 
-ooV(27r) V(2rc) 
+ j-UkH, + M?#i + ikWA + WoW) - (rf#a + -rt^i + WW. 
+ tIt??-^ + t^A^' + TiVa ^7 + T4T^^3^4-Hr7 ' + Th^hH7 + TaWi^e 
+ ttWPJ^o + rrioifi-^ii)] (6.69) 
6.33. Now let £ be a normal variate. We will determine $ in terms of x such that 
f 1 -t 
e a dy = F(x), (6.70) 
J 
.F(:r) being the distribution function given by the Type A expansion (6.69). 
We have 
!. 
1 e~i dy = F(£) = F(x + $ - x) 
= //o xe 2 »y ~ "St—J" //o \e 2 % + • • • etc., 
by Taylor's theorem, 
=f .T^T)6"^ ^ " ^ir)a(aj) " {*-^p-H&)<*) - ^—^^(xHx) . (6.71) 
and this is equal to (6.69). . 
The next step is to invert this series so as to give (x — £) in powers of x. Assuming 
x — £ = aQ + axz + aa#a + . . . etc., . . . (6.72) 
158 STANDARD DISTRIBUTIONS 
we see that when x — 0, £ = — a0, x — £ is of order w-i ; and hence, to order n~2 we have 
from (6.71), with x = 0, 
£2 Jt3 a s 
f_^(+0)+|(-l)=O„-^ 
and this is equal to the expression in square brackets in (6.69) with x — 0. 
We then find 
a0 = k- ih - ihh + &h + ihk + ihh + Afc - ftUi - AW* + AfeV«- 
We can now find ax in (6.72) by identifying coefficients in x, and so on. After some algebraic 
reduction we find, writing the terms in descending order in n, 
x-£-l1+ ±13(X* - 1) + tfiX - ±hhx + AM*3 - 3») - AS(4a!« - 7a?) - falt 
+&lk -AW^Ba?1 -3) -#!*,(*■ -1) +tW(«* ~6*2 +3) +M?(12«" ~7) 
- t*tW«(11s4 - 42a:2 + 15) + ^8(692:* - 187a; + 52) - %%x + j^a; 
• + tflh* ~ AW^8 - 15x) - AW»8 - x) + TfhUx* - 10a;3 + 15a;) 
- }l>l%x + ^Za£j(36a;3 - 49a;) - -^^(Sx6 - 32a;3 + 36a:) + AW^Us8- 2lx) 
- tWAC7*8 - 48*3 + 51*) - *irW5(138a;» - 187a;) + ^^(11 la;* 
- 547a;3 + 456a;) - TJrA(948x* - 3628a;3 + 2473a;). . . . ■ . (6.73) 
This is our. required expression of the variate £ in terms of the variate x. To order 
n~a at' least £ will be normally distributed. 
It is often more convenient to express x in terms of £. This may be done by noting that 
& — £ = 9(x) = 9(£ + & — i) 
= g(£) +(x- £)g'(e) +__ . 
= g(t) + g'(S) fe(« + x~=i g'ik)} +. . . 
and by continuing the process 
*-e=g(€) + g{M® + g(W*(€) + toWtf) + <7(£)?'8(£) + ttWWtf) 
+ i<7 W"(£) + (6.74) 
Hence, using the value of £ given by (6.73) we find, after some reduction, 
x - £ = h + tf,(*■ - 1) + iW + A^tf8 ~ 3£) - AS(2f' ~ 5£) ~ W.(f■ -1 1) 
+ tM>(£4 - G£z + 3) - AW*4 - 6*2 + 2) + *h?(12£4 ~ 63? + 17) 
■ - *?£ - AV«(*8 - 3£) + T**W - 10£3 + 15£) + A^(l«f' - 25£) 
- *MS(3p - 24£3 + 29£) - T-folMW ~ 17£3 + 2U) + rb^U^8 
-103£? + 107£) -^^3(252? - 1688£3 + 1511£) . . . .(6.75) 
Example 6.5 
Consider again the distribution of Example 6.3— 
dF = ™—vfi"*^""1 dx 0 < a: < oo. 
A?) 
i 
We have already found that, in standard measure, this tends to the normal form, and 
that Kr is of order p 2. 
We will take Zx and Za of (6.67) to be zero, which implies that our normal variate £ is 
to have the same mean and variance as that of x. We have 
Z3 = 2p~* h = 6p-i Z6 = 24p-^ Z6 * 120p-». 
THE NORMALISATION OF FREQUENCY FUNCTIONS 159 
(6.69) then becomes 
I 
1 _,.. , 1 
,-te'LlH. 4- -iff, 4- —H. 4- ~H. 4- -Lh. 4- — 
e~iJ' dx - -r^-*-i**)*Ha 4- ~Ha 4- A-H* 4- ~H. 4- r^-,H« 4- ^-.H 
4p 18^o 5©' 1253s 
»r 
L8 
-«VW V(2w) \3#* " ' 4p ' 18p ° ' 5^ * ' 12jpS 6 ' I62p 
+ 6^6 +' 3^T«Ht + 15^' + 72^8 + iQi^11}' 
Let us, as a simple illustration, find the distribution function of x for p = 9, x = 12. 
The mean of the distribution is then- 9 and its variance 9, so that this corresponds to 
a deviation (12 — QJy'Q in standard measure, equal to unity. It is found from (6.30) 
and an additional equation for i?n that 
Ha = 0, Ha = - 2, Hi = - 2, H6 = 6, H, = 16, H, = - 20, He = - 132, H0 = 28, 
H10 = 1216, Hxl = 936. 
We then find for the distribution function 
j: 
1 e~^dx + -—-—e~?(0-015,163,5). 
_ooV2rc V(2tt) 
The values for thes normal funotion are obtained from the tables and we get the value 
0-841,345 + (0-241,970,7)(0-015,163,5) = 0-8450, 
which is exact to four places. The approximation is evidently fairly good, even for values 
of p as low as 9. 
We could have found the same result by using (6.73). Substituting x = 1 in that 
equation we find 
£ = 1-015,386, 
and the distribution function for the normal integral with deviate equal to this value of 
£ is 0-8450 as before. 
Suppose now we wish to find the deviate x whose distribution function is F(x) = 0-99 
when p = 15. 
The normal deviate £ corresponding to such a value is found from tables to be 2-326,348. 
We then have from (6-75) 
* -f=3?(fa A 1] + i{i* ~3I) +•et0- 
which will be found to give 
x = 2-697,22. 
This is the value in standard measure. The deviate in ordinary measure is 
15 + aV15 = 25-45. 
This is exact to two places of decimals. 
The example shows that, notwithstanding the non-convergence of the infinite Type A 
series, a satisfactory approximation may be obtained from its first few terms, at least in 
certain cases. We may remark without proof that by an adaptation of a procedure given 
by Cram6r (1928) it may be shown that an asymptotic expansion does in fact exist for 
the distribution of this example. 
NOTES AND REFERENCES 
An excellent account of Pearson's distributions is given in Elderton's book. Examples 
of the fitting of the distributions to the data of experience abound in Biometrika. 
160 
STANDARD DISTRIBUTIONS 
For the Type A series see Charlier (1906 and 1931), Henderson (1922), Cramer (1928) 
and Bowley (1928). For the Type B series see Charlier, Jordan (1927), Aroian (1937) 
and Steffensen (1930). 
Charlier has also proposed a Type C series, as to which see his paper of J. 928 and the 
brochure of 1931. 
For the convergence of the Type A. series and its relationship to elementary errors 
see two admirable papers by Cram6r (1926 and -1928). 
. A. very good general account of these distributions and an examination of the 
possibility of extending them to the bivariate case is given by Pretorius (1930), who gives a 
number of references. Up to the present no entirely satisfactory system of bivariate 
distributions corresponding either to those of Pearson or to those of Charlier has been 
found. ■ 
For some early efforts by Edge worth to transform distributions to the normal form, 
see Bowley (1928) and Pretorius (1930). The approach of sections 6.32 and 6.33 is due 
to Cornish and Fisher (1937), who give some tables which are useful in this type of work. 
The polynomials Hr(x) are frequently referred to by English writers as Hermite 
polynomials, but they are really due to Tohebycheff (Memoires de VAcademie de Saint Pdtersbourg, 
1860). Hermite's papers on this subject followed four years later (Comptes r&ridus, 58, 
93 and 266). 
Aroian, L. A. (1937), "The Type B Gram-Charlier series," Ann. Math. Statist., 8, 183. 
Bowley, A. L. (1928), F. Y. Edgeworth's Contributiona to Mathematical Statiatica, Royal 
Statistical Society. ^ 
Charlier, C. V. L. (1906), Researches into the Theory of Probability, Lund. 
(1928), "A new form of the frequency function," Meddelandefrdm Lunds Astronomiska 
Observatorium, Series II, No. 51. 
(1931), Applications a Vastronomic (one of the series in Borel's Traite du calcul des 
Probability, Gauthier-ViUars, Paris). 
Comrie, L. J. (1939), Tables of tan~xx and log (1 + x*), Cambridge University Press. 
Cornish, E. A., and Fisher R. A. (1937), "Moments and cumulants in the specification 
of distributions," Revue de VInst. Int. Stat., 5, 307. 
Cramer, H. (1926), " On some classes of series used in mathematical statistics," Den sjette 
Skandinaviske Matematikercongres, Copenhagen. 
(1928), "On the composition of elementary errors,".Skandinavisk Aktuarietidskrift, 
13 and 141. 
Edgeworth, F. Y. (1904), "The Law of Error," Cambridge Phil. Trans., 20, 30, 113 (and 
an appendix issued with bound reprints). 
Elderton, Sir W. P. (1938), Frequency Curves and Correlation, 3rd edition, Cambridge 
University Press. 
Fisher, Arne (1922), Frequency Curves, Macmillan. 
Henderson, J. (1922), " On expansions in tetrachoric functions," Biometrika, 14, 157. 
Jordan, C. (1927), Statistique Mathimatique, Gauthier-Villars, Paris. 
J0rgensen, N. R. (1916), Undersegelser over Frequensflader og Korrelation, Busck, Copenhagen. 
Pearson, K. (1925), " The fifteen constant bivariate frequency surface," Biometrika, 17, 268. 
Pretorius, S. J. (1930), " Skew bivariate frequency surfaces examined in the light of numerical 
illustrations," Biometrika, 22, 109. 
Romanovsky, V. (1924), " Generalisation of some types of the frequency curves of Professor 
Pearson," Biometrika, 16, 106. 
EXERCISES 161 
Steffensen, J. >(1930), Some Recent Researches in the Theory of Statistics and Actuarial Science, 
Cambridge University Press. 
Wishart, J. (1926), " On Romanovsky's generalised frequency curves," BiometriJca, 18, 221. 
EXERCISES 
6.1. Show that for the Pearson distributions 
.dlogy __. x 
~dx B0 + Bxx + B2x* 
the range is unlimited in both directions if B0 + Bxx + B2xz has no real' roots; limited 
in one direction if the roots are real and of the same sign ; and limited in both directions 
if the roots are real and of opposite sign. 
6.2. Show that the Pearson Type VI curve may be written 
... I i X"\ — v tonh- — 
and discuss the relationship with the Type IV curve. 
6.3-. Assign the following distributions to one of Pearson's types:— 
dF = ke *x &% 
kdt 
dF = 
'2a\!±I 
■2 
dF = lc(l -r*)dr 
N-y-2 
dF = k>f~*(l - rj*)~"2~~ drj 
(All these distributions are important in the theory of sampling.) 
6.4. Show that for the Type B series the coefficients of equation (6.03) may be written 
symbolically— 
b, = i(^' - w)W 
jlltin ~ \J)fHi-iim + uV[/-2]W 
» 
(C. Jordan, 1927.) 
6.5. Show that, in the notation of 6.30, 
Vy(w, x) = - ^y(m, z + 1), 
Hence that ) y(m, x) = 1 — Im{X + 1), 
8-0 
A.S. M 
162 STANDARD DISTRIBUTIONS 
where Im is the incomplete ^function ; and hence that the sum of the first.(k + 1) terms 
of the Type B series is given by 
60{1 - Im(k + 1)} - (6X +. b&x + . . -M». *) 
(C. Jordan, 1927.) 
6.6. Show that if y is a-function of x which it is desired to represent approximately 
by the form 
r 
then the values of the c's appropriate to the expansion of y in this form are such -as to 
minimise the sum 
2 
f" \y - Xc^{x)<x) 
V«(«) 
> dx 
6.7. i Show that for a Pearson distribution -4 = =■—rr—r~i—o the characteristic 
/ o0 4- oxx + biX* 
function obeys the relation 
bi6m + (1 + 26a + bi6)% + {a + bi + bo6)<l> = 0> 
where 6 = it. Deduce the recurrence relation between the moments. 
Show also that the cumulative function obeys the relation 
"^ + &f\ + <J + 2b' + «& + (a + 6" + M> " °- 
Hence show that the cumulants obey the recurrence relations 
{1 4- (r 4- 2)6a}jcr+1 + rblKf + rfiJ T y J/ca^.j + T ~ W3/cr_2 + . . . 
4- K . W+i ^r-j 4- • • • 4- r ~ Vr-iic,| = 0. 
6.8. Show that no distribution which is not completely determined by its moments 
can be expanded in a convergent Type A series. 
6.9. If the distribution 
dF = e~ix% dx — oo < x < oo 
is transformed by x = T(log10 £ —'■ I) 
and ^ = log10e, X = e6'*', 
show that fa = k*(X + 3) - 4 
0a - 3 = X\X2 4- 2A 4- 3) - 6 
and that p^)* - S^f^)* - /4 = 0, 
EXERCISES 
163 
where fix is the first moment about the start of the transformed curve. Thence that 
I = 2 log p\ - | log (fi2 + ^i2) 
bk2 = 2 log & - 21. 
6.10. Show that if a function in standard measure is expanded in a Type A series 
the coefficients of the second and third terms depend respectively on fa and fa and thus 
provide measures of skewness and kurtosis. 
\ 
\ 
CHAPTER 7 
PROBABILITY AND LIKELIHOOD 
7.1. The previous six chapters have dealt with the theory of statistical distributions 
from a descriptive point of view. It has been explained that the distributions occurring 
in practice exhibit certain regular features which permit of representation by mathematical 
forms ; that they can be characterised by certain parameters such as moments and cumu- 
lants ; and that certain general theorems about distribution and frequency functions can 
be deduced. We now begin a study of a different kind,, namely, the inquiry whether any 
statements can be made about populations or their parameters and distributions when 
only a sample of the populations is available for scrutiny. Except in trivial cases it is 
not possible to make any statements on these matters with the categorical certainty of 
deductive logic ; but it is possible, and indeed it is necessary if scientific inquiry is to go 
forward at all, to make statements of a less definite nature in terms of probability. In 
this chapter we shall accordingly be concerned with the theory of probability as it affects 
statistics and in subsequent chapters with its applications in statistical theory. 
7.2. In ordinary speech we use the words " probability ", " chance " or " likeli-. 
hood " to describe an attitude of mind towards some proposition of whose truth we are 
not certain. We say that it is improbable that life exists on Mars, that the chances are 
that if a penny is tossed ten times it will come down heads at least once, that it is likely 
to rain to-morrow, and so on. It is rarely indeed in practical affairs tha^t we are confronted 
with a proposition of whose truth we are absolutely certain. Nevertheless, we often have 
to assume that such propositions are true or untrue in order to reach decisions and to act 
in a rational way. 
The attitude of doubt we adopt is described in terms of probability. We say that 
the propositions are more or less probable and accept or reject them accordingly. 
7.3. A little introspection will convince the reader that all the attitudes of mind to 
which we relate the concept of probability have certain things in common :— 
(a) They concern propositions. The mind considers a proposition which lias meaning 
and assumes towards it a certain attitude of doubt. It is very common both in 
mathematics and in statistics to speak of the probability of an event, or even of a variate-value ; 
but these are Condensed expressions for the proposition that an event will happen or that 
a member of a population has a given variate-value, and, though very convenient shorthand 
expressions which will often be used in the sequel, must not be allowed to obscure the 
essential fact that propositions are concerned. 
(6) There are degrees of probability. We say that it is very improbable that a hundred 
tosses with a penny will not result in a head ; that it is more probable that horse A will 
win a race than that horse B will do so ; that the probability of having wet weather in the 
course of an English summer is so great as to be near certainty. But it does not follow 
(and some writers on the logic of probability do not admit) that every pair of probabilities 
can be compared. It could with consistence be maintained that, whereas we may compare 
the probability of getting ten trumps in a game of cards with that of getting eight, there 
is no way of comparing the probabilities of the propositions, say, that there exists a planet 
outside the orbit of Pluto and that the human race will ultimately go bald, 
164 
PROBABILITY AND LIKELIHOOD 105 
(c) The degree of probability attributed to a proposition varies according to the amount 
of- relevant evidence available to the particular mind considering the proposition. If wo 
know that a horse has won its three previous races we attach a greater probability to the 
proposition that it will win the next. If we know that a penny has heads on both sides 
the probability that it will come down heads when tossed is so great as to amount to 
certainty ; and so on. 
(d) Pursuing this last point, we see that certainty can be regarded as a limiting form 
of probability. As a proposition becomes more and more probable it tends towards cortain 
truth ; as it becomes more and more improbable it tends towards certain untruth. 
7.4. The object of the theory of probability is to give to the somewhat indefinite 
notions described above the precision of a science, and, since numerioal measurement is 
the greatest precision which a science can possess, to measure probability numerically. 
Several writers have explored the more general problem, foreshadowed as early as Leibniz, 
of developing a logic of probabilities, and the reader who is interested may refor to the 
work of Keynes (1921), F. P. Ramsey (1931) and Johnson (1921-4). From the statistical 
viewpoint the interest of this subject centres in the numerical theory of probability which 
alone will concern us in this book. 
It is at this point that we arrive at the first of the differences of opinion among 
authorities on the theory of probability. Some writers try to include all the ideas generally 
associated with the word " probability " within the scope of their theory, which is thus 
applicable to any of the attitudes of doubt covered by the meaning of the word. The 
principal modern exponent of this viewpoint is Jeffreys, whose book (1939) should cortainly 
be read by all serious students of the subject. Most statisticians, on the other hand, are 
concerned with the probabilities of propositions of a particular kind, namely, those which 
form the members of populations of propositions. Under the more general theory, it lias 
meaning to speak of the probability of an isolated proposition such as the one that 
{Shakespeare's plays were written by Francis Bacon. In statistics we are more usually concerned 
with the proposition which asserts the happening of some event which could have arisen 
in a specified number of ways, such as the throwing of a number with an ordinary die. 
The first approach takes probability to be an undefined idea, like the straight lino of Euclidean 
geometry, and builds up the theory from certain axioms. The second approach Necks to 
define probability in terms of the relative frequency of events and thus to throw the theory 
back on to the pure mathematics of abstract ensembles (KolmogororT, 1933) or to the 
limiting properties of sequences (von Mises, 193G). The reader who is perplexed by the 
controversy between the adherents of the axiomatic and the frequency theories will find 
many of his difficulties resolved by the consideration that the two theories cover dill'eronl. 
domains of thought, or rather, that the axiomatic theory attempts to cover a wider domain 
than the frequency theory. 
7.5. This, however, does not explain away the whole of the difficulty, and the reader 
will have to choose for himself among the various possible sets of fundamental ideas forming 
the starting-point of the theory. When we consider the concept of probability as a 
psychological matter we can either suppose that further analysis is impossible or unprofitable, 
in which case the axiomatic approach seems inevitable ; or we can ask how the mind comes 
to take up an attitude of belief in propositions which confront it. It is not necessary hero 
to go into this question at length, but there would, in my own opinion, be a considerable 
measure of agreement that the concept of probability is founded on our experience of the 
) 
166 PROBABILITY AND LIKELIHOOD 
frequency of observed phenomena. When we say that the Probability of a coin coming 
down heL on being tossed is one-half we have in mind, I thmk that ii it is tossed a. hu^ 
number of times it will come down heads in approximately half the cases. H.ven m extreme 
cases, say, when we attempt to assess the probability of a horse winning a given race an 
event which cannot be repeated, we are, I think, picturing our estimation as one of a number 
of similar acts and assessing the relative frequency of the horse's victory m that, population. 
But it has to be admitted that, even if this be true, there is no necemitij to use the 
concept of frequency in the axiomatisation of the theory. The concept of a Htraight line 
may very well be founded on our experience of the local properties of rays of light, but it. 
does not follow that the indefinable of Euclidean geometry are to be analysed into optical 
concepts. 
The Basic Rules of Direct Probability 
7.6. For our present purposes the problems of fundamental may bo paused over, 
since all parties are agreed on the rules governing the calculus of direct probabilities. (Tin- 
so-called "inverse " probabilities will require more discussion and will ho dealt with later.) 
We therefore enunciate these rules without attempting to deduce thorn from more primitive 
propositions. 
In the first place it is assumed that probability is measurable on a eontinuou.s scale, 
so that any probability can be expressed as a real number. Wo .shall, in fact, suy that t\ 
probability is x, a real number. This assumption implies, among other things, that any 
two probabilities maybe compared; for if they are measured by the numbers ;r- and y 
we may say that the probability of the first is greater than, equal to, or loss than, that of 
the second according as x > y, x = y, or x < y. 
7.7. The probability of a proposition q on data p i« written P(q \ p). We have (hen 
Rule 1:— 
If p entails q, P(q \ p) = 1 (7.1) 
If p entails not-q, P(q \ p) = 0 . . . . (7.2) 
This rule defines the end-points of our scale of probability. Certainty that, a propu.sif ion 
is not true is represented by zero, certainty that it is true by unity. Any probability lies 
in the range 0 to 1. 
7.8. Rule 2.—If q1 . . . qn are a set of equally probable and mutually exclusive 
propositions on data p, and if Q is a subset of m of these propositions, then 
P(Q\P)^~ (7.») 
This proposition is the starting-point of the frequency theory of probability it is 
usually stated in some such form as : if of a set a£n mutually exclusive ami equally probable 
events m are distinguished by some characteristic A, the probability of an event, bearing 
A is — 
n 
- ^f ^t™ ,t° ^ r"le &0m the 1°8ical tfewpomt is that H contains 1.1k, ,,„„,.,* 
^ff* and uis *•"? ^^if °™ <«l°Pta « as a definition The matVmut i.'al 
theonst doahng mth^probability mi the mathematician's facile way, overcmn, M.i» Imul.lo 
•ather by acceptmg the crcular deflation, or by defining probability pmvly an a mJ-rty 
of sets of pomte. For example, such a definition might be : if of an agK,4at„ „ f, I jo t« 
BASIC RULES OF DIRECT PROBABILITY 167 
n in number m are characterised by some quality A, the probability of any member bearing 
Ml 
A is, by definition, the number —. To take a more sophisticated line, we can regard the 
n 
objects as points of a set, attach set-functions to them obeying certain axioms and 
postulates, and thus build up the theory of probability as a branch of the theory of set-functionR. 
Any verification of the theory, any test whether it provides a reasonably accurate picture 
of the way things happen in the world, is referred to experimental physics. The 
mathematician, of course, is used to this devolution of responsibilities, but the statistician is 
concerned with concordance between theory and practice and cannot always leave 
experimental verification to others, 
7.9. Ride 3.—If the probabilities of n mutually exclusive propositions qt . . . qn 
on data p are Px . . . Pn, then the probability on data p that one of them is true is 
P,+Pa . . . + Pn. 
This is generally known as the " theorem of the addition of probabilities ". In the 
language of the textbooks, the probability that one of n mutually exclusive events will' 
happen is the sum of their separate probabilities. 
7.10. Rule 4.—The probability of two propositions q and r on data p is the product 
of the probability of q given p and that of r given q and p. Symbolically, 
P(9.r | P) = P{q I p)P{r\ qp). ..... (7.4) 
Since q and r appear symmetrically we also have 
P(qr | p) = P(r \ p)P(q \ rp) (7.5) 
From the frequency standpoint this rule is almost self-evident. If of a set n, (a) bear 
the characteristic A, (b) the characteristic B, and (ab) both characteristics, then the rule 
states that 
(ab) = (a) (ab) = (6) (ab) 
n n ' (a) n (b)' 
a simple arithmetical proposition. 
More generally we have 
■P(ffi?« •••&!#)= ptei I p)p(q* 12ip)p(q* I ffi2ip) • • • Pith I &-i • • ■ w) (7.6) 
a result which follows from the repeated application of Rule 4. 
If, as a particular case, 
P(qr | p) = P(q | p)P(r \ p) (7.7) 
we have, in virtue of (7.4), 
P(r\p) =P(r\qp), (7.S) 
and q is then said to be irrelevant to r, given p. A knowledge of q does not afl'oct the 
probability of r on data p. 
7.11. The above four rules and various elaborations of them provide the basis of 
the direct theory of probability, which is concerned with problems of the type : given a sot 
of propositions with known probabilities, determine the probability of some contingent 
proposition. This is a branch of pure mathematics and will be found discussed, for example, 
in most textbooks of algebra. Ultimately all problems in this branch of the theory are 
reducible to the counting of the number of ways in which certain events can happen. The 
following examples will illustrate the type of investigation involved. 
* 
168 PROBABILITY AND LIKELIHOOD 
Example 7.1 
What is the probability that a specified player will get a hand containing 13 cards 
of one suit at a single deal at a game of bridge 1 
We have to consider here the total number of ways in which a given player can be 
(52\ 
j ways. 
Of these ways only, four will contain cards of one suit. 
We then assume that all the possible deals are equally probable and are thus able to 
(52\ 
— J, so that the probability is 
4 
P = 
Q 
_ 4.39! 13! 
52! * 
Factorial expressions of this kind may be found from tabled logarithms of factorials or 
by the use of the Stirling approximation. In this particular case we find 
P = 6 x 10"12 approximately. 
Example 7.2 
% letters, to each of which corresponds an envelope, are placed in the envelopes at 
random. What is the probability that no letter is placed in the right envelope ? 
The condition that the letters are put in the envelopes " at random " is to be 
interpreted as meaning that every possible way of assigning the letters to envelopes is equally 
probable. The question, under Rule 2, then reduces to the purely algebraic one : in what 
proportion of the possible cases does no letter get into the right envelope ? 
Suppose that un is the number of ways in which all the letters go wrong. Consider any 
particular letter. If this occupies another's envelope and vice-versa, which can happen 
in n — 1 ways, the number of ways in which the remaining n — 2 letters can go wrong is 
un_o. But if the letter occupies another's place, which can happen in (n — 1) ways and 
not vice-versa, there are un_x ways in which the others can go wrong. Hence we have 
the difference equation 
We may re-write this 
un = {n - l)K_i + wrt_2). 
un - nun_x = - (un_1 - n - 1 un_2) 
and putting vn = un - nun_x 
we find vn = - vn_1} 
= (- If-'v 
2- 
Thus un — nun_x = (— l)n--(w2 — u{). 
But Ut = 0 and wa = 1 and thus 
un -nun_x = (- l)n 
f 1 1 ( — l)n1 
whence „„ = ra!|_ __ + ... L_±j. 
BASIC RULES OF DIRECT PROBABILITY 
169 
The total number of possible ways is n\ Thus the probability required is 
1-1 + (- l)n 
2! 3! ' ' " ~W~' 
i.e. the first (n — 1) terms of 1 — e_1. 
Example 7.3 
Three pennies are tossed. What is the probability that they fall either all heads or 
all tails ? 
We assume that the probability of a head with any penny is I and that the result 
with one penny is independent of that with the others. Then there are eight possible 
and equiprobable cases, HHH, HHT, HTH, HTT, THH, THT, TTH, TTT. Two of 
these give us all heads or all tails and hence the required probability is \. 
Now consider this argument: there are two possibilities, either the three coins all fall 
alike or two of them are alike and the other different. Of "these two possibilities one is 
of the type required and therefore the probability is \. 
Consider also this argument: there are four possibilities, three heads, two heads and 
a tail, two tails and a head, three tails. Two of these four are of the type required and 
therefore the probability is |. 
Finally, consider this argument: of the three coins two must fall alike. The other 
must either be the same as these two or different. Thus there are two possibilities and 
again the chance is |. 
These three arguments are fallacious. They assume equiprobability among events 
which are not equiprobable and the application of Rule 2 is not legitimate. For example, 
in. the first case, it is true that there are two possibilities, but they are not equal under 
our assumptions. The reader may care to examine why this is so and how the other two 
arguments break down on the same point. 
Example 7.4 
Peter and Paul play a game with two dice. Peter plays first by throwing the dice 
together. If the total number of points is a prime number other than 2 he wins outright; 
if it is even he throws again under the same conditions ; in other cases the throw passes 
to Paul, who throws under the same conditions. What is the probability of Peter's winning ? 
It is to be assumed that the probabilities of thro whig any number 1 to 6 with either 
die are equal. The possible throws are 2, 3, 4 ... 12 and the number of ways in which 
they can occur are :— 
Total points . . .2 34567 89 10 11 12 Total 
No. of ways ...12345054 3 2 130 
Thus, according to Rule 2, the probability (1) of throwing a prime is y>-, (2) of throwing 
an even number is J-*-, (3) of throwing neither is fa. 
These three events are mutually exclusive. Let P be the probability of Peter's 
winning. Now if Peter throws a prime other than 2 he wins outright, and the probability 
of his doing so is thus ^i ; if he throws an even number he throws again, and his proba- 
18P 
hility of winning in this case (according to Rule 4) is -^- ; if he throws neither the throw 
DO 
passes to Paul, whose chance is then P, so that Peter's chance of winning is ^-(1 — P). 
170 PROBABILITY AND LIKELIHOOD 
Thus, according to Rule 3, we have 
14 18P 4 
^~36+ 36 +36U r)t 
giving 
1 22" 
7.12. It is possible to carry mathematical problems on the foregoing lines to great 
lengths, and a considerable amount of ingenuity has been expended in doing so. The 
important thing to note from the point of view of the theory of probability is that in all 
such cases certain probabilities are stated a priori, either explicitly or implicitly in some 
such form as " the dice are perfect " or " the selection is made at random ". One of the 
most formidable problems of statistics is that only in exceptional cases is there any prior 
certainty about the probabilities of observed events. 
Probability in a Continuum 
7.13. Up to this point we have considered only probabilities of finite and discrete 
ovontB ; but we may also ask whether any meaning can be attached to probabilities in 
a continuum. For example, if a square is inscribed in a circle, what is the probability that 
a point taken at random in the circle is also inside the Square ? If a line is divided into 
thrco segments, what is the probability that they can form a triangle ? What is the proba^ 
bility that a*'< x0 where x is a positive real number less than yQ ? And so on. 
All probabilities of this kind must be considered as limits. Consider the first example, 
that of the square inscribed in the circle. Imagine the whole figure divided into small 
cells of area e by a rectangular mesh. If we assume that the occurrence of a point in a cell 
is equally probable for all cells, the probability that a point falls inside both circle and 
square is the ratio of the number of cells in the latter to those in the former, neglecting the 
colls at the edges which become of diminishing importance as e —»- 0. In fact, the required 
probability can be made as near the ratio of the area of the square to that of the circle as 
wo please by taking e small enough. We may say that the probability is that ratio, which 
2 
is easily soon to be -, an incommensurable number. ■ 
n 
Wo should get the same limiting form of probability if we took other meshes which 
adequately represented areas ; but it is most important to specify the method of procession 
to the limit in speaking of probabilities in a continuum. Otherwise the1 result has no 
moaning. The following example will illustrate the point. 
Example 7.5 
Consider a straight line OA bisected at B. What is the probability that a point chosen 
at random on the line falls into the segment OB 1 
Let us suppose in the first place that the line is divided into n equal segments of length 
---. If we interpret the choosing of a point at random to mean the choice of one of these 
intervals, the probability is obviously \ as n —»- oo, for there will be half the intervals 
in the segment OB. 
Now let OP be drawn perpendicular to OA and equal to it in length, and imagine a star 
of % + 1 lines drawn tlirough P, including OP and PA, so as to divide the angle OP A 
H 
THE VON MISES APPROACH 171 
into equal angles —. These lines cut off- segments on OA, and we may, if we regard 
equal angles as having equal probability, assign to these segments an equal probability, 
for they subtend equal angles at P. If we make this convention it is evident that as 
n —>■ oo the probability of a point falling into any segment on OA is proportional to the 
angle subtended at P. For example, the probability that a point falls in the segment OB' 
is tan-1!/-. 
Now this is not the same answer that we got by assuming all small segments of OA 
equally probable. There is nothing paradoxical in this—the two answers are different 
because the two limiting processes were different. On a little reflection it will be clear 
that by moving the point P on the perpendicular to OA and taking a star of lines as before 
we can make the probability of obtaining a point in OB have any value we like. It is 
thus abundantly clear that the concept of probability in a continuum depends on the 
limiting process by which that continuum is reached from' a finite subdivision of equiprobable 
intervals, 
7.14. We have spoken above of the sslection of objects " at random ". In the 
mathematical theory of probability it is customary to define randomness in terms of 
probability itself. A member of a population is said to be chosen at random if it is chosen by 
a random method ;" and a random method is one which makes it equally probable that / 
each member of the population will be chosen. Randomness is extremely important in 
the theory of sampling and we shall consider it at some length in the next chapter. At 
this point it is sufficient to note that when we speak of random choice we really mean 
a method of selection which gives to certain propositions an equal probability and hence 
allows us to apply the calculus of probability a priori. The justification for this is, in the 
ultimate analysis, empirical. It is found in practice that there exist selective processes 
which educe members of a population in such a way that the constituent events may be 
regarded as equiprobable ; and the theory of sampling is largely concerned with samples 
generated by such processes. 
It may be noted that, for continuous probabilities,\randomness is dependent on the 
process to the limit just as probability itself is. 
The Approach of von, Mises 
7.15. Suppose now we have a population of objects, each of which bears one of 
a nmnber of characteristics. To simplify the exposition we will suppose that there are 
two characteristics denoted by 0 and 1. Suppose we draw members from this population 
and replace each member after drawing. Then the process of continued selection will 
generate a series such, for example, as 
K = 01100100111010111100100 (7.9) 
Von Mises (1936) takes as the foundation of his theory of probability an infinite sequence 
of this kind, the Irregular Kollektiv, obeying the following laws :— 
(a) The proportions of 0's in the first n terms tends to a limit as n —>■ oo. This limit 
is called the probability of the zero in the Kollektiv. 
(b) If a subsequence is picked out of the Kollektiv by some method which is 
independent of the Kollektiv itself (e.g. every third member, every member whose ordinal is 
172 
PROBABILITY AND LIKELIHOOD 
a square, every member following a zero, etc.), the limit of zeros also tends to p for n —>■ oo ; 
and this for every such subsequence. 
The Irregular Kollektiv might, in fact, be described as the infinite random series. It 
has no systematic qualities ; for if, for example, the series consisted of repetitions of 
0110, thus 
K = 011001100110 (7.10) 
the subsequence consisting of every (4r + 3)th would consist entirely of unities and the 
condition (b) would be violated. 
7.16. It is not difficult to show that probability defined in this way obeys the four rules 
enunciated earlier in this chapter. Some authorities have, -however, found difficulty in 
accepting the basic concept of the Irregular Kollektiv and attributing any meaning to its 
existence. It has even been claimed that the idea is self-contradictory, though this von Mises 
strongly contests. 
However this may be, the von Mises approach represents, in my own opinion, the nearest 
to a satisfactory basis of the frequency theory of probability that has been g ven. The 
mathematics of the subject are much the same in any of the frequency theories once the 
fundamental rules have been established, but when it comes to relating theory to experience 
the von Mises method has decided advantages. For a discussion of this subject, reference 
may be made to the works listed at the end of this chapter ; in particular I have given (1941) 
the outline of a theory which in my view eliminates the difficulties associated with the 
Irregular Kollektiv. 
Probability and Statistical Distribution 
7.17. We now proceed to consider the relationship between the theory of probability 
and that of statistical distributions. Suppose we have a statistical population, finite and 
discontinuous, distributed according to a variate x. If we take a member at random from 
this population the probability that it bears an assigned variate-value x0 is the frequency 
function f(x0), for this is the proportion of members bearing that value. Further, the 
probability that it bears a value less than or equal to x0 is the distribution function F{xli)> 
as follows at once from Rule 3 and the definition of the distribution function. 
This is the essential link between probabilities and distributions. The distribution 
function gives the probability that a membet of the population chosen at random will bear 
a specified value of the variate or less. We must, however, consider whether this statement 
can still be regarded as true for populations which are infinite or continuous. 
Suppose in the first instance that the population is infinite and discontinuous. In such 
a case we cannot select a member at random, but we may, in the manner of 7.13, imagine 
a selection from a finite population which tends to the infinite form under consideration. In 
this finite population the proportion of members with values less than or equal to some x0 
will be F(x0) and thus, with due regard to the nature of the limiting process, we may still say 
that in the infinite population the probability of a value less" than or equal to x0 is F(x0). 
Similarly for a continuous distribution. In Chapter 1 we considered the continuous 
form as a limiting expression of 
AF = f(x) Ax. 
If a member is chosen at random from this population in such a way that equal ranges Ax 
are equally probable, the probability that it falls in the range Ax is f(x) Ax. In the limit we 
may say .that the probability of obtaining a value less than or equal to x0 in taking a member 
SAMPLING DISTRIBUTIONS 173 
at random from a continuous population is I dF = F(x0). It must, however, be remem- 
J —00 
bered that the nature of the process to the limit should be specified. 
Hereafter, in speaking of selecting a member at random from a population dF = f(x) dx 
we shall assume that what is meant is a selection random in the limit for intervals dx, i.e. 
such that intervals dx are equally probable. 
The Concept of Random Variable 
7.18. The idea of a variable x which can appear with varying'degrees of probability 
dF = f(x) dx has been elevated by mathematicians into a distinct concept, that of a random 
variable. In ordinary analysis no such idea appears. We write " a variable x " meaning 
that we are considering propositions about numbers which may be any of a certain range ; 
there is no thought that one of these values is to be considered more frequently than others 
or that it will occur more frequently in practice. The random variable, on the other hand, 
is to be regarded as defined by a distribution function. It may take any values in a given 
range, but the values are distinguished by an associated function. 
7.19. Let us consider what is meant by the addition of random variables. In ordinary 
analysis, given two variables x and y, we may define a third variable 
z = x + y, 
which merely means that when x = x0 and y — y0, z will be x0 -f- y0. If x and y are random 
variables, can we attach any useful meaning to z ? 
If the joint distribution function of a; and y is Fi2, we have that the frequency of # < x0 
and y <y0i& F19 (x0, y0). Consider some value z0. We may then determine from Fie th£ 
frequency such that x + V < 20 which will, in fact, be the integral 
J JeWi, (*, y) 
taken over the region for which x + y < z0. 
This integral defines a function of z0 which is in fact a distribution function, for it is zero 
at — oo, non-decreasing, and unity at -f- oo. We may then define this as the distribution 
function of the random variable z and say that z is the sum of the random variables x and y. 
7.20. More generally, suppose we have n random variables distributed in the 
multivariate form dF{xx . . . xn). We may then define a random variable z by a functional 
equation 
z = z(xj_ . . . xn). ..... (7.11) 
The distribution function of z0 is the integral of dF(xL . . . xn) over all values of xt . . . xn 
such that Zo > z( ). We may regard the equation (7.11) as defining a new random 
variable z with this as its distribution function. 
Sampling Distributions 
7.21. We have noted that if a member of a population is chosen at random, the 
probability that it will bear a variate-value not greater than x is the distribution function 
F(x). Similarly, if we choose a member from a multivariate population, the probability 
that it will bear a value of the first variate not greater than xu of the second not greater 
than xi} . . . of the rath not greater than xw is the multivariate distribution function 
174 PROBABILITY AND LIKELIHOOD 
F(xx, x2, . . . xn). Further, if the variates are independent, as defined in 1.33, the rth 
variate being distributed as dFr(xr), this probability is equal to 
JFifri) F*fa) ■ • • Fn{xn). 
Now suppose that we have a selective process, which we will call sampling, applied to 
a univariate population in such a way that it abstracts a group of n members. If this process 
is repeated it will generate a multivariate distribution, each sample exhibiting n values 
xx . . . xn. The nature of this multivariate distribution depends on the sampling process 
as well as the population. If the distribution is G(xx . . . xn), then this function represents 
the probability that a random sample will result in n values, the first not greater than xlt 
the second not greater than x2, and so on. 
There is one type of sampling process of outstanding importance in statistical theory, 
namely that in which the distribution G(xx . . . xn) is the product of factors Gx(xx)t 
G2(x&) . . . On(xn). In such a case the sampling is said to be simple. The distributions of 
the values independent one of another, and we may thus say that the selection 
of any member is independent of that of any other. Moreover, if the sampling is random, 
every 0r(x) will be equal to F(x), the distribution function of the population. Thus in this 
case we have, for the distribution of the variate-values in samples of n obtained by a simple 
random method, . 
dF{xx . . . xn) = dF(xx) dF(xt) . . . dF(xn) 
= fMfM • • • f{xn)dx1dxi . . . dxn, . . (7.12) 
and F(x1) F(xt) . . . F(xn)ia the probability that in such a sample1 the first value will not 
exceed xx, and so on. Moreover, since the x'a appear symmetrically in (7.12) their order is 
not material. The equation gives the probability that cme member of the sample will not 
exceed x1} another x*; and so on. 
7.22. Suppose now we have a sample of n members of the population with variate- 
values xx . . . xn. We may construct from these values some function, say 
z = z{xx . . . xn), . (7.13) 
which might, for example, be the mean or the variance. We may then ask : on certain 
hypotheses as to the way in which the sample was derived, what is the probability that z is 
not greater than some assigned value z0 ? In terms of frequency, if all possible samples 
xx . . . xn were drawn and z computed for each of them, what proportion would fail to 
exceed some value z0 ? 
As an illustration, suppose we draw a sample of two from the normal population 
1 _£!. 
dF = —j-—r e 2a* dx. 
Let the sampling be simple and random. Then in virtue of (7.12) the probability of values in 
the ranges centred at xx and x2 is 
dP = -L- exp J - ^{x\ + x\)\dxx dx,. . ■ . . (7.14) 
Consider now the quantity 
= xx + x3 
2 
What is the probability that z shall be not greater than some assigned z0 ? It is seen to be 
the integral of dP in equation (7.14) over the region,such that \(xx -f- &2) <z0, i.e. 
I 
BAYES' THEOREM 
175 
-P(* < Zo) ""absf ".C exp {" S5iw + ^Y*1 dx» 
Write 
z = i(zi + #a) 
y = J(*i — aj,). 
The integral becomes 
Thus 
^fJLexp {- &+yt) }dzdy 
= —t I ' exp ( — ~)dz. . . . . . (7.15) 
1 _*S 
P(z„ — £dz0 <s <20 + i&o) = —7— e «' dz0, . . (7.16) 
G-yTl 
a result which, remembering the relation between probability and the distribution function", 
we may express by saying that z is distributed normally with variance \a2. The distribution 
function of the statistic z is given by (7.15) and its frequency function by (7.16). 
7.23. In the more general case of a statistic z = z(xx . . . xn) we see that the 
probability of z <z0 is obtained by integrating the joint distribution of xx . . . xn over the 
domain of x'b such that z0 > z{xx . . . xn). This gives us the distribution function of the 
random variable z defined in terms of the random variables by the equation 
z = z[xx . . . xn). We shall develop this subject systematically in Chapter 10. 
When the a-values are chosen by a simple random process the distribution of z is called 
a simple random sampling distribution, or more shortly a sampling distribution. Unless 
otherwise specified the words " sampling distribution " are always to be taken to refer to 
sampling under simple random conditions. 
Bayes* Theorem 
7.24. We now revert to the theory of probability. Suppose that qx . . . qn are 
alternative propositions and let H be the information available, p some additional 
information. Then by Rule 4 
P(qrP | H) = P(p | H) P(qr | pH) 
= P(qr\H).P(p\qrH) 
whence 
P(qr | pH) __ P(p J grH) 
.P(qr\H) P(p\H)' 
Thus 
PfelFgl-^lffijgl^ . . . .(7.17). 
Since the truth of one of the ^'s is certain we have, summing for all q'a, 
. _ VP(qr 1 JBQ P(p 1 qrH) 
2j WW) ' ' ' " '( ' 
176 PROBABILITY AND LIKELIHOOD 
whence, from (7.17), 
Pta I vH) - P(<tr\H).P(p\qrH) 
P(?' I pH> - EP(lr I H) P(P | irH) (7-19) 
or, for variations in qr, 
P(qr\pH)<x:P(qr\H)P(p\qrH).- .... (7.20) 
This is Bayes' Theorem. It states that the probability of qr on data p and H is proportional 
to the product of that of qr on H and p on qr and H. 
The principal application of the theorem lies in reasoning from observed events to the 
hypothesis which may explain them. The theory of this subject is accordingly known as that 
of "inverse " probability. Suppose/in fact, that an event can be explained on the mutually 
exclusive hypotheses qt . . . qn and let H be the data known before the event happens, so 
that H is the basis on which we first judge the relative probabilities of the q'a. Now suppose 
the event to happen. Then Bayes,' theorem states that the probability of qr after it Ms 
happened (i.e. on data H and p) varies as the probability before it Jiappened multiplied by the 
probability that it happens on data qr and H. The probability P(qr | pH) is-therefore called 
the posterior probability, P(qr | H) the prior probability, and P(p \ qrH) will be called the 
likelihood. 
In this book the word " likelihood " will be used solely in this special sense. 
7.25. The practical use of Bayes' theorem depends on a knowledge of the prior 
probabilities. When they are known we can calculate and compare the posterior 
probabilities of the hypotheses, and if we have to choose one in preference to others we choose tfya 
one with the greatest posterior probability. But we are rarely, if ever, given the prior 
probabilities. And this brings us to what is perhaps the most contentious point in the 
modern theory of probability. 
Bayes stated (though he appears to have felt more hesitation than most of his followers) 
that if there was no known reason for supposing that the prior probabilities were different, 
they were to be assumed equal. This is Bayes' postulate, which is to.be distinguished from 
the theorem of (7.19). It immediately resolves the difficulty of applying the theorem, and 
before discussing the postulate and describing other approaches to the matter, it may be 
useful to give two examples of the use of the postulate in practical problems. 
Example 7.6 
An urn contains four balls, which are known to be either (a) all white, or (/;) two white 
and two black. A ball is drawn at random and found to be white. What is the 
probability that all the balls are white ? 
We have here two hypotheses, qx and q9. On qt the probability of getting a white ball 
is 1, on q2 it is £. Prom (7.19) we have 
P(Ql | pH) = 
P(qx | H) + \P{g% | H) 
I ^qilPU) P(gi\H) + iP{qu\Hr 
Now, in accordance with Bayes' postulate we assume 
P(?11 H) = P(qt | H) = \ 
and find 
-P(<Zi kp#) = t 
Pitt* I P&) = h 
BAYES' THEOREM 
177 
We are thus led to prefer the hypothesis qx that all the balls are white, since this has the 
greater posterior probability. 
Example 7.7 
From an urn full of balls of unknown colour a ball is drawn at random and replaced. 
The process is continued m times and a black ball is drawn each time. What is the 
probability that if a further ball is drawn it will be black ? 
The question as framed does not admit of a definite answer, for, there being an infinite 
number of possible colours and combinations of colours, we do not know what are the 
hypotheses which are to be compared. Let us suppose that the balls are either black or 
white, and thus consider the hypotheses (1) that all are black, (2) that all but one are black, 
(3) that all but two are black, and so on. The problem still lacks precision, for the number 
of balls is not specified. Suppose there are N balls. We shall later let N tend to infinity to 
get the limiting case. 
Consider the hypothesis that there are B black balls and N—B white ones. The prob- 
ability of choosing a black ball is -^ and that of doing so m times in succession, in virtue of 
Rule 4, is (jr=) . If the q'a have equal prior probabilities we have, from (7.19), 
P(9r I PH) = -vV 
r 
Now the probability of getting a further black ball on hypothesis qr is -^=. Since the 
hypotheses q are mutually exclusive, the probability of getting a further black ball is, in 
virtue of Rules 3 and 4, 
£jPfcl#*>- ^. 
E\n' 
This is the answer to the limited form of the question. As N —>■ oo this tends to the quotient 
of definite integrals 
•l 
xm+1 dx 
o m + 1 
f 
Jv, 
I 
1 m + 2' 
xm dx 
o 
This is a particular case of the so-called Succession Rule of Laplace. Enthusiasts have 
applied it indiscriminately in some such unconditioned form as the statement that if an event 
is observed to happen m times in succession the chances are m + 1 to 1 that it will happen 
again. This is clearly unjustified. 
7.26. The principal "difficulties arising out of Bayes' postulate appear from the 
standpoint of the frequency theory of probability. If we adopt the axiomatic approach, in which 
A.S. N 
178 PROBABILITY AND LIKELIHOOD 
probability is a measure of attitudes of mind, it is reasonable to take prior probabilities to be 
equal when nothing is known to the contrary, for the mind holds them in equal doubt. The 
frequency theory, however, would require the states of events corresponding to the various 
q'a to be distributed with equal frequency in some population from which the actual q has 
emanated, if Bayes' postulate is to be applied. This has appeared to some statisticians, 
though not to all, to be asking too much of the universe. The postulate is one of the crucial 
points in the theory of probability. Adherents of the axiomatic school accept it. Many of 
those of the frequency school explicitly reject it. 
There is still so much disagreement on this subject that one cannot put forward any set 
of viewpoints as orthodox. One thing, however, is clear—anyone who rejects Bayes' 
postulate must put something in its place. The problem which Bayes attempted to solve is 
supremely important in scientific inference and it scarcely seems possible to have any 
scientific thought at all without some solution, however intuitive and however empirical, to 
the problem. We are constantly compelled to assess the degree of credence to be accorded 
to hypotheses on given data; the struggle for existence, in Thiele's phrase, compels us to 
consult the oracles. 
* 
The Principle of Maximum Likelihood 
7.27. The school of statisticians which rejects Bayes' postulate has substituted for it 
an apparently different principle based on the use of likelihood. Reverting to equation (7.19) 
we see that for any qr and H 
P(qr\pH)KP(qr\H)L(p\qrH), .... (7.21) 
where we now write L(p \ QjJH) for the likelihood function. The Principle of Maximum 
Likelihood states that when confronted with a choice of hypotheses q we are to select that one 
(if it exists) which maximises L(p \ qjJH). In other words, we are to choose the hypothesis 
which gives the greatest probability to the observed event. 
It is to be particularly noted that this is not the same thing as choosing the hypothesis 
with the greatest probability. In fact, some adherents of the frequency theory of probability 
deny any meaning to the expression " probability of. a hypothesis ", and the principle of 
maximum likelihood was introduced largely to replace the notion of " inverse " probability 
which leads to the use of such a phrase. 
7.28. Suppose (as is nearly always the case in statistical work) that the hypotheses 
with which we are concerned assert something about the numerical value of a parameter 0. 
In such a case we shall speak of a statistical hypothesis. For instance, the hypotheses might 
be qx = 0 < 0, q2 = 0 > 0, in which case there are two alternatives. Or we might have 
g^ = 0 = 1, #2 = 0 = 2, and so on, in which case there is a denumerable infinity of 
hypotheses. 
If now 0 can have only discontinuous values, we may, confronted with an observed 
event p, require to estimate 0, or to ask what is the " best " value of 0 to take on the evidence 
p. The method of Bayes would state that the " best " value was the most probable value. 
In (7.21) we should seek for that qr which made P(qr | pH) a maximum. If we know nothing 
of the prior probabilities P(qr \ H) we should, in accordance with Bayes' postulate, assume 
all such probabilities equal. We then merely have to find that qr which maximises L(p | #,-#). 
In other words, the postulate of Bayes and the principle of maximum likelihood result in the 
same answer and are equivalent. 
\ 
\ 
THE PRINCIPLE OF MAXIMUM LIKELIHOOD 179 
7.29. This position apparently does not hold if the permissible values of 0 are 
continuous. We must now replace such expressions as P(qr \ H) by P(0O —%dd0 < 0 < 0O -\- %dd01 H) 
and in place of (7.21) we get 
P(00 - ldd0 < 0 < 0O -f \dd01 pH) oc P(0O - %ddQ < 0 < 0O + lddQ \H) 
X L(p | 0O - ^0O < 0 < 0O + ldd0, S). ..' . (7.22) 
If we now require the " best " value of 8, we should, in accordance with Bayes' postulate, 
take the prior probability to be a constant and once again we should have to maximise L for 
variations of 0. 
We might, however, have chosen to represent our hypotheses, not by 0, but by some 
variate 0 functionally related to 0, e.g. the standard deviation instead of the variance. In 
this case we should have reached equation (7.22) with <j> written everywhere instead of 6; 
we should have taken the prior probability as constant; and we should have arrived at, 
the conclusion that we should maximise L for variations of <j). 
But,are we being consistent in so doing ? If we assume that the elementary intervale 
of 0 are equiprobable we cannot assume the same of $, and thus the use of Bayes' postulate 
appears to involve self-contradiction. The principle of maximum likelihood is free from 
this difficulty, for if L(6) is to be maximised for variations of 0 it will, at the same time, be 
maximised for variations of <f>, since 
dL = dLH 
dd d<f> dd 
and the two sides of this equation vanish together. 
7.30. This is one of the grounds on which adherents of the frequency school have 
rejected Bayes' postulate in favour of the principle of maximum likelihood ; but in my view 
the matter has been misunderstood. It would seem that Bayes' postulate and the principle 
give the same answer in the continuous case as well as in the discontinuous case when proper 
regard is had to the limiting processes involved. We saw in 7.13 that in speaking of 
probability in a continuum it was essential tp specify the nature of the process to the limit. 
If wc regard 0 (from the frequency viewpoint) as having emanated from a population by 
a process random in the limit for intervals dO,' then Bayes' postulate applied to this process 
will clearly give a different answer from that obtained, by supposing that 0 emanated by 
a process random in the limit for dcf> \ — ~^\d0\. The two are different just as the 
probabilities in Example 7.5 are different, and for the same reason. Thus the apparent 
inconsistency is not an inconsistency at all, but a difficulty introduced by ignoring the limiting 
process in continuous populations.* 
For an extended discussion of this subject reference may be made to Kendall (1940). 
In the present volume it need not concern us to take it farther, though considerable use will 
be made of the principle of maximum likelihood in Volume 2. It will there be seen that the 
principle has many important statistical properties. No one, in fact, denies the importance 
* A further difficulty arises if 9 can lie in an infinite range, for then Bayon1 postulato apparently 
leads to the conclusion that prior probabilities in any finito range are zero and hence so are posterior 
probabilities. This does not arise in the likelihood method. Jeffreys overcomes it by assuming that the 
prior probability in such a caso is inversely proportional to the paramotor 0. Looking at the problem 
generally, wo neod not bo surprisod that the difficulty appears sinco tho ranging of 0 over an infinite 
range is also a limiting process. In practice we are never so ignorant a prion as to suppose that 0 can 
be any value however large with the same probability, and if wo conuidor the range as determinate 
but unknown, likelihood and Bayes' postulate continue to be applicable and to give the same results. 
180 PROBABILITY AND LIKELIHOOD 
of the principle or its usefulness in certain cases ; the controversy hitherto has centred on 
the considerations by which the acceptance of the principle as a rule of conduct is to bo 
justified. The reader who cannot accept Bayes' postulate and the foregoing arguiiiont that it is 
virtually identical with the principle has a choice of courses. He can accept the prineiple 
as a new and distinct postulate of scientific inference ; he can regard it as justified by its 
mathematical and statistical properties ; or he can rely on a more sophisticated approach 
which will be touched on in Chapter 9, namely, that the principle leads to estimates of 
parameters with minimum sampling variance when such exist. At this stage he may bo 
prepared to accept it on intuitive grounds.* 
7.31 Although in the remainder of the present volume Bayes' postulate and the 
principle of maximum likelihood will not often appear explicitly, we shall frequently use 
a type of argument which is, in the ultimate analysis, based on them. A certain event or 
series of events is observed ; on a hypothesis H the occurrence of these events is found to be 
highly improbable ; and therefore H is rejected in favour of some hypothesis whieh makes 
the observations more probable. To take a very simple example, we toss a ponny twenty 
times and find that it comes down heads every time. If the penny wove unbiased 
(hypothesis E) the odds against this event would be 220 — 1 to 1. Thus wo reject II in 
favour of the hypothesis that it is in fact biased in favour of the heads. 
It will readily be seen that this type of argument is a somewhat indefinite form of the 
inverse type with which we have been concerned. The chief difference lies in the fuet that 
it is used to reject unlikely hypotheses rather than to accept the most likely, powsibly a. safer 
but certainly a less precise procedure. 
The Central Limit Theorem 
7.32. To conclude this chapter we prove an important theorem which givo.s the normal 
distribution a central place in the theory of probability and the theory of sampling. It lias 
already been shown that the distribution appears as the limiting form of the binomial and 
the Pearson Type III distribution when expressed in standard measure. Wo shall prove 
a much more general result, due to Laplace but first proved rigorously by LiapounofV, t hut 
under certain conditions the sum ofw independent random variables distributed in whatever 
form tends, when expressed in standard measure, to the normal form as n tends to intinil v. 
This is the famous Central Limit Theorem. 
Let us note in the first place a simple but powerful result connected with the* charaeter- 
istic functions of sums of independent random variables. If we have n suoh variables 
distributed as dF1 . . . dFn the element of frequency of their sum z = xt + . . .»«„ is t he 
integral of dFt . . . dFn through the element of volume between z and z -|- dz. Thus the 
characteristic function of their sum, being the integral of eits through the range of z, is equal to 
d.F 
/•no -oo 
. . . e«* dFx . 
J —00 J —00 
==\_JiiXldFi\ eitx'dF3 . . . p e^dF, 
— <f>l ^2 ■ • • <f>n- 
* An approach of a rather different kind has been developed in reccmt yeara by Noyinmi (1937)' 
n^eTcoTd voTuIe0' ^^ °^ °* *"* *"»■***>». ** -ount of this thLy ^ b(! ^ 
THE CENTRAL LIMIT THEOREM 
181 
That is to say, the characteristic function of the sum of a number of independent random 
variables is the product of their characteristic functions. The cumulative function is 
accordingly the sum of their cumulative functions. 
Now as to the Central Limit Theorem itself. We first of all outline the proof briefly 
and unrigorously to indicate its essential features, and then give a rigorous proof. Suppose 
we have distributions Fx . . . Fn, all with finite second moments and with characteristic 
functions ^x . . . <f>n. We have for any Fr 
Mt) = 1 + ^ + ^ + Rr 
when R is a remainder term. Similarly we have 
fr[t) = fa— + /"2-jjj- + -"r- 
Hence the cumulative function of the sum of the independent variates will be 
We can without loss of generality take the mean of the sum as origin, so that 2/^ = 0, and 
now transforming to standard measure by the transformation f = —r=— we find 
V(t) = =£ + Of ER N 
lb 
Since E^it is of order n the remainder term will be of order ^y, i.e. of order w~* ; and thus 
lim W(t) = 
2! V(^>)7 
n 
tends to zero. We shall then have 
~2 
lim 0(t) = e a 
and hence in virtue of the converse of the First Limit Theorem (4.12) the distribution of 
the sum of the random variables tends to normality. 
7.33. The rigorous enunciation of the theorem and its proof are as follows:— 
If n independent random variables are distributed in the forms Ft . . . Fn with finite 
n 
variances //81 . . . /*a,n and Mn =./1 ju,zj, then the sum of the variables divided by -\/Mn 
tends to the normal form, provided that for any e > 0 
lim — y\ z^ = 0 (7.23) 
The implications of this condition, which is a modification by Cramer of one due to 
Lindeberg, are not very obvious, but it involves that 
VMn-+ oo and ^->0, . . . .(7.24) 
in other words, that the total variance tends to infinity but that the proportional-contribution 
of each constituent tends to zero. To see that (7.24) follows from (7.23) we note that if Mn 
does not tend to infinity it must, being an increasing function, tend to a constant. It would 
182 PROBABILITY AND LIKELIHOOD 
» 
follow from (7.23) that the sum of the integrals, each of which is positive and not small for 
every e, would tend to zero, which is impossible. Further, if ¥j^ did not tend to zero, then 
at least one of the terms in (7.23) would not do so, and thus the sum would not do so. 
We have 
(t \ C°° ifjr 
J\x\>eVMn J\x\<eVMn 
Expanding the exponential with a Maclaurin remainder we have 
+ f (1 + ^+^+e»W o<|.|.|*|<i. 
jM<tVMn\ VMn 2Mn bMn* J 
We may without loss of generality suppose the mean to be zero and hence we find 
™\y/MJ 2 Mn ) 2Mn J ,,| >*,,*/„ ' 
+ r%Tr[ t*\z\*dF, O<|H|0'|<1. 
Thus for some T > 1 we have, for \t\ <T, remembering that 
f \x*\dF <eVMn [ x*dF 
J|J"|<eVM„ ' ' J\s\<eVMn 
*(vk) -i - r'Sit+e"£K+L„.xHF) ° < > °" > <i- 
Hence, in virtue of (7.24) the coefficient of 6" is as small as we please and thus <j>J -jmi 
tends to unity as n —>■ oo uniformly for \t\ <T. Thus we have 
<vk) -(1 + Mvm) ->} 
for sufficiently large n and | <r\ | < e. Thus for e < \, 
•(vk)^'^+"(—L„..-'> 
Summing for j we have, in virtue of (7.23), 
y( * \ = ~Jl + 20"T3(e + vanishing quantity) 
and thus for I 11 < T 
ito v(*) = = », 
WMJ 2 
the convergence being uniform in any finite winterval. The theorem follows from the 
converse of the First Limit Theorem. 
NOTES AND REFERENCES 183 
7.34. The following comments will amplify the. above proof. 
(a) The Lindeberg condition (7.23) is necessary as well as sufficient. A proof is given by 
Cramer (1937). 
(6) The condition may be put in other forms, for which see Cramer (1937), Uspensky 
(1937) and the original memoir by Liapounoff (1901). 
(c) The sum of random variables whose distributions have not a finite second moment 
may not tend to normality. It will be seen in Chapter 9 that the mean of n variables each of 
which is distributed in the form 
dF = -—-—- — oo < x < oo 
1 + & 
is also distributed in that form, however large n may be. 
(d) Liapounoff has also given some remarkable results showing how close the limiting 
form is to the sum of n variables. In fact, if Fn is the distribution function of the sum, 
and F that of the normal form 
|7„_,| <„„!?£» • ■ 
where c is a constant, p3n is a function of the third moments of the constituent distributions. 
NOTES AND REFERENCES 
The logio of the theory of probability will be found dealt with in the books by Keynes 
(1921), F. P. Ramsey (1931) and Johnson (1921). All these take the axiomatic approach 
from probability as an undefined idea. The frequency approach has been discussed from the 
more logical angle by Venn (1888), whose book, though out of print and to some extent out of 
date, is still worth reading. 
The mathematical theory of probability has been treated by LeVy (1925), Jeffreys 
(1939) and Uspensky (1937), all three books excellent of their kind. Von Mises' approach is 
described in his book (1936) and an axiomatisation in a paper by Dorge (1934). See also 
Kendall (1941). 
For inverse probability and likelihood see the review by Kendall (1940). There are 
scores of papers, mostly controversial in character, on this subject, but a beginning of 
a systematic reading may be made with the papers by Fisher (1921, 1930), Neyman (1937), 
and the book by Jeffreys (1939). 
For the central limit theorem see Cramer (1937), and for an extension to the case when 
the variables are dependent, Bernstein (1927). 
Bayes, T. (1763), " An essay towards solving a problem in tl;e doctrine of chances," Phil. 
Trans., 53, 370. 
Bernstein, S. (1927), " Sur I'extension du th^oreme limite du calcul d,es probabilites aux 
sommes de quantit6s d6pendantes," Math. Ann., 97, 1. 
Borel,E. (editor) (1925 and later years), Traite du Calcul des Probabilites et de ses Applications, 
Gauthier-Villars, Paris. 
Oam6r, H. (1937), Random Variables and Probability Distributions, Cambridge University 
Press. 
Dorge, K. (1934), " Eine-Axiomatisierung der von Misesschen Wahrsoheinlichkeitstheorie," 
Jber. dtsch. Mat. Ver.,-43, 39. 
184 PROBABILITY AND LIKELIHOOD 
Fisher, R. A. (1921), "On the mathematical foundations of theoretical statistics," Phil. 
Trans., A, 222, 309. 
(1930), " Inverse Probability," Proc. Camb. Phil Soc.; 26, 528. 
Jeffreys, H. (1939), The Theory of Probability, Cambridge University Press. 
Johnson, W. E. (1921-24), Lqgic (3 volumes), Cambridge University Press. 
Kendall, M. G. (1940), " On the method of maximum likelihood," J. Roy. Statist. Soc, 103, 
388. 
(1941), "A theory of randomness," Biometrika, 32, 1. 
Keynes, J. M. (1921), A Treatise on Probability, Macmillan. 
Kolmogoroff, A. (1933), Qrundbegriffe der Wahrscheinlichkeitsrechnung, Berlin. 
LeVy, P. (1925), .Calcul des ProbabiliUs, Gauthier-Villars, Paris. 
Liapounoff, A. (1901), " Nouvelle forme du th^oreme sur la limite de probability," Mem. 
Acad. Sci. St. Pitersbourg, 12, No. 6. 
von Mises, R. (1936), Wahrscheinlichkeit, Statistik und Wahrheit, Springer, Berlin. (English 
translation, 1939, as Probability, Statistics and Truth, W. Hodge.) ■ 
Neyman, J. (1937), " Outline of a theory of statistical estimation based on the classical theory 
of probability," Phil. Trans., A, 236, 333. 
Ramsey, F. P. (1931), The Foundations of Mathematics, Kegan Paul. 
Uspensky, J. V. (1937), Introduction to Mathematical Probability, McGraw-Hill, New York 
and London. 
Venn, J. A. (1888), The Logic of Chance, Macmillan. 
EXERCISES 
7.1. If each of an aggregate of N objects can possess or not possess any of n 
characteristics A, B, . . . K ; and if (ab . . . f) is the number of objects possessing A, B . . . F, 
show that the number of objects possessing at least one of A, B, . . . K is 
E{a) - E{ab) + E{abc) . . . + (- \)n-\E{ab . . . h). 
In each of a packet of cigarettes there is one of a set of cards numbered from 1 to n. 
If a number N of packets is bought at random, the population of packets is large and the 
numbers are equally frequent, show that the probability of getting a comolete set of cards is 
. i-e)p^)"-©f-f-T+-+(-^(.-i)®* 
7.2. Three points are taken at random on a circle. Show that the probability that 
they he on the same semi-circle is f. (Assume that in the limit elementary intervals of aro 
are equiprobable.) 
Explain the fallacy in the following argument: One pair of the points must 
lie on a semicircle terminating at one of them. The probability that the third point lies 
on this semicircle is \, which is therefore the required answer. 
7.3. A simple random sample of n values, drawn from the normal 
population 
EXERCISES 185 
Show that the value of m which maximises the likelihood of this event is 
n 
which is therefore the " best" estimate of the mean of the population. ( 
7.4. Show that if ^> is the probability of a zero in the Irregular Kollektiv the 
probability un that there will be r consecutive zeros in a set of n members ohosen at random obeys 
the recurrence relation 
«*+l =«» + (!- un-r)Pr(l ~ P) 
and hence that 
where vn> r = JT (- 1)'( _? ) {p'(l - p) }>. 
7 = 0 \ J / 
CHAPTER 8 
RANDOM SAMPLING 
The Sampling Problem 
8.1. In the previous chapter we have referred incidentally to the sampling problem, 
which can be stated quite simply : given a sample from a population, to determine from it 
the properties of that population. We noted that only in exceptional cases is it possible to 
make assertions about the population with complete certainty, and that consequently it is 
necessary to fall back on statements of a less categorical kind expressible in terms of 
probability. 
8.2. In order to be able to apply the theory of probability to this problem it is necessary 
that the sampling should be random. In actual practice we often meet with samples which 
are not random, having been chosen purposively for some reason or other. In such 
circumstances it is not, as a rule, possible even to make precise statements in probability ; and where 
a decision has to be taken one is forced to rely on subjective judgments of an unsatisfactory 
kind. No numerical estimate of the probabilities can be made. It is for this reason that 
random sampling becomes of primary importance in statistical investigations from sample- 
to population. From this point onwards we shall deal only with random samples, and to 
avoid constant repetition shall leave it to be understood that where a " sample " or 
a " sampling distribution " is referred to, random conditions are assumed. 
8.3. It is useful to begin a discussion of random sampling by considering the types of 
parent population from which samples can be chosen. 
(a) In the first place, the population may be finite and existent, e.g. the population of 
human beings in Europe at a fixed point of time, or the population of apples on a given 
tree, A sampling process which extracts members one at a time from this population 
will evidently eventually exhaust the supply of members if continued long enough. Thus 
the sampling, though random, is not simple in the sense of 7.21, for the probability of 
a given member being chosen varies according to what has already been abstracted. 
We may, however, reduce this process to one of simple sampling by replacing the 
members after withdrawal. The population then remains the same at each trial. The two 
cases, are sometimes distinguished as " sampling without replacement " and " sampling with 
replacement ". 
Furthermore, we may also in many cases regard the sampling as simple to an adequate 
approximation even when there is no replacement. If the population is large compared with 
the size of the sample, the abstraction of relatively few members will not materially affect the 
constitution of the remaining population, which may thus be regarded as approximately the 
same for subsequent samplings. 
(b) Sampling with replacement from a finite population may, in fact, be regarded as 
sampling from an infinite population, for the process, will never exhaust the supply. We 
may, however, have to deal with a population which is infinite in rather a different sense, 
namely, that of a limiting form. We may, for example, wish to consider the probability of 
a sample from the positive integers or the real numbers from 0 to 1. The latter case presents 
itself in sampling from a continuous frequency-distribution which we must necessarily regard 
as infinite. 
186 
RANDOMNESS IN SAMPLING 187 
Thus, if we replace an obsarvational distribution by a. conceptual continuous 
mathematical distribution, we replace at the same time a finite population by an infinite population. 
The drawing of random samples from such a population is attended by the circumstances 
referred to in 7.13 and 7.29, namely, that the process to the limit must be taken into account. 
(c) Thirdly, the population may be purely hypothetical. Consider, for example, the 
throws of a die. We may picture the continual throwing as a sampling process drawing 
existent members from some non-existent population. In such cases what we are really 
doing is constructing by mental fiction an imaginary population round the sample. 
The concept of the hypothetical population is necessitated by ideas of frequency in 
probability. It is not required (and indeed has been explicitly rejected by Jeffreys) in the 
approach .which takes probability as an undefinable measurement of attitudes of doubt. 
But if we take probability as a relative frequency, then to speak of the probability of a sample 
such as that given by throwing a die or growing wheat on a plot of soil, we must consider the 
sample against the background of a population. There are obvious logical difficulties in 
regarding such a sample as a selection—it is a selection without a choice—and still greater 
difficulties about supposing the selection to be random ; for to do so we must try to imagine 
that all the other members of the population, themselves imaginary, had an equal probability 
of assuming the mantle of reality, and that in some way the actual event was chosen to do so. 
This is, to me at all events, a most baffling conception. At the same time, it has to be 
admitted that certain events such as dice-throwing do happen as if the constituents were 
chosen at random from an existent population, and it accordingly seems that the concept of 
the hypothetical papulation can be justified empirically. 
Randomness in Sampling 
8.4. In its colloquial use the word " random " is applied to any method of choice 
which lacks aim or purpose. We speak of drawing names at random out of a hat, choosing 
plants at random from a field of corn, selecting family budgets at random from the 
population, meaning thereby that the selection is completely haphazard. 
Now it is found in practice that choice by a human being is not random in the stricter 
sense that it produces equally frequently events which we are entitled to expect to have 
equal prior probabilities. Some examples will make this clear. 
> 
Example 8.1 
In the course of certain work at the Rothamsted Experimental Station sets of eight 
wheat plants were chosen for measurement. Six of these were chosen by approved mothods, 
TABLE 8.1 
Distribution of Plants chosen hapliazardly in Ranks 1 to 8. 
(F. Yates, Ann. Eug&ri. Lond., 6, 202.) 
Date. 
May 31st 
June 28th 
Observation. 
Shoot height . 
Ear height 
Numbers bearing Specified Rank. 
1 
9 
9 
2 
7 
19 
.3 
11 
27 
4 
8 
23 
5 
11 
16 
6 
18 
10 
7 
' 21 
6 
8 
31 
4 
TOTAJC, 
116 
112 . 
188 .RANDOM SAMPLING 
referred to below, and may be taken to be truly random, The other ^~J^» 
haphazardly by eye. If, in any set, the eight plants were ranged m order of magnitude, tlm 
IS by eye coulk have any number from one to eight; and if they m common w. I, 
the other six, were chosen at random, they should occupy these places with approxmm e l.v 
equal frequency in a large number of sets. Table 8.1 shows what actually occurred on two 
different occasions (a) on May 31st, before the ears of wheat had formed, and (ft) on Juno -hi ii, 
after the ears had formed. 
The divergence of actual from expected results is quite striking.. On May .list, betoio 
the ears had formed, the observer was strongly biased towards the taller shoots ; whereas m 
June he was biased strongly towards the central plants and avoided short and tall plants 
Thus it is seen that bias can appear even in a trained observer, and that tho.biiw need 
not be consistent in over- or under-estimation in different circumstances. 
Example 8.2 
The following table shows the frequencies of final digits in a number of measurement* 
made by four different observers:— 
TABLE 8.2 
Bias in Scale Reading. Distribution of Final Digits in Measurements by 
Four Observers. 
(G. U. Yule, J.R. Statist. Soc, 90, 570.) 
Final Digit. 
■ 
0 
1 
2 
3 
4 
5 
6 
7 
8 
9 
Total 
Frequency of Final Digit per 1000. 
A 
158 
97 
125 
73 
76 
71 
90 
56 
126 
129 
1001 
B 
122 
98 
98 
90 
100 
112 
98 
99 " 
101 
81 
999 
C 
251 
37 
SO 
72 
55 
222 
71 
75 
72 
(55 
1000 
JD 
358 
4» 
, DO 
(13 
37 
211 
(12 
70 
44 
10 
1000 
It is hard to suppose that there was any genuine difference which would load to t ho 
appearance of certain digits at the expense of others, and we may confidently hu]m>ohc that, 
the deviations from approximate equality indicate bias on the part of 11 in observer. 
Observer A had decided preference for 0, 2, 8 and 9, avoiding the centre of the nculo. 
Observer B is quite good, his deviations from expected values being small, though he also 
showed some preference for 0. Observer C was poor, rounding off one measurement, in two 
to the whole or half unit. Observer D was obviously very bad indeed, nearly r>7 per cent, of 
his measurements being rounded off to the whole or half unit. 
The observations were all made by reading a scale, those under A being on drawings to 
RANDOMNESS IN SAMPLING 
189 
the nearest tenth of a millimetre, those under B, C, and D being measurements on the heads 
of living subjects to the nearest millimetre. We may conclude from this that different 
observers may exhibit different degrees of bias even under comparable circumstances, and 
that even those who are aware of the existence of the possibility of bias and the necessity for 
taking great care (as observer A was) may nevertheless fail to avoid it. 
Example 8.3 
An observer was placed before a machine consisting of a oircular disc divided into ten 
equal sections in which were inscribed the digits 0 to 9. The diso rotated at high speed and 
every now and then a flash occurred from a nearby electric lamp of suoh short duration that 
the disc appeared at rest. The observer had to watch the disc and write down the number 
occurring in the division indicated by a fixed pointer. 
This was a machine designed for the provision of truly random numbers (see below, 8.10) 
and had been found by another observer to do so. But this particular observer produced 
•a definite bias. The frequencies of digits in 10,000 run off by him are shown in Table 8.3. 
TABLE 8.3 
Distribution of Digits obtained by an Observer in using a Randomising Machine. 
(Kendall and Babington Smith, Supp. J.R. Statist. Soc, 6, 51.) 
Digit. 
Frequoncy 
0 
1083 
1 
865 
2 
1053 
3 
884 
4 
1057 
5 
1007 
6 
1081 
7 
997 
8 
1025 
9 
948 
Total. 
10,000 
If tlie observer was unbiased the digits should appear in approximately equal numbers ; 
but there is a bias in favour of all the even numbers and against the odd numbers 1, 3 and 9. 
The cause of this bias is obscure, for the observer did not have to estimate (as in the previous 
example) but merely to write down something which he saw, or thought he saw. The 
■explanation seemed to be that he had a strong number-preference, i.e. that he actually mis- 
saw the numbers, or that his brain controlled his ocular impressions and censored them. We 
have here to deal with one of the deadliest forms of bias in psychology. 
Example 8.4 
Every year a number of crop repdrters in England and Wales estimate the prospective 
yields of certain crops, forecasts being obtained at different periods of the year and final 
estimates whon the crop is harvested. Table 8.4 shows the average estimated yield of 
potatoes at the various times for the years 1929-1936. 
This table exhibits very clearly an effect which has shown itself in nearly all the English 
crop reports (and appears also in other countries), namely, the chronic pessimism of crop 
forecasts. In every case but one in the above table the forecasts arc below the final yield. 
Nor do crop reporters seem able to learn by experience that they are underestimating. 
Nothing in this table indicates that the differences between foreoast and final estimate 
diminished during the period concerned. 
It should also be noticed that these estimates are the weighted average of a large number 
of independent observations. One of the commoner misunderstandings in this type of work 
/ 
% I 
190 RANDOM SAMPLING 
TABLE 8.4 
Bias in Crop Forecasting. Forecasts of Yields of Potatoes in England 
and Wales (Tons per Acre). 
(From the official agricultural statistics.) 
Year. 
1929 
1930 
1931 
1932 
1933 
1934 
1935 
1930 
Sept. 1st. 
Yield. 
5-7 
6-0 
, 5-5 
0-4 
6-4 
6-0 
5-6 
0-0 
% Difference 
from Final. 
- 17-4 
- 7-7 
0-0 
- 3-0 
- 4-5 
- 15-5 
- 9-7 
- 3-2 
Oct. 1st. 
Yield. 
6-2 
6-1 
5-3 
6-2 
6-2 
(J-3 
5-7 
5-9 
% Difference 
from Final. 
- 10-1 
- 6-2 
- 3-6 
- 6-1 
- 7-5 
- 11-3 
- 8-1 
- 4-8 
Nov. 1st. 
Yield. 
6-5 
6-1 
5-3 
6-3 
6-4 
6-7 
60 
5-8 
% Difference 
from Final. 
- 5-8 
- 6-2 
- 3-6 
-4-5 
- 4-5 
- fv(S 
- 3-2 
- 0-5 
Final 
Estimate. 
6-9 
6-5 
5-5 
6-6 
6-7 
7-1 
(5-2 
6-2 
is based on the supposition that, though individuals may make mistakes, their errors will 
cancel out in the aggregate. Our present example shows this to be untrue in general. 
There can appear a systematic bias affecting all the individuals performing estimates. 
8.5. The foregoing examples are enough to indicate that human bias is very prevalent. 
Trained observers may be biased even when conscious of their own imperfections ; different 
observers may be biased in different ways in similar circumstances ; and the same observer 
may be biased in different ways in different circumstances. It is abundantly clear that we 
must look for true randomness elsewhere than in mere lack of purpose on the part of human 
observers. There may be persons whose psychological processes are so finely balanced that 
they can deliberately select random samples, but few statisticians who have experimented in 
this interesting field would regard themselves as among them. 
8.6. In Chapter 7 we saw that the primary function of randomness in probability was 
that it ensured that certain primitive events were equally probable. Wo may say that 
a method of selection is random for a population U if, when applied to U, it gives all members 
an equal probability of being chosen ; or, in the language of frequency, if, when continually 
applied to U, it educes the members approximately equally frequently. 
But this is not enough. Suppose we had a population of two members A and B, and 
sampled with replacement. Then a method which chooses A and B alternately and produces 
the series ABAB . . . educes each member approximately equally frequently ; but it is 
not what we customarily mean by a random method. What we require of a random method 
is that in such circumstances it should produce^ series like that of von Mises (7.15) in which 
no systematic arrangement is evident. Not only single characteristics, but all possible 
groups of characteristics should appear equally frequently. 
8.7. A further point is to be noted. We may, in drawing the sample, be interested in 
one particular variate exhibited by the members, and it is possible that a method may give 
a satisfactory random sample so far as this variate is concerned without doing so for other 
THE TECHNIQUE OF RANDOM SAMPLING 191 
variates. Suppose, for example, we are anxious to take a random sample from the 
inhabitants of a particular street. If we are concerned with a variate such as eye-colour it 
might be sufficient to choose a house every so often, say every tenth house, and take the 
inhabitants of that house as part of the sample. Such a method would not give every 
inhabitant an equal chance of being chosen; but if we look back to the time when the 
inhabitants took up residence we may imagine that the colour of their .eyes did not influence 
their geographical distribution, and thus that if we consider the allocation of the inhabitants 
in some way independent of eye-colour, and then take every tenth house, we may suppose 
that so far as eye-colour is concerned the sample is random. But the matter would stand 
differently if we were sampling for income. If for instance every tenth house was a corner 
house and thus inhabited by a person of more than average income, our sample would no 
longer be random with respect to income. Looking back, as before1, to .the time when 
inhabitants took up residence, we see that they can no longer be regarded as distributed at 
random, for those with larger incomes will tend to be attracted towards the more expensive 
houses. 
Thus a method which is random for one population may not be so for another ; and even 
in the same population a method random for one variate may not be so for another. 
Randomness is relative. 
The Technique of Random Sampling 
8.8. Suppose, then, that we are given a population and a variate is specified. How are 
we to draw a random sample, i.e. how can we find a method which is random foj: that 
population and that variate ? The answer lies partly in theory and partly in practice. 
(a) In the first place we must require that there is no,obvious connection between the 
method of selection and the properties under consideration. The method and the properties 
must be independent so far as our prior knowledge is concerned. In sampling a field of 
wheat for shoot height, for example, we must not use a method which could be influenced by 
that height, such as skimming a hoop over the field and selecting the plants round which it 
fell (for the hoop might tend to catch on the taller plants). Again, in sampling the 
inhabitants of a town by choosing names from a telephone directory we should undoubtedly 
tend to get the more well-to-do classes and hence, if the variate under consideration is wealth 
or any related characteristic such as number of children, political opinion, standard of 
education and so on, the sample would not be random. If we were concerned with 
characteristics such as height, hair colour, or blood group the sample might be random, though it is 
not difficult in many similar cases to think of reasons why the variate might be linked with 
wealth. 
' If this matter is viewed from the standpoint of the axiomatic theory of probability the 
absence of knowledge about relationship between the method and the characteristic under 
consideration may be sufficient to ensure randomness, for the probabilities of elementary 
propositions then become equal—the probabilities being measures of prior attitudes of mind.* 
But if the frequency viewpoint is adopted it is not enough that there should be absence of 
knowledge of this kind, for unknown to the observer there may be relations which will prevent 
the elementary propositions from being true in approximately equal proportions. The 
presumption is that if we make as great an effort as possible to ascertain whether any 
relationship exists and fail to find it,1 there is no relationship ; and hence we can assume randomness 
* At least, this is my interpretation of the position ; but the writers on the axiomatic theory have 
not discussed randomness at any length, being content to define it in terms of probability, and I may 
be putting a gloss on their views which they would not accept. 
192 RANDOM SAMPLING 
with more or less confidence. But in this approach the assumption of randomness is 
ultimately part of the general uncertainty of the inference from sample to population. 
(6) Secondly, we may rely on previous experience of a random method to justify its use 
on new occasions. This is evidently an extrapolation, and though most people would regard 
it as reasonable, the fact has to be realised. The axiomatic theory of probability can embrace 
this extrapolation within its scope, for the probabilities given by the method are assessable 
in terms of prior knowledge ; but the frequency theory has to take, the extrapolation as an 
additional assumption. 
8.9. One of the most reliable methods of drawing random samples consists of 
constructing a model of the population and sampling from the model. We may, for instance, note 
down the characteristics of each member on a card and sample by choosing cards from the 
pack corresponding to the whole population. This is the method adopted in lotteries and 
the process is known as lottery or ticket sampling. It is moderately effective but suffers in 
practice from two disadvantages: the labour of constructing the card population, and the 
danger of bias in the drawing of cards. Example 12.1 below, for instance, shows that the 
ordinary processes of shuffling and dealing playing-cards may fail to be satisfactory. To be 
reasonably satisfied about the randomness of the shuffling entails a good deal of trouble and 
labour, and the same object can be attained much more simply by the use of random sampling 
numbers, whioh we now consider. 
Random Sampling Numbers 
8.10. The easiest way of constructing a miniature population is to attach an ordinal 
number to each member, mostly simply by numbering the members from 1 .onwards. The 
set of ordinals so obtained is the miniature population and the problem of drawing a random 
sample reduces to finding a series of random numbers. The advantages of this method are 
obvious : no physical model population has to be constructed ; the numbering can be carried 
out in any convenient manner; and the series of random numbers can be applied to any 
enumerable population so that any series of random numbers has a very wide range of 
application. 
One point should be made clear here. If the numbering of the population is carried out 
in such a way as to be independent of certain characteristics of the population, any set of 
numbers will serve to draw a sample random with respect to those characteristics. The 
randomness in such a case lies, so to speak, in the allocation of ordinals to the population, 
not in deciding which ordinals to select for the sample. But in practice a procedure of this 
kind is of no value, since it only throws back to the difficulty of numbering the population 
" at random ". The usual course is to number the population in any convenient way, related 
to the characteristics or not, and then seek for a set of numbers which are a random set from 
the possible ordinals of the population. 
8.11. One of the more obvious ways of drawing random samples from an enumerated 
population is to use haphazard numbers taken from some totally unrelated source. Suppose, 
for instance, we wished to take a sample from the visible stars in the sky. We will ignore the 
small complications due to the existence of double stars and unresolved objects. Since the 
position of a star on the celestial sphere is defined by latitude and longitude, what is then 
required is a series of random pairs of latitudes and longitudes. At first sight it seems 
plausible to take an ordinary atlas and choose the figures set out in the index for place-names 
arranged alphabetically; for there is little reason to expect any relationship between the 
RANDOM SAMPLING NUMBERS 
103 
distribution of stars in the sky and the"distribution of places on the Earth's surface. A little 
reflection, however, will show that the method is unsound. There are large stretches of 
territory and sea on the Earth which have no place-names on them—the poles, deserts and 
oceans ; consequently no numbers will occur for these regions and there will be corresponding 
areas on the celestial sphere which have no chance of being included. 
8.12. As a next attempt we might take a book containing a number of digits, e.g. 
a telephone directory, or a set of statistical tables or mathematical tables, open it at hazard 
and choose the digits which first strike the eye, or which occur at the top of the page, and so 
on. -This is an improvement, but it is still open to some objection. 
(a) Telephone directories. Table 1.4 on page 6 shows the distribution of 10,000 digits 
taken from the London telephone directory. Pages were chosen by opening the directory 
haphazardly, numbers of less than four digits and numbers in heavy type were ignored ; and 
of the four-figure numbers remaining the two right-hand ones were taken for all numbers on 
the page. If the numbers were random we should expect about 1000 of each digit in the total 
of 10,000. Actually there are very considerable deviations from this expectation, and we shall 
see in a later chapter that they cannot be explained as sampling fluctuations. There are 
significant deficiencies in 5's and 9's, due to several causes such as the tendency to avoid 
these digits because they sound alike, the reservation of numbers ending in 99 for testing 
purposes by telephone engineers and so on. It is evident that tables of random numbers 
could not be constructed from directories such as this. 
(b) Mathematical tables. Evidently care has to be exercised in using mathematical 
tables in constructing random series. Suppose, for instance, we take a set of logarithm 
tables. There are clearly relationships between successive logarithms, expressible by the 
fact that differences are approximately constant if the interval is small. Moreover there is 
a very curious theorem about digits in certain classes of table which throws theoretical doubt 
on the method. Consider the logarithms to base 10 of the natural numbers from 1 onwards. 
Suppose we choose the kth. digit in each and so obtain a series of numbers 0-9. Then the 
proportional frequency of any digit in this series does not tend to a limit as the length of the 
series increases, whatever h may be.* Just what does happen does not appear to be known, 
but it would seem that certain systematic effects begin to show themselves and these will 
obviously endanger the randomness of the series. 
(c) Statistical tables. If we have a volume of statistics such as populations of towns 
and rural districts there are some grounds for supposing that if the numbers are large—say 
four figures or more—the final digits will be random. Here again, however, the use of such 
tables requires care—they may have been compiled by an observer with number preferences, 
and some rounding up may have taken place. 
i 
8.13. However, the necessity for the ordinary student to construct random series of 
his own has been obviated by the publication of various tables of Random Sampling Numbers. 
There are three such available :— 
(a) Tippett's numbers comprise 41,600 digits taken from census reports combined into 
fours to make 10,400 four-figure numbers (Tracts for Computers, No. 15). 
(6) Kendall and Babington Smith's numbers comprise 100,000 digits grouped in twos 
and fours and in 100 separate thousands (Tracts for Computers, No. 24). These numbers were 
* Cf. J. Franel, Viertdjahrschrift der Naturforsclienden Qesellschaft in Zurich (1917), 62, 286. So 
groat a mathematician as Poincare made a mistake on this point. 
A.s. O 
194 
RANDOM SAMPLING 
obtained from a machine specially constructed for the purpose on the lines very briefly 
described in Example 8.3. 
(c) Fisher and Yates' numbers comprise 15,000 digits arranged in twos {Statistical Tables 
for Biological, Agricultural and Medical Research). These numbers were obtained from the 
15th-19th digits in A. J. Thompson's tables of logarithms and were subsequently adjusted, 
it having b.een found that there were too many sixes. 
Before considering the basis of these tables it may be helpful to give some examples of 
their use. Here are the first 200 of the Kendall-Babington Smith tables :— 
TABLE 8.5 
Random Sampling Numbers. 
{Tracts for Computers, Np. 24.) 
23 15 
05 54 
14 87 
38 97 
97 31 
75 48 
55 50 
16 03 
67 49 
26 17 
59 01 
43 10 
50 32 
51 94 
18 99 
83 72 
53 74 
40 43 
05 17 
75 53 
59 93 
35 08 
62 23 
58 53 
08 70 
76 24 
90 61 
50 05 
78 80 
- 94 25 
97 08 
18 37 
10 03 
59 01 
12 58 
86 95 
44 10 
22 11 
94 32 
41 54 
23 03 
96 22 
54 38 
42 87 
88 21 
67 44 
13 43 
08 34 
16 95 
05 13 
Example 8.5 \ 
To draw a sample of 10 men from the population of 8585 men of Table 1.7. 
The first process is to number the population ; and here, as in most similar cases, one 
numbering has already been provided by the frequency-distribution. We take numbers 
1 and 2 to be those in the group 57- inches, numbers 3 to 6 those in the group 58-, and so on, 
those in the group 77- inches being numbers 8584 and 8585. 
Now we take 10 four-figure numbers from the tables, e.g. reading across in Table 8.5 
we have 
2315, 7548, 5901, 8372, 5993, 7624, [9708], [8695], 2303, 6744, 0554, 5550. 
The two numbers in square brackets are greater than 8585 and we ignore them. We 
now select the individuals corresponding to the remaining 10 numbers. They will be found 
to be in the intervals 65-, 70-, 68-, 72-, 68-, 70-, 65-, 69-, 63-, 68- inches respectively. 
The mean of these values considered as located at the centres of intervals is 68-24, as 
against a value in the population of 67-46. 
Example 8.6 
To draw a sample of 12 from the population in the following bivariate table, showing 
the relation between inoculation and attack in cholera. 
Inoculated .... 
Not inoculated 
Totals .... 
Not Attacked. 
276 
(0001-3312) 
473 
(3349-9024) 
749 
Attackod. 
3 
(3313-3348) 
66 
(9025-9816) 
69 
r 
Total. 
279 
v 539 
818 
RANDOM SAMPLING NUMBERS 195 
There are now 818 members. We could, of course, take three-figure numbers from 
the tables, obtaining, e.g. from Table 8.5 
231, 575, 485, etc. 
But this is rather troublesome as the numbers-are not grouped in threes.' It is more 
convenient to take four-figure numbers as before and to associate each member of the population 
with 12 numbers in the tables, e.g. the first would correspond to 0000-0011, the second 
to 0012-0023, and so on. We then get the numbers shown in brackets in the above 
table. Numbers above 9816 we ignore as before. 
The two numbers omitted in the previous example oan now be used, and we find the 
following results :— 
Inoculated .... 
Not inoculated 
Totals .... 
Not Attacked. 
3 
8 
11 
Attacked. 
0 
1 
1 
Total. 
3 
9 
12 
Here, for example, the member corresponding to the number 2315 falls in the not-attacked: 
inoculated class, and so on. 
It has so happened in this example that no member in the very small class inoculated : 
attacked clasp has been selected. Suppose we had had a series containing 
3314, 3323, 3333, 334l'. 
All these fall into the group and there are four of them, as against only three members 
in the population. Had we been confronted with this position we should have had to 
decide whether the sampling was to be with or without replacement. If it was without 
replacement, we should have to suppose that the first three numbers in the group 3313-3348 
exhausted that part of the population and ignore all numbers of the group occurring 
subsequently. 
Example 8.7 
To construct a series of random permutations of the numbers 1 to 5. 
Here we are not concerned with the digits 0, 6, 7, 8 and 9, and so ignore them in the 
table of random numbers. We read through the ta,ble and note the digits as they occur, 
e.g. in Table 8.5 we have 2315, 7548, etc. The 7 is to be ignored and also the second 5, 
for one 5 has already occurred. We then reach the permutation 23154. Then we start 
again, the next series being 8, 5901, 8372, 5993, 7624, etc., giving the permutation 51324; 
and so on. 
Example 8.8 
1 _iL* 
To take a random sample of 10 from the normal population dF = —rx-e 2 dx. 
■yZTC 
This is a particularly interesting case, for we have to select a sample from an infinite 
population. Such a process, as has been seen, oan only be considered as a limiting one. 
/ 
196 
RANDOM SAMPLING 
Consider the frequencies of the normal curve in ranges 0-1 on each side of the mean. 
These may be obtained very simply from tables of the normal integral by differencing 
and in fact are given in many tables of that integral, e.g. that of Appendix Table 2. Suppose 
the frequencies rounded up to four places of decimals, e.g. those near the mean would be 
and the total 
Upper Limit of 
Interval. 
0-0 
01 
0-2 
0-3 
frequencies 
00- 
398 
01- 00394 
0-2- 00387 
0-3- 6-0375, etc. 
are given by the normal integral itself, e.g. 
Frequency up to 
that Limit. 
0-5000 
0-5398 
0-5793 
0-6179, otc. 
Upper Limit of 
Interval. 
- 01 
- 0-2 
- 0-3 
— 0-4 
Frequency up to 
that Limit. 
0-4602 
0-4207 
0-3821 
0-30S5, etc. 
We may now attach a four-figure random number to this population, which is finite and 
discontinuous : e.g. the number 5461 corresponds to a variate-value + 0-1- and the number 
3500 to — 0-4-. 
Had we" taken the table to n places of decimals we should have required n- figure 
numbers. Furthermore, we can make the approximation more exact by taking a finer 
variate interval. Such matters as this are to be decided in the light of the degree of 
approximation required. 
8.14. Random Sampling Numbers must obey certain conditions before they can be 
used. Any set of numbers whatever is random in the sense that it might arise, with however 
great improbability, from random sampling; but such a set might not be suitable as a 
table of Random Sampling Numbers. From the examples already given it is clear that 
we desire such a table to have very great flexibility. It should give random results in as 
many cases as possible, whether used in part or in whole. 
Now it is impossible to construct a table of Random Sampling Numbers which will 
satisfy this requirement entirely. Suppose, to take an extreme case, we constructed a 
table of 1010" digits. The chance of any digit being a zero is -^ and thus the chance that 
any given block of a million digits are all zeros is 10~10'. Such a set should therefore arise 
fairly often in the set of 10(lul0_Q) blocks of a million. If it did not, the whole set would 
not be satisfactory for certain sampling experiments. Clearly, however, the set of a million 
zeros is not suitable for drawing samples in an experiment requiring less than a million 
digits. 
Thus, it is to be expected that in a table of Random Sampling Numbers there will 
occur patches which are not suitable for use by themselves. The unusual must be given 
a chance of occurring in its due proportion, however small. Kendall and Babington Smith 
attempted to deal with this problem by indicating the portions of their table (5 thousands 
out of 100) which it would be better to avoid in sampling experiments requiring fewer 
than 1000 digits. 
8.15. If a table of random numbers is used to draw members from a population 
of ten, we expect the members to appear in approximately equal proportions. In other 
words we expect such a table to contain the ten digits 0-9 in approximately equal pro- 
SAMPLING PROM ATTRIBUTES 197 
portions. Similarly we expect the hundred pairs 00-99 to appear in approximately equal 
proportions, and so on. Various tests of this kind, based on a comparison between actual 
frequencies and those required to satisfy the laws of probability, can be devised!. No 
table can satisfy them all, but if it satisfies tests which (a) ensure the randomness of the 
numbers for the commoner types of sampling inquiry for which it is likely to be used and 
(6) are capable of revealing any particular sort of bias to which the numbers are susceptible 
in virtue of their mode of formation, it is likely to be of general application. 
For a more detailed,discussion of these matters and the results of tests on the Tippett 
tables, the Kendall-Babington Smith tables and the Fisher-Yates tables, reference may be 
made to the works listed at the end of the chapter. 
Sampling from a Continuous Population 
8.16. Random Sampling Numbers offer the best method known at the present time 
of drawing random samples from an enumerable universe, and as was seen in /Example 8.6, 
may also be used to draw samples from a continuous population specified mathematically. 
But cases sometimes occur in which they cannot be employed. For instance, if we wish 
to take a sample of milk or flour, we cannot in practice number each particle and extract 
it from the population for examination. In such cases we are usually compelled to fall 
back on more intuitively grounded procedure. To take a random sample from a milk 
churn, for example, we might stir the contents thoroughly and scoop up a sample 
haphazardly. Sometimes, when the population is of manageable size, we can proceed 
systematically by dividing it into a number of parcels and selecting. parcels by the ordinary 
technique of random numbers. Most sciences have their own peculiar sampling problems 
and no attempt can be made here to discuss them all. At this point we leave the technique 
of random sampling and assume hereafter, unless the contrary is stated, that the material 
we are discussing has been obtained by a random process. 
Sampling from Attributes 
8.17. As an introduction to the general sampling problems we shall consider the 
sampling of attributes, which raises all the difficulties of principle but is not obscured by 
too much mathematics. 
Suppose we have a random sample from a population whose members all exhibit either 
an attribute A or its negative not-A. Our sample is n in number, and a proportion p, or 
a number pn, exhibit the attribute ; and consequently a proportion q, or number qn 
(p -\- q — 1) do not. We will assume that the population is large, or that sampling is with 
replacement, so that the probability of obtaining an A at any drawing is not affected by 
other drawings and is therefore a constant, say w. 
The problems we have to consider are of three types :— 
(a) Suppose we have some reason for supposing that the proportion of A'b in the 
population is given by a known m. Does the observed proportion p bear out this hypothesis 
or is it so divergent from m as to lead us to doubt the hypothesis ? In an experiment with 
plants exhibiting two strains of a quality such as height in pea plants, we may wish to 
test whether the breeding follows the simple Mendelian law of dominant and recessive. 
If we begin with two pure strains tall and short, cross-breed a first generation and then 
produce a second generation by interbreeding, the proportional frequencies of " short" 
and " tall " in this generation will be £ and J if " short " is dominant and J and f if " tall " 
is dominant, provided that the simple Mendelian law holds. Suppose we carry out such 
198 RANDOM SAMPLING 
an experiment and find that for1 400 plants the frequencies are 70 and 330. Can the diver- 
gence from the theoretical values 100 and 300 have arisen by chance, oris it large enough 
to" throw doubt on the hypothesis that the simple Mendelian law is operating . 
(6) In the foregoing tywe of problem we have some reason for tenting a value ol m 
given a priori ; but we may know nothing of m, and in such a case our prmcipal problem 
is to estimate it from the sample. t 
(c) Then, having estimated it, we wish to know the degree of reliability of the estimate. 
How far is the estimate likely to deviate from the real value of w 1 
8.18. Consider first of all the first type of problem, in which m is given a priori. If 
we choose samples of n from the population they will, on our supposition that the 
probability of obtaining an A remains constant, te arrayed by the binomial n (x -I rn)», that, 
is to say, the probability of obtaining pn A's and qn not-A's in a sample of n is the U>rm in 
wpn yon in (^ + ta)n, i.e. ( jm^1^- Thus the probability of obtaining pn or fewer A'h 
is the sum of the first pn + 1 terms in this binomial. 
If this probability is small we have the choice of three possibilities :— 
(a) An improbable event has occurred. 
(6) The hypothesis is not true, i.e. the proportion of A'b in tho population is not m. 
(c) The sampling process is not random. 
We can usually exclude (c) by taking care with the sampling process, and merely have 
to balance (a) and (6). It is in general accordance with the notions discussed in t ho previous 
chapter that we dismiss a hypothesis which gives rise to an improbable event in favour 
of one which makes it more probable. Thus if the improbability is great we reject t he 
hypothesis, i.e. we are led to doubt (with greater or less force, according to the degree of" 
improbability) the supposition that the proportion of A'b in tho population really is m. 
Per contra, if the event is probable it casts no doubt on the hypothesis 
Example 8.9 
In certain coin-tossing experiments a coin was tossed 20 times and eame down heads 
15 times. Does this conflict with the hypothesis that the coin was unbiased? 
The hypothesis we have to test here is that w == L The probability that in :!0 tosses 
we should get 5 tails or fewer is the sum of the first 0 terms in (| + .J)ao, and will be found 
from Table 5.2 to be 0-0207. Thus the probability of such an event is small the odds 
are 50 to 1 against the event—and we suspect the hypothesis accordingly. If, on tho 
other hand, we had supposed the value of w to be 0-7 we should have found the lirsl. 
6 terms of (0-3 + 0-7)20 to be 0-4163, so that the .event is no longer improbable, and we 
should not have rejected the hypothesis. 
8.19. In the example just given we purposely took a fairly low value of u in order 
that the terms of the binomial could be calculated directly. In practice //. is often fairly 
large—100 or more—and the evaluation and summation of individual terms would be 
most tedious. We can, if complete accuracy is desired, use the method of summation 
given m 5.7, and evaluate from the incomplete JS-function. But for all ordinary purposes 
it is quite enough to use the normal approximation to the binomial. We saw in Kxmnnlu 
4.6 that as n -> oo the binomial (x + ro)» tends to the normal distribution with mean \i m 
and variance nmX. Probabilities can therefore be evaluated from the normal integral 
i 
STANDARD ERROR 199 
In fact, for many purposes, it is not even necessary to carry out the actual evaluations. 
From the tables of the integral (Appendix Table 2) we note that the probability of a 
deviation as great or greater in absolute value than the standard deviation is 0-3173 ; than 
twice the s.d. is 0-0455 ; than thrice the s.d. is 0-0027 ; than four times the s.d. is 0-00006. 
Thus if we mid np differs from nm by more than twice Vnmx we begin to doubt the 
hypothesis, and if the difference is more than thrice Vnzax we may confidently reject it. 
Similarly, if the proportion p differs from w by more than twice . / -£ we begin to 
doubt the hypothesis, and so on. It makes no difference whether we compare the actual 
frequencies or the proportions. 
Example 8.10 
In some dice-throwing experiments Weldon threw ,di°e 49,152 .times, and of these 
25,145 yielded a 4, 5, or 6. Is this consonant with the hypothesis that the dice were 
unbiased ? 
If the dice are unbiased the probability of a 4, 5 or 6 is |. Thus nm is 24,576 and the 
observed np is 569 in excess of this value. 
Vnmx = V(i X|X 49,152) = 110-9. 
The observed deviation is more than 5 times this quantity and we accordingly suspect 
very strongly that the dice were biased. 
/Standard Error 
8.20. The quantity <\/(wn7#) is a particular case, appropriate to the binomial, of an 
important statistical concept known as the standard error. It is the standard deviation of 
the sampling distribution of the parameter m. . It is particularly important in .the class of 
cases, which is relatively large, wherein that sampling distribution can be taken to be 
normal either exactly or to an adequate degree of approximation. 
8.21. Let us now turn to the case in which no value of zo is given a priori. If the 
sample gives a proportion of A'b equal to p, what shall we take as our estimate of ro? 
The most obvious course is to take p itself; and this is the course dictated by the more 
sophisticated ideas described in Chapter 7. 
Consider first of all the method of maximum likelihood. The probability of obtaining 
np A'b and nq a's is 
( 
$"*". ...... (8.1) 
This is proportional to the likelihood, and neglecting constants we have to maximise 
for variations of m. We have, since L is non-negative, that «— and «— (log L)( = yr-l 
vanish together and it is therefore sufficient to maximise log L. We have 
giving m=p. (8.2) 
\ 
200 RANDOM SAMPLING 
The method of Bayes will give the same result if we suppose the possible values of w 
equally distributed between 0 and 1 for dm —> 0. For then 
P(m\p) = W , (8.3) 
. (8.4) 
which, as before, is maximised when m = p. 
There is another way of looking at this problem of estimation. Suppose we took a 
large number of samples from the population of m A'b and % note's. Our estimate of 
w would be p in each case, p varying from sample to sample ; and the mean value of all 
such estimates would be 
E(p)=£p^ynPxn° (8.6) 
/> = (! 
-Zlnpl1 
n { \n 
= -S{np{^)W"t>(x)nal 
= lm.nZ\[ '" * )row»-1(z)na! 
so that the mean value of our estimate over all possible samples is m. Such an estimate 
is called unbiased—if we follow the rule of estimation the average of our estimates in a 
large number of cases will be exactly the correct value w. It may thus be argued that the 
unbiased estimate should be taken as a reliable estimate of rn. 
8.22. In this case, therefore, all the approaches lead to the same conclusion (a 
happy state of affairs which, as we shall see in the sequel, does not always exist). Consider 
now the next stage of the problem : what is the reliability of the estimate ? In other 
words, how far is the estimate likely to differ from the true value ? 
We know that if the sample value p differs from w by t J—, the probability of the 
difference becomes smaller as t increases. Thus, with an assigned degree of probability 
we can say that it is improbable that p will differ from w by more than an assigned amount. 
But to specify this amount exactly we require to know m ; and this is precisely the quantity 
we are trying to find. 
The problem can only be solved as an approximation. If n is large the standard 
error of xn is of the order w~*, so that we may put 
, & 
Hz - p)\\ . 
pqn* )] ' 
I 
STANDARD ERROR 
201 
neglecting terms of order n~z, 
= Ml+JM , t . . (8.6) 
V n \ 2 pqn* J __ 
Thus for large n the standard error of zn is approximately equal to Kf —', and we thus 
^/ ft 
reach the fundamental result that in large samples of attributes the standard error may be 
calculated by using the estimates of the parameters under estimate instead of the 
(unknown) values of those parameters themselves. 
Example 8.11 
In a sample of 600, 240 are found to possess the attribute A. Thusp = 0-40, np = 240, 
V(nP<l) = 12. We can thus regard it as somewhat improbable that nm differs from 240 
by more than twice this amount, 24, and highly improbable that it differs by more than 36. 
We thus can say with some confidence that nm lies in the range 240 ± 24 and with great 
confidence that, it lies in the range 240 ± 36. * 
8.23. We now turn to a gensral consideration of the problems of sampling which 
have been exemplified above. In the first place, let us note the role of the sampling 
distribution in this branch of the subject. We construct from the observations some statistic t. 
The sampling distribution of this statistic will in general (but not always) depend on some 
parameters of the parent population. The probability of the observed t then permits the 
making of statements, by inverse probability, likelihood or otherwise, about these parameters, 
and thus we are enabled to draw inferences about the parent population. The sampling 
distribution is thus fundamental to the whole subject and several subsequent chapters 
will be devoted entirely to the methods of finding distributions when the parent is specified. 
If we wish to test some hypothesis about the parent which is expressible by the 
determination of certain parameters a priori, the problem is fairly simple. Given the values 
of the parameters, we can determine from the sampling distribution the probability of the 
observed value of the statistic, and use this to assess the acceptability of the hypothesis. 
Complications can arise even here, however, for in general, several statistics can be compiled 
from the same sample, and they need not necessarily all lead to the same conclusion about 
the hypothesis; for example, a sample might have a mean which throws doubt on the 
hypothesis and a variance which does not. We shall discuss this difficulty more fully in 
the second volume. 
8.24. When the parameters of the population are not given a priori, we have the 
double problem of estimating the parameters from the sample and assigning probable 
limits to the estimates so obtained. We have already touched on some of the principles 
of estimation and shall develop the topic more systematically in due course. When we 
have obtained an estimate—itself a statistic—we soek its sampling distribution and 
therefrom can assign probable limits to the population value. A special class of cases arises 
when we can find a statistic whose sampling distribution depends on only one parameter 
of the population (as in the case of attributes). 
8.25. These latter types of problem permit of certain important' approximations, 
namely in the case when the sample is large. We saw in Chapter 7 that under very general 
conditions the sum of n independent variables, distributed in whatever form, tends to 
normality as n tends to infinity. Now many of the ordinary statistics in current use can 
202 RANDOM SAMPLING 
be expressed as the sum of variates, e.g. all the moments; and many others may also 
be shown to tend to normality for large samples. Thus we may approximate— 
(a) By taking a statistic, calculated from the sample as if it were a population, to be 
the estimate of the corresponding statistic in that population, e.g. the variance of the 
sample may be taken as an estimate of the variance of the population. 
(b) By calculating the mean and variance of the sampling distribution by using, 
instead of the unknown parameter values, the statistic values calculated according to (a). 
(c) By assuming that the distribution is normal and hence determining probabilities 
from the normal integral with the aid of the sampling mean and sampling variance (the 
latter being the square of the standard error). 
8.26. Just how large n must be for such approximations to be valid is not always 
easy to say. For some distributions, particularly that of the mean, quite a satisfactory 
approximation is given by low values of n, say n > 30. For others n has to be much higher 
before the approximation begins to give satisfactory results, e.g. for the product-moment 
correlation coefficient (below, 14.5) even values as high as 500 are not good enough. 
8.27. In the following three chapters we discuss the approximate and accurate 
methods for determining sampling distributions. Chapter 9 deals with large samples 
and is thus devoted mainly to methods for determining standard errors. Chapter 10 deals 
with methods for determining sampling distributions exactly. Chapter 11 discusses 
methods of approximating to sampling distributions by finding their lower moments. 
NOTES AND REFERENCES 
For some interesting discussions of problems of sampling generally, see Jensen (1926), 
Bowley (1926), Hilton (1924), Kiser (1934), Yates (1935) and Neyman (1934). For a 
discussion of random sampling, see Kendall and Babington Smith (1938 and 1939) and Kendall 
(1941). The various tables of random numbers are referred to in the Introduction. 
Bowley, A. L. (1926), " Measurement of the Precision attained in Sampling," j$ull. Int. 
Stat. Inst., 22, premier livre. 
Hilton, John (1924), " Enquiry by sample; an experiment and its results," Jour. Roy. 
Statist. Soc, 87, 544. 
Jensen, A. (1926), " Report on the representative method in statistics," Bull. Int. Stat. 
Inst., 22, premier livre. 
Kendall, M. G., and Babington Smith, B. (1938), " Randomness and random sampling 
numbers," Jour. Roy. Statist. Soc, 101, 147, and (1939) "Second paper on 
random sampling numbers," Supp. J.R. Statist. Soc, 6, 51. 
Kendall, M. G. (1941), "A theory of randomness," Biometrika, 32, 1. 
Kiser, C. V. (1934), " Pitfalls in sampling for population study," Jour. Amer. Stat. Assoc, 
29, 250. 
Neyman, J. (1934), " On two different aspects of the representative method," Jour. Roy. 
Statist. Soc, 97, 558. 
Yates, F. (1935)/ "Some examples of biased sampling," Ann. Eug&n., Lond., 6, 202. 
Yule, G. Udny (1927), " On reading a scale," Jour. Roy. Statist. Soc, 90t 570. 
1 i 
EXERCISES 
' 8.1. Of 10,000 babies born in a particular country 5100 are male. Taking this to be 
EXERCISES 203 
a random sample of the births in that country, show that it throws considerable doubt 
on the hypothesis that the sexes are born in equal proportions. 
Consider how far this conclusion would be modified if the sample consisted of 1000 
births, 510 of which were male. 
8.2. If the number of members of a population bearing an attribute 4 is relatively 
small, .show that the standard error of the number of A'b in the sample is the square root 
of that number. Show also that the number of A'b in the sample is an unbiased and 
.a maximum likelihood estimate of the parameter of the Poisson distribution expressing 
the distribution of the number of .4's in large samples from the population. 
8.3. By considering the hypergeometric distribution, show that if samples of n are 
drawn from a finite population of N without replacement, and a proportion m of that 
population bear an attribute A, then the standard error of the proportionp in the sample is 
Show also that p is an unbiased estimate of m. 
8.4. (TchebychefFs inequality). Show that for any distribution 
L(l-tyT<Ldr 
and hence that for any member drawn at random 
P(|a;|> aV>*) <—• 
<x. 
Show further that the variance of the sampling distribution of proportions bearing 
an attribute A in samples of n from a population of attributes is not greater than -r-. Hence 
the probability that an observed proportion p differs from the true proportion m by more 
than amount k is not greater than v—jra* 
(Thin gives us an exact result, no assumptions about the normality of the limiting 
form of the binomial or the use of estimates in calculating standard errors being involved. 
The limits arc, however, much too wide.) 
8.5. If a proportion w has to be estimated from a simple random sample with 
proportion p, and if/i&the prior probability of zn, then the posterior probability of m is, 
according to Bayes' theorem, proportional to 
Show that this is a maximum if 
1 df p—m 
f am tu(l — to) 
Hence, in general, as n increases, the solution tends to m = p, whatever the prior 
probability of w. In other words, the maximum likelihood estimate is an approximation to 
that given by Bayes' theorem as n tends to infinity, even if Bayes' postulate is not assumed. 
CHAPTER 9 
STANDARD ERRORS 
9.1. Towards the close of the last chapter we discussed the estimation of statistical 
parameters from large samples and the type of judgment of their reliability which depends- 
on the use of the standard error. It was remarked that, for large samples, an estimate 
of a parameter may be obtained by calculating from the sample values the value of the 
parameter in the sub-population composed by the sample; and it was established that 
for samples of n the standard error gives a valid measure of precision, provided that (a) the 
sampling distribution of the statistic under discussion approaches normality and (b) that ti is 
large in the sense there defined. It was also pointed out that a sufficiently accurate estimate 
of standard errors involving parent parameters could be obtained by using as the parameter 
values the corresponding statistics from the sample itself. 
Since the majority of statistics in current use do tend to normality^ the theory of large 
samples is, in the main, devoted to the determination of standard errors. In this cliapter 
we describe the principal methods available for the purpose, and incidentally derive formulae 
for the standard errors of the various statistics considered in previous chapters. To avoid 
the usual square roots associated wifch the standard error we shall write our results as 
sampling variances and covariances. Thus, for a statistic t we write the variance of its 
sampling distribution as var t. The covariance of the joint distribution of two statistics 
* t and u, that is, the first product-moment of their joint sampling distribution, is written 
cov (t, u). We shall also consider the distributions in large samples of some statistics which 
do not tend to normality. 
9.2. By definition, the rth moment of a statistic t, that is the rth moment of its 
sampling distribution, is the mean value of V taken over all possible samples, and may be 
written E(tT) (cf. 3.35). If the joint distribution of the variates xx . . . xn, from which 
t is calculated, is dF{xi . . . xn), then the rth moment of t is the integral of trd,F (considered 
as a function of the x's) over the domain of the as's. In particular, if the sample is simple 
and random and the parent distribution is dF, we have 
(.00 /.00 
E{tT) = ... V dFfa) . . . dF(xn). 
J —00 J —00 
We are particularly interested in this chapter in the first and second moments of t, 
that is, the mean and variance of its sampling distribution. It may be recalled that the 
mean value of a sum is the sum of the mean values amd that, if the variables are independent, 
the mean value of a product is the product of the mean values (3.36). These two results 
will be repeatedly required. 
Standard Errors of Moments 
9.3. In the first place we consider the standard errors of the wide class of statistics 
depending on the moments, including the mean, variance, the Pearson measures of skewness 
and kurtosis and the moments and cumulants themselves. 
The sampling distributions of moments tend to normality under Very general conditions 
in virtue of the Central Limit Theorem. In fact, if the parent distribution is represented 
204 
/ 
STANDARD ERRORS OF MOMENTS 205 
l / i\ w 
by f(x) dx, the distribution of the jth power of x, say y, is easily seen to be -f\yf )y 1 dy 
and the jth moment is the sum of n independent variates, each of wliich is distributed in 
that form. 
It is not so obvious that functions of the moments such as bt and bt (the sample values 
of the parameters fii and /?2) will tend to normality, and special investigations may have 
to be made for particular statistics. Even at the present time it is often assumed without 
proof that certain' statistics tend to normality, the feeling apparently being (so far as any 
feeling uprises into consciousness) that as most statistics do tend to normality the onus 
is on an objector to prove that any particular statistic does not. This is very dangerous 
to accurate inferential reasoning and the point is one to be borne in mind wherever a standard 
errot is used. 
On a similar point, it should also be remembered that some statistics tend to normality- 
more rapidly than others, and a given n may be large for some purposes but not for others. 
So far as it is possible to generalise with safety, we can usually (but not always) assume 
values of n greater than 500 to be large ; values greater than *100 are often great enough 
to be large for our purposes ; values below 100 are suspect in many instances ; and values 
below 30 are very rarely large. 
In the following we shall adopt the usual convention in regard to the distinction of 
parameters and statistics by writing Greek letters to represent the former and Roman 
letters to represent the latter. We have, then, for the rth moment-statistic m'r, 
corresponding to the rtn moment parameter jn'n 
n 
/=i 
and for the mean-moment 
lA 
mr=-2j(Xl~m'i)r- ...... (9-2) 
"/-! 
9.4. Consider now the mean value of m'r. Since the ar's are independent we have 
E{m'r) = -£E{xr) 
The sampling variance of m'r is, by definition, E(m'r — /*,'.)2 and thus 
2 
var 
K) = eQsw - /.;|s 
= hni Mxn} ■ - 2^;zk) + n v;2] 
= ^[{i7(^)}2]-^a 
= iBT{27(a,*) + £(x/xkny - f*'r\ 
IV 
the second summation extending over the n(n — 1) cases in whioh j ?±7c (permutations 
206 STANDARD ERRORS 
of j and ft thus being allowed). Since the *'s are independent the mean value of the product 
is the product of the mean values, and thus 
var K) = —2{nju'2r +-n{n - l)ft'r2} - /4a 
-jW-*1) <"••*) 
This is an exact result. . . . 
In a similar way, if we have two moments, m'q, mr, their sampling eovananee is given by 
oov (m'q, m'r) = E{{m'q - fiq){mr - fi'r)} 
- iO - «)$* - "$ 
= "(/4+r - /V*r)> (l)'"^ 
which reduces to (9.4) if q = r, as it must, for the firet product-moment of two identical 
variables is their variance. 
9.5. The formulae for moments about the mean are not so simple, for the mean itself 
is subject to sampling fluctuations. We have, in fact, 
E(mr) = -E{Z(x -mi)r} (iUl) 
Now putting r = 1 in (9.4) we find 
var (mi) =-(f*'2 - ^) 
1 
= -/*■ (!>-7) 
and thus the standard error of the mean is J—■ Consequently, if the distribution in 
anywhere near'normality, nearly all the values of x will lie within a range of the true value 
of order n~l. To order w~* we may then, taking an origin at the mean of the pandit 
population, neglect powers of m\ higher than the first, and we then obtain from (IMS) 
E{mr) = -E{E{xT - rm^-1)} 
ib 
= -eIex* - r-ExZx'-A 
n [ n J 
Now the second term in the expectation on the right will involve tho moments ft\fir^x and 
will vanish since we have ohosen our origin at the mean of the parent distribution \u\ ^ o). 
We shall then have, to order »""* 
■ ^K)=/*r, (0.8) 
STANDARD ERRORS OF MOMENTS 207 
a result which is not, like (9.3), exact, but is an approximation to order n~*. To this 
order we have 
v&t (mr) =—E(mr - firY 
fit 
= ~#KS)-/v> 
The expectations of other terms occurring in the squaring vanish, since they contain fii. 
1 y2 1 
The expectation of —j.—E{xi2x1^r~l) is of order -^ and is thus to be neglected. The 
lb 7% lb 
remaining terms give us 
Var («lr) = -(/*& - [ir* + ^ [*\-l - *rPr-\ Vr + l)- • ■ (9-9) 
Similarly it appears that 
0OV(Wr, mq) = -{{*r+Q ~ Wq + T(Ul* tlr-l /*ff-l ~ ^r-1 /*ff+l ~ Wr+1 P>q-l) ■ (9-10) 
Example 9.1 
From (9.7) we have 
var (w?.,) = —. 
Now, for the height distribution of Table 1.7 we'found (Examples 2.1 and 2.6) that 
m\ = 67-46 y/?na = 2-57. Suppose we regard this distribution as a simple random sample 
from the adult male inhabitants of the United Kingdom living at the time when the data 
were collected. What can we say about the mean of the population ? 
The standard error of the mean depends on [i%. This is an unknown quantity, but we 
may, in accordance with the general principles of large sample theory, use ma instead. 
We then find 
/ 2-57 
Standard error of m\ = / = 0-028 approximately. , 
Thus we can say that the population mean probably lies in the range of twice this amount 
on either side of the sample mean, i.e. in the range 67-46 ± 0-056, and very probably in 
thrice the range, i.e. 67-46 ± 0-084. Our estimate of the mean would almost certainly be 
less than a tenth of an inch in error. 
Example 9.2 
From equation (9.9) with r = 4 we find 
var (ra4) - -(/*„ — {4 — B/iKfi% -f lB^A**)- 
lb 
208 STANDARD ERRORS 
In Chapter 11 we shall show how to obtain this result by other methods of a more exact 
character and confirm that it is, in fact, exact to order n~1. 
Example 9.3 
To show that in samples.from a symmetrical population the first produot-moment 
between the mean and any mean-moment of even order vanishes to order w-1. 
We have, by definition 
K, mr) = i/[|^){W - ^r/"1)}}] 
cov 
= ±E{ZV") -\Z(x? xr1)], 
the other terms vanishing, since they involve the unit power of x, if we take an origin at 
the mean of the parent population, 
Now if r is even, /ur+1 and yar_i, being moments of odd order, will vanish for a 
symmetrical population and hence 
cov (wj, mr) = 0. 
In the language of the theory of correlation (Chapter 14) the mean and the even moment 
about the mean are uncorrected to order n~l. , ~ 
t 
Standard Errors of Functions of Moments 
9.6. From the expressions we have just derived for the sampling variances and 
covariances of moments, approximate expressions can be obtained for the sampling variances 
of functions of the moments. Suppose 0(m) is such a function. We have the functional 
relation in differences 
A<1> = ^-Am, + ^i-Amt -f . . . + 0{Am)*. . . . (0.11) 
drrii. 9m„ 
Now any variations in m due to fluctuations of sampling are of order n~i. To our 
approximation, therefore, we may neglect the terms of order (Am)* in (9.11), and the variation A(f> 
is then seen to be a linear function of the variations Am and is equivalent to an equation 
in differentials ; that is to say, since the m's are distributed normally in the limit, so will 
<j> be. We have, from (9.11), 
E{j>) = Z^-#(m,). 
dmt 
Hence, measuring from the means of the m's and cf>, we find, squaring and taking mean values, 
r w=*{(S)S var (m''}+zi& & °°T (m" m4 ■ • (9-12) 
the first summation extending over all the m's appearing in 0, and the second over all 
m's such that j ?£ h. 
Similarly, for two functions 01} 0a, we have 
STANDARD ERRORS OF FUNCTIONS OF MOMENTS 209 
Example 9.4 
To find the sampling variance of the fourth cumulant. We have 
, Ki = fAi — 3^2 
'■ d,Ki = dpi — 6fiadfia. 
Henoe, squaring and taking mean values, 
var (/c4) =„var (^4) — 12/^ cov (,m4, fiz) -f 36^| var (fit). 
Making the appropriate substitutions from (9.9) and (9.10) we have 
var. (k4) = -{(it - fil + 16^* _ 8 _ 12^B(^, - p4p, - 4^§) + 36^1(p4 - /*j)} 
= -{^a — 12/4a/*a — 8/*B/u8 — ^f + 48^4/m| + 64/^a — 36/4}. 
For a normal parent, /*4 = 3a4, fz0 = 15<t°, /^b = lOScr8 and we have 
24 
var (/c4) = —0s. 
71 ' 
Example 9.5 
To find the sampling variance of the coefficient of variation 
jt _ 100\/raa 
V ; • 
Taking logarithms and then differentials we have 
dV __ dma dm[ 
V 2ma m'i' 
Whence, squaring and taking mean values, 
var V var ma 
Fa 4wi2 
L, cov (m.mi) + ™g - V^j=i3 - J*r + ^Y 
"a 
To our order of approximation we may write //r = mr and find 
var v = lYe^ - -A + 4\Y 
» \ 4/^2 /"2/"i - A*i V 
For a normal parent this gives (fiA = 3//|, /^ = 0) ; 
var 7 - t(s + Si) " ^i1 + To*) 
72 
= — approximately. 
9.7. On the above principles the standard errors of the more usual functions of 
moments, such as the measures of skewness and kurtosis, have been worked out and 
tabulated (see Tables for Statisticians and Biometricians and the references at the end of 
the chapter). 
In applications of results derived by the foregoing methods a few points are to be noted : 
(a) The sampling variances are to be used only when the statistic under consideration 
is calculated from the moments. For instance, if the standard deviation of a normal 
curve is 
A.S. 
estimated by taking / [ - j times the mean deviation of the sample, instead of 
210 STANDAKD ERRORS 
the more usual root-mean-square, the formula var (a) = —^f" ^vaNo from (!U>) is 
not aT>T>licable (see below, 9.11). 
(b) From (9.4) and (9.9) it will be seen that the sampling variance of a moment depends 
on the moment of twice the order, i.e. becomes very large for higher momenta, oven when 
n is large. This is the reason why such moments have very limited practical application. 
(c) Some measures calculated from the moments tend to normality very slowly. 
V&i or h (the sample values of V& or &) are cases in point, and more refined methods 
which we discuss in Chapter 11 are preferable to the use of the standard error. r 
(d) The order of the approximation makes it necessary to exorcise care in the 
neighbourhood of vanishing values of standard errors. For instance, if the coefficient of variation 
V = 0 in a sample, the formula of Example 9.5 would give var V -- 0. But it does not, 
of course, follow that there is no variation at all in the population, though none exists iu 
the samule and the presence of variation in the parent will be unlikely if the sample is at 
p 
all large. When 7 = 0 the quantities neglected in our approximation giving var J' ~- v) 
become of some relative importance, though they are still small. 
(e) It is interesting to compare the sampling fluctuations, as expressed in the sampling 
variance, with Sheppard's corrections to the moments. Writing temporarily ,ij for the 
uncorrected variance in the sample, sf f°r the corrected variance, we have 
si , 1 A2 
_! = i _ 
a? 12 sf ' 
where h is the interval width. For many practical cases, if d is the number of intervals, 
dh is about equal to f^, and thus 
sf d* 
— = 1 — —^approximately. 
For a normal population we have 
da = dfit 
2V^ 
i 
and hence var <r = J_ var ua 
_ l*« — fA 
= £? = ?L 
2n 2n 
co. Th^,]fn is' 8av> 1000' the standard error of a is about 0-0224 a =-- 2-24 nor cent of a 
Sheppard ..correct™ in a case where d = 20 is only 0-375 per cent, of ,1( L. onlv about 
th^OOO m o^f ™% ^ I8 ^ ^ t0 ^^ thG COTreCti0™' «™ ^ » ^« 
than 1000 m order to avoid systematic error: but the correction should not ho mis- 
. Similar considerations apply a fortiori to the higher momenta. 
STANDARD ERRORS OP BIVARIATE MOMENTS 211 
Standard Error of Bivariate Moments 
9.8. Extensions of the above formulae to the bivariate case are made without difficulty, 
only slightly more complicated algebra being involved. The reader will be able to verify 
the following formulae for himself: 
™r K-, 8) = ~(/4r, 2s - t*r,2S) (9-14) 
cov(ml.t8,mUiV) = ^(fir+UiB+v~ firi8 fiUtV) (9.15) 
var {mr> 8) = -{{i2fi 28 - fi\8 + r*p2t 0 p*r_Ua + s2/zQt 2 fi\a^ 
+ 2^1.1/«r-l,s/*r,s-l - 2rPr+l.aPr-l,8 ~ 2^r, 8+1 Vr, s-l) ■ (9-16) 
coy (mrt a, mu> v) = -(fir+u< 8+v ~ ,«r, « /*«, t> + r^«> Pr-1. « ^«-i. ° 
+ svfiQtit ptt a_j ^tt> „_! + rvplt! ^r_1( a AV«-i, 
+ -S^l, l ,",., s-l /*m-1, d ~ tt/*r+l, s J^w-l, v 
~~ ^r, s+1 i"u, u-1 ~~~ *7*r-l, a /"u+1, v "~ B^r, s+1 /"«, t>-l) • (9-1*7) 
Example 9.6 
The coefficient of correlation is defined by 
m, i 
r = 
•\/(maom02) 
We have - = ^ - ^m'° - ^m°a, 
r mxl ' mao woa* 
mu 1 / v var (m.n) , ,var (ra20) cov (mu, mi0) , . .. . , 
Thus — var (r) = i—1-1-' + J V-=l' — -—A_iii—!»' -f. similar terms, 
from which, substituting appropriate values from (9.16) and (9.17) and writing fiT8 for 
mra in the result, we have 
var 
(r) = ^!fe + j£|! + j£l4 + *_£■!_ - JL3I_ _ Jiil. \ 
n V/^n /*20 ^02 i"aoA*oa /^n^ao /^ii/^ob/ 
p being the same function of the /z's as r is of the m's. For the bivariate normal distribution 
the substitution of values of Example 3.15 gives 
var (r) = - (1 — p2)2. 
n r 
The use of the standard error to test the significance of the correlation coefficient is 
not, however, to be recommended. 
8ki7ulard Errors of Quantiles 
9.9. Among the various quantities measuring location and dispersion which we 
considered in Chapter 2 there was one group, namely the quantiles, which are not symmetric 
functions of the observations and whose sampling variances cannot accordingly be 
determined by the above methods. We proceed to consider them now. 
Suppose the parent distribution is represented by dF(x) =f{x) dx. The probability that, 
of a sample of n, (I -* 1) fall below a value x1} one falls in the range xx ± \ dxt and the 
remaining (n — I) fall above xx is proportional to 
Fix,)1'1 f(x,) dxx{l - F(xl)}^ = JV-i(l - F,)n-1 dFu , . . (9.18) 
212 STANDARD ERRORS 
where Fx = F(xx). This expression is accordingly the distribution function of xlt the 
member of the sample below which a proportion — of the members fall, i.e. the Ith quantile. 
Put I — nq 
so that n — I = n(l — q) 
=*np, say. 
The distribution (9.18) has a modal value given by differentiating the frequency function with 
respect to xly i.e. (taking logarithms first) by 
(Z-l)t + (^_Z)(_A_+^ = 0 . . .(9.19) 
f 
this equation being satisfied by the modal value x. Now for large n, the factor -j will in 
general be small compared with the other terms in (9.19), I and n — I being large. We may 
therefore neglect it, and (9.19) becomes, to order n~1f 
F 1 -F 
or F(x) = q. 
This is in accordance with our general assumptions. To order w-1 the quantile of the sample 
is the quantile of the parent. 
Now let us investigate the distribution (9.19) in the neighbourhood of the modal value. 
Put 
F, = q + £. 
(9.18) becomes (neglecting constants) 
(q + £)«*(£ - f)»». 
Taking logarithms and expanding we have, except for constants, 
nq log (1 + -J + wp log (l - -J 
—*(+!-$••■)+■*(-!-ij!---) 
n£a 
= — -^- + terms of order £3 and higher degree in £. 
2pq 
Now for large samples £ will be small compared with q, and we neglect the terms of higher 
order. Thus the distribution of £ is 
or, evaluating the necessary constant by integration, 
dFssB n7-\exp(:T^)^ .... (9.20) 
showing that £ is in the limit distributed normally with variance 
var (£) = ?i (9.21) 
n 
STANDARD ERRORS OF QUANTILES 213 
This is the variance of f, which is a proportion. To find the variance of xx we note that 
d£ = dFx = fidxt and hence that 
var (xj) = ^. (9.22). 
In practice this formula is often applied to grouped frequency-distributions, arid in such 
applications it is to be remembered that/!, the ordinate of the parent, is to be taken as the 
frequency per unit interval at xlt this being the beet estimate of the ordinate. 
Example 9.7 
1 
If Xi is the median, p = q = | and we have var (median) 
where/i is the median ordinate. For instance, if the parent population is normal, the median 
ordinate is (from Appendix Table 1) - 0-39894, <r2 being the variance of the parent. Hence 
a 
the standard error of the median is 
y'n 2 x 0-39894 
a 
1-2533 
■\/n 
The standard error of the mean in samples of n from a normal population is —r-, which 
is thus considerably smaller than the standard error of the median. 
9.10. To find the covariance of two quantiles we generalise equation (9.18). If we 
have a random sample of n individuals the probability that (I — 1) lie below xlt one lies at 
#i ± \dxx, (n — Z — .m) lie between xx and xs, one at x2 ± \dx2, and the remaining (m — I) 
above x2 is 
dF oc Fj-^Fi - Fx)n-l-m(l - F%)m~HFl dF2, . . ■. (9.23) 
where Fx = F{xl)i Fs = F(xt). 
We put I = qxn 
m = p2n 
and find, for the equations'giving the modal values corresponding to (9.19), 
?i (?2 - Qi) 
Pi Ft - F1 
0 
?i ~-3l - p* = o 
F2 - Fx 1 - F% 
giving, for the limiting modal values, 
F(£2) = q2J < ■ j 
fix) 
The conditions as to the relative smallness of -s~ are satisfied in any ordinary case. Now 
f{x) 
put 
Fi = <7i 4- £ i 
F* = ga 4- £». 
214 STANDARD ERRORS 
The joint distribution of £i.and |a then becomes 
dF oc (Ql + f^fo, - qx + f, - ^-^(p, ~ *■) p,n^i #,. 
On proceeding as in the previous section, taking logarithms, expanding and neglecting terms 
in |3 and higher, we find ultimately 
tWocexp/- —* -xfe?-2^a+^l)W^a. . . (9.25) 
Thus the joint distribution of ^ and |a tends to the bivariate normal form, and on comparing 
(9.25) with the canonical form (Example 3.15) we see that 
1 __ «(79 
(1 - p2) var (£0 ~ (ja - q^q, 
1 ' _ npx 
(1 - p2) var (fa) ~* ((zT^iJiTa 
p2 p n 
(1 - p2) cov fof.) (1 - p2)V>ar (fj var (£,)} (ff. - ff0' 
whence it is easy to find 
var (£0 
var (|s) 
cov (f!, £2) 
71 
2>a?a 
7?. 
_Mi 
71 
(9.26) 
The asymmetiy of the result for the covariance is due to the fact that pa relates necessarily 
to the upper quantile. For the corresponding expression in xv and xt we have 
cov {xlt xt) = 2fi}- (9.27) 
With equations (9.26) and (9.27) we can find expressions for the variances of the quantile 
range and similar statistics. 
Example 9.8 
The variance of the difference d of two quantiles at xL and .ra is given by 
■ dd = dxx — dx2, 
var (<3) = var (xt) + var (#2) — 2 cov (j^, xa) 
»l ft ft Af. J 
When the quantiles are the two quartiles, pa = g^ = J, ^i = <Za = h and for the variance 
of the semi-interquartile range we have 
var^.q.^-^-^J, 
where/i,/a, are the frequencies per unit interval at the two quartiles,/a relating to the upper 
quartile. As fu /a have to be estimated from the sample, we- may also write 
var (s.i.q.) = ——( — + —b ' ), 
where glt gx are the actual sample frequencies at the quartiles and <r2 is the sample variance. 
STANDARD ERROR OF THE MEAN DEVIATION 215 
For instance, if the parent distribution is normal, gt = ga and we find 
var (s.i.q.) = ——- 
From the tables the deviate corresponding to the quartile is 0-6746 and the ordinate at this 
point, <7i = 0-3178, so that the standard error of the semi-interquartile range is 
VnA X 0-3178 
= 0-7867-% 
yn 
9.11. In amplification of the point mentioned in 9.7 (a) it is worth while stressing 
again the fact that a standard error is related to the way in which a parameter is estimated. 
For instance, the standard deviation of a normal curve can be estimated from a sample in 
several ways : from the second moment; by taking / (- ) times the mean deviation ; by 
taking ■ - times the semi-interquartile range ; and so on. Each method will have 
its appropriate standard error, that for the first, for example, being . ., and that for 
I* 6495a1 
the third —- - '• ■. At a later stage considerations such as this will lead us to the inquiry, 
what is the estimate, if any, with the minimum sampling variance ? For present purposes 
it is enough to note the importance of not using a quoted formula without reference to the 
method of estimation of the parameter conoerned. 
9.12. The methods we have developed provide the standard errors for large samples 
of most of the measures of location and dispersion and the measures introduced in Chapters 2 
and 3. There remain a few on which we have not yet touched, viz. the mean deviation, 
■Gini's coefficient of mean difference, and the range. We consider them briefly in turn. 
.Standard Error of the Mean Deviation 
9.13. The mean deviation, as was pointed out in Chapter 2, is relatively speaking 
a complicated function, and the mathematical difficulties attendant on absolute values are 
well illustrated in discussions of its sampling variance. In fact, no general discussion of the 
•sampling distribution appears to have been undertaken. The following exact value of the 
sampling variance in samples from a normal population was discovered by Helmert in 1876 
and rediscovered by Fisher in 1920. 
var 
(m.d.) = ? <l_iW* -f VMti - 2)} - n + sin-i-i->) 
n n2 \2 n — 1/ 
ff2/ 2\ 
r^ _[ l —-) for large n. (9.28) 
n\ n) 
The proof follows the general methods described in the next chapter. It is quoted here 
for the sake of completeness. 
216 STANDARD ERRORS 
Standard Error of the Mean Difference 
9.14. Nair (1936) has given a general expression for the standard error of Gini's mean 
difference without repetition. In the manner of 2.24 it is easy to see that the 
coefficient may be written 
Ax = , 2 ix{2t7-(7t + l)7} . . . .(9.29) 
n(n — 1) 
where U = / j (jxj) 
n 
y - Z*» 
and we write n in preference to N for the number of observations, since we are dealing 
with a sample. 
In our usual notation, the probability that the ,/th observation in order of magnitude 
in a series of n observations has value in the range x ± \dx is 
dF = r—rS ^ Ff~xQ- ~ filT1 
Mite - l)!(n -j)\ 
Hence the mean value of U is given by 
+ (n- \)F zJSLpjW jw-«(i _F)n-A 
«»f xdF{\ + (n - \)F) (9.30) 
J-00 
Similarly 
Thus 
S(V) = nixdF. ........ (9.31) 
E{Al) = nW=T){2E(U) " (n + 1)J&(7)} 
jjz(2.F - 1) 
dF. 
In the same way (but we omit the details) Nair finds 
4 
F(Al) = ^—^{1, + 2(n - 2)I2) v . (9.32) 
STANDARD ERROR OF THE MEAN DIFFERENCE 217 
where 
I1=\ xz{(» -.1) - 4(% -2)^ + 4(91 -2)F2}dF 
J — 00 
7a = I x2 dJPjp xx dFx{(n - 3) - 2(n - 5)i?\ - 2(w - l).Fa + 4(» — 3)^ Ft) 
J —00 J —00 
and finally 
v&T {Ax) = E{A\) - {E{Ar)}2. . x . . .(9.33) 
For three particular cases these integrals are worked out, giving : 
Normal Parent, 
1 zJUl. 
dF = —7= e 2<r* dx, — oo < x < co 
o"V2ot 
jw = -^ 
... 4o-2 f» + 1 , 2(9i-2) V3 2(29i'-3)1 
var (A A = — -J —I- h — — ' > , . • (9.34) 
v y n(n — 1)\ 3 sr re J v ' 
/^~i 
ct_2 
91 
(0-8068)2 (9.35) 
Exponential Parent: 
] -X 
dF = - e <j dx, 0 < x < oo 
^(JO = <r (9.36) 
/. , o 2(29i — 1) 
™(^') = *%rri) (9'37) 
~S5"- •■••••• (9-38) 
Rectangular Parent: 
dF = rdx 0 <x <k 
k 
E{A,) =ik (9.39) 
Tar(J1)-?.r4±-,-x (9-4°) 
9 5n(w — 1) 
^7— . ....... (9.41) 
4591 x ' 
9.15. We now turn to consider some statistics which are peculiar in more ways than 
one—the extreme values of a sample (or, more generally, the mth value from the top or the 
bottom of a sample) and the range. One of the unusual features of the distribution of mth 
values is that as n increases it diverges more and more from normality ; and it seems doubtful 
whether the distribution of range tends to any limit at all—certainly it does not tend to 
normality in all cases. 
A further difference between the quantities we are now considering and the others we have 
218 STANDARD ERRORS 
already discussed is that mth values and range in the sample are not used to estimate. » 
values and range in the population. In fact most of the results wo .shall obtain relate 
parents which have an -infinite range. What, then, is the use of those statistics'( I 
answer is that they may provide an estimate of parent parameters which do exist. I' 
instance, an estimate of the variance of a normal population is given by dividing the, nam| 
range w by a constant dn depending on the number in the sample. This estimate, thou, 
not so accurate as some (in the sense that its sampling variance is not so small), is extremt 
easy to calculate and is often useful. We wish, therefore, to know its sampling variant 
that is to say the sampling variance of the range. 
Distribution of mth Values 
9.16. We consider first of all the distribution of with values from the top, that f 
mth values from the bottom being similar. In particular, m may bo unity, in which euse v 
get the greatest member of a sample. 
Quantiles are special cases of this class of statistic, the ratio mjn remaining finite as 
tends to infinity. ' In the case we now discuss m remains finite, so that the ratio m/n tends i 
aero. 
■ The distribution of mth values from the top is, as in equation (9.18), 
dF oc F1n~m{l - F^'1 dFv . . . . (iu: 
When the form of Fx is known, this equation is sometimes capable of exact solution, as in 11 
following example. 
Fxample 9.9 
Consider the rectangular distribution dF = dx, 0 <aj < 1. Hove F(.r) -.■=■. .>• and Mi 
distribution of the mth value from the top is 
dF oc xn~m{\ - x)m-* dx, 
the Pearson Type I curve. We have, for the first moment, 
n — m -f 1 . m 
and for the variance 
fX, = — _ A 
n ■+- 1 n + 1 
_ (n — m + 1)(2» — m) 
^~ (n + l)(n + 2) ' 
but this sampling variance cannot be used in the ordinary way if m. in finite, for the eurv* 
does not tend to normality. However, we may easily obtain exact values for the 
probabilities associated with the mth values, from the integrals of the Type 1 curve. Tn fact, Mm 
probability that a given value will not be attained or exceeded in, in the u.sual notation 
Ix(n - m + 1, m). 
9.17. From this point our discussion of the limiting form of (9.42) as n tends to infinity 
is confined to the case wherein the parent population is a continuous frequency-distribution 
of unlimited range of the exponential type, i.e. such that it tends to zero with large x as fast 
bs or faster than dF = fl-N ^ and that ^log/(z) j exceeds some fixed number a* x 
tends to infinity The normal curve obeys this criterion, which implies, among other thinim, 
that all moments exist. , e 
DISTRIBUTION OF mTH VALUES 219 
For the mode of (9.42) we have, as in (9.19), 
For large n and finite m the mode xx will be a large value, and both/^ and 1 — Fx tend to zero. 
Accordingly we may put 
dp 
r,_ cTxh *■ 
K-fr_rd i-'. 
in accordance with the rule known as L'JEEopital's. Hence 
In — wi)Cj- — m—A— = 0 
v 'Fl 1 — Ft 
m 
Ftf) = 1 - _. 
n 
Now expand Fx in the neighbourhood of £ by Taylor's theorem. We get 
F(x) = F(A) + x-=j$fW + ^~^f'^) + • • • 
= 1 —(a? — aj)—rm — — -r^- —»{f(%)r + • • • 
—/* — f2 
(the last term in virtue of /j ^ _ i, = —— in the neighbourhood of the mode) 
m 
= 1 — — exp i — {x — x)—f(£) ^approximately . . . (9.43) 
771 
= 1 -- e-y»>, say. (9.44) 
The distribution of the with value from the top may be written, from (9.42), 
dFm oc (± - 1 j d{F») 
*YYh 
To our approximation, from (9.44), since — is small, 
(y-1)"1"^":"^"1) 
w—1 
Thus dFm oc exp (- mym - me-2'"1) <%m 
■and, on evaluating the constants by integration, we get 
dFm = ,m _ 1)lexP (- W^m ~ me-v-)dym. . . . (9.45) 
220 
STANDARD ERRORS 
The new variable ym is defined in terms of x — x by 
, ^nf(cS) 
yn = (x- x)^-l. 
m 
In a similar way, for the rath value from the bottom we find 
ram 
dF = (m _ !)i exP (m*& - m&nV) dy> - - (9-46) 
„j/ being written for the variable defined by 
f = V. 
n , 
In particular, for the extremes (m =1) we have 
dF = exp (- y — e~v) dy (top value) . (9.47) 
dF = exp (y — ev) dy (bottom value) . ... . (9.48) 
9.18. These unusual limiting forms, which are due to Gumbel (1934), the extreme cases 
being due to Fisher ard Tippett (1928), are very far from normal for moderate or low values 
of m. For the moments of (9.45) we have (omitting the suffix of y for convenience) 
m <n r00 - 
t~t 
dt 
Put e-& = -. We get 
m 
= - log m + ^{log ^ («»)} 
= - logm + y - ZjAW (9,49) 
where y is Buler's constant. For the rth moment about the mean we have 
(W2. — Ij! J o 
r(m) [_dqr J0„o 
These formulae have been worked out further by Gumbel, from whose numerical results the 
following are chosen :— 
m 
1 
3 
5 
10 
Mean 
0-577 
0-176 
0-103 
0-051 
Pi 
1-139 
0-621 
0-468 
0-324 
' 02-3 
2-400 
0-763 
0-437 
0-212 
These figures, which relate to the distribution from the top, show clearly that the limiting 
distribution is far from normal. The distribution from the bottom is similar, odd moments 
including the mean having the same magnitude but opposite sign, even moments being the 
same. 
41 
DISTRIBUTION OF mTH VALUES 
221 
Moreover, the limiting forms (9.45) and (9.46) are reached extremely slowly, Fisher and 
Tippett (1928) have shown in the case m = 1 that they do not provide a very satisfactory 
approximation for values ofn less than 1012. For practical purposes, therefore, there is still 
no adequate general approximate form for the distribution of mth values. 
9.19. The casern = 1, corresponding to the extremes of the sample, has, however, 
been studied in more detail. In this case equation (9.42) becomes 
dF = — Fxn dxx. 
dxx 
By using the published tables of the normal integral Flf Tippett (1925) has evaluated F for 
values of n up to 1000, and given diagrams yielding the variances, and ^x and fi9, which are 
reproduced in Tables for Statisticians and Biometricians, Part II. The following values are 
quoted from his results :— 
n 
2 
5 
10 
100 
500 
1000 
Mean 
0-564 
1-163 
1-539 
2-508 
3037 
3-241 
Standard Deviation 
0-826 
0-669 
0-587 
0-429 
0-370 
0-351 
Pi 
0019 
0092 
6-168 
0-429 
0-570 
0-613 
A 
3-062 
3-202 
3-331 
3-765 
4003 
' 4-088 
The values of ^x and /92 illustrate the point that as n increases, the distribution of the extreme 
value diverges more and more from the normal form. 
The limiting values as n —>■ oo can be derived by the use of characteristic functions. 
In fact, we have for the distribution of the top value, 
j>{t) = [ eixt exp {— x — e~x)dx, 
J —oo 
which, on substituting e~x = f, gives 
0(0 - j"V<i«-'# 
= T(l - it). 
Hence 
Kl(U) + ^~. + • - . = log <f>(t) = log r(l - it) 
Thus /cx = fi\ = y 
n" 
*, = «,=£,= _ = 1-644934 
^3 = 2£, = 2-404114 
uA - 3rf = 6#4 = ~ = 6-493939 
15 
whence & = 1-299 
0, = 5-4. 
* Cf. Edwards, Integral Calculus, vol. 2, article 916. Sr here is / ^ 
222 
STANDARD ERRORS 
These are evidently far from equal to the values for n = 1000 given above. Clearly the 
limiting form is an inadequate approximation for values of n much higher than 1000. 
9.20. The problem of bridging the gap between Tippett's values and the limiting 
form has been considered by Fisher and Tippett (1928), and the argument which they 
employ is interesting. Concentrating for a moment on the upper value, we note that the 
upper member of a sample of Jpn members is the upper member of a sample of Jc of upper 
members of samples of n. Both distributions will tend to the same limiting form, if it 
exists ; and consequently the limiting value must be such that the extreme member of a 
sample of n from it must itself have that distribution. That is to, say, if F is the 
probability of an observation being less than x, 
F»(x) = F(anx + bn) (9.51) 
where an and bn are functions of n. 
It may be shown from this equation that F must be one of three forms:— 
dF = exp(— x — e~x)dx (9.52) 
dF — ZZ+iexV(—x~A)dx (9.53) 
dF =A(-x)A-1 exp { — {-x)A}dx .... (9.64) 
The first we have already reached. The second and third arise if the original distribution, 
instead of tending to infinity exponentially, tends less rapidly such that 
lim (1 — F)x^ exists and is not zero. 
The distribution (9.54) itself has (9.52) as a limiting form as A tends to infinity. It 
has therefore been proposed as a " penultimate " form, to bridge the gap between n = 1000 
and n = 1012, which is apparently the first point at which the ultimate form provides a 
reasonable approximation. For the penultimate form we have 
i\= A(- x)A-lafe~{-x)Adx 
J —oo 
and on putting 
x = U 
= (-iyr(i + 0 
The following values illustrate the relationship between the known form (n = 500, 1000) 
and the penultimate form : 
• 
1 
— * 
A 
0-0768 
0-0845 . 
n 
1000 
600 
Standard Deviation 
Penultimate 
0-3433 
0-3604 
Actual 
0-3514 
0-3704 
Pi 
Penultimate 
0-548 
0-498 
Actual 
0-618 
0-570 
/Ja 
Penultimate 
3-852 
3-751 
Actual 
4088 
4-003 
DISTRIBUTION OF RANGE 
223 
Distribution of Range 
9.21. The range is the difference of the highest and the lowest value of a sample, and 
the simultaneous distribution of top and bottom values is, from (9.23), 
dF oc (Fl- Ft)71-2 dFx dF2 
' =*n(n - 1)(F1 - F2)n~2 dFx dF2. . . .(9.65) 
The distribution function of the range w is then given by integrating this distribution over 
values of F1 and F% such that xe — ^ < w. So far as I am aware, it is not known whether 
limiting forms of this distribution exist or what they are. It is, however, evident that 
for large n the range is also large^ and it seems doubtful whether the difference of two 
variates which (for an unlimited curve) tend to -f oo and — oo respectively has any general 
limiting form. In any case one would suspect that the limiting form is reached slowly. 
For particular cases equation (9.56) is soluble explicitly. The normal oase has been 
fairly completely studied by Tippett.(1925) and E. S. Pearson (1926 and 1932). Tippett 
found the first four moments of the distribution of the range, tabulated the mean values 
for values of n up to 1000 and gave a diagram for deteimming standard errors. (These 
tables-and diagram are reproduced in Tables for /Statisticians and Biometricians, Part II.) 
Briefly, his approach is as follows :— 
From (9.55) we have, for the mean range JE(w), 
B(w) = n(n - l)f dPA'1 {Fx - Ft)*-\xx - x%)dF%. . . (9.56) 
J —oo J — oo 
On expanding (Fj_ — F%)n~2 we ge,t terms under the second integral sign like 
1 f^ ' Us+1 
Then *<•) = »|g__^L_gr)^F^-s Us+l *,,. 
B„t £ 7,.-- uM «■, - l?;^£±"]"_m - --^- af_m *.->- r>™ *» 
Hence 
oo 
{1 - (1 - Fj* - Fj"} dxx (9.57) 
— oo 
In a similar way it is found that 
var (w) = 2f" P {1 - Ff - (1 - Fz)n - (Fx - Ft)« f dxl dx2 
J —oo J —oo 
- \E{w))2 (9.58) 
- 
224 STANDARD ERRORS 
This equation was used by Tippett to obtain values of the variance for n up to 1000. The 
following values illustrate the general behaviour of the distribution :— 
n 
2 
10 
100 
500 
000 
Standard Deviation 
0-853 
0-797 
0-605 
0-524 
0-497 
Pi 
(approximate) 
0-99 
016 
„0-21 
0-29 
0-31 
0. 
(approximate) 
3-87 
3-15 
3-38 
3-50 
3-54 
Again it would appear that as n increases, the distribution of range diverges more and more 
from the normal form. 
The distribution function of the range in normal samples has recently been tabulated 
by E. S. Pearson and Hartley (1942). 
List of Standard Errors of Commonly Occurring Statistics 
9.22. In view of the general utility of the standard error it may be convenient to 
bring together at this point for reference a number of sampling variances and other results. 
Some of these have already been obtained in this chapter ; others are direct consequences 
of the formulae or methods developed; and some will be proved later in the book. 
Mean, var (m\) = — = — where a is the standard deviation of the parent. This 
v u n n 
is true in particular for a normal parent. The mean is always estimated from the mean 
of the sample. 
Variance, var (ma) = — —. For the normal parent var (ma) = —. Tables 
n n 
are given for this case in T.S.B. I *. These results are appropriate to the case where the 
variance is estimated from the sample variance. For numerical results for other cases 
see Davies and E. S. Pearson (1934). 
I II lfi\ Q-2 
Standard Deviation, var (s) == ^-~——. For normal parent var (s) = —. These 
are the values for estimates from the square root of the sample variance. See previous 
note on variance. 
Third Moment about the Mean, var (m3) = ^6 -/4 - W„ + V^ For normal 
v ' n 
parent var (m3) = —. The third and higher moments are always estimated from the 
moments of the sample. 
Fourth Moment about the Mean, var (in4) = (/"B ~ A ~ ^^ + 16^a^. For 
n 
normal parent var (m4) = . 
Coefficient of Variation, var (V) = —( A 2 + ^h —> )• For normal parent 
n \ 4^2 fix IHP1J 
72 
var [V) = — approximately. Tables given in T.S.B. I. 
* An abbreviation for Tables for Statisticians and Biometricians, Part I. 
STANDARD ERRORS OF COMMONLY OCCURRING STATISTICS 225 
0t. var (0X) = r v ' (-L-1 Z2-± Z2-1 —'. For normal parent var 
no 
(0X) = —• Tables given in T.S.B. I. The distribution is fairly skew for moderately large 
7b 
n and the methods of Chapter 11 provide better tests of 0X as a measure of departure from 
normality. See 11.23. (The 0's are defined in equation (3.65).) 
0a. var (0.) - <*■ - ^ + ^ - ^ 16'A - ^ + ^ Tables given in 
T.S.B. I. 
Pearson Measure of Skewness (Equation (3.64)). Tables given in T.S.B. I. Probably 
skew for moderate n. See note on 0X. 
Pearson Mode. Formulae and tables given in Yasukawa (1926), the results of oourse 
being only applicable to modes calculated from the Pearson formula (equation (3.62)). 
Distribution may be skew for moderate n. 
Coefficient of Contingency. See 13.14. 
Coefficient of Association. See 13.8. 
Tetraclboric r. See 14.28. 
Mean Deviation. General formulae not known. See 9.13. For normal parent 
o-V 2\ 
var (m.d.) = — ( 1 J. 
x n\ Tt) 
(0-8068)2<r2 
Oini's Mea7i Difference. See 9.14. For normal case var {Ax) = * . 
Media7b. var (ma) =-A « "where y0 is the modian ordinate of the sample. For 
N e 4n?/o4 
normal parent var (me) = ---'---'-—. For small samples from normal population, tables 
and formulae given in Hojo (1931). Results to higher order in n given by K. Pearson (1931). 
Quartiles. var (Q) = -,-—r where y is ordinate at the quartile concerned. For normal 
\ 'J I 
parent, var (Q) = — — —. Results for small samples from normal population given in 
Hojo (1931). 
Semi-interquartile range, var (s.i.q.) = j-fyjf/a + ^""a ~~ ^7) where 2/i> V*> are the 
, . s (0-78t)7)2ffa 
quartile ordinates. For normal parent var (s.i.q.) = . 
Deciles. For the normal parent, variances are 
(l-2«80)«tra 
for dccilea 4, 6 
'^ 7 
n 
(l-3180)aff* 
n 
>> 
i> 
2 8 (1,428H>ag2 
"' 71 
(l-7()94)aor2 
n 
A.S. 
Q 
226 STANDARD ERRORS 
Range. See 9.21. ,^ — p2.)2 
Correlation Coefficient. See 14.10. For normal case var (r) = ----- . But it 
is better to use Fisher's transformation (14.18) or the Tables by David (HUW)^ ^ 
Coefficient of Regression. See 14.10 and 14.11. For normal case var (b,) - 
(T-N, 
Standard Errors, of Sums and Differences 
9.23. Suppose we have two variables xu xa, which may or may not bo independent. 
We have, if z is their sum, 
• E(z) "= #(#0 + itffc,). 
n 
or the mean of z is the sum of the means of xx and xz. If then wo measure a^ and :r, about 
their respective means, the mean of z is zero and thus 
var z = E(z2) = E{xx + a;B)2 
= E{x\) + 2E(x1xg) + #(*§) 
= var xx + 2 cov (j^, #2) -(- var ,ta. . (N..V.I) 
Similarly for the difference of two variables we have 
var z = var a:i — 2 cov (xx, x&) + var .r2.... (iUKi) 
In particular, if a^ and a;2 are independent their covariaiico vanishes, for it becomes 
the product of the two means, each of which is zero. In thin important caw we have, for 
the sum, 
var (xx + xz). = var xx + var x2 .... (li.iil) 
and for the difference 
var (xx — x2) = var xx + var xs. .... (lUii!) 
These results are of fundamental importance : the variance of the sum or diiWencn 
of two independent random variables is the sum of their variances. (Generally if 
z = axxx ~f~ a%x% ~f~ . . . a^lxn 
and the n variables are independent, 
var z = a\ var xx + a\ var x2 + . . . ajj var xtl. . . . (<).(>:$) 
In particular we have, for the sampling variance of the difference of the means of two 
independent samples, say m\ and p[, 
»"i -Pi =— +—?, (».«.n 
nx wa 
^a and o, being the respective variances and nx n2 the respective nmnbers in the samples. 
Example 9.10 
A random sample of 1,000 men from the North of England shows their mean wage* 
to be 47 shillings a week with a standard deviation of 28 shillings. A random sample of 
1,500 men from the South gives a mean wage of 49 shillings a week with a standard 
devotion of 40 shillings. Required to discuss the question whether tho mean level of wnue.s 
differs between North and South. 
The difference of the means is 2 shillings and we wish to know whether this is significant. 
STANDARD ERRORS OF SUMS AND DIFFERENCES ' 227 
From (0.64), taking as usual with large samples the unknown variances to be those of the 
samples, we find 
28a 402 
var (difference) = + 
1000 1500 
= 1-851. 
The standard error is thus 1-36 and the difference in means, being less than twice this 
amount, is hardly significant of any real difference. Had the difference been three shillings 
instead of two we should probably have concluded that the difference, being more than 
twice the standard error, was significant. 
There is an alternative approach to this problem which is worth noticing. Suppose 
we assume as our hypothesis under test that the distribution of wages in the two areas 
is the same. Then we may combine the sample figures to give a new estimate of the mean 
and variance in this distribution, e.g. the mean might be taken to be given by 
(1000 x 47) + (1500 X 49) 
2500 
= 48-2 shillings. 
In the first sample the sum of squares of deviations about the mean 47 is 
v 1000 X 282 = 784,000, 
and henoo the sum about the origin is 784,000 + (472 x 1000) = 2,093,000. Similarly 
in tho second sample the sum of squares of deviations about the origin is 
1500 (40a -+- 492) = 6,001,500. 
Tho second moment of the whole about the origin is then ^y^Tr" — 3597-8, and honoo 
the variance is 3597-8 — (48"-2)a == 1274-56. We might take this as our estimate of the 
variance in the population and our problem would then be : does the mean in one of the 
parts of the whole sample, say the first, 47 shillings, differ significantly from tho mean 
of the whole, 48-2 shillings I \ 
Now at first sight it looks as if this is a case for the application of (9.64). We have 
two means, 47 and 48-2, with respective variances 784 and 1274-56, and require to know 
whether the means are significantly different. But the samples are no longer independent, 
for one of them is part of tho other, and a modified formula must be used. If tho means of 
the separate samples are m\( = —-AJ and p'J == —ExA the mean of the two together 
is given by 
nxm\ + n%'p\ __ Exx + Zx% 
The difference of m[ and this quantity, say q, is then 
1 rn 2jXi -p jLiXo 
q = —Exx ■ 
nx nx + n 
n-L + ?ii [nx 
a 
4-iX-y ~~ JLlX% 
}■ 
Thus . %>=;^y^-»^ -•*;} - °- 
228 
STANDARD ERRORS 
and hence 
var q = E(q*) = } J}fclZxx - SxX\ 
Since a^ and 3?g are independent, this reduces to 
1 (nl \ 
(»i 4- n^Vi / 
rc2 
In our case w: = 1000, ma = 1500 and our estimate of jua is 1274-56. The variance of the 
difference then becomes, on substitution, 0-7647. The observed difference is 48-2 — 47 = 1-2. 
Once again this is less than twice the standard error (= y-7647 = 0-87) and again we 
conclude that the difference is not significant. 
REFERENCES 
Davies, O. L., and Pearson, E. S. (1934), " Methods of estimating from samples the 
population standard deviation," Supp. Jour. Boy. Stat. Soc, 1, 76. 
Fisher, R. A. (1920), "A mathematical examination of the methods of determining the 
accuracy of an observation by the mean error and the mean square error," Monthly 
Notices Roy. Astr. Soc, 80, 758. 
and Tippett, L. H. C. (1928), " Limiting forms of the frequency distribution of the 
largest or smallest member of a sample," Proc. Gamb. Phil. Soc, 24, 180. 
Gumbel, E. J. (1934), " Les valeurs extremes des distributions statistiques," Annates de 
VInstitut Henri Poincare, 5, 115. 
Hartley, H. O. (1942), "The range in normal samples," Biometrika, 32, 334. 
Helmert (1876), Astronomische Nachrichten, 88, No. 2096. 
Hojo, T. (1931), " Distribution of the Median, Quartiles and Interquartile distance in 
samples from a normal population," Biometrika, 23, 315. 
Kondo, T. (1929), " On the standard error of the mean square contingency," Biometrika, 
21, 376. 
Nair, U. S. (1936), " The standard error of Gini's mean difference," Biometrika, 28, 428. 
Pearson, E. S. (1926), " A further note on the distribution of range in samples taken from 
a normal population," Biometrika, 18, 173. 
and Adyanthaya, N. K. (1928), " The distribution of frequency constants in small 
samples from non-normal symmetrical and skew populations," Biometrika, 20a, 
356. 
(1932), " The percentage limits of the distribution of range in samples from a normal 
population," Biometrika, 24, 404. 
and Haines, Joan (1935), " The use of range in place of standard deviation in small 
samples," Supp. Jour. Boy. Stat. Soc, 2, 83. 
and Hartley, H. O. (1942), " The probability integral of the range in samples of n 
observations from a normal population," Biometrika, 32, 301. 
Pearson, K., and Filon, L. N. G. (1898), " On the probable errors of frequency constants 
and on the influence of random selection on variation and correlation," Phil. 
Trans., 191 A, 229. 
EXERCISES 229 
Pearson, K., " On the probable errors of frequency constants," Part I, Biometrika, 1903, 
2, 273 ; Part II, Biometrika, 1913, 9, 1 ; Part III, Biometrika, 1920, 113. 
(1913), "On the probable error of a coefficient of correlation as found from a 
fourfold table," Biometrika, 9, 22. 
(1915), " On the probable error of a coefficient of mean square contingency," 
Biometrika, 10, 590. ' ' * 
^1931), " On the standard error of the median to a third approximation," Biometrika, 
23, 361. 
and Pearson, M. V. (1931), "On the mean character and variance of a ranked 
individual and on the mean and variance of the intervals between ranked individuals," 
Biometrika, 23, 564. 
Tippett, L. H. C. (1925), " On the extreme individuals and the range of samples taken 
from normal population," Biometrika, 17, 364. 
Yasukawa, K. (1926), " On the probable error of the mode of skew frequency distributions," 
Biometrika, 18, 263. 
EXERCISES 
9.1. Show that the mean value of the variance is given exaotly by 
w x n — 1 ' 
E(mB) = Lit 
n 
and that its variance is given exactly by 
\ n J n n3 
Hence verify that the formulae of this chapter as applied to the variance of a sample 
are accurate to order n~x. 
9.2. In the height distribution of Table 1.7 it has been found that 
m2 = 6-616 
m3 = — 0-207 
m„ = 137-689. 
Regarding the distribution as a random sample from a population which id approximately 
normal, show that m3 does-not differ significantly from zero (which, of course, must be so 
if the assumption of normality is to be maintained) and that mt has a standard error of 
about 4 per cent, of its value. 
9.3. Verify that the standard error of the first decile in samples from a normal popu- 
, .. . l-709ff 
lation is • 
n 
9.4. In the distribution of Australian marriages of Table 1.8 it has been found that 
the mean is 29-4 years, the standard deviation 8 years approximately. The median 
frequency is about 63,150. Taking this distribution to be a random sample, show that the 
standard error of the mean is 0-015 years and that of the median 0-043 years. 
230 STANDARD ERRORS ' 
9.5. If a series of random samples of different sizes is drawn from a population in 
which the proportion of members bearing an attribute A is m, show that the variants of tho 
proportions of A in such sets is <. ~ W' where H is the harmonic mean of the numbers 
in the samples. 
9.6. Show that the sampling variances of the first four cumulants, as calculated 
from the moments, are given to order n~x by 
1 
var kx = -k2 
n 
var «r2 = -(k1 -f 2/c2) 
w 
var ks = -(Ka + 9*4*8 -f 9/cf -f 6*1) 
n 
var kx = -(kb + 16/c6k2 + 48/c5k8 + Uk\ -f 72*,** -\- 144***, + 24/c2). 
w - . - 
9.7. If the variate range is divided into sub-ranges and the frequency of a largo 
sample falling into the _pth range is fp, show that 
var/,=/,(l-4) 
°ov (/„./«) « -;fpft 
n"1™* 
and hence find expressions for the sampling variance of the rth moment about an arbitrary 
point. 
9.8. Show that in odd samples of n from a rectangular population of unit rnngo 
the sampling variance of the distribution of the median is given exactly bv - - 1 
CHAPTER 10 
EXACT SAMPLING DISTRIBUTIONS 
10.1. The role of the sampling distribution in statistical inference has been indicated 
in Chapter 8. In the present chapter we propose to give an account of the main methods 
of finding such distributions when the population from which the sample was derived is 
speoified. It will, as usual, be assumed that the sampling is simple and random. Thus, 
if the parent distribution is dF(x) the simultaneous distribution of n values xx . . . xn is 
dF(xt) dF(xa) . . . dF(xn) ; and if, z .is a statistic 
z = z{xx . . . x7J (10.1) 
the distribution function of z is given by 
F(z) = [•■■[ dF(xi) • • • dF{xn) .... (10.2) 
the integration being taken over the domain of the rr's suoh that z{xx . . . xn) < ze. 
Formally, (10.2) is the solution of our problem, which thus reduces to the purely 
mathematical one of evaluating certain multiple integrals or sums. The methods with 
which we are here concerned are fundamentally devices of various kinds to facilitate the 
integrative process. They may be classified into four groups :— 
(a) straightforward evaluation of the integral (10.2) by ordinary analytical processes 
suoh as a convenient change of variable ; 
(b) the use of geometrical terminology to effect the same object and to avoid cumbrous 
analytical formulae ; 
(c) the use of characteristic functions ; and 
(d) other analytioal methods, including mathematical induction. 
10.2. As an illustration of the straightforward analytical approach, let us find the 
distribution of the sums of squares of n variables, each of which is distributed normally 
with unit variance and zero mean. The joint distribution of the n variables is then the 
1 _«.' 
product of n quantities of type . e a, that is to say 
dF ,= —?-^ exp{— \{x\ + x\ + . . . xn2)}dxl . . . dxn. . . (10.3) 
(2?r)T 
We require the sampling distribution of 
z = x\ + x\ + . . . xn* (10.4) 
We have thus to evaluate the multiple integral 
F = I ... I exp (— \Sx2) dxx . . . dxn 
J J . J(2tt)£ 
over the domain of x'a conditioned by (10.4). 
231 
232 EXACT SAMPLING DISTRIBUTIONS 
Make the transformation to variables z, 61} 02, . . . 0n-i 
x1 = zi cos 0X cos 02 . . . cos dn_1 
xa = zi cos 0X cos 61 . . . cos 0n_2 sin 0n_x 
Xj = z* cos 61 cos 02 . . . cos dn_j sin 0n_y+1 
arn = zi sin 0X 
The Jacobian of this transformation is given by 
. (10.5) 
d(xu 
*J 
d(z, 6U . . . 0B-i)J 
n-2 
cos 0„_3 sin 0M_! 
cos 0n_a sin 0n_t 
cos 0n_2 sin 0W_! 
• • 
COS 0„_i 
sin 0X 
COM flj 
0 
• ■ 
0 
which is equal to \z 2 times the determinant 
cos 0! cos 02 • . . cos 6n_1 cos 0X cos 02 
— sin 0X cos 0S . . . cos 0n_x — sin 0X cos 02 
| — cos 0i sin 0S . . . cos dn_1 — cos d1 sin 02 
■ •• ••• •■• 
— cos 0! cos 02 . . . sin 0„_1 + cos 0X cos 02 
Taking out common factors in columns we find that this determinant is equal to 
cos71-1 0X cos"-2 0a . . . cos 6n_1 sin d1 sin 0a ... sin 0n_1 times 
1 
— tan 0X 
— tan 02 
— tan 0! 
— tan 02 
— tan 0„_2 
— tan 0n_j 
- tan 0rt_2 
cot0, 
m-1 
1 
tan 0X 
ian 0a 
• • • 
COt0n_2 
0 
. . . 
0 
0 
1 
cot 0l 
0 
• • • 
0 
0 
and, on subtracting each column from the preceding one, the determinant in found to reduce 
to cos71-2 0! cos71-3 0S . . . cos 0n_2. 
Thus our integral becomes 
f f 1 "~2 
1 . . . I —e~*3\z2 cos"-2 0X . . . cos 0W_2 dzddi . . . (1Qn-\ 
J J (2w)5 
(l«.«) 
The advantage of the transformation is that the limits of the variables are now much 
simpler, z itself can vary from 0 to z and the 0's from 0 to n or 2n. Thus the integral 
(10.6) divides into a product of integrals, those in 0 being constant, and we find for our 
distribution function of z 
F2 71^ 2 
F(z) = k^ e~isz~T~ dz. .... (10.7) 
The constant k may be evaluated by integration between 0 and oo and wo have 
1 = k\ e-**z 2 dz 
Jo 
= k&r 
©• 
THE ANALYTICAL METHOD 
233 
Hence the distribution sought is 
dF = i— e-** aV dz, 0 < z < oo . . . (10.8) 
a Pearson Type III curve. 
/ . 
10,3. The essential feature of the change of variables is the simplification of the 
domain of integration as defined by the limits of the new variables. In general, we usually 
take the statistic whose sampling distribution is being sought to form one of the new variables 
and choose n — 1 others in any way which may be convenient to the particular problem. 
Then, if J is the Jacobian of the transformation, namely 
j __ o(3?i . . . Xn) 
d(z, elt . . . *„_!)' 
the integral (10.2) becomes 
F(z) = f . . . [/(*,) . . . /fo) dfo' ' ' ' *n) .dzdd, . . . ddn_lt ' . (10.9) 
J J 0{Z, 0ls . . . 0n_!) 
f(Xj) being the frequency function of the parent and xt being expressed in terms of z and 
the 0's. The integration now takes place with respect to the 0's, which can usually be ' 
chosen so as to vary between limits which are independent of z ; and thus the indefinite 
integral (10.2) is replaced by more easily calculable definite integrals. 
As always in such cases J is subject to an ambiguity of sign which must be determined 
so as to make the transformed integral positive. The validity of the variate-transformation 
depends on the familiar conditions governing the change of variable in a multiple integral. ' 
For example, it is a sufficient condition that the new variables and their first derivatives 
shall be continuous in the x'a and that J does not change sign in the domain of integration.* 
Some further examples will make the general type of investigation clear. 
Example 10 J 
To find the distribution of the mean of a sample of n values xx . . . xn from the 
distribution 
dec 
dF = -; v — oo < x < oo. 
n{l + x2) 
The joint distribution is 
and the statistic z is given by 
nz = } Xj. . . . „ . (10.11) 
We have to integrate (10.10) over a domain of x'b subject to Ex < nz. Let us take new 
variables a^-= xlt x2 = xt, . . . xn_x = xn_1 and 
Xft === nz —~ X\ — x% —~ . . ■ x^ j. 
* See, for example, de la Valine Poussin, Cours d'analyse infinitesimal, 1920, vol. l,para. 286; vol. 2, 
para. 18. 
234 EXACT SAMPLING- DISTRIBUTIONS 
Here J is evidently equal to the constant n. Our new variables xx . . . a?n_1 may extend 
from — oo to + oo and the new variable z from — oo to z. We then have 
• '«-]>]".„• • r sg(i+vHi+(~^-•--*-,» •(m2) 
and the frequency function of 2 is given by the (n — l)-fold multiple integral 
in (10.12). This integral may be evaluated by step-by-step integration. We have 
1 = 1 
(1 +a2){?:2 + {a -*x)*} {a2 + (r + l)2}{a2 + (r - l)2} 
r 2ax a% + r2 — 1 , 2a2 - 2ax a2 - r2 -f L ~] 
La;2 + 1 z2 + 1 + r2 + {a - x)'2 r* + (a -,x)*J 
Whence, integrating with respect to x from — 00 to + 00, we find on the right 
—^ ; — [a log (x* + 1) — a log {r2 + (a — x)H 
{a2 + (r + l)2}{a2 + (r-1)2}L &K ' & \ t \ )s 
+ (0* + ra - 1) tan"1 a; + a% ~ r* + l tan-i g ~ CT"1°° 
r r _]_«, 
reduci11^ t0 K^X"1 + (r + 1) j (1°'13) 
Thus in (10.12), taking a? = a^_l9 r = 1, a = ?&z — xx — ... — a;n_2, we find that the 
(n — l)-fold integral reduces to 
J-oo' ' ' J-oo^-1 fit (1 + V) {22 + (»W -*l - ■ .- ■ -Z„-2)2}' 
Integrating with respect to xn_%, xn^x . . . successively, we reduce this eventually to 
n* _ 1 
rc{n2 + (nz)z] ~ ^T+~^) (10'14) 
Thus the distribution of z is given by 
dz ' 
dF = 7t(rTzY) -°o<z<oo. . . .(10.15) 
and is thus the same as that of a single observation. 
This is an interesting example of the failure of the Central Limit Theorem, the mean 
of samples of n failing to tend to normality for large n. The second mometit of the 
distribution does not exist. 
Example 10.2 
To find the distribution of a linear function of n variables xx . . . xn where x* is 
distributed riormally with zero mean and variance Vj. 
Let the linear function be 
z ~ axxx + . . . + anxn. . _ . . . . (10.16) 
Then by a transformation & = —L. we have 
z = Earfvfa , . (10.17) 
and ijj is now distributed with zero mean and unit variance. Our proLlem is thus equivalent 
to finding the distribution of a linear function of variables each of which is normally 
distributed with zero mean and unit variance. 
THE ANALYTICAL METHOD 
235 
Consider a transformation of type 
ti = hZi + hi» + • . • + ln^n 
£i = wh£i -f m^2 + . . . +w„£« 
. (10.18). 
. (10.19) 
£„ = Pi£i + p»£* + . . . + pjn , 
and let us determine the Z's . . . p's suoh that 
1^+1711*+ . . .pf = 1, aUj 1 
Vk + wyrcfc + . . . ^fc = 0, all j, h, j ^ h J 
This can always be done, for the conditions impose only n -{- \n(n — 1) conditions on the 
nz constants. 
We have then 
n n 
£tf = (Wi + • • . + ljn)2 + • • • + (P^i + • • • + PntnY = JV 
in virtue of (10.19). The joint distribution of the £'s is by hypothesis 
JL exp(- lE^)Ud^ 
(2tt)2 
where 
= —L— exp(- $Z£*)J rid£ 
(2te)2 
J = 
. (10.20) 
a* 
at 
= - 
i 
ay 
The determinant -^ is then, from (10.18), 
Wi W8 
n 
Pi p* ■ • • r« 
and multiplying this by the equal determinant 
h 
k 
the 
1 
0 
0 
m.n . . 
product 
0 . . . 
1 . . . 
0 . . . 
■ Pi 
. p* 
■ Vn 
is 
0 
0 
1 
= 1. 
Thus - = ± 1 and (10.20) becomes 
1 
(2te)2 
- exp(-12^)77^. 
i o o 
(10.21) 
236 EXACT SAMPLING DISTRIBUTIONS 
Now the £'& may vary from — oo to oo, and if we require the distribution of one of the 
£'s, say £x ( = lx^t + . . . lj;n), we have to integrate over all values of £ such that 
Zlfij < £j. This is equivalent to a range of £1 from — oo to £1 and of the other £'s from 
'— oo to + oo. Thus the integral of (10.21) becomes the product of (n — 1) definite integrals 
each equal to I e_i:' d£ = (2tt)* and the integral fl_i:' d£, and hence reduces to 
J —00 J —00 
*«> - SeW*-/"*" *• - • • • <10-22> 
In other words, £ is distributed normally with unit variance and zero mean. £ is an 
arbitrary linear function Zlfa subject to the condition thai Elf = 1. Referring to (10.17) 
we see that the slightly more general linear function z = Eafa — Ea^v^ will be distributed 
normally about zero mean with variance Ea?vi} for then ■ J* „i Jr has coefficients 
l-^A)* 
(a* \/va \ 
= (Ea *v)t) keying tne condition Elf = 1 and is distributed with unit varianoe. 
The Geometrical Method 
10.4. A considerable amount of cumbrous analysis may usually be avoided by the 
use of geometrical representation of the domain of integration. We may imagine the values 
xt . . . xn attaching to any given sample as the co-ordinates of a point in an h -dimensional 
Euclidean hyperspace. The function dFlx-^) . . . dF(xn) may then be regarded as the 
density at the point and the total frequency between zx and z% will be the integral of this 
density (the weight) in a region lying between the two loci z[xx . . . xn) = zt and 
z(xt . . . xn) = za, which in general will be hypersurfaces in the w-fold space, i.e. will 
themselves be spaces of (n — 1) dimensions. The distribution function of z will be the 
total weight between the hypersurface corresponding to z = — oo and that corresponding 
to z ; and the frequency function will be the element of weight between the hypersurfaces 
z — \dz and z -f- \dz. 
■Example 10.3 
Consider again %he problem of Example 10.2. M the w-fold £-space the density is 
given by 
1 
(27t)f 
c*p(-i2E»). 
The statistic z (= Ea^) determines a hyperplane 
z = Ea^vfa (10.23) 
and we have to find the total weight between this hyperplane and the corresponding 
hyperplane at — oo, i:e. the weight on one side—the " lower " side—of the hyperplane (10.23). 
Now Z?2 is the square of the distance of the point ^ . . . £n from the origin and is 
therefore unohanged by any rotation of the co-ordinate axes. Choose such a rotation 
which brings the axis of one variable perpendicular to the hyperplane (10.23), meeting it 
in Q. Let P be the sample point £x . . . £n and O the origin. Then 
2?» = OP2 = 0Q* + QP\ 
THE GEOMETRICAL METHOD 237 
so that the density at P is 
1 • s 
(2n)S 
For variation over the hyperplane OQ* is constant and the integral of e~iQpi is thus a 
constant independent of OQ. Hence the frequency function of z is given by 
k being some constant. 
But OQ is the distance from 0 to the hyperplane and is given by 
002 = 
z2 
Hence 
f{z) = h exp i - £ 
2 
i.e. « is distributed normally with variance Ea^Vj about zero mean. 
The reader will find it instructive to compare this example with the previous one. 
They are, in effect, the same thing expressed in different language. 
Example 10.4 
Consider again the illustration of 10.2. The elegance of the geometrical approach is 
well brought out by the analogous derivation of the result there obtained. 
In fact, our density function, as before, is given by 
7ce-iOP\ "; 
We require the distribution of the statistic z = OP2, and the density is obviously constant 
over the surface z = constant, that is to say the (n — l)-dimensional hypersphere. The 
frequency function of z is then the integral of this constant density between the hyperspheres 
z and z -f- dz, i.e. is proportional to e~iopi times the element of the volume of the hyper-' 
sphere, whioh itself is-proportional to the nth power of the radius OP. Thus we have 
d,F = ke~lopt^-OPn dz 
dz 
= ke~ia zi("-2) dz, 
giving, on ovaluation of the constant, 
dF = -~e-**zK'»-a>rf2 
as before. 
Now suppose that the quantities xt . . . xn, while still being normally distributed with 
unit varianoe, are subject to p linear restrictions of type 
ajX-L + aax2 + . . . anxn — b. 
In the w-space the variables x will then be constrained to lie on p hyperplanos. The first 
will out the hypersphere of constant density in a hypersphere of one lower dimension, also, 
of course, of constant density ; the second will cut this in a hypersphere of one lower 
dimension still, and so on. The result of the linear restrictions will be to constrain the 
238 EXACT SAMPLING DISTRIBUTIONS 
variables to a hypersphere of p lower dimensions, and thus the distribution of z in these 
circumstances will be as before, but with n — p instead of n, i.e. 
dF = = \ e-** zK«-p-a> <fe . . . (10.24) 
Example 10.5. The sampling distribution of the mean and-variance in normal samples 
Writing x for the mean of a sample, we have, for the variance <sa, 
s2 = -Six - x)* 
n 
= -Sx% — x%. 
n 
In samples from a normal population with zero mean and unit variance the density at the 
point xx . . . xn is proportional to 
exp (- \Sx*) = exp {- %(ns* rf nx2)}. . . . (10.25) 
Let us find the sampling distributions of s and x. From (10.25) it is seen that the density 
funotion can be expressed simply in terms of those quantities, and we then have to find some 
transformation of the volume element dxx . . . dx^. 
In the w-space consider the unit vector whose direction cosines are ——, ——, . . . ——, 
1 -\/n yV -yV 
say OQ where 0 is the origin. If P is the sample point, let PM be the perpendicular from 
P on to OQ. Then the length of OM is 
—- + -i- + . . . -f- = xy/n. 
yn yw yw 
The length of OP is y/Sx*. Thus the length of PM is (Sx» - nx*)* = s\/n. 
The element of volume at P may be regarded as the product of an elemental increment 
in OM, equal to dx, and the elemental volume in the perpendicular hyperplane through M. 
In the hyperplane the contours of equal density, as in the last example, are hyperspheres 
of radius s-y/n centred at M, and consequently the element of volutin is equal to k dx sn~%ds 
multipliep by other elements which need not concern us since they are independent of 
x and s. We have then for the element of frequency 
dF ocexp {- l(ns* + nx2)}sn~2 dx ds . . . . (10.26) 
and this splits into two factors 
dF oc e~inft dx (10.27) 
dF oc e-*MV-s ds (10.28) 
Thus in samples from a normal population the distributions of mean and variance are 
independent. Equation (10.27) is equivalent to the result found in Examples 10.2 and 10.3. 
Equation (10.28) is new. We have 
dF oc e-*™V-3 dsa 
and, on.evaluation of the constant, 
n-l 
THE GEOMETRICAL METHOD 239 
It is interesting to compare this with the distribution of the previous example. In the 
latter case we found the distribution of the sum of squares of the variables measured from 
a fixed point. In this case we have found the distribution of - th of the sum of the squares 
measured from the sample mean. A comparison of the form (10.29) with that of (10.24) 
shows that the distribution of variances is, except for oonstants, the same as that of sums 
of squares when subject to one linear constraint. 
* 
Exam.p'e 10.6. " Student's " distribution > 
In the previous example we have 
xy/n OM , . 
—V = -j^-- = cot fa 
a^/n PM ^ 
where ^ is the angle POM! 
If, then, we define a statistics z = -, z will be constant over the cone obtained by 
s 
rotating PO about the unit vector, keeping the angle <f> constant. The distribution of z will 
then be given by determining the weight between the cones defined by <f> and <f> + dj>. 
Consider the intersection of these cones with the hypersphere of radius OP. They 
will cut off an annulus on the sphere whose " content " (the w-dimensional analogue of 
volume) will be proportional to OP d<f>.PMn~2 
= OF"'1 sin71" V dfa 
The density function is constant and proportional to e~iOP' on the hypersphere and thus 
the total frequency between the cones will be proportional to 
i 
p e-*oiJ,OP»-1 sin71- *<f> d<f> d(OP) 
oc smM~2^ d<f>, 0 < <£ <je. 
The distribution of z,(= cot <f>) is then given by 
7c dz 
dF oc 
n 
(1 + z2)*" 
or, on evaluation of the constant, 
1 d? 
dF^-—-.--^ - — (10.30) 
\ sr ' v(1 + *2)ir 
Since z is the ratio of two functions of the variables of unit dimension this distribution 
holds for samples from a normal population irrespective of the scale, that is to say, 
irrespective of the variance of the parent population. 
The distribution is usually put in a slightly different form. 
Put * = 1 *^n ^ = V(n - l)z. 
\»-I 
■2(x 
- a")2}' 
240 EXACT SAMPLING DISTRIBUTIONS 
(10.30) .then becomes 
*P \z i—w %-XZ • ■ • (10-31) 
i/v<SnT\ 
S) (>+')* 
* (10.32) 
where v =n — 1. 
This celebrated expression is known as " Student's " distribution after the nom deplume 
of its discoverer (1908).* The distribution funotion may be evaluated from the incomplete 
Z?-function, but special tables have been prepared. One such, due to " Student " himself, 
is given as Appendix Table 3. 
Example 10.7. Distribution of the mean of samples from a rectangular population 
Consider now a sample of n values from the rectangular distribution 
dF = dx 0 <£ < 1. 
In the n-space the density function will be a oonstant everywhere inside a lrypercube 
0 <Xj <l, j = 1, . . . n . . . . (10.33) 
and zero elsewhere. The unit vector will be the long diagonal of this cube. If P is the 
sample point {xt . . . xn) and PM the perpendicular on to this diagonal, then, as shown 
in Example 10.5, OM = x-\/n. Thus, for the distribution of x we require the element of 
weight (which in this case is proportional to the element of volume) between the hyperplanes 
x and x + dx ; and this is equivalent to rinding the content of the hyperplane (its " area ") 
cut off by the various faces of the hypercube. The complication of the problem arises from 
the fact that as x increases this region.changes its shape according to the number of edges 
of the hypercube cut by the hyperplane. 
Consider the " quadrants " 
r^o'orl}^1' 2' • ' ■*' ' * ' ' (10'34) 
whose corners are the corners of the hypercube. Any one of the corners may have 0 or 
lor 2 . . . or n of its co-ordinates equal to unity and the rest zero. We divide the quadrants 
into (n + 1) sets according as the corner has 0, 1, . . . n of its co-ordinates equal to unity, 
that is, according as 
r= JTV, 
;=i 
is equal to 0, 1, ... n. A quadrant of the tth set may be called Qt. There will be I 1 
different Qt'a. Let S be any point of Q0, i.e. any point whose co-ordinates are all > 0^ 
* Strictly speaking, " Student's " distribution is that of (10.30), the modified form (10.32) being 
due to R. A. Fisher. The latter form is therefore sometimes referred to as Fisher's ^-distribution. 
THE GEOMETRICAL METHOD 241 
and let just 8 of its oo-ordinates be > 1, Then 8 will belong to just ( J = 1, Q0; I )Qi& \ 
)Qi'a, and so on. Now if s > 0, 
JT(- iyr j = (i - i)« = o. ... . . (io.35) 
Hence, if whenever a point belongs to a Qt we give it a density (—1)' and then sum over 
all Q, the resultant density will be 1 or 0 according as the point belongs to the hypercube 
or not. 
Let the segment of the hyperplane 
z = Z(x) (10.36) 
lying in Q0 have content Vn(z). Then the segment lying in any member of (10.34) will 
have content Vn(z — r) which is zero if r > z. Further, the segment of (10.36) lying in 
any member of (10.34) will have the content 
2^(-l)r(^W-r) (10.37) 
where k = [g], = the greatest integer less than z. 
To find Vn(z), let Vn-\{z) be the projection of Vn(z) perpendicular to one of the axes, 
so that 
vn(z) = v»p.-i(*). 
Now Vn(z) is the content of the w-dimensional region bounded by (10.36) and the co-ordinate 
hyperplanes—a region whose base is therefore of content Vn(z). The perpendicular from 
0 to this base is -^-. Hence 
and ^lW-_L_(^_j)F..lW 
or YJz) = ^-L^J^-^V^z). . . .(10.38) 
Since Fa(g) = z\/2 repeated applications of this formula give 
VnW ~ {n _ !)!« • 
Substituting in (10.37) we find for the content of the region common to the hypercube 
and the hyperplane 
{n 
^Xy l)'^){z - T),,~l • • • • <«>■*»> 
for values of z between h and k -\- 1. 
Since 
f 
o V» 
A.S. XI 
242 EXACT SAMPLING DISTRIBUTIONS 
the distribution of the mean m = - is given by 
* ----*■+- \ . (10.40) 
- <m 
n n 
ret 
h 
This is the required distribution. It is unusual in consisting of n arcs of degroo (n » 1) 
in m, having (n — l)-point contact at their joins, that is at the points -(£ — 1,2, . . . n). 
The distribution is symmetrical since the hyperplane z = constant i« porpondioular to 
the long diagonal, which itself is an axis of symmetry of the hypercube. 
For particular values n = 2, 3, 4, (10.40) gives the following results for the frequency 
function :— 
n = 2: 4m, 0 < m *•' & 
4(1 — m), I < m < I 
27m2 n ^ .i 
n == 3 : —2—, 0 < m ^ £ 
^{m2-3(m-|)2}, * <w <| 
^(1-m)2, f<m<l 
n = 4: _ -^-w8, 0 < m < J 
1 98 
^{m8-4(m _£)*}, J<»n<i 
128 
^{(l_m)3_4(|-m)3}, 
I < M < £ 
—(1- m)3, £ <ra ■-; 1. 
If the frequency curve be drawn it will be found to resemble a normal curvo in 
appearance. The distribution, of course, tends to normality as n increases in virtue of tho (Antral 
Limit Theorem. 
The Method of Characteristic Functions 
10.5. It has already been noted that the characteristic function of tho Hum of 
n independent variables is the product of their characteristic functions!. This wimplo 
property enables us to find the sampling distribution of a wide class of HtatiHticH whidi 
are expressible as sums, and particularly of the mean. 
If we have a sample of n values from a population whose characteristic function in <j)(t), 
the characteristic function of their sum is <f>n. Thus the distribution function of thoir 
Bum z is 
*w - ,(o) = *£\sz£?r m . . . .(„,,,, 
and the frequenoy function is 
M " ^Ij"^ dt (10-42) 
il\m 
THE METHOD OF CHARACTERISTIC FUNCTIONS 243 
The following examples will illustrate the power of these results. 
Example 10.8. Distribution of the Mean for the Binomial 
The characteristic function of the binomial (q + p)r is 
(q + pe^y. 
The c.f. of the sampling distribution of the sum of n values is then 
{q + pe**)™ 
and that of the distribution of the mean [ - of that sum) is 
\n J 
/ if\rn 
\q + pen) . 
But this is the c.f. of the binomial 
. (q +i>)rn, • (10.43) 
the interval being - instead of unity; and henoe this distribution is that of the mean. 
Example 10.9. Distribution of the Mean for the Poisson Distribution 
The characteristic function of the Poisson distribution whose general term is e~x —. is 
r! 
exp {A(e« - 1)}. 
s 
The c.f. of the mean is then 
exp nX\en — l) 
and lience the distribution of the mean is the Poisson distribution, whose general term is 
e-nxW m . • . ' . . . (10.44) 
the interval being - instead of unity. 
Example 10.10. Distribution, of the Mean for the Normal Population 
The characteristic function of the normal distribution 
1 _it£r'il' 
dF = — -« * a* dx 
c\/{2jc) 
is exp {— '^aff2 + iti-i). 
The o.f. of the distribution of the mean of n values is then 
exp J - ^ + M - exp | - f~ + U,\ . . . (10.45) 
This is the c.f. of a normal distribution with mean u and variance —. which is therefore 
^ n 
the distribution required. 
244 EXACT SAMPLING DISTRIBUTIONS 
Example 10.11. Distribution of the Mean for the Type III Population 
The characteristic function of the distribution 
dF = —-Kea[-\ _, a>0 
r{y) \aj a 
1 . 
is 
(1 - itay 
The c.f. of the distribution of the mean of n values is then 
1 
( 
_ ita\'ly' 
~n) 
This is the c.f. of the distribution 
dF = —-.e a (_)• 10.46 
1 (y?i) \a J a 
Example 10.12. Distribution of the Mean for the Rectangular Population 
The characteristic function of the distribution dF = dx is 
eil - 1 
f eitxdx = 
Jo 
it 
it 
The c.f. of the mean of n values is then ( —:— ^ , and the frequency function is thus 
n 
1 f 
a 
f(x)=~\ «-«»/ en -l\ dt (10.47) 
27rJ_oo [it 
n 
This integral is everywhere holomorphic and the range of integration may then be 
ohanged to the contour r consisting of the real axis from — oo to — c, the small semicirole 
of radius c and centre at the origin, and the real axis from c to oo. Thus 
it 
™ - y, 
'on i \ n 
-^/6 ^ dt 
it 
to 
it] 
_tyj!f.-«. Vt-lt(*\&z*- ■ • -(10.48) 
f -pigs 
Now — dz = 0 if g > 0 
,n-l 
= - 2mn-l—— if g < 0. 
{n - 1)! 
This may be seen by integrating along a contour consisting of r and the infinite 
semicircle above the real axis if g > 0 and below it if g < 0. 
THE METHOD OE CHARACTERISTIC FUNCTIONS 245 
Substituting in (10.48) we find 
«-«. 
X 
This, with a few changes of notation, is the same as (10.40). 
10.6. General expressions may also be derived for the distributions of geometrio 
means and the moments about fixed points. 
In fact, if y = log #, the characteristic function of y is 
/•CO nPQ 
ait) = eitk)<>xdF = xildF. 
The distribution of the sum of n independent values of y, say nz, is then given by 
If00 1 fi-ilnz 
F(nz) - F(0) =-^\ - -f — a»dfc . . .(10.49) 
and the distribution of the mean is that of z. But z = log u, where u is the geometric 
mean, and hence the distribution of u may be found. 
The frequency function, when it exists, is 
1 f00 
Similarly the characteristic function of a power of the variate, say xry is given by 
0(t) = p e^r dF 
J —CO 
and thus the distribution of the rth moment, say z, by 
J(/az) - i^(0) = L.__._J:/3»^. . . .(10.50) 
J -00 *' 
1 
Example 10.13. Distribution of the Oeometric Mean in Samples from a Rectangular Population 
If the population is 
1 
dF = - dx 0 < x < a, 
a 
the characteristic function of log x is 
•00 
e-itnB aJi fa 
-00 
246 EXACT SAMPLING DISTRIBUTIONS 
The frequency function of u = Slog* » then given by 
dt 
j j«oo e-Uuanit 
_. _L| i______<ft w log a — w > o. 
2wJ-«"(l'+!«)• 
This integral may be evaluated in the manner of Example 10.12 and wo find 
f{u) = L__* _ , 
u 
whence, putting z — e», we find for the distribution of tho geometric moan z 
*2>=ssHr <io-5i) 
Example 10.14. Distribution of the Second-order Moment about the Po'p'iilntion Mean in 
Samples from a Normal Population 
If the distribution is 
1 -.£.' 
dF — —r^—ve to* dx 
a\/{z7t) 
the characteristic function of a:2 is 
1 eitx* e 2a* dx 
tfVWJ — 
(1 - 2aHt)* 
The c.f. of the mean of n values, say ma, is then 
1 
( 
— (111.52) 
n J 
and the frequency function of this is 
\ r z-^rz&- 
n.(i -*2=«^ 
n J 
This may be integrated in the manner of the previous example, or tho result, written down 
directly from the consideration that (10.52) is the characteriHlic function of tho distribution 
jtt n~2~ _™»« '1-1 , 
(2a2)2Tf|j 
a result which may be compared with that of Example (10.4), to which it. in equivalent. 
The Method of Induction 
10.7. The distribution of the sum of two independent variatos may bo obtained 
directly without the intervention of characteristic functions. If Fx(xi) and Fz(.)\) arts 
the distribution functions, the distribution function of z ~ xx -f #a in given by 
= (°° f ldFxdF*, (10.54) 
dF 
■ 00 J —00 
J —oo L ' J —op 
THE METHOD OF INDUCTION 247 
the domain of integration being that for which xx -f xi < z 
dFa 
= P Fx(z - x%) dF*. . (10.56) 
J —00 
If, further, F is difFereritiable, the frequency funotion of z is given by 
M = T M* - *.) /. dxif .... (10.56) 
J —oo 
/t and f2 being the frequency* functions of xx and a?a. 
(10.50) can be used to obtain successively the distribution of the. sum of any number 
of variables whose individual distributions are known. If all the variables have the same 
distribution the general form may be suggested when the results for two or three variates 
have been worked out. Its correctness oan then be verified by induction. The following 
examples illustrate the method. 
Example 10.15 
Consider again the distribution 
dF = —-——- — oo < x < oo. 
7r(l + X2) 
By (10.50) the distribution of the sum of two independent variables each of whioh has this 
distribution has the frequency function 
)-n^vV^-xy)\i~T^) dx = ^Ta1)' 
Thin suggests the general form 
n 
If this is correct, then the form for (n -f 1) variables is 
J _«> ^vt"T7*Tlr^Ar+Tv dx 
,-»fl±i)/{,. + (B + l).} 
n{z* + (n + l)*y 
The result holds for n = 1, 2, and is therefore true in general. 
Example 10.W 
In Example 10.4 we found that the distribution of the sums of squares of n independent 
variates is given by 
dF = l -e-*' zU»-2) dz (10.57) 
*r(*) 
Suppose we had surmised this form from an examination of a few oases for low n. Let 
x be another variate distributed normally about zero mean with unit varianoe. We require 
the distribution of z + %2- 
248 EXACT SAMPLING DISTRIBUTIONS 
Let #* = v. Then v has the distribution 
Then, from (10.56) the frequency function of the distribution of u = z + v is given by 
P __L_ e-*<«-*> (u - z)~i —L-- e~*s z«»-2> dz 
2-rr(«r^ 
f (tt - z)-*Z*<»-2> dz 
J —CO 
which is the same as (10.57) with n + 1 for w. Hence the distribution holds generally. 
The Distribution of a JRatio 
10.8. Cases not infrequently arise in which we wish to find the sampling distribution 
of the ratio of two statistics, z1} za. The problem becomes somewhat complicated when 
the divisor z2 may be negative, but relatively simple in the contrary case. 
If F1}F B are the distribution functions of zx and za and v = —, then for the distribution 
za 
function of v we have 
poo rue, 
dF = dF, dF2 dv 
J — CO J — CO 
= r ^(w,) clF* dv, . . " . . (10.58) - 
J -co 
or, in terms of frequency functions, 
poo 
f(v) = Za/i(^a)/2(Z2) rf2. (10.59) 
J —00 y 
Example 10.17 
Consider again the distribution of the ratio x/s discussed in Example 10.6. Here x is 
the mean of samples of n from a normal population and is thus distributed as 
dF ac e a*1 dx. 
s is distributed as 
dF oce *+er-* d$, 
as we have found in equation (10.29). 
THE DISTRIBUTION OF A RATIO 249 
Then the distribution of v = - is, from (10.59), a constant times 
s 
f 
s.e 2a« e 2ai sn * d$ oc 
n » 
(1 + tf2)2 
which then gives us the distribution (10.30) on the evaluation of the constant. 
Example 10.18. Fisher's z-distribution 
Suppose we have two independent samples of nx and n2 members respectively from 
normal populations with variances a\ and a\. The distributions of the sample variances 
and s\ f = -S(x — x)A are then 
dF OCe «*»■ s^^dSi 0 < 8t < 00 
^ a e «*,■ *,».-* ^Sa 0 < s2 < oo 
The distribution of the ratio t = — is then, from (10.59), given by 
/(<) oc J" ,, exp (- ^^(,,()-.-« exp ^-'^,«.-« <fe, 
°crexp{(-ii-s)sik,+',"3<"'"a<iS! 
tt/u^S^S- °<<<C° • • ' •(10'60) 
Tliis is usually expressed in a somewhat different form. Put 
z = I log ^(n> ~ 1}1 = 1 log M^JZ4^. 
We find for the frequency function of z 
g(jll-l)C 
/Yz) oc - — » — oo <z < oo 
/>'LJ^l)i!! + n» ~ lV("'+n,~2) 
\ of of / 
or, writing ^ = Wj — 1 and v% =nt — 1, and evaluating the constant term 
Oil O's"' ''i*'"1 ''a4''* ^ 
In particular, if rrf = a\ we get Fisher's z-distribution of the ratio of two variances from 
a norma] population 
u, u3 M ,2 VA| ^ . . . . (10.61) 
The distribution function of z may be obtained from tables of the incomplete i?-ninction. 
Special tables showing, for various values of vx and vi} the values of z corresponding to 
F(z) = 0.99 and 0.95, have been prepared and are given as Appendix Tables 4 and 5. 
250 • EXACT SAMPLING DISTRIBUTIONS 
10.9. Up to this point we have been mainly concerned with the distribution of a 
single swtistic compiled from the members of a sample which is random and »m>l« 1 to 
meLds may, however, readily be generalized to obtain the snnultanoons ,1*i n b at* nrf 
several statistics. For example, if there are several statotics *,, s .„, ami tho joint 
distribution of the sample values*, . . . *. is represented by «■(«„ . . . *•„),«,, ,W<*«- 
istic function of the z's is given by 
m, ... g=r ■ ■ • r exp<** -+-■•■+^ aF (xi • • ■Xn) -{u)M) 
J — 00 J —00 
and the frequency function of the z's (if it exists) by 
/(*i, • • ■ «p) = Ts^r ■ ■ ■ f eXp (_ *Vl ■ " ' ~ *WM ' ' ' '^ 
( ;J-°° J"°° *» . • • dtp . . (10.04) 
Examples of the use of these results will occur in the sequel. 
NOTES AND REFERENCES 
A systematic account of the various methods for deriving sampling distributions has 
not previously been given, except in regard to characteristic functions, as to which sec 
Kullback (1934). The geometrical method is largely due to R. A. Fisher, whose use of it 
to derive the sampling distribution of the correlation coefficient (1915) is a beautiful example 
of the power of the method (cf. Chapter 14). See also Uspenaky (l!):i7). 
Some of the distributions derived in the foregoing examples are olassieal. For 
" Student's " distribution see his paper of 1908 and Fisher's paper of 1925. The 
distribution of the sums of squares of values from a normal population was discovered by Helmcrt 
in 1876 but forgotten until Karl Pearson rediscovered it in 1900. Tho distribution of the 
mean of samples from a rectangular population is traceable as far back as Lagrange 
(Miscellanea Taurinensia, 1770-73), but was forgotten and rediscovered simultaneously by Hall 
and Irwin (1927), the former using the geometrical method and the latter eliaraelerisUr 
functions. For the distribution of means from Pearson curves, see Irwin (lO.'iO), For 
Fisher's z distribution, see his paper of 1915 and that of 1924. For the distribution of 
a ratio, see Cramer (1937) (Exercises 10.8-10.11 below), Geary "(1930), Fiellor (li»:J2), 
and Nicholson (1941). The distribution of the ratio of two normal variables exhibits some 
unusual features; it may, for example, be bimodal. 
Cramer, H. (1937), Random Variables and Probability Distributions, Cambridge ITniversily 
Press. 
Fieller, E. C. (1932), "The distribution'of an index in a normal bivariaie population," 
Biometrika, 24, 428. 
Fisher, R. A. (1915), " The frequency distribution of the values of the correlation coefficient 
in samples from an indefinitely large population," Biometrika, 10, r>07. 
(1924), " On a distribution yielding the error functions of several well-known statistics," 
Proc. International Math. Congress at Toronto, 80,r). 
(1925), "Applications of 'Student's' distribution," Metmn, 5, No. 3, Of). 
Geary, R. C. (1930), " The frequency distribution of the quotient of two normal variables," 
Jour. Boy. Statist. Soc, 93, 442. 
Hall, P. (1927), " The distribution of means for samples of size N drawn from a population 
in which the variate takes values between 0 and 1, all such values being equally 
probable," Biometrika, 19, 240. 
» 
EXERCISES 251 
Irwin, J. 0. (1927), " On the frequency-distribution of the means of samples from a 
population having any law of frequency with finite moments," Biometrika, 19, 225, 
and (1929), 21, 431. 
(1930), " On the frequency-distribution of the means of samples from populations 
of certain of Pearson's types," Metron, 7, No. 4, 51. 
Kullback, S. (1934), " An application of characteristic functions to the distribution problem 
of statistics," Ann. Math. Statist., 5, 263. 
(1935), " On samples from a multivariate normal population," Ann. Math. Statist., 
6, 203. \ ' 
Nicholson, C. (1941), " A geometrical analysis of the frequency distribution of the ratio 
between two variables," Biometrika, 32, 16. 
Pearson, Karl (1900), " On the oriterion that a given system of deviations from the probable 
, ... is such that it oan be reasonably supposed to have arisen from random 
sampling," Phil. Mag., 50, 157. * 
" Student" (1908), " The probable error of a mean," Biometrika, 6, 1. 
Uspensky, J. V. 1[1937), Introduction to Mathematical Probability, McGraw-Hill, New York 
and London. 
EXERCISES 
10.1. Derive by the method of characteristic functions the expression for the sampling 
distribution of the mean of samples from the population 
dF = /i f 2\> ~ °° <x < °°- 
7t{l + a;2) 
10.2. Show that the distribution of the geometric mean g in samples of n from the 
Type III population 
dF = —_) v dx 0 < x < oo 
is dF = 
i\n) {fip) }-Z* K > L^-1 r(t + i) _U 
(Kullback, 1934.) 
10.3. Show that the difference of two values drawn at random from the Poisson 
population whose general term is e-A— is distributed in the form whose general term is 
e-u T<Z(2A), where d can take all integral values from — co to co and Td(2X) is Bessel's 
modified function of the first kind of order d and argument 2A. (Cf. Example 4.5.) 
(Irwin, 1937, Jour. Roy. Statist. Soc, 100, 415.) 
10.4. Show that the distribution of the mean of samples of n from the Type II 
population 
dF cc a^r^l - vf'1 dx p > 0, 0 < x .< 1 
is given by ' 
m = \ *r?{rip)y*J" l^^jL cos {n*p) dp, 
where Jr{z) is the Bessel coefficient of order r in. z. 
(Irwin, 1927.) 
252 EXACT SAMPLING DISTRIBUTIONS 
10.5. Show that the distribution of the geometric mean of n variables, one from each 
of the populations with frequency functions 
1-i j."-1_i 
xp-le-x tfi+n e-x XP n e~x 
is the same as the distribution of the arithmetic mean of n independent variables distributed 
in the first of these forms. 
(Kullback, 1934.) 
10.6. Show that the difference of two variates, z, each of which is distributed in the 
Type III form 
p-xrp-l 
- dF=-m~dx ■ . • 
has the frequency function 
=)-„l\~+T*)p dt 
2p-1 
2—j»r(£) 2 
where Kr(x) is the Bessel function of second order and imaginary argument. 
(K. Pearson, ^touffer and David, 1932, Biometrika, 24, 293.) 
10.7. If a frequency function is given as the sum of a number of terms of the Type A 
scries 
/(*)=a(s){l+£ffJg)+...+?ftfig 
show that the sum S of n independent variates has a frequency function 
m - «w{i + £*.(£) + • • • %fl{% 
where £ = a\/n and 
' v3\vt\ . . . vk\(N - v3 . . . - vk)\ k ' 
the summation being taken over all values of the v's.for which 
3v3 + 4:vt + . . . + kvk =j. 
(Baker, 1930, Ann. Math. Statist., 1, 199.) 
10.8. A theorem of CrameYs (1937) states that if two independent variables, xv and 
Xt, with finite mean values, distribution functions Fx and Ft and characteristic functions 
f \6 
<f>i, ^a are such that ^(0) = 0, so that x2 is non-negative, and I -L 
the distribution function of v = — is given by 
X2 
F(v) = ir u*) - mm-*>) dt 
2^J_00 it 
t 
dt converges, then 
EXERCISES 253 
and the frequency function, if it exists, by 
Use this result to obtain the distributions of Examples 10.17 and 10.18. 
10.9. Show that the ratio of two independent normal variables has frequency function 
JK } V(2w) (*S + <&*)' l *? + <*»■ J 
where mx, o^ are the mean and standard deviation of the first variate, mz, a2 those of the 
second variate, and it is assumed that w2 is so large compared with cra that the range of 
the second variate is effectively positive. 
Hence show that ~^-,—s-^-r is normally distributed about zero mean with unit 
variance. 
(Geary, 1930.) 
10.10. Show that the ratio of two independent variables distributed as 
dF oc e~^x~mi){x - miyh~1dx, 0 < wx < x < oo 
dF oc e~v'{x-f,h)(x — m^-Hx, 0 < mz < x < oo 
has a frequency function 
flirt = VlPl ^ m* f $l>1~1 - ('Pl " l\(t\ P^'l~2 
n (pi - i)i -I A + y^ya V i /W A + 7^Y'a+1 
/^ - 2\^2\pa(j?a + l)p»j- 
V 2 Art/ A + y^y7l+'2 
, y^c^P* f l^-1 _ (Pl - l\(l\ (Pi + 1)^)1~2 . 
(pi-1)!y«ui + y^Y**1 \ * /Wa + wy+i ' 
where f = m1 — m2«;. (This includes Fisher's ^-distribution as a particular case.) 
-3 
2 
10.11. Show that the ratio of two variates v = —, where xx is distributed normally 
x% 
with mean mt and variance c2 and the second like a standard deviation in normal samples, 
i.e. with distribution function given by 
dF oc e-A3-nu)(s - wa)f'_1 ds 0 < m8 < s < oo 
has a frequency function given by 
where I = wx — wi2«;. 
(This includes " Student's " distribution as a particular case.) 
CHAPTER 11 
APPROXIMATIONS TO SAMPLING DISTRIBUTIONS 
11.1. In the previous chapter we have considered methods of deriving sampling 
distributions in an exact form when the parent population is completely specified. Those 
methods are not applicable when the parent is not completely known, and they may in 
any case lead to results which are difficult to apply in practice, e.g. by yielding an integral 
which has not been tabulated. In such cases we can frequently deal with the problem 
by finding approximate forms for the sampling distribution, particularly by ascertaining 
its lower moments and then fitting a tractable type of curve such as one of the Pearson 
class. 
A procedure of this kind has, in fact, already been considered in Chapter 9, wherein 
it was seen that approximate expressions could be derived for the first and second moments 
of sampling distributions in terms of the lower moments of the parent.- When the sampling 
distribution tends to normality this, in effect, solves our problem, for the first and second 
moments determine a normal distribution. The methods of this chapter are really 
developments of this idea. We shall discuss exact methods of finding the moments of sampling 
distributions in terms of parent moments. Our results are important not only on their 
own account, but in giving an accurate method of judging the degree of approximation 
of the expressions for large n discussed in Chapter 9. In particular we shall be able to take 
up some points which had to be left on one side in that chapter—e.g. the rapidity with 
which some functions of the moments such as ■y/bi approach normality. 
11.2. It is as well to recall that there are three different types of moment concerned 
in the investigation : (a) the moments of the "parent population, (b) the moments of the 
sample and (c) the moments of the sampling distribution. They will be reforred to as 
parent-moments (parameters), sample-moments (moment-statistics) and sampling-moments 
respectively. Similarly we shall consider parent-cumulants, sample-cumulants and 
sampling-cumulants. 
11.3. In Chapter 9 we obtained the exact results 
E{m'r) = fj,'r 
var (m'r) = E(mr - pr)* = -(/4. - tf) 
and noted that formulae for sampling moments about the mean were more difficult to obtain. 
Although we shall later reject this approach in favour of another, it is instructive to consider 
what happens if we try to generalize the procedure of that chapter to our present problem. 
Suppose, for example, we are interested in the sampling distribution of the variance. -The 
above equations give us the first two sampling moments of the second moment about an 
arbitrary point. For the first sample moment of the variance we have 
E(m%) = e[±S{x*) - 
\_n 
254 
- (H.l) 
Ml 
APPROXIMATIONS TO SAMPLING DISTRIBUTIONS 26S 
n — 1 / nin — 1) .„ 
=*= fi2^ -> ----->!2 
n n2 
" _n — 1 , . 
— f.1%- ....... ^ll.Zj 
This is exact and may be compared with the approximate expression given'by the methods 
of Chapter 9, viz. : 
E[mt) = jua (11.3) 
We might then proceed to find the second, third . . . sampling moments of the variance 
and thus obtain more and more information about its sampling distribution. For example, 
we have f6r the fourth moment 
■E(rn$) = #[^M - ^(*)>2]4 
-*[*w 
*2)}4 -i{^2)}3{^)>»+ i{W}»{^)}' 
n& wH 
- ±mx*)}{£(z)}' + ^{Ifc)}8I • • (11.4) 
We can then find the expectations of the individual terms by an easy extension of the 
method already used. Weexpress any power in terms of products of the type £(xf Xjf . . . xf) 
when j ?± Jc ^ . . . ^ I; the mean value of such a product, the x'a being independent, 
is n(n — 1) . . . (n' — I + l)fi^ . . . fi'p. Without loss of generality we may take our 
origin at the mean of the parent, so that pix — 0 and other moments are those about the 
mean of the parent. The rest is mere algebra. For example, for the first term in (11.4) 
we have 
{£(*«)}*« Or?+ai + . . . zn*Y- 
= Sx8 + 42a;/V + SExfxJx? + %Ex}x£ + i^V^V • (1L5) 
The numerical coefficients require a little watching. That of xfxk*t for example, is 3, not 
6 as in the multinomial expansion of {x\ + . . . jcn2)4 because j and k can be interchanged. 
The mean value of (11.5) is then 
■ /n6 + 4m (» — l)n«fis + Gn(n — l)(n — 2)^f + 3»(» — l)fi( + n(n — l)(n — 2){n — 3)/4- 
A similar evaluation of the other terms in ((11.4) leads eventually to the result 
3.1 • 
E{m\) = —(fii - /4V +—.b(/*b - 4/*fl/*2 - 24/^//3 - lSfi'l + 48^4/4 + 96^2 _ 30^) 
lb lv ^ 
-Upt - 40^./*, - 96^8 - Ufi\ + 336^4Ju§ + 528^, - 306/4) 
+ -5(6^ - 9Gfiafi2 - 176W3 - I02fil + 924^| + 1232/^ _ 1044^*) 
o(4/*8 - 88^0/*2 - 160/^ - 95/4 + 1050/t4^| + 1360^2 - 1395^) 
IV 
1_ 
11.4. Systematic investigations of the sampling moments on these lines (thoiigh by 
+ — C"a - 28/^a - 56Wa - 35^ + 420//</4 + CeO/tjj/*, - 630/4) . (11.6) 
256 APPROXIMATIONS TO SAMPLING DISTRIBUTIONS 
a somewhat different method) were carried out by Tschuprow (1919) and, for the particular 
case of the variance, by Church (1925), who corrected some misprints in Tschuprow's results. 
Unfortunately the resulting formulae are exceedingly complicated—the above is one of 
the simpler cases—and are obviously unsuitable for practical work. 
It then began to be appreciated that their complexity might be due to the use of a 
special type of symmetric function of the observations, namely the moments, and the 
question arose whether other functions might have simpler properties. Thiele had already 
introduced the parameters which are now known as cumulants, and had defined some 
statistics which were the same functions of the moment-statistics as the cumulants are 
of the moments. He also gave some expressions for the sampling cumulants of these 
functions. In 1928 C. C. Craig developed this work and gave a number of further results. 
Even these, however, were sufficiently complicated and were reached only after some 
labour, and Craig himself remarked that " it rather seems that the best hopes of effectively 
further simplifying the problem of sampling for statistical characteristics lie either in the 
discovery of a new kind of symmetric functions of all the observations ... or in the 
abandonment of tne method of characterizing frequency functions by symmetric functions 
of the observations altogether." 
About the same time R. A. Fisher discovered such a new kind of symmetric function, 
the ^-statistics, and his remarkable paper of 1928 forms the basis of nearly all subsequent 
work on the subject. The new statistics have the valuable property of yielding particularly 
simple sampling formulae which can be obtained directly by combinatorial methods, obviating 
most of the algebraic labour inherent in the older methods. 
JSeminvariant Statistics 
11.5. It will be observed that equation (11.6) does not contain the parent-mean /u\. 
In deriving it we took an arbitrary mean at the parent mean, which simplified the algebra 
to some extent. The independence of E(m\) of this parent mean is, however, not due to 
this accidental circumstance. In fact any transformation of the variate from one origin 
to another leaves wa unchanged, for ra2 = Z{x — m^)2 and the transformation increases 
each x and m\ by the same amount, leaving their difference unaffected. Consequently 
if raa is independent of the location of the origin, so must be its sampling moments. Thus 
our sampling formulae are very much simplified if we use statistics which are independent 
of the origin. In equation (11.6) there are terms corresponding to fiB, jUa/ti, p\, /u^l and [i\. 
If we had to take account of possible terms in ju\ there would be additional terms such as 
M-'iPu HPi2) and s0 on) our formula containing 22 types of term instead of only 5. 
11.6. A statistic which is independent of the origin of calculation is said to be semin- 
variant. The moment-statistics about the mean are seminvariant. We now consider a 
second family of statistics kp (p = 1, 2, . . .), symmetric in the observations xx . . . xn, 
such that the mean value of kp is the pth cumulant, i.e. 
E{K) =kp (n-7) 
« Note first of all that lcp is uniquely determined by this definition ; for if there were 
two functions kp and k'p obeying (11.7) their difference k.p — k'p would have a zero mean 
value. But this difference is itself a symmetric function and can therefore be expressed 
as the sum of terms Exvt Ex^ a$-1, etc., and hence its mean value is a series of terms each 
of which is a product of moinents. The vanishing of this series would imply a relationship 
among the moments which is impossible except perhaps for particular parent populations. 
Hence kv — k'p must vanish identically and thus kp = k'p. 
j 
SEMINVARIANT STATISTICS 257 
Secondly, note that the fc's are in fact seminvariant, except for &x which is equal to 
the mean itself. In fact, we have by Taylor's theorem 
&j,(3x 4- h, x2 + h, . . . xn + h) = kp(x1} x2, . . . xn) + —Dkp(x1} xs, . . .-xn) 
+ -fi2kp(xl3 zt, . . . x„) + . . . . (11.8) 
where 
axi dXz oxn 
Taking mean values, and remembering that kp itself is independent of the origin, except 
for kx, we have 
fp = Kp + T~^{Dkp) +, etc. .... (11.9) 
• ' ■*■■ 
Thus E{Dkp) and other terms on the right vanish separately, for (11.9) is an identity in h. 
In virtue of the remark above, this implies that Dkp = 0, D2kp == 0, and so on ; and hence, 
from (11.8), 
kp(Xi -\- fi, xt + h, . . . xn + h) = Kp\Xi, x2, . . . xn), \ 
i.e. kp is seminvariant. The exception to this rule js kx which has as its mean value kx =m[ 
and thus 
kx = -2(x). ...... (11.10) 
• lb 
11.7. We now proceed to find explicit expressions for the ^-statistics in terms of 
the observations By definition kp is degree p in these observations (for kp is 
of order p in the moments, that is, the sum of the orders of the moments comprising any 
term in k}) is p). We may then write 
kp = 2£(Xjr*xf* . . . V^i+i'" • ' • x*,+** ' • • **, + .+».P'M(Pi"1 - • • flM (H-ll) 
where the second summation extends over all the ways of assigning the nx + 7za + . . . nH 
subscripts (including permutations) from the n available and the first summation extends 
over all partitions of the number ^, [p"lp%* . . . pB"a)- ^(^i*1 ■ • • V"s) is a number 
depending on the partition. 
We have 
pin1 + p£i» + • • • + 2yr„ = p. • • • • (H.12) 
and define p by 
Tti -{- 7ti -{-... -\- 7Cy = p. . . . . (11.13) 
On taking mean values of (11.11) we have, since the cc's are independent, 
k.p = Ziip,*fit** . . . ,V)AB}, .... (11.14) 
where B is the number of ways of picking out the p subscripts from n, permutations allowed, 
and is therefore equal to n(n — 1) . . . (n — p + 1) = ww. 
Now from equation (3.31), we have 
A.S: 3 
258 APPROXIMATIONS TO SAMPLING DISTRIBUTIONS 
the summation extending over all partitions subject to (11.12) and (11.13). On identifying 
corresponding terms in (11.14) and (11.15) we find the values of the A'a and on substituting 
in (11.11) obtain finally 
k» = - ^r : S'WT, . *] {Pai)«'Jj. . ....„/ • • (n-16> 
the explicit expression of kp in terms of the x's. 
We may notice an important simplification of this expression which is crucial in a 
discussion of the sampling properties of the k'a. Apart from factors in p and n a typical 
term in (11.16) may be written 
XpJ pl\ ' ' ' px\ ' ' \ pa\)' 
7t\\ . . . 71, 
s' 
where, it is to be remembered, permutations of the subscripts are allowed. There will 
be a term of this type corresponding to every partition of p into ri'a and of p into p's. 
Consequently we may write 
kp =S ^ S(xvxyt • • • ^v,)' • • . (11.17) 
where there is a term in the second summation corresponding to every possible way of 
assigning the subsoripts. In this assignment, subscripts are. regarded as distinct entities. 
For example, if from the n subscripts we choosey to be 1, px to be 2, . . . p2 to be nx + 1, 
and so on, there will be as many different terms as there are ways of choosing px from 
the l's, and so on, i.e. 
' (pJY* • ■ • (p.)"'*ii • ■ ■ */ ' (1U8) 
In fact, (11.16) is a condensed form of (11.17) in which all the terms leading to the same 
a;-produot are added together, their number being given by (11.18). 
Expression of k-Statistics in terms of Symmetric Products and Sums 
11.8. Writing 
i 
[Pi"1^*' • • • *>«"'] = SfaV'xp . . . x?*) ij£j ?i . . . &1 . . (li.io) 
so that, for instance, 
[21] =Z(x?Xi) 
[2*1] = Z{x? x* xk) 
we see that the mean value of [p±ni . . . p6">] is n^/Up"1 . . . ^ "«. ' We oan then write down 
the k'a in terms of the symmetric product sums [pn] at once from the expressions of cumulants 
in terms of moments. For instance, from (3.33) we have k3 = jj,'3 — Bjj,^ -f- 2^'j3 and 
hence 
k _ [3] 3[21] 2[13] 
n w[2J n^ 
.E2 SL + SL__ . ru2m 
n n(n — 1) n[n — \)[n — 2) " " • \ • / 
^-STATISTICS IN TERMS OE SYMMETRIC PRODUCTS AND SUMS 259 
a result which, of course, can be obtained directly from (11,16), In fact, there are three 
partitions of 3, (3), (21), and (l3). Prom (11.16) we then have 
k = I -M3! 4. (~ 1)1!3![21] (- 1)22!3![13] 
3 n'te!)1!! "*" n(n - 1)2!1U!1! "r 
(3!)1!! ' n(i 
_[3] 3[21] 
+ 
L)2!1I1!1! ' n(n - l)(n - 2)(1!)33! 
2[13] 
n n(n — 1) n{n — l)(n — 2) 
as before. ^ 
It is, however, more useful for practical calculation of the ^-statistics to express them 
in terms of the power sums defined by 
sr = £(xr). . , . . . . (11.21) 
This can be done by expressing the product sums (11.19) in terms of power sums (a 
procedure which may be faciKtated by the use of tables of symmetric functions) or directly 
as follows ;— 
Assume 
Since S(k3) = ks — fi2 we have 
' fi3 = aaE{s3) + a^is^) + «,#(*•}). 
Hence, for moments about an arbitrary point 
p's — 3^i + 2/^i3 = a0(njLt's) + a^n^ + n(n — l)/^/*!} 
+ a^n^ + 3n(n — l)^i + n(n - l)(n — 2)//^},. 
from wliich we find, identifying coefficients, 
1 = n(a0 + ax + a2) 
— 3 = n(n — l)(ax + 3a2) 
2 = n(n — l)(n — 2)aa 
* 
whence, solving for aQ, ax and a2, we find 
Jc3 — -p](w2^3 — Sne2Si + 2sJ). 
11.9. The first eight ^-statistics in terms of the power sums are as follows:— 
n 
** = flj2)(ns* ~~ si) 
&8 = ^is](w'aa - 3ws2si + 2s}) 
^4 = p"]{(w3 + w2)s< - Hn* + ^^i - 3(wa - n)s\ + 12w525? - <$s\} 
&6 = -fBi{(w4 + 5w3)s5 - 5(ra3 + Sn^s^ - 10(n* - n*)s3s2 + 20(rc2 + 2?i)SaS? ' 
n[ 
+ 30(w2 — rc)^! — QOns^l + 24s?} 
(11.22) 
260 
APPROXIMATIONS TO SAMPLING DISTRIBUTIONS 
1ca = -jgj{(w6 + 16?i4 + IIti3 - 4n2)sa - 6(ti4 + 16w3 + llw2 - 4n]a^i 
- 15(ti4 — 4ti3 — n2 + 4w)s4s2 — 10(w4 — 2ti3 + 5ra2 — 4w)s| 
+ 30(ti3 + 9wa + 2w)s4s? + 120(ti3 — n)*,^ + 30(ti3 — 3ti2 + 2w)^ 
- 120(ti2 + 3w)s3sJ - 270(w2 - n)s\s\ + 360ws2s* - 120*}} 
k, = -^{(n9 + 42w5 + 119w4 - 42w3)s7 - l(nb + 42w4 + 119w3 - 42*12)505! 
- 21(w6 + 12w4 — 3>i3 + I8n2)sss2 — 35(w6 + 5w3 — 6wa)s4s3 
+ 42(w4 + 27w3 + 44wa - 12»)a,a? + 210(w4 + 6w3 - 13w2 + Cn)^,^ 
+ 140(w4 + 5w2 - finjajfl! + 210(w4 - 3«3 + 2n*)szs\ 
- 210(rc3 + 13w2 + &n)sxsl — 1260(w3 + n2 - 2w)s3sas2 
- 630(»8 - 3w2 + 271)4?! + 840(wa + 4m)saa* + 2520(7?2 - n)sls\ 
- 2520ns^ + 720sJ} 
&8 = -^{(n7 + 99w6 + 7 Bin6 + 141»* - 398n8 + 120n2)sB - 8(»" + 99»B -f 757»* 
+ 141w» - 398»" + 120/i)aVi - 28(»» + 37ftB - 39»* - 157»» 
+ 278wa - 120n]stflt - 56(wfl 4- 9w5 - 23rc4 + lllw3 - 218/i3 + 120w)sBs3, 
- 35(»B + wB 4- 33w4 - 12l7i3 4- 206t?,2 — \20n)sj + 56(7iB + 68ti4 + 359ti3 
- 8ti2 - mn)s0al + 336(nB + 237i4 - 31ti3 - 23ti2 +' SOnfrtffr 
+ 560(7i6 + 5ti4 + 5w3 + 5ti 2 — flnja^,^ + 420(ti6 + 2ti4 — 25ti3 
+ 46tz2 — 24n)a4a| + 560(?i5 — 4ti4 + 11«3 — 20ti2 + 12t?,)s^2 
- 336(ti4 4- 38ti3 + 99ti2 - I8n)s&sl - 2520(ti4 + 10?i3 - 17«a + 67i)s4s3sf 
- 1680(w4 4- 2ti3 4- 7ti2 — IOti)sls\ — 5040(ti4 — 2ti3 — ti2 + 2n)flafl5«1 
- 630(7i4 - 6ti3 + llTi2 - 67?,)4 + 1680(ti3 + 17ti2 + 12ti)s45{ 
4- 13,440(ti3 4- 2ti2 - 3t?,)s3s8sJ + 10,08O(«3 - 3ti2 + 27i)s:]sf 
- 6720(ti2 4- 5n)saal - 255200(n2 - w)s^sj 4- 2051607isas{ - 5040sJ} 
In particular, we have 
k1 = m[ 
/Co 
71 
n — 1 
Tin* 
w 
3 (72, - l)(n - 2) s 
ht 
71s 
(71 - l)(n - 2)(» - 3) 
expressing the k's, in terms of the moment statistics. 
{(n 4- l)ra4 - 3(n — l)m2} 
(11.22)- 
(11.23) 
11.10. There is a well-known theorem of symmetric functions which states that any 
rational integral algebraic symmetric function of jbi . . . xn can be expressed uniquely, 
rationally, integrally and algebraically in terms of the symmetric sums sr. It can thus be 
so expressed in terms of the &'s, for from equations such as (11.22) the s's can be so expressed 
in terms of the k'a. Thus an investigation of the sampling constants of any symmetria 
function expressible in terms of rational integral algebraic symmetric functions can be 
translated into an investigation, concerning the k's. 
To round off this account of the relationship between the &'s and the s's we may refer 
to two interesting operational properties. Write Kp for the same function of the differential 
I 
SAMPLING CUMULANTS OP ^-STATISTICS 261 
d d 
operators — . . . -— as kp is of the x's, and Sp for the same function of the operators as 
sp is of the x's. Then 
f/S,p =pl , _ A (11.24) 
1^p\sp1spi • ' • SP„J —■ u J . 
where (px . . . pm) is any partition of p other than p itself; and 
8pkp=p] • 1 • 
$qkp = 0, q^p\ <1U5> 
Methods of proof and applications of these results are given in the exercises at the 
end of the chapter. 
* 
Sampling Cumulants of k-Statistics 
11.11. The problem of determining the sampling moments or the sampling cumulants 
of ^-statistics is that of finding mean values of powers and products of those statistics. 
To any number a with partition (a1aia2a' . . . asa') there will correspond a moment 
filial . . . a„a<) = E(k?1 . . . V) .... (11.26) 
and a cumulant k (a^ . . . a*') related to the moments by the identity (cf. equation (3.54)) 
*{,<«,* . . . a.4£ . • . §} = log jW . . . bjjfi ■ ■ ■ ^} • (11-27) 
For example, the fourth cumulant of k2 will correspond to the fourth moment of kz, 
which is the mean value of k\. These quantities will be written k(24) and ,«(24), in 
accordance with (11.26). Again the cumulant k(32) corresponds to the moment ^(32), the mean 
value of k3k2, or their covariance in their joint sampling distribution. Generally, in the 
simultaneous distribution of the k's there will be a separate formula of degree a for every 
partition of a. 
Now the product ka*x . . . kUila» is h6mogeneous and of total degree a in the x's. 
Hence, when mean values are taken //(ft]"1 . . . a/*) will be homogeneous and of total order 
a in the parent /t's. Since the k'h themselves aro of homogeneous order in the //s it follows 
that /c(r/1cti . . . a8a') is of homogeneous order in the k's. Hence we get the first rule for 
the sampling of ^-statistics (which is true of seminvariants generally):— 
Rule 1. «(a.1a' . . . asa>) consists of the sum of terms each of which, except for 
constants, is a product of parent k's of order a. 
For instance, k(24) is of total order 8 and is therefore the sum of terms in k8, KaKa, 
k\, Khk\ and k\. Similarly k(32) will contain a term in k6 and one in /<r3KB and no others. 
As seen in the next rule, no terms in Ki appear (as again is true of seminvariants generally). 
Rule 2. No term in K(a1ctl . . . «/•) contains k1}. except k(1) itself. 
This follows as in 11.5. The ^-statistics are seminvariant and hence their sampling 
distribution cannot depend on the variable quantity /cx. The exception occurs when we 
are dealing with the only statistic which is dependent on the origin, namely klf and here 
k(1) = k± as is evident from the definitions. 
11.12. We now enunciate and illustrate the rules by which the terms in K(af* . . . afla«) 
can be found. As the proof of the vahdity of the rules is difficult to grasp until their nature 
has been comprehended we defer a proof until later in the chapter. 
262 
APPROXIMATIONS TO SAMPLING DISTRIBUTIONS 
To find the term in tcbfl . . . ^J* in K^af1 . . . a/») consider the two-way array 
a. 
a, 
a. 
h 
• • • 
. (11.28) 
a 
where there is a row corresponding to every k in the term Kb Pl . . . Kb £» and a column 
corresponding to every part in «(«] 
<*i 
aH*>). Consider the various ways in which the 
body of the table can be completed by the insertion of numbers whose row and column 
sums are the respective b and a numbers ; e.g. if we are seeking the coefficient of /cuk:| in 
k(422) we shall consider such arrays as 
2 2 2 
1 1 . 
1 1 . 
4 4 2' 
6 
2 
2 
10 
2 
1 
1 
4 
3 
1 
• 
4 
1 
• 
1 
2 
6 
2 
2 
10 
3 3 . 
1 . 1 
. 1 1 
4 4 2 
6 
2 
2 
10 
. (11.29) 
Then the rules by which these arrays give the coefficients of Kbfl . . . KbJ™ are as follows: 
Rule 3. Every array in which the numbers in the body of the array fall into two or 
more blocks, each confined to separate rows or columns, is to be ignored. 
For instance, in the foregoing example 
4 2 
. 2 
4 4 2 
6 
2 
2 
10 
is to be ignored, since the 2x2 block in the top left-hand corner has no row or oolumn 
number in common with the entry in the bottom right-hand corner. 
Rule 4. Subject to the ignoration of terms enjoined by Rule 3, to the coefficient of 
Kbfl - - • 'lKbIm m «(«iai • ■ • asa') there will be a contribution corresponding to each way of 
completing the array (11.28). Such of these as do not vanish are composed of a numerical 
coefficient multiplied by a function of n. 
Rule 5. The numerical coefficient is the number of ways in which the column totals, 
considered as composed of distinct individuals, can be allocated to form the array concerned, 
§: 
m* 
divided by ft! ft 
Rule 6. The function of n, called the pattern function, depends only on the 
configuration of zeros in the array, not on the actual numbers composing it or on the row and column 
totals. The function is given by considering the separations of the rows into distinct 
groups or separates. 
SAMPLING CUMULANTS OF ^-STATISTICS 
263 
(i) With one separate there is associated the number, n, with two separates 
n(n — 1) ... , with q separates n(n — 1) . . . (n — q -f- 1). 
(ii) In each separation we count the number of separates in which a particular column 
is represented by a non-zero entry. If in p separates, we assign the factor 
(-l)p-i(p-pi 
n(n — 1) . . . (n — p + 1)' 
(iii) This is done for each column. 
(iv) The various faotors given by (ii) and (iii) are multiplied together for each separation, 
multiplied by the factor appropriate under (i) and the results summed to give 
, the pattern function. 
Rule 7.. Any array containing a row which consists of a single non-zero entry has 
a vanishing pattern function and is to be ignored. 
Rule 8. Any array containing a column which consists of a single non-zero entry 
has a pattern function - times that of the array obtained by omitting that column. 
Rule 9. Any array the non-zero elements of which consist of two groups connected 
only by a single column has a vanishing pattern function and is to be ignored. 
Example 11.1 
, As an illustration of these rules (which are not as difficult as they look), suppose we 
seek for the coefficient of kqk\ in k(422). If the reader will write down the thirty or so possible 
arrays with column totals 4, 2, 2 and row totals 6, 2, 2, he will find that the only ones which 
do not vanish are those of (11.29) ancj permutations of rows and columns with the same 
sum, namely 
6 
2 
2 
10 
2 
1 
1 
4 
2 
1 
1 
4 
2 
. 
• 
2 
6 
2 
2 
10 
2 
1 
1 
4 
3 1 
1 . 
. 1 
4 2 
6 
2 
2 
10 
3 
1 
• 
4 
2 
1 
1 
4 
1 
. 
1 
2 
2 
1 
1 
4 
3 1 
. 1 
1 . 
4 2 
6 
2 
2 
10 
(a) 
(b) 
(c) 
(d) 
3 
• 
1 
4 
2 
1 
1 
4 
1 
1 
• 
2 
6 
2 
2 
10 
3 3 . 
1 . 1 
. 1 1 
6 
2 
o 
(e) 
4 4 2 10 
3 3 . 
. 1 1 
1 . 1 
4 4 2 
6 
0 
2 
10 
(11.30) 
to) 
With practice the reader will find it unnecessary to write down arrays such as*(c), (d) 
and (e), which are merely obtained from (6) by permuting rows and columns, but for clarity 
at this stage they have been set out in full. There is one trap here to be particularly 
noticed. In array (b) the two columns summing to 4 and the two rows summing to 2 are 
different, and their permutations result in 4 different arrays. But in array (/), though 
the rows and columns are different, there are only 2 different arrays. 
Each of these arrays contributes to the coefficient required. Consider first of all that 
from (a). The numerical coefficient is (j )(').— = 72. The first factor in 
V 2i\111!/ V ^illll/ 2i\ 
brackets is the number of ways of allocating 4 individuals in the partition 2, 1, 1, similarly 
264 APPROXIMATIONS TO SAMPLING DISTRIBUTIONS 
for the second, and we divide by 2! since there are 2 members of the row totals the same, 
this being the only /? factor. 
1 ' 
Under Rule 8, the pattern function is - times that of 
X X 
X X 
X X 
• 
There are five separations of this, one of one separate, three of two separates and one of 
three separates. The contributions respectively under Rule 6 will be found to be 
\nj\nj n 
n(n - \){n - 2U 
n(n — 1)J \n(n — 1)J n(n — 1) 
(-1)22! U (-1)22! I 
n(n — l)(n — 2) J [n(n — \){n — 2)J n{n — l)(n — 2), 
The sum of these is , and hence the contribution from array (a) in (11.30) is 
(n — \)(n — 2) j \ i \ i 
72 
(n - l)(n - 2) 
Now for arrays (b) to (e), which have all the same numerical factor and the same pattern 
function and can therefore be considered togethen For any one the numerical factor is 
\211!llJV3!ll/Vl!l!/'2! 48 
and that of the four together is thus 192. 
Under Rule 6 the pattern function will depend on the configuration 
xxx 
x x 
x . x 
where x stands for a non-zero entry and a period for a zero entry. There are five 
separations of this, one of one. separate, three of two separates, and one of three separates. The 
contribution from the first is 
111 1 
n = - 
n n 7i n2 
for each column has a non-zero entry in the separate. The contribution from the three 
separation's given respectively by isolating the first, second and third row will be found to be 
Iffy trfy J \ I J_ _1_ I _. 
k '[n*(n - l)3 n*(n - l)a ^ n3(n - 1)2J 
2n - 3 
n2(n - l)2' 
The contribution from the separation of three separates is 
n{n 
Din - Q)f ' 21 ~ 1 - 1 "I = ' 2 
)[n \jttyb - 1)(» - 2) n(n - 1) n(n-l)j ~ »"(» - l)2' 
The pattern function is the sum of these three contributions and is thus . 
SAMPLING CUMULANTS OF Jfe-STATISTICS 265 
> 
32 
The contribution from arrays (/) and (g) in (11.30) will be found to be -.—■——%. 
Hence, adding all the contributions together, we find that the coefficient of k^k\ in 
k(422) is 
72 192 32 _ 8(377?, - 65) - 
(n - l)(n - 2) ■' (n - l)2 (n - l)a (n - l)«(n - 2)' 
as shown in equation (11.62) below. 
11.13. Rule 10. The expression for any Kia^1 . . .) which contains a unit part may 
be obtained from that without the part by (1) dividing throughout by n and (2) increasing 
the suffix of one of the k's by unity in every possible way. 
For example, it may be shown that 
Kd . 2k2, 
k(2«) = 2* + 
Hence «r(2*l) = —" + 
n n — 1 
kb 4ksk2 
n* n(n — 1) * 
k(2212)= — -I -I- ^a^s 
w3 %2(n — 1) w,a(n — 1)' 
and so on. 
11.14. The reader may be inclined to doubt whether this rather elaborate 
combinatorial procedure represents much of an advance on the straightforward algebraical 
approach considered earlier in the chapter. A few trials of the two methods in particular 
cases will soon convert him to the former. The division of the coefficients into a numerical 
factor and a pattern function greatly simplifies the method and in fact all the functions 
likely to be required for practical purposes have been tabulated by.Fisher (1928) or can 
be derived thorefrom by an iterative process given by Fisher and Wishart (cf. Exercise 11.11). 
Example 11.2 
To find the variance of the second moment statistic wa. 
From (11.23) we have 
h — n 
n — 1 
Hence var m2 = [ ) var ka 
\ n ) 
- m- 
(22). 
k(22) consists of two terms, one in /c4 and one in k\. The only array contributing to the first is 
2 2 4 
2 2 4 
266 
APPROXIMATIONS TO SAMPLING DISTRIBUTIONS 
with a numerical factor unity and a pattern function -. The arrays giving the second 
are of type 
2 
2 
2 2 4 
If any entry in this were a 2 the row in which it appeared would contain only a single 
entry and hence the array would vanish. The only contributing array is therefore 
1 
i 
o 
l 
l 
2 
2 
2 
(2! \2 1 
—'- 1 . — q= 2. The pattern function will be .found to be 
l 
(n-l) 
Hence 
K(22) = 1± + J^L 
n 7i — l 
- m^ 
var ml 
n 
(n- l)2 (3 -n)(n - 1) 2 
l4 
«,< 
71' 
As 7i becomes large this result tends to 
1 
n 
G"« - A*i)» 
confirming the approximation given by equation (9.9). 
Example 11.3 
To find the third moment of kz we require k(28). This will be the sum of factors in 
kb, KiKa, Kj and k\. 
" The coefficient of the first is —-. For the second we have to consider the array 
n2 , . 
1 
1 
2 
1 
1 
2 
2 
• 
2 
4 
2 
6 
all others vanishing except the two equivalent partitions obtained when the column with 
the singld entry appears in the first or second place. The numerical factor is then 
SAMPLING CUMULANTS OF ^-STATISTICS 
267 
The pattern function is - times that of. 
n 
x 
x 
X 
X 
i.e. is - 
The coefficient of k^k2 is then 
12 
n(n — 1), " "*"" "" ' ' n(n — 1)" 
For the term in k% the only contributory array is 
1 1 1 
1 1 1 
2 2 2 
3 
3 
6 
with a factor ( --^-t) . — = 4 and pattern function —- ~ -. 
\11 1!/ 2! r n{n ~,1)9 
For the last term we have to consider the array 
11.2 
1.12 
. 1 112 
2 2 2 | C 
with a numerical coefficient 8 and a pattern function 
we get 
k(2») = -8 4- ~-—,-v + 
(n 
4(n - 2) 
1)! 
Collecting terms together 
*l + 
8 
— tf8 
—X™-9» 
\2 * 
n2 n(n — 1) n(n — l)2"a ' (n — l)s 
This is also the value of the third moment /w(28) measured about the mean of the sampling 
8k!! 
distribution Ka. We see that if the parent is normal the third moment reduces to ,-—-- -, 
(n — l)a 
i.e. is of order n~2, indicating a rapid tendency towards symmetry. 
Example 11.4 
Few things illustrate the usefulness of expressing the formulae in terms of cumulants 
and the power of the combinatorial method better than the simplification imported when 
the parent population is normal. In this case only terms in k8 survive, all higher oumulants 
vanishing. 
As an illustration let us prove that K(pq) = 0 for normal samples unless p = q. 
The only term which can appear in K(pq) is k^v+{i) and evidently, if p -j- q is odd, even 
this cannot do so. If p -j- q is even we have to consider the array 
2 
2 
p z 
V +2 
268 APPROXIMATIONS TO SAMPLING DISTRIBUTIONS 
Now if any entry in this array is 2 the array vanishes since the row concerned will contain 
only one entry. The reverse can only happen if all the entries are unity, in which case 
the sums p arid q must be equal. This establishes the result. 
Example, 11.5 ' 
Any Kfaf1 . . . a8a') containing n parts is of order n"l'l~1). For example, «:(3a2a) 
is of order w-8. • 
To prove this result we have to consider only the pattern function. Consider the array 
a 
ax ax . . . as • a 
To the single separate there corresponds under Rule 6 the function n(~) = n~^n~lK 
Furthermore, no pattern function can be of greater order in to ; for in an array with more 
than one row, with a separates there is associated the factor nlq]—t-,-~?-t • . . ~r—r, where 
px is the number of separates in the first column containing a non-zero entry, and so on. 
If there is more than one entry in the jth. column the factor n[p? must be at least of order 
n2 and thus the pattern function of order less than w"(5I_1); and if there is only one entry 
in each column the order must be %~(JI~1) unless the function vanishes. Hence the result. 
11.15. By the above methods Professor Fisher worked out the sampling formulae 
for degree not greater than 10, and gave some of the 12th degree. The following are the 
results, with a number of corrections. 
Second k-Statistic 
*(22) = -4 + -M- (11.31) 
n n —1 x 
«(23)=^+ )2KtK\.+ t^ "",?,«§ + ; ^W*I (n-32> 
w2 n(n — 1) n(n — l)2 3 (n — l)2 
m\ _ *« _l 24 , 32fa -2) , 8(4^2 - 9» + 6) , 
K{Z ] ~ n~* +n\n - 1)KbK* + n*{n-\)*K*K* + n\n - 1)» Ki 
_l 144 * , 96(n - 2) 2 48 4 
+ —, T7lK*K2 + —, TTa^a^+T 7Tq4 .... (11.33) 
n(n — l)2 n(n — l)3 (n — l)3 
.„. k10 , 40k6k2 . 80(w - 2) 40(5w2 - 12w -f 9) 
w4 w3(w — 1) w8(w — l)2 nz(n — l)3 
16(n - 2)(6n2 — 12» + 7) „ 480 " 2 1280(w - 2) 
+ n\n - \Y ** + n\n - l)2*a*2 + rc2(rc - 1)3 *6*3*8 
32Q(4?ia - 9w -f- 6) 9 ,_ 480(2?i2 - In + 6) a 1920 8 
+ n*{n - 1)* **** + rc2(n - 1)* *4*3 +^-l)^4"2 
1920(n-2) 384 
+ n(n-l)« 3*2 + (rc.-l)* - ( d ' 
SAMPLING CUMULANTS OF ^-STATISTICS - 269 
,06X 1 . 60 , 160(n - 2) , 240(2wa - 5n + 4) 
.(2e) .-„ + ___ KloKl + ^ _ ^ + ^(„ _ i)f1- ^ KiKl 
96(n-2)(7?i2 —14?i+9) 4(113?i*—5207i8+9507i2--8007a, + 265) 2 
+ n«(n - 1)* *7*5 + rc*(rc - 1)« *8 
-l 1200 2 4800(n - 2) 2400(5wa — 12^+9) * 
' n«(»- l)2*8"2 + »>(» _ 1)8 *'*3*' + n.(ft _ 1)4 *•*«*" 
160(?i - 2)(31n - 53) 2 960(w - 2)(6?ia - 12n + 7) , 
+ w3(rc - 1)* "*** + " rc8(rc - 1)« "*"* 
1920(?i - 2)(9?ia — 23?t + 16) 480(1 In3 - 41wa + 59w — 31) 
J7 z-r= K5KtK3 -j- — —— ; 
n*(n — 1)B • ns(n — l)s 
9600 38400(rc - 2) 9600(4rca -9^+6) 
+ n»(» - l)8 a a + rc2(rc - 1)* B*3 2 + rca(rc - 1)6 K*K* 
28800(2^2 - 7n + 6) 2 960(% - 2)(5n - 12) 4 28800 
+ II«(lI - 1)« •*lK3'Ca + tt2(W _ 1)6 "* + ft(ft _ 1}4 K*K* 
38400(71 - 2) 3840 
+ n(n-l)« J*2 + (T^Tp *a (1L36) 
Third Jc-Statistic 
1 Q Q fi« 
k(32) = -k0 + -^-«4Ka + -4 + ~ -4 .... (11.36) 
n n — 1 w — 1 (w — l)('/i — 2) 
/o,v 1 , 27 27(3?2, - 4) 27(4?l - 7) 
wa »(n — 1) w(w — l)2 n(n — l)2 
54(4* - 7) , 162(fin-12) 36(7n2 - 30» + 34) 
+ ()l«l)i(n-2) B 2 ^ (n - 1)> - 2) K4K3"a + (» - l)2(/t - 2)a~ *3 
■ 108n(5»-12) a 
+ (n - l)»(n - 2)« ' s ( } 
1 , 54 , 108(2n - 3) , 27(17wa - 49?i + 35) 
' nz n2(n — 1) n2(n — l)a. w2(n — l)3 
108(7?i2 - 20ft + 16) 27(17?ia - 47?i + 39) , 27(37?i - 70) 2 
+ £i"(n - 1)» "7"B + »»(n - l)3 KS + ^-l)a(W-2)K8"2 
324(19?i2 - 67?i + 54) 162(65w2 - 245?i + 234) 
7i(rt — l)8(?i — 2) n(n — \y\yi — 2) 
108(82w3 - 481wa + 958n — 640) 2 108(59?ia — 220n + 224) 2 
+ " n(n - l)*{n - 2)2 "9K'a + w(n - 1)3(tT=^2J "*"* 
324(75?i3 - 473ua + lOlHn — 756) 
n(n — l)3(w — 2)2 
27(173?i4 - 1503w8 + 4962wa - 7380?i + 4200) 3 
+ ' n{n - l)3(n - 2)3 T-"4 
108(7lwa - 263?i + 234) 8 648(79?ia -'343?i + 378) 2 
+ .(n - l)*(n - 2)a "8K2 + (n - mn - 2)2 "B"3"2 
486(63w2 - 290w + 352) 2 972(99w8 - 688n2 + 1612?i - 1280) „ 
+ (n - lj8(w - 2)a K4^2 + (n - 1)8(?* - 2)8 ^^ 
270 APPROXIMATIONS TO SAMPLING DISTRIBUTIONS 
162(87ti3 - 594rc2 + U20n - 1176) 4 972(23ti2 - 103ti + 118) 4. 
+ (n - l)»(n -2)3 ^ + (n - l)»(fi - 2)8 KiK* 
. 648?i(103?i2 - 51 On. +640 2 8 648?ia(57i - 12) 8 . 
+ ' {n _ i)8(?l _ 2)3 *3*2 + (n - 1)«(» - 2)3 *« • • • (u-38> 
Fourth Ic-Statistic 
'Q^ 1 , 16 ,48 , 34 72n , 
x w n — 1 n — \ n — 1 (w — l)(w — 2) 
+ (n - l)(n - 2) *3*a + (» - l)(n - 2)(n - 3) 2 * ( d9; 
1 , 48 , 16(13n - 17) , 12(41% - 65) 
v ' wa w(w — 1) n(n — l)2 _ 7j(ti — l)a 
48(16n-29) 12(37n - 70) ■ 72(1 In - 19) , 
+ "71(71 -l)a" ' 5 + ~n(n - l)2 6 + (n - l)2"(?n"2) Cb"3 
, 288(19w - 41) . 48(203tj - 523) 
(n — l)2(ft — 2) (n — l)a(?i — 2) 
144(5671a - 257to + 302) a 1440(4ti - 11) 2 
+ („ J i)i(„ _ 2)a *^8 + (n~- l)a(w 4- 2) *5*a 
• , 1152(22ti2 - 106?i + 133) , 8(709wa - 3430??, + 4456) , 
+ -1 (w_1)2(?l_2)2 *5*4*8 + (n - l)*(n - 2)~a *4 
288(19% 3 - 98?i2 + 125w + 2) , 1728(24^- 140%2 + 200ti + 4) a 
+ (n - \)\n - 2)2(7i - 3)_ K°Ki + " ~"(n"17"!)^ -~2)"(n - 3) *6'C3*2 
432(497i3-2877ia + 4087i+12)2 , 864(1037z3-629w2+948?a+24) „ 
, + (7l-l)a(7l-2)2(7l-3) ***■ + ~ (7l-l)2(W-2)2(7l-3) "4"3"a 
288(4l7iA-3847i8-)-1209yia-128271-36) 4 288ti(53712-17971-52) 4 
+ (7i-l)a(7i-2)2(w-3)a *3 + (7i-l)a(7i-2)2(?i-3) *4*2 
1728n(29n* — 196w2+ 31771+^2) , , , 1728w(71 + 1)(712-5ti+2) , 
+/ (n-'^V-^'^'-a)'"" "sK (7i-l)2(7i-2)a(7i-3)a *2 * ( 0) 
Fifth Jc-Statistic 
,R2\ 1 , 25 100 ' 200 , 125 a 
7i 71—1 71 — 1 W — 1 71—1 
20071 ' „ , 1200n . 85071 
+ 7 Tw 1T\ KoK2 + 7 Tw ?T\ kb«s«2 + -. ^-. --. K2Ka 
(n — 1)(ti — 2) (ti — 1)(tj — 2) (ti — l)(n — 2) 4 
150071. 2 60071(71 + 1) 3 
+ (n - l)(n - 2)"4"8 + (n - l)(n - 2)(n - 3) K4"2 
18007a(7i + 1) , 12(^a(7i + 5) ■ 
^ (n - l)(n - 2)(n - 3) 3 2 ^ (n - l)(n ^ 5)(n - 3)(n - 4) 2 * * ^1"41' 
PRODUOT-CUMULANT FORMULAE OF ^-STATISTICS 271 
Sixth k-Statistic - 
k(62) = I Kl2.-\ ?— (36 k10k2 + 180k9k3 + 465k8k4 + "780k7k5 + 461k*) 
71 71 —— 1 
+ 7 ,w „•; (450«8^ + 3600k7k3k3 + 7200k„k4kb + 6300k6k§ 
(w — l)(n — 2) 
+ .4500k|k:2 + 21M0K6KtK3 + 4950k£) 
+ / -,w(,> +ow 5\ (2400^^3 + 21800*.^ + 15300^ 
(n — l)(n — 2)(n — 3) 
+ 54000k ^k, + 8100k|) 
+ -, ~^SvT1^/ 7-,(5400^/f| + 21600^) 
(w — l)(w — 2)(w — 3)(w — 4) . 
+ n<n + 1^?l2 + 15W ~ 4J 720.1 (11.42) 
(n - l)(n - 2)(n - 3)(n - 4)(n - 5) 2 v ' 
Product-Gumulant Formulae 
l ft 
k(32) = - *b -) k3k2 ...... , . . (11.43) 
n w — 1 
18 6 F 
k(42) = - k0 H = k4k, H r*s (11.44) 
n n — 1 w — 1 
k(52) = -k7 -) KBKa -) k4k3 ........ (11.45) 
n n — 1 n — 1 . 
1 , 12 ,30 , 20 a ... 
k(62) =- k6 +. kbk3 H kbk3 -) -k| (11.46) 
ww — 1 n — \ n — 1 
1 14 42 70 
k(72) = - k„ -) -k,kH 7*6*3 H *5*4 (II-47) 
n n — 1 w — 1 n — 1 
k(82) = -Kl0 + rfs^a H rf?f« H 7*6*4 "I 7*6 • • • (H-48) 
n n — 1 n — 1 w — 1 w — 1 
1 , 12 .30 36n 3 ^ m 
k(43) = -ic7 H -K&i H 7*4*3+ , Tw ov^^i • (11.49) 
x ' n n — 1. w — 1 (n — l)(n — 2) 
k(53) =iKB + —?— (15KaK2 + 45*,*, + 30^) + (60^,4 + 90/c!|,c2) . (11.50) 
71 w — 1 [n — L){n — Z) 
k(63) = -k0 H L_ (18k7k2 + 63KflKa + 105k0k4) 
w w — 1 
+ / ^ ox (90^i + 360k4k3kb + 904) (11-61) 
(n — l)(n — 2) 
*(73) =-k10 + _1_(21k8kb + 84«7«3 + 168k6k4 + lOSfcg) 
w w — 1 , 
+ _ ^^ _ 2) (126^*1 + 630kbk3k3 + 420^, + 630k4k|) . . (11.52) 
272 APPROXIMATIONS TO SAMPLING DISTRIBUTIONS 
#c(54) = - k, H {Z0k7k2 + 10kbk3 + 120kbk4) 
71 71 — 1 
7h 
+ . _ 1)(>> _ 2) (120k.*» + 600k4k3k2 + 180**) 
+ 7 ^/"W, ^240^1 (11.53) 
(n — l)(n — 2)(n — 3) 
k(64) = -k10 + —L-(24*BKf + 96k7k8 + 194k„k4 + 120k|) 
n n — 1 
+ _ ^ _ 2) (180^/ff + 1080kbk3kb + 720^s + 1260k4k3) 
w2 w(w — 1) n(n — l)2 n(n — l)2 
+ (rc--l)2*J** + (ft - l)2*»*a (1L56) 
/«*« 1 , 24 20(3u - 4) 20(5u - 7) 
n% n(n — 1) n(n — l)8 n(n — l)2 
120 „ . 480 . 120 ■ s ,'_ „, 
+ (F3Tp^i + (^Tiy2^^+ (^7^^ (H.57) 
/«<m 1 ' 28 , 12(7% - 9) 4(41» - 56) 
w2 w(?i — 1) n(n — l)2 w(n — l)2 
. 20(5n - 7) , , 168 . , 840 , 560 . 
w(w — l)2 (w — l)2 s (w — l)2 (n — l)2 
1 ; TVt>K*K3 .. .■>....,. (11.58) 
(w — 1)* v ' 
._29. 1 21 6(8n - 11) , 9(3w - 5) . 
"(3 2) - J? *B + ^^T) "8Ka + n(» - 1)« *B*3 + ^1? * 
18(fln -11) 18(9n - 20) 36rc 
+ ()l _ i)i(n _ 2)K'K* + (n- l)*(n - 2) *3*a + (n-l)«(»-2)*« ' (1L°9) 
/*oo\ 1 , 26 24(3n-4) 10(lln-17) 
W2 W(W — 1) ?l(?l — l)2 ?1(W. — l)a 
36(5n - 9) 2 12(61n - 128) 36(5?i - 12) 
+ (n - mn - 2) "B"2 + (n - l)a(w - 2) *4*3*S! + (» _ i)i(„ _ 2) "8 
360w , 
+ (n - l)»(n - 2)^3,fi (11.60) 
,KOO, 1 , 31 , lOln - 131 . 5(37w - 55) 
jc(532) = — k10 + — k6k2 H ^ —— K7Ka + -i- rro'^^* 
w2 ?i(w — 1) w(w — l)2 w(?i — l)2 
5(23rt - 35) o -30(97i - 16) 2 30(45n - 92) 
+ n(n - l)2 ^ + (n - 1)> - 2) K°K* + (n - \)*{n - 2) "*"*** 
PRODUCT-CUMULANT FORMULAE OP ^-STATISTICS 273 
_,_ 60(15to - 31) 2 30(45to-103) 2 720ro 
~*~ (to - 1)2(to - 2)*4*3 + (to-1)2(to -2)/C4"3 + (to - l)2(n - 2)*4** 
+ (^TTTp(w - 2)^ (U,61) 
,,»„, 1 . 32 8(13w - 37) , 4(49ro - 73) 
toz , to(to — 1) n(n — l)2 to(to — l)2 
+ t^-pLszM* + i <K37> -.65) ^ + 
1536 
TO(TO - l)2 5 (TO - 1)2(TO - 2) 2 (» - 1)2 
K5K3K2 
144(7to - 15) 2 72(21to - 50) 2 96(10to2 - 27to - 1) , 
"T 7~ i »2/, ^r ^i^a i j- TVzTZ, o\ K*Ka T" 7- , »9/ ^77— ^r KiKz 
(to - 1)2(to - 2) * (to - 1)2(to -2) 3 ' (to - 1)2(to - 2)(» - 3) 
144(17to2 - 53to - 2) 192(n)(n + 1) , 
^ (to - 1)2(to - 2)(to - 3) 3 2 ^ (to - 1)2(to - 2)(» - 3) 2 ' 
W432)-I^' 1 33 K « ,6(19^-25) 3(6571 - 107) 
to2 to(to — 1) to(to — l)2 to(to — l)2 
6(19to - 34) 2 18(19n - 33) 2 72(23to - 62)- 
n(n — l)2 (to — 1)2(to — 2) (to — 1)2(to — 2) 
(11.62) 
54(19to - 48) 2 54(33to2 - 148to + 172) 2 2 
(n _ i)2(w _ 2)*4*8 + (to - 1)2(to - 2)2 *4*3 
72to(17to - 40) ' 108to(27to - 70) 216TO2 
+ (TO - 1)2(TO^2)2 * 2 + (TO - 1)2(TO - 2)2"3 2 + (TO - 1)2(TO - 2)2*2 " lll"MJ 
,0o3x 1 , 30 , 2(31to - 53) , 12(9ro2 - 23to + 16) 
*(323) = --/<(, + - ;-;*?*■ + ■-— fv--««fa + u TVs *»** 
' TO3 TO2(TO — 1) TO2(TO — l)2 TO2(TO — l)8 
to(to — l)2 to(to — l)3 to(to — l)3 (to — l)3 
„ „v 1 36 , 4(23to - 37) . 4(47to2 - 120to + 81) 
x ' 743 TO2(TO — 1) TO2(TO — l)2 TO2(TO — l)3 
12(9to2 - 24ro + 17) 360 288(5to - 7) 
+ ^TTTp *5 + ^^llp^ + to(to - l)3 ~^3"B 
+ 144(7to -10) ^ + 2^49to_-95)k ^ + ™ 2160 
to(to — l)3 to(to — l)3 (to — l)3 [n — l)3 
1 37 , 6(17to - 27) , 3(61to2 - 166to + 117) 
V ' TO3 TO2(TO — 1) TO2(TO — l)i TO2(TO — l)3 
2(59to2 - 154ro + 113) 2 6(67ro - 131) 0 
+ ^«(n _ 1)3 *5 + ^(w _ i).(n _-2)"6"8 
24(7 Ito2 - 246to -f 202) 36(29to2 - 103to + 93) 2 
+ —to(to - 1)3TtoT=T2)- k*K>k> + ~n(n - l)3(W"-"2) ^a 
36(38to2 - 155to + 160) 2 72(14ro - 23) 3 
+ n(n _ i).(n _ 2)~ "4"3 + (to^1)3(to~- 2) K'K* 
144(19ro-44) 288to__ 
^ (TO - 1)3(TO - 2) 3 2 ^ (TO - 1)3(TO - 2) 2 
A.S. T 
274 
APPROXIMATIONS TO SAMPLING DISTRIBUTIONS 
11.16. Additional formulae for the case of a normal parent population have been 
worked out by Wishart (1930). There are two general formulae:— 
Km = 2r~1{r - 1)! k ' 
K[l} (n- l)'-i 8 
)' 
k{vPVF) = -j—i—~~: 'tt KorK(pq) 
■ (ta - i)! (» - i)r 
. (11.67) 
. (11.68) 
and the following specific formulae of degree 12 and upwards (those of degree 10kand lower, 
of oourse, being derivable from equations (11.30) to (11.66) by putting all k's higher than 
the second equal to zero). 
K(3,2«)== 34,660* 
[n - l)b{n - 2) 2 
K(W) = 7776n»(5n, - 12) 
K{SZ)-(n-l)Hn-2)*K* 
v ' (n — l)6(n — 2)3 
_ l,741,824n.»(fin - 12) 
«**)- ' {n _ m% _ 2)»" 8 ' 
466,560ra8(22?i2 - 111% + 142) fi 
*{3} (S"~i)»orr2)« K* 
k{3°2) = 
18 
% - 1 
k2k{&) 
«(3a22) = /„36°1^^(36) 
k(4222) = 
k(4228) = 
(w - l)2 
1920n{n + 1) fl 
(n - l)*{n - 2){n - 3)*2 
23,0407^(71 + 1) ^7 
(w - 1)4(% - 2)(n - 3)" 
Ki 
W4«24) = 322,56e»(n + 1) 8 
^4 ^ ~ (n _ i)6()l _ 2)(n - 3) 8 * 
__ 20,736ufo + l)fo* - 5% + 2) 
1 ' (n - l)*[n - 2)*[n - 3)2 8 " 
W4822) = 290,304n(?i + l)(^a - 5n + 2) 
^ ' (n _ i)«(w _ 2)> - 3)2 8 
= 4,644,864%(ra + l)(rc2 - fin + 2) . 
^ ' (» - l)e(% - 2)2(?i - 3)2 8 
^ 
(11.69) 
(11,70) 
(11.71) 
(11.72) 
(11.73) 
(11.74). 
(11.75) 
(11.76) 
(11.77) 
(11.78) 
(11.79) 
(11.80) 
(11.81) 
"W = {n - iy{T-^fr - B)> {53ra* ~ 428%8 + 1025W2 - 4?4W + 18°}k|- (11-82> 
k(4*2) = 
k(4422) = 
16 
n - 1 
*bk(44) 
288 
(n - l)2 
"M*4) . 
K(*6) = —ri— Kl° approximately . 
n< 
. (11.83) 
. 011.84) 
. (11.85) 
PROOF OF THE VALIDITY OF THE RULES 275 
i 
In virtue of the result of Example 11.4, expressions of odd degree vanish, e.g. 
K(32r) = K(52r) = 0. Further, in virtue of (11.68), K{p2r) = 0 if p > 2, for K(p) = Kp = 0 
for the normal distribution. Methods of proof of (11.67) and (11.68) are suggested in 
Exercise 11.9. Exact results for k(46) and k(4g) are given by Hsu and Lawley (1939). 
Proof of the Validity of the Rules 
11.17. We now proceed to prove the validity of the rules enunciated and exemplified 
above. Rules 1 and 2 have already been proved. 
As a preliminary let us define an operator dp such that 
dv(j!r = r{r - 1)' . . . (r -p + l)^_p r > p" 
• dvHp=P]- \ . • (H.86) 
and dp{AB) = (dpA)B + A[dpB) (11.87) 
so that d acting on a product is distributive. 
In virtue of (11.87) we have 
= ^ (t*'r)m h?r 
It follows that if / is a polynomial function in the ^'s 
dJ = -XdPVi+Xdv^+ • • • .(11.88) 
t OjLli 0/J.2 
and this also holds if / can be expanded in a series of polynomials in the //'s. 
Now consider the expression defining the seminvariants in terms of the moments (3.11) . 
exp ( Kit. + . . . + kp— + . . . J = 1 + ju[t + . .". t-^- + . . . 
On operating on both sides by dp there results 
exp Ik^ + . . . + kp— + . . .AldpK-J, + . . . + dpKp— + . . .J = V> + //I^ + • • • 
= ^(l + Pit + . . . fj,'p— + . . .) 
and hence , 
dvKit + . . . + dpKp— -f • • ■ = tp. 
This is an identity in t and hence 
dpKp = p\ ) 
dqKp=0 q^P) 
For example, 
Ki = Pi — *A*3/*'l _ 3/")i2 + 12i"'2/fl2 — 6/*'l*f 
diKt ='4//, — 4^3 — 12/w'2ju'1 — 12//2/Ui + 24/j^ +.24^^ — 24^ 
= 0 
diKi = 12^'8 - 24^2 - 12/^ + 24^2 
= 0 
d3Kt = 24^ — 24^ 
= 0 
diKi = 4! 
(11.89) 
i„'3 
276 
APPROXIMATIONS TO SAMPLING DISTRIBUTIONS 
11.18. Now in accordance with Rule 1, whioh we have already established, «(a1ai. . . asaa) 
and hence //(aiai . . . aaas) may be expressed in terms of parent k's by an equation of the 
form 
fi(a^a^ . . .) = Z{A{Kb*- k6A . . .)} • ' • • (H.90) 
where A is a factor which it is our object to find. Operate on both sides of (11.90) by 
(<?6 A Bbf' . . .). Every term on the right is annihilated except that in faf1 Kbf* . . .) 
and we have ' 
4.(&il)ft {b,\fi . . . |8il ft! . . . = {dbf* a6A . . . ){x{a^a^ . . . ) . (11.91) 
We now consider an operatpr 6P, analogous to dp, which, when acting on a power of x (of 
any suffix), reduces the exponent by p and multiplies by r(r — 1) . . . (r — p + 1); and 
we will suppose the operator to be distributive.* Regarding ^(a^1 a2a' . . .) as the mean 
value of (&attl ha*1 . . .) we see that the result of operating by the d'a on the mean value is 
the same as that given by taking the mean value of the operation of the d'a. But this 
latter operation results in a constant, which is equal to its mean value ; and we thus have 
A = (W)* (W)A ■ ' ■' h 0.1 [K^ *** " '}' ' ' ' (11'92> 
Our rules are concerned with the evaluation of this operation. 
11.19. Consider now a completed array of type (11.28). A little reflection will show 
that there is one such array for every term in (11.92) which does not vanish by operation, 
and that every term in (11.92) will have its corresponding completed array. The numbers 
in the body of the array are the powers of a; occurring in the ^-product.; added horizontally 
they compose the orders of the operators ; added vertically they compose the orders of 
the corresponding k's. A completed array is, so to speak, a chart of part of the operation ; 
and the whole operation is the sum of all possible completed arrays. 
The operation (11.92) gives us the coefficients in ^{a^ asa« . . .), but we wish to find 
those in the corresponding K(atai aaa« ...)-. The necessary allowance is made by Rule 3, 
which we now prove ; that is, the coefficient of (k^1 Kbf* . . .) in «(a1ai a2a' . . .) is given by 
all completed arrays, ignoring those which are resolvable into separate blocks each confined 
to separate rows and columns. 
. Referring to equation (11.27), expressing the relation between multivariate moments 
and cumulants, we see that «:(a1ai af1 . . .) is the sum of terms composed of products of 
one, two, three . . . multivariate moments. The first term is p.{a^ aaa« . . .) itself. 
Consider a two-part term such as ^(a^*'1 aBa'« . . .)//(a1a'1 aBa'* . . .), where a'x + a.[ = a1} 
etc. Its coefficient in the expansion on the right-hand side of (11.27) is 
2' t "»' t "■ 
1! 1! a.[\ oc^I 0C2! a^l 
and hence the coefficient with which it appears in the formula for ^af1 aaa» . . .) is 
—r\—777 ~. 771 • • • == I ~~r ]( ~~t J • • • . • • (11.9o) 
ax! ax I a2I 02 ! \ai/\a2/ 
Now fiia^ a/1 . . .) will itself have an array of type (11.28) with column totals (af'1 aBa'«. . . 
and row,totals, say (b/1 b/1 . . .); and similarly for /u(a1a'» aBa'« . . .). Provided that 
/ dp dp Bp \ 
* 6P may be regarded as equivalent to ( —- 4- 5—- 4- • • . z-— \, i.e. to Sp in the notation 
\ ox{y ox2p oxjf J 
of 11.10. 
PROOF OF THE VALIDITY OF THE RULES 277 
f}'t -\- fa z= Pi these arrays will correspond to terms in the k's which, when multiplied, will 
give a term in (kl^ k^ . . .). Thus the product of these terms may be considered as an 
array of type (11.28) with oolumn totals («xai «aa" . . .) and row totals (6/1 6/» . . .) and 
with the body of the table resolvable into two separate blocks. Since there are <xx columns 
of total -au there will be I M f ,21 products of this type in the expression which gives 
ixfaf1 aBa» . . .). This factor is the same as (11.93) but of opposite sign. Hence, if we 
ignore the separate two-part blocks in the array for fj. we shall have allowed for the products 
of two moments which must be subtracted from p to give k. 
Now some of these separate blocks will themselves be separable into two blocks, and 
in subtracting them all from fi(a1'Xlaia' . . .) we subtract too much. For example, if there 
are three separate blocks, L, M, N, we shall, by considering L and (M + N) as two blocks, 
have subtracted L, 31, N. We shall have done the same by considering M and {L + N), 
and N and (L + M) as two blocks. That is, we' have subtracted 2L, 2M, 2N too much. 
We must restore these blocks to the array for ju again. Such additions, summed over all 
blocks of three, will be found to equal the terms in the expansion of (1L.27) which result 
from the product of three moments. > 
In restoring these blocks we restore too many of the cases where there are four separate 
blocks. These must be subtracted again, and correspond to the negative term in (11.27) 
involving the product of four moments. Proceeding in this way we establish Rule 3. 
11.20. Now we proceed to Rules 4, 5 and 6, which are the fundamental rules of the 
whole process. Consider again the array of type (11.28) to fix the ideas, say, 
2 
1 
1 
4 
3 
1 
• 
4 
1 
B 
1 
2 
6 
2 
10 
. (ir.94) 
This array will represent a number of terms in the operation, each of which consists of the 
operation of,0o on a term x%.xz.x (the first row), 0B on x.x (the second row), and so on. 
Provided that the suffixes of the x'a in any row are alike, every suffix of the x'a will provide 
a term, for Jcp contains terms with every distribution of powers (adding to p) and suffixes. 
There will, for instance, be terms of the following kind:— 
A 
it 
x3 
x\ 
z2 
. 
xx 
• 
*». 
x\ 
xx 
%>i 
x\ 
xl 
. 
xx 
• 
x-ii 
0 
X{ 
xl 
xl 
x\ 
xx 
• 
xx 
- 
dC\ 
In fact, for any completed array, we have terms in which 
(i) all the x'a have the same suffix (n in number, one for each suffix), 
(ii) all the x'a but one row have the same suffix (n(n — 1) in number), 
(iii) all the x's but two rows have the same suffix and the remaining two are the same 
(n(n — 1) in number), 
and so on. These cases correspond to the various separations dealt with in Rule 5. 
Now in case (i) the term in any column arises from the term in a* in Jcp and (apart 
from numerical factors which are considered presently) is n~1, from equation (11.16). 
Hence any column which contains an entry contributes a factor n~l and the total function 
278 APPROXIMATIONS TO SAMPLING DISTRIBUTIONS 
of n arising from case (i) is the product of n and of {n~l) to the power of the number of 
columns containing a non-zero entry. 
Similarly in cases (ii) and (Hi) the w-function for each separation is the product ol 
n(n - 1) and, for each column, a factor in n~l or ~ - according as the column contains 
non-zero entries in one or in both parts of the separat on; and so on. 
This explains the origin of the pattern function as described in Rule 0. But in order 
to establish that rule completely (and incidentally to establish Rules 4 and 5) wo have to 
show that the numerical coefficients arising from each separation are the samo. When 
this is done the validity of Rule,6 is demonstrated,' for the separate contributions in n may 
be added together to give the pattern function and the whole multiplied by the numerical 
coefficient. • ' 
0X may be considered as the operation of picking out an x from tho operand in all 
possible ways and replacing it by unity. Similarly -^ may be regarded as picking out 
id z's with the same suffix and replacing them by unity. It is thus evident that operating 
on a & product by a 0 product bl bl of the same degree will yield a result which iH 
Oy. Oj! ... 
the number of ways in which sets of x'a can be picked out of the k product ho that, each 
set contains 6X of one suffix, b2 of a second suffix (which may be the same as the first), and 
so on. 
Now consider the operation (11.92) in which the k's are expressed in the shnpliiieu 
form (11.17). The operations 0 being distributive, we shall emerge from the operation 
with a sum of terms comprising all the possible ways in which the individual .t'h can bo 
picked out of the k product such that the row and column totals of the two-way array are 
satisfied. Consider the sets corresponding to a particular array, such as (11.{)-!). Tho 
contribution to the total will consist of the ways of picking out individuals Hitch that 
, (i) from the individuals in the first &4 are chosen four in tho partition (2, 1, 1), 
(ii) from the second kt are chosen four in the partition (3, 1), 
(iii) from.the k% are chosen two in the partition (1, 1), 
(iv) these are associated in all possible ways such that individuals in a row arise from 
the same suffix. 
On consideration it will be seen that the total number of ways of doing this is the number 
of ways of allocating the individuals from column totals as required by Rule 15 ; and this i,i 
true whether sets of rows have the same suffix or not. 
Rules 5 and 6, and hence Rule 4, follow at once. 
11.21. The remaining rules are ancillary. 
Rule 7 follows from Rule 2. In fact, the pattern function is independent of the number* 
composing the array, and the pattern with a row containing one element can therefore 
form the skeleton of an array in which that element is unity; and this would entail the 
appearance of k15 which by Rule 2 is impossible. 
Rule 8 Mows from Rule 6. The column containing the single element appears in just 
one separate of all the separations, and the contributions to the pattern function are Uuih 
all multiplied by n x owing to its presence. 
Rule 10 Mows from Rule 8. The addition of a unit part is equivalent to the addition 
of an extra column containing unity. This multiplies all pattern Amotions by -- luavea 
PROOF OF THE VALIDITY OF. THE RULES 279 
numerical coefficients unchanged and increases the suffix of every k according to the row 
in which the unit appears. 
11.22. There only remains to prove Rule 9. Note that any pattern function can be- 
evaluated linearly in terms of the functions of the pattern obtained by omitting one of 
the columns. For example, consider the right-hand column of 
XXX 
* X * * '- (H.95) 
XX.. 
and the contributions to the pattern function from it. The 15 separations which are 
possible with four rows can be divided into two classes, that in whioh the two rows 
in the fourth column lie in the same separate and that in which they do not. In separations 
of the first type the contributions from the first three columns will be the contributions 
of all separations of 
xxx 
XX (11.96) 
XX. 
in which the first two rows are amalgamated. Considering the function of the first three rows 
x x 
...... (11.97) 
X X 
in which amalgamation has not taken place, we see that the contribution consists of all 
contributions which do not occur in the first. Calling the first A and the second B, we see 
that the contribution is 
JU _L_._IB-A)-- — A - — --1—v B, 
n n{n — 1) ?* — 1 n(n — 1) 
i.e. a linear function of the derived patterns A and B. The proof of the general result 
follows exactly the same lines. 
Now if a pattern may be divided into two groups connected only by a single column 
we can reduce it step by step by omitting the other columns. We end up with this single1 
column, and the pattern funotion of this column must vanish ; for the column total 
a corresponds to ku, whose mean value the one-column array expresses, and since by definition 
this mean value is kh no composite terms such as would be given by two rows or more 
can appear. 
11.23. As an illustration of the way in which the sampling formulae can be used to 
approximate to a sampling distribution, let us consider the distribution of V^i m samples 
from a normal population. We have, in terms of the sample moments, 
„ _ mn _ n — 2 ka 
m,' -\/n{n — T) k/ 
For a normal distribution the variance of fc3, k(32) is, by (11.36), equal to 
Gn 
(n - l)(n - 2)' 
280 APPROXIMATIONS TO SAMPLING DISTPvIBUTIONS 
We therefore consider the statistic 
x 
n — 1 
= /fo-y-2)^-, .... (11.98) 
V&i (11.99) 
var 
V{6(^ - 2)} 
which will, to order n~x, have unit variance. We have 
,_ /(» - IX" - 2) h!l +k'~ "']-' . . .(11.100) 
V Qn Kz*l k2 J 
Since the population is symmetrical the mean value of # is zero. We then have, expanding 
(11.100), 
x* = (* ~ lKn ~ 2>. i_ (jbj - - ££(*, - *,) + I *§(*, - *a)2 - i? h\{k, - k*V 
071 Kg [_ Ka Kg Kg 
+ "*5(*. - **)* - ^fci(fcB - *2)5 + -6m* - *B)fl + • • \ . . (n.ioi) 
K2 Ki K% . J 
The variance may be obtained by taking mean values of both sides, and since kb is the 
mean value of k% we have 
^ _ (, - l)in - 2)U{3i) _ iM3a2) + 6 (3a22) _ !%(3*23) 
OTt Kg ^ Kg Kg Kg 
IK Ol OQ "} 
+ i^(322*) - ^,u(322<) + -?a*(3*2") + . . A (11.102) 
Kg= K| Kg J 
We now express the product p'a in terms of product k's by using equation (11.27) 
and identifying coefficients. For a normal distribution K(32r) = 0 and we will take our 
approximation to order n~i, so that k's of five parts or more may be neglected. We 
then find, 
var x = (* ~ ^ "" 2^l"tt(3») - -k(322) + -i{*(3222) + k(32)k(2*)} 
- ^{k(3223) + 3k(322)k(22) + k(32)k(23)}+ I? {6k(3222)k(22) + 4k(322)k(2°) 
21 
+ k(32)k(24) + 3k(32)k2(22)} - _{15k(322)k2(22) + 10k(32)k(28)k(22)} 
+ i8.15K(32)K3(22)"I (11.103) 
K% J 
Substituting the values of equations (11.31) to (11.85) we find, after some purely algebraic 
reduction, 
6 28 120 
var x -1 - —- +_^rry2 - ^r=iy3+ ■ ■ • 
6 , 22 70 . 
= 1 h -= = + (11.104) 
n n2 n* v ' 
In a similar way (for details, see E. S. Pearson, 1930) we find 
1056 , 24,132 
Ufa) = 3 - _ + -J— - (11.105) 
(jbt{x) is ^ero, for the distribution is symmetrical. 
THE MULTIVARIATE CASE 
281 
Thus it appears that as n —> oo the second moment of x tends to unity and the fourth 
moment to 3, which is in conformity with tendency to normality. But the tendency is 
by no means very rapid. When n = 100 the variance is approximately 0-942 and in 
assuming a? to be distributed with unit variance we should commit an error of about 
6 per cent. 
11.24. There are two ways of improving on the first approximation that x is normally 
distributed with unit variance. In the first place we may consider a transformation to 
a new variate £, chosen so that £ is normally distributed to order n~2. Secondly, 
we may fit a Pearson curve to the distribution of x, using the values of moments given 
by (11.104) and (11.105). The appropriate curve is the Type VII 
■dF oc 
*2\ —7/1 
/l + ~ \ dx (11.106) 
The first line was adopted by Fisher (1928), who obtained the following transformation,: 
+1+£.) - li1 - ^y - s*> - s <* ■ -io*3+16*> • (n-io7> 
I = x 1 
The second was adopted by E. S. Pearson (1930), who tabulated the 1 per cent, and 5 per 
cent, significance points of (11.106), thatis to say the values of the deviates x for various 
values of n such that 99 per cent, and 95 per cent, of the total frequency of the sampling 
distribution falls within a range of ± x on each side of the mean. ^ 
The Multivariate Case 
11.25. The foregoing results can be generalised to the multivariate case, and we 
give an outline of the extension to that of two variates. / 
Given any bipartite number pp' we shall have for any partition {(i3]i)'l^1C2?!#>2^, ■ ■ ■} 
and the bivariate cumulant kpj/ a ^-statistic kvv, wIiobo mean value is kvp>. Explicitly 
* , - P» PM r (- iy~^ - *>' r {x^y^' x»Viy*Pl' ■ • • X>V'*M tii im 
kpv.-p\p\L _ L W)ni W)ni m mmni[ntlmmm ■ (H.108) 
In particular, corresponding to (11.22) we. have 
fCii 
«?ai 
#31 
~Tvi \nsn sio ,soi) 
—|3j (»2«il — 2^io fin — nsta S01 + 2*f0 Sqx) 
n 
[4] {n2(n + l)sax — n(n + l)s30 s01 — 3n(w. — l)s11 s20 
— 3w(?i + l)sB1 s10 + 6nslx s\0 + 6ns20 s1Qs01 — 6s01 s?( 
10 | 
nJaa — 
n 
(n-l){n-2){n-3) 
'i . n 2(n + l) 2(n + l) (n-1) 
n 
n 
n 
2(71 — 1) 8 « - 2i- 2 
- fin + — SX1 Sxo ^io H~ — soa ^io H sao %i 
71 
8 
72. 
2 
n 
2 
71' 
--24)*oij 
(11.109) 
282 
APPROXIMATIONS TO\ SAMPLING DISTRIBUTIONS 
In generalisation of the mean value functions of the k'a we may write, for example, 
S(kM fcu) = A*Q J 
E&toku. h02) = J l \ 
with corresponding k's. The latter may be expressed in terms of the cumulants of the 
bivariate distribution as in the univariate ; and the coefficients will now depend on partitions 
of bipartite numbers. Our rules still apply (and in particular the pattern functions 
appropriate to particular arrays are the same) ; but the numerical' coefficients associated 
with completed arrays are modified, for we now have to consider the number of ways of 
allocating two different sorts of individuals in a two-way partition of a bipartite number. 
An example will make the modification clear. 
(2 2 1\ 
j. The total degree is 
10 and, the orders of the produot being 6, 2, 2, we have to consider arrays of type 
6 
2 
2 
4 4 2 
10 
i.e. those we discussed above. The pattern functions are those we have already found. 
For the numerical coefficients we have to regard the column totals as consisting of the two 
types of object in number (2, 2), (2, 2), and (1, 1) and the row totals (3 ,3), (1, 1) and 1, 1). 
For instance, the array 
2 
1 
1 
4 
2 
1 
1 
4 
2 
a 
• 
2 
6 
2 
2 
10 
might be written either as 
(1,1) 
(0,1) 
(1,0) 
(1,1) (1,1) 
(1,0) . 
(0, 1) . 
(2,2) (2,2) 
or as 
(2,0) 
(0,1) 
(0,1) 
(0,2) 
(1,0) 
0,0) 
(1,1) 
(1,1) 
(3,3) 
(1,1) 
(1,1) 
(11.110) 
(5,5) 
(3,3) 
(1,1) 
(1,1) 
(11.111) 
(5,5) 
(2,2) (2,2) (1,1) 
each of which will make a contribution to the numerical coefficient. It will be found that 
no other arrays are possible except those obtained by permuting the first two columns. 
The numerical coefficient in (11.110) and the permuted array together is 
i^mmw^- 
THE MULTIVARIATE CASE 
That in (11.111) and the permuted array is 
The total contribution is thus 20. 'The pattern function is 
283 
(n - 1)(» - 2j* 
In the same way it will be found that for the partitions 
2 
1 
1 
4 
3 
1 
• 
4 
1 
a 
1 
2 
6 
2 
2 
10 
3 
1 
• 
4 
3 
, 
1 
4 
. 
1 
1 
2 
6 
2 
2 
10 
the coefficients are 48 and 8. Thus the desired coefficient of k^k^ is 
20 48 8 4(19w - 33) 
+ 
+ 
(n - 1) {n - 2) ^ (» - l)2 ^ (w.- I)2 (w - l)2(n - 2)* 
Example 11.6 
To find an exact expression for the covariance of the estimates of variance of two 
correlated variables, i.e. 
*(o 2) 
This will clearly consist of three terms, in /cafl, k02 k20 and k\v For the first we have 
the partition 
(2,0) (0,2) (2,2) 
(2,0) (0,2) 
with pattern function - and numerical coefficient unity. For the second no contribution 
exists, the only arrangement being 
(2, 0) . (2, 0) 
(0, 2) (0, 2) 
(2,2) 
(2,0) (0,2) 
(2,2) 
which has a vanishing pattern function. For the third term we have 
(1,0) (0,1) 
(1,0) (0,1) 
the pattern function for which is 
(2,0) (0,2) 
1 
(1. 1) 
(1.1) 
(n - 1) 
/2 0\ 1 
(2, 2) 
and numerical coefficient 2. Hence 
2 „ 
n - 1 
Lu- 
11.26. In conclusion it may be noted' that the method of expectations may be used 
to derive sampling moments of the distribution of samples from a finite population. The 
algebra becomes much more complex because the sample values are no longer independent 
1 
284 APPROXIMATIONS TO SAMPLING DISTRIBUTIONS 
and we cannot, for example, write E{2xfxh*) = prp'a. Tschuprow (1923) andls&erlis (1931) 
have investigated the subject systematically, the latter giving formulae for the first four 
moments of the mean and the first lour of the second moment. Some of these had been 
obtained earKer by Tschuprow himself,■ Neyman (1925) and Church (1926). We quote the 
following formulae, in which N represents the number in the population :— 
Moments of the Mean : 
E{m\) = fi[ (11.112) 
*K ~ *>■ " iJ^T/1' (1U13) 
Elm' - i^» - {N ~ n){N ~ 2n)n (11 1X4> 
*<* ~ & " nW - i'-2){N - »){(i^2 ~ 6Nn + N + *^ ' 
+ 3N{N - n - \){n - 1 )/«§} . (11.115) 
Moments of the Sample Variance: 
^(«.) = {n~ 1]f, g. ....:., .(11.110) 
n[JS — 1) 
J (n-l)N V N(N-n)[n-l) f/,T AT ,,... 1% 
- {N*n - 2N2 + GN - 3n - 3)/*f } . (11.117) 
Church also gives the third and fourth moments of the sample variance, the formulae taking 
several pages to write down. His version of the fourth moment contains errors which 
were corrected by Isserlis. The statistical usefulness of these results seems to be somowhat 
limited, but it would be interesting to inquire how far the combinatorial method appropriate 
to A-statistics can be extended to the tease of the finite population. 
NOTES AND REFERENCES 
For earlier work on the expectations of moments see Tschuprow (1919) and Church 
(1925). Thiele (1903) seems to have been the first to appreciate the possibilities of using 
other symmetric functions, but owing to the fact that he defined his sample " semi- 
invariants " to be the^ same function of the observations as the parent seminvariantB 
(our cumulants) are of the parent values, his formulae remained complicated. Later 
investigations on similar lines were carried out by Craig (1929) and St. Georgosou (1932). 
A-statistics were introduced by Fisher in 1928 and subsequently applied to several 
problems by himself (1930) and Wishart (1929a). The theory of the sampling of the statistics 
has been developed by Fisher and Wishart (1930) and applications to the normal 
distribution given by Wishart (19296 and 1930), E. S. Pearson (1930), and Hsu and Lawley (1939). 
The general theory of the A-statistics and other seminvariant statistics has been discussed 
by Kendall (1940a, 19406, 1940c, 1942) and Dressel (1940). Dressel has called attention 
to the relationship between seminvariant statistics and the seminvariants of the theory 
of binary quantics. The reader who refers to Fisher's basic paper of 1928 should boware 
of misprints. Methods of deriving bivariate formulae from univariate formulae by symbolic 
processes are given by Kendall (1940c). 
\ 
NOTES AND REFERENCES 285 
Reference is made in the text to the work by Tschuprow (1923) andlsserlis (1931) on 
finite populations. The latter's method appears to be the simplest known at the present 
time. 
Church, A. E. R. (1925), " On the moments of the distributions of squared standard 
deviations for samples of n drawn from an indefinitely large population," Biometrika, 
17, 79. 
(1926), " On the means and squared standard deviations of small samples from any 
v population," Biometrika, 18, 321. 
Craig, C. C. (1929), " An application of Thiele's seminvariants to the sampling problem," 
Metron, 7, 3. . 
Dressel, P. L. (1940), " Statistical seminvariants and their estimates with particular emphasis 
on their relation to algebraic invariants," Ann. Math. Statist., 11, 33. - 
Fisher, R. A. (1928), "Moments and product-moments of sampling distributions," Proc. 
Lond. Math. Soc, 30, 199-238. 
.(1930), " The moments of the distribution for normal samples of measures of departure 
from normality," Proc. Roy. Soc. A, 130, 16. 
and J. Wishart (1930), " The derivation of the pattern formulae of two-way partitions' 
from those of simpler patterns," Proc. Lond. Math. Soc, 32, 195-208. 
Hsu, C. T., and Lawley, D. N. (1939), " The derivation of the fifth and sixth moments of 
6a in samples from a normal population," Biometrika, 31, 238. 
Isserlis, L. (1931), " On the moment distributions of moments in the case of samples drawn 
from a limited universe," Proc. Roy. Soc, 132 A, 586. 
Kendall, M. G. (1940a), "Some properties of /c-statistics," Ann. Eugen. Lond., 10, 106. 
(19406), " Proof of Fisher's rules for ascertaining the sampling semi-invariants of 
^-statistics," Ann. Eugen. Lond., 10, 215. 
(1940c), " The derivation of multivariate sampling formulae from univariate formulae 
by symbolic operation," Ann. Eug&h. Lond., 10, 392. 
(1942), "On seminvariant statistics," Ann. Eugen. Lond., 11, 300. 
Neyman, J. (1925), "Contributions to the theory of small samples drawn from a finite 
population," Biometrika, 17, 472. 
Pearson, E. S. (1930), " A further development of tests for normality," Biometrika, 22, 239. 
St. Georgescu, N. (1932), "Further contributions to the sampling problem," Biometrika, 
24, 65. 
Thiele, T. N. (1903), Theory of Observations, London, C. and E. Layton (reprinted in Ann. 
Math. Statist., 2, 165). 
Tschuprow, A. A. (1919), " Oh the mathematical expectation of the moments of frequency 
distributions," Biomekika, 12, 140 and 185, and (1921) 13, 283. 
(1923), " On the mathematical expectation of moments of frequency distributions," 
Metron, 2, 461 and 646. 
Wishart, J. (1929a), " A problem in combinatorial analysis giving the distribution of 
certain moment statistics," Proc Lond. Math. Soc, 29, 309. 
(19296), " The correlation between product moments of any order in samples from a 
normal population," Proc Roy. Soc Edin., 49, 78. 
(1930), " The derivation of certain high-order sampling product-moments from a normal 
population," Biometrika, 22, 224. 
(1933), " A comparison of the semi-invariants of the distributions of moment and 
semi-invariant estimates in samples from an infinite population," Biometrika, 
25, 52. 
286 APPROXIMATIONS TO SAMPLING DISTRIBUTIONS 
EXERCISES 
11.1. Show that the pattern functions of the following patterns:— 
x x x' x x x x 
XX. XX.. 
1,1 
are 
(Fisher, 1928.) 
-r- and —: respectively. 
[n - l)2 n{n - l)2 * J 
11.2. Show that the pattern function of the pattern 
XXX... 
?\ /\ sS • • • 
1 r (_ 1)P-1 1 
with p-columns-is ——,■{ 1 — 7^ ,.„ , V /Tn. , lrk00 v 
^ nP_1\ (w — 1)P-1J (Fisher, 1928.) 
11.3. Verify the fonnulBe of equations (11.33) and (11.39). 
11.4. Show that the generating function of the moments of the ^-statistics, 
l Mil - 7la\j 
is given by 
[exp {^#1 H- *t£i + *A + . . .} exp L& + lji» + . . A"| 
where Kr is the same function of the operators x- as &r is of the observations a; and sr = ^(af.) 
Deduce that 
KM - *I 
■■ ^j.(«pA,. • • •) = 0 
where (£>ipa . . .) is any partition of p. (Fisher, 1928.) 
Note that if M(z) is the moment-generating function of z, that of 
.«='<*> *"■be KD'L. "d that of r by [{/©M..o 
Hence that the generating function of the moments of £ may be written ■ 
' 11.5. Show that 
E{k {x1 + h, xi} . . . arn)} = /<„ + - hP 
n 
and hence that 
Spkp = p\ 
Sqkp = 0 q^p 
where flj, is the same function of the operators =- as sp is of the observations x. 
(Kendall, 1940a.) . 
EXERCISES 287 
11.6. Show that the generating function of the moments of the ^-statistics is given by 
[expj^p + ^p + . . .JexpflfeA + k2t2 + ...}] 
and hence derive the result of-the previous exercise. 
(Kendall, 1941.)' 
11.7. Use Exercise 11.5 to show that, in the expression of kp in terms of the 
symmetric sums a, the sum of the coefficients is -. * 
n 
Show similarly that if 
Sp = Apkp + -4p_i,i kp_1k1 + . . . 4- APiPt • • • pm Srp, • • • "x, + • • • 
then 
4 4 A 
1 = _? -1- P-1' 1 _|_ _|_ Pi ' • • Pm _i_ 
n n% ' nm 
(Kendall, 1940a.) 
11.8. Referring to the result of 11.22, show that for a normal parent population the 
effect of adding a new part 2 to k{cli* . . . a*') is to give pattern functions -. — times 
\n ~~ 1) 
those of the original. Show also that the effect on the numerical coefficient in an array 
is, to multiply by twice the number of rows in the array. Deduce that the effect of adding 
a new part 2 is equivalent to operating by 
2k* d 
n — 1 dic2' 
.(Fisher and Wishart, 1930.) 
11.9. Use the previous exercise to establish equations (11.G7) and (11.68). 
11.10. In generalisation of Exercise 11.8 show that for a multivariate normal parent 
the effect of adding a covariance kpQ (p, q referring to the pth. and gth variates) is equivalent 
to operating by 
ra \ / rs 
where kpq is the covariance of the variates p and q. v 
("Fisher and Wishart, 1930.) 
11.11. If a pattern contains a column with three entries ; if the patterns obtained 
by suppressing this column and (1) amalgamating the three rows, (2) amalgamating the 
pairs of three rows, and (3) leaving the rows unamalgamated are A, B1} B2, J53 and G 
respectively, show that the pattern function for the original pattern is 
n A- -r„ i4r—™ (*i + B* + *») + ~, ^ «S>- 
(n - l){n - 2) (n - l)(n - 2)x J/ ' n(n - l)(n - 2) 
288 APPROXIMATIONS TO SAMPLING DISTRIBUTIONS 
Deduce that the function of the pattern 
XXX 
XXX 
XXX 
X X 
w3 _ 8na + 17W 4- 2 
IS 
{% - l)2(w - 2)\n - 3) 
(Pisher and Wishart, 1930.) 
11.12. Show that 
/l 1\ _ 1 1 1 
*\l l) ~nKzi + 5TnKu + ^Z7iKioK°l 
and k^0 2j =-k2B + -—iku 
and hence that if p = *" and r = ^V-:. 
then to order w-1 for normal samples 
var r =-(1 — p2)2. 
11.13. Show that for a bivariate normal population k^ and k^ have zero co variance 
unless t -\- u = v -\- w. 
(Wishart, 1930.) 
11.14. Show that for a bivariate normal population 
dF = l- x exp/- l(£ - 3£*» + ^1 
2Jtc71c72(l - p*)* V\ \a\ fflc72 + criyj 
w--G g-^^^pw* 
where /4*0fc igthe jth difference of the A:th power of zero and F refers to the hypergeometrio 
function. 
(Wishart, 19296.) 
11.15. Use the methods of this chapter to verify that to order w-1 
var (m4) = -(pa — pi + ^/4fii — &fisfi3). 
TV 
11.16. Referring to Exercise 11.4, show that the moment-generating function 
k k k 
s, t3, . . .) of the statistics jfca> 7-51 ts • ■ • t^ • ■ ■ 
/Cj> /to «t»2T 
is given symbolically by 
M'(tZl t3 . . .) = 
) = expJT.Br, -f ra-g\ + . . .!#(*„ *, . . .), 
EXERCISES 289 
where M(t2, t3 . . .) is the moment-generating function of the ^-statistics and Kr is the same 
3 
function'of ■=■ as kr is of xJ 
at 
Noting that in normal samples the distribution of h 8 is independent of that of the other 
1 M:) show that 
Jf'(Tlf r3 . . .) = exp^r3^. + . . \(i _ J^hX"*-"M(U, h . . .). 
Hence, if the number r refers to kr, that 
A*(. . . W) ~/i(... 5<4»3«2-')|^l - j—^j 
where 2j = a + & + c + • • • 
and hence that «(. . . 5C463°) = «(... 6e4*3a2-')(^:--)^^)- ' ' -' {n +-^-?-W. 
n , [n — \)1 
Deduce that 
Gn(n — 1) 
1F W) (n - S 
^W-J (n - 2)= 
2)(n + l)(n + 3) 
108rc2(rc - l)2(rc,2 + 27rc - 70) 
I3(w 4- 1)(m, + 3)(» + 5)(» + 7)(rc + 9) 
Hence verify the formulae of equations (11.104) and (11.105). (This remarkable result 
is due to Fisher (1930). The independence of &a and the other statistics may be^een by 
considering the n-fold sample space, &a appearing as the square of a length and the others 
as angles (of. Geary (1933), Biometrika, 25, 184).) 
11.17. Defining y by the relation 
= /(n^ l)(n - 2)(n - 3) k, 
y \] 24?i(w + 1) le* 
show that the moments of the distribution of y in samples from a normal population are 
/*i = 0 
12 , 88 532 
/h = 1 +—_... 
n n2 n3 
*T\£0-£ 
4811 _ 130,005 
~8rc» ~16»»'~ 
. . 408 32,196 , 1,118,388 
n ?i- n3 
($. S. Pearson, 1930, by the method of 11.23, before the exact results of the previous 
exercise had been given. He fitted, a Pearson Type IV to the distribution by using these 
moments, and tabulated the 1 per cent, and 5 per cent, significance points.) 
A.S. 
CHAPTER 12 
THE x2-DISTRIBUTION 
12.1. Among the sampling distributions of current statistical theory the normal 
distribution is perhaps of widest application in virtue of the tendency of many statistics 
to normality in large samples, irrespective of the nature of the parent population. There 
is one other distribution, closely related to the normal, which has a somewhat similar general 
applicability and we give an- aocount of it in this chapter. 
Suppose we have a number of compartments or cells determined by specified ranges 
of a variate-value or by some qualitative character, such as the intervals of a univariate 
1 frequency-distribution, the cells of a bivariate distribution, or a simple classification of 
individuals into two classes, A and not-A. Suppose these cells are filled by random 
sampling from a parent population and that in the parent the proportion of members in 
the jth cell is n^ In a sample of n there will occur proportions of, say, p} in the jth cell, 
the observed numbers being accordingly np^ If the sampling were such as to give an 
exact representation of the parent these numbers would be mij. Our fundamental problem 
is to determine how far the npj can, to any acceptable degree of probability, diverge from 
,the nn} by random sampling fluctuations. We shall then be able to test the accuracy 
of the hypothesis on which the n's were determined. 
A few examples from material occurring in earlier chapters will illustrate the problem. 
In Table 6.1 on page 117 were given the actual occurrences of throws of dice, n being 
26,306, the cells being eleven in number according to the number of " successes," and a 
third column showing the theoretical frequencies based on the hypothesis that the 
sampling obeyed a binomial law. The observed frequencies are our np's and the theoretical 
frequencies our wjt's. The question is, are the differences between the two such as can 
have arisen by sampling fluctuations alone ? If not, then we must reject our hypothesis 
as to the generation of these dice-throws according to the binomial law. 
Again, in the table of Example 8.6 were shown some results of inoculation against 
cholera. The question which interests us here is whether inoculation does in fact restriofc 
or prevent attack. If it does not, we expect to find the same proportion of attacked in the 
inoculated class as in the not-inoculated class, e.g. the proportion of attacked in the former 
would be -gfe x 279 = 23-5 approximately as against an observed 3. The former number 
is an nn and the latter an np. Once again we should examine the differences, and if they 
were large enough to be inexplicable on the basis of sampling alone, should reject the 
hypothesis of independence of inoculation and attack, concluding that inoculation was to some 
extent preventive.- 
12.2. Consider then samples of n with a division of the possible classification into 
p cells; and suppose the members distributed simply at random in these cells. 
Then the probability of there being Z2 members in the first cell, Za in the seoond, and 
so forth, is the term in n^1 n^ ... in the multinomial form 
(ji! + ot, + . . . np)nt 
that is to say, is 
290 
THE ^-DISTRIBUTION 291 
If the Z's are not small we have, in virtue of Stirling's approximation to tne suctorial, 
p l^l+ie 1i\/(2ti) . . . lplp+ie lpy (2jt) 
and since n = 2JI this becomes 
^r-sr • • • • (i2-3) 
NOW put Xj = UTlj 
and £ - l* ~ n7l1 - l* ~ ** ■ 
Then from (12.3) we have 
log T - log (constant) = zUl, + J) log ij 
= 27ft + 1) log —A 
= - S{X + i + £v*) log (l + ~r\ » 
If A is large, £ will be small compared with X and to first order we have, expanding the 
•logarithm, 
log T - log (constant) = -T(A + \ + £\W~ - |~) 
»>- 
= - 27{K" + £ V* + 0(A-»)} ..... . (12.4) 
Now 
r(,V^) ■= ^(?; - ^) = w- - n = 0 
and hence, to order X-i, 
log T — log constant = — £2£a. 
T cc exp(- Ji7|*) (12.5) 
Hence the frequency T varies as that of the sum of p normal variates of unit variance which 
are subject to the constraint Z(£-\/X) = 0 but otherwise independent. 
Now put 
Z* =££* = £$-^Ml (12.6) 
Then the frequency of %2 is that of the sum of squares of (p — 1) independent normal variates 
of unit variance. Its distribution is then, from Example 10.4, given by 
292 THE ^-DISTRIBUTION 
Furthermore, if there are certain constraints on the- cell frequencias expressible by 
k linear equations among them, the distribution remains of the same form but is now 
dF = 1—rv e-**y-i dX (12.9) 
«^->rg) 
where v, known as the number of degrees of freedom, is p — k, that is the number of cells 
whose frequencies can be assigned without restriction. (Cf. Example 10.4.) 
12.3. The distribution (12.9) is usually known as the ^-distribution, though it is 
actually that of x- However, #2and not x is *ne quantity which always occurs in practical 
calculations and most tables of the distribution function have x2 as the argument. (12.9) 
is only an approximation, and relies on the fact that for large X the StirKng approximation 
to the factorial will hold and that deviations from theoretical X'a are negligible to order n~*. 
In point of fact, the approximation is very good and the ^-distribution may confidently 
be applied when the theoretical cell frequencies are, say, not less than 20. 
Before dealing with the applications of the above results we will consider in more 
detail the properties of the distribution. 
Properties of the x%~Distribution 
12A. Writing x2 = £we have for the distribution 
dF = , * e-KC*-1,^, 0<£<oo . . .(12.10) 
2±".T 
» 
<f>(t) = — '- i (12.11) 
a. Pearson Type III distribution. The characteristic function is 
1_ ._ 
(1 - 2it)l 
whence, for the cumulants, we have 
Kr=v 2'-1(r - 1)! (12.12) 
and for moments about the mean 
[i2 = 2v "" \ 
fj,a = 8v 
^ = 48v + 12va V (12.13) 
fi, = 32v (5v + 12) 
pt = 40v(3v2 + 52j/ + 96)J 
Since Kr is linear in v, /ur, which can contain only - powers of fjt%, must be of degree 
Hr r — 1 
in v, i.e. - if r is even and —■?— if r is odd. 
As v tends to infinity the y,2-distribution tends to normality, for in standard measure 
we have 
vit t 2it \— * 
««=r"rv(S))' 
PROPERTIES OF THE ^-DISTRIBUTION 
log «£(*) -+-^ -1[- 2tt _ 1/ 2i* V 1 
V(2v) 2\V(2V) 2\V(2v)/ " ' J 
The tendency is, however, rather slow, and there are better approximations as we shall see 
in a moment. 
*• 
12.5. The frequency curves given by (12.9) extend from zero to infinity. In the 
case v = 1 the curve is merely the positive half of the normal curve. In other cases it is 
zero at the origin, rises to a mode and then falls off again to infinity. The maximum ordinate 
of the ^-distribution (not the ^-distribution) is given by 
£(a-hY-i, = 0, 
namely, by 
X2=v-L (12.14)' 
and that of the ^-distribution or C-distribution by 
|(e-K <*-i j = o, 
namely, by 
Z=X2=V_2. . . . . . (12.15) 
The skewness of the ^-distribution in the form (mean— mode)/(standard deviation) is then 
v — (v — 2) 
V v' 
y/2v 
12.6. The distribution function of (12.10)-is an incomplete ZVfunction. We have 
-'£. 
or, in the notation of Pearson's tables, 
= '(v(F+T,'*») (12-,6) 
Homo special tables have, however, been constructed. 
(a) Eldorton's table (Tables for Statisticians and Biometricians, Part I) gives the 
values of P = f dF = 1 - F(C) = 1 - F(X2) for values of v = 2 (1) 29 and ^ = 1 (1) 30 ; 
30 (10) 70. In this table, which is to six places, our v is denoted by ri — 1. 
(h) A table by Yule, reproduced at the end of this volume, supplements Elderton's 
by giving P for v = 1, %* = 0 (0-01)1 ; 1 (0-1) 10. 
(c) Kelley (1938) gives a four-place table of P for -^ from 0 (0-1) 4-1 and v— 1 (1) 10 ; 
12, 15, 19, 30. 
(d) Fisher and Yates (1938) give tables in an inverted form, showing the values of 
294 THE ^-DISTRIBUTION 
X2 for certain values of P and v, namely P = 0-99, 0-98, 0-95, 0-90 (0-10)'0-10, 0-05, 0-02, 
0-01 and 0-001 ; .and v = 1 (1) 30. 
(e) Thompson (Biometrika, 32 (1941), 187) gives tables in the inverted form for 
P = 0-996, 0-990, 0-976, 0-960, 0-750, 0-500, 0-250, 0-100, 0-060, 0-026, 0-010, 0-006 and 
v = 1 (1) 30 (10) 100. 
For generalise the incomplete P-function tables are probably the best, as interpolation 
in Elderton's table does not give very great accuracy. The significance points tabled by 
Fisher and Yates ate, however, sufficient for many practical applications of the 
^distribution in carrying out statistical tests. We reproduce at the end of the volume a diagram 
which will serve for such purposes. It shows, for co-ordinates v and %*, the curves 
P = constant, so that for given v and %% it is easy to determine whether P falls between 
any of the values for which the curves are drawn. 
12.7. Except for Thompson's table, the tables do not cover the region, for which 
v > 30, and for such values an approximation to the normal distribution may be 
employed. There are two such id common use:— 
(a) (Fisher) that V(2%2) is normally distributed about mean -\/(2v — 1) with unit 
variance ; 
/y"\* 2 
(b) (Wilson and Hilferty (1931)) that ( — J is normally distributed about mean 1 — — 
with variance /— . The second is more aceurate, but involves more arithmetical- work 
V 9v 
in applications. 
The relative speed of approach to normality of %% and V(2%2) may be compared as 
follows:— , 
For x2 we} have, from (12.13), 
,o 8v /8 
^ = V/?1 " (*JJ - Jv ' " ' ' ■ • <15U7) 
12 
y2 =/?a - 3 =— . ., . . . (12.18) 
For the moments of % we have 
2M--2)jT 
1 f°° -V 
2M--2)jr/l) Jo 
r/v + r\ 
= 21 I 2 J 
P 
Pi 
i) 
Using the expansion 
log r(x + 1) - | log (2n) + (x + i) log x - x + * 
12a; 
(12.19) 
Thus fi\ = ^2 
- PROPERTIES OF THE ^-DISTRIBUTION 295 
an extended form of Stirling's .formula, we find after substitution and reduction 
ft - V*(i - £ + J5-. + fggi + - • •} 
whence Ml = "(l - % + g^, + j^i + • ■ •) 
Also fa =v 
fa = (v + I) fa 
fa = (v + 2)v 
whence we find for moments about the mean 
-1 1 . 
^ " 2 " 8^ + ' ' * 
1 , 
^ ~ 4^ ' * ' 
Hence for the constants of V(2%2) we nave 
yi = —7=- + • • • (12.20) 
V2i; 
y2 =00'-a) (12.21) 
A comparison with (12.17) and (12.18) shows that V(2#2) tends to normality with 
considerably greater rapidity than #2. Moreover, the expression for /x[ of % is equal to -\/(v — £) 
to order v~% and henoe V(2#2) is distributed about mean ^(2v — 1) to that order, with 
variance which is unity to order v_1. 
12.8. For the Wilson-Hilferty approximation, consider the distribution of %2 about 
its mean value v. Let us find the distribution of f — J = y, say, h as yet being 
undetermined. Write f = x2'— v. Then 
=(l+9" 
r 2! va 
Talcing mean values and using the results of (12.13), we find, after some reduction, 
_ , h{h - 1) uJy2) , , 
fa(y) = 1 + 2, - ^~ + etc. 
^ 1 A(^ - x) * Mh ~ 1)(/t ~ 2)(3^ ~ *) 
1 ' v 6v2 
+ **** ~ '^-g^^ + 0»->). . . .(12.22) 
y 
296 THE ^-DISTRIBUTION 
If in this we put rh =hwe find the mean value of yr and thus 
yr(y, = i +r^rh~1\l + etc (12.23) 
We now choose h so that the third term in (12.22) vanishes, i.e. we take h = £> We 
then find 
Pity) 
2 80 Ay ,x 
9r 3V 
2 ." 4 
56 
pM = 1 
M) 
or, transferring to the mean, 
^ 9i> 3V 37r8 v ' 
. . 2 104 . n/ _4X 
9r 3V 
32 
^) = 3V + °^~4) 
A. 1 fK 
My) = gv - 3v + °(v~4) 
(12.24) 
We now find 
yi = 
21 
38 
5,'a 
y2 = - 
9v 
(12.25) 
(12.20) 
2\i 
Comparison with (12.17), (12.18), (12.20) and (12.21) shows that (%-\ tends to symmetry, 
as measured by y1} and mesokurtosis more rapidly than either %* or V(%X2)- To order 
2 2 
v-2 the variance is, from (12.24), equal to — and the mean 1 — —. The result may also 
vv vv 
be expressed by saying that 
'Shi-m 
. (12.27) 
is distributed normally about zero mean with unit' variance. 
The following table, quoted from Garwood (1936), shows some comparisons of the two 
approximations with the actual values. In each case, there have been worked out the values 
are 
/•CO 
of x* corresponding to P ■■= \ dF of 0-01, 0-05, 0-95 and 0-99. The exact values 
denoted by mT, those given by Fisher's approximation mF and those by Wilson and Hilferty's 
approximation by mw. 
\ 
EXAMPLES OF THE USE OF THE % ^DISTRIBUTION • 
TABLE 12.1 ' * 
>" 
Comparison of Approximations to the xz Integral. 
(From Garwood, Biometriha, 28, 437.) 
297 
0-99 
0-95 
P = 0-01 
005 
V 
40 
60 
80 
100 
40 
60 
80 
100 
42 
62 
82 
102 
42 
62 
82 
102 
mT 
11-082 
18-742 
26-770 
35032 
13-255 
21-594 
30-196 
38-965 
33-103 
45-401 
57-347 
69-067 
29-062 
40091 
52-009 
63-287 
mF 
10-764 
18-414 
26-436 
34-694 
13-116 
21-455 
30-056 
38-825 
32-700 
45-003 
56-953 
68-676 
28-919 
40-54S 
51-920 
03-144 
mT — mP 
0-318 
0-328 
0-334 
0-338 
0-139 
0-139 
0140 
0140 
0-403 
0-398 
0-394 
0-391 
0-143 
0143 
0143 
0-143 
mw 
11-070 
18-732 
26-761 
35-025 
13-254 
21-594 
30-196 
38-965 
33-113 
45-409 
57-355 
69*074 
\ 
29060 
40-689 
52-068 
63-286 
mT — mw 
0-012 
0-010 
0-009 ■ 
0-007 
0-001 
0000 
0-000 
0-000 
- 0010 
- 0-008 
- 0-008 
- 0007 
0-002 
0-002 
0001 
0001 
The m.w approximation is evidently very good and the mF approximation is fair. 
12.9. A third method of approximation may be obtained by the method of 
6.32 and 6.33, and in faot our Example 6,5 was virtually based on the %a-distribu- 
/2 12 48V2 
tion. From equation (6.75) with lt = l% = 0, l3 = i<a = /-, l4 = Kt = —, l6 = ——, 
^ • \j v v v* 
L = —-, we find from (6.73) or (6.75) a normal deviate whose distribution function is 
that of x2> 
Examples of the Use of the x*-DiMribution 
12.10. We now proceed to consider some examples of the uso of the ^-distribution 
in comparing observation and hypothesis. 
Example 12.1 
In Table 12.2 we repeat some of the material of Table 5.4, giving the distribution of 
first hands at whist according to the number of trumps held. The observed frequencies 
are our npi = lj. The theoretical figures a^ are based on the hypothesis that the distribution 
n 2)2 
follows the hypergeometric series. The last oolumn shows the contributions -—:— to 
A 
the total x2' To avoid small frequencies we have grouped together those frequencies 
of 7 or over. 
298 THE ^-DISTRIBUTION 
4 
TABLE 12'.2 
Distribution of First Hands at Whist according to Number of Cards held of a given Suit. 
Hands Dealt but not Played. 
Number of -Cards. 
o ...... 
1 . 
2 
3 
4 
6 
6 
m 
» 
B 
B 
■ • 
• 
"7 and over 
Total .... 
Observed Frequency. 
1 
36 
290 
696 
937 
861 
444 
115 
, 32 
3400 
Theoretioal Frequency. 
A 
43-6 
272-2 
700-0 
973-6 
811-3 
■ 424-0 
1413 
34-2 
3400-0 
(I - A)8 
A 
1-661 
1-164 
0023 
1-369 
1-943 
0-943 
4-896 
0-142 
12140 
The total %2 is seen to be 12-140. The number of degrees of freedom v is one fewer 
than the number of cells (excluding the total), namely 7. From the diagram in the appendix 
it is seen that the probability of getting a value as great as this or greater on random 
sampling (=* I dF = P J lies between 0-1 and 0-05, very olose to the former. The odds 
are therefore about 9 to 1 against getting the observed frequencies or frequencies which 
diverge to a greater extent from theoretical frequencies. This is hardly great enough for 
us to be able to say that an improbable event has ooourred, and therefore we need not 
reject the hypothesis that the observed frequencies did in fact arise according to the . 
hypergeometric law and that the discrepancies are merely sampling effects. 
The matter stands dhTerently with the distribution of Table 12.3 showing the distribution 
of 25,000 deals of whist classified in the same way as that of Table 12.2. Here %2 = 174130 
TABLE m 
Distribution of First Hands at Whist according to Number of Cards held of a-given Suit. 
Hands actually Played. 
Number of Cards. 
0 
1 
2 
3 
4 
6 
6 
7 
8 and over 
, 
Total .... 
Observed Frequency. 
1 
216 
1,724 
6,262 
7,440 
6,371 
2,960 
862 
166 
20 
26,000 
Theoretical Frequency. 
A 
320 
2,002 
6,147 
7,168 
5,966 
3,117 
1,039 
220 
&1 
24,999 
(I - A)a 
A 
34453 
38-603 
2-569 
11-110 
27-634 
8-947 
33-656 
13-255 
3-903 
174130 
EXAMPLES OF THE USE OF THE ^-DISTRIBUTION ' 299 
and v = 8 (one more than in the previous example because we have grouped only those 
frequencies of 8 and over). From th6 diagram it is clear that the chance of getting such 
a value or a greater one is exceedingly small, certainly'less than 1 in 10,000. This very 
rare event leads us to reject the hypothesis that the hypergeometric distribution is operating.. 
The explanation is probably that these deals were taken from aotual play, whereas those 
of the previous table' were obtained without aotually playing the hands. It is evident 
that in card play certain kinds of oard (e.g. those of the same suit) tend to collect together 
and the shuffling is apt to be somewhat perfunctory. Thus the condition of realisation of 
the hypergeometric distribution, that the selection is random, was probably violated. 
Example 12.2 
In some classical experiments on pea-breeding Mendel obtained the following frequencies 
for different lands, of seeds in crosses from plants with round yellow seeds and wrinkled 
green seeds :— 
Observed Theoretical 
Round and yellow 315 • 312-75 
Wrinkled and yellow 101 104-25 
Round and green . . . . - 108 104-25 
Wrinkled and green : . . . 32 34-75 
Total . . 556 55600 , 
On the Mendelian theory of inheritance the frequencies should be in proportion 9, 3, 3, 1 
and the theoretical frequencies are shown in the last column. > 
We find 
y2 = i±*W + <i?5)! + (^ (*z?>! = 0.4700 ■ 
Z 312-75 ^ 104-25 104-25 ^ 34-75 
The number of degrees of freedom v = 3. The probability of obtaining the value of x2 
or greater is seen to be between 0-9 and 0-95. There is thus nothing in the value of %z to 
lead us to reject the Mendelian hypothesis. 
12.11. Consider now a table of the type of Table 12.4, which shows the frequencies 
of a number of men according to eye colour and hair colour. If, on some hypothesis as 
to the relationship between eye and hair colour we determine theoretical frequencies in 
tlio body of the table, leaving the row and border columns unchanged, then there are 
a number of linear constraints on these frequencies. 
In fact, if in such a table there are r rows and s columns it will be found that only 
(r _ 1)(.9 — 1) cells can be filled up arbitrarily. There are ra cells altogether ; but the fact 
that the rows and columns must add to assigned totals imposes r + s constraints. These, 
howover, are not independent, for the sum of the- border column frequencies is equal to 
that of the border row frequencies and thus there are only r -j- s — 1 independent linear 
constraints. Hence rs - (r + s - 1) = (r - l)(s - 1) cells are independent and this is v, 
the number of degrees of freedom associated with such a table. 
Example 12.3 
In Table 12.4 suppose that eye and hair colour are independent. Then the expected 
frequency in any cell with a row total x and a column total y will be —, where n is the 
300 
-THE ^-DISTRIBUTION 
TABLE 12.4 
Distribution of 6800 Males according to Colour of Eye and Hair. 
(Ammon, Zur Anthropologie der Badener.) 
. Hair colour 
JO 
*o 
o 
Blue .... 
Grey or Green 
Brown .... 
Totals 
Fair. 
1768 
046 
115 
2829 • 
Brown. 
807 
1387 
438 
2632 
■ Black. 
189 
746 - 
288 
1223 
Red. 
47 
53 
16 
116 
Totals. 
2811 
3132 
857 
6800 
total frequency. For instance, the expected number of men with fair hair and blue eyes is 
2811 v 2829 
= 1169. The theoretical frequencies obtained in this way are— 
6800 
Blue . 
Grey or Green 
Brown 
Fair 
1169 
1303 
357 
Brown 
1088 
1212 
332 
Black 
606 
563 
154 
Red 
480 
53-4 
146 
Henco 
X 
2 = r(1768-1169)a 
1169 
= 1075-2. 
v = (4 -'l)(3 - 1) = 6. 
..The value of %2 is very improbable, P being less than 0-000,001. We accordingly reject 
the hypothesis of independence and conclude that hair oolour and eye colour are associated. 
12.12. It is useful to note that xz niay be put into a form which is sometimes more 
convenient for calculation. We have 
a, x 
= Z- - 211 + LI 
I2 
= E- n. 
I 
. (12.28) 
When the X's are not integers it is easier to work with this formula, the squaring of,the 
larger numbers I involving less arithmetic than the squaring of the smaller but non-integral 
numbers Z — A. 
12.13. In the foregoing examples the theoretical frequencies A were calculated without 
reference to the experimental data other than totals which merely subjected them to linear 
EXAMPLES OF THE USE OF THE ^-DISTRIBUTION 
301 
constraints and hence preserved the Type III, distribution of %%. There also.arises the 
much more difficult case in which certain parameters necessary for the determination of 
the theoretical frequencies have to be determined from the data themselves. Suppose, 
for example, we attempt to represent a frequency-distribution by a normal curve. We 
have then to decide on the mean and variance of this curve, and they can, as a rule, only 
be estimated from the data themselves. The question then arises, what happens to the 
^-distribution if, instead of the unknown parameters leading to the theoretical frequencies A, 
we use estimates leading to the estimated theoretical frequencies, say, A'? That is to say, 
how does the distribution of the statistic 
«■ = ^-^ 
compare with that of 
y2 _ yd ~ *T 
(12.29) 
This problem has not yet been completely solved. The nearest approach to a solution 
has been reached by R. A. Fisher (1924), who showed that xl is distributed in the Type IH 
form, provided that 
(a) the sample is large and that each cell-frequency is large ; 
(6) that the number of degrees of freedom is reduced by unity for every parameter 
estimated; 
(c) that the principle of estimation involved is such as to minimise %%. This is 
equivalent, for large samples, to taking a maximum likelihood estimate. 
Departing from our usual practice, we shall have at this stage to state this result 
without proof. It cannot be adequately discussed until we have dealt with the principles 
of estimation in the second volume. 
Example 12.4 s 
The following table shows the distribution of 12 dice thrown 26,306 times, a 5 or 6 being 
reckoned a success. We have encountered these data before in Table 5.1. 
TABLE 12.5 
Distribution of 12 Dice thrown 26,306 times, a 5 m 6 being reckoned a Success. 
Number of 
Succossos. 
0 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 and over .... 
Totals 
Observed 
Frequency. 
185 
1,149 
3,205 
5,475 
6,114 
5,194 
3,067 
1,331 
403 
105 
18 
26,306 
Frequoncy of , 
26,300(£ + J)". 
203 
1,217 
3,345 
5,576 
0,273 
5,018 
2,927 
1,254 
392 
87 
14 
26,306 
Frequoncy of 
20.300(0-6023 + 0-3377)ia. 
187 
1,146 
3,215 
5,405 
0,269 
5,115 
3,043 
1,330 
424 
96 
10 
26,306 
302 THE ^-DISTRIBUTION 
* 
The third column shows the frequencies if the dice were perfect, that is the frequencies 
of the binomial law 26,306 (f + i)22. We find, in the usual way, 
2 (203 - 185)2 , , 0KQ/n 
y2 = ^ i—l etc. = 35-941. 
x 185 
v = 10. 
P is very small, less than 0-000,1 and we conclude that the hypothesis is to be' rejected, 
i.e. that the dioe were not perfect or that something was wrong with the sampling.' In 
this particular case great care was in fact taken in rolling the dice and the balance lies in 
favour of rejecting the hypothesis that they, were entirely unbiassed. 
Let us then reconsider the data. If the dice are biassed, what is the true probability 
of getting a 5 or 6 ? This we must estimate from the data and it has already been seen 
that a maximum likelihood estimate is the mean number of suocesses in the sample itself. 
This is found to be 0-3377 and the last column in Table 12.5 shows the frequencies 
26,306(0-6623 + 0-3377)12. The agreement with observation is evidently closer and we 
find now #a = 8-201. v is now 9, for we have estimated one parameter. P is now about 
0-5, so that the 'observed frequencies are in quite good accord with theory. 
12.14. Since %z is the sum of a certain number of normal variates each with zero 
mean and unit variance, a number of different values of x2 mav b® added together and 
will be distributed in the Type III form with a number of degrees of freedom equal to the 
sum of the individual numbers. This result enables us to combine the results of a set of 
experiments so as to determine the probability of the whole set taken together. For 
example, Table 12.6 shows the data for inoculation against, cholera on a certain tea estate. 
TABLE 12.6 
Inoculation against Cholera on a certain Tea Estate. 
Not inoculated t 
Totals 
Attacked. 
431 
291 
'722 
Not Attacked. 
5 
9 
14 
Totals. 
436 
300 
736 
We find x°' = 3-27, v = 1 and P, from Appendix Table 7, about 0-071. This is 
small, but not small enough to reject the hypothesis, particularly when we note that 
the theoretical frequencies in the not-attacked column are far from large. 
The results for six such estates were :— 
X2 P v 
3-27 0071 1 
934 0-0022 1 
608 ' 0014 1 
251 • 011 ' 1 
561 0-018 1 
1-59 0-21 1 
23-40 6_ 
Here only one value of P is less than 0-01, and we might be inclined to doubt the reality 
THE 2x2 BIVARIATE TABLE 
303 
of association between inoculation and immunity. The sum of %2 is, however, 28-40, and 
v = 6, for which values-we find P < 0-000,1 ; so that together the results are significant. 
The 2 x 2 Bivariate Table 
12.15. We now return to a point which has been mentioned incidentally. If the 
theoretical frequencies in cells are small, the Type III distribution will hold only as an 
approximation depending on how closely the binomial distributions in individual cells are 
adequately represented by normal distributions. For some problems we can overcome the 
difficulty by grouping small frequencies, as has been done above in Example 12.1. But 
such a process sacrifices information and cannot always be carried out, e.g. in a 2 x 2 
bivariate table. * 
Consider in the first place the symmetrical binomial (| + i)10- The second column 
in the following table shows the probability P that the number of successes-in the" first 
column will be at least attained (for the smaller of the pair) or attained or exceeded (for the 
larger of the pair). 
Successes P P1 Pa 
0, 10 00010 00008 ' 00022 
• 1, 9 00108 00057 00134 
2, 8 00547 " 00290 00569 
3i 7 01719 01030 01714 
4, 6 0-3770 0-2635 0-3759 
If we regard this frequency as that of a single cell (v = 1) we should, for the 
corresponding ^-distribution, have the positive half of the normal curve. The values 
corresponding to x2 = !(52, 42, 32, 22, l3) are shown in the third column as Px. They may be obtained 
from Appendix Tables 6 and 7, e.g. for the last term we have %2 = ■} = 0-4, P = 0-52709, 
Px = I of this = 0-2635. 
The correspondence between P and Px is evidently not very good. We can, however, 
improve it considerably by a correction due to Yates (1934). The distribution of %* is 
continuous, whereas that of the binomial is not. To bring the two into comparability 
we really should consider the binomial frequency at the value r as spread over the range 
r — I to r + ^. For example, for a deviation 3 corresponding to 8 successes we should 
take a deviation 2-5, giving %2 = 
2(2-5)! 
= 2-5 instead of 3. The values given by the 
corrected %2 are shown as Pfl above. The agreement between Pa and P is evidently a great 
improvement on that between Pt and P. 
When the theoretical proportion in a cell is not £, the binomial distribution is skew, 
and there do not appear to be any simple corrections to compensate for this effect. The 
continuity correction will, however, result in an improvement if the theoretical frequency 
is near | and is probably best made in all circumstances. 
12.16. The 2x2 table may also be dealt with by exact methods. Consider in fact 
the table 
a 
c 
a + c 
b 
d 
b +d 
a +b 
c + d 
n 
i 
m 
304 
THE ^-DISTRIBUTION 
If the two variates are independent, the number of ways in which a table with such 
marginal totals can be constructed from the n sample members is 
/ n \f n \ n\ n\ 
\a + c, 
z)\a + b) {a +c)\ (6 + d)\ (a + 6)! (c + d)\' 
The number of ways in which the body of the array can be completed is 
n\ 
a\b\c\d\ 
Consequently the probability of the distribution of the table is 
(a + c)! (6 + d)\ (a + 6)! (c + d)\ 
(12.30) 
n\ q\ 61 c! d\ 
Thus the successive probabilities for d == 0, 1, 2, . . . are the successive terms in the 
hypergeometric series 
F{-(c-\-d), - (6 +d), a -d + ], 1}. . . .(12.31) 
Example 12.5 (from F. Yates, .1934, quoting data by M. Hellman). 
The following table shows the number of children classified according to the nature 
of the teeth and type of feeding. 
Breast fed 
Bottle fed 
Totats 
Normal Teeth. 
4 
1 
5 
Malocoluded Teeth. 
16 
21 
37 
Totals. 
20 
22 
42 
From (12.30) the probability of obtaining no normal breast-fed children if the attributes 
are independent is (a = 0) 
51 37' 20' 221 
^lV^J^J^^ - 0-03096. 
421 201 01 5! 17! 
The probabilities of obtaining 1,2, . . . children are obtained by multiplying successively 
, 5X20 4X19 3X18 
1 X 18' 2 X 19' 3 X 20' ~ 
Number of Normal 
Breast-fed Children. 
0 
1 
2 
3 
4 
6 
Probabili 
00310 
01720 
0-3440 
0-3096 
01253 
00182 
1-0001 
Thus the probability of getting four or more normal breast-fed children is 0-1435, and 
NOTES AND REFERENCES 305 
we conclude that there is nothing to reject the hypothesis that breast feeding exerts no 
effect on the condition of the teeth. Had we used the x2 test in the ordinary way we 
should have found P = 0-0612, less than half the true value. The continuity correction 
makes a great improvement, giving P = 0-1427. 
NOTES AND REFERENCES 
The ^-distribution, though known to Helmert in 1876, was rediscovered and applied 
to statistical problems by Karl Pearson in 1900. In 1922 Yule and Fisher gave 
respectively experimental and theoretical evidence for what is now accepted as the correct method 
of detennining the number of degrees of freedom in a bivariate table ; but Pearson himself 
seems never to have acknowledged the soundness of this method, and some papers written 
between 1920 and 1930 on this subject are controversial and therefore not to be accepted 
uncritically. 
For the use of the distribution in testing hypotheses when parent parameters have 
to be estimated, see Fisher (1924). For the exact test in a 2 x 2 table and the continuity 
correction, see Yates (1934). 
More recently, Cochran (1936) and Haldane (1937a, 19376, 1939a, 19396) have 
discussed the distribution of x2 in bivariate tables when some hypothetical frequencies are 
small. 
Cochran, W. G. (1936), " The ^-distribution for the binomial and Poisson series, with 
small expectations," Ann. Eugen., Lond., 7, 207. 
(1938). " Note on J. B. S. Haldane's paper [1937a below]," Biometrika, 29, 407. 
Fisher, R. A. (1922), " On the interpretation of x2 from contingency tables and the 
calculation of P, " Jour. Boy. Statist. Soc, 85, 87. 
(1924), " The conditions under which x2 measures the discrepancy between observation 
and hypothesis," Jour. Boy. Statist. Soc, 87, 442. 
Garwood, F. (1936), " Fiducial limits for the Poisson distribution," Biometrika, 28, 437. 
Haldane, J. B. S. (1937a), " The exact value of the moments of the distribution of x2 used 
as a test of goodness of fit when expectations are small," Biometrika, 29, 133. 
(19376), " The first six moments of x2 f°? an w-fold table with n degrees of freedom 
when some expectations are small," Biometrika, 29, 389. 
(1937c), " The approximate normalisation of a class of frequency distributions," 
Biometrika, 29, 392. 
(1939a), " Corrections to formulae in papers on the moments of x2" Biometrika, 31, 
220. 
(19396), " The mean and variance of x2 when used as a test of homogeneity when 
samples are small," Biometrika, 31, 346. 
(1939c), " The cumulants and moments of the binomial distribution and the cumulants 
of x2 for an w x 2 fold table," Biometrika, 31, 392. 
Wilson, E. B„ and Hilferty, H. M. (1931), " The distribution of chi-square," Nat. Atad. 
Sci., 17, 694. 
Yates, F. (1934), " Contingency tables involving small numbers and the x2 test," Supp. 
Jour. Boy. Statist. Soc, 1, 217. 
Yule, G. U. (1922), " An appHcation of the x2 method to association and contingency tables 
with experimental illustrations," Jour. Boy. Statist. Soc, 85, 95. 
A.S. X 
306 THE x ^DISTRIBUTION 
■> EXERCISES 
12.1. By the method of 12.8 show that 
is approximately normally distributed with unit variance about zero mean. 
(Haldane, 1939.) 
12.2. Use the ^-distribution to show that the distribution of digits from telephone 
directories (Table 1.4) could not in all probability have arisen by random sampling from 
a population in which each of the ten digits occurred with the same frequency. 
12.3. Show that'for a" 2 x n bivariate table v = n — 1 and 
nxni{ 
1 
where ay, a%i are the frequencies in the jth column and nu na are the border sums of the 
two rows. 
12.4. Show that if v is even 
and hence that the values of P for given %* can be derived from tables of the Poisson 
exponential limit. 
12.5. Show that in a 2 x 2 table whose frequencies are 
a 
r 
d 
(a +b + c + d)(ad - be)2 
(a + b)(c + d)(b + d)(a + c) 
the theoretical frequencies being those obtained on the hypothesis that the two variates 
are independent. 
12.6. An experiment gives on hypothesis H %2 = 9, v = 8. When repeated it gives 
the same result. Show that the two taken together do not give the same confidence in 
E as either taken separately. 
EXERCISES 
307 
12.7. (Data from Report on the SpaMinger Experiments in Northern Ireland, 1931- 
1934, H.M. Stationery Office, 1935.) In experiments on the immunisation of cattle from 
tuberculosis the following results were secured :— 
\ 
Inoculated with vaccine . 
Not inoculated or inoculated with 
Totals 
Died of Tuberculosis 
or very seriously 
affected. 
G 
8 • 
14 
Unaffected or only 
slightly affected. 
13 
i 
3 
16 
Totals, 
19 
11 
30 
Show that for this table, on the hypothesis that inoculation and susceptibility to 
tuberoulosis are independent, %2 = 4-75, P = 0-029 ; with a correction for continuity 
the corresponding,probability is 0-072 ; and that by the exact method of 12.15 P — 0-070. 
12.8. (Data from Yule, Jour. Anthrop. InsL, 1906, 36, 325.) 
Sixteen pieces of photographic paper were printed down to different depths of colour 
from nearly white to a very deep blackish-brown. Small scraps were cut from- each sheet 
and pastel on cards, two scraps on each card one above the other, combining scraps from 
the several sheets in all possible ways, so that there were 256 cards in the pack. Twenty 
observers then went through the pack independently, each one naming each tint either 
" light," " medium " or " dark." 
The following table shows the name assigned to each of the two pieces of paper:— 
Namo assigned to 
Lower Tint. 
Light 
Medium 
Dark 
Totals 
Name assigned to.Upj 
Light. 
850 
618 
540 
2008 
Modium. 
571 
593 
450 
1620 
>er Tint. 
Dark. 
580 
455 
457 
1402 
Totals. 
2001 
KI15G 
1453 
5120 
Show that there is a significant association between the name assigned to one piece 
and the name assigned to the other. 
CHTAPTER 13 
ASSOCIATION AND CONTINGENCY 
13.1. This and the next three chapters deal with the relationship between two or 
more variables. We shall consider populations, the members of which each bear one of 
each of several different sets of qualities or one value of each of several different variables, 
and shall discuss the measurement of the relationship among the qualities or variables 
in the populations. The corresponding questions of sampling will also fall for consideration. 
We, may denote this branch of the theory by the general name of Theory of Dependence. 
Association 
13.2. Consider in the first instance a population classified according to whether 
each member bears or- does not bear an attribute A. The presence of the attribute wo 
may denote by A and the absence by'a. We shall' assume that each member must either 
be an A or an a, so that a = not-A and A = not-oc. 
Suppose that each member of the population is classified according to two attributes 
A and B. Each may then be one of four kinds, AB, Aft, aJB and ce|8. For example, if the 
attributes are the possession of blue eyes (^4) and the possession of male sex (B), we dial I 
have the four possible classes AB = blue-eyed males, Afi = blue-eyed females, u.B = not- 
blue-eyed males, a/3 = not-blue-eyed females. Denoting the number in any clafw by the 
letters appropriate to that class in ordinary round brackets, we may'then specify the 
population in the tabular form :— 
A'b 
not-^'s 
TOTAL9 
ZJ's not-B's 
1 
{AB) 
(olB) 
(B) 
(48) 
(«i») 
(0) 
Totals. 
(-4) 
(«) 
N 
or, more simply, by 
a 
c 
a + c 
b 
d 
b +d 
a + b 
c + d 
N 
where a = (AB), etc. Here N is the total number in the population. 
308 
ASSOCIATION 
309 
13.3. If there is no relationship between the attribute A and the attribute B, that 
is to say if the possession of A is irrelevant to the possession of B, then there must be 
the same proportion of A'a among the B's as among the /?'s. We thus define two attributes 
to be independent if 
{B) (0) (ld"d) 
b a -f b 
(13.4) 
or 
a 
a + c b +d N 
It follows at once that each of the following is true :— 
c d c + d 
(13.5) 
. (13.6) 
»** 
a + b c+d ' N (13"7) 
These are derivable by simple algebra from (13.4) and, in words, will be found to express 
the same fundamental fact that the proportion of members bearing an attribute X is the 
same among the 37's as among the not-Y's. It also follows that 
a 
a 
+ c 
a 
+ b 
b 
b + d 
c 
c+d 
d 
N 
a + c 
N 
_b +d 
• _ [a + b)(a + c) 
a ^ 
(13.8) 
with three similar equations in b, c and d. 
If now in any given table 
(o + 6)(a + c) 
a> -N ~ -" 
or, in the alternative notation, 
(A)(B) 
(AB)> 
N 
we shall say that A and B are positively associated. Per contra, if 
(AB)< 
y 
(13.9) 
(13.10) 
(13.11) 
they arc said to be negatively associated or disassociated. 
Example 13.1 
Association between inoculation against cholera and exemption from attack. 
(Greenwood and Yule (1915), Proc. Roy. Soc. Medicine, 8, 113.) The following table shows 818 
Inoculated .... 
Not-inoculated . 
Totals 
Not Attnckrul. 
27G 
473 
749 
Attacked. 
3 
66 
60 
Totals. 
279 
530 
818 
310 ASSOCIATION AND CONTINGENCY 
cases classified according to inoculation against cholera (attribute A) and freedom from 
attack (attribute B). 
If the attributes were independent the frequenoy in the inoculated-not-attacked class 
279 X 749 
would be ^r-r— = 255. The observed frequency is greater than this and hence in-' 
olo 
oculation is positively associated with exemption from attack. 
13.4. The reader will recognise in this example a type of 2 x 2 table which was 
disoussed in connection with the %2-distribution. In fact, if the data are considered as a 
sample there arises at once the, question how far the positive association, which certainly 
exists in the sample, is indicative of real association in the parent population. The %*- 
distribution, as shown in the previous chapter, provides an objective method of forming 
a judgment on this matter. %2 itself, however, does not provide an adequate measure of 
the intensity of the association. Altogether apart from sampling questions, we sometimes 
wish to compare the strength of associations in different populations or between different 
attributes, and some coefficients proposed for the purpose will now be considered. 
13.5. The more obvious desiderata in a coefficient-measuring association are (a) 
that it shall vanish when the attributes are independent; (6) that it shall be -f- 1 when 
there is complete positive association and — 1 when there is complete negative association ; 
(c) that it should increase as the frequencies proceed from dissociation to association. As 
to this latter point, consider the difference between, observed and " independence" 
frequencies in the cell corresponding to (AB), viz. :— 
6 = (AB) - !^5- (13.12) 
Since the border frequencies are constant it is evident that the difference in any cell between 
observed and " independence " frequencies is ± S and thus d determines uniquely the 
departure from independence. We may interpret condition (c) as meaning that our 
coefficient should increase with 6. It may be noted that 
d = a Aa + b)(a + c) 
a + b + c -f- d 
ad — be 
(13.13) 
N 
Following Yule (1900, 1912) we define the coefficient of association Q by the equation 
A _ ad — be _ Nd 
ad + be ad + be' ' ' ' ' 
It is zero if the attributes are independent, for then 5=0. It can equal + 1 only if be = 0, 
in which case there is complete association (either no a's are i?'s or no A's are /?'s), and — 1 
only if ad '= 0, in which case there is complete disassociation. Furthermore, Q increases 
be 
with 6, for if e = —^ 
ad 
then Q 1 ~ S 
1 +e 
and -^ is negative, as is — ,• so that --f is positive. 
ae "de do 
ASSOCIATION 
311 
A somewhat similar coefficient, also due to Yule, is the coefficient of colligation 
Y = ■ jr~ . ..... (13.15) 
J. 
This also satisfies our-conditions, as the reader may verify for himself. 
" ' ad 
13.6. Yet a third coefficient, which will be shown below to be related to #2, is 
Nd 
+ y/{A)(a){B){fi) 
(ad — be) 
. (13.16) 
+ {(a + b)(a .+ c)(b + d)(c + d)}i 
This is evidently zero when <5 = 0 and increases with 6. If V = 1 we have 
(a + b)(a + c)(b + d)(c + d) = (ad - bc)\ ■ 
giving 
4abcd + a2(bc + bd -\- cd) + b2(ac + ad + cd) 
+ c2(ab + ad + 6d) + d2(ac + at + 6r.) = 0. 
Since no frequency can be negative this oan only vanish if at least two of a, 6, c, d are zero. 
If the frequencies in the same row and column vanish the case is purely nugatory. We 
have then only to consider a = 0,d = 0or& = 0,c = 0. In the first case V = 1, in the 
second V =■ — 1. It cannot lie outside these limits. 
13.7. It will be observed that whereas V is unity only if two*frequencies in the 2x2 
table vanish, Q and Y are unity if only one frequency vanishes. This raises a point in 
connection with the definition of complete association. We shall say that association is 
complete if all A's are B's, notwithstanding that all B's are not A'sl If all dumb men 
are deaf there is complete association between dumbness and deafness, however many 
deaf men there are who are not dumb. The coefficient V is unity only if all ^4's 
are B's and all B's are A's, a condition which we could, if so desired, describe as absolute 
association. 
It is necessary to point out in this connection that statistical association is different 
from association in the colloquial sense. In current speech we say that A and B are 
associated if they occur together fairly often ; but in statistics they are associated only 
if A occurs relatively more or less frequently among the B's than among the not-5's. If 
90 per cent, of smokers have poor digestions we cannot say that smoking and poor digestion 
are associated until it is shown that less than 90 per cent, of non-smokers have poor 
digestions. 
Example 13.2 
Consider again the data of Example 13.1. For the various coefficients we have 
Q = (276 X 66) - (3 X 473) _ 
v (276 x 66) + (3 x «3) 
312 ASSOCIATION AND CONTINGENCY 
3 x 473 
1 
-I 
„ 276 x 66 
/3 X 473 
■276 x 66 
v = (276 X 66) - (3 X 473) = Qmms 
V(279 X 539 X 749 X 69) - 
These values are, as might be expected, different, although they all refer to the same intensity 
of association. Comparisons, however, naturally fall to be made between values of 
coefficients of the same type, and the fact that different types give different values does not 
affect their usefulness or the comparability of members of any one type. 
13.8. The methods of Chapter 9 may be used to give the standard errors of the three 
coefficients based on material obtained by random sampling. 
We recall that for any of the four frequencies a, b, c, d we have results such as 
vara = ——-—- .... (lo.lv) 
N 
cov (a, b) = — —. . . . . . (13.18) 
The first is merely another way of writing the' expression for the variance of a binomial. 
The second follows from 
0 = var (a + b) = var a + var 6 + 2 cov (a, b). 
be 
Then for e = —3 we have, writing A for the differential to avoid confusion with tho 
frequency d, 
m As _ Ab Ac Aa Ad 
e b c a d ' 
whence ' I?£i = z ^ + 2z!± ™J?'-bA 
ea a2 \ ab y 
Substitution from (13.17) and (13.1S) gives 
1.1.1.1 
™e=e\a+b+c+d (13-19) 
Now 
and hence 
giving 
In a similar way we have 
Q 
Q 
var Q 
var Y 
1 - 
- e 
1 +e 
= — 
(i 
__(i 
_(i 
2^le 
1 - £a; 
2 vare 
-e2)2 
- Q2)2/ 
10 
,0 
e 
i+l+i + a)- • • • (13-20> 
S+i+l + a) • ■ *<1321> 
PARTIAL ASSOCIATION 313 
The sampling variance of 7 may be found similarly but involves rather more lengthy 
algebra. Yule (1912) gives the result 
^i +(K+^ ){(a + 6)(a+c)(6+^)(c+i)}i 
_ 3 ((q + 6 - e - j)i _ (a + c-6-<Z)2|-| j 
T I (a + 6)(c + d) (a + c)(6 + d) J J * * ( } 
In applying these formulae it is, as usual in large-sample theory, assumed that the 
observed frequencies may be used instead of theoretical frequencies in the sampling 
variances. 
Example 13.3 
Reverting to Example 13.2, we have for the standard error of Q 
l -Q% 
\j\a b c dj 
= i^JO-8666)" //_1_ 1 ± J_> 
2 V \276 T3 473T 66, 
= 0-0798. 
The coefficient Q thus probably lies in the range 0-856 ± 0-239 in the population from 
which these data were derived, assuming of course that the sampling was random. 
Partial Association 
13.9. The coefficients described above measure the dependence of two attributes in 
the statistical sense, but in order to decide whether such dependence has any causal 
significance it is often necessary to consider associations in sub-populations. Suppose, for 
example, a positive association is noticed between inoculation and exemption from attack. 
It is natural to infer that the inoculation confers exemption, but this is not necessarily so. 
It might be that the people who are inoculated are drawn largely from the richer classes, 
who live in better hygienic conditions and are therefore better equipped to resist attack 
or less exposed to risk. In other words, the association of A and B might be due to the 
association of both with a third attribute G (wealth). 
Now it is clear that this explanation would not hold if the hygienic circumstances were 
constant in the population. If we then consider the association of A and B in the sub- 
populations (C) (well-to-do classes) and (y) (poorer classes) and find that it persists, the 
explanation is rejected. Furthermore, if the association in (y) was weaker than that 
in (C), there would be some indication that hygienic conditions are related to exemption 
from attack, though not constituting the only factor concerned. 
13.10. Associations in sub-populations are called partial associations. Analogously 
to (13.9), A and B are said to be positively associated in the population of C's if 
{AJBO»mm, (13.23) 
314 * ASSOCIATION AND CONTINGENCY 
where (ABO) represents the number of members bearing the attributes A, B and O ; and 
so on. We may also define coeffioients of partial association, colligation, etc., such as 
Q _ (ABC)WC) - (APC)(«BC) 
vab.c - {ABC){rfC) + (Apc)(*Bcy * ; • (6-4) 
which is derived from (13.14) by adding C to all the symbols representing the frequencies. 
Example 13.4 . 
Galton's " Natural Inheritance " gives some particulars, for 78 families containing not 
less than six brothers or sisters, of eye-colour in parent and child. Denoting a light-eyed 
child by A, a light-eyed parent by B and a light-eyed grandparent by C, we trace every 
possible line of descent and record whether a light-eyed child has light-eyed parent and 
grandparent, the number of such being denoted by (ABC) and so on. The symbol (Afiy), 
for example, denotes the number of light-eyed children whose parents and grandparents 
have not light eyes. The eight possible classes are 
(ABO) = 1928 (oJBG) = 303 
(ABy) = 596 (c/JBy) = 225 
(ApO) = 552 (a£C) = 395 
(Apy) = 508 (cxPy) = 501 
The first question we discuss is : does there exist any association between parent and 
offspring with regard to eye-colour ? We consider both, the grandparent-parent group 
(association of B'a and C's) and the parent-child group (association of A's and B's). We 
have, for the former : f 
Proportion of light-eyed among children of light-eyed parents, 
(BC) 2231 „AO 
\t^ = —— = 70-2 per cent. 
(G) 3178 * 
Proportion of light-eyed among children of not-light-eyed parents, 
(By) 821 
(y) ■ 1830 
and for the latter, analogously, 
= 44-9 per cent.; 
(AB) 2524 onf7 
V = 3052 = 82"7 POT Cent- 
(A0) 1060 K. 0 
V,T = ^ = 54-2 per cent. 
W) 1956 * 
Frequencies such as (A{i) are calculable direct from the eight classes given above, e.g. 
(Afi) = (AfiG) + (Afiy) = 552 ■+- 508 = 1060. 
Evidently there is some positive association between parent and offspring in regard to 
eye-colour. 
Consider now the relationship between eye-colours of grandparents and grandchildren. 
We have: 
Proportion of light-eyed among grandchildren of light-eyed grandparents 
(AG) 2480 „.. . 
- (5) 8178 ~ 78-° Per Cent" 
.PARTIAL ASSOCIATION 
315 
Proportion of light-eyed among grandchildren of not-light-eyed grandparents 
The association between grandparents and grandchildren is also positive. 
In tabular form the data are :— 
Grandparents. 
Parents. 
CO 
1 
B 
J 
Totals 
__________ 
G 
2231 
947 
3178 
Y 
821 
1009 
1830 
Totals. 
3052 
1966 
5008 
A 
a 
TOTALS 
B 
2524 
528 
3052 
P 
1060 
896 
1966 
Totals. 
3584 
1424 
5008 
Grandparents. 
s 
■a 
•s 
o 
A 
a 
Totals 
Q 
2480 
698 
3178 
y 
1104 
726 
1830 
Totals. 
3584 
1-121 
A008 
The coefficients of association Q and Y are 
Grandparents—parents 
Paronts—children 
Grandparonts—grandchildren 
Q 
0-4S7 
0-603 
0-401 
Y 
0260 
0-336 
0-209 
Now the question arises : is the resemblance between grandparent and grandchild due 
merely to that between grandparent and parent, parent and child ? To investigate this, 
we must consider the associations of grandparent and grandchild in the sub-populations 
" parents ligh't-eyed " and " parents not-light-eyed," that is, the associations of A and O 
in B and /?. We have :— 
Parents Light-eyed 
Proportion of light-eyed amongst grandchildren of light-eyed grandparents 
(ABC) 1928 
PC) 
2231 
= 86-4 per cent. 
316 ASSOCIATION AJSTD CONTINGENCY 
Proportion of light-eyed amongst grandchildren of not-light-eyed grandparents 
• ; _ (AB^ _ 596 = 
(By) 821 * 
* 
Parents not Light-eyed 
Proportion of light-eyed amongst the grandchildren of light-eyed grandparents 
- Wjl - 55? = 58-3 per cent. 
¥ (PC) 947 F 
Proportion of light-eyed amongst the grandchildren of not-light-eyed grandparents 
(|9y) 1009 * 
In both cases the partial association is well marked and positive. The association 
between grandparents and grandchildren cannot, then, be due wholly to the associations 
between grandparents and parents, parents and children. There is ancestral heredity, as 
it is called, as well as parental heredity. The relevant four-fold tables are :— 
a 
a 
-3 
3 
Parents light-eyed 
Grandparents. 
AB 
oLB 
Totals 
BC 
1928 
303 
2231 
By 
596 
225 
821 
Totals. 
2524 
528 
3052 
3 
c 
a 
Parents not-light-eyed. 
Grandparunts. 
Ap 
<l(3 
Totals 
PC 
552 
395 
947 
Py 
508 
501 
1009 
Totals. 
1000 
800 
1956 
The coefficients of association and colligation are :— 
Qac.b = 0-412 
Yac.b = 0-216 
Qac.p 
Yacp 
0-159 
0-080 
13.11. If there are p different attributes under consideration the number of partial 
associations can become very large, even for moderate p. For example, we can choose 
two in ( ^ 1 ways and consider their associations in all the possible sub-populations of the 
other (p — 2), which are seen to be 3P~2 in number. Thus there are f 7>)3P~2 associations. 
In practice, however, we need only consider a few of them. 
PARTIAL ASSOCIATION 
317 
One result in this connection is worth noticing. We have, generalising d of equation 
(13.13):— 
^ a + ^ - {(^0, - WW) + {(ABy) - Mmi} 
-^-iAP-m){iAG){B0)-^m 
(B)(0)(AC) (A)(B)(C)*\ 
N ^ N J 
-'--pfeH-TH^-T} 
= &AB ~ 
N 
(0)(y) 
&AO &BO. 
. (13.25) 
If, then, A and B are independent in both (C) and (y), dAB0 — dAB.y = 0 and 
&jb = jfifrfco *bo. (13.26) 
i.e. they are not independent in the population as a whole unless C is independent of A or B 
or both in that population. 
This peculiar result indicates that illusory associations may arise when two populations 
{G) and (y) are amalgamated, or that real associations may be obscured. If A and C, 
B and C, are associate 1 we have, from (13.25), 
N 
&AB = tQ\ty) ^AB ^*i0 ~^ ^AB0 ~^ ^^.By 
so that even if A and B are independent in (C) and (y) they will appear as associated in 
the two together. Again, if A and B are associated positively in (C) and negatively in (y), 
dAB may be zero; that is to say, they may appear as independent in the whole population. 
Example 13.5 
Consider the case in which a number of patients are treated for a disease and there 
is noted the number of recoveries. Denoting A by recovery, a by not-recovery, B by 
treatment, /? by not-treatment, suppose the frequencies are 
A 
a. 
Totals 
B 
100 
50 
150 
200 
100 
300 
Totals. 
300 
150 
450 
Here (AB) = 100 = -—-X—, so that the attributes are independent. So far as can be 
seen, treatment exerts no effect on recovery. » 
318 
ASSOCIATION AND CONTINGENCY 
Denoting male sex by C and female sex by y, suppose the frequencies among males- 
and females are 
Males 
AG 
*C 
Totals 
BC 
80 
40 
120 
pc 
100 
80 
.180 
Totals. 
,180 , 
120 
300 
Femaloa 
Ay 
ay 
TOTAL9 
By 
20 
10 
' 30 
Py 
100 
20 
120 
.Totals. 
120 
30 
160 
In the male, group we now have 
= (80 x 80) - (100 X 40) = 
Vab'° (80 X 80) + (100 X 40) 
and in the female group 
QAB.y = ~ 0-429. 
Thus among the males treatment is positively associated with recovery, and among the 
females negatively associated. The apparent independence in the two together is due to 
the cancelling of these associations in the sub-populations. 
Contingency 
13.12. We now turn to the more general case in which a population is divided into 
a number of categories Au A2 . . . Ap instead of simply dichotomised into two, A and 
not-A. If there is a second classification into BXi B2 . . . Bq the frequencies may be 
arranged in the form:— 
Bt 
B, 
• 
■ 
• 
B* 
Totals 
-• 
Ax 
{AM 
(^i*,)4. 
• • ■ 
(AlBq) 
• Ml) 
A, 
(AM 
(A2B2) 
■ ■ ■ 
(A2Bg) 
(A,) 
. . . 
. . . 
. . . 
. . . 
. . . 
■ ■ a 
\ 
AP 
(ApBj) 
(ApBJ 
• * ■ 
• ■ ■ 
• • • 
(ApBg) 
<A9) 
Totals. 
(Bi) 
(B2) 
m 
m 
a 
(Bv) 
N 
. (13.27) 
CONTINGENCY 319 
This is known as a contingency table. We> have already encountered an example in , 
Table 12.4. Ordinary bivariate frequency tables can, of course, be regarded as contingency 
tables, but there is a difference : in the bivariate table the order of rows and columns is 
determined by the variate-values, whereas in the contingency table the order of rows and 
columns is, in general, arbitrary. 
In (13.27) the frequency in the ith column and jth row is denoted by {A^). As 
in the case of the 2x2 table we write 
dq = {AtBt) - &MA (13.28) 
and define the attributes as independent if every d is zero. We have: 
= 2V--^-=0 . . . .' .' . (13.29) 
and, in conformity with the notation of (12.6) we have 
' 7V752 
*a - *mk <13-30> 
X2 is sometimes called the square contingency and -^ the mean square contingency. 
13.13. We have already seen in Chapter 12 how x2 mav De used t° test the 
hypothesis that the observed frequencies could have arisen by random sampling from a 
population in which the attributes were independent. We now proceed to consider the 
construction of measures of dependence. 
Evidently #2 = 0 if and only if each 3=0. Thus x2 vanishes if and only if the 
attributes are independent in the observed population. Furthermore, as x2 becomes 
larger the observed frequencies deviate more and more from the " independence " 
frequencies ; it thus provides some sort of measure of the strength of relationship in 
contingency tables. For example, in the 2x2 table we have 
V2 (equation (13.16)) = ^, . . „ .(13.31) 
which illustrates the relationship between V and x2- 
X2 itself, however,, does not constitute a very useful'coefficient since it may increase 
without limit. Following Karl Pearson we may put 
0 - Jwij* -....; (13.32) 
and call C Pearson's coefficient of contingency. Even this has its limitations. It vanishes, 
as it should, when there is complete independence ; but in general it cannot attain unity. 
Consider, for example, a t x t table in which the diagonals [A^B^ are of frequency a^ and 
320 
ASSOCIATION AND CONTINGENCY 
all other compartments are zero. Obviously no greater degree of dependence is possible. 
We then have 
n 2 
°u - *t - ]f 
%a = 
^--W 
OLi 
and 
. (13.33) 
If £ = 5, for example, the maximum value of C is 0.894. 
To remedy this effect Tschuprow proposed the coefficient 
r «2 ii 
T = 
* 1 (13.34) 
^NViv - i)te - i)j K 
This can attain unity when p = q; but it is still not dear how it behaves when p and 
q are unequal. 
Example 13.6 
TABLE 13.1 
Distribution of Schoolchildren according to Intelligence and Standard of Clothing. 
(From W. H. Gilby (1911), Biometrika, 8, 94.) 
Very well clad . 
Well clad ... . 
Poor but passable . 
Very badly clad 
Totals 
A and B 
33 
41 
39 
17 
130 
G 
48 
100 
58 
13 
219 
D 
113 
202 
70 
22 
407 
E 
209 
255 
61 
10 
535 
F 
194 
138 
33 
10 
375 
a 
39 
15 
4 
1 
59 
1 
Totals. 
03(5 
751 
205 
73 
1725 
The above table shows the distribution of 1725 schoolchildren who were classified 
(1) according to their standard of clothing and (2) according to their intelligence, the 
standards in the latter case being A = mentally deficient, B = slow and dull, C = dull, 
D = slow but intelligent, E = fairly intelligent, F = distinctly capable, G = very able. 
Eequired to discuss whether there is any association between standards of clothing and 
intelligence. 
We note in the first place that a table of this kind could, theoretically, be discussed 
CONTINGENCY 
321 
by considering all the possible 2x2 comparisons to be extracted from it; e.g. for the 
corners of the table we have 
• 
Very well clad, 
Very badly clad . 
Totals 
A and B 
33 
17 
50 
G 
39 
1 
40 
Totals. 
72 
18 
90 
Here, for example, 54 per cent, of the very well clad were very able, but only 5 per cent. 
of the very badly clad. However, what we really require is not a series of individual 
comparisons of this kind but a general comparison over the whole table, and it is for such 
purposes that the coefficient of contingency is designed. • 
We then proceed to work out the " independence " frequencies, e.g. that in the top 
left-hand corner of the table is —r—-— = 47-930. The contribution to %a from this 
(14*930)a 
compartment is then —.-,_ r^ =4-651. It will be found that the sum of the contributions 
47-930 
from the 20 compartments is 174-92. We then have 
Sj 1725 + 174-92 
indicating a considerable degree of association. For the Tschuprow coefficient we have 
. T- I "™ - 0-162. 
V 1725^15 
There is evidently some general relationship between the two attributes, though not 
a very strong one. The reader may verify for himself by using the %l test that the values 
of O and T are significant, i.e. could not have arisen by sampling from independent attributes. 
13.14. The sampling variance of the coefficient of contingency is difficult to arrive 
at in virtue of sheer algebraical complexity ; and it is not clear how far the use of a standard 
error is legitimate in this connection. Reference may be made for the formulae to K. Pearson 
(1915a) and Kondo (1929). For the even more complicated question of partial contingency 
see K. Pearson (19156). 
" 13.15. In concluding this chapter we point out that all the measures of association 
and contingency discussed therein in no way rely on the possibility of the measurement 
of attributes on a variate-scale, or even on the possibility of arranging them in order. 
Rearrangement of rows and columns in the two-way tables does not affect the values of 
the coefficients.* In the next chapter we shall consider the relationship between variates 
* Except that it may change the sign of a coefficient of association. This is equivalent to a slight 
change of standpoint in what is regarded as a positive association—for example, positive association 
between fair hair and blue eyes is equivalent to negative association between fair hair and not-blue eyes. 
A.S. Y 
322 ASSOCIATION AND CONTINGENCY 
and certain coefficients based on the assumption that the attribute classifications are 
made according to the divisions of a variate-scale. These coefficients (tetrachoric r, biserial 
r\, etc.) have been used as measures of association, but they are essentially different in 
character from those discussed in this chapter. The reader who refers to memoirs written 
on this subject between 1900 and 1920 will find it useful to remember this faot. 
NOTES AND REFERENCES 
The fundamental memoir on association of attributes is that of Yule (1900), who 
introduced the coefficient Q in it. In a later paper (1912) Yule reviewed the whole subject 
and proposed the coefficient denoted in this chapter by Y. This memoir contained some 
criticisms of Karl Pearson's coefficient now known as tetrachoric r (cf. Chapter 14) and 
evoked a reply from Pearson and Heron (1913) which is remarkable for having missed 
the point over more pages (173) than perhaps any other memoir in statistical history. 
Pearson's coefficient of contingency C was introduced in 1904. Corrections to this 
coefficient were subsequently proposed, being based on the notion of an underlying variate 
(K. Pearson, 1913). For references to the other coefficients proposed on this basis, see 
Chapter 14. 
Kondo, T. (1929), " On the standard error of the mean square contingency," Biometrika, 
21, 376. 
Pearson, K. (1904), "On the theory of contingency and its relation to association and 
normal correlation," Drapers' Company Research Memoirs, Biometric Series I, 
Dulau and Co., London. 
(1913), "On the measurement of the influence of broad.categories on correlation," 
Biometrika, 9,116. 
and Heron, D. (1913), " On theories of association," Biometrika, 9, 159. 
(1915a), " On the probable error of a coefficient of mean square contingency," 
Biometrika, 10, 590. 
(19156), " On the general theory of multiple contingency, with special reference to 
partial contingency," Biometrika, 11, 145! 
Yule, G. U. (1900), " On the association of attributes in statistics," Phil. Trans., A, 194, 257. 
(1912), " On the methods of measuring the association between two attributes," 
Jour. Boy. Statist. Soc; 75, 579. 
EXERCISES 
13.1. Show that the coefficient of association is greater in absolute value than the- 
coefficient of colligation, except when both are zero or unity. 
13.2. Show that for a contingency table with a constant number of rows and columns 
the Pearson coefficient of contingency C is equal to the Tschuprow coefficient T for two 
values of -^, one of which is zero ; that for ^ between these values C > T, and for ^f 
greater than the higher value T > C. 
i 
EXERCISES 
323 
13.3. The following table shows 68 lobelia plants classified according to whether they 
were oross- or self-fertilised and above or below average height. 
- 
Cross-fertilised 
Self-fertilised .... 
Totals 
i 
Abpve Average, j Below Average. 
i 
) 
17 
12 
20 
17 
22 
39 
Totals. 
34 
34 
68 
Show that Y = 0-150 and that this is not significant of association if these data are 
a random sample from lobelia plants generally. 
13.4. In the hair- and eye-colour of Table 12.4 show that C = 0-37 and T = 0-25. 
13.5. In a paper discussing whether laterality of hand is associated with laterality 
of eye (measured by astigmatism, acuity of vision, etc.) T. L. Woo^ obtained the following 
results (Biometrika, 20 a, pp. 79-148):— 
Ocular Laterality for Goneral Astigmatism. 
a | 
'I "3 
■« cd 
co 
3^ 
1 "" ' 
Loft-handed . 
Ambidextrous 
Right-handed. 
Totals 
" Loft-eyed." 
t 
34 
27 
67 
118 
■ Ambiocular. 
62 
28 
106 
195 
" Right-eyed." 
28 
20 
62 
100 
Totals. 
124 
76 
214 
413 
Show that laterality of eye is only slightly associated with laterality of hand, and that 
the association is not significant. 
CHAPTER 14 
PRODUCT-MOMENT CORRELATION 
f 
14.1. At the end of Chapter 1 there were given a few examples of bivariate frequency 
tables. We now proceed to consider such tables in greater detail and to discuss methods 
of measuring the dependence of the two variates represented in them. It is, of course, 
possible to treat the problem by the methods of the previous chapter and regard the tables 
as contingency tables ; but when data are classified according to a numerical variable more 
exact methods are available in an important class of cases. 
The types of bivariate distribution arising in practice are not so easy to classify as the 
univariate types. Table 1.15 on page 20, showing the distribution of beans according to 
length and breadth, and Table 1.25 on page 27 showing the number of cows according to 
age and milk yield, evidently correspond more or less to the unimodal univariate distribution, 
for not Only the border frequencies but the frequencies in individual rows and columns arc 
of the unimodal type. Biometric distributions are often of tliis character. On the other 
hand, Table 1.26 on page 28, showing discount rates and bank reserves, has the border 
column of the unimodal type and the border row of the J-shaped type. In Tables ] 4.1 to 
14.3 are given three more examples of the kind of material encountered in practice. 
Table 14.1 shows the distribution of a number of persons according to age and highest 
audible pitch ; Table 14.2 the distribution of registration districts according to proportion 
of male births and total number of births ; and Table 14.3 shows the distribution of sons 
according to stature of son and stature of father. 
324 
TABLE 14.1 
Distribution of 3379 Persons according to Age and Highest Audible Pitch 
(From Y. Koga and G. M. Morant, Biometrika, 15, 346.) 
The numbers in brackets are explained in Example 14.1, p. 331. 
Age, years. 
- 
a 
o 
o 
CD 
CO 
is per 
co .2 
"TV U 
nd vil 
CO 
a 
■5 
>ifcoh, 
lible r 
■I 
1 
Highc 
5- 
7- 
9- 
11- 
13- 
15- 
17- 
19- 
21- 
23- 
25- 
27- 
29- 
31- 
33- 
TOTALS 
5-5- 
— 
— 
— 
— 
— 
3 
(5) 
3 
(0) 
9 
(-5) 
— 
— 
— 
15 
8-5- 
— 
— 
— 
— 
(8) 
4 
(4) 
23 
(01 
39 
(-4) 
11-5- 
1 1 
(18) 
— 
1 
(12) 
2 
(9) 
n 
26 
(3) 
74 
(0) 
112 
(-3) 
10 ' 27 
(-8) j (-6) 
l 
i * 
,(-12) 
1 
(-16) 
i 
[ 
(-28) 
84 
1 5 
(-9) 
4 
(-12) 
(-IS) 
i 
(-21) 
262 
14-5- 
i 
1 
(12) 
— 
2 
(8) 
i 
17-5-1 
3 
(6) 
.— 
9 
(4) 
3 | 7 
(6) i (3) 
21 
(4) 
49 
(2) 
153 
(0) 
158 
(-2) 
40 
(2) 
121 
(1) 
201 
(0) 
303 
(-D 
37 . 50 
(-4) ( (-2) 
| 14 
(-6) 
3 
(-8) 
1 (-10) 
442 
: n 
(-3; 
6 
(-4) 
! 
1 
(-7) 
805 
20-5- 1 
(0) 
(0) 
3 
(0) 
2 
(0) 
32 
(0) 
105 
(0) 
178 
(0) 
164 
22 
, 3 
' (0) 
23-5- 
— 
1 
(-5) 
2 
(-4) 
1 
(-3) 
33 
(-2) 
72 
(-1) 
110 
(0) 
68 
(1) 
1 U 
\ (2) 
2 
(31 
1 , — 
(0) i 
(0) ' 
26-5- 
(-14) 
3 
(-12) 
— 
4 
(-8) 
4 
(-6) 
27 
(-4) 
50 
(-2) 
103 
(0) 
47 
(2) 
4 
(4) 
i 1 
i (6) 
29-5- ' 
1 
(-18) 
— 
2 
(-12) 
7 
(-9) 
18 
(-6) 
36 
(-3) 
47 
(0) 
24 
(3) 
2 
(6) 
— , 1 
i (12) 
! 
32-5- 
1 
(-24) 
— 
4 
(-16) 
5 
(-12; 
20 
(-8) 
28 
(-4) 
34 
(0) 
21 
(4) 
1 
(8) 
1 
(12. 
_ , _ | _ 1 _ ! _ 
! : : 
514 
• (7) 
301 
244 
- i 
i 
138 i 115 
1 
35-5- 
— 
1 
(-25) 
2 ■ 
(-20) 
10 
(-15) 
21 
(-10) 
21 
(-5) 
25 
(0) 
6 
(5) 
1 
(10) 
38-5- 
2 
(-36) 
— 
5 
(-24) 
8 
(-18) 
22 
(-12) 
21 
(-6) 
18 
(0) 
2 
(6) 
41-5- 
1 
(-4 2) 
— 
7 
(-28) 
4 
(-21) 
19 
(-14) 
IS 
(-7) 
13 
(0) 
3 
(7) 
I 
-I-!- 
1 
- 
87 
i 
1 
i 
1" 
65 
44-5- 
3 
(-48) 
2 
(-40) 
6 
(-32) 
6 
(-24) 
15 
(-16) 
13 
(-8) 
3 
(0) 
1 
(8) 
49 
47-5- 
1 
(-54) 
2 
(-45) 
12 
(-36) 
11 
(-27) 
16 
(-18) 
4 
(-9) 
3 
(0) 
— 
49 
50-5- 
3 
(-60) 
— 
14 
(-40) 
4 
(-30) 
6 
(-20, 
3 
(-10) 
1 
(0) 
— 
31 
53-5- 
3 
(-66) 
1 
(-66) 
5 
(-44) 
6 
(-33) 
5 
(-22) 
2 
(-11) 
— 
— 
22 
56-5- 
(-841 
5 
(-72) 
1 
(-60) 
11 
(-48) 
2 
(-36) 
3 
(-24) 
1 
(0) 
— 
24 
59-5- 
2 
(-78) 
— 
6 
(-52) 
4 
(-39) 
1 
(-26) 
1 
(0) 
— 
14 
62-5- 
4 
(-84) 
— 
8 
(-56) 
4 
(-421 
1 
(-28) 
— 
17 
65-5- 
3 
(-90) 
1 
(-75) 
3 
(-60) 
2 
(-45) 
1 
(-30) 
— 
. 
10 
68-5- 
2 
(-96) 
— 
2 
(-64) 
— 
— 
% 
— 
4 
71-5- 
1 
(-102) 
— 
— 
1 
(-51) 
— 
— 
2 
74-5- 
3 
(-108) 
— 
2 
(-72) 
— 
— 
— 
5 
77-5- 
— 
1 
(-95) 
1 
(-76) 
— 
— 
2 
TOTALS 
3 
45 
10 
104 
93 
310 
576 
1051 
957 
165 
41 
16 
2 
2 
4 
3379 
326 
PRODUCT-MOMENT CORRELATION 
TABLE 14.2 
Shomng the Number of Registration Districts in England and Wales exhibiting (I) a given 
Proportion of Male Births, (2) a given Total Number of Births during the Decade 1881-90. 
(The Data as to Total Births and Numbers of Male and Female Births from Decennial 
Supplement to Report of the Registrar-^Oeneral. Table from H. D. Vigor and G. U. Yule, 
Jour. Roy. Stat. Soc, 69, 1906.) 
•3 
1 
p 
B 
0- 4 
4- 8 
8- 12 
12- 18 
16- 20 
20- 24 
24- 28 
28- 32 
32- 36 
30- 40 
40- 44 
44- 48 
48- 62 
52- 60 
50- 00 
00- 04 
04- 08 
08- 72 
72- 70 
76- 80 
80- 84 
84- 88 
88- 92 
92- 96 
90-100 
100- 04 
104- 08 
148- 62 
Totals 
(1) Proportion of Male Births per 1O00 oC all Births. 
s 
i 
i 
o 
i 
— 
CO 
1 
1 
1 
CO 
— 
1 
2 
2 
1 
1 
1 
2 
1 
2 
2 
rH 
a 
00 
2 
2 
4 
3 
4 
2 
(1 
8 
5 
2 
15 
IA 
9 
7 
1 
1 
18 
o 
18 
20 
5 
1 
2 
40 
I 
19 
27 
16 
10 
5 
1 
3 
2 
3 
2 
2 
2 
3 
1 
98 
o 
o 
S 
12 
20 
17 
8 
9 
5 
5 
9 
3 
4 
3 
5 
0 
5 
126 
3 
10 
21 
42 
18 
12 
9 
4 
3 
2 
4 
2 
3 
1 
2 
1 
120 
lO 
lO 
14 
39 
16 
8 
4 
3 
1 
1 
1 
2 
1 
1 
88 
00 
IA 
12 
18 
7 
(1 
1 
1 
1 
1 
1H 
2 
IA 
9 
10 
4 
1 
1 
1 
20 
1 
=1 
4 
1 
1 
11 
2 
lO 
3 
1 
4 
5i 
IA 
1 
1 
rH 
CO. 
1A 
1 
1 
CO 
: 
CO 
2 
1 
3 
CO 
m 
— 
1A 
1 
1 
1 
1 
Totals. 
149 
204 
80 
48 
26 
16 
15 
12 
7 
(! 
8 
11 
11 
7 
3 
5 
3 
3 
4 
1 
2 
2 
2 
1 
1 
032 
14.2. In accordance with the definitions of the previous chapter we may say that 
two variables are independent in a bivariate table if the observed frequency in the jth 
row and ith eolumn (A{Bj), is equal to 
N 
In such a case, for any two rows j and k 
the frequencies will be proportional to (4<)(5y) and (Ai)(JSk) ; so that the distribution in 
any row is similar and similarly situated to that in any other row. It will, for example, 
have the same mean and variance. Similarly for the columns. 
The measures of dependence we shall consider are related to the extent to which row 
and column means and variances differ among themselves. If the variates are independent 
all row means and all column means are equal. The converse is not true in general, but 
becomes so in an important special case when the distribution is normal. 
REGRESSION , 327 
TABLE 14.3 
Distribution of 1078 Sons according to (1) Stature of Father and (2) Stature of Son: One 
or Two Sons only of each Father. 
(From Karl Pearson and Alice Lee, Biometrika, 2 (1903), 415.) 
Measurements in inohes. Note that where a height falls on the border-line of an interval, 
one-half of the individual is assigned to each contiguous interval. 
a 
o 
**1 
o 
ti 
a 
$ 
ci 
~ 
50'5-60-5 
00'6-01'5 
01'5-02 5 
02'5-03'S 
03-5-04-5 
04'5-66'5 
86-6-06'6 
86 '5-07-5 
07'5-08-5 
08'5-00-5 
00'5-70'5 
70'5-71-5 
71-5-72-5 
72-5-735 
73-5-74-5 
74-5-75-5 
75-5-70-5 
70'5-77'5 
77-5-78-5 
785-70-5 
Totals 
(1) Stature of Father. 
. 
m 
at 
5-5 
00 
m 
— 
— 
— 
1 
2 
— 
— 
— 
— 
— 
— 
— 
— 
— 
— 
— 
3 
in 
Q 
Z 
as 
IQ 
0 25 
0-25 
1 
0'5 
1-5 
— 
— 
— 
— 
— 
— 
— 
— 
— 
35 
m 
|H 
5-fl 
o 
<o 
— 
0'25 
0-26 
1'5 
0-5 
1 
2 
1-5 
1 
— 
— 
— 
— 
— 
— 
— 
8 
. 
m 
5-62 
|H 
CO 
2-26 
3-75 
2 
226 
4'75 
2 
— 
— 
— 
— 
— 
— 
— 
17 
m 
« 
5-6 
<M 
.CO 
0-5 
0-5 
0'5 
2 25 
3 
3 25 
5 25 
3-5 
7'5 
5 25 
1 
— 
— 
— 
1 
—. 
— 
33'5 
•P 
■*< 
5-6 
m 
CO 
0-5 
— 
1 
2 
4'25 
06 
0-5 
13'75 
10 
5 
2'5 
3 25 
0-25 
_ 
— 
— 
■— 
01-5 
m 
m 
5-6 
3 
l 
— 
0'26 
4 
8 
18-5 
10 
1076 
10'25 
12-75 
5-75 
6 
3 
075 
1-6 
— 
—■ 
05 5 
m 
CO 
3 
3 
— 
0-26 
6 
0-26 
1075 
10-75 
26'5 
24-25 
18-25 
18-75 
8-75 
1-25 
075 
1-5 
— 
^— 
142 
m 
i> 
5-6 
CO 
CO 
1 
0-5 
2-75 
3 
7-5 
17-5 
2575 
31-5 
1(1 
11-75 
1075 
7 
2-5 
— 
— 
137'5 
. 
IQ 
5-68 
t> 
CO 
— 
or> 
125 
125 
5-5 
10 
li)'5 
236 
24 
10 6 
10 
775 
7-5 
5<26 
1 
1-25 
1-25 
~~" 
154 
m 
at 
5-6 
s 
— 
— 
1'5 
36 
5 25 
12'5 
20 5 
20 
226 
1475 
1075 
0-5 
225 
2 
0 25 
0-25 
1 
— 
141-5 
m 
o 
5-7 
a 
CO 
—, 
0'25 
075 
2 5 
2 
1375 
13 26 
21'6 
ior> 
2075 
11-25 
(1 
2-5 
— 
— 
1 
1 
~~" 
11(1 
m 
|H 
5-7 
o 
t> 
— 
0'25 
1-25 
— 
25 
3 25 
8'5 
10 
14'5 
1076 
10 
7-5 
0 5 
2'5 
0'5 
— 
— 
— 
78 
m 
(N 
5-7 
|H 
t> 
1 
0'5 
0-6 
3-5 
0-26 
8 
8-5 
025 
325 
0-76 
1 
— 
0'25 
0-25 
40 
. 
IQ 
CO 
J, 
ifl 
t> 
— 
— 
— 
J- 
— 
1 
2-25 
2-25 
3-6 
5 
275 
3-25 
325 
1-75 
1 
1-5 
075 
0-25 
28-5 
9 
-* 
5-7 
CO 
t> 
— 
— 
— 
— 
— 
— 
— 
— 
1'6 
1 
((■5 
0'5 
— 
0-6 
— 
— 
— 
— 
4 
m 
m 
5-7 
-* 
i- 
— 
— 
— 
— 
1 
1 
1 
0'5 
2 
— 
— 
— 
— 
— 
5-5 
T0TAI8 
'2 
1-6 
3-6 
20-5 
38-5 
01-6 
80-5 
148 
173-5 
140-5 
128 
108 
03 
42 
20 
8'5 
4 
4 
3 
(>5 
1078 
Regression 
14.3. The rows and columns will be referred to by the general term " arrays " and 
we shall consider the two variates x and y, x being taken to vary horizontally, i.e. in rows, 
and y vertically, i.e. in columns. Consider then the means of arrays. Take two axes 
OX and 0 Y at right angles representing the variates x and y. On this frame of reference 
plot the points whose ,absoissae are the centres pf the aj-intervals and whose ordinates are 
the means of the corresponding distributions of y in the columns centred at the appropriate 
aj's. Similarly, plot the means of the ^-distributions against the centres of the 
corresponding ^-intervals. (In practice it is useful to distinguish the two, the means of x's being 
denoted by small ciroles and those of y's by crosses.) Fig. 14.1 shows such a diagram 
for the data of Table 14.3 and Fig. 14.2 for the data of Table 14.2. 
The means of arrays will in general lie more or less closely round smooth curves. For 
example, in Fig. 14.1 t^iey lie approximately on straight lines, whereas in Fig. 14.2 one set of 
means certainly does not. Such curves are called regression curves and their equations with 
respect to OX and 0 Y are called regression equations. If the lines are straight the regression 
is said to be linear ; if not, curvilinear or skew. 
To put these geometrically expressed ideas in analytical language, suppose the mean 
328 
PRODUCT-MOMENT CORRELATION 
of a; in the array centred at yi is xt. Then the points (xt, yt) may be represented by a 
^motional equation— 
z=M ■' (14-!) 
which is the regression equation of x on y. If the regression is linear, 
x = fy + a.. 
There will also be an equation 
the regression of y on x. 
c 
«8 
Father's Stature (inches). 
y = g{x) (14.2) 
Proportion of Male Births per 1000 births. 
ISO U9Q 500 r^SIO 520 530 540 
;,.< 
ta 
£.£ 
to 
c 
67 
09 
.71 
73 
75 
*? 
-N. 
4^ 
6L R 
H 
6b 
M 
o 
"•^ 
*-- 
\ 
°\ 
"\< 
\ 
X 
0 S 
^ 
• 
68 
K 
PJ 
^ 
i 
•^ 
vH 
f 
' 
70 
S 
\ 
\ 
c 
k 
\ 
72 
■\^ 
/ 
O 
+—+-,.> 
4~-K. 
/O 
c 
20 
30 
* 
c 
2 40 
*fe 
v. 
<u 
"I 50 
3 
C 
a 
+o 
£ 60 
70 
80 
/ 
'+-+ 
e>—=r 
+ 
fti- 
i 
^K 
xK=F 
Jt 
-+-.f" 
Fig. 14.1.—Regression Lines of Data of Table 14.3. 
Means of rows shown by circles, and corresponding 
regression line by RR ; means of columns shown by 
crosses, and corresponding regression line by CC. 
Fig. 14.2.—Regression Lines of Data of Table 14.2. 
Means of rows shown by circles, and corresponding 
regression line by RR ; means of columns shown by 
crosses, and regression line by CC. 
In this chapter we shall mainly be concerned with the case in which regressions are 
linear or very nearly so. 
14.4. In an observed distribution the means of arrays will not as a rule lie exactly 
on straight lines, or indeed on any simple curvUs, although they may be very near to doing 
so. The question then arises : if the regression is " approximately " linear, what is the best 
line to take as the regression line ? The question may ,^e answered by an appeal to the 
method of least squares. The regression of x on y, say x = fly + a, will be determined by 
minimising the sum 
U = ZNAxt - fiyt - a)2, .... (14.3) 
REGRESSION 329 
the summation extending over all ^-intervals. Here Nt represents the frequency in the 
ith. row, and we note that H(Nt x4) is equal to the total frequency N times the mean of x for 
the whole distribution. 
From (14.3) we have, for the minimal values of a and /?, 
- — = 2£Nt(sBt -Pyt-*)=0 • . . . (14.4) 
9/7 
- — = VSNtyAzt - fa - *) = 0 .... (14.5) 
Now choose the origin at the means of x and y for the distribution. Then ENi xt = 0 
and ENiyt = 0 and hence, from (14.4) 
a = 0. 
Prom (14.5) we then have 
Z{Niyixi)-(iZ{Niyi*) = Q. 
Now since the origin has been chosen at the mean of x and y, ^(^ yi xt) = N cov (x, y) 
and i^JVf y^) = N var y. Hence we have 
covJ*,jrt_ 
var y 
The equation of the regression of x on y, taking X and Y to be current co-ordinates, 
will then be 
z = coL(2,1)y 
var y 
Referred to an arbitrary origin for which the means of x, y are x, y, the equation is 
(X - a) = ^ <*'%-£) (14.8) 
var i/ 
Similarly we find for the regression of y on x 
(Y -y) = C^^L](X -x). . . . . (14.9) 
var x 
Equations (14.8) and (14.9) are fundamental. If the regressions are exactly linear they 
give the regression equations ; if not, they give the " best " straight regression lines in the 
sense of the method of least squares. 
The Coefficient of Product-moment Correlation 
cov (x i/\ cov (x ?/) 
14.5. The coefficients —■-■ ' and -'— are called regression coefficients and 
var y var x 
will be denoted by (}x and /?a respectively.* 
We now define 
p = OW' 
_ cov^j,) 
(var x var yy 
* There is little danger of confusion between this notation and the use of /3U {}% to indioate 
measures of skewnees and kurtosis. The two rarely occur in the same context. 
330 PRODUCT-MOMENT CORRELATION 
p is called the coefficient of product-moment correlation or briefly the correlation coefficient. 
It provides an important measure of the relation between two variates for which the 
regressions are approximately linear. In this expression the square root is to have positive 
sign. 
Let us note in the first place that p cannot be greater than unity in absolute value. 
For we have, taking an origin at the means and summing for all pairs of values of x, y over 
the population, 
£{x- M2 = £(*2) - 2^(ay) + (i\Z{y*) 
= Z{x*){\ -2/?^ + ^,} 
.= Z{x*){\ -p2) (14.11) 
Thus 1 — pa cannot be negative. 
Furthermore, if p = ± 1, E{x — /?#)2 = 0 and hence every x -h (}$ = 0. Thus the 
variates are linearly related by the equation X — ^Y = 0. Ifp = 0 the regression 
equations become X = 0, Y = 0, for then cov (x, y) = 0. Hence the means of arrays are 
the same for all arrays. 
14.6. If p = + 1 we say that the variates are perfectly positively correlated ; if 
0 < p < 1, that they are positively correlated; if p = 0, that they are uncorrected ; if 
0 > p > — 1, that they are negatively correlated ; and if p = — 1, that they are perfectly 
negatively correlated. 
" Uncorrelated " is not the same thing as " independent." If the variates are 
independent they are uncorrelated, but not vice-versa. Table 14.2 and Fig. 14.2 illustrate this 
point. The regression lines, as shown in the figure, are close to X = 0, Y = 0 and the 
correlation is, in fact, very small (— 0-014). But the variates are obviously far from 
independence and if the data are grouped, in columns up to 494-5, by single columns up 
to 521-5, and over 521-5, and by rows 8-11, 12-13, 14-15, 16-17, 18-19, 20-2,1, 22-23, 
24-25, 26-27, 28 and over, the coefficient of contingency is 0-47. 
14.7. The calculation of the correlation coefficient in numerical examples requires 
that of the means and variances of the two variates and their covariance. The last Ls the 
only new type appearing, the others being calculable from border frequencies in the manner 
exemplified in Chapter 3. 
Taking an arbitrary origin, we have,- if the means of x and y are a and 6, 
N oov (x, y) = E{x — a)(y — b) 
= S(xy) - aZ{y) - bZ{x) + Nab 
= S{xy) - Nab 
cov (a, y) =-^E{xy) -ab. .... (14.12) 
Thus we may, as in the univariate case, take an arbitrary origin for arithmetical convenience, 
calculate the produet sum E(xy) and determine the covariance by the use of (14.12). The 
THE COEFFICIENT OF PRODUCT-MOMENT CORRELATION ■ 331 
calculation of the product sum is exemplified below. As seen in 3.30, no Sheppard's 
corrections are required for the first product moment. 
Example 14.1 
To find the correlation coefficient and regression lines for the data of Table 14.1. 
We find the means and variances from the border columns in the usual way. 
An arbitrary mean is taken for x (age) at the centre of the interval 20-5 years and for 
y (highest pitch) at the centre of the interval 19- thousand vibrations which may be taken 
as 19,995. We find 
Z{x) = 2,604, fz[(x) = 0-770,642 
*Z{y) = - 708, ^[(y) = - 0-209,529 
Z(x*)= 47,392, jx2{x) = 13-348,229 
-%2) = 8894, fh(y) = 2-504,904. 
To find the produot sum E{xy) we require the product xy for each non-zero cell of the table. 
These products are shown in brackets in Table 14.1. Then we have, reading the table 
from loft to right and from top to bottom, 
S{xy) = (1 x 0) + (1 X - 14) + (1 X - 84) ' 
+ (1 X 18) + (1 X 12) ■+ (3 x 6) + etc. 
= - 12,535, 
whence cov {x, y) = —^p- - (0-770,642)( -0-209,529) 
3379 
= - 3-548,205. 
Thus p=^°J^y)_= -0-6136, 
a substantial negative correlation. The highest audible pitch docreases with increasing age. 
We also find 
/5l==co^^)==_ 1-417 
var y 
, , 0, = ^L^L.V). = - 0-2658. 
var x 
The regression equations, for the units of the table and with our arbitrary means, are then 
X — 0-7706 = — 1-417(F + 0-2095) 
Y + 0-2095 = — 0-2658(X - 0-7706). 
Example 14.2 
The following device is often useful in calculating product moments. We recall that 
2Z(xy) = Z{x + yY - Z(x*) - Z{y*) 
= Z{x*) + Z(y») - Z(x - y)\ 
Thus we may find E{xy) from either £(x + y)a or Z(x — y)'1, and these quantities are often 
more convenient to calculate. 
For example, in the preceding example we note that x + y is constant down the 
diagonals running from the bottom right-hand to the top left-hand corner of the table. 
Taking x -\- y tobe zero in the cell centred at x = 20-5- and y = 19-, we may, in our 
1 
332 PRODUCT-MOMENT CORRELATION 
units, take it to be -f- 1 in the cell 23-5-, 19-, and -1 in 17-5-, 19-, and so on. If we 
sum up the diagonals we get— 
x +y. 
-9 . 
-8 
-7 
-6 
-5 
-4 
-3 
-2 
-1 
0 
1 
2 
3 
Sum. 
1 
1 
5 
11 
20 
93 
207 
434 
594 
637 
418 
281 
185 
x + y. 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
14 
15 
Total 
Sum. 
124 
112 ' 
00 
59 
38 
23 
'21 
9 
9 
2 
4 
1 
3379 
The total is 3379, which provides a check on the work. We then find the sum of squares 
in the ordinary way, obtaining 
Z{x 4- y)z = 31,216 = Z{x*) + Z(y*) + 2 I{xy). 
Then Z{xy) = £(31,216 - 47,392 - 8894) 
= — 12,535 as before. 
The rest of the calculation follows the same lines as in Example 14.1. 
We should have obtained the same result for Z(xy) if we had summed up the other 
diagonal. Which diagonal is chosen depends on how the frequencies lie in the table. 
Example 14.3 
m 
In the foregoing the regression lines and the correlation coefficient were arrived at 
from a consideration of grouped frequencies in a bivariate table. We may, however, apply 
the same ideas to ungrouped material. There are no longer means of arrays, but the 
regression lines are still to be interpreted as the lines of best fit to the N pairs of variate- 
values and the correlation coefficient as a measure of relationship between variates. 
Table 14.4 shows the yields of wheat and potatoes in 48 counties of England in 1936. 
In this particular case it is hardly worth while taking an arbitrary origin other than that 
given. We find (x = wheat, y = potatoes) 
£(x) = 
Z(y) = 
£{x*) = 
Ay2) = 
E{xy) = 
= 758-0, 
= 291-1, 
= 12,170-48, 
= 179103, 
= 4612-64, 
i"i(«) = 
My) = 
i"a(«) = 
My) = 
/"n(3»2/) = 
0-326,888 
15-791,667 
6-064,583 
4-174,930 
0-533,958 
0-326,888 
P V(4-174,930 X 0-533,958) 
= 0-2189. 
fix = 0-6122, 
0, = 0-07830. 
THE COEFFICIENT OF PRODUCT-MOMENT CORRELATION 
333 
TABLE 14.4 
Yields of Wheat and Potatoes in 48 Counties in England in 1936. 
County. 
„* 
Bedfordshire .... 
Huntingdonshire 
Cambridgeshire .... 
Ely 
Suffolk, West .... 
Suffolk, East .... 
Essex 
Hertfordshire .... 
Middlesex 
Norfolk 
Lines. (Holland) 
„ (Kesteven) . 
„ (Lindsey) . . . 
Yorkshire (East Riding) . 
Kent 
Surrey 
Sussex, East .... 
Sussex, West .... 
Berkshire 
Hampshire 
Isle of Wight .... 
Nottinghamshire 
Leicestershire .... 
Rutland 
Wheat 
(cwts. 
per acre). 
16-0 
160 
16-4 
20-5 
18-2 
16-3 
17-7 
16-3 
16-5 
16-9 
21-8 
15-6 
15-8 
16-1 
18-5 
12-7 
15-7 
14-3 
13-8 
12-8 
12-0 
156 
158 
16-6 
Potatoes 
(tons 
per acre). 
5-3 
6-6 
61 
5-5 
6-9 
6-1 
6-4 
6-3 
7-8 
8-3 
5-7 
6-2 
60 
6-1 
6-6 
4-8 
4-9 
51 
55 
6-7 
6-5 
52 
52 
7-1 
County. 
Northamptonshire . 
Peterborough 
Bucldnghamshire . 
Oxfordshire .... 
Warwickshire 
Shropshire . 
Worcestershire . 
Gloucestershire 
Wiltshire. 
Herefordshire 
Somersetshire 
Dorsetshire . 
Devonshire . 
Cornwall 
, 
, 
. 
« 
■> 
. 
. 
. 
. 
. 
Northumberland 
Durham .... 
Yorkshire (North Riding). 
(West Riding) . 
Cumberland 
Westmorland 
Lancashire . 
Cheshire . 
Derbyshire . 
Staffordshire 
t . 
t . 
. 
. 
. 
Wheat 
(cwts. 
per acre). 
14-3 
14-4 
.15-2 
14-1 
15-4 
16-5 
14-2 
13-2 
13-8 
' 14-4 
13-4 
11-2 
14-4 
15-4 
18-5 
16-4 
17-0 
16-9 
17-5 
15-8 
19-2 
17-7 
152 
171 
Potatoes 
(tons 
per acre). 
4-0 
5-6 
6-4 
6-9 
5-6 
61 
5-7 
5-0 
6-5 
6-2 
5-2 
6-6 
5-8 
6-3 
6-3 
5-8 
5-9 
6-5 
5-8 
5-7 
7-2 
6-5 
54 
6-3 
. 3 
v. 
^ r- 
& 5 
1 
* : 
X 
c x 
x 
X . 
X / * 
X I X 
*/ 
X / 
/x 
yOWK 
X 
X X 
r 
■ 
/\ 
X 
X 
/ 
/ 
X 
X 
10 IZ /4 16 18 ZO 
Wheat Yield (cwts.f>eracre}, 
Fia. 14.3.—Scatter Diagram of the Data of Table 14.4. 
n 
334 PRODUCT-MOMENT CORRELATION 
The regression lines are 
X - 15-792 = 0-6122 (7 — 6-065), 
Y - 6-065 = 0-0783 (X - 15-792) 
The data are shown in a graphical form in Fig. 14.3\ Corresponding to each pair of 
values {x, y) there is plotted a point with those values as abscissa and ordinate. The 
totality of points furnishes what is known, for obvious reasons, as, a scatter diagram. The 
two regression lines are also shown. 
The Bivariate Normal Distribution 
14.8. The distribution 
2naxat{\ - p*)* * \ 2(1 - p*)\<% 0lo2 ^ a\)) ff K ' 
has already arisen (5.24) as the natural extension to two variates of the univariate normal 
distribution. In writing p in (14.13) we have anticipated a result which will now be proved, 
namely that p in that equation is in fact the correlation coefficient of the distribution. 
The characteristic function of (14.13) is 
<f>(t, u) = exp [— $(t*o{ + 2utpOi02 + w2of] 
whence var x = of, var y = o\ 
cov (z, y) = pda2 
and the correlation coefficient is .. \ \. = p, as stated. 
The exponent in (14.13) may be written 
Thus for any fixed y, x is distributed normally about a mean given by 
ox Oz 
and hence the means of the ft-arrays of infinite thinness lie on the line 
X _PY 
Ci o2 
and this is the regression of x on y. Similarly the means of ^-arrays, lie on 
Y _PX 
"~— -— j ■ • • • • 
Ofl Ox 
i 
the regression of y on x. Thus the regression lines of the bivariate normal surface are 
exactly linear. 
ITurthermore, from (14.14) it is seen that the variance of an array of x for fixed y is 
i.e. is independent of y. Similarly the variance of an array of y is 
oi(i-p") 
: and is independent of x. 
(14.16) 
(14.17) 
I 
\ 
THE BIVARIATE NORMAL DISTRIBUTION 335 
Thus the variance of ^-arrays is the same for all arrays ; and so for y. Distributions 
for which this is true are called homoscedastic. 
If p = 0 the distribution becomes the product of two normal distributions and the 
variates are independent. Thus two uncorrelated normal variates are independent. 
14.9. A criterion for linearity of regression for the general bivariate surface may 
be obtained in terms of moments or cumulants. Taking an origin at the means of the two 
variates we have, if the regression of x on y is linear, 
x = p<y 
or, if f(x, y) is the frequency function, 
/»00 pOO 
xf(x, y) dx = p$ \ f{x,y)dx. 
J — 00 J —00 
Multiplying both sides by yp and integrating over the range of y, we have 
/•OO /»00 iKO /•«) 
ypxf(x,y)dxdy = ^\ yv+1f(x,.y)dxdy 
J —00 J —00 , J —00 J —00 
or fihp = jS1A*o,p+i- (14.18) 
In particular fin — /?i ftQ2, 
so that 0o2 0i,P = 0n0o,p+i> (14.19) 
a condition on the central moments. Recalling the definition of bivariate cumulants 
we see that the univariate mean moments are related to cumulants in the same way as 
moments filiP to cumulants k1p and hence we also have 
, Ki. v ==/?i Ko,p+i (14.20) 
*02 "l.p = Kn Ko,p+i (14.21) 
Similarly, if the regression of y on x is linear we shall have 
020 0p, i =0ii 0p+i,o\ _ _ (14 22) 
K20 Kp, 1 = KU Kp + 1,Q J 
Under certain conditions these equations are sufficient for linearity of regression. For 
instance, if (14,18) is true for all p, then *• 
j^oo yV ^{Eoo{X ~ M f{X' V) dX] = °" 
The expression in curly brackets is thus a function, not necessarily positive, whose moments 
vanish, and under certain general conditions this implies that the function itself vanishes, 
i.e. that 
r_ 
(& - Piy) /(*. y) dx = o 
or x = 0$, 
so that the regression of x on y is linear. 
336 
PRODUCT-MOMENT CORRELATION 
Example 14.4 (from Wicksell, Biometrika, 25, 126) ■ 
Consider the bivariate distribution of the squares of variates x, y which are distributed 
in the bivariate normal form. The characteristic function of these variates is 
proportional to 
a. 
eitx*+iuy* exp _ 
X' 
2pxy y* 
2(1 - p2) \a\ a1ai a\) , 
- Ll>p - 2-raE*1 - »«i -"W - 
This is proportional to (compare Exeroise 1.5) 
1 
2pxy 
ffiffa 
y 
2 
+ ^{1 -2oJ(l -p")iu 
m} cfody. 
1 
_l{l-2of(l-p"W 
fficra 
T,{1 — 2o-l(l -p*)iu 
4 
which, except for constants, reduces to 
{(1 - 2a\it){\ - 2a\i%)'- V<y2ff|^)(4-M)}-*. 
This is the characteristic function, for when t = u = 0 it reduces to unity. 
Now the frequency-distribution represented by this function is evidently not normal 
but its regressions are linear, for we have, taking logarithms, 
giving, on identifying coefficients, 
*„.o=*(P-l)K2of)p 
^(i = V^i(2(T?y-y 
*ll = 2paorfff| ; N 
and henoe 
*20 Kp, 1 = ^ll Kp+1, 0- • 
Sampling of Regression and Correlation Coefficients 
14.10. We new turn to consider the sampling problems associated with the coefficients 
of correlation and regression. 
First of all, as to standard errors. In Example 9.6 on page 211 we have anticipated 
the determination of the sampling variance of- the correlation coefficient itself, obtaining 
the result for the normal case 
var r = -(1 — p2) 
2\2 
n 
s. 
. (14.23) 
Here, as usual, the Roman r is written for the value of p in the sample and n is the number 
in the sample. The result of (14.23) is not of great value, since the distribution of r tends 
to normality very slowly if p is not close to zero. It is probably as well not to use (14.23) 
unless n is greater theEn 500. 
SAMPLING OF REGRESSION AND CORRELATION COEFFICIENTS 337 
In the manner of Chapter 9 we have 
<3&2 (3mu _ dfflw 
6a mlx mB0' 
giving 
var 6fl _ var mu var mB0 _ 2 cov (mn, mfl0) 
b\ mfi m|0 w»iiWi0 
• -» 
Substituting from (9.16) and (9.17), and writing the sampling values ra instead of the parent 
fi'a, we have 
var bt = -si —- + —s ). ... (14.26) 
or, for the normal case, on using the values of Example 3.15, 
61/1 - r«\ 
var 6a = 
bl(l 
ra 
l™V(l-r*) (14.26) 
w var x 
Similarly 
var bt = -1 T?^(l - ra) (14.27) 
?i var y v 
To our order of approximation it is indifferent whether we write 1 —ra or 1 —p2 
on the right-hand side of these equations. 
Example 14.5 
In the data of Table 14.3 (height of fathers and height of sons) we find r = -f- 0-51, 
for n = 1078. From inspection of the table we see that the distribution is reasonably 
close to the normal type, and in this case n is large enough to justify the use of the standard 
error. We have then 
{1 - (0-51)2}2 
var r = - 
1078 
= 0-000,508. 
Thus the standard error is about 0-023. The correlation is thus undoubtedly significant, 
if the data were obtained by random sampling. It is improbable that the parent 
correlation p lies outside the range 0-51 ± 0-05, and very improbable that it lies outside the 
range 0-51 ± 0-075. 
Estimates of Correlation and Regression Coefficients in Normal Samples 
14.11. In large-sample theory the sample values of the correlation and regression 
coefficients may be taken as estimates of the population values in the usual wayv They 
can also be used in small-sample theory and it may, in fact, be shown that they are estimates 
giving maximum likelihood to samples from a bivariate normal population. 
a.s. z 
338 
PRODUCT-MOMENT CORRELATION 
The joint probability of n sample values (x1} y-y) . . . (xn, yn) from a bivariate normal 
population with means m, and wifl is 
dF = 
-„exP 
{2n)na1na2n{l - p2)2 
L 2(1- 
A2 9r(»-mi 
(14.29) 
+ E (V—-pYXjdz,. Ay, . . . dxn dyn (14.28) 
The likelihood function may then be written 
L cc ! n exp ["- ±-.{A - 2PB + 0)1 . 
and thus, for the maximisation of log L we have 
7 TET - - «i^-«(- 3*»" - mi) + —^ - »•>} = 0 
L 9mx 2(1 '— jo2) [ ff| ffiffa J 
giving — 2"(a; — mx) — £- 27(2/ — m2) = 0 . 
(14.30) 
1 37" 
Similarly from ■=■ —— = 0 we have 
L dm2 
(14.31) 
■t- 2(x —ml) S{y — ro«) = 0 
, Thus from (14.30) and (14.31), p not being unity in general, 
Z{x — wii) = 2"(2/ — mfl) = 0 
)«!=- Six) 
n 
i raa = - 2"(2/) 
n 
so that our estimates of the means are the means of the sample. 
We also find, equating — log L, -=— log L and =- log £ to zero and cancelling factors 
oox oo"fl dp 
(14.32) 
in Oi, cra and (1 — p2) respectively, 
-n + 
1 
1 
(X - PJ?) = o 
(-pB + C) = 0 
whence 
1 -pz p 
n n 
(14.33) 
DISTRIBUTION OF COVARIANCE IN NORMAL CASE 339 
giving for the estimates of a\, of and p 
of = -S{x -wO* 
n 
ai = -Z{y - «,)■ 
= i7(a_— m0(2/ - m2) 
9 {2{x - raj2 £[y"-~mt)*}i 
. (14.34) 
whioh, on substituting the estimates of rat and wa given by (14.32), become the sample 
variances and correlation coefficient. 
Distribution of Sample Means, Variances and Govariance in Normal Samples 
14.12., In accordance with the result of the previous section we will take our estimates 
of the parameters rals mfl, a\, of and p to be the corresponding sample values x, y, s'l, si 
and r. The joint distribution of the sample values is given by (14.28) and it is remarkable 
that the exponent in that expression can be expressed solely in terms of the five parameters 
and their estimates. We have, in fact, 
^/x - mx\2 _ s(oj- mjiy - m3) + ^/y - ra3\2 
„(x — x-{-x — ?n1)2 . .. 
= 2;x -- + two similar terms 
= S - - 0 - + .£ „ — + four similar terms, 
the product terms vanishing because Z(a; — x) = E{y — y) = 0, 
f*f + [x - Wi)2! , , y . „ , 
= ?u » -^- f + two similar terms 
\ of ffiffa of J 
+ J* _ *»* + «l (14.35) 
lof ffiffa 0£j 
Wc proceed to find the joint distribution of the five statistics, and to do so require 
to express the frequency element (14.28) in terms of them. The non-differential part of 
that element is given by the exponential of (14.35). It remains to express in the requisite 
form the volume element dxx . . . dxn dy-^ . . . dyn. 
Generalising the geometrical approach of Chapter 10, we may imagine a sample space 
of 2n dimensions, n for x and n for y. The sample point may vary in the avspace and the 
2/-space, but not independently so. In fact, if P represents the point (a?i . . . xn) in the 
#-space and fhe point (yx . . . ytl) in the y-sp&oe, and if Ou 0a are the points (x . . . x), 
(y . . . y), then for any given r we have 
r = E{x - x){y — y) = Z{x - x)[y - y) 
nsx8% • {E{x - x)*Z{y - ?y)2}* 
and thus r is the cosine of the angle, say 6, between P0^ and Q02, so that if P and r are 
340" PRODUCT-MOMENT CORRELATION 
fixed Q varies on the cone in the y-space obtained by rotating 0*Q such that the angle 
made with OJ? is constant. 
The element in the aj-space is proportional to s™~2 dsl dxx as was seen in Example 10.5. 
For given r, y and sa the point Q varies on the zpne of the hypersphere of radius s^y/n, 
centre y and (n — 1) dimensions. This zone has radius sa Vn sin 0= s^n (1 — r2)* and 
width sa\/n dQ = —■—^ and thus its content is proportional to {s^n{\ — r2)1}71-3 
£p£_*,that is, to 8-~\l - r'M 
Thus the volume element may be written 
dv oc s^~2 dsi. dx 5an-'2 dsz dy (1 - ra)~2~ dr 
oc s^'2 s*1-2 dax ds* (1 - ra)V dr dx dy . . . (14.36) 
and the joint frequency element of the five variables is then proportional to 
n V\{x — raj2 0 
(x - mjjy — to,) (y - wfl)2] 
+K3-^.+S]*(I"7) 
This fundamental result is due to R. A. Fisher (1915). 
14.13. One important property of (14.37) may be remarked. The distribution may 
be factorised into two parts, one containing only x and y and the other only Si, sa and r, 
namely (except for constants) 
jn r n ((£ — rax)2 n (x — mMy — mfl) , (y — ma)2]~l ,_ ,_ ,.,nm 
cLFccerp - — - v -2pK ™- I' + Kif . 3; \dxdy . (14.38) 
and • 
«LF oc exp f- n H - ?^i£5 + HV"2 --""'(I - '■P* *i *. *" • (14-39) 
Thus we see that in normal samples the distribution of means is entirely independent of 
that of variances and covariance. 
Before leaving (14.38") we may also-note that the means are themselves distributed 
9 » 
in the bivariate normal form, with mean (x) = mu mean (y) = mfl, var (x) = —, var (y) = — 
(all of which results are already familiar), and 
oov{x,y) =^£t (14.40) 
so that the correlation between x and y is p, the correlation in the parent population. 
14.14. We may now use (14.37) to obtain the distribution of the correlation 
coefficient, namely, by integrating with respect to Sj. and s2 from 0 to oo. Let us first of all 
evaluate the constant to be attached to it from the consideration that 
J* = i. 
DISTRIBUTION OF COVARIANCE IN NORMAL CASE 
Make the variate transformation 
841 
4 
a = -4. 
n 
b = 
of 2(l-p2j" 
r5i«a n 
<ri<rfl"2(l -p2) 
c=%. 
n 
We have for the Jacobian of the transformation 
(14.41) 
d[a, b, c) _ 
0 
0 
2st n 
"5? "2(1 -pa) 
ovS*2(l-p«) ~o^2(T^V) l^'2(l-p») 
rsi 
w 
0 
_ sfs2, nz 
0 
2s„ 
w 
"ff2,2(l -p2) 
2acw 
and also the relation 
fo»('l - p2)8 fflor,(l - p2) 
62 
r2 = 
ac 
The integral then becomes 
| exp[ - a + 2,6 _ c].|2«°i(l^-^)pf2Co|(l^ P»)p 
xfc-gre.'Wi-P'U.ft.i. 
\ ac/ 2acw 
2n-3 ffin- 
1 n ra-1 n /,2^w-l r "n-i 
'n_i —— exp[- a + 2^6 - c] {ac - b*)~2~ da db dc . (14.42) 
where the limits of a and c are 0 to oo and those of b are — oo to -j- oo. This integral may 
b2 
be evaluated in terms of the /'-function. Putting £ = a we find 
c 
f / 62\ 7^— w~* 
exp (— |) exp J + 2pb — c J£ 2 c 2 df d& dc 
= ^(^) J exp [- i{(6 - pcy + (1 - pV}] 
= r(l^?V» J exp{- (1 - p^c^-T1 dc 
w~4 
c 2 cZ6 dc 
= v* „_i 
(1 - P2)— 
__ n r{n - 2) 
ra-1 
2W~3(1 -j02)~2~ 
. (14.43) 
342 
PRODUCT-MOMENT CORRELATION 
Collecting up the terms from (14.42) and (14.43) we findjor the joint distribution of slt 
aa and r 
d.F = 
n 
w-l 
n-l 
Ttof-1 o-a'1-1 (1 - P2)~2~ r{n - 2) 
Now put 
SiS2 
r n (s\ 2Prsls2 s!\ ~| 
v — 4 
Si7i-2 Sa»-2(i _ y2j a ^Si ^ ^r (14.44) 
<72Si 
C = ^, z=log^, r = r. ■ 
We find, for the Jacobian of the transformation 
S2 Sl 
d(8lt 8%, r) 
L _I 
Si \ Sa 
0 0 
_ JL 
o,1o,2 
0 
0 
1 
The exponent in (14.44) becomes 
exp [" 2(r~-y-){CeB"2K + Ce~0}] 
and after a little reduction the distribution becomes 
dF = 
rt 
71-1 
71-1 
jr(l - p*)T- r(n - 2) 
On integration with respect to £ we have 
[n 1 w-4 
- a _ 2) £(cosh z ~ Pr) C"~8d% «fe(l - ra)"a rfr. 
^ 
_ (1 - p2)V r(» - 1) . (1 - r2)^ 
w r(n — 2) (cosh 2 — pr)71-1 
Putting — pr = cos 0 we have, since 
dz 0 
dzdr. 
/•CO 
Jo 
cosh z + cos 0 sin 0 
dJP = 
_ (i - p2)"1 _ -* ^-a / M 
—~-3Vsinf9/ 
7rr(w - 2) 
d( — cos 0)T 
Adr 
71-1 
= ^-P2)2 (1 _ r2)^__^-l fcos-i( - Pr)\ 
nl\n -2)K > d(rp)n-z\^/(L - p*r*)] 
This is as simple a form as can be given in terms of elementary funotions. 
(14.45) 
, 14.15. In the particular case p = 0, (14.45) reduces to 
DISTRIBUTION OF COVARIANCE IN NORMAL CASE 343 
a form surmised by " Student " in 1908. This distribution provides a test of the hypothesis 
that'an observed r arose from an uncorrelated normal population. Its distribution function 
may be obtained from incomplete .B-funotions, or more conveniently' by putting 
' = V(l - r') V(" - 2) (W-47) 
which transforms (14.46) to 
dF = -- V sir- 7 -tw-t • • • (14-48) ' 
The integral of this funotion has been tabulated and is given as Appendix Table 3. 
Fisher and Yates have also tabulated some of the significance points of t and of r itself, 
i.e. the values of r (for various n) for which the distribution funotion takes specified values. 
14.16. The general distribution (14.45) has been studied in some detail, but laok 
of space prevents the inclusion of the extensive analysis involved. We will here indicate' 
only the more important features of the results. 
First, as to the shape of the frequency curves. When n = 2 the distribution becomes 
= (1 -p«)coa-i (-/*■) dr 
• jr(l - r2) V(l - P*r*) 
and the frequency curve may be written 
y = ^"(l-Asmfl ° = C0S"1(" >*>■■■• <14-49) 
For r = ± 1 the ordinate is infinite and the distribution will be found to be U-shaped. . 
When n = 3 we find 
' f 1 0 cos 01 1 /,,™i 
y = Vo i-^-Ta ^Trr 71 5\-i» .... 14.50) 
[sina0 sin30J (1 — r2)1 x 
again a U-shaped distribution. For n = 4, 
y = -r^-7i{0 - 3cot0 + 30cot20} (14.51) 
sin8 0 x ' 
If p = 0 this reduces to the rectangular form y = \. In other cases the curve is J-shaped, 
increasing from a jninimum at r = — 1 to a maximum (but not an infinite maximum) 
at r = -j- 1. 
For n > 4 the frequency curves are unimodal and tend to normality with large n, 
though slowly. Some interesting photographs of models of these curves are given in the 
"'Co-operative Study" (1917). 
14.17. The moments of the distribution are expressible in terms of hypergeometric 
functions. Returning to (14.44) let us write 
a, -^ , a, 
344 PRODUCT-MOMENT CORRELATION 
/ 
After a little rearrangement the distribution becomes 
Putting ux = —, m2 = — and expanding the term in exp (pr-^-) we have 
n-l ti-4 
'X^'^V^fo.fr . . (14.63) 
Integrating for m8 from 0 to oo we find for the distribution of u-l and r 
dF = 2^3Z exp (- WK-'d - r^T<te,drjtffi? rf* ~ \ + ^ ■«*¥*. 
Multiplying by r and integrating from — 1 to + 1 we find 
*i>-2)exp( WMl *■ * 4iW+Wr\~T-)B\-2--—2-)-2 
and finally, integrating with respect to uy we obtain 
^ifrX-a-P'f^f1 P2m r/n + 2j\ /n-2 2j+3\ /rc+2j\ e 
Substituting for the 5-function in terms of P-functions and remembering that 
r(x) r(x + i) = ~ r(2x), 
we find 
n-l ,n^ 
Pi(r) = 
\V__U , JMlP2 , fofln + l).jn(fo + 1) P4 , 1 
r(n ~ ^iY" + ^ i(n + 1) 21 ^ |(n + l).l(n + 3) 4! + ' ' */ 
n-l f%y 
P(i-o*)—r*('i 
- 7n-l\ /n + 1\ ^(^' **' *{n + l)> p2) 
r(VW-r) 
and since ^(a, ft y, x) = (1 - a)*-*- ^(y - a, y - ft y, a;) 
DISTRIBUTION OF COVARIANCE IN NORMAL CASE 346 
In a similar way it may be shown that 
£ (r) = 1 - (1 - p*) 2JZ* F(l, 1, \{n + 1), p*) . (14.66) 
n — 1 
These series converge fairly rapidly for moderate or large n. 
14.18. The ordinates and distribution function of the correlation coefficient are not 
expressible in terms of simple mathematical functions. They have, however, been tabulated 
by David (1938) for values of n = 2(1)26, 60, 100, 200 and 400 ; for p = 0-0(0-1)0-9 ; and 
for r = — 1-00(0-06) -j- 1-00, with finer intervals in places. 
For many practical purposes it is sufficient to use a transformation of the distribution 
due to Fisher (1921). Putting ^ 
1 + r 
r = tanh z, z ■= \ log - 
1 — r 
p = tanh £, £ = \ log —~^- 
l — pj 
. (14.67) 
we may expand the frequency function of r in powers of z — £, = x say, and inverse powers 
of n. Fisher gives the following expansion: 
f _ »:=_? a-tWi + Ux + I 2+P 4- 4~P2^ + nSll A 
4 + 12p2 + 9p* 8 - 2p» + 3p4 
+ pX\lQ(n-l) + ~^T^ + 24 X ) + { 128(^-1)2 + 
za 
64(w - 1) 
+ ^^v+!i_^^_1)) + <^,v+ _j . _(1468) 
Taking moments about x = 0 we find, on transferring to the mean, 
ft " al^T)!1 + sjr-'ll + ■ • ) <14-59> 
ft = ^TJ1 + sfi-tTT) + 48(» - 1). +••]• • • <14-6°) 
ft " V^lf (14'61) 
„ _ 1 U I 224 ~ 48p2 ~ 3p< I U72 ~ 228p" ~ 141p' ~ 3p° I 1 (14 62) 
^ - (5^ir4 + ~iel^n-)— + 32(» - i)« + • • 7 • (14-62) 
The remarkable thing about the transformation is that the distribution of r, which is very 
skew, becomes the distribution of z — £, which is nearly symmetrical. In fact, 
Yl = (n- l)i{p2 ~ A) + (14'63) 
^ = S^T)+ <14"64> 
346 PRODUCT-MOMENT CORRELATION 
Thus we may take z - £ tp be approximately normaUy distributed with mean and 
variance given by (14.59) and (14.60). As a slightly rougher approximation we may take 
*<-"«-5(irh] ■. • • • •(U-05) 
1 1 • 4 — p2 
var (z — £) = + -^t rr« 
x n — 1 2{n — l)-5 
which is approximately equal for small p to 
1 + 2 
n - 1 (?i - l)a 
1 
= approximately ..... (14.60) 
n — 3 
When ?i is moderate we may take a still rougher approximation by assuming z — £ to be 
normally distributed about zero mean with variance ——--. Some comparisons of the 
various approximations are given in the introduction to David's tables, and it appears 
that for n > 50 the forms (14.65) and (14.66) are adequate. The approximation given 
by (14.59) and (14.60) appears to hold satisfactorily for values of n as low as 11. 
14.19. Except in the case of the normal parent very little exact knowledge is available 
about the sampling distribution of the correlation coefficient. There is, however, some 
empirical evidence to justify the use of the above results when the population does not 
differ very much from the normal. IE. S. Pearson (1931), in ^dealing with some 
experimental results, concluded that " the results suggest that the normal bivariato surface can be 
mutilated and distorted to a remarkable degree without affecting the frequency 
distribution o£r." The subjeot does not seem to have been investigated mathematically except 
in special cases. ' 
Example 14.6 
In Example 14.3 we obtained for the correlation coefficient between wheat and potatoen 
a value of 0-2182. Suppose we'regard the 48 counties as giving a random sample of the 
yields of wheat and potatoes, either for a wider area or for an extended period of yearn. 
The question then is, can such a value have arisen by chance from a population in which 
the yields of wheat and potatoes are uncorrelated ? 
' From prior knowledge of crop yields we can assume with some confidence approximate 
normality in the parent population. Let us then test the hypothesis that the correlation 
in this population is zero. 
We have 
11 1 +r 11 1-2189 nnn„ 
z = * lo& fZ7 = * lo& o^78n = °'2225 
' £ = 0 
= 0-1491. 
V(n - 3) V(45) 
The deviation z — £ is thus 0-2225, or about 1-49 times the standard error. This is not 
very improbable and the observed correlation may thus be accidental. 
DISTRIBUTION OP REGRESSION COEFFICIENTS IN NORMAL SAMPLES 347 
Example 14.7 
In a sample of 50 a correlation "coefficient is found to be -j- 0-5. What is the 
probability that a value equal to or less than this should hava been obtained from a normal 
population in whioh the correlation is + 0-7 ? 
The exact value, from David's table, is, to five decimal plaoes, 0-01289. Let us first 
of all take the approximation which assumes z — £ to be distributed about zero mean with 
variance -. —. We have 
{n - 3) 
z = 1 W LiT = 0-5493 
2 to l —r 
1 +-? = 0-8673 
P 
C = h log f-i-* = 0- 
1 - p 
= 0-1459, z - £ = - 0-3180. 
V(n - 3) 
i 
The deviation is thus 2-18 times the standard error, $nd the required probability, from 
the table of the normal integral, 0-0146 approximately, compared with the true value 
of 0-0129. The approximate test is not quite stringent enough. 
Let us then take z — £ to be distributed normally about mean ——-—— = 0-00714. 
The deviation is then — 0-3251, or 2-23 times the standard error, giving a probabiKty of 
about 0-0129, almost the exact value. 
Example 14.8 
In a sample of nx there is observed a correlation of rx and in a second sample of w2 
a correlation of r2. Are the sample values rt and r2 compatible with the hypothesis that 
the samples arose from the same population ? 
Suppose the hypothesis were true, and that p is the correlation coefficient in the 
population. Then if zx =vtanh~1 ru 22 = tanh"1 r2, £ = tanh-1 p, we know that if the population 
were normal, zx — £ will be distributed approximately with variance — and 2a — £ 
srence zx — za = (zi — £) — (z2 
approximately normally with variance 
with variance -• Thus the difference zx — za = (zi — £) — (22 — £) *s distributed 
7bx — o 
x-.+ 
ni— 3 na — 3' 
and this will provide a test of the hypothesis. 
Distribution of Regression Coefficients in Normal Samples 
14.20. Turning again to equation (14.44) we have, substituting 6a = —, the joint 
Si 
frequenoy-distribution of su s2 and bt 
.dF«°**[-w^)$r2£:+§]3>n-is*"-ii-Wd3M (i4-67) 
348 PRODUCT-MOMENT CORRELATION 
Integration with respect to sa gives for the distribution of at and b, 
L 2(1~P)K °'i0'a ^JJ 
A further integration with respect to 8X gives for the distribution of 6a 
Ca 
tZ6s 
or, on evaluation of the constant, 
^ = /« _ i\ T^a 7 D(r,\2l» • • (14-68) 
v^(v)<-2 S(i-^+(6'-?)r 
The distribution of the regression coefficient 6X is obtainable by interchanging the suffixes 
1 and 2. 
The form (14.68) is a Pearson Type VII distribution, symmetrical about the point 
bs = —, the population regression coefficient. It tends to normality fairly rapidly, and 
the use of the standard error for regressions is therefore valid for lower values of n than 
in the case of the correlation coefficient. For small samples, however, (14.68) is not. of 
much use since it depends on-the unknown quantities au cra andp, i.e. the population variances 
and covariance. 
14.21. It is possible to find statistics other than bx and 62 which will provide a tost 
of the regressions. Write 
« = /(«'~m'1£- •..".. (14.69) 
We now return to the distribution of the quantities a, b, c of equation (14.41), namely, ■ 
n—4 
dF oc exp [— a + 2pb — c] (ac — b2) * dadbdc . . . (14.70) 
We have from (14.69) 
b — pa y 
u = c 
V(ac - 62)' 
and on substituting for c in (14.70) we have, after a little reduction, 
exp 
dF oc — 
\-all -h^Xldadu 
The integral of the second part on the right for 6 will be found to give a factor proportional to 
W + l) ( u* \-i 
U 
and hence for the distribution of a and u we find 
n-l 
dF oc a 2 exP(~a + ptyda du 
a ' n-i 
(1 + U*)~ 
\ 
DISTRIBUTION OF REGRESSION COEFFICIENTS IN NORMAL SAMPLES 349 
Hence the distributions of a and u are independent, and for that of u we have 
dF oc du n_v . ~ . . . (14.71) 
(1 +u2)~T- 
This distribution does not contain any of the parent parameters. If we put 
{4 - bis*)* 
(14.72) 
then t is distributed in " Student's " form 
dF oc %——— (14.73) 
and may be tested accordingly. 
Example 14.9 
In Example 14.3 we found for the regression of 7 (potato yield) on X (wheat yield) 
(7 - 6-065) = 0-0783 (X - 15-791). 
The regression coefficient is small. Could it have arisen from a population in which there 
is no correlation, i.e. in whioh /?a = 0 ? 
From Example 14.3 we have 
fc2 = 0-0783 V(« - 2) = 6-7823, s\ = 4-1749, ^ = 0-5340. 
Hence from (14.72) 
= W_(^_2) = 2.06 
V(4 - 4bi) 
Appendix Table 3 does not carry us as far as v = 46. From the Fislier-Yates tables, 
however, we have the following values of t for P = 0-05 : 
v = 40 t = 2021 ; v = 60 * = 2-000, 
and for P = 0-02 ; 
v = 40 t = 2;423 ; v = 60 t = 2-390. 
Thus in our case P evidently lies between 005 and 0-02, and tho regression may not be 
significant, i.e. the two variates may be independent. This confirms the conclusion 
reached in Example 14.6 from consideration of the correlation coefficient. 
14.22. Up to this point we have considered the correlation coefficient mainly as a 
measure of the relationship between two variates, and this is the standpoint which will 
mainly concern us in this and the succeeding chapter. We may, however, turn for a time 
to a consideration of the regression equations, which have an importance of their own. 
Assuming that the regression is approximately linear, we have two equations 
X-x = pi(Y-y)\ 
Y -y=^(Y -x)j [ ,iV 
expressing the relations between the means of variate arrays and the variate-values 
determining those arrays. A problem which frequently presents itself in practice is the following : 
given a member of the population exhibiting a variate-value x, what is its y-value ? Evidently 
there is in general no unique answer to this question. For any given x there will be an 
350 • PRODUCT-MOMENT CORRELATION 
array of y% any one of which might be exhibited by the member under consideration. 
But in the absence of any special knowledge it is reasonable to take as the best estimate 
of y the mean of this array. If the population is normal the mean will be the modal value, 
and if it is approximately normal the mean will be a reasonable estimate, the greater .part 
of the population values lying distributed within a range of two or three times the standard 
deviation of the array. 
In fact, the question as put is too restrictive.. There is no unique value of y 
corresponding to a given x> and we are entitled to enquire only after the distribution of y's or 
their principal characteristics. 
Now the mean required is given by the regression equation, and hence that equation 
may be used to estimate, the y-value corresponding to a given x. If at the same time the 
variance of the y-array. can be determined, the probable limits of error of the estimate 
may also be assigned. This is particularly easy for normal populations because, as we 
have seen (14.8), the variance of alls-arrays is "crf( 1 — p2) and that of the ^-arrays o|(l — pa). 
As usual in large samples, we can use the sample values to calculate these variances ; or 
we may take the variance of the array direct from observation. 
Example 14.10 
In Example 14.1 we found for the regression equations, in the units there employed, 
X - 0-7706 = - 1-417 (Y + 0-2095) 
7 + 0-2095 = - 0-2658(Z - 0-7706). 
Suppose we require to estimate the highest audible pitch for a man 34 years of age. In 
our units this corresponds to an tc-value of £(34 — 22) = 4. Our estimate of y is then 
- 0-2095 - 0-2658(4 - 0-7706) 
= — 1-0679 units. 
This corresponds, in vibrations per second, to 
19,995 - (1-0679) X 2000 
= 17,900 vibrations. 
The variance of the estimate is s\(\ — r2) 
= 13-3482{1 - (0-6136)2} thousands2 
= 8-322 thousands2, 
so that the standard error is V8-322 = 2-9 units = 5-8 thousand vibrations. The estimate 
is evidently not very accurate, for the value of y can vary within two or three times this 
range without very great improbability. 
If the problem had been set in the reverse form: what is the age corresponding to 
a vibration of 17-9 thousands, we should have 
X = 0-7706 - 1-41'7(- 1-0679 + 0-2095) 
= 1-99 units 
= 27-98 years. • 
This is not very close to 34 years, the age from which we started ; and in general if f is 
the estimate of x, given y = n, n is not the estimate of y, given x = f. We have a right to 
expect such a concordance only when r is near unity or when £ and n are near the means 
of the distribution, where the regression lines intersect. 
THE CORRELATION RATIOS 351 
The Correlation Ratios 
14.23. For any bivariate distribution we have, if xp is the mean of the pth z-array 
and x the mean of the whole, - ' 
S(x - x)2 = Z(x - xp + xp - x)2 
= Z(x - xvY -h Z(Xp - x)\ . . . (14.75) 
the product term 2Z(x — xp)(xp — x) vanishing because constant for any given 
array. 
The correlation ratio of x on y, r}^, is defined by 
i ^-%^w '• • • • •(14'76> 
and similarly that of y on x, r}yx, by 
nvz Sty -y)2 { ' 
Analogously to (14.75) we have 
£{x - M* = E{x -xp+xp- p\y)2 
■ = E{x - xp)2 + E{xp - M*; . . . (14.78) 
But, from (14.11), 
2(x - £#)■ = (1 - P W* *)a> ■ 
and from "(14.76), 
(1*- rj^^x - z)* = Z(x - xp)\ 
Taking these results in conjunction with (14.78) we find 
£(x - x)2{rj2xy ~ P2) = Z(2p ~ M*- 
Henoe rj oannot be less than p. If and only if ?; = p, xp — flty vanishes for each 
array, i.e. the regression is linear, r\2 — r2 may thus be used as an index of linearity of 
regression. 
Example 14.11 
The calculation of the correlation ratios is based on equation (14.76). As an illustration 
we will find those for the data of Table 14.1. The means of the horizontal arrays and the 
array frequencies are shown in Table 14.5. 
Calculation of the 
Highoat 
Audible Pitch 
5- 
7- 
9- 
11- 
13- 
15- 
17- 
19- 
21- 
■ 23- , 
25- 
27- 
29- 
31- 
33- 
TABLE 
Correlation Ratio 
Frequency Mean £p 
3 
45 
10 
104 
93 
310 
576 
1051 
957 
165 
41 
16 
2 
2 
4 
4-606,667 
9-111,111 
9-700,000 
8-817,308 
6-333,333 
3-022,581 
1-064,236 
0-101,808 
'- 0-801,463 
- 1-278,788 
- 1-512,195 
- 1-562,500 
- 1-000,000 
- 3-000,000 
- 1-750,000 
14.5 
Vxu for Me Data 
*&n ~~ •** 
3-896,025 
8-340,469 
8-929,358 
8046,666 
5-562,691 
2-251,939 
0-293,594 
- 0-668,834 
- 1-572,105. 
- 2-049,430 
- 2-282,837 
- 2-333,142 
- 1-770,642 
- 3-770,642 
- 2-520,642 
of Table 14.1. 
■(xl7 - x)2 
15179,011 
69-563,423 
79-733,434 
64-748,834 
30-943,531 
5-071,229 
0-086,197 
0-447,339 
2-471,514 
4-200,163 
5-211,345 
5-443,552 
3-135,173 
14-217,741 
6-353,636 
352 PRODUCT-MOMENT CORRELATION 
We have , already found that 
Z{x*) = 47,392 Z{x) = 2604, 
from which 
£(x - x)* = £(x*) - ± (Z(x))* 
= 45,385-25. 
From the table we now have 
Z(xp -x)z = 19,095-88. 
It should be noticed that in forming this sum we multiply each (xp — x)2 in the last column 
of Table 14.5 by the corresponding frequency in the second column, for the summation 
takes place over all values of a;. 
We then find 
_ 19,095-88 __ 
Vxv~ 45,385-25 ° 4^0>751> \ 
giving 7]xy = 0-6487. Similarly it may be shown that r\vx =■ 0-6231. The correlation 
coefficient is — 0*6136. 
We have 
rj*^ - r2 = 0-044 
V\x -r* = 0-012. 
These values are close to zero and the regressions are thus approximately linear. 
14.24. We shall see in the next chapter that rjz is closely related to a statistic JR, the 
multiple correlation coefficient, which is of rather greater importance. We accordingly 
defer a full discussion of the sampling distribution of rj2 until that chapter, but will here 
derive it in the special case of samples from an uncorrelated bivariate population. 
From (14.75) and (14.76) we have 
Now if the population is normal and the arrays are of narrow width, the distribution 
in each array will be normal. We have already seen that in a normal distribution the 
mean is distributed independently of the variance. Hence Z{x — xp)2, which is the sum 
of numerical multiples of array variances, is independent of the array means and hence of 
the quantity E(xp — x)2. Thus the numerator and denominator of (14.79) are independent. 
Further, if the variates are uncorrelated and therefore (in the normal case) independent, 
the distributions in parent arrays have all the same mean and variance, those of the total 
distribution. Without loss of generality we may take the mean to be zero and the variance 
to be unity. 
It was seen in Example 10.5 that the sum of squares of a variates, each distributed 
normally with zero mean and unit variance, is given by 
f dF oc e-* t**-2) dt (14.80) 
and that the distribution of sum of squares about the mean is the same in form but has the 
index of 2 reduced by unity. Now E(x — xp)2 summed over any given array containing 
Np members is the sum of squares about the mean of Np variates and is thus distributed 
in the form (14.80) with Np — 1 degrees of freedom, that is to say with a = Np — 1. 
THE CORRELATION RATIOS 353 
Thus .the sum of {x — xp)2 for the whole array will be- distributed in the form (14.80) with 
2-i (NP — l)=N—p degrees of freedom, i.e. as 
p 
dF ace-*W-P-Vdt (14.81) 
The mean xp will be distributed in the normal form 
dF xe-Wp^dXp 
and consequently^^ — x)2, which is equal to 2~iNP@p ~ #)2 (tne summation now ex- 
■" p 
tending over the p arrays), will be distributed in the form (14.80) with p — 1 degrees of 
freedom; i.e., writing u for the sum, as 
dF oce-^u^-^du. .... (14.82) 
1 — ri2 t 
To find the distribution of r— we then have to find that of -, t and u being inde- 
r]* u ° 
pendent. 
We have for the joint distribution 
dF oc exp [- \{t + u)] *Ktf-p-*> wto-3 dtdu . . . (14.83) 
Put £ = - £ = t + u. 
u «, 
The Jacobian of the 'transformation is 
d(£, Q = t + u 
d(t,u) u2 ■ 
and (14.83) becomes 
Thus £ and £ are independent and we have for the distribution of £ 
ti(N-p-2) 
1 — ?7a 
whence, on putting £ = i— we ^nc^ 
dF oc (1 - rj*pN-p-*) (7]2) 2 d(rj2) 
1 (1 - n*)*N-"'-V (t78)*<p-3> dfr*) . . (14.85) 
which is the distribution required. 
14.25. The distribution function of (14.85), which is a Pearson Type I curve, may 
bo obtained from the incomplete U-function. It is sufficient for ordinary purposes, however, 
to use the tabulated forms of Fisher's z-distribution (Example 10.18). In fact, putting 
in (14.85) 
vl = p - 1 
Vi = -V - p 
e2, = *?* N ~ P 
A.S. 
1 — r\% p -*■ 1 
AA 
354 
PRODUCT-MOMENT CORRELATION 
we find 
dF oc 
erv 
,ViB 
(vie2s + v%)^+r^ 
the form of equation (10.62). Appendix Tables 4 and 5 give the values of z, such that equal 
or greater values will be attained with probability 0-05 and 0-01. These tables are due 
to Fisher and reproduced from his Statistical Methods for Research Workers. In practice, 
however, ^is only calculated for large-values of N outside the range of these tablos, and 
we may either use the approximation suggested therein or special Tables by T. L. Woo 
reproduced in Tables for Statisticians and Biometricians, Part II. 
14.26. It is easy to show that the first two.moments of (14.85) and the constants 
yx and ya are given by 
. _P-I -.....' 
N-l 
2{p~l)(N-p) 
(N - l)2(iV + l) 
2 _ 16(tf - 2p + 1)*(N + 1) 
Y\ — 
Yz = 
{p - l)(N - p)(N + 3)2 * : 
6(N + 1){2(N - 1)2.+ (p + 1)(N - p)(N - 13)} 
Thus, to order N 
-l 
(p--;ipr_j,)(tf+ 3)(tf+ 5) 
16 
(14.86) 
(14.87) 
(14.88) 
(14.89) 
n 
Y* 
p-\ 
12 
p~=\) 
(14.90) 
and thus rf does not tend to normality for large N for any finite number of arrays p. 
Tetrachoric r 
14.27. We now proceed to consider two coefficients designed for the measurement 
of dependence and based on the product-moment correlation coefficient, tetrachdrio r and 
biserial rj. Both those coefficients are, in effect, estimates of a putative product-moment 
correlation for data which are not specified with the detail of an ordinary bivariate table. 
Suppose we have a fourfold table 
a 
c 
a + c 
b 
, d 
b +d . 
a + b 
o + d 
N 
(14,91) 
If this table is derived by a double dichotomy of a bivariate frequency-distribution 
z0exp 
_ 1 tx\ 
. . 2(1 - P2)U 
2pxy y«N 
TETRACHORIC r 
355 
we may ask, what is the value of p in terms of a, b, c, d and N ? This problem is, in fact, 
determinate. 
If the population is normal the array totals will be normal, and thus the frequencies 
(a + c) and (b + d) correspond to a dichotomy of the normal curve, i.e. there exists an 
h' such that 
zdx dy = ' 
J —00 J —00 ' 
poo poo 
J h' J -o 
zdx dy 
N 
b +d 
N 
(14.92) 
or 
If* f ,z2l , a +c 
o-1V(27r)J-oc I ff?J- 
2V 
_J_rexpj-ftV=6+^ 
iV ' 
Putting h = — we have 
£exp(-^)^=(6+y^ 
(14.93) 
so that ft can be derived from the tables of the normal integral. 
Similarly there will be a k denned by 
I exP (- W ^y = —^-Jfy—' 
We then require to solve for p the equation 
7/ " MT^JTIIexpI- KL^7°)(X* - %m + yi)ldxdy ■ iUM) 
We will expand the integral on the right in ascending powers of p. The characteristic 
function of the distribution is 
<f,(t, u)= exp {- l(t* + 2Ptu + w2)}. 
Thus 
7 1 ^ oo /* oo ^oo ^ oo 
fl = _L dx\ dy\ <f>(t, uyr*"-*™ dt du 
N 4jraJ,t Jk J_ocJ-« 
= 4^}°° dx\" dy\" J" exp {- ^(«2 + W2} _ itx _ iuy } J^ (-P|W # ^ (u<95) 
The coefficient of (— p); is the product of two integrals, the first of which is 
If00 f°° 
— dx\ exp (- ^2 - ite)^* 
and the second a similar expression in k, y and u. Now from 6.24 the integral with respeot 
to t is equal to 
(-i)> #,(*), l 
>-!*■ 
V2ot 
L' 
356 
PKODUCT-MOMENT COBRELATION 
and hence the double integral is 
[ 
<-i>"<"',-.<»>i'-"T- 
Hence, from (14.95), 
'f-^E^.Bs-M.^'-*'- 
In the notation of 6.27 we write r for the tetrachorio function of A and t' for that of Jc, 
and we then have 
i-fyw 
. (14.00; 
/=o 
The tetrachoric functions have been tabled up to t1d (Tables for Statisticians and Bio- 
metricians, Parts I and II) and, with their aid, (14.96) can be solved by successive 
approximation. Examples will be found in the introduction to the Tables. 
14.28. It is to be realised that the coefficient obtained by the solution of 
equation (14.96) is not a product-moment correlation, but an estimate of the parameter 
p in a bivariate normal population. It is not an estimate of the product-moment correlation 
in non-normal populations. Its practical, use is limited largely by arithmetical 
inconvenience, both in the solution" of (14.96) and in the determination of sampling variances. 
Karl Pearson (1913) has given expressions for these quantities, but as nothing is known of 
the distribution of tetrachoric r it is not clear how far the use of a standard error is justifiable. 
Biserial r\ 
14.29. Suppose now that we have a 2 x q-fold table, the dichotomy being according 
to some qualitative factor and the other classification either to a numerical variate or to 
some variate permitting the arrangement of the classes in order. 
Table 14.6 will illustrate the type of material under discussion. The data relate to 
TABLE 14.6 
Showing 1426 Criminals, classified according to Alcoholism and Type of Crime. 
(C. Goring's data, quoted by K. Pearson, 1909.) 
Alcoholic 
Non-alcoholic 
Totals 
Arson. 
50 
43 
03 
Rape. 
88 
02 
150 
Violence. 
155^ 
110 
265 
Stealing. 
379 
300 
679 
Coining. 
18 
14 
32 
Fraud. 
(53 
144 
207 
Totals. 
703 
(J73 
142G 
1426 criminals classified according to whether they were alcoholic or not and according to 
the crime for which they were imprisoned. The order of the crime-classification is deter- 
BISERIAL rj 357 
mined by its relationship with intelligence, arson being associated with low intelligence and 
fraud with high. 
If the population is normal, rj = p. We have 
t _2Np(yp-yY 
N var y 
*? yx 
_I(Npypz-2Npypy + Npy^) 
N var y 
= z(fp tip*) -Jl_ (U.97) 
\N var y) var y 
Since l(M*\. » j»,fc—' 
\N var y) N var y v p var 
Thus 
var y 
,. lj£d&t,.»«J!R__£_ . . . .(U.98) 
N var 2/p var y var 2/ 
But the mean variance of arrays, weighted according to the numbers in arrays, 
= var ?/(l — p2) = var y{\ — ?/2). Taking this as equal to var yp we have 
rj* = (1 -rj2)Z^ NpVp* ^ $ 
giving 
N var yp) var y 
1 r(xPyA_ 
_ N \var yj 
y2 
V2 = iV \var^/ vary (U 99) 
1 r(Npyp*\ 
N \var yj 
1 + 
The use of this expression lies in the fact that the quantities in it can be estimated, 
from the data on certain assumptions. If we suppose that the quantity according to which 
dichotomy has been made (in our example, alcoholism) is capable of representation by 
a variate which is normally distributed, and thus that each y-array is a dichotomy of 
a normal curve, the quantities —~— and -r^-— can be obtained from the tables of 
1 v"var y V^ar yp 
the normal integral. For example, the two frequencies alcoholic and non-alcoholic are, 
50 
for arson, 50 and 43. Thus the proportional frequency in the alcoholic group is '—- = 0-5376 
DO 
and the deviation corresponding to this frequency is seen from the tables to be 0-09461 
y 
which is thus . p— for this ?y-array. 
v™*yP 
Example T4.12 
9P 
JUUI lillO UttLO; LU. -LtlLHO 1.1 
the Np are as follows :— 
Arson. 
ALooholio . , . 0-5376 
VVvars/, . . 0-0944 
Np . 93 
t.U U1IC |J1' 
Rape. 
05867 
0-2190 
150 
ujjui unmcii 
Violence. 
0-5849 
0-2144 
265 
j.± ov.jucinj.it 
Stealing. 
0-5582 
01463 
679 
3B, U11L. VOl 
Coining. 
0-5625 
0-1573 
32 
HOB «JJL . 
yvar 
Fraud. 
0-3043 
- 0-5119 
207 
yP 
Total. 
0-5281 
0-0704 
1426 
t 
358 PRODUCT-MOMENT CORRELATION 
Then from (14.99) we have 
_i- {93(0-0944)* +...}- (0-0704)' 
^ = L : 
1 +^26(93(0-0944)2+ . . . } 
giving rj2 = 0-05456 
, , 7] = 0-234, 
which, on our various assumptions, may be taken as approximating to the supposed product- 
moment correlation coefficient. 
As for tetrachoric r, the sampling distribution of biserial r\ is unknown. . Expressions 
for its sampling variance have been derived by K. Pearson (1915), but are to be used with 
considerable reserve. 
14.30. Something may also be said about the assumptions on which tetrachoric r and 
biserial r\ are based, particularly that of normality. In supposing that a given fourfold 
table is the double dichotomy of a normal population, we are assuming that the attributes 
or variates concerned are capable of representation on a normal scale and that it was, in 
fact, this scale which .determined the classification given. This assumption is evidently 
a considerable one and cannot always be made with much confidence. In dividing criminals 
into alcoholic and non-alcoholic it would, for example, be assumed that " alcoholism " 
is a quantity which varies continuously from one subject to another; or perhaps that 
propensity to alcoholism was such a yariate. At one end of the scale we should have 
chronic inebriety, at the other the most austere teetotalism. It would be further assumed 
that if the degree of alcoholism could be measured, the population of criminals would be 
distributed according to the alcoholic variate in a normal form; and it would be further 
assumed that the data which are given would have been arrived at by a dichotomy of the 
population according tothe variate. How far assumptions of this kind are justified depends 
on previous knowledge and the circumstances of individual cases ; but even so it remains 
largely a matter of personal opinion. The reader will meet widely divergent views in the 
literature of the subject. 
1 t 
Intra-cla88 Correlation 
14.31. There sometimes arise, mainly in biological work, cases in which we require 
the correlation between members of one or more families. We might, for example, wish 
to examine the correlation between heights of brothers. The question then arises, which 
is the first variate and which the second ? In the simplest case we might have a number 
of families each containing two brothers. Our correlation table has two variates, both 
height, but in order to complete it we must decide which brother is to be related to which 
variate. One way of doing so would be to take the elder brother first, or the taller brother ; 
but this would provide the answer to different questions, the correlation between elder and 
younger brothers, or between taller and shorter brothers ; not the correlation between 
brothers in general. ' 
The problem is met by entering in the correlation table both possible pairs, i.e. those 
obtained by taking both brothers first. Generally, if the family contains h members, there 
will be k(k — 1) entries, each member being taken first in association with each other 
/ 
INTRA-CLASS CORRELATION 
359 
member second. If there -are p families with k1} ka . . . kp members there will be 
p 
ykifti — 1) entries in the correlation table. As a simple illustration consider five families 
of three brothers with heights 
1st family 
2nd family 
3rd family 
4th family 
5th family 
69, 70, 72 inches 
70, 71, 72, „ 
71, 72, 72 „ 
68, 70, 70 „ 
71, 72, 73 „ 
There will be 30 entries in the table, which will be as follows :— 
Height (inches). 
1 
1 
a. 
68 
69 
70 
71 
72 
73 
Totals 
68 
— 
— 
2 
— 
— 
— 
2 
69 
— 
— 
1 
— 
> 1 
— 
2 
70 
2 
1 
2 
1 
2 
8 
71 
^^~ 
1 
4 
1 
6 
72 
— 
1 
2 
4 
2 
1 
10 
73 
^^~ 
1 
1 
2 
Totals. 
2 
2 
S 
6 
10 
.2 
30 
Here, for example, the pair 69, 70 in the first family is entered as (69, 70) and (70, 69) 
and the pair 72, 72 in the third family twice as (72, 72). 
The table is symmetrical about the diagonal, as it evidently must be. We may calculate 
the product-moment coefficient in the usual way. We find var x = var y = 1-716, 
, T 0-516 
cov (xy) = 0-516 and hence p = = 0-301. 
The actual compilation of such a table is, however, both tedious and unnecessary. 
The coefficient p can be found by direct methods', as follows :— 
Suppose there are p families, with variate-values xlx . . . a;lfci, xn . . . x^, . . . 
xpl . . . xpJef, the families numbering ku h2 . . . kp. In the correlation table each member 
• of the ith family will appear kt — 1 times (in association with the other members of the 
family), and thus the mean of each variate is given by 
x=y=^E{{ki-\)E'{xl})}, 
(14.100) 
360 PRODUCT-MOMENT CORRELATION 
the first summation taking place over the p families and the second over all members of 
the ith family. Similarly 
var x = var y = !*{(*, - l)Er(xtj - z)2} . • • (14.101) 
and cov (xy) = ^7E E (xtj - x)(xu - x), . j^l, . r • (14.102) 
& < i.i 
the summation Z extending over all possible pairs for which j ^ I. Thus the coefficient p is 
l.i t 
given by 
i 1,1 
p 
S^-^E'ixii-xY 
i 1 
(14.103) 
This can be thrown into a rather more convenient form. We have 
E E (xiS - x)(x{l -x)=E E' {xi} - x){xu - x) - E E (% - x)2 
i 1,1 i 1>l i 1 
(where the summation iT now extends over all possible pairs, including j — I) 
n 
= E h^xt-x^-EE (xtj-x)\ . . .(14.104) 
i i 1 
x{ being the mean of the ith. family. 
Thus 
-2*f& -x)* - E E{xi} - a=)a 
p=± —±JL (14.10/5) 
p Efa - \) E {Xii - x)* 
i 1 
If all the families have the same number of members this formula is somewhat simplified. 
Denoting by v the variance of x, and by vm the variance of means of families (about tho 
mean x), we have 
_pkh)m —phv 
P (k — \)pkv i ' 
- (F^T)(v -1) • • • • <«•"'«> 
The coefficient p is called the Intra-class Correlation Coefficient, to distinguish it from 
the ordinary product-moment coefficient. 
Example 14.13 
Let us use formula (14.106) to find the intra-class coefficient for the example of the 
above section. With a working mean at 70 inches, the values of the variates are 
- 1, 0, 2 ; 0, 1, 2 ; 1, 2, 2 ; - 2, 0, 0 ; 1, 2, 3. . 
Hence x = 1|. ^ = i{(- l)2 + 0* +...}= *Z 
, , 386 
var (a;) = —. 
1 ; 225 
The means of families are 
B_ 15 25 — 10 30 
15' 15' 15' "IF' 15' 
4 
. INTRA-CLASS CORRELATION 361 
and the deviations from a 
— 8 2 12 — 23 17 
15 ' 15' 15' 15 ' 15' 
Thus ,m =^{g)2 + etc. 
1030 
1125' 
Hence, from (14.106) 
— 1(3.1030.225 _ A 
~ 2{ 1125.386" J 
= 0-301, 
a result we have already found directly. 
14.32. One caution is necessary in the interpretation of the intra-class correlation 
coefficient. Prom (14.106) it is seen that intra-class p cannot be less than r - though 
it may attain ^- 1. It is thus a skew coefficient in the sense that, unlike product-moment 
correlation and association, a negative value has not the same significance (as a departure 
from independence) as the equivalent positive value. 
14.33. The sampling distribution of intra-class p for the case of a normal population 
and equal numbers in families may be obtained as follows :— 
It may be shown, precisely as in (14.25), that the ratio of two sums of squares about 
means, £ = —, based on N — p and p — 1 sums, is distributed as 
tl(N-p-2) Jt 
<^ */---.,, w,v_V,> (14.107) 
(>+fri? 
provided that the sums are independent and emanate from normal populations. Here 
of, a\ are the population variances relating to v^, vt respectively. 
Consider now p families of k members, pic in all, as p samples of k from a normal 
population in which the intra-class coefficient is A. Writing I for the sample intra-class coefficient 
we have 
■ - frrlxTi -1)- ■ ■ ■ ■ <14-108> 
where £ = -- -. Now vm relates to means of samples and is distributed independently 
m 
of v — vm, as in the case of (14.25). We may therefore substitute for f in (14.107), with 
N = pk and p = p. Furthermore, since the population value of v — vm is a'i and that of 
vm is 7—^i we have 
m k — 1 
3 _ 1_ f ha\ _ - 
k - I \[k - iRf + o\ 
°* °* (14.109) 
(k - l) o\ + o\ 
m 
362 
PRODUCT-MOMENT CORRELATION 
After a little reduction (14.107) becomes 
pCfc-D-2 % Pzl „ 
dJf oc ^rj— 
{1 -A+A(ifc-l)(l -*)}" 
. (14.110) 
As for the product-moment coefficient, this form may be brought closer to normality by 
putting 
I = tanh z, X = tanh £. 
In the particular case &, = 2 we find 
dF oc 
e-^sech*3-^ dz> 
coshP-*(z - C) 
which has the remarkable property of depending only on z — C, i.e. of being the same in 
form for any C or A. Writing z — f = a; we may derive from (14.111) the expansion 
_ n* - \) 
n-1 
y s : " ., -€~ 2*1 + 
*• P(n - l)V(2w) I 12 
■[■ 
% -1 
as* 
giving 
\n - igs _ (» - 
[ 45 
0i =- — 
-i)a8nr, , «2 
isr^/JL1" ¥ " t- 
48 + 384 
T-, 1 + 
j_ 
/«2 
2(n - 1)1 ' 2(n - 1) ' 
1 '1+1.+ : 
• r • 
n - 1{ ' 2(» — 1) ' 6(» - l)a 
1 
i"s = — 
+ 
i"4 
(n - l)3 
1 
+ . . . 
(n - 1)2| 
3 + 
5 
+ 
19 
.1. (14.112) 
(14.113) 
(14.114) 
(14.115) 
(14.110) 
(14.117) 
n - 1 ' (» - l)a ". ■ ' ' ' (14-U«) 
illustrating the tendency to normality. z - f may be taken to be distributed normally 
about zero mean with variance approximately. 
For the general case the substitution 
2(k - 1)1 = h - 2 + h tanh (z - 6)\ 
2(k~l)X = k~2 + &tanh(C-0)f * " ' -(14.119) 
where tanh 0 = —_ reduces (14.110) to 
whence 
yi = 
» — 1 4(n — l)2 
1 
[n - If 
rf^v 
exP(-<^±i(,_0)l 
2Tr{(i-i)j)-2}r[!L:^ * L 2 
# -1 
X sech 
where, as usual, x = z — f. 
2 
-(a; — 6)dx . 
. (14.120) 
NOTES AND REFERENCES . 363 
NOTES AND REFERENCES 
The classical theory of product-moment correlation, beginning with Galton and Karl 
Pearson, was established by Yule (1897, 1907). The sampling problem for the normal case 
was solved by Fisher (1915) and studied by subsequent writers, culminating in Miss David's 
tables of 1938. For experimental work on the sampling distribution see E. S. Pearson 
(1931). A method of deriving the distribution alternative to the geometrical approach and 
relying on characteristic functions has been given by Kullback (1935). 
Tetrachoric p and biserial r\ are both inventions of Karl Pearson's, but the tetrachoric 
series has been discovered by many writers, priority apparently being due to Mehler (1876). 
For controversy on the nature and scope of tetrachoric p see references in previous chapter. 
Intra-class correlation i£ formally equivalent to a linear function of the ratio of two 
variances and thus becomes a branch of quadratic analysis (analysis of variance) which will 
be dealt with in the second volume. 
Co-operative Study (1917) (H. E. Soper, A. W. Youn&, B. M. Cave, A. Lee and K. Pearson), 
" On the distribution of the correlation coefficient in small samples," Biometrika, 
11, 328. 
David, F. N. (1938), Tables of the Correlation Coefficient, Cambridge University Press. 
Fisher, R. A. (1915), " The frequency distribution of the values of the correlation coefficient 
in samples from an indefinitely large population," Biometrika, 10, 507. 
(1921), " On the probable error of a coefficient of correlation deduced from a small 
sample," Metron, 1, No. 4, 3. 
Kullback, S. (1934), " An application of characteristic functions to the distribution problem 
of statistics," Ann. Math. Statist, 5, 263. 
Mehler, G. (1876), " Reihenentwicklung nach Laplaceschen Functionen hoherer Ordnung," 
J. fttr Math., 66, 161. 
Pearson, K. (1909), "On a new method of determining correlation between a measured 
characteristic A and a character B, etc.," Biometrika, 7, 96. ■ 
(1910), " On a new method of determining correlation when one variable is given by 
alternative and the other by multiple categories," Biom,etrika, 7, 248. 
(1911), " On the correction necessary for the correlation ratio rj," Biometrika, 8, 254, 
and (1923), 14, 412. 
(19 J 3), " On the probable error of a coefficient of correlation as found from a fourfold 
table," Biometrika, 9, 22. 
.(1915), "On the probable error of biserial r)," Biometrika, 11, 292. 
and Pearson, E. S. (1Q22), " On polychoric coefficients of correlation," Biometrika, 
14, 127. 
Pearson, E. S. (1931), " The test of significance for the correlation coefficient," Jour. Amer. 
' Statist. Ass., 26, 128, and (1932) 27, 424. See also Cheshire, L., Oldis, E., and 
Pearson, E. S. (1932), ibid., 27, 121. 
Ritchie-Scott, A. (1918), "The correlation coefficient of a polychoric table," Biometrika, 
12, 93. 
Yule, G. U. (1897), " On the theory of correlation," Jour. Roy. Statist. Soc, 60, 812. 
(1907), "On the theory of correlation for any number of variables treated by a new 
system of notation," Proc. Roy. Soc, A, 79. 182. 
364 PRODUCT-MOMENT CORRELATION 
EXERCISES 
14.1. Show that the data of Table 1.25 have the following constants (x = age> 
y = milk yield): 
mean m = 6-22 years. mean y = 18-G1 gallons. 
V(var x) = 2-21 , „ V(™r y) = 3-37 ' „ 
p = 0-219, ^ = 0-242, ^ = 0-266. 
14.2. Show that for the data of Table 14.2 
p = - 0-014, Vxu = 0-14, rjyx = 0-38. 
14.3. Show that the smaller angle between the regression lines is 
1 — p2 o1ol 
arc tan 
p a\ + of" 
14.4. If a bivariate normal surface is dichotomised at its medians and a is tho 
proportional frequency in the positive compartment of the 2x2 table so generated (i.o. the^ 
compartment including the limits + oo), show that 
p = cos (1 — 2a)7c. 
(Sheppard, Phil. Trans. Roy. Soc, 1808, 192A, 101.) 
14.5. Show that the ordinates of the sampling distribution of the correlation cooflioioiit 
r in samples from a normal parent with correlation p obey the recurrence relation 
2w -*1 n - 1 
where n is the sample number and 
pr V(l ~ P2) V(l - r2) (1 -/>")(! -r") 
(Co-operative Study, 1917.) 
14.6. By the transformation cosh z - Pr = p show that the ordinate of the* 
1 + u 
distribution of r may be expressed as 
v -.n~2 (1 ~/>2)^ (1 - r2)^ I\n-l) 
f l2 a 12.32 qg l2.32.5a ' K3 -J 
\ 2»n - | ^ 22.22 (n - \){n + J) + 22.22.22 (n - \)(n + |)(W"- J) + " ' ' J 
where q = —±_rL, 
2 
14.7. Show that the characteristic function of 
2(1 -p»)af' 2 (l-p2)<W °3 " 2(1 - pt)of 
EXERCISES 365 
in normal samples is 
(1-P2F 
{(l-ftJU -it3) -P*(l + •*.)■}»' 
where tt refers to Qlt and so on. Hence show that the distribution of variances and co- 
variances has the same characteristic function, except for constants, but with the value 
of % reduced by unity. Show that the simultaneous distribution of these quantities is then 
that of equation (14.42) with 6X = na, d2 = npb, 63 = nc. 
(Kullback, 1934.) 
14.8. From the distribution of equation (14.42) show that the distribution of 
v = — /— and r is given by 
,_. vn-*(l - r2)V~ 
dF oc £ --„--t dr dv. 
(1 - 2prv + w8)"-1 
Integrating for r from — 1 to + 1 by putting 
u(X + u) — (1 — u)(X — fi) , , , 2 o 
w(A + ^) + (1 - u)(X - fi) 
show that the distribution of v is 
^ = 2(1 ~ P2H~ _, 2>»-2 f _ _V^_ 
»/^ ~ 1 ft ~ A (1 + w")»-4 (1 +i>2) 
I 2 3 ■ 2 ; 
(This gives the distribution of the variance ratio when the variates are correlated. 
The result is due to S. S. Bose (1935), Sankhyd, 2, 65. The derivation was given by 
Finney, Bi&metrika, 1938, 30, 190.) 
14.9. Show that in samples from a normal bivariate population the variance of 62 is 
given exactly by 
w<6l>-_L-£*<i'-„.> 
and that for the distribution of 62 , 
Vi = « 
■ 
6 
n 
"2 
2\2 f 
Vl = 
n — 5 
14.10. By considering the joint distribution of Si and 6a *n normal samples, show that 
the regression of 6a on sx is linear, but that of Sj, on 6a is not linear and does not tend to 
linearity for large samples. 
14.11. Writing the bivariate frequency function in the form 
so that the jth moment about the origin of the y array for given x is 
/"}(«)=[_ dyyn9Av)> 
366 PRODUCT-MOMENT CORRELATION 
show that 
(where <f> is the characteristic function of the distribution) so that 
Verify that the bivariate normal distribution has linear regressions and is homoscedastio. 
14.12. (Data of E. M. Elderton, quoted by K. Pearson, 1910.) The following table 
shows 811 sons classified according to alcoholism of parent and health of son :— 
Son. 
s 
Pn 
Alcoholic . 
Non-alcoholic 
TOTAIiS 
Healthy. 
122 
328 
450 
Fairly 
healthy. 
9' 
-4 
37 
46 
Delicate. 
24 
71 
95 
Phthisical or 
epileptic. 
8 • 
37 
45 
Died 
young. 
42 
133 
175 
Totals. 
205 
606 
811 
Show that biserial rj = 0-08.9, indicating little correlation between health of son and 
consumption of alcohol by parent. 
14.13. (Data from O. H. Latter, Biometrika, 4, 1905, p. 363.) 
The following table shows the length of cuckoos' eggs fostered by various birds :— 
Length of Egg (units £ millimetre), 
Foster Parent. 
Hedge-Sparrow 
Totals 
40 
1 
7 
8 
41 
1 
6* 
6 
42 
8 
14 
2 
24 
43 
3 
8 
5 
16 
44 
9 
9 
• 
14 
32 
45 
* 
13 
6 
13 
32 
46 
20 
3 
13 
36 
47 
6 
2 
3 
11 
48 
11 
5 
16 
49 
2 
— 
2 
50 
2 
— 
3 
5 
Totals. 
76 
54 
58 
188 
Show that the coefficient of intra-class correlation is + 0-22. 
* 
14.14. A series of measurements are subject to errors of observation which may be 
supposed unoorrelated with the magnitudes of the measurements. If x1} yx refer to the 
EXERCISES 
367 
observed deviations from arithmetic means and x, y to the true deviations, show that 
£{xi!Ji) = £{xy)> but that var xx > var x ; var y^ > var y. Hence show that the observed 
correlation is less than the true correlation. 
14.15. If three variables X1} Xz, X9 are uncorrelated and the deviations are small 
compared with their mean values M1} Ma and M3, show that the variance of -^ is 
approximately 
MIfvax X1 _ 2 cov (I,, X2) var Z,\ 
JffV Ml M^M2 + M\ ) 
X X 
and that the correlation between =i and —s is 
X9 X3 
vl 
VK + «&)(*£ + «8) - 
where v\ = —vt-=^, etc. • 
1 M^ 
Note that this is "positive, so that there is a V spurious " correlation between the two 
indices -=-x and —*. 
X% Xa ^ 
CHAPTER 15 
PARTIAL AND MULTIPLE CORRELATION 
15.1. The product-moment coefficient of correlation can, as has been seen in the 
last chapter, be used to measure the relationship between two variates which are distributed 
either exactly or approximately in the normal form. When we come to interpret such- 
a correlation, however, we meet the same sort of problem whifch arose in Chapter 13 in 
connection with associations : if a variate 1 is correlated with a variate 2, may this not 
,be due to the fact that both are correlated with a variate 3 ? The question may be decided 
by considering the correlation of 1 and 2 in the sub-populations for which variate 3 is. 
constant, and in this chapter we consider the theory of such partial correlations, which 
bear an obvious analogy to the partial associations of Chapter 13. The subject may best 
be broached by extending to several variables the theory of linear regression developed 
for two variables in the previous chapter. 
15.2. Suppose, in fact, that there is given a set of N individuals considered according ' 
to p variates xlt xi} . . . xv, so that to each individual there correspond p variate-values. 
We'may, for example, be given a set of men according to height, weight, age and income, 
or a set of counties according to wheat-yields, hours of sunshine per annum, inches of 
rainfall per annum, and mean height above sea-level. In general, any variate may be 
considered as dependent on the others and for any variate, say xu we may require to find 
the " best " linear relation of the form 
Xx = a + J?**, + 0.x, + . . . {SpXp .... (15.1) 
a generalisation of (14.8). As before, the constants may be determined by the principle 
of least squares, i.e. so that 
U = E{xx - a - foxs — ... - ppXp)* .... (15.2) 
is a minimum, the summation extending over the N members of the population. We 
shall then have 
2 7T 
^- = E{xx — a - p2x2 —•...— 0pxp) = 0, ... . (15.3) 
and if we take the variables measured from their means, this reduces to a = 0. With 
"• BU 
this convention we have (p — 1) equations of type ^r = 0, i.e. 
or 
cov (xk, Xj) — 0a cov (xk, xt) . . . — fik var xk — ... — 0p cov {xkxp) = 0, 
h = 2, 3, ... p. . . (15.4) 
These (p — 1) equations can be solved for the (p — 1) quantities /? and hence the required 
form (1-5.1) is determinate. 
15.3. In the notation introduced by Yule we write 
Xi = £l2.S4..,J>X8 -f 013.24...p X3 -f- . . . + 0lp.23..(p-l)-2p> • (15.5) 
368 
PARTIAL AND MULTIPLE CORRELATION 369 
which is the regression equation of Xt on Xa . . . Xp, referred to the means of the variates. 
The quantities /? are called Partial Regression Coefficients. The first subscript to the lefb 
of the period in each /? is that of the variate on the left of the regression equation, and 
the second subscript is that of the variate to which it is attached. These are called Primary 
Subscripts. The subscripts on the right of the period are those of the remaining variables 
and are called Secondary Subscripts. 
When no confusion is likely to arise we can write (15.5) in the simpler form 
Xx = /32Z2 + . . . + fivXv, ..... (15.6) 
that is to say, we may drop the first primary and the secondary subscripts. 
The order of the primary subscripts is material, ft12,k being different from p^i.fc; but 
that of the secondary subscripts is not. 
Write 
#1.23...p>== xl Pl2.U..pXi~ • • • — Plp.23...(p-1) xp' • • (15.7) 
^1.23...? mav t*1611 ke called the residual of xx of order p. It is the difference between 
the observed xx and the value given by the regression equation. If all the residuals are 
zero, and only in this case, the regression is exactly linear. The /5's were determined so 
as to make the sum of squares of residuals a minimum. 
Write also 
var (#1.23...*,) = 0I23...P (lr>-8) 
so that o,1.23i.ip is the standard deviation of residuals and corresponds to the standard 
deviation of arrays'considered in 14.22. 
15.4. From (15.7), equations (15.4) may be written 
Z{xkxi.n...v) = °> k = 2 . . . p . . . . (15.9) 
and generally we shall have 
£(xkxJ.l2..U-l)U+»...p) = °' j^k, . . . (15.10) 
i.e. the covariance of any residual and any variate is zero, provided that the subscript of 
the latter occurs among the secondary subscripts of the former. 
More generally still, 
^0*1.34...?) x2.U...p) = £{#l.34...7> {%* —' P-23.4...?j x3 ~ • • • — P2p.3i...0)-l)Xp) } 
and each term on the right vanishes in virtue of (15.9) except the first, so that 
£{xi.m...v xz.M...p) = ^(xi.:u...px*) • • • (15.11) 
= £(3l*3.3i..../j) • • • (15.12) 
by symmetry. 
Thus the covariance of any two residuals is unaltered by omitting any or all of the 
secondary subscripts of either which are common to both. Conversely the covariance of 
any residual with p secondary suffixes and a residual with those p secondary suffixes and 
q additional ones is unaltered by adding to the former any of the q of the latter. 
As a corollary, any covariance is zero if all the subscripts of one residual occur among 
the secondary subscripts of the second. 
15.5. In virtue of these results we have 
0 = -2/(a?2.34...p ^1.23...};) 
= Eix2.U...v (#1 — ^12.34...p x* ~ ' • •)} 
=" *<(x2.3t...p xl) — Pl2.3±...p £(x2.3t...p xi) 
= £(x2.3<L...pxl.U...p) ~ A.2.34...P £(x2.3<L...i>)2> 
A.S. B 3 
370 PARTIAL AND MULTIPLE CORRELATION 
and thus, writing q for the group of suffixes 34 . . . p, we have 
_ coy fa.g, sg,q) 
/?12-° ~ var (**.«) (16'13) 
a generalisation of (14.6). 
Similarly 
ftl-»" var^.J (16-U) 
We may then define a coefficient p12.<z = P12.34 ...p hy the equation 
Pl2.ff = (012.fl021.ff)* 
= cov fa.g, sa.g) ^ ,lg x 
{var (z2>ff) var (aj^) }* 4 * 
This is a generalisation of (14.10). p12q is evidently the product-moment coefficient of 
correlation between #1>g and cc2.fl. 
15.6. From p variates we can pick out two in f ^ 1 ways and find the regression of 
each on the other and their correlation ; we can also pick out three in f J ways and find 
the regression of each on the other two ; and so on. The number of possible regressions 
and correlations is thus very large, but they can all be expressed in terms of the variances 
of the variates and the correlations between pairs. 
We shall call the coefficients with k secondary subscripts regressions, correlations, etc., 
of the Jcth order. The correlation between a pair of variates pik is thus of zero order, and 
'our result may be stated in the form that coefficients of any order are expressible in terms 
of those of zero order. The proof follows from the expressions which we proceed to derive, 
giving coefficients of any order in terms of those of lower order. We have 
-£(aj1.23...p) = £(xl.23...(p-1) x1.23...p) 
= 2{xi.23...(p-i) (xi — Pip.23...(p-i)xP ~ terms in xt to a^-i))} 
= £(x1.23...(p-l)) — Pip. 23... (p-iyEfal. 23. ..(p-1)3^.23. ..(p-1)) > 
hence, dividing by N, 
var (£i.23...p) = var (#i.23...(p-d) — 0ip.23...(p-i) 0pi.23,..(p-i) var (^^...(p-d) 
= ™ 0*1.23...<p-l))(l -Plp.23...(p-1)) (15.16) 
which may be regarded as a generalisation of (14.11). By continuing the process we have 
™fo.s3...p)™™r(3i)(l -Pi22)(l ~ A.2KI-PH.32) ■ • • 
(l — />ip.23...(p-i)) • • (15-17) 
or ^p* = (1 - plMl - A.2) - . • (1 - />?p.23...(p-i)) ■ • ■ (15-18) 
The subscripts of the p's can be eliminated in a .different order, giving alternative forms 
such as 
fl^JL = (i _ ^(1 _ /)fi3)(i - pl2„) . . etc. 
Thus the variance of a residual of order p — 1 is expressible in variances of zero order and 
correlations of order p — 2. 
\ 
PARTIAL AND MULTIPLE CORRELATION 
371 
= 0 
= 0 
* 15.7. Equations (15.4) may be written 
Pl2 ala2 ~~ &2.34...p a2 ~ ^13.2...J3 /% a2aZ ~ ■ • 
Pl3 alaZ ~~ ^12.34...pP23 a2a$ ~ /^13.2.. .p a3 ~~ • • 
etc. Adding the expression for £(%l.23...p)> i-e- 
al — a 1.23..-.p ~~ /^12.34...33Pl2 ala2 — ^13.2. ..p Pl3 ^l0^ 
we have j) equations from which, on elimination of the /Ts, there results 
= 0 
2 2 
°i ~~ °"i.23...p 
/>3i ^sCi 
P12 CiCa 
o 
^2 
p32 Csffa 
pis Ciffa 
= 0 
where, of course, pik = pki. Dividing the ith row by at and the kth. column by ak, we get 
Write 
•1 _ q1.88...p 
«i 
Pia 
Pip 
1 
0)*= ^ 
Pl2 
1 
P2,j 
' Pia 
1 
/>13 
P23 
P3p 
Pl3 
P23 
P2„ 
1 
P2y, 
- 0 
(15.19) 
(15.20) 
Pip Pip P'ip ■ ■ * 
and (oL1 for the minor of the first row and column of this determinant. Then from (15.19)— 
of 
al.23...p • 
Wll 
. (15.21) 
. (15.22) 
Generally it may be shown in exactly the same way that 
C0V (xl.l...l-l, l + l, ...p xm.l...m-l, m+l, ..ji) 
co,vx 
where mlm is the minor of the Zth row and mth column in (15.20). 
This result shows that the variances and covariances of residuals of any order can 
be expressed in terms of the correlations and variances of zero order. 
15.8. We have, as in (15.16), 
£(xl.M...p x2.U...p) = ^(a;1.34...(p-l) ^2.34...(p-1)) — $2p.34.. .(p-l) ^(Xl.U.. .(p-1) X/).M...(p-l))- 
Substituting 
o- _ a °2.34...(}>-l) 
P2p.34...(p-1) — Pp2.34...(p-1) —~2 
ffp .34...(p-l) 
and expressions for the covariances in terms of variances and regressions, and writing 
q for the group of secondary suffixes 34 ... (p — 1), we find 
^12.qp °2.qp = $12.q a2.q — Plp.q Pp'2.</ °2.<7» 
372 PARTIAL AND MULTIPLE CORRELATION 
whence, in virtue of (15.15), • 
P12.QP ; n o [10.46) 
■ ■ L — RZp.q Pp2.q 
expressing the partial regression coefficient in terms of those of next lower order. 
Writing down the similar equation for ^i.qp an^ taking square roots, we find 
U1 ~ Pip-giK1 — Pzp.q)f 
a fundamental equation giving the correlation coefficient in terms of those of lower orders. 
15.9. From the above results it is clear that the whole complex of partial regressions, 
correlations and variances or covariances of residuals is completely determined by the 
variances and correlations, or by the variances and regressions, of zero order. It is 
interesting to consider this result from the geometrical point of view. 
Suppose in fact that we have N sets of observations of p variates 
X^ . . . #ip, #21 . . . Xop, • • •> •''jVl " • • xNpm 
Consider a Euclidean (flat) space of N dimensions. To each set of values xlk . . . xNk 
there will .correspond one point in this space, and the totality of points representing all 
observations will be p in number. (This method of representation, it should be noted, 
is not that of N points in a p-w&y space, which was the one used in some of the sampling 
discussions in Chapter 10.) Call these points Qlt Q2, . . . Qp. We will assume that the 
x's are measured about their mean, and take the origin to be P. 
The quantity Na* may then be interpreted as the square of the length of the vector 
joining the point Q\{= xn, . . . xNl) to P. Similarly plm may be interpreted as the cosine 
of the angle Qt P Qm, for 
_ _2(xjl Xjm)_ , 
plm (2k"„ 2^m)k 
which is the formula for the cosine of the angle between PQt and PQm. 
Our result may then be expressed ■ by saying that all the relations connecting the 
p points in the N-spa.ce are expressible in terms of the lengths of the vectors OQ and the 
angles between them ; and the theory of partial correlation and regression is thus exhibited 
as formally identical with the trigonometry of an ^-dimensional constellation of points. 
15.10. The reader who prefers the geometrical way of looking at this branch of the 
subject will have no difficulty in translating the foregoing equations into trigonometrical 
terminology. We will here indicate only the more important results required for later 
sampling investigations. 
Note in the first place that the p points Q and the point P determine (except perhaps 
in degenerate cases) a space of p dimensions in the i^-space. Consider the point Qi.2...p 
whose co-ordinates are the N residuals xlm2...p- In virtue of (15.9) the vector PQU2.,.P 
is orthogonal to each of the vectors PQZ . . . PQp and hence to the space of (p — 1) 
dimensions defined by P, Qz, . . . Qp. 
Consider now the residual vectors Qx q, Q2iai .where q represents the secondary suffixes 
34 ... (p — 1). The cosine of the angle between them, say 0, is pw q and each is 
orthogonal to the space P, Q3 . . . Q(P-iy Now take M on PQp such that MQlq and MQ^.^ 
are perpendicular to PQp. Then MQlg is perpendicular to the space P, Q3, . . . Qp and 
% 
/ 
PARTIAL AND MULTIPLE CORRELATION 
373 
so is MQ2 q. The cosine of the angle between them, say tf>, is p12.w (of. Fig. 15.1). Thus, 
to express />12iflp in terms of p12 q we have to express <f> in terms of 6, or the angle between 
the vectors PQUn and PQz.q in terms of that between their projections on the hyperplane 
perpendicular to PQp. We have 
(Qi.a Q2.a)2 = PQla + PQlg ~ 2PQi.« PQ*-n cos * 
= MQl n + MQlq - 2MQL,2 MQ2,2 cos <f>. 
PQlq = PM • + MQlQ 
PQlq = PM* + MQia 
MQliq MQ^„ cos $ = - PM2 + PQUfr PQimll cos d 
MQlqMQ2f] , n PM PM 
Now r>n q is *^e sine °ftne angle ^pQ\.q> tne cosine of which angle is plp q. 
in (15.25) we find 
Further 
and 
and hence we find 
COS d> = COS0 -Plp.r/P'2,,.7 
9 {(1 -p?,.,)(l -?L,, 
)}' 
. (15.25) 
Substituting 
. (15.26) 
which is equation (15.24) in a slightly different form. The expression of a correlation 
coefficient in terms of those of the next lowest order is thus capable of interpretation as 
" the projection of an angle on to a space of one fewer dimensions. 
* 
Example 15.1 
In an investigation ihto the relationship between weather and crops, Hooker (1907) 
found the following means, standard deviations and correlations between the yields of 
seeds hay (a^) in cwts. per acre, the spring rainfall (#«,) in inches and the accumulated 
temperature above 42° F. in the spring (ar8) for an English area over 20 years:— 
xx = 28-02 ■ cfi = 4-42 pla = + 0-80 
£a = 4-91 a2 = 1-10 plz = — 0-40 
£a = 594 a3 = 85 pa3 = — 0-56 
374 PARTIAL AND MULTIPLE CORRELATION 
The question of primary interest here is the influence of weather on crop yields, and 
we consider only the regression of %x on the other two variates. From the correlations 
of zero order it appears that yield and rainfall are positively correlated but that yield and 
accumulated' spring temperature are negatively correlated. The question is, what 
interpretation is to be placed on this latter result ? Does high temperature adversely affeot 
yields or may the negative correlation be due to the fact that high temperature involves 
less rain, so that the beneficial effect of warmth is more than offset by the harmful effect 
of drought ? 
To decide this question, let us calculate the partial correlations and regressions. From 
(15.24) we have 
Pl2 ~ Pl3 P23 
*2-3 - V(l - P?3)(l - A) 
= 0-80 - (- 0-40)(- 0-56) 
~ VU - (0-40)2}{l - (0-56)2} 
= 0-759. 
Similarly p13 2 = 0-097 
P23.1 = — 0-436. 
We next require the regressions /S and the variances of residuals. From (15.14) we have 
a C0V (^l.S X2.S 
Pl2.3 ' „. 
) 
var a;2i8 
_ Pl2.8 ^l.S 
°2 3 
This, however, involves the calculation of ax<3 and a23 which are not in themselves of interest. 
We can obviate the process by noting that from (15.16) 
o'l.a.s = ai.s(l — Pis.s)* 
°2.13 = G2Al — Pl2.3)4 
so that 
o P12.3 aia.$. 
Pia.3 ~ 
°2.13 
The standard deviations o123 and a2.n are °f som0 interest and may be calculated from 
(15.18). We have 
o-i.23 =ffi(l — /»!«)*(! -Pi3.a)r 
the two forms offering a check on eaoh other. 
From the first we have 
<rL23 = 4-42{1 - (0-8)2}*{l - (0-097)2}* 
= 2^64. 
Similarly o^l3 = 0-594 
C3.12 =70-1. 
and we also find 
013.2 = 0-00364. 
PARTIAL AND MULTIPLE CORRELATION 
375 
The regression equation of Xx on Za and Z8 is then 
Xx - 28-02 = 3-37(Xa - 4-91) + 0-00364(Xs - 594). 
This equation shows that for increasing rainfall the yield increases and that for 
increasing temperature the yield also increases, other things being equal. It enables us to isolate 
thte effects of rainfall from those of temperature and study each separately. The positive 
regression /?13-a means that there is a positive relation between yield and temperature when 
the effect of rainfall is eliminated. The partial correlations tell the same story. Although 
pi3 is negative, Pn 2 is positive (though small), indicating that the negative value of pla is 
due to complications introduced by the rainfall factor. 
The foregoing procedure avoids the use of determinants arithmetic, but the reader 
who prefers to do so may use equations (15.21). For example; 
l 
from which 
O) = 
■ 
1 
.0-80 
- 0-40 
=> 0-2448 
0)n = 
1 
-0-56 
= 0-6864, 
0-80 
1 
-0-56 
-0-56 
1 
- 0-40 
- 0-56 
1 ' 
or, 2i, = o-x /— = 2-64 as before. 
15.11. When the work involves more than three variables it is desirable to systematise 
the arithmetic. Considerable assistance may be derived from tables of quantities such as 
i-,W(i-p«>. V(1 _ pl)(l _^ 
Kelley (1910, 1938) and Miridr (1922) have given tables for this purpose. Trigonometrical 
tables, are also useful in some cases. For instance, given p we can find 6 = cos-1p and 
henco sin 0 ( = VC1 — i°2))> cosec0 (= ,,. __ tX etc- 
For detorminantal work some systematic method of reduction such as the Doolittle method 
is useful. 
Example 15.2 f • * 
In some investigations into the variation of crime among cities in the U.S.A., Ogburn 
(1935) found a correlation of — 0-14 between crime rate (Xx) as measured by the number 
of known offences per thousand inhabitants and church membership (Z5) as measured 
by the number of church members of 13 years of age or over per 100 of total population 
of 13 years of age or over. The obvious inference is that religious belief acts as a deterrent 
to crime. Let us consider this more closely. 
If Za = percentage of male inhabitants, 
X3 — percentage of total inhabitants who are foreign-born males, and . 
Xi = number of children under 5 years old per 1000 married women between 
15 and 44 years old, 
376 PARTIAL AND MULTIPLE CORRELATION 
Ogburn finds— 
Pls = + 0-44 p24 = - 0-19 
p13 = _ 0-34 />M = - 0-35 
Pll = - 0-31 Pai = + 0-44 
Plt = - 0-14 ' Pa8 = + 0-33 
p28 = + 0-25 PiB = - 0-85. 
From this and other data given in his paper it may be shown that we have, for the regression 
of Xx on the other four variates, 
Xx - 19-9 = 4-51(3:, - 49-2) - 0-88(X3 - 30-2) - 0-072(X4 - 4814) + 0:63(Xfl - 41-6), 
and for certain partial correlations 
Pi«.8 = - 0-03 
P16.4 = + 0-25 
pis.M = + 0-23. " 
Now we note from the regression equation that when the other factors are constant 
Xi and X6 are positively related, i.e. church membership appears to be positively associated 
with crime. How does this effect come to be masked so as to give a negative correlation 
in the coefficient of zero order plB ? 
We note in the first place that the correlation between crime and church membership 
when the effect of xi} the percentage of foreigners, is excluded, is near zero. The correlation 
when xi} the number of young children, is excluded, is positive ; and the correlation when 
both aj8 and xt are excluded is again positive. It appears in fact from the regression equation 
that a high percentage of foreigners and a high proportion of children act as deterrents 
to crime. Now both these factors are positively correlated with church membership 
(foreign immigrants being mainly Catholic and more fecund). These correlations 
submerge the positive influence on crime of church membership among other members of the 
population. The apparently negative effect of church membership appears to be due to 
the more law-abiding spirit of the foreign immigrants and the fact that they are also more 
zealous churchmen. 
The reader may care to refer to Ogbum's paper for a more complete discussion. 
The Multivariate Normal Distribution 
15.12. We now turn to consider the generalisation of the univariate and bivariate 
"normal distributions to the case of p variables. 
Consider the multivariate distribution 
This has p variates and evidently reduces, when p = 1 or 2, to the normal type. We shall 
take it to be the generalisation of the normal distribution, and proceed to consider how 
the constants a are related to the correlations of the variates. It is, of course, assumed 
that the a's are such as to ensure the convergence of the distribution function. For this 
it is necessary and sufficient that the quadratic form E a^ — — shall be positive-definite 
i.e. that there is a real linear transformation reducing it to the sum of squares of p (or, 
in degenerate cases, fewer) new variates. 
i 
THE MULTIVARIATE NORMAL DISTRIBUTION 377 
Make the transformation 
r> 
X" — 
;= i 
f = ^lri^i (15.28) 
r ;= i 
and choose the Z's so that the exponent of (15.27) becomes - $22". o4en we have 
•£- tCa 
ur u8 
and honce, writing (a) for the matrix of the quantities a, (Z) for that of the Z's and (?) for 
the transpose of (Z), we have 
(ce)(Z)(Z) = 1. . . • . . . (15.29) 
Further, the Jacobian of the transformation is | Z |, the determinant of the Z's, and hence 
the integral of dF is given by 
2/o | 11 I • • • f exp {- \ZP} dtt . . . d£p = (2jt)%01 11. 
J — oo J — oo 
Hence, since from (15.29) | a | | Z |2 = 1, we have 
V* = ~ = L^Jl (15.30) 
[2n)s | Z | (2n)2 
Let VLB now find the characteristic function of the distribution. We have to integrate 
over the range of x's the exponential of 
- OteS -'2K)] 
= - i[27(*. -Eitrar ZJ * + EWk °j°kZlir Ul 
a r j,k r 
The first part reduces on integration to a constant. The second gives the exponential 
of a series of terms of second degree in t, the coefficient of tjtk a]Ok being 
r 
Now £ lir lkr is the minor of the jth row and &th column in the matrix (f)(Z) and hence, from 
(15.29), in the matrix (a)-1 = (A) say. Hence we may write 
<j>{tu . . . tp)=exv{-$i:{Ajkojoktjtk)}. . . . (15.31) 
Bnt when thiR is expanded the term in a^ tfa is — pik by definition and hence (.4) is the 
matrix (to) of equation (15.20). Thus 
Wu • • • tp) =exp {-lE{Pik a^tfa)}. . . . (15.32) 
Furthermore, 
(a) = (A)-1 = (co)-1 
and henue the distribution itself may be written in the form 
dF = _i_ expj- -L zLr, *t XJ)Y^ . . 3. . . (16.33) 
For example, with the bivariate form 
1 P 
P 
co | = . j 
378 PARTIAL AND MULTIPLE CORRELATION 
and hence cb = 1 — p2, <un = con = 1, co12 = co21 = — p, so that the distribution becomes 
the familiar form 
• 2tt(1 - p")* P 1 2(1 - p^lo-f orlflr, ~* o%)] a, a,' 
15.13. For any fixed xt . . . xp the exponent of (15.33) reduces to the normal 
univariate form in xt with mean 
— /wi8- + co18^ + . . -co*J>) (15.34) 
colt\ a2 aa * cfpJ 
Thus the regression of xt on the other variates is exactly linear. The variance of xt in any 
arrav is —- and the distribution is thus homoscedastic. It follows generally that the 
regression of any variate on any or all of the others is linear. Comparing (15.33) with 
(15.21) we see that the distribution may be written 
dF = _^ exp J - \E Pr^...vx^s \dXx m 4 m dx^ (15.35) 
(2n)?tfl . . . Op 0>* [ °V.12...pCf,.19...pJ 
where the secondary suffixes in the p and c's do not, of course, contain r and s. 
Since every x is normally distributed, every linear function of x is so, as may be seen 
at once from (15.33). In particular the residuals are normally distributed. 
If in (15.33) we make the substitution 
£i = a;,. 
£2 = ^2.1 
C3 = £3.21 
£4 = ^4.321 etC. 
the exponent will be a quadratic function of the C's." In this function all product terms 
^•^k,j^k, must vanish, for the covariance of ^ and £k vanishes in virtue of the remark at the 
end of 15.4. It follows that the distribution function may be written in the form' 
dF = _ l— exp {- U* + §i + ^p + . ." . ) W dx2A. . (15.36) 
From this it appears that the joint distribution of any two residuals xf q and xkiQ is of 
the bivariate normal form with correlation p1k q. Consider, for example, x21 and #3.21- 
Each is normally distributed and is uncorrelated with and independent of the other variables 
in (15. C6). If £3.21 is expressed in terms of residuals of the second order, i.e. asa;3il— ^32.1 #2. u 
the joint distribution of #2.i and x.iml becomes of the bivariate form with correlation jo23.i J 
and so generally. 
These results are important in the interpretation of regressions and correlations in 
the normal case. In the general case a coefficient such as pjk represents the average 
dependence, so to speak, of x^q and xkiQ, being based on the sum E(x1q xkq). In the 
normal case p1ktQ is constant for all the sub-populations corresponding to particular assigned 
values of the other variables. 
Sampling Distributions of Partial Correlation and Regression Coefficients 
15.14. We now consider the sampling distributions of the coefficients of partial 
correlation and regression. For large samples the values of Chapter 14 appropriate to 
DISTRIBUTION OF PARTIAL COEFFICIENTS 379 
correlations and regressions of zero order may be used (subject to the proviso as to the 
unreliability of the standard error for p unless the sample is very large). For example, the 
variance of pikq in the normal case is given by 
var(rift.a)=i(l-PVa)a. • • - • • (15-37) 
where n is the sample number ; and that of the regression coefficient by 
var(6ifc.a)=I ^jL*S. . . . ' . (15.38) 
The proof of these results by the direct methods of Chapter 9 is a very tedious piece of 
algebra. They follow simply, however, from the remark of the previous section that the 
correlation between any two deviations ximQ and xka is of the normal type with coefficient 
Pjk.g > f°r i* follows that pjka is distributed as the correlation between two normal variates. 
Similar considerations apply to the regression coefficients. It will be shown presently 
that if the original distribution was based on n observations, that of pik q is of the form 
of the correlation pik based on n — s observations, where s is the number- of secondary 
subscripts in q; but as our equations are only true to order ri-1 the divisor in (15.37) 
and (15.38) may remain at n without further error. 
15.15. Consider now the geometrical representation of 15.9. Suppose we have 
three points Q, R, S in the n-fold space, represented by xx . . . xn, yt . . . yn, zx . . . zn 
respectively, the origin being P and the variables measured from their mean. Then the 
coefficient of correlation between x and y is the cosine of the angle QPR, that between 
y and z the cosine of EPS and that between z and x the cosine of JSPR. Now imagine a 
sphere described with unit radius and centre P, cutting PQ, PR and PS in Q', R', S'. Then 
will the partial correlation r^^ be the cosine of the angle of the spherical triangle Q'S'R', 
and so for the other two partial correlations. This was, in effect, proved in 15.10, for tho 
angle Q'S'R' is the angle between the projections of PQ and PR upon the space 
perpendicular to PS. 
Now we may make an orthogonal transformation, corresponding to a rotation of tho 
■co-ordinate axes, without affecting the correlations ; moreover, if the n values of one 
variate x are independent and normally distributed so will be the n values of the 
transformed variates. Let us then make such a transformation and take PS as one of the new' 
co-ordinate axes. It is then apparent that the distribution of rWi3, which is the cosine 
■of an angle in the space perpendicular to PS, is the same in form as that of txv except that, 
being in (n — 1) dimensions, it is based on (n — 1) independent pairs of normally distributed 
variates instead of n. ' 
Hence for samples from a normal population the distribution of the partial correlation 
coefficient of the first order from n sets of observations is the same as that of a correlation 
of zero order from (n — 1) sets of observations. By a repetition of the same argument 
it follows that the distribution of a correlation coefficient of the sth order is that of the 
correlation of zero order from (n — a) sets of observations. The results of the previous 
chapter are thus immediately applicable to partial correlations. If, of course, s is small 
compared with n, the distribution of partials is sensibly the same as that of ordinary 
correlations, which confirms the approximation of the previous section. 
380 PARTIAL AND MULTIPLE CORRELATION 
The Multiple Correlation Coefficient 
15.16. As in 14.22, the multivariate regression equation can be used to estimate 
the values of one variate from given values of the others ; but in order to see how good 
such as estimate is likely to be we require to know whether the values." predicted " by 
the regression equation are in close relationship to the observed values. Consider the 
regression of Xt on the other variates: 
■^1 = &2.34..p-3^2+ ^13.24...p -^3 + • • • 
= ^1.23.. .p. say-. : (15.39) 
If we substitute an observed set of values xz . . . xp we shall get a quantity e123...jj» 
say, differing from the observed x1 by the residual quantity £C1.23...P> so that 
Zl — ffl.23...p = «1.23...p. .... (15.40) 
We may then judge of the accuracy of the representation of the observed a^'s by the 
regression equation by correlating xx and e^^:^kV. We have 
-^(e1.23...p) = ^>{xi — Xl.1$...p)* 
= JVr(oJ-o}.M...p) . (15.41) 
and -27(*iCi.«3...p) = sixl) ~ ^.sb.-.p) 
-^(orJ-af.M.-.p) \ • ; • (15.42) 
Henoe the correlation between xx and eli23...p, say -Ri(2...P)> is given by I 
o = cov(a;ie1,23...p) 
"1{*-P) (valvar *.„..*)* 
= (°1 ~ g1.23...p)* ' 
giving 
«aK2...p) = l-^|^ (15.43) 
<*i 
j^i(2...p) 1& called the Multiple Correlation Coefficient between xx and We 
have, similarly, multiple correlation coefficients of any variate on some or all of the others, 
e.g. -Ri<23)> ^4<123)> e*c- 
Two alternative forms of R are worth noticing. From (15.43)-and (15.21) we have 
. ^i(2...p) = 1 -—, • ■ • . . . (15.44) 
and from (15.43) and (15.17) 
1 - *!(2...p) = (1 — />12)(1 — P13.2) ■ • • (1 -/>?p.2...(p-l)) . (15.45) 
15.17. Prom the latter equation it follows that since no p is greater than unity, R 
must be at least as great as the absolute value of any p entering into (15.45). JR itself is 
essentially positive, for a1 > cr^ .23...P (equation (15.18)). 
It follows that if R = 0 all the constituent p's must be zero, and conversely. In this 
case &! is completely uncorrected with any of the other variates and the regression equation 
is quite useless as a means 6f estimating the dependent variable: 
On the other hand, if jR = 1 the correlation between the observed and the estimated 
value given by the regression equation is perfect, i.e. xx is a linear function of the other 
variates. R thus provides a measure of the relationship between xx and the remaining 
variates. 
THE MULTIPLE CORRELATION COEFFICIENT 381 
15.18. The coefficient R has an interesting geometrical interpretation. It was noted 
in 15.10 that the residual vector P#i.2...p is orthogonal to the space of {p — 1) 
dimensions denned by P, Q2 . . . Qp. Consequently the angle between this vector and the 
^-dimensional space P, Q1 . . . Qp is the complement of the angle Q\ P#li2...p, that 
is to say its sine is H1(2...p). From this standpoint we see that if R = 0, PQ[ 'is also 
orthogonal to the space P, Qs, . . . Qp} i.e. that xx is uncorrelated with xa . . . xp. If 
jR = 1, PQ1 lies in the space and x1 is linearly dependent on a:2 . . . xp. 
15.19. The coefficient R, as mentioned in 14.24, is analogous to the correlation 
ratio rj, and in fact from some points of view the two are formally identical. Given a set 
of variate-values we may consider the variance of x1 as composed of the sum of two variances, 
for we have, by definition, 
var^ = a\=a\ -fff.8...p + fff.g...p 
= var K.2...p) + var (ccx — el42...p)- . . (15.46) 
Thus the variance of x may be regarded as the sum of the variances (1) of the deviations 
of ccx from the values given by the regression equation, and (2) of those values themselves. 
We may write (15.46) as 
var (*,) = ft,,,) +*!(!- «!(>...pJ). • . (15.47) 
Now consider again equation (14.75) in the form 
var x = var x{r\\ + 1 - ^} 
= °inlv + 4(i - vly) (15-4S) 
The relation with (15.47) is evident. It is redeemed from triviality by the fact that, just 
a.s the two parts on the right-hand side of (15.48) are independent in samples from an un- 
■corrolated normal population, so are those in (15.47) in samples from a multivariate 
normal population for which the parent R is zero. For in that case x1 is independent of 
t.he other variables and therefore deviations of xx from the regression values are independent 
of the deviations of those values about their mean. 
15.20. From this fact we can derive the sampling distribution of R (the sample 
value of the'multiple correlation coefficient) when R (the population value) is zero and the 
population is normal. In fact, as in 14.24, we see that —-^—- is the quotient of two 
independent variables. The numerator is distributed in the Type III form with N — p 
degrees of freedom, for it is a multiple of the variance of xlm2$...v J var xi WU<1 be distributed 
as the Hum of the squares of N variates about their mean, i.e. with iV — 1 degrees of freedom, 
var x{ a with N' — 2 degrees of freedom, and so on, every additional subscript lowering 
the degrees of freedom by unity, as in 15.15. Further, the denominator is distributed 
in the Typo III form with p — 1 degrees of freedom, for it is the difference of var xlt which 
has N — 1 degrees, and var x12...v which has N —p degrees.* Thus the distribution 
of li2 is formally the same as (14.85) with R2 instead of rj2, i.e. is 
dF = - (1 - R2)W-p-2) {R2)H»-VdR2. . . (15.49) 
* It is not, of course, true in general that the difference of two Type III variates is distributed 
in the Type III form. In the present ca9e we can find an orthogonal transformation of the variables 
x to now independent normal variables, of which one may be taken to be the residual xliWiiP 
382 PARTIAL AND MULTIPLE CORRELATION 
This can be reduced to the z-form by writing 
... R* N-p\ 
' vi = p — 1, Vi = N — p t 
The mean value of R* is the positive quantity (p — 1 )/(iV — 1). 
. (15.50) 
15.21. We proceed to find the distribution of R in samples from a normal 
multivariate population when R is not zero. Two preliminary remarks are necessary. 
In the first place, any multivariate normal population can, by a linear transformation, 
be transformed to new variates which are normally distributed and independent. One 
such transformation' has been given in 15.13. 
Secondly, any linear transformation leaves the multiple correlation coefficient- 
invariant, that is to say, the coefficient between xx and the same as that between 
xx and the transformed variables £a • • • £p- Referring to (15.43) we see that, apart 
from the constant of, ■Bi(2...33) depends only on of 2...p> and since the regressions are 
chosen so as to minimise this quantity, the same minimum is reached whether we use the 
variables xt . . . xp or the linearly related variables f3 . . . £p. Conversely, if the 
correlation between xx and £a is a maximum for all possible sets of £'s, then that correlation is ' 
the multiple correlation coefficient between xx and the £!s, and xx is uncorrected with 
S3 • • • Sp- 
From the geometrical standpoint of 15.10, let us take the sample vectors PQa . . .PQP 
and in the space defined by these vectors choose another set P$fl . . . PjSp. which are 
mutually orthogonal. These will correspond to the transformed variates £, and the angle' 
between PQ1 and the space remains unaltered, i.e. R is invariant. 
Let us now choose £2 so that the correlation between xx and £s is a maximum in the 
population. Then if PSt is th& sample vector corresponding to £8, PQX will be orthogonal 
to all the other ^vectors PS3 . . . PSP (since xx is then independent of £3 . . . l-p). 
In any .given sample value the correlation between xx and £a will not be equal, in general, 
to U,(though the correlation in the population is jR), but to a quantity r, <say, varying from 
sample to sample and equal to cos-1Q1P/Sf2. Let PT be the vector representing the sampling 
v 
regression formula ^ hj....p %• This will lie in the x-£ space (cf. Fig. 15.2). Then 
j-i 
Fig. 16.2. 
THE MULTIPLE CORRELATION COEFFICIENT 383 
PT is such as to make the greatest possible angle with QtP, the angle being cos-1 R, and 
the perpendicular from Qx on to the x-% space meets PT in a point, say K. Let the 
point L be taken on PS* such that LK is perpendicular to PK and join QJj. Let the 
angle KPL be ip. 
Then Q^L* = &P2 + PL* - 2Q1P.PL r 
= QXK* + KL* 
= &P2 - PZ2 + PL* - PK* 
and hence 
, _ PK* = PZ PJ^ 
f &P.PJ& QxP'Pi 
= R cos y. ....... (15.51) 
R and y are independent. 
Now we consider the sampling distribution of the correlation coefficient r. It is to 
be remembered that xx and £a are distributed in the bivariate normal form. The 
distribution of r is then given by the formulae of 14.14 and may be written 
dF.** \ 2 / (1 -r*fr-dr 
rC4 
© 
n-l foo dz 
X >-^ (1 - jRa)"a~ 7 u » ,„ , • • (15.52) 
V:* iYn ~ 1N1 J -»(coshz ~ Rr) 
since R is the population value of r. If R — 0 the second factor in (15.52) reduces to 
-unity. We may therefore regard the second factor as the effect on the frequency density 
in the region dr of a population correlation jR. Now we have already seen that when 
R = 0 the distribution of R corresponding' to that of r is given by (15.49). We have 
then to find by what factor (15.49) is to be multiplied to allow for the variable frequency 
density. Such factor is the second part on the right-hand side of (15.52) with Rcoatp 
substituted for r (as given by (15.51) ) and integrated over the permissible domain of tp. 
It will be seen from Fig. 15.2 that for fixed P and Sti T may vary in the space of (p — 1) 
dimensions determined by P and Sz . . . Sp ; and for constant ip it will He on the cone 
in that space obtained by rotating TP about PSS. The element of area cut off by this 
cone on the unit hypersphere is proportional to its solid angle, that is to ainv~3 ■$, and 
hence the frequency of y in the range dtp is 
r(-2-) 
>.. f_/_sinP-3vdv' .... (15.53) 
Thus the density factor is 
384 PARTIAL AND MULTIPLE CORRELATION 
Finally the distribution of R is 
r(t) 
xpp tin",**, , . 1 (15.56) 
Jo J-« (cosh 2 — Ui2cosy)n_1 
This may be expressed as a hypergeometric function. Expanding the integrand in 
powers of cos ip we have, since odd powers vanish on integration, 
- fn + 2/-2N sin ^~3 y cos 2^y) 
"' 2A 23 ■/ (cosh2)"-i+2^ v > 
and since i cos2*yship~V —B\J———, -—— ) 
J.^cosh^-^2^ V2 2 / 
the integral becomes 
in+V2) i-*1- 21V) ii- n-±^) w 
A, - 2\ fn - 1\ 
- V\ L - ,'\j '{t^ ^ ^ ™°l • • <i5-56> 
rS)r(V) l J 
whence we find, from (15.55), after a little further reduction, 
I a / «—1 P-3 W—p-2 
<tf» = ^ \ 3 ' (1 - i^p-fiZap-fl - jR«)—=3— rfi?2 
T In —p\ r/p — V 
« 
»'{*-fi-TJ.TJ'B'I,l- • •('«•"! 
15.22. Writing a = |(# — 1), b = £(?i — JP) we have 
dF = ^T^rTi^1 ~ ■R2)a+& (i22)""1 (X.- i22)&_1 F(a + b,a + b,a, R*R*) dR* . (15.58) 
= TWm (l{-~Rmr^ (i22)a_1 (1 ~ i22)b_1 X ^(- 6, - 6, a, *■*») dB" . (15.59) 
It may be shown that 
ft[(R*) = 1 1- (1 - R*) F(l, 1, a + 6 + 1, tf2). . . (15.60) 
In particular, when R = 0 we have the known result 
* i"!^2) = -T-S = ~~ (16.61) 
THE MULTIPLE CORRELATION COEFFICIENT 885 
For large n we have approximately 
a + o 4- i 
For the second moment 
M&) = r^JLtw^iiyil FV* 2,a+b + 2,R*) 
(a 4- o)(a 4-6 4-1) 
OU " > F*(ltlta +b+l,R*) . . .(15.63) 
(a 4- bY 
or approximately 
«,(£«) = 4^2(! -*2)2 (15>64) 
n 
which, however, breaks down near R — 0. It would, in fact, appear that the distribution 
of R tends to normality when R 7± 0 but not when jR = 0 (cf. Exercise 15.3). 
Example 15.3 
From Example 15.1 we have found o> = 0.2448, colx = 0.6864, from which we have 
0-2448 
^("J ~ 0^6864 
= 0-6433, 
indicating that the regression equation is a fairly close representation of the data, since 
E, the correlation between observed a^'s and those provided by the equation, is high, 
about 0.80. 
It is hardly necessary to test the significance of such a value, but we will do so to 
illustrate the arithmetic involved. If xx were uncorrected with the other variates we should 
have jR = 0, and on the assumption that the population is normal (a reasonable assumption 
for crop yields, sunshine and rainfall records) we may use equation (15.60). We have, 
since p = 3, n = 20 
_ 0-6433 17 
Z ~ 2 l0ge 0^3567" 2 
= 1-36 
i»! = 2, v2 = 17. 
From Appendix Table 5 the 1 per cent, significance point of z for vv = 2, vs = 17 is 0-9051, 
so that the observed R is almost certainly significant, z being much greater than oan be 
accounted for by sampling alone. 
NOTES AND REFERENCES -s 
The theory of partial correlation is mainly due to Yule (1907). The reader may refer 
to M. Ezekiel's book (1930) for a detailed discussion of the practical side of correlation 
analysis. See also a paper on the theoretical side by Frisch (1929). 
For a knowledge of the sampling properties of the partial correlations we are indebted 
to Yule (1907), who pointed out the applicability of large sampling " normal" formulae 
for coefficients of zero order to the partial coefficients, and to R. A. Fisher (1924), who 
is responsible for the exact result for small samples from a normal population and the 
A.S. ' 0 0 
386 
PARTIAL AND MULTIPLE CORRELATION 
distribution of the multiple correlation coefficient (1928). Some approximate results for. 
the latter had been obtained by Isserlis (1917) and P. Hall (1927). Wishart (1931, 1932) 
has studied the exact distribution of B and the formally equivalent r\. Both of Fisher's 
papers are notable examples of the power of the geometrical method of deducing sampling 
distributions. 
In comparing formulae given by various writers it is as well to examine whether the 
total number of variates (our p) or the number of dependent variates (p — 1) is being 
used as a constant in the equations. 
Ezekiel, M. (1930), Methods of Correlation Analysis, Chapman and Hall, London ; John 
Wiley and Sons, New York. 
Fisher, R. A. (1924), "The distribution of the partial correlation coefficient," Metron, 
3, 329. 
(1928)i " THe general sampling distribution of the multiple correlation coefficient," 
Proc Roy. Soc, A, 121, 654. 
Frisch, R. (1929), "Correlation and Scatter in statistical variables," Nordic Statistical 
Journal, 1, 36. 
Hall, P. (1927), " Multiple and partial correlation coefficients in the case of an n-fold variate 
system," Biometrika, 19, 100. 
Hooker, R. H. (1907), " The correlation of the weather and the crops," Jour. Boy. StaL 
Soc, 65j 1. 
Isserlis, L. (1914), " On the partial correlation ratio, Part I, Theoretical," Biometrika, 
. ' 10, 391, and "Part II, Numerical," ibid. (1916), 11, 50. 
(1917), " The variation of the multiple correlation coefficient in samples drawn from 
an infinite population with normal distribution," Phil. Mag., 34, 205. 
Kelley, T. L. (1916), " Tables to facilitate the calculation of partial coefficients of correlation 
and regression equations," Bulletin of the University of Texas, No. 127. 
(1938), The Kelley Statistical Tables, Macmillan. 
Miner, J. R. (1922), " Tables of Vl — r2 and 1 — r2 for use in partial correlations, etc.,'* 
Johns Hopkins Press, Baltimore. 
Ogburn, W. F. (1935), "Factors in the variation of crime among cities," Jour. Amer. 
Statist. Ass., 30, 12. 
Wishart, J. (1931), " The mean and second-moment coefficient of the multiple correlation 
coefficient in samples from a normal population," Biometrika, 22, 353. 
(1932), " Note on the correlation ratio," Biometrika, 23, 441. 
Yule, G. U. (1907), " On the theory of correlation for any number of variables treated 
by a new system of notation," Proc. Roy. Soc, A, 79, 182. 
EXERCISES 
15.1. Show that 
a _ ^12.34. ..p + #Lp.23...(p-l) Ap2.43.. .(p-1) 
Pl2.34...(ZJ-l) ; o o 
1 "~' Pip.23...(p-1) Ppl.23...(p-1) 
and that 
_ Pl2.34-.-p ~t~ Pip.23...(p-1) P2p,13...(p-1) 
Pl2.34...(p-1) — Tj -jj xfTi ~2 vi 
V1 ~~ Pip 23...{p-l)t V1 ~~ P2p.13...{p-1)) 
(Yule, 1907:) 
EXERCISES . 387 
15.2. Show that for p variates there are (^) correlation coefficients of order zero 
and ( liv) °f order s. Show further that there are (^)2P~2 correlation coefiicients 
altogether and ( g)22'-1 regression coefiicients. 
15.3. Show that for given p12, p13, pia must lie in the. range 
±ii 2 2 i 2 2 \1 
. V1 — Pl2 — Pl3 + Pl2 PlZl 
and that if a^ and xz, xx and ica are uncorrelated no inference can be drawn from that fact 
as to the correlation between xz and x3. 
i • 
15.4. Show that if plz be zero, /d12i3 will not be zero unless at least one of pia, pia is zero. 
15.5. If the correlations of zero order among a set of variables are all equal to p, 
show that every partial correlation of the sth order is ——7-—r. 
(1 +sP) 
15.6. Show that the distribution of the multiple correlation coefficient M tends, 
in normal samples, for large n, to the form 
dF = (^2) ' exp {- 15" - J/?"} 
where 02 = R*(n - #), £2 = i2a(» - #). 
In particular, where p = 4, 
^ = _L I [exp {- i(5 - £)2} - exp {- i(5 + £)2}] dB. 
v ^ P 
Thus, when B = 0 the distribution of Z? does not tend to normality, but when /? is not 
zero and is thus large for finite Ry B is distributed approximately normally about 8 with 
variance unity. 
(Fisher, 1928.) 
15.7. Show that the distribution function of E in normal samples may be written, 
if n — p is even, in the form 
X W-i, - 
(Fisher, 1928.) 
> 
/ 
CHAPTER 16 
RANK CORRELATION 
16.1. In previous chapters we have considered the dependence of attributes, as 
measured by coefficients of association, and that of variables as measured (in the normal 
case at least) by product-moment correlation. In this chapter we shall consider a type 
of relationship which, in a sense, occupies an intermediate position between the two, the 
correlation of ranks. ■ 
Consider a set of individuals which oan be arranged in order according to some quality, 
such as a set of men according to ability or a set of musical compositions according to the 
degree of preference with which they are regarded by some observer. An ordered 
arrangement of the objects will be called a ranking and the ordinal number of a given individual 
in the ranking is called his rank. Thus with a ranking of n individuals there will be one 
rank corresponding to each of the n ordinal numbers 1 to n. 
16.2. Ranking is less general than the classification of attributes in the sense that 
the division of a population into classes A and nqt-A, or Ax, Aa . . . Ap, does not require 
any ordering of those classes ; the measures of contingency and association discussed in 
Chapter 13 are invariant under rearrangements of columns or rows in the tables. On the 
other hand, individuals arranged in an ordinary frequency table have their interrelationships 
more closely defined than if they are merely ranked, so that ranking is in a sense more 
general than measurement according to a variate-scale. To put the point in a slightly 
different way, a ranking is invariant under any transformation which stretches the scale 
of measurement of the variate. 
t 
16.3. In practice, ranked data usually arise in two ways:— 
(a) !From material which could be measured on a variate-scale but which is not so 
measured for reasons of economy, lack of adequate instruments, and so forth. This class 
includes the case where the data are given as measurements but are then ranked on the 
basis of those measurements in order, for example, to reduce the arithmetical work in 
investigating correlations. 
(b) From material which is believed to be capable of measurement theoretically but 
cannot be measured in practice, e.g. human preferences for food or intelligence. Ranking 
methods are sometimes applied rather uncritically to* material which the experimenter 
considers to be capable of ranking, whether it has been demonstrated to be so or not. We 
shall return to tlpg point below. 
It is always possible by suitable conventions to impose a scale of measurement and 
hence a variate-system on ranked material; but the process is sometimes rather artificial 
and we shall in the first instance consider ranked material as such, without reference to 
the possibility of there being any pre-existent or superimposed variate in the background. 
Spearman's Coefficient of Bank Correlation 
16.4. Consider a set of n individuals ranked according to two variables in the orders 
Ji, Xit . . . Xn, Yx, Yz . . . Yn, where the X'a and the Y'& are permutations of the 
388 
Individual 
Ranking 1 
Ranking 2 
A1 
X, 
Y1 
■Az • < 
X-2 . , 
X 2 • ■ 
• • An 
■ • Xn 
■* n. 
SPEARMAN'S COEFFICIENT OF RANK CORRELATION 389 
numbers 1 to n. Our problem is to discuss the relationship between the Z's and the F's. 
If the individuals are denoted by A-^ . . . An we may write the-rankings in the form 
9 o * ■ (16.1) 
We note first of all that the concordance between rankings is perfect if and only if 
Xj = Yj for all j. It is natural to consider the differences Xj — Yj (= djt say) as measuring 
the difference between the two rankings. They are zero if and only if the concordance 
is perfect and their magnitude to some extent reflects the divergence of the rankings from 
perfect concordance. We also note that 
n n n 
YAdi^j^xi~YJYi^^^ • • • <i6-2> 
1=1 1=1 i=i 
for each of the sums of X and Y is the sum of the first n natural numbers. We might 
then take E \ d | as a measure of discordance, and a coefficient based on this quantity was in 
fact proposed by Spearman (1906). It is however subject to several disadvantages, similar 
to those attaching to the mean deviation, and a more suitable measure is obtained by 
using U{dz). It is easy to see that the maximum value possible for E{d*) is -—-—. For 
E{d%) is the greatest if the <2's are as different as possible, i.e. if one ranking is the reverse 
of the other, so that the d's are (n — 1), (n — 3) . . . — (n — 3), — (n — 1), though not 
necessarily in that order. In this case 
SiXjY,) = 1(») + 2(% - 1) + 3(n - 2) + • • • n{n - (n - 1)} 
= l{(n + 1) - 1} + 2{(n + 1) - 2} + • • • n {{n + 1) - (n)} 
= <n + l)2>-i> 
_ _ ;■ 
1=1 t=i 
_n(n + !)(*+ 2) 
—— m m m • m m m m \A.\J*Oj 
Thus Z{d*) = Z{X*) + Z(Y*) - 2Z(XY) 
n(n + l){2n -f 1) n{n -f l)(n + 2) 
3 3 
nz — n 
3 
We then define 
(16.4) 
1 6^(^2) n**\ 
p = 1 - .....,,.. (lo.ol 
r n3 — n 
as the Spearman coefficient of rank correlation. If the concordance between rankings is 
perfect E(dz) = 0 and p = 1. If the discordance is perfect p = — 1. In other oases p lies 
between these limits. 
390 RANK CORRELATION" 
It is worth noticing that p is the product-moment coefficient of correlation between 
X and Y when we regard the ranks as variate-values. For we then have 
/Mi(Z)=yMi(r)=7L+l (16.6) 
_ v n(n + l)(2w + 
n var X = n var Y = -i—!—'- - 
6 
_ nz — n 
12~ 
n cov (X, Y) = 2(XY) - n{/ji[(X)}* 
i»_n(^jy 
(16.7) 
= - \E{X - r)a + £(X2) - -(w +1)s 
w3 — % 
- *2OT, 
12 
so that the product-moment correlation coefficient of X and Y is 
f 
12 
\Ed> 
\ fnz — n\ 
= 1 _ = p. 
16.5. There is an element of artificiality in the Spearman coefficient as defined which 
We must remove. The ranks are ordinal numbers and cannot without justification be 
operated on by the laws of cardinal arithmetic. For instance, if Ax is ranked 4th and 
8th by two observers, dx is (4 — 8) ; but what does 4th minus 8th mean, and what 
significance is to be attached to its square ? It is not entirely trivial to note that the necessary 
transition from ordinals to cardinals may be made without invoking a variate-scale. When 
we rank a member as r we mean that in the set of n, (r — 1) members are ranked higher. 
This number (r — 1) is a cardinal and in our particular example 4th minus 8th may be 
regarded as meaning that the difference of the number of members ranked higher by the 
two observers was 4. 
Example 16.1 . 
Two judges in a beauty contest rank the 10 competitors in the following order : 
643127 98 10 5 
416758 10 9 32 
What is the rank correlation 1 
The differences between the ranks are 
2 3*— 3-6—3—1—1—173 
whioh sum to zero as they should. 
Thus Z{d2) = 4 + 9 + 9 + 36 + etc. 
= 128 
p - 1 - 6^= 0.224.. 
r 990 
AN ALTERNATIVE COEFFICIENT OF RANK CORRELATION 391 
This indicates some sort of concordance between the standards of the two judges, but not 
■a very strong concordance. • 
Example 16.2 
In the previous example there was no information about the " real" order of the 
competitors, and p merely served to measure the degree of agreement between judges. 
Consider, however, the following case, where an objective order is known: In a test for 
ability to distinguish shades of colour, ten discs were prepared ranging from light to dark 
Ted, and a subject was asked to arrange them in order. The true order, as determined by 
a. colorimetric method, was 
1, 2, 3, 4, 5, 6, 7, 8, 9, 10. 
The order produced by the subject was' 
4, 7, 2, 10, 3, a, 8, 1, 5, 9. 
What sort of a judge is he ? *. 
The differences are i 
- 3, - 5, 1, - 6, 2, 0, - 1, 7, 4, 1 
•and ' 2{d*) = 142, p = 0-139. 
The coefficient is low and we conclude that the observer was a poor judge. 
An Alternative Coefficient 
16.6. A second coefficient of rank correlation which has certain advantages may 
be obtained as follows : Consider again the ranking of the previous example 
472 10 368159 . . . . (16.8) 
Consider the order of the nine pairs of numbers obtained by taking the first number 4 
with each succeeding number. The first pair, 4, 7, is in the correct order (in the sequence 
1, 2, . . . 10) and we therefore allot it the score + 1- The second pair, 4, 2, is in the wrong 
order and we therefore score — 1. The nine scores will be found to be 
■+1 — 1 + 1 — 1 + 1 + 1 — 1 + 1 + 1, totalling + 3. 
.Consider next the scores of the second number 7, with its eight succeeding numbers. They 
■are 
— 1 + 1-1—1 + 1—1—1 + 1, totalling — 2. 
Proceeding thus with each number we find 9 scores as follows :— 
+ 3, - 2, + 5, - 6, + 3, 0, - 1, + 2, + l. 
The total of these scores is + 5. 
Now the maximum score obtained if the numbers are all in the objective order 1, 2, 
... 10, is 45. We therefore define the rank correlation coefficient x as the ratio of the 
actual score to the maximum score, i.e., in the present case, 
T-i-o.m, 
as compared with p== 0-139 for the Spearman coefficient. " 
Generally, if there are n individuals the maximum score, obtained if and only if they 
392 RANK CORRELATION 
are in the order (1, 2 . . . n), is (n — 1) 4- (n — 2) -f- . . . -f- l = ——-—-. Denoting 
the actual score by 8, we have then for the coefficient of rank correlation 
% = , . (16.9) 
n{n — 1) 
16.7. The actual calculation of 8 may be shortened considerably. Looking again, 
at the ranking (16.8) we see that the number 1 has two numbers on its right and seven 
on its left. We therefore score 2 — 7 = — 5 and strike out the 1. In the remaining 
ranking, the number 2 has 6 numbers on its right and two on its left, and hence we score 
6 — 2 = + 4; we then strike out the 2 and proceed with the 3.as before. It will be found 
that the scores obtained are 
- 5, + 4, + 1, + 6, - 3, 0, + 3, 0, - 1. 
The total-of these scores is- + 5, and is equal to S. The rule-is quite general. Its validity 
is evident from the consideration that instead of taking each number with its succeeding 
numbers we consider pairs contributing to 8 in a different way. Taking the number 1 
first, and remembering that all other numbers are greater than 1, we see that any number, 
on the left must contribute — 1, and any number on the right + 1, to 8. When 1 is struck 
out the procedure remains valid for 2, and so on. 
Alternatively the following procedure may be adopted. Considering again (16.8), we 
see that the number 4 has on its right 6 greater numbers, the 7 has 3 greater numbers, 
and so on, the numbers being 
6, 3, 6, 0, 4, 2, 1, 2, 1. 
totalling 25. There must therefore be 46 — 25 = 20 numbers lying to the right of successive 
numbers in the ranking which are less than those numbers, and hence S = 25 — 20 = 5 as 
before. Generally, if the number obtained by counting greater numbers is k, 
8 = 2k- n{n - 1} 
2 
4& 
and thul t = — — 1 (16.10) 
n(n — 1) 
A check may be obtained by counting greater numbers lying to the left. If the total 
of such numbers is I 
s=Mn-l) _2l 
2 
T = 1 / 4 ix ' (10.11) 
n(n — 1) N 
16.8. The extension of the use of % to the case where no objective order is given 
requires a little further consideration. Suppose we have two rankings as follows :— 
A\ A^ A% At As A$ Aj Ag Ag Ai0 
P 6-9 4 3 5102 18 7}.. . (16.12) 
Q 65 10 2397418 
AN ALTERNATIVE COEFFICIENT OF RANK CORRELATION 393 
t may be obtained by arranging one ranking in the natural order (1, 2 . . . n) thus : 
^9-8 -"-7 ■«■ 4 -"3 -"-5 -"1 -"10 -".Q -"2 Ag 
P' 1234.56 7 89 10 
(?' 472 10 368 159 
(16.13) 
and then finding t between P' and #' as in the preceding section. We have however to 
show that if we arrange Q in the natural order, giving 
\ 
^9-9 ^0.4 ^O-B -"-8 ^O-a Jl\ ^0.7 ■"■ 10 -"-6 ■"■31 
P" 8 3 5 19 6 2 7 10 4 I . . (16.14) 
#" 1 2 3 4 5 6 7 8 9 10 J 
then t between P" and Q" is the same as that between P' and #'. That this must be so 
may be seen as follows :— 
. In (16.13) the successive contributions to 8 are, as found by the method of 16.6, 
■ +3, -2, +5, -6, +3, 0, 1, +2, +1. 
Consider now the contributions to S from (16.14) when the short method of 16.7 is used. They 
will be found to be exactly the same. If the permutation Q' begins with a0 the contribution to 
Sq, from pairs involving a0 will be (n — a0) — (a0 —■ 1). In P" the a„th number will be 1 and 
the contribution to Sp„ will also be (n — a„) — (a„ — 1). If the second number in Q' is 
ax the contribution to SQ.. will be (n — ax) — {ax — 1) ± 1 according to whether ax is 
greater than a„ or not. In P" the axth number will be 2 and the contribution to SP» is 
also (n — ax) — (ax — 1) ± 1 according to whether 1 lies on the left or the right of 2 in 
P", i.e. whether at is greater than a0 or not; and so on. 
In practical calculations it is not necessary to carry out the rearrangements. Consider 
again (16.12). The number 1 in Q has an 8 above it in P. In the ranking of the ,4's 8 
has two members to the right and seven to the left. Score therefore, — 5, arid strike 
out Ae. The number 2 in Q has a 3 above it in P, and A3 has six members to its right (ignoring 
Ae) and two to its left, score + 4; and so on, the scores being 
- 5, + 4, + 1, + 6, - 3, 0, + 3, 0, - 1 
totalling + 5 which is equal to S. 
16.9. Like p, r is + 1 only if the correspondence between two rankings is perfect 
and — 1 only if the rankings are inverted. In actual practice the values given by the 
two coefficients bear a nearly constant ratio (of. 16.24) and one appears to be as good as 
the other so far as providing a measure of ranking concordance is/concerned, p is, 
however, easier to calculate and is probably the most convenient to use. Against this must be 
set certain difficulties in its sampling distribution, which will be referred to below, and the 
fact that t can be generalised to the case of partial rank correlations. 
16.10. In considering the interpretation of any particular value of p or t the question 
naturally arises, are such values significant in the statistical sense, i.e. can they have arisen 
by chance from a population in which the qualities under consideration are independent ? 
And further, can we assign a standard error to the observed values ? The second question 
is not an easy one to answer, or even to understand unless ranks are related to variate- 
values. In the sampling of variates we are given a set of n values emanating from a 
population of values. In the ranking case we are given n ordinal numbers, but it is useless 
394 RANK CORRELATION 
to ■ consider them as emanating from a population of (different) ordinal numbers. The 
point will be considered later when we introduce the concept of grades (16.25). 
The sampling problem, however, acquires a definite meaning if the two qualities under 
consideration are independent. In such a case the pairs of rankings of n members drawn 
at random are independent; and consequently in a large number of samples there will 
occur in- equal amounts every ranking according to one quality associated with every 
ranking according to the other. We are thus led to consider the distributions of p and t 
in populations consisting of all possible associations of all possible rankings. Clearly no 
generality is lost if we fix one ranking as the order (1, 2 . . . ri) and consider its correlations 
with the n\ possible permutations of those numbers. If a given p or r cannot, to an 
acceptable degree of probability, have arisen from such a population, we are justified in concluding 
that the two qualities have some definite relationship in the population. 
Sampling Distribution of Spearman's p in the Case of Independence 
16.11. Consider then the distribution of values of p in the population obtained by 
correlating the order (1, 2 . . . n) with every possible permutation of the n natural numbers. 
We shall, in fact, find it more convenient to consider the distribution of £{d2), which is 
simply related to p by equation (16.5). Certain elementary properties of .the distribution 
are obtainable immediately. 
(a) Any value*' of £{d2) must be even; for Z(d) = 0 and hence the number of odd 
values of d, and thus of d2, is even. 
(b) The possible values of £[d2) range from 0 to ^(n3 — n) and hence there are 
$(n* — n) + 1 of them. 
(c) The distribution is symmetrical, about a central value if %{n3 — n) is even, or 
about two adjacent central values if it is odd. This follows from the fact that to any value 
of p corresponding to a permutation P there will correspond a negative value of p, of the 
same absolute value, arising from P inverted. For if P ib Xl X* . . . Xn, the inverted 
n 
permutation is Xn, Xn_1, . . . Xx. 2(d2) calculated from P is then 2_. {%t ~ *)2 an(* 
n 
that from p inverted is /,(-£* — n ■+- 1 + i)2. The sum of these two is 
2W) 4 El?) -.IziXJ) +Z(Xi*) + E{n + 1 - i)2 - 2H{Xi(n + 1 - i)}. 
The first, second, fourth and fifth terms in this expression are equal to £(i2)> i.e. to fcn(n -±- 1) 
(2n + 1). The sum of the third and sixth is 
- 2{n + 1) E{X) = -n(n + l)2. 
Thus the sum of the two ^{d2) is 
f»(» -1- 1)(2» -f 1) - n{n -f l)2 
= ±(nz-n). 
Thus we see from (16.5) that'the sum of the corresponding p's is zero. 
(d) It follows that all odd moments of the distribution of £{d2) about the mean vanish. 
16.12. Consider the deviations between the order 1, 2, . . ., n and an order X. If 
one deviation is known, then certain deviations become impossible for other ranks. For 
instance, if the deviation d± between Xx and 1 is (n — 1), then Xt = n, and it is impossible 
DISTRIBUTION OP SPEARMAN'S p 395 
for the deviation between Xa and 2 to be (n — 2) ; or for the deviation between X3 and 3 
to be (n — 3), and so on. Consider then the array : 
2 10 
1 0 - 1 
0 — 1 —2 
- (n - 5) - (n - 4) - (n - 3) 
- (■» - 4) —.(» - 3) - (n - 2) 
- (w - 3) — (w - 2) — (» - 1) 
If dfc has the value in the rth row and the Mb. column, then dl cannot have the value in the 
rth row and the Zth column ; and so on. 
In fact, any permissible set of deviations is given by taking n entries from the above 
table so that no row or column contributes more than one entry.' 
Hence to get £[d2) for any permissible set, write 
n — 1 
n — 2 
n — 3 
2 
1 
0 
n 
n 
n 
m 
- 2 
- 3 
- 4 
1 
0 
- 1 
n — 3 . . 
n — 4 . . 
n — 5 . . 
• • ■ • 
0 . 
- 1 . 
-2 . 
E 
t\ 
a0 a1 a* a0 . . . a^"15 
a1 a0 a1 a4 . . . a^~2)' 
a* , ax a° a1 . . . a(u~3)' 
a(n-l)» a(n-2)' a(B-3)» a<»-4)« a0 
and £(d2) is given by the index of a of one of the terms obtained from E by choosing n 
factors so that no row or column appears more than once and multiplying them together. 
Thus the distribution of £(d*) is given by the totality of n\ terms which can be constructed 
in that way. E will be taken to be equal to the polynomial in a given by the sum of these 
terms—the so-called " permanent." 
16.13. E bears an obvious analogy to the determinant, but it cannot be regarded 
as such and expanded accordingly. If it could, the distribution of 27(da) would be obtained 
without difficulty, for a determinant with the elements of E as given above may be shown 
to be equal to 
(1 _ a2)71"1 (1 — a*)n~2 (1 - aflf-4 ... (1 — a2(n-15). 
E, in fact, lacks the fundamental property of the determinant in that it does not change 
sign if two rows or columns are interchanged. 
Nevertheless certain of the rules of determinantal algebra remain true for E. The 
most valuable is that E may be expanded in terms of its minors of any order in the usual 
way. Expansion of this type is, in fact, rather easier with E than with the determinant, 
for all terms of E are essentially positive and there are no difficulties with signs. Such 
expansions were used in obtaining the distributions given below. There are also certain 
devices which assist the expansion of E in virtue of its symmetry. Two which will be found 
useful are as follows :—■ 
(a) Any minor of E is symmetrical in powers of a, i.e. is of the form 
40a>° +Ala*-2 + A&*-* + . . . +^4am~4 + A2a™~2 + A0am. 
(b) The effect of shifting a minor bodily across E is to multiply each term of its 
expansion by a constant power of a, 
CD 
CD 
B 
a cd 
f 
00 00 00 
if*. bO O 
-J -J -J -J -J 
oo oa it*, bo o 
OS OS OS OS OS 
oo oa rf»- to O 
Ol 
oo 
Ol Ol Ol Ol 
OS if*. bO O 
if*. tf»- lf> if*, if*, 
oo os if*- bo o 
CO CO CO CO CO 
00 OS if- bO © 
bo bo bo bo bo 
oo os if*, bo © 
00 OS if*. bO © 00 OS if*- bO © 
bo 
os 
bo 
If*. 
bo 
© 
^1 
© 
Ol 
© 
© 
* 
If*. 
© 
CO 
© 
* 
h- bO © bO I-" 
CO (-■ if*. bO bO bOifvl-'COI-i 
l-< if*. CO OS ^1 OS if>-©OS©OS © OS © tf». OS ^aCC(f.M 
h-> Ol OS CD OS bO 
CDCD0D© 00 CO 00 CO <I CO <I 33 <I <I 
&CO rf*- CR> ifkOIbOOOOO I—' if*. bO !-■ 00 
mOSOlOS if*. © OS bO !-■ -J Ol bO 00 OS 
l-* bO bO bO bO 
»(*. if*. © i— CO 
CD 1—■ 1—■ 1—■ t—■ 
• pu oo if>. os if*. 
g- if*. U*. oo if*. 
B 
OS -J Ol OS Ol 
00 -J CD OS Ol 
if*. OS Cl OS ^1 
bo bo co bo co 
oo if*, if*. © io 
1—1 l_1 1—1 1—1 
^1 CO OS CD if*- 
Ol © 00 00 ^3 
OS if*- Ol CO Ol 
if*- CD if*. CD i—• 
© bo bo if*, -J 
if*. bO bO if>- CO 
bO CD CD bO bO 
1—" 1—t 1—1 1—« 
co to © to co 
if*. CO OS CD © 
if*- CO CO CO bO 
© 00 if*. -J OS 
© © CD CD if*. 
tO CO K) tO tO 
© if*, if*, oo co 
^1 CD 00 ^1 ^1 
CO © if*- © if*- 
b9t9b9HH 
^1 CO CO 00 ^1 
cs oo -j co oo 
bO bO bO l-J !-■ 
l-J © If*. If*. bO 
Ol Ol if*- CO bO 
if*, oi os oi os 
to to co <i oi 
If*. CD if*. © if*. 
(-1 
OS CD OS Ol 
bO I-" I-" 
CD if*. © OS 
If*. bO I-1 
^1 bO Ol ^1 
to 
CO 
If*. 
Ol 
os 
00 
s- 
TABLE 16.2 
jSpearman's p. Probability that £{d%) will be Attained or Exceeded for Values of n from 4 to 8 
inclusive. 
E(d»). 
n = 4 
n = 5 
n = 6 
n - 7 
n = 8 
n =■ 5 
n = 6 
n = 7 
n = 8 
H = 0 
n = 7 
n •= 8 
n = 7 
n = 8 
n = 8 
0 
30 
0-22(3 
0-043 
0-867 
0-0158 
(10 
0-008 
0-433 
0-700 
00 
0-083 
0-441 
120 
f)-150 
2 
0-058 
0-002 
0-000 
1-000 
1-000 
32 
0-173 
0-600 
0-840 
0-062 
02 
0-061 
0-420 
0-750 
02 
0-060 
0-420 
122 
0-134 
n ■= 8 
4 
0-833 
0-058 
0-002 
0-999 
1-000 
34 
0-117 
0-540 
0-823 
0-943 
04 
0-020 
0-301 
0-732 
04 
0-055 
0-397 
124. 
0-122 
160 
0-014 
6 
0-702 
0-033 
0-083 
0-007 
0-090 
36 
0-007 
0-600 
0-802 
0-034 
00 
0-017 
0-357 
0-709 
00 
0-044 
0-376 
120 
0-108 
152 
0-011 
8 
0-625 
0-883 
0-071 
0-094 
0-000 
38 
0-042 
0-400 
0-778 
0-024 
08 
0-0383 
0-331 
0-000 
98 
0-033 
0-352 
128 
0-008 
10 
0-542 
0-825 
0-040 
0-088 
0-008 
40 
0-0*83 
0-401 
0-751 
0-015 
70 
0-0*14 
0-207 
0-008 
100 
0-024 
0-332 
130 
0-085 
164 
0-0*77 
12 
0-458 
0-775 
0-032 
0-083 
0-006 
42 
0-357 
0-722 
0-002 
72 
0-278 
0-048 
102 
0-017 
0-310 
132 
0-070 
150 
0-0*54 
14 
0-375 
0-742 
0-012 
0-076 
0-005 
44 
0-329 
0-703 
0-802 
74 
0-240 
0-624 
104 
0-012 
O-201 
134 
0-060 
158 
0-0*30 
16 
0-208 
0-658 
0-870 
0-007 
0-002 
46 
0-282 
0-000 
•0-878 
70 
0-22 
2 
0-003 
100 
0-0 *( 
\-2 
0-208 
130 
0-057 
100 
0-0*23 
18 
0-167 
0-608 
0-851 
0-056 
0-080 
48 
0-240 
0-043 
0-806 
78 
0-108 
0-580 
108 
0-0*34 
0-250 
138 
0-048 
102 
3-0*11 
20 
0-042 
0-625 
0-822 
0-045 
0-980 
50 
0-210 
0-600 
0-850 
80 
0-177 
0-550 
110 
0-0*14 
0-231 
140 
0-042 
104 
0-0u57 
22 
0-475 
0-700 
0-031 
0-082 
52 
0-178 
0-580 
0-837 
82 
0-151 
0-533 
112 
0-0*20 
0-214 
142 
0-035 
24 
0-302 
0-751 
0-017 
0-077 
1— 
54 
0-140 
0-547 
0-820 
- 
84 
0-133 
0-512 
114 
0-195 
144 
0-020 
160 
0-0*20 
26 
0-342 
0-718 
0-000 
0-071 
56 
0-12\ 
0-518 
0-805 
86 
0-118 
0-488 
110 
0-180 
140 
0-023 
108 
0-0*25 
28 
0-258 
0-671 
0-882 
0-965 
58 
0-088 
0-482 
0-780 
88 
0-100 
0-407 
118 
0-103 
148 
0-018 
397 
398 
e.g. the minors 
M = ■ 
and M' = - 
are related by 
• 
" a" 
a1 
a4 
' a4 
a1 
RANK CORRELATION 
a1 
a° 
a3 
a8 
a4 
a3 
a] 
a° 
a 
a9 
a4 
1 L 
10 
= a0 + 2a2 + 2a4 + a6 
= o12(a° + 2a2 + 2a4 +"ae) 
M' = ikfa12. 
16.14. The tajbles on pp. 396-7 show the frequencies ofi7(da)for values of n from 1 to 
8 inclusive and the probabilities that a given value, of £{d2) will be attained or exceeded 
on random sampling for n from 4 to 8 inclusive. 
16.15. The distributions of Table 16.1 are peculiar in several respects. For lower 
values of n they are distinotly bimodal. For n = 7 and n = 8 the frequency polygons have 
an unusual serrated profile, that for the latter being shown in Fig. 16.1, though normality 
20 
40 
150 168 
60 80 8i 90 110 
Values of 27(d*). 
Fig. 16.1. spearman's p. Frequency Polygon, of 2(d?) for n = 8. 
is beginning to emerge. It will be shown below that as w->co the distribution tends to 
normality, but it is not immediately obvious how a serrated polygon of this kind can do so. 
DISTRIBUTION OF SPEARMAN'S p 399 
I think that the tails of the curve smooth out first, and that as n increases the smoothness 
runs up the curve towards the apex. 
16.16. The calculation of frequencies for n greater than 8 would be a tedious process 
and can be obviated by finding curves which satisfactorily approximate to the distribution, 
at least so far as its distribution function is concerned. For this purpose we will find the 
second and fourth moments of p about its mean. The first and third, of course, are zero. 
Suppose we measure the rank numbers from their mean, writing for the new variables 
x = X — \{n -f- 1), y = Y — \{n + 1). Then from 16.4 we have 
p = ™ = — E{xy), say, 
n3 — n N 
where N = ^' ~ n\ Since E(P) = /i\{p) = 0 we have 
var p = ~E(Zxy)* 
= i-2tf {£(*V)} + ^ Zizft. y&,)) . . . (16.15) 
where i ^ j. Now for any value of x, y may have any value from 1 to n. Hence 
EZ{x*y*) = 7iE{x*)E{y*) 
n 
= — ...... (16.16) 
n 
Further, in the product term of (16.15) there are n(n — 1) pairs of values i ?±j and thus 
EEiXjXj yfli) =n[n - 1) E^x, y^) 
= n(n — 1) Efaxj)* 
1 
n(n — 1) 
{Ez&y 
= , 1 .AW - Z(x*)y 
n{n — 1) 
N* 
n(n — 1) ' 
Hence, substituting from (16.16) and (16.17) in (16.15) we have 
1 , 1 
var p = —|- 
(16.17) 
n n(n — 1) 
w — 1 
By the same technique it may be shown that 
( ) = 3(25rc3 - 38rc2 - 35n + 72)- 
/MP' 25n[n + l)(w - l)3 
(16.18) 
(16.19) 
400 RANK CORRELATION 
16.17. Consider now the Type II symmetric distribution 
dF = — - — (1 - a2)V dx, — 1 < x < 1 . . (16.20) 
*(*■ ^) 
The first and third moments are, of course, zero. The second and fourth are given by 
n — 2> 
B 
?) , 
"=^Eip— • • • -(16-21) 
*- ) .l^-^T • ■ • • <16-22> 
5 
{*•B-?-2) 
The distribution thus has its first three moments the same as those of Spearman's p in the 
case of independence. The fourth moments are the same to order n~2, the difference being 
3 f 25w3 - 38ra2 — 35ft + 72^ — 36 . 
w2 — ll 25n(n — l)2. j n 
3 
i.e. of lower order in n than the moments themselves. It has therefore been suggested 
that the distribution (16.20) may be used instead of that of p to give the distribution function 
of the latter for moderate or large n. Tests on the distributions of Table 16.1 indicate 
that this is a justifiable approximation. 
For instance, when n = 8 the distribution (16.20) becomes 
and by direct integration the probability of obtaining a value of x greater than x0 in absolute 
value is 
1-t(*°-24 + j) ™ 
In comparing this with the values of the p-distributjon it is as well to make a continuity 
correction, similar to that of 12.15, to allow for the fact that the distribution of p is 
discontinuous whereas that of x is continuous. If the values of £{dz) are regarded as spread 
over a range of one unit on each side of the actual value, the range of £[d2) is increased from 
£(w3 — n) to ${n* — n) -f 2, each terminal contributing a unit. Instead of writing x = p 
we will then write 
x = l - ai7(d2.) .■ .... (16.24) 
£(w3 — n) + 1 v ' 
Now from Table 16.2 the probability of obtaining a value of p greater than \ in absolute 
value, corresponding to Z{d*) outside the range 14 to 154 inclusive, is 2 x 0-0053 = 0'0106. 
14 
The appropriate x from (16.24) is 1 — — = 0-835, and this on substitution in (16.23) gives 
the probability of 0-0098. Similarly the chance of getting a value of £{d%) outside the range 
26 to 142 inclusive is 0-0576. That given by (16.23) is 0-0561. The agreement is evidently 
good enough for most practical purposes and would, of course, improve as n increases. 
DISTRIBUTION OF SPEARMAN'S p 401 
16.18. If we put, in (16.20), 
we obtain the distribution 
**-,, ,u on ,- ,w ^-n=r. • • • (16.25) 
(n - 2)* B(l fr - 1) A + <» V-y 
the " Student " distribution of Example 10.6. If n is large the continuity correction may 
be neglected and to this approximation 
x = p, 
so that p may be tested in " Student's " distribution by writing 
t = p(j-Ejkf • .'.... (16.26) 
Example 16.3 
In Example 16.2 we found a value of p = 0-139. Is this significant ? 
We have n = 10 and from (16.26) 
8 
t 
= 0-397. 
From Appendix Table 3 we see that the chance of getting such a value or greater in 
absolute value is about 0-70 (= 2(1 — 0-65)). The value cannot therefore be regarded as 
significant. 
16.19. As n tends to infinity the Z?-distribution tends to the normal form and we 
therefore suspect that p-also tends to normality. That this is in fact so may be seen as 
follows ; the proof being due to Hotelling and Pabst (1936). 
The general moment of p of even order is given by 
/"2« = ^s E{xlVl + . . . xnyn)*« . (16.27) 
n n 
where 82 is written for 5J xt2 and generally Sp for 2_, xip- When the parenthesis is expanded 
we may, in virtue of the independence of x and y, take expectations term by term, regarding 
the #'s as constant. Now 
W"1 Vs) = n{n \_ 1} ^i2a Vt), etc. 
▲.8. DD 
402 RANK CORRELATION 
Hence 
"* =^{^^s+^)(r^1^)S+«li^^)(2'^"!a!'a!')a + etcj (16'28) 
where the coefficients A depend on a but not on n. We proceed to show that the term 
of greatest degree in % in (16.28) is the term Ifo" xf . . . xp2). 
The numerator of any term in (16.28), being a symmetric function of tile x% can be 
expressed in terms of the symmetric sums .Sp. Further Sp vanishes if p is odd. Since 
any Sk is of degree k + 1 in n, the degree of a non-vanishing term Sai SUl . . . Sap is 
T^oj + 1) = 2a + p. Consequently the term of highest degree in n must contain as 
high &p as possible, that is to say as many S's as possible, subject to the requirement that 
the subscript of each S must be even. 
Now consider a term 
Z{x^ . . . xp«p) = Zc0SaiSai ...£«, + Z^S^+^S^ . . . S^ 
+ ZCiSai+at+tttSai ...£«„+..., etc. . . (16.29) 
If the a's are all even the term of highest degree on the right is, as just remarked> 
2a -\-p. If the a's are not all even, suppose there are m even ones and 2q odd oncR 
(m -f- 2q = p). Then the first term in (16.29) Vanishes and the term of highest degree 
which does not vanish must be obtained by grouping q pairs of odd a's, and henco is of 
degree 2a + m + q = 2a- + p — q% 
Now in (16.28) the degree of the denominator in each term is the number of different 
x's in the numerator. Thus the term of highest degree in a; is of degree 
2(2a + p — q) — (m + 2q) = 4a — m + 2p — 4q 
= 4a + m. 
This will be a maximum when m is a maximum and therefore when q is zero, in which case 
m = a. Hence the greatest degree in % in (16.28) arises from the term H(x^ xf . . . .r a) 
as stated. Now in the expansion of 
(Zi2/i+ . . . XnVn)* 
the coefficient of x\ . . . x2 y\ . . . y2 is, by the multinomial theorem, *2-^- and hence 
2a 
_ 1 (2a)i {Z(x* . . . s.»)}» ■ 
I ^2* ~Sf*~& ^ .... (16.30) 
The term of highest degree in n in Z(x\ . . . xa2) is that in S2«, the coefficient of which is 
evidently the reciprocal of that of Z(x\ . . . x2) in 
■ s2« = (x2+ ... xn2r, 
i.e. = al 
Thus, from (16.30), 
^2a 2-.alV+ Vn»+V/ 
DISTRIBUTION OF t 
403 
Now u2 = and thus 
n — 1 
£=->-2^ (16.31) 
//2a 2aa! 
i.e. to the moments of the normal distribution, of unit variance. It follows from the Second 
Limit Theorem of 4.24 that the distribution of p tends to normality. The tendency is not, 
however, very rapid and we have already noticed the peculiar character of the distribution 
for lower n. 
Distribution of Tin the Case of Independence 
16.20. We now consider the distribution of the coefficient r under similar conditions, 
that is to say in a population obtained by correlating a given ranking with all the n\ possible 
rankings. 
Consider a given ranking of the numbers 1,2, . . . n and the effect of inserting an 
additional number (n + 1) in the various possible places in the ranking, from the first place 
(preceding the first number) to the last place (following the last number). 
Inserting a number at the beginning will add — n to the value of 8 of equation (16.9). 
Inserting it between the first and second will add — (n — 2) to 8 ; and so on. Thus to 
any frequency-distribution of 8 for given n, say f(8, n), there will correspond frequencies 
f{8 — n, n)} f(8 — (n — 2), n) . . . f(8 + n} n), the sum of which gives f(S, n + 1). If the 
frequency of a given 8 is the coefficient of Xs in a polynomial P(x), then the corresponding 
values of 8 in the frequency for (n + 1) are the coefficients of 
(x~n + ar<n-2> + . . . + xn~2 4- xn)P(x). 
But the frequency-distribution of 8 when n = 2 is given by x~x + xx, there being one 
value 8 = — 1 and one value 8 = 1. Thus the frequencies of S for rankings of n are the 
coefficients of x8 in the array , 
f = (z-i + x){x~2 + 1 + x*){x~* + x-1 + x1 + xA) .. . . (x-^-V + a;~(n-3) 
+ . . . + z(n-3) -+■ a*"-") . . . (16.32) 
It follows that the distribution of 8, and hence that of t, is symmetrical about zero. 
The values of 8 are either all odd or all even, according to whether AJ 1 is odd or even. 
The actual frequencies may be calculated by a figurate triangle, as follows:— 
Valuo of n Frequencies of 8 
1 1 
2 11 
3 12 2 1 
4 13505 3 1 
5 1 4 9 15 20 22 20 15 9 4 1 
(16.33) 
In this array a number in the rth row is the sum of the r numbers above it and the (r — 1) 
numbers to the left' of that number. A little reflection will show that-this rule follows 
from (16.32). The formation of the array is quite simple and several devices shorten the 
arithmetic. For instance, in part of the array towards the left a number in the rth row is . 
the sum of the number immediately above it and the number immediately to the left. 
The array is symmetrical and the total in the rth row is r\ 
404 
RANK CORRELATION 
The following tables show the frequency-distribution of 8 for values of n from 1 to 10 
inclusive and the probability that a value of J3 will be attained or exceeded. 
TABLE 16.3 
Bank Coefficient x. Distribution- of S for Values of n from 1 to 10 (only the Positive Half 
of the Symmetrical Distribution shovm). 
s 
0 
, 2 
4 
6 
8 
10 
12 
14 
16 
18 
20 
22 
24 
26 
28 
30 
32 
34 
36 
1 
1 
- 
4 
6 
5 
3 
1 
. 
Values of n 
5 
22 
20 
15 
9 
4 
1 
8 
3,836 
3,736 
3,450 
3,017 
2,493 
1,940 
1,415 
961 
602 
343 
174 
76 
27 
7 
1 
■ 
9 
29,228 
28,675 
27,073 
24,584 
21,450 
17,957 
14,395 
11,021 
8,031 
, 5,545 
3,606 
2,191 
1,230 
628 
285 
111 
35 
8 
1 
s 
1 
3 
5 
7 
9 
11 
13 
15 
17 
19 
21 
23 
25 
27 
29 
31 
33 
35 
37 
39 
41 
43 
45 
Values 
^ 
2 
1 
■ 
3 
2 
1 
1 
. 
6 
101 
90 
71 
49 
29 
14 
5 
1 
of n 
7 
573 
531 
455 
359 
259 
169 
98 
49 
20 
6 
1 
10 
250,749 
243,694 
230,131 
211,089 
187,959 
162,337 
135,853 
110,010 
86,054 
" 64,889 
47,043 
32,683 
21,670 
13,640 
8,095 
4,489 
2,298 
1,068 
440 
155 
44 
9 
1 
16.21. As may be seen by comparing Tables 16.1 and 16.3, the distribution of 8, 
and henoe that of t, is much smoother than that of E{dz) and p. We show below that it 
tends to normality, and in fact the tendency is so rapid that for values of n greater than 
10 the normal distribution provides an adequate approximation. We proceed to find the 
second and fourth moment of the distribution. 
If we differentiate the. expression /in (16.32) and equate a; to 1 we evidently obtain 
the first moment of S; and generally, writing 6 for the operator x—, 
ox 
nip, = (d*f)x=1 (16.34) 
For example, when r = 1 we have 
»l iix = (- 1 + 1)(1 + 1 + 1) . . , (1 + 1 + . . . + 1) 
+ (1 + 1)(_ 2 + 2)( . . . ) 
+ etc. ■ 
DISTRIBUTION OP t 
TABLE 16.4 
405- 
Probability that S attains or exceeds a Specified Value. (Shown only for Positive Values. 
Negative Values obtainable by Symmetry.) 
s 
0 
2 
4 
6 
8 
10 
12 
14 
16 
18 
20 
22 
24 
26 
28 
30 
32 
34 
36 
Values of n 
4 
0-625 
0-375 
0-167 
0042 
5 
0-592 
0-408 
0-242 
0-117 
0042 
0-0B83 
8 
0-548 
0-452 
0-360 
0-274 
0-199 
0138 
0-089 
0-054 
0031 
0016 
0-0B71 
0-0228 
00387 
00319 
0-0*25 
9 
0-540 
0-460 
0-381 
0-306 
0-238 
0-179 
0-130 
0090 
0-060 
0038 
0-022 
0012 
0-0 263 
0-0 229 
0-0212 
0-0a43 
00312 
0Q*26 
00528 
s 
1 
3 
5 
7 
9 
11 
13 ' 
16 
17 
19 
21 
23 
25 
27 
29 
31 
33 
36 
37 
39 
41 
43 
45 
6 
0-500 
0-360 
0-235 
0-136 
0068 
0-028 
00283 
0-0»14 
n 
Values of n 
7 
0-500 
0-386 
0-281 
0-191 
0-119 
0-068 
0-035 
0015 
0-0a54 
0-0214 
00320 
i 
10 
0-500 > 
0-431 
0-364 
0-300 
0-242 
0-190 
0146 
0108 
0-078 
0-054 
0036 
0023 
0-0H4 
0-0283 
0-0*46 
0-0223 
00211 
0-0347 
0-0H8 
0-0*58 
00*15 
0-0B28 
00928 
When r =*= 2 the operation on / will result in two types of terms, those in wliich 'both 
operations operate on one factor of/ and those in which the operations operate on separate 
factors. When x = 1 these last vanish and thus 
n 
1„. - (1 + 1,£ + (3- + »■)£ + ■(,, + 1'V' + ^ 
+ 
(» - l2 + n - 32 + .... + n - 3a + n - 12)%! 
n 
A*i = f-11+|-2l + |(li + 31) + 
. . + -In - la + n - 3a + . . .) 
n 
This may be summed by the ordinary methods of elementary algebra, and we find 
_n(n - 1)(2* +5) 
In a like manner it appears that 
A*4 = ^T-l)jl + £(n - 2) + ^(n - 2)(n - 3) + ??(n - 2)(n - 3)(» - 4) 
+ ^(n - 2)(n - 3)(» - 4)(n - 5) 
. (16.36) 
406 RANK CORRELATION 
• 16.22. To prove that the1 distribution of -t tends to normality as n —> oo we shall 
show that 
Consider the effect of operating on/in (16.32) by 6 2a times and then putting x = 1. • There 
will appear terms like 
J>2" + (f - 2)2* +■ . . . + (r - 2)2* + r2"] 
, Ti!r-y2-x-i-(y-2)2«-^ . . . +(r-2)2*-1+r2"-n f-g-(*-2) + '. . . (f,+ 2) + *\ 
etc. Any term with an odd superscript vanishes. Consider now the sum of terms like 
»i{f2+-;-+r'}{"+ ■;•+''} ... {■'+-;-+1"}' . ue.*) 
It will be shown below that this term contributes the greatest power of n to the sum 
giving n\ //2a. 
In virtue of the multinomial form of Leibniz' theorem on the differentiation of a product, 
the factor by which this term is multiplied in the expansion of 02a/ is 
(2a)l __ (2k)! • : 
2! ... 2! 2* 
Hence p2a ~^r {Sum of terms like (16.37)} . .' . (16.38) 
1 r2 
Each of these terms is of type —{r2 -f- (r — 2)2 -f . . . (r — 2)a + r2} i.e. is of order —. 
The sum will then tend to the sum of terms like — (12.22 . . . a2), each term containing 
a squares of the numbers 1, 2 ... n — 1. Call this 7ta. 
1 
Then tu is-: times the sum of terms in 
a! 
-{I2 + 22 + . . . n - l2}* .... (16.39) 
which contain a different factors. 
\ Now (16.38) is of order — /-^ pf-. Hence if na tends to equality with the sum (16.39) 
Pi* 
al 
and henoe, from (16.38) 
(2a)! W 
that (16.38) 
that sums of terms like 
2* a! 
We have then to show that (16.38) tends asymptotically to the sum of its terms a! 7ta, i.e. 
' 1*.22 ... (a - l)2, lfl.22 ... (a - 2)2 
tend in comparison to zero. This may be shown inductively. Consider first of all 
{1« + 2» + , . . (n — l)2}2 = 2ret -h 1* + 2* + . . . (w - 1*). 
DISTRIBUTION OF t 
407 
m6 m5i 
The expression on the left ^ —. But the sum of fourth powers on the right ^ —, which 
is of lower order. Hence the sum on the right ~ 2jt2. We then have 
(ia + 22 +...(« - I)2}* ~ 27r2{l2 + . . . {n - l)2} 
^ 6^3 + terms of type l4 . 22. 
These terms will be less in sum than 
2{12 + 22 + . . . (n - 1)2}{14 + 2* -{-... (n - 1)*} 
which <^2.—.—, of degree 8. But the expression on the left is of degree 9. Hence 
o O 
{la + 22 + . ,_ . (n _ l)2}3 ^ 63r3j an(i so on- 
We can now justify the assertion that the maximum power of n arises from terms like 
{12.22 . . . a2). In fact, by a' similar line of reasoning to that just given it will appear 
that sums of terms like (14.22 . . . (a — l)2) are of lower degree in n. This completes 
the demonstration. 
16.23. In using the normal distribution to approximate to the jtf-distribution it is 
•desirable to make a correction for continuity by subtracting unity (half the interval) from 
JS in order to obtain the probability that a given value will be attained or exceeded. For 
instance, when n = ,9 we have from (16.35) 
var is — = 92. 
IS 
19 
The normal deviate corresponding to S = 20 is then —— 3= 1-981. The probability 
of a normal deviate as great as or greater than this is 0-0238. The value from Table 16.4 
is 0-022. Had we made no correction for continuity we should have found a normal deviate 
of 2-085 with a probability of 0-0185. 
Example 16.4 
In 16.6 we found for a certain ranking of 10, t = 0-111, S = 5. The Spearman 
■coefficient for the same ranking, 0-139, has already been seen to be non-significant. What 
conclusion should we reach about t on this point? 
From Table 16.4 it is seen that the probability of a deviation greater than or equal to 
5 is 0-364, and that of a deviation greater than or equal to 5 in absolute value is then 
0-73 approximately. The corresponding value for p is 0-70. In either case the coefficient 
could well have arisen from an " independent " population and is not significant. 
16.24. Different as p and t are in conception and method of calculation, they are very 
•closely related. Cogent reasons (but not a rigorous proof) have been given for belief in the 
validity of the equation (for the population in which all rankings occur equally frequently)— 
cov (£, Z{d*)) = - ^n(n + l)2(ra - 1) 
from which the product-moment correlation between p and t is 
*<» + !> -1-i (16.40) 
V{2w(2n + 5)} 4:n 
408 
RANK CORRELATION 
(Kendall and others, 1938). For values of n occurring in practice the correlation between 
p and t is thus very high. It also appears that the regression of p on t is approximately 
linear over the material part of the range, that is, unless both are very close to unity ^ In 
such a case, recalling the values of the variances of the two coefficients, we shall have 
_ / 1 /18n(n - 1) 
P ~ a/to-ZTT^/ 4(2w + 5) 
3t. 
2' 
so that t will be about two-thirds of the value of p when n is large. 
Grades 
16.25. Up to this point we have considered the problem of rank correlation without 
reference to any variate system which might underlie the rankings. In certain classes of 
inquiry this is inevitable ; for example, we might shuffle a pack of cards and use the rank 
correlation between the orders before and after shuffling to measure the efficacy of tho 
process of mixing. The early theory of rank correlation was, however, developed from 
rather a different view-point. The qualities,considered were measurable, and always in 
theory (and often in practice) it was possible to find a product-moment coefficient of 
correlation. . The use of Spearman's p was regarded as a substitute for such a coefficient, 
suitable either because the necessary measurements could not be carried out, whereas the 
ranking could, or because time was saved in working out rank correlations. 
It is not immediately evident what meaning can be attached to ranking in a continuous 
population, for the members thereof are not denumerable. 
The remark of 16.5 offers one way of overcoming the difficulty. The ranking of an 
individual as r can be regarded as a numerical statement to the effect that there are (r — I) 
members " above " that individual, that is to say (r — 1) members who are given precedence. 
Quantities have already been considered in connection with continuous populations which 
express the same idea, namely, the quantiles. The pth. decile, for example, is the variato- 
value such that p tenths of the total frequency lie below it. We will then define the grade 
of an individual as the proportion of the total frequency with a lower variate-value than 
that borne by that individual. If we have a discontinuous population N in number, tho 
grade of an individual ranked according to the variate-values as r (from the lower to the 
(r — 1) 
higher values) will be —«■—• If the population is continuous its members cannot be 
ranked ; but if we choose a sample of n members and rank them, an estimate of the grade 
of the rth member may be obtained by assuming that one-half of that member is to be 
assigned to each of the ranges into which its variate-value divides the variate-range, so that 
its grade is then taken to be 
{(r-!)+£} = (f - 1)_ 
n n 
16.26. For a continuous bivariate population there will be no rank correlation, but 
there will, in general, be a grade correlation. Consider the bivariate normal population 
whose frequency function is 
GRADES 409 
where, to avoid- confusion with Spearman's p, we have denoted the product-moment 
coefficient by p\ 
Let 
£ = r [ zdxdy = —tjx—, f e-**' dx 
f°° fw 1 p 
i? = zdydx = . e-*w <fy 
J -00 J -00 V (2W) J -00 J 
. (16.42) 
Then £ and ?? are the grades and if a; and y are independent so are £ and ??. £ is a function 
of x and is distributed in the form 
dF(£) = d£, 0 < £ < 1 . * . . . . (16.43) 
and similarly for rj. Thus'the mean and variance of both £ and rj are £ and -^ respectively. 
For the Spearman coefficient between £ and 97 we may then take . • 
/.CO /.CO 
p = 12 fyzdxdy (16.44) 
J —00 J —00 
remembering, however, that this is a generalisation of p to grades. Prom (16.44) we then 
have 
£-"]"-!>&**• 
1 
Now log z = - ^tt——jySx1 — 2p'xy + y2) — £ log (1 — p'2) — constant. 
Thus l|i = 7,-A^2 - V** + 2/2) + r-^-7-a + P' 
z dp' (I - p'2)2V r * ' " ' ' 1 - p'a ' 1 - p'a 
_ l dh ' 
z da; 3?/ 
and hence ^ = 121 £« -——dxdy. 
dp J -00 J -00 ete <^/ 
By a partial integration with respect to x this is equal to 
»j>[*ar..-»r-.*j>!P* 
The first term vanishes and thus 
By a partial integration with respect to y we find 
dp J -co J -00 fy 3* 
whenoe/from (16.42) 
» . 12 r r ,„ r <2-p*)*' -v*y + <2-p-a)yaiixfa_ « 
410 RANK CORRELATION 
Integrating we have, since p vanishes with p\ 
p = - sin-1 £_ 
y n 2 
or p' = 2sin^ (16.45) 
6 
i 
16.27. This formula is due to K. Pearson, but its value is problematical. It represents 
the relationship between the produot-moment and the grade correlations when the variates 
are normal. It has, however, been used to transform a rank correlation obtained from 
a small sample of n values into a putative produot-moment coefficient in that sample, or 
even worse, in the population from whioh the sample is derived, whether normal or not. 
The reader may care to list for himself the assumptions made in adopting such a procedure 
and to reflect on their justification. We shall not notice the process again, but we may 
note that in no case is p very different from 2 sin -£ in numerical value. If p = 0-6, 
2 sin — = 0-618, and this is about the greatest difference that can ooour. 
« 
16.28. Equation (16.45) has also been advocated as an easy, though perhaps 
inacourate, method of calculating a product-moment coefficient. The idea is that when 
a set of bivariate values is given they shall be replaoed by ranks, the rank coefficient 
calculated, and the value of p derived from (16.45). Apart from the theoretical objections, 
suoh a prooedure involves no saving of labour if the number of values is greater than 30 or 40. 
Various formulae have been offered for the standard error of an estimate of the parent 
produot-moment correlation based on (16.45). Some of those in current statistical 
textbooks are inoorrect, and it may be doubted whether the use of any one is justified. The 
reader may consult Eells (1929) for a list of these formulae. 
The Case of m Rankings 
16.29. We now oonsider the more general case in which there are m rankings of n 
instead of two. Our problem is to discuss the general agreement among the set of m. 
It is natural in the first instance to consider the average p or t in the ( j possible 
pairs which can be chosen from the set of m. For example, if we have three rankings of 
six as follows : — 
P 5 4 1 6 3 2 
Q 2 3 1 5 6 4 
£416 32 5. . . . (16.46) 
' • 
the Spearman p's between PQ, QR and RP respectively are ±§; — li, — if-, so that the 
average p, say p^, is equal to — -fa = — 0-26. We shall consider a slightly different 
■coefficient linearly related to pm. 
Suppose we sum the ranks in the columns of (16.46), obtaining the numbers 
11 8 8 14 11 11. 
THE CASE OP m RANKINGS 411 
These numbers must sum to 63 (and in general to mn\n + )\ g^ reflect the degree, of 
resemblanoe among the rankings. If the concordance were perfect the sums would be 
3, 6, 9, 12, 15, 18, though not necessarily, of course, in that order, and in such a case would 
be as different as possible. On the other hand, when there is little or no resemblance, as 
in the example given, the sums are approximately equal. It is thus natural to take the 
variance of these sums as providing a measure of the ranking concordance. 
Let 8 be the sum of the squares of deviations from the mean '. If the.con- 
cordance is perfect the sums are m, 2m, . . . nm and the sum 8 is ——^- -. Write then 
W = „ l^S . (16.47) 
m2(w3 — n) v ' 
Then W may vary from 0 to 1 and we shall call it the coefficient of concordance. In the 
above example it will be found that 8 = 25-5, W = 0-16. 
16.30. W is connected with Pav by the relation 
mW - 1 
Pav = _ 1 (16.48) 
In fact, if the rankings, measured from the mean £(n + 1), are xlx xlt . . . xin, xtl . . . x2n, 
- . . , xml . . . xmn, the average p is 
m ?i 
1 12 vS vS , 
^rry^rzr^ 2j 2jXiiXu> % *h ' • • • (16-49> 
m(m — 1) .. 
n . m . „ n m 
mini - l)(w3 - n) \2j \2j Xij) 2a 2j Xy 
12 f„ n*-n 
8 — m. —— 
ra(m — l)(nz — n)[ 12 J 
^mW - 1 
m — 1 
pav is the intra-class correlation for the m sets of ranks considered as variate-values. It 
, , , - 1 
cannot be less than -: rr- 
(m - 1) 
16.31. To test whether an observed value of W is significant it is necessary to consider 
the distribution of W (or, more conveniently, of S) in the population obtained by permuting 
the n ranks in all possible ways in each of the m rankings. No generality is lost in supposing 
one ranking fixed and the others will then give rise to (w!)m_1 values of 8. We will ascertain 
the distributions for some low values of n and m and show how to approximate for larger 
values by the use of a continuous distribution- 
412 
RANK CORRELATION 
For the case m = 2 the distribution of S is that of Z in Table 16.1. The distributions 
have also been found for n = 3, m = 2 to 10 ; n = 4, m = 2 to 6 ; and % = 5, m = 3. 
Tables 16.5 to 16.8 give the probabilities based on these distributions in a form analogous 
to Tables 16.2 and 16.4. 
TABLE 16.5 
Concordance Coefficient W. Probability that a given Value of S will be Attained or Exceeded 
for n = 3 and Values of m from 2 to 10. 
Values of m 
s 
0 
2 
6 
8 
14 
IS 
24 
26 
32 
38 
42 
50 
54 
56 
62 
72 
74 
78 
86 
96 
98 
104 
114 
122 
126 
128 
134 
146 
150 
152 
158 
162 
168 
182 
200 
2 
1-000 
0-833 
0-500 
0167 
3 
1-000 
0-944 
0-528 
0-361 
0-194 
0-028 
4 
1-000 
0-931 
0-653 
0-431 
0-273 
0125 
0069 
0-042 
0-0046 
/ 
t 
5 
1-000 
0-954 
0-691 
0-522 
0-367 
0-182 
0124 
0093 
0039 
0-024 
0-0085 
0-0377 
6 
k 1-000 
0-956 
0-740 
0-570 
0-430 
0-252 
0-184 
0-142 
, 0-072 
0-052 
0-029 
0012 
0-0081 
0-0055 
0-0017 
00313 
1 
7 
1-000 
0-964 
0-768 
0-620 
0-486 
0-305 
0-237 
0192 
0-112 
0085 
0-051 
0-027 
0-021 
0016 
0-0084 
00036 
0-0027 
0-0012 
00332 
0-0332 
0-0421 
8 
1-000 
0-967 
0-794 
0-654 
0-531 
0-355 
0-285 
0-236 
0-149 
0-120 
0079 
0-047 
0-038 
0-030 
0-018 
0-009& 
0-0080 
0-0048 
0-0024 
00011 
0-0386 
0-0326 
0-0*61 
0-0461 
0-0461 
0-0536 
9 
1000 
0-971 
0-814 
0-685 
0-569 
0-398 
0-328 
0-278 
0-187 
0154 
0-107 
0069 
0-057 
0-048 
0031 
0019 
0016 
0010 
0-0060 
00035 
00029 
00013 
0-0366 
00335 
00320 
0-0497 
0-0454 
00411 
00411 
00411 
0-04ll 
0-0660 
10 
1-000 
0-974 
0-830 
0-710 
0-601 
0-436 
0-368 
0-316 
0-222 
0-187 
0135 
0 092 
0-078 
0-066 
0-046 
0-030 
0-026 
0-018 
0-012 
0-0075 
0-0063 
00034 
0-0020 
00013 
0-0383 
00351 
0-0337 
0-0318 
00311 
0-0485 
0-0444 
0-0420 
0-04ll 
0-0B21 
0-0799 
\ 
THE CASE OF m RANKINGS 413 
TABLE ]6.6 
Concordance Coefficient W. Probability that a given Value of 8 will be Attained or Exceeded 
for n = 4 and ra = 3 and 5. 
8 
1 
3 
5 
9 
11 
13 
17 
19 
21 
25 
27 
29 
33 
35 
37 
41 
43 
45 
49 
51 
53 
57 
59 
m = 3 
1000 
0-958 
0-910 
0-727 
0-608 
0-524 
0-446 
0342 
0-300 
0-207 
0175 
0-148 
0075 
0-054 
0-033 
0-017 
00017 
0-0017 
m = 5 
1000 
0-975 
0-944 
0-857 
0-771 
0-709 
0-652 
0-561 
0-521 
0-445 
0-408 
0-372 
0-298 
0-260 
0-226 
' 0-210 
0-162 
0-141 
0123 
0107 
0-093 
0075 
0-067 
£ 
61 
65 
67 
69 
73 
75 
77 
81 
83 
85 
89 
91 
93 
97 
99 
101 
105 
107 
109 
113 
117 
125 
m = 5 
0-055 
0-044 
0034 
0-031 
0-023 
0-020 
0017 
0-012 
0-0087 
0-0067 
0-0055 
00031 
0-0023 
0-0018 
0-0016 
00014 
00364 
00333 
0-032l 
0-0314 
0-0*48 
0-0530 
Cq 
OOOOOOOOOOh 
iP-050<ICn*»-tO«OCOOiO 
2 
to 
© 
OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOh-' 
OOOOOOQOOOOOOOOOOOOi-'i-'i-'i-'t«t«COCOCO*«.OlOl 
^J!J!^OOOOOHHHOS050lOiO<(OOfrOl!OOlfkMOlCOMO».. _ _ 
l^.aooneo 
2 
II 
i 
©©©ppppppp©©p©©©©©p©p©p©©p©p©p©pppppp©i-' 
d6666666oHHHHHHHiiti»i!bwwwwi^6i6i6csiiffi<^Mdb®®<(56 
wi(>.i^oi©aj<i(JocoOi-jb5C3ioi«o«oi-|woi<iHW<icowHiKioa<i<icoit»-<iifk-oi«oo 
-siHwaoawco©(»(f>-<ioiW(fk<iooooiO"JcooiciHK)Htf».«oco©K)tO(f>-tf».o-Joio 
Cft 
J>3 
S3 
OS 
C0<I0>aiSi0lCl0l0l|^i(>-I^^WWUUWt9(9(9KiHHHHOOOOO(0(Cffi(00CIX»I10 
Cq 
o 
THE CASE OF m RANKINGS 
TABLE 16.8 
415 
Concordance Coefficient W. Probability that a given Value of S will be Attained or Exceeded 
for n = 5 and m = 3. 
s 
0 
2 
4 
. 6 
S 
10 
12 
11 . 
18 
20 
22 
24 
26 
28 
30 
32 
■ 34 
36 
38 
40 
42 
m = 3 
1-000 
1000 
0-988 
0-972 
0-941 
0-914 
0-845 
0-83L 
0-768 
0-720 
0-682 
0-649 
0-595 
0-559 
0-493 
0-475 
0-432 
0-406 
0-347 
0-326 
0-291 
0-253 
8 
44 
46 
48 
50 
52 
54 
56 
58 
60 
62 
64 
66 
68 
70 
72 
74 
76 
78 
80 
82 
86 
90 
m = 3 
■ 0-236 
0=213 
0-172 
0163 
0-127 » 
0117 
0096 
0-080 
0063 
0-056 
0-045 
0038 
0-028 
0-026 
0-017 . 
0-015 
0-0078 . 
00053 
0-0040 
0-0028 
0-0390 
0-0*69 
16.32. These distributions may be obtained by two methods. The first consists 
of building up the distribution for (m -j- 1) and n from that for m and n. For example, 
with m = 2 and n = 3 we have the following values of the sums of ranks, measured about 
their mean:— 
Type Frequency 
- 2 0 2 1 
- 2 1 1 2 
-10<1 2 
00 0 1 
Here — 2, 1, 1, and 2, — 1, — 1 are taken to be identical types, for they give the same 
value of S and will also give similar types when we proceed to m = 3 as follows. 
In the case m = 3, each of the above type will appear added to the six permutations 
of _ *fo, 1 ; e.g. the type — 2,0,2 will give one each of — 3, 0, 3 ; — 3, 1, 2 ; — 2, - 1, 3 ; 
— 2, 1, 1 ; —1,—1,2; and — 1, 0, 1. These types are then counted for eaoh of the 
four basic types of m = 2 and we get:— 
- 3 
- 3 
- 2 
— 2 
- 1 
0 
Type 
0, 
1 
0 
1 
0 
0 
3 
2 
2 
1 
1 
0 
Frequency 
1 
6 
6' 
6 
15 
2 
416 
RANK CORRELATION 
The case m = 4 is treated by considering the- numbers of types obtained by adding 
the six permutations of — 1, 0, 1 to the types for m = 3 ; and so on. 
This method is quite convenient for n = 2 and n = 3. For re = 4 it becomes difficult 
owing to the labour of considering 24 permutations at each stage and to the increase in 
the number of types. For n = 5 there are 120 permutations and the labour become? 
excessive. 
The second method is a generalisation of the .©-function of 16.12. For m 
rankings, the distribution of S is given by the expansion of an m-dimensional 2&-funotion. 
For example, with m = 3 there would be a three-dimensional .©-function the bottom plane 
of which would be 
3(n+i)i a 
at 
3(7i+i)i a 
2 I 
r4_«s+ini /s+b_wm±>ii 
a1- J . . . a L "J 
L+2-^p)U (s+._«GMi>li 
a1 - J aL J . . . 
The plane above this would be 
■ffl'... .H-'n 
hn + 2 
a l 
3(iH-l)1 
a J 
jn+3_3C"±i)|2 |2n+3_3in+l)J» 
and so on. 
The ^-function is 'difficult to handle in more than three dimensions, but for the two- 
and three-dimensional case it is manageable and was used to obtain the distribution of 
S for n = 5 and m = 3. 
16.33. We now proceed to find the first four moments of the distribution of S. The 
method is similar to that used for p but is somewhat more complicated. 
Writing xif for the deviation from the mean -——' of the jth member of the tth 
ranking, we have, as in 16.30, 
m m Hav 
,1,1 12 A A, 
i.ft-l 7=1 
Write 
n, 
/-i 
X 
'kj 
(16.50) 
(16.51) 
mhTetulTphaVeTf Vfr &omltQm ^ thus any term ^ appears again as RR 
m the sum ER* Then the moments of W are derivable from those of the B% which 
THE CASE OF m RANKINGS 417 
in turn are derivable from those of Spearman's p. In fact, writing N = -———- we have 
from (16.18) and (16.19) - 
E{Rik) = 0, E(R^) = 0 
E(R^) = N* —?— 
TO - 1 
. (16.52) 
E(V) - W{ 25^ + 1)(» - 1)3 ]J 
We next require the moments of 
pav = m{m - \)NIj ik 
but complications arise because in some cases the J?'s are correlated among themselves. 
Any two J?'s are independent, i.e. 
E{Blk Blm)=0, (16.53) 
unless of course i = I, k = m. This may be seen by reference to (16.51), the x'b being 
independent. Similarly 
E(Bik Rlm Rnv) = °> 
except when we have a set with " circular " suffixes such as 
E(Bik Rui Ru)> (16.54) 
for in this case the a;'s cease to be independent. Similarly any four R'a are independent 
unless they form a set such as 
R~ik Bkl Rim Rmi- (16.55) 
We have 
(n n n * 
Xia, XIc<x / t xIeP Xip / t xly Xiy J 
a=l /S=l y=l ' 
= E [ {ExjJ x^x^ + E xkoL XwiXfr xv -f xv xla)} {E xly xiy}] 
= E [ {E{xjJ) Ex^x^} + E(xk0L xkp) {Ex^ xl(X - Exia Ex^ }] x [E xly xiy] 
= {E(x^) - E(xkaxkp)} EiEx^x^ 
= n{-+ , * 1V^» 
n n(n — l)j n 
{n - l)2 
We then have 
(16.56) 
1 wjy} 
m(m — 1)N 
= 0 (16.57) 
A.S. B B 
418 RANK CORRELATION 
m*(m - l)*ti*E^*** +ZRici%ih + ZBuBJ 
1 .mzSBa*) 
m\m — 1)2N2 
2 / n -^a 
= ^.m(m — 1) 
2 i 1 _ ■ 
wi(m — l)"ra — 1 
all other terms vanishing, 
(16.58) 
_8m(m- l)(wi - 2) 
~ m3(m-l)3iV3 E(RVcR*iRn) 
_ 8(m-2) 1 , . 
~ma(m-l)a>-l)2' \ ' ( 0-5l) 
From these results we have, for the first three moments of TF, 
u\ (about 0)=— (1G.60) 
m 
_ 2(m - 1) 
^"^"(n-l) (16'61) 
_'8(m-l)(m-2) 
^3 m-(n - 1)» ' " " , " (16-62) 
In a similar way—we omit the lengthy algebra—it may be shown that 
_ 24(m —• 1) T25n3 - 38?t2 - 35rc + 72 
^* ~m7(?i - l)a\ 25(n3 - n) 
+ 2(n - l)(m - 2) + }(n + 3)(m - 2)(m - 3)} . . . (16.63) 
16.34. The distribution of W is evidently asymmetrical since fa^O unless m = 2. 
Consider then the possibility of approximating to the distribution by the Type I form 
dF = rSJL^WP'i(i-Wy-1dWt 0<W<1 . .(16.64) 
The first two moments of this are 
°> = ^ ) 
. (16.65) 
u\ (about 0) = ——— 
a Pq 
(P + q)2ip. + q + 1) 
1) J 
THE CASE OF m RANKINGS 419 
Identifying the values of (16.60), (16.61) and (16.65), we find 
P _ 1 
p + q m 
pq _ 2(m — 1) 
(P + qY{p +q + l) m3(» - 1) 
giving 
p = }(n - 1) - - 
m (..... (16.66) 
q = (m - 1) ji(n - 1) - i} 
It will be found that the third moment about the mean of the Type I form is 
8(m - l)(m - 2) _ 8(m - l)(m — 2) f 2 
} 
m4(n — l)(mn + m — 2) mB(7i — 1) \ m(n — 1) + 2J 
so that the third moments of the W-distribution and the Type I distribution are 
approximately equal if m and n are not small. Similarly the fourth moments will be found to 
differ by a small quantity. We may therefore use the Type I distribution to approximate 
to that of W. It appears likely that as n, m —>- oo the distribution of W tends to the Type I 
form, but this has not been rigorously demonstrated. 
16.35. The significance of W can then be tested in the Type I distribution, namely, 
by the use of incomplete ^-functions. More conveniently, we may transform (16.64) 
to the form 
dF = k — dz (16.67) 
by the transformation 
2 — I log 
(V].e28 + vt) 2 
(m - \)W 
1 - W 
2 
■Vl = (n - 1) - 
m 
v, = (m - l)|(n, - 1) - -?j 
and test in the z-distribution which has been tabulated. 
In making this test it is desirable, for low values of m and n, to make the usual correction 
for continuity by subtracting unity from S (equation (16.47)) and increasing the divisor 
—^———— by 2. Let us examine the approximation of the test in some cases wherein 
id* 
the exact values are known from Tables 16.5 to 16.8. 
For n = 3, m = 9, the 1 per cent, level is given approximately by S = 78 (Table 16.5). 
For such a value, with continuity corrections, 
W = I78 ~ ^ = 0-4695 
12 ^ 
z = 0-979 
16 128 
V. 
420 RANK CORRELATION 
By linear interpolation of reciprocals in Appendix Table 5 we should require, for complete 
agreement, a value of z equal to 0-954. 
For n = 4, m = 6, the 1 per cent, point is approximately S = 100. v± = 8/3, v2 = 40/3. 
We have W = 0-5556, z = 0-916. Prom Table 16.5 we should require a value of 0-893. 
For n = 5, m = 3, there is no very convenient value of 8 close to the 1 per cent, point. 
For P = 0-015, S = 74, and for P = 0-0078, S = 76. 
For 8 = 74 (with continuity corrections) z = 1-020 
S = 76 ( „ „ „) z = 1-089. 
By interpolation from the tables z = 1-075. The use of the 2-test would lead to the 
correct conclusion that a value of 8 equal to 74 falls below, and that of 76 above, the 
1 per cent, point. 
For valuqs of m and n not included in Tables 16.5 to 16.8 it thus appears that the z-test 
with continuity corrections will give sufficiently aocurate results, if n is greater than 3, 
e,t the 1 per cent, points. It may be presumed that the results at the 5 per cent, points are 
equally good and probably better. But for finer values of significance, such as 0-1 per 
cent., it is doubtful whether the test is sound. The tails of the distribution of 8 for moderate 
values of m and n are very irregular. ' , , 
16.36. A somewhat more approximate test of W has been given by Friedman (1937), 
who denned a statistic 
Xr* = m(n,- 1)W (16.68) 
and showed that the distribution of xr2 tends to that of the Type III %a as n tends to infinity, 
v with (n — 1) degrees of freedom. This test appears reasonably satisfactory for moderate 
m and n, though not so accurate as ours. Friedman has also provided (1940) some 
significance levels of %rz calculated on the basis of the 3-test. 
Example 16.5 
In some experiments in random series a paok of ordinary playing cards was shuffled 
and the order of the 13 cards of each suit from the top of the pack was noted. The pack 
was then reshuffled and again the orders noted. This was done 28 times. The question 
to discuss was whether the shuffling was good, in the sense that the cards were thoroughly 
mixed at each shuffle. 
Here, for each suit, say diamonds, we have 28 rankings of 13. The sums of ranks 
were 183, 137, 171, 207, 188, 160, 225, 174, 216, 192, 236, 239, 220. The mean is 196, 
and 8 = 11,522 ; W (without continuity corrections, which are not worth making for these 
. values of. m and n) = 0-08075, z = 0-432. This falls just beyond 'the 1 per cent, point. 
Similarly for the clubs W was found to be 0-0535 ; for the hearts, 0-0245 ; and for 
spades, 0-0342. None of these values is significant, and we conclude that the randomisation 
introduced by the shuffling was good, at all events, so far as this test was concerned. It 
may be added that the shuffling was done with much more care than would be taken in 
an ordinary game of cards. 
Example 16.6 
In psychological work there has sometimes been a confusion between the 
determination of a1 measure of agreement between subjects and that of-ah objective order based 
on experimental rankings. It may therefore be as well to point out that in its psycho- 
ESTIMATION OF A TRUE RANKING 
421 
logical applications the test of W is one of concordance between judgments. There may 
be quite a high measure of agreement about something which is incorrect. 
A number of students were given 12 photographs of persons unknown to them, and 
asked to rank them in what they judged from the photographs to be their intelligence. 
For 16 students the sums of ranks were 
112, 94, 101, 84, 97, 75, 104, 84, 102, 146, 125, 124. 
The mean is 104. 8 — 4472, W ±= 0-1222. z = 0-368, and is barely significant, being 
between the 1 per cent, and the 5 per cent, points. 
For 111 students the sums were 
818, 670, 908, 410, 706, 526, 780, 485, 596, 1044, 959, 756 
W = 0-2378, z = 1-768. 
This is highly significant and it is to be inferred that community of judgment exists between 
students or groups of students. But there was little relationship between the judgments 
and the intelligence of the photographed subjects as given by the Binet Intelligence Quotient. 
Estimation of a True Ranking 
16.37. Suppose we have m sets of n.rankings which show a significant concordance. 
Assuming that the relations between the rankings reflect the true ranking of the objects, 
how are we to estimate that ranking ? or again, assuming merely a significant concordance 
between observers, what is the ranking " nearest " to their rankings ? 
An intuitive approach to this problem would probably lead us to this solution : the 
object whose true rank is 1 is that for which the sum of ranks is least; that whose rank 
is 2 is the one for which the sum of ranks is least but one ; and so on. For example, if 
there are three rankings of five objects totalling 9, 7, 4, 10, 15, we should take the third 
as rank 1, the second as 2, the first as 3, the fourth as 4 and the fifth as 5. 
This solution can be given a firmer theoretical basis. It is the " best" in a least- 
squares sense. In fact, suppose the true ranking is X1} Xa . . . Xn, where as usual the 
Z's are a permutation of the first n integers. Suppose the sums of ranks are 8U 82 . . . 8n. 
• Consider the sum 
U = S(8t - mXtY ' . (16.69) 
If all the rankings were correct, each 8i would be mX^ so that this quantity represents 
in a sense the divergence from complete agreement. Our " best " estimate of the X'a 
will be given by minimising U. Now 
U = Z(St*) + ma ZiXf) - 2m Z8iXi 
and since the first two terms are constant we have to maximise Z(Si X{), the #'s being 
given and the Z's the numbers 1 to w. Evidently this will be done by multiplying the 
biggest 8 by n, the next biggest by (n — 1), and so on. The result follows. 
There is, of course, an mdetenninacy in this method if any two of the 8's are equal. 
Paired Comparisons 
16.38. When the objects which are being ranked are known to be measurable according 
to the quality concerned, no question as to the legitimacy of ranking arises., But cases 
occur in which it is by no means clear that ranking is legitimate, as for instance in the 
arranging of human beings according to intelligence or of pieces of music by human beings 
according to preference. To require an observer to carry out a ranking in such a case 
422 RANK CORRELATION 
may be equivalent to asking him to arrange English towns in order of geographical position 
(which is two-dimensional), or a number of fruits according to taste (which is probably 
four-dimensional). The observer may attempt to comply in the full beKef that he is doing 
something within his powers, but if the quality under consideration is not measurable 
on a linear scale the resulting ranking may fail to give either a real picture of his preferences 
or of the variation of the quality in the individuals. For example, in judgments of 
intelligence, it is not impossible that the observer should judge A more intelligent than 
B, B than 0, and C than A, if the individuals are presented for his consideration one pair 
at a time. The likelihood of this happening is obviously increased when we are dealing 
with tastes in music, eatables or film stars ; and in practice the event is not uncommon. * 
Such " inconsistent" preferences can never appear in ranking, for if A is preferred to B 
.and B to O, then A must automatically be shown as preferred to 0. 
16.39. We therefore consider a more general method of investigating preferences. 
With n objects, we shall suppose that each of the I 9 J possible pairs is presented to an 
observer and his preference of one member of the pair noted. If the object A is preferred 
to B we write A—>B or B*—A. The f ) preferences of a single observer may be 
represented in tabular form as shown in Table 16.9. 
In this table, which is shown for the six objects A to Ft an entry of unity in column Y 
and row X means X—>» F, and is thus accompanied by a complementary zero in row T 
and column X. The diagonals are blocked out. For example, in the table, A—>-B, -4->C, 
D-^-A, etc. 
TABLE 16.9 
Tabular Representation of Paired Comparison Schema. 
A 
B 
C 
D 
E 
F 
A 
— 
0 
0 
1 
0 
0 
B 
1 
— 
1 
0 
0 
1 
c 
1 
0 
— 
0 
0 
0 
D 
0 
1 
1 
— 
1 
1 
E 
1 
1 
1 
0 
— 
0 
F 
1 
0 
1 
0 
1 
— 
The arrangement of the objects A to F in the row and column headings is quite arbitrary. 
There are (w!)2 ways of representing the same configuration of preferences in such a table 
PAIRED COMPARISONS 423 
according to the permutations of objects in row and column ; but in practice it is generally 
desirable to have the order in row and column the same, and even among the n\ possible 
arrangements sb given there are often practical considerations which determine one order 
as more convenient than others. 
16.40. Paired comparisons may also be represented geometrically by a method 
which can be illustrated for the case of the six objects as follows :— 
Fig. 16.2.—Geometrical Representation of the Scheme of Preferences of Table 16.9. 
We represent the six objeots A to F by the six vertices of a regular hexagon and join 
the vertices in all possible ways by straight lines. If A—>B we draw an arrow on the line 
AB pointing from A to B. The arrows shown on Fig. 16.2 correspond to the preferences 
shown in Table 16.9. ' 
16.41. If an observer makes preferences of type A—>~B—>-C—>A we say that the triad 
ABO is inconsistent. In the geometrical representation an inconsistent triad is shown 
by a triangle in which all the arrows go round in the same direction. W,e may thus speak 
of a " circular " triad of preferences. In Fig. 10.2 the triads ACD, BEF and three others 
are circular. { 
It is also possible to have inconsistent triads of greater extent; but any such circuit 
must contain at least two circular triads. Suppose, for instance, that ABCD is circular, 
e.g. that A-+-B—>-C—>D—>A. Then either A-+C or C-**A. In the first case ACD is circular, 
in the second ABC. Similarly either ABB or BCD is circular. Thus the circular tetrad 
must contain just two circular triads. On the other hand it is possible for a tetrad to 
contain circular triads without being itself circular. } 
Similarly, if ABCDE is circular either ABC or ACDE is circular and either BCD or 
BDEA is circular. If the two tetrads are circular there must be at least three circular 
triads (not necessarily four, because ADE may be common to both]T. It is easy to see 
by an actual example based on this configuration that there need not be-more than three 
circular triads ; and it is clear that there must be at least three. For if the tetrads are 
424 RANK CORRELATION 
not circular then ABC and BCD must be so and then either GDE is circular or ABOJ8 
is soj adding at least one more. 
Generally, it appears that a circular %-ad must contain at least (n — 2) ciroular triads ; 
but it may contain more, and the fact that an w-ad contains (n — 2) circular triads does 
not mean that it is itself circular. In discussing inconsistences, therefore, it seems best 
to confine attention to circular triads, which, so to speak, constitute the inconsistent elements 
of the configuration, and to ignore the more ambiguous criteria associated with circular 
polyads of greater extent. 
16.42. We now prove the following theorems : 
T.nn.Hn ia _ 
24 
(1) The maximum possible number of circular triads is n ~~ n if n is odd ftnci 
24 
an8 — ^/ft ' 
——— if n is even; and the minimum number is zero. 
(2) These limits oan always be attained by some configuration of preferences. 
Consider a polygon of the type shown in Kg. 16.2 with n vertices. There will bo 
(n — 1) lines emanating from each vertex. Let al9 aj, ...,«„ be the number of lines 
at the respective vertices on which the arrows leave the vertex. 
Then 
n — 1 
lw-0 
and the mean value of ocr is 
- Beta, a,-Z(*-TL-iy 
^(O-*-!—!! (io.7o) 
We now show that if the direction of a preference is altered and the effect is to' increase 
the number of circular triads by d, T is reduced by 2 d; and conversely. Consider tho 
preference A—*-B. The only triads affected by altering this to B—+A are thoso containing 
the line AB. Suppose there are a preferences of type A—>X (including A—>B) and /S 
preferences of type B-^-X. Then four possible types of triad arise: 
A—>X<—B, say p in number 
A+-X-+B, 
A—>X—>B, which must number a — p — 1 
A<—X*—B, „ „ „ £ — p. 
When the preference A—*B is reversed the first two remain non-circular. Tho third 
becomes circular, the fourth ceases to be so. The reduction in the value of T is 
aa_(a_l)2+02_(jg + 1)a 
= 2(a - fi - 1) 
= 2d, say. 
The increase in the number of circular triads is 
(a_,p_l) -tf-p) =a-iS-l 
= d. 
* 
COEFFICIENT OF CONSISTENCE IN PAIRED COMPARISONS 425 
More generally, if as the result of reversing any number of preferences T is decreased 
by 2 d, then d must be an integer and the number of circular triads must be increased by d. 
This clearly follows from the previous results, for the reversal of preferences can take place 
one at a time and the effect on T and the number of circular triads is cumulative. 
We now investigate the maximum and minimum values of T. It is clear from the 
definition that T is greatest when the oc's are the natural numbers 1,2, . . ., n ; and this 
is a possible case because it corresponds to ordinary ranking. Hence max. (T) = — 
3 
n 
12 
For the minimum value, consider tho polygon A1} Az, . . ., An. Set up the 
preferences Ax—>A2~>- . . . An—>AX. Clearly at any vertex this results in one arrow entering 
and one leaving the vertex, i.e. the contribution to a is unity at each vertex. Next set 
up the 'preferences Ax—>AS—>A6~>. . . . This circuit may either visit each vertex once, 
or not. In the latter oase we proceed to an unvisited vertex and set up the preferences 
Ar—*-Ar+2—>j4r+4-—> . . ., and so on. Again there will be a unit contribution to all the oc's. 
We then set up the preferences Ai—>Ac-J^A1—^, etc., and so on; and in this way we 
shall ultimately complete the preference scheme. 
If n is odd all the preferences described will consist of circular tours of the polygon, 
n i 
and thus the value of a for each vertex will be —-—. If n is even, the last preference 
Ai—>-Ain+1 will not be a tour but will consist of the single line joining one vertex with the 
Tit WW 
symmetrically opposite vertex. Thus here will be - vertices for which a = - and - 
vortices for which a = -. In this case T = -. 
2 4 
Now it is clear from the definition of T that it cannot be less than zero, or if % is even, 
be loss than -. The configuration just given shows that these minima are, in fact, attainable. 
Thus T can vary from a maximum of -^— to a minimum of zero or -. Hence 
the maximum number of circular triads, being half the variation from maximum to minimum 
of T (the maximum of T corresponding to the ranking case in which there are no incon- 
sistcnces), is —— if % is even and —^7" u % is odd. 
This establishes the two results enunciated at the beginning of this section. 
Coefficient of Consistence in Paired Comparisons 
16.43. If d is the number of circular triads in an observed configuration of preferences 
we define 
24d ,, .\ 
£ = 1 - _ , n odd 
B -* I (16.71) 
24d ( 
= 1 -, n even 
n* — 4% / 
and call £ the coefficient of consistence. If and only if it is unity, there are no inconsistences 
in the configuration, which may therefore be represented by a ranking. As £ decreases to 
zero the inconsistence, as measured by the number of circular triads, increases. 
For example, in the configuration of Fig. 16.2 there are five circular triads, ABD, 
ACD, AFD3 AED and BEF. The maximum possible number is 8. Thus £ = 0-375. 
426 RANK CORRELATION ' 
C can also be interpreted in the light of.Table 16.9. Suppose in tha.t table, we sum 
the rows. (The column sums are determined by the row sums and add no fresh information.) 
T^esZ of any row will be the a-number for that vertex in the polygon which correspond 
to the object defining the row. T will then be the value of the sum of squares of deviation 
of row totals from the mean value ^, that is to say, will be the variance of the row 
sums multiplied by n. £ is thus a linear function of this variance ; but it cannot be tested 
in the ^distribution as if Table 16.9 were a contingency table, for the border cells are not 
independent or linearly dependent. 
16.44. If an individual observer produces a configuration of preferences which show 
inconsistence there are usually several explanations ; he may be an incompetent judge, 
the objects may be so alike that consistent differentiation is not possible, or his attontion 
may wander during the course of the experiment. We discuss these questions lator. Thoy 
• are mentioned here to explain the motive for the next stage of the mathematics. With 
what probability can a- value of £ arise by chance if the observer allots his preferences at 
random with respect to the quality under consideration 1 
With n objects there are 2™J possible configurations of preferences. We proceed to 
investigate the distribution of d in this population of 2v 2' different members. The method 
consists of proceeding from the distribution for n to that for (n + 1). 
For n = 3 there are eight configurations, of which two give one circular triad and six 
no circular triads. Consider the effect of adding a new vertex D to the vortices ABO. 
Four cases arise: 
(1) D-+ all A, B, G. * 
. (2) Z>-> two of A, B, G. 
(3) Z>-> one of A, B, G. 
(4) !D-> none of A, B, 0. 
The last two are symmetrical with the first two and need not be separately considered. 
Situation (1) arises in one way and clearly does not add any new circular triads other 
than those already existing in the configuration ABC. It therefore contributes six values 
d = 0 and two values d = 1. So does situation (4). 
Situation (2) arises in three ways, according as D<—A, B, or C. The configurations 
so reached are similar and we may take any one, say D+-C, as the single preference. If 
A~*—G then DAG is not circular and if B*r-G then DBG is not circular. On the other hand 
A—>G and B—*-G will each produce a circular triad. We then have the cases 
0 
A+-C-+B 
A-^C-+B 
A+-C*-B 
A-+C+-B 
No. of Circular 
Triads added. 
0 
1 
1 
2 
1 
We now consider AB. In the first two cases just enumerated the direction of AB 
does not matter and no circular triads are added. With the third A—>B gives no circular 
triad but A*—B adds one. With the fourth A—>B adds one and A*—B adds none. 
COEFFICIENT OF AGREEMENT . 427 
Thus the number of circular triads occurring for these four cases is found to be 
No. of Circular 
Triads. 
0 
1 
2 
Frequency. 
2 
2 
4 
We must multiply the frequency by three and by two to allow for similar symmetrical 
arrangements, and the final results are * 
No. of Circular 
Triads. ' 
0 
1 
2 
Total 
Frequency. 
24 
16 
24 
64 
The principles of this method are clear enough and the work may be formalised by 
a number of conventions which we omit to save space. In common with many similar 
combinatorial problems, however, troubles arise from the sheer number of possibilities and 
the difficulty of ensuring that nothing is overlooked. Up to the present the distribution 
of d for n up to and including 7 is known. The frequencies and probabilities are given in 
Table 16.10. 
Paired Comparisons for m Observers: Coefficient, of Agreement 
16.45. We now consider the investigation of similarities of judgments for m observers. 
Suppose that in a table of the form of Table 16.9 we enter a unit in the cell in row X and 
column Y whenever X—>Y and count the units in each cell. A cell may then contain 
any number from 0 to m. If the observers are in complete agreement there will be ( J cells 
/n\ i 
containing the number m, the remaining II cells being zero. The agreement may be 
complete even if there are inconsistences present. 
Suppose that the cell in row X and column Y contains the number y. 
Let 
-© 
(16.72) 
the summation extending over the n(n — 1) cells of the table (the diagonal cells being 
ignored). E is then the sum of the number of agreements between pairs of judges. Put 
22? 
428 • 
RANK CORRELATION 
TABLE 16.10 , 
Paired Comparisons. Frequency (f) of Values of d and Probability (P) that Values will 
be Attained or Exceeded. 
Value 
ofrf. 
' 0 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
14 
Total 
n = 2 
/ ' 
2 
* 
- 
2 
P 
1-000 
1 
-- 
— 
n 
f 
6 
2 
8 
= 3 
P 
1-000 
0-250 
— 
n 
f 
24 
16 
24 
64 
= 4 
P 
1000 
0-625 
0-375 
-, 
* 
n 
f 
120 
120 
240 
240 
280 
24 
* 
1,024 
= 5 
P 
1-000 
0-883 
0-766 
0-531 
0-297 
0023 
»• 
— 
n - 
720 
960 
2,240 
2,880 
6,240 
3,648 
8,640 
4,800 
2,640 
32,768 
= 6 
P 
1000 
0-978 
0-949 
0-880 
0-792 
0-602 
0-491 
0-227 
0081 
— 
n = 
f 
5,040 
8,400 
21,840 
33,600 
75,600 
90,384 
179,760 
188,160 
277,200 
280,560 
384,048 
244,160 
233,520 
72,240 
2,640 
2,097,152 
7 
P 
1-000 
0-998 
0-994 
0-983 
0-967 
0-931 
0-888 
0-802 
0-713 
O-580 
0-447 
0-263 
0-147 
0036 
0001 
— 
The maximum number of agreements, occurring if f ) cells each contain m, is ( )( \ 
and thus in the case of complete agreement, and only in this case, u = 1. The further we 
go from this case, as measured by agreements between pairs of observers, the smaller 
m 
u becomes. The minimum number of agreements occurs when each cell contains — if 
m is even or 
(m ± 1). 
if m is odd. That is, if m is even, the minimum number of agreements is 
and in this case 
m — 1 
When m is odd the minimum value of u is found to be 
(16.74) 
u = . 
m 
. (16.75) 
16.46. We shall call u the Coefficient of Agreement. It is unity if and only if there 
is complete agreement in the comparisons. Its minimum value is not — 1 except when 
m = 2. This, however, is to be expected in a measure of agreement, for there can be no 
such thing as complete disagreement among three or more observers in paired comparisons. 
COEFFICIENT OF AGREEMENT 
429 
If observer P differs in certain comparisons from observers Q an<l R, the two latter must 
agree on' those comparisons. 
When m = 2, u reduces to 
957 
u = ~ — 1 (16.76) 
(3 
and E becomes twice the number of cases in which the two observers agree about a 
comparison, u is thus a generalisation of a coefficient t. For general ra, if the entries in the 
table were constrained to the ranking type, u would be the average interoorrelation r between 
observers taken two at a time. 
16.47. In discussing the significance of u it is desirable to know whether the set of 
preferences which give rise to it could have arisen by chance if the preferences had been 
assigned at random with respect to the quality under consideration. The procedure which 
first suggests itself is a generalisation of the method used for the case of m rankings. That 
is to say, we sum the entries in the rows of the table and consider the variance of these 
entries. If the preferences are allotted at random we expect to find about equal numbers 
given to each object, and the variance will be low ; in other cases it will be higher. 
The difficulty about this suggestion is that it has not been found possible to ascertain 
the distribution of the variance in the 2 ^2' possible sets of preferences. The case m = 1, 
corresponding to the distribution of d for inconsistences, is difficult enough to solve. For 
higher values of ra no distributions are known except in trivial cases. 
A test can, however, be devised by using the coefficient u. Consider one cell in the 
table in row X and column Y and let it contain the number y. Then the corresponding 
cell in row Y and column X will contain m' — y. Thus these two contribute to E the amount 
Q+CV> 
Now, of the total ways in which the units can be distributed in the first cell there 
will be [ ] in which y units occur. Consequently the distribution of E in the cell and the 
corresponding cell is given by the expression 
j. fl)+f)(V)+(yrna+... + (;)(vwy + ...+£) (16.7,) 
and since the distribution in other pairs of cells is independent jf the preferences are allotted 
at random the distribution of E for the whole table is given by 
D{E) =fN (16.78) 
where N = (A. 
16.48. The distributions have been worked out for the following values of m and n : 
w = 3, n = 2to8; m = 4, ?i = 2 to 6 ; m = 5, w=2to 5; m = 6, w = 2 to 4. 
Tables 16.11 to 16.14 give the probabilities based on these distributions, i.e. the probabilities 
that a given valjie of E will be attained or exceeded. 
430 
RANK CORRELATION 
For constant n the distribution tends to the Type III form as m tends to infinity. 
In fact, for a single pair of related cells the variate-value corresponding to a frequency 
( ) is { o)H"(-oJ' which is a quadratic in y. Were the variate-value a linear function 
of y the distribution for the single cell would tend to normality in accordance with the 
well-known property of the binomial. The case of the quadratic value corresponds to 
a transformation of the variate of the type x* = y, and the transform of the normal form 
exp ( — x2) dx becomes the Type HI form exp (— y) y~* dy. Since the N cells are 
independent and the sum of variates in the same Type III form is also distributed in that1 
A--i 
form, it follows that Z is in the limit distributed as exp (— Z) Zi dZ except perhaps 
for some constants. Thus £ or some multiple of "it is distributed as %%. 
For constant m the distribution tends to normality with increasing n. 
TABLE 16.11 
Agreement in Paired Comparisons. The Probability P that a Value of Z will be Attained 
or Exceeded, for m = 3, n = 2 to 8. 
n = 2 
E 
1 
3 
• 
P 
1-000 
0-250 
■ 
■ 
n = 3 
E 
3 
5 
7 
9 
■ 
P 
1-000 
0-578 
0-156 
0-016 
- 
n 
E 
6 
S 
10 
12 
14 
16 
18 
- 
i 
= 4 
P 
1-000 
0-822 
0-466 
0-169 
0-038 
0-0046 
0-0s24 
• 
■ 
n 
E 
10 
12 
14 
16 
18 
20 
22 
24 
26 
28 
30 
V 
= 5 
P 
1-000 
0-944 
0-756 
0-474 
0-224 
0-078 
0-020 
0-0035 
0-0342 
0-0*30 
0-0695 
Ti ' 
n 
E 
15 
17 
19 
21 
23 
25 
27 
29 
31 
33 
35 
37 
39 
41 
43 
45 
\ 
= 6 
P 
1-000 
0-987 
0-920 
0-764 
0-539 
0-314 
0-148 
0-057 
•0-017 
0-0042 
0-0s79 
0-0H2 
0-0*12 
0-0B92 
0-0'43 
0-0893 
' 
n 
E 
21 
23 
25 
27 
29 
31 
33 
35 
37 
39 
41 
43 
45 
47 
49 
51 
53 
55 
57 
59 
61 
63 
= 7 
P 
1-000 
0-998 
0-981 
0-925 
0-808 
0-633 
0-433 
0-256 
0-130 
0-056 
0-021 
0-0064 
0-0017 
0-Os37 
0-0*68 
0-0*10 
0-0512 
0-0612 
0-0886 
0-0fl44 
0-01015 
0-01B23 
n 
E 
28 
30 
32 
34 
36 
38 
40 
42 
44 
46 
48 
50 
52 
54 
56 
58 
60 
62 
64 
66 
68 
70 
72 
74 
76 
78 
80 
82 
84 
i 
= 3 . ' 
P 
1-000 
1-000 
0-997 
0-983 
0-945 
0-865 
0-736 
0-572 
0-400 
0-250 
0-138 
0-068 
0-029 
0-011 
0-0038 
0-0011 
0-0329 
0-0*60 
0-0*13 
0-0522 
0-0B32 
0-0740 
0-0842 
0-0fl36 
0-01024 
0*0n13 
0-01348 
0-01412 
0-01014 
£ 
K« 
o a> ip>- 
1-000 
0-375 
0-063 
OSbOMbOtOHii-'l-iH 
OOOOOOOOH-1 
OOOChMOJ'JO 
"Onojooeoio 
MmhOos^IOOO 
H 
^ 
H 
H 
I 
ascnoicucn^hf^h^rf^h^cococococotototo i ».. 
00)^(90090>^b9ClOt)a^bSClOt)3)^ [ lT 
! 
oopppopopoopoopooH- 
OOOOOOOOOOOt^bSCOOl-Jcbo 
-; oi oi it. w vOOObSOlCOOlWa^O 
5HOiMHWMw«oiti.eo^oocoa)b500 
Ocoii^-cobsastsscnco 
i^-bsocoosi^-bsoooosi^-tacoooii^-bso 
oooopopppppooopopt^1 
ooooooooo^bbcooicjoocbcbo 
oo »OOOj-'k9 0l!D0l0l00CC!D^^t0C 
HWHtOOlVsOOMOO^^^OOMOlHO 
OffiasoeoooooooaioovKi 
oa^woooa^woooa 
oopopopppopp 
© ©'© ooooooooo 
(-it-ieGDtf^ioioioioii^^ 
u ohhOiMhCOhOihOI 
50rf^i(i.bOCOC00500lOCRO 
l—i CT> 
_ _ . 
D 
h 
13 
H 
D 
3 
II 
to 
II 
CO 
II 
B 
Ol 
3 
n 
Ol 
1 
to 
s 
o 
Co 
Co 
8. 
9 
II §■ 
off. 
© a- 
S eg- 
ii r 
o 
OS CO 
to 
1-000 
0-625 
0-125 
GOOll(*-b©l-'©CDGO<ia> 
©©©©©©©©©H-1 
OtotP-co-acooiCOhf^o 
o 
COCOCOCCtOtOMbObOtObOMbObOHHHI-ii-ii-ii-il-' 
©©©©©©©©©©©©©©©©©©©©©HJ 
ooocoooooooHHii^oia-Jiiio 
en ft BCOOOHt«lMBMCO«JHM«0)0^eO 
CO Ol Oi 1—i Ol © CO 
O3OlOlOl0lCl0lo')f>->f>-)^£>->f>-£>->f>>>f>- 
o<ia>ii^-cob£ii-iOcoco<ia>oii^-cob9 
ooooooooo o_oooooo 
ooooooooo0000000 
SDS$Gi<i>--'bS5DboeoeDbSi£-<iHicoi5- 
63MCOi|i.vjHCOOl^O^ H-«CO <I o CO 
tn 
H 
h 
h 
to 
s 
CO 
s 
hP^*-cooocowcocococotocototototototototototo 
HO<DXK]a0l^Wb9l-<O(DK>^Q0li^CCb9i->O 
09000900090009000000HH 
66 6o66^Hii«^6ic)-Jdbai©!c6©66 
Oh-'bOCOOSCObO<Ii(i-bOI—' 1—1 WHOOO||V«JCO©00 
- CDffiiMBOOvltfKOvieo i^.-i-' COOlbObOCOOlCDOO 
00 
h 
13 
3 
II 
Ol 
s 
Ol 
•\l-J<I<Ivl,sl<I<IvlOSC4CBO5C0OSCBOSC4CBOlOlOl 
h 
ooopopoopooooooooooooo 
0-0000© ©-© oooooooooooooo 
""-"-"oooioi-oioifrfr^eoMeoeoO©©©©!—' 
K^^^^^^^^^w^^^^^Swcooieo^ 
oii|i-cocoi-'00<i<icoa)bscocoi(i.a)o«bS--JCoS 
5000000000000000<I 
O <i a i(i w m h'©to 
o©o©©o©©© 
[S/jCOOlbSOlCOCObS 
GO-J 
3 
05 
H 
3 
3& 
i 
?> 
§ 5" 
II & 
a ^ 
*•? 
Hi 
s- 
a Q ► 
II 0. g 
Oi St P 
o £* *- 
-^ s- 
?? 
I a 
g» » 
TO ** 
J* t^ 
jf. 
as. 
«5 
OS 
3 
432 
BANK CORRELATION. 
TABLE 16.14 
Agreement in Paired Comparisons. The Probability P that a Value of U will be Attavned 
or Exceeded, for m = 6 and n = 2 to 4. 
/ n ■■ 
S 
6 
7 
10 
15 * 
- 
= 2 
P 
1-000 
0-688 
0-219 
0-031 
n 
S 
18 
19 
20 
21 
.22 
23 
24 
26 
27 
28 
29 
30 
31 
32 
36 
36 
37 
40 
45 
= 3 
P 
1-000 
0-969 
0-832 
0-626 
0-523 
0-468 
0-303 
0-180 
0-147 
0088 
0-061 
0 040 
0034 
0023 
00062 
0-0029 
0-0020 
0-0358 
0-0*31 
n ■■ 
' 2 
36 
37 
38 
39 
40 
41 
42 
43 
44 
45 
46 
47 
48 
49 
50 
51 
52 
53 
54 
= 4 
P - 
1-000 
0-999 
0-991 
0-959 
0-896 
0-822 
0-756 
0-669 
0-656 
0-466 
0-409 
0-337 
0-257 
0-209 
0-175 
0-133 
0097 
0-073 
0-057 
n 
S 
55 
56 
67 
58 
59 
60 
61 
62 
63 
64 
65 
86 
67 
68 
69 
70 
71 
72 
73 
= 4 
P 
0043 
0029 
0020 
0-016 
0-011 
0-0072 
0-0049 
00034 
00026 
0-0016 
0-0383 
0-0*66 
0-0»48 
00326 
0-0316 
0-0*86 
0-0*68 
0-0*48 
0-0*16 
n 
S 
74 , 
= 4 
P 
0-0*12 
75 * 0-0B89 
76 
77 
80 
81 
82 
85 
90 
i 
0-0B49 
0-0B32 
0-0°68 
0-0H7 
0-0°12 
0-0734 . 
00°93 
16.49. The first of these results suggests that the Type III distribution will provide 
an approximation to the distribution (16.78) when m is moderately large. We proceed to 
find the first four moments of (16.78). 
It is sufficient to find the first four moments of (16.77), those of (16.78) being obtainable 
therefrom in virtue of the relationships which connect cumulants of independent distributions. 
The rth moment of (16.77) about the origin is given by 
•"^-K'aT'L- (ie-79> 
since 2m is the total frequency. Thus we have * 
•""I - z (?)('• - -+^) - 2-(™)+*(:>" - "»> • <i6-8°> 
Sums such as S( \rp can be obtained by operating on the binomial (1 -j- x)mp times 
by ajr-, e.g. we find 
COEFFICIENT OF AGREEMENT 
and henoe, substituting in (16.80), 
\ , l/m\ 
Thus the mean of the distribution (16.78) is given by 
'■m\ 
433 
i»i = m 
2 
(16.81) 
In a similar way we find 
!** = *#(' 
3; 
,r/m\f3m2 - 15m + 17 , 3 ,T/ . ,1 
i"4 = ( 2 ){ 8 ~~ + ^(m ~ m)\ 
(16.82) 
These are the moments of S. Those of' u are obtained by dividing by an appropriate 
(<fn\ 
1 and it may be noted in particular that the mean of u is zero. 
16.50. The first four moments of the Type III distribution 
dF = ke-v* Tfl-1 dx 
q q 2q 3q(q__+_2) 
pi 
are 
p' p2' p3' 
Equating the second and third moments to those given by (16.82) we find 
Nm(m - 1)" 
<l = 
p = 
2(m - 2)2 ' 
2 
* (16.83) 
m — 2 
To make the first moments correspond we move the origin of the JL'-distribution a distance 
(wi\ wi ~~' 3 
J - to the right. We thus reach the approximation to the ^-distribution, 
coinciding in the first three moments, 
2x Nm(m — 
dF = Ice i">-*x '2('»-^r dx, 
where 
x 
m\m — 3 
-*-<£^ 
4s 
or, transforming to the more usual %* form by putting x2 = —~2> we find that 
{'-O^U 
2 «i — 2 
. (16.84) 
is distributed as %2 with 
v = 
Nm(m — 1) 
/ o\* - • • • • (16.85) 
(m — 2)a x ' 
degrees of freedom. 
The fourth moments of £ and the %2 approximation differ by terms of order N~* and 
m~l compared with their absolute values. 
a.s. u" v 
434 
RANK CORRELATION 
\ 
16.51. It only remains to be seen how large m and n must be for this to provide- 
a satisfactory approximation. 
Consider first the distributions for m = 3. When n = 8, N = 28, we have, for the 
approximation, 427 distributed with 168 degrees of freedom. From Table 16.11 we see 
that for E = 54, P = 0-011 and for E = 58, P = 0-00il. Applying a continuity correction 
by deducting unity from E we find for the #2 approximation with %2 = 4 x 53, v = 168, 
P = 0-011, and with %2 = 4 x 57, P = 0-00114. The correspondence is very close, in 
spite of the low value of m. 
For m = 4, n = 5, N = 10, the approximation gives 22" — 30 distributed with 
30 degrees of freedom. For E = 40 and 41, this gives, with continuity corrections of 0-5, 
half the variate-interval, %2 = 49 and 51, v = 30. From the diagram at the end it is seen 
that these values lie one on either side of the 1 per cent, value ; and this is in accordance 
with the exact values of P, which are seen from Table 16.12 to be 0-016 and 0-0088. Similarly 
we find that the values of E, 37 and 38, lie on either side of the 5 per cent, level, which is 
again in accordance with the exact values, P = 0-060 and 0-038. 
For m = 6, n = 4, N = 6, the approximation gives E — 33-75 distributed with 
11-25 degrees of freedom. For E = 59 and 60 the corresponding #2 values are seen to* 
lie on either side of the 1 per cent, point, which accords with the exact value of Table 16.14. 
We conclude that the %2 approximation provides an adequate test of significance for 
the values of m and n outside the range for which Tables 16.13 and 16.14 give exact values. 
Example, 16.7 
A class of boys (ages 11 to 13 inclusive) were asked to state their preferences with 
respect to certain school subjects. Each child was given a sheet on which were written 
the possible pairs of subjects and asked to underline the one preferred in each case. The 
results were as follows : ' 
21 boys, 13 school subjects. The preferences are shown in Table 16.15, which is in 
the form described in 16.39 ; e.g. there were 18 boys who preferred Art to Religion. 
TABLE 16.15 
Preferences of 21 Boys in 13 Subjects. 
1 2 3 4 6 6 7 8 9 10 11 12 13 Totals 
1. Woodwork 
2. Gymnastics 
3. Art 
4. Science 
6. History 
6. Geography 
7. Arithmetic 
8. Religipn 
9. English Literature 
10. Commercial subjects 
11. Algebra 
12. English Grammar 
13. Geometry 
. 7 
1 
6 
6 
6 
6 
3 
3 
3 
1 
0 
1 
14 
— 
7 
9 
8 
3 
7 
5 
5 
1 
5 
3 
2 
20 
14 
— 
11 
7 
11 
6 
3 
5 
6 
4 
5 
2 
15 
12 
10 
— 
10 
9 
6 
7 
8 
8 
4 
4 
6 
15 
13 
14 
11 
— 
7 
10 
9 
7 
6 
8 
7 
6 
16 
18' 
10 
12 
14 
— 
7 
7 
8 
8 
5 
6 
4 
16 
14 
16 
15 
11 
14 
— 
12 
10 
8 
6 
8 
6 
18 
16 
18 
14 
12 
14 
9 
— 
9 
7 
7 
.5 
7 
18 
16 
18 
13 
14 
13 
11 
12 
— 
11 
8 
8 
6 
18 
20 
16 v 
13 V 
15 
13 
13 
14 
10 
— 
11 
11 
7 
20 
16 
17 
17 
13 
16 
15 
14 
13 
10 
— 
11 
8 
21 
18 
16 
17 
14 
15 
13 
16 
13 
10 
10 
— 
8 
20 
19 
19 
16 
16 
17 
15 
14 
15 
14 
13 
13 
— 
Total 
211 
183 
160 
154 
140 
137 
116 
116 
106 
91 
82 
81 
61 
1638 
COEFFICIENT OF AGREEMENT 
435 
The calculation of E for this table, in which the objects are arranged in order of total 
number of preferences, may be shortened by noting that E, as given by equation (16.72), 
may be transformed into the form 
E = 2V) - mZ{y) + (")(*), 
where the summation now takes place over the half of the table below the diagonal. Since 
the numbers in this half are smaller than those in the other half there is a considerable 
saving in arithmetic. 
We find / ■ E = 9718 
and hence 
u = 
2 x 9718 
— 1 = 0-186. 
There is thus a certain amount of agreement among the children, indicated by the 
positive value, of u. Is this significant ? 
We note first of all that this distribution of preferences could not have arisen by chance 
to any acceptable degree of probability. In fact, xz = 412-4 (equation 16.84)) and v = 90-7. 
The large value of v justifies the use of the normal approximation to the ^-distribution 
and we find -\/(2%2) — V(2v ~ 1) = 15"3> a verv improbable result on the hypothesis of 
a random allocation of preferences. 
The distribution of circular triads was as follows :— 
No. of TriocUi. 
0 
1 
4 
6 
7 
8 
10 
Frequency. 
1 
1 
6 
2 
2 
1 
1 
No. of Triads. 
12 
17 
21 
25 
29 
39 
Total 
Frequency. 
1 
3 
1 
1 
1 
1 
21 
The total number of circular triads was 242 with a mean of 11-5. Only one boy was 
entirely consistent. On the other hand, for n = 13 the maximum number of circular 
triads is 91, with a mathematical expectation of 71-5. It is thus clear that, except perhaps 
for one boy, we cannot suppose that any boy allotted preferences at random. We are 
again led to conclude that the boys are genuinely capable of making distinctions, and that 
consistently on the whole. Half the boys have coefficients of consistence £ greater than 0-92. 
We conclude that the boys can make preferences and that in their view the subjects 
are sufficiently different to enable a reasonably consistent set of preferences to be made. 
So far as these data are concerned there would be no objection to the assumption that 
a scale of preferences can be set up. With this in mind, we can say that the value of 
u indicates a certain amount of agreement, though not a strong one, between the boys as 
to which subjects they prefer. 
436 RANK CORRELATION 
NOTES AND REFERENCES 
Spearman has suggested another coefficient of rank correlation, viz. 
WS - 1' 
* 
but this " footrule " is unreliable as a measure of dependence—it cannot, for example, 
attain — 1. For earlier work on rank correlation see Spearman (1904, 1906), K. Pearson 
(1907) and " Student " (1921). The distribution of p in /the case of independence was 
given by Kendall and others (1939). Pitman (1937) had previously suggested that it 
could be approximately represented by the J3-distribution. 
The coefficient t was suggested by Kendall in 1938. In practice p is probably more 
convenient. It is, however, remarkable that r is unique among correlation coefficients in 
depending only on linear processes, so that machines may be constructed to calculate it. 
Furthermore, t can be adapted to give partial rank correlation coefficients (Kendall, 1942). 
The problem of in rankings was considered by Friedman in 1937 and by Babington 
Smith and Kendall and by Wallis in 1939. Friedman (1940) has reviewed this work and 
provided some useful tables based on the Type I approximative distribution. Wallis has 
pointed out that the coefficient W is the ranking analogue of the correlation ratio. Kelley 
(Statistical Method) had considered pav as a measure of concordance in rankings. 
i ■ REFERENCES 
Eells, W. C. (1929), " Formulas for probable errors of coefficients of correlation," J. Amer. 
Statist. Ass., 24, 170. 
Friedman, M. (1937), " The use of ranks to avoid the assumption of normality implicit 
in the analysis of variance," Jour. Amer. Statist. Ass., 32, 675. 
(1940), " A comparison of alternative tests of significance for the problem of m 
rankings," Ann. Math. Statist, 11, 86. 
Hotelling, H., and Pabst, M. R. (1936), " Rank correlation and tests of significance 
involving no assumption of normality," Ann. Math. Statist., 7, 29. 
Kendall, M. G. (1938), " A new measure of rank correlation," Biometrika, 30, 81. 
, M. G., Kendall, S. F. H., and Babington Smith, B. (1939), " The distribution of 
Spearman's coefficient of rank correlation in a universe hi which all rankings 
occur an equal number of times," Biometrika, 30, 251. 
and Babington Smith, B. (1939), " The problem of m rankings," Ann. Math. Statist., 
10, 275. 
and Babington Smith, B. (1940), " On the method of paired comparisons," Biometrika, 
31, 324. ^ 
(1942), " Partial Rank Correlation," Biometrika, 32, 277. 
Pearson, K. (1907), " On further methods of determining correlation," Drapers Co. Memoirs^ 
Biometric Series IV, London, Dulau & Co. 
Pitman, E. J. G. (1937), " Significance tests which may be applied to samples from any 
populations ; Part II. The correlation coefficient test," J. Roy. Statist. Supp., 
4, 225, and (1938) " Part III. The analysis of variance," Biometrika, 29, 322. 
Spearman, C. (1904), " The proof and measurement of association between two things, 
Amer. J. Psychol., 15, 88. 
(1906), " A footrule for measuring correlation," Brit. Jour. Psychol., 2, 89. * 
" Student " (1921), " An experimental determination of the probable error of Dr. Spearman's 
correlation coefficients," Biometrika, 13, 263. 
EXERCISES 
437 
Wallis, W. A. (1939), " The correlation ratio for ranked data," Jour. Amer. Statist. Ass., 
34, 533. 
EXERCISES' 
16.1. Show that the coefficients of rank correlation p between the natural order 1, 
. . 10 and the following rankings are — 0-37 and +0-45 respectively. 
7, 10, 4, 1, 6, 8, 9, 5, 2, 3 ; 
10, 1, 2, 3, 4, 5, 6, 7, 8, 9. 
Show that the corresponding values of t are — 0-24 and +0-60. 
16.2. Defining 
%r2 = m(n — 1) W 
show that approximately %r2 is distributed as x2 in *k© Type III form with v =n—1 
degrees of freedom. 
(Friedman, 1937.) ' 
1 16.3. Show that W is the ratio of the sum of squares between columns and the total 
sum of squares (the rankings being regarded as arrayed one below the other) and hence 
that W is the square of the correlation ratio r]yx2 for such an array (the ranks being regarded 
as variate-values). The " sum of squares between columns " means the sum of squares 
of deviations of column means from their mean. 
(Wallis, 1939.) 
16.4. Show that Spearman's " footrule " 
R = 1 - 
327 Id 
?t2 — 1 
can attain, but not exceed, the value 1, and can be as small as, but not smaller than, — J. 
16.5. Verify formula (16.63). 
16.6. The following table shows the preferences of 25 girls in 11 school subjects! 
1 2 3 4 5 6 7 8 9 10 11 Totals 
1. Gymnastics 
2. Science 
■8. Art 
4. Domestic Science 
6. History 
6. Arithmetic 
7. Goography ■ 
8. English Literature 
9. Religion 
10. Algebra 
11. English Grammar 
16 
0 
8 
6 
8 
4 
4 
4 
7 
3 
10 
— 
13 
10 
8 
10 
4 
6 
' 7' 
9 
8 
19 
12 
— 
9 
9 
7 
15 
8 
9 
6 
9 
17 
16 
115 
— 
9 
14 
12 
10 
11 
14 
11 
20 
17 
16 
16 
— 
11 
7 
13 
12 
10 
7 
17 
15 
18 
11 
14 
— 
13 
12 
13 
9 
7 
21 
21 
10 
13 
18 
12 
— 
11 
10 
11 
11 
21 
19 
17 
16 
12 
13 
14 
— 
11 
12 
11 
21 
18 
16 
14 
13 
12 
16 
14 
— 
14 
8 
18 
16 
19 
11 
16 
16 
14 
13 
11 
— 
13 
22 
17 
1G 
14 
18 
18 
14 
14 
17 
12 
— 
Total 
186 
166 
147 
121 
121 
121 
112 
105 
106 
104 
88 
1376 
Show that the coefficient of agreement u is 0-082 ; that this is significant; but that 
the girls are less alike in preferences than the boys of Example 16.7. 
v 
/ 
APPENDIX TABLES 
APPENDIX TABLE 1 
Normal Distribution. Frequency Function of the Normal Distribution at every Tenth of the 
Standard Deviation, with First and Second- Differences. The value of the central ordinate 
at zero is 1/V2jr. 
x-. 
a 
00 
01 
02 
0-3 
0-4 
0-5 
0-6 
0-7 
0-8 
0-9 
10 
1-1 
12 
1-3 
1-4 
1-5 
1-6 
17 
1-8 
1-9 
20 
21 
2-2 
2-3 
2-4 
y- 
0-39894 
0-39695 
0-39104 
0-38139 
0-36827 
0-35207 
0-33322 
0-31225 
0-28969 
0-26609 
0-24197 
0-21785 
019419 
017137 
0-14973 
0-12952 
• 0-11092 
009405 
, 0-07895 
006562 
005399 ' 
0-04398 
0-03547 
0-02833 
002239 
AH-)- 
199 ■ 
591 
965 
1312 
1620 
1885 
2097 
2256 
2360 
2412 
2412 
2366 
2282 
2164 
2021 
1860 
1687 
1510 
' 1333 
1163 
1001 
851 
714 
594 
486 
A*. 
- 392 
- 374 
- 347 
- 308 
- 265 
- 212 
- 159 
- 104 
- 52 
0 
+ 46 
+ 84 
+ 118 
+ 143 
+ 161 
+ 173 
+ 177 
+ 177 
+ 170 
+ 162 
+ 150 
+ 137 
+ 120 
+ 108 
+ 91 
X 
a' 
2-5 
2-6 
2-7 
2-8 
2-9 
3-0 
31 
3-2 
3-3 
3-4 
3-5 
3-6. 
3-7 
3-8 
3-9 
40 
41 
4-2 
4-3 
4-4 
4-5 
4-6 
4-7 
4-8 
y- 
001753 
0-01358 
001042 
000792 
0-00595 
000443 
0-00327 
000238 
000172 
000123 
000087 
000061 
0-00042 
0-00029 
000020 
0-00013 
0-00009 
0-00006 
0-00004 
0-00002 
0-00002 
000001 
0-00001 
0-00000 
AH-)- 
395 
316 
250 
197 
152 
116 
89 
66 
49 
36 
26 
19 
13 
9 
7 
4 
3 
2 
2 
— 
___ 
— 
— 
— 
A*. 
+ 79 
+ 66 
+ 53 
+ 45 
+ 36 
+ 27 
+ 23 
+ 17 
+ 13 
+ 10 
+ 7 
+ 6 
+ 4 
+ 2 
+ 3 
i 
— 
— 
— 
— 
^^_^ 
— 
— 
— 
Precision of Interpolation.—Owing to the magnitude of the second differences, simple interpolation 
near the beginning of the table may give an error up to 5 in the fourth place ; the use of'second 
diffeiences will bring this down to 1 or 2 in the last place, third differences being small. Where third 
diffeiences are greatest, in the neighbourhood of x/a = 0-6, the error may be as large as 3 in the last 
place unless the third difference is used. 
438 
APPENDIX TABLES 
439 
APPENDIX TABLE 2 
Normal ^Distribution. The Distribution Function F of the Normal Distribution, tabulated at 
every Tenth of the Standard Deviation, with First and Second Differences. 
X 
a' 
0-6 
0-1 
0-2 
0-3 
0-4 
0-6 
0-6 
0-7 
0-8 
0-9 
1-0 
1-1 
1-2 
1-3 
1-4 
1-6 
1-6 - 
1-7 
1-8 
1-9 
2-0 
21 
2-2 
23 
2-4 
F. 
0-60000 
0-63983 
0-67926 
0-61791 
0-65542 
0-69146 
0-72675 
0-76804 
0-78814 
0-81694 
0-84134 
0-86433 
0-88493 
0-90320 
0-91924 
0-93319 
0-94620 
0-96643 
0-96407 
0-97128 
0-97725 
0-98214 
0-98610 
0-98928 
0-99180 
^x(+)- 
3983 
3943 
3865 
3751 
3604 
3429 
3229 , 
3010 
2780 
2540 
2299 
2060 
1827 
1604 
1395 
1201 
1023 
864 
721 
597 
489 
396 
318 
262 
199 
d»(-)- 
40 
78 
114 
147 
175 
200 
219 
230 
240 
241 
239 
233' 
223 
209 
194 
178 
159 
143 
124 
108 
93 
78 
66 
53 
44 
X 
a' 
2-5 
2-6 
2-7 
2-8 
2-9 
30 
3-1 
3-2 
3-3 
3-4 
3-5 
3-6 
3-7 
3-8 
3-9 
4-6 
4-1 
4-2 
4-3 
4-4 
F. 
0-99379 
0-99534 
0-99653 
0-99744 
0-99813 
0-99866 
0-99903 
0-99931 
0-99962 
0-99966 
0-99977 
0-99984 
0-99989 
0-99993 
0-99995 
0-99997 
0-99998 
0-99999 
0-99999 
0-99999 
i 
AH+). 
166 
119 
91 
69 
52 
38 
28 
21 
14 
11 
7 
6 
4 
2 
2 
1 
1 
— 
— 
— 
AH-V 
36' 
28 
22 
17 
14 
10 
7 
7 
3 
4 
.JL_ 
_^_ 
F attains the exact value 0-99999 between 4-26 and 4-27. 
Precision of Interpolation.—Simple interpolation may lead to an error of 3 or 4 at most in the fourth 
place of decimals in the region where second differences are large ; the use of the second difference will 
bring this do-vrn to 2 or 3 in the last place, the largest errors tending to occur at the beginning of the 
table, where the third difference may be used if the greatest possible precision is desired. 
oora^eicn^MtJbi^ocbra^oacn^co&H^ocbra^iACn^Go&i^ocbcb 
©oooopoopopoppppppppppppppopppppppppppppppppppppopppppppoppop 
cpsbcbcbcb<bcbebcbcbcbeecbicbcbcbcbcbcbcbcbcbcbcbcbcbcbcbcbc»c»c»c»c»c» 
£^^^^^I^^UWC4CCWWWM(9MMb9(9t9MMMMOOOOtSISCOGOOD<^0>SOlOl|^WCl9(9HOe<OlO(COHCD<l^(Sea>COO 
^^ffia^UHM!OX<ll9OlWt9O!O<01l|>-t9OXa>^Hcea^HQ0l^H>aWCDl(i.CDl(>-00t9a>(DHb9C0WH(D0lOC00ll|>.b900HC>9C0t9O 
OI CI Oi 
©0000000 GO ©00000000000000000000000000000000000000000000000000 
oooooooooooooooooooooooooo<i<i<i<i<i--a--a--a--aoiOiOiOiOiOiC3tC3tC3ttf>-tf>-tf>-totocoboboi—"Ooeooo~aosoiWfcooooosrf»-fcoeooscoo~acoo 
n oi 
oooooooooooooooooooooooooooooooo 
cbcbcbcbcbcDcbcbcbcbcbcbtbtbcbcDcbcbcbcbcbcDCDCDCDcbebebcDCDcbcbcbcbcecocbcbcDCDcDcb 
ecocDiDcococDcDcDcDcee(DcDCDtoQoot)UXUoiia)Ooot>n>a>)v]>a>aa>aciffiOioiit>-^cocob9i-iocDot>>aoiit>-t90xoiwo^£o<uo 
CnOlOlCniF^^lF^000000t0t9H^I^OOCD0000^030l^00t«OCD^Cn00I^CDin00Oint9^t9^OC0CninC>Cnt900t9^^WCDC4^lfkto0000^O 
ot ot 
oooooooooo<Doooooooo<poooooooooooo 
cbcbcbcbcbcbcbebcbcbcbcbcbcbcbcDCDcbcbcocbcocbcbcbiocbcbcbcbcbcbebcbi 
eoeoeoeoeoeoeoeoeoeoeoeocoeoeoeoeoeoeoeoeoeo<Deooooooooooooooo~a~a-q~acsosto«rf»-rf»-cofcoi—"Oeoooosoicoi-'cooscoi—■ ~j & h-i<iwo 
ooxoooo^^^^^aaoaoio<o<^^wcoMi-ioo<oa)aoiwi<90ot)Cicoo>awQoi^ub90i>)a)Ooffiwxt3Wcooi3<oooooi04>--40 
Ot Of DI Ot Ot Ot 
ooooooooooooooooooooooooooooooooo 
icbicbebcbcbcbcbioebcbcbcbcbcbcbcbcbcbcbcbcbcbcDcbcbebcbcbcoicb 
coto(DCDCDtocDecDtececDececotocDco(OcDcecDecDCDcecDXXXXxx>a«)>aao>AO(ii>'4i.w(9HOto>aoicoHe^ib.HX(kH-jcoo 
cocD<scDcocooooooooooooooo^^^iAiAiACncnifk^oot9i^ovoo^oicoi^oinooocnOCncDb9^0iOicoooioocDOooiot9»9i--iZit90iooo 
et at ot 
OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO 
cocbcbcbcbcDracDCDcbcDcbcbcDcbcbcbcbcbcDocDCDCDcDcbcbcDcbcbcDcbcDCDCDCDcocDCDCDcocbcbcDCDW 
tOCD(DCDeCOCDCD(DCDCee(DCDCDtOCDCDCD(DCSffi(0(OCD(0(D(OCDXXXXXX^<09aaO(ll>-WWK)Ote>aait>-K)(D<l(i.HXib.|-i>)tdO 
COCDCOCDCDCOOtO(OCO(OCDXXXXOO<-^-4aO>OlOlit>-il>-WMHtDXOa^(90>acoeOlOil>->]CDOOXi(>.COt9Wt3ewOlOlCOXCOO>XO 
ot et 01 ot 01 ot at 
^OOOOOOOOOOOOO'OOOOOOOOOOOOOOOOOOOOO OOOOOOOO OOOOOOOOOOOOOO 
O«0eoeoeocoeoeoeoeoeo<oeoeoeoeoeoeoeoeotoeoeoeoeoeoeooooooooo~a~a~aososoi0irf»-cofcoi—"eo oo-os ^t30«)i(>-HX4i.H>awo 
oeoeoeoeoeoeoeoeoeoeoeooooooooo~a~a~aoscsoirf»-cofcoi—'©oo-aotbocoaibsooco-aocococoi—'OocooiosOii—'Oi-ac»iK<r>tf>-oiOoo 
ti n at gi ot ot ot ot 
H^OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO 
©cbcbcbcbcbicbcbebcbcbebcbcbcbcbcbcbcbcbcbcbcbcbcbcbcbcbcbcbcbcbcbcbi 
ocDcecDececD(DcD3cetocecDecetocecDeceto(DcDXXXX>av]«]aaoii^wt3Hoxa)(>-t90<^i-ixoiH>)wo 
ocDcs(D(DCDtetocDCDXxxx«]>)ao>oiOiit>-coi-'oxoiit>-Hxoiooiowoiaail>-ooixx^co<a)>]Oio4>-<eo 
n ei ot ot oioioioioi ot 
H_. © © © © © © © © © © © © © © © © © © © © O © © © © © © © © © © © © © © © © © © © © © © © © © © © © 
Oeoeoeoeoeoeoeoeoeoeoeoeooeoeoeoeoeotoeoeooocooooo-q~aosos0irf»-cofcoi-'00o~a0ifcoo~arf»-i--,oooii—' ~a eo © 
Ocoeoeoeoeoeoeoeooooooo~a~a~aosOiOirf»-fcoi-'OOoojcooosfco~JfcoOi~aooooosfco~aoooot(».fcoeooooii—■ ift. ~j co o 
Ot Oi Of Ot Ot Ot Ot 
H^OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO 
OtDCDCDCDCDCDCDOCDCDCD(D(B(DOCecO0OXXXX«]^C:OOtll>-il^COHOX<OlCOO-4O(HXOlH>)COO 
OtS(0(DCDCDtCCDXXXX^-4aOli(>-COI<9H(D-qit>-HXi|>-eco>)CDOOXlt>-(DHI-iOO(CDOCDQMOKCDO 
ot ot 
«k 
II 
1—" 
fcO 
CO 
*. 
pi 
a> 
<i 
oo 
CD 
1—" 
© 
CD 
TABLE 3 
proceeding by Intervals of 0-1 from 0 to 6, and for Values of v from 1 to 20. 
tables by " Student " in Metron, 5, 1925.) 
t. 
0 
0-1 
0-2 
0-3 
0-4 
0-5 
0-6 
0-7 
0-8 
0-9 
1-0 
1-1 
1-2 
1-3 
1-4 
1:5 
1-6 
1-7 
1-8 
1-9 , 
20 
21 
2-2 
2-3 
2-4 
2-5 
2-6 
2-7 
2-8 
2-9 
30 
31 
3-2 
3-3 
3-4 
3-5 
3-6 
3-7 
3-8 
3-9 
4-0 
4-1 
4-2 
4-3 
4-4 
4-5 
4-0 
11. 
0-500 
0-539 
0-577 
0-615 
0-652 
0-686B 
0-720 
0-751 
0-780 
0-806 
0-831 
0-853 
0-872 
0-890 
0-905* 
0-919 
0-931 
0-941 
0-950 
0-958 
0-965 
0-970 
0-975 
0-979 
0-982 
0-985 
0-988 
0-990 
0-991 
0-993 
0-994 
0-995 
0-996 
0-996* 
0-997 
0-997* 
0-998 
0-998 
0-998B 
0-999 
0-999 
0-999 
0-999 
0-999 
0-999B 
0-99911 
1-000 
12. 
0-500 
0-539 
0-578 
0-615 
0-652 
0-687 
0-720 
0-751 
0-780 
0-807 
0-831* 
0-853B 
0-873 
0-891 
0-907 
0-920 
0-932 
0-943 
0-951* 
0-959X 
0-966 ' 
0-971 
0-976 
0-980 
0-983 
0-986 
0-988 
0-990 
0-992 
0-993 ' 
0-994B 
0-995 
0-996 
0-997 
0-997 
0-998 
0-998 
0-998B. 
0-999 
0-999 
0-999 
0-999 
0-999 
0-999B 
" 1-000 
13. 
0-500 
0-539 
0-578 
0-615* 
0-652 
0-687 
0-721 
0-752 
0-781 
0-808 
0-832 
0-854 
0-874 
0-892 
0-907* 
0-921 
0-933 
0-943* 
0-952» 
0-960 
0-967 
0-972 
0-977 
0-981 
0-984 
0-987 
0-989 
0-991 
0-992* 
0-994 
0-995 
0-996 
0-996* 
0-997 
0-998 
0-998 
0-998 
0-999 
0-999 
0'999 
0-999 
0-999 
0-999* 
1-000 
i 
14. 
0-500 
0-539 
0-578 
0-616 
0-652 
0-688 
0-721 
0-752 
0-781° 
0-808 
0-833 
0-855 
0-875 
0-893 
0-908 
0-922 
0-934 
0-944 
0-053 
0-961 
0-967 
0-973 
0-977 
0-981 
0-985 
0-987 
0-989* 
0-991 
0-993 
0-994 
0-995 
0-996 
0-997 
0-997 
0-998 
0-998 
0-999 
0-999 
0-999 
0-999 
0-999 
0-999* 
1-000 
. 15. 
0-500 
0-539 
0-578 
0-616 
0-653 
0-688 
0-721 
0-753 
0-782 
0-809 
0-833 
0-856 
0-876 
0-893 
0-909 
0-923 
0-935 
0-945 
0-954 
0-962 
0-968 
0-973* 
0-978 
0-982 
0-985 
0-988 
0-990 
0-992 
0-993 
0-994* 
0-995* 
0-996 
0-997 
0-998 
0-998 
0-998 
0-999 
0-999 
0-999 
0-999 
0-999 
0-999* 
1-000 
l 
! 
16. 
0-500 - 
0-539 
0-578 
0-616 
0-653 
0-688 
0-721* 
0-753 
0-782 
0-809 
0-834 
0-856 
0-876 
0-894 
0-910 
0-923* 
0-935 
0-946 
0-955 
0-962 
0-969 
0-974 
0-979 * 
0-982 
0-985* 
0-988 
0-990 
0-992 
0-994 
0-994* 
0-996 
0-997 
0-997 
0-998 
0-998 
0-998* 
0-999 
0-999 
0-999 
0-999 
0-999* 
1-000 
17. 
0-500 
0-539 
0-578 
0-616 
0-653 
0-688 
0-722 
0-753 
0-783 
0-810 
0-834 
0-857 
0-877 
0-894* 
0-910 
0-924 
0-936 
0-946 
0-955 
0-963 
0-969 
0-974* 
0-979 
0-983 
0-986 
0-988* 
0-991 
0-902 
0-994 
• 0-995 
0-996 
0-997 
0-997 
0-998 
0-998 
0-999 
0-999 
0-999 
0-999 
0-999 
0-999* 
1000 
18. 
0-500 
0-539 
0-578 
0-616 
0-653 
0-688 
0-722 
0-754 
0-783 
0-810 
0-835 
0-857 
0-877 
0-895 
0-911 
0-924* 
0-936* 
0-947 
0-956 
0-963 
0-970 
0-975 
0-979 
0-983 
0-986 
0-989 
0-991 
0-993 
0-994 
0-995 
0-996 
0-997 
0-997* 
0-998 
0-998 
0-999 
0-999 
0-999 
0-999 
0-999* 
1-000 
19. 
0-500 
0-539 
0-578 
0-616 
0-653 
0-689 
. 0-722 
0-754 
0-783 
0-810 
0-835 
0-857* 
0-878 
0-895 
0-911 
0-925 
0-937 
0-947 
0-956 
0-964 
0-970 
0-975 
0-980 
0-983* 
0-987 
0-989 
0-991 
0-993 
0-994 
0-995 
0-996 
0-997 
0-998 
0-998 
0-998* 
0-999 
0-999 
0-999 
0-999 
0-999* 
1-000 
■ 
20. 
0-500 
0-539 
0-578 
0-616 
0-653 
0-689 
0-722 
0-754 i 
0-783 
0-811 
0-835 
0-858 
0-878 
0-896 
0-912 , 
0-925 
0-937 
0-948 
0-956* 
0-964 
0-970 
0-976 
0-980 
0-984 
0-987 
0-989 
0-991 
0-993 
0-994* 
0-996 
0-996* 
0-997 
0-998 
0-998 
0-999 
0-999 
0-999 
0-999 
0-999 
1-000 
Note.—The methods by which " Student" calculated the Matron tables are explained in notes by him 
and R. A. Fisher in that journal, vol. 5, Part 3, 1925, pp. 18-24. The four figures of those values have been 
rounded up to three in the above table, except when the four-figure value concluded with a 5, in which case 
it is shown in full. In columns in which values greater than 0-9995 occur the first is written 1-000 and the 
remainder left blank. 
441 
442 APPENDIX TABLES 
APPENDIX TABLE 4 
(Reprinted from Table VI of Prof. R. A. Fisher's Statistical Methods for Research Workers, 
Oliver and Boyd, Ltd., Edinburgh, by kind permission of the author and the publishers.) 
5 Per Cent. Points or the Distribution ot z. 
• 
W 
0 
1 
£ 
, 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
22 
23 
24 
25 
26 
27 
28 
29 
30 
60 
00 
Values of v-,. 
1. 
2-5421 
1-4592 
1-1577 
1-0212 
0-9441 
0-8948 
0-8606 
08355 
0-8163 
0-8012 
0-7889 
0-7788 
0-7703 
0-7630 
0-7568 
0-7514 
0-7466 
0-7424 
0-7386 
0-7352 
0-7322 
0-7294 
0-7269 
0-7246 
0-7225 
0-7205 
0-7187 
0-7171 
0-7155 
0-7141 
0-6933 
0-6729 
2. 
2-6479 
1-4722 
1-1284 
0-9690 
0-8777 
0-8188 
0-7777 
0-7475 
0-7242 
0-7058 
0-6909 
0-6786 
0-6682 
0-6594 
0-6518 
0-6451 
0-6303 
0-6341 
0-6295 
0-6254 
0-6216 
0-6182 
0-6151 
, 0-6123 
0-6097 
0-6073 
0-6051 
0-6030 
0-6011 
0-5994 
0-5738 
0-5486 
3. 
2-6870 
1-4765 
1-1137 
0-9429 
0-8441 
0-7798 
0-7347 
0-7014 
0-6757 
0-6553 
0-6387 
0-6250 
0-6134 
0-6036 
0-5950 
0-5876 
0-5811 
0-5753 
0-5701 
0-5654 
0-6612 
0-5574 
0-5540 
0-5508 
0-5478 
0-5451 
0-5427 
0-5403 
0-5382 
0-5362 
0-5073 
0-4787 
4. 
2-7071 
1-4787 
1-1051 
0-9272 
0-8236 
0-7558 
0-7080 
0-6725 
0-6450 
0-6232 
0-6055 
0-5907 
0-5783 
0-5677 
0-5585 
0-5505 
0-5434 
0-5371 
0-5315 
0-5265 
0-5219 
0-5178 
0-5140 
0-5106 
0-5074 
0-5045 
,0-5017 
0-4992 
0-4969 
0-4947 
0-4632 
0-4319 
5. 
2-7194 
1-4800 
1-0994 
0-9168 
0-8097 
0-7394 
0-6896 
0-6525 
0-6238 
0-6009 
0-5822 
0-5666 
0-5535 
0-5423 
0-5326 
0-5241 
0-5166 
0-5099 
0-5040 
Q-4986 
0-4938 
0-4894 
0-4854 
0-4817 
0-4783 
0-4752 
0-4723 
0-4696 
0-4671 
0-4648 
0-4311 
0-3974 
6. 
2-7276 
1-4808 
10953 
0-9093 
0-7997 
0-7274 
0-6761 
0-6378 
0*6080 
0-5843 
0-5648 
0-5487 
0-5350 
0-5233 
0-5131 
0-5042 
0-4964 
0-4894 
0-4832 
0-4776 
0-4725 
0-4679 
0-4636 
0-4598 
0-4562 
0-4529 
0-4499 
0-4471 
0-4444 
0-4420 
0-4064 
0-3706 
8. 
2-7380 
1-4819 
1-0899 
0-8993 
0-7862 
0-7112 
0-6576 
0-6175 
0-5862 
0-5611 
0-5406 
0-5234 
0-5089 
0-.4964 
0-4855 
0-4760 
,0-4676 
0-4602 
0-4535 
0-4474 
0-4420 
0-4370 
0-4325 
0-4283 
0-4244 
0-4209 
0-4176 
0-4146 
0-4117 
0-4090 
0-3702 
0-3309 
12. 
2-7484 
1-4830 
1-0842 
0-8885 
0-7714 
0-6931 
0-6369 
0-5945 
0-5613 , 
0-5346 
0-5126 
0-4941 
0-4785 
0-4649 
0-4532 
0-4428 
0-4337' 
0-4255 
0-4182 
0-4116 
04055 
0-4001 
0-3950 
0-3904 
0-3862 
0-3823 
0-3786 
0-3752 
0-3720 
0-3691 
0-3255 
0-2804 
24. 
2-7588 
1-4840 
1-0781 
0-8767 
0-7550 
0-6729 
0-6134 
0-5682 
0-5324 
0-5035 
0-4795 
0-4592 
0-4419 
0-4269 
0-4138 
0-4022 
0-3919 
0-3827 
0-3743 
0-3668 
0-3599 
0-3536 
0-3478 
Q-3425 
0-3376 
0-3330 
0-3287 
0-3248 
0-3211 
0-3176 
0-2654 
0-2085 
CO. 
2-7693 
1-4851 
1-0716 
0-8639 
0-7368 
0-6499 
0-5862 
0-5371 
0-4979 
04657 
0-4387 
0-4156 
0-3957 
0-3782 
0-3628 
0-3490 
0-3366 
0-3253 
0-3151 
0-3057 
0-2971 
0-2892 
0-2818 
0-2749 
0-2685 
0-2625 
0-2569 
0-2516 
0-2466 
0-2419 
01644 
0 
8 
o 
CO 
*>■ 
OS 
0-7636 
0-6651 
0-5999 
o 
5522 
o 
5152 
o 
OS 
o 
*>- 
0-3908 
0-2913 
o 
C5 
o 
0-9784 
0-8025 
0-7086 
0-6472 
o 
6028 
o 
5087 
0-5189 
0-4574 
0-3746 
0-2352 
eobcbsbstobsbsbsbsbs 
o«ooo<iaoi)^wbSH-" 
OOOOOOOOOO 
H H H H (O » |l3 tO « |fL 
Huissibootoobsao 
OXOl^HOwOltSWOO 
ooocoooooo 
00000000000000030000 
booiooi-'4^oobo~ai-'~J 
CiSi—>h-ictfOOOlOiOCObO 
oooooooooo 
OlOlOlC5Ci-J-J00COCO 
UCXOUOi-' Ol O C5 b© 
HMOiHOM-lOlOO 
ooocoooooo 
CO CO © © I— 1— "—'bOCCCC 
Oi00bSO:O4^C0Oi©~J 
oooooooooo" 
i^.~ji-'Cisoifk.eooiH-oo 
O©i^.oiso~aeooicirf^ 
OOOOOOOOOO 
bsosoi^co^cooibseo 
cseoeooibsboooioo 
oooooooooo 
OlOlOlOlOtOOCbffiO 
~JO0O0COCO©©>—'^bS 
OHOiOvlOQKtS'J 
OOOOOOOOOO 
oiaacioioioioicnoi 
bscsi— o bs oo rf*- h- eo ~j 
^.5DOi~JbOI-'OlC7ll-'C«9 
oooooooooo. 
OlOlSC-JCOXOCH 
I—'~JbOOS*-f— SSOiClOl 
»ocr. ciocnoicco 
oooooooooo 
WWWWWGOCtftf*-tf>-#>- 
cooitoooxicacs-a© 
Values of va. 
OCOOO~JCSOlrf>-C>9bOl-' 
OOOOOOOi—' i—'i—' 
1^ CI CI C! «J 00 C O i-'00 
Oih<i^hOOM5!W 
•JHi bOH" CO -J CO -J C5 GO 
OOOOOOOOOO 
oooodocbebebcbebcbeb 
OOOGtDOl-'bSOJCTasOO 
ccco^icni^.i^-~Ji-'~j~J 
OOOOOOOOOO 
-jdooodooodooooocbeb 
tOOH(fl«fOi«]®H 
oooitototort^-ootoi—'to 
Ol -J 00 CD H-"001—1-JCOC3 
oooooooooo 
~J~J~J~J~J~J00000000 
i^oiaowvoM^a 
ifk-bsooi-'W00r|i.ifk.~a 
WH-aoi^cowoowi^ 
■•oooooooooo 
~J~J~J~J~J~J~J~J0000 
©H-"bSG0>f»-ei~JCOl-'C0 
OlI^WWOtOOWOHOt 
00 O fcS-Oi OWM-JHifk 
oooooooooo 
OSO5C~J~J~J~J~J~J00 
<100©OHW*.OiC»H 
OOCiSi-JH«jCiC;h 
00^-©~J~J#"i-'bSl^-C» 
OOOOOOOOOO 
w*oio-j<OHtaoi-a 
ot^ifaowocMai 
Ol -J CO GO 1—i«J WClOOt 
oooooooooo 
CM35<I«0W!O-J00MO 
tt^i^-OlCOCOOOlbSbSOt 
oooooooooo 
6oioioicicnooaro 
oia>coccooociooi^.ot 
wdHOcsHeorosooo 
ooooooo.ooo 
tfk.if>.t£-£i.CnGlOlOlGSCS> 
iPU-JOOCMOl-JOlf1 
mOM(Oi^CDOhh(» 
1—< 
'0©(»»10!Oi*.WbOH 
I—'I—"b©b©G0G0Ol-J|N3>—• 
Ol>]HClH(Ot90(OOl 
OOOOObSOl^-^Jl^-OlOJ 
oiao; owwofflOOi 
H^.«bStO©I^HCOOl 
l-i|-"000GOlb©Oll^-~J00 
ifkH-jHCntoMOooi 
eoeoOOi-'bsm-cibsbs 
CObSCO~J©l^.~Jl-'00~J 
COtfi-OlbOI-'COOlOli^.l^. 
OOtOOOHMMffiMW 
CO b© -J GO O >—' 00 -J CO >—' 
cucowoaooioooo-a 
rfk.COrf^©00l^.C»C»00Ol 
OOtDCOOHWOMM 
csoi^oooco^Kieoto 
ifk.ooiifk.ifk.^Ji-'Oeoeo 
CSCiCOOOGOl^-t-'GOI-'^J 
OOOONHHHMi^ 
OOOOtOtOOHWObow 
l(i.<IM00D00OOC0W 
CO)—■COl^OOOCOOibSCO 
OOOOHHHHbOl^ 
oodoootboH'wojissw 
i—irfi.eoca^a;rfi-oiccirfi. 
OO00hOOi<1O©00 
(fk.rfi.osrfi.oascoeoif^bs 
OOOOMH^'^H-'lsSl^- 
•JoodotboH'WCJiiw 
<IHCCObSI^Wlf»-{OOl 
rfi-Ol-atOI—lOltOOOCOOO 
rf^~JGOClCO~J~JCO~JOl 
OOOOOHHHBif. 
•^•vidocbcb^-i&baibscc 
bocsi-'bsifk-cc^ioeooo 
tptCOCOOOOCOOfftCOCO 
OOOOOMMH-bSlfk- 
00Gocpa5C5co©G0©-j 
HOOOnmoOHO© 
CSOllfk-OOCO^JOlf'-l-'lfk- 
1 
1—1 
bS 
to 
J^ 
Ol 
p 
00 
1—1 
JO 
24. 
8 
• 
*—• 
e 
CO 
OQ 
0 
Hj 
>i 
to 
w 
O 
a 
o 
N 
CD 
^ ST 
CD 2 
M pU 
p> & 
^B 
Ul 
- a> 
P- 
B" 
<J 
o 
O 
1-4 
B' 
p- 
o 
B 
s- 
00 
►-■• 
o 
p 
o 
ct- 
Cr* 
CD 
*4 
f-" 
DQ 
& 
O 
M 
00 
iS>. 
Co 
<s>. 
8 
ce> 
Co 
P> 
% 
p) 9 
5 
pr ce> 
CD Q 
(ZJ »» 
CD ??* 
s i 
Co 
IS 
■SzJ 
M 
M 
W 
t-« 
Cx 
444 
APPENDIX TABLES 
APPENDIX TABLE 6 
Distribution Function of %* for One Degree of Freedom for Values of %* from %* =*= 0 to 
X2 = 1 by steps of 0-01. 
X1 
0 
0-01 
0-02 
0-03 
0-04. 
0-05 
0-06 
0-07 
0-08 
0-09 
0-10- 
on 
0-12 
0-13 
0-14 
0-16 
0-16 
017 
0-18 
0-19 
0-20 
0-21 
0-22 
0-23, 
0-24 
0-25 
0-26 
0-27 
0-28 
0-29 
0-30 
0-31 
0-32 
0-33 
0-34 
0-35 
0-36 
0-37 
0-38 
0-39 
0-40 
0-41 
0-42 
0-43 
0-44 
0-46 
0-46 
0-47 
0-48 
0-49 
0-50 
P 
1-00000 
0-92034 
0-88754 
0-86249 
0-84148 
0-82306 
0-80650 
0-79134 
0-77730 
0-76418 
0-75183 
0-74014 
0-72903 
0-71843 
0-70828 
0-69854 
0-68916 
0-68011 
0-67137 
0-66292 
0-65472 
0-64677 
0-63904 
0-63152 
0-62421 
0-61708 
0-61012 
0-60333 
0-59670 
0-59022 
0-58388 
0-57768 
0-57161 
0-56566 
0-55983 
0-55411 
0-54851 
0-54300 
0-53760 
0-53230 
0-52709 
0-52197 
0-51694 
0-51199 
0-50712 
0-50233 
0-49762 
0-49299 
0-48842 
0-48393 
0-47950 
A 
7966 
3280 
2505 
2101 
1842 
1656 
1516 
1404 
1312 
1235 
1169 
1111 
1060 
1015 
974 
938 
905 
874 
845 
820 
795 
773 
752 
731 
713 
696 
679 
663 
648 
634 
620 
607 
595 
583 
572 
560 
551 
540 
530 
521 
512 
503 
495 
487 
479 
471 
463 
457 
449 
443 
436 
X2 
0-50 
0-51 
0-52 
'0-53 
0-54 
0-55 
0-56 
0-57 
0-58 
0-59 
0-60 
0-61 
0-62 
0-63 
0-64 
0-65 
0-66 
0-67 
0-68 
0-69 
0-70 
0-71 
0-72 
0-73 
0-74 
0-75 
0-76 
0-77 
0-78 
0-79 
0-80 
0-81 
0-82 
0-83 
0-84 
0-85 
0-86 
0-87 
0-88 
0-89 
0-90 
. 0-91 
0-92 
0-93 
0-94 
0-95 
0-96 
0-97 
0-98 
0-99 
1-00 
P 
0-47950 
0-47514 
0-47084 
0-46661 
0-46243 
0-45832 
0-45426 
0-45026 
0-44631 
0-44242 
0-43858 
0-43479 
0-43105 
0-42736 
0-42371 
0-42011 
0-41656 
0-41305 
0-40959 
0-40616 
0-40278 
0-39944 
0-39614 
0-39288 
0-38966 
0-38648 
0-38333 
0-38022 
0-37714' 
0-37410 
0-37109 
0-36812 
0-36518 
0-36227 
0-35940 
0-35655 
0-35374 
0-35096 
0-34820 
0-34548 
0-34278 
0-34011 
0-33747 
0-33486 
0-33228 
0-32972 
0-32719 
0-32468 
0-32220 
0-31974 
0<31731 * 
A 
436 
430 
423 
418 
411 
406 
400 
395 
389 
384 
379 
374 
369 
365 
360 
355 
351 
346 
343 
338 
334 
330 
326 
322 
318 
315 
311 
308 
304 
301 
297 
204 
291, 
287 
285 
281 
278 
276 
272 
270 
267 
264 
261 
258 
256 
253 
251 
248 
246 
243 
241 
4 
APPENDIX TABLES 445 
APPENDIX TABLE 7 
Distribution Function of %% for One Degree of Freedom for Values of %% from 1 to 10 by 
Steps of 01. 
Z2 
10 
1-1 
1-2 
1-3 
1-4 
1-5 
1-6 
1-7 
1-8 
1-9 
2-0 
21 
2-2 
2-3 
2-4 
2-5 
2-6 
2-7 
2-8 
2-9 
3-0 
3-1 
3-2 
3-3 
3-4 
3-5 
3(> 
3-7 
3-8 
3-9 
4-0 
4-1 
4-2 
4-3 
4-4 
4-5 
4-6 
4-7 
4-8 
4-9 
5-0 
5-1 
5-2 
5-3 
5-4 
5-5 
P 
0-31731 
0-29427 
0-27332 
0-25421 
0-23672 
0-22067 
0-20590 
019229 
017971 
0-16808 
015730 
0-14730 
013801 
012937 
0-12134 
011385 
0-10686 
010035 
0-09426 
0-08858 
008326 
0-07829 
007364 
0-06928 
000520 
006137 
0-05778 
0-"05441 
005125 
004829 
004550 
0-04288 
0-04042 
003811 
003594 
003389 
0-03197 
003016 
0-02846 
0-02686 
002535 
002393 
002259 
002133 
002014 
001902 
A 
2304 
2095 
1911 
1749 
1605 
1477 
1361 
1258 
1163 
1078 
1000 
929 
864 
803 
749 
699 
65 f 
609 
568 ' 
532 
497 
465 
436 
408 
383 
359 
337. 
316 
296 
279 * 
262 
246 
231 
217 
* 205 
192 
181 
170 
160 
151 
142 
134 
126 
119 
112 
106 
X* 
5-5 
5-6 
5-7 
5-8 
5-9 
6-0 
6-1 
■ 6-2 
6-3 
6-4 
6-5 
6-6 
6-7 
6-8 
6-9 
7-0 
71 
7-2 
7-3 
7-4 
7-5 
7-6 
7-7 
7-8 
7-9 
8-0 
8-1 
8-2 
8-3 
8-4 
8-5 
8-6 
8-7 
8-8 
8-9 
9-0 
9-1 
9-2 
9-3 
9-4 
9-5 
9-6 
9-7 
9-8 
9-9 
10-0 
P 
0-01902 
0-01796 " 
0-01697 
0-01603 
0-01514 
0-01431 
001352 
0-01278 
0-01207 
0-01141 
0-01079 
001020 
000964 
0-00912 
0-00862 
0-00815 
000771 
0-00729 
000690 
000652 
0-00617 
000584 
0-00552 
000522 
000494 
000468 
0-00443 
000419 
000396 
000375 
000355 
000336 
000318 
000301 
000285 
000270 
0-00256 
000242 
000229 
000217 
0-00205 
0-00195 
0-00184 
000174 
000165 
000167 
A 
106 
99 
94 
89 
83 
* 79 
74 
71 
66 
62 
59 
56 
52 
50 
47 
44 
42 
39 
38 
35 
33 
32 
30 
28 
26 
25 
24 
23 
21 
20 
19 
18 
17 
16 
15 
14 
14 
13 
12 
12 
10 
11 
10 
9 
8 
8 
0-999 
0-001 
0-0001 
Appendix Diagram.—Contour Lines of the Surface P = f(v, jf). 
INDEX 
(Rpferences o 
Abortion, distribution of women according to term 
of, (Table 1.23), 26. 
Abrupt distributions, corrections for grouping to, 
79 ; refs., 85-6. 
Absolute moments, 56 ; Liapounoff's inequality 
for, (Exercise 3.H), 88. 
Accidents, exemplified by Poisson distribution, 
124. 
Adyanthaya, N. K., refs., distribution of frequency 
constants in small samples, (under Poarson), 
228. 
Age, correlation with highest audible pitch, 
(Table 14.1), 325; (Example 14.1), 331. 
Agreement, coefficient of, 427—9 ; significance of, 
429-35. 
Agricultural Research Institute, Oxford, data 
from Roport of, (Table 1.9), 9. 
Alcoholism and crimo, Qoring's data on, (Tablo 
14.6), 350. 
and health, (Exercise 14.12), 306. 
Ammon, O., data from (hair and oyo-colour), 
(Tablo 12.4), 300. 
Anthropometric Committee of British Association, 
data from "Report of, (Table 1.7), 8; 
(Tablo 1.10), 10. 
Antimodo, definition of, 35. 
Approximations to sampling distributions, see 
(Sampling distributions. 
Arithmetic mean, arc Mean, arithmetic. 
Aroian, L. A., fitting of Type B distribution to 
data, (Example 0.4), 156 ; refs., Typo B 
sbrios, 160. 
Arrays, in bivariato distributions, 327. 
Association, generally, 308-23; cootticionts of, 
310-13; partial, 313-17 ; illusory, 317-18. 
Asymmetrical distributions, 10 ; see Skowi-ioHH. 
Attributes, sampling of, 197-201 ; in Poisson 
distribution, (Kxorciso 8.2), 203 ; in finite 
populations, (Exorcise 8.3), 203. 
Australian marriagos, distribution of, (Table 1.8), 
9; moments of, (Examplo 3.1), 50-2; 
/*! and /32 of, (Example 3.10), 82. 
Average, see Moan. 
corrections to moments, 7,4-5 ; see also 
Sheppard's corrections. 
6, (sampling value of f}u measure of skownass), 
279-80. 
Babington Smith, B., data from, on random 
numbers, (Table 8.3), 189 ; Random Sampling 
Numbers, 193, 197 ; refs. (under Kendall), 
to pages.) 
202 ; distribution of Spearman's p, problem 
of m rankings, method of paired 6om- 
parisons, 436. 
Baker, G. A., distribution of means in Type A 
serios, (Exercise 10.7), 252. 
Bayes, T., refs., doctrine of chances, 183. 
Bayes' theorem, 175-7 ; postulate 176-8 ; 
comparison with maximum likelihood, 178-80 ; 
in estimating proportion of attributes, 200 ; 
(Exorcise 8.5), 203. 
Beans, distribution of, (Tablo 1.15), 20 ; histogram 
of, (Figure 1.4), 20 ; fitting of Poarson 
distribution to, (Example 6.1), 143-4; Gram- 
Charlier series fitted to, (Example 6.2), 151. 
Bernouilli polynomials, definition of, 58. 
numbers, footnote, 69; 71, 78. 
Bernstein, S., refs., extension of central limit 
theorem, 183. 
p\, /?2 (skewness and kurtosis), 81 ; standard orrors 
of, 225 ; sampling distributions of, 279-80, 
(Exercise 11.17), 289; generalised /3, 82. 
Z?-function, in summing binomial, 120. 
Bias in sampling, 187-90 ; in choosing plants, 
(Examplo 8.1), 187-8 ; in scale roading, 
(Example 8.2), 188 ; in roading randomising 
machine, (Examplo 8.3), 189; in orop- 
roporting, (Examplo 8.4), 189-00. 
Binomial distribution, general proportios, 116-20; 
moments of, 117, (Examplo 3.2), 52; 
distribution function of, 119-20; yx and yB 
of, (Examplo'3.17), 82; factorial moments 
of, (Exorcise 3.0), 87 ; limiting form, 
(Example 4.6), 103 ; arising from mixed 
population, 122-4; with negative index, 
125-6 and (Exercise 5.7), 130 ; bivariate 
form, 133-4 ; cninulants of, (ISxorciso 5.1), 
135 ; incomplete moments of, (Exercises 
5.2 and 5.3), 135 ; in sampling of attributes, 
198 ; distribution of means of, (Example 
10.8), 243. 
Birth-ratos, distribution of in Local Government 
Areas, (Table 1.1), 3; frequency polygon 
of, {Table, 1.1), 4. 
Biserial ry, 350-8. 
Hivariate binomial distribution, 133-4. 
froquoncy-distributions, 10-22. 
moments and cumulantB, 79-81 ; standard 
errors of, 211 ; ^-statistics and oumulants, 
281-3. 
normal distribution, 22 ; (Example 3.15), 
79-80; moments of, (Exorcise 3.15), 89; 
7 » 
I 
448 m. 
as limit of bivariate binomial, 133-4 ; 
correlation and regression of, 334-6 ; 
multivariate form, 376-7. 
Bivariate Poisson distribution, (Exercise 5.8), 136. 
Borel, E., refe;, Traiti^du Oalcul des Probability, 
22, 183. 
Bortkiewicz, L. von, data from on suicide, (Table 
. 1.6), 7. 
Bose, S. S., distribution of variance ratio, 
(Exercise 14.8), 365. 
Bowley, A. L., refe., F. T. Edgeworth's 
Contributions to Mathematical Statistics, 160 ; 
representative method, 202. 
Brood-mares, distribution of fecundity in, (Table 
1.20), 24. 
Call discount rate, distribution of weekly returns 
according to, (Table 1.26), 28. 
Camp, B. H., refe., distribution functions of 
binomial and hypergeometrio, 134. 
Card-shuffling, tested by xs, (Example 121), 297-9; 
tested by rankings, (Example 16.5), 420. 
Carleman, T., criteria for uniqueness in the 
problem of moments, 109 ; refe., Lea fonctions 
qiuxsi-analytiques, 114. - 
Carver, H. C, Sheppard corrections for discrete 
variables, 85. 
Cauchy distribution, (Example 3.12), 67-8 ; 
characteristic function of, (Example 4.2), 95-6 ; 
distribution of mean of samples from, 
(Example 10.1), 233-4 and (Example 10,15), 
247. 
Cave, B. M., refe., sampling of correlation 
coefficient (under Co-operative Study), 363. 
Census of Population} data from Housing Report, 
(Table 1.24), 27. 
of Production, data from, on size of firms, 
(Table 1.17), 23. 
Central Limit Theorem, 180-3. 
Characteristic functions, as moment-generating 
functions, 54; general theory of, 90-115; 
limiting properties of, 99-104; multi- , 
variate, 104-5 ; conditions for a function 
to be, 98-9; in sampling distributions, 
242-6. 
Charlier, C. V. L., Types A and B series, 147 (see 
Gram-Charlier series); refe., expansion of 
frequency functions, 160. 
Cheshire, L., refe., significance of correlation 
coefficient, (under E. S. Pearson), 363. 
Chi-square, see #!. 
^'-distribution; generally, 290-307 ; properties 
of, 292-7 ; in 2 x 2 tables, 303 ; correction 
for continuity, 303-4; as square' 
contingency, 319. 
Cholera, inoculation against, (Table 12.6), 302 ; 
(Example 13.1), 309; (Example 13.2), 
311-12; (Example 13.3), 313. 
Church, A. E. R., sampling moments, 256 ; refe., 
284-5. 
Circular triads, in preferences, 423. 
Class-frequency, definition, 2. 
Class-interval, definition, 2; ambiguities in, 5. 
Cloudiness, distribution of days according to, 
(Table 1.11), 10. 
Cochran, W. G., refs., x2-distribution, 305. 
" Cocked-hat " us synonym for unimodal, 29. 
Coefficients of association, contingency, correlation, 
etc., see under Association, Contingency, 
Correlation, etc. 
Coin-tossing, as example of sampling of attributes, 
(Example 8.9), 198. 
Colligation, coefficient of, 311. 
Combinatorial method, in sampling of 1c-statistics, 
see ^-statistics. 
Comparisons, paired, see Paired comparisons. 
Comrie, L. J., refs., Tables of arc, tan x and log 
(1 + xz), 160. 
Concentration, coefficient of, 43 ; curve of, 43-4 
- and (Figure 2.3), 44. 
Concordance, coefficient of, 411 ; see also m 
rankings. 
Consistence, coefficient of, 425. 
Contingency, 318-22 ; coefficient of, 319. 
Continuity correction to %2, 303-5. 
Continuous frequency functions, 13; sampling 
from, 197. 
Continuum, probability in, 170. 
Co-operative Study on correlation coefficient, refs., 
363. 
Cornish, E. A., refe., moments and oumulants in 
specification of distributions, 160. 
Corrections for grouping in calculation of moments, 
30, 41 ; when distribution is abrupt, 79, 
refe., 85-6 ; see also Sheppard's corrections. 
Correlation, coefficient of product-moment, 
generally, 324-67 ; definition, 329 ; calculation 
of, 330—4 ; in bivariate normal distribution, 
334 ; sampling of, 336-48 ; standard error 
of, (Example 9.6), 211; Fisher's 
transformation of, 345 ; tables of (David), 345. 
coefficient of multiple correlation, 380-1 ; 
sampling of, 381-5. 
coefficient of partial, 368-79; definition, 
370 ; in terms of coefficients of lower ordors, 
372 ; geometrical interpretation, 372-3 ; 
examples of, (weather and crops, Example 
15.1), 373-5, (crime and religion, Example 
15.2), 375-6 ; in multivariate normal 
distribution, 376-8 ; sampling distribution of, 
378-9. 
intra-class,, 358-62. 
rank, generally, 388-421 ; Spearman's 
coefficient, 388-91 ; sampling of, 394-403; 
coefficient t, 391-3 ; sampling of, 403-8. 
See also m rankings. 
INDEX 
449 
Correlation ratios, definition, 361 ; sampling dis 
tribution in uncorrelated normal population 
362-3 ; relation with multiple correlation 
381 ; for ranked data (Wallis), 437. 
Covarianoe, definition, 79 ; notation for, 204 
calculation of, 330; distribution of in 
normal samples, 339-42. 
Cows, distribution of according to age and milk- 
yield, (Table 1.26), 27. 
Craig, C. C, corrections to moments of discrete 
distribution, 77, refs., 86 ; (Exercise 3.13), 
88; sampling of cumulants, 266, refs., 
286. 
Cramer, H., convergence of Gram-Charlier series, 
161-2,169, refs., 160 ; central limit thoorem, 
181-3; distribution of a ratio (Exercise 
10.8), 262 ; refs., Random Variables and 
Probability Distributions, 114, 183, 260. 
Crime and alcoholism, Goring's data on, (Table 
14.6), 366. 
correlation with religion, (Example 15.2), 
•376. 
Crop-reporters, bias in, (Example 8.4), 189-90. 
Crops, correlation with weather, (Example 16.1), 
373. 
Cuckoo's eggs, distribution of length of, (Exercise 
14.13), 366. 
Cumulants, definition, 60 ; invariantive proportios 
of, 61 ; relations with moments, 61-4; 
existence of, 64-5 ; calculation of, 66-8 ; 
in bivariate case, 80 ; Sheppard's 
corrections to, 78 and (multivariate case) 80-1 ; 
generating fuuotions for, 90 ; of normal 
distribution, 129. 
Cumulative function, 90. 
Curve of concentration, see Concentration. 
Dairy farms, distribution according to coats of 
milk-production, (Table 1.9), 9. 
David, P. N., distribution of difference of Typo 
HI variates, (Exercise 10.6), 252 ; Tables 
of tlie Correlation Coefficient, 345, ivfa.f 
363. 
Davies, O. L., refs., estimation of standard 
deviation, 228. 
Deaf-mutes, distribution of children of, (Tablo 
1.19), 24. 
Deaths, from scarlet fevor, (Table 1.3), 6 ; 
distribution of, according to age at death, 
(Table 1.12), 11. 
Deciles, definition, 36 ; interdecile rango, 38 ; 
standard errors of, 225. 
de Finetti, B., refs., calculation of mean differonco,- 
47. 
Degrees of freedom, in ^'-distribution, 292 ; in 
contingency table, 299. 
de la Valine Poussin, C. J., refs., Govts a"analyse, 
footnote, 233. 
■ A.S. 
Domoivre, A., discoverer of normal distribution, 
131. 
Denjoy, A., theorem on uniqueness of quasi- 
analytic functions, (Exercise 4.10), 115. 
Dice, throws with, Weldon's data (Table 1.14), 19 j 
(Table 1.16), 23 ; (Example 8.10), 199; 
(Table 12.5), 301. 
Digits, distribution of, from telephone directory, 
(Table 1.4), 6. 
Direct probability, see Probability. 
Dirichlet integrals, in Inversion Theorem, 91-2. 
Discontinuous frequenoy-functions, 12. 
variate, examples of distribution according 
to, 6-7. 
Dispersion, measures of, 38-48 ; see also Standard 
Deviation. 
Distribution curve, 36-7. 
< functions, 12-15 ; determined by 
characteristic function, 91-4 ; limiting properties of, 
99-104, 110-13; determination by 
moments, 105-10; standard .distributions, 
116-63, see also Standard , Distributions ; 
relation with probability, 172-3. 
Doodson, A. T., relation of mean, median and 
mode, 35, 46 ; refs., 47. 
D6rge, K., refs:, axiomatisatioii of von Mises' 
thoory of probability, 183. 
Dressel, P. L., refe., seminvariants, 84-5, 285. 
Edgeworth, F. Y., citing Weldon's dice data, 
(Table 1.14), 19 ; form of Gram-Charlier 
sories, 148-9 ; rofs., law of error, 160. 
Edwards, J., rofs., Integral Calculus, footnote, 68 
and footnote, 221. 
Eells, W. C, formulae for probable errors of 
'correlation coefficients, 410; refe., 436. 
Egyptian skulls, distribution of, (Table 1.22),' 
25. 
Eldorton, E. M., data on hoalth of son and 
alcoholism, of father, (Exorcise 14.12), 366. 
Elderton, Sir William P., Hardy's method of 
calculating iuotorial momonts, 59 ; corrections 
for moments whon the distribution is 
symmetrical, 85 and (Exercise 3.10), 87-8; 
fitting of Poarson distributions, 143 ; on 
Gram-Charlior series, 153 ; tables of #2, 293 ; 
refs., Frequency Curves and Correlation, 85, 
160. 
Error, standard, sec Standard Error. 
Estimates, of proportions of attributes, 199-200 ; 
in largo samples generally, 201-3; of a 
ranking, 421. 
Euler-Maclaurin sum formula, 69. 
Expectation, 84. 5'ee also Mean Valuos. 
Extreme valutas of sample, distribution of, 217-22. 
Eye-oolour, rotation with hair-colour, (Example 
12.3), 299 ; in parent and child, (Example 
i 13.4), 314. 
* 
450 
INDEX 
Factorial moments, 66-60; definition, 66; in 
terms of ordinary moments, 57-8 ; 
calculation of, 68-60; Sheppard's corrections 
to, 77-8 i generating function for, 90; of 
binomial, 118 ; of hypergeometric, 
(Exercise 6.4), 136. 
Families deficient in room space, distribution of, 
(Table 1.24), 27. 
Fathers, height of, distribution of sons according 
to, (Table 14.3), 327. 
Fay, E. A., data from Marriages of the Deaf in 
America, (Table 1.19), 24. 
Fecundity, distribution of brood-mares according 
to, (Table 1.20), 24. 
Feeding and teeth in infants, (Example 12.6), 304. 
Fieller, E. C, refe. distribution of a ratio, 260. 
Filon, L. N. G., refs. (under Pearson), standard 
errors of frequency constants, 229. 
Finite populations, sampling from, 283-4. 
Finney, D. J., sampling of variance ratio, (Exercise 
14.8), 366. 
Firms in the Food, Drink and Tobacco Trades, 
distribution of, (Table 1.17), 23. 
First hands at whist, distribution of, (Table 6.4), 
128; (Example 12.1), 299-300. 
First Limit Theorem, 100-1; converse of, 101-3. 
Fisher, Arne, modified form of Gram-Charlier 
series, 163; fitting of Type B to data 
(Example 6.4), 166; refs., Frequency 
Ourve8, 160. 
Fisher, R. A., Sheppard's corrections, 76-7 ; 
introduction of word " cumulant," 86; 
random sampling numbers, 194, 197 ; 
distribution of mean deviation, 216 ; 
distribution of extreme, 220 ; z -distribution, 
see 2-distribution; ^-statistics, 266, 268 ; 
measures of departure from normality, 
(Exercise 11.16), 289; tables of %*, 293; 
normal approximation to #a, 294-6 ; 
distribution of £a when parameter estimated 
from data, 301; distribution of variances 
and covariance in normal samples, 340 ; 
transformation of correlation coefficient, 
346 ; distribution of multiple correlation 
coefficient (Exercises 16.6 and 16.7), 387 ; 
refs., moments and cumulants in 
specification of distributions, 160 ; mathematics of 
statistics, 184; inverse probability, 184; 
distribution of mean deviation, 228 ; 
distribution of extreme values, 228 ; 
distribution of correlation coefficient, 260, 363; 
distribution of well-known statistics, 260 ; 
applications of Student's distribution, 260 ; 
^-statistics, 286; distribution of %*, 306 ; 
distribution of partial coefficients, 386; 
distribution of multiple correlation, 386. 
Food, Drink and Tobacco Trades, distribution of 
firms, (Table 1.17), 23. 
Footrule, Spearman's, ■ 436. 
Franel, J., theorem on distribution of digits in 
mathematical tables, footnote, 193. 
Frechet, M., proof of Second Limit Theorem, 112, 
113, refs., 114. 
Frequency (class-frequency), definition of, 2. 
Frequency-distributions, generally, 1-28 ; genesis 
of, 18. 
Frequency-functions, 12-16 ; discontinuous, 12; 
determined by characteristic function, 91-4; 
normalisation of, 166-9. 
Frequency-polygons, 4 ; bivariate form, 20. 
Friedman, M., tests of significance in m rankings, 
420, refs., 436. 
Frisoh, B., moments of binomial, 68 and (Exercise 
5.3) 136 ; refs., moments and cumulants, 
86, 134; correlation'analysis, 386. 
Galbrun, H., convergence of Gram-Charlier series, 
162. 
Galton, Sir Francis, data from Natural Inheritance, 
(Example 13.4), 314; correlation, 363. 
Galton's ogive, see Distribution curve. 
Galvani, L., refs. (under Gini), median for 
qualitative characteristics, 47. 
yx, y2 (skewness and kurtosis), 82. 
/"-function, in summing Poisson series, 122. 
Garwood, F., data from, (Table 12.1), 297-8; 
refs., fiducial limits for Poisson distribution, 
306. ** 
Geary, B. C, distribution of measures of departure 
from, normality, (Exercise 11.16), 289; 
distribution of a ratio, (Exercise 10.9), 263 
and refs., 260. 
Geiger, H., see Butherford. 
Generating functions, for moments and cumulants, 
' 90. See also Characteristic functions. 
Geometric mean, see Mean, geometric. 
German women, distribution of suicides, (Table 
1.6), 7. 
Gilby, W. H., data from, on intelligence and 
clothing in schoolchildren, (Table 13.1), 320. 
Gini, C, coefficient of concentration, 43; 
coefficient of mean difference, 42 ; standard 
error, 216, 226 ; refs., mean difference, 47 ;. 
median for qualitative characteristics, 47. 
Qlosaind morsitans (tsetse fly), distribution of 
trypanosomes in, (Table 1.13), 12. 
Goring, C., data on alcoholism and crime, (Table 
14.6), 366. 
Gosset, W. S., see " Student." 
Grades, 408 ; relation with ranks, 408-10. 
Graduation curve, see Distribution curve. 
Grain, distribution of plots according to yield of, 
(Table 1.18), 23. 
Gram-Charlier series, Type . A, 147-64 ; Edge- 
' worth's form, 148-60; fitting to bean 
data, s (Example 6.2), 161; distribution of 
INDEX 
451 
means from, (Exercise 10.7), 252 ; Type B, 
154-6 ; Type C, 160. 
Greenwood, M., data on industrial accidents, 
(Table 5.3), 124; on inoculation against 
cholera, (Example 13.1), 309. 
Grouping of frequency-distributions, corrections 
for, see Sheppard's corrections. 
Gumbel, E. J., distribution of mth values, 220-2 
and refe., 228. 
Haines, J., refe. (under Pearson), use of range, 228. 
Hair-colour, relation with eye-colour, (Example 
12.3), 299-300. 
Haldane, J. B. S., refe., oumulants and moments 
of binomial, 134 and (Exercise 5.1), 135 ; 
X2 with small expectations, 305 ; 
normalisation of frequenoy functions, 305 and 
(Exercise 12.1), 306. 
Half-invariants (seminvariants), see Cumulants. 
Hall, Sir A. D., data on yield of grain, (Table 1.18), 
23. 
Hall, P., refs. distribution of mean from rectangular 
population, 250 ; multiple correlation, 386. 
Hamburger, H., problem of moments, 107 and 
refs., 114. 
Hardy, Sir G. F., calculation of factorial moments, 
59. 
Harmonic mean, see Mean, Harmonic. 
Hartley, H. O., distribution of range, 224 and 
refs., 228. 
Health of son and alooholism of parent, (Exoroise 
.14.12), 366. 
Height, distribution of men according to, (Table 
1.7), 8 ; frequency-polygon of, (Figure 1.3), 
8 ; mean, (Example 2.1), 30-1 ; median, 
(Example 2.4), 35 ; quartiles, (Example 
2.5), 36 ; distribution curve, (Figure 2.2), 
i 37 ; mean deviation and standard 
deviation, (Example 2.6), 39-40 ; mean 
difference, (Example 2.8), 45-6 ; factorial and 
ordinary moments (Example 3.7), 59-00 ; 
fitted to normal ourve, (Table 5.5), 132 ; 
standard error of mean, (Example 9.1), 207. 
, in fathers and sons, (Table 14.3), 327 ; 
correlation, (Example 14.5), 337. 
, distribution of plants according to, (Table 
8.1), 187. 
Hellman, M., data on teeth and feeding in infants, 
(Example 12.5), 304. 
Helly, W., theorems on convergent sequences of 
functions, 100, 112. 
Helmert, W., distribution of mean deviation, 215 ; 
distribution of sums of squares, 250, 305. 
Henderson, J., refs., expansion in tetrachorio 
functions, 160. 
Hermite, C, polynomials, 145, 160. See Toheby- 
oheff-Hermite polynomials. 
Heron, D., refs. (under Pearson), coefficients of 
association, 322. 
Heterotypic frequency-distributions, 145. 
Highest audible pitch and age, bivariate 
distribution according to, (Table 14.1), 325 ; 
correlations and regressions, (Example 14.1), 
331 ; correlation ratios (Example 14.11), 
351-2. 
Hilferty, M. M., limiting distribution of %2, 294-6 
and refs. (under Wilson), 305. 
Hilton, J., refs., inquiry by sample, 202. 
Histogram, 4; bivariate, 20. 
Hojo, T., refe., distribution of median, quartiles 
and semi-interquartile range, 228. 
Homoscedastic distributions, 335. 
Hooker, R. H., data on weather and crops, 
(Example 15.1), 373 and refe., 386. 
Hotelling, Hi, distribution of Spearman's p, 401 
s and refs., 436. 
Hsu, C. T., sampling cumulants of normal 
distribution, 275 and refe., 286. 
Hypergeometrio distribution, generally, 126-8; 
moments of, 127 ; example of, (Table 5.4), 
128 ; factorial moments of, (Exercise 6.4), 
135 ; limiting forms of, 132-3. 
funotion, 127. 
Hypothetical population, 187. 
Illusory association, 317. 
Income, distribution of persons by, (Table 1.2), 3 ; 
histogram of, (Figure 1.2), 4; distribution 
ourve of, (Figure 2.1), 37. 
Incomplete moments, 43 ; of binomial, refs., 134 
and (Exercises 5.2, 5.3), 135. 
Independence, definition, 21 ; in association tables, 
309 ; in bivariate frequency tables, 326. 
Index, distribution of, see Ratio. 
Induction, in finding sampling distributions, 
246-8. 
Inequalities for moments, 56 ; rofs. (Shohat), 86; 
Liapounoff's, 56 and (Exercise 3.14), 88. 
Inoculation against cholera, see Cholera. 
against tuberculosis in oattle, (Exercise 12.7)r 
307. 
Intelligence, distribution of schoolchildren 
according to, (Example 13.6), 320. 
Interdecile range, 38. 
Interquartile range, 38 ; standard error of semi- 
interquartile range, (Example 9.8), 214. 
Interval (class-interval), see under Class. 
Intra-class correlation, '358-62. 
Inverse probability, 176 ; see Bayes' theorem. 
Inversion theorem, 91-8 ; examples of use of, 
94-8. 
Irregular Kollektiv of von Mises, 171-2. 
Irwin, J. O., distribution of means, (Exercises 
10.3 and 10.4), 251 and refe., 251. . 
GG* 
452 INDEX 
J-shaped distribution, 10. 
Jackson, Dunham, on mdeterminaoy of median, 
46 and refe., 47. 
Jeffreys, H., logic of probability, 165; refs., 
Theory of Probability, 184. 
Jensen, A., refs., representative method in 
statistics, 202. 
Johannsen, W., bean data cited by Pretorrus, 
(Table 1.15), 20. * 
Johnson, W. E., logic of probability, 166 ; refs., 
Logic, 184- 
Jordan, C, refe., Statistique maiMmatique, 160; 
Type B series (Exercises 6.4 and 6.6), 161-2. 
Jorgensen, N. R., tables of Tohebyoheff-Hermite 
polynomials, 147, 151; refs., Undersagelser 
over Frequensflader og Korrelation, 160. 
A-statistics, definition 256; general properties, 
256-60 ; sampling cumulants^ of, 260-89 ; 
in multivariate case, 281-3. 
k, as criterion of type in Pearson distributions, 140. 
Kelley, T. L„ tables of ^2, 293 ; tables of 
correlation coefficient, 376 and refs., 386 ; refs., 
Kelley Statistical Tables, 386.- 
Kendall, M. G., data from, (Table 1.4), 6 ; Shep- 
pard corrections, 76; multivariate oumu- 
lants, 80; randomness, 172 ~, maximum 
likelihood, 179 ; data from, (Table 8.3), 189 ; 
Random Sampling Numbers, 193, 197; 
refs., Sheppard corrections, 86; 
multivariate sampling formulae, 86; 
randomness, 184; maximum likelihood, 184; 
randomness and random sampling numbers, 
202; ^-statistics, 285; rank correlation 
and paired comparisons, 436. 
Kendall, S. F. H., refe., distribution of Spearman's 
p, 436. 
Keynes, J. M., (now Lord Keynes), on probability, 
■ 165; refe., Treatise on Probability, 184. 
Kiser, C. V., refe., pitfalls in sampling, 202. 
Koga, Y., data from, (Table 14.1), 326. 
Kollektiv of von Mises, 171-2. 
Kolmogoroff, A., probability as abstract ensembles, 
165 ; refs., Orundbegriffe der Wahrschein- 
lichkeitstheorie, 184. 
Kondo, T., refs., standard error of mean square 
contingency, 321, 322. 
Kullbaek, S., refe., distributions and characteristic 
functions, 251, 363; arid (Exercises 10.2 
and 10.5) 261, 262, (Exercise 14.7), 364-5. 
Kurtosis, 52. 
Lagrange, J. L., distribution of mean from 
rectangular population, 260. 
Laplace, P. S. (Marquis de), characteristic 
functions, 113; continued fraction for the 
normal distribution, 129-30; succession 
rule, (Example 7.7), 177 ; early work on 
Central Limit Theorem, 180. 
Large samples, approximations in theory of, 201-2. 
See Standard Error. 
Laterality of hand and eye, (Exercise 13.6), 323. 
Latter, O. H., data on length of cuckoo's eggs, 
(Exercise 14.13), 366. 
Lawley, D. N., sampling cumulants of ^-statistics, 
276 and refs. (under Bjsu), 286. 
Least squares, in determination of regression lines, 
328-9, 368. 
Lee, A., data from, on fecundity of mares, (Table 
1.20), 24; on stature of fathers and sons, 
(Table 14.3), 327 ; refs., sampling of 
correlation coefficient, (under Co-operative Study), 
363. 
Leibniz, G. W., logio of probabilities, 166. 
Leptokurtosis, 82. 
Levy, P., refs., Calcvl des ProbabUites, 22, 184; 
characteristic functions, 113, 114. 
Liapounoff, A., inequality for moments, 66 and 
(Exercise1 3.14), 88 ; proof of Central Limit 
Theorem, 180, 183 ; refs., limit theorems in 
probability, 184. 
Likelihood, 176 ; principle of maximum likelihood, 
178-80 ; in estimating proportion of 
attributes, 199-200; relation with Bayes' 
theorem, 178-80 and (Exercise 8.6), 203. 
Limit theorems for distributions, see First Limit 
Theorem, Second Limit Theorem. 
Lindeberg, J. W., condition for validity of Central 
Limit Theorem, 181. 
Linear regression, 327-9, 368-76. 
Location, measures of, 29-38. See Mean, etc. 
Lottery sampling, 192. 
m rankings, 410-21. 
Maoaulay's essays, distribution of sentence length 
in, (Table 1.21), 26. 
Male births, distribution of registration districts 
according to, (Table 14.2), 326 ; constants 
of, (Example 14.2), 364. 
Malocclusion of teeth in infants, (Example 12.6), 
304. 
Markoff, A., refs., Second Limit Theorem, 113. 
Marriages, distribution of Australian, see 
Australian ; of deaf in America, (Table 1.19), 
24. 
Martin, E. S., refs., corrections to moments, 86. 
Maximum likelihood, 178. See Likelihood. 
Mean, arithmetic, definition of, 29 ; properties of, 
32 ; relation with median and mode, 35, 
46 ; as first moment, 39; standard error 
of, 224 ; distribution of, in normal samples, 
(Example 10.6), 238-9; in rectangular 
samples (Examples 10.7 and 10.12), 240-2, 
244; in Poisson distribution, (Example 
10.9), 243; in binomial, (Example 10.8), 
INDEX 
455 
243; in Type HE distribution (Example 
10.11), 244. 
Mean deviation, about mean, 38 ; about median, 
38; standard error of, 216. 
difference, 42 ; calculation of, 45 and 
(Exercise 2.10) 48 ; standard error, 216-17, 225. 
, geometric, 32 ; less than arithmetic mean, 
33-4; distribution of, 245-6 ; from 
rectangular population, (Example 10.13), 246 ; 
from Type III distribution (Exercise 10.2), 
261. 
, harmonic, 32 ; less than arithmetic and 
geometric means, 33-4. 
square contingency, 319. 
values, 84 ; in sampling problems, 264-6. 
Measures of location, dispersion and, skewness, 
29-±8, 81-2. 
Median, 34 ; relation with mean and mode, 36, 
46 ; standard error of (Example 9.7), 213, 
226. 
Mehler, G., refs., expansion in tetrachoric series, 
363. 
Mendelian law, test of, as sampling of attributes, 
197-8; in pea breeding, (Example 12.2), 
299. 
Mercer, W., data from, (Table 1.18), 23. 
Merzrath, E., refs., bivariate 
frequency-distributions and correlation, 86. 
Mesokurtosis, 82; in normal distribution, 129. 
Milk, costs of production of, (Tablo 1.9), 9. 
Milk-yield, distribution of cows according to, 
(Table 1.25), 27 ; covariance and variances, 
(Exercise 14.1), 364. 
Milne-Thomson, L. M., Calcrdus of Finite 
Differences, footnote, 69. 
Miner, J. R., tables of correlation coefficients, 375 
and refs., 386. 
Mises, R. von, probability as limit in sequences, 
166, 171-2; refs., Wakrscheinliclilceit, Sla- 
tistik und Wahrheit, 184. 
Modp, 36; relation with median and moan, 35, 
40 ; standard error in Poarson distributions, 
226. 
Momonts, preliminary, 39 ; Sheppard's corrections 
to, 41; definition, 49 ; about one point in 
terms of those about another, 49 ; 
calculation of, 60—4; generating functions for, 
64-6, 90 ; absolute moments, see Absolute ; 
factoiial moments, see Factorial; in terms 
of factorial moments, 67-8 ; relationship 
with oumulants, 61—4; corrections for 
grouping, 68-78; multivariate, 79-80 ; 
corrections to multivariate, 80-1 ; as 
characteristics of a distribution, 83—4 ; 
problem of moments, 105-10 ; of binomial, 
117, 118; of hypergeometric, 127; of 
normal distribution, 129 ; standard errors 
of, 204-11, 225; distribution of, 245. See 
also Sheppard's corrections, Second Limit 
Theorem, Cumulants. 
Montel, P., theorem on convergent sequences of 
functions, 100. 
Moore, G., data from, (Table 1.20), 24. 
Morant, G., refs., random occurrences in space and 
time, 134 ; data from (Table 14:1), 326. 
with values, distribution of, 217-22. 
Multiple correlation, see Correlation. 
Multivariate : distributions, 19-22 ; normal 
distribution, 376-7 ; sampling distributions, 
260; correlation, see Correlation; 
moments and oumulants, 79-81; 
characteristic functions, 104-6 ; A;-statistics, 281-3. 
Nair, XT. S., distribution of mean difference, 216, 
225 and refe., 228. 
Neyxnan, J., on theory of estimation, footnote, 
180; refs.* estimation, 184; 
representative method, 202; sampling from finite 
population, 284, 285. 
Nicholson, C, refs., distribution of a ratio, 251. 
Normal distribution, generally, 128-32; moments 
of, (Example 3.4), 53-4; cumulants of, 
(Example 3.10), 67 ; providing standard of 
kurtosis, 82; characteristic function of, 
(Examplo 4.1), 94 ; as limit of binomial, 
(Example 4.6), 103 ; determined uniquely 
by its momonts, (Example 4.7), 109-10; 
as limit of Poisson distribution, (Example 
4.8), 113; distribution function of, 129-30; 
as one of Pearson's types, 141; in Central 
Limit Thoorem, 180-3; in sampling of 
attributes, 198-9 ; distribution of mean in 
samples from, (Example 10.2), 234-6, 
(Example 10.3), 236-7, (Example 10.10), 
243 ; distribution of variance in samples 
from, (Example 10.6), 238-9; sampling of 
A-statistics from, 274; distribution of 
measures of departure from, (Exercise 
11.16), 288 ; bivariate form, see Bivariate; 
multivariate form, 376-7. 
Normalisation of frequency-functions, 166-9. 
Norris, N., rofs., inequalities among averages, 47. 
Norton, J. P., data from Statistical Studies in the 
New York Money Market, (Table 1.26), 28. 
Ogbum, W. F., correlation of crime and religion, 
(Example 15.2), 376 and refe., 386. 
Ogive of Galton, see Distribution curve. 
Oldis, E., refs., significance of correlation 
coefficient, (under E. S. Pearson), 363. 
Pabst, M. R., distribution of Spearman's p, 401, 
and refs. (under Hotel ling), 436. 
Paciello, U., refs., calculation of mean difference, 
47. 
454 
ESTDEX 
Paired comparisons, &1-36. 
Pairman, E., refs., corrections to abrupt 
distributions, 85. 
Parameters, definition, 29; of location, 29-38; 
of dispersion, 38-48. 
Partial: association, 313-18 ; contingency, 321-2; 
correlation, see Correlation ; regression, see 
Begression. 
Pattern functions, in sampling ^-statistics, 262-5, 
277-8, 279, (Exercise 11.11), 287. 
Pea breeding, (Example 12.2), 299. 
Pearce, T. V., data from, (Table 1.23), 26. 
Pearse, G. E., data from, (Table 1.11), 10; refe., 
corrections when ordinates are infinite, 86. 
Pearson, E. S., distribution of range, 223, 224; 
sampling of correlation coefficient, 346; 
distribution of y/b, 280-1, (Exercise 11.17), 
289 ; refe., range, 228 ; estimating standard 
deviation, 228; distribution of frequency 
constants in skew population, 228; tests 
for normality, 286; correlation coefficient, 
363; polychoric coefficients, (under K. 
Pearson), 363. 
Pearson, Karl, data from: trypanosomes, (Table 
1.13), 12 ; fecundity of mares, (Table 1.20), 
24; whist deals, (Table 5.4), 128; height 
of fathers and sons, (Table 14.3), 327; 
quoting data by Goring on crime, (Table 
14.6), 356; quoting data by Elderton on 
alcoholism, (Exercise 14.12), 366. 
Coefficient of variation, '43 ; measure of 
skewness, 81; coefficient of contingency, 
319-20; sampling of contingency coefficients, 
321; sampling of tetrachoric r, 356, and of 
biserial rj, 358; grades and Spearman's p, 
410. 
Refe., corrections to abrupt distributions 
(under Pairman), 85 ; skew variation, 134; 
moments of hypergeometric, 134; 15-con- 
stant frequency surface, 160; standard 
errors of frequency constants, 228-9 ; mean 
character of ranked individual, 229 ; 
distribution of x2, 251, 305 ; of difference of Type 
Ulvariates, (Exercise 10.6), 252; sampling 
of contingency coefficients, 322; multiple 
contingency, 322 ; sampling of correlation 
coefficient, (under Co-operative Study), 363; 
probable error of biserial rj, 363; rank 
correlation, 436. 
Pearson, M. V., refs., mean character of ranked 
individuals, 229. 
Pearson distributions, as limit of hypergeometric, 
132-3; generally, 137-45 ; recurrence 
relation for moments, 138 ; skewness of, 138; 
inflections of, 138; fitting of, 143-5; 
quadrature of, 145; generalisation by 
Romanovsky, refs. 160, 161; distribution 
of means from (ref. Irwin), 250. 
Pitman, E. J. G., refe., significance tost applicable 
to samples from any population, 430. 
Platykurtosis, 82. 
Poincar6, characteristic functions, 113. 
Poisson distribution, generally, 120-2 ; oumulimtH 
of, (Example 6.9), 66 ; moments of, (Kxit- 
oise 3.3), 86 ; normal distribution as 
limiting form of, (Example 4.8), 113 ; diHtribu- 
tipn function of, 122 ; in mixed populations 
122-4; bivariate form, (Exercise 6.8), 130 ; 
sampling of attributes from, (Exerciso 8.2), 
203 ; distribution of means from, (Examplo 
10.9), 243. 
■Polynomials, see Tchebycheff-Hormite polynomials. 
Populations, as basis of statistical theory, 1 ; 
existent, 18-19 ; hypothetical, 19 ; t,ypen 
in sampling, 186-7. 
Posterior probability, 176. 
Potatoes, bias in estimates of yiold, (Example 8.4), 
189-90. 
and wheat, correlation of yields, (Table 
14.4), 333, (Example 14.3), 332-4. 
Preterms, S. J., data from, on Australian marria^cM, 
(Table 1.8), 9; on beans, (Table 1.13), 20 
and (Table 6.1), 150; refs., skew bivaritiie 
distributions, 160. 
Principle of Maximum Likelihood, 178. AYn 
Likelihood. 
of moments, 83; in fitting Pearson'h dw- 
tributions, 143. 
Prior probability, 176. 
Probability, generally, 164-86; logic of, 106; 
basic rules of direct probability, 100-70; 
in a continuum, 170-1; von Mises' 
approach, 171-2; and statistical distributionH, 
172-3; Bayes' theorem, 176-8; hiveine 
probability, 176; posterior and prior, 170. 
functions, 14. 
Problem of moments, 105-10; refs. 113-14. 
Product-moment correlation, sea Correlation. 
Quadrature of Pearson distributions, 146. 
Quantiles, definition, 36; graphical 
determination of, 37-8; standard errors of, 211-13. 
Quartiles, definition, 36 ; interquartile range as 
measure of dispersion, 38 ; standard errors 
of, 225. 
Radioactive element (polonium), distribution of 
particles from, (Table 6.2), 155, (Examplo 
6.4), 156. 
Ramsey, F. P., logic of probability, 166; reft., 
The Foundations of Mathematics, 184. 
Random variables, definition, 173 ; addition of 
173. 
Sampling Numbers, 192-7. 
Randomising machine, (Example 8.3), 189. 
INDEX 
455 
Randomness, 171 ; random sampling, generally, 
186-203 ; technique of, 191-7. 
Range, definition, 38 ; distribution of, 223-4. 
Rank correlation, aee Correlation. 
Ranking, estimation of, 421. 
Rankings, problem of m, aee m rankings. 
Ratio, distribution of, 248-9 ; CrameVs theorem 
(Exercise 10.8), 252; Geary's theorem 
(Exercise 10.9), 253 ; refs., 250-1. 
Rectangular population, transformation of 
frequency-distribution to, 18 ; as one of 
Pearson's distributions, 142; distribution 
of mean of samples from, (Example 10.7), 
240 and (Example 10.12), 244; 
distribution of geometric mean (in samples from, 
(Example 10.13), 245-6. 
Recurrence relations for moments of binomial, 118. 
Registrar-General's Statistical Review of England 
and Wales, data from, (Table 1.1), 3 ; 
(Table 1.3), 6; (Table 1.11), 11. 
Registration districts, distribution according to 
births, (Table 14.2), 326. 
Regression, definition, 327-9 ; coefficients of, 329 ; 
criterion for linearity of, 335-6 ; sampling 
of coefficients of, 336-7, 347-9 ; standard 
error of coefficients, 337 ; significance of, 
358*9; partials, 368-79; sampling of 
partials, 378-9. 
Religion, correlation with crime, (Example 15.2), 
375-6. 
Resorves and bank deposits, distribution of, 
(Table 1.26), 28. 
Residuals, in regression equations, 369. 
Ritchie-Scott, A., refs., correlation coefficient of 
polychoric table, 363. 
Romanovsky, V., refs., method of moments, 86 ; 
moments of hyporgeometric, 134 and 
(Exercise 6.2), 135 ; generalisation of Pearson 
distributions, 160. 
Room-space, distribution of families deficiont in, 
(Table 1.24), 27. 
Rothamsted Experimental Station, data from, 
(Table 8.1), 187. 
Rutherford, Lord, data on emission of radioactive 
particles, (Example 6.4), 1C6. 
St. Georgescu, N., refs., sampling moments, 285. 
Saltus, in distribution function, 14. 
Sampling, preliminary, 174 ; simple, 174 ; random 
sampling, aee Random ; sampling problem, 
186 ; with and without replacement, 186-7 ; 
randomness in, 187-97 ; lottery or tickot, 
192; from continuous population, 197; 
from attributes, 197-202. 
< distributions, 173-5 ; role in sampling 
problems, 201 ; exact, 231-63 ; derivation 
by analytical methods, 231-6, by 
geometrical methods, 236-42, by characteristic 
functions, 242-6, by induction, 246-8 ; of 
a sum, 246-7 ; of a ratio, 248-9; 
multivariate, 250 ; approximations to, 264-89. 
Sampling moments, generally, 254-89. See Cumu- 
lants, fc-statistics. 
Scalo reading, bias in, (Example 8.2), 188. 
Scarlet fever, deaths from, (Table 1.3), 5. 
Schoolchildren, distribution according to 
intelligence and clothing, (Example 13.6), 320. 
Second Limit Theorem, 110-13. 
Semi-interquartile range, as measure of skewness, 
38 ; standard error of, 215. 
Seminvariant statistics, 84-6, 266, refs. (Dressel 
and Kendall), 286. 
Seminvariants, 61, 84-5, refs., 84-5. See Cumu- 
lants. 
Sentences, distribution of according to length, 
(Table 1.21), 26. 
Sheppard, W. F., tablos of normal distribution, 
130 and refs., 134; correlation coefficient, 
(Exercise 14.4), 364. 
Sheppard's corrections, 68-74; as average 
corrections, 74-5 ; for discrete data, 77, 
(Exercise 3.13), 88 ; to factorial moments, 
77-8; to cumulants, 78; multivariate 
case, 80-1 ; compared with sampling 
fluctuations, 210. 
Shirley poppies, distribution of, (Table 1.6), 7. 
Shohat, J., refs., Stieltjes integrals, 22 ; inequalities 
for momentR, 86 ; Second Limit Theorem, 
112, 113, 114. 
Shuffling of cards, see Card-shuffling. 
Simple sampling, 174, 
Skew distributions, 10. 
Skewness, 10 ; measures, of, 81-2 ; of Pearson 
distributions, 138; standard error of, 225. 
Skulls, Egyptian, distribution of, (Table t.22), 25. 
Sons, distribution of according to stature, (Table 
14.3), 327. 
Soper, H. E., refs., Frequency Arrays, 134; 
sampling of correlation coefficient (under 
Co-operative Study), 363. 
Spahlinger vaccine, data on, (Exercise 12.7), 307. 
Spearman, C, coefficient of rank correlation, 
388-91 ; sampling of, 394-403; footrulo, 
436 ; refs., rank correlation, 436. - 
Square contingency, 319. See y^. 
Standard deviation, 39; standard error of, 224. 
: Distributions, 116-36,. 137-63. See under 
Binomial, Hyporgeometric,, Poisson, 
Normal, Pearson distributions, Gram-Charlier 
series, Normalisation of 
frequency-functions. 
errors, 199 ; in attributes, 199-201; 
generally, 204-30; compared with Sheppard 
corrections, 210 ; of sum and difference, 
226. (For standard errors of particular 
statistics, soe under those statistics.) 
456 
INDEX 
Standard measure,43; effect on oumulants of 
transformation to, 61 ; on characteristic 
function of transformation to, (Example 4.6), 
103. 
Statistic, definition of, 2. 
Statistical hypothesis, 178. 
Statistical Abstract, data quoted from, (Table 1.2), 
3. 
Beview of England and Wales, data from, 
(Table 1.1), 3 ; (Table 1.3), 6 ; (Table 1.12), 
11. 
Studies in the New York Money Market 
(Norton), 28. 
Statistics, definition of, 1-2. 
Stature, see Height. 
Steffensen, J., on Type B series, 153, 154; refe., 
Becent Besearcln.es in the Theory of Statistics 
and Actuarial Science, 161. 
Stereogram, 20-1. 
Stieltjes, J., problem of moments (Exercise 3.12), 
88, 106-7, 109; refe., 114. 
integrals, 16-16 ; refe. (Shohat), 22. 
Stigmatic rays, distribution of poppies according 
to, (Table 1.5), 7. 
Stouffer, K. A., distribution of difference of Type 
TTT variates, (Exercise 10.6), 252. 
"Student" (W. S. Gosset), refs., Poisson 
distribution, 134; probable error of mean, 
251; sampling of Spearman's coefficients 
of rank correlation, 436. 
"Student's" distribution, (Example 10.6), 239- 
40; (Example 10.17), 248; in testing 
correlation coefficient, 343; in testing 
Spearman's p, 401; in testing regression 
coefficients, 349. 
Succession rule of Laplace, (Example 7.7), 177. 
Suicides, distribution of, (Table 1.6), 7. 
Sum of two variates, distribution of, 246-7. 
Sur-tax and super-tax, distribution of incomes 
liable to, (Table 1.2), 3 and histogram 
(Fig. 1.2), 4. 
/-distribution, see " Student's " distribution. 
Tchebycheff, P. L., problem of moments, 114; 
inequality, (Exercise 8.4), 203. 
Tchebycheff-Hermite polynomials, 145-7; refe., 
160. 
Teeth and feeding in infants, (Example 12.5), 304. 
Telephone directory, distribution of digits from, 
(Table 1.4), 6, 193. 
Term of abortion, distribution of women according 
to, (Table 1.23), 26. 
Tetrachoric functions, 151, 356. 
r, 354-6. 
Thiele, T. N., cumulants, 61 ; quotation about 
oracles, 178; sampling cumulants, 256; 
refs., Theory of Observations, 86, 285. 
Thompson, C, tables of x2, 294. 
Ticket sampling, 192. 
Tippett, L. H. C, Random Sampling Numlwrft, 
193, 197 ; distribution of extromo vnluotf, 
220 and refe., 229 ; distribution of mngo,. 
223-4 and refe., 229. 
Tocher, J. F., data from, (Tablo 1.25), 27. 
Transformation of a variate, 16, 21-2. 
Trigonometrical representation of corrolutiom,, 
372. 
Truncated distributions, 11. 
Trypanosomes, distribution of, (Tablo 1.13), 12. 
Tsohuprow, A. A., sampling momenta, 2G0 ; 
coefficient of contingency, 320 ; refri., sampling 
moments, 284, 286. 
Tsetse flies, distribution of trypanosomon in, 
(Table 1.13), 12. 
Type A, Type B series, see Gram-Charlior serins. 
Type I distribution, 139-40. 
II distribution, 141-2 ; distribution of inoann 
from (Exercise 10.4), 261. 
m distribution, characteristic function and 
moments of, (Example 3.(5), fifi-O ; 
oumulants of, (Example 3.11), 07 ; gonomlly, 
142 ; as sampling distribution of sum of 
variances, 231-3; distribution of nicnriH 
from (Example 10.11), 244; of goom«l.rio 
means from, (Exercise 10.2), 251 ; ilin- 
tribution of differences from, (Exi.>rutm> 
10.6), 252. 
TV distribution, 140-1. 
V distribution, 141; moments and 
cumulants, (Exercise 3.12), 67-8. 
VI distribution, 140. 
VII distribution, 142; momenta of, 
(Exorcise 3.1), 86. 
Types Vni-XII distributions, 142-3. 
U-shaped distributions, 10-11. 
Unbiased estimates, 200. 
Unimodal distributions, 29. 
Uspensky, J. V., refs., Central Limit Theorem* 
183 ; Introduction to Mathematical 
Probability, 184, 251. 
Variable, random, see Random variable 
Variance, 39 ; as half mean-square of difinronccw, 
42; ^ standard error of, 224 ; distribution 
of, in normal samples, (Example 10.6), 
238-9, (Example 10.14), 240; of second 
mean-moment, (Example 11.2), 206; third 
moment of, (Example 11.3), 200. 
Variate, definition of, 2; transformations of, 
16-18, 21-2. 
Variation, coefficient of, 43 ; standard error of. 
(Example 9.5), 209, 224. 
Venn, J. A., refe., Logic of Chance, 184. 
Vigor, H. D., data from, (Table 14.2), 326. 
INDEX 
457 
"Wallis, W. A., refs., correlation ratio for ranked 
data, 437 and (Exercise 16.3), 437. 
Weather, correlation with crops, (Example 16.1), 
373-6. 
Weierstross, K., diagonal process, 100 ; theorem 
on series of polynomials, (Exercise 4.7), 116. 
Weight, distribution of men according to, (Table 
1.10), 10. 
Weldon, W. F. R., dice-throwing data, (Table 1.14), 
19; (Table 1.16), 23; (Table 6.1), 117; 
(Example 8.10), 199. 
Wheat, correlation of yields with potatoes, (Table 
14.4), 333 ; (Example 14.3), 332-4. 
plants, distribution of ranks according to 
height, (Table 8.1), 187. 
Whist, distribution of first hands at, (Table 6.4), 
128; (Example 12.1), 297-8. 
Whitaker, L., refs., Poisson distribution, 134. 
Wicksell, S. D., example from, (Example 14.4), 336. 
Willcox, W. F., definitions of statistics, 1, refs., 22. 
Wilson, E. B., limiting distribution of x2, 294-6 
and refe., 305. 
Wishart, J., introduction of word " cumulant," 
85; refs. : Romanovsky's generalisation 
of Poarson distributions, 161 ; derivation 
of pattern formulae (under Fmher), 285; 
sampling cumulant formulae, 285; 
distribution of multiple- correlation and 
correlation rati oh, 3K(5. 
Wold, H., Nheppanl'H corrections, 7J, 80 ; refs., 86. 
Women, distribution of according to term of 
abortion, (Table 1.23), 2(5. 
Woo, T. L., date from, on skulls (Table 1.22), 25 ; 
on association of hand and oyo, (Exercise 
13.5'), 323 ; tables of correlation ratio, 354. 
Yasukawa, K., refs., standard error of mode, 
226. 
Yates, F., data from, height of plants, (Table 8.1), 
187; Random Sampling Numbers, 194, 
197 ; tables of x2, 293 ; correction to x2 
for grouping, 303 and (Example 12.3) 304; 
refs., bias in sampling, 202 ; correction to 
X\ 306. 
Yield of grain, distribution of, (Table 1.18), 23; 
of wheat and potatoes, correlation of, 
(Table 14.4), 333, . (Example 14.3), 332- 
334. 
Young, A. W., refs., sampling of correlation 
coefficient (under Co-operative Study), 363. 
Yule, G. Udny, data from, poppies, (Table, 1.6), 7 ; 
sentence length, (Table 1.21), 26 ; industrial 
accidents, (Table 6.3), 124; prints on 
photographic paper, (Exercise 12.8), 307; 
inoculation against cholera, (Example 13.1), 
309 ; births and registration districts, 
(Table 14.2), 326; data compiled from 
Fay, (Table 1.19), 24. 
Negativoly indexed binomials, (footnote), 
125 ; normal distribution (Exercise 6.6), 
136 ; bias in scale-reading, 188; tables of 
X2, 293 ; coefficients of association, 310-13; 
rofn., reading a scale, 202; degrees of 
freedom in contingency tables, 306; theory 
of correlation, 303, 385, 386. 
s-distribntion, (Example 10.18), 249 ; hi testing 
correlation ratio, 353-4 ; in testing multiple' 
correlation coefficient, 381-2; in testing 
concordance in rankings, 419. 
