An Introduction 
to Probability Theory 
and Its Applications 
WILLIAM FELLER A906-1970) 
Eugene Higgins Professor of Mathematics 
Princeton University 
VOLUME II 
, INC, 
4873 
Preface to the First Edition 
AT THE TIME THE FIRST VOLUME OF THIS BOOK WAS WRITTEN (BETWEEN 1941 
and 1948) the interest in probability was not yet widespread. Teaching was 
on a very limited scale and topics such as Markov chains, which are now 
extensively used in several disciplines, were highly specialized chapters of 
pure mathematics. The first volume may therefore be likened to an all- 
purpose travel guide to a strange country. To describe the nature of 
probability it had to stress the mathematical content of the theory as well 
as the surprising variety of potential applications. It was predicted that 
the ensuing fluctuations in the level of difficulty would limit the usefulness 
of the book. In reality it is widely used eVen today, when its novelty has 
worn off and its attitude and material are available in newer books written 
for special purposes. The book seems even to acquire new friends. The 
fact that laymen are not deterred by passages which proved difficult to 
students of mathematics shows that the level of difficulty cannot be measured 
objectively; it depends on the type of information one seeks and the details 
one is prepared to skip. The traveler often has the choice between climbing 
a peak or using a cable car. 
In view.of this success the second volume is written in the same style. 
It involves harder mathematics, but most of the text can be read on different 
levels. The handling of measure theory may illustrate this point. Chapter 
IV contains an informal introduction to the basic ideas of measure theory 
and the conceptual foundations of probability. The same chapter lists the 
few facts of measure theory used in the subsequent chapters to formulate 
analytical theorems in their simplest form and to avoid futile discussions of 
regularity conditions. The main function of measure theory in this connection 
is to justify formal operations and passages to the limit that would never be 
. questioned by a non-mathematician. Readers interested primarily in practical 
results will therefore not feel any need for measure theory. 
To facilitate access to the individual topics the chapters are rendered as 
self-contained as possible, and sometimes special cases are treated separately 
ahead of the general theory. Various topics (such as stable distributions and 
renewal theory) are discussed at several places from different angles. To 
avoid repetitions, the definitions and illustrative examples are collected in 
VI1 
PREFACE 
chapter VI, which may be described as a collection of introductions to the 
subsequent chapters. The skeleton of the book consists of chapters V, VIII, 
and XV. The reader will decide for himself how much of the preparatory 
chapters to read and which excursions to take. 
Experts will find new results and proofs, but more important is the attempt 
to consolidate and unify the general methodology. Indeed, certain parts of 
probability suffer from a lack of coherence because the usual grouping and 
treatment of problems depend largely on accidents of the historical develop- 
ment. In the resulting confusion closely related problems are not recognized 
as such and simple things are obscured by complicated methods. Consider- 
able simplifications were obtained by a systematic exploitation and develop- 
ment of the best available techniques. This is true in particular for the 
proverbially messy field of limit theorems (chapters XVI-XVII). At other 
places simplifications were achieved by treating problems in their natural 
context. For example, an elementary consideration of a particular random 
walk led to a generalization of an asymptotic estimate which had been 
derived by hard and laborious methods in risk theory (and under more 
restrictive conditions independently in queuing). 
I have tried to achieve mathematical rigor without pedantry in style. For 
example, the statement that 1/A + ?2) is the characteristic function of 
\e~^ seems to me a desirable and legitimate abbreviation for the logically 
correct version that the function which at the point ? assumes the value 
1/A + f2) is the characteristic function of the function which at the point 
x assumes the value \e~^. 
I fear that the brief historical remarks and citations do not render justice 
to the many authors who contributed to probability, but I have tried to give 
credit wherever possible. The original work is now in many cases superseded 
by newer research, and as a rule full references are given only to papers to 
which the reader may want to turn for additional information. For example, 
no reference is given to my own work on limit theorems, whereas a paper 
describing observations or theories underlying an example is cited even if it 
contains no mathematics.1 Under these circumstances the index of authors 
gives no indication of their importance for probability theory. Another 
difficulty is to do justice to the pioneer work to which we owe new directions 
of research, new approaches, and new methods. Some theorems which were 
considered strikingly original and deep now appear with simple proofs 
among more refined results. It is difficult to view such a'theorem in its 
historical perspective and to realize that here as elsewhere it is the first step 
that counts. 
1 This system was used also in the first volume but was misunderstood by some subsequent 
writers; they now attribute the methods used in the book to earlier scientists who could 
not have known them. 
ACKNOWLEDGMENTS 
Thanks to the support by the U.S. Army Research Office of work in 
probability at Princeton University I enjoyed the help of J. Goldman, L. Pitt, 
M. Silverstein, and, in particular, of M. M. Rao. They eliminated many 
inaccuracies and obscurities. All chapters were rewritten many times 
and preliminary versions of the early chapters were circulated among friends. 
In this way I benefited from comments by J. Elliott, R. S. Pinkham, and 
L. J. Savage. My special thanks are due to J. L. Doob and J. Wolfowitz for 
advice, and criticism. The graph of the Cauchy random walk was supplied by 
H. Trotter. The printing was supervised by Mrs. H. McDougal, and the 
appearance of the book owes much to her. 
William Feller 
October 1965 
IX 
THE MANUSCRIPT HAD BEEN FINISHED AT THE TIME OF THE AUTHOR'S DEATH 
but no proofs had been received. I am grateful to the publisher for providing 
a proofreader to compare the print against the manuscript and for compiling 
the index. J. Goldman, A. Grunbaum, H. McKean, L. Pitt, and A. Pittenger 
divided the book among themselves to check on the mathematics. Every 
mathematician knows what an incredible amount of work that entails. / 
express my deep gratitude to these men and extend my heartfelt thanks for 
their labor of love. 
May 1970 Clara N. Feller 
XI 
Introduction 
THE CHARACTER AND ORGANIZATION OF THE BOOK REMAIN UNCHANGED, BUT 
the entire text has undergone a thorough revision. Many parts (Chapter 
XVII, in particular) have been completely rewritten and a few new sections 
have been added. At a number of places the exposition was simplified by 
streamlined (and sometimes new) arguments. Some new material has been 
incorporated into the text. 
While writing the first edition I was haunted by the fear of an excessively 
long volume. Unfortunately, this led me to spend futile months in shortening 
the original text and economizing on displays. This damage has now been 
repaired, and a great effort has been spent to make the reading easier. 
Occasional repetitions will also facilitate a direct access to the individual 
chapters and make it possible to read certain parts of this book in con- 
junction with Volume 1. 
Concerning the organization of the material, see the introduction to the 
first edition (repeated here), starting with the secorid paragraph. 
I am grateful to many readers for pointing out errors or omissions. I 
especially thank D. A. Hejhal, of Chicago, for an exhaustive and penetrating 
list of errata and for suggestions covering the entire book. 
January 1970 WILLIAM FELLER 
Princeton, NJ. 
xm 
Abbreviations and Conventions 
Iff is an abbreviation for if and only if. 
Epoch. This term is used for points on the time axis, while time is 
reserved for intervals and durations. (In discussions of 
stochastic processes the word "times" carries too heavy a 
burden. The systematic use of "epoch," introduced by 
J. Riordan, seems preferable to varying substitutes such as 
moment, instant, or point.) 
Intervals are denoted by bars: a, b is an open, a, b a closed interval; 
1 i 
half-open intervals are denoted by a, b and a, b. This 
notation is used also in higher dimensions. The pertinent 
conventions for vector notations and order relations are 
found in V,l (and also in IV,2). The symbol (a, b) is 
reserved for pairs and for points. 
Si1, 3l2, %r stand for the line, the plane, and the r-dimensional Cartesian 
space. 
1 refers to volume one, Roman numerals to chapters. Thus 
1; XI,C.6) refers to section 3 of chapter XI of volume 1. 
^ indicates the end of a proof or of a collection of examples, 
n and $t denote, respectively, the normal density and distribution 
function with zero expectation and unit variance. 
O, o, and ~. Let u and v depend on a parameter x which tends, say, 
to a. Assuming that v is positive we write 
(remains bounded 
—> I. 
f(x) U{dx). For this abbreviation see V,3. 
Regarding Borel sets and Baire functions, see the introduction to chapter V. 
u = 0{v)\ 
u = o(v) 
xr 
Contents 
CHAPTER 
I The Exponential and the Uniform Densities .... 1 
1. Introduction 1 
2. Densities. Convolutions 3 
3. The Exponential Density 8 
4. Waiting Time Paradoxes. The Poisson Process ... 11 
5. The Persistence of Bad Luck 15 
6. Waiting Times and Order Statistics ....... 17 
7. The Uniform Distribution 21 
8. Random Splittings 25 
9. Convolutions and Covering Theorems ...... 26 
10. Random Directions 29 
11. The Use of Lebesgue Measure 33 
12. Empirical Distributions . .36 
13. Problems for Solution 39 
CHAPTER 
II Special Densities. Randomization . . . . . . . .45 
1. Notations and Conventions 45 
2. Gamma Distributions 47 
*3. Related Distributions of Statistics 48 
4. Some Common Densities 49 
5. Randomization and Mixtures 53 
6. Discrete Distributions 55 
* Starred sections are not required for the understanding of the sequel and should be 
omitted at first reading. 
xvn 
Xviii CONTENTS 
7. Bessel Functions and Random Walks 58 
8. Distributions on a Circle 61 
9. Problems for Solution 64 
chapter 
III Densities in Higher Dimensions. Normal Densities and 
Processes 66 
1. Densities 66 
2. Conditional Distributions 71 
3. Return to the Exponential and the Uniform Distributions 74 
*4. A Characterization of the Normal Distribution . . . 77 
5. Matrix Notation. The Covariance Matrix 80 
6. Normal Densities and Distributions 83 
*7. Stationary Normal Processes 87 
8. Markovian Normal Densities. . 94 
9. Problems for Solution 99 
CHAPTER 
IV Probability Measures and Spaces 103 
1. Baire Functions 104 
2. Interval Functions and Integrals in %r 106 
3. o--Algebras. Measurability 112 
4. Probability Spaces. Random Variables 115 
5. The Extension Theorem 118 
6. Product Spaces. Sequences of Independent Variables. . 121 
7. Null Sets. Completion 125 
CHAPTER 
V Probability Distributions in %r 127 
1. Distributions and Expectations 128 
2. Preliminaries 136 
3. Densities 138 
4. Convolutions 143 
CONTENTS 
5. Symmetrization 148 
6. Integration by Parts.' Existence of Moments .... 150 
7. ChebyshevYInequality '. . . 151 
8. Further Inequalities. Convex Functions 152 
9. Simple Conditional Distributions. Mixtures . . . . 156 
*10. Conditional Distributions 160 
*11. Conditional Expectations . . . 162 
12. Problems for Solution 165 
CHAPTER 
VI A Survey of some Important Distributions and Processes 169 
1. Stable Distributions in ft1 .169 
2. Examples 173 
3. Infinitely Divisible Distributions in %l 176 
4. Processes with Independent Increments 179 
*5. Ruin Problems in Compound Poisson Processes . . . 182 
6. Renewal Processes 184 
7. Examples and Problems 187 
8. Random Walks 190 
9. The Queuing Process . . . .... . . . . .194 
10. Persistent and Transient Random Walks 200 
11. General Markov Chains 205 
*12. Martingales 209 
13. Problems for Solution 215 
CHAPTER 
VTI Laws of Large Numbers. Applications in Analysis . . 219 
1. Main Lemma and Notations 219 
2. Bernstein Polynomials. Absolutely Monotone Functions 222 
3. Moment Problems . 224 
*4. Application to Exchangeable Variables 228 
*5. Generalized Taylor Formula and Semi-Groups . . . 230 
6. Inversion Formulas for Laplace Transforms .... 232 
XX CONTENTS 
*7. Laws of Large Numbers for Identically Distributed 
Variables 234 
*8. Strong Laws . 237 
*9. Generalization to Martingales 241 
10. Problems for Solution 244 
CHAPTER 
VIII The Basic Limit Theorems 247 
1. Convergence of Measures 247 
2. Special Properties 252 
3. Distributions as Operators 254 
4. The Central Limit Theorem 258 
*5. Infinite Convolutions 265 
6. Selection Theorems 267 
*7. Ergodic Theorems for Markov Chains 270 
8. Regular Variation .275 
*9. Asymptotic Properties of Regularly Varying Functions . 279 
10. Problems for Solution 284 
CHAPTER 
IX Infinitely Divisible Distributions and Semi-Groups . . 290 
1. Orientation 290 
2. Convolution Semi-Groups 293 
3. Preparatory Lemmas 296 
4. Finite Variances 298 
5. The Main Theorems 300 
6. Example: Stable Semi-Groups 305 
7. Triangular Arrays with Identical Distributions.... 308 
8. Domains of Attraction 312 
9. Variable Distributions. The Three-Series Theorem . . 316 
10. Problems for Solution 318 
CONTENTS XXI 
CHAPTER 
X Markov Processes and Semi-Groups 321 
1. The Pseudo-Poisson Type 322 
2. A Variant: Linear Increments 324 
3. Jump Processes 326 
4. Diffusion Processes in ft1 332 
5. The Forward Equation. Boundary Conditions . . . 337 
6. Diffusion in Higher Dimensions 344 
7. Subordinated Processes 345 
8. Markov Processes and Semi-Groups .:.... 349 
9. The "Exponential Formula" of Semi-Group Theory . . 353 
10. Generators. The Backward Equation 356 
CHAPTER 
XI Renewal Theory 358 
1. The Renewal Theorem 358 
2. Proof of the Renewal Theorem 364 
*3. Refinements 366 
4. Persistent Renewal Processes '. 368 
5. The Number Nt of Renewal Epochs 372 
6. Terminating (Transient) Processes 374 
7. Diverse Applications 377 
8. Existence of Limits in Stochastic Processes 379 
*9. Renewal Theory on the Whole Line 380 
10. Problems for Solution 385 
CHAPTER 
XII Random Walks in ft1 389 
1. Basic Concepts and Notations 390 
2. Duality. Types of Random Walks 394 
3. Distribution of Ladder Heights. Wiener-Hopf Factor- 
ization 398 
3a. The Wiener-Hopf Integral Equation .402 
CONTENTS 
4. Examples 404 
5. Applications 408 
6. A Combinatorial Lemma 412 
7. Distribution of Ladder Epochs 413 
8. The Arc Sine Laws 417 
9. Miscellaneous Complements 423 
10. Problems for Solution . . 425 
CHAPTER 
Xffl Laplace Transforms. Tauberian Theorems. Resolvents . 429 
1. Definitions. The Continuity Theorem 429 
2. Elementary Properties : 434 
3. Examples 436 
4. Completely Monotone Functions. Inversion Formulas . 439 
5. Tauberian Theorems 442 
*6. Stable Distributions 448 
*7. Infinitely Divisible Distributions 449 
*8. Higher Dimensions 452 
9. Laplace Transforms for Semi-Groups 454 
10. The Hille-Yosida Theorem 458 
11. Problems for Solution 463 
CHAPTER 
XIV Applications of Laplace Transforms 466 
1. The Renewal Equation: Theory........ 466 
2. Renewal-Type Equations: Examples 468 
3. Limit Theorems Involving Arc Sine Distributions ... . 470 
4. Busy Periods and Related Branching Processes ... 473 
5. Diffusion Processes 475 
6. Birth-and-Death Processes and Random Walks . . . 479 
7. The Kolmogorov Differential Equations 483 
8. Example: The Pure Birth Process 488 
9. Calculation of Ergodic Limits and of First-Passage Times 491 
10. Problems for Solution 495 
contents xxiii 
CHAPTER 
XV Characteristic Functions 498 
1. Definition. Basic Properties 498 
2. Special Distributions. Mixtures 502 
2a. Some Unexpected Phenomena 505 
3. Uniqueness. Inversion Formulas ,. . 507 
4. Regularity Properties 511 
5. The Central Limit Theorem for Equal Components . . 515 
6. The Lindeberg Conditions 518 
7. Characteristic Functions in Higher Dimensions . . . 521 
*8. Two Characterizations of the Normal Distribution . . 525 
9. Problems for Solution 526 
CHAPTER 
XVI* Expansions Related to the Central Limit Theorem . . 531 
1. Notations . 532 
2. Expansions for Densities 533 
3. Smoothing 536 
4. Expansions for Distributions 538 
5. The Berry-Esseen Theorems 542 
6. Expansions in the Case of Varying Components . . . 546 
7. Large Deviations . 548 
CHAPTER 
XVII Infinitely Divisible Distributions 554 
1. Infinitely Divisible Distributions 554 
2. Canonical Forms. The Main Limit Theorem .... 558 
2a. Derivatives of Characteristic Functions 565 
3. Examples and Special Properties 566 
4. Special Properties . 570 
5. Stable Distributions and Their Domains of Attraction . 574 
*6. Stable Densities 581 
7. Triangular Arrays 583 
CONTENTS 
*8. The Class L 588 
*9. Partial Attraction. "Universal Laws" 590 
*10. Infinite Convolutions 592 
11. Higher Dimensions 593 
12. Problems for Solution . 595 
CHAPTER 
XVIII Applications of Fourier Methods to Random Walks . 598 
1. The Basic Identity 598 
*2. Finite Intervals. Wald's Approximation . . . 601 
3. The Wiener-Hopf Factorization 604 
4. Implications and Applications 609 
5. Two Deeper Theorems . 612 
6. Criteria for Persistency 614 
7. Problems for Solution 616 
CHAPTER 
XIX Harmonic Analysis 619 
1. The Parseval Relation 619 
2. Positive Definite Functions 620 
3. Stationary Processes 623 
4. Fourier Series 626 
*5. The Poisson Summation Formula 629 
6. Positive Definite Sequences 633 
7. L2 Theory 635 
8. Stochastic Processes and Integrals 641 
9. Problems for Solution 647 
Answers to Problems 651 
Some Books on Cognate Subjects 655 
Index 657 
An Introduction 
to Probability Theory 
and Its Applications 
CHAPTER I 
The Exponential and 
the Uniform Densities 
1. INTRODUCTION 
In the course of volume 1 we had repeatedly to deal with probabilities 
defined by sums of many small terms, and we used approximations of the 
form 
A.1) P{a < X < b} «a [ f{x) dx. 
The prime example is the normal approximation to the binomial distribution.1 
An approximation of this kind is usually formulated in the form of a limit 
theorem involving a succession of more and more refined discrete probability 
models. In many cases this passage to the limit leads conceptually to a new 
sample space, and the latter may be intuitively simpler than the original 
discrete model. 
Examples, (a) Exponential waiting times. To describe waiting times by 
a discrete model we had to quantize the time and pretend that changes 
can. occur only at epochs2 <5, 2<5,. ... The simplest waiting time T is the 
waiting time for the first success in a sequence of Bernoulli trials with 
probability pd for success. Then P{T > nd} = A—^)" and the expected 
waiting time is E(T) = 6jpd. Refinements of this model are obtained by 
letting <5 grow smaller in such a way that the expectation dfp& = a. remains 
1 Further examples from volume 1: The arc sine distribution, chapter III, section 4; 
the distributions for the number of returns to the origin and first passage times in 111,7; the 
limit theorems for random walks in XIV; the uniform distribution in problem 20 of XI,7. 
2 Concerning the use of the term epoch, see the list of abbreviations at the front of the 
book. 
1 
2 THE EXPONENTIAL AND THE UNIFORM DENSITIES I.I 
fixed. To a time interval of duration t there correspond n «a tjd trials, 
and hence for small d 
A.2) P{T > t} ** A - <5/a)'/<5 ^ e~t/a 
approximately, as can be seen by taking logarithms. This model considers 
the waiting time as a geometrically distributed discrete random variable, 
and A.2) states that "in the limit" one gets an exponential distribution. 
From the point of view of intuition it would seem more natural to start 
from the sample space whose points are real numbers and to introduce 
Jhe exponential distribution directly. 
(b) Random choices. To "choose a point at random" in the interval3 
0, 1 is a conceptual experiment with an obvious intuitive meaning. It can 
be described by discrete approximations, but it is easier to use the whole 
interval as sample space and to assign to each interval its length as prob- 
ability. The conceptual experiment of making two independent random 
choices of points in 0, 1 results in a pair of real numbers, and so the natural 
sample space is a unit square. In this sample space one equates, almost 
instinctively, "probability" with "area." This is quite satisfactory for some 
elementary purposes, but sooner or later the question arises as to what the 
word "area" really means. > 
As these examples show, a continuous sample space may be conceptually 
simpler than a discrete model, but the definition of probabilities in it depends 
on tools such as integration and measure theory. In denumerable sample 
spaces it was possible to assign probabilities to all imaginable events, 
whereas in. general spaces this naive procedure leads to logical contra- 
dictions, and our intuition has to adjust itself to the exigencies of formal logic. 
We shall soon see that the naive approach can lead to trouble even in relatively 
simple problems, but it is only fair to say that many probabilistically 
significant problems do not require a clean definition of probabilities. Some- 
times they are of an analytic character and the probabilistic background 
serves primarily as a support for our intuition. More to the point is the 
fact that complex stochastic processes with intricate sample spaces may lead 
to significant and comprehensible problems which do not depend on the 
delicate tools used in the analysis of the whole process. A typical reasoning 
may run as follows: if the process can be described at all, the random 
variable Z must have such and such properties, and its distribution must 
therefore satisfy such and such an integral equation. Although probabilistic 
arguments can greatly influence the analytical treatment of the equation in 
question, the latter is in principle independent of the axioms of probability. 
3 Intervals are denoted by bars to preserve the symbol (a, b) for the coordinate notation 
of points in the plane. Se^ the list of abbreviations at the front of the book. 
1.2 DENSITIES. CONVOLUTIONS 3 
Specialists in various fields are sometimes so familiar with problems of 
this type that they deny the need for measure theory because they are unac- 
quainted with problems of other types and with situations where vague 
reasoning did lead to wrong results.4 
This situation will become clearer in the course of this chapter, which 
serves as an informal introduction to the whole theory. It describes some 
analytic properties of two important distributions which will be used 
throughout this book. Special topics are covered partly because of significant 
applications, partly to illustrate the new problems confronting us and the 
need for appropriate tools. It is not necessary to study them systematically 
or in the order in which they appear. 
Throughout this chapter probabilities are defined by elementary integrals, 
and the limitations of this definition are accepted. The use of a probabilistic 
jargon, and of terms such as random variable or expectation, may be justified 
in two ways. They may be interpreted as technical aids to intuition based on 
the formal analogy with similar situations in volume 1. Alternatively, every- 
thing in this chapter m&y be interpreted in a logically impeccable manner 
by a passage to the limit from the discrete model' described in example 2(a). 
Although neither necessary nor desirable in principle, the latter procedure 
has the merit of a good exercise for beginners. 
2. DENSITIES. CONVOLUTIONS 
A probability density on the line (or Si1) is a function / such that 
B.1) 
JM-oo 
—oo 
For the present we consider only piecewise continuous densities (see V,3 
for the general notion). To each density / we let correspond its distribution 
function5 F defined by 
B.2) Fix) = [' f(y) dy. 
4 The roles of rigor and intuition are subject to misconceptions. As was pointed out in 
volume 1, natural intuition and natural thinking are a poor affair, but they gain strength 
with the development of mathematical theory. Today's intuition and applications depend 
on the most sophisticated theories of yesterday. Furthermore, strict theory represents 
economy of thought rather than luxury. Indeed, experience shows that in applications 
most people rely on lengthy calculations rather than simple arguments because these 
appear risky. [The nearest illustration is in example 5(o)J 
5 We recall that by "distribution function" is meant a right continuous non-decreasing 
function with limits 0 and 1 at ±oo. Volume 1 was concerned mainly with distributions 
whose growth is due entirely to jumps. Now we focus our attention on distribution functions 
defined as integrals. General distribution functions will be studied in chapter V. 
4 THE EXPONENTIAL AND THE UNIFORM DENSITIES 1.2 
It is a monotone continuous function increasing from 0 to 1. We say that 
/ and F are concentrated on the interval a < x < b if / vanishes outside 
this interval. The density / will be considered as an assignment of prob- 
abilities to the intervals of the line, the interval a, b ~ {a < x < b) having 
probability 
B.3) F(b) - F(a) = f/(*) dx. 
Ja 
Sometimes this probability will be denoted by P{a, b). Under this assign- 
ment an individual point carries probability zero, and the closed interval 
a < x <, b has the same probability as a, b. 
In the simplest situation the real line serves as '''sample space" that is, 
the outcome of a conceptual experiment is represented by a number. (Just 
as in volume 1, this is only the first step in the construction of sample spaces 
representing sequences of experiments.) Random variables are functions 
defined on the sample space. For simplicity we shall for the time being 
accept as random variable only a function U such that for each / the event 
{U < /} consists of finitely many intervals. Then 
B.4) G{t) = P{U < t) 
is well defined as the integral of / over these intervals. The function G 
defined by B.4) is called the distribution function of U. If G is the integral 
of a function g, then g is called the density of the distribution G or (inter- 
changeably) the density of the variable U. 
The basic random variable is, of course, the coordinate variable6 X as 
such, and all other random variables are functions of X. The distribution 
function of X is identical with the distribution F by which probabilities 
are defined. Needless to say, any random variable Y = g(X) can be taken 
as coordinate variable on a new line. 
As stated above, these terms may be justified by mere analogy with the 
situation in volume 1, but the following example shows that our model 
may be obtained by a passage to the limit from discrete models. 
Examples, (a) Grouping of data. Let F be a given distribution function. 
Choose a fixed d > 0 and consider the discrete random variable X^ which 
for (n—l)d < x < nd assumes the constant value nd. Here n — 0, ±1, 
±2,.... In volume 1 we would have used the multiples of d as sample 
6 As far as possible we shall denote random variables (that is, functions on the sample 
space) by capital boldface letters, reserving small letters for numbers or location parameters. 
This holds in particular for the coordinate variable X, namely the function defined by 
X(x) = *. 
1.2 DENSITIES. CONVOLUTIONS 
space, and described the probability distribution of Xa by saying that 
B.5) P{X6=n6} = F(nd) - F«n-1N). 
Now Xa becomes a random variable in an enlarged sample space, and its 
distribution function is the function that for nd <, x < (n+\N equals 
F(nd). In the continuous model, Xa serves as an approximation to X 
obtained by identifying our intervals with their right-hand endpoints (a proce- 
dure known to statisticians as grouping of data). In the spirit of volume 1 we 
should treat Xa as the basic random variable and 6 as a free parameter. 
Letting 6 -*¦ 0 we would obtain limit theorems stating, for example, that 
F is the limit distribution of Xs. 
(b) For x > 0, the event {X2 < x) is the same as {-V* < X <, V*}; 
the random variable X2 has a distribution concentrated on 0, oo and 
given there by F(-Jx) — F(—y/x). By differentiation it is seen that the 
density g of X2 is given by 
g(x) = H/(V*) + f{-yjx))lslx for x > 0 g(x) = 0 for x < 0. 
The distribution function of X3 is given for all x by F($/x) and has 
density \f{Vx)jV&.' 
The expectation of X is defined by 
f+oo 
B.6) E(X) = xf(x) dx, - 
J—00 
provided the integral converges absolutely. The expectations of the approxi- 
mating discrete variables X6 of example (a) coincide with Riemann sums 
for this integral, and so E(Xa)-*E(X). If u is a bounded continuous 
function the same argument applies to the random variable «(X), and the 
relation E(«(Xa))-> E(«(X)) implies 
B.7) E(«(X))= u(x)f(x)dx; 
J— 00 
the point here is that this formula makes no explicit use of the distribution of 
m(X). Thus the knowledge of the distribution of a random variable X 
suffices to calculate the expectation of functions of it. 
The second moment of X is defined by 
J'+oo 
x2f{x) dx, 
—00 
provided the integral converges. Putting /lc = E(X), the variance of X is 
again defined by 
B.9) Var (X) = E((X-//J) = E(X2) - fi\ 
6 THE EXPONENTIAL AND THE UNIFORM DENSITIES 1.2 
Note. If the variable X is positive (that is, if the density / is concen- 
trated on 0. oo) and if the integral in B.6) diverges, it is harmless and 
convenient to say that X has an infinite expectation and write E(X) = oo. 
By the same token one says that X has an infinite variance when the integral 
in B.8) diverges. For variables assuming positive and negative values the 
expectation remains undefined when the integral B.6) diverges. A typical 
example is provided by the density ^( 
The notion of density carries over to higher dimensions, but the general 
discussion is postponed to chapter III. Until then we shall consider only 
the analogue to the product probabilities introduced in definition 2 of 1; 
V,4 to describe combinations of independent experiments. In other words, 
in this chapter we shall be concerned only with product densities of the form 
f(z)g(y)< fix)g(y) Kz), etc., where /, g, . . . are densities on the line. 
Giving a density of the form f(x) g(y) in the plane 3l2 means identifying 
"probabilities" with integrals: 
B.10) P{A} =JJ f{x) g(y) dx dy. 
A 
Speaking of "two independent random variables X and Y with densities 
f and g" is an abbreviation for saying that probabilities in the (X, Y)-plane 
are assigned in accordance with B.10). This implies the multiplication 
rule for intervals, for example P{X > a, Y > b) = P{X > a}P{Y > b). 
The analogy with the discrete case is so obvious that no further explanations 
are required. 
Many new random variables may be defined as functions of X and Y, 
but the most important role is played by the sum S = X + Y. The event 
¦A — (S < s) is represented by the half-plane of points (z, y) such that 
x + y < s. Denote the distribution function of Y by G so that one has 
g(y) = G'(y). To obtain the distribution function of X + Y we integrate 
in B.10) over y < s — x. with the result 
J'+UO 
G(s-x)f(z)dz. 
—oo 
For reasons of symmetry the roles of F and G can be interchanged without 
affecting the result. By differentiation it is then seen that the density of 
X + Y is given by either of the two integrals 
JM-oo /*+oo 
f(s-y) g(y) dy = f(y) g(s-y) dy. 
— oo J—oo 
The operation defined in B.12) is a special case of the convolutions to 
be introduced in V,4. For the time being we use the term convolution 
1.2 DENSITIES. CONVOLUTIONS 7 
only for densities: The convolution of two densities f and g is the function 
defined by B.12). It will be denoted by f* g. 
Throughout volume 1 we dealt with convolutions of discrete distributions, 
and the rules are the same. According to B.12) we have f*g — g*f. 
Given a third density h we can form (f * g) * h and this is the density of 
asum X + Y + Z of three independent variables with densities /, g, h. 
The fact that summation of random variables is commutative and associative 
implies the same properties for convolutions, and so /* g * h is independent 
of the order of the operations. 
Positive random variables play an important role, and it is therefore 
useful to note that if f and g are concentrated on 0, oo the convolution 
f*g of B.12) reduces to 
B.13) /* g(s) = \Sf(s-y) g(y) dy = f/(*) g(s-x) dx. 
Jo Jo 
Example, (c) Let / and g be concentrated on 0, oo and defined there 
by f{x) = aer" and g{x) = pe**. Then 
B.14) /*g(*)=«/? x>0. 
0-oc 
(Continued in problem 12.) *¦ 
Note on the notion of random variable. The use of the line or the Cartesian 
spaces %n as sample spaces sometimes blurs the distinction between random 
variables and "ordinary" functions of one or more variables. In volume 1 
a random variable X could assume only denumerably many values and it was 
• then obvious whether we were talking about a function (such as the square 
or the exponential) defined on the line, or the random variable X2 or ex 
defined in the sample space. Even the outer appearance of these functions 
was entirely different inasmuch as the "ordinary" exponential assumes all 
positive values whereas ex had a denumerable range. To see the change in 
this situation, consider now "two independent random variables X and Y 
with a common density /." In other words, the plane 31* serves as sample 
space, and probabilities are defined as integrals of f(x)f(y). Now every 
function of two variables can be defined in the sample space, and then it 
becomes a random variable, but it must be borne in mind that a function of 
two variables can be defined also without reference to our sample space. For 
example, certain statistical problems compel one to introduce the random 
variable /(X)/(Y) [see example VI, 12(</)]. On the other hand, in introducing 
our sample space 3l2 we have evidently referred to the "ordinary" function/ 
defined independently of the sample space. This "ordinary" function induces 
many random variables, namely /(X), /(Y), /(X±Y), etc. Thus the 
same / may serve either as a random variable or as an ordinary function. 
8 THE EXPONENTIAL AND THE, UNIFORM DENSITIES 1.3 
As a rule (and in each individual case) it will be clear whether or not 
we are concerned with a random variable. Nevertheless, in the general 
theory there arise situations in which functions (such as conditional prob- 
abilities and expectations) can be considered either as free functions ©V as 
random variables, and this is somewhat confusing if the freedom of choice 
is not properly understood. 
Note on terminology and notations. To avoid overburdening of sentences it is customary 
to call E(X), interchangeably, expectation of the variable X, or of the density /, or of 
the distribution F. Similar liberties will be taken for other terms: For example, convolution 
really signifies an operation, but the term is applied also to the result of the operation and 
the function f*g is referred to as "the convolution." 
In the older literature the terms distribution and frequency.function were applied to 
what we call densities; our distribution functions were described as "cumulative," and the 
abbreviation c.d.f. is still in use. 
3. THE EXPONENTIAL DENSITY 
For arbitrary but fixed a > 0 put 
C.1) f(z) = a*-™, F(x) = 1 - e-ax, for x > 0 
and F(x) = f(x) = 0 for x < 0. Then / is an exponential density, F its 
distribution function. A trite calculation shows that the expectation equals 
or1, the variance or2. 
In example 1 (a) the exponential distribution was derived as the limit 
of geometric distributions, and the method of example 2(a) leads to the 
same result. We recall that in stochastic processes the geometric distribution 
frequently governs waiting times or lifetimes, and that this is due to its 
"lack of memory," described, in 1; XIII,9: whatever the present age, the 
residual lifetime is unaffected by the past and has the same distribution as the 
lifetime itself It will now be shown that this property carries over to 
the exponential limit and to no other distribution. 
Let T be an arbitrary positive variable to be interpreted as life- or 
waiting time. It is convenient to replace the distribution function of T 
by its tail 
C.2) U{t) = P{T > /}. 
Intuitively, U(t) is the "probability at birth of a lifetime exceeding, /." 
Given an age s, the event that the residual lifetime exceeds t is the same 
as {T > s-{-t} and the conditional probability of this event (given age s) 
equals the ratio U{s+t)IU(s). This is the residual lifetime distribution, and it 
coincides with the total lifetime distribution iff 
C.3) U(s+t) = U(s) V(t), s,t>0. 
1.3 THE EXPONENTIAL DENSITY 9 
It "was shown in X; XVII,6 that a positive solution of this equation is necessarily 
of the form U(t) = e~at, and hence the lack of aging described above in 
italics holds true if the lifetime distribution is exponential. 
We shall refer to this lack of memory as the Markov property of the 
exponential distribution. Analytically it reduces to the statement that 
only for the exponential distribution F do the tails U — 1 —/" satisfy 
C.3), but this explains the constant occurrence of the exponential dis- 
tribution in Markov processes. (A stronger ve&ion of the Markov property 
will be described in section 6.) Our description referred to temporal processes, 
but the argument is general and the Markov property remains meaningful 
when time is replaced by some other parameter. 
Examples, (a) Tensile strength. To obtain a continuous analogue to 
the proverbial finite chain whose strength is that of its weakest link denote 
by U(t) the probability that a thread of length t (of a given material) can 
sustain a certain fixed load. A thread of length s-t-t does not snap iff the 
two segments individually sustain the given load. Assuming that there is no 
interaction, the two events must be considered independent and U must 
satisfy C.3). Here the length of the thread takes over the role of the time 
parameter, and the length at which the thread will break is an exponentially 
distributed random variable. 
(b) Random ensembles of points in space play a role in many connections 
so that it is important to have an appropriate definition for this concept. 
Speaking intuitively, the first property that perfect randomness should have 
is a lack of interaction between different regions: the observed configuration 
within region Ax should not permit conclusions concerning the ensemble 
in a non-overlapping region A2. Specifically, the probability p that both 
Ax and A2 are empty should equal the product of the probabilities px and 
p2 that Ax and A2 be empty. It is plausible that this product rule cannot 
hold for all partitions unless the probability p depends only on the volume 
of the region A but not on its shape. Assuming this to be so, we denote 
by U(t) the probability that a region of volume t be empty. These prob- 
abilities then satisfy CP3) and hence U(t) — e~at\ the constant a depends 
on the density of the ensemble or, what amounts to the same, on the unit of 
length. It will be shown in the next section that the knowledge of 1/@ 
permits us to calculate the probabilities pn(t} that a region of volume / 
contains exactly n points of the ensemble; they are given by the Poisson dis- 
tribution pjt) = ^~"'(a0n/«!. We speak accordingly of Poisson ensembles 
of points, this term being less ambiguous than the term random ensemble 
which may have other connotations. 
(jc) Ensembles of circles and spheres. Random ensembles of particles 
present a more intricate problem. For simplicity we assume that the particles 
10 THE EXPONENTIAL AND THE UNIFORM DENSITIES 1.3 
are of a spherical or circular shape, the radius p being fixed. The con- 
figuration is then completely determined by the centers and it is tempting to 
assume that these centers form a Poisson ensemble. This, however, is 
impossible in the strict sense since the mutual distances of centers necessarily 
exceed 2p. One feels nevertheless that for small radii p the effect of the 
finite size should be negligible in practice and hence the model of a Poisson 
ensemble of centers should be usable as an approximation. 
For a mathematical model we postulate accordingly that the centers form 
a Poisson ensemble and accept the implied possibility that the circles or 
spheres intersect. This idealization will have no practical consequences if the 
radii p are small, because then the theoretical frequency of intersections 
will be negligible. Thus astronomers treat the stellar system as a Poisson 
ensemble and the approximation to reality seems excellent. The next two 
examples show how the model works in practice. 
(d) Nearest neighbors. We consider a Poisson ensemble of spheres (stars) 
with density a. The probability that a domain of volume t contains no 
center equals e~at. Saying that the nearest neighbor to the origin has a 
distance >r amounts to saying that a sphere of radius r contains no star 
center in its interior. The volume of such a ball equals %nrz, and hence in a 
Poisson ensemble of stars the probability that the nearest neighbor has a 
distance >r is given by e~tjr(zr*. The fact that this expression is independent 
of the radius p of the stars shows the approximative character of the model 
and its limitations.i 
In the plane, spheres are replaced by circles and the distribution function 
for the distance of nearest neighbors is given by 1 — e~avrZ. 
(e) Continuation: free paths. For ease of description we begin with the 
two-dimensional model. The random ensemble of circular disks may be 
interpreted as the cross section of a thin forest. I stand at the origin, which 
is not contained in any disk, and look in the direction of the positive a>axis. 
The longest interval 0, t not intersecting any disk represents the visibility 
or free path in the ^-direction. It is a random variable and we denote 
it by L. 
Denote by A the region formed by the points at a distance <,p from a 
point of the interval 0, t on the a>axis. The boundary of A consists of the 
segments 0 <, x <, t on the lines y — ±p and two semicircles of radii p 
about the origin and the point (t, 0) on the a>axis. Thus the area of A 
equals 2pt + np2. The event {L > t} occurs iff no disk center is con- 
tained within A, but it is known in advance that the circle of radius p 
about the origin is empty. The remaining domain has area 2pt and we 
conclude that the distribution of the visibility L is exponential: 
P{L > t} = 
1.4 WAITING TIME PARADOXES. THE POISSON PROCESS 11 
In space the same argument applies and the relevant region is formed by 
rotating our A about the a>axis. The rectangle 0 < x < t, \y\ < p is 
replaced by a cylinder of volume np2t. We conclude that in a Poisson en- 
semble of spherical stars the free path L in any direction has an exponential 
distribution: P{L > t} = e~vaptt. The mean free path is given by 
E(L) = 
The next theorem will be used repeatedly. 
Theorem. If Xt,... ,Xn are mutually independent random variables 
with the exponential distribution C.1), then the sum Xx 4- • • • + Xn has a 
density gn and distribution function Gn given by 
C.4) gn(x) = a K ' e~*x x>0 
C.5) Gn(x) = l -e-*\l +- + ...+ ^-[) *>0. 
Proof. For n = 1 the assertion reduces to the definition C.1). The 
density gn+1 is defined by the convolution 
C-6) gn+1(t) = I gn(t-x) gl(x) dx, 
Jo 
and assuming the validity of C.4) this reduces to 
C-7) gn+1(t) = — e-** \xn dx = a ±-j- e^ 
(n — 1I Jo n\ 
Thus CA) holds by induction for all n. The validity of C.5) is seen by 
differentiation. *. 
The densities gn are among the gamma densities to be introduced in 
11,2. They represent the continuous analogue of the negative binomial 
distribution found in 1; VI ,8 for the sum of n variables with a common 
geometric distribution. (See problem 6.) 
4. WAITING TIME PARADOXES. THE POISSON PROCESS 
Denote by X1} X2,. .. mutually independent random variables with the 
common exponential distribution C.1), and put So = 0, 
D.1) Sn = \1-\ h An, n = l,2, 
We introduce a family of new random variables N(f) as follows: N(f) is 
the number of indices k ^ 1 such that Sk <, t. The event {N(f) = n} 
12 THE EXPONENTIAL AND THE UNIFORM DENSITIES 1.4 
occurs iff Sn <, t but Sn+1 > t. As Sn has the distribution Gn the 
probability of this event equals Gn(t) — Gn+1(t) or 
D.2) P{N@ = n} - e~' 
n\ 
In words, the random variable N(t) has a Poisson distribution with ex- 
pectation cct. 
This argument looks like a new derivation of the Poisson distribution 
but in reality it merely rephrases the original derivation of 1; VI ,6 in terms 
of random variables. For an intuitive description consider chance occurrences 
(such as cosmic ray bursts or telephone calls), which we call "arrivals." 
Suppose that there is no aftereffect in the sense that the past history permits 
no conclusions as to the future. As we have seen, this condition requires 
that the waiting time Xx to the first arrival be exponentially distributed. 
But at each arrival the process starts from scratch as a probabilistic replica of 
the whole process: the successive waiting times Xk between arrivals must 
be independent and must have the same distribution. The sum Sn represents 
the epoch of the nth arrival and N(?) the number of arrivals'within the 
interval 0, t. In this form the argument differs from the original derivation 
of the Poisson distribution only by the use of better technical terms. 
(In the terminology of stochastic processes the sequence {Sn} constitutes 
a renewal process with exponential interarrival times Xk; for the general 
notion see VI,6.) 
Even this simple situation leads to apparent contradictions which illustrate 
the need for a sophisticated approach. We begin by a naive formulation. 
Example. Waiting time paradox. Buses arrive in accordance with a 
Poisson process, the expected time between consecutive buses being a. 
I arrive at an epoch, t. What is the expectation E(W<) of my waiting time 
Wt for the next bus ? (It is understood that the epoch t of my arrival is 
independent of the buses, say noontime sharp.) Two contradictory answers 
stand to reason: 
(a) The lack of memory of the Poisson process implies that the distribution 
of my waiting time should not depend on the epoch of my arrival. In this 
case E(W«) = E(W0) = a. 
(b) The epoch of my arrival is "chosen at random" in the interval, 
between two consecutive buses, and for reasons of symmetry my expected 
waiting time should be half the expected time between two consecutive 
buses, that is E(W<) = lor1. 
Both arguments appear reasonable and both have been used in practice. 
What to do about the contradiction? The easiest way out is that of the 
formalist, who refuses to see a problem if it is not formulated in an 
impeccable manner. But problems are not solved by ignoring them. 
1.4 WAITING TIME PARADOXES. THE POISSON PROCESS 13 
We now show that both arguments are substantially, if not formally, 
correct. The fallacy lies at an unexpected place and we now proceed to 
explain it.7 ^. 
We are dealing with interarrival times Xx = S1} X2 = S2 — S1}. . . . By 
assumption the Xk have a common exponential distribution with expectation 
a. Picking out "any" particular Xk yields a random variable, and one has 
the intuitive feeling that its expectation should be a provided the choice 
is done without knowledge of the sample sequence Xt, X2',.... But this 
is not true. In the example we chose that element Xk for which 
< / < Sk, 
where t is fixed. This choice is made without regard to the actual process, 
but it turns out that the Xk so. chosen has the double expectation 2a-1. 
Given this fact, the argument (b) of the example postulates an expected 
waiting time a and the contradiction disappears. 
This solution of the paradox came as a shock to experienced workers, 
but it becomes intuitively clear once our mode of thinking is properly 
adjusted. Roughly speaking, a long interval has a better chance to cover 
the point t than a short one. This vague feeling is supported by the following 
Proposition. Let Xx, X2,... be mutually independent with a common 
exponential distribution with expectation or1. Let t > 0 be fixed, but 
arbitrary. The element Xk satisfying the condition Sk_x < t <, Sk has the 
density 
a.2ze~ax for 0 < x < t 
D.3) vt(x) = 
a.{l-\-a.t)e~ax for x > t. 
The point is that the density D.3) is not the common density of the Xk. 
Its explicit form is of minor interest. [The analogue for arbitrary waiting 
time distributions is contained in XI,D.16).] 
Proof. Let k be the (chance-dependent) index such that Sk_x < t < Sk 
and put Tut equal to S* — Sk_v We have to prove that L< has density 
D.3). Suppose first x < t. The event {Lt < x} occurs iff Sn = y and 
t—y < Xn+1 <, x for some combination n, y. This necessitates 
t-x<,y <,t. 
Summing over all possible n and y we obtain 
D.4) P{L, < *} = f f gn(y) ¦ [e-*lt-v) - e~*x) dy. 
n=lJi~x 
7 For a variant of the paradox see example VI,7(a). The paradox occurs also in general 
renewal theory where it caused serious trouble and contradictions before it was properly 
understood. For the underlying theory see XI.4. 
14 THE EXPONENTIAL AND THE UNIFORM DENSITIES 1.4 
But gx(y) + g2(y) + * * * = a identically, and so 
D.5) P{L, < x) = 1 - e~ax - axe 
—ax 
By differentiation we get D.3) for x < t. For x > t a similar argument 
applies except that y ranges from 0 to / and we must add to the right side 
in D.4) the probability e~at — e~ax that 0 < t < Sx < x. This completes 
the proof. *. 
The break in the formula D.3) at x = t is due to the special role of the 
origin as the starting epoch of the process. Obviously 
D.6) \imvt(x) = a.2 
<- 
which shows that the special role of the origin wears out, and for an "old" 
process the distribution of Lt is nearly independent of t. One expresses 
this conveniently by saying that the "steady state" density of Lt is given 
by the right side in D.6). 
With the notations of the proof, the waiting time W< considered in the 
example is the random variable ,Wt = Sk — t. The argument of the proof 
shows also that 
oo rt 
/a n\ ?{Wt <[ x} = e~at — e~a x+t + 2, I Sn{y)[e~a <rV ~~ e~* x+t~v ] dy 
V*m') n=l Jo 
= 1 - e~ax 
Thus W, has the same exponential distribution as the Xk in accordance 
with the reasoning (a). (See problem 7.) 
Finally, a word about the Poisson process. The Poisson variables N(?) 
were introduced as functions on the sample space of the infinite sequence 
of random variables X1? X2,..... This procedure is satisfactory for many 
purposes, but a different sample space is more natural. The conceptual 
experiment "observing the number of incoming calls up to epoch t" yields 
for each positive t an integer, and the result is therefore a step function with 
unit jumps. The appropriate sample space has these step functions as sample 
points; the sample space is a function space—the space of all conceivable 
"paths." In this space N(r) is defined as the value of the ordinate at epoch 
t and Sn as the coordinate of the nth jump, etc. Events can now be considered 
that are not easily expressible in terms of the original variables Xn. A typical 
example of practical interest (see the ruijn problem in VI,5) is the event that 
N@ > a + bt for some t. The individual path (just as the individual 
infinite sequence of ± 1 in binomial trials) represents the natural and un- 
avoidable object of probabilistic inquiry. Once one gets used to the new 
phraseology, the space of paths becomes most intuitive. 
1.5 THE PERSISTENCE OF BAD LUCK 15 
Unfortunately the introduction of probabilities in spaces of sample 
paths is far from simple. By comparison, the step from discrete sample 
spaces to the line, plane, etc., and even to infinite sequences of random 
variables, is neither conceptually nor technically difficult. Problems of a 
new type arise in connection with function spaces, and the reader is warned 
that we shall not deal with them in this volume. We shall be satisfied with 
an honest treatment of sample spaces of sequences (denumerably many 
coordinate variables). Reference to stochastic processes in general, and to 
the Poisson process in particular, will be made freely, but only to provide 
an intuitive background or to enhance interest in our problems. 
Poisson Ensembles of Points 
As shown in 1; VI ,6, the Poisson law governs not only "points dis- 
tributed randomly along the time axis," but also ensembles of points (such 
as flaws in materials or raisins in a cake) distributed randomly in plane or 
space, provided t is interpreted as area or volume. The basic assumption 
was that the probability of finding k points in a specified domain depends 
only on the area or volume of the domain, but not on its shape,"and that 
occurrences in non-overlapping domains are independent. In example 
3(b) we used the same assumption to show that the probability that a domain 
of volume / be empty is given by er**. This corresponds to the exponential 
distribution for the waiting time for the first event, and we see now that the 
Poisson distribution for the number of events is a simple consequence of it. 
The same argument applies to random ensembles of points in space, and we 
have thus a new proof for the fact that the number of points of the ensemble 
contained in a given domain is a Poisson variable. Easy formal calculations 
may lead to interesting results concerning such random ensembles of points, 
but the remarks about the Poisson process apply equally to Poisson en- 
sembles; a complete probabilistic description is complex and beyond the 
scope of the present volume. 
5. THE PERSISTENCE OF BAD LUCK 
As everyone knows, he who joins a waiting line is sure to wait for an 
abnormally long time, and similar.bad luck follows us on all occasions. 
How much can probability theory contribute towards an explanation? 
For a partial answer we consider three examples typical of a variety of 
situations. They illustrate unexpected general features of chance fluctuations. 
Examples, (a) Record values. Denote by Xo my waiting time (or financial 
loss) at some chance event. Suppose that friends of mine expose themselves 
to the same type of experience, and denote the results by Xlf X2,.... 
To exclude bias we assume that Xo, Xlt... are mutually independent 
16 THE EXPONENTIAL AND THE UNIFORM DENSITIES 1.5 
random variables with a common distribution. The nature of-the latter 
really does not matter but, since the exponential distribution serves as a 
model for randomness, we assume the X,- exponentially distributed in 
accordance with C.1). For simplicity of description we treat the sequence 
{X;} as infinite. 
To find a measure for my ill luck 1 ask how long it will take before a 
friend experiences worse luck (we neglect the event of probability zero that 
Xjt = Xo). More formally, we introduce the waiting time N as the value of 
the first subscript n such that Xn > Xo. The event {N>«—1} occurs 
iff the maximal term of the «-tuple Xq, Xlt... , Xn-1 appears at the initial 
place; for reasons of symmetry the probability of this event is rr1. The 
event {N = n} is the same as {N > «—1} — {N > «}, and hence for 
n = 1,2, . . . , 
E.1) P{N = „} = l- - -±¦= —L-; 
n n+1 n(n+l) 
This result fully confirms that 1 have indeed very bad luck: The random 
variable N has infinite expectation^. It would be bad enough if it took on the 
average 1000 trials to beat the record of my ill luck, but the actual waiting 
time has infinite expectation. 
It will be noted that the argument does not depend on the.condition that 
the Xk are exponentially distributed. It follows that whenever the variables 
X; are independent and have a common continuous distribution function 
F the first record value has the distribution E.1). The fact that this 
distribution is independent of F is used by statisticians for tests of independ- 
ence. (See also problems 8-11.) 
The striking and general nature of the result E.1) combined with the 
simplicity of the proof are apt to arouse suspicion. The argument is really 
impeccable (except for the informal presentation), but those who prefer to 
rely on brute calculation can easily verify the truth of E.1) from the direct 
definition of the probability in question as the («+l)-tuple integral of 
an+ie-a(x0+-+xn) over the region defined by the inequalities 0 < a-0 < xn 
and 0 < Xj < x0 for j = 1,. . . , n—1. 
An alternative derivation of E.1) is an instructive exercise in conditional probabilities; 
it is less simple, but leads to additional results (problem 8). Given that Xo = x, the 
probability of a greater value at later trials is p = e~ax, and we are concerned with the 
waiting time for the first "success" in Bernoulli trials with probability p. The conditional 
probability that N = n given X,, = x is therefore pXl—p)n~i. To obtain P{N = n) 
we have to multiply by the density cce~ax of the hypothesis Xo = x and integrate with 
respect to x. The substitution 1 — e~ax = t reduces the integrand to tn~i(\—t), the 
integral of which equals n~i — (n+l)~1 in agreement with E.1). 
(b) Ratios. If X and Y are two independent variables with a common 
exponential distribution, the ratio Y/X is a new random variable. Its 
L6 WAITING TIMES AND ORDER STATISTICS 17 
distribution function is obtained by integrating a.2e~a(x+v) over 0 < y < tx, 
0 < x < go. Integration with respect to y leads to 
E.2) 
The corresponding density is given by (l+/)~2. It is noteworthy that the 
variable Y/X has infinite expectation. 
We find'here a new confirmation for the persistence of bad luck. Assuredly 
Peter has reason for complaint if he has to wait three times as long as Paul, 
but the distribution E.2) attributes to this event probability \. It follows 
that, on the average, in one out of two cases either Paul or Peter has reason 
for complaint. The observed frequency increases in practice because very 
short waiting times naturally pass unnoticed. 
(c) Parallel waiting lines. 1 arrive in my car at the car inspection station 
(or at a tunnel entrance, car ferry, etc.). There are two waiting lines to 
choose from, but once I have joined a line 1 have to stay in it. Mr. Smith, 
who drove behind me, occupies the place that 1 might have chosen and I 
keep watching whether he is ahead of or behind me. Most of the time we 
stand still, but occasionally one line or the other moves one car-length, 
forward. To maximize the influence of pure chance we assume the two 
lines stochastically independent; also, the time intervals between successive 
moves are independent variables with a common exponential distribution. 
Under these circumstances the successive moves constitute Bernoulli trials 
in which "success" means Jthat I move ahead, "failure" that Mr. Smith 
moves. The probability of success being \, we are, in substance, dealing with 
a symmetric random walk, and the curious properties of fluctuations in 
random walks find a striking interpretation. (For simplicity of description 
we disregard the fact that only finitely many cars are present.) Am I ever 
going to be ahead of Mr. Smith? In the random walk interpretation the 
question is whether a first passage through +1 will ever take place. As we 
know, this event has probability one, but the expected waiting time for it is 
infinite. Such waiting gives ample apportunity to bemoan my bad luck, and 
this only grows more irritating by the fact that Mr. Smith argues in the same 
way. > 
6. WAITING TIMES AND ORDER STATISTICS 
An ordered «-tuple (xly > .. , xn) of real numbers, may be reordered in 
increasing order of magnitude to obtain the new «-tuple 
(*(i)» *<«» • • • >x(n)) where x{1) <, x{2) <; • • • ^ x 
(n)' 
18 THE EXPONENTIAL AND THE UNIFORM DENSITIES 1.6 
This operation applied to all points of the space 3in induces n well-defined 
functions, which will be denoted by XA),. .. , X(n). If probabilities are 
defined in %n these functions become random variables. We say that 
(XA),.. . , X(n)) is obtained by reordering (Xl5... , Xn) according to 
increasing magnitude. The variable X(k) is called kth-order statistic8 of 
the given sample Xl5. . . , Xn. In particular, XA) and X(n) are the sample 
extremes; when n — 2v + 1 is odd, X(v+1) is the sample median. 
We apply this notion to the particular case of independent random 
variables Xl5.. . , Xn with the common exponential density a.e~ax. 
Examples, (a) Parallel waiting lines. Interpret Xlf.. ., Xn as the lengths 
of n service times commencing at epoch 0 at a post office with n counters. 
The order statistics represent the successive epochs of terminations or, as 
one might say, the epochs of the successive discharges (the "output process"). 
In particular, XA) is the waiting time for the first discharge. Now if the 
assumed lack of aftereffect is meaningful, the waiting time XA) must have 
the Markoy property, that is, XA) must be exponentially distributed. As 
a matter t>f fact, the event {XA) > t} is the simultaneous realization of 
the n events {Xk > t}, each of which has probability e'; because of the 
assumed independence the probabilities multiply and we have indeed 
F.1) 
o—wxt 
We can now proceed a step further and consider the situation at epoch 
XA). The assumed lack of memory seems to imply that the original situation 
is restored except that now only n — 1 counters are in operation; the 
continuation of the process should be independent of XA) and a replica of the 
whole process. In particular, the waiting time for the next discharge, 
namely XB) — XA), should have the distribution 
F.2) P{XB)-XA) > 1} - e-(n-1)at 
analogous to F.1). This reasoning leads to the following general proposition 
concerning the order statistics for independent variables with a common 
exponential distribution. 
8 Strictly speaking the term "sample statistic" is synonymous with "function of the 
sample variables," that is, with random variable. It is used to emphasize linguistically the 
different role played in a given context by the primary variable (the sample) and some 
derived variables. For example, the "sample mean" (Xx+ hXn)/n is called a statistic. 
Order statistics occur frequently in the statistical literature. We conform to the standard 
terminology except that the extremes are usually called extreme "values." 
1.6 WAITING TIMES AND ORDER STATISTICS 19 
Proposition9 The n variables XA), XB) — XA),. .. , X(n) — X()l_1) are 
independent and the density of X(lc+X) — X(fc) is given by (n—k)oLe~in~k)ai. 
Before verifying this proposition formally let us consider its implications. 
When n = 2 the difference XB) — XA) is the residual waiting time after the 
expiration of the shorter of two waiting times. The proposition asserts that 
this residual waiting time has the same exponential distribution as the original 
waiting time and is Independent of XA). This is an extension of the Markov 
property enunciated for fixed epochs t to the chance-dependent stopping 
time XA). It is called the strong Markov property. (As we are dealing with 
only finitely many variables we are in a position to derive the strong Markov 
property from the weak one, but in more complicated stochastic processes the 
distinction is essential.) 
The proof of the proposition serves as an example of formal manipulations 
with integrals. For typographical simplicity we let n — 3. As in many 
similar situations we use a symmetry argument. With probability one, no 
two among the variables X^ are equal. Neglecting an event of probability 
zero the six possible orderings of Xu X2, X3 according to magnitude there- 
fore iepresent six mutually exclusive events of equal probability. To cal- 
culate the distribution of the order statistics it suffices therefore to consider 
the contingency Xx < X2 < X3. Thus 
{A) > tlt XB)—XA) > t2, XC)—XB) > 
- 6P{XX > tlt X2-X, > t2, X3-X, 
(Purely analytically, the space 3l3 is partitioned into six parts congruent to the 
region defined by xt < x2 < x3, each contributing the same amount to the 
integral. The boundaries where two or more coordinates are equal have 
probability zero and play no role.) To evaluate the right side in F.3) we 
have to integrate a.3e~a{xi+X2+X3) over the region defined by the inequalities 
X\ ^> ti, X2 — Xi > t2, X3 — X2 > ?3. 
A simple integration with respect to x3 leads to 
F.4) 
i ~ 
dxx — 
9 This proposition has been discovered repeatedly for purposes of statistical estimation 
but the usual pr ofe are computational instead of appealing to the Markov property. See 
also problem 13. 
20 THE EXPONENTIAL AND THE UNIFORM DENSITIES 1.6 
Thus the joint distribution of the three variables XA), XB)—XA), XC)—XB) 
is a product of three exponential distributions, and this proves the proposition. 
It follows in particular that E(X(fc+1)—X(fc)) = l/(«— k)<x. Summing over 
k = 0, 1,. . . , v—I we obtain 
F.5) E(X(V)) = 1A + -i- + • • • + —L_\. 
a\/j n — 1 n — v+lj 
Note that this expectation was calculated without knowledge of the distri- 
bution of X(v) and we have here another example of the advantage to be 
derived from the representation of a random variable as a sum of other 
variables. (See 1; IX,3.) 
(b) Use of the strong Markov property. For picturesque language suppose 
that at epoch 0 three persons A, B, and C arrive at a post office and find 
two counters free. The three service times are independent random variables 
X, Y, Z with the same exponential distribution. The service times of A. 
and B commence immediately, but that of C starts at the epoch XA) 
when either A or B is discharged. We show that the Markov property 
leads to simple answers to various questions. : 
(i)" What is the probability that C will not be the last to leave the post 
office? The answer is J, because epoch XA) of the first departure establishes 
symmetry between C and the other person being served. 
(ii) What is the distribution of the time T spent by C at the post office? 
Clearly T = XA) + Z is the sum of two independent variables whose 
distributions are exponential with parameters 2a and a. The convolution 
of two exponential distributions is given by B.14), and it is seen that T has 
density u(t) = 2a(e~a' - e~2a<) and E(T). - 3/B<x). 
(iii) What is the distribution of the epoch of the last departure? Denote 
the epochs of the successive departures by XA), XB), XC). The difference 
XC) — XA); is the sum of the two variables XC) — XB) and XB> — XA). 
We saw in the preceding example that these variables are independent and 
have exponential distributions with parameters 2a and a. It follows that 
XC) — XA) has. the same density u as the variable T. Now XA) is 
independent of XC) — XA) and has density 2ae~2aV The convolution 
formula used in (ii) shows therefore that XC) has density 
4<x[e-at-e~2at-*te-2at] 
and E(XC)) = 2/a. 
The advantage of this method becomes clear on comparison with direct 
calculations, but the latter apply to arbitrary service time distributions 
(problem 19). 
(c) Distribution of order statistics. As a final exercise we derive the 
distribution of Xik). The event {X(Jfc) < t) signifies that at least k among 
1.7 THE UNIFORM DISTRIBUTION 21 
the n variables X, are <,t. This represents at least k "successes" in n 
independent trials, and hence 
F.6) 
By differentiation it is seen tha'tthe density of X{k) is given by 
F.7) n(n~\\ _e-«<)*-y-<«-*>«', 
This result may be obtained directly by the following loose argument. 
We require (up to terms negligible in the limit as A-*0) the probability 
of the joint event that one among, the variables X^ lies between t and 
t + h and that k — 1 among the remaining n — 1 variables are <,t, while 
the other n — k variables are >t + h. Multiplying the number of choices 
and the corresponding probabilities leads to F.7). Beginners are advised to 
formalize this argument, and also to derive F.7) from the discrete model. 
(Continued in problems 13, 17.). > 
7. THE UNIFORM DISTRIBUTION 
The random variable X is distributed uniformly in the interval a, b if 
its density is constant = {b—dy1 for a < x < b and vanishes outside 
this interval. In this case the variable (X-?a)(b—a)*1 is distributed uniformly 
in 0,1, and we shall usually use this interval as standard. Because of the 
appearance of their graphs the densities of the uniform distribution function 
are called "rectangular." 
With the uniform distribution the interval 0, 1 becomes a sample space 
in which probabilities of intervals are identical with their lengths. The 
sample space corresponding to two independent variables X and Y that 
are uniformly distributed over 0,1 is the unit square in Si\ and probabilities 
in it are defined by their area. The same idea applies to triples and n-tuples. 
A uniformly distributed random variable is often called a "point X chosen 
at random.'" The result of the conceptual experiment "« independent 
random choices of a point in 0, 1" requires an H-dimensional hypercube 
for its probabilistic description, but the experiment as such yields n points 
Xj,. . , , Xn in the same interval. With unit probability no two of them are 
equal, and hence they partition 0, 1 into n + 1 subintervals. Reordering 
the ri points Xlf. . . , Xn in their natural order from left to right we get n 
new random variables which will be denoted by XA),. . . , XM. These are 
the order statistics defined in the last section. The subintervals of the 
partition are now 0, XA), then XA), XB), etc. 
22 THE EXPONENTIAL AND THE UNIFORM DENSITIES 1.7 
The notion of a point chosen at random on a circle is self-explanatory. To 
visualize the result of n independent choices on the circle we imagine the 
circle oriented anticlockwise, so that intervals have left and right endpoints 
and may be represented in the form a, b. Two points X± and X2 chosen 
independently and at random divide the circle into the two intervals Xlt X2 
and X2, Xj. (We disregard again the zero-probability event that Xx = X2.) 
Examples, (a) Empirical interpretations. The roulette wheel is generally 
thought of as a means to effect a "random choice" on the circle. In numerical 
calculations to six decimals the rounding error is usually treated as a random 
variable distributed uniformly over an interval of length 10~6. (For the error 
committed by dropping the last two decimals, the discrete model with 100 
possible values is more appropriate, though less convenient in practice.) 
The waiting time of a passenger arriving at the bus station without regard to 
the schedule may be regarded as uniformly distributed over the interval 
between successive departures. Of wider theoretical interest are the appli- 
cations to random splittings discussed in section 8. In many problems of 
mathematical statistics (such as non-parametric tests) the uniform distri- 
bution enters in an indirect way: given an arbitrary random variable X 
with a continuous distribution F the random variable F(X) is distributed 
uniformly over 0, 1. (See section 12.) 
(b) The induced partition. We prove the following proposition: n in- 
dependently and randomly chosen points Xlt... , Xn partition 0, 1 into 
n + 1 intervals whose lengths have the common distribution given by 
G.1) P{L>f} = (l-f)n, 0</<I. 
This result is surprising, because intuitively one might expect that at least the 
two end intervals should have different distributions. That all n + 1 
intervals have the same distribution becomes clear on considering the 
equivalent situation on the (oriented) circle of unit length.10 Here n + 1 
points Xlt. .. , Xn+1 chosen independently and at random partition the 
circle into n +1 intervals, and for reasons of symmetry these intervals 
must have the same distribution. Imagine now the circle cut at the point 
Xn+1 to obtain an interval in which Xlt.. . , Xn are chosen independently 
10 For a computational verification note that the probability of the event 
" x(*) > ') 
equals the integral of the constant function 1 over the union of the n\ congruent regions 
defined either by the string of inequalities xx < • • • < xk < xk + t < xk+1 < • • • < xn or 
by similar strings obtained by permuting the subscripts. A more streamlined calculation 
leading to a stronger result is contained in example III, 3(c). 
1.7 THE UNIFORM DISTRIBUTION 23 
and at random. The lengths of the n + 1 intervals of the induced partition 
are the same, and they have a common distribution. That this distribution is 
given by G.1) may be seen by considering the leftmost interval 0, XA). Its 
length exceeds t iff all n points Xlt... ,Xn are in t, 1, and the probability 
of this event is A— t)n. 
It is a good exercise to verify the proposition in the special case n = 2 by 
inspection of the three events in the unit square representing the sample 
space. (Continued in problems 22-26.) 
(c) A paradox (related to the waiting time paradox of section 4). Let two 
points Xx and X2 be chosen independently and at random on the circle of 
unit length. Then the lengths ofthe two intervals Xlt X2 and X2, Xt are 
uniformly distributed, but the length X of the one containing the arbitrary 
point P has a different distribution {with density 2x). 
In particular, each of the two intervals has expected length \, but the 
one containing P has expected length f. The point P being fixed* but 
arbitrary, one has the feeling that the interval covering P is chosen "without 
advance knowledge of its properties" (to borrow a phrase from the philos- 
ophers of probability). Certainly naive intuition is not prepared for the 
great difference between covering or not covering an arbitrary point, but after 
due reflection this difference becomes "intuitively obvious." In fact, how-, 
ever, rather experienced writers have fallen into the trap. 
For a proof imagine the circle cut at P leaving us. with two points chosen 
independently and at random in 0,1. Using the same notation as before the 
event {X < t) occurs iff XB) — XA) > 1 — t and by G.1) the probability 
for this equals t\ The variable X has therefore density 2t, as asserted. 
(Beginners are advised to try a direct computational verification.) 
(d) Distribution of order statistics. If Xlt... , Xn are independent and 
distributed uniformly in 0,1, the number of variables satisfying the in- 
equality 0 < X,- ^ t < 1 has a binomial distribution with probability of 
"success" equal to t. Now the event {X(k) <, t} occurs iff at least k among 
the variables are <,t and hence 
G.2) 
This gives us the distribution function of the &th-order statistics. By 
differentiation it is found that the density of X(k) is given by 
G.3) 
This may be seen directly as follows: The probability that one among the 
X, lies between t and t + h, and that k — 1 among the remaining ones 
24 THE EXPONENTIAL AND THE UNIFORM DENSITIES 1.7 
are less than / while n — k are greater than t + h, equals 
Divide by h and let h -+ 0 to obtain G.3). 
(e) Limit theorems. To see the nature of the distribution of XA) when 
n is large it is best to introduce E(XA)) = (n+l)'1 as a new unit of measure- 
ment. As n -*¦ oo we get then for the tail of the distribution function 
e~*. 
G.4) P{*XA) > t} = (l - '-J- 
It is customary to describe this relation by saying that in the limit XA) is 
exponentially distributed with expectation rr1. Similarly 
G.5) 
„-* 
and on the right one recognizes the tail of the gamma distribution G2 of 
C.5). In like manner it is easily verified that for every fixed k as «—»- oo 
the distribution of nXik) tends to the gamma distribution Gk (see problem 33). 
Now Gk is the distribution of the sum of k independent exponentially 
distributed variables while X{k) is the sum of the first k intervals considered 
in example (b). We can therefore say that the lengths of the successive 
intervals of our partition behave in the limit as if they were mutually in- 
dependent exponentially distributed variables. 
[In view of the obvious relation of G.2) with the binomial distribution 
the central limit theorem may be used to obtain apprpximations to the 
distribution of Xik) when both n and k are large. See problem 34.] 
(/) Ratios. Let X be chosen at random in 0, 1 and denote by U the 
length of the shorter of the intervals 0, X and X, 1 and by V = 1 — U 
the length of the longer. The random variable U is uniformly distributed 
between 0 and | because the event {U < t < ?} occurs iff either X < t or 
1 — X < t and therefore has probability 2t. For reasons of symmetry V 
is uniformly distributed between ? and 1, and so E(U) = J, E(V) = f. 
What can we say about the ratio V/U? It necessarily exceeds 1 and it lies 
between 1 and t>l iff either 
^X^i or i<X*?. 
For t > 1 it follows that 
1.8 RANDOM SPLITTINGS 25 
and the density of this distribution is given by 2(/+l). It is seen that 
V/U has infinite expectation. This example shows how little information 
is contained in the observation that E(V)/E(U) = 3. ». 
8. RANDOM SPLITTINGS 
The problem of this section concludes tjie preceding parade of examples 
and is separated from them partly because of its importance in physics, 
and partly because it will serve as a prototype for general Markov chains. 
Formally we are concerned with products of the form Zn = XjX2 • • • Xn 
where Xlt. . ,Xn are mutually independent variables distributed uni- 
formly in 0, 1. 
Examples for applications. In certain collision processes a physical 
particle is split into two and its mass m divided between them. Different 
laws of partition may fit different processes, but it is frequently assumed 
that the fraction of parental mass received by each descendant particle is 
distributed uniformly in 0, 1. If one of the two particles is chosen at random 
and subject to a new collision then (assuming that there is no interaction 
so that the collisions are independent) the masses of the two second-generation 
particles are given by products mX1X2, and so on. (See problem 21.) With 
trite verbal changes this model applies also to splittings of mineral grains or 
pebbles, etc. Instead of masses one considers also energy losses under 
collisions, and the description simplifies somewhat if one is concerned with 
changes of energy of the same particle in successive collisions. As a last 
example consider the changes in the intensity of light when passing through 
matter. Example !0(a) shows that when a light ray passes through a sphere 
of radius R "in a random direction" the distance traveled through the 
sphere is distributed uniformly between 0 and 2R. In the presence of uniform 
absorption such a passage would reduce the intensity of the incident ray by a 
factor that is uniformly distributed in an interval 0,a (where a<\ 
depends on the strength of absorption). The scale factor does not seriously . 
affect our model and it is seen that n independent passages would reduce 
the intensity of the light by a factor of the form Zn. > 
To find the distribution of Zn we can proceed in two ways. 
(i) Reduction to exponential distributions. Since sums are generally 
preferable to products we pass to logarithms putting Y* = — logX*. The 
Yk are mutually independent, and for t > 0 
(8.1) 
Now the distribution function (?„ of the sum Sn = Yx + • • • + Ytt of n 
independent exponentially distributed variables • was calculated in (S.5), 
26 THE EXPONENTIAL AND THE UNIFORM DENSITIES 1.9 
and the distribution function of Zn = e~s» is given by 1 — <7n(log r1) 
where 0 < / < 1. The density of this distribution function is '"^(log t~x) 
or 
(8.2) /n@ = "J-(log-T, 0 < r < l. 
n-l\ tj 
Our problem is solved explicitly. This method reveals the advantages 
to be derived from an appropriate transformation, but the success depends 
on the accidental equivalence of our problem with one previously solved. 
(ii) A recursive procedure has the advantage that it lends itself also to 
related problems and generalizations. Let Fn(t) — P{ZM < /} and 
0 < / < 1. By definition Fx(t) = t. Suppose Fn_x known and note that 
Zn = Zn_1Xn is the product of two independent variables. Given Xn = x 
the event {Zn < /} occurs iff Zn_x < t/x and has probabili' Fn-i(t/x). 
Summing over all possible x we obtain for 0 < / < 1 
(8.3) Fn(t) = [Fn_x(tlx) dx = Cf^Ix) dx + t. 
Jo Jt 
This formula permits us in principle to calculate successively F2, F3,.... 
In practice it is preferable to operate with the corresponding densities /„. 
By assumption fx exists. Assume by induction the existence of fn_v 
Recalling that /n_x(s) = 0 for s > 1 we get by differentiation from (8.3) 
(8.4) /n@ =(*/„-!f1)-, 0< r < 1, 
Jt \xj x 
and trite calculations show that fn is indeed given by (8.2). 
9. CONVOLUTIONS AND COVERING THEOREMS 
The results of this section have a mild amusement value in themselves 
and some obvious applications. Furthermore, they turn up rather un- 
expectedly in connection with seemingly unrelated topics, such as significance 
tests in harmonic analysis [example 111,3(/)], Poisson processes [XIV,2(a)], 
and random flights [example 10(e)]. It is therefore not surprising that all 
formulas, as well as variants of them, have been derived repeatedly by 
different methods. The method used in the sequel is distinguished by its 
simplicity and applicability to related problems. 
Let a > 0 be fixed, arid denote by Xl5 X2,. .. mutually independent 
random variables distributed uniformly over 0, a. Let Sn = Xx + • • • + Xn. 
Our first problem consists in finding the distribution Un of Sn and its 
density un = U'n. 
1.9 CONVOLUTIONS AND COVERING THEOREMS 27 
By definition ux{x) *= ar1 for 0 < x < a and ux{x) = 0 elsewhere 
(rectangular density). The higher un are defined by the convolution formula 
B.13) which in the present situation reads 
(9.1) un+1(x) = - \aun{x-y) dy = - [Un(x) - Un(x-a)}. 
a Jo a 
It is easily seen that 
xa~2 0 < x < a 
(9.2) (*)= ~ 
Ba-x)a~2 a-^xK.la, 
and, of course, uz(x) = 0 for all other x. The graph of u2 appears as an 
isosceles triangle with basis 0, 2a, and hence u2 is called triangular density. 
Similarly u3 is concentrated on 0,3a and is defined by three different 
quadratic polynomials in the three thirds of this interval. For a general 
formula we introduce the following 
Notation. We write 
(9-3) 
for the positive part of the real number x. In the following the ambiguous 
symbol x$ stands for (x+)n, namely the function that vanishes for x <, 0 
and equals xn when x ^ 0. Note that (x—a)+ is zero for x < a and a 
linear function when x > a. With this notation the uniform distribution 
may be written in the form 
(9.4) Ux(x) - (*+ - (x-a)+)ar\ 
Theorem 1. Let Sn be the sum of n independent variables distributed 
uniformly over 0, a. Let Un(x) = P{Sn <, x} and denote by un = U'n the 
density of this distribution. Then for n « 1, 2,. . . and x ^0 
(9.5) un(x) = 4-; i(-i)v(nW-™>:; 
(9.6) UnUx) 
an+ n\ 
(These formulas remain true also for x < 0 and for n = 0 provided 
x\ is defined to equal 0 on the negative half-axis, and 1 on the positive.) 
Note that for a point x between (k—l)a and ka only k terms of the 
sum are different from zero. In practical calculations it is convenient to 
disregard the limits of summation and to pretend that v varies from — oo 
28 THE EXPONENTIAL AND THE UNIFORM DENSITIES 1.9 
to oo. This is possible, because with the standard convention the binomial 
coefficients in (9.5) vanish for v < 0 and v > n (see 1; 11,8). 
Proof. For n = 1 the assertion (9.5) reduces to (9.4) and is obviously 
true. We now prove the two assertions simultaneously by induction. 
Assume (9.5) to be true for some n > 1. Substituting into (9.1) we get 
un+i as the difference of two sums. Changing the summation index v in 
the second sum to v — 1 we get 
which is identical with (9.6). Integrating this relation leads to (9.5) with n 
replaced by n + 1, and this completes the proof. »» 
(An alternative proof using a passage to the limit from the discrete model 
is contained in problem 20 of 1; XI,7.) 
Let a = 2b. The variables Xk — b are then distributed uniformly over 
the symmetric interval —b,b, and hence the sum of n such variables has 
the same distribution as Sn — nb. It is given by Un(x+nb). Our theorem 
may therefore be reformulated in the following equivalent form. 
Theorem la. The density of the sum of n independent variables distributed 
uniformly over —b,b is given by 
(9.7) un(x+nb) = * i(-l)v(nW + (n-2v)b)l~\ 
Bb) (n — 1)! v=o \v} 
We turn to a theorem which admits of two equivalent formulations both of 
which are useful in many special problems arising in applications. By 
unexpected good luck the required probability can be expressed simply in 
terms of the density un. We prove this analytically by a method of wide 
applicability. For a proof based on geometric arguments see problem 23. 
Theorem 2. On a circle of length t there are given n^_2 arcs of length a 
whose centers are chosen independently and at random. The probability 
(pn(t) that these n arcs cover the whole circle is 
(9.8) <Pn(t) n{l)l{t) 
which is the same as 
(9.9) 9'n@()( 
Before proving it, we reformulate the theorem in a form to be used later. 
Choose one of the n centers as origin and open the circle into an interval of 
1.10 RANDOM DIRECTIONS 29 
length /. The remaining n — 1 centers are randomly distributed in 67/ 
and theorem 2 obviously expresses the same thing as 
Theorem 3. Let the interval 0, / be partitioned into n subintervals by 
choosing independently at random n — 1 points X1?.. . , Xn_x of division. 
The probability <pn{t) that none of these subintervals is of length exceeding a 
equals (9.9). 
Note that 9?n(/), considered for fixed / as a function of a, represents the 
distribution function of the maximal length among the n intervals into which 
0, / is partitioned. For related questions see problems 22-27. 
Proof. It suffices to prove theorem 3. We prove the recursion formula 
(9.10) 
<pn(t) = (n-1) ?„_&- 
Jo 
Its truth follows directly from the definition of q>n as an (n—l)-tuple 
integral, but it is preferable to read (9.10) probabilistically as follows. The 
smallest among X1?... , Xn_x must be less than a, and there are n — 1 
choices for it. Given that X, = x, the probability that X,- is leftmost 
equals [(/— x)/t]n-2. The remaining variables are distributed uniformly over 
x, / and the conditional probability that they satisfy the conditions of the 
theorem is <pn-i{t—x). Summing over all possibilities we get (9.10).11 
Let us for the moment define un by (9.8). Then (9.10) reduces to 
(9.11) un(t) 
which is exactly the recursion formula (9.1) which served to define un. It 
suffices therefore to prove the theorem for n = 2. But it is obvious that 
<p2(t) =1 for 0 < / < a and <p2(t) — Ba—t)/t for a < t < 2a, in 
agreement with (9.8). ». 
10. RANDOM DIRECTIONS 
Choosing a random direction in the plane 3t2 is the same as choosing 
at random a point on the circle. If one wishes to specify the direction by 
its angle with the right a>axis, the circle should be referred to its arc length 
6 with 0 <, 6 < 2tt. For random directions in the space ft3 the unit 
sphere serves as sample space; each domain has a probability equal to its 
area divided tjy 4n. Choosing a random direction in %3 is equivalent to 
u Readers who feel uneasy about the use of conditional probabilities in connection with 
densities should replace the hypothesis Xt =» x by the hypothesis x — h < Xf < x, which 
has positive probability, and pass to the limit as h -* 0. 
30 THE EXPONENTIAL AND THE UNIFORM DENSITIES 1.10 
choosing at random a point on this unit sphere. As this involves a pair of 
random variables (the longitude and latitude), consistency would require 
postponing the discussion to chapter III, but it appears more naturally in 
the present context. 
Propositions, (i) Denote by L the length of the projection of a unit vector 
with random direction in 3t3 on a fixed line, say the x-axis. Then L is 
uniformly distributed over 0,1, and E(L) = ?. 
(ii) Let U be the length of the projection of the same vector on a fixed 
plane, say the x,y-plane. Then U has density //Vl — t2 for 0 < / < 1, 
and E(U) = \-n. 
The important point is that the two projections have different distributions. 
That the first is uniform is not an attribute of randomness, but depends on 
the number of dimensions. 'Hie counterpart to (i) in ft2 is contained in 
Proposition, (iii) Let L be the length of the projection of a random unit 
vector in ft2 on the x-axis. Then L has density 2/(tt\/1—x2), and 
E(L) = 2/tt. 
Proofs, (iii) If 0 is the angle between our random direction and the 
y-axis, then L = |sin 0| and hence for 0 < a; < 1 we get by symmetry 
2 
A0.1) P{L < x) = P{0 < 0 < arc sin x] = - arc sin x. 
77 
The assertion now follows by differentiation. 
(i), (ii). Recall the elementary theorem that the area of a spherical zone 
between two parallel planes is proportional to the height of the zone. For 
0 < / < 1 the event {L <, t) is represented by the zone \xx\ <[ / of 
height It, whereas {U <,../}¦ corresponds to the zones \x3\ ;> y/l—t2 of 
total height 2 — 2vT—T2. This determines the two distribution functions 
up to numerical factors, and these follow easily from the condition that both 
distributions equal 1 at / = 1 > 
Examples, (a) Passage through spheres. Let S be a sphere of radius r 
and N' a point on it. A line drawn through N in a random direction 
intersects S in P. Then: The length of the segment NP is a random variable 
distributed uniformly between 0 and 2r. 
To see this consider the axis NS of the sphere and the triangle NPS 
which has a right angle at P and an angle 0 at N. The length of NP is 
then 2r cos 0. But cos 0 is also the projection of a unit vector in the 
1.10 RANDOM DIRECTIONS 31 
line NP into the diameter NS, and therefore cos 0 is uniformly distributed 
in 671 
In physics this model is used to describe the passage of light through 
"randomly distributed spheres." The resulting absorption of light was 
used as one example for the random-splitting process in the last section. 
(See problem 28.) 
(b) Circular objects under the microscope. Through a microscope one 
observes the projection of a cell on the xx, a;2-plane rather than its actual 
shape. In certain biological experiments the cells are lens-shaped and 
may be treated as circular disks. Only the horizontal diameter of the disk 
projects in its natural length, and the whole disk projects into an ellipse 
whose minor axis is the projection of the steepest radius. Now it is generally 
assumed that the orientation of the disk is random, meaning that the direction 
of its normal is chosen at random. In this case the projection of the unit 
normal on the ar3-axis is distributed uniformly in 0, 1. But the angle between 
this normal and the ar3-axis equals the angle between the steepest radius and 
the xlt a:2-plane and hence the ratio of the minor to the major axis is dis- 
tributed uniformly in 0,1. Occasionally the evaluation of experiments was 
based on the erroneous belief that the angle between the steepest radius 
and the xlt a;2-plane should be distributed uniformly. 
(c) Why are two violins twice as loud as one? (The question is serious 
because the loudness is proportional to the square of the amplitude of the 
vibration.) The incoming waves may be represented by random unit vectors, 
and the superposition effect of two violins corresponds to the addition 
of two independent random vectors. By the law of the cosines the square 
of the length of the resulting vector is 2 -f- 2 cos 0. Here 0 is the angle 
between the two random vectors, and hence cos 0 is uniformly distributed 
in —1,1 and has zero expectation. The expectation of the square of the 
resultant length is therefore indeed 2. 
In the plane cos 0 is not uniformly distributed, but for reasons of symmetry 
its expectation is still zero. Our result therefore holds in any number of 
dimensions. See also example V,4(e). ». 
By a random vector in 3t3 is meant a vector drawn in a random direction 
with a length L which is a random variable independent of its direction. 
The probabilistic properties of a random vector are completely determined 
by those of its projection on the ar-axis, and using the latter it is frequently 
possible to avoid analysis in three dimensions. For this purpose it is impor- 
tant to know the relationship between the distribution function V of the 
true length L and the distribution F of the length Lx of the projection on 
the z-axis. Now Lx = XL, where A. is the length of the projection of a 
unit vector in the given direction. Accordingly, X is distributed uniformly 
32 THE EXPONENTIAL AND THE UNIFORM DENSITIES 1.10 
over 0, 1 and is independent of L. Given X = #, the event {Lx <, t) 
occurs iff L < t/x, and so12 
A0.2) Fit) = \lV(t/x) dx t > 0. 
For the corresponding densities we get by differentiation 
and differentiation leads to 
A0.4) v{t) - -//'(/), t > 0. 
We have thus found the analytic relationship between the density v of the 
length of a random vector in fP3. and the density f of the length of its pro- 
jection on a fixed direction. The relation A0.3) is used to find / when v 
is known, and A0.4) in the opposite direction. (The asymmetry between 
the two formulas is due to the fact that the direction is not independent of 
the length of the projection.) 
Examples, (d) Maxwell distribution for velocities. Consider random 
vectors in space whose projections on the #-axis have the normal density 
with zero expectation and unit variance. Since length is taken positive we 
have 
A0.5) /(/) = 2n(/) = N/2/^e-i't, / > 0. 
From A0.4) then 
A0.6) t>(/) = V2MV-*'1, />0. 
This is the Maxwell density for velocities in statistical mecnanics. The 
usual derivation combines the preceding argument with a proof that / must 
be of the form A0.5). (For an alternative derivation see 111,4.) 
(e) Lord Rayleigh's random flights in St3. Consider n unit vectors whose 
directions are chosen independently and at random. We seek the distribution 
of the length Ln of their resultant (or vector sum). Instead of studying this 
resultant directly we consider its projection on the x-axis. This projection is 
obviously the sum of n independent random variables distributed uniformly 
over —1,1. The density of this sum is given by (9.7) with b = 1. Sub- 
stituting into A0.4) one sees that the density of the length Ln is given by1* 
12 This argument repeats the proof of (8.3). 
18 The standard reference is to a paper by S. Chandrasekhar (reprinted in Wax A954)] 
who calculated t>3, vv.vt and the Fourier transformof ^Because he used polar coordi- 
nates, his Wn{x) must be multiplied by A*nx* to obtain our »*. 
I.I 1 THE USE OF LEBESGUE MEASURE 33 
This problem occurs in physics and chemistry (the vectors representing, 
for example, plane waves or molecular links). The reduction to one dimension 
seems to render this famous problem trivial.. 
The same method applies to random vectors with arbitrary length and 
thus A.0.4) enables us to reduce random-walk problems in 3t3 to simpler 
problems in ft1. Ijven when explicit solutions are hard to get, the central 
limit theorem provides valuable information [see example VIII,4F)]. *¦ 
Random vectors in Jl2 are defined in like manner. The distribution V of the true length 
and the distribution F of the projection are related by the obvious analogue to A0.2), 
namely 
A0.8) 
However, the inversion formula A0.4) has no simple analogue, and to express V in terms 
of F we must depend on the relatively deep theory of Abel's integral equation.u We state 
without proof that if F has a continuous density /, then. 
JV2 / x \ dd 
/(-r-al-r-Sfl. 
„ J \sin0/sinz0 
o \ / 
(See problems 29-30.) 
Example, if) Binary orbits. In observing a spectroscopic binary orbit astronomers 
can measure only the projections of vectors onto a plane perpendicular to the line of sight. 
An ellipse in space projects into an ellipse in this plane. The major axis of the true ellipse 
lies in the plane determined by the line of sight and its projection, and it is therefore 
reasonable to assume that the angle between the major axis and its projection is uniformly 
distributed. Measurements determine (in principle) the distribution of the projection. The 
distribution of the true major axis is then given by the solution A0.9) of Abel's integral 
equation. ^- 
11. THE USE OF LEBESGUE MEASURE 
If a set A in 0, 1 is the union of finitely many non-oveflapping intervals 
Ilt 72, .. . of lengths Xx, X^, ... , the uniform distribution attributes to it 
probability 
A1.1) P{A} = Xl + Xt+ -". 
The following examples will show that some simple, but significant, problems 
14 The transformation to Abel's integral equation is by means of the change of variables 
> viW = K(^V and xs^Q = y- 
then A0.8) takes on the form 
dy' 
= 
Jo 
7=7 T 
o Vy(t -y) 
34 THE EXPONENTIAL AND THE UNIFORM DENSITIES I.I 1 
lead to unions of infinitely many non-overlapping intervals. The definition 
A1.1) is still applicable and identifies P{A} with the Lebesgue measure of 
A. It is consistent with our program to identify probabilities with the integral 
of the density f(x) = 1, except that we use the Lebesgue integral rather than 
the Riemann integral (which need not exist). Of the Lebesgue theory we 
require only the fact that if A is the union of possibly overlapping intervals 
/l5 /2,. . . the measure P{A} exists and does not exceed the sum Xx + A2 + • • • 
of the lengths. For non-overlapping intervals the equality A1.1) holds. The 
use of Lebesgue measure conforms to uninhibited intuition and simplifies 
matters inasmuch as many formal passages to the limit are justified. A set N 
is called a null set if it is contained in sets of arbitrarily small measure, that is, 
to each e there exists a set A => N such that P{A] < e. In this case 
P{A^} = 0. 
In the following X stands for a random variable distributed uniformly 
in 07T- 
Examples, (a) What is the probability of X being rational! The sequence 
h> h h h I» i> • • • contains all the rationals in 0, 1 (ordered according to 
increasing denominators). Choose e < \ and denote by Jk an interval of 
length 6fc+1 centered at the ?th point of the sequence. The sum of the 
lengths of the Jk is e2 + e3 + • • • < e, and their union covers the rationals. 
Therefore by our definition the set of all rationals has probability zero, and 
so.. X is irrational with probability one. 
It is pertinent to ask why such sets should be considered in probability 
theory. One answer is that nothing can be gained by excluding them and that 
the use of Lebesgue theory actually simplifies matters without requiring new 
techniques. A second answer may be more convincing to beginners and 
non-mathematicians; the following variants lead to problems of un- 
doubted probabilistic nature. 
(b) With what probability does the digit 7 occur in the decimal expansion 
of XT In the decimal expansion of each x in the open interval between 0.7 
and 0.8 the digit 7 appears at the first place. For each n there are 9n-1 
intervals of length 10~n containing only numbers such that the digit 7 appears 
at the nth place but not before. (For n — 2 their endpoints are 0.07 and 
0.08, next 0.17 and 0.18, etc.) These intervals are non-overlapping, and 
their total length is ^A + tV + (tVJ + .*:•*) = 1- Thus our event has 
probability 1. 
Notice that certain numbers have two expansions, for example 0.7 = 
= 0.6999 .... To make our question unequivocal we should therefore 
specify whether the digit 7 must or may occur in the expansion, but our 
argument is independent of the difference.. The reason is that only rationals 
can have two expansions, and the set of all rationals has probability zero. 
1.11 THE USE OF LEBESGUE MEASURE 35 
(c) Coin tossing and random choice. Let us now see how a "random 
choice of a point X between 0 and 1" can be described in terms of discrete 
random variables. Denote by Xk(x) the kth decimal of x. (To avoid 
ambiguities let us use terminating expansions when possible.) The random 
variable X* assumes the values 0, 1, ,9, each with probability TV, and 
the Xk are mutually independent. By the definition of a decimal expansion, 
we have the identity 
00 
A1.2) X 
This formula reduces the random choice of a point X to successive choices 
of its decimals. 
For further discussion we switch from decimal to dyadic expansions, that 
is, we replace the basis 10 by 2. Instead of A1.2) we have now 
00 
A1.3) X 
where the X* are mutually independent random variables assuming the 
values 0 and 1 with probability \. These variables are defined on the interval 
0, 1 on which probability is equated with Lebesgue measure (length). This 
formulation brings to mind the coin-tossing game of volume 1, in which the 
sample space consists of infinite sequences of heads and tails, or zeros and 
ones. A new interpretation of A1.3) is now possible in this sample space. 
In it, the X* are coordinate variables, and X is a random variable defined by 
them; its distribution function is, of course, uniform. Note that the second 
formulation contains two distinct sample points 0111111 and 1000000 even 
though the corresponding dyadic expansions represent the same point \. 
Nevertheless, the notion of zero probability enables us to identify the two 
sample spaces. Stated in more intuitive terms, neglecting an event of prob- 
ability zero $he random choice of a point X bet-ween 0 and 1 can be effected 
by a sequence of coin tossings; conversely, the result of an infinite coin- 
tossing game may be represented by a point x of 0, 1. Every random variable 
of the coin-tossing game may be represented by a function on 0,1, etc. 
This convenient and intuitive device has been used since the beginning of 
probability theory, but it depends on neglecting events of zero probability. 
(d) Cantor-type distributions. A distribution with unexpected properties 
is found by considering in A1.3) the contribution of the even-numbered 
terms or, what amounts to the same, by considering the random variable 
00' 
A1.4) 
v-l 
(The factor 3 is introduced to simplify the discussion. The contribution 
36 THE EXPONENTIAL AND THE UNIFORM DENSITIES 1.12 
of the odd-numbered terms has the same distribution as f Y.) The distri- 
bution function F(x) = P{Y < x} will serve as example for so-called 
singular distributions. 
In the calci lation we refer to Y as the gain of a gambler who receives 
the amount 3 • 4~k if the ?th toss of a fair coin results in tails. This gain 
lies between 0 and 3D-1+4~2H ) = 1. If the first trial results in 1 the 
gain is >|, while in the contrary case Y < 3D~2-f 4~3-f • • •) = 4. 
Thus the inequality J < Y < f cannot be realized under any circumstances, 
and so F{z) — \ in this interval of length \. It follows that F can have no 
jump exceeding \. 
Next notice that up to a factor \ the contribution of the trials number 
2, 3,... constitute a replica of the whole sequence, and so the graph of F 
in the interval 0, \ differs from the whole graph only by a similarity 
transformation 
A1.5) F{x) = \F{4x), 0<x<l 
It follows that F(x) = { throughout an interval of length | centered at 
x = ¦§-. For reasons of symmetry, F{x) = ? throughout an interval of 
length \ centered at x = -}. We have now found three intervals of total 
length i + f = | in each of which F assumes a constant value, namely 
I, ?, or f. Consequently, F can have no jump exceeding \. There remain 
four intervals of length Yt each, and in each of them the graph of F differs 
from the whole graph only by a similarity transformation. Each of the 
four intervals therefore contains a subinterval of half its length in which 
F assumes a constant value (namely i, •§, -f, ?, respectively). Continuing 
in like manner we find in n steps 1 -f 2 + 22 -f • • • + 2n-1 intervals of 
total length 2 + 2~2 + 2~3 + \- 2~n = 1 - 2~n in each of which F 
assumes a constant value. 
Thus F is a continuous function increasing from F@) = 0 to F(l) = 1 
in such a way that the intervals of constancy add up to length 1. Roughly 
speaking, the whole increase of F takes place on a set of measure 0. We 
have here a continuous distribution function F without density /. > 
12. EMPIRICAL DISTRIBUTIONS 
The "empirical distribution function" Fn of n points alt . . . , an on the 
line is the step function with jumps Mn at alt. . . , an. In other words, 
1 
n Fn(x) equals the number of points ak in —co,x, and Fn is a distribution 
function. Given n random variables X1} . . . , Xn, their values at a particu- 
lar point of the sample space form an «-tuple of numbers and its empirical 
distribution function is called the empirical sample distribution. For each 
1.12 EMPIRICAL DISTRIBUTIONS 37 
x, the value Fn(x) of the empirical sample distribution defines a new 
random variable, and the empirical distribution of (X1}. . . , Xn) represents 
a whole family of random variables depending on the parameter x. (In 
technical language we are concerned with a stochastic process with x as 
time parameter.) No attempt will be made here to develop the theory of 
empirical distributions, but the notion may be used to illustrate the occurrence 
of complicated random variables in simple applications. Furthermore, the 
uniform distribution will appear in a new light. 
Let Xj, .. . , Xn stand for mutually independent random variables with 
a common continuous distribution F. The probability that any two variables 
assume the same value is zero, and we can therefore restrict our attention 
to samples of n distinct values. For fixed x the number of variables Xk 
such that Xk <, x has a binomial distribution with probability of "success" 
p = F(x), and so the random variable Fn(x) has a binomial distribution with 
possible values 0, l/n,... , 1. For large n and x fixed, Vn(x) is therefore 
likely to be close to F(x) and the central limit theorem tells us more about 
the probable deviations. More interesting is the (chance-dependent) graph 
of Fn as a whole and how close it is to F. A measure for this closeness is 
the maximum discrepancy, that is, 
A2.1) Dn = sup \Fn(x) - F(x)\. 
— 00<X<00 
This is a new random variable of great interest to statisticians because of 
the following property. The probability distribution of the random variable 
Dn is independent of F (provided, of course, that F is continuous). 
For the proof it suffices to verify that the distribution of Dn remains 
unchanged when F is replaced by a uniform distribution. We begin by 
showing that the variables Yk = F(Xk) are distributed uniformly in 0,1. 
For that purpose we restrict t to the interval 0, 1, and in this interval we 
define v as the inverse function of F. The event {F{Xk) <, t) is then identical 
with the event {X* <, v(t)} which has probability F(v(t)) = t. Thus 
P{Yjfc <, t} = t as asserted. 
The variables Y1}... , Yn are mutually independent, and we denote 
their empirical distribution by Gn. The argument just used shows also that 
for fixed t the random variable Gn@ is identical with Fn(v(t)). Since 
/ = F(v(t)) this implies that at every point of the sample space &R 
sup |Gn@ - /| = sup |Fn(y@) - F(v(t))\ = Dn. 
This proves the proposition. 
The fact that the distribution of Dn is independent of the underlying dis- 
tribution F enables statisticians to devise tests and estimation procedures 
38 THE EXPONENTIAL AND THE UNIFORM DENSITIES 1.12 
applicable in situations when the underlying distribution is unknown. In 
this connection other variables related to Dn are of even greater practical use. 
Let Xi, .. . , Xn, Xf, . . . , Xf be 2n mutually independent random 
variables with the common continuous distribution F, and denote the 
empirical distributions of (Xlf-. . . , Xn) and (Xf, . . . , Xf) by Fn and 
Ff, respectively. Put 
A2.2) Dnn = sup |Fn(*) - 
This is the maximum discrepancy between the two empirical distributions. 
It shares with Dn the property that it does not depend on the distribution 
F. For this reason it serves in statistical tests of "the hypothesis that 
(X1}... , Xn) and (Xf,. . . , Xf) are random samples from the same 
population." 
The distribution of Dn n was the object of cumbersome calculations 
and investigations but in 1951 B. V. Gnedenko and V. S. Koroljuk showed 
that the whole question reduces to a random-walk problem with a well-known 
solution. Their argument is pleasing by its elegance and we use it as 
illustration of the power of simple combinatorial methods. 
Theorem. P{Dn>n < r//?} equals the probability in a symmetric random 
walk that a path of length 2n starting and terminating at the origin does not 
reach the points ±r. 
Proof. It suffices to consider integral r. Order the In variables X1}... ., 
Xf in order of increasing magnitude and put ek = 1 or ck = — 1, according 
to whether the kth place is occupied by an X, or an Xf. The resulting 
I orderings 
are equally likely. The resulting 2«-tuples (elt.. . , e2n) are therefore in a 
one-to-one correspondence with the paths of length 2« starting and termin- 
ating at the origin. Now if et + • • • + ^ = k the first j places contain 
(j+k)/2 unsuperscripted and (j—k)/2 superscripted variables, and so 
there exists a point x such that Fn(x) = (j.+k)j2n and Ff (x) = (j—k)/2n. 
But then |Fn!>;) — Ff (x)\ = \k\jn and hence Dnn > \k\ln. The same 
argument in reverse completes the proof. > 
An explicit expression for the probability in question is contained in 1, XIV,(9.1). In fact 
P{Dn>n < r/«} - wrn 
is the probability that a particle starting at the origin returns at epoch 2« to the origin 
without touching ±r. The last condition can be realized by putting absorbing barriers at 
±r, and so wrn is the probability of a return to the origin at epoch 2« when ±r are 
absorbing barriers. [In 1; XIV,(9.1) the interval,is 0, a rather than —r,r. Our wr>n is 
identical with ur>2n(r).] 
1.13 PROBLEMS FOR SOLUTION 39 
It was shown in 1; XIV that a limiting procedure leads from random walks to diffusion 
processes, and in this way it is not difficult to see that the distribution of V«Dnn tends to 
a limit. Actually this limit was discovered by N. V. Smirnov as early as 1939 and the 
similar limit for V nDn by A. Kolmogorov in 1933. Their calculations are very intricate 
and do not explain the connection with diffusion processes, which is inherent in the 
Gnedenko-Koroljuk approach. On the other hand, they have given impetus to fruitful 
work on the convergence of stochastic processes (P. Billingsley, M. F. Donsker, Yu. V. 
Prohorov, A. V. Skorohod, and others). 
It may be mentioned that the Smirnov theorems apply equally to discrepancies Dmn of 
the empirical distributions of samples of different sizes m and n. The random-walk 
approach carries over, but loses much of its elegance and simplicity (B. V. Gnedenko, 
E. L. Rvateva). A great many variants of Dm§n have been investigated by statisticians. 
(See problem 36.) 
13. PROBLEMS FOR SOLUTION 
In all problems it is understood that the given variables are mutually independent. 
1. Let X and Y have densities ae~ax concentrated on 0, oo. Find the densities 
of 
(i) X3 . (ii) 3 + 2X . 
(Hi) X - Y (iv) IX - Y| 
(v) The smaller of X and Y3 (vi) The larger of X and Y3. 
2. Do the same problem if the densities of X and Y equal ? in —1,1 and 0 
elsewhere. 
3. Find the densities for X + Y and X - Y if X has density xe-**(x > 0) and 
the density of Y equals hr1 for 0 < x < h. 
4. Find the probability that A2 — 2ak + b has complex roots if the coefficients 
a and b are random variables whose common density is 
(i) uniform, that is, hr1 for 0 < x < h 
(ii) exponential, that is, <. a*ra* for x > 0. 
5. Find the distribution functions of X+Y/X and X+Y/Z if the variables X, 
Y, and Z have a common exponential distribution. 
6. Derive the convolution formula C.6) for the exponential distribution by a 
direct passage to the limit from the convolution formula for the "negative 
binomial" distribution of 1; VI,(8.1). 
7. In the Poisson process of section 4, denote by Z the time between epoch t 
and the last preceding arrival or 0 (the."age" of the current interarrival time). Find 
the distribution of Z and show that it tends to the exponential distribution as 
t^ co. 
8. In example 5(a) show that the probability of the first record value occurring 
at the nth place and being <>x equals 
!\ —e-ax)n+l 
Conclude that the probability distribution of the first record value is 1 — A + ax)e-ax. 
[More generally, if the X, are positive and subject to an arbitrary continuous 
distribution F, the first probability equals [n(n +tj^F^+^x) and the distribution 
of the first record value is F - A -F) log A -FT1-] 
40 THE EXPONENTIAL AND THE UNIFORM DENSITIES 1.13 
9. Downward runs.. The random variable N is defined as the unique index 
such that Xj ;> X2 ;>••;> XN_i < XN. If the X, have a common continuous 
distribution F prove that P{N = n) = (n-l)jnl and E(N) ¦= e. 
Hint: Use the method ofexample 5(a) concerning record values. 
10. Platoon.formation in traffic.15 Cars start successively at the Origin and travel 
at different but constant speeds along an infinite road on which no passing is possible. 
When a car reaches a slower car it is compelled to trail it at the same speed. In this 
way platoons will be formed whose ultimate size depends on the speeds of the cars 
but not on the times between successive departures. 
Consider the speeds of the cars as independent random variables with a common 
continuous distribution. Choose a car at random, say the next car to depart. 
Using the combinatorial method of example 5(a) show that: 
(a) The probability that the given car does not trail any other car tends to ?. 
(b) The probability that it leads a platoon of total size n (with exactly n — \ 
cars trailing it) tends1 to l/(/z + l)(/z+2). 
(c) The probability that the given car is the last in a platoon of size n tends to 
the same limit.' 
11. Generalization1* of the record value example 5(a). Instead of taking the single 
preliminary observation Xo we start from a sample (Xlf. .., X^) with order statistics 
(XA),.. ., X(m)). (The common distribution F plays no role as long as it is 
continuous.) 
(a) If N is the first index n such that X^,^ ? X(m) show that P{N > n) = 
=m/(m+n). [In example 5(a) we had m = 1.] 
(b) If N is the first index n such that X^,^ > X(m_r+1) show that 
For ^2 we have E(N) < oo and 
(c) If N is the first index such that X^^ falls outside the interval between 
XA) and X(m) then 
P{N > n} = , /*m~1} n , and E(N) < oo. 
(m+n)(m+nl) 
12. (Convolutions of exponential distributions). For j = 0,..., n let X^ have 
density A^-^* ^for x > 0 where A3 ?& Ak unless j = k. Put 
Show that Xo + : • • + Xn has a density given by 
Hint: Use induction, a symmetry argument, and B.14). No calculations are 
necessary. 
15 G. F. Newell, Operations Research, vol. 7 A959), pp. 589-598. 
16 S. S. Wilks, J. Australian Math. Soc., vol. 1 A959) pp. 106-112. 
1.13 PROBLEMS FOR SOLUTION 41 
13. (Continuation). If Y^ has the density je~ix, the density of the sum 
Yi + • • ¦ + Yn is given by 
Jfc-1 
Using the proposition of example 6(b) conclude that fn_1 is the density of the 
spread X(n) — XA) of a sample • Xlf..., X,, if the X, have the common 
density e~x. 
14. Pure birth processes. In the pure birth process of 1; XVII,3 the system passes 
through a sequence of states EQ~* Ex-*¦•••, staying at Ek for a sojourn time X* 
with density Ajfc?-<i**. Thus Sn = Xo + • • • + X« is the epoch of the transition 
En -> En+1. Denote by Pn(t) the probability of En at epoch t. Show that 
Pn(O = P{Sn > /} - P{S?_1 > /} and hence that Pn is given by formula (*) of 
problem 12. The differential equations of the process, namely 
should be derived (a) from A), and (b) from the properties of the sums Sn. 
Hint: Using inductively a symmetry argument it suffices to consider the 
factor of e~xot. 
15. In example 6{a) for parallel waiting lines we say that the system is in state 
k if k counters are free. Show that the birth process model of the last example 
applies with Xk = (n —k)a. Conclude that 
From this derive the distribution of X(k). 
16. Consider two independent queues of m and n > m persons respectively, 
assuming the same exponential distribution for the service times. Show that the 
probability of the longer queue finishing first equals the probability of obtaining n 
heads before m tails in a fair coin-tossing game. Find the same probability also 
by considering the ratio X/Y of two variables with gamma distributions Gm and 
Gn given in C.5). 
17. Example of statistical estimation. It is assumed that the lifetimes of electric 
bulbs have an exponential distribution with an unknown expectation a. To 
estimate a a sample of n bulbs is taken and one observes the lifetimes 
xA) < xB) < • • • < x(r) 
of the first r bulbs to fail. The "best unbiased estimator" of a is a linear 
combination U = ^XA) + • • • + ^{r) such that ?(U) = a and Var (U) 
is the smallest possible. Show that 
U =(XA) + ---+X(r))i +X(r)(/i-r)i, and then Var(U)=ia~2. 
Hint: Do the calculations in terms of the independent variables X(A.) — X(k_1) 
(see example 6(b)). 
18. If the variables Xlt.. ., X,, are distributed uniformly in OTT show that the 
spread X(n) — XA) has density n(n — l)zn~2(l —x) and expectation (/z—1 )/(/* +1). 
What is the probability that all n points lie within an interval of length tl 
42 THE EXPONENTIAL AND THE UNIFORM DENSITIES 1.13 
19. Answer the questions of example 6(b) when the three service times are 
distributed uniformly in 0,1. (Note: The problem involves tedious calculations, 
but may provide a useful exercise in technical manipulations.} 
20. Four points are chosen independently and at random on a circle. Find the 
probability that the chords XXX2 and X3X4 intersect: (a) without calculation 
using a symmetry argument; (b) from the definition by an integral. 
21. In the random-splitting process of section 8 denote by Xll5 X12, X21, X22 
the masses of the four fragments of the second generation, the subscript 1 referring 
to the smaller and 2 to the larger part. Find the densities and expectations of these 
variables. 
Note. The next few problems contain new theorems concerning random par- 
titions of an interval [see example 7(b)]. The variables Xx Xn are supposed 
independent and uniformly distributed over 0, /. They induce a partition of this 
interval into n + 1 subintervals whose lengths, taken in proper order, will be 
denoted by Llt..., Ln+1. [In the notation of order statistics we have 
22. Denote by pn(t) the probability that all n + 1 intervals are longer than h. 
[In other words, pn(t) = P{min Lk > h}, which is the tail of the distribution 
function of the shortest among the intervals.] Prove the recurrence relation 
_ n p-* n-1 
'nJo 
Conclude that pn(t) = r~"(r - ( 
23. From a recurrence relation analogous to (*) prove without calculations that 
for arbitrary xx > 0 xn+1 ;> 0 
(**) P{LX > xlt. . ., Ln+1 > xn+1} = t~*(t - xx - 
[This elegant result was derived by B. de Finetti17 from geometrical considerations. 
It contains many interesting special cases. When x$ = h for all j we get the preced- 
ing problem. Example l(b) corresponds to the special case where exactly one among 
the Xj is different from zero. The covering theorem 3 of section 9 follows from 
(**) and the formula 1; IV,A.5) for the realization of at least one among n + 1 
events.] 
24. Denote by qn(t) the probability that all mutual distances of the Xk exceed 
h. (This differs from problem 22 in that no restrictions are imposed on the end 
intervals Lx and Ln+1.) Find a relation analogous to (*) and hence derive qn{t). 
25. Continuation. Without using the solution of the preceding problems show a 
priori that pn(t) = (t -2A)n/-n^n(r-2fi). 
26. Formulate the analogue to problem 24 for a circle and show that problem 
23 furnishes its solution. 
27. An isosceles triangle is formed by a unit vector in the a:-direction and another 
in a random direction. Find the distribution of the. length of the third side (i) in 
51* and (ii) in Jls. 
17 Giornale Istituto Italiano degli Attuari, vol. 27 A964) pp. 151-173, in Italian. 
1.13 PROBLEMS FOR SOLUTION 43 
28. A unit circle (sphere) about 0 has the north pole on the positive x-axis. A 
ray enters at the north pole and its angle with the z-axis is distributed uniformly 
over —\-n, \tt. Find the distribution of the length of the chord within the circle 
(sphere). 
Note. In &2 the ray has a random direction and we are concerned with the 
analogue to example 10(a). In &3 the problem is new. 
29. The ratio of the expected lengths of a random vector and of its projection 
on the x-axis equals 2 in &3 and tt/2 in &*. Hint: Use A0.2) and A0.8). 
30. The length of a random vector is distributed uniformly over 0, 1. Find the 
density of the length of its projection on the x-axis (a) in &3, and (b) in &2. 
Hint: Use A0.4) and A0.9). 
31. Find the distribution function of the projection on thex-axis of a randomly 
chosen direction in &*. 
32. Find the analogue in &4 to the relation A0.2) between the distributions of 
the lengths of a random vector and that of its projection on the x-axis. Specialize 
to a unit vector to verify the result of problem 31. 
33. A limit theorem for order statistics, (a) Let Xx Xn be distributed 
uniformly in 0, 1. Prove that for k fixed and n -* oo 
where Gk is the gamma distribution C.5) [see example 7(e)]. 
(b) If the Xjt have an arbitrary continuous distribution function F, the same 
limit exists for P{X(A) <, <E>(x//i)} where <E> is the inverse function of F. (Smirnov.) 
34. A limit theorem for the sample median. The «th-order statistic X(n) of 
(Xx X2n_i) is called the sample median. If the X,- are independent and 
uniformly distributed over 0, 1 show that 
where 9? stands for the standard normal distribution. 
35. Continuation. Let the Xj have a common distribution F with a con- 
tinuous density /. Let m be the theoretical median, that is, let F(m) = \. Show 
that 
P{X(n) < 
whence, using the preceding problem, 
Pjx(n) - m - 
36. Prove the following variant of the Gnedenko-Koroljuk theorem in section 12: 
P 
where r = 1, 2,. .., n. (In contrast to the original formulation the absolute 
values on the left are omitted and so only one absorbing barrier at r occurs in the 
associated random walk.) 
44 THE EXPONENTIAL AND THE UNIFORM DENSITIES 1.13 
37. Generation of exponentially distributed variables from uniform ones.18 Let 
X1( X2,.. . be independent and uniformly distributed in 0,1. Define the random 
variable N as the index such that Xx > X2 ^ • • •• > XN_! < XN (see problem 9). 
Prove that 
P{XX < x, N = n} = 
xn-\ 
(n-l)l n\ ' 
whence P{XX < x, N even} = 1 - e~x. 
Define Y as follows: A "trial" is a sequence Xx XN; it is a "failure1' 
if N is odd. We repeat independent trials as long as necessary to produce a 
"success." Let Y equal the number of failures plus the first variable in the 
successful trial. Prove that P{Y < x) = 1 — e~x. 
18 J. von Neumann, National Bureau of Standards, Appl. Math. Series, No. 12 A951) 
pp. 36-38. 
CHAPTER II 
Special Densities. 
Randomiza ti on 
The main purpose of this chapter is to list for reference the densities 
that will occur most frequently in the following chapters. The randomization 
procedure described in the second part is of general use. Its scope is 
illustrated by deriving certain distributions connected with Bessel functions 
which occur in various applications. It turns out that this simple probabilistic 
app-oach replaces involved calculations and hard analysis. 
1. NOTATIONS AND CONVENTIONS 
We say that a density / and its distribution F are concentrated1 on an 
interval I = a~b if fix) = 0 for all x outside /. Then F(x) = 0 for 
x < a and F{x) =1 for x > b. Two distributions F and G, and also 
their densities / and g, are said to be of the same type if they stand in the 
relationship 
A.1) G(x) = F(ax+b), g(x) = qf(ax+b), 
where a > 0. We shall frequently refer to b as a centering parameter, to 
a as a scale parameter. These terms are readily understood from the fact 
that when F serves as distribution function of a random variable X then 
G is the distribution function of 
A.2) Y = ^-=-$. 
a 
In many contexts only the type of a distribution really matters. 
1 According to common usage the closed interval / should be called the support of /. 
A new term is introduced because it will be used in the more general sense that a distribution 
may be concentrated on the set of integers or rationals. 
45 
46 SPECIAL DENSITIES. RANDOMIZATION II. 1 
The expectation m and variance a2 of / (or of F) are defined by 
A-3) 
m 
J"+OO /*+0O /*+O0 
xf(x) dx, a2 = (x~mJf(x) dx = x2f(x) dx - m\ 
— oo J—oo J—oo 
provided the integrals converge absolutely. It is clear from A.2) that in 
this case g has expectation (m — b)/a and variance a2\a2. It follows that 
for each type there exists at most one density with zero expectation and unit 
variance. 
We recall from 1,B.12) that the convolution / = /i */2 of two densities 
/x and f2 is the probability density defined by 
d-4) /(*)= hi*-y)h{y)dy. 
When fi and /2 are concentrated on 0, oo this formula reduces to 
A-5) f{x) = | Xh{x-y)f,{y) dy, x > 0. 
The former represents the density of the sum of two independent random var- 
iables with densities fx and f2. Note that for g{(x) =f(x+bi) the con- 
volution g = g1 * g2 is given by g(x) =f(x+b1-\-b2) as is obvious from A.2). 
Finally we recall the standard normal distribution function and its density 
defined by 
A.6) n(*) = -^ e~ix\ 3l(*) = -j= f e~iv* dy. 
/2 J2 J 
Our old acquaintance, the normal density with expectation m and variance 
<r2, is given by 
1 ix—m\ 
-n a > 0. 
a \ a ) 
Implicit in the central limit theorem is the basic fact that the family of 
normal densities is closed under convolutions; in other words, the convolution 
of two normal densities with expectations mx, m2 and variances a2, a\ is 
the normal density with expectation m1 + m2 and variance a2 = a2 + <?l- 
In view of what has been said it suffices to prove it for mx = m2 = 0. It is 
asserted that 
A,7) 
\I2tt a 
and the truth of this assertion becomes obvious by the change of variables 
z = y{ajaxa2) — x(a2faa1) where x is fixed. (See problem 1.) 
II.2 GAMMA DISTRIBUTIONS 47 
2. GAMMA DISTRIBUTIONS 
The gamma function T is defined by 
B.1) r(f) = f Q°zt-1e-xdxi t > 0. 
Jo 
[See 1; 11,A2.22).] It interpolates the factorials in the sense that 
r(«-H) = «! for /i = 0,1,.;.. 
Integration by parts shows that T(t) = (/—1) T(t— 1) for all / > 0. 
(Problem 2.) 
The gamma densities concentrated on 0, oo are defined by 
B-2) U») = ~ «^«-", v>0, x>0. 
F(v) 
Here a > 0 is the trivial scale parameter, but v > 0 is essential. The special 
case /a>1 represents the exponential density, and the densities gn of 1,C,4) 
coincide with fatn (n — 1, 2,...). A trite calculation shows that the 
expectation of /a>v equals v/a, the variance v/a2. 
The family of gamma densities is closed under convolutions: 
B.3) /«.,•/«.*=/«.*-, i">0, v>0. 
This important property generalizes the theorem of 1,3 and will be in constant 
use; the proof is exceedingly simple. By A.5) the left side equals 
B.4) 
After the substitution y = xt this expression differs from fa.^+yiz) by a 
numerical factor only, and this equals unity since both /a>|l+» and B.4) are 
probability densities. 
The value of the last integral for x = 1 is the so-called beta integral 
B(ju, v), and as a by-product of the proof we have found that 
Ffji+v) 
for all ju > 0, v > 0. [For integral // and v this formula is used in 1; 
VI,A0.8) and A0.9). See also problem 3 of the present chapter.] 
As to the graph of /lv, it is clearly monotone if v <, 1, and unbounded 
near the origin when v < 1. For v > 1 the graph of /ltV is bell-shaped, 
attaining at a; » v — 1 its maximum (v— iy~1e~{v~1)lT(v) which is close 
to [2tt(v-1)H (Stirling's formula, problem 12 of 1; 11,12). It follows from 
48 SPECIAL DENSITIES. RANDOMIZATION II.3 
the central limit theorem that 
B.6) fa v j - 1 -> n(x), v -*- oo. 
a ' \ a / 
*3. RELATED DISTRIBUTIONS OF STATISTICS 
The gamma densities play a crucial, though sometimes disguised, role in 
mathematical statistics. To begin with, in the classical (now somewhat 
outdated) system of densities introduced by K. Pearson A894) the gamma 
densities appear as "type III." A more frequent appearance is due to the 
fact that for a random variable X with normal density n the square X2 
has density x-bn(xb) =f^^(x). In view of the convolution property B.3) 
it follows that: 
If Xj, . . . , Xn are mutually independent normal variables with expectation 
0 and variance a2, then X2 -f- • • ¦ + X2 has density /1/2ffi,B/2. 
To statisticians %2 = X2 -f- • • • + X2n is the "sample variance from a 
normal population" and its distribution is in constant use. For reasons of 
tradition (going back to K. Pearson) in this connection /i.in is called chi- 
square density with n degrees of freedom. 
In statistical mechanics X2 -f- X2 + X2. appears as the square of the speed 
of particles. Hence v(x) = 2xf^ ^3(x2) represents the density of the speed 
itself. This is the Maxwell density found by other methods in 1,A0.6). (See 
also the example in 111,4.) 
In queuing theory the gamma distribution is sometimes called Erlangian. 
Several random variables (or "statistics") of importance to statisticians 
are of the form T = X/Y, where X and Y are independent random vari- 
ables, Y > 0. Denote their distributions by F and G, respectively, and 
their densities by / and g. As Y is supposed positive, g is concentrated 
on 0, co and so 
C.1) P{T < t} = P{X ? rY} = [^Fity) g(y) dy. 
Jo 
By differentiation it is found that the ratio T = X/Y has density 
Too 
C.2) w(t)= f(ty)yg(y)dy. 
Examples, (a) If X and Y have densities fiiim and /j.ln, then X/Y 
has density 
This section treats special topics and is not used in the sequel. 
II.4 SOME COMMON DENSITIES 49 
In fact, the integral in C.2) equals 
C-4) -r, 
and the substitution \{\+i)y = j reduces it to C.3). 
In the analysis of variance one considers the special case 
and Y= 
where Xx,. . . , Xm, Y1}.. . , Yn are mutually independent variables with 
the common normal density n. The random variable F = {nXjmYj) is called. 
Snedecor's statistic and its density (m[n) w{{m\n) x) is Snedecor's density, or the 
F-density. The variable Z = log ?F is Fisher's Z-statistic, and its density 
Fisher's T-density. The two statistics are, of course, merely notational 
variants of each other. 
(b) Student's T-density. Let X, Yl5... , Yn be independent with the 
common normal density n. The variable 
C.5) T = 
is known to statisticians as Student's T-statistic. We show that its density is 
given by 
C.6) w(r) = -—-T^:—- , where Cn 
In fact, the numerator in C.5) has a normal density with zero expectation and 
variance nt while the density of the denominator is given by Ixfi.tn(x2). 
Thus C.2) takes on the form 
C.7) l 
The substitution s = \i\+t2ln)yt reduces the integral to a gamma integral 
and yields C.6). > 
4. SOME COMMON DENSITIES 
In the following it is understood that all densities vanish identically outside 
the indicated interval. 
(a) The bilateral exponential is defined by |ae~a'*' where a is a scale 
parameter. It has zero expectation and variance 2<x~2. This density is 
the convolution of the exponential density a.e~*x (x > 0) with the mirrored 
density <x?*x (x < 0). In other words, the bilateral exponential is the density 
of Xjl — X2 when Xx and X2 are independent and have the common 
50 SPECIAL DENSITIES. RANDOMIZATION II.4 
exponential density cue'** {x > 0). In the French literature it is usually 
referred to as the "second law of Laplace," the first being the normal 
distribution. 
(b) The uniform (or rectangular) density pa and the triangular density ra 
concentrated on —a, a are defined by 
D.1) *>«(*) = f, TB(*)-=i(l-fe!V \x\<a. 
2a a\ a J 
It is easily seen that pa* pa = r2a. In words: the sum of two uniformly 
distributed variables in —a, a has a triangular density in —2a, 2a. [The 
repeated convolutions pa*' " * pa are described in 1,(9.7).] 
(c) Beta densities in 0, 1 are defined by 
D.2) /U*) 
where fi > 0 and v > 0 are free parameters. That D.2) indeed defines a 
probability density follows from B.5). By the same formula it is seen that 
/?MtV has expectation v/(^+v), and variance fivl[(ji+v)f(jj,-\-v-\-\)). If 
fi < 1, v < 1, the graph of $M(V is U-shaped, tending to oo at the limits. 
For fi > 1, v > 1 the graph is bell-shaped. For // = v ~ 1 we get the 
uniform density as a special case. 
A simple variant of the beta density is defined by 
O<1<00- 
If the variable X has density D.2) then Y = X — 1 has density D.3). 
In the Pearson system the densities D.2) and D.3) appear as types I and VI. The 
Snedecor density C.3) is a special case of D.3). The densities D.3) are sometimes called 
after the economist Pareto. It was thought (rather naively from a modern statistical 
standpoint) that income distributions should have a tail with a density — Ax** as x -+¦ oo, 
and D.3) fulfills this requirement. 
(d) The so-called arc sine density 
D.4) -_L_ t 0 < x < 1 
-nv x(\—x) 
is actually the same as the beta density {3^ ^, but deserves special mention 
because of its repeated occurrence in fluctuation theory. (It was introduced 
in 1; 111,4 in connection with the unexpected behavior of sojourn times.) 
The misleading name is unfortunately in general use; actually the distribu- 
tion function is given by 2tt~] arc sin Jx. (The beta densities with fi + v = 1 
are sometimes referred to as "generalized arc sine densities.") 
II.4 SOME COMMON DENSITIES 51 
(<?) The Cauchy density centered at the origin is defined by 
D.5) yt{x) = - ¦ -2, - oo < x < oo, 
where t > 0 is a scale parameter. The corresponding distribution function 
is \ + 77-1 arc tan (xft). The graph of yt resembles that of the normal 
density but approaches the axis so slowly that an expectation does not exist. 
The importance of the Cauchy densities is due to the convolution formula 
D-6) ys* Yt = /,+.<• 
It states that the family of Cauchy densities D.5) is closed under convolutions. 
Formula D.6) can be proved in an elementary (but tedious) fashion by 
a routine decomposition of the integrand into partial fractions. A simpler 
proof depends on Fourier analysis. 
The convolution formula D.6) has the amazing consequence that for 
independent variables Xx,. .'. , Xn with the common density D.5) the 
average (Xx + ¦ • • + Xn)jn has the same density as the X}. 
Example. Consider a laboratory experiment in which a vertical mirror 
projects a horizontal light ray on a wall. The mirror is free to rotate about 
a vertical axis through A. We assume that the direction of the reflected 
ray is chosen "at random," that is, the angle <p between it and the perpen- 
dicular AO to the wall is distributed uniformly between — \-n and \tr. 
The light ray intersects the wall at a point at a distance 
X = t - tan <p 
from O (where t is the distance AO of the center A from the wall). It 
is now obvious that the random variable X has density D.5).2 If the 
experiment is repeated n times the average (Xx+-•-+Xn)/« has the same 
density and so the averages do not cluster around 0 as one should expect by 
analogy with the law of large numbers. > 
The Cauchy density has the curious property that if X has density yt then 2X has 
density y%t — yt * 7t- Thus 2X = X + X is the sum of two dependent variables, bur its 
density is given by the convolution formula. More generally, if U and V are two independ- 
ent variables with common density yt and X = aU + 6V, Y = cU + d\, then X + Y 
has density y(a+b+c+d)i wn'ch is the convolution of the densities y@+j,)< of X and 
2 A simple reformulation of this experiment leads to physical interpretation of the 
convolution formula D.6). Our argument shows that if a unit light source is situated 
at the origin then yt represents the distribution of the intensity of light along the line 
y = t of the «,y-plane. Then D.6) expresses Huygens' principle, according to which the 
intensity of light along y — s + t is the same as if the source were distributed along the 
line y — t following the density yt. (I owe this remark to J. W. Walsh.) 
52 SPECIAL DENSITIES. RANDOMIZATION II.4 
of Y; nevertheless, X and Y are not independent. (For a related example see problem 
1 in 111,9.) 
[The Cauchy density corresponds to the special case n — 1 of the family C.5) of 
Student's T- densities. In other words, if X and Y are independent random variables 
with the normal density n, then X/|Y| has the Cauchy density D.5) with t — 1. For 
some related densities see problems 5-6.] 
The convolution property B.3) of the gamma densities looks exactly 
like D.6) but there is an important difference in that the parameter v of 
the gamma densities is essential whereas D.6) contains only a scale parameter. 
With the Cauchy density the type.is stable. This stability under convolutions 
is shared by the normal and the Cauchy densities; the difference is that 
the scale parameters compose according to the rules- o2 ^= a\ 4- a\ and 
t = tx -f /2 respectively. There exist other stable densities with similar 
properties, and with a systematic terminology we should call the normal and 
Cauchy densities "symmetric, stable of exponent 2 and 1." (See VI, 1.) 
(/) One-sided stable distribution of index ?. If 91 is the normal distribution 
of A.6), then 
D.7) Fa(x) = 2[1 - 9t(a/V*)], x > 0, 
defines a distribution function with density 
D.8) fa(x) = -?= • —= e~^\ x>0. 
2 j3 
Obviously no expectation exists. This distribution was found in 1; 111,G.7) 
and again in 1; X,l as limit of the distribution of recurrence times, and 
this derivation implies the composition rule 
D."9) fa *fp =fy where y *= a + 0. 
(A verification by elementary, but rather cumbersome, integrations is" 
possible. The Fourier analytic proof is simpler.) If Xlt . . . , Xn are 
independent random variables with the distribution D.7), then D.9) implies 
that (Xx + - • +Xn)n~2 has the same distribution, and so the averages 
(Xi-f • • •+XJ/r are likely to be of the order of magnitude of n\ instead of 
converging they increase over all bounds. (See problems 7 and 8.) 
{g) Distributions of the form e~x~a(x > 0, a > C) appear in connection with order 
statistics (see probiem 8). Together with the variant 1 — e~x" they appear (rather 
mysteriously) under the name of Weibull distributions in statistical reliability theory. 
(h) The logistic distribution function 
D.10) F(t) = , a > 0 
may serve as a warning. An unbelievably huge literature tried to establish a transcendental 
"law of logistic growth"; measured in appropriate units, practically all growth processes 
II.5 RANDOMIZATION AND MIXTURES 53 
were supposed to be represented by a function of the form D.10) with / representing 
time. Lengthy tables, complete with chi-square tests, supported this thesis for human 
populations, for bacterial colonies, development of railroads, etc. Both height and weight 
of plants and animals were found to follow the logistic law even though it is theoretically 
clear that these two variables cannot be subject to the same distribution. Laboratory 
experiments ort bacteria showed that not even systematic disturbances can produce other 
results. Population theory relied on logistic extrapolations (even though they were 
demonstrably unreliable). The only trouble with the theory is that not only the logistic 
distribution but also the normal, the Cauchy, and other distributions can be fitted to the 
same material with the same or better goodness of fit? In this competition the logistic 
distribution plays no distinguished role whatever; most contradictory theoretical models 
can be supported by the same observational material. 
Theories of this nature are short-lived because they open no new ways, and new con- 
firmations of the same old thing soon grow boring. But the naive reasoning as such has not 
been superseded by common sense, and so it may be useful to have an explicit demonstration 
of how misleading a mere goodness of fit can be. 
5. RANDOMIZATION AND MIXTURES 
Let F be a distribution function depending on a parameter 0, and u a 
probability density. Then 
Jr+oo 
! F{x, 0) uF) dd 
— 00 
is a monotone function of x increasing from 0 to 1 and hence a distribution 
function. If F has a continuous density /, then W has a density w given 
by 
E.2) w(x)=r:°f(zj)u(d)d6. 
J-CO 
Instead of integrating with respect to a density u we can sum with respect 
to a discrete probability distribution: if 61, 02>.. . are chosen arbitrarily 
and if pk > 0, I,pk = 1, then 
E.3) H*)= 
defines a new probability density. The process may be described proba- 
bilistically as randomization; the parameter 6 is treated as random variable 
and a new probability distribution is defined in the z, 0-plane, which serves 
as sample space. Densities of the form E.3) are called mixtures, and the term 
is now used generally for distributions and densities of the form E.1) and 
E.2). 
We do not propose at this juncture to develop a general theory. Our 
aim is rather to illustrate by a few examples the scope of the method and its 
3 W. Feller, On the logistic law of growth and its empirical verifications in biology, Acta 
Biotheoretica, vol. 5 A940) pp. 51-66.- 
54 SPECIAL DENSITIES. RANDOMIZATION II.5 
probabilistic content. The examples serve also as preparation for the notion 
of conditional probabilities. The next section is devoted to examples of 
discrete distributions obtained by randomization of a continuous parameter. 
Finally, section 7 illustrates the construction of continuous processes out of 
random walks; as a by-product we shall obtain distributions occurring in 
many applications and otherwise requiring hard calculations. 
Examples, (a) Ratios. If X is a random variable with density /, then 
for fixed y > 0 the variable X(y has density f{xy)y. Treating the parameter 
y as random variable with density g we get the new density 
J 
f{xy)y g(y) dy. 
— 00 
This is the same as formula C.2) on which the discussion in section 3 was 
based. 
In probabilistic language randomizing the denominator y in X/y means 
considering the random variable X/Y, and we have merely rephrased the 
derivation of the density C.2) of X/Y. In this particular case the terminology 
is a matter of taste. 
(b) Random sums. Let Xx, X2,... be mutually independent random 
variables with a common density /. The sum Sn = Xx +• • • • + Xn has 
the density /"¦*, namely the n4o\& convolution of / with itself. [See 1,2.] 
The number n of terms is a parameter which we now randomize by a prob- 
ability distribution P{N = n} = pn. The density of the resulting sum 
SN with the random number N of terms is 
E.5) w = I pjn*. 
As an example take for {/?„} the geometric distribution pn = qpn~l-> and 
for / an exponential density. Then fn* = gn is given by B.2) and 
E.6) w(x)= _ _ 
n=i (n—1)! 
(c) Application to queuing. Consider a single server with exponential 
servicing time distribution (density f(t) — fie*11*) and assume the incoming 
traffic to be Poisson, that is, the inter-arrival times are independent with 
density Xe~x\ X < p. The model is described in 1; XVII,7F). Arriving 
customers join a (possibly empty) "waiting line" and are served in order 
of arrival without interruption. 
Consider a customer who on his arrival finds n ^ 0 other customers in 
the line. The total time that he spends at the server is the sum of the service 
times of these n customers plus his own service time. This is a random 
II.6 DISCRETE DISTRIBUTIONS 55 
variable with density /<»+i>*. We saw in 1; XVII,G.10) that in the steady 
state the probability of finding exactly n customers in the waiting line equals 
qpn with p = Xjfi. Assuming this steady state we see that the total time T 
spent by a customer at the server is a random variable with density 
2qpnfln+1)*(t) = 9A*«""i(PA*07«! =Cu-A)e-("-A)t. 
71=0 71=0 
Thus E(T) = 1/Gu-A). (See also problem 10.) 
(d) Waiting lines for buses. A bus is supposed to appear every hour 
on the hour, but is subject to delays. We treat the successive delays Xfc as 
independent random variables with a common .distribution F and density /. 
For simplicity we assume 0 <. Xfc <. 1. Denote by T^ the waiting time of 
a person arriving at epoch x < 1 after noon. The probability that the bus 
scheduled for noon has already departed is F(x), and it is easily seen that 
F(t+x) - F(x) for 0 < / < 1 -x 
E.7) P{Tx</} = 
1 - F(x) + F(x) F(t+x-\) for \r-x < t < 2-x 
and, of course, P{TX < t} = 1 for all greater /. The corresponding 
density is given by ' 
/(f+ar) for 0<t<l-x 
F(x)f(t+x-l) for l-x <t <2-x. 
Here the epoch x of arrival is a free parameter and it is natural to randomize 
it. For example, for a person arriving "at random" the epoch of arrival is a 
random variable distributed uniformly in 0, 1. The expected waiting time 
in this case equals \ + a2 where a2 is the variance of the delay. In other 
words, the expected waiting time is smallest if the buses are punctual and 
increases with the variance of the delay. (See problems 11-12.) y 
6. DISCRETE DISTRIBUTIONS 
This section is devoted to a quick glance at some results of randomizing 
binomial and Poisson distributions. 
The number Sn of successes in Bernoulli trials has a distribution depending 
on the probability p of success. Treating p as a random variable with 
density u leads to the new distribution 
F.1) PIS* = k) = (r})\\k(l-pr-ku(p)dp k = 0,...,n. 
Example, (a) When u(p) = 1 an integration by parts shows F.1) to be 
independent of k, and F.1) reduces to the discrete uniform distribution 
56 SPECIAL DENSITIES. RANDOMIZATION II .6 
P{Sn = k) = (/i + l)-1. More illuminating is an argument due to Bayes. 
Consider n + 1 independent variables Xo,. . . , Xn distributed uniformly 
between 0 and 1. The integral in F.1) (with u = 1) equals the probability 
that exactly k among the variables Xl5. .. , Xn will be <X0 or, in other 
words, that in an enumeration of the points Xo,. . . , Xn in order of magni- 
tude Xo appears at the (&+l)st place. But for reasons of symmetry all 
positions are equally likely, and so the integral equals (« + l)-1. > 
In gambling language F.1) corresponds to the situation when a skew coin 
is picked by a chance mechanism and then trials are performed with this coin 
of unknown structure. To a gambler the trials do not look independent; 
indeed, if a long sequence of heads is observed it becomes likely that for our 
coin p is close to 1 and so it is safe to bet on further occurrences of heads. 
Jwo formal examples may illustrate estimation and prediction problems of 
this type. 
Examples. (b) Given that n trials resulted in k successes (— hypothesis 
H), what is the probability of the event that p < a? By the definition of 
conditional probabilities 
F2) 
This type of estimation with u(p) = 1 was used by Bayes. Within the 
framework of our model (that is, if we are really concerned with a mixed 
population of coins with known density u) there can be no objection to 
the procedure. The trouble is that it used to be applied indiscriminately to 
judge "probabilities of causes" when there was no randomization in sight; 
this point was fully discussed in example 2(e) of 1; V in connection with a 
so-called probability that the sun will rise tomorrow. 
(c) A variant may be formulated as follows. Given that n trials resulted 
in k successes,.what is the probability that the next m trials will result 
in j successes ? The preceding argument leads to the answer 
I p\l—p)n~k 
Jo 
(See problem 13.) > 
Turning to the Poisson distribution let us interpret it as regulating the 
number of "arrivals" during a time interval of duration t. The expected 
II.6 DISCRETE DISTRIBUTIONS 57 
number of arrivals is at. We illustrate two conceptually different ran- 
domization procedures. 
Examples, (d) Randomized time. If the duration of the time interval is a 
random variable with density u, the probability pk of exactly k arrivals 
becomes 
F.4) pk 
For example, if the time interval is exponentially distributed, the probability 
of k = 0, 1,. .. new arrivals equals 
F.5) Pk = [M p 
* J?k\ a + fi \a + p) 
which is a geometric distribution. 
(e) Stratification. Suppose there are-several independent sources for 
random arrivals, each source having a Poisson output, but with different 
parameters. For example, accidents in a plant during a fixed exposure time 
t may be assumed to represent Poisson variables, but the parameter will vary 
from plant to plant. Similarly, telephone calls'origin&ting at an individual 
unit may be Poissonian with the expected number of calls varying from unit 
to unit. In such processes the parameter a appears as random variable with 
a density u, and the probability of exactly n arrivals during time t is 
given by 
/•oo 
F.6) Pn(t) = 
Jo n\ 
For the special case of a gamma density u =//j,v+i we get 
F.7) Pn(t) = 
; 
which is the limiting form of the Polya distribution as given in problem 24 of 
1; V,8 and 1; XVII,(]0.2) (setting P = a~\ v = a'1 - 1). > 
Note on spurious contagion. A curious and instructive history attaches to the distribution 
F.7) and its dual nature. 
The Polya urn model and the Polya process which lead to F.7) are models for true 
contagion where every accident effectively increases the probability of future accidents. 
This model enjcfyed great popularity, and F.7) was fitted empirically to a variety of 
phenomena, a good fit being taken as an indication of true contagion. 
By. coincidence, the same distribution F.7) has been derived previously (in 1920) by 
M. Greenwood and G. U. Yule with the intent that a good fit should disprove presence of 
contagion. Their derivation is roughly equivalent to our-stratification model, which starts 
58 SPECIAL DENSITIES. RANDOMIZATION II.7 
from the assumption underlying the Poisson process, namely, that there is no aftereffect 
whatever. We have thus the curious fact that a good fit of the same distribution may be 
interpreted in two ways diametrically opposite in their nature as well as in their practical 
implications. This should serve as a warning against too hasty interpretations of statistical 
data. 
The explanation lies in the phenomenon of spurious contagion, described in 1; V,2(d) 
and above in connection with F.1). In the present situation, having observed m accidents 
during a time interval of length * one may estimate the probability of n accidents during 
a future exposure of duration / by a formula analogous to F.3). The result will depend 
on m, but this dependence is due to the method of sampling rather than to nature itself; 
the information concerning the past enables us to make better predictions concerning the 
future behavior of our sample, and this should not be confused with the future of the whole 
population. 
7. BESSEL FUNCTIONS AND RANDOM WALKS 
Surprisingly many explicit solutions in diffusion theory, queuing theory, 
and other applications involve Bessel functions. It is usually far from obvious 
that the solutions represent probability distributions, and the analytic theory 
required to derive their Laplace transforms and other relations is rather 
complex. Fortunately, the distributions in question (and many more) may be 
obtained by simple randomization procedures. In this way many relations 
lose their accidental character, and much hard analysis can be avoided. 
By the Bessel function of order p > — 1 we shall understand the function 
Ip defined for all real x by4 
00 
G.1) /,(*)= 2 
We proceed to describe three procedures leading to three different types 
of distributions involving Bessel functions. 
(a) Randomized Gamma Densities 
. For fixed p > — 1 consider the gamma density fliP+k+1 of B.2). Taking 
the parameter k as an integral-valued random variable subject to a Poisson 
distribution we get in accordance with E.3) the new density 
<» fk oo tkrp+k 
G.2) Wp(x) = e-<2 - /1>p+fc+1(*) = e-*~*I . 
*=o k\ fc=o k\F(p+k+l) 
Comparing terms in G.1) and G.2) one sees that 
G.3) wJx) = e-*-*>I(ritf U2>/tx), x > 0. 
4 According to standard usage Ip is the "modified" Bessel function or Bessel function 
"with imaginary argument." The "ordinary" Bessel function, always denoted by Jp, is 
defined by inserting (—1)* on the right in G.1). Our use of the term Bessel function 
should be understood as abbreviation rather than innovation. 
II.7 BESSEL ^FUNCTIONS AND RANDOM WALKS 59 
If p > — 1 then wp is a probability density concentrated on 0, oo. (For 
p = — 1 the right side is not integrable with respect to x.) Note that / is 
not a scale parameter, so that these densities are of different types. 
Incidentally, from this construction and the convolution formula B.3) 
for the gamma densities it is clear that 
G.4) wp */1-v = wp+v. 
(b) Randomized Random Walks 
In discussing random walks one pretends usually that the successive 
jumps occur at epochs 1, 2,.... It should be clear, however, that this 
convention merely lends color to the description and that the model is entirely 
independent of time. An honest continuous-time stochastic process is 
obtained from the ordinary random walk by postulating that the time 
Intervals between successive jumps correspond to independent random variables 
with'the common density e~K In other words, the epochs of the jumps are 
regulated by a Poisson process, but the jumps themselves are random 
variables assuming the values +1 and —1 with probabilities p and q 
independent of each other and of the Poisson process. 
To each distribution connected with the random walk there corresponds 
a distribution for the continuous-time process, which is obtained formally 
by randomization of the number of jumps. To see the procedure in detail 
consider the position at a given epoch /. In the basic random walk the nth 
step leads to the position r > 0 iff among the first n jumps \(n+r) are 
positive and \{n—r) negative. This is impossible unless n — r = 2v is 
even. In this case the probability of the position r just after the nth jump is 
\Kn+r)J 
In our Poisson process the probability that up to epoch / exactly n — 
= 2v + y jumps occur is e~ltnln\ and so in our time-dependent process the 
probability of the position r^O at epoch / equals 
G-6) 
and we reach two conclusions. 
(i) If we define I_r = Ir for r = 1, 2, 3,... then for fixed t> 0, p, qt 
G.7) ar(t) = y]Jjpi4f^IT{2yfpq 0, r = 0, ± 1, ± 2,. . ., 
represents a probability distribution (that is, ar > 0, ]? ar = 1). 
(ii) In our time-dependent random walk ar{t) equals the probability of 
the position r at epoch t. 
60 SPECIAL DENSITIES. RANDOMIZATION II.7 
Two famous formulas for Bessel functions are immediate corollaries of this result. 
First, with the change of notations 2Vpq t = x and pjq = «2, the identity V ar(t) = 1 
becomes 
+ 00 
G.8) eix(u+u x) = 2 urlr(x). 
— 00 
This is the so-called generating function for Bessel functions or Schlomilch's formula (which 
sometimes serves as definition for Ir). 
Second, it is clear from the nature of our process that the probabilities ar(t) must satisfy 
the Chapman-Kolmogorov equation 
00 
G.9) aT{t+r)= ? ak(t)ar-.k(r), 
which expresses the fact that at epoch t the particle must be at some position k and that 
a transition from k to r is equivalent to a transition from 0 to r — k. We shall return 
to this relation in XVII,3. [It is easily verified directly from the representation G.6) and 
the analogous formula for the probabilities in the random walk.] The Chapman- 
Kolmogorov relation G.9) is equivalent to 
G.10) Ir(t + r) = ^ h(OIr-k(r) 
which is known as K. Neumann's identity. 
(c) First Passages 
For simplicity let us restrict our attention to symmetric random walks, 
p = q = \. According to 1; 111,G.5), the probability that the first passage 
through the point r > 0 occurs at the jump number In — r is 
G.11) \2~ n^r. 
2n — r\ n } 
The random walk being recurrent, such a first passage occurs with probability 
one, that is, for fixed r the quantities G.11) add up to unity. In our time- 
dependent process the epoch of the A:th jump has the gamma density flk of 
B.2). It follows that the epoch of the first passage through r > 0 has density 
V r /2n"~ V\ 2~2n+r 
n 2n — r\ n ) 
G.12) 
= e 
t 
' Bn-r-\)\\2n-r) n\{n-r)\ t 
Thus: (i) for fixed r = 1, 2,... 
G.13) vr(t) = 
defines a probability density concentrated on 0, oo. 
II.8 DISTRIBUTIONS ON A CIRCLE 61 
(ii) The epoch of the first passage through r > 0 has density vr. (See 
problem 15.) 
This derivation permits another interesting conclusion. A first passage 
through r + p at epoch / presupposes a previous first passage through r 
at some epoch s < t. Because of the independence of the jumps in the time 
intervals 0, s and s, t and the lack of memory of the exponential waiting 
times we must have 
G.14) vr*vP = vr+p. 
[A computational verification of this relation from G.12) is easy if one 
uses the corresponding convolution property for the probabilities G.11).] 
Actually the proposition (i) and the relation G.14) are true for all positive 
values of the parameters r and p.5 
8. DISTRIBUTIONS ON A CIRCLE 
The half-open interval 0, 1 may be taken as representing the points of 
a circle of unit length, but it is preferable to wrap the whole line around the 
circle. The circle then receives an orientation, and the arc length runs from 
— oo to oo but x, x ± 1, x ± 2,. .. are interpreted as the same point. 
Addition is modulo 1 just as addition of angles is modulo 2tt. A probability 
density on the circle is a periodic function <p ]> 0 such that 
Jo 
(8.1) <p{x)dx = l. 
Jo 
Examples, (a) Buffon's needle problem A777). The traditional formu- 
lation is as follows. A plane is partitioned into strips of unit width parallel 
to the y-axis. A needle of unit length is-thrown at random. What is the prob- 
ability that it lies athwart two strips? To state the problem formally 
consider first the center of the needle. Its position is determined by two 
coordinates, but y is disregarded and x is reduced modulo 1. In this way 
"the center of the needle" becomes a random variable X on the circle 
with a uniform distribution. The direction of the needle may be described 
by the angle (measured clockwise) between the needle and the y-axis. A 
turn through tt restores the position of the needle and hence the angle is 
determined only up to a multiple of tt. We denote it by Ztt. In Buffon's 
needle problem it is implied that X and Z are independent and uniformly 
distributed variables* on the circle with unit length. 
5 W. Feller, Infinitely divisible distributions and Bessel functions associated with random 
walks, J. Soc. Indust. Appl. Math., vol. 14 A966), pp. 864-875. 
6 The sample space of the pair (X, Z) is a torus. 
62 SPECIAL DENSITIES. RANDOMIZATION II.8 
If we choose to represent X by values between 0 and 1 and Z by values 
between — \ and \ the needle crosses a boundary iff \ cos Z7r > X or 
\ cos Z77 > 1 — X. For a given value z between —\ and \ the probability 
that X < I cos 277 is the same as the probability that 1 — X < \ cos 277 ^ 
namely \ cos 277. Thus the required probability is 
f* 2 
(8.2) cos ztt • dz = — . > 
A random variable X on the line may be reduced modulo 1 to obtain a 
variable °X on the circle. Rounding errors in numerical calculations are 
random variables of this kind. If X has density / the density of °X is 
given by7 
+00 
(8.3) <p(x) = ^f(x+n). 
—00 
Every density on the line thus induces a density on the circle. [It will be 
seen in XIX,5 that the same <p admits of an entirely different representation 
in terms of Fourier series. For the special case of normal densities see 
example XIX,5(e).] 
Examples, (b) Poincare's roulette problem. Consider the number of 
rotations of a roulette wheel as a random variable X with a density / 
concentrated on the positive half-axis. The observed net result, namely 
the point °X at which the wheel comes to rest, is the variable X reduced 
modulo 1. Its density is given by (8.3). 
One feels instinctively that "under ordinary circumstances" the density 
of °X should be nearly uniform. In 1912 H. Poincare put this vague feeling 
on the solid basis of a limit 'theorem. We shall not repeat this analysis 
because a similar result follows easily from (8.3). The tacit assumption 
is, of course, that the given density / is spread out effectively over a long 
interval so that its maximum m is small. Assume for simplicity that / 
increases up to a point a where it assumes its maximum m = /(<*), and 
that / decreases for x > a. For the density <p of the reduced variable °X 
we have then 
J*+oo 
f(s) ds. 
— CO 
For fixed x denote by xk the unique point of the form x+n such that 
7 Readers worried about convergence should consider only densities / concentrated on 
a finite interval. The uniform convergence is obvious if / is monotone for x and — x 
sufficiently large. Without any conditions on / the series may diverge at some points, but 
<p always represents a density because the partial sums in (8.2) represent a monotone 
sequence of functions whose integrals tend to 1. (See IV, 2.) 
II.8 DISTRIBUTIONS ON A CIRCLE 63 
a+k <,zk< a+k+\. Then (8.4) may be rewritten in the form 
+ 00 pa+k+1 
(8.5) <p(x) -1=2 [/(**)-/(*)] ds. 
fc=-oo Ja+k 
For k < 0 the integrand is <0, and so 
?(*) - 1 <|[/(a+/c) -f(a+k+\)] = /(a) = m. 
A similar argument shows that cp(x) — 1 > — m. Thus \<p(z)—1| < w 
and so 9? is indeed nearly constant. 
The monotonicity conditions were imposed only for the sake of exposition 
and can be weakened in many ways. [Neat sufficient conditions can be 
obtained using Poisson's summation formula, XIX,5B).] 
(c) Distribution of first significant digits. A distinguished applied mathe- 
matician was extremely successful in bets that a number chosen at random in 
the Farmer's Almanac, or the Census Report or a similar compendium, would 
have the first significant digit less than 5. One expects naively that all 9 
digits are equally likely, in which case the probability of a digit <4 would be 
¦f-. In practice8 it is close to 0.7. 
Consider the discrete probability distribution attributing to the digit k 
probability pk = Log (k+\) — Log & (where Log denotes the logarithm 
to the basis 10 and k = 1,.. . , 9). These probabilities are approximately 
/?! = 0.3010 p2 = 0.1761 p3 = 0.1249 /?4 = 0.0969 
ph = 0.0792 P6 « 0.0669 p1 = 0.0580 p8 = 0.0512 p9 = 0.0458, 
and it is seen that the distribution {pk} differs markedly from the uniform 
distribution with weights } = 0.111 • • • . 
We now show (following R. S. Pinkham) that {pk} is plausible for the 
empirical distribution of the first significant digit for numbers taken at 
random from a large body of physical or observational data. Indeed, such 
a number may be considered as a random variable Y > 0 with some 
unknown distribution. The first significant digit of Y equals k iff 
\0nk <, Y < 10n(Ar+l) for some n. For the variable X = Log Y this 
means 
(8.6) n + Log k < X < n + Log (k+1). 
If the spread of Y is very large the reduced variable °X will be approxi- 
mately uniformly distributed, and the probability of (8.6) is then close to 
Log(A: + l) - Log/: = pk. 
8 For empirical material see F. Benford, The law of anomalous numbers, Froc, Amer. 
Philos. Soc., vol. 78 A938) pp. 551-572. 
64 SPECIAL DENSITIES. RANDOMIZATION II.9 
The convolution formula A.5) and the argument leading to it remain 
valid when addition is taken modulo 1. Accordingly, the convolution of 
two densities on the circle of length 1 is the density defined by 
*) = Cfi(*-y) 
Jo 
(8.7) 
If Xj and X2 are independent variables with densities fx and /2 then 
Xj + X2 has the density w. Since these densities are periodic, the con- 
volution of the uniform density with any other density is uniform. (See 
problem 16.) 
9. PROBLEMS FOR SOLUTION 
1. Show that the normal approximation to the binomial distribution established 
in 1; VII implies the convolution formula A.7) for the normal densities. 
2. Using the substitution x = $y2 prove that T(^) = */n. 
3. Legendres duplication formula. From B.5) for n = v conclude that 
rBv) =-i 22v-ir(v)r(v+?>. 
Hint: Use the substitution 4B/— yz) = s in 0 < y < \. 
4. If g(x) = \e~\x\ find the convolutions g *g and g * g * g as well as g**. 
5. Let X and Y be independent with the common Cauchy density y^(x) of 
D.5). Prove that the product XY has density 2tT% {x - X)^g\x\ 
Hint: No calculations are required beyond the observation that 
a-\ J1 
A +s)(a+s) f+5 a+s ' 
6. Prove that if 
/(*) = - * _x then f*f(x) = 1 
J n ex + e x J J jjZ 
_ e x 
() by considering the variables log |X| and log |Y| of the preceding problem; 
(b) directly by the substitution e2* = t and a partial fraction decomposition. 
(See problem 8 of XV,9.) 
7. If X has the normal density n then obviously X~2 has the stable density 
D.8). From this conclude that if X and Y are independent and normal with zero 
expectations and variances a\ and o\, then Z =XY/v/X2 + Y2 is normal with 
variance o\ such that l/cr3 = 1/^1 + llo2 (L. Shepp). 
8. Let Xl5..., Xn be independent and X(n) the largest among them. Show that 
if the X; have: 
(a) the Cauchy density D.5), then 
P{n-lX{n} <, x} -> e-i/{*x>, x>0 
(b) the stable density D.8), then 
P{/r2X(n) <; x} - <r«v S7u^ x > 0. 
II.9 PROBLEMS FOR SOLUTION 65 
9. Let X and Y be independent with densities / and g concentrated on 0, oo. 
If E(X) < oo the ratio X/Y has a finite expectation iff 
s 
Jo 
y lg(y)dy 
10. In example 5(c) find the density of the waiting time to the next discharge (a) 
if at epoch 0 the server is empty, (b) under steady-state conditions. 
11. In example 5(d) show that 
E(TX) = F(x)(/z +1 -x) + f *" "tfit +x) dt, 
Jo 
where /* is the expectation of F. From this verify the assertion concerning E(T) 
when x is uniformly distributed. 
12. In example 5(d) find the waiting time distribution when /(/) = 1 for 
0 <t < 1. 
13. In example 6(c) assume that u is the beta density given by D.2). Evaluate 
the conditional probability F.3) in terms of binomial coefficients. 
14. Let X and Y be independent with the common Poisson distribution 
p{X = n} = e~Un\n\ Show that 
P{X-Y = r} = e~2%\Bt), r = 0, ±1, ±2, .... 
[See problem 9 of V,ll.] 
15. The results of section l.c remain valid for unsymmetric random walks 
provided the probability of a first passage through r > 0 equals one, that is, 
provided p >q. Show that the only change in G.11) is that 2~2n+r is replaced by 
pnqn~r, and the conclusion is that for p > q and r = 1, 2, . . ., 
defines a probability density concentrated on / > 0. 
16. Let X and Y be independent variables and °X and °Y be the same 
variables reduced modulo 1. Show that °X + °Y is obtained by reducing X + Y 
modulo 1. Verify the corresponding formula for convolutions by direct calculation. 
CHAPTER III 
Densities in Higher Dimensions. 
Normal Densities and Processes 
For obvious reasons multivariate distributions occur less frequently than 
one-dimensional distributions, and the material of this chapter will play 
almost no role in the following chapters. On the other hand, it covers 
important material, for example, a famous characterization of the normal 
distribution and tools used in the theory of stochastic processes. Their true 
nature is best understood when divorced from the sophisticated problems 
with which they are sometimes connected. 
1. DENSITIES 
For typographical convenience we refer explicitly to the Cartesian plane 
3l2, but it will be evident that the number of dimensions is immaterial. We 
refer the plane to a fixed coordinate system with coordinate variables 
Xx, X2. (A more convenient single-letter notation will be introduced in 
section 5.) 
A non-negative integrable function / defined in 'Ji2 and such that its 
integral equals one is called a probability density, or density for short. (All 
the densities occurring in the chapter are piecewise continuous, and so the 
concept of integration requires no comment.) The density / attributes to 
the region Q the probability 
A.1) 
provided, of course, that Q. is sufficiently regular for the integral to exist. 
All such probabilities are uniquely determined by the probabilities of 
rectangles parallel to the axes, that is, by the knowledge of 
A.2) P{a1<X1<b1,a,<X2<ib2}=\1 ( i 
Jai Jat 
66 
Ill-1 DENSITIES 67 
for all combinations at < b{. Letting ax — a2 — — oo we get the distribution 
function F of /, namely 
A.3) F(xx, x2) = P{XX <,xx, X2 ? s2}. 
Obviously /"(&!, z2) — F(ax, x2) is the probability of a semi-finite strip of 
width bx — ax and, the rectangle appearing in A.2) being the difference of 
two such strips, the probability A.2) equals the so-called mixed difference 
b2) - F(ax, b2) - F{bx, a2) + F(alt a2).- 
It follows that the knowledge of the distribution function F uniquely 
determines all probabilities A.1). Despite the formal analogy with the situa- 
tion on the line, the concept of distribution function F is much less useful 
in the plane and it is best to concentrate on the .assignment of probabilities 
A.1) in terms of the density itself. This assignment differs from the joint 
probability distribution of two discrete random variables A; IX,1) in two 
respects. First, integration replaces summation and, second, probabilities 
are now assigned only to "sufficiently regular regions" whereas in discrete 
sample spaces all sets had probabilities. As the present chapter treats only 
simple examples in which the difference is hardly noticeable, the notions and 
terms of the discrete theory carry over in a self-explanatory manner. Just as 
in the preceding chapters we employ therefore a probabilistic language 
without any attempt at a general theory (which will be supplied in chapter V). 
It is apparent from A.3) that1 
A.4) . P{XX < xx} = F(xx, oo). 
Thus Fx(x) = F(x, oo) defines the distribution function of Xx, and its 
density fx is given by 
J 
/ (*, y) dy: 
— 00 
When it is desirable to emphasize the connection between X1 and the pair 
(Xl5 X2) we again speak of Fx as marginal distribution2 and of fx as marginal 
density. 
The expectation [xx and variance a\ of Xx—if they exist—are given by 
J'+co /*+<» 
*i/(*i, *¦) dxx dx2 
— 00 J —00 
and 
J'+co f+oo 
(xx-fxxJf(xx\ x2) dxx dx2. 
— 00 «/—00 
1 Here and in the following ?/(oo) = lim U(x) as x -+ oo and the use of the symbol 
?/(<») implies the existence of the limit. 
2 Projection on the axes is another accepted term. 
68 DENSITIES IN HIGHER DIMENSIONS III. 1 
By symmetry these definitions apply also to X2. Finally, the covariance 
of Xx and X2 is 
—00 «/— 
(xi— /"i)O2-,)/(*!, x%) dx1 dx 
The normalized variables Xj-oC1 are dimensionless and their covariance, 
namely p = Cov (Xx, X2)a~1a21, is the correlation coefficient of Xx and X2 
(seel; IX,8). 
A random variable U is a function of the coordinate variables Xx and 
X2; again we consider for the present only functions such that the prob- 
abilities P{U < /} can be evaluated by integrals of the form A.1). Thus 
each random variable will have a unique distribution function, each pair 
will have a joint distribution, etc. 
In many situations it is expedient to change the coordinate variables, 
that is, to let two variables Ylt Y2 play the role previously assigned to 
Xx, X2. In the simplest case the Y, are defined by a linear transformation 
A.9) Xx = alxYx + fl12Y2, X2 = a^Yx + fl22Y2, 
with determinant A = axxa22 ~ aX2a2x > 0. Generally a transformation 
of the form A.9) may be described either as a mapping from one plane 
to another or as a change of coordinates in the same plane. Introducing 
the change of variables A.9) into the integral A.1) we get 
A.10) P{O} =JJ f(axxyx+ax2y2, a2xyx+a22y2) • A dyx dy2 
the region Q* containing all points (yx, y2) whose image (xlt x2) is in O. 
Since the events (Xx, X2) e Q. and (Yx, Y2) e D+ are identical it is seen that 
the joint density of (Yx, Y2) is given by 
A.11). g(yi, V*) =f{axxyx+ax^y2, a2xyx+a22y2) • A. 
All this applies equally to higher dimensions. 
A similar argument applies to more general transformations, except 
that the determinant A is replaced by the Jacobian. We shall use explicitly 
only the change to polar coordinates 
A.12) Xi = Rcose, X2 = Rsin0 
with (R, 0) restricted to R ;> 0, — n < 0 < n. Here the density of 
(R, 0) is given by 
A.13) g{r, 6) =f(r cos 6, r sin 0)r. 
In three dimensions one uses the geographic longitude q> and latitude 6 
(with —it < <p < it and — \tt <, 0 <> \tt). The coordinate variables in the 
Ill-1 DENSITIES 69 
polar system are then defined by 
^1.14) Xx = R cos* cos 0, X2 = Rsin<fccos 0, X3 — R sin 0. 
For their joint density one gets 
A.15) g(r, (p, 0) =/(rcos 9? cos 0,rsin 9? cos 0, r sin 0)r2cos 0. 
In the transformation A.14) the "planes" 0 = — \tt and 0 = \tt corre- 
spond to the half axes in the ^-direction, but this singularity plays no role 
since these half axes have zero probability. A similar remark applies to the 
origin for polar coordinates in the plane. 
Examples, (a) Independent variables. In the last chapters we considered 
independent variables Xx and X2 with densities fx and /2. This amounts 
to defining a bivariate density by f(x1, z2) = /i(zi)/2(>2), and the/f 
represent the marginal densities. 
(b) "Random choice." Let F be a bounded region; for simplicity we 
assume F convex. Denote the area of F by y and put / equal to y~l 
within F and equal to 0 outside F. Then / is a density, and the probability 
of any region Q <= F equals the ratio of the areas of Q and F. By obvious 
analogy with the one-dimensional situation we say that the pair (Xlt X2) 
is distributed uniformly over F. The marginal density of Xx at the abscissa 
xx equals the width of F at xx in the obvious sense of the word. (See 
problem 1.) 
(c) Uniform distribution on a sphere. The unit sphere 2 in three dimen- 
sions may be represented in terms of the geographic longitude <p and 
latitude 0 by the equations 
A.16) xx = cos 9? cos 0, x2 = sin 9? cos 0, x3 = sin 0. 
To each pair (9?, 0) such that —v < 9? <, v, — \tt < 0 < \tt there cor- 
responds exactly one point on the sphere and, except for the two poles, 
each point of S is obtained in this way. The exceptional role of the poles 
need not concern us since they will have probability 0. A region il on the 
sphere is defined by its image in the 9?, 0-plane, and the area of Q equals 
the integral of cos 0 dtp dO over this image [see A.15)]. For the conceptual 
experiment "random choice of a point on 2" we should put 4nP{Q} — area 
of Q. This is equivalent to defining in the q>, 0-plane a density 
D77-)-1 COS 0 for -77- < <f < 77", |0| < ?tT 
A.17) g(<p,6) = 
0 elsewhere. 
With this definition the coordinate variables are independent, the longitude 
being distributed uniformly over —77, it. 
70 DENSITIES IN HIGHER DIMENSIONS 11 I.I 
The device of referring the sphere 2 to the 9?, 0-plane is familiar from 
geographic maps and useful for probability theory. Note, however, that 
the coordinate variables are largely arbitrary and their expectations and 
variances meaningless for the original conceptual experiment. 
(d) The bivariate normal density. Normal densities in higher dimensions 
will be introduced systematically in section 6. The excuse for anticipating 
the bivariate case is to provide an easy access to it. An obvious analogue 
to the normal density n of 11,B.1) is provided by densities of the form 
c ¦ e~Q(Xl-X2) where q(xlt x2) = axx\ + 2bxxx2 + a&\. It is easily seen that 
e"*" will be integrable iff the a5 are positive and axa2 — b2 > 0. For pur- 
poses of probability theory it is preferable to express the coefficients a{ and 
b in terms of these variances and to define the bivariate normal density 
centered at the origin by 
A.18) 
<p(xx, x2) = exp 
2ttoxo2\I 1— p2 
[-*b>( 
xx xxx2 x2\ 
— - 2p h — 
where ax > 0, a2 > 0, and — 1 < p < 1. The integration with respect to 
x2 is easily, performed by the substitution / = x2ja2 — pxljal (completing 
squares), and it is seen that 9? indeed represents a density in ft2. Further- 
more, it becomes obvious that the marginal distributions for X, and X2 
are again normal3 and that E(Xt) = 0, Var (Xt) = of, Cov (X1( X2) = 
= paxa2. In other words, p is the correlation coefficient of Xx and X2. 
Replacing xt by x{ — c^ in A.18) leads to a normal density centered at the 
point (cl5 c2). 
It is important that linear transformations A.9) change a normal distribution 
into another normal distribution. This is obvious from the definition and A.11). 
[Continued in example 2(a).] 
(e) The symmetric Cauchy distribution in 3l2. Put 
A.19) u(x1,x2) ± X 
To see that thi*s is a density note4 that 
1 1 y 
J*+co 
— ao 
y)dy = — ¦ 
77 I + X\ 
3 Contrary to a widespread belief there exist non-normal bivariate densities with normal 
marginal densities (two types are described in problems 2, 3; two more in problems 5 
and 7 of V,12). In the desire to deal with normal densities, statisticians sometimes introduce 
a pair of new coordinate variables Yx=gx(Xx), Y2 =^2(X2) which are normally 
distributed. Alas, this does not make the joint distribution of (Yj,Y2) normal. 
4 The substitution y = Vl + x\ tan t makes the calculation easy. 
III.2 CONDITIONAL DISTRIBUTIONS 71 
It follows that u is a density and that the marginal density of Xx is the 
Cauchy density yx of 11,D.5). Obviously Xx has no expectation. 
Switching to polar coordinates [as in A.12)] R 'gets a density independent 
of 0 and so the variables R and 0 are stochastically independent. In 
the terminology of 1,10 we can therefore say that with the symmetric Cauchy 
distribution (Xx, X2) represents a vector in a randomly chosen direction with a 
length R whose, density is given by r\/(l+r2)", whence P{R <, r} = 
= 1 - sJ(l+r2)-\ [Continued in example 2(c).] 
(/) The symmetric Cauchy distribution in ft3. Put 
A.21) v(xlt x2, x3) = — 
2 
It is easily seen5 that the marginal density of (Xx, X2) is the symmetric 
Cauchy density u of A.19). The marginal density of Xx is therefore the 
Cauchy density yv (Continued in problem 5.) > 
Although it will not play an explicit role in the sequel it should be mentioned 
that we can define convolutions just as in one dimension. Consider two 
pairs (Xx, X2) and (Yx, Y2) with joint densities / and g, respectively. 
Saying that the two pairs are independent means that we take the four- 
dimensional space with coordinate variables Xx, X2, Ylt Y2 as sample space 
and define in it a density given by the product f(x1, x2) g{yx, ya). Just as in 
&1 it is then easily seen that the joint density v of the sum (Xx+Yx, X2+Y2) 
is given by the convolution formula 
JN-oo /*+« 
/(«! - *i, *2 - *z) g(^i, *2) dxx dx2 
—oo %/—oo 
which is the obvious analogue to 1,B.12). (See problems 15-17.) 
2. CONDITIONAL DISTRIBUTIONS 
Suppose that the pair (Xx, X2) has a continuous density / and that the 
marginal density fx of Xx is strictly positive. Consider.the conditional 
probability of the event X2 <, r\ given that ? < Xx <> ? + h, namely 
r$+h fir 
I dxj f(x,y)dy 
B.1) P{X<|^<X^^ + M "" 
fx(x)dx 
Js 
Dividing numerator and denominator by h, one sees that as h —*¦ 0 the 
5 Use the substitution z = Vl +xf + x| tan /. 
72 DENSITIES IN HIGHER DIMENSIONS III.2 
right side tends to 
B-2) Ufn) = 77- f" /(*, y) dy. 
For fixed ? this is a distribution function in r\ with density 
B3) ? 
We call wf the conditional density of X2 g/yert ///a/ Xx = ?. 7/re conditional 
expectation of X2 gwert //raz Xx = ? 4s defined by 
B.4) E(X2 I X, = |) = -j- (%/(?, y) 
provided that the integral converges absolutely. With ? considered as a 
variable the right side becomes a function of it. In particular, we may 
identify ? with the coordinate variable Xx to obtain a random variable 
called the regression of X2 on Xx and denoted by E(X2 | Xx). The appear- 
ance of X2 should not obscure the fact that this random variable is a function 
of the single variable Xx [its values being given by B.4)]. 
So far we have assumed that fi(€) > 0 for all ?. The expression B.4) 
is meaningless at any place where /i(f) = 0, but the set of such points has 
probability zero and we agree to interpret B.4) as zero at all points where fx 
vanishes. Then E(X2 j Xx) is defined whenever the density is continuous. 
(In V,9-ll conditional probabilities will be introduced for arbitrary 
distributions.) 
, Needless to say, the regression E(XX | X2) of Xx on X2 is. defined in like 
manner. Furthermore, a conditional variance Var (X21 Xx) is defined by 
obvious analogy with B.4). 
These definitions carry over to higher dimensions, except that a density 
in %z gives rise to three bivariate and three univariate conditional densities 
(See problem 6.) 
Examples, (a) The normal density. For the density A.18) obviously 
B.5) ut(y) = , ^ o^ o exp 
1 —p2)cr2 J 
2A 
which is a normal density with expectation />((T2/(Ti)| and variance 
(\-P2)o\. Thus 
B.6) E(X2 | Xx) = p{a2fax)XXt Var (X2 | Xx) = A - P2)a22. 
It is one of the pleasing properties of the normal distribution that the 
regressions are linear functions. 
III.2 CONDITIONAL DISTRIBUTIONS 73 
Perhaps the earliest application of these relations is due to Galton, and 
one of his examples may illustrate their empirical meaning. Imagine that 
Xx and X2 represent the heights (measured in inches from their respective 
expectations) of fathers and sons in a human population. The height of a 
randomly chosen son is then a normal variable with expectation 0 and 
variance a\. However, in the subpopulation of sons whose fathers have a 
fixed height ?, the height of the sons is a normal variable with expectation 
p(o2jox)? and variance o\(\—p2) < a\. Thus the regression of X2 on Xx 
indicates how much statistical information about X2 is contained in 
observation of Xx. 
(b) Let Xx and X2 be independent and uniformly distributed in 0,1. 
Denote by XA) the smaller and by XB) the larger among these variables. 
The pair (XA), XB)) has a density equal to the constant 2 within the triangle 
0 <, xx <, x2 <, 1, and vanishing elsewhere. Integration over x2 shows that 
the marginal density of XA) is given by 2A — xx). The conditional density 
of XB) for given XA) = xx therefore equals the constant 1/1— xx within 
the interval xx, 1 and zero elsewhere. In other words, given the value xx 
of XA) the variable XB) is uniformly distributed over xx,l. 
(c) Cauchy distribution in 3i2. For the bivariate density A.19) the marginal 
density for Xx is given in A.20), and so the conditional density of X2 for 
given Xx is 
B.7) uk{y) = y 
Note that u? differs only by the scale factor Vl + I2 from the density 
uo(y) and so all the densities u^ are of the same type. Conditional expecta- 
tions do not exist in this example. (See problem 6.) > 
In terms of the conditional densities B.3) the distribution function of 
X2 takes on the form 
Jfy C+ 
I 
—ao J—o 
In other words, the distribution of X2 is obtained by randomization of the 
parameter ? in the conditional densities u?, and so every6 distribution may 
be represented as mixture. Despite this theoretical universality there is a 
great difference in emphasis. In some situations [such as example (a)] 
one starts from a bivariate distribution for (Xx, X2) and derives conditional 
distributions, whereas in true randomization the conditional probabilities 
6 We have so far considered only continuous densities, but the general case will be covered 
in V,9. The notion of randomization was discussed in 11,5. 
74 DENSITIES IN HIGHER DIMENSIONS III.3 
ux are the primary notion and the density f(z, y) is actually defined by 
ux(y)fi(x). (This procedure of defining probabilities in terms of conditional 
probabilities was explained in an elementary way in 1; V,2.) 
3. RETURN TO THE EXPONENTIAL AND THE 
UNIFORM DISTRIBUTIONS 
The object of this section is to provide illustrative examples to the 
, preceding sections and at the same time to supplement the theory of the 
' first chapter. 
Examples, (a) A characteristic property of the exponential distribution. 
Let Xx and X2 be two independent random variables with densities fx and 
fi, and denote the density of their sum S = Xx+X2 by g. The pairs 
(Xx, S) and (Xx, X2) are related by the linear transformation Xx = X1} 
X2 = S — Xi with determinant 1 and by A.11) the joint density of the 
pair (Xx, S) is given by fi(x)f2(s—x). Integrating over all x we obtain the 
marginal density g of S. The conditional density u, of Xx given that S = s 
satisfies 
nn u(x\ 
W--U u*\x) 77 
g(s) 
In the special case of exponential densities fx(x) =f2(x) = a.e~*x (where 
x > 0) we get us(x) = s'1 for 0 < x < s. In other words, given that 
Xx + X2 = s, the variable Xx is uniformly distributed over the interval 
0, s~. Intuitively speaking, the knowledge that S = s gives us no due as 
to the possible position of the random point Xx within the interval 0, s. 
This result conforms with the notion of complete randomness inherent in 
the exponential distribution. (A stronger version is contained in example 
(d). See also problem 12.) 
(b) Random partitions of an interval. Let X1} . . . , Xn be n points 
chosen independently and at random in the (one-dimensional) interval 
0, 1. As before we denote by XA), XB),.. . , X(n) the random points 
Xi,... , Xn rearranged in increasing order. These points divide the 
interval 0, 1 into n + 1 subintervals which we denote by Ilt /2, .., , /n+i 
numbering them from left to right so that X(/) is the right endpoint of /,-. 
Our first aim is to calculate the joint density of (XA),... , X(n)). 
The sample space corresponding to (Xx,... , Xn) is the w-dimensional 
hypercube F defined by 0<:rA.<l, and probabilities equal the n- 
dimensional volume. The natural sample space with the' X(k) as coordinate 
variables is the subset Q of F containing all points such that 
0<x1<t"-<,xn<\. 
III.3 RETURN TO EXPONENTIAL, UNIFORM DISTRIBUTIONS 75 
The volume of Q is 1/n! Evidently the hypercube F contains n\ con- 
gruent replicas of the set Q and in each the ordered rt-tuple (XA),. .. , X(n)) 
coincides with a fixed permutation of Xlf... , Xn. (Within F, in particular, 
x<*> = Xfc.) The probability that X, = Xk for some pair j ¦? k equals zero> 
and only this event causes overlaps among the various replicas. It follows 
that for any subset A c Q the probability that (XA),... , X(n)) lies in A 
equals the probability that (X1}... , Xn) lies in one of the n\ replicas of A, 
and this probability in turn equals n\ times the volume of A. Thus 
P{(XA),. . . , X(n)) 6 A) equals the ratio of the volumes of A and of Q, 
which means that the n-tuple (XA),. . . , X(n)) is distributed uniformly over 
the set Q. The joint density of our /z-tuple equals n\ within Q and 0 
outside. 
From the joint density of (XA),.. . , X(n)) the density of X{k) may be 
calculated by keeping xk fixed and integrating over the remaining variables. 
The result is easily seen to agree with the density calculated by other methods 
in 1,G.2). 
This example was treated in detail as an exercise in handling and com- 
puting multivariate densities. 
(c) The distribution of the lengths. In the random partition of the preceding 
example denote the length of the kth interval Ik by Ufc. Then 
C.2) Ux = XA), Vk = X{k) - X(k_1} for k = 2, 3,... ,n 
This is a linear transformation of the form A.9) with determinant 1. The 
set Q. of points 0 < xx < • • • <. xn < 1 is mapped into the set Q* of 
points such that w,- > 0, ux + • • • + un < 1, and hence (Ult. . . , Un) 
is distributed uniformly over this region. This result is stronger than the 
previously established fact that the Vk have a common distribution function 
[example I,7(?) and problem in 1,13.] 
(d) Once more the randomness of the exponential distribution. Let 
X1} . . . , Xw+1 be independent with the common density a.e~ax for x > 0. 
Put S,- = Xx + h Xy. Then (S1} S2,... , SB+1) is obtained from 
(X1}. . . , Xn+i) by a linear transformation of the form A.9) with deter- 
minant 1. Denote by Q the "octant" of points x}. > 0. The density of 
(Xj,. . . , Xn+1) is concentrated on Q. and is given by 
if x} > 0. The variables Sl5. .. , Sn+1 map Q onto the region Q* defined 
by 0 < sx <, s2 <t ' • • <L sn+1 < oo, and [see A.11)] within Q* the density 
of (Sx, . .v. , Sn+1) is given by an+1e-aSn+1. The marginal density of Sn+1 
is known to be the gamma density <x.nJrlsne~a*ln\ and hence the conditional 
density of the n-tuple (Slf . . . , Sn) given that Sn+1 = s equals nls~n for 
0 < ^ < • • • < sn < s (and zero elsewhere). In other words, given that 
16 DENSITIES IN HIGHER DIMENSIONS III.3 
Sn+i = s the variables (S1} . . . , Sn) are uniformly distributed over their 
possible range. Comparing this with example (b) we may say that given 
Sn+i = s, the variables (Su . . . , Sn) represent n points chosen independently 
and at random in the interval 0, s numbered in their natural order from left 
to right. 
(e) Another distribution connected with the exponential. With a view to 
a surprising application we give a further example of a transformation. 
Let again X1} . . . , Xn be independent variables with a common exponential 
distribution and Sn = Xx + • • • + Xn. Consider the variables Vlf. . . , Un 
defined by 
C.3) Ufc = Xfc/Sn for k = l,...,n-l, Un = SB, 
or, what amounts to the same, 
C.4) Xfc = UfcUn for k<n, Xn = UBA-Uin U^). 
The Jacobian of C.4) equals UJJ. The joint density of (X1}: . . , Xn) is 
concentrated on the region Q, defined by xk > 0, and in it this density is 
given by ane-a(a!l+"+a:"). It follows that the joint density of (Ult. . . , UB) 
is given by anw^~1e'a-Un in the region Q* defined by 
<1, «fc>0 k = \, . . . ,n 
and that it vanishes outside Q*. An integration with respect to un shows 
that the joint density for (U1} . . . , U^) equals (rr- IX1. in Q* and 0 
elsewhere. Comparing with example (c) we see that (U^ r. . , Un_!) has 
the same distribution as if Uk were the length of the kth interval in a random 
partition of 0, I by n — 1 points. 
(/) A significance test in periodogram analysis and the covering theorem. 
In practice, any continuous function of time / can be approximated by a 
trigonometric polynomial. If the function is a sample function of a stochastic 
process the coefficients become random variables, and the approximating 
polynomial may be written in the form 
C.5) 2(xv cos "V+Yv sin covt) = %RV cos (covt—<l»v) 
v=l v=l 
where R^ = X^ -f- Y^ and tan <$v = Yv/Xv. Conversely, reasonable 
assumptions on the random variables Xv, Yv lead to a stochastic process 
with sample functions given by C.5). For a time it was fashionable to 
introduce models of this form and to detect "hidden periodicities" for 
sunspots, wheat prices, poetic creativity, etc. Such hidden periodicities 
used to be discovered as easily as witches in medieval times, but even strong 
faith must be fortified by a statistical test. The method is roughly as follows. 
A trigonometric polynomial of the form C.5) with well-chosen frequencies 
a)lf... , (on is fitted to some observational data, and a particularly large 
amplitude Rv is observed. One wishes to prove that this cannot be due to 
III.4 CHARACTERIZATION OF THE NORMAL DISTRIBUTION 77 
chance and hence that cov is a true period. To test this conjecture one asks 
whether the large observed value of Rv is plausibly compatible with the 
hypothesis that all n components play the same role. For a test one 
assumes, accordingly, that the coefficients X1}. . . , Yn are mutually 
independent with a common normal distribution with zero expectation and 
variance a2. In this case (see 11,3) the R^ are mutually independent and 
have a common exponential distribution with expectation 2a2. If an 
observed value Rj; deviated "significantly" from this predicted expectation 
it was customary to jump to -the conclusion that the hypothesis of equal 
weights was untenable, and Rv represented a "hidden periodicity." 
The fallacy of this reasoning was exposed by R. A. Fisher A929) who 
pointed out that the maximum among n independent observations does 
not obey the same probability distribution as each variable taken separately. 
The error of treating the worst case statistically as if it had been chosen at 
random is still common in medical statistics, but the reason for discussing 
the matter here is the surprising and amusing connection of Fisher's test of 
significance with covering theorems. 
As only the ratios of the several components are significant we normalize 
the coefficients by letting 
Since the R^. have a common exponential distribution we can use the 
preceding example with Xs = R2. Then V1 = U1}. . . , Vn_! = Un_1} but 
Vn = 1 — Ux — • • • — UB_x. Accordingly, the n-tuple (Vlt..•. , Vn) is 
distributed as the length of the n intervals into which 0, 1 is partitioned by 
a random distribution of n—\ points. The probability that all V, be less 
than a is therefore given by formula 1,(9.9) of the covering theorem. This 
result illustrates the occurrence of unexpected relations between apparently 
unconnected problems.7 > 
•4. A CHARACTERIZATION OF THE NORMAL 
DISTRIBUTION 
Consider a non-degenerate linear transformation of coordinate variables 
D.1) Y1 = fluXi + a12X2, Y2 = a21Xx + a22X2, 
7 Fisher derived the distribution of the maximal term among the V^ in 1929 without 
knowledge of the covering theorem, and explained in 1940 the equivalence with the 
covering theorem after W. L. Stevens had proved the latter. [See papers No. 16 and 37 ip 
Fisher's Contributions to Mathematical Statistics, John Wiley, New York A950).] For 
an alternative derivation using Fourier analysis see U. Grenander and M. Rosenblatt 
A957). 
* This section treats a special topic and is not used in the sequel. 
78 DENSITIES fN HIGHER DIMENSIONS 111.4 
and suppose (without loss of generality) that the determinant A = 1. If 
X1 and X2 are independent normal variables with variances o\ and o\ 
the distribution of the pair (Ylf Y2) is normal with covariance 
[see example 1 (</)]. In this case there exist non-trivial choices of the coeffi- 
cients ajk such that Yx and Y2 are independent. The following theorem 
shows that this property of the univariate normal distribution is not shared 
by any other distribution. We shall here prove it only for distributions with 
continuous densities, in which case it reduces to a lemma concerning the 
functional equation D.3). By the use of characteristic functions the most 
general case is reduced to the same equation, and so our proof will really 
yield the theorem in its greatest generality (see XV,8). The elementary 
treatment of densities reveals better the basis of the theorem. 
The transformation D.1) is meaningful only if no coefficient aik vanishes. 
Indeed, suppose for example that an = 0. Without loss of generality we may 
choose the scale parameters so that a12 = L Then Yx = X2, and a glance 
at D.4) shows that in this case Y2 must have the same density as Xx. In 
other words, such a transformation amounts to a mere renaming of the 
variables, and need not be considered. 
Theorem. Suppose that Xx and X2 are independent of each other, and 
that the same is true of the pair Yl9 Y2. If no coefficient aJk vanishes then 
all four variables are normal. 
The most interesting special case of D.1) is presented by rotations, namely 
transformations of the form 
D.2) Yi = Xx cos co + X2 sin co, Y2 = —Xx sin co + X2 cos co 
where co is not a multiple of \tt. Applying the theorem to them we get 
Corollary. If X1 and . X2 are independent and there exists one rotation 
{A.2) such that Yx and Y2 are also independent, then Xx and X2 have normal 
distributions with the Same variance. In this case Yx and Y2 are independent 
for every co. 
Example. Maxwell distribution of velocities. In his study of the velocity 
distributions of molecules in 3i3 Maxwell assumed that in every Cartesian 
coordinate system the three components of the velocity are mutually 
independent random variables with zero expectation. Applied to rotations 
leaving one axis fixed our corollary shows immediately that the three com- 
ponents are normally distributed with the same variance. As we saw in 
JI,3 this implies the Maxwell distribution for velocities. > 
III.4 CHARACTERIZATION OF THE NORMAL DISTRIBUTION 79 
The theorem has a long history going back to Maxwell's investigations. Purely prob- 
abilistic studies were initiated by M. Kac A940) and S. Bernstein A941), who proved 
our corollary assuming finite variances. An impressive number of authors contributed 
improvements and variants, sometimes by rather deep methods. The development 
culminates in a result proved by V. P. SkitoviC.8 
Now to the proof in the case of continuous densities. We denote the 
densities of X, and Y, respectively by w,- and ff. For abbreviation we put 
D.3) yx = a11z1 + a12x2, y2 = a21x1 
Under the conditions of the theorem we must have 
D.4) fMAb/i) = «i(*i) u2(x2). 
We shall show that this relation implies that 
D.5; ffy/) = ±e^lv\ «,(*) = ± 
where the exponents are polynomials of degree 2 or lower. The only 
probability densities of this fcrm are the normal densities. For distributions 
with continuous densities the theorem is therefore contained in the following 
Lemma. Suppose that four continuous, functions f and Uj are connected 
by the functional equation D.4), and that no coefficient ajk vanishes. The 
functions are then of the form D.5) where the exponents are polynomials of 
degree <,!. 
(It is, of course, assumed that none of the functions vanishes identically.) 
Proof. We note first that none of our functions can have a zero. Indeed, 
otherwise there would exist a domain Q in the xlf x2-plane in which the 
two members of D.4) have no zeros and on whose boundary they vanish. 
But the two sides require on the one hand that the boundary consists of 
segments parallel to the axes, on the other hand of segments parallel to the 
lines yi — const. This contradiction shows that no such boundary exists. 
We may therefore assume our functions to be strictly positive. Passing to 
logarithms we can rewrite D.4) in the form 
D.6) 9?! (ft) + cp2{y2) = (Oifo) + co2{x2). 
For fixed hx and h2 define the mixed difference operator A by 
D.7) Av(xiy x2) = u(*i+^i> x2+h2) — vfa+hi, x2—h2) — 
— v(x1—h1, x2+h2) + v(xx — hlf x2—h2). 
8 Izvestia Acad. Nauk SSSR, vol. 18 A954) pp. 185-200. The theorem: Let Xx,..., 
be mutually independent, Yx = Sa^., and Y2 = 26,-X,. where no coefficient is 0. If 
and Y2 are independent the Xi are normally distributed. 
80 DENSITIES IN HIGHER DIMENSIONS III.5 
Because each ojj depends on the single variable x, it follows that Aco, = 0. 
Also 
D.8) 
where we put for abbreviation 
D.9) tx = a-Ji-L + a12h2, i2 = a^ — a12h2. 
We have thus A 9^ + Acp2 = 0 with <p}- depending on the single variable 
y,-. Keeping y2 fixed one sees that A9?1(y1) is a constant depending only on 
hx and h2. We now choose hx and h2 so that tx = t an4 t2 = 0, where 
/ is arbitrary, but fixed. The relation A^ = const, then takes on the form 
D.10) ¦ vAVi+t) + 9>iB/i-/) - 2^B/0 = /!(/). 
Near a point y1 at which 9?! assumes a minimum the left side is >0, and 
hence such a point can exist only if A(t) > 0 fof all / in some neighborhood 
of the origin. But in this case <px cannot assume a" maximum. Now a 
continuous function vanishing at three points assumes both a maximum and a 
minimum. We conclude that if a continuous solution of D.10) vanishes at 
three distinct points, then it is identically zero. 
Every quadratic polynomial qiy-,) = a.y\ + $yx + y satisfies an equation 
of the form D.10) (with a different right side), and hence the same is true of 
the difference y^yj — q(yi). But q can be chosen such that this difference 
vanishes at three prescribed points, and then (piiy^ is identical with q. 
The same argument applies to <p2, and this proves the assertion concerning 
yi and f2. Since the variables X3 and Y3 play the same role, the same 
argument applies to the densities u}. > 
5. MATRIX NOTATION. THE COVAJRIANCE MATRIX 
The notation employed in section 1 is messy and becomes more so in 
higher dimensions. Elegance and economy of thought may be achieved by 
the use of matrix notation. 
For ease of reference we summarize the few facts of matrix theory and the notations 
used in the sequel. The basic rule is: first rows, then columns. Thus an a by ^ matrix A 
has a rows and /? columns; its elements are denoted by ajk, the first index indicating the 
row. If B is a fibyy matrix with elements bjk the product AB is the abyy matrix with 
elements a^by. + a-2b2k + • • • + ajpbpk. No product is defined if the number of columns 
of A does not agree with the number of rows of B. The associative law (AB)C = A(BC) 
holds, whereas in general AB ?* BA. The transpose A? is the <3 by a matrix with elements 
oT = akt. Obviously (AB?) = B^A?. 
A one by a matrix with a single row is called a row vector; a matrix with a single 
column, a column vector.9 A row vector r = (rv .. ., ra) is easily printed, but a column 
9 This is really an abuse of language. In a concrete case z1 may represent pounds and 
x2 cows: then (xltx2) is no "vector" in the strict sense. 
HI.5 MATRIX NOTATION. THE COVARIANCE MATRIX 81 
vector is better indicated by its transpose cT = (cx,. . ., ca). Note that cr is an a by a 
matrix (of the "multiplication table" type) whereas re is a one by one matrix, or scalar. 
In the case a = 2 
re = 
The zero vector has all components equal to 0. 
Matrices with the same number of rows and columns are called square matrices. With 
a square matrix A there is associated its determinant, a number which will be denoted by 
\A\. For our purposes it suffices to know that the determinants are multiplicative: if A 
and B are square matrices and C = AB, then \C\ =\A\ -\B\. The transpose AT has 
the same determinant as A. 
By identity matrix is meant a square matrix with ones in the main diagonal and zeros 
at all other places. If / is the identity matrix with r rows and columns and A an r by r 
matrix, obviously IA = AI = A. By inverse of A is meant a matrix A'1 such that 
AA~X = A~XA = I. [Only square matrices can have inverses. The inverse is unique, for 
if B is any inverse of A we have AB = I and by the associative law A-1 = (A~1A)B = B.] 
A square matrix without inverse is called singular. The multiplicative property of deter- 
minants implies that a matrix with zero determinant is singular. The converse is also true 
if \A\ 9*0 then A is non-singular. In other words, a matrix A is singular iff there exists 
a non-zero vector x such that xA = 0. 
A square matrix A is symmetric if aik = aki, that is, if AT = A. The quadratic form 
associated with a symmetric r by r matrix A is defined by 
where xv . , ., xT are indeterminates. The matrix is positive definite if xAxT > 0 for ail 
non-zero vectors x. It follows from the last criterion that a positive definite matrix is non- 
singuiar. 
Rotations in 4la. For completeness we mention briefly a geometric application of matrix 
calculus although it will not be used in the sequel. 
The inner product of two row vectors x = (xv .... xa) and y = (yv . . . , va) is defined 
by 
The length L of x is given by L* = xxT. If x and y are vectors of unit length the angle 
6 between them is given by cos d = xyT. 
An a by a matrix A induces a transformation mapping x into if — xA; for the 
transpose one has $T = ATxT. The matrix A is orthogonal if the induced transformation 
preserves lengths and angles, that is to say, if any two rov^ vectors have the same inner 
product as their images: Thus A is orthogonal iff for any pair of row vectors x, y 
x.4ATyT = xyT. 
This implies that AAT is the identity matrix / as can be seen by choosing for x and y 
vectors with a — 1 vanishing components. We have thus found that A is orthogonal iff 
AAT = I. Since A and AT have the same determinant it follows that it equals +1 or 
— I. An orthogonal matrix with determinant I is called a rotation matrix and the induced 
transformation is a rotation. 
82 DENSITIES IN HIGHER DIMENSIONS III.5 
From now on we denote a point of the r-dimensional space 3ir by a 
single letter to be interpreted as a row vector. Thus x = (xx,. . . , xr) and 
f(x) =/0ri> • • • » xr)> etc- Inequalities are to be interpreted coordinatewise: 
x < y iff xk < yk for k = 1, . . . , r and similarly for other inequalities. 
In the plane 3t2 the relation x < y may be read as "a: lies southwest of 
y." A novel feature of this notation is that two points need not stand in 
either of the relations x <, y or y < x, that is, in higher dimensions the 
inequality < introduces only a partial ordering. 
We write X = (X1}. . . , X,) for the row vector of the coordinate variables 
and use this notation for random variables in general (mainly for normally 
distributed variables). 
If the variables Xlt. . . , Xr have expectations E(Xy) we write E(X) for 
the row vector with components E(X^). The vector X — E(X) has zero 
expectation. More generally, if M is a matrix whose elements MiJc are 
random variables we write E(M) for the matrix of elements E(M^) 
assuming that it exists. 
Definition. If E(X) — 0 the covariance matrix Var (X) of X is the sym- 
metric r by r matrix with elements E(X^Xfc) {provided they all exist). In 
other words 
E.1) Var (X) = E(XrX). 
For arbitrary X we define Var (X) to be the same as Var (X—E(X)). 
The use of row vectors necessitates writing a linear transformation from 
ftr to %m in the form 
E.2) Y 
that is, 
E.3) y* 
where A is an r by m matrix. Obviously E(Y) = E(X)A whenever E(X) 
exists. To find the variances we assume without loss of generality E(X) = 0. 
Then E(Y) = 0 and 
E.4) E(YrY) = E(ATXTXA) = 
We thus have the important result that 
E.5) Var(Y) = /4rVa 
Of particular interest is the special case m — 1 when 
E.6) Y = aA + • • • + a,Xr 
III.6 NORMAL DENSITIES AND DISTRIBUTIONS 83 
is an ordinary random variable. Here Var (Y) is the (scalar) quadratic form 
E.7) Var (Y) = 
The linear form E.6) vanishes with probability one if Var (Y) = 0 and 
in this case every region outside the hyperplane J akxk — 0 carries zero 
probability. The probability distribution is then concentrated on an 
(r—l)-dimensional manifold and is degenerate when considered in r 
dimensions. We have now proved that the covariance matrix of any non- 
degenerate probability distribution is positive definite. Conversely, every 
such matrix may serve as covariance matrix of a normal density (see theorem 
4 of the next section). 
6. NORMAL DENSITIES AND DISTRIBUTIONS 
Throughout this section Q stands for a symmetric r by r matrix, and 
q(x) for the associated quadratic iform 
F.1) . q(x) = 2 4i*xixk = xQxT 
where x = (xl7. . . , xr) is a row vector. Densities in 3lT defined by an 
exponential with a quadratic form in the exponent are a natural counterpart 
of the normal density on the line, and we start therefore from the following 
Definition. A density <p in r dimensions is called normal1® and centered at 
the origin of it is of the form 
F.2) <p(x) = y • e-iQix) 
where y is a constant. A normal density centered at a — {ax, a2, . . . ,ar) 
is given by cp(x—a). 
The special case of two dimensions was discussed in examples 
and 2(a). 
We take 'JiT with the probability distribution of F.2) as sample space 
and denote by X = (X2, . . . , Xr) the row vector formed by the coordinate 
variables. Its covariance matrix will be denoted by M: 
F.3) M = Var (X) = E(XTX). 
Our problem consists in investigating the nature of the matrices Q and M\ 
and the relationship between them. 
First we observe that no diagonal element of Q can vanish. Indeed, if we 
had qn = 0, then for fixed values of arlf . . . , xr_x the density F.2) would 
10 "Degenerate" normal distributions will be introduced at the end of this section. 
84 DENSITIES IN HIGHER DIMENSIONS II 1.6 
take on the form y-^e~aXr+b and the integral with respect to xr would 
diverge. We now introduce the substitution y — xA defined by 
F.4) y1 = xu . . . , y^_x = xT_x, yT = qlfXl + • • • + qT1xr. 
It is seen by inspection that q{x) — y^qTT is a quadratic form in zlt . . . , zr_1 
not involving xT. Thus 
F.5) q{*) = — yl+q{y). 
<\tt 
where q{y) is a quadratic form in ylt . . . , yr_v This shows that the vector 
Y = XA has a normal density that factors into two normal densities for Yr 
and (Ylf . . . , Yr_2), respectively. The first conclusion to be drawn is the 
simple but important 
Theorem 1. All marginal densities of a normal density are again normal. 
Less expected is 
Theorem 2. There exists a matrix C with positive determinant such that 
Z = XC is a row vector whose components rLj are mutually independent 
normal variables. 
The matrix C is not unique; in fact, the theorem can be strengthened 
to the effect that C can be chosen as a rotation matrix (see problem 19). 
Proof. We proceed by induction. When r = 2 the assertion is contained 
in the factorization F.5). If the theorem is true in r — 1 dimensions, the 
variables Yl9 . . . , Yr-1 are linear combinations of independent normal 
variables Z1? . . . , Zr^2 while Yr itself is normal and independent of the 
remaining variables. Since X = Y^ it follows also that the X}- are 
linear combinations of Z2, . . . , Zr_2 and Yr. The determinant of A 
equals q^, and F.5) implies that it is positive. The determinant of the 
transformation X —> Z is the product of the determinants of A and the 
transformation Y —> Z and hence it is positive. > 
Theorem 3. The matrices Q and M are inverses of each other and 
F.6) y2 = Btt)' • \M\ 
where \M\ = \Q\-1 is the determinant of M. 
Proof. With the notations of the preceding theorem put 
F.7) D = E(ZrZ) = CTMC. 
This is a matrix with diagonal elements E(Zp = or? and zero elements 
outside the diagonal. The density of Z is the product of normal densities 
n(zajl)a~1 and hence induced by the matrix D~l with diagonal elements 
III.6 NORMAL DENSITIES AND DISTRIBUTIONS 85 
o\~2. Now the density of Z is obtained from the density F.2) of X by 
the substitution x = zC~* and multiplication by the determinant \C~X\. 
Accordingly 
F.8) zD~lzT = xQxT 
and 
F.9) Bir)'|0|=/-|C|2. 
From F.8) it is seen that 
F.10) Q = CD~1CT, 
and in view of F.7) this implies Q = A/. From F.7) it follows also that 
\D\ = \M\ • \C\2-, and hence F.9) is equivalent to F.6). > 
The theorem implies in particular that a factorization of M corresponds 
to an analogous factorization of Q and hence we have the 
Corollary. If (Xl5 X2) is normally distributed then Xx and X2 are indepen- 
dent iff Cov (Xj, X2) = 0, that is, iff Xx and X2 are uncorrelated. 
More generally, if (Xl5 . . . , Xr) has a normal density then (Xl5 . . . , Xn) 
and (Xn+1, . . . „ Xr) are independent iff Cov (X,, Xk) = 0 for j < n, k > n. 
Warning. The corollary depends on the joint density of (Xl5 X2) being 
normal and does not apply if it is only known that the marginal densities of 
Xi and X2 are normal. In the latter case the density of (Xl5 X2) need not 
be normal and, in fact, need not exist. This fact is frequently misunderstood 
(see problems 2-3). 
Theorem 4. A matrix M is the covariance matrix of a normal density iff 
it is positive definite. 
Since the density is induced by the matrix Q = A/ an equivalent 
formulation is: A matrix Q induces a normal density F.2) iff it is positive 
definite. 
Proof. We saw at the end of section 5 that every covariance matrix of 
a density is positive definite. The converse is trivial when r = 1 and we 
proceed by induction. Assume Q positive definite. For xx = ¦ • • = 
= xr_x = 0 we get q(x) = qrrx2r and hence qrr > 0. Under this hypothesis 
we saw that q may be reduced to the form F.5). Choosing xr such that 
yr = 0 we see that the positive defihiteness of Q implies q(x) > 0 for all 
choices of xlf . . . ,xr_v By the induction hypothesis therefore q corre- 
sponds to a normal density in r — 1 dimensions. From F.5) it is now obvious 
that q corresponds to a normal density in r dimensions, and this completes 
the proof. > 
86 DENSITIFS IN HIGHER DIMENSIONS III.6 
We conclude this general theory by an interpretatio.n of F.5) in terms of 
conditional densities which leads to a general formulation of the regression 
theory explained for the two-dimensional case in example 2{a). 
Put for abbreviation ak = —q^fq^, so that 
Fqr a probabiliscic interpretation of the coefficients ak we recall that Yr 
was found to be independent of Xl5 . . . , X^. In other words, the ak are 
numbers such that 
F.12) T = Xr-alX1 ^-iX^ 
is independent of (Xl5 . . . , Xr_x), and this property uniquely characterize* 
the coefficients ak. 
r To obtain the conditional density of Xr for given Xx = xx'. . . , XT_1 = 
= xr_x we must divide the density of (X1? . . . , Xr) by the marginal density for 
(Xl5 . . . , Xr_x). In view of F.5) we get an exponential with exponent 
— \y\\qrr. It follows that the conditional density of X,. for given Xx = 
= xx, . . . , Xr_! = xr_x is normal with expectation axxx + • * * + a^x^ 
and variance l/q^. Accordingly 
F.13) E(X, | Xlf . . . , X^) « axXx + • • • + a 
We have thus proved the following generalization of the two-dimensional 
regression theory embodied in B.6). 
Theorem 5. If (Xx.,. . . , Xr) has a normal density, the conditional density 
of Xr for given Xx, . . . , Xr-1 is again normal. Furthermore, the conditional 
expectation F.13) is the unique linear function of Xx, . . . , Xr_x making T 
independent of (Xx,. . . , X,^). The conditional variance equals Var (T) = 
Example. Sample mean and variance. In statistics the random variables 
F.14) x = i(Xx+- • -+X,), a2 = - i(Xfc-xJ 
r rfc-i 
are called the sample mean and sample variance of X = (Xl5. . . , X,.). 
It is a curious fact that if Xx, . . . , X, are independent normal variables with 
E(Xfc) = 0, E(X?) = or2, the random variables X and d2 are independent.11 
The proof illustrates the applicability of the preceding results. We put 
Yk = Xk — X for k < r— 1 but Yr = X. The transformation from X 
to Y = (Yx, . . . , Yr) being linear and non-sirrgular, Y has a normal 
density. Now E(YfcYr) = 0 for k <, r—l and so Yr is independent of 
11 That this fact characterizes the normal distribution in R1 was shown by R. C. Geary 
and by E. Lukacs. 
111.7 STATIONARY NORMAL PROCESSES 87 
(Ylf . . . , Y^)- But 
F.15) rd2 = Y2 + • • • + Yr2_x + (Yx+- • •+Yr_1J 
depends only on Yl5 . . . , Yr_l5 and thus &2 is indeed independent of 
Yr = X > 
General Normal Distributions 
It follows from the lemma that if X = (Xl5. . . , Xr) has a normal 
density, every non-zero linear combination Yx = axXx + • • • + arXr also 
has a normal density. The same is true of every pair (Yl5 Y2) provided 
that no linear relationship cxYx + c2Y2 = 0 holds. In this exceptional 
case the probability distribution of (Yl5 Y2) is concentrated on the line 
with the equation cxyx + czy2 = 0 and hence it is singular // viewed as a 
two-dimensional distribution. For many purposes it is desirable to preserve 
the term normal distribution also for degenerate distributions concentrated 
on a lower-dimensional manifold, say on a particular axis. The simplest 
general definition is as follows: The distribution of Y = {Yx, . . . , Yp) is 
normal if there exists a vector X = (Xl5 . . . , Xr) with normal r-dimensional 
density such that Y = a + XA where A is a (constant) r by p matrix and 
a = (ax, . . . , ap). If p > r the distribution of Y is degenerate in p 
dimensions. For p < r it is non-degenerate iff the p forms defining Yk 
are linearly independent 
*7. STATIONARY NORMAL PROCESSES 
The purpose of this section is partly to supply examples of normal distri- 
butions, partly to derive some relations of considerable use in the theory of 
discrete stochastic processes and time series. They are of an analytic character 
and easily separaled from the deeper stochastic analysis. In fact, we shall 
be concerned only with finite-dimensional normal densities or, what amounts 
to the same, their covariance matrices- The reference to random variables is 
essential for probabilistic intuition and as a preparation for applications, but 
at the present stage we are concerned only with their joint distributions; 
the random variables themselves are used merely as a convenient way of 
describing all marginal densities by indicating the corresponding collections 
(X ,. ., , Xafc). By the same token a reference to an infinite sequence 
{Xfc} implies merely that the number of terms in (Xx, . . . , Xn) may be 
taken arbitrarily large. 
We shall, in fact, consider a doubly infinite sequence {. . . , X_2, X_x, . . .}. 
By this we mean simply that corresponding to each finite collection 
* Not used in the sequel. In particular, section 8 can be read independently. (See 
also XIX,8.) 
88 DENSITIES IN HIGHER DIMENSIONS III.7 
(Xn , . . . , X,,r) we are given a normal density with the obvious consistency 
rules. The sequence is stationary if these distributions are invariant under 
time shifts, that is, if all r-tuples of the form (Xn +v, . . . , Xnr+V) with 
fixed «i,..., nr have a common distribution independent of v. For 
r = 1 this implies that the expectations and variances are constant, and 
hence there is no loss in generality in assuming that E(Xn) = 0. The joint 
distributions are completely determined by the covariances pjk'= E(X>Xfc), 
and the stationarity requires that pjk depends only on the difference \k — j\. 
Accordingly we put pjJ+n = rn. Thus 
G.1) rn = E(XfcXfc+n) = E(X^Xfc), 
whence rn = r_n. In effect we are dealing only with sequences of numbers 
rn that can serve as covariances for a stationary process. 
Throughout this section {Zn} stands for a doubly infinite sequence of 
mutually independent normal variables normed by 
G.2) E(Z7i) = 0, E(Z*) = l. 
Three methods of constructing stationary sequences in terms of a given 
sequence {Zn} will be described. They are in constant use in time series 
analysis and may serve as an exercise in routine manipulations. 
Examples, (a) Generalized moving average processes. With arbitrary 
constants b0, bx, . . . , b^ put 
G.3) Xn = b0Zn + bJLn_x + •¦-+ bNZn_N. 
In the special case of equal coefficients bk= l/(iV+l) the variable Xn 
is an arithmetic average of the type used in time series analysis to "smooth 
data" (that is, to eliminate local irregularities). In the general case G.3) 
represents a linear operator taking the stationary sequence {Zn} into a 
new stationary sequence {Xn}. The fashionable term for such operations is 
"filters." The sequence {Xn} has covariances 
G.4) rk = r_k = E(XnXn+k) = ? bvbv+k (k > 0) 
the serieS having finitely many terms only. 
Since 2 \bvbv+k\ < b\ + b*+k the expression G.4) makes sense also for 
infinite sequences such that ]? b\ < oo. It is easily seen that the limit of 
a sequence of covariance matrices is again a covariance matrix and, letting 
N-+co, we conclude that for any sequence b0, bx, b2, . . . such that 
^b\ < oq the numbers rk of G A) may serve as covariances of a stationary 
process {Xn}. Formally we get for the new process- 
G.5) Xn = f 
111.7 STATIONARY NORMAL PROCESSES 89 
It can be shown without difficulty that every stationary process with co- 
variances G.4) is of this form,' but the relation G.5) involves infinitely many 
coordinates and we cannot justify it at present. (See XIX,8.) 
. (b) The auto-regression process. Since the inception of time series analysis 
various theoretical models have been proposed to explain empirical phe- 
nomena such as.economic time series, sunspots, and observed (or imagined) 
periodicities. The most popular model assumes that the variables Xn of 
the process are related to our sequence Zn of independent normal variables 
of G.2) by an auto-regression equation of the form 
G.6) floXn + a^X^y + • • • + aNXn_N = Zn. 
This model is based on the empirical assumption that the value of the 
variable Xn at epoch n (price, supply, or intensity) depends on its paM 
development superimposed on a "random disturbance',' Zn which is not 
related to the past. As is frequently the case, the assumption of linear 
dependence serves to simplify (or make possible) a theoretical analysis. 
More general models may be obtained by letting N—>¦ oo or by letting the 
Zn be the variables of another stationary process. 
If a0 ?? 0 one may chosse (Xo, . . . , X^^) in an arbitrary way and then 
calculate XN, XN+1,. . . and X_l5 X_2,. . . recursively. In this sense G.6) 
determines a process, but we ask whether there exists a stationary solution. 
To answer this question we rewrite G.6) in a form not involving the 
immediate predecessors of Xn. Consider G.6) with n replaced successively 
by n — 1, n—2, . . . , n—v. Multiply these equations by blf b2, . . . , bv, 
respectively, and add to G.6). The variables Xn_l5 .. . , Xn_v will not 
appear in the new equation iff the b} are such that 
G.7) aobx + a-J}^ = 0, . . . , ao6v + axbv_x + • • • + avb0 = 0 
with b0 = \. The resulting identity is then of the form 
G.8) a0Xn =>b0Zn + ^1Zn_1 + • ¦ • + Z>vZn_; + Yn,v 
where Ynv is a linear combination of Xn_^.u . . . , Xn_N_v (with co- 
efficients that are of no interest). In G.8) we have expressed the variable 
Xn as a resultant of the chance contributions at epochs n, n—\, . . . , n—v 
and a variable Yn v representing the influence of the time before epoch 
n — v. As v -> oo this time becomes the "infinitely remote past" and 
in most situations it will have no influence. In passing to the limit we shall 
(at least temporarily) assume this to be the case, that is, we are looking 
for a process satisfying a limiting relation of the form 
G.9) a0Xn = | bkZn_k.. 
Jfc-0 
90 DENSITIES IN HIGHER DIMENSIONS III.7 
[Roughly speaking we assume that the residual variables Yn v tend to zero. 
Other possible limits will be indicated in example (d).] 
Processes of the form G.9) are the object of example (a) and we saw 
that a stationary solution exists whenever ]? b\ < oo. (If the series diverges, 
not even the expressions for the covariances make sense.) To solve the 
equations G.7) for bk we use the formal generating functions 
G.10) A(s) = J aks\ B(s) = J bksk. 
The equations G.7) hold iff A(s) B(s) = aobo and, A being a polynomial, 
B is rational. We can therefore use the theory of partial fractions developed 
in 1; XI,4. If the polynomial A(s) has distinct roots su . . . , sN we get 
A-\ Am 
G.11). B(s) = — + • • • + v 
and hence 
G.12) bn = 
n—1 
Obviously ^ b\ < oo iff all roots satisfy \s)\ > I, and it is easily verified 
that this remains true also in the presence of multiple roots. We have 
thus shown that a stationary solution of the auto regression model G.6) 
exists whenever all roots of the polynomial A{s) lie outside the unit disk. 
The covariances of our process are given by G.4) and in the process the 
"infinitely remote past" plays no role. 
Our solution {Xn} of the auto-regression equation G.6) is unique. Indeed, 
the difference of two solutions would satisfy the homogeneous equation 
G.13) and we shall now show that the condition \st\ > I precludes the 
existence of a probabilistically meaningful solution of this equation. 
(c) Degenerate processes. We turn to stationary sequences {Yn} satisfying 
the stochastic difference equation 
G.13) a0Yn + ^Y,^ + • • • + aNYn_N = 0. 
They represent an interesting counterpart to the auto-regression processes 
governed by G.6). Typical examples are 
G.14) Yn = A(ZX cos nco + Z_x sin na>) 
and 
G.15) Yn = a1Z1+(-ira2Z_1 
where the coefficients and co are constants, and Zx and Z_x independent 
normal variables normed by G.2). These processes satisfy G.13), the first 
with a0 = a2 = 1 and ax = — 2 cos co, the second with a0 = —a2= 1 
and ax = 0. They are degenerate in the sense that the whole process is 
III.7 STATIONARY NORMAL PROCESSES 91 
completely determined by two observations, say Yfc_! and Yk. These two 
observations can be taken as far back in the past as we please, and in this 
sense the process is completely determined by its "infinitely remote past." 
The same remark applies to any process satisfying a difference equation 
of the form G.13), and hence these processes form the counterpart to 
example (b) where the infinitely remote past had no influence at all. > 
These examples explain the general interest attaching to the stochastic 
difference equation G.13). Before passing to its theqry we observe that any 
process {Yn} satisfying G.13) satisfies also various difference equations of 
higher order, for example 
a0Yn + (a1-a0)Yn_1 H + {ax-a^Y^ - aNYn_N_v. 
To render the problem meaningful we must suppose that G.13) represents 
the difference equation of lowest order satisfied by {Yn}. This amounts to 
saying that the //-tuple (Yl5.. . , YAr) is non-degenerate with a normal 
density in N dimensions. It implies that ao?? 0 and aN ^ 0. 
It will now be shown that the theory of stationary solutions of the difference 
equation G.13) is intimately related to the "characteristic equation" 
G.16) ao|A' + axf~r + • • • + a* = 0. 
To each quadratic factor of the polynomial on the left there corresponds 
a second-order stochastic difference equation, and through it a process of 
the form G.14) or G.15). Corresponding to the factorization of the character- 
istic polynomial we shall thus represent the general solution of G.13) as a 
sum of components of the form G.14) and G.15). 
As before we assume the centering E(Yn) = 0. The whole theory depends 
on the following 
Lemma 1. A stationary sequence with E(YnYn+Jfc) = rk satisfies the 
stochastic difference equation G.13) iff 
G.17) atfn + a^.x H + aNrn_N = 0. 
Proof. Multiplying G.13) by Yo and taking expectations leads to G.17). 
Squaring the left side in G.13) and taking expectations yields ]? a,(]? akrk_5), 
and so G.17) implies that the left side in G.13) has zero variance. This 
proves the lemma. > 
We proceed to derive a canonical forir for rn. It is, of course, real, but it 
involves the roots of the characteristic equation G.16), and we must therefore 
resort to a temporary use of complex numberc. 
Lemma 2. If {Yn} satisfies G.13), but no difference equation of lower 
order, then the characteristic equation G.16) possesses N distinct roots 
92 DENSITIES IN HIGHER DIMENSIONS III.7 
fi, . • • . ?a' of unit modulus. In this case 
G.18) '» = *i?"+ "• + <*?# 
»»-/7/i Cj>6 for j=\,...,N. 
Proof. Suppose first that the characteristic equation G.16) has N distinct 
roots flt .. . , ?iV. We solve G.17) by the method of particular solutions 
which was used for similar purposes in volume 1. Inspection shows that 
G.18) represents a formal solution depending on N free parameters 
Ci, . . . , cN. Now the rn are completely determined by the N values 
rl5 . . . , rN, and to show that every solution of G.17) is of the form G.18) 
it suffices therefore to show that the Cj can be chosen so that the relations 
G.18) yield prescribed values for rlt . . . , rN. This means that the cj must 
satisfy N linear equations whose matrix A has elements ajk = ?3k (j, 
k = I, . . . , N). The determinant of A does not vanish,12 and hence the 
desired solution exists. 
We have thus established that (in the case of distinct roots) rn is indeed 
of the form G.18). Next we show that only roots of unit modulus can 
effectively appear in it. We know that aN ^ 0, and hence 0 is not a root of 
the characteristic equation. Next we note that the covariances rn are 
bounded by the common variance r0 of the Yn. But if gt is not of unit 
modulus then |?,r->°o either as n->oo or as n —* — oo. It follows 
that for each j either |?,| = 1 or else cj = 0. 
Suppose now that ?x and ?2 are a pair of conjugate roots and cx ^ 0. 
Then t-x is of unit modulus and hence ?2 = I. The symmetry relation 
rn = r_n therefore requires that c2 = cx. Again, ?* + ?? is real, and there- 
fore cx must be real. Thus the complex roots appear in G.18) in conjugate 
pairs with real coefficients, and if some coefficient c,- vanished, rn would 
satisfy a difference equation of order less than N. Accordingly all roots 
are of unit modulus, all cj are real and c,- 5^ 0. 
To show that the c, are actually positive we introduce the covariance 
matrix R of (Yl5 . . . , YN). Its elements are given by r,-_fc, and it is 
easily verified from G.18) that 
G.19) R = ACAT 
where C is the diagonal matrix with elements c,, A is the matrix introduced 
12 The determinant is usually called after Vandermonde. To show that it does not vanish 
replace ?, by a free variable x. Inspection then shows that the determinant is of the form 
xP{x) where P is a polynomial of degree N — 1. Now P(x) = 0 for x = ?2,.. ., ?n 
because for these values of x two columns of the determinant become identical. The 
determinant can therefore not vanish for any other value of x, and in particular not for 
Ill .7 STATIONARY NORMAL PROCESSES 93 
above, and A is its conjugate (that is, it is obtained from A by replacing 
f, by ?7l). Now R is real and positive definite, and therefore for any 
complex jV-dimensional non-zero row vector x = u + iv 
G.20) xRxT =vRuT+ vRvT > 0. 
Letting y = xA this reduces to 
G-21) T* 
Since the determinant of A does not vanish this inequality holds for 
arbitrary y, and thus ct > 0 as asserted. 
To complete the proof we have to show that the characteristic equation 
can not have multiple roots. Assume that gx = ?2 but the other roots are 
distinct. We get again a representation of the form G.18) except that the 
term cx^, is replaced by cxn^. The boundedness of rn again necessitates 
that cx = 0. In the case of one double root we would therefore get a 
representation of the form G.18) with fewer than N non-zero terms, and we 
have seen that this is impossible. The same argument shows more generally 
that no multiple roots are possible. >> 
We now state the final result for the case that N is an odd integer. The 
modifications required for even N should be obvious. 
Theorem. Suppose that the stationary sequence {Yn} satisfies the differ- 
ence equation G.13) with N = 2v + 1, but no difference equation of lower 
order. The characteristic equation G.16) possesses v pairs of complex roots 
?] = cos oij ± /sin oij {with coj real), and one real root co0 = ±1. The 
sequence {Yn} is of the form 
G.22) Yn = A0Z0 • col + 2A;fz; cos no>j + Z_,- sin 
where the Z,- are mutually independent normal variables with zero expectations 
and unit variances, and the Xj are constants. For this sequence 
G.23) rn = Ageo? + ? X) cos nw,, 
i=l 
Conversely, choose real Av ~? 0 arbitrary and co0 = ± I, and let 
co1, . . . , oij be distinct real numbers with 0 < <wy < it. Then ij.22) defines a 
stationary process with covariances G.23) and satisfying a difference equation of 
order 2v + 1. but no difference equation of a lower order. 
Proof. Let the X,- and coj be numbers, and the Z, normal variables 
satisfying the conditions of the theorem. Define the variables Yn by G.22). 
A trite calculation shows that the covariances rn of {Yn} are given by 
94 DENSITIES IN HIGHER DIMENSIONS III.8 
G.23). There exists a real algebraic equation of the form G.16) with the 
roots I,- described in the theorem. The rj then satisfy the difference equation 
G.17), and by lemma 1 this implies that the Yn satisfy the stochastic 
difference equation G.13). By construction this is the equation of lowest 
degree satisfied by the Yn. 
Conversely, let {Yn} stand for the solution of a given difference equation 
G.13). The covariances rn of {Yn} determine the numbers A, and <w,- 
appearing in G.22). Consider these equations for n = 0, 1,. .. , 2v as a 
linear transformation of an arbitrary //-tuple of normal variables 
(Z_v, . . . , Zv) into (Yo, . .. , YN). This transformation is non-singular, 
and hence the covariance matrices of the two //-tuples determine each other 
uniquely. We have just shown that if the covariance matrix of the Z, 
reduces to the identity matrix the Yk will have the prescribed covariances 
rn. The converse is therefore also true, and so there exist normal variables 
Z, satisfying the conditions of the theorem and such that G.22) holds for 
n = 0,.. . , jV. But both sides of these equations represent solutions of the 
stochastic difference equation G.13), and since they agree for 0 < n < N 
they are necessarily identical. > 
8. MARKOVIAN NORMAL DENSITIES 
We turn to a discussion of the particular class of normal densities occurring 
in Markov processes. Without loss of generality we consider only densities 
centered at the origin. Then E(Xfc) = 0 and we use the usual abbreviations 
(8.1) E(Xj) = 4, E(X,Xfc) = aj(jkpik. 
The pjk are the correlation coefficients and pkk = 1. 
Definition. The r-dimensional normal density of (X1?. . . , Xr) is Marfco- 
vian if for k <, r the conditional density of Xk for given Xl5 . . . , Xk_x is 
identical with the conditional density of Xk for given X 
Roughly speaking, if we know Xfc_x (the "present") then the additional 
knowledge of the "past" Xl5.. . , Xfc_2 does not contribute any relevant 
information about the "future," that is, about a#y X, with j > k. 
As usual in similar situations, we apply the term Markovian interchangeably 
to (Xl5... ,Xr) and its density. 
Theorem 1. For (Xl5 . . . , X,) to be Markovian each of the following 
two conditions is necessary and sufficient: 
(i) For k <r 
(8.2) E(Xfc IX^..., Xk_x) = E(Xk 
III.8 MARKOVIAN NORMAL DENSITIES 95 
(ii) For j?v<,k?r 
(8-3) Pik = PivPvk- 
For (8.3) to hold it suffices that 
(8-4) Pik = pj.k-iPk-i.k, j <> k. 
Proof. Identity of densities implies equality of expectations and so (8.2) 
is trivially necessary. On the other hand, if (8.2) is true, theorem 5 of section 
6 shows that the conditional density of \k for given Xlf . . . , X^ depends 
only on X^j, but not on the preceding variables. Now the conditional 
density of Xk for given Xfc_x is obtained by integrating out the variables 
X1?. . . , Xt_2, and hence the two conditional densities are identical. Thus 
(8.2) is necessary and sufficient. 
Referring again to theorem 5 of section 6 it is clear that the variable 
(8,5) T = Xk-E(Xk\Xk_1) 
is identical with 
(8.6) T = Xfc P*-i,*Xjfc_i. 
because this is the only variable of the form Xk — cXk-1 uncorrelated to 
Xfc_i. By the same theorem therefore (8.2) holds iff T is uncorrelated also to 
Xl5 .. . , Xfc_2, that is, iff (8.4) holds. Thus (8.4) is necessary and sufficient. 
As it is a special case of (8.3) the latter condition is sufficient. It is also 
necessary, for repeated application of (8.4) shows that for j < v < k < r 
/o n, Pik _ Pi,kr-l Pi.k-i Piv 
\P-') — — — — Pi* 
Pvk Pv,k-1 Pv,k-2 Pvv 
and so (8.4) implies (8.3). > 
Corollary. If (Xl5 . . . , Xr) is Markovian, so is every subset (Xai, .... Xa) 
with ax < a2 < • • • < av < r. 
This is obvious since (8.3) automatically extends to all subsets. >> 
Examples, (a) Independent increments. A (finite or infinite) sequence 
{XjJ of normal random variables with E(Xfc) = 0 is said to be a process 
with independent increments if for j < k the increment Xk — X/ is 
independent of (Xlf. . . , Xy). This implies, in particular, E(Xj(Xk—Xj)) = 
= 0 or 
(8.8) Pik = - j < k. 
Comparing this with (8.3) one sees that a. normal process with independent 
96 DENSITIES IN HIGHER DIMENSIONS III.8 
increments is automatically Markovian. Its structure is rather trite: Xk is 
the sum of the k mutually independent normal variables 
t X-Ic-I- 
(b) Autoregressive models. Consider a normal Markovian sequence 
Xlt X2,. . . with E(Xk) = 0. There exists a unique constant ak making 
Xk — a^X^ independent.of X^, and hence of Xx,. . . , Xk_v Put 
and, recursively, 
(8.9) X" ' KZl 
Xfc — okXk_1 + /¦jtZJfc /c — 2, 3, 
The variables Zk thus defined are easily seen to be independent and 
(8.10) E(Zfc) = 0, E(Z*)=l. 
Now the converse is also true. If the Zk are normal and satisfy (8.10), 
then (8.9) defines a sequence {Xn} and the very structure of (8.9) shows 
that {Xn} is Markovian. As an exercise we verify it computationally. 
Multiply (8.9) by X,- and take expectations. As Zk is independent of 
Xi,. . . , Xk_x we get for j < k 
(8.11) ak = . 
tfjfc-i Pj,k-i 
Now (8.4) is a simple consequence of this, and we know that it implies 
the Markovian character of the Xk. Thus (Xx,. . . , Xr) is Markovian iff 
relations of the form (8.9) hold with normal variables Z,- satisfying (8.10). 
[This is a special case of example 7(b).] > 
So far we have considered only finite sequences (Xx, .. . , Xr), but the 
number r plays no role and we may as well speak of infinite sequences 
{Xn}. This does not involve infinite sequence spaces or any new theory, 
but is merely an indication that a distribution for (Xl5. . . , Xr) is defined 
for all r. Similarly, we speak of a Markovian family {X(t)} when any 
finite collection Xx = X(/x),. . . , X, = X(/r) is Markovian. The description 
depends on the functions 
(8.12) E(X2@) = <r2(t), E(XE) X@) = o(s) a(t) P(s, t). 
In view of the criterion (8.3) it is obvious that the family is Markovian iff 
for s < t < t 
(8.13) 
III.8 MARKOVIAN NORMAL pENSITIES 97 
Despite the fancy language we are really dealing only with families of 
finite-dimensional normal distributions with covariances satisfying (8.13). 
As explained in greater detail at the beginning of section 7, the sequence 
{Xn} is stationary if for each fixed w-tuple (ax, . . . , an) the distribution of 
(Xarf_v,. . . , XHn+v) is independent of v. A finite section of such a sequence 
may be extended to both sides, and hence it is natural to consider only 
doubly infinite sequences {. .. , X_2, X_x, Xo, Xx,. . . }. These notions 
carry over trivially to families (X(f)}. 
For a stationary sequence {X,,} the variance a\ is independent of n 
and in the Markovian case (8.3) implies that pik = p^2~^. Thus for a 
stationary Markovian sequence 
(8.14) E(XiXJfc) = cr2/I*"" 
where a2 and p are constants, |p| ^ 1. Conversely, a sequence with 
normal distributions satisfying (8.14) is Markovian and stationary. 
In the case of a stationary family (X(f)} the correlation p(s, t) depends 
only on the difference \t—s\ and (8.13) takes on the form 
p{t) p(r) = p{t+r) for t, t>0. 
Obviously p{r) = 0 would imply p(t) = 0 for all t > t and also 
p(\r) = 0, and so p can have no zeros except if p(t) = 0 for all t > 0. 
Hence p(t) = e~Xt by the repeatedly used result of 1; XVII, 6. Accordingly, 
for a stationary Markovian family 
(8-.15) E(X(s) XE+0) = <?e~x\ t > 0 
except if X(.s) and X(t) are uncorrelated for all s t* t. 
Example, (c) Stationary sequences may be constructed by the scheme 
of the last example. Because of (8.11) we must have 
(8.16) X, = pX^ + orVr^ Zk. 
For each k it is possible to express Xk as a linear combination of Zk, 
Zk_x, .. . , Zk_v, and Xk_v. A formal passage to the limit would lead to the 
representation 
(8.17) X, = 
j=0 
of {Xk} in terms of a doubly infinite sequence of independent normal 
variables Z, normed by (8.10). Since |p| < 1 the convergence of the series 
is plausible, but the formula as such involves an infinite sequence space. 
[See the remarks concerning G.5) of which (8.17) is a special case.] 
98 DENSITIES IN HIGHER DIMENSIONS III.8 
It may be useful to discuss the relation of theorem 1 to the direct description 
of Markovian sequences in terms of densities. Denote by gt the density 
of Xt- and by gik(x, y) the value at y of the conditional density of Xk 
given that Xt- = x. (In stochastic processes gik is called a transition density 
from Xt- to Xfc.) For normal Markovian sequences g{ is the normal density 
with zero expectation and variance a\. As for the transition probabilities, 
it was shown in example 2{a) that 
(8.18) fafry)- 
where n stands for the standard normal density. However, we shall not 
use this result and proceed to analyze the properties of gik by an independent 
method. As usual we interpret the subscripts as time parameters. 
The joint density of (Xt-, X,-) is given by ?,•(«) ?t-,-(s, y). The joint density 
for (Xt-, Xf, Xk) is the product of this with the conditional density for Xk 
for given X,- and Xt, but in view of the Markovian character the index / 
drops out if i <j <k and the density of (X4, X>, Xfc) is given by 
(8.19) gi(*)gu(**y)gikto,*)- 
In the Markovian case the density of every w-tuple (Xai, . . . , X^J is given 
by a product of the form (8.19), but the densities gik cannot be chosen 
arbitrarily. Indeed, integration of (8.19) with respect to y yields the 
marginal density for (Xt-, Xk) and so we have the consistency condition 
J»+oo 
gifr. y) gi*(y*z) 
— 00 
for all / < j < k. This is a special case of the Chapman-Kolmogorov identity 
for Markov processes.13 Very roughly, it expresses thai a transition from x 
at epoch i to z at epoch k takes place via an arbitrary intermediate position 
y, the transition from y to z being independent of the past. It is 
obvious that with any system of transition probabilities gik satisfying the 
Chapman-Kolmogorov identity the multiplication scheme (8.19) leads to a 
consistent system of densities for (X., X2, . . . , X,) and the sequence is 
Markovian. We have thus the following analytic counterpart to theorem 1. 
Theorem 2. A family {gik} can serve for transition densities in a normal 
Markovian process iff it Satisfies- the Chapman-Kolmogorov identity and 
gik(x, y) represents for each fixed x a normal density in y. 
13 Other special cases were encountered in 1; XV,A3.3) and XVII,(9.1). Note that the 
system (8.19) is the analogue to the definition 1; XV,fl.l) of probabilities for Markov 
chains, except that "there summation replaces the integration and that only stationary 
transition probabilities were considered. 
III.9 PROBLEMS FOR SOLUTION 99 
Both theorems contain necessary and sufficient conditions and they are 
therefore, in a sense, equivalent. They are, nevertheless, of different natures. 
The second is really not restricted to normal processes; applied to families 
{X@} it leads to differential and integral equations for the transition 
probabilities and in this way it serves to introduce new classes of Markovian 
processes. On the other hand, from theorem 2 one would not guess that the 
gik are necessarily of the form (8.18), a result implicit in the more special 
theorem 1. 
For reference and later comparisons we list here the two most important 
Markovian families (X(f)}. 
Example, (d) Brownian motion or Wiener-Bachelier process. It is defined 
by the condition that X@) = 0, and that for i > s the variable 
X@ — X(s) be independent of X(s) with a variance depending only on 
t — s. In other words, the process has independent increments [example 
(a)] and stationary transition probabilities [but it is not stationary since 
X@) = 0]. Obviously E(X2(f)) = <r2t and E(X(^)X@) = <r2s for s < t. 
For t > t the transition densities from (t, x) to (t, y) are normal with 
expectation x and variance o\t—t). They depend only on (y—x)[(T—t), 
and the Chapman-Kolmogorov identity reduces to a convolution. 
(e) Omstein-Vhlenbeckprocess. By this is meant the most general normal 
Stationary Markovian process with zero expectations. Its covariances are 
given by (8.15). In other words, for t > t the transition density from 
(t, x) to (t, y) is normal with expectation e~X{T~l)x and variance 
a2{\—e~iX(r'~i)). As t->oo the.expectation tends to 0 and the variance to 
<r2. This process was considered by Ornstein and Uhlenbeck from an entirely 
different point of view. Its connection with diffusion will be discussed 
in X,4. > 
9. PROBLEMS FOR SOLUTION 
1. Let O. be the region of the plane (of area \) bounded by the quadrilateral 
with vertices @, 0), A, 1), @, ?), (h !) and tne triangle with vertices (^, 0),-(l, 0), 
A, ^). (The unit square is the union of O and the region symmetric to Q with 
respect to the bisector.) Let (X, Y) be distributed uniformly in D. Prove that the 
marginal distributions are uniform and the X + Y has the same density as if 
X and Y were independent.14 
Hint: A diagram renders calculations unnecessary. 
2. Densities with normal marginal densities. Let u be an odd continuous function 
on the line, vanishing outside —1,1. If \u\ < B-7re)~h then 
n(x)n(y) + u(x)u(y) 
14 In other words, the distribution of a sum may be given by the convolution even if the 
variables are dependent. This intuitive example is due to H. E. Robbins. For another 
freak of the same type see H,4(c). 
100 DENSITIES IN HIGHER DIMENSIONS III.9 
represents a bivariate density which is not normal, but whose marginal densities are 
both normal. (E. Nelson.) 
3. A second example. Let <px and <p2 be two bivariate normal densities with 
unit variances but different correlation coefficients. The mixture ^(q>1 + q>2) is 
not normal, but its two marginal densities coincide with n. 
Note. In the sequel all random variables are in &1. Vector variables are indicated 
by pairs (Xx, >'O> etc- 
4. Let Xj,.. . , Xn be independent random variables with the common density 
/ and distribution function F. If X is the smallest and Y the largest among them, 
the joint density of the pair (X, Y) is given by 
n(n -1 )f(x)f(y)[F(y) -F{x)T~\ y > s. 
5. Show that the symmetric Cauchy distribution in Jl3 [defined in A.21)] 
corresponds to a random vector whose length has the density v(r) = 47r~1r2(l +r2)-2 
for r > 0. [Hint; Use polar coordinates and either A.15) or else the general 
relation 1,A0-4) for projections.] 
6. For the Cauchy distribution A.21) the conditional density of X3 for given 
and the bivariate conditional density of X2, X3 for given Xx = 
7. Let 0 < a < 1 and /(*, y) = [A +ax)(l +ay) - a]e-x-y-axy for x > 0, 
2/ > 0 and f(x, y) = 0 elsewhere. 
(ff) Prove that / is a density of a pair (X, Y). Find the marginal densities and 
the distribution function. 
(b) Find the conditional density ux(y) and E(Y | X), Var (Y | X). 
8. Let / be a density concentrated on 0, 00. Put u(x, y) = f(x +y)/(x +y) for 
x > 0, y > 0 and u(x, y) = 0 otherwise. Prove that u is a density in ft2 and 
find its covariance matrix. 
9. Let Xlf X2, X3 be mutually independent and distributed uniformly over 
0,1. Let XA), XB), XC) be the corresponding order statistics. Find the density of 
the pair 
/X X 
\XB) XC)j 
and show that the two ratios are independent. Generalize to n dimensions. 
10. Let Xlt X2, X3 be independent with a common exponential distribution. 
Find the density of (X2-Xlf X3-Xx). 
11. A particle of unit mass is split into two fragments with masses X and 
1 — X. The density / of X is concentrated on 0, 1 and for reasons of symmetry 
f(x)=f(l — x). Denote the smaller fragment by Xx the larger by X2. The two 
fragments are split independently in like manner resulting in four fragments with 
masses Xu, Xl2, X21, X22. Find (a) the density of Xn. (Jb) The joint density of Xn 
and X22. Use (Jb) to verify the result in (a). 
¦x 
II1-9% 1,;, PROBLEMS FOR SOLUTION 101 
12, 
I —" ' 
l~ 
cond 
13, 
given 
14 
*rf* 
r?- 
!, X2, ... be independent with the common normal density n, and 
+ Xfc. If m<n find the joint density of (Sm, Sn) and the 
for Sm given that Sn = /. 
preceding problem find the conditional density of Xf + • • • + X2 
• • + x*. 
, Y) have a bivariate normal density centered at the origin with 
E(X2) =E(Y2) = 1, and E(XY) = P. In polar coordinates (X, Y) becomes 
(R, *) where R2 = X2 + Y2. Prove that * has a density given by 
0 < tp < 2v 
2ttA —2p sin <f cos q>) 
and is uniformly distributed iff P = 0. Conclude 
P{XY > 0} = \ + 7T-1 arc sin P and P{XY < 0} = iTl arc cos P. 
15. Let / be the uniform density for the triangle with vertices @,0), @, 1), 
A, 0) and g the uniform density for the symmetric triangle in the third quadrant. 
Find /*/, and f*g. 
Warning. A tedious separate consideration of individual intervals is required. 
16. Let / be the uniform density in the unit disk. Find /* / in polar coordinates. 
17. Let u and v be densities in ft2 of the form 
u(x, y) =f(v/x2+ y2), v(x, y) =g(Vz2 +,y2). 
Find u * v in polar coordinates. 
18. Let X = (Xx,. . ., Xr) have a normal density in r dimensions. There 
exists a unit vector a = (alf. .. , ar) such that 
Var faX! + • • • +aJLr) > Var (c^ + • ¦ ¦ +crXr) 
for all unit vectors c = (clt..., cr). If a = A, 0,. .., 0) is such a vector then 
Xx is independent of the remaining X,-. 
19. Prove the 
Theorem. Given a normal density in &r the coordinate axes can be rotated in 
such a way that the new coordinate variables are mutually independent normal 
variables. 
In other words: in theorem 2 of section 6 the matrix C may be taken as a 
rotation matrix. 
Hint: Let Y = XC and choose a rotation matrix C such that 
Yr = axXx + ¦ ¦ ' + a,Xr 
where a = (alf. . ., ar) is the maximizing vector of the preceding example. The 
rest is easy. 
20. Find the general normal stationary process satisfying 
(a) Xn+2 + Xn=0 
(b) Xn+2 - Xn = 0 
(c) xw+3 — x^+2 + xn+1 — xn = o. 
21. A servo-stochastic process. (H. D. Mills.) A servomechanism is exposed 
to random shocks, but corrections may be introduced at any time. Thus the 
102 DENSITIES IN HIGHER DIMENSIONS III.9 
error Yn at time n is (in proper units) of the form Yn+1 =?„ — €„+ Xn+1, 
where Cn is the correction and the Xn are independent normal variables, 
Z(Xn) = 0, E(X?) = 1. The Cn are, in principle, arbitrary functions of the past 
observations, that is, of Yk and Xk for k <, n. One wishes to choose them so as 
to minimize Var (Yn) (which is a measure of how well the mechanism works), 
and Var (Cn) (which is a measure of how hard it works). 
(a) Discuss the covariance function of {Y,,} and show that Var (Yn) ^ 1. 
(b) Assuming that Var (Cn) -*¦ a2, Var (Yn) -*- az (tendency to stationarity) 
show that a>^(a + a-1). 
(c) Consider, in particular, the linear device Cn = a + p(Yn— b), 0 < p <, 1. 
Find the covariance function and a representation of the form G.8) for Yn. 
22. Continuation. If there is a time lag in information or adjustment the model 
is essentially^the same except that Cn is to be replaced by Cn+#. Discuss this 
situation. 
CHAPTER IV 
Probability Measures and Spaces 
As stated in the introduction, very little of the technical apparatus of 
measure theory is required in this volume, and most of the book should 
be readable without the present chapter.1 It is nevertheless desirable to 
give a brief account of the basic concepts which form the theoretical back- 
ground for this book and, for reference, to record* the main theorems. 
The underlying ideas and facts are not difficult, but proofs in measure 
theory depend on messy technical details. For the beginner and outsider 
access is made difficult also by the many facets and uses of measure theory; 
excellent introductions exist, but of necessity they dwell on great generality 
and on aspects which are not important in the present context. The following 
survey concentrates on the needs of this volume and omits many proofs 
and technical details.2 (It is fair to say that the simplicity of the theory is 
deceptive in that much more difficult measure theoretic problems arise in 
connection with stochastic processes depending on a continuous time 
parameter. The treatment of conditional expectations is deferred to V, 
10-11; that of the Radon-Nikodym theorem to V,3.) 
Formulas relating to Cartesian (or Euclidean) spaces 3lr are independent 
of the number of dimensions provided x is read as abbreviation for 
1This applies to readers acquainted with the rudiments of measure theory as well as 
to readers interested primarily in results and facts. For the benefit of the latter the definition 
of integrals is repeated in V,l. Beyond this they may rely on their intuition, because in 
effect measure theory justifies simple formal manipulations. 
2 An excellent source for Baire functions and Lebesgue-Stieltjes integration is found 
in E. J. McShane and T. A. Botts, Real analysis, D. Van Nostrand, Princeton, 1959. 
Widely used are presentations of general measure theory in P. R. Halmos, Measure theory, 
D. Van Nostrand, Princeton, 1950 and in N. Bourbaki, Elements de mathe'matiques [livre 
VI, chapters 3-5] Hermann, Paris, 1952 and 1956. For presentations for the specific 
purposes of probability see the books of Doob, Krickeberg, Loeve, Neveu, and 
Hennequin-Tortrat. 
103 
104 PROBABILITY MEASURES AND SPACES IV. 1 
1. BAIRE FUNCTIONS 
We shall have to decide on a class of sets for which probabilities are 
defined and on a class of functions acceptable as random variables. The 
two problems are not only related but their treatment is unified by a stream- 
lined modern notation. . We begin by introducing it and by recalling the 
definition of convergence in terms of monotone limits. 
" The indicator* of a set A is the function which assumes the value 1 at 
all points of A and the value 0 at all points of the complement A'. It will 
be denoted by iA: thus iA(x) = I if xeA and 1A(x) = 0 otherwise. 
Every set has an indicator, and every function assuming only the values 
1 and 0 is the indicator of some set. If / is an arbitrary function, the 
product \Af is the function that equals f on A and vanishes elsewhere. 
Consider now the intersection C = A n B of two sets. Its indicator 
1C equals 0 wherever either 14 or \B vanishes, that is, 1C = inf A^, iB) 
equals the smaller of the two functions. To exploit this parallelism one 
writes / n g instead of inf (/, g) for the function which «t each point x 
equals the smaller of the values of f(x) and g(x). Similarly / U g = 
= sup (/, g) denotes the larger of the two values.4 The operators Pi and 
U are called cap and cup respectively. They apply to arbitrary numbers of 
functions, and one writes 
(l.l) /1n-n/n = n/l) fx 
To repeat, at each point x these functions equal, respectively, the minimum 
and the maximum among the n values fi(x), . . . ,fn(x). If fk is the indi- 
cator of a set Ak then' A.1) exhibits the indicators of the intersection 
Ax n • ¦ • n An and of the union Ax U • • • U An. 
Consider now an infinite sequence {/„}. The functions defined in A.1) 
depend monotonically on n, and hence the limits (J^iA anc* U"=i/t 
are well defined though possibly infinite. For fixed j 
is the limit of the monotone sequence of functions fjC\--- n/^n, and 
the sequence {w,} itself is again monotone, that is, wn = wx u • • • U wn. 
With our notations wn -> (J^i wk- Bv definition wn(x) is the greatest 
lower bound (the infimum) of the numerical sequence fn(z),fn+i(x)> 
3 This term was introduced by Loeve. The older term "characteristic function" is 
confusing in probability theory. 
4 Many writers prefer the symbols v and A for functions and reserve r\ and u for 
sets. Within our context there is no advantage in the dual notation. 
IV. 1 BAIRE FUNCTIONS 105 
Hence the limit of wn is the same as lim inf/n and thus 
A.3) lim inf/n = 0 H/*. 
In this way the lim inf is obtained by a succession of two passages to the 
limit in monotone sequences. For lim sup/n one gets A.3) with n and 
u interchanged. 
All these considerations carry over to sets. In particular, we write 
A = lim An iff \A = lim \An. In words, the sequence {AJ of sets con- 
verges to the set A iff each point of A belongs to all An with finitely many 
exceptions, and each point of the complement A' belongs at most to 
finitely many An. 
Example, (a) The set {An i. o.}. As a probabilistically significant example 
of limiting operations among sets consider the event A defined as "the 
realization of infinitely many among a given sequence of events Alt A2,.. . ." 
[Special cases were considered in 1; VIII,3 (Borel-Cantelli lemmas) and in 
1; XIII (recurrent events).] More formally, given a sequence {An} of sets, 
a point x belongs to A iff it belongs to infinitely many Ak. Since 0 and 1 
are the only possible values of indicators this definition is equivalent to 
saying that \A = lim sup 1An. In standard notation therefore A = 
lim sup An, but the notation {An i. o.} (read "An infinitely often") is 
more suggestive. It is due to K. L. Chung. 
Our next problem is to delimit the class of functions5 in %r with which 
we propose to deal. The notion of an arbitrary function is far too broad 
to be useful for our purposes, and a modernized version of Euler's notion 
of a function is more appropriate. Taking continuous functions as given, 
the only effective way of constructing new functions depends on taking limits. 
As it turns but, all our needs will be satisfied if we know how to deal with 
functions that are limits of sequences {/„} of continuous functions, or 
limits of sequences where each /„ is such a limit, and so on. In other words, 
we are interested in a class 33 of functions with the following properties: 
A) every continuous function belongs to 23, and B) if /i,/2, . . . belong to 
33 and a limit f{x) = lim/n(;z) exists for all z, then / belongs to IB. 
5 We are, in principle, interested only in finite-valued functions, but it is sometimes 
convenient to permit ± oo as values. For example, the simple theorem that every mono- 
tone sequence has a limit is false for finite-valued functions and without it many for- 
mulations become clumsy. For this reason we adhere to the usual convention that all 
functions are to the extended real line, that is, their values are numbers of ±oo. In 
practice the values ± oo will play no role. To make sure that the sum and product of two 
•runctions are again functions one introduces for their values the conventions oo + oo = oo, 
oo — oo= 0, oo-oo = oo,0-oo= 0, etc. 
106 PROBABILITY MEASURES AND SPACES IV.2 
Such a class is said to be closed under pointwise limits. There is no doubt 
that such classes exist, the class of all functions being one. The intersection 
of all such classes is itself a closed family, and obviously is the smallest such 
class. Prudence requires us to limit our considerations to this smallest class. 
The smallest closed class of functions containing all continuous functions 
is called the Baire class and will be denoted by 23. The functions in 23 are 
called Baire functions* 
We shall use this notion not only for functions defined in the whole 
space but also for functions defined only on a subset (for example, \/x 
or log x in Si1). 
It is obvious from the definition that the sum and the product of two 
Baire functions are again Baire functions, but much more is true. If w is 
a continuous function in r variables and fx,.. . ,fr are Baire functions, 
then w(/x,.. . ,/r) is again a Baire function. Replacing w by wn and 
passing to a limit it can be shown that more generally every Baire function 
of Baire functions is again a Baire function. Fixing the value of one or 
more variables leads again to a Baire function, and so on. In short, none 
of the usual operations on Baire functions will lead outside the class, and 
therefore the class SB is a natural Dbject for bur analysis. It will turn 
out that no simplifications are possible by considering smaller classes. 
2. INTERVAL FUNCTIONS AND INTEGRALS IN %r 
We shall use the word interval, and the indicated notation, for sets of 
points satisfying a double inequality of one of the following four types: 
a,b: . a < x < b a,b: a < x < b 
i 1 i 
a, b: a < x <b a,b: a < x < b. 
In one dimension this covers all possible intervals, including the degenerate 
interval of length zero. In two dimensions the inequalities are interpreted 
coordinate-wise, and intervals are (possibly degenerate) rectangles parallel 
to the axes. Other types of partial closure are possible but are herewith 
excluded. The limiting case where one or more coordinates of either a or b 
are replaced by ± oo is admitted; in particular, the whole space is interval the 
— 00, 00. 
A point function / assigns, a value f(x) to individual points. A set 
function F assigns values to sets or regions of the spface. The volume in 
6 This definition depends on the notion of continuity but not on other properties of 
Cartesian spaces. It ? therefore applicable to arbitrary topological spaces. 
IV. 2 INTERVAL FUNCTIONS AND INTEGRALS IN ftr 107 
ft3, area in ft2, or length in ft1 are typical examples but there are many 
more, probabilities representing a special case of primary concern to us. 
We shall be interested only in set functions with the property that if a set 
A is partitioned into two sets Ax and A2, then F{A} = F{AX) + F{A2}. 
Such functions are called additive.7 
As we have seen, it occurs frequently that probabilities F{I) are assigned 
to all intervals of the r-dimensional space ftr and it is desired to extend 
this assignment to more general sets. The same problem occurs in elementary 
calculus, where the area (content) is originally defined only for rectangles 
and it is desired to define the area of a more general domain A. The simplest 
procedure is first to define integrals for functions of two variables and then 
to equate "the area of A" with the integral of the indicator \A (that is the 
function that equals 1 in A and vanishes outside A). In like manner we 
shall define the integral 
B.1) E(u) = f u(x) F{dz} 
of a point function u with respect to the interval function F. The prob- 
ability of A will then be defined by E(I^). In the construction of the 
integral B.1) the interpretation of F plays no role, and we shall actually 
describe the general notion of a Lebesgue-Stieltjes integral. With this pro- 
gram in mind we now start anew. 
Let F be a function assigning to each interval / a finite value F{I). Such a 
function is called (finitely) additive if for every partition of an interval / 
into finitely many non-overlapping intervals /x, . . . , In. 
B-2) 
Examples, (a) Distributions in ft1. In volume 1 we considered discrete 
probability distributions attributing probabilities px,p2, ... to the points 
au a2, . . . . Here F{1} is the sum of the weights pn of all points an con- 
tained in /, and E(w) = ]? u(an)pn. 
(b) If C is any continuous monotone function increasing from 0 at — oo 
to 1 at oo one may define F{a, b] = G(b) — G(a). 
(c) Random vectors in ft2. A vector of unit length issues from the origin in 
a random direction. The probability that its endpoint lies in a two- 
dimensional interval / is proportional to the length of the intersection of 
/ with the unit circle. This defines a continuous probability distribution 
without density. The distribution is singular in the sense that the whole 
7 Empirical examples for additive functions are the mass and amount of heat in a region, 
the land value, the wheat acreage and the number of inhabitants of a geographical region, 
the yearly coal production, the passenger miles flown or the kilowatt hours consumed 
during a period, the number of telephone calls, etc. 
108 PROBABILITY MEASURES AND SPACES IV.2 
probability is carried by a circle. One may think that such distributions are 
artificial and that the circle rather than the plane should serve as natural 
sample space. The objection is untenable because the sum of two independent 
random vectors is capable of all lengths between 0 and 2 and has a positive 
density within the disk of radius 2 [see example \,4(e)]. For some problems 
involving random unit vectors the plane is therefore the natural sample 
space. Anyhow, the intention was only to show by a simple example what 
happens in more complicated situations. 
(d) We conclude with an example illustrating the contingency that will 
be excluded in the sequel. In JL1 put F{I} = 0 for any interval I = a, b 
with b < oo and F{I) = 1 when / = a, oo. This interval function is 
additive but weird because it violates the natural continuity requirement 
that F{a, b) should tend to F{a, oo} as b.-+ oo. > 
The last example shows the desirability of strengthening the requirement 
B.2) of finite additivity. We shall say that an interval function F is count ably 
additive, or a-additive, if for every partitioning of an interval I into count ably 
many intervals /x, /2, . . . , 
B.3) FW-IHh}- 
["Countably many" means finitely or denumerably many. The term 
completely additive is synonymous with countably additive. The condition 
B.3) is manifestly violated in the last example.] 
We shall restrict our attention entirely to countably additive set functions. 
This is justified by the success of the theory, but the restriction can be 
defended a priori on heuristic or pragmatic grounds. In fact, if An = 
Ix U • • • U In is the union of the first n intervals, then An-+I. One 
could argue that "for n sufficiently large An is practically indistinguishable 
from /." If F{I) can be found by experiments, F{An} must be "practically 
indistinguishable" from F{I}, that is, F{An} must tend to F{J}. The 
countable additivity B.3) expresses precisely this requirement 
Being interested principally in probabilities we shall consider only non- 
negative interval functions F wormed by the condition that F{- o, oo} = 1. 
This norming imposes no serious restriction when F{ — oo, ooj < oo, but 
it excludes interval functions such as length in 311 or area in SI2. To make 
use of the following theory in such cases it suffices to partition the line 
or the plane into unit intervals and treat them separately. This procedure 
is so obvious and so well known that it requires no further explanation. 
A function on 5lr is called a step function if it assumes only finitely 
many values, each on an interval. For a step function u assuming the 
values «!,... ,an on intervals Ix, . . . ,In (that is, with probabilities 
FV.2 INTERVAL FUNCTIONS AND INTEGRALS IN 3lr 109 
F{IX), . . . , F{In}), respectively we put 
B-4) E(«) = ax F{Ii\ + • • • +an F{In) 
in analogy with the definition of expectation of discrete random variables. 
[It is true that the partioning of the space into intervals on which u is 
constant is not unique, but just as in the discrete case the definition B.4) 
is easily seen to be independent of the partition.] This expectation E(w) 
satisfies the following conditions: 
(a) Additivity for linear combinations: 
B.5) E(<x.1u1+a.2u2) = axE(wx) + a2E(«2). 
(b) Positivity: 
B.6) u > 0 implies E(u) > 0. 
(c) Norming: For the constant function 
B.7) E(l) = 1. 
The last two conditions are equivalent to the mean value thoerem: 
a <! u <*. $ implies a <; E(w) < /? and so the function E(m) represents a 
sort of average* 
The problem is to extend the definition of E(w) to larger classes of 
functions preserving the properties (a)-(c). The classical Riemann integration 
utilizes the fact that to each continuous function u on 0, 1 there exists a 
sequence of step functions un such that un —*¦ u uniformly on 0,1. By 
definition then E(u) — lim E(un). It turns out that the uniformity of the 
convergence is unnecessary and the same definition for E(w) can be used 
whenever un —> u pointwise. In this way it is possible to extend E(«) to 
all bounded Baire functions, and the extension is unique. When it comes to 
unbounded functions divergent integrals are unavoidable, but at least for 
positive Baire functions it is possible to define E(w) either as a number or as 
8 When F represents probabilities E(«) may be interpreted as the expected gain of a 
gambler who can gain the amounts ax, a2,.. . . To grasp the intuitive meaning in other 
situations consider three examples in which u{x) represents, respectively, the temperature 
at time x, the number of telephone conversations at time x, the distance of a mass point 
from the origin, while F represents, respectively, the duration of a time interval, the 
value (cost of conversation) of a time interval, and mechanical mass. In each case integra- 
tion will be extended over a finite interval only, and E(«) will represent the accumulated 
"temperature hours." the accumulated gain, and a static moment. These examples will 
show our integration with respect to arbitrary set functions to be simpler and more 
intuitive than Riemann integration where the independent variable plays more than one 
role and the "area under the curve'' is of no heJp to the beginner. One should beware of 
the idea that the concept of expectation occurs only in probability theory. 
110 PROBABILITY MEASURES AND SPACES IV.2 
the symbol oo (indicating divergence). No trouble arises in this respect 
because the Lebesgue theory considers only absolute integrability. Roughly 
speaking, starting from the definition B.4) for expectations of simple functions 
it is possible to define E(w) for general Baire functions by obvious approxi- 
mations and passages to the limit. The number E{u) so defined is the 
Lebesgue-Stieltjes integral of u with respect to F. (The term expectation 
is preferable when the underlying function F remains fixed so that no 
ambiguity arises.) We state here without proof9 the basic fact of the 
Lebesgue theory; its nature and scope will be analyzed in the following 
sections. [A constructive definition of E(w) is given in section 4.] 
Main theorem. Let F be a countably additive interval functions in 3ir with 
F{ — oo, oo} = lv There exists a unique Lebesgue-Stieltjes integral E(u) on 
the class of Baire functions such that: 
If u > 0 then E(u) is a non-negative number or oo. Otherwise E(u) 
exists iff either E(u+) or E(u~) is finite; in this case E(u) = E(u+) —E(u~). 
A function u is called integrable if E(u) is finite, then 
(i) If u is a step function, E(u) is given by B.4). 
(ii) Conditions B.5)-B.7) hold for all integrable functions. 
(iii) (Monotone convergence principle.) Let ux < u2 < • • • —*- u where 
un is integrable. Then E(un) —*¦ E(u). 
The change of variables vn = un+1 — un leads to a restatement of the last 
principle in terms of series: 
If vn is integrable and vn > 0, then 
B.8) 2 E(vn) = EB vn) 
in the sense that both sides are meaningful (finite) or neither is. It follows 
in particular that if v > u > 0 and E(w) = co then also E(i') = oo. 
What happens if in (iii) the condition of monotonicity is dropped? 
The answer depends on an important lemma of wide applicability. 
Fatou's lemma. If un > 0 and un is integrable, then 
B.9) E(lim inf un) < limmf E(«B). 
In particular, if un —>¦ u then lim inf E(un) > E(u). 
Proof. Put vn = un n un+1 n • ¦ - . Then vn- < un and hence 
E(vn) < E(un). 
9 The method of proof is indicated in section 5. As usual, u+ and u~ denote the positive 
and negative parts of «, that is, u+ = u u 0 and — u~ = u Pi 0. Thus u = u+ — u~. 
IV.2 INTERVAL FUNCTIONS AND INTEGRALS IN' %r \\\ 
But (as we saw in section 1) vn tends monotonically to lim inf un, and so 
E(yn) tends to the left side in B.9) and the lemma is proved. [Note that 
each side in B.9) can represent oo.] ^. 
As example (e) will show, the condition of positivity cannot be dropped, 
but it can be replaced by the formally milder condition that there exists 
an integrable function U such that un > U. (It suffices to replace un by 
un — U.) Changing un into — un we see that // un < U and E(U) < oo, 
then 
B.10) lim sup E(«B) < E(lim sup un). 
For convergent sequences the extreme members in B.9) and B,10) coincide 
and the two relations together yield the important 
Dominated convergence principle. Let un be integrable and un-+u 
pointwise. If there exists an integrable U such that \un\ < U for all n, 
then u is integrable and E(wn) -*¦ E(w). 
This theorem relates to the only place in the Lebesgue theory where a 
naive formal manipulation may lead to a wrong result. The necessity of the 
condition \un\ <, U is illustrated by 
Example, (e) We take 0, 1 as basic interval and define expectations by 
the ordinary integral (with respect to length). Let 
un{x) ={n + l)(n + 2)z«(l - x). 
These functions tend pointwise to zero, but nevertheless 1 = E(wn)—>-1. 
Replacing un by — un it is seen that Fatou's inequality B.9) does not 
necessarily hold for non-positive functions. &> 
We mention without proof a rule of ordinary calculus applicable more 
generally. 
Fubini's theorem for repeated integrals. If u > 0 is a Baire function and 
F and O are probability distributions then 
F{dx] u(z, y) G{dy) = G{dy) u(x, y) F{dx) 
— 00 «/—00 J—OO J — OO 
with the obvious interpretation in case of divergence. Here x and y maybe 
interpreted as points in %m and %n, and the theorem includes the assertion 
that the two inner integrals are Baire functions. (This theorem applies to 
arbitrary product spaces and a better version is given in section 6.) 
Mean approximation theorem. To each integrable u and e > 0 it is 
possible to find a step function v such that E(|w — v\) < e. 
112 PROBABILITY MEASURES AND SPACES IV.3 
Instead of step functions one may use approximation by continuous 
functions, or by functions with arbitrarily many derivatives and vanishing 
outside some finite interval. [Compare the approximation theorem of 
example VIII,3(a).] 
Note on Notations. The notation E(u) emphasizes the dependence on u 
and is practical in contexts where the interval function F is fixed. When 
F varies or the dependence on F is to be emphasized, the integral notation 
B.1) is preferable. It applies also to integrals extended over a subset A, for 
the integral of u extended over A is (by definition) the same as the integral 
of the product \Au extended over the whoie space. We write 
1 
u(z) F{dx) = E(iAu) 
(assuming, of course, that the indicator tA is a Baire function). The two 
sides mean exactly the same thing, the left side emphasizing the dependence 
on F. When A — a, b is an interval the notation J? is sometimes pre- 
ferred, but to render it unambiguous it is necessary to indicate whether 
the endpoints belong to the interval. This may be done by writing a + 
or a —. > 
In accordance with the.program outlined at the beginning of this section 
we now define the probability of a set A to equal E(I^) whenever \A is a 
Baire function; for other sets no probabilities are defined. The consequences 
of this definition will now be discussed in the more general context of 
arbitrary sample spaces. 
3. o-ALGEBRAS. MEASURABILITY 
In discrete sample spaces it was possible to assign probabilities to all 
subsets of the sample space, but in general this is neither possible nor desir- 
able. In the preceding chapters we have considered the special case of 
Cartesian spaces %T and started by assigning probabilities to all intervals. 
It was shown in the preceding section that such an assignment of probabilities 
can be extended in a natural way to a larger class % of sets. The principal 
properties of this class are: 
(i) If a set A is in ^t so is its complement A' — <3 — A. 
(ii) If {An} is any countable collection of sets in $T, then also their 
union \J An and intersection f\ An belong to 91. 
In short, % is a system closed under complementation and the formation 
of countable unions and intersections. As was shown in section 1 this implies 
that also the upper and lower limit of any sequence {An} of sets in 31 again 
belongs to $T. In other words, none of the familiar operations on sets in $T 
IV.3 CT-ALGEBRAS. MEASUREABILITY 113 
will lead us to sets outside % and therefore no need will arise to consider 
other sets. This situation is typical inasmuch as in general probabilities will 
be assigned only to a class of sets with the properties (i) and (ii). We there- 
fore introduce the following definition which applies to arbitrary spaces. 
Definition 1. A a-algebra10 is a family % of subsets of a given set E 
enjoying the properties (i) and (ii). 
Given any family $ of sets in S, the smallest a-algebra containing all 
sets in $ is called the a-algebra generated by $. 
In particular, the sets generated by the intervals of %r are called the Borel 
sets of %r. 
That a smallest cr-algebra containing 3r exists is seen by the argument used 
in the definition of Baire functions in section 1. Note that, <3 being the 
union of any set A and its complement, every a-algebra contains the space 
Examples. The largest cr-algebra consists of all subsets of <3. This algebra 
served us well in discrete spaces, but is too large to be useful in general. The 
other extreme is represented by the trivial algebra containing only the whole 
space and the empty set. For a non-trivial example consider the sets on the 
line 311 with the property that if x e A then all points x ± 1, x ± 2, . .. 
belong to A (periodic sets). Obviously the family of such sets forms a 
cr-algebra. > 
Our experience so far shows that a principal object of probability theory 
is random variables, that is, certain functions in sample space. With a 
random variable X we wish to associate a distribution function, and for 
that purpose it is necessary that the event {X < /} has a probability assigned 
to it. This consideration leads us to 
Definition 2. Let S2l be an arbitrary a-algebra of sets in S. A real-valued 
function u on S is called %-measurab!exl if for each t the set of all points 
x where u(x) < / belongs to 31. 
The set where u{x) < t is the union of the countable sequence of sets 
where u{x) < t — n~l, and therefore it belongs to %. Since % is closed 
under complementation it follows that in the above definition the sign <, 
may be replaced by <, >, or >. 
10 An algebra of sets is defined similarly on replacing the work "countable" in (ii) by 
finite. A T-algebra is often called "Bore! algebra," but this leads to a confusion with the 
last part of the definition. (In it intervals may be replaced by open sets, and then this 
definition applies to arbitrary topological spaces.) 
11 This term is a bad misnomer since no measure is yet defined. 
114 PROBABILITY MEASURES AND SPACES IV.3 
It follows from the definition that the %-measurable functions form a 
closed family in the sense introduced in section 1. 
The following simple lemma is frequently useful. 
Lemma 1. A function u is ^.-measurable iff it is the uniform limit of a 
sequence of simple functions, that is of functions assuming only countably many 
values, each on a set in 31. 
Proof. By the very definition each simple function is 3l-measurable, and 
because of the closure property of 3l-measurable functions every limit of 
simple functions is again 31-measurable. 
Conversely, let u be 31-measurable. For fixed e > 0 define the set An 
as the set of all points x at which (n — l)e < u{x) <; we. Here the integer 
n runs from — oo to oo. The sets An are mutually exclusive and their 
union is the whole space <3. On the set An we define a^ix) = (« — l)e and 
a({x) = ne. In this way we obtain two functions af and af defined on 
S and such that 
C.1) af < u 
— a( = e 
at all points. Obviously u is the uniform limit of af and a€ as e —*• 0. 
Lemma 2. In 9?r the class of Baire functions is identical with the class of 
functions measurable with respect to the a-algebra 31 of Borel sets. 
Proof, (a) It is obvious that every continuous function is Borel measurable. 
Now these functions form a closed class, while the Baire functions form the 
smallest class containing all continuous functions. Accordingly, every Baire 
function is Borel measurable. 
(b) The preceding lemma shows that for the converse it suffices to show 
that every simple Borel-measurable function is a Baire function. This amounts 
to the assertion that for every Borel set A the indicator \A is a Baire function. 
Now Borel sets may be defined by saying that A is a Borel set if and only if 
its indicator \A belongs to the smallest closed class containing all indicators 
of intervals. Since Baire functions form a closed class containing all indicators 
of intervals12 it follows that \A is a Baire function for every Borel set A. > 
We apply this result to the special case of the Cartesian space %T. In 
section 2 we started from a completely additive interval function and defined 
P{A) = E(I^) for every set A whose indicator \A is a Baire function. The 
present setup shows that under this procedure, the probability I*{A} is 
defined iff A is a Borel set. 
12 To see this for an open interval /, let v be a continuous function vanishing outside 
/ and such that 0 < v(x) ^1 for xe I. Then Vv -*• 1r. 
IV.4 PROBABILITY SPACIS. RANDOM VARIABLES 115 
Approximation of Borel sets by intervals. In view of the last remark, probabilities in 
SV' are as a rule defined on the a-algebra of Borel sets, and it is therefore interesting that 
any Borel set A can be approximated by a set B consisting of finitely many intervals in the 
following sense: To each e > 0 there exists a set C such that P{C) < e and such that 
outside C the sets A and B are identical (that is, a point in the complement C' belongs 
either to both A and B, or to neither. One may take for C the union of A — AB and 
B - AB). 
Proof. By the mean approximation theorem of section 2 there exists a step 
function v > 0 such that E(|1^—v\) < ?e. Let B be the set of those points x at 
which v(x) > ?. Since v is a step function, B consists of finitely many intervals. It is 
easily verified that 
»-1*(*)| <, 2E\lA(x)-v(x)\ < e 
for all x. But |1^—1?| is the indicator of the set C consisting of all points that belong 
to either A or B but not to both. The last inequality states that P{C) < e, and this 
completes the proof. ^ 
4. PROBABILITY SPACES. RANDOM VARIABLES 
We are now in a position to describe the general setup used in probability. 
Whatever the sample space <© probabilities will be assigned only to the sets 
of an appropriate a-algebra 31. We therefore start with 
Definition 1. A probability measure P on a a-algebra 31 of sets in S 
is a function assigning a value P{A} > 0 to each set A in 31 such that 
P{<3>} = 1 and that for every countable collection of non-overlapping sets 
An in 31 
This property is called complete additivity and a probability measure may 
be described as a completely additive non-negative set function on % subject 
to the norming13 P{S} = 1. 
In individual cases it is necessary to choose an appropriate cr-algebra and 
construct a probability measure on it. The procedure varies from case to 
case, and it is impossible to describe a general method. Often it is possible 
to adapt the approach used in section 2 to construct a probability measure 
on the Borel sets of .'Jlr. A typical example is provided by sequences of 
independent random variables (section 6). The starting point for any 
13 The condition P{S} = 1 serves norming purposes only and nothing essential changes 
if it is replaced by P{S} < °o. One speaks in this case of a finite measure space. In prob- 
ability theory the case P{S} < 1 occurs in various connections and in this case we speak 
of a defective probability measure. Even the condition P{S} < °o may be weakened by 
requiring only that S be the union of countably many parts Sn such that P{Sn} < °o. 
(Length and area are typical examples.) One speaks then of a-finite measures. 
116 PROBABILITY MEASURES AND SPACES IV.4 
probabilistic problem is a sample space in which a cr-algebra with an 
appropriate probability measure has been selected. This leads us to 
Definition 2. A probability space is a triple (S, 21, P) of a sample space S, 
a a-algebra 21 of sets in it, and a probability measure P on 21. 
To be sure, not every imaginable probability space is an interesting object, 
but the definition embodies all that is required for the formal setting of a 
theory following the pattern of the first volume, and it would be sterile to 
discuss in advance the types of probability spaces that may turn up in practice. 
Random variables are functions on the sample space, but for purposes of 
probability theory we can use only functions for which a distribution function 
can be defined. Definition 2 of section 3 was introduced to cope with this 
situation, and leads to 
Definition 3. A random variable X is a real function which is measurable 
with respect to the underlying a-algebra %. The function F defined by 
F(t) = P{X < t} us called the distribution function of X. 
The elimination of functions that are not random variables is possible 
because, as we shall presently see, all usual operations, such as taking sums 
or other functions, passages to the limit, etc., can be performed within the 
class of random variables without ever leaving it. Before rendering this point 
more precise let us remark that a random variable X maps the sample 
space <5 into the real line 311 in such a way that the set in S in which 
a < X < b is mapped into the interval a, b, with corresponding prob- 
ability F{b) — F(a). In this way every interval / in Si1 receives a prob- 
ability F{I}. Instead of an interval / we may take an arbitrary Borel set 
F on ft1 and consider the set A of those points in S at which X assumes 
a value in F. In symbols: A = {X e F}. It is clear that the collection of all 
such sets forms a cr-algebra 3IX which may be identical with 21, but is 
usually smaller. We say that 2^ is the a-algebra generated by the random 
variable X. It may be characterized as the smallest cr-algebra in S with 
respect to which. X is measurable. The random variable X maps each set 
of 2IX into a Borel set F of ft1, and hence the relation F{F} = P{A} 
defines uniquely a probability measure on the cr-algebra of Borel sets on ft1. 
For an interval I — a,b we have F{I) — Fib) — F(a) and so F is identical 
with the unique probability measure in ft1 associated with the distribution 
function F by the procedure described in section 2. 
This discussion shows that as long as we are concerned with only one par- 
ticular random variable X we may forget about the original sample space and 
pretend that the probability space is the line ft1 with the cr-algebra of Borei 
sets on it and the measure induced by the distribution function F. We saw 
IV. 4 PROBABILITY SPACES. RANDOM VARIABLES 117 
that in 'Ji1 the class of Baire functions coincides with the Borel measureable 
functions. Taking 311 as sample space this means that the class of random 
variables coincides with the class of Borel measurable functions. Interpreted 
in the original sample space this means that the family of Baire functions of 
the random variable X coincides with the family of all functions that are 
measurable with respect to the cr-algebra 9lx generated by X. Since 9lx <= % 
this implies that any Baire function of X is again a random variable. 
This argument carries over without change to finite collections of random 
variables. Thus an r-tuple (X1? . . . , Xr) maps S into 3lT so that to an 
open interval in 3ir there corresponds the set in S at which r relations of 
the form ak < Xk < bk are satisfied. This set is 1-measurable because it is 
the intersection of r such sets. As in the case of one single variable we may 
now define the a-algebra %x generated by X1? . . . , Xr as the smallest 
a-algebra of sets in S with respect to which the r variables are measurable. 
We have then the basic 
Theorem. Any Baire function of finitely many random variables is again a 
random variable. 
A random variable U is a Baire function of X1? . . . , Xr if it is measurable 
with respect to the a-algebra generated by Xx, . . . , Xr. 
Examples, (a) On the line 9?1 with X as coordinate variable, the function 
X2 generates the cr-algebra of Borel sets that are symmetric with respect to 
the origin (in the sense that if x e A then also — x e A). 
(b) Consider 3l3 with X1? X2, X3 as coordinate variables, and the cr- 
algebra of Borel sets. The pair (X1? X2) generates the family of all cylindrical 
sets with generators parallel to the third axis and whose basis are Borel sets 
of the (X1? X2) plane. > 
Expectations 
In section 2 we started from an interval function in 5lr and used it to 
construct a probability space. There we found it convenient first to define 
expectations (integrals) of functions and then to define the probability of a 
Borel set A equal to the expectation ?AA) of its indicator. If one starts 
from a probability space the procedure must be reversed: the probabilities 
are given and it is necessary to define ihs expectations of random variables in 
terms of the given probabilities. Fortunately the procedure is extreuely 
simple. 
As in lhz preceding section we say that a random vanabie TJ :? s:;npie if 
i*. assumes only coantably many values ax, a*, . . . each on a set A ¦ ^siong- 
inp ;o rhe basic o-abebra Ul. To such variables the discrete uieory oi volume 
118 PROBABILITY MEASURES AND SPACES IV.5 
1 applies and we define the expectation of U by 
D.2) 
provided the series converges absolutely; otherwise we say that U has no 
expectation. 
Given an arbitrary random variable and an arbitrary e > 0 we defined in 
C.1) two simple Tandom variable ae and oy such that ae = ae + e and 
qe < U jC <7C. With any reasonable definition of E(U) we must have 
D.3) EBf) ^ E(U) ^ E(ac) 
whenever the variables q€ and a€ have expectations. Since these functions 
differ only by e the same is true of their expectations, or else neither 
expectation exists. In the latter case we say that U has no expectation, 
whereas in the former case E(U) is uniquely defined by D.3) letting e -> 0. 
In brief, since every random variable U is the uniform limit of a sequence 
of simple random variables an the expectation of U can be defined as the 
limit of E(dn). For example, in terms of a€ we have 
D.4) E(U) = lim f n e P{(n-1> < U < ne} 
€-»0 —00 
provided the series converges absolutely (for some, and therefore.all e > 0). 
Now the probabilities occurring in D.4) coincide with the probabilities 
attributed by the distribution function F of U to the intervals 
1 
(« — l)e, ne. It follows that with this change of notations our definition of 
E(U) reduces to that given in section 2 for 
D.5) E(U) 
/N-oo 
= t F{dt). 
J—ao 
Accordingly, E(U) may be defined consistently either in the original prob- 
ability space or in terms of its distribution function. (The same remark was 
made in 1; IX for discrete variables). For this reason it is superfluous to 
emphasize that in arbitrary probability spaces expectations share the basic 
properties of expectations in %r discussed in section 2. 
5. THE EXTENSION THEOREM 
The usual starting point in the construction of probability spaces is that 
probabilities are assigned a priori to a restricted class of sets, and the domain 
of definition must be suitably extended. For example, in dealing with 
unending sequences of trials and recurrent events in volume 1 we were given 
the probabilities of all events depending on finitely many trials, but this 
domain of definition has to be enlarged to include events such as ruin, 
recurrence, and ultimate extinction. Again, the construction of measures in 
IV. 5 THE EXTENSION THEOREM 119 
%r in section 2 proceeded from an assignment of probabilities F{I} to 
intervals, and this domain of definition was extended to the class of all Borel 
sets. The possibility of such an extension is due to a theorem of much wider 
applicability, and many constructions of probability spaces depend on it 
The procedure is as follows. 
The additivity of F permits us to define without ambiguity 
E.1) 
for every set A which is the union offinitely many non-overlapping intervals 
Ik. Now these sets form an algebra %0 (that is, unions, intersections, and 
complements of finitely many sets in %0 belong again to 2I0). From here 
on the nature of the underlying space %r plays no role, and we may consider 
an arbitrary algebra %0 of sets in an arbitrary space S. There exists 
always a smallest algebra % of sets containing %0 which is closed also 
under countable unions and intersections. In other words, there exists a 
smallest cr-algebra % containing $I0 (see definition 1 of section ?\ In the 
construction of measures in %r the cr-algebra % coincided with the cr-algebra 
of all Borel sets. The extension of the domain of definition of probabilities 
from %0 to % is based on the general 
Extension theorem. Let %0 be an algebra of sets in some space S. Let F 
be a set function defined on %0 such that F{A} > 0 for every set A E%0, that 
F{S} = 1, and that the addition rule E.1) holds for any partition of A into 
countably many non-overlapping sets Ik e %0. 
There exists then a unique extension of F to a countably additive set 
function (that is, to a probability measure) on the smallest a-algebra $1 
containing 2I0. 
A typical application will be given in the next section. Here we give 
a more general and more flexible version of the extension theorem which 
is more in line with the development in sections 2 and 3. We started from 
the expectation B.4) for step functions (that is, functions assuming only 
finitely many values, each on an interval). The domain of definition of this 
expectation was then extended from the restricted class of step functions 
to a wider class including all bounded Baire functions. This extension leads 
directly to the Lebesgue-Stieltjes integral, and the measure of a set A is 
obtained as the expectation of its indicator \A. The corresponding abstract 
setup is as follows. 
Instead of the algebra $I0 of sets we consider a class 33O of functions 
closed under linear combinations and the operations n and U. In other 
words, we suppose that if ux and u2 are in 33O so are the functions14 
E.2) OC^ + 0C2W2, «x H ll2, MX U Ho. 
14 
Our postulates amount to requiring that 23a be a linear lattice. 
120 PROBABILITY MEASURES AND SPACES IV.5 
This implies in particular that every function u of 93O can be written in the 
form u — u+ — u~ as the difference of two non-negative functions, namely 
u+ = u U 0 and u~ — u (~\ 0. By a linear functional on 53O is meant an 
assignment of values E(w) to all functions of 93O satisfying the addition rule 
The functional is positive if u > 0 implies E(w) ;> 0. The norm of E is 
the least upper bound of E(|«|) for all functions u e 23O such that \u\ <: 1. 
If the constant function 1 belongs to 33O the norm of E equals E(l). 
Finally, we say that E is countably additive on 33O if 
E.4) 
whenever ]? uk happens to-be in 33O. An equivalent condition is: if {vn} 
is a sequence of functions in 33O converging monotonically to zero, then15 
E.5) E{vn) -> 0. 
Given the class 33O of functions there exists a smallest class 33 containing 
33O and closed under pointwise passages to the limit. [It is automatically 
closed under the operations E.2).] An alternative formulation of the 
extension theorem is as follows.16 Every positive, countably additive linear 
functional of norm 1 on 93O can be uniquely extended to a positive countably 
additive linear functional of norm 1 on all bounded {and many unbounded) 
functions of 58. 
As an example for the applicability of this theorem we prove the following 
important result. 
F. Riesz representation theorem.17 Let E be a positive linear functional 
of norm 1 on the class of continuous functions on %r vanishing at infinity n 
15 To prove the equivalence of E.4) and E.5) it suffices to consider the case uk > 0, 
vk > 0. Then E.4) follows from E.5) with vn = un+1 + «n+2 + • • • and E.5) follows 
from E.4) on putting uk = vk — vk+1 (that is, ^uk = i>j).. 
16 The basic idea of the proof (going back to Lebesgue) is simple and ingenious. It is 
not difficult to see that if two sequences {«„} and {u'n} of functions in 23O converge 
monotonically to the same limit u then E(un) and E(u'n) tend to the same limit. For such 
monotone limits u we can therefore define E(u) = lim E(«n). Consider now the class 
2?! of functions u such that to each e > 0 there exist two functions u and ii which are 
either in 23O or are monotone limits of sequences 23O and such that u < u < u and 
E(u) — E(u) < e. The class S^ is closed under limits and for functions in ^ the defini- 
tion of E(«) is obvious since we must have E(u) < E(«) < E(«). 
The tour deforce in this argument is that the class ^ is usually greater than 23 and the 
simple proof is made possible by proving more than is required. (For a comparison 
between 23 and 23X see section 7.) 
17 Valid for arbitrary locally compact spaces. For an alternative proof see V,l. 
18 u vanishes at infinity if for given e > 0 there exists a sphere (compact set) outside 
which \u(x)\ < c. 
IV. 6 PRODUCT SPACES 121 
There exists a measure P on the o-algebra of Bore I sets with P{3T} = 1 
such that E(m) coincides with the integral of u with respect to P. 
In other words, our integrals represent the most general positive linear 
functionals. 
Proof. The crucial point is that if a sequence {vn} of continuous func- 
tions vanishing at infinity converges monotonically to zero, the convergence 
is automatically uniform. Assume vn ^> 0 and put \\vn\\ = max vn(x). 
Then E(vn) <, \\vn\\, and so the countable additivity condition E.5) is 
satisfied. By the extension theorem E can be extended to all bounded 
Baire functions and putting P{A} = E(l^} we get a measure on the a- 
lgebra of Borel sets. Given the measures P{A} we saw that the Lebesgue- 
Stieltjes integral is uniquely characterized by the double inequality D.3) 
and this shows that for u continuous and vanishing at infinity this integral 
coincides with the given functional E(«). >. 
6. PRODUCT SPACES. SEQUENCES OF INDEPENDENT 
VARIABLES 
The notion of combinatorial product spaces A; V,4) is basic for prob- 
ability theory and is used every time one speaks of repeated trials. Describing 
a point in the plane %2 by two coordinates means that 3i2 is taken as the 
combinatorial product of its two axes. Denote the two coordinate variables 
by X and Y. Considered as functions in the plane they are Baire 
functions, and if a probability measure P is defined on the a-algebra of 
Borel sets in %2 the two distribution functions P{X < x} and P{Y <; y) 
exist. They induce probability measures on the two axes called the marginal 
distributions (or projections). In this description the plane appears as the 
primary notion, but frequently the inverse procedure is more natural. For 
example, when we speak of two independent random variables with given 
distributions, the two marginal distributions are the primary notion and 
probabilities in the plane are derived from it by "the product rule." The 
procedure is not more complicated in the general setup than for the plane. 
Consider then two arbitrary probability spaces, that is, we are given 
two sample spaces SA) and SB), two a-algebras HA) and UB) of sets in 
SA) and SB), respectively, and probability measures PA) and PB) defined 
on them. The combinatorial product (SA), SB)) is the set of all ordered 
pairs (xa),x{2)) where x(i) is a point in S(i). Among the sets in this product 
space we consider the "rectangles," that is, the combinatorial products 
(A{1), AB)) of sets A(i) e VL{i). With sets of this form we wish to associate 
probabilities by the product rule 
F.1) P{(A{1), A{2)) = 
122 PROBABILITY MEASURES AND SPACES IV.6 
Now sets which are unions of finitely many non overlapping rectangles 
form an algebra Uo, and F.1) defines in a unique way a countably additive 
function on it. Accordingly, by the extension theorem there exists a unique 
probability measure P defined on the smallest a-algebra containing all 
rectangles and such that the probabilities of rectangles are given by the product 
rule F.1). This smallest a-algebra containing all rectangles will be denoted by 
UA) X UB), and the measure will be called product measure. 
Of course, other probability measures can be defined on the product 
space, for example in terms of conditional probabilities. Under any 
circumstances the underlying cr-algebra U of sets will be at least as large 
as UA) x UB), and it is rarely necessary to go beyond this algebra. The 
following discussioti of random variables is valid whenever the underlying 
algebra U is given gy U = UA) x UB). 
The notion of random variable (measurable function) is relative to the 
underlying cr-algebra and with our setup for product spaces we must dis- 
tinguish between random variables in the product space and those on 
SA) and ?B). The relationship between these three classes is fortunately 
extremely simple. If u and v are random variables on SA) and SB) we 
consider in the product space the function w which at the point (x{1), x{2)) 
takes on the value 
F.2) w(xMt z<2>) = u(x™) • r(zB>). 
We show that the class of random variables in the product space (SA), G>B)) 
is the smallest class of finite-valued functions closed under pointwise passages 
to the limit and containing all linear combinations of functions of the form F.2). 
To begin with, it is clear that each factor on the right in F.2) is a random 
variable even when considered as a function on the product space. It follows 
that w is a random variable, and hence the class of random variables in 
(SA), GB)) is at least as extensive as claimed. On the other hand, the 
random variables form the smallest class of functions that is closed under 
passages to the limit and contains all linear combinations of indicators of 
rectangles. Such indicators are of the form F.2) and therefore the class of 
random variables cannot be larger than claimed. 
The special case of the product of two spaces %m and %n with probability 
measures F and G occurred indirectly in connection with Fubini's theorem 
B.11) concerning repeated integrals. We can now state the more general 
theorem, which is not restricted to %r. 
Fubini's theorem for product measures. For arbitrary non-negative Baire 
functions u. the integral of u with respect to the product measure equals the 
repeated integrals in B.11). 
IV.6 PRODUCT SPACES 123 
(It is understood that the integrals may diverge. The theorem is obvious 
for simple functions and follows in general by the approximation procedure 
employed repeatedly.) The generalization to product spaces with three or 
more factors is too obvious to require comment. 
We turn to the problem of infinite sequences of random variables, which we 
encountered in volume 1 in connection with unlimited sequences of Bernoulli 
trials, random walks, recurrent events, etc., and again in-chapter III in 
connection with normal stochastic processes. Nothing need be said when 
infinitely many random variables are defined on a given probability space. 
For example, the real line with the normal distribution is a probability space 
and {sin nx) is an infinite sequence of random variables on it. We are here 
concerned with the situation when the probabilities are to be defined in terms 
of the given random variables. More precisely, our problem is as follows. 
Let 31 °° denote the space whose points are infinite sequences of real 
numbers (xltx2,...), (that is, 3i °° is a denumerable combinatorial product 
or real lines). We denote the «th coordinate variable by Xn (that is, Xn is 
the function in 3i°° which at the point x = (x, x2,.. .) assumes the value 
xn). We suppose that we are given the probability distributions for X/, 
(X1? X2), (Xj, X2, X3),. . . and wish to define appropriate probabilities 
in 31 °°. Needless to say, the given distributions must be mutually consistent 
in the sense that the distributions of (X1?. . . , Xn) appear as marginal 
distributions for (X1? . . . , Xw+1), and so on. 
Let us now formalize the intuitive notion of an "event determined by the 
outcome of finitely many trials." We agree to say that a set A in 5V° 
depends only on the first r coordinates iff there exists a Borel set Ar in 
3ir such that x = (xlf x2, . . .) belongs, to A iff (xx, . . . , xr) belongs to Ar. 
The standard situation in probability is that the probabilities for such sets 
are prescribed, and we fac« the problem of extending this domain of definition. 
We state without proof the basic theorem derived (in slightly greater 
generality) by A. Kolmogorov in his now classical axiomatic foundation of 
probability theory A933). It anticipated and stimulated the development of 
modern measure theory. 
Theorem 1. A consistent system of probability distributions for X1? 
(X1? X2), (X1? X2, X3), . . . admits of a unique extension to a probability 
measure on U, the smallest a-algebra of sets in %°° containing all sets 
depending only on finitely many coordinates.19 
The important point is that all probabilities are defined by successive 
passages to the limit starting with finite-dimensional sets. Every set A 
19The theorem applies more generally to products of locally compact spaces; for 
example, the variables Xn may be interpreted as vector variables (points in Jt1") 
124 PROBABILITY MEASURES AND SPACES IV.6 
in U can be approximated by finite-dimensional sets in the following sense. 
Given e > 0 there exists an n and a set An depending only on the first 
n coordinates and such that 
f6.3) P{A -A n AJ < 6, P{An -AnAn}<e. 
In other words, the set of those points that belong to either A or An but 
not to both has probability < 2e. It follows that the sets An can be . '-osen 
such that 
F-4) P{An}-+P{A}. 
Theorem 1 enables us to speak of an infinite sequence of mutually independent 
random variables with arbitrarily prescribed distributions. Such sequences 
did in fact occur in volume 1, but we had to be careful to define the prob- 
abilities in question by specific passages to the limit, whereas theorem 1 
provides the desirable freedom of moton. This point is well illustrated by 
the following two important theorems due, respectively, to A. Kolmogorov 
A933) and to E. Hewitt and L. J. Savage A955). They are typical for 
probabilistic arguments and play a central role in many contexts. 
Theorem 2. (Zero-or-one law for tail events.) Suppose that the variables 
Xk are mutually independent and that for each n the event A is independent 
of20 Xi, . . . , XB. Then either P{A} = 0 or P{A} = 1. 
Proof. In principle the variables Xk can be defined in an arbitrary 
probability space, but they map this space into the product space SV° in 
which they serve as coordinate variables. There is therefore no loss 
of generality in departing from the setup described in this section. 
With the notations used in F.3) the sets A and An are independent 
and so the first inequality implies P{A} — P{A}P{An} < e. Therefore 
P{A} = P2{A}. > 
Example, (a) The series 2Xn converges with probability zero or one. 
Similarly, the set of those points where lim sup Xn = oo has either prob- 
ability zero or one. > 
Theorem 3. (Zero-or-one law for symmetric events.) Suppose that the 
variables Xk are mutually independent and have a common distribution. 
If the set A is invariant under finite permutations of the coordinates21 then 
either P{A) = 0or P{A} = 1. 
20 More precisely, A is independent of every eve;... defined in terms of Xlt . . . , Xn. 
Jn other words, the indicator of A is a random variable independent of Xj,. . . , Xn. 
21 More precisely, if (ax, a2, . , .) i.s a point of A and nx and n, are two arbitrary 
integers it is supposed that A contains also the point obtained by exchanging ani and an% 
while leaving all other coordinates fixed. This condition extends automatically to permuta- 
tions involving k coordinates. 
IV.7 NULL SETS. COMPLETION 125 
Proof. As in the last proof we use the Xk as coordinate variables and 
refer to the sets An occurring in F.3). Let Bn be the set obtained from 
An by reversing the first In coordinates and leaving the others fixed. By 
hypothesis then F.3) remains valid also when An is replaced by Bn. It 
follows that the set of points belonging to either A or AnC\Bn but not to 
both has probability < 4e, and therefore 
F.5) PDn5J->P{4 
Furthermore An depends only on the first n coordinates and hence Bn 
depends only on the coordinates number n + 1,. . . , In. Thus An and 
Bn are independent and from F.5) we conclude again that P{A} — P2{A}. > 
Example, (b) Put Sn = Xx + • • • + Xn and let A be the event 
{Sn e/i. o.} where / is an arbitrary interval on the line. Then A is in- 
variant under finite permutations. [For the notation see example \{a).] > 
7. NULL SETS. COMPLETION 
Usually a set of probability zero is negligible and two random variables 
differing only on such a null set are "practically the same." More formally 
they are called equivalent. This means that all probability relations remain 
unchanged if the definition of a random variable is changed on.a null set, 
and hence we can permit a random variable not to be defined on a null set. 
A typical example is the epoch of the first occurrence of a recurrent event: 
with unit probability it is a number, but with probability zero it remains 
undefined (or is called oo). Thus we are frequently dealing with classes of 
equivalent random variables rather than with individual variables, but it is 
usually simplest to choose a convenient representative rather than to speak 
of equivalence classes. 
Null sets give rise to the only point where our probabilistic setup goes 
against intuition. The situation is the same in all probability spaces, but it 
suffices to describe it on the line. With our setup, probabilities are defined 
only for Borel sets, and in general a Borel set contains many subsets that are 
not Borel sets. Consequently, a null set may contain sets for which no 
probability is defined, contrary to the natural expectation that every subset 
of a null set should be a null set. The discrepancy has no serious effects and 
it is easily remedied. In fact, suppose we introduce the postulate: if A <= B 
and F{B} = 0, then F{A] — 0. It compels us to enlarge the cr-algebra II 
of Borel sets (at least) to the smallest cr-algebra Ux containing all sets of 
il and all subsets of null sets. A direct description is as follows. A set 
A belongs to Ux iff it differs only by a null set22 from some Borel set A0. 
22 More precisely, it is required that both A - A n A0 and A0 — A r\ A0 be contained 
in a null set. 
126 PROBABILITY MEASURES AND SPACES IV.7 
The domain of definition can be extended from U to Uj simply by putting 
P{A} — P{A0}. It is almost trivial that this definition is unique and leads to a 
completely additive measure on Ui. By this device we have obtained a 
probability space satisfying our postulate and in which the probabilities of 
Borel sets remain unchanged. 
The construction so described is called the Lebesgue completion (of the 
given probability space). This completion is natural in problems concerned 
with a unique basic probability distribution. For this reason the length of 
intervals on Si1 is usually completed to a Lebesgue measure which is not 
restricted to Borel sets. But the completion would invite trouble when one 
deals with families of distributions (for example with infinite sequences of 
Bernoulli trials with unspecified probability p). In fact, Ux depends on the 
underlying distribution, and so a random variable with respect to VLX may 
stop being a random variable when the probabilities are changed, 
Example. Let ax, a2, . . . be a sequence of points on Si1 carrying prob- 
abilities pltp2, • • • where 2 pk = 1. The complement of {a^ has prob- 
ability zero and so Ui contains all sets of Si1. Every bounded function u 
is now a random variable with expectation %pku(ak) but it would be 
dangerous to deal with "arbitrary functions" when the underlying distribution 
is not discrete. > 
CHAPTER V 
Probability Distributions in 
This chapter develops the notion of probability distribution in the r- 
dimensional space %r. Conceptually the notion is based on the integration 
theory outlined in the last chapter, but in fact no sophistication is required 
to follow the development because the notions and formulas are intuitively 
close to those familiar from volume 1 and from the first three chapters. 
The novel feature of the theocy is that (in contrast to discrete sample 
spaces) not every set carries a probability and not every function serves as 
random variable. Fortunately this theoretical complication is not noticeable 
in practice because we can start from intervals and continuous functions, 
respectively, and restrict our attention to sets and functions that can be 
derived from them by elementary operations and (possibly infinitely many) 
passages to the limit. This delimits the classes of Borel sets and Baire 
functions: Readers interested in facts rather than logical connections need 
not worry about the precise definitions (given in chapter IV). Rather they 
should rely on their intuition and assume that all sets and functions are 
"nice." The theorems are so simple1 that elementary calculus should suffice 
for an understanding. The exposition is rigorous under the convention that 
the words set and function serve as abbreviations for Borel set and Baire 
function. 
An initial reading should be restricted to sections 1-4 and 9. Sections 
5-8 contain tools and inequalities to which one may refer when occasion 
arises. The last sections develop the theory of conditional distributions 
and expectations more fully than required for the present volume where 
the results are used only incidentally for martingales in VI,11 and VII,9. 
1 It should be understood that this simplicity cannot be achieved by any theory restricted 
to the use of continuous functions or any other class of "nice" functions. For example, 
in 11,(8.3) we defined a density <p by an infinite series. To establish conditions for <p to 
be nice would be fedious and pointless, but the formula is obvious in simple cases and the 
use of Baire functions amounts to a substitute for a vague "goes through generally."— 
Incidentally, the few occasions where the restriction to Baire functions is not trivial will 
be pointed out. (The theory of convex functions in 8.b is'an example.) 
127 
128 PROBABILITY DISTRIBUTIONS IN ftr V.I 
1. DISTRIBUTIONS AND EXPECTATIONS 
Even the most innocuous use of the term random variable may contain 
an indirect reference to a complicated probability space or a complex 
conceptual experiment. For example, the theoretical model may involve 
the positions and velocities of 1028 particles, but we concentrate our attention 
on the temperature and energy. These two random variables map the original 
sample space into the plane ft2, carrying with them their probability distri- 
butions. In effect we are dealing with a problem in two dimensions and the 
original sample space looms dimly in the background. The finite-dimensional 
Cartesian spaces 3lr therefore represent the most important sample spaces, 
and we turn to a systematic study of the appropriate probability distributions. 
Let us begin with the line Jl1. The intervals defined by a < x < b and 
a < x < b will be denoted by a, b and a, b. (We do not exclude the 
limiting case of a closed interval reducing to a single point. Half-open 
intervals are denoted by a, b and a, b. In one dimension all random 
variables are functions of the coordinate variable X (that is, the function 
which at the point x assumes the value x). All probabilities are therefore 
expressible in terms of the distribution function 
A.1) F(x) = P{X < x}, -oo < x < oo. 
In particular, I — a,b carries the probability P{/} = F(b) — F{a). 
The flexible standard notation P{ } is impractical when we are dealing 
with varying distributions. A new letter would be uneconomical, and the 
notation PF{ } to indicate the dependence on F is too clumsy. It is by 
far the simplest to use the same letter F both for the point function A.1) 
and the corresponding interval function, and we shall write F{I} instead 
of P{/}. In other words, the use of braces { } will indicate that the 
argument in F{A} is an interval or set, and that F appears as a function 
of intervals (or measure). When parentheses are used the argument in 
F(a) is a point. The relationship between the point function F( ) and 
the interval function F{ } is indicated by 
A.2) F{x) = F{- oo, x }, F{a, b} =* F(b) - F{a). 
Actually the notion of the point function F(x) is redundant and serves 
merely for the convenience of analytical and graphical representation. 
The primary notion is the assignment of probabilities to intervals. The 
point function F( ) is called the distribution function of the interval 
function F{ }. The symbols F( ) and F{ } refer to the same thing 
and no confusion can arise by references to "the probability distribution 
V.I DISTRIBUTIONS AND EXPECTATIONS 129 
F." One should get used to thinking in terms of interval functions or measures 
and using the distribution function only for graphical descriptions.2 
Definition. A point function F on the line is a distribution function if 
(i) F is non-decreasing, that is, a < b implies F{a) < F{b) 
(ii) F is right continuous,z that is, F(a) — F(a-\-) 
(iii) F( — oo) = 0 and F(co) < oo. ¦ 
F is a probability distribution function if it is a distribution function and 
F(oo) = 1. ^Furthermore, F is defective if F(oo) < 1. 
We proceed to show that every distribution function induces an assignment 
of probabilities to all sets on the line. The first step consists in assigning 
probabilities to intervals. Since F is monotone, a left limit F(a—) exists 
for each point a. We define an interval function F{I} by 
F{ a,b}= F(b) - F(a-), F{a, b) = F(b-) - F(a) 
F{a, b } = F(b) - F(a), F{ a, b} = F(b-) - F(a- 
For the interval a, a reducing to the single point F{ a, a } = F{a) — F(a~), 
which is the jump of F at the point a. (It will be seen presently that F is 
continuous "almost everywhere.") 
To show that the assignment of values A.3) to intervals satisfies the 
requirements of probability theory we prove a simple lemma (which readers 
may accept as being intuitively obvious). 
Lemma 1. {Countable additivity.) If an interval I is the union ofcount ably 
many non-overlapping intervals Ix, /2, . . . , then 
d-4) 
Proof. The assertion is trivial in the special case I — aTb and Ix = a, ax, 
h — ax, a2, ...,/„ = an-i, b. The most general finite partition of I — a, b 
is obtained from this by redistributing the endpoints ak from one sub- 
interval to another, and so the addition rule A.4) holds for finite partitions. 
In considering the case of infinitely many intervals Ik it suffices to assume 
2 Pedantic care in the use of notations seems advisable for an introductory book, but it is 
Sloped that readers will not indulge in this sort of consistency and will find the courage to 
write F(I) and F(x) indiscriminately. No confusion will result and it is (fortunately) 
quite customary in the best mathematics to use the same symbol (in particular 1 and =) 
on the same page in several meanings. 
3 As usual we denote by /(a+) the limit, if it exists, of f{x) as x -*¦ a in such a way 
that x > a, and by/(co) the limit of f(x) as x -* oo. Similarly for/(a—) and /(-co). 
This notation carries over to higher dimensions. 
130 PROBABILITY DISTRIBUTIONS IN 3ir V.I 
/ closed. In consequence of the right continuity of the given distribution 
function F it is possible to find an open interval If containing Ik and such 
that 0 < F{If) - F{Ik) < € ¦ 2~k for preassigned e > 0. Now there exists 
a finite collection If, . . . , If covering / and hence 
A.5) F{I] < F{/*} + • ¦ ¦ + F{/*} < F{/x} + • • • + F{In} 
Thus 
But the reversed inequality is also true since to each n there exists a finite 
partition of / containing Iu . . . , In. This concludes the proof. > 
As explained in IV,2 it is now possible to define 
for every set A consisting of finitely or denumerably many disjoint intervals 
Ak. Intuition leads one to expect that every set can be approximated by such 
unions of intervals, and measure theory justifies this feeling.4 Using the 
natural approximations and passages to the limit it is possible to extend 
the definition of F to all sets in such a way that the countable additivity 
property A.7) is preserved. This extension is unique, and the resulting 
assignment is called a probability distribution or measure. 
Note on terminology. In the literature the term distribution is used loosely 
in various meanings, and so it is appropriate here to establish the usage to 
which we shall adhere. 
A probability distribution, or probability measure, is an assignment of 
numbers F{A) > 0 to sets subject to condition A.7) of countable additivity 
and the norming F{ — co, co} = 1. More general measures (or mass distri- 
butions) are defined by dropping the norming condition; the Lebesgue 
measure (or ordinary length) is the most notable example. 
As will be recalled from the- theory of recurrent events in volume 1, we 
have sometimes to deal with measures attributing to the line a total mass 
p = F{ — oo, c6) < 1. Such a measure will be called defective probability 
measure with defect 1 — p. For stylistic clarity and emphasis we shall 
occasionally speak of proper probability distributions, but the adjective 
proper is redundant. 
The argument of a measure m{A) is a set and is indicated by braces. 
With every bounded measure m there is associated its distribution function, 
1 
that is, a point function defined by m(x) — m{ — co, x }. It will be denoted 
by the same letter with the argument in parentheses. The dual use of the 
4 The convention that the words set and function serve as abbreviations for Borel set 
and Baire function should be borne in mind. 
V.I DISTRIBUTIONS AND EXPECTATIONS 131 
same letter can cause no confusion, and by the same token the term distri- 
bution may stand as abbreviation both for a probability distribution and its 
distribution function. 
In 1; IX a random variable was defined as a real function on the sample 
space, and we continue this usage. When the line serves as sample space 
every real function becomes a random variable. The coordinate variable 
X is basic, and all other random variables can be expressed as functions of it. 
The distribution function of the random variable u is defined by 
P{"PO < x) and can be expressed in terms of the distribution F of the 
coordinate variable X. For example, X3 has the distribution function 
given by F{^x). 
A function u is called simple if it assumes only countably many values 
flj, a2, . . . . If An denotes the set on which a equals an we define the 
expectation E(w) by 
A.8) nu) 
provided the series converges absolutely. In the contrary case u is said not 
to be integrable with respect to F. Thus u has an expectation iff E(|w|) 
exists. Starting from the definition A.8) we define the expectation for any 
arbitrary bounded function u as follows. Choose e > 0, and denote by 
An the set of those points x at which («—l)e < u(x) < ne. With any 
reasonable definition of E(t/) we must have 
A.9) 2 (n.-l)e- F{An} < E(u) < 2 mr • F{An). 
(The extreme members represent the expectations of two approximating 
simple functions a and a such that a < u < a and a — a — e.) Because 
of the assumed boundedness of u the two series in A.9) contain only finitely 
many non-zero terms, and their difference equals e2F{^B} = e. Re- 
placing € by |e will increase the first term in A.9) and decrease the last. 
It is therefore not difficult to see that as e -»- 0 the two extreme members 
in A.9) tend to the same limit, and this limit defines E(«). For unbounded 
u the same procedure applies provided the two series in A.9) converge 
absolutely; otherwise E(«) remains undefined. 
The expectation defined in this simple way is called the Lebesgue-Stieltjes 
integral of u with respect to F. When it is~ desirable to emphasize the 
dependence of the expectation on F the integral notation is preferable and 
we write alternatively 
/M-oo 
A.10) E(t<)= u(x)F{dx} 
J-oo 
with x appearing as dummy variable. Except on rare occasions we shall 
be concerned only with piecewise continuous or monotone integrands 
132 PROBABILITY DISTRIBUTIONS IN %r V.I 
such that the sets An will reduce to unions of finitely many intervals. The 
sums in A.9) are then simple rearrangements of the upper and lower sums 
used in the elementary definition of ordinary integrals. The general 
Lebesgue-Stieltjes integral shares the basic properties of the ordinary 
integral and has the additional advantage that formal operations and 
passages to the limit require less care. Our use of expectations will be limited 
to situations so simple that no general theory will be required to follow the 
individual steps. The reader interested in the theoretical background and 
the basic facts is referred to chapter IV. 
Examples, (a) Let F be a discrete distribution attributing weights 
Pi,p2, ... to the points ax, a2, . . . . Then clearly E(w) = 23 u(ak)pk 
whenever the series converges absolutely. This is in agreement with the 
definition in 1; IX. 
(b) For a distribution defined by a continuous density 
J*+oo 
u{x)f{x) dx 
—oo 
provided the integral converges absolutely. For the general notion of 
density see section 3. >¦ 
The generalization to higher dimensions can be described in a few words. 
In %2 a point a; is a pair of real numbers, x == (#,, x2). Inequalities 
are to be interpreted coordinate-wise;5 thus a <b means ax < bx and 
a2 < b2 (or "a lies southwest of b"). This induces only a partial ordering, 
that is, two points a and b need not stand in either of the two relations 
a < b or a > b. We reserve the word interval for the sets -defined by the 
four possible types of double inequalities a < x < b, etc. They are 
rectangles parallel to the axes which may degenerate into segments or points. 
The only novel feature is that the two-dimensional interval a, c with 
a < b < c is not the union of a, b and b, c. Corresponding to an interval 
function assigning the value F{I} to the interval / we may introduce its 
distribution function defined as before by F{x) — F{ — oo, x}, but an 
expression of F{a, b } in terms of this distribution function involves all four 
vertices of the interval. In fact, considering the two infinite strips parallel 
to the a:2"axis and with the sides of the rectangle a, b as bases one sees 
immediately that F{a, b) is given by the so-calied mixed difference 
A.12) F{aTb } = Fib,, b2) - F(au b2) - F{b^ a2) + F(au a2). 
5 This notation was introduced in III.5. 
V.I DISTRIBUTIONS AND EXPECTATIONS 133 
For a distribution function the right side is non-negative. This implies that 
F(x1, x2) depends monotonically on x1 and x2, but such monotonicity 
does not guarantee the positivity of A.12). (See problem 4.) 
The limited value of the use of distribution functions in higher dimensions 
is apparent: were it not for the analogy with ft1 all considerations would 
probably be restricted to interval functions. Formally the definition of 
distribution functions in ft1 carries over to ft2' if the condition of mono- 
tonicity (i) is replaced by the condition that for a <, b the mixed difference 
in A.12) be non-negative. Such a distribution function induces an interval 
function as in A.3) except that again the mixed differences take over the role 
of the simple differences in ft1. Lemma 1 and its proof remain valid.6 
A simple, but conceptually important, property of expectations is sometimes taken 
for granted. Any function m(X) = u(Xlt X2) of the two coordinate variables is a random 
variable and as such it has a distribution function G. The expectation E(m(X)) is now 
defined in two ways; namely, as the integral of u(xt,x2) with respect to the given 
probability in the plane, but also by 
A.13) E(«)= yG{dy) 
J—oo 
in terms of the distribution function G of u. The two definitions are equivalent by the 
very definition of the former integral by the approximating sums IV,D.3).7 The point 
is that the expectation of a random variable Z (if it exists) has an intrinsic meaning 
although Z may be considered as a function either on the original probability space S 
or on a space obtained by an appropriate mapping of S; in particular, Z itself maps S 
on the line where it becomes the coordinate variable. 
From this point on there is no difference between the setups in ft1 and 
ft2. In particular, the definition of expectations is independent of the number 
of dimensions. 
To summarize formally, any distribution function induces a probability 
measure on the a-algebra of Borel sets in ft/, and thus defines a probability 
space. Restated more informally, we have shown that the probabilistic 
setup of discrete sample spaces carries over without formal changes just as in 
the case of densities, and we have justified the probabilistic terminology 
employed in the first three chapters. If we speak of r random variables 
6 The proof utilized the fact that in a finite partition of a one-dimensional interval 
the subintervals appear in a natural order from left to right. An equally neat arrangement 
characterizes the checkerboard partitions of a two-dimensional interval a, b, that is, 
partitions into mn subintervals obtained by subdividing separately the two sides of a, b 
and drawing parallels to the axes through all points of the subdivisions. The proof of the 
finite additivity requires no change for such checkerboard partitions, and to an arbitrary 
partition there corresponds a checkerboard refinement. The passage from finite to 
denumerable partitions is independent of the number of dimensions. 
7 A special case is covered by theorem 1 in 1; IX,2. 
134 PROBABILITY DISTRIBUTIONS IN %r V.I 
Xx, . . . , Xr it is understood that they are defined in the same probability 
space so that a joint probability distribution of (Xx, . . . , Xr) exists. We are 
then free to interpret the Xk as coordinate variables io the sample space 3iT. 
It is hardly necessary to explain the continued use of terms such as 
marginal distribution (see 111,1 and 1; IX, 1), or independent variables. 
The basic facts concerning such variables are the same as in the discrete 
case, namely: 
(i) Saying that X and Y are independent random variables with (one- 
dimensional) distributions /"and G means that the joint distribution function 
of (X, Y) is given by the products F{x1) G(x2). This statement may refer to 
two variables in a given probability space or may be an abbreviation for the 
statement that we introduce a plane with X and Y as coordinate variables 
and define probabilities by the product rule. This remark applies equally 
to pairs or triples of random variables, etc. 
(ii) If the m-tuple (Xx, . . . , XTO) is independent of the «-tuple 
(Yx, . . . , Yn) then w(Xx, . . . , XTO) and y(Yx, . . . , Yn) are independent 
(for any pair of functions u and v). 
(iii) If X and Y are independent, then E(XY) = E(X) E(Y) whenever 
the expectations of X and Y exist (that is, if the integrals converge 
absolutely). 
The following simple result is frequently useful. 
Lemma 2. A probability distribution F is uniquely determined by the 
knowledge of E(w) for every continuous function u vanishing outside some 
finite interval. 
Proof. Let / be a finite open interval and v a continuous function that is 
positive in / and zero outside /. Then Vv(x) —> 1 at each point x e I, 
and hence E(^/v) —»- F{I). Thus the knowledge of the expectations of our 
continuous functions uniquely determines the values F{I) for all open 
intervals, and these uniquely determine F. > 
Note I.8 The F. Riesz representation theorem. In the preceding lemma the expectations 
were defined in terms of a given probability distribution. Often (for example, in the moment 
problem of VII,3) we start from a given functional, that is, from an assignment of values 
E(w) to certain functions. We inquire whether there exists a probability distribution F 
such that 
/M-oo 
A.14) E(w)= u(x) F{dx). 
J —oo 
It turns out that three evidently necessary conditions are also sufficient. 
8 This note treats a topic of conceptual interest but will not be used in the sequel. For an 
alternative approach see IV,5. 
V.I DISTRIBUTIONS AND EXPECTATIONS 135 
Theorem. Suppose that to each continuous function u vanishing outside a finite interval 
there corresponds a number E(w) with the following properties; (i) The functional is linear, 
that is, for all linear combinations 
ECqw! + c2u2) = CjEO/j) + c2E(w2); 
(ii) it is positive, that is, u > 0 implies E(w) > 0; (iii) it has norm 1, that is, 0 <? u 
implies E(w) <[ 1, but for each e > 0 /Aere ejf«/j w .ywcA /Aa/ 0 <? w <? 1 
E(w)> 1 -e. 
/Aere exists a unique probability distribution F for which A.14) is true. 
Proof. For arbitrary t and h > 0 denote by z(A the continuous function of x that 
equals 1 when x <; /, vanishes for a; > / + A, and is linear in the intermediate interval 
/ <[ x <; / -f- A. This function does not vanish at infinity, but we can define E(zt h) by 
simple approximations. Choose a function \un\ <; 1 such that E(wn) is defined and 
un(x^ = z«,/»(a;) f°r N < n- V m > n the difference um — un vanishes identically v/ithin 
the interval —n, n and from the fact that E has norm 1 one concludes easily that 
E(wn—«m) —•¦ 0. It follows that E(wn) converges.to a finite limit, and this limit is obviously 
independent of the particular choice of the approximating un. It is therefore legitimate to 
define E(zt+h) = lim E(wn). It is easily seen that even within this extended domain of 
definition the functional E enjoys the three properties postulated in the theorem. 
Now put Fh(t) = E(zth). For fixed A this is a monotone function going from 0 to 1. 
It is continuous, because when 0 < 8 < A the difference zt+s h — zt h has a triangular 
graph with height d/h, and hence Fh has difference ratios bounded by I/A. As A -*0 
the functions Fh decreases monotonically to a limit which we denote by F. We show that 
F is a probability distribution. Obviously F is monotone and F(— co) = 0. Furthermore 
F(t) > Fh(t—h) which implies that F(oo) = 1. It remains to show that F is continuous 
from the right. For given / and e > 0 choose A so small that F(t) > Fh(t) — e. 
Because of the continuity of Fh we have then for d sufficiently small 
F{t) > Fh(t) - € > Fh(t+S) - 2c > F(t+8) - 2e 
which proves the right-continuity. 
Let u be a continuous function vanishing outside a finite interval a, b. Choose a = 
= a0 < ax < • • • < an = b such that within each subinterval ak_x, ak the oscillation 
of u is less than e. If A is smaller that the smallest among these intervals, then 
A15) Uh= 
*=1 
is a piecewise linear function with vertices at the points ak and ak + A. Since u(ak) = 
= uh(ak) it follows that \u—uh\ <; 2e, and hence \E(u)—E(uh)\ <; 2e. But as A -+0 
A.16) E(uh) - 2 u(a^F{ak.lt ak} 
and this sum differs from the integral in A.14) by less than e. Thus the two sides in A.14) 
differ by less than 3e, and hence A.14) is true. > 
Note II. On independence and correlation. Statistical correlation theory goes back to a 
time when a fonnalization of the theory was impossible and the notion of stochastic 
136 PROBABILITY DISTRIBUTIONS IN 3ir V.2 
independence was necessarily tinged with mystery. It was understood that the independ- 
ence of two bounded random variables with zero expectation implies E(XY) = 0, but 
this condition was at first thought also to be sufficient for the independence of X and Y. 
The discovery (hat this was not so led to a long search for conditions under which the 
vanishing of correlations would imply stochastic independence. As frequently happens, 
the history of the problem and the luster of partial results easily obscured the fact that 
the solution is extremely simple by modern methods. The following theorem contains 
various results proved in the literature by laborious methods. 
Theorem. The random variables X and Y are independent iff 
A.17) E(«(X) • v(Y)) = E(m(X)) • E(KY)) 
for all continuous functions u and v vanishing outside a finite interval. 
Proof. The necessity of the condition is obvious. To prove the sufficiency it suffices 
to show that for every bounded continuous function E(w) agrees with the expectation 
of w with respect to a pair of independent variables distributed as X and Y. Now A.17) 
states this to be the case whenever w is of the form w(X, Y) = w(X) v(Y). Every bounded 
continuous function w can be uniformly approximated9 by linear combinations of the 
form E ckuk(X) vk(Y), and by passing to the limit we see the assertion to be true for arbi- 
trary bounded continuous w. ^ 
2. PRELIMINARIES 
This section is devoted largely to the introduction of a terminology for 
familiar or obvious things concerning distribution functions in 311. 
Just as in the case of discrete variables we define the kth moment of a 
random variable X by E(Xfc), provided the integral exists. By this we mean 
that the integral 
B.1) E(X*) 
= zkF{dz} 
J — 00 
converges absolutely, and so E(X*) exists iff E(|X|*) < oo. The last 
quantity is called the kth absolute moment of X (and is defined also for 
non-integral k > 0). Since \z\a < \z\b + 1 when 0 < a < b, the existence 
of an absolute moment of order b implies the existence of all absolute 
moments of orders a < b. 
If X has an expectation m, the second moment of X — m is called the 
variance of X: 
B.2) Var (X) = E((X-mJ) = E(X2) - m\ 
Its properties and significance are the same as in the discrete case. In 
particular, if X and Y are independent 
B.3) Var (X + Y) = Var (X) + Var (Y) 
whenever the variances on the right exist. 
See problem 10 in VIII ,10. 
V.2 PRELIMINARIES 137 
[Two variables satisfying B.3) are said to be uncorrelated. It was shown 
in 1; IX,8 that two dependent variables may be uncorrelated.] 
It will be recalled how often we have replaced a random variable X by 
the "reduced variable" X* = (X-m)/or where m = E(X), and az i 
Var (X). The physicist would say that X* is "expressed in dimensionless 
units." More generally a change from X to (X— /?)/a with a>0 amounts 
to a change of the origin and the unit of measurement. The distribution 
function of the new variable is given by F(aa;-f/S), and in many situations 
we are actually dealing with the whole class of distributions of this form 
rather than with an individual representative. For convenience of expression 
we introduce therefore 
Definition 1. Two distributions Fx and F2 in ft1 are said to be of the same 
type10 if F2(z) — Fx{<zx+P) with a > 0. We refer to a as scale factor, (S 
as centering {or location) constant. 
This definition permits the use of clauses such as "F is centered to zero 
expectation" or "centering does not affect the variance." 
A median ? of a distribution F is defined as a number such that F(?) > ? 
but F(?—) < \. It is not necessarily defined uniquely; if F(z) = \ for 
all x of an interval a, b then every such a; is a median. It is possible 
to center a distribution so that 0 becomes a median. 
Except for the median these notions carry over to higher dimensions 
or vector variables of the form X = (X^ . . . , Xn); the appropriate vector 
notation was introduced in 111,5, and requires no modification. The 
expectation of X is now a vector, the variance a matrix. 
The first things one notices looking at the graph of a distribution function 
are the discontinuities and the intervals of constancy. It is frequently 
necessary to say that a point is not in an interval of constancy. We introduce 
the following convenient terminology applicable in all dimensions. 
Definition 2. A point x is an atom if it carries a positive mass. It is a 
point of increase of F iff F{I} > 0 for every open interval I containing x. 
The distribution F is concentrated on the set A if the complement A' 
has probability F{A'} = 0. 
The distribution F is atomic if it is concentrated on the set of its atoms. 
Example. Order the rationals in 0, 1 in a sequence ru r2, . . . with 
increasing denominators. Let F attribute probability 2~k to rk. Then F 
is purely atomic. Note, however, that every point of the closed interval 
0,1 is a point of increase of F. 
10 The notion was introduced by Khintchine who used the German term Klasse, but 
in English "a class of functions'' has an established meaning. 
138 PROBABILITY DISTRIBUTIONS IN %r V.3 
Because of the countable additivity A.7) the sum of the weights of the 
atoms cannot exceed unity and so at most one atom "carries a weight >?, at 
most two atoms carry weights >?, etc. It is therefore possible to arrange 
the atoms in a simple sequence alt a2,. . . such that the corresponding 
weights decrease: Pi^p2> • • • . In other words, there exist at most 
denumerably many atoms. 
A distribution without atoms is called continuous. If there are atoms, 
denote their weights by pltp2t. .. and let p = l,pk > 0 be their sum. Put 
B-4) Fa(x) = p~l J Pk, 
ak<x 
1 
the summation extending over all atoms in the interval — co,z. Obviously 
Fa is again a distribution function, and it is called the atomic component 
of F. If p = 1 the distribution F is atomic. Otherwise let q = 1 —p. 
It is easily seen that [F—pFa]lq = Fc is a continuous distribution, and so 
B.5) F = pFa+ qFc 
is a linear combination of two distribution functions of which JFa is atomic, 
Fc continuous. If F is atomic B.5) is true with p = 1 and Fc arbitrary; 
in the absence of atoms B.5) holds with p = 0. We have thus the 
Jordan decomposition theorem. Every probability distribution is a mixture 
of the form B.5) of an atomic and a continuous distribution; here p ^ 0, 
q^O, p+q=l. 
Among the atomic distributions there is a class which sometimes encumbers 
simple formulation by trite exceptions. Its members differ only by an 
arbitrary scale factor from distributions of integral-valued random variables, 
but they occur so often that they deserve a name for reference. 
Definition 3. A distribution F in Si1 is arithmetic11 if it is concentrated on a 
set of points of the form 0, ±A, ±2A, . . . . The largest X with this property 
is called the span of F. 
3. DENSITIES 
The first two chapters were devoted to probability distributions in 
such that 
C.1) F{A] = <p(x)'dx 
11 The term lattice distribution is, perhaps, more usual but its usage varies: according 
to some authors a lattice distribution may be concentrated on a set of points a, a±h, 
<x±2X,. .. with a arbitrary. (The binomial distribution with atoms at ±1 is arithmetic 
with span 1 in our terminology, but a lattice distribution with span 2 according to the 
alternative definition,) 
V.3 DENSITIES 139 
for all intervals (and therefore all sets). The distributions of chapter III 
are of the same form, the integration being with respect to the Lebesgue 
measure (area or volume) in 3ir. If the density <p in C.1) is concentrated on 
the interval 0, 1 then C.1) takes on the form 
C.2) F{A} = ( <p(x)U{dx} 
where U stands for the uniform distribution in 0,1. The last formula 
makes sense for an arbitrary probability distribution U, and whenever 
F{ — oo, oo} = 1 it defines a new probability distribution F. In this case we 
shall say that <p is the density of F with respect to U. 
In C.1) the measure U is infinite whereas in C.2) we have U{—oo, oo} = 
= 1. The difference is not essential since the integral in C.1) can be broken 
up into integrals of the form C.2) extended over finite intervals. We shall 
use C.2) only when U is either a probability distribution or the Lebesgue 
measure as in C.1) but the following definition is general. 
Definition. The distribution F is absolutely continuous with respect to the 
measure U if it is of the form C.2). In this case <p is called a density12 of 
F with respect to U. 
The special case C.1) where U is the Lebesgue measure is of course the 
most important and we say in this case that <p is an "ordinary" density. 
We now introduce the abbreviation 
C.3) F{dx} = <p(x) U{dx). 
This is merely a shorthand notation to indicate the validity of C.2) for all 
sets and no meaning must be attached to the symbol dx. With this notation 
we would abbreviate C.1) to F{dx} = <p{x) dx and if U has an ordinary 
density u then C.2) is the same as F{dx] = <p{x) u(x) dx. 
Examples, (a) Let U be a probability distribution in HI1 with second 
moment ra2. Then 
F{dx} = — x2 U{dx] 
m2 
is a new probability distribution. In particular, if U is the uniform 
distribution in OTT then F(x) = a;3 for 0 < x < 1, and if U has density 
e~x (z > 0) then F is the gamma distribution with ordinary density 
\x2e~x. 
{b) Let U be atomic, attaching weights pi,p2, • • • totheatoms alt a2, . ¦ • 
(where Z pk = 1). A distribution F has a density <p with respect to U iff 
12 
In measure theory <p is called a Radon-Nikodym derivative of F with respect to U. 
140 PROBABILITY DISTRIBUTIONS IN 3ir V.3 
it is purely atomic and its atoms are among alt a2,. . . . If F attributes 
weight qt to at the density <p is given by <p(ak) = qkfpk. The value of <p 
at other points plays no role and it is best to leave 9? undefined except at 
the atoms. ^ 
In theory the integrand 9? in C.2) is not uniquely determined, for if N 
is a set such that U{N} = 0 then 9? may be redefined on N in an arbitrary 
manner without affecting C.2). However, this is the only indeterminacy 
and a density is uniquely determined up to values on a null set.13 In practice 
a unique choice is usually dictated by continuity conditions, and for this 
reason one speaks usually of "the" density although "a" density would be 
more correct. 
For any bounded function v the relation C.3) implies obviously14 
C.4) v(z) F{dx) = v(x)(p(x) 
In particular, if <p is bounded away from 0 we can choose v = 9?-1 to 
obtain the inversion formula for C.2): 
C.5) U{dx] = — F{dx). 
<p{x) 
A useful criterion for absolute continuity is contained in a basic theorem 
of measure theory which we accept without proof. 
Randoh-Nikodym theorem. 15 F is absolutely continuous with respect to U 
C.6) F{A} = 0 whenever U{A} = 0. 
13 In fact, if both q> and q>x are densities of F with respect to U consider the set A 
of all points x such that >tp(x) > <pt(x) ¦+¦ e. From 
1 U{dx) 
= I ?»(*) U{dx) = f 
Ja Ja 
it follows that U{A} = 0, and since this holds for every e > 0 we see that q>(x) = q>x{x) 
except on a set N such that U{N) = 0. 
14 Readers who feel uneasy about the new integrals should notice that in the case of 
continuous densities C.4) reduces Jo the familiar substitution rule for integrals. The 
following proof in the general case uses a standard argument applicable in more general 
situations. Formula C.4) is trivial when v is simple, that is, assumes only finitely many 
values. For every bounded v there exist two simple functions of this nature such that 
v < v < v and v — v < e, and so the validity of C.4) for all simple functions implies its 
truth in general. 
15 Often called Lebesgue-Nikodym theorem. The relation C.6) may be taken as a 
definition of absolute continuity, in which case the theorem asserts the existence of a density. 
V.3 DENSITIES 141 
This expression may be rephrased by the statement that [/-null sets 
are also F-null sets. We give an important corollary although it will not be 
used explicitly in this book. 
Criterion. F is absolutely continuous with respect to U iff to each € > 0 
there corresponds a d > 0 such that for any collection of non-overlapping 
intervals Ilt . . . , In 
C.7) IU{Ik}<6 implies j>,F{Ik}<« 
i i 
An important special case arises when 
C.8) F{I) < a - U{I) 
for all intervals. Then C.7) is trivially true with <5 = eja, and it is easily 
seen that in this case F has a density <p with respect to U such that <p < a. 
*3a. Singular Distributions 
The condition C.6) of the Radon-Nikodym theorem leads one to the 
study of the extreme counterpart of absolutely continuous distributions. 
Definition. The probability distribution F is singular with respect to U if 
it is concentrated on a set N such that U{N} — 0. 
The Lebesgue measure U{dx} = dx plays a special role and the word 
"singular" without further qualification refers to it. Every atomic distribution 
is singular with respect to dx, but the Cantor distribution of example 1,11 (d) 
shows that there exist continuous distributions in HI1 that are singular with 
respect to dx. Such distributions are not tractable by the methods of calculus 
and explicit representations are in practice impossible. For analytic purposes 
one is therefore forced to choose a framework which leads to absolutely 
continuous or atomic distributions. Conceptually, however, singular 
distributions play an important role and many statistical tests depend on their 
existence. This situation is obscured by the cliche that "in practice" singular 
distributions do not occur. 
Examples, (c) Bernoulli trials. It was shown in example 1,1 l(c) that 
the sample space of sequences SS • • • F • • • can be mapped onto the unit 
interval by the simple device of replacing the symbols S and F by 1 and 0, 
respectively. The unit interval then becomes the sample space, and the 
outcome of an infinite sequence of trials is represented by the random 
variable Y = 2 2~kXk where the Xk are independent variables assuming 
the values 1 and 0 with probabilities p and q, Denote the distribution of 
Y by Fv. For symmetric trials F± is the uniform distribution and the 
model becomes attractive because of its simplicity. In fact, the equivalence 
* Although conceptually of great importance, singular distributions appear in this 
book only incidertaliy. 
142 PROBABILITY DISTRIBUTIONS IN 3ir V.3 
of symmetric Bernoulli trials with "a random choice of a point in 0, 1" 
has been utilized since the beginnings of probability theory. Now by the law 
of large numbers the distribution Fv is concentrated on the set Np consisting 
of points in whose dyadic expansion the frequency of the digit 1 tends to p. 
When p # a the set Na has probability zero and hence the distributions Fp 
are singular with respect to each other; for p # \ the distribution Fv is 
singular with respect to the uniform distribution dx. An explicit representa- 
tion of Fv is impractical and, accordingly, the model is not in common use 
when p # \. Two points deserve attention. 
First, consider what would happen if the special value p — \ presented 
a particular interest or occurred frequently in applications. We would 
replace the dyadic representation of numbers by triadic expansions and 
introduce a new scale such that now F^ would coincide with the uniform 
distribution. "In practice" we would again deal only with absolutely 
continuous distributions, but the reason for this lies in our choice of tools 
rather than in the nature of things. 
Second, whether a coin is, or is not, biased can be tested statistically 
and practical certainty can De reached after finitely many trials. This is 
possible only because what is likely under the hypothesis p = \ is extremely 
unlikely under the hypothesis p = \. A little reflection along these lines 
reveals that the possibility of a decision after finitely many trials is due to 
the fact that Fv is singular with respect to F± (provided p # f). The 
existence of singular distributions is therefore essential to statistical practice. 
(d) Random directions. The notion of a unit vector in 3l2 with random 
direction was introduced in 1,10. The distribution of such a vector is 
concentrated on the unit circle and is therefore singular with respect to the 
Lebesgue measure (area) in the plane. One might object that in this case the 
circle should serve as sample space, but practical problems sometimes 
render this choice impossible. [See example 4(e).] > 
Lebesgue decomposition theorem. Every probability distribution F is a 
mixture of the form 
C.9) F=p-Fs+q-Fac 
(where p > 0, q > 0, p + q = 1) of. two probability distributions such that 
Fs is singular and Fac absolutely continuous with respect to a given 
measure U. 
The Jordan decomposition B.5) applies to Fs and hence F can be 
written as a mixture of three probability distributions of which the first 
is atomic, the second absolutely continuous with respect to U{dx), the 
third continuous but singular. 
V.4 convolutions 143 
Proof. To simplify the language a set N with U{N} — 0 will be called 
nullset. Let p be the least upper bound (the sup) of F{N} for all nullsets 
N. To each n there exists a nullset Nn such that F{Nn} > p . Then 
1 n 
F{A}<- for any nullset A in the complement N'n. For the union 
N — U Nn tnis implies U{N} = 0 and F{N} = p, and hence no nullset 
in the complement Nf can carry positive probability. 
If p = 1 it follows that F , is singular, whereas p = 0 means that F 
is absolutely continuous. When 0 < p < 1 the assertion holds with the 
two probability distributions defined by 
C.10) p • Fa{A} = F{AN), q • Fac{A} = F{AN'}. > 
4. CONVOLUTIONS 
It is difficult to exaggerate the importance of convolutions in many 
branches of mathematics. We shall have to deal with convolution in two 
ways: as an operation between distributions and as an operation between 
a distribution and a continuous function. 
For definiteness we refer explicitly to distributions in 311, but with the, 
vector notation of section 1 the formulas are independent of the number of 
dimensions. The definition of convolutions on a circle follows the pattern 
described in 11,8 and requires no comment. (More general convolutions 
can be defined on arbitrary groups.) 
Let F be a probability distribution and <p a bounded point function. 
(In our applications <p will be either continuous or a distribution function.) 
A new function u is then defined by 
C+CO 
D.1) «(*)= <p(x-y) F{dy}. 
J — 00 
If F has a density / (with respect to dx) this reduces to 
r+oo 
D.2) «(*)= <p{x-y)f{y) dyT 
J — oo r 
Definition 1. The convolution of a function <p with a probability distri- 
bution F is the function defined by D.1). It will be denoted by u = F*k <P- 
When F has a density f we write alternatively it = f* <p. 
Note that the order of the terms is important: the symbol q>ic F is 
in general meaningless. On the other hand, D.2) makes sense for arbitrary 
integrable / and (p (also if / is not non-negative), and the symbol * is 
used in this generalized sense. Needless to say, the boundedness of <p was 
assumed only for simplicity and is not necessary. 
T 
144 PROBABILITY DISTRIBUTIONS IN %r V.4 
Examples, (a) When F is the uniform distribution in 0, a then 
D.3) u(x) =, a~l F (p(s) ds. 
Jx—a 
It follows that u is continuous; if <p is continuous u has a continuous 
derivative, etc. Generally speaking u will behave better than (p. and so 
the convolution serves as smoothing operator. 
(b) The convolution formulas for the exponential and the uniform 
distributions [1,C.6) and 1,(9.1)] are special cases. For examples in 3i2 see 
111,A.22) and problems 15-17 of chapter III. ^ 
Theorem 1. If <p is bounded and continuous, so is u = Fie <p; if <P is a 
probability distribution function, so is u. 
Proof. If <p is bounded and continuous then u^+h) -> u(x) by the 
dominated convergence principle. For the same reason right-continuity 
of (p implies right-continuity of u. Finally, if <p goes monotonically from 
0 to 1 tlis same is obviously true of u. > 
The next theorem gives an interpretation of Fie (p when <p is a distribu- 
tion function. 
Theorem 2. Let X and Y be independent random variables with distri- 
butions F and G. Then 
J'+OO 
¦ G{t-x)F{dx}. 
— 00 
Proof.16 Choose e > 0 and denote by In the interval ne < x < (« + l)e; 
here n = 0, ±1, The event {X + Y < t) occurs if X>'e In_x, 
Y < t — ne} for some n. The latter events are mutually exclusive, and as 
X and Y are independent we have therefore 
D.5) P{X + Y < t) < ^ G(t—ne) • F{In}. 
On the right we have the integral of the step function G€ assuming in In the 
value G(t—ne). Since G€(y) < G (t+e—y) we have 
D.6) P{X + Y < t} < \ JG{t+e-x) F{dx). 
The same argument leads to the reversed inequality with e replaced by 
—e. Letting e->0 we get D.4). > 
16 D.4) is a special case of FUbini's theorem IV,B.11). The converse of theorem 2 is false: 
we saw in 11,4(c), and in problem 1 of 111,9, that in exceptional cases formula D.4) may hold 
for a pair of dependent variables X, Y. 
V.4 CONVOLUTIONS 145 
Example, (c) Let F and G be concentrated on the integers 0, 1, 2,... 
and denote the weights of k by pk and qk. The integral in D.4) then reduces 
to the sum ]? G(t—k)pk. This is a function vanishing for / < 0 and 
constant in each interval n — \ < / < n. The jump at / = n equals 
in agreement with the convolution formula 1; XI,B.1) for integral-valued 
random variables. > 
Each of the preceding theorems shows that for two distribution functions 
the convolution operation FifG yields a new distribution function U. 
The commutativity of the addition X -f Y implies that Fir G = G ~k F. 
A perfect system might introduce a new symbol for such convolutions 
among distribution functions, but this would hardly be helpful.17 Of course, 
one should think of U as an interval function or measure: for each interval 
/ = a, b obviously 
D.8) U{I} = rXG{I-y} F{dy) 
J— 00 
where, as usual, I — y denotes the interval a—y,b—y. (This formula 
automatically carries over to arbitrary sets.) Because of the commutativity 
the roles of F and G in D.8) may be interchanged. 
Consider now three distributions Fly F2, Fv The associative law of 
addition for random variables implies that (Fx * F2) if F3 = F1 i? (F2 ic F3) 
so that we can dispense with the parentheses and Write Fx * F2ir Fz. We 
summarize this in theorems 3 and 4. 
Theorem 3. Among distributions the convolution operation if is com- 
mutative and associative. 
17 In other words, the symbol A * B is used when the integration is with respect to the 
measure A. This convolution is a point function or measure according as B is a point 
function [as in D.1)] or a measure [as in D.6)]. The asterisk • is used for an operation 
between two functions, the integration being with respect to Lebesgue measure. h\ our 
context this type of convolution is restricted almost exclusively to probability densities. 
A more general definition of a convolution between two functions may be defined by 
/•'+O0 
where m stands for an arbitrary measure. Sums of the form D.7) represent the special 
case when m is concentrated on the positive integers and attributes unit weight to each. 
In this sense the use of the asterisk for the convolutions between sequences in 1; XI,2 
is consistent with our present usage. 
146 PROBABILITY DISTRIBUTIONS IN %r V.4 
Theorem 4. If G is continuous (= free of atoms), so is U = F*G. If G 
has the ordinary density cp, then U has the ordinary density u given by D.1) 
Proof The first assertion is contained in theorem 1. If 93 is the density 
of G then an integration of D.1) over the interval / leads to D.8), and so 
u is indeed the density of the distribution U defined by D.8). ». 
It follows in particular that If F and G have densities / and g, then 
the convolution F-k G has a density h =/¦ g given by 
D.9) Kx)=["*f{x-y)g{y)dy. 
«/—00 
In general h will have much better smoothness properties than either / or g. 
(See problem 14.) 
Sums Sn = Xj + • • • + Xn of n mutually independent random vari- 
ables with a common distribution F occur so frequently that a special 
notation is in order. The distribution of Sn is the n-fold convolution of F 
with itself. It will be denoted by Fn*. Thus 
D.10) F1 * = F, F(n+1> * = Fn * • F 
A sum with no terms is conventionally interpreted as 0, and for consistency 
we define F°* as the atomic distribution concentrated at the origin. Then 
D.9) holds also for n = 0. 
Jf F has a density / then Fn* has the density /*/*•••*/(« times). 
We denote it by fn*. These notations are consistent with the notation 
introduced in 1,2. 
Note. The following examples show that the convolution of two singular distributions can 
have a continuous density. They show also that an effective calculation of convolutions 
need not be based on the defining formula. 
Examples, (d) The uniform distribution in 0, 1 is the convolution of two Cantor-type 
singular distributions. In fact, let Xx, X2,... be mutually independent random, variables 
assuming the values 0 and 1 with probability ?. We saw in example 1,1 l(r) that the variable 
X = y\ 2~fcXfc has a uniform distribution. Denote the contributions of the even and odd 
terms by U and .V, respectively. Obviously U and V are independent and X = 
= U + V. The uniform distribution is therefore the convolution of the distributions of 
U and V. But obviously U has the same distribution as 2V, and the variable V differs 
only notationally from the variable $Y of example I,\l(d). In other words, the dis- 
tributions of U and V differ only by scale factors from the Cantor distribution of that 
example. 
(e) Random tutors in #2. The distribution of a unit vector with random direction (see 
1,10) is concentrated on the unit circle and therefore singular with respect to the Lebesguc 
measure in the plane. Nevertheless, ihe resultant of two independent vectors has a length 
2 1 
L which is a random variable with the density = concentrated on 0, 2. In fact, 
77 V42 
77 V4- 
by the law of the cosines L = V2 — 2 cos a; = J2 sin ?«>| where a> is the angle between 
V.4 convolutions 147 
the two vectors. As ?eo is distributed uniformly in 0, n we have 
D.11) P{L <,r} = P{|2 sin }a>| <> r) = - arc sin ?r, 0 < r < 2 
7T 
which proves the assertion. (See problem 12.) > 
4a. Concerning the Points of Increase 
It is necessary here to interrupt the exposition in order to record some elementary facts 
concerning the points of increase of F ~k G. The first lemma is intuitively obvious, whereas 
the second is of a technical nature. It will be used only in renewal theory, and hence 
indirectly in the theory of random, walks. 
Lemma 1. If a and b are points of increase for the distributions F and G, then a + b 
is a point of increase for F* G. If a and b are atoms, the same is true of a + b. Further- 
more, all atoms of F* G are of this form. 
Proof. If X and Y are independent then 
a-b\ < <r} ? P{|X-a| < *<r} • P{|Y-6| < h). 
The right side is j« ^sitive for every e > 0 if a and b are points of increase, and so 
a + b is again a point of increase. 
Denote by Fa and Ga the atomic components of F and G in the Jordan decom- 
position B.5). The atomic component of F-k G is obviously identical with the convolution 
FaicGa, and hence all atoms of Fie G are of the form a + b, where a and b are 
atoms of F and G, respectively. > 
The intrinsic simplicity of the next lemma suffers by the special role played on one hand 
by arithmetic distributions, on the other hand by distributions of positive variables. 
Lemma 2. Let F be a distribution in &1 and 2 the set formed by the points of increase 
of F, F2*, Fs*,.... 
(a) If F is not concentrated on a half-axis then 2 is dense in — oc, oo for . F not 
arithmetic, and 2 = {0, ±A, ±2A, . ..} for F arithmetic with span A. 
(b) Let F be concentrated on 0, oo but not at the origin. If F is not arithmetic then 
2 is "asymptotically dense at oo" in the sense that for given e > 0 and x sufficiently 
large the interval x,x + e contains points of 2. If F is arithmetic with span A then 2 
contains all points nX for n sufficiently large. 
Proof. Let 0 < a < b be two points in the set 2 and put h — b — a. We distinguish 
two cases: 
(i) For each e > 0 it is possible to choose a, b such hat h < e. 
(ii) There exists a <$ > 0 such that h > 6 for ail possible choices. 
Let In denote the interval na < x <^ nb. If n(b — a) > a this interval contains 
na, (n 4- \)a as proper subinterval, and hence every point x > x0 = a2/(b — a) belongs 
to at least one among the intervals I^h,.... By lemma 1 the n + 1 points na 4- kh, 
k = 0,... ,n, belong to 2, and they partition In into n subintervals of length h. 
Thus every point x > x0 is at a distance <,h\2 from a point of 2. 
In the situation of case (i) this implies that 2 is asymptotically dense at +°o. If then 
F is concentrated on 0, oo there is nothing to be proved. Otherwise let — c < 0 be a 
148 PROBABILITY DISTRIBUTIONS IN %r V.5 
point of increase of F. For arbitrary y and n sufficiently large the interval 
contains a point s of 2. Since s — nc again belongs to 2 it follows that every interval 
of length e contains some points of 2, and thus 2 is everywhere dense. 
In the situation of case (ii) we may suppose that a and b were chosen such that h < 26. 
It follows then that the points na + kh exhaust all points of•? within /n. Since (n + l)a 
is among these points this means that all points of 2 within In are multiples of h. Now 
let c be an arbitrary (positive or negative) point of increase of F. For n sufficiently large 
the interval /„ contains a point of the form kh + c, and as this belongs to 2 it follows 
that c is a multiple of h. Thus in case (ii) the distribution F is arithmetic. > 
A special case of this theorem commands interest. Every number x > 0 can be 
represented uniquely in the form x = m + f as the sum of an integer m and a number 
0 ^ f < 1. This ? is called the fractional part of x. Consider now a distribution F 
concentrated on the two points — 1 and a > 0. The set S contains all points of the form 
na. — m and hence the fractional parts of a, 2a, .... This F is arithmetic if a = p\q 
where p and q are positive integers without common divisors, and in this case the span 
of F equals \\q. We have thus the following corollary (to be sharpened in the equi- 
distribution theorem 3 of VIII, 7). 
Corollary. If a. > 0 is an irrational number the set formed by the fractional parts of 
a, 2a, 3a, ... is dense in 0, 1. 
5. SYMMETRIZATION 
If the random variable X has the distribution F we shall denote the 
distribution of —X by ~F. At points of continuity we have 
E.1) ~F{x)=\ -F(-x) 
and this defines ~F uniquely. The distribution F is called symmetric if 
~F = F. [When a density / exists this means that f(—x) =f(x).] 
Let Xx and X2 be independent with the common distribution F. Then 
Xx — X2 has the symmetric distribution °F given by 
E.2) °F= 
Using the symmetry property °F(x) = 1 — °F(—x) it is readily seen that 
J*+oo 
F{x+y) F{dy). 
— 00 
We shall say that °F is obtained by symmetrization of F. 
Examples, (a) Symmetrization of the exponential leads to the bilateral 
exponential [II,4(a)]; the uniform distribution on 0, 1 leads to the triangular 
distribution t2 of 11,D.1). 
(b) The distribution with atoms of weight ? at ±1 is symmetric, but not 
the result of a symmetrization procedure. 
V. 5 SYMMETRIZ ATION 149 
(c) Let F be atomic, attributing weights po,pi,... to 0, 1,.... The 
symmetrized distribution °F is atomic and the points ±« carry the weight 
Jfc=O 
When F is the Poisson distribution we get for n > 0 
E-5) qn = e^J, -f—- = e-2° /nBa) 
where /„ is the BesseIfunction defined in 11,G.1). (See problem 9.) *. 
Many messy arguments can be avoided by symmetrization. In this 
connection it is important that the tails of F and °F are of comparable 
magnitude, a statement made more precise by the following inequalities. 
Their meaning appears clearer when expressed in terms of random variables 
rather than the distribution itself. 
Lemma 1. Symmetrization inequalities. If Xx and X2 are independent 
and identically distributed, then for t > 0 
E.6) V{\Xl-X2\>t}<2V{\X1\>\t}. 
If a > 0 is chosen so that P{X{ < a) > p and also P{Xt- > — a) > p, 
then 
E.7) PflXi-X.1 > /} ^ p PflXxl >t + a). 
In particular, ifO is a median for X3- 
E.8) 
Proof. The event on the left in E.6) cannot occur unless either |XX| > \t 
or |X2| > \t and hence E.6) is true. The event on the left in E.7) occurs 
if Xx> t + a, X2 < a, and also if Xj < — / — a and X2 > —a. This 
implies E.7). ». 
Symmetrization is frequently used for the estimation of sums of independ- 
ent random variables. In this connection the following inequality is 
particularly useful. 
Lemma 2. If X2, . . . , Xn are independent and have symmetric distri- 
butions then Sn = Xj + • • • + Xn has a symmetric distribution and 
E.9) PflXi-l- • -+XJ > /} > |P{Max |X,| > /}. 
If the X; have a common distribution F then 
E.10) PflX^' • -+XJ > 
150 PROBABILITY DISTRIBUTIONS IN %r V.6 
Proof. Let the random variable M equal the first term among Xx,. . . , Xn 
that is greatest in absolute value and put T = Sn — M. The pair (M, T) is 
symmetrically distributed in the sense that the four combinations (±M, ±T) 
have the same distribution. Clearly 
E.11) P{M > /} < P{M > /, T ^ 0} + P{M > /, T < 0}. 
The two terms on the right have equal probabilities, and so 
E.12) P{S >t} = P{M + T > /} > P{M > /, T > 0} > 
which is the same as E.9). 
(b) To prove E.10) note that at points of continuity 
E.13) P{Max |X,| < /} = (F@ - F(-t))n < e-nH 
This implies E.10) because 1 — x < e~* when 0 < x < 1. ^ 
6. INTEGRATION BY PARTS. EXISTENCE OF MOMENTS 
The familiar formula for integration by parts can be used also for arbitrary 
expectations in %x. If u is bounded and has a continuous derivative u', then 
F.1) f u(x) F{dx} = u(b) F(b) - u(a) F(a) - ("u'ix) F(x) dx. 
Ja ¦ Ja 
Proof. A simple rearrangement reduces F.1) to the form 
F.2) f+[u(b)-u(x)) F{dx) - [bu'(x)[F'{x)-F{a)] dx = 0. 
Ja Ja ' , 
Suppose \u'\ < M and partition a, b into congruent intervals Ik of 
length h. It is easily seen that the contribution of Ik to the left side in F.2) 
is in absolute value less than 2MhF{Ik). Summing over k we find that the 
left side is in magnitude <2Mh, which can be made as small as we please. 
Thus the left side in F.2) is indeed zero. > 
As an application we derive a frequently used formula [generalizing 
Lemma 1. For any a > 0 
F.3) fV F{dx} = a fV^fl-F^)] dx 
Jo Jo 
in the sense that if one side converges so does the other. 
Proof. Because of the infinite interval of integration F.1) does not apply 
directly, but for every b < oo we have after a trivial rearrangement 
F.4) 
[b+x*F{dx} = -b*[i-F(b)] + Sx*-\l-F{x)]dx. 
Jo Jo 
V.7 CHEBYSHEV'S INEQUALITY 151 
Suppose first that the integral on the left converges as b -> oo. The contri- 
bution of b, oo to the infinite integral is ^.ba[l—F(b)], and this quantity 
therefore tends to zero. In this case the passage to the limit b -> oo leads 
from F.4) to F.3). On the other hand, the integral on the left is smaller 
than the integral on the right and hence the convergence of the second entails 
the convergence of the former, and hence F.3). +. 
An analogue to F.3) holds for the left tail. Combining the two formulas 
we get 
Lemma 2. The distribution F possesses an absolute moment of order 
a > 0 iff \x\*-x{\ - F{x) + F(-x)] is integrate over 0, oo. 
As an application we prove 
Lemma 3. Let X and Y be independent random variables, and S = X+Y. 
Then E(|S|a) exists iff both E(|X|a) and E(|Y|a) exist. 
Proof. Since the variables X and X—c possess exactly the same moments 
there is no loss of generality in assuming that 0 is a median for both X and 
Y. But then P{|S| > /} > |P{|X| > /}, and by the last lemma 
E(|S|a) < oo implies E(|X|a) < oo. This proves the "only if" part of the 
assertion. The "if" y u* follows from the inequality |S|a ^ 2a(|X|a + I Y|a) 
which is valid, becai *. at no point can |S| exceed the larger of 2|X| 
and 2|Y|. 
7. CHEBYSHEV'S INEQUALITY 
Chebyshev's inequality is among the most frequently used tools in prob- 
ability. Both the inequality and its proof are the same as in the discrete 
case A; IX,6) and we repeat it mainly for reference. Interesting applications 
will be given in VII, 1. 
Chebyshev's inequality. If E(X2) exists 
G.1) P{|X| > r} < /~2E(X2) t>0. 
In particular, if E(X) = m and Var (X) = a2, 
G.2) P{|X -m\>t}< o*[t2. 
Proof. If F stands for the distribution of X, 
E(X2) > f x2 F{dx] >t2( F{dx} 
J\x\>t J\x\>t 
which is the same as G.1). 
152 PROBABILITY DISTRIBUTIONS IN 3ir V.8 
The usefulness of Chebyshev's inequality depends (not on sharp numerical estimates 
but) on its simplicity and the fact that it is specially adapted to sums of random variables. 
Many generalizations are possible, but they do not share these desirable properties. (Most 
of them are so simple that it is better to derive them as occasion arises. For example, a 
useful combination of Chebyshev's inequality with truncation procedures is described in 
A fairly general method for deriving non -trivial inequalities may be described as follows. 
If u ^> 0 everywhere and u(x) > a > 0 for all x in an interval / then 
G.3) F{I} <, fl"lE(«(X)). 
On the other hand, if u <^ 0 outside / and u <? 1 in / we get the reversed inequality 
F{l} ^> E(«(X)). Choosing for u polynomials we obtain inequalities depending only on 
the moments of F. . . 
Examples, (a) Let u(x) = (x+cf with c >0r Then u(x) > 0 for all x and 
u(x) ;> (t+cJ for x ;> / > 0. Therefore 
G.4) P{X>,}<—L-2E((X+cJ). 
If E(X) = 0 and E(X2) = a2 the right side assumes its minimum for c = o^jt and hence 
G.5) 
This interesting inequality was discovered independently by many authors. 
F) Let X be positive (that is, F@) = 0) and E(X) = 1, E(X2) = b. The polynomial 
u(x) = h~2(x—a)(a+2h—x) is positive only for a < x < a + 2ft, and u(x) ^ 1 every- 
where. When 0 < a < 1 it is readily seen that E(«(X)) ;> [2h(l —a) — b]h~2. Choosing 
h = 6A —a) we get by the remark preceding these examples 
G.6) P{X>a}>(l-aJb-1. 
(c) If E(X2) = 1 and E(X4) = M, the last inequality applied to X2 shows that 
G.7) P{|X| > /} > (l-t^Mr1 if 0 < / < 1. > 
For Kolmogorov's generalization of Chebyshev's inequality see section S(e). 
8. FURTHER INEQUALITIES. CONVEX FUNCTIONS 
The inequalities collected in this section are of widespread use and are by 
no means typical for probability. Most common is Schwarz' inequality. 
The others are given mainly because of their use in stochastic processes and 
statistics. (This section is meant for reference rather than for reading.) 
(a) Schwarz' Inequality 
In its probabilistic version this inequality states that for two arbitrary 
random variables <p and y defined on the same space 
(8.1) 
V.8 FURTHER INEQUALITIES. CONVEX FUNCTIONS 153 
whenever these expectations exist. Furthermore, the equality sign holds 
only if a linear combination aq> + by is zero with probability one. More 
generally, if F is an arbitrary measure on the set A then 
(8.2) 
x) W{x) F{dx}j? JV(*) F{dx] -JVC*) F{dx) 
for arbitrary functions for which the integrals on the right exist. Taking 
for F the purely atomic measure attaching unit weight to integers we get 
Schwarz' inequality for sums in the form 
(8.3) B WiJ < 2 <p\ 2 y>l 
In view of the importance of (8.1) we give two proofs pointing to different 
generalizations. The same proofs apply to (8.2) and (8.3). 
First proof. We- may assume E(y>2) > 0. Then 
(8.4) E^ + tyJ = E(<p2) + 2/ E{qnp) + t2 E{y>2) 
is a quadratic polynomial in / which, being non-negative, has either two 
complex roots or a double root X. The standard solution for quadratic 
equations shows in the first case that (8.1) holds with strict inequality. In 
the second case E{q>-\-t\pJ = 0 and so <p + tip = 0 except on a set of 
probability zero. 
Second proof. As we are free to replace <p and ip by constant multiples 
a<p and by) it suffices to consider the case E(<p2) = E(y2) = 1. Then (8.1) 
follows trivially taking expectations in the inequality 2 \<py)\ < <p2 + y>2. > 
(b) Convex Functions. Jensen's inequality 
Let u be a function defined on an open interval /, and P = (?, m(?)) 
a point on its graph. A line L passing through P is said to support u at 
? if the graph of u lies entirely above or on L. (This excludes vertical lines.) 
In analytical terms it is required that 
(8.5) h(*)>h(I) + A-(z-|) 
for all x in /, where X is the slope of L. The function u is called convex 
in I if a supporting line exists at each point x of /. (The function u is 
concave, if — u is convex.) 
We proceed to show that this definition implies the various properties 
intuitively associated with convexity as exemplified by convex polygonal lines. 
Let F be an arbitrary probability distribution concentrated on / and 
suppose that the expectation E(X) exists. Choosing .? = E(X) and taking 
expectations in (8.5) we get 
(8.6) E(w) > w(E(X)) 
154 PROBABILITY DISTRIBUTIONS IN 3lf V.8 
whenever the expectation on the left exists. This statement is known as 
Jensen s inequality. 
By far the most important is the case where F is concentrated at two 
points xx and *2 and attributes weights 1 — / and / to them. Then 
(8.6) takes on the form 
(8.7) A-0 i/fo) + t u(x2) > u({\ -0*i + tx2). 
This inequality admits of a simple geometric interpretation which we state 
in the following. 
Theorem 1. The function u is convex iff all its chords lie above or on the 
graph of u. 
Proof, (i) Necessity. Let u be convex and consider the chord over an 
arbitrary interval xlf x2. As t runs from 0 to 1 the point A— t)xx + tx2 
runs through the interval xlf x2 and the left side in (8.7) is the ordinate of the 
corresponding point on the chord. Thus (8.7) states that the points of the 
chord lie above or on the graph. 
(ii) Sufficiency^ Assume that u has the stated property and consider 
the triangle formed by three points Plt P2, Pz on the graph of u with 
abscissas x1 < x2 < xz. Then P2 lies below the chord PtPz, and among 
the three sides of the triangle PXP.Z has the smallest slope, P2PZ the largest. 
Outside the interval x2, xz the graph of u therefore lies above the line 
P2PZ. Now consider xz as a variable and let xz-*x2+. The slope of 
P2PZ decreases monotonically but is bounded from below by the slope 
of iW Thus the lines P2P3 tend to a line L through P2. Outside x2, xz 
the graph of u is above the line P2PZ, and hence the whole graph lies 
above or on L. Thus L supports u at x2, and as x2 is arbitrary, this 
proves the convexity of w. ». 
Being the limit of chords, the line L is a right tangent. In the limiting 
process the abscissa x3 of Pz tends to x2, and Pz to a point on L. Thus 
Ps-+P*. The same argument applies for an approach from the left, and 
we conclude that the graph of u is continuous and possesses right and left 
tangents at each point. Furthermore, these tangents are supporting lines 
and their slopes vary monotonically. Since a monotone function has at 
most denumerably many discontinuities we have proved 
Theorem 2. A convex function possesses right and left derivatives at all 
points, and these are non-decreasing functions. They are the same except 
possibly at countably many points. 
Obviously this theorem again expresses necessary and sufficient conditions 
for convexity. In particular, if a second derivative exists, u is convex 
iff u" ^ 0. 
V.8 FURTHER INEQUALITIES. CONVEX FUNCTIONS 155 
Usually (8.7) is taken as definition of convexity. For t = \ we get the 
inequality 
(8.8) u 
stating that the midpoint of the chord lies above or on the graph of u. If 
u is continuous this property guarantees that the graph can never cross a 
chord and hence that u is convex. It can be shown more generally that any 
Baire function1* satisfying (8.8) is convex. 
(c) Moment Inequalities 
We prove that for any random variable X 
(8.9) «(/) = log E(|X|9, '>0, 
is a convex function of t in every interval in which the integral exists. In 
fact, by Schwarz' inequality (8.1) 
(8.10) Ea(|X|') < E(|X|'+*) E(|X|'-*), O^h<t, 
provided the integrals converge. Putting xx = t — h and x2 = t + h we 
see that (8.8) holds and so u is convex as asserted. 
Since w@) < 0 the slope t~x u(t) of the line joining the origin to (t,u(t)) 
varies monotonically and hence (EflXI*)I7' is a non-decreasing function 
of t > 0. 
(d) Holder's Inequality 
Let p > 1, q > 1 and p~x + q~x = 1. Then for <p ^ 0, y> > 0 
(8.11) 
whenever the integrals exist. 
(Schwarz' inequality (8.1) is the special case p = q = \, and (8.2) and 
(8.3) generalize similarly.) 
Proof. For x > 0 the function u = log x is concave, that is, it satisfies 
(8.7) with the inequality reversed. Taking antilogarithms we get for 
*i, *2 > 0 
(8.12) x\-lxl < A - 0*i + tx% 
As in the second prp.of of Schwarz' inequality it suffices to consider integrands 
normed by E(<pp) = E(y«) = 1. Let t = q~x and 1 — t = p~l. The 
assertion E((p.y>) < 1 then follows directly taking expectations in (8.12) 
with xx = <pp and x2 = yf*. p. 
18 Every u satisfying (8.8) is either convex, or else its oscillations in every interval range 
from — oo to oo. See G. H. Hardy, J. E. Littlewood, and G. P16ya, Inequalities, Cambridge, 
England, 1934, in particular p. 91. 
156 PROBABILITY DISTRIBUTIONS IN 3ir V.9 
(e) Kolmogorov's Inequality 
Let X1} . . . , Xn be independent random variables with finite variances and 
E(Xfc) = 0. Then for any x > 0 
(8.13) P{max [1^1, . . . , |SJ] > x) < x~2 E(S*) 
This important strengthening of Chebyshev's inequality was derived for 
discrete variables in 1; IX, 7. The proof carries over without change, but 
we rephrase it in a form which will make it evident that Kolmogorov's 
inequality applies more generally to submartingales. We shall return to this 
point in VII, 9. 
Proof. Put x2 = t. For fixed t and j = 1,2, ... ,n denote by At the 
event that S2 > t, but S* < t for all subscripts v <j. In words, A,- is 
the event that j is the smallest among the subscripts k for which Sj* > t. 
Of course, such an index j need not exist, and the union of the events A,- is 
precisely the event occurring on the left side of Kolmogorov's inequality. 
Since the events A5 are mutually exclusive this inequality may be restated in 
the form 
(8.14) ipl^l^r^S*) 
5=1 
Denote by \Aj the indicator of the event A,-, that is, \Aj is a random 
variable which equals 1 on A5 and equals 0 on the compliment of As. Then 
^. < 1 and so 
(8.15) 
3 = 1 
We shall show that 
(8.16) U 
Since S^ > t whenever A5 occurs, the right side is >fP{^-}, and so (8.15) 
reduces to the assertion (8.14). 
To prove (8.16) we note that Sn = Sy + (Sn—S,) and hence 
(8.17) E(S^) > E(S^ 1^.) + 2E((Sn-S,)S/U.). 
The second term on the right vanishes because the variables Sn — S, = 
= Xm + • • • + Xn and S^A. are independent and so the multiplication 
rule applies to their expectsf;ons. Thus (8.17) reduces to the assertion 
(8.16). > 
9. SIMPLE CONDITIONAL DISTRIBUTIONS. MIXTURES 
In 111,2 we introduced a "conditional density of a random variable Y 
for a given value of another variable X" in the case where .the joint dis- 
tribution of X and Y has a continuous density. Without any attempt 
V.9 SIMPLE CONDITIONAL DISTRIBUTIONS. MIXTURES 157 
at generality we proceed to define an analogous concept for a wider class 
of distributions. (A systematic theory is developed in sections 10 and 
For any pair of intervals A and B on the line put 
(9.1) Q(A, B) = P{XgA,Yg B}. 
With this notation the marginal distribution for X is given by 
(9.2) ii{A) = Q(A, ft1). 
If fx{A) > 0 theconditionalprobability of the event {Y G B} given 
is 
(9.3) 
(If p{A} = 0 this conditional probability is not defined.) We use this 
formula when A is the interval Ah = x, x + h and let /t—>0+. Under 
appropriate regularity conditions the limit 
(9.4) q(x, B) = 
will exist for all choices of x and B. Following the procedure and reasoning 
used in 111,2 we write in this case 
and call q "the conditional probability of the event {Y G B} given that 
X = a;." This constitutes an extension of the notion of conditional prob- 
abilities to situations in which the "hypothesis" has zero probability. No 
difficulties arise when q is insufficiently regular, but we shall not analyze 
the appropriate regularity conditions'because a general procedure will be 
discussed in the next section. This naive approach usually suffices in 
individual cases, and the form of the conditional distribution can frequently 
be derived by intuitive reasoning. 
Examples, (a) Suppose that the pair X, Y has a joint density given by 
f(x, y). For simplicity we assume that / is continuous and strictly positive. 
Then 
q(x, B) = -f- f f{x, y) dy 
where fx(x) =r q(x, — oo, oo) is the marginal density of X. In other words, 
for fixed x the set function q has a density given by f(x, t/)//x(a;). 
(b) Let X and Y be independent random variables with distributions 
F and G, respectively. For simplicity we assume that X > 0 [that is, 
158 PROBABILITY DISTRIBUTIONS IN 3tr V.9 
f@) = 0]. Consider the product Z = XY. Then 
(9.6) P{Z <, / | X = x) = G(tjx) 
and the distribution function U of Z is obtained by integrating (9.6) with 
respect to F. [See 11,C.1). The assertion is a special case of formula (9.8) 
below.] In particular, when X is distributed uniformly over 0, 1 
(9.7) U(t) = I G(t/x) dx. 
Jo 
This formula can be used as a convenient starting point for the theory 
of unimodal distributions.19 
For a further example see problems 18-19. ^ 
The following theorem (due to L. Shepp) is a probabilistic version of a formal criterion 
found by A. Khintchine. 
Theorem. U is unimodal iff it is of the form (9.7), that is, iff it is the distribution of the 
product Z = XY of two independent variables such that X is distributed uniformly in 0,1. 
Proof. Choose h > 0 and denote by Uh the distribution function whose graph is the 
polygonal line agreeing with U at the points 0, ±h,.... [In other words, Uh(nh) = 
= U(nh) and Uh is linear in the interval between nh and (n+l)k.) It is obvious from the 
definition that U is unimodal iff all Uh are unimodal. Now Uh has a density uh which is 
a step function, and every step function with discontinuities at the points nh can be written 
in the form 
where f(x) = 1 for 0 < x < 1 and </(x) = 0 elsewhere. The function (*) is monotone 
in 0, oo and in — oo, 0 iff pn > 0 for all n, and it is a density if ji,pn = 1. But in this 
case (*) is the density of the product Zh = XYh of two independent variables such that 
X is distributed uniformly in 0, 1 and P{YA = nh} = pn. We have thus proved that 
Uh is unimodal iff it is of the form (9.7) with G replaced by an arithmetic distribution Gh 
concentrated on the points 0, ±h,.... Letting h -»• 0 we get the theorem by monotone 
convergence. 
(See problems 25-26 and problem 10 in XV,9.) ^ 
Under appropriate regularity conditions q(x, B) will for fixed x represent 
a probability distribution in B and for fixed B a continuous function in x. 
Then 
(9.8) Q(A,B)=(q(x,B)v{dx}. 
JA 
J9 A distribution function U is called unimodal with the mode at the origin iff the graph 
of U is convex in — oo, 0 and concave in 0, oo [see 8F)]. The origin may be a point of 
discontinuity, but apart from this unimodality requires that there exist a density u which 
is monotone in — oo, 0 and in 0, oo. (Intervals of constancy are not excluded.) 
V.9 SIMPLE CONDITIONAL DISTRIBUTIONS. MIXTURES 159 
In fact, the right side obviously represents a probability distribution in 
the plane, and the differentiation described in (9.4) leads to q(x, B) 
Formula (9.8) shows how a given distribution in %2 can be expressed in 
terms of a conditional and a marginal distribution. In the terminology 
of 11,5 it represents the given distribution as a mixture of the family of 
distributions q(x, B) depending on the parameter x with /u serving as the 
distribution of the randomized parameter. 
In practice the procedure is frequently reversed. One starts from a 
"stochastic kernel" q, that is a function q(x, B) of a point x and a set B 
such that for fixed x it is a probability distribution and for fixed B a Baire 
function. Given an arbitrary probability distribution /u the integral in 
(9.8) defines probabilities for plane sets of the form (A, B) and hence a 
probability distribution in the plane. Usually- (9.8) is expressed in terms of 
point functions. Consider a family of distribution functions G(d, y) 
depending on a parameter 6, and a probability distribution /u. A new 
distribution function is then defined by 
J*~f-00 
G(x,y)ju{dx}. 
— 00 
[This formula represents the special case of (9.8) when A = — oo, oo and 
q(x, —oo,y) = G(x,y).] Such mixtures occur in 1; V and are discussed 
in 11,5. In the next section it will be shown that q can always be interpreted 
as a conditional probability distribution. 
Examples, (c) If Fx and F2 are distributions pFl +A— p)F2 is a 
mixture @ < p < 1) and represents a special case of (9.9) when /u is 
concentrated on two atoms. 
(d) Random sums. Let X1} X2, ... be independent random variables 
with a common distribution F. Let N be a random variable independent 
of the Xj and assuming the values 0, 1,. . . with positive probabilities 
po,Pi, ¦ • • • We are interested in the random variable SN = Xx + • • • + XN. 
The conditional distribution of SN given that N = n is Fn*, and so the 
distribution of SN is given by 
(9.10) 
n=0 
which is a special case of (9.9). In this case each hypothesis N = n carries 
a positive probability pn and so we have conditional probability distributions 
in the strict sense. Other examples are found in 11,5-7. (See problems 21 
and 24.) 
160 PROBABILITY DISTRIBUTIONS IN 3ir V.10 
*10. CONDITIONAL DISTRIBUTIONS 
It would be pointless to investigate the precise conditions under which 
conditional probabilities q can be defined by the differentiation process 
in (9.4). The main properties of conditional probabilities are embodied in 
the relation (9.8) expressing probabilities of sets in terms of conditional 
probabilities, and it is simplest to use (9.8) as definition of conditional 
probabilities. It does not determine q uniquely because if for each set B 
we have q(x, B) = q{x, B) except on a set of ^-measure zero, then (9.8) 
will remain true with q replaced by q. This indeterminacy is unavoidable, 
however. For example, if /u, is concentrated on an interval / no natural 
definition of q is possible for x outside /. By the very nature of things 
we are really dealing with the whole class of equivalent conditional prob- 
abilities and should refer to a rather than the conditional probability 
distribution q. In individual cases there usually exists a natural choice 
dictated by regularity requirements. 
For definiteness we consider only events specified by conditions of the 
form Xg A and Yefi, where X and Y are given random variables and 
A, B are Borel sets on the line. Let us begin by examining the different 
meanings that may be attached to the phrase "conditional probabi'ity 
of the event {Y G B} forgiven X." The given value of X may be either a 
fixed number or indeterminate. With the second interpretation we have 
a function of X, that is, a random variable. It will be denoted by P{B | X} 
or ^(X, B), etc. For the value at a fixed point x we write for emphasis 
P{Y G B | X = x) or q{x, B). 
Definition 1. Let the set B be fixed. By P{Y e B | X} {in words, "a 
conditional probability of the event {Y G B} for given X") is meant a function 
q(X, B) such that for every set A in 311 
A0.1) P{XeA,YeB} = [q{z,B)pi{dz} 
JA 
where /u, is the marginal distribution of X. 
When x happens to be an atom the hypothesis X = x has positive 
probability and P{Y eB\X-pe} is already defined by (9.3) with A 
consisting of the single point x. But in this case A0.1) reduces to (9.3) and 
our definitions and notations are consistent. 
We show that a conditional probability P{Y e B | X} always exists. In 
fact, clearly 
A0.2) 
* This section should be omitted at first reading. 
V.10 CONDITIONAL DISTRIBUTIONS 161 
Considered for fixed B as a function of A the left side defines a finite 
measure, and A0.2) implies that this measure is absolutely continuous with 
respect to /u (see the Radon-Nikodym theorem in section 3). This means 
that our measure is defined by a density q, and so A0.1) is true. 
So far the set B was fixed, but the notation q(x, B) was chosen with a 
view to vary B. In other words, we wish to consider q as a function of 
two variables, a point x and a set B on the line. It is desired that for 
fixed x the set-function q be a probability measure, which requires that 
q(x, Si1) = 1 and that for any sequence of non-overlapping sets Blt B2,. . . 
with union B 
A0.3) ' q(x, B) = J q(x, Bk). 
Now if the terms on the right represent conditional probabilities for Bk 
this sum yields a conditional probability for B, but there is an additional 
consistency requirement that A0.3) be true for our choice of q and ail x. 
[Note that definition 1 does not exclude the absurd choice q(x, B) = 17 
at an individual point x.) It is not difficult to see that it is possible to choose 
q(x, B) so as to satisfy these conditions.20 This means that there exists a 
conditional probability distribution of Y for given X in the sense of the 
following. 
Definition 2. By a conditional probability distribution of Y for given X is 
meant a function q of two variables, a point x and a set B, such that 
(i) for a fixed set B 
A0.4) q(X, B) = P{YgB|X} 
is a conditional probability of the event {X e B} for given X. 
(ii) q is for each x a probability distribution. 
In effect a conditional probability distribution is a family of ordinary 
probability distributions and so the whole theory carries over without 
20 
It is easiest to choose directly only the values q(x, B) when B is an interval in a dyadic 
subdivision of $0. For example, let Bx = 0, oo and B2 = — oo, 0. Choose for q(x, B^) 
any conditional probability for Bl such that 0 < q{x, B^) < 1. Then q(x, B2) = 
= 1 — q(x, Bx) is automatically a legitimate choice. Partition B1 into Bu and Bl2 and 
choose q{x, Bn) subject to 0 < q(x, Bn) ^ q(x, BJ. Put q(x, Bl2) = q(x, Bx) - q(x, Bu) 
and proceed in like manner refining the subdivision indefinitely. The additivity require- 
ment A0.3) then defines q{x, B) for all open sets B and hence for all Borel sets. 
This construction depends only on the existence of a so-called net, namely a partition 
of the space into finitely many non-overlapping sets each of which is partitioned in like 
manner and each point of the space is the unique limit of a contracting sequence of sets 
appearing in the successive partitions. The assertion is therefore true in &r and in many 
other spaces. 
162 PROBABILITY DISTRIBUTIONS IN 3tr V.ll 
change. Thus when q is given21 the following definition introduces a new 
notation rather than a new concept. 
Definition 3. A conditional- expectation E(Y | X) is a function of X 
assuming at x the value 
—to 
A0.5) 
provided the integral converges {except possibly on an x-set of probability zero). 
E(Y | X) is a function of X, that is a random variable. For clarity it is 
occasionally preferable to denote its value at an individual point x by 
E(Y | X = x). From the very definition we get 
A0.6) E(Y) = T °°E(Y | x)fj.{dx) or E(Y) = E(E(Y | X)). 
J —oo 
¦11. CONDITIONAL EXPECTATIONS 
We have now defined a conditional expectation E(Y | X) in terms of a 
conditional distribution, and this is quite satisfactory as long as one deals 
only with one fixed pair of random variables X, Y. However, when one 
deals with whole families of random variables the non-uniqueness of the 
individual conditional probabilities leads to serious difficulties, and it is 
therefore fortunate that it is in practice possible to dispense with this un- 
wieldy theory. Indeed, it turns out that a surprisingly simple and flexible 
theory of conditional expectation can be developed without any reference to 
conditional distributions. To understand this theory it is best to begin with a 
closer scrutiny of the identity A0.5). 
Let A be a Borel set on the line and denote by 1^(X) the random variable 
that equals one whenever Xg A and zero otherwise. We integrate the two 
sides in A0.5) with respect to the marginal distribution [x of X, taking the 
set A as domain of integration. The result may be written in the form 
J" , f+0° 
E(Y | x) fx{dx] = \A{x) E(Y x) (x{dx). 
A J-oo 
The variable X maps the sample space S on a real line, and the last 
integral refers only to functions and measures on this line. The random 
variable Y1^(X), however, is defined in the original sample space, and 
therefore a better, notation is indicated. Obviously 1^(X) is the indicator 
of a set B in S, namely the set of all those points in S at which X 
21 For a more flexible general definition see section 11. 
* The theory of this section will be used only in connection with martingales in VI,12 
and VII,9. 
V.ll CONDITIONAL EXPECTATIONS 163 
assumes a value in A. As we saw in IV, 3, the sets B that in this manner 
correspond to arbitrary Borel sets A on the line form a or-algebra of sets 
in S which is called the algebra generated by X. Thus A1.1) states that 
U = E(Y | X) is a function of X that satisfies the identity 
(\\ 2^ E(Y1 ") = EfUl ") 
for every set B in the or-algebra generated by X. We shall see that this 
relation may be used as a definition of conditional expectations, and it is 
therefore important to understand it properly. A simple example will 
explain its nature. 
Examples, (a) We take the plane with coordinate variables X and Y as 
sample space and suppose for simplicity that the probabilities are defined by 
a strictly positive continuous density f(x, y). The random variable X 
assumes a constant value along any line parallel to the y-axis. If A is a 
set on the ar-axis, the corresponding plane set B consists of all such lines 
passing through a point of A. The left side of A1.2) is the ordinary integral 
of yf(x, y) over this set, and this can be written as an iterated integral. Thus 
A1.3) E(Y1JS) = I dx (*%/(*, y) dy. 
JA J-oo 
The right side of A1.2) is the ordinary integral of a function \J(x)fx(x), 
where/i is the marginal density of X. Thus in this case A1.2) states that 
(H.4) U(x) = -|- [%/(*, y) dy, 
in accordance with the definition A0.5) of conditional expectation and in 
accordance with intuition. 
(b) (Continuation.) We show now that A1.2) defines a conditional expecta- 
tion U even when no densities exist and the probability distribution in the 
plane is arbitrary. Given a Borel set A on the ar-axis, the left side in A1.2) 
defines a number ^{A}. Obviously fj,l is a measure on the Borel sets of the 
ar-axis. Another such measure is given by the marginal distribution /u of X, 
which is defined by p{A) = EA#). It is therefore obvious that if /u{A} = 0 
then also ^{A} = 6. In other words, /n,1 is absolutely continuous with 
respect to fxx and by the Radon-Nikodym theorem of section 3 there exists a 
function U such that 
A1.5) 
JA 
This differs only notationally from A1.2). Of course, A1.5) remains valid 
if U is changed on a set Of ^-measure 0, but this non-uniqueness is inherent 
in the notion of conditional expectation. >• 
164 PROBABILITY DISTRIBUTIONS IN %r V.li 
This example shows that A1.2) may be used to define a conditional 
expectation U(X) = E(Y | X) for an arbitrary pair of random variables 
X, Y in an arbitrary probability space [provided, of course, that E(Y) 
exists]. But this approach leads much further. For example, to define a 
conditional e. pectation E(Y j X1} X2) with respect to a pair Xlf X2 of 
random variables we can use A1.2) unchanged except that B will now be an 
arbitrary set in the or-algebra 23 generated by Xx and X2 (see IV,3). Of 
course, U will be a function of X1} X2, but we saw in IV,4 that the class of 
Baire functions of the pair (X1} X2) coincides with the class of all 23- 
measurable functions. Thus we may cover all imaginable cases by the 
following definition first proposed by Doob. 
Definition. Let (&,M, P) be a probability space, and 23 a a-algebra of 
sets in ty. (that is, 23 c ty). Let Y be a random variable with expectation. 
A random variable U is called conditional expectation of Y with respect to 
23 if it is ^-measure able and A1.2) holds for all sets B of 23. In this case we 
write U = E(Y | 95). 
In the particular case that 2* is the a-algebra generated by the random 
variables Xl5 . . . , Xr the variable U reduces to a Baire function of 
Xl5 . . . , Xr and will be denoted by E(Y | Xl5 . . . , Xr). 
The existence of E(Y | 23) isestablished by the method indicated in example 
(b) using an abstract Radon-Nikodym theorem. 
To see the main properties of the conditional expectation U = E(Y J 23) 
note that A1.2) holds trivially when 1B is replaced by a linear combination 
of indicators of sets Bj in 23. But we saw in IV,3 that every 23-measurable 
function can be uniformly approximated by such linear combinations. 
Passing to the limit we see that A1.2) implies that more generally E(YZ) = 
= E(UZ) for any 23-measurable function Z. Replacing Z by ZAB and 
comparing with the definition A1.2) we see that 
A1.6) E(YZ | 23) = Z E(Y | 23) 
for any ^-measurable function Z. This is a relation of great importance. 
Finally, consider a or-algebra 23O c 23 and let Uo = E(Y | <80). For a 
set B in 23O we can interpret A1.2) relative to 23O as well as relative to 23, 
and thus we find that for B in 23O 
i?) = E(U017i). 
Thus by the very definition Uo = E(U [ 23O), and so 
A1-7) E(Y | 23O) = E(E(Y | 23)<8O) if 23O c 23. 
For example, 23 may be the algebra generated by the two variables Xl5 X2 
V.I2 PROBLEMS FOR SOLUTION 165 
while 33O stands for the algebra generated by Xx alone. Then A1.7) 
reduces to E(Y | Xx) = E(E(Y | Xx, X2) | Xx). 
Finally we note that A1.2) implies that for the constant function 1 the 
conditional expectation equals 1, no matter how 93 is chosen. Thus A1.6) 
implies that E(Z | 93) = Z for all' ^-measurable variables Z 
It is hardly necessary to say that the basic properties of expectation carry 
over to conditional expectation. 
12. PROBLEMS FOR SOLUTION 
1. Let X and Y be independent variables with distribution functions F and 
G. Find the distribution functions of22 (a) X u Y, (b) X n Y, (c) 2X u Y, (d) 
X3 uY. 
2. Mixtures. Let X, Y, Z be independent; X and Y have distributions F 
and G, while P{Z = 1} =/?, P{Z = 0} = ^ (/> + ^ = 1). Find the distribution 
functions of (a) ZX + A - Z)Y, (b) ZX + A - Z)(X u Y), (c) ZX + 
A - Z)(X n Y). 
3. If F is a continuous distribution function show that 
JM-oo [i 
F(x)F{dx) = 2/4/ = 
—oo Jo 
(a) from the very definition of the first integral (partitioning — oo, oo into sub- 
intervals) and (b) from the interpretation of the left side as E(F(X)) where F(X) 
has a uniform distribution. More generally, putting G(x) = Fn(x), 
J' 
Fk(x)G{dx} = 
n + k 
4. Let F{x, y) stand for a probability distribution in the plane. Put U(x, y) = 0 
when x < 0 and y < 0, and U(x,y) = F(x,y) at all other points. Show that 
U is monotone in each variable but is not a probability distribution. [Hint: 
Consider the mixed differences.] 
5. Prescribed marginal distributions?* Let F and G be distribution functions 
in Jl1 and 
U(x, y) = F(x)G(y)[l + a(l - F 
where |a| ^ 1. Prove that U is a distribution function in Jl2 with marginal 
distributions F, G and that U has a density iff F and G have densities. 
Hint: If w(x, y) = u(x)v(y), the mixed differences of w [defined in A.12)] 
are of the form Am Ay. Note also that A(F2) <, 2 AF. 
22 If a and b are numbers, a u 6 = max (a, i) denotes the larger of the two, 
a n b = min (a, b) the smaller. For functions fvg denotes the function which at the 
point x assumes the value f(x) ng(x) (seelV.l). Thus X u Y and X n Y are random 
variables. 
23 This problem contains a new example for a non-normal distribution with normal 
marginal distributions (see problems 2 and 3 in 111,9). It is due to E. J. Gumbel. 
166 PROBABILITY DISTRIBUTIONS IN 3ir V.I 2 
6. Within the unit square put U(x, y) = x if x <, y and U(x, y) = y if 
x > y. Show that U is a distribution function concentrated at the bisector (hence 
singular). 
7. FrecheVs maximal distribution with given marginal distributions. Let F and 
G be distribution functions in ft1 and U(x, y) = F(x) n G(y). Prove: (a) C/ is 
a distribution function with marginal distributions F and G. (Z>) If V is any 
other distribution function with this property, then V < U. (c) U is concentrated 
on the curve defined by F(x) = G(y), and hence singular. (Problem 5 contains a 
special case.) 
8. Denote by U the uniform distribution in — h, 0 and by T the triangular 
distribution in -h, h [see II, 4(Z>)]. Then F* U and F* F have the densities 
h-\[F(x + h) - F(x)] and h~2 [F(x + y) - F(x - y)] dy. 
Jo 
9. The independent variables X and Y have Poissort distributions with ex- 
pectations pt and qt. If Ik is the Bessel function defined in II, G.1) show that 
P{X - Y = k) = e~-W(pla)*IMBtVpa). 
10. For a distribution function F such that 
= 
J 
eaxF{dx) 
exists for — a < a < a we define a new distribution F# by (p(a.)F#{dx} 
= e*xF{dx}. Let Fx and F2 be two distributions with this property and F 
F2. Prove that (with obvious notations) 9?(a) = ^(a) ^2(a) anc^ ^ 
11. Let F have atoms a1? a2,. . . with weights p1,p2,-- • Denote by p 
the maximum of p!,p2, • • • • Using lemma 1 of section 4a prove 
(a) The atoms of F+ F have weights strictly less than p except if F is con- 
centrated at finitely many atoms of equal weight. 
(b) For the symmetrized distribution °F the origin is an atom of weight 
p = ^/?v- Tne weights of the other atoms are strictly less than p . 
12. Random vectors in ft3. Let L be the resultant of two independent unit 
vectors with random directions (that is, the endpoints are distributed uniformly 
over the unit sphere). Show that P{L <, t) = /2/2 for 0 < / < 2. [See example 
4(e).] 
13. Let the Xk be mutually independent variables assuming the values 0 and 
1 with probability \ each. In example 4(d) it was shown that X = ^ 2~A:XA: is 
uniformly distributed over OTT. Show that ]T 2~3kX3k has a singular distribution. 
14. (a) If F has a density / such that p is integrable, then the density /2 
of Fie F is bounded. 
(b) Using the mean approximation theorem of IV,2 show that // / is bounded 
then f2 is continuous. 
[If / is unbounded near a single point it can happen that fn* is unbounded for 
every n. See example XI,3(a).] 
15. Using Schwarz' inequality show that if X is a positive variable then 
E(X-") > (E(X^))-1 for all p > 0. 
V.I2 PROBLEMS FOR SOLUTION 167 
16. Let X and Y have densities / and g such that fix) >g(x) for x < a 
and fix) <, g(x) for x > a. Prove that E(X) <, E(Y). Furthermore, if fix) = 
= g(x) = 0 for x < 0 then E(X*) <, E(Y*) for all A:. 
17. Let X1? X2,... be mutually independent with the common distribution F. 
Let N be a positive integral-valued random variable with generating function 
P(s). If N is independent of the X,. then max [X1?.. . , XN] has the distribution 
nn 
18. Let X1?. .. , Xn be mutually independent with a continuous distribution 
F. Let X = max [Xlf... , XJ and Y = min [X1?.. . , XJ. Then 
P{X ^ z, Y > y) = (F(z) - F(y))n for 2/ < x 
and 
P{Y > 2/1 X = x] = [(F(x)-F(y))lF(x)]»-\ 
19. Using the same notations one has for each fixed k < n 
for x < t 
Derive this (a) by an intuitive argument considering the event {Xk = X}, and 
(b) formally from (9.4). 
20. Continuation. Prove that 
21. Random sums. In example 9(c) let Xk equal 1 and —1 with probabilities 
p and q = 1 — p. If N is a Poisson variable with expectation / the distribution 
of 5N is identical with the distribution occurring in problem 9. 
22. Mixtures. Let the distribution G in (9.9) have expectation m(x) and 
variance o2(x). Prove that the mixture U has expectation and variance 
J*+QO /M-00 /»QO 
m{x)p{dx}, b = o\x)i*{dx} + ( 
— 00 J—00 J—00 
23. With obvious notations E(E(Y | X)) = E(Y) but 
Var (Y) = E(Var (Y | X)) + Var (E(Y | X)). 
Problem 22 is a special case. 
24. Random sums. In example 9(c), E(SN) = E(N)E(X), 
Var (SN) = E(N) Var (X) + (E(X)J Var (N). 
Prove this directly and show that it is contained in the last two problems. 
Note. The following problems refer to convolutions of unimodal distributions 
defined in footnote 19 of section 9. It has been conjectured that the convolution 
of two such distributions is again unimodal. One counterexample is due to K. L. 
Chung, and problem 25 contains another. Problem 26 shows the conjecture to 
be valid for symmetric2* distributions. This result is due to A. Wintner. 
24 For the difficulties arising in the unsymmetric case see I. A. Ibragimov, Theory of 
Probability and Its Applications, vol. 1 A956) pp. 225-260. [Translations.] 
168 PROBABILITY DISTRIBUTIONS IN %r V.I 2 
25. Let u(x) = 1 for 0 < x < 1 and u{x) = 0 elsewhere. Put 
e (x\ 1 - c (x\ 
v(x) =-.u[-\ + ——u[ r) 
a \a) b \b) 
where 0 < a < b. If e and a are small and b large, then w = v *v is not 
unimodal although v is. 
Hint to avoid calculations: The convolution of two uniform densities is the 
triangular density and hence w(a) > e2a~x and w(b) > e2b~1 and the integral of 
w from b to 2b is >|A—cJ. It follows that w must have a minimum between 
a and b. 
26. Let F be a uniform distribution and G unimodal. If both F and G are 
symmetric show by simple differentiation that the convolution FirG is uni- 
modal. Conclude (without further calculations) that the statement remains true 
when F is any mixture of symmetric uniform distributions, and hence that the 
convolution of symmetric unimodal distributions is unimodal. 
CHAPTER VI 
A Survey of Some Important 
Distributions and Processes 
This chapter is the product of the deplorable need to avoid repetition and 
cross references between chapters intended for independent reading. For 
example, the theory of stable distributions will be developed independently 
by semi-group methods (IX), by Fourier analysis (XVII), and—at least 
partly—by Laplace transforms (XIII). Giving the definitions and examples 
at a neutral place is economical and makes it possible to scrutinize some 
basic relations without regard to purity of methods. 
The miscellaneous topics covered in this chapter are not necessarily 
logically connected: the queuing process, has little to do with martingale 
theory or stable distributions. The chapter is not intended for consecutive 
reading; the individual sections should be taken up as occasion arises or 
when their turn comes up. Sections 6-9 are somewhat interrelated, but 
independent of the rest. They treat some important material not covered 
elsewhere in the book. 
1. STABLE DISTRIBUTIONS IN ft1 
Stable distributions play a constantly increasing role as a natural general- 
ization of the normal, distribution. For their description it is convenient to 
introduce the short-hand notation 
A.1) U = V 
to indicate that the random variables U and V have the same distribution. 
Thus U = aV + b means that the distributions of U and V differ only by 
location and scale parameters. (See definition 1 in V,2.) Throughout this 
section X, Xlt X2,. . . denote mutually independent random variables with a 
common distribution R and Sn = Xx -\ + Xn. 
169 
170 SOME IMPORTANT DISTRIBUTIONS AND PROCESSES VI. 1 
Definition 1. The distribution R is stable (in the broad sense) if for each n 
there exist constants cn > 0, yn such that1 
A.2) Sn±cnX+yn 
and R is not concentrated at one point. R is stable in the strict sense if A.2) 
holds with yn = 0. 
Examples will be found in section 2. An elementary derivation of some 
basic properties of stable distributions is so instructive that we proceed 
with it at the cost of some repetition. The systematic theory developed 
in chapters IX and XVII does not depend on the following discussion. 
Theorem 1. The norming constants are of the form cn = nxl* with 
0 < a < 2. The constant a will be called the characteristic exponent of R. 
Proof. The argument, is greatly simplified by symmetrization. If R is 
stable so is the distribution °R of Xx — X2 and the norming constants cn 
are the same. It suffices therefore to prove the assertion for. a symmetric 
stable R. 
We start from the simple remark that Sm+n is the sum of the independent 
variables Sm and Sm+n — Sm distributed, respectively, as cmX and cnX 
Thus for symmetric stable distributions 
Similarly, the sum Srk can be broken up into r independent blocks of k 
terms each, whence crk = CrCk for all r and k. For subscripts of the form 
n = ry we conclude by induction that 
A.4) if n = ry then cn = cyr. 
Next put v = m+ n and note that because of the symmetry of the 
variables in A.3) we have for t > 0 
A.5) P{X > t) ? |P{X2 > tcjcn). 
It follows that for v > n the ratios cjcv remain bounded. 
To any integer r there exists a unique a such that cr = r1'*. To prove 
that cn = nxla it suffices to show that if cp — pllp then /? = a. Now by 
A.4) 
if n = r* then cn = n1/a 
if v = pk then cv = vvp. 
But for each v = pk there exists an n = rj such that n < v < rn. Then 
cv = vu' < (rnI" = r1 
1 For an alternative form see problem 1. 
VI. 1 STABLE DISTRIBUTIONS IN ft1 171 
Since the ratios cjcv remain bounded this implies that /? <> a. Inter- 
changing the roles of r and p we find similarly that /? ^ a and hence 
To prove that a <, 2 we remark that the normal distribution is stable 
with a = 2. For it A.3) reduces to the addition rule for variances, and the 
latter implies that any stable distribution with finite variances necessarily 
corresponds to a = 2. To conclude the proof it suffices therefore to show 
that any stable distribution with a > 2 would have a finite variance. 
For symmetric distributions A.2) holds with yn = 0, and hence we can 
choose a t such that P{|SJ > tcn} < J for all n. For reasons of symmetry 
this implies that n[l—R(tcn)] remains bounded [see V,E.10)]. It follows 
that xa[\ — R(x)] < M for all x > t and an appropriate constant M. 
Thus the contribution of the interval 2k~1 < x < 2* to the integral for 
E(X2) is bounded by M2(%-*)k, and for a > 2 this would be the general 
term of a convergent series. >¦ 
The theory of stable distributions simplifies greatly by the gratifying fact 
that the centering constants yn may be disregarded in practice. This is so 
because we are free to center the distribution R in an arbitrary manner, 
that is, we may replace R(x) by R(x+b). The next theorem shows that, 
except when a = 1, we can use this freedom to eliminate yn from A.2). 
Theorem 2. If R is stable with an exponent a^ 1 the centering constant 
b may be chosen so that R(x + b) is strictly stable. 
Proof. Smn is the sum of m independent variables each distributed as 
cnX + yn. Accordingly 
A-6) Smn = cnSm + myn = cncmX + cnym + myn. 
Since m and n play the same role this means that we have identically 
0-7) (cn-n)ym = (cm-m)yn. 
When a.= 1 this statement is empty,2 but when a^ 1 it implies that 
yn = b(cn—n) for all n. From A.2) one sees finally that the sum S^ of n 
variables distributed as X'. — b satisfies the condition S^ = cnX'. >- 
The relation A.3) was derived from A.2) under the sole assumption that 
yn = 0 and holds therefore for all strictly stable, distributions. It implies that 
A.8) s^X, + t1/aX2 4 (s+t)VaX 
whenever the ratio sjt is rational A simple continuity argument3 leads to 
2 For the case a = 1 see problem 4. 
3 Concerning the continuity of stable distributions see problem 2. 
172 SOME IMPORTANT DISTRIBUTIONS AND PROCESSES VI. 1 
Theorem 3. If R is strictly stable with exponent a then A.8) holds for 
all s>0 and t > 0. 
For the normal distribution A.8) merely restates the addition rule for the 
variances. In general A.8) implies that all linear combinations a1X2 4- a2X2 
belong to the same type. 
The importance of the normal distribution 91 is due largely to the 
central limit theorem. Let X1?. . . , Xn be mutually independent variables 
with a common distribution F having zero expectation and unit variance. 
Put Sn = Xx + • • • + Xn. The central limit theorem4 asserts that the 
distribution of Snn^ tends to 91. For distributions without variance 
similar limit theorems may be formulated, but the norming constants must 
be chosen differently. The interesting point is that all stable distributions 
and no others occur as such limits. The following terminology will facilitate 
the discussion of this problem. 
Definition 2. The distribution F of the independent random variables Xk 
belongs to the domain of attraction of a distribution R if there exist norming 
constants an > 0, bn such that the distribution of a~1(Sn—bn) tends to R. 
Our last statement can now be reformulated to the effect that a distribution 
R possesses a domain of attraction iff it is stable. Indeed, by the very definition 
each stable R belongs to its own domain of attraction. That no other 
distribution appears as limit becomes plausible by the argument used in 
theorem 1. 
Our results have important and surprising consequences. Consider, 
for example, a stable distribution satisfying A.8) with a < 1. The average 
(XXH hXn)//i has the same distribution as X^*1/", and the last 
factor tends to oo. Roughly speaking we can say that the average of n 
variables is likely to be considerably larger than any given component Xk. 
This is possible only if the maximal term Mn = max [Xl5 . . . , Xn] is likely 
to grow exceedingly large and to receive a preponderating influence on the 
sum Sn. A closer analysis bears out this conclusion. In the case of positive 
variables the expectation of the ratio Sn/Mn tends to A —a), and this is 
true also for any sequence {XJ whose distribution belongs to the domain of 
attraction of our stable distribution. (See problem 26 of XIII, 11.) 
Note on history. The general theory of stable distributions was initiated5 by P. Levy 
A924), who found the Fourier transforms of all strictly stable distributions. (The others 
4 The central limit theorem proves that the normal distribution is the only stable distri- 
bution with variance. 
5 The Fourier transforms of symmetric ^stable distributions were mentioned by Cauchy, 
but it was not clear that they.really corresponded to probability distributions. This point 
was settled by G. Polya for the case a < 1. The Holtsroark distribution of example 2(c) 
was known to astronomers, but not to mathematicians. 
VI.2 EXAMPLES 173 
were originally called quasi-stable. As we have seen, they play a role only when a = 1, 
and this case was analyzed jointly by P. Levy and H. Khintchine.) A new and simpler 
approach to the whole theory was made possible by the discovery of infinitely divisible 
distributions. This new approach (still based on Fourier analysis) is also due to P. Levy 
A937). The interest in the theory was stimulated by W. Doblin's masterful analysis of 
the domains of attraction A939). His criteria were the first to involve regularly varying 
functions. The modern theory still carries the imprint of this pioneer work although 
many authors have contributed improvements and new results. Chapter XVIII contains 
a streamlined treatment of the theory by the now classical Fourier methods, while chapter 
IX presents the same theory by a direct approach which is more in line with modern 
methods in Markov processes. Great simplifications and a unification of many criteria 
were made possible by the systematic exploitation of J. Karamata's theory of regularly 
varying functions. An improved version of this theory is presented in VIII,8-9. 
2. EXAMPLES 
(a) The normal distribution centered to zero expectation is strictly stable 
with cn = \ln. 
(b) The Cauchy distribution with arbitrary location parameters has density 
1 c 
The convolution property 11,D.6) shows that it is stable with a = 1. 
(c) Stable distribution with a = \. The distribution 
B.1) Fix) = 2[1 - 91A/7*)], x > 0 
with density 
B.2) fix) = -pLr e~1/2l*\ x>0 
\f2nx3 
[and fix) = 0 for x < 0] is strictly stable with norming constants cn = n\ 
This can be shown to be elementary integrations, but it is preferable to 
take the assertion as a consequence of the fact that F has a domain of 
attraction. Indeed, in a symmetric random walk let Sr be the epoch of the 
rth return to the origin. Obviously Sr is the sum of r independent identically 
distributed random variables (the waiting times between successive returns). 
Now it was shown at the end of 1; 111,G.7) that 
B.3) P{Sr < rH} — F{t) r -> oo. 
Thus F has a domain of attraction and is therefore stable. [Continued 
in example (e).] 
(d) The gravitational field of stars (Holtsmark distribution). In astro- 
nomical terms the problem is to calculate the x-component of the gravi- 
tational force exercised by the stellar system at a randomly chosen point O. 
The underlying idea is that the stellar system appears as a "random aggregate" 
174 SOME IMPORTANT DISTRIBUTIONS AND PROCESSES VI.2 
of points with "randomly varying masses." These notions could be made 
precise in terms of Poisson distributions, etc., but fortunately no subtleties 
are required for the problem at hand. 
Let us agree to treat the density of the stellar system as a free parameter 
and to let Xx stand for the ^-component of the gravitational force of a 
stellar system with density A. We seek the conceivable types of such 
distributions. Now the intuitive notion of a "random aggregate of stars" 
presupposes that two independent aggregates with densities s and t may be 
combined into a single aggregate of density s + t.> Probabilistically this 
amounts to the postulate that the sum of two independent variables 
distributed as Xs and Xt should have the same distribution as Xs+t. We 
indicate this symbolically by 
B.4) Xf+X,4xH.l. 
Considering that a change of density from 1 to A amounts to a change of 
the unit of length from 1 to l/'VA and that the gravitational force varies 
inversely with the square of the distance we see that Xt must have the 
same distribution as t$Xv This means that the distributions of Xt differ 
only by a scale parameter and B.4) reduces to A.8) with a = f. In other 
words, Xx has a Symmetric Stable distribution with exponent f. It will turn 
out that (up to the trivial scale parameter) there exists exactly one such 
distribution, and so we have solved our problem without appeal to deeper 
theory. The astronomer Holtsmark obtained an equivalent answer by 
other methods (see problem 7) and, remarkably, before P. Levy's work. 
(e) First-passage times in Brownian motion. We start from the notion 
of a one-dimensional diffusion process, that is, we suppose that the in- 
crements X(j+O — X(s) for non-overlapping time intervals are inde- 
pendent and have a symmetric normal distribution with variance /. We 
assume as known that the paths depend continuously on time. If X@) = 0 
there exists an epoch Ta at which the particle reaches the position a > 0 
for the first time. To derive the distribution function Fa(t) = P{Ta < /} 
we observe that the notion of an additive process presupposes a complete 
lack of after-effect (the strong Markov property). This means that the incre- 
ment X(r+Ta) — a of the abscissa between epochs Ta and Ta + / is 
independent of the process before Ta. Now to reach a position a+b > a 
the particle must first reach a, and we conclude that the.residual waiting 
time Ta+6 — Ta before reaching a + b is independent of Ta and has the 
same distribution as T6. In other words, Fa * Fh — Fa+b. But the transition 
probabilities depend only on the ratio x2jt and therefore Ta must have the 
same distribution as a2Tt. This means that the distributions F^ differ 
only by a scale parameter and hence they are stable with exponent a = ?. 
VI. 2 EXAMPLES 175 
This argument, based on dimensional analysis, proves the stability of 
the first-passage distribution but does not lead to an explicit form. To 
show that F coincides with the distribution of example (c) we use a reasoning 
based on symmetry (the so-called reflection principle). Because of the 
assumed continuity of paths the event {X(t) > a} can occur only if the level 
a has been crossed at some epoch Ta < t. Given that Ta = t < t we have 
X(t) = a, and for reasons of symmetry the probability that X(t) — X(t) > 0 
is \. We conclude that 
[Z.J) x \la <. t/ = Z.r\\\t) Z> a) =¦ L\Y — Vi\aj\ t)\ 
which is equivalent to B.1). 
(/) Hitting points in two-dimensional Brownian motion. A two-dimensional 
Brownian motion is formed by a pair (X(t), Y(t)) of independent one- 
dimensional Brownian motions. We are interested in the point (a, Za) at 
which the path first reaches the line x = a > 0. As in the preceding example 
we note that the path can reach the line x = a+b > a only after crossing 
the line x = a; taking (a^ZJ as new origin we conclude that Za+6 has 
the same distribution as the sum of two independent variables distributed 
as Za and Z6. Now an obvious similarity consideration shows that Za has 
the same distribution as aZ1 and we conclude that Zo has a symmetric 
stable distribution with exponent a = 1. Only the Cauchy distribution fits 
this description, and so the hitting point Za has a Cauchy distribution. 
This instructive dimensional analysis does not determine the scale param- 
eter. For an explicit calculation note that Za = Y(Ta) where Ta is the 
epoch when the line x = a is first reached. Its distribution is given in B.5) 
while Y(r) has normal density with variance t. It follows that Zn has a 
density given by6 
B.6) fe-W ae-*1* ,. a 
7r(a2+x2) ' 
(We have here an example for the subordination of processes to which we 
shall return in X,7.) 
(g) Stable distributions in economics. Arguments related to the dimen- 
sional analysis in the last two examples have been used by B. Mandelbrot 
to show that various economic processes (in particular income distributions) 
should be subject to stable (or "Levy-Pareto") distributions. So far the 
strength of this interesting theory, which has attracted attention among 
economists, resides in the theoretical argument rather than observations. 
[For the apparent fit of the tails of the distribution to many empirical 
phenomena from city size to word frequency see II,4(/j).] 
The substitution y — i(x2+a2)/t reduces the integrand to e~y. 
176 SOME IMPORTANT DISTRIBUTIONS AND PROCESSES VI .3 
(h) Products. There exist many curious relations between stable distri- 
butions of different exponents. The most interesting may be stated in the 
form of the following proposition. Let X and Y be independent strictly 
stable variables with characteristic exponents a and /? respectively. 
Assume Y to be a positive variable (whence fi < 1). The product XY1'" 
has a stable distribution with exponeni a/5. In particular, the product cf a 
normal variable and the square root of the stable variable of example (?) is a 
Cauchy variable. 
The assertion follows as a simple corollary to a theorem concerning 
subordinated processes7 [example X,7(c)]. Furthermore, it is easily verified 
by Fourier analysis (problem 9 of XViI,12) and, for positive variables, also 
by Laplace transforms [XIII,7(<?) and problem 10 of XIII,11]. 
3. INFINITELY DIVISIBLE DISTRIBUTIONS IN ft1 
Definition 1..A distribution F is infinitely divisible if for every n there 
exists a distribution Fn such that F = F%*. 
In other words,8 F is infinitely divisible iff for each n it can be represented 
as the distribution of the sum Sn = Xln + •••+' Xnn of n independent 
random variables with a common distribution Fn. 
This definition is valid in any number of dimensions, but for the present 
we shall limit our attention to one-dimensional distributions. It should be 
noted that infinite divisibility is a property of the type, that is, together 
with F all distributions difftring from F only by location parameters are 
infinitely divisible. Stable distributions are infinitely divisible and dis- 
tinguished by the fact that Fn differs from F only by location parameters. 
Examples, (a) On account of the convolution property 11,B,3) all gamma 
distributions (including the exponential) are infinitely divisible. That the 
same is true of their discrete counterpart, the "negative binomial" (including 
the, geometric) distributions was shown in 1; XII,2(e). 
(b) The Poisson and the compound Poisson distributions are infinitely 
divisible. It will turn out that all infinitely divisible distributions are limits 
of compound Poisson distributions. 
7 For a direct verification requiring a minimum of calculations find the distribution of 
Z = Xi^/Y1 + X2y/Y2 by first calculating the conditional distribution of Z given that 
Yx = yx and Y2 = y2. The distribution of Z is a function of yx + y2 and the change of 
variables u — yx + y2, v — yx — y2 shows that it differs only by a scale factor from that 
of the two summands. The same calculation works for sums of n similar terms. 
8 It should be understood that the random variables Xfc#n serve merely to render 
notations simpler and more intuitive. For fixed n the variables Xln,. .. , X^ n are 
supposed to be mutually independent, but the variables Xi-m and Xkn with m 5* n need 
not be defined on the same probability space. (In other words, a joint distribution for 
Xfc m and Xk n need not exist.) This remark applies to triangular arrays in general. 
VI.3 INFINITELY DIVISIBLE DISTRIBUTIONS IN ft1 177 
(c) The distribution 11,G.13) connected with Bessel functions is infinitely 
divisible but this is by no means obvious. See example XIII,7(</). 
(d) A distribution F carried by a finite interval is not infinitely divisible 
exceptif it is concentrated at one point. Indeed, if |Sn| <a with probability 
one then |Xfc.B| < an'1 and so Var (Xfc> J < a*rr*. The variance of F is 
therefore <a2n~1 and hence zero. >. 
Returning to definition 1 let us consider what happens if we drop the 
requirement that the Xkn have the same distribution and require only 
that for each n there exist n distributions Fln,... , Fn_n such that 
C-D F = F1>n* • • • * Fn,n. 
Such generality leads to a new phenomenon best illustrated by examples. 
Examples, (e) If F is infinitely divisible and U arbitrary, G — U-k F 
can be written in the form C.1) with Gln = U and all other Gkn equal 
t0 Fn-i- Here the first component plays an entirely different role from all 
other components. 
(/) Consider a convergent series X = ]? Xk of mutually independent 
random variables. The distribution F of X is the convolution of the 
distributions of Xx, X2,.. . , Xn_t and the remainder (Xn+Xnfl-f- • •) 
and so F is of the form C.1). Such distributions will be studied under the 
name of infinite convolutions. Example 1,11 (c) shows the uniform distri- 
bution to be among them. > 
The distinguishing feature of these examples is that the contribution of 
an individual component Xv n to Sn is essential, whereas in the case of 
equally distributed components the contribution of each tends to zero. 
We wish to connect infinitely divisible distributions to the typicai limit 
theorems involving "many small components." It is then necessary to 
supplement our scheme by the requirement that the individual components 
Xk n become asymptotically negligible in the sense that for each e > 0, 
C-2) P{!Xfc,n! > €} < e (*=l,...,n) 
for n sufficiently large. In the terminology of VIII,2 this means that the 
Xkn tend in probability to zero uniformly in k = 1,. . . , n. Systems of 
variables of this type appear so often that it is convenient to give them a 
name. 
Definition 2. By a triangular array is meant a double sequence of random 
variables Xkn (k = 1, 2, ...,«; n = 1, 2, . . .) such that the variables 
Xlw7l, . . . , Xnn of the «th row are mutually independent. 
The array is a null array {or has asymptotically negligible components) if 
C.2) holds. 
178 SOME IMPORTANT DISTRIBUTIONS AND PROCESSES VI.3 
More generally one may consider arrays with rn variables in the nth 
row with rn —*¦ oo. The gain in generality is slight. (See problem 10.) 
Example, (g) Let {X,} be a sequence of identically distributed independ- 
ent random variables and Sn = Xx -f • • • + Xn. The normalized sequence 
Sna~x represent the nth row sum of a triangular array in which Xk_„ = 
= Xfcfl. This array is a null array if an -*¦ oo. An array of a different sort 
was considered in the derivation of the Poisson distribution in 1; VI,6. > 
In chapters IX and XVII we shall prove the remarkable fact that a limit 
distribution of the row sums Sn of a triangular null array (if it exists) is 
infinitely divisible. As long as the asymptotic negligibility condition C.2) 
holds it does not matter whether or not the components Xk n have acommon 
distribution, and in C.1) we may replace the equality sign by a limit: the 
class of infinitely divisible distributions coincides with the class of limit 
distributions of the row sums of triangular null arrays. . • 
Examples for applications, (h) The shot effect in vacuum tubes. Variants 
and generalizations of the following stochastic process occur in physics 
and in communication engineering. 
We propose to analyze the fluctuations in electrical currents due to the 
chance fluctuations of the numbers of electrons arriving at an anode. It is 
assumed that the arrivals form a Poisson process, and that an arriving 
electron produces a current whose intensity x time unit later equals I{x). 
The intensity of the current at epoch t is then formally a random variable 
where the Tk represent the epochs of past electron arrivals. (In other words, 
the variables t—T1} T2—T1} T3—T2,. . . are mutually independent and 
have a common exponential distribution.) 
A direct analysis of the sum C.3) by the methods of stochastic processes 
is not difficult, but the simple-minded approach by triangular arrays may 
serve as an aid to intuition. Partition the interval —oo,t into small 
subintervals with endpoints tk — t — kh (where k — 0, 1,...). By the 
very definition of the Poisson process the contribution of the interval 
**» *k-i t0 tne sum m C-3) is comparable to a binomial random variable 
assuming the value 0 with probability I — <xh and I(t—tk) with probability 
aJi. The expectation of this variable is <xh I(kh), its variance cnh(l—a.h)P(kh). 
We take h = \j\n and construct the triangular array in which Xkn is 
the contribution of the interval tk, tk_v The row sums have then expectation 
a.h ^ I(kh) and variance aJi(l —xh) 212(kh). If any meaning can be attached 
VI.4 PROCESSES WITH INDEPENDENT INCREMENTS 179 
to the series C.3) the distributions of the row sums must tend to the distri- 
bution of X@ and so we must have 
C.4) E(X(f)) = a ri(s) dst Var (X@) = a f "'l\s) ds. 
Jo «M> . 
These conclusions are easily confirmed by the theory of triangular arrays. 
The relations C.4) are known as Campbell's theorem. At present it does 
not appear deep, but it was proved in 1909 decades ahead of a systematic 
theory. At that time it appeared remarkable and various proofs have been 
given for it. (Cf. problems 22 in VIII.10 and 5 in XVII,12.) 
(/) Busy trunklines. A variant of the preceding example may illustrate 
the types of possible generalizations. Consider a telephone exchange with 
infinitely many trunklines. The incoming calls form a Poisson process, and 
an arriving call is directed to a free trunkline. The ensuing holding times have 
a common distribution F; as usual, they are assumed independent of the 
arrival process and of each other. The number of busy lines at epoch t is a 
random variable X(t) whose distribution can Be derived by the method of 
triangular arrays. As in the preceding example we partition 0, t into n 
intervals of length h = t\n and denote by Xfcn the number of conversations 
that originated between n — kh and n — (k—\)h 'and are still going on at 
epoch /. When n is large the variable Xkn assumes in practice only the 
values 0 and 1, the latter with probability <xh[\ —F(kh)]. The'expectation of 
Sn is then the sum of these probabilities, and passing to the limit we conclude 
that the number of busy lines has expectation 
C.5) 
Jo 
Note that the integral equals the expectation of the holding times. > 
Historical note. The notion of infinite divisibility goes back to B. de Finetti A929). 
The Fourier transforms of infinitely divisible distributions with finite variance were 
found by A. Kolmogorov A932), and those of the general infinitely divisible distributions 
by P. Levy A934), who also treated the problem from the point of view of stochastic 
processes. All subsequent investigations were strongly influenced by his pioneer work. 
The first purely analytical derivations of the general formula were given in 1937 inde- 
pendently by Feller and Khintchine. These authors proved also that the limit distributions 
of null arrays are infinitely divisible. 
4. PROCESSES WITH INDEPENDENT INCREMENTS 
Infinitely divisible distributions are intimately connected with stochastic 
processes with independent increments. By this we mean a family of random 
variables X{t) depending on the continuous time parameter t and such that 
the in' ements X(tk+1) — X(tk) are mutually independent for any finite set 
180 SOME IMPORTANT DISTRIBUTIONS AND PROCESSES VI.4 
h < h < ' ' < ln- At this juncture we require no theory of stochastic 
processes; we argue simply that //certain phenomena can be described 
probabilistically, the theory will lead to infinitely divisible distributions. In 
this sense we have considered special processes with independent increments 
in 1; XVII,1 and in example III,8(a). We limit our attention to numerical 
variables X(t) although the theory carries over to vector variables. 
The process has stationary increments'if the distribution of X($-\-t) — X(s) 
depends only on the length / of the interval but not on s. 
Let us partition the interval s, s+t by n + 1 equidistant points 
s = h<h<-'<tn = s + t and put Xfc.n = X(tk) - X^). The 
variable X(s-\-t) — X(s) of a process with stationary independent increments 
is the sum of the n independent variables X* n with a common distribution 
and hence X(.s+0 — X(s) has an infinitely divisible distribution. We shall 
see that the converse is also true. In fact, a one-parametric family of 
probability distributions Qt defined for / > 0 can serve as the distribution 
of X(^+0 — X(s) ;n a process with stationary independent increments iff 
D.1) Qs+l = Qs*Qt s,t>0. 
A family of distributions satisfying D.1) is said to form a semi-group see 
IX,2). Every infinitely divisible distribution can be taken as element Qt 
(with / > 0 arbitrary) of such a semi-group. 
Before passing to the non-stationary case let us consider typical examples. 
Examples, (a) The compound Poisson process. With an arbitrary prob- 
ability distribution F and a > 0 
<4.2) 2f = ,--?(^V 
defines a compound Poisson distribution, and it is easily verified that D.1) 
holds. Suppose now that Q( represents the distribution of X(t) — X@) 
in a stochastic process with stationary independent increments. When F 
is concentrated at the point 1 this process reduces to an ordinary Poisson 
process and D.2) to 
D.3) P{X@ - X@) = n} = e~at — . 
n\ 
The general model D.2) may be interpreted in terms of this special 
Poisson process as follows. Let Yx, Y2,. . . be independent variables with 
the common distribution F, and let N(f) be the variable of a pure Poisson 
process with P{N(f) = n] = e-at{<xt)njn\, and independent of the Yk. Then 
D.2) represents the distribution of the random sum Yt + • • • + YN(t). 
In other words, with the nth jump of the Poisson process there is associated 
an effect Yn, and X(/) — X@) represents the sum of the effects occurring 
VI.4 PROCESSES WITH INDEPENDENT INCREMENTS 181 
during 0, /. The randomized random walk studied in 11,7 is a compound 
Poisson process with Yk assuming the values ± 1 only. Empirical applica- 
tions are illustrated at the end of this section. 
(b) Brownian motion or the Wiener-Bachelier process. Here X@) = 0 
(the process starts at the origin) and the increments X(f+.s) — X(s) have 
a normal distribution with zero expectation and variance t. Wiener and 
Levy have shown that the sample functions of this process are continuous 
with probability one, and this property characterizes the normal distribution 
among all infinitely divisible distributions. 
(c) Stable processes. The relation A.8) for a strictly stable distribution 
merely paraphrases D.1) with Qt(x) = R{r~xlax). Thus this distribution 
defines transition probabilities in a process with stationary independent 
increments; for a = 2 it reduces to Brownian motion. > 
The main theorem of the theory (see chapters IX and XVII) states that the 
most general solution of D.1)—and hence the most general infinitely divisible 
distribution—may be represented as a limit of an appropriate sequence of 
compound Poisson distributions. This result is surprising in view of the 
great formal difference between examples (a) and (b). 
Even in non-stationary processes with independent increments the 
distribution of X(t+s) — X(t) appears as the distribution of the row 
sums of our triangular array {Xfcn}, but a slight continuity condition 
must be imposed on the process to assure that C.2) holds. Example (e) will 
explain this necessity. Under a slight restriction only infinitely divisible 
distributions appear as distributions of X(t +s) — X(t). 
Examples, (d) Operational time. A simple change of the time scale will 
frequently reduce a general process to a more tractable stationary process. 
Given any continuous increasing function <p we may switch from the 
variable X(/) to Y(/) = X(<p(t)). The property of independent increments 
is obviously preserved, and with an appropriate choice of <p the new 
process may also have stationary increments. In practice the choice is 
usually dictated by the nature of things. For example, at a telephone 
exchange nobody would compare an hour at night with the busy hour 
of the day, while it is natural to measure time in variable units such that the 
expected number of calls per unit remains constant. Again, in a growing 
insurance business claims will occur at an accelerated rate but this departure 
from stationarity is removed by the simple expedient of introducing an 
operational time measuring the frequency of claims. 
(e) Empirical applications. An unending variety of practical problems 
can be reduced to compound Poisson processes. Here are a few typical 
examples, (i) The accumulated damage due to automobile accidents, fire, 
lightning, etc. For applications to collective risk theory see example 5(a). 
182 SOME IMPORTANT DISTRIBUTIONS AND PROCESSES VI.5 
(ii) The total catch by a fishery boat in search of schools offish (J. Neyman). 
(iii) The content of water reservoirs due to rainfall and demand. Other 
storage facilities are treated in like manner, (iv) A stone at the bottom of a 
river lies at rest for such long periods that its successive displacements are 
practically instantaneous. The total displacement within time 0, t may be 
treated as a compound Poisson process. (First treated by different methods 
by Albert Einstein Jr. and G. Polya.) (v) Telephone calls, or customers, 
arriving at a server require service. Under appropriate conditions the total 
service time caused by arrivals within the time interval 0, t represents a 
compound Poisson process. The remarkable feature of this process is that 
the value of X(t) is not observable at epoch t because it depends on service 
times that still lie in the future, (vi) For energy changes of physical particles 
due to collisions see example X, 1F). > 
¦5. RUIN PROBLEMS IN COMPOUND POISSON 
PROCESSES 
Let X(t) be the variable of a compound Poisson process, that is, the 
increment X(t+s) — X(s) over any time interval of duration t has the 
probability distribution Qt of D.2). Let c > 0 and z > 0 be fixed. By 
ruin we mean the event 
E.1) {X(t)>z + ct}. 
We regard c as a constant and z > 0 as a free parameter, and we denote 
by R(z) the probability that no ruin will ever occur. We shall argue formally 
that if the problem makes sense R(z) must be a non-increasing solution of 
the functional equation E.2). First a few examples may indicate the variety 
of practical situations to which our problem is applicable. 
Examples, (a) Collective risk theory.9 Here X(f) stands for the accumu- 
lated amount of claims within the time interval 0, t against an insurance 
company. If is assumed that the occurrence of claims is subject to a Poisson 
process and that, the individual claims have the distribution F. In principle 
these "claims" may be positive or negative. (For example, a death may free 
the company of an obligation and increase the reserves.) In practice a grow- 
ing company will measure time in operational units proportional to the 
* This section treats a special topic. It is of great practical interest, but will not be 
referred to in the present book except for examples where it will be treated by new methods. 
9 A huge literature i& devoted to this theory (inaugurated by F. Lundberg). For a 
relatively recent survey see H. Cramer, On some questions connected with mathematical risk, 
Univ. Calif. Publications in Statistics, vol. 2, no. 5 A954) pp. 99-125. Cramer's asymptotic 
estimates (obtained by deep Wiener-Hopf techniques) are obtained in an elementary manner 
in examples XI,7(a) and Xll,5(d). 
VI.5 RUIN PROBLEMS IN COMPOUND PO1SSON PROCESSES 183 
total incoming premiums [see example 4(d)]. It may then be assumed 
that in the absence of claims the reserves increase at a constant rate c. 
If 2 stands for the initial reserves at epoch 0, the company's total reserve at 
epoch t is represented by the random variable z + ct — X@, and "ruin" 
stands for a negative reserve, that is, failure. 
(b) Strong facilities. An idealized water reservioir is being filled by 
rivers and rainfall at a constant rate c. At random intervals the reservoir 
is tapped by amounts Xx, X2,. .. . The compound Poisson model applies 
and if z stands for the initial content at epoch 0 then z + ct — X(t) 
represents the content at epoch t provided that no ruin occurs before t. 
For the huge literature on related problems see the monographs listed at the 
end of the book. 
(c) Scheduling of patients.10 We agree to treat the times devoted by a 
doctor to his patients as independent random variables with an exponential 
distribution and mean duration a-1. As long as treatments continue without 
interruption the departures of treated patients are subject to an ordinary 
Poisson process. Let X(t) stand for the number of such departures within 
0, t. Suppose that z patients are waiting at epoch 0 (the beginning of the 
office hours) and that thereafter new patients arrive at epochs c~\ 2c1, 
3c-1,.... The doctor will not be idle as long as X(t) < z + ct. &• 
The following formal argument leads to an equation determining the 
probability of ruin R. Suppose that the first jump of the sample function 
occurs at epoch t and has magnitude x. For no ruin ever to occur it is 
necessary that x < z + ct and that for all t > t the increments X(t) — x 
be <, z — x + ct. Such increments being independent of the past the latter 
event has probability R(z—x+cr). Summing over all possible t and x we 
get 
E.2) R(z) = y.e-aTdr R(z+Ct-x) F{dx). 
JO J— oo 
This is the desired equation, but it can be simplified. The change of variable 
s = z -\~ ct leads to 
E.3) R(z) = - I %-<«/*><•-«> rfs f R(s-x) F{dx}. 
C Jz J-oo 
Consequently R is differentiable, and a simple differentiation leadr. to the 
final integro-differential equation 
E.4) R'(z) = - R(z) - - p R(z - x)F{dx). 
10 R. Pyke, The supremum andinfimum of the Poisson process,, Ann. Math. Statist., vol. 
30 A959) pp. 568-576. Pyke treats only the pure Poisson process but obtains more precise 
results (by different methods). 
184 SOME IMPORTANT DISTRIBUTIONS AND PROCESSES VI.6 
Note that by definition R(s) = 0 for s < 0 so that the integral on the 
right is the convolution F+ R. We shall return to E.4) in examples 9(d); 
XI,7(a); and XII,5(</). 
6. RENEWAL PROCESSES 
The basic notions of renewal theory were introduced in 1; XIII in 
connection with recurrent events. It will be seen that the introduction of a 
continuous time parameter depends on notational, rather than conceptual, 
changes. The salient feature of recurrent events is that the successive waiting 
times I1* are mutually independent random variables with a common 
distribution F\ the epoch of the nth. occurrence is given by the sum 
F.1) Sn = T1 + ---+Tn. 
By convention So = 0 and 0 counts as occurrence number zero. 
Even in stochastic processes depending on a continuous time parameter 
it is frequently possible to discover one or more sequences of epochs of 
the form F.1). In such cases surprisingly sharp results are obtainable by 
simple methods. Analytically we are concerned merely with sums of 
independent positive variables, and the only excuse for introducing the term 
"renewal process" is its frequent occurrence in connection with other 
processes and the tacit implication that the .powerful tool of the renewal 
equation is used.11 
Definition 1. A sequence of random variables Sn constitutes a renewal 
process if it is of the form F.1) where the Tfc are mutually independent 
variables with a common distribution F such that12 F@) = 0. 
The variables being positive there is no danger i 
eveni 
i i 
i i 
• i 
m 
the integral diverges (in which case we write fi = oo). The expectation 
fi will be called mean recurrence time. As usual in similar situations, it is 
irrelevant for our present analysis whether the variables Tfc occur in some 
stochastic process or whether the sequence {T,} itself defines our probability 
space. 
In most (but not all) applications the T, can be interpreted as "waiting 
times" and the Sn are then referred to as renewal {or regeneration) epochs. 
i 1 
It seems intuitively obvious that for a fixed finite interval I = a,b the 
number of renewal epochs Sn falling within / is finite with probability 
11 For a more sophisticated generalization of the recurrent events see J. F. C. Kingman, 
The stochastic theory of regenerative events, Zeitschrift Wahrscheinlichkeitstheorie, yol. 2 
A964) pp. 180-224. 
12 An atom of weight p < 1 at the origin would have no serious effect. 
VI.6 RENEWAL PROCESSES 185 
one and hence is a well-defined random variable N. If the event {Sn e /} 
is called "success" then N is the total number of successes in infinitely many 
trials and its expectation equals 
F.2) } f 
n=0 n=0 
For the study of this measure we introduce, as usual, its distribution function 
defined by 
F.3) U(x) = | 
It is understood that U(x) = 0 for x < 0, but U has an atom of unit 
weight at the origin. 
In the discrete case studied in 1; XIII the measure U was concentrated 
at the integers: uk stood for the probability that one among the Sn equals 
k. Since this event can occur only once, uk can also be interpreted as the 
expected number of n for which Sn = k. In the present situation ?/{/} 
must be interpreted as an expectation rather than as a probability because 
the event {Sne/} can occur for many n. 
It is necessary to prove that U(x) < oo. From the definition of con- 
volutions for distributions concentrated on 0, oo it is clear that 
Fn *(x) < Fn(x), and hence the series in F.3) converges at least geometrically 
at each point where F(x) < 1. There remains the case of distributions 
concentrated on a finite interval, but then there exists an integer r such that 
Fr*(x) < 1. The terms with n = r, 2r, 3r,... . form a convergent subseries, 
and this implies the convergence of the whole series in F.3) because its terms 
depend monotonically on n. 
As in the discrete case the renewal measure U is intimately connected with 
the renewal equation 
F.4) Z = z+F*Z 
Spelled out it reads 
F.5) Z(x) « z(x) + (XZ(x-y) F{dy}, x > 0, 
Jo 
where the interval of integration is considered closed. Actually the limits of 
integration may be replaced by — oo and oo provided it is understood that 
z(x) = Z(x) = 0 for x < 0. We shall adhere to this convention. 
The basic fact concerning the renewal equation is contained in 
Theorem 1. If z is bounded and vanishes for x < 0 the convolution 
Z= U+z defined by 
F.6) 
Z(x) = \Xz{x-y) U{dy} 
Jo 
186 SOME IMPORTANT DISTRIBUTIONS AND PROCESSES VI.6 
represents a solution of the renewal equation F.5). There exists no other 
solution vanishing on — oo, 0 and bounded on finite intervals. 
Proof. We know already that the series F.3) defining U con- 
verges for all x. Taking the convolution with z it is seen that Z is 
bounded on finite intervals and satisfies the renewal equation. The difference 
of two such solutions would satisfy V=F-k V, and hence also V = 
= F"** V for all n. But Fn*(x)-+0 for all x, and if V is bounded this 
implies that V(x) = 0 for all x. ^ 
We shall return to the renewal equation in XI, 1 where we shall study the 
asymptotic properties of U and Z. (For a generalized version of the renewal 
equation see section 10.) 
It should be noticed that U itself satisfies 
F.7) . U(x) = 1 + f U(x-y) F{dy}, x > 0, 
Jo 
which is the special case of the renewal equation with z = 1. This can be 
seen directly by a probabilistic reasoning known as "renewal argument" 
which is of frequent use. Since 0 counts as a renewal epoch the expected 
number of renewal epochs in the closed interval 0, x is one plus the expected 
number in the half-open interval 0, x. This interval contains renewal 
epochs only if Tx < x\ given that Tx = y < x, the expected number of 
renewal epochs in 0, x equals U(x—y). Summing r«ver y we get F.7). 
Two simple generalizations of the renewal process are useful. First, by 
analogy with transient recurrent events we may permit defective distributions. 
The defect q = 1 — F(co) is then interpreted as probability of termination. 
Abstractly speaking, the real line is enlarged by a point Q. called "death," 
and Tfc is either a positive number or Q. For ease of reference we introduce 
the informal 
Definition 2.13 A terminating or transient renewal process is an ordinary 
renewal process except that F is defective. The defect q= 1 — F(oo) is 
interpreted as probability of termination. 
For consistency, 0 is counted as renewal epoch number zero. The prob- 
ability that the process effectively survives the renewal epoch number n 
equals (!— q)n and tends to 0 as n —*¦ oo. Thus with probability one a 
terminathi" process terminates at a finite time. The total mass of Fn * is 
A—^)" ana so the expected number of renewal epochs is U(co) = qrx < oo. 
This is, so to speak, the expected number of generations attained by the 
13 
See example l(f) lor an illustration and problem 4 for a generalization. 
VI.7 EXAMPLES AND PROBLEMS 187 
process. The probability that Sn <, x and the process dies with this «th 
renewal epoch is qFn *(x). We have thus the 
Theorem 2. In a terminating renewal process, qU is the proper prob- 
ability distribution of the duration of the process {age at time of death). 
The second generalization corresponds to delayed recurrent events and 
consists in permitting the initial waiting time to have a different distribution. 
In such cases we begin the numbering of the T, with j = 0 so that now 
So = To 5* 0. 
Definition 3. A sequence So, Slf... forms a delayed renewal process if it 
is of the form F.1) where the Tk are mutually independent strictly positive 
(proper or defective) variables and Tx, T2, . . . (but not To) have a common 
distribution. 
7. EXAMPLES AND PROBLEMS 
Examples such as self-renewing aggregates, counters, and population 
growth carry over from the discrete case in art obvious manner. A special 
problem, however, will lead to interesting questions to be treated later on. 
Example, (a) An inspection paradox. In the theory of self-renewing 
aggregates a piece of equipment, say an electric battery, is installed and 
serves until it breaks down. Upon failure it is instantly replaced by a like 
battery and the process continues without interruption. The epochs of 
renewal form a renewal process in which' Tk is the lifetime of the kth battery. 
Suppose now that the actual lifetimes are to be tested by inspection: 
we take a sample of batteries in operation at epoch t > 0 and observe their 
lifetimes. Since F is the distribution of the lifetimes for all batteries one 
expects that this applies also to the inspected specimen. But this is not so. In 
fact, for an exponential distribution F the situation differs only verbally from 
the waiting time paradox in 1,4 where the lifetime of the inspected item has 
an entirely different distribution. The fact that the, item was inspected at 
epoch t changes its lifetime distribution and doubles its expected duration. 
We shall see in XI,D.6) that this situation is typical of all renewal processes. 
The practical implications are serious. We see that an apparently unbiased 
inspection plan may lead to false conclusions because what we actually 
observe need not be typical of the population as a whole. Once noticed the 
phenomenon is readily understood (see 1,4), but it reveals nevertheless 
possible pitfalls and the necessary interplay between theory and practice. 
Incidentally, no trouble arises if one decides to test the first item installed 
after epoch t. > 
This is a good occasion to introduce three random variables of interest in 
renewal theory. In the preceding example all three refer to the item in 
188 SOME IMPORTANT DISTRIBUTIONS AND PROCESSES VI.7 
operation at epoch t > 0 and may be described by the self-explanatory 
terms: residual lifetime, spent lifetime, and total lifetime. The formal 
definition is as follows. 
To given t > 0 there corresponds a unique (chance-dependent) subscript 
N, such that SN# < t < SN<+1. Then: 
(a) The residual waiting, time is SN<+1 — t, the time from t to the next 
renewal epoch. 
(b) The spent waiting time is t — SN<, the time elapsed since the last 
renewal epoch. 
(c) Their sum SN<+1 — SN< = TN<+1 is the length of the recurrence 
interval covering the epoch t. 
The terminology is not unique and varies with the context. For example, 
in random walk our residual waiting time is called point of first entry or 
hitting point for the interval t, oo. In the preceding example the word 
lifetime was used for waiting time. We shall investigate the three variables 
in XI,4 and XIV,3. 
The Poisson process was defined as a renewal process with an exponential 
distribution for the recurrence times T3. In many server and counter 
problems it is natural to assume that the incoming traffic forms a Poisson 
process. In certain other processes the interarrival times are constant. To 
combine these two cases it has become fashionable in queuing theory to 
admit general renewal processes with arbitrary interarrival times.14 
We turn to problems of a fairly general character connected with renewal 
processes. The distribution underlying the process is again denoted by F. 
We begin with what could be described roughly as the "waiting time W 
for a large gap" Here a renewal process with recurrence times T,- is 
stopped at the first occurrence of a time interval of duration ? free of 
renewal epochs, whereupon the process stops. We derive a renewal equation 
for the distribution V of the waiting time W. As the latter necessarily 
exceeds ? we have V(t) = 0 for t < ?. For t > ? consider the mutually 
exclusive possibilities that Tx > ? or Tx = y < ?. In the first case the 
waiting time W equals ?. In the second case the process starts from 
scratch and, given that Tx = y, the (conditional) probability of {W < t) 
is V{t—y). Summing over all possibilities we get 
G.1) K@ = 1 " F(& + f +V(t-y) F{dy}, t > ?, 
Jo 
and, of course, V(t) = 0 for t < ?. This equation reduces to the standard 
14 The generality is'somewhat deceptive because it is hard to find practical examples 
besides the bus running without schedule along a circular route. The illusion of generality 
detracts from the sad fact that a non-Poissonian input is usually also non-Markovian. 
VI.7 EXAMPLES AND PROBLEMS 189 
renewal equation 
G.2) V = z + G • V 
with the defective distribution G defined by 
G.3) G(x) = F(x) if x<?; G(x) = F(|) if x > ? 
and 
G.4) z(x) = 0 if x < ?; z(ar) = 1 - /r(f) ,y a- > |. 
The most important special case is that of gaps in a Poisson process 
where F(t) = 1 — e~ct and the solution V is related to the covering 
theorems of 1,9 [see problem 15 and example XIV,2(a). For a different 
approach see problem 16.] 
Examples for empirical applications, (b) Crossing a stream of traffic.15 
Cars move in a single lane at constant speed, the successive passages forming 
a sample from a Poisson process (or some other renewal process). A 
pedestrian arriving at the curb—or a car arriving at an intersection—will 
start crossing as soon as he observes that no car will pass during the next ? 
seconds, namely the time required for his crossing. Denote by W the time 
required to effect the crossing, that is, the waiting time at the curb plus ?. 
The distribution V of W satisfies G.1) with F(t) = 1 — e~ct. [Continued 
in examples XI,7(b) and XIV,2(a).] 
(c) Type II Geiger counters. Arriving particles constitute a Poisson 
process and each arriving particle (whether registered or not) locks the 
counter for a fixed time f. If a particle is registered, the counter remains 
"dead" until the occurrence of an interval of duration ? without new 
arrivals. Our theory now applies to the distribution V of the duration of 
the dead period. (See 1; XIII,11, problem 14.) 
(d) Maximal observed recurrence time. In a primary renewal process 
denote by Zt the maximum of T^ observed16 up to epoch t. The event 
(Z< < ?} occurs iff up to epoch t no time interval of duration ? was free 
of renewal epochs, and. so in our notations P{Z, > ?} = V(t). > 
A great many renewal processes occurring in applications may be described 
as alternating or two stage processes. Depending on the context the two stages 
may be called active or passive, free or dead, excited or normal. Active and 
passive periods alternate; their durations are independent random variables, 
each type being subject to a common distribution. 
15 For the oldty literature and variants (treated by different methods) see J. C. Tanner, 
The delay to pedestrians crossing a road, Biometrika, vol. 38 A951) pp. 383-392. 
16 More precisely, if n is the (chance-dependent) index for which Sn_x <, t < Sn then 
Zt = max [Tlf. . . , Tn_x, ?]. Variables of this nature were studied systematically by 
A. Lamperti. 
190 SOME IMPORTANT DISTRIBUTIONS AND PROCESSES VI.8 
Examples, (e) Failures followed by delays. The simplest example is given 
by actual replacements of a piece of equipment if each failure is followed by 
a delay (to be interpreted as time for discovery or repair). The successive 
service times Tlf T2, . . . alternate with the successive dead periods Ylf 
Y2, . . . and we get a proper renewal process with recurrence times T, + Y,. 
The same process may. be viewed as delayed renewal process with the first 
renewal epoch at Tlf and recurrence times Y, + Ti+1. 
(/) Lost calls. Consider a single telephone trunkline such that the 
incoming calls form a Poisson process with interarrival distribution 
G{t) = 1 — e~ct while the durations of the ensuing conversations are 
independent random variables with the common distribution F. The 
trunkline is free or dead, and calls arriving during dead periods are lost and 
have no influence on the process. We have here a two stage process in which 
the distribution of the recurrence times is Fie G. (See problem 17 as well 
as problems 3-4 in XIV, 10.) 
(g) Last come first served. Sometimes the distributions of the alternating 
waiting times are not known a priori but must be calculated from other data. 
As an example consider a data processing machine in which new information 
arrives in accordance with a Poisson process so that the free periods have an 
exponential distribution. The time required to process the new information 
arriving at any epoch has a probability distribution G. 
Busy and free periods alternate, but the duration of busy periods depends 
on the manner in which information arriving during a busy period is treated. 
In certain situations only the latest information is of interest; a new arrival 
is then processed immediately and all previous information is discarded. 
The distribution V of the duration of the busy periods must be calculated 
from a renewal equation (see problem 18). 
(h) Geiger counters. In type I counters each registration is followed by 
a dead period of fixed duration ? and arrivals within the dead period 
have no effect. The process is the same as described in example (e), the 
T, having an exponential distribution, the Y> being equal to |. In type 
II counters also the unregistered arrivals produce locking and the situation 
is the same except that the distributions of the Y, depend on the primary 
process and must be calculated from the renewal equation G.1) [example 
(*)]• > 
8. RANDOM WALKS 
Let Xlf X2,.... be mutually independent random variables with a 
common distribution F and, as usual, 
(8.1) So = 0, Sn = X1 + • • • + Xn. 
We say that Sn is the position, at epoch n, of a particle performing a 
VI. 8 RANDOM WALKS 191 
general random walk. No new theoretical concepts are introduced,17 but 
merely a terminology for a short and intuitive description of the process 
{Sn}. For example, if / is any interval (or other set), the event {Sn e /} 
is called a visit to /, and the study of the successive visits to a given interval 
/ reveals important characteristics of the fluctuations of Sx, S2, . . . . The 
index n will be interpreted as time parameter and we shall speak of the 
"epoch /*." In this section we describe some striking features of random 
walks in terms of the successive record values. The usefulness of the results 
will be shown by the applications in section 9. A second (independent) 
approach is outlined in section 10. 
Imbedded Renewal Processes 
A record value occurs at epoch n > 0 if 
(8.2) Sn>S, ; = 0, l,...,#i-l. 
Such indices may not exist for a given sample path; if they do exist they 
form a finite or infinite ordered sequence. It is therefore legitimate to speak 
of the first, second, ... . , occurrence of (8.2). Their epochs are again random 
variables, but possibly defective. With these preparations we are now in a 
position to introduce the important random variables on which much of the 
analysis of random walks will be based. 
Definition. The kth {ascending) ladder index is the epoch of the kth 
occurrence of (8.2). The kth ladder height is the value of Sn at the kth 
ladder epoch. (Both random variables are possibly defective.) 
The descending ladder variables are defined in like manner with the inequality 
in (8.2) reversed.18 
The term ascending will be treated as redundant and used only for emphasis 
or clarity. 
In the graph of a sample path (So, Slt. ..) the ladder points appear as 
the points where the graph reaches an unprecedented height (record value). 
Figure 1 represents a random walk {Sn} drifting to — oo with the last 
positive term at n = 31. The 5 ascending and 18 descending ladder points 
are indicated by • and O, respectively. For a random walk with Cauchy 
variables see figure 2. (page 204) 
17 Sample spaces of infinite random walks were considered also in volume 1, but there 
we had to be careful to justify notions such as "probability of ruin" by the obvious limiting 
processes. Now these obvious passages to the limit are justified by measure theory. 
(See IV.6.) 
18 Replacing the defining strict inequalities by > and ^ one gets the weak ladder 
indices. This troublesome distinction is unnecessary when the underlying distribution is 
continuous. In figure 1 weak ladder points are indicated by the letter w. 
192 
SOME IMPORTANT DISTRIBUTIONS AND PROCESSES 
VI.8 
Figure 1. Random Walk and the Associated Queuing Process. The variables Xn of the 
random walk {Sn} have expectation — 1 and variance 16. Ascending and descending ladder 
points are indicated by • and O, respectively. The seventh ladder point is B6, 16) and 
represents with high probability the maximum of the entire random walk. 
[The letter w indicates where a record value is assumed for a second or third time; 
these are the weak ladder points defined by (8.2) when the strict inequality is replaced 
by >.] 
Throughout the graph Sn exceeds its expected value —//. In fact, n = 135 is the first 
index such that Sn < — n (namely S135 = —137). This accords with the fact that the 
expectation of such n is infinite. 
The variables Xn are of the form Xn = 3n — sfn, where the variables &n and rfn are 
mutually independent and uniformly distributed over 1, 3, 5, 7, 9 and 2, 4, 6, 8, 10, 
respectively. In example 9(a) the variable Ww represents the total waiting time of the nth 
customer if the interarrival times assume the values 2, 4, 6, 8, 10 with equal probabilities 
while the service times equal 1,3,5, 7, or 9, each with probability i. The distribution of Xn 
attributes probability E — k)/25 to the points ±2k — 1, where k — 0, 1, 2, 3, 4. 
Example, (a) In the "ordinary" random walk F has the atoms 1 and —1 
with weights p and q. The ascending ladder variables are defective if 
q > p, the defect plq [see 1; XI,C.9)]. The kth ladder height necessarily 
equals k and for this reason volume 1 mentions only ladder epochs. The 
A:th ladder index is the epoch of theirs/ visit to the point k. Its distribution 
VI. 8 RANDOM WALKS 193 
was found in 1; Xl,4(d) and in the special case p = ? already in tneorem 
2 of 1; 111,4. 
ladder index ^ is the epoch of the first entry into 0, oo, and the 
first ladder height Jf x equals S^. The continuation of the random walk 
beyond epoch 3TX is a probabilistic replica of the entire random walk. 
Given that 5TX = n, the occurrence of a second ladder index a an epoch 
k > n depends only on Xn+1, . . . , X^, and hence the number of trials 
between the first ladder index and the second is a random variable J7~2 which 
is independent of 3~x and has the same distribution. In this way it is seen 
more generally that the kth ladder index and the kth ladder height may be 
written in the form 
where the 3~';- and 2tif j are mutually independent random variables distributed, 
respectively, as &~1 and Jf v In other words, the ladder indices and heights 
form (possibly terminating) renewal processes. 
For terminating processes it is intuitively obvious that Sn drifts to 
-co, and with probability one Sn reaches a finite* maximum. The next 
section will show that the ladder variables provide a powerful tool for the 
analysis of a class of processes of considerable practical interest. 
Example, {b) Explicit expressions. Let F have the density defined by 
(8.3) ^1 if *<0; ^ if *>0. 
a + b a + b 
This random walk has the rare distinction that all pertinent distributions 
can be calculated explicitly. It is of great interest in queuing theory because 
/ is the convolution of two exponential densities concentrated on 0, co 
and — oo,0, respectively. This means that X;- may be written as the 
difference X/= <^;- — s^j of two positive exponentially distributed random 
variables. Without loss of generality we assume a < b. 
The ascending ladder height 3fx has the density ae~bx; this variable is 
defective and its defect equals (b —a)jb. The ascending ladder epoch 
has the generating function b~xp{s) where 
(8.4) 2p{s) = a + b - J(a+bJ - Aabs. 
The defect is again (b—a)jb. 
The descending ladder height J^~ has density ae"* for x < 0, the 
descending ladder epoch $~~ has the generating function arxp(s). In 
the special case a = b it reduces to 1 —\Jl—s, and this generating 
function is famiiiar from ordinary random walks (or coin tossing). [For 
proofs and other results see XII,4-5 and XVIII,3. See also example 4 e).\ 
194 SOME IMPORTANT DISTRIBUTIONS AND PROCESSES VI.9 
9. THE QUEUING PROCESS 
An incredibly voluminous literature19 has been devoted to a variety of 
problems connected with servers, storage facilities, waiting times, etc. 
Much progress has been made towards a unification, but the abundance 
of small variants obscures the view so that it is difficult to see the forest for 
the trees. The power of new and general methods is still underrated. We 
begin by a formal introduction of a stochastic process defined by a recursive 
scheme that at first sight appears artificial. Examples will illustrate the wide 
applicability of the scheme; later on we shall see ihat sharp results can be 
obtained by surprisingly simple methods. (See XII,5.) 
Definition 1. Let Xx, X2, ... be mutually independent random variables 
with a common {proper) distribution F. The induced queuing process is the 
sequence of random variables Wo, Wx, . . . defined recursively by Wo = 0 and 
xxr _wn + xn+1 // wn + xn+1>o 
0 if Wn + Xn+1<0 
In short, Wn+1 = (Wn+Xn+1) \j 0. 
For an illustration see figure 1. 
Examples, (a) The one-server queue. Suppose that "customers" arrive 
at a "server" the arrivals forming a proper renewal process with inter- 
arrival times20 s#x, jrf2, ¦ • • (the epochs of arrivals are 0, s/lf srfx + J2/2,. . . 
and the customers are labeled 0, 1, 2, . . .). With the nth customer there 
is associated a service time 38 n, and we assume that the 38 n are independent 
of the arrivals and of each other and subject to a common distribution. 
The server is either "free" or "busy"; it is free at the initial epoch 0. The 
19 For references consult the specialized books listed in the bibliography. It would be 
difficult to give a brief outline of the development of the subject with a proper assignment 
of credits. The most meritorious papers responsible for new methods are now rendered 
obsolete by the progress which they initiated. [D. V. Lindley's integral equation of queuing 
theory A952) is an example.! Other papers are noteworthy by their treatment of (some- 
times very intricate) special problems, but they find no place in a skeleton survey of the 
general theory. On the whole, the prodigal literature on the several subjects emphasizes 
examples and variants at the expense of general methods. An assignment of priorities is 
made difficult also by the.many duplications. [For example, the solution of a certain 
integral equation occurs in a Stockholm thesis of 1939 where it is credited to unpublished 
lectures by Feller in 1934. This solution is now known under several names.] For the 
history see two survey papers by D. G. Kendall of independent interest: Some problems in 
the theory of queues, and Some problems in the theory of dams, J. Roy. Statist. Soc. Series 
B vol. li A951) pp. 151-185, and vol. 19 A957) pp. 207-233. 
20 Normally the interarrival times will be constant or exponentially distributed but it is 
fashionable to permit arbitrary renewal processes; see footnote 14 to section 7. 
VI.9 THE QUEUING PROCESS 195 
sequel is regulated by the following rule. If a customer arrives at an epoch 
where the server is free, his service commences without delay. Otherwise 
he joins a waiting line (queue) and the server continues uninterruptedly to 
serve customers in the order of their arrival21 until the waiting line dis- 
appears and the server becomes "free." By queue length we mean the number 
of customers present including the customer being served. The waiting time 
Wn of the nth customer is the time from his arrival to the epoch where his 
service commences; the total time spent by the customer at the server is 
Wn + &n- (F°r example, if the first few service times are 4,4, 1, 3, ... 
and the interarrival times are 2, 3, 2, 3,.. . , customers number 1,2,... 
join queues of length 1, 1,2, 1,. . . , respectively, and have waiting times 
2,3,2,2,...). 
To avoid trite ambiguities such as when a customer arrives at the epoch 
of another's departure we shall assume that the distributions A and B of 
the variables s/n and 8&n are continuous. Then the queue length at any 
epoch is well defined. 
We proceed to devise a scheme for calculating the waiting times Wn 
recursively. By definition customer number 0 arrives at epoch 0 at a free 
server and so his waiting time is Wo = 0. Suppose now that the nth 
customer arrives at epoch t and that we know his waiting time Wn. His 
service time commences at epoch t + Wn and terminates at epoch 
t + Wn + @n- The next customer arrives at time t -f- «s/n+1. He finds 
the server free if Wn + 8§n < «s/n+1 and has a waiting time Wn+i 
= Wn + ^n -~ ^n+i if this quantity is > 0. In other words, the sequence 
{Wn} of waiting times coincides with the queuing process induced by the 
independent random variables 
(9.2) Xn = &»_!-*/», n = l,2, ... 
(b) Storage and inventories. For an intuitive description we use water 
reservoirs (and dams), but the model applies equally to other storage 
facilities or inventories. The content depends on the input and the output. 
The input is due to supplies by rivers and rainfall, the output is regulated 
by demand except that this demand can be satisfied only when the reservoir 
is not empty. 
Consider now the water contents22 0, Wx, W2,... at selected epochs 
0, tx, t2, . . .. Denote by Xn the actual supply minus the theoretical (ideal) 
21 This "queue discipline" is totally irrelevant to queue length, duration of busy periods, 
and similar problems. Only the individual customer feels the effect of the several dis- 
ciplines, among which "first come first served," "first come last served " and "random 
choice" are the extremes. The whole picture would change if departures were permitted. 
22 For simplicity we start with an empty reservoir. An adjustment to arbitrary initial 
conditions causes no difficulties [see example (c)]. 
196 SOME IMPORTANT DISTRIBUTIONS AND PROCESSES VI.9 
demand during rn_lt rn and let us pretend that all changes are instantaneous 
and concentrated at the epochs rx, t2, . . . . We start with Wo = 0 at 
epoch 0. In general the change Wn+1 — Wn should equal Xn+1 except 
when the demar i exceeds the contents. For this reason the Wn must satisfy 
(9.1) and so the successive contents are subject to the queuing process induced by 
{Xk} provided the theoretical net changes Xk are independent random 
variables with a common distribution. 
The problem (for the mathematician if not for the user) is to find conditions 
under which the Xk will appear as independent variables with a common 
distribution F and to find plausible forms for F. Usually the rk will be 
equidistant or else a sample from a Poisson process, but it suffices for our 
purposes to assume that the rk form a renewal process with interarrival times 
ja^, j/2, .... The most frequently used models fall into one of the following 
two categories: 
(i) The input is at a constant rate c, the demand &n arbitrary. Then 
Xn = cs/n — 8&n. We must suppose this Xn to be independent of the 
"past" Xx, . . . , Xn_x. (The usual assumption that ss/n and 0&n be in- 
dependent is superfluous: there is no reason why the demand 3&n should 
not be correlated with the duration -s^n.) 
(ii) The output is at a constant rate, the input arbitrary. The description 
is the same with the roles of srf'n and 3§n reversed. 
(c) Queues for a shuttle train.23 A shuttle train with r places for passengers 
leaves a station every hour on the hour. Prospective passengers appear 
at the station and wait in line. At each departure the first r passengers in 
line board the train, and the others remain in the waiting line. We suppose 
that the number of passengers arriving between successive departures are 
independent random variables s/lt s/2,. . . with a common distribution. 
Let Wn be the number of passengers in line just after the nth departure, 
and assume for simplicity Wo = 0. Then Wn+1 = Wn + s/n+1 — r if 
this quantity is positive, and Wn+1 = 0 otherwise. Thus Wn is the variable 
of a queuing process (9.1) generated by the random walk with variables 
-r. > 
We turn to a description of the queuing process {Wn} in terms of the 
random walk generated by the variables Xk. As in section 8 we put So = 0, 
Sn = Xx 4- • • • 4- XB and adhere to the notation for the ladder variables. 
For ease of description we use the terminology appropriate for the server 
of example (a). 
—23P. E. Boudreau, J. S. Griffin Jr., and Mark Kac, An elementary queuing problem, 
Amer. Math. Monthly, vol. 69 A962) pp. 713-724. The purpose of this paper is didactic, 
that is, it is written for outsiders without knowledge of the subject. Although a different 
mode of description is used, the calculations are covered by those in example XII,4(c). 
VI.9 THE QUEUING PROCESS 197 
Define v as the subscript for which Sx ;> 0, S2 > 0, . . . , Sv_x > 0, but 
Sv < 0. In this situation customers number 1,2,..., v — 1 had positive 
waiting times Wx = Sl5. . . , Wv_x = Sv_x, and customer number v was 
the first to find the server free (the first lucky customer). At the epoch of 
his arrival the process starts from scratch as a replica of the whole process. 
Now v is simply the index of the first negative sum, that is, v is the first 
descending ladder index, and we denote it consistently by &~~. We have 
thus reached the first conclusion: The descending ladder indices correspond 
to the lucky customers who find the server free. Put differently, the epochs of 
arrival of the lucky customers constitute a renewal process with recurrence 
times distributed as 3T~. 
In practical cases the variable «^~~ must not be defective, for its defect 
p would equal the probability that a customer never finds the server free 
and with probability one there would be a last lucky customer followed by 
an unending queue. It will turn out that 3T~ is proper whenever 
Suppose now that customer number v— 1 arrives at epoch r. His 
waiting time- was Wv_x = Sv_x and so the epoch of his departure is 
t -|- Wv_x -I- @y-v The first lucky customer (number v) arrives at epoch 
t -|- s/v when the server was free for 
sfy - Wv_x - ®y_x = -Sv_x - Xv = -Sv 
time units. But by definition Sv is the first descending ladder height 
Jf ~. As the process starts from scratch we have reached the second con- 
clusion: The durations of the free periods are independent random variables 
with the same distribution as —2^~ (the recurrence time for the descending 
ladder heights). In other words, customer number 3~~ + • • • + 3~~ is 
the rth customer who finds the server free. At the epoch of his arrival the 
server has been free for — Jf ~ time units. 
It should now be clear that between successive ladder epochs the segments 
of the graph for the queuing process {Wn} are congruent to those for the random 
walk but displayed vertically so as to start at a point of the time axis (figure 1). 
To describe this analytically denote for the moment by [n] the last descending 
ladder index < n; in other words, [n] is a (random) index such that [n] < n 
and 
(9.3) s[«]^s, ; = 0,l,...,n. 
This defines [n] uniquely with probability 1 (the distribution of Xt being 
continuous). Clearly 
(9-4) Wn = Sn - S[n]. 
This relation leads to the most important conclusion if we look at the 
198 SOME IMPORTANT DISTRIBUTIONS AND PROCESSES VI.9 
variables Xx, . . . , XB in reverse order. Put for abbreviation X^ = 
Xn, . . . , Xn = Xv The partial sums of these variables are 
Q' Y' _i_ . . . _i_ V C C 
a* — Al "+" + AJfc = &n — &n-k> 
and (9.4) shows that the maximal term of the sequence 0, S^,. . . , Sn has 
subscript n — [n] and equals Wn. But the distribution of (X(, . . . , X^) is 
identical with that of (Xl5 . . . , Xn). We have thus the basic 
Theorem.24 The distribution of the queuing variable Wn is identical with 
the distribution of the random variable 
(9.5) Mw = max [0, Slt . . . , SJ 
in the underlying random walk {Xk}. 
The consequences of this theorem will be discussed in cnapter XII. 
Here we show that it permits us to reduce certain ruin problems to queuing 
processes despite the dissimilarity of the appearance. 
Example. (d) Ruin problems. In section 5 ruin was defined as the event 
that X(/) > z + ct for some / where X(/) is the variable of a compound 
Poisson process with distribution D.2). Denote the epochs of the successive 
jumps in this process by tx, t2, . . . . If ruin occurs at all it occurs also at 
some epoch rk and it suffices therefore to consider the probability that 
Sn = X(rn) — crn > 2 for some n. But by the definition of a compound 
Poisson process X(rn) is the sum of n independent variables Yk with 
the common distribution F, while rn is the sum of n independent expo- 
nentially distributed variables stfk. Accordingly we are in effect dealing with 
the random walk generated by the variables Xk = Yk — cs/k whose 
probability density is given by the convolution 
(9.6) 
- r 
C Jx 
Ruin occurs iff in the random walk the event {Sn ^ z) takes place for some n. 
To find the probability of ruin amounts therefore to finding the distributions 
of the variables Wn in the associated queuing process. 
(e) A numerical illustration. The most important queuing process arises 
when the interarrival and service times are exponentially distributed with 
expectations I/a and Ijb, respectively, where a < b. From the character- 
istics of this process described in example 8(Z>), one can conclude that the 
waiting time of the nth customer has a limit distribution W with an atom of 
24 Apparently first noticed by F. Pollaczek in 1952 and exploited (in a different context) 
by F. Spitzer, The Wiener-Hopf equation whose kernel is a probability density, Duke 
Math. J., vol. 24 A957) pp. 327-344. For Spitzer's proof see problem 21. 
VI.9 
THE QUEUING PROCESS 
199 
weight 1 — a\b at the origin and density ae~{b~a)x for x > 0. The 
expectation equals 
a 
The free periods of the counter have the same 
b{b-a) 
density as the first descending ladder height, that is, ae~ai. In this case the 
free periods and the interarrival times have the same distribution (but this is 
not so in other queuing processes). 
The number N of the first customer to find the counter empty has the 
generating function p{s)ja with./? defined in (8.4). Consider now the busy 
period commencing at epoch 0, that is, the time interval to the first epoch 
when the server becomes free. This period being initiated by customer 
number 0, the random variable N also equals the number of customers during 
the initial busy period. An easy calculation shows that its expectation equals 
bj{b—a) its variance ab{a+b)j{b—df. 
Finally, let T be the duration of the busy period. Its density is given 
explicitly by XIV, F.16) with cp' = a and cq = b. This formula involving 
a Bessel function does not lend itself to easy calculations, but the moments 
of T can be calculated from its Laplace transform derived by different 
methods in examples XlV,4(a) and XIV,6F). The result is 
E(T) = 
1 
(b-a) 
and 
1 
(b-af 
In the queuing process busy periods alternate with free periods, and their 
expectations are \{b— a) and I/a, respectively. Thus {b->~a)\a is a measure 
of the fraction of the time during which the server is idle. More precisely: if 
U(/) is the idle time up to epoch /, then t^EV^t)^ {b—a)[a. 
Waiting time 
(steady 
state) 
Busy period 
No. of cus- 
tomers per 
busy period 
Table 1 
b = 1 
fl=0.5a=0.6fl= 0.7 a = 0.8 a = 0.9 a = 0.95 
Expectation 
Variance 
Expectation 
Variance 
Expectation 
Variance 
1 
3 
2 
12 
2 
6 
1.5 
5.3 
2.5 
25 
2.5 
15 
2.3 
10 
3.3 
63 
3.3 
44 
4 
24 
5 
225 
5 
200 
9 
99 
10 
1900 
10 
1700 
16 
15 
19 
399 
399 
,000 
399 
,200 
200 SOME IMPORTANT DISTRIBUTIONS AND PROCESSES VI. 10 
In the table the expected service time is taken as unit, and so a represents 
the expected number of customers arriving during one service time. The 
table shows the huge variances of the busy periods. It follows that fantastic 
fluctuations of the busy period must be expected. One sees that the customary 
reliance on expectations is very dangerous in practical applications. For a 
busy period with variance 225 the fact that the expectation is 5 has little 
practical significance. 
The multidimensional analogue to our queuing process is more intricate. The founda- 
tions for its theory were laid by J. Kiefer and J. Wolfowitz [On the theory of queues with 
many servers, Trans, Amer. Math. Soc, vol. 78 A955) pp. 1-18]. 
10. PERSISTENT AND TRANSIENT RANDOM WALKS 
We proceed to a classification of random walks which is independent of 
section 8 and closely related to the renewal theory of section 6. Given a 
distribution function F on the line we introduce formally an interval function 
defined by 
*=o 
The series is the same as in F.2), but when F is not concentrated on a 
half-line the series may diverge even when / is a finite interval. It will be 
shown that the convergence or divergence of A0.1) has a deep significance. 
The basic facts are simple, but the formulations suffer from the unfortunate 
necessity of a special treatment for arithmetic distributions.25 
For abbreviation we let Ih stand for the interval —h<x<h and 
Ih + / for t—h < x <t+h. 
Theorem 1. (i) If F is non-arithmetic either U{I) < co for every finite 
interval or else U{I} = co for all intervals. 
(ii) If F is arithmetic with span X either (/{/}< co for every finite 
interval or else U{I} = co for every interval containing a point ofthe form nX. 
(iii) // U{Ih} < oo then for all t and h > 0 
A0.2) U{Ih + /} < U{I2h}. 
For ease of reference to the two cases we introduce a definition (in which 
F receives an adjective rightfully belonging to the corresponding random 
walk). 
Definition. F is transient if U{I] < oo for all finite intervals, and per- 
sistent otherwise. 
25 F is arithmetic if all its points of increase are among the points of the form 0, ±A, 
±2A, The largest A with this property is called the span of F. (See V,2.) 
VI. 10 PERSISTENT AND TRANSIENT RANDOM WALKS 201 
Besides its probabilistic significance the theorem has a bearing on the 
integral equation 
A0.3) Z=z+F+Z 
which is the analogue to the renewal equation F.4). We use this integral 
equation as starting point and prove theorem 1 together with 
Theorem 2. Let z be continuous, and 0 < z(x) <, /x0 for \x\ < h and 
z(x) = 0 outside Ih. If F is transient then 
f+OO 
A0.4) Z(*)= z{x-y)U{dy} 
J — ao 
is a uniformly continuous solution of A0.3) with 
A0.5) 0 < Z(x) < ^ • U{IU}. 
Z assumes its maximum at a point in Ih. 
Proof of the two theorems, (i) Assume that U{Ia} < oo for some a > 0. 
Choose h < \cn and let z vanish outside Ih but not identically. We try to 
solve A0.3) by successive approximations putting Zo = z and, recursively, 
A0.6) Zn{x) = z{x) + J_ Zn_x{x-y) F{dy). 
With Un defined by 
A0.7) Un{I} = F°*{/} + • • • + Fn 
we have obviously 
f + ao 
A0-8) Zn[x) = z{x-y)Un{dy), 
l—CO 
(the integration extending in effect over an interval of length < 2h). The 
function Zn so defined is continuous, and we prove by induction that it 
assumes its maximum /.in at a point ?n such that z(?n) > 0. This is 
trivially true for Zo = z. If it is true for Zr_x one sees from A0.6) that 
z{x) = 0 implies Zn(x) < ^n_x whereas jun ^ Zn(?n_x) > Zn_^n_^ = 
It follows that the interval Ih + ?„ is contained in I2h and so by A0.8) 
A0.9) ixn < ix, ¦ U{l2h} 
which proves that the functions Zn remain uniformly bounded. Since 
Zo < Zx < ¦ ¦ ¦ it follows that Zn->Z with Z satisfying A0.5). 
By monotone convergence it follows from A0.6) and A0.8) that the limit 
Z satisfies the integral equation A0.3) and is of the form A0.4). The in- 
equality A0.5) holds because of A0.9). The upper bound depends only on 
202 SOME IMPORTANT DISTRIBUTIONS AND PROCESSES VI. 10 
the maximum ju0 of z, and we are free to let z(x) = ju0 for all x within a 
proper subinterval In of 4. In this case we get from A0.8) 
A0.10) Zn{x)>^Un{In+x} 
This inequality holds for all rj < /?, and hence also for r\ = h (since Ih 
is closed). The last two inequalities together prove the truth of A0.2). This 
implies that U{I} < 00 for intervals of length < h. But every finite interval 
can be partitioned into finitely many intervals of length < h, and therefore 
U{I) < 00 for all finite /. Finally, taking differences in A0.4) it is seen that 
\Z{x+6) - Z{x)\ < U{I2h} ¦ sup \z{x+6) - z(x)\. 
Accordingly, Z is uniformly continuous and all assertions concerning 
transient F are proved. 
(ii) There remains the case where U{Ia} = 00 for every a > 0. Then 
A0.10) shows that Zn{x) -*• 00 for all x in a neighborhood of the origin. 
If / is a point of increase of F it follows from A0.6) that Zn{x) -*¦ 00 for 
all a; in a neighborhood of /. By induction the same is true of each point of 
increase of F2*, F3*,.... Assume F non-arithmetic. If F were con- 
centrated on a half-line we would have U{Ia} < 00 (section 6). By lemma 2 
of V,4a therefore the points of increase of F2*, F3*, . . . are dense on the 
line and so Zn(x)->co everywhere. This implies Un{I}-*- 00, for all 
intervals. With the obvious modification this argument applies also to 
arithmetic distributions, and so the theorems are proved. > 
In chapter XI we shall return to the renewal equation A0.3), but now we 
turn to the implications of theorem 1 for random walks. Let Xl5 X2,. .-. 
be independent random variables with the common distribution F, and put 
Sn = Xx -I- • • • + Xn. By "visit to / at epoch n = 1, 2, ..." is meant the 
event that Sn e I. 
Theorem 3.26 If F is transient the number'of visits to a finite interval I is 
finite with probability one, and the expected number of such visits equals U{I}. 
If F is persistent and non-arithmetic every interval I is visited infinitely 
often with probability one. If F is persistent and arithmetic with span X then 
every point nX is visited infinitely often with probability one. 
Proof. Assume F transient. The probability of a visit to / after epoch n 
does not exceed the nth. remainder of the series in A0.1) and so for n suffi- 
ciently large the probability of more than n visits is < e. : is proves 
the first assertion. 
26 The theorem is a consequence of the second zero-or-one law in IV,6. If <p(I+t) is 
the probability of entering / + / infinitely often then for fixed / the function q> can 
assume only the values 0 or 1. On the other hand, considering the first step in the random 
walk one sees that <p = F-k <p and hence <p = const (see XI,9). 
VI. 10 PERSISTENT AND TRANSIENT RANDOM WALKS 203 
Assume now F persistent and non-arithmetic. Denote by ph(t) the 
probability of a visit to Ih + /. It suffices to prove that ph{t) =1 for all 
h > 0 and all /, for this obviously implies the certainty of any number of 
visits to each interval. 
Before proceeding we observe that if Sn = x we may take x asa new 
origin to conclude that the probability of a subsequent visit to 4 equals 
ph{—x). In particular, if a; is a point in Ih+t the probability of a subsequent 
visit to 4 is < p2h(-t). 
We begin now by showing that ph@) = 1. For an arbitrary, but fixed, 
h > 0 denote by pr the probability of at least r visits to 4- Then 
P\ 4- p2 4- * * ' is the expected number of visits' to 4 and hence infinite by 
the definition of persistency. On the other hand, the preliminary remark 
makes it clear that Pr+i < pr' P2h(W- The divergence of ]?/>r therefore 
requires that p2h@) = 1. 
Passing to general intervals Ih+t, assume first that F is not arithmetic. 
By lemma 2 of Y,4a every interval contains a point of increase of Fk* for 
some k and therefore the probability ph(t) of entering Ih+t is positive for 
all h > 0 and all /. But we saw already that even after entering Ih+t a 
return to 4 ls certain, and by the preliminary remark this implies that 
p2h( — t)= 1. Since h and / are arbitrary this concludes the proof for non- 
arithmetic distributions. But the same argument 'applies to arithmetic 
distributions. »¦ 
In testing whether the series A0.1) for U{I} converges one has usually to 
rely on limit theorems which provide information only for very large intervals. 
In such situations it is advisable to rely on the following 
Criterion. If F is transient, then x~lU{I^ remains bounded as x -> oo. 
The assertion is obvious from A0.2) since any interval Inh may be parr 
titioned into n intervals of the form Ih+t. As an illustration of the method 
we prove 
Theorem 4. A distribution with expectation ju is persistent if ju = 0, 
transient if ju j? 0. 
Proof. Let ju = 0. By the weak law of large numbers there exists an 
integer n€ such that P{|Sn| < en} > \ for all n > ne. Accordingly 
Fn *{Ia} > \ for all n such that n? < n < a/e. If a > 2en€ there are more 
than a/Be) integers n satisfying this condition, and hence U{Ia} > ajDe). 
Since.e is arbitrary this implies that the ratio fl-1i7{4} is not bounded, and 
hence F cannot be transient. 
If ju > 0 the strong law of large numbers guarantees that with a prob- 
ability arbitrarily close to one Sn will be positive for all n sufficiently large. 
204 SOME IMPORTANT DISTRIBUTIONS AND PROCESSES VI. 10 
The probability of entering the negative half-line infinitely often is therefore 
zero, and thus F is transient. ^ 
In a persistent process the sequence {Sn} necessarily changes sign 
infinitely often and so the ascending and descending ladder processes are 
persistent. It may come as a surprise that the converse is false. Even in a 
transient random walk {Sn} may change signs infinitely often (with probability 
one). In fact, this is the case when F is symmetric. Since a finite interval 
—a, a will be visited only finitely often this implies (very roughly speaking) 
that the changes of signs are due to occasional jumps of fantastic magnitude: 
|Sn| is likely to grow over all bounds, but the fantastic inequality 
Xn+1 < —Sn — a will occur infinitely often however large the constant a. 
Figure 2 illustrates the occurrence of large jumps but is not fully representa- 
tive of the phenomenon-because it was necessary to truncate the distribution 
in order to obtain a finite graph. 
v" 
\t 
h i i i i I i i i i I i i i i I i i i i'1 I i i i i 
Figure 2. Random Walk Generated by the Cauchy Distribution. (The distribution was 
truncated so as to eliminate jumps of the magnitude of the graph.) 
VI. 11 GENERAL MARKOV CHAINS 205 
11. GENERAL MARKOV CHAINS 
The generalization of the discrete Markov chains of 1; XV to Cartesian 
(and more general) spaces is simple. In the discrete case the transition 
probabilities were given by a stochastic matrix with elements piS whose 
rows were probability distributions. Now we have to consider transitions 
from a point x to an arbitrary interval or set F in %n; we denote the 
probability of this transition by K(x, F). The novel feature is that we must 
impose some regularity conditions to ensure that the necessary integrations 
can be performed. Continuity would do for most practical purposes, but 
nothing is gained by restricting the full generality. 
Definition 1. A stochastic kernel K is a function of two variables, a point 
and a set, such that K(x, F) is (i) for a fixed x a probability distribution in 
Y, and (ii) for any interval Y a Baire function in x. 
It is not required that K be defined on the whole space. If x and Y are 
restricted to a set Q. we say that K is concentrated on Q. Sometimes it is 
necessary to admit defective distributions and we speak then of substochastic 
kernels. Frequently K will be of the form 
A1.1) K{x,Y)=jk{x,y)dy 
and in this case k is called a stochastic density kernel. Following the 
convention of V,C.3) we indicate A1.1) by the shorthand notation 
K{x,dy) = k{x,y)dy. 
[Strictly speaking, k represents densities with respect to Lebesgue measure 
or length; densities with respect to an arbitrary measure m would be 
denoted by K{x, dy) = k{x,y) m{dy}.] 
Before giving a formal definition of Markov chains we can assemble 
the appropriate analytical apparatus by analogy with the discrete case. 
The probability of a transition from x to F in two steps is defined by 
A1.2) K{2\x, F) = f K{x, dy) K(y, F), 
Jn 
the integration extending over the whole space or the set Q. on which K is 
concentrated. Relation A1.2) states that the first step leads from x to some 
point y and the second from y to F. The crucial assumption is that, given 
the intermediate point y, the past history in no way influences the further 
transitions. A similar argument holds for the higher transition probabilities 
K{nK If we put Ka) = K we have for arbitrary positive integers 
A1.3) K{m+n)(x, F) = f K{m)(x, dy)K(n\y, F); 
Jn 
206 SOME IMPORTANT DISTRIBUTIONS AND PROCESSES VI. H 
this reduces to A1.2) when m = n = 1. Keeping m = 1 and letting 
«= 1, 2, 3, ... we get an inductive definition for KM. For consistency we 
define A:(o) to stand for the probability distribution concentrated at the 
point x (the so-called Kronecker delta kernel). Then A1.3) is valid for 
m ^ 0, n > 0. The operation A1.3) between two kernels occurs frequently 
also outside probability theory and is known as composition of kernels. It is 
in all respects similar to matrix multiplication. 
It is hardly necessary to emphasize that the kernels K{n) are stochastic. 
If K has a density, the same is true of K(n) and the composition formula 
for densities is 
A1.4) k{m+n)(x, z) = f kim\x, y) k{n\y, z) dy. 
Jn 
Examples, (a) Convolutions. If k(x, y) = f(y~x), where / is a prob- 
ability density, the composition A1.4) reduces to ordinary convolutions. 
The same is true generally if K is homogeneous in the sense that 
K(x, F) = K(z+s, T+s) 
where F + s is the set obtained by translating F through s. For con- 
volutions On the circle see theorem 3 in VIII,7. 
(b) Energy losses under collisions. In physics successive collisions of a 
particle are usually treated as a chance process such that if the energy (or 
mass) before collision equals x > 0 the resulting energy (mass) is a random 
variable Y such that P{Y eF}- K(x, F) where K is a stochastic kernel. 
The standard assumption is that only losses are possible, and that the ratio 
Yjx has a distribution function G independent of x; then P{Y < y) 
= G(yjx) which defines a stochastic kernel. 
In a related problem in stellar radiation [example X,2(b)] Ambarzumian 
considered the special case G(y) = yx for 0 < y < 1 where A is a positive 
constant. This corresponds to a density kernel lyx~x x~x concentrated on 
0 < y < x and it is easily verified that the higher densities are given by 
A1.5) kM(x, y) - -^—y-— log - , 0<y<x. 
The particular value A = 1 corresponds to a uniform distribution (the 
fraction lost is "randomly distributed") and A1.5) then reduces to 1,(8.2). 
[Continued in example X,l (a).] 
(c) Random chains. Consider a chain (or polygonal line) in 5l3 whose 
links have unit length and where the angles between adjacent links depend 
on a chance mechanism. Many (frequently rather involved) variants occur 
in polymer chemistry, but we consider only the case where the successive 
angles are independent random variables. 
VI. 11 GENERAL MARKOV CHAINS 207 
By length L of a chain with endpoints A and B we mean the distance 
between A and B. Addition of a unit link to a chain of length x results in 
a chain length v^2 + 1 — 2x cos 6 where 6 is the angle between the new 
lmk and the line through A and B. We treat 6 as a random variable and 
consider in particular two distributions that are of special interest in chemistry. 
(i) Let 0 equal 60° or 120° with probabilities \ each. Then cos 0 = ±\ 
and the length of the prolonged chain is subject to the stochastic kernel 
K(x, T) attributing probabilities \ to the two points \Jx2 ± x + 1. For 
fixed x the distribution K(n) is concentrated on 2n points. 
(ii) Let the direction of the new link be chosen "at random," that is, 
suppose cos 6 to be uniformly distributed in —1,1. (See 1,10.) The 
prolonged chain has a length L between x + 1 and \x — 1|. Within ihis 
range by the law of the cosines 
P{L <y} = P{2x cos 6 > x2 + 1 + y*} = \ - [x2 + l +y2]/4x. 
Thus the length is determined by the stochastic density kernel 
k(x,y) = yj2x \x-i\ < y < x + 1. 
The length Ln+1 of a chain with n + 1 links has density k{n)(l,y). (See 
problem 23.) 
(d) Discrete Markov chains. A stochastic matrix (p{i) may be considered 
as a stochastic density k(i,j) = pi5 defined on the set Q. of positive integers 
and with respect to the measure m attributing unit weight to each integer. 
Absolute and Stationary Probabilities 
Saying that a sequence Xo, X1?.. . is subject to the transition probabilities 
K{n) means that K{n)(x, Y) is the conditional probability of the event 
{Xm+n 6 F} given that Xm = x. If the probability distribution of Xo is 
y0 the probability distribution of Xn is given by 
A1.6) yn{T) = f yo{dx]¦ KM(x, T). 
Jn 
Definition 2. The distribution y0 is a stationary distribution for K if 
Yn - Yo for al1 "> that is> if 
A1.7) 
The basic facts concerning stationary distributions are the same as in 
the case of discrete Markov chains. Under mild regularity conditions on 
K there exists a unique stationary distribution and it represents the asymptotic 
distribution of Xn under any initial distribution. In other words, the influence 
of the initial state fades away and the system tends to a steady state governed 
208 SOME IMPORTANT DISTRIBUTIONS AND PROCESSES VI. 11 
by the stationary solution. This is one form of the ergodic theorem. (See 
Examples, (e) The queuing process {Wn} defined in (9.1) is a Markov 
process concentrated on the closed interval 0, oo. The transition prob- 
i— 
abilities are defined only for x, y > 0 and there K(x, 0, y) = F(y — x). 
The existence of a stationary measure will be proved in VIII,7. 
(/) Let Xx, X2,.. . be mutually independent positive variables with a 
distribution F with a continuous density / concentrated on 0, oo. Define 
a sequence of random variables Yk recursively by 
CH-8) Y^X,, Yn+1 = |Yn-Xn+1|. 
Then {Yn} is a Markov chain concentrated on 0, oo with transition densities 
f{x-y) + f(x+y) 0<y<x 
A1.9) k{x, y) = 
f(x+y) y>x>0. 
The defining equation for a stationary density g is 
A1.10) g(y) = ( g(x+y)f(x) dx + Pg(aO/(*+y) dx. 
JO JQ 
If F has a finite expectation u then 
is a stationary probability density. In fact, a simple integration by parts 
will show that g satisfies27 A1.10) and we know from V,F.3) that g is a 
probability density. (See problem 22.) 
(g) A technical application.28 A long transmission line consists of indi- 
vidual pieces of cable whose characteristics are subject to statistical fluctua- 
tions. We treat the deviations from the ideal value as independent random 
variables Yx, Y2, . . . arid suppose that their effect is additive. Reversing 
a piece of cable changes the sign of its contribution. Assume that the 
deviations y.c are symmetric and put Xk = |YA|. An efficient construction 
of a long tn nsmission line now proceeds by the following inductive rule: 
the (« + l)st piece of cable is attached in that position which gives its error a 
27 How does one discover such a thing? Assuming hopefully that g and / have deriv- 
atives we may differentiate A1.10) formally. An integration by parts leads to the relation 
g'(y) = —g(Q)f(y) showing that g must be of the form A1.11). Direct verification then 
proves the validity of A1.11) without differentiability conditions. 
28 Adapted from a discrete model used by H. von Schelling, Elektrische Nachr.-Technik, 
vol. 20 A943) pp. 251-259. 
VI. 12 MARTINGALES 209 
sign opposite to the sign of the accumulated error of the preceding n pieces. 
The accumulated errors then follow the rule A1.8); the stationary density 
A1.11) is actually a limiting distribution: the error of a line consisting of n 
pieces is {for large n) distributed approximately with density A1.11). On the 
other hand, if the pieces of cable were combined randomly, the central limit 
theorem would apply and the variance of the error would increase linearly 
with n, that is, with the length of the cable. The simple procedure of testing 
the sign of the error thus permits us to keep it in bounds. *> 
In the preceding examples a Markovian sequence Xo, X1? . . . was defined 
in terms of an initial distribution y0 and the transition probabilities K. 
The joint distribution of (Xo, X1?. . . , Xn) is of the form 
yo{dxo}K(xo, dxj • • • K{xn_u dxn) 
discussed in 111,8 and 1; XV, 1. We have here a typical example of the 
advantage of defining absolute probabilities in terms of conditional ones. A 
more systematic way would be to start from the postulate 
A1.12) P{Xn+1 e T | Xo = x0, . . . , Xn = xn} = K(xn. O 
as definition. Here the Markov property is expressed by the fact that the 
right side is independent of x0, xx,. . . , xn_v so that the "past history" has 
no effect. The disadvantage of this definition is that it would involve us in 
problems of existence of conditional probabilities, their uniqueness, etc. 
(For Markov processes depending on a continuous time parameter see 
chapter X.) 
*12. MARTINGALES 
For a first orientation we may consider a stochastic process {Xn} such 
that the joint distribution of (X1, . . . , Xn) has a strictly positive continuous 
density pn. Conditional densities and expectations are then defined every- 
where in the elementary way of 111,2. The variables X., and Yn are 
supposed to have expectations. 
The sequence {Xn} will be called absolutely fair if for n — 1,2,... 
A2.1) E(XL) = 0; E(Xn+l j Xlt . . . , X ) = 0. 
A sequence {Y,,} is a martingale if 
A2.2) E(Yntl| Yx ?„)¦= Y,f, n= 1,2,... 
(A more flexible definition will be given presently.) 
* Martingales are treated because of their great importance, but they are not used as 
a too! in this book. 
210 SOME IMPORTANT DISTRIBUTIONS AND PROCESSES VI. 12 
The connection between the two types is simple. Given an absolutely 
fair sequence {Xn} put 
A2.3) Yn = Xx + • • •• + Xn + c 
where c is a constant. Then 
A2-4) E(Yn+11 Xlf... , Xn) = Yn. 
The conditioning variables X, may be replaced by the Yk, and so A2.4) is 
equivalent to A2.2). On the other hand, given a martingale {Yn} put 
X1 = Y1- E(Yi) and Xn+1 = YB. Then {Xn} is absolutely fair and A2.3) 
holds with c = E(YX). Thus {Yn} is a martingaleiff it is of the form A2.3) 
with {Xn} absolutely fair. 
The concept of martingales is due to P. Levy, but it was J. L. Doob who 
realized its unexpected potentialities and developed the theory. It will 
be shown in VII,9 that under mild boundedness conditions the variables 
Yjt of a martingale converge to a limit; this fact is important for the modern 
theory of stochastic processes. 
Examples, (a) Classical gambling is concerned with independent variables 
Xn with E(Xn) = 0. SOch a game is absolutely fair29 and the partial sums 
Sn = Xx -f- • • • -f- Xn constitute a martingale. Consider now an ordinary 
coin-tossing game in which the gambler chooses his stakes according to 
some rule involving the outcome of previous trials. The successive gains 
cease to be independent random variables but the game remains absolutely 
fair. The idea of a fair game is that the knowledge of the past should not 
enable the gambler to improve on his fortunes. Intuitively this means that 
an absolutely fair game should remain absolutely fair under any system of 
gambling, that is, under rules of skipping individual trials. We shall see that 
this is so. 
(b) PolycCs urn scheme of [1; V,2(c)]. An urn contains b black and r 
red balls. A ball is drawn at random. It is replaced and, moreover, c balls 
b 
of the color drawn are added. Let Yo = and let Yn be the proportion 
of black balls attained by the «th drawing. Then {Yn} is a martingale. In 
this case the convergence theorem guarantees the existence ^f a limit distri- 
bution [see examples VII,4(a) and VII,9{aj\. 
(c) Concordant30 functions. Let {Xn} be a Markov chain with transition 
probabilities given by the stochastic kernel K. Nothing is assumed con- 
cerning the expectations of Xn. The function u is called concordant with 
28 The practical limitations of this notion are discussed in 1; X,3. It will be recalled 
that there exrst "fair" games in which with probability >1 — c the gambler's gain at the 
nth trial is as large as, say, n/logn. 
80 This term was introduced by G. Hunt. 
VI. 12 MARTINGALES 211 
respect to A! if 
A2.5) u(x) = (K(x, dy) u(y). 
Define random variables Yfc by Yk = u(Xk) and assume that all expectations 
exist (for example, that u is bounded). The relation A2.5) is the same as 
E(Yfc+11 Xfc = x) = u(x), and thus E(Yk+11 Xfc) = Yfc. Since {Xfc} is 
Markovia:'. this implies A2.4), and since Yfc is a function of Xfc, this in 
turn implies A2.2) (see V,10tf). Thus {Yn} is a martingale. This 
result is of great value in the boundary theory for Markov cnains because 
the existence of a limit for Yn usually implies the existence of a limit for 
the given sequence {Xn}. [See examples (/) and VII,9(c).] 
(d) Likelihood ratios. Suppose it is known that in a stochastic process 
X1? X2,. . . the joint densities of (X1? . .. , Xn) are either pn or qn, but we 
do not know which. To reach a decision statisticians introduce the new 
random variables 
" ~ Pn(Xlt. . . , XJ " 
Under sufficient regularity conditions it is plausible that if the true densities 
are pn the observed values of X1?. . . , Xn will on the average cluster around 
points where pn is relatively large. If this is so Yn is likely to be small or 
large according as the true density is pn or qn. The asymptotic behavior 
of {Yn} is therefore of interest in statistical decision theory. 
For simplicity we assume that the densities pn are strictly positive and 
continuous. If the pn represent the true densities, then the conditional 
density of Xn+1 for given X1? . . . , Xn equals the ratio pn+ilpn, and hence 
A2.7) E(Yn+l | X, = xu . . . , Xn = xn) = 
l'""xxa'I]¦ Pnf^ " "xznf dy- 
The factors pn+1 cancel. The second denominator is independent of y, 
and the integral of qn^ is given by the marginal density qn+1. Thus A2.7) 
reduces to qjpn and so A2.4) is true. Accordingly, under the present 
conditions the likelihood ratios Yn form a martingale. »- 
The conditioning used in A2.2) is not particularly fortunate because one 
has frequently to replace the conditioning variables \\, . . . , Yn by some 
functions of them. [Such was the case in A2.4).] A greater defect is revealed 
by example (a). The underlying process (say coin tossing or roulette) is 
represented by a sequence of random variables Zn, and the gambler's 
gain at the (rt-H)st trial is some function of Zx, . . . , Zn+1 and, perhaps, 
other variables. The observable past is represented by (Zl5... , Zn), 
212 SOME IMPORTANT DISTRIBUTIONS AND PROCESSES VI. 12 
which may provide more information than the past gains. For example, 
if the gambler skips trials number 1, 3, 5,... the knowledge of his gains up 
to epoch 2« is at best equivalent to the knowledge of Z2, Z4, . . . , Z2n. 
Here the additional knowledge of Zt, Z3, . . . could in principle imply an 
advantage, and absolute fairness in this case must be based on conditioning 
by Z1? . . . , Zn. Thus conditioning with respect to-several sets of random 
variables may be necessary, and to take care of all situations it is best to use 
the conditioning with respect to arbitrary cr-algebras of events. 
Consider then a sequence {Y,J of random variables in an arbitrary 
probability space and denote by 5ln the cr-algebra of events generated by 
(Vj,. . . , Yn) (see V,10a). The defining relation A2.2) is now the same as 
E(Yn+11 5In) = Yn. We want to take this as the defining relation but 
replace the cr-algebra 5ln by a larger cr-algebra S3n. In most cases 93n will 
be generated by Y1? . . . , Yn and additional random variables depending 
on the past. The idea is that any random variable depending on the past 
must be measurable with respect to 93n, and in this sense 93n represents the 
information contained in the past history of the process. As this information 
grows richer with time we shall suppose that the 23 n increase, that is, 
A2.8) 93i c 932 c • • • . 
Definition 1. Let Y{, Y2, . . . be random variables with expectations. Let 
93i, 232, . . . be a-algebras of events satisfying A2.8). 
The sequence {Yn} is a martingale with respect tot {23n} iff 
A2.9) . E(Y.n+1|93n)= Yn. 
[Because of the non uniqueness of the conditional expectations, A2.9) 
should be read "there exists a version of the conditional probability for 
which A2.9) is true." This remark applies in the sequel.] 
Note that A2.9) implies that Yn is 23 „ measurable, and this has two 
important consequences. Since 23 „ => 937I_1 the basic identity V,A1.7) for 
iterated expectations shows that 
E(Yn+1 
By induction it is seen that the definition A2.9) entails the stronger relations 
A2.10) E(Yn4l|23,) = Yfc, *fc=l;2,...,/i. 
It follows in particular that every subsequence YVi, YVJ>, . . . of a martingale 
is again a martingale. 
Next we note that 93„ contains the cr-algebra generated by the variables 
Yx,.. . , Yn, and the same argument shows that {Yn} is also a martingale 
with respect to {3Tn}. Thus A2.9) implies A2.2). 
VI.12 MARTINGALES 213 
Example, (e) Let the a-algebras 33n satisfy A2.8) and let Y be an 
arbitrary random variable with expectation. Put Yn = E(Y 23n). Then 
Yn is 23n-measurable and hence A2.9) is true. Thus {Yn} is a martingale. > 
Returning to example (a) it is now easy to prove the impossibility of 
systems of a fairly general type. Let {YJ be a martingale with respect to 
{23n}. To describe the gambler's freedom to skip the «th trial we introduce a 
decision function en; this is a 93,,_i measurable31 random variable assuming 
only the values 0 and 1. In the event en = 0 the gambler skips the nth trial; 
in the event en = 1 he bets and in this case his gain at the «th trial is 
Yn — Yn_i. Denoting his accumulated gain up to and including the «th 
trial by Zn we have 
A2.11) Zn = ZB_1+€ll[YI-Y)^1]. 
By induction it is seen that Zn has an expectation. Furthermore, Zn_1? en, 
and Yn_! are 93n_x measurable and hence [see V,(l 1.6)] 
A2.12) E(Zn | SB^) = Z^ + en[E(Yn | «„_,) - ?„_,]. 
Since {Yn} is a martingale the expression within brackets vanishes, and so 
{Zn} is a martingale. We have thus proved a theorem due to P. R. Halmos 
implying the 
Impossibility of systems. Every sequence of decision functions el5 e2,. . . 
changes the martingale {Yn} into a martingale {Zn}. 
By far the most important special case concerns optional stopping. By 
this is meant a system where the first N trials are accepted and all succeeding 
ones skipped; the Nth trial is the last. Here N (the stopping epoch) is a 
random variable such that the event {N > k} is in 23*. (In the notation of 
the theorem ek = 1 for N > k — 1 and efc = 0 for N < k — 1.) We 
have thus the 
Corollary. Optional stopping does not affect the martingale property. 
Examples. (/) A simple random walk on the line starts at the origin; 
the particle moves with probability p one step to the right, with probability 
q = 1 — p to the left. If S,( is the position of the particle at epoch n it 
is easily seen that Yn = (qlpf" constitutes a martingale with E(Yn) = 1 
and E(Y0) — 1. [This is a special case of example (c).] 
In the ruin problem the random walk, is stopped when it first reaches 
one of the positions —a or b, where a and b are positive integers. In this 
31 This condition guarantees that the decision is made on the basis of past history of 
observations. No mathematical theory can disprove prescience of the future, we must 
exclude it from our models. 
214 SOME IMPORTANT DISTRIBUTIONS AND PROCESSES VI. 12 
modified process —a < Sn < b and with probability one Sn is ultimately 
fixed at b or at —a. Denote the corresponding probabilities by x and 
1 — x. Since Sn is bounded 
But E(Sn) = 1 because the expected value of the martingale remains 
constant. The right side, therefore, equals 1, and this linear equation 
determines x. We have thus found the probability x of termination at b 
derived by different methods in 1; XIV,2. The formula breaks down when 
p — q, but in this case {Sn} is a martingale and the same argument shows 
that x = aj{a+b). Although the result is elementary and known, the argu- 
ment illustrates the possible uses of martingale theory. 
, (g) On systems. Consider a sequence of independent random variables 
Xn where Xn assumes the value ±2" with probability ? each. A gambler 
tosses a coin to decide whether he takes the «th bet. The probability that his 
first try occurs at epoch n is 2~n and in this case his gain is ±2". Thus 
the gambler's gain at his first try is a random variable without expectation. 
The system theorem therefore depends on the fact that we have not changed 
the time parameter. »- 
It is frequently necessary to work with absolute values and inequalities, 
and for such purposes it is convenient to have a name for processes satisfying 
A2.9) with the equality replaced by an inequality. 
Definition 2. The sequence {Yn} is a submartingale32 if it satisfies the 
martingale definition A2.9) with the equality sign replaced by >. 
It follows again immediately that every submartingale satisfies the stronger 
conditions. 
A2.13) E(Yn+1193fc) > Yk, k = \,...,n. 
Lemma. If u is a convex function and {Yn} a martingale, then {u(Yn)} is 
a submartingale provided the expectation of u(Yn) exists. In particular, 
{| Yn|} is a submartingale. 
The proof is immediate from Jensen's inequality [V,(8.6)] which applies to 
conditional expectations as well as to ordinary ones. It states that 
A2.14) E(«(Yn+l) | ©„)> u(E(Yn+l | 8 J), 
and the right side equals u(Yn). 
32 
The older term "lower semi-martingale" is now falling into disuse. 
VI. 13 PROBLEMS FOR SOLUTION 215 
The same proof shows that if {YJ is a submartingale and u a convex 
non-decreasing function, then (w(Yn)} is again a submartingale, provided 
u(Yn) has an expectation. 
13. PROBLEMS FOR SOLUTION 
1. The definition A.2) of stable distributions is equivalent to: R is stable iff 
to arbitrary constants cx, c2 there exist constants c and y such that 
CjXi + c2X2 = cX + y. 
2. Every stable distribution is continuous. It suffices to prove it for symmetric 
R. From A.3) conclude: If R had an atom of weight p at the point s > 0, 
then it would have an atom of weight >p2 at each point of the form s(cm + cn)jcm+n 
(see V,4tf). Furthermore, a unique atom of weight p < 1 at the origin would 
induce an atom of weight p% at the origin for Ric R whereas stability requires an 
atom of weight p. 
3. For F to be stable it suffices that A.2) holds for n = 2 and 3. (P. Levy) 
Hint: Products of? the form c{c\ where j, k = 0, ±1, ±2,... are either dense 
in 0, co or powers of a fixed number c. The latter must be shown to be im- 
possible in the present case. 
Note. Curiously enough it does not suffice that A.2) holds for n = 2. See 
example XVII,3(/) and problem 10 of IX,10. 
4. For a stable distribution with exponent a = 1 the centering constants in the 
defining relation A.2) ;aiify ymn = myn + nym [see A.6)]. The analogue to 
A.8) is 
logs) + t(X2 + y logt) = (s + 0(X + /log (s + /)). 
5. If F and G are stable with the same exponent a so is Fjc G. Find the 
centering constants yn for F-fc G in terms of the constants for F and G. 
6. For a symmetric stable distribution R the symmetrization inequality V,E.11) 
implies that n[l—R(cnx)] remains bounded. Conclude that R has absolute 
moments of order <a. [Use V,F.3).] By symmetrization the last statement 
carries over to unsymmetric R. 
7. Alternative derivation of the Holtsmark distribution. Consider a balf of 
radius r about the origin and n stars (points) placed independently and randomly 
in it. Let each star have unit mass. Let X1; . . . , Xn be the ^-components of 
the gravitational force due to the individual stars, and Sn = Xx + • • • + Xn. 
Let r-» oo, n-> oo so that ^im^-^ A. Show that the distribution of Sn 
tends to the symmetric stable distribution with characteristic exponent f. 
8. Show that the preceding problem is not essentially modified if the mass of a 
star is assumed to be a random variable with unit expectation provided the masses 
of the stars are mutually independent and also independent of the position of the 
stars. 
9. Holtsmark distribution, four dimensions. The four-dimensional analogue to 
the Holtsmark distribution is a symmetric stable distribution with characteristic 
exponent -*. (In four dimensions the gravitational force varies inversely as the 
third power of the distance.) 
216 SOME IMPORTANT DISTRIBUTIONS AND PROCESSES VI. 13 
10. A triangular array {Xk_„} with rn components in the nth row can be trans- 
formed into an essentially equivalent array with n components in the nth row by 
adding dummy variables assuming only the value zero and repeating certain rows 
an appropriate number of times. 
11. Let {Xk „} be a triangular null array with a common distribution Fn for 
X1>B, • . . , XB>n. Does P{max (|X1>B|,.. . , |Xnn|) > e} tend to zero? 
12. Find the density for the renewal function U of F.3) if F has the density 
(a) f(x) = e~\ and (b) f(x) = «r* 
13. In a terminating process F has density pce~ct. Find the distributions of 
the lifetime and of the number of renewal epochs. 
14. Generalized terminating renewal process. Instead of assuming that with 
probability q the process terminates instantaneously we let it (with probability q) 
continue for a random duration with a proper distribution Fo "ind then stop. In 
other words, the renewal epochs are of the form Tx + • • • + Tn + Y where 
the last variable has a different distribution. Show that the distribution V of the 
duration of the process satisfies the renewal equation 
(*) V = qF0 + Fie V ' (F(co) = 1 - q). 
15. Show that the waiting time problem for large gaps reduces to a special case 
of the process described in the last problem. Put G.1) into the form (*). 
16. Poisson process and covering theorems. We recall from example 111,3 (d) 
that if in a Poisson process n renewal epochs occur in 0, t their (conditional) 
distribution is uniform. The probability I — V{t) that no gap of length ? appears 
follows therefore from the covering theorem 3 in 1,9 in the form 
(t) 
o—CX 
(a) Verify that this is indeed a solution of G.1) when F(x) = 1 — e 
(b) Given (f) is the unique solution of G.1), derive the covering theorem from 
it. (This is an instance of a proof by randomization. See problem 5 of 1; XII, 6.) 
17. The waiting time for the first lost call in example 7(/) should be interpreted 
as the total lifetime of the terminating process obtained by stopping the original 
process when for the first time a call arrives during a busy period.33 Show that the 
distribution H of the duration of the busy, period of the terminating process is 
given by H{dt} — e-** F{dt}, and the recurrence times have the distribution 
G * H. (See also problems 3-4 in XIV, 10.) 
18. Let V be the distribution of the busy period in. example 7(g) ("last come 
first served"). Show that V satisfies the renewal equation V(t) = A(t) + B * V(t) 
where A and B are defective distributions given by A{dx} = e~cx G{dx], and 
B{dx} = [1 — G(x)]ce~cx dx. (Show that we are concerned with a generalized 
terminating process in the sense of problem 14: a renewal process generated 
by B is followed by an undisturbed dead period, the latter having a distribution 
proportional to A.) 
33 This simple approach replaces complicated procedures suggested in the literature and 
leads to simpler explicit results. For explicit solutions and estimates see XI,6. 
VI. 13 PROBLEMS FOR SOLUTION 217 
19. Small gaps in a Poisson process.34 A "coincidence" is said to occur at epoch 
Sn if the distance of the renewal epochs S^ and Sn is <, ?. Find a renewal 
equation for the distribution of the waiting time for the first coincidence. From 
this renewal equation conclude that the distribution is proper. 
20.35 Generalization. In the standard renewal process Sn = Tx + • • • + Tn find 
a renewal equation for the distribution of the waiting time to the first occurrence 
of the event {Tn ^ Yn}, where the Yk are independent of the process and of 
each other and have common distribution G. 
21. Let alt. . . , an be a finite numerical sequence with partial sums 
Sk = ax + ¦ ¦ ¦ + ak. 
Define recursively 
vx = ax u 0, v2 = (Vl +<*„_!> u 0,... , vn = (vn_1 +ax) u 0. 
Prove by induction that vn = max [0,5!,... ,sn]. Show that this implies the 
theorem of section 9. 
22. In example 11(/) assume that f(x) = 1 for 0 < x < 1. Prove that 
g(y) = 2A - y) is a stationary density and that k(n)(x, y) = g(y) for n > 2. 
If f{x) = 0Le~*x then g(x) = f{x). 
. Define a stochastic density kernel concentrated on 0, 1 by 
k(x, y) = |A - x)'1 if 0 < x < y < 1 
and 
k(x, y) = fac-1 if 0 < y < x < 1. 
Find a stationary density. (It satisfies a simple differential equation.) Interpret 
probabilistically. 
24. A Markov chain on 0, 1 is such that if Xn = x then X^! is uniformly 
distributed on 1 — as, 1. Show that a stationary density is given by 2x. (T. 
Ugaheri.) 
25. A Markov chain on 0, oo is defined as follows: If Xn =xf then Xn+1 is 
uniformly distributed over 0, 2x (here n = 0, 1,. . .)• Show by induction that 
the n-step transitions have density kernels given by 
if 
and k{n)(x, y) =0 elsewhere. 
34 For variants (treated differently) see E. N. Gilbert and H. O. Pollak, Coincidences 
in Poisson patterns, Bell System Technical J., vol. 36 A957) pp. 1005-1033. 
35 The "large gap" problem admits of a similar generalization with an analogous answer. 
CHAPTER VII 
Laws of Large Numbers. 
Applications in Analysis 
In the first part of this chapter it is shown that certain famous and deep 
theorems of analysis can be derived with surprising ease by probabilistic 
arguments. Sections 7 and 8 treat variants of the laws of large numbers. 
Section 9 concains a restricted version of the martingale convergence theorem 
and stands somewhat apart from the remainder. 
1. MAIN LEMMA AND NOTATIONS 
By way of preparation consider a one-dimensional distribution G with 
expectation B and variance a2. If Xx, . . . , Xn are independent variables 
with the distribution G, their arithmetic mean Mn = (Xj + • • • + XJ/r1 
has expectation 6 and variance a2/*. For large n this variance is small 
and Mn is likely to be close to 6. It follows that for every continuous 
function w(Mn) is likely to be close to u{6). This remark constitutes the 
weak law of large numbers. It is slightly generalized in the following lemma, 
which despite its simplicity will prove a source of valuable information. 
For n = 1,2, ... consider a family of distributions FnQ with expectation 
6 and variance cr?@); here 6 is a parameter varying in a finite or infinite 
interval. For expectations we use the notation 
J»+OO 
"(*) Fnfi{dz}. 
—oo 
Lemma 1. Suppose that u is bounded and continuous, and that a\{0) -*¦ 0 
for each 6. Then 
A.2) En»-* 
The convergence is uniform in every closed interval in which a2n@) —> 0 
uniformly. 
219 
220 LAWS OF LARGE NUMBERS. APPLICATIONS IN ANALYSIS VII. 1 
Proof. Obviously 
J"+oo 
\u(x)-uF)\Fn>e{dx). 
— oo 
There exists a d depending on 6 and e such that for \x—6\ < d the 
integrand is <e. Outside this neighborhood the integrand is less than some 
constant M, and by Chebyshev's inequality V,G.2) the probability carried 
by the region \x—6\ > d is less than o%@)fir2. Thus the right side will be 
<2e as soon as n is so large that a\(d) < ed2jM. This bound on n is 
independent of 6 if a\(d) -*- 0 uniformly and if u is uniformly 
continuous. > 
Examples, (a) If Fn e is a binomial distribution concentrated on the 
points k\n (k = 0, . . '. , n), then ol{6) = 0A -O)^1 -> 0 and so 
i=o \n/ 
uniformly in 0 < 0 < 1. The implications are discussed in section 2. 
(b) If Fn>B is the Poisson distribution attaching probability e-ne(n6)kjk\ 
to the point k/n, we have a\F) = djn and so 
A.5) e~r " 
W kl 
uniformly in every finite 6-interval. This formula is valid also for non- 
integral n. (Continued in sections 5 and 6.) 
(c) Taking for Fn>e a gamma distribution with expectation 0 and 
variance djn we get 
A.6) —±— ru(x) ¦ l^T'e-™' 
(n-1)! Jo \0/ 0 
uniformly in every finite interval. Again this formula holds for non-integral 
n provided (n — 1)! is replaced by F(/z). It will be shown in section 6 that 
A.6) is an inversion formula for Laplace transforms. 
(d) Statisticians frequently face the situation described at the beginning 
of this section but consider the expectation 0 an unknown parameter to 
be estimated from observations In statistical language the relationA.2) 
then states that w(Mn) is an asymptotically unbiased estimator for the unknown 
parameter w@). [The estimator would be unbiased if the two sides in A.2) 
were equal.] > 
We shall see that each of these examples leads to important results of 
independent interest, but some preparations are necessary for the further 
development of the theory. 
VII. 1 MAIN LEMMA AND NOTATIONS 221 
Notations for Differences 
In the next few sections we shall employ the convenient notations of the 
calculus of finite differences. Given a finite or infinite numerical sequence 
a0, ax, . . . the differencing operator A is defined by Aa{ = ai+1 — at. It 
produces a new sequence {AarJ, and on applying the operator A a second 
time we get the sequence with elements 
A2art. = Aai+1 - Aa{ = ai+2 - 2ai+1 + at. 
Proceeding in like manner we may define the rth power Ar inductively by 
Ar = AA1"-1. It is easily verified that 
A.7) &ra< = 
For consistency we define A0 as the identity operator, that is, A°a{ = at. 
Then A.7) holds for all r ^ 0. Of course, if a0, . . . , an is a finite sequence 
the variability of r is limited. 
Many tedious calculations can be avoided by noticing once and for all a 
curious reciprocity relation valid for an arbitrary pair of sequences {aj 
and {cj; it enables us to express the differences Arat in terms of Arct and 
vice versa. To derive it multiply A.7) by I \cr and sum over r = 0,. . . , v. 
The coefficient of ai+j is found to equal 
v\*-i/v-j\ 
ol k I Ci+k' 
(Here the new summation index k = r — j was introduced.) The last sum 
equals Av-jc}, and thus we have found the 
General reciprocity formula 
r=0 \77 3=0 
Examples, (a) {Inversion formula?) Consider the constant sequence with 
at = 1 for all /. Then A°at = 1 but all other differences vanish, and so 
A.8) reduces to 
A.9) co=i(V)(-l)v-Av-V,. 
If we change the sequence {c,} into {ck+j} we see that A.9) remains valid 
with c0 and c, replaced by ck and ck+j, respectively, and so A.9) is an 
inversion formula expressing the given sequence in terms of its differences 
Here v can be chosen arbitrarily. 
222 LAWS OF LARGE NUMBERS. APPLICATIONS IN ANALYSIS VII.2 
(b) Let 0 < 0 < 1 be fixed and define cr = 6r. Then 
Akcr = er(i -d)k(-iy 
and so A.8) takes on the form 
r=o 
We deal frequently with sequences with terms ak = u(x+kh) obtained 
from a function u by fixing a point x and a span h > 0. For obvious 
reasons it is then convenient to replace the difference operator A by the 
difference ratios A = h~xA. Thus 
h 
A.11) A u(x) = [u(x+h)-u(z)]/h 
and more generally 
h 
A.12) A" u(x) = /ri ) 
h i=o\j/ 
In particular, A0 u{x) = u{x). 
h 
2. BERNSTEIN POLYNOMIALS. ABSOLUTELY 
MONOTONE FUNCTIONS 
We return to the important relation A.4). The left side is a polynomial, 
called the Bernstein polynomial of degree n corresponding to the given 
function u. To emphasize this dependence we shall denote it by Bn u. Thus 
B.1) Bn ( 
where for convenience we put rr1 = h. Comparing with A.10) one sees 
that Bn tl may be written in the alternative form 
B.2) BntM = i (n)(hey Ar «@), h = -. 
r=o \ rj h n 
An amazing number of far-reaching conclusions can be drawn from the 
discovery that the representations B.1) and B.2) for the Bernstein poly- 
nomial are equivalent. Before proceeding in this direction we restate for the 
record the result derived in example l(ar). 
Theorem 1. If u is continuous in the closed interval 0, 1 the Bernstein 
polynomials Bn u@) tend uniformly to u{6). 
In other words, for given e > 0 
B.3) |fin,u@)-w@)|< e, O<0<1, 
VII.2 BERNSTEIN POLYNOMIALS. ABSOLUTELY MONOTONE FUNCTIONS 223 
for all n sufficiently large. The famous Weierstrass approximation theorem 
asserts the possibility of uniform approximation by some polynomials. The 
present theorem is sharper inasmuch as it exhibits the approximating 
polynomials. The above proof is due to S. Bernstein. 
As a first application of our dual representation for Bernstein polynomials 
we derive a characterization of functions that can be represented by a power 
series with positive coefficients: 
B.4) u(x) = p0 +Plx +p,x* + -••,/?,> 0, 0 < x < 1. 
Obviously such a function possesses derivatives of all orders and 
B.5) uM(x) ^ 0, 0 < x < 1. 
For many purposes in analysis it is.important that the converse is also true, 
that is, any function with the property B.5) admits of a power series represen- 
tation B.4). This was first observed by S. Bernstein, but the usual proofs are 
neither simple nor intuitive. We shall show that the representation B.2) for 
Bernstein polynomials leads to a simple proof and to the useful result that 
the two properties B.4) and B.5) are equivalent to a third one, namely 
B.6) A* u?Q) > 0, k = 0,...,«- 1, h = - . 
h n 
These results are of interest for probability theory, because if {pr} is a 
probability distribution on the integers, B.4) defines its generating function 
(see 1; XI), and so we are dealing with the problem of characterizing 
probability generating functions. Among our functions they are distinguished 
by the obvious norming condition w(l) = 1. However, the example 
u(x) = A— x)-1 shows that functions with the series representation B.4) 
need not be bounded. 
Theorem 2. For a continuous function u defined for 0 < x < 1 the three 
properties B.4), B.5), and B.6) are fully equivalent. 
Functions with this property are called absolutely monotone in 0, 1. 
Proof. We proceed in two steps and consider first only probability 
generating functions. In other words, we assume now that u is continuous 
in the closed interval 0, 1 and that u{\) = 1. 
Obviously B.4) implies B.5). If B.5) holds then u and all its derivatives 
are monotone. The monotonicity of u implies that A u{x) > 0, while the 
h 
monotonicity Of u implies that A u(x) depends monotonically on x and 
h 
hence A2 u(x) > 0. By induction we conclude that B.5) implies B.6), 
h 
and also An k@) > 0. 
h 
224 LAWS OF LARGE NUMBERS. APPLICATIONS IN ANALYSIS VII.3 
Assume B.6) for k = 0, ...,«. In view of B.2) the polynomial Bn u 
has non-negative coefficients, and B.1) shows that Bnu{\) = 1. Thus 
B,,u is a probability generating function, and the continuity theorem of 1; 
XI,6 (or below VIII,6) assures us that the limit u of Bnu is itself a probability 
generating function. (The assumption w(l) = 1 guarantees that the 
coefficients add to unity.) This concludes the proof for bounded functions. 
If u is unbounded near 1 we put 
B.7) ,(.)-,(Jtl^,(!tl), 0?*?l 
where m is an arbitrary integer. The preceding proof applies to v and shows 
that each of the properties B.5) and B.6) imply the validity of a power series 
expansion B.4) at least for 0 < x < (m — \)jm. Because of the uniqueness 
of power series representations and the arbitrariness of m this implies that 
B.4) holds for 0 < x < 1. > 
3. MOMENT PROBLEMS 
In the last theorem we encountered sequences {ak} all of whose differences 
were positive. In the present section we shall be concerned with the somewhat 
related class of sequences whose differences alternate in sign, that is, sequences 
{ck} such that 
C.1) (-iy^ck>0, r = 0, 1,.... 
Such sequences are called completely monotone. 
Let F be a probability distribution on 0, 1 and denote by E(w) the 
integral of u with respect to F. The A:th moment of F is defined by 
C.2) ck = E(X*) = f V F{dx] 
Jo 
it being understood that the interval of integrat-on is closed. 
Taking successive differences one finds that 
C.3) (-l)'A'cfc = E(X*(l -X)r-k) 
and hence the moment sequence {ck} is completely monotone. Now let u 
be an arbitrary continuous function in 0, 1 and let us integrate the expression 
B.1) for the Bernstein polynomial Bnu with respect to F. In view of C.3) 
we get 
E(BB.U) = 
VII.3 MOMENT PROBLEMS 225 
where we put for abbreviation 
C.5) ,<«> = /nV_i)«-'A«-'c,. 
With the special choice u{x) = 1 we have Bnu(x) = 1 for all x, and hence 
the p(.n) add to unity. This means that for each n the pl.n) define a probability 
distribution attributing weight p\n) to the point jh =j/n. (Here j = 0,...,«.) 
We denote this probability distribution by Fn, and the expectations with 
respect to it by En. Then C.4) reduces to En(u) = E(J?B>B); In view of the 
uniform convergence Bn u -> u this implies that 
C.6) E»-*E(u). 
So far {ck} was the moment sequence corresponding to a given distri- 
bution F. But we may start from an arbitrary completely monotone sequence 
{ck} and again define p(.n) by C.5). By definition these quantities are non- 
negative, and we proceed to show that they add to cQ. Indeed, by the basic 
reciprocity formula A.8) 
C.7) 2u(jh)plr = 2cr[ )hrArii(O). 
;=0 r=0 \r/ /i 
For the constant function u = 1 the right side reduces to cQ, and this 
proves the assertion. 
We see thus that any completely monotone sequence {ck} subject to the 
trivial norming condition c0 — 1 defines a probability distribution {/?'n)}, 
and the expectation En(u) of u with respect to it is given by C.7). It is 
interesting to see what happens when n—> oo. For simplicity let u be a 
polynomial of degree N. Since h = l/n it is not difficult to see that 
Ar «@) -> w(r)@). Furthermore n(n — 1) • • • (n — r + l)hr -> 1. The series 
h 
on the right in C.7) contains at most N + 1 terms, and so we conclude that 
as n —>¦ oo 
N „ 
C.8) En(« 
r=o r! 
for every polynomial of degree N. In particular, when u(x) = xr we get 
C.9) En(X0 - cr. 
In other words, the rth moment of the probability distribution Fn tends to cr. 
It is therefore plausible that there should exist a probability distribution F 
whose rth moment coincides with cr. We formulate this as 
Theorem 1. The moments cT of a probability distribution F form a 
completely monotone sequence with c0 = 1. Conversely, an arbitrary completely 
226 LAWS OF LARGE NUMBERS. APPLICATIONS IN ANALYSIS VII.3 
monotone sequence {cr} subject to the norming c0 — 1 coincides with the 
moment sequence of a unique probability distribution. 
This result is due to F. Hausdorff and was justly celebrated as a deep and 
powerful result. The systematic use of functional analysis led gradually to 
simplified proofs, but even the best purely analytical proofs remain relatively 
intricate. The present approach is new and illustrates how probabilistic 
reasoning can simplify and render intuitive complicated analytical arguments. 
We shall not only prove the theorem but give an explicit formula for F. 
From C.8) we know that for any polynomial u the expectations En(w) 
converge to a finite limit. From the uniform approximation theorem B.3) 
it follows that the same is true for any function u continuous in 0,1. We 
denote the limit of En{u) by E(u). The relation C.6) is then valid under 
any circumstances, but if we start from an arbitrary completely monotone 
sequence {ck} we have to prove1 that there exists a probability distribution 
F such that the limit E(«) coincides with the expectation of u with respect 
to F. 
For given 0 < t <, 1 and e > 0 denote by ut e the continuous function 
i—I 
on 0, 1 that vanishes for x > t + €, equals 1 for x < t, and is linear 
between t and t + €. Let Ue(t) — Ec(w(>€). If t < r then obviously 
"t.c < «r,€ anc* the maximum of the difference uT€ — uue is <(t—t)Jc 
For fixed €>0 it follows that Ue(t) is a continuous non-decreasing function 
of t. Again, for fixed t the value U€(t) can only decrease as e -*¦ 0, and 
hence Ue(t) tends to a limit which we denote by F(t). This is a non-decreas- 
ing function going from 0 to 1. It is automatically right continuous,2 but 
this is of no importance since we could achieve this in any case by changing 
the definition of F at its jumps. 
For the distribution function Fn of the probability distribution C.5) we 
have trivially from the definition of ut€ if d > e 
C.10) En(«(_,.€) < Fn(t) ? En(«(>€). 
As n -> co tjie two extreme members tend to Ue(t — d) and Ue(t), 
respectively. If t and t — d are points of continuity of F, we let e —*¦ 0 
to conclude that all limit points of the sequence {Fn(t)} lie between F(t — 6) 
1 We could stop the proof here, because the assertion is contained in either of two results 
proved elsewhere in the book: 
(a) In the Rie$z representation theorem of V,l since E(w) is obviously a positive linear 
functional of norm 1. 
(b) In the basic convergence theorem of VIII,1. 
The proof in the text (which partly repears thai of V.I) is given to reader this chapter 
self-contained and to lead to the inversion formula. 
2 Por a verification see the proof of the Riesz representation theorem in V,l. 
VIT.3 MOMENT PROBLEMS 227 
and F(t). Finally, letting <5-> 0 we conclude that Fn(t) -> F(t) for each t 
which is a point of continuity for F. This relation reads explicitly 
C.1D I 
i<nt\jj 
For r > 1 an integration by parts permits us to write the rth moment of 
Fn in the form 
C.12) E 
B(Xr) = 1 - r fV^x) dx. 
Jo 
We have shown in C.9) that the left side tends to cr, and since Fn -> F this 
proves that the rth moment of F coincides with cr. This concludes the 
proof of theorem 1. > 
Note that if F is an arbitrary probability distribution function on 0, 1 
then C.11) represents an inversion formula expressing F in terms of its 
moments. We restate this in 
Theorem 2. For the probability distribution F of theorem 1 the limit 
formula C.11) holds at each point of continuity. 
To avoid misconceptions it should be pointed out that the situation is radically different 
for distributions that are not concentrated on some finite interval. In fact, in general a 
distribution is not uniquely determined by its moments. 
Example. The log-normal distribution is not determined by its moments. The positive 
variable X is said to have a log-normal distribution if log X is normally distributed. 
With the standard normal distribution the density of X is denned by 
L x>0, 
/(*) L 
V2tt 
and f(x) =^0 for x < 0. For -1 < a <; 1 put 
C.13) fa{x) =f(x)[l+a sin Btt log*)]. 
We assert that fa is a probability density with exactly the same moments of /. Since fa > 0 
it suffices to show that 
*00 
xkf(x) sin Btt log x) dx = 0, k = 0, 1,. . . 
Jo 
The substitutions log x = t = y + k reduce the integral to 
1 C+oo 1 C 
_ e-it*+kt sin B7r/) dt = -^ eW e~W sin 
and the last integral vanishes since the integrand is an odd function. (This interesting 
example is-due to C. C. Heyde.) > 
This negative result should not give rise to undue pessimism, for suitable regularity 
conditions can remove the source of trouble. The best result is a theorem of Carleman 
228 LAWS OF LARGE NUMBERS. APPLICATIONS IN ANALYSIS VII.4 
to the effect that a distribution F on —00,00 is uniquely determined by its moments if 
C.14) 
that is, if the series on the left diverges. In this book we shall prove only the weaker 
statement that F is uniquely determined by its moments whenever the power series 
2 ju2ntn/Bn)\ 1 mverges in some interval. (Section 6 and XV,4.) Both criteria put 
restrictions on tne rate of growth of /j,2n. Even in the most general situation the knowledge 
of finitely many moments f*0, fj.x,. . . , /un leads to useful inequalities for F similar to 
those derived in V,7 from the knowledge of /j,0 and //vs 
*4. APPLICATION TO EXCHANGEABLE VARIABLES 
We proceed to derive a beautiful result due to B. de Finetti which may 
serve as a typical example of the ease with which theorem I of section 3 leads 
to surprising results. 
Definition. The random variables Xx, . . . , Xn are exchangeable 4 if the 
n! permutations (Xfci, . . . , XkJ have the same n-dimensional probability 
distribution. The variables of an infinite sequence {Xn} are exchangeable if 
Xx, . . . , Xn are exchangeable for each n. 
As the examples will show, there is an essential difference between finite 
and infinite sequences. We consider here the special case of an infinite 
sequence {Xn} of exchangeable variables assuming the values 0 and 1 only. 
The next theorem asserts that the distribution of such a process {Xn} are 
obtained by randomization of the binomial distribution. As usual we put 
Sn = Xx + • • • + Xn and interpret the event (Xfc = 1} as success. 
Theorem. To every infinite sequence of exchangeable variables Xn assuming 
only the values 0 and 1 there corresponds a probability distribution F con- 
centrated on 0,1 such that 
P{XX = 1, . . . , Xfc = 1, Xfc+1 = 0, . . . , Xn = Oj = 
Jo 
D.2) P{Sn = k) = (") jV(l-0)"-fc F{d6}. 
* Not used in the sequel. 
3 The first sharp results were obtained by Markov and Stieltjes around 1884. The 
recent literature on the subject is inexhaustible. See, for example, A. Wald, Trans. Amer. 
Math. Soc, voL 46 A939) pp. 280-306; H. L. Royden, Ann. Math. Statist., vol. 24 
A953) pp. 361-376 [gives bounds on F(x) — F(—x)]. For a general survey see the mono- 
graph by J. A. Shohat and J. D. Tamarkin, The problem of moments, New York, 1943 
(Math Surveys No. 1). See also S. Karlin and W. Studden A966). 
4 The term symmetrically dependent is also in use. 
VII.4 APPLICATION TO EXCHANGEABLE VARIABLES 229 
Proof. For brevity denote the left side in D.1) by pk n (with 0 <; k <; n) 
Put c0 = 1 and for n = 1,2,... 
D.3) *« =/>„.» = 
Then from the probabilistic meaning 
D-4) p^i.n -Pn-l.n-1 ~ Pn.n = ~ 
and hence 
D-5) Pn-2.n = Pn-i.n-1 ~ Pn-l.n = 
Continuing in this way we get for k < n 
D.6) pk,.n = Pk.n_x - pk+Un = (- l)n-kA"-kck. 
All these quantities are non-negative and hence the sequence {cn} is com- 
pletely monotone. It follows that cr is the rth moment of a probability 
distribution F, and so D.1) merely spells out the relation D.6). The 
assertion D.2) is contained in it because there are I I ways in which k 
successes can occur in n trials. y 
Generalizations. It is not difficult to apply the same argument to variables 
capable of three values, but we have then two free parameters, and instead 
of D.2) we get a mixture of trinomial distributions with F a bivariate 
probability distribution. More generally, the theorem and its proof are 
readily adapted to random variables assuming only a finite number of values. 
This fact naturally leads to the conjecture that the most general symmetrically 
dependent sequence {XJ is obtained by randomization of a parameter 
from a sequence of independent variables. Individual cases are not diffi- 
cult to trust but the general problem presents the inherent difficulty that 
"parameters" are not well defined and may be chosen in weird ways. A 
version of the theorem has been proved nevertheless in very great generality.5 
The theorem makes it possible to apply laws of large numbers and the 
central limit theorem to exchangeable variables, (See problem 21 in VIII,10.) 
The next example shows that in individual cases the theorem may lead to 
surprising results. The other examples show that the theorem fails for finite 
sequences. 
Examples, (a) In Polya's urn model of 1; V,2 an urn contains originally 
b black and r red balls. After each drawing the ball is returned and c 
balls of the color drawn are added to the urn. Thus the probability of a 
5 E. Hewitt and L. J. Savage, Symmetric measures on Cartesian products, Trans. Amer. 
Math. Soc., vol. 80 A956) pp. 470-501. A martingale treatment is found in Loeve A963). 
See also H. Biihlmann, Austauschbare stochastiche Variabeln und ihre Grenzwertsa'tze, 
Univ. of California Publications in Statistics, vol. 3, No. 1 A960) pp. 1-36. 
230 LAWS OF LARGE NUMBERS. APPLICATIONS IN ANALYSIS VII.5 
black ball in each of the first n drawings equals 
b(b+c)---(b + (n-l)c) \c I \ c J 
D.7) cn = 
Put Xn = 1 or 0 according as the wth drawing results in black or red. 
The easy calculation in 1; V,2 shows that these variables are exchangeable 
and hence cn represents the nth moment of a distribution F. The appear- 
ance of D.7) reminds one of the beta integral 11,B.5), and inspection shows 
that F is the beta distribution II,D.2) with parameters // = b/c and v = r/c. 
Again using the beta integral it is seen that D.1) agrees with 1; V,B.3), and 
D.2) with 1; V,B.4). 
(b) Consider the 6 distinguishable distributions of 2 balls in 3 cells and 
attribute probability i to each. Let Xf equal 1 or 0 according as the cell 
number i is occupied or empty. The variables are exchangeable but the 
theorem does not apply. Indeed, from D.3) we get c0 = I, cx = \, c2 = i, 
c3 = 0 and here the sequence stops. If it were the beginning of a completely 
monotone sequence {cn} we would have c4 = c5 = • • • = 0. But then 
A4cx = — i < 0 against the rule. 
(c) Let Xx,. . . , Xn be independent with a common distribution and 
Sn = Xx + 1- Xn. Put Yfc = Xfc - n-xSn for k = 1, . . . , n-1. The 
variables (Yx,. . . , Yn_x) are exchangeable but their joint distribution is 
not of the form suggested by de Finetti's theorem. > 
*5. GENERALIZED TAYLOR FORMULA AND SEMI-GROUPS 
The preceding three sections dealt with consequences of the limit relation 
in example l(a) involving the binomial distribution. We now pass to 
example \{b) involving the Poisson distribution. Since this distribution 
represents a limiting form of the binomial distribution one may hope that 
our simple treatment of Bernstein polynomials may be extended to the 
present situation. The starting point for this treatment was the identity 
A.10) in which the binomial distribution appears on the right. If we put 
6 = x/v and let v —»- oo this binomial distribution tends to the Poisson 
distribution with expectation x, and A.10) passes into6 
E.1) ^ 
r=o r! ;=o r! 
* This section may be omitted at first reading. 
6 For a direct proof of the identity E.1) it suffices to substitute for A1"^ its defining 
expression A.7). The left side then becomes a double sum, and the right side is obtained 
by an obvious rearrangement of its terms. 
VII. 5 GENERALIZED TAYLOR FORMULA AND SEMI-GROUPS 231 
We use this identity with / = 0 and at = u(Jh) where u is an arbitrary 
bounded continuous function on 0, oo, and h a positive constant. With 
x = hd the relation E.1) becomes 
E.2; f °- A' «@) = *"•'*! u(jh) ^ 
On the right we recognize the expectation of u with respect to a Poisson 
distribution, and we know from example \{b) that it tends to uF). To 
record this result in a more natural form we replace u(d) by u{6 + t) 
where f > 0 is arbitrary. We have thus proved the 
Theorem. For any bounded continuous function on 0, oo 
E.3) i^> 
r-0 rl n 
here7 0 > 0 and A-*-()+. 
This is a fascinating theorem first proved by E. Hille using much deeper 
methods. The left side represents,the Taylor expansion of u except that the 
derivatives are replaced by difference ratios. For analytic functions the left 
side approaches the Taylor series but the theorem applies also to non- 
differentiable functions. In this sense E.3) represents a generalization of the 
Taylor expansion and reveals a new side of its nature. 
There is another way of looking at E.3) which leads to the so-called 
exponential formula of semi-group theory. (See theorem' 2 in X,9.) The 
left side of E.3) contains the formal exponential series and it is natural to use 
it to define an operator exp 0A. The relation E.3) is then abbreviated to 
E.4) exp 0 A M(f)->«(*+0). 
To write it more consistently in terms of operators we introduce the translation 
operator8 T{B) sending u into the function ug defined by u0(t) = u(t+d). 
7 It will be noticed that the argument remains valid if both 6 and h are negative 
provided u is defined on the whole line. 
8 This is the proof, due to M. Riesz, given (in slightly greater generality) in E. Hille 
and R. S. Phillips, Functional analysis and semi-groups, AMS Colloquium Publications, 
vol. 31 A957) p. 314. Understandably the authors did not consider it helpful to refer 
to the linear interpolation E.7) as a Poisson randomization of the semi-group parameter or to 
take Chebyshev's inequality for granted. The probabilistic content was noted by D. G. 
Kendall: It is fully exploited in K. L. Chung, On the exponential formulas of semi-group 
theory, Math Scandinavica, vol. 10 A962) pp. 153-162. 
232 LAWS OF LARGE NUMBERS. APPLICATIONS IN ANALYSIS VII.6 
Then 7X0) = 1 is the identity operator and 
E.5) A 
h 
In operator language E.3) now becomes 
E.6) ^ 
The main information conveyed by this formula is that the whole family 
of operators TF) is determined by the behavior of T{h) for small h. 
In retrospect it is now clear that our derivation of E.6) applies to a much 
more general class of operators. The right side in E.2) is simply a linear 
combination of values of u and may be interpreted as an interpolation 
formula for u. An analogous interpolatory expression is meaningful for 
every family of operators {T{B)} defined for 6 > 0. Indeed, for fixed 
6 and h > 0 the operator 
E.7) Ah@) = e-*1 % U?$ T{kh) 
fc=o k! \hj 
is a weighted linear combination of operators T(kh). The weights are given 
by the Poisson distribution and are such that as h —>- 0 a neighborhood of 
6 preponderates, the complement carrying a weight tending to 0. This 
makes it at least plausible that with any reasonable notion of convergence 
and continuity we shall have Ah(B) -*- T(d) for any continuous family of 
operators TF). In particular, if the operators TF) form a semi-group, one 
has T{kh) = {T(Ji))k and the interpolatory operator Ah{6) is the same as 
appears on the left in E.6). It is therefore not surprising that the "exponential 
formula" E.6) is generally valid for continuous semi-groups of bounded 
operators. We shall return to the proof in X,9. 
6. INVERSION FORMULAS FOR 
LAPLACE TRANSFORMS 
The preceding section and example 1 (b) were based on a special case of the 
law of large numbers which may be stated as follows: If X is a random 
variable with a Poisson distribution of expectation Xd then for large X the 
probability of the event |X — X6\ > Xe is small. For P{X <; Xx) we get 
therefore as X -> oo 
0 if 6>x 
1 if 0<x. 
The expression on the left is a special case of E.2) when u assumes only the 
values 0 and 1, and so F.1) is contained in the theorem of the preceding 
section. The usefulness of this formula in analysis will now be illustrated by 
VII.6 INVERSION FORMULAS FOR LAPLACE TRANSFORMS 233 
applications' to Laplace transforms, a topic treated systematically in chapter 
XIII. 
Let F be a probability distribution concentrated on 0, oo. The Laplace 
transform of F is the function <p defined for X > 0 by 
F.2) <p(X) ~ Te-M F{dd}. 
Jo 
The, derivatives cp{k)(X) exist and are obtained by formal differentiation: 
F.3) (-1)* <p{k\X) = f V" Bk F{dd} . 
Jo 
From this identity and F.1) one sees that at every point of continuity of F 
F.4) 2 ^ V) 
This is an inversion formula of great use. It shows, in particular, that a 
distribution F is uniquely determined by its Laplace transform. 
The same argument leads to a great variety of related inversion formufas 
applicable under various circumstance. In fact, A.6) is an inversion formula 
for Laplace integrals of the form 
F.5) w(X) = I °VAx u(x) dx. 
Jo 
Formal differentiation can be performed as in F.3), and A.6) states that 
if u is bounded and continuous, then 
F.6) 
uniformly in every finite interval. 
[These inversion formulas hold under much wider conditions, but it 
seemed undesirable at this juncture to let the ballast of new terminology 
obscure the simplicity of the argument. An abstract version of F.6) appears 
in XIII.9.] 
If the distribution F possesses moments fix,... , fi2n its Lap/ace transform satisfies the 
inequalities 
9 
F.7) 
*- Kl 
k=0 fc=O 
which are of frequent use. To verify them we start from the well-known inequalities' 
2n —1 ( l)fc/*-' 2n ( l)fyfc 
F.8) y ^—f- <e-* < y ; , r > o. 
*-> k\ ^~ k\ 
Ar=O fc = 0 
9 Simple differentiation shows by induction that the difference between any two members 
in F.8) is a monotone function of /. 
234 LAWS OF LARGE NUMBERS. APPLICATIONS IN ANALYSIS VII.7 
Replacing / by Xt and integrating with respect to F one gets F.7). It follows, in particular, 
that 
*~ k\ 
in any interval 0 <, A < Aq in which the series on the right converges. It is known from 
analytic function theory that in this case the series in F.9) uniquely determines <p(X) for 
all A > 0, and hence the moments /ilt fi2,. . . determine the distribution F uniquely when- 
ever the series in F.9) converges in some interval |A| < Ao. This useful criterion holds also 
for distributions not concentrated on 0, oo, but the proof depends on the use of charac- 
teristic functions (see XV,4). 
*7. LAWS OF LARGE NUMBERS FOR IDENTICALLY 
DISTRIBUTED VARIABLES 
Throughout this section we use the notation Sn = Xx + • • • + Xn. 
The oldest version of the law of large numbers states that if the Xfc are 
independent and have a common distribution with expectation fx and 
finite variance then10 for fixed e > 0 as n —> oo 
G.1) n\rr^n - fx\ > e} -> 0. 
This chapter started from the remark that G.1) is contained in Chebyshev's 
inequality. To obtain sharper results we derive a variant of Chebyshev's 
inequality applicable even when no expectation exists. Define new random 
variables X^ by truncation of Xfc at an arbitrary, but fixed, level ±sn. 
Thus 
Xfc when \Xk\<sn 
\ * '*") **k — 
0 when \Xk\ > sn. 
Put 
G.3) S; = Xi + • • • + X;f m'n = E(S;) = n 
Then obviously 
G.4) P{isn-m;i >t}< p{is;-m;i > t} + p{sn 
because the event on the left cannot occur unless one of the events on the 
right occurs. 
* The topics of this section are related to the oldest probabilistic theory but are of no 
particular significance in the remainder of this book. They are treated for their historical 
and methodological interest and because many papers are devoted to partial converses of 
the law of large numbers. 
10 G.1) is equivalent to w^ — m -?-> 0, where -?-> signifies "tends in probability 
to." (See VIII,2.) 
VII.7 LAWS OF LARGE NUMBERS 235 
This inequality is valid also for dependent variables with varying distri- 
butions, but here we are interested only in identically distributed independent 
variables. Putting t — nx and applying Chebyshev's inequality to the first 
term on the right, we get from G.4) the following 
Lemma. Let the Xfc be independent with a common distribution F. Then 
for x > 0 
G.5) 
- Sn - E(XD 
n 
sn}. 
As an application we could derive Khintchine's law of large numbers 
which states that G.1) holds for all e > 0 whenever the Xfc have finite 
expectation fi. The proof would be essentially a repetition of the proof for 
the discrete case given in 1; X,2. We pass therefore directly to a stronger 
version which includes a necessary and sufficient condition. For its formula- 
tion we put for t > 0 
G.6) T(t) = [l-F(t)+F(-t)]t 
and 
G.7) G@ = - P x2 F(dx) = -r@ + - Cxiix) dx. 
t J-t t Jo 
(The identity of these two expressions follows by a simple integration by 
parts.) 
Theorem 1. (Generalized weak law of large numbers.) Let the Xk be 
independent with a common distribution F. In order that there exist constants 
fxn such that for each e > 0 
G.8) P{|«-iSn-^|>e}->0 
it is necessary and sufficient that11 r(t) -> 0 as t -> oo. In this case G.8) 
holds with 
G.9) 
in = (nx 
J—n 
Proof, (a) Sufficiency. Define fxn by G.9). We use the truncation G.2) 
with s — n. Then fin = E(X^) and the preceding lemma the left side of 
G.8) is <e~2a(n) + r(n), which tends to 0 whenever r(t) -> 0. Thus this 
condition is sufficient. 
(b) Necessity. Assume G.8). As in V,5 we introduce the variables °X^ 
obtained directly by symmetrization of Xfc. Their sum °Sn can be obtained 
11 It follows from G.7) that t(/)-*0 implies <r(/)-*0. The converse is also true; see 
problem 11. For a different proof of theorem 1 see XVII ,2a. 
236 LAWS OF LARGE NUMBERS. APPLICATIONS IN ANALYSIS VII.7 
by symmetrization of Sn — rtfx. Let a be a median of the variables Xk. 
Using the inequalities V,E.6), V,E.10), and V,E.7) in that order we get 
2P{|Sn-«yu| > ne} > P{|°SJ > 2/7e} > HI'- exp (-nF^X^ > 2ne})] 
> |[1 - exp (-JnPflXJ > 2ne+\a\})]. 
In view of G.8) the left side tends to 0. It follows that the exponent on the 
right tends to 0, and this is manifestly impossible unless r(t) -> 0. > 
The condition r(t) —*¦ 0 is satisfied whenever F has an expectation fi. 
The truncated moment fxn then tends to ju and so in this case G.8) is 
equivalent with the classical law of large numbers G.1). However, the 
classical law of large numbers in the form G.1) holds also for certain.variables 
without expectation. For example, if F is a symmetric distribution such that 
t{\ — F@]-+0 then Pfln^SJ > e}-+0. But an expectation exists only 
if 1 — F(t) is integrable between 0 and oo, which is a stronger condition. 
(It is interesting to note, that the strong law of large numbers holds only 
for variables with expectations. See theorem 4 of section 8). 
The empirical meaning of the law of large numbers was discussed in 1; X with special 
attention to. the classical theory of "fair games." We saw in particular that even when 
expectations exist a participant in a "fair game" may be strongly on the losing side. On 
the other hand, the analysis of the St. Petersburg game showed that the classical theory 
applies also to certain games with infinite expectations except that the "fair entrance fee" 
will depend on the contemplated number of trials. The following theorem renders this 
more precise. 
We consider independent positive variables Xk with a common distribution F. [Thus 
F@) <= 0.]. The Xk may be interpreted as possible gains, and an as the total entrance fee 
for n trials. We put 
G.10) Ms) = fS* F{dx), KS) = P(s), 
s1 ts) 
= fS* 
Theorem 2. //; order that there exist constants an such that' 
G.11) P{|fl;1Sn-l|>e}->0 
it is necessary andsufficient that12 p(s) -*¦ co as s -*¦ oo. In this case there exist numbers sn 
such that 
G.12) nn(sn) = sn 
and G.11) holds with an = n/j(sn). 
Proof, (a) Sufficiency. Assume p(s) -* oo. For large n the function n[i{s)ls assumes 
values > 1, but it tends to 0 as s ->¦ oo. The function is right continuous, and the limit from 
the left cannot exceed the limit from the right. If sn is the lower bound of all s such that 
n[i(s)s~l < 1 it follows that G.12) holds. 
12 It will be seen in VIII,9 (theorem 2) that P(s) -* oo iff ju(s) varies slowly at infinity. 
The relation G.11) is equivalent to ^-iSu p ) 1 (see V1II,2). 
VII.8 STRONG LAWS 237 
Put fin = ju(sn) = E(X{). We use the inequality G.5) of the lemma with x = ejun to 
obtain 
G.13) 
Jn 
- 1 
1 
An integration by parts reduces E(X[2) to an integral with integrand x[l — F(x)], and by 
assumption this function is o(ji(x)). Thus E(X^2) = 0(v*»»)» and in view of G.12) this 
means that the first term on the right in G.13).tends to 0. Similarly G.12) and the definition 
G.10) of p(s) show that n[l — F(sn)] -+0. Thus G.13) reduces to G.11) with c^ = nfin. 
(b) Necessity. We now assume G.11) and use the truncation G.2) with sn = 2an. Since 
2) <, snnn we get from the basic inequality G.5) with x = 
G.14) P{Sn > nii + ea) ? • 
Since we are dealing with positive variables 
G.15) P{Sn < 2an} ? P{maxXfc? 2an} 
By assumption the left side-tends to 1, and this implies n[l —FBan)]-+0 (because 
x <, e~A~x) for x <, 1). If nfin\an tended to zero the same would be true of the right side 
in G.14) and this inequality would manifestly contradict the assumption G.11). This 
argument applies also to subsequences and shows that n/njan remains bounded away 
from zero; this in turn implies that />Bon) -*• co. 
To show that p(x) -*¦ co for any approach x -*¦ co choose an such that 
2an < x <, 2an+1. Then p(x)> {la^ajan^, and it is obvious that G.11) necessitates the 
boundedness of the ratios an+1/an. >¦ 
*8. STRONG LAWS 
Let X1( X2,. . . be mutually independent random variables with a common 
distribution F and E(Xfc) = 0. As usual we put Sn = Xx + • • • + Xn. 
The weak law of large numbers states that for every e > 0 
(8.1) P{n-1)SJ>6}->0. 
This fact does not eliminate the possibility that n~1Sn may become arbitrarily 
large for infinitely many n. For example, in a symmetric random walk the 
probability that the particle passes through the origin at the nth step tends to 
0, and yet it is certain that infinitely many such passages will occur. In 
practice one is rarely interested in the probability in (8.1) for any particular 
large value of n. A more interesting question is whether n~x \Sn\ will 
ultimately become and remain small, that is, whether rr1 |SJ < e 
simultaneously for all n > N. Accordingly we ask for the probability of the 
event13 that n-xSn-^0. 
* This section may be omitted at the first reading. 
13 It follows from the zero-or-one law of IV,6 that this probability equals 0 or 1, but we 
shall not use this fact. 
238 LAWS OF LARGE NUMBERS. APPLICATIONS IN ANALYSIS VII.8 
If this event has probability one we say that {Xk} obeys the strong law of 
large numbers. 
The next theorem shows that this is the case whenever E(XX) = 0. [That 
this statement is much stronger than the weak law of large numbers follows 
from the fact that (8.1) was seen to hold also for certain sequences {Xk} 
without expectation. By contrast the existence of an expectation is a necessary 
condition for the strong law. In fact, the converse to the strong law, discussed 
at the end of this section, shows that in the absence of an expectation the 
averages n'1 |SJ are certain infinitely often to exceed any prescribed 
bound a.] 
Theorem 1. (Strong law of large numbers.) Let X1} X2, . . . be independent 
identically distributed variables with E(X) = 0. Then «-1Sn —»¦ 0 with prob- 
ability 1. 
The proof depends on truncations and is in effect concerned with sequences 
with varying distributions. To avoid duplications we therefore postpone the 
proof and prepare for it by establishing another theorem of wide applicability. 
Theorem 2. Let X1} X2, . . . be independent random variables with arbitrary 
distributions. Suppose that E(Xfc) = 0 for all k and 
(8.2) |e(S2) < oo. 
Then the sequence {Sn} converges with probability one to a finite limit S. 
Proof. We refer to infinite-dimensional sample space defined by the 
variables Xk. Let A(e) be the event that the inequality |Sn—SJ > e holds 
for some arbitrarily large subscripts n, m. The event that {Sn} does not 
converge is the monotone limit as e—>0 of the events A(e), and so it 
suffices to prove that P{^(e)} = 0. Let Am(e) be the event that 
|Sn — SJ > e for some n > m. Then A(e) is the limit as tn—*-oo of the 
decreasing sequence of events Am(e), and so it suffices to prove that 
P{Am(e)} -> 0. Finally, for n > m let Am>n{e) be the event that 
|St — S J > e for some m <k <n. By Kolmogorov's inequality 
(8.3) P{AmJe)} < e Var (SB-Sm) = e ? E(X*). 
Letting n —* oo we conclude that 
(8.4) 
k=m+l 
and the right side tends to 0 as m -> oo. 
VII.8 STRONG LAWS 239 
This theorem has many applications. The following is a variant to be used 
for the proof of the strong law of large numbers. 
Theorem 3. Let X1} X2. . . be independent variables with arbitrary distri- 
butions. Suppose that E(XA.) = 0 for all k. If bx < b2 <•••-> oo and if 
(8.5) 25 
then with probability one the series ? b~1Xk converges and 
(8.6) *;1sB-o. 
Proof. The first assertion is an immediate consequence of theorem 2 
applied to the random variables J^X*. The following widely used lemma 
shows that the relation (8.6) takes place at every point at which the series 
converges, and this completes the proof. > 
Lemma 1. ("Kronecker's lemma".) Let {xk} be an arbitrary numerical 
sequence and 0 < bx < b2 <•••—>- oo. If the series ]?" ^kxk converges, then 
(8.7) *'+^ + *-^o. 
Proof. Denote the remainders of our convergent series by pn. Then for 
n = 1, 2, . . . 
and hence xn = 6n(pB_i - pn), 
(8.8) *1 + Y' + ** =-Pn + ~ 2Mbw-bJ + f • 
bn bn k=i on 
Suppose \pk\ < e for k > r. Since bn ->• oo the contribution of the first 
r terms in the sum tends to zero, while the remaining terms add to at most 
e(bn — br)/bn < e. Thus (8.7) is true. > 
Before returning to the strong law of large numbers we prove another 
lemma of a purely analytic character. 
Lemma 2. Let the variables Xk have a common distribution F. Then for 
any a > 0 
(8.9) 2pdx*l >ok}< oc 
if and only if E(Xk) exists. 
Proof. According to lemma 2 of V,6 an expectation E(XX) exists if and 
only if 
(8.10) f°°[l-F(a:)+F(-a:)] dx < oo. 
Jo 
240 laSvs of large numbers, applications in analysis VII.8 
The series in (8.9) may be considered as a Riemann sum to the integral, and 
since the integrand is monotone the two relations (8.9) and (8.10) imply each 
other. ». 
We are finally in a position to prove the strong law of large numbers.14 
Proof of theorem 1. We use truncation and define new random variables as 
follows: 
x; = xfc, x^ = o if \xk\ < k 
(8.11) J 
x; = o, xi = xk if \xk\>k. 
Since a finite expectation exists we conclude from the preceding lemma with 
a = 1 that 
(8.12) 2P(X^0}<oo 
and this implies that with probability one only finitely many variables X^ 
will be different from 0. Thus, with obvious notations, -S'^^O with 
probability one. 
Next we shall prove that 
(8.13) J ^E(X^) < oo. 
By theorem 3 this implies that with probability one 
(8.14) n-^ 
But E(X^) -> 0 and hence obviously 
(8.15) n-lE(S'n) = n-xiE(X;) -> 0. 
To conclude the proof it remains only to verify the assertion (8.13). Now 
(8.16) E(X;2)=2 f x2F{dx). 
3=1 J}-l<\x\<} 
It follows that 
(8.17) 2^2E(X;2)=2 f z2 F{dz}fk-2. 
l ' 3=1 Jj-l<|xl< J k=j 
Jt=l 
The inner sum is less than 2/y and so the right side is less than 
oo /» r+co 
(8.18) 2 1*1 F{*x} = \* 
3 = 1 Jj-l<\x\<j J-oo 
This accomplishes the proof. 
14 
For a more direct proof see problem 12. 
VII.9 GENERALIZED MARTINGALES 241 
We saw that the weak law in the form G.8) applies also to certain sequences without 
expectation. This is in striking contrast to the strong law for which the existence of E(XX) 
is necessary. In fact, the next theorem shows that in the absence of a finite expectation the 
sequence of averages Sn/n is unbounded with probability one. 
Theorem 4. (Converse to the strong law of large numbers.) Let Xv X2,... be independent 
with a common distribution. If E(|Xj|) = oo then for any numerical sequence {cj with 
probability one 
(8.19) limsuplw-^-cJ = co 
Proof. Let Ak stand for the event that \Xk\ > ak. These events are mutually independ- 
ent and by lemma 2 the absence of an expectation implies that 2 P{An} diverges. By 
the second Borel-Cantelli lemma (see 1; VIII ,3) this means that with probability one 
infinitely many events Ak will occur, and so the sequence \Xk\lk is unbounded with 
probability one. But since Xk = Sk — Sfc_x the boundedness of |Sn|/« would entail the 
boundedness of \Xk\/k, and so we conclude that the sequence of averages SJn is un- 
bounded with probability one. 
This proves the assertion (8.19) for the special case ck = 0, and the general case may be 
reduced to jit by symmetrization. As in V,5 we denote by °Xk the symmetrized variables 
Xk. From the symmetrization inequality V,E.1) it follows that E(|°Xfc|) = co, and so the 
sequence of averages .°SJn is unbounded with probability one. But °Sn may be obtained 
by symmetrization of Sn — cn, and so the probability that (Sn—cn)jn remains bounded 
is zero. ^ 
*9. GENERALIZATION TO MARTINGALES 
Kolmogorov's inequality of V,8(e) provided the main tool for the proofs 
in section 8. A perusal of these proofs reveals that the assumed independence 
of the variables was used only to derive certain inequalities among expecta- 
tions, and hence the main results carry over to martingales and submartin- 
gales. Such generalizations are important for many application? and they 
throw new light on the nature of our theorems. 
We recall from VI, 12 that a finite or infinite sequence of random 
variables Ur constitutes a submartingale if for all r 
(9.1) E(Ur|2y>U, for k = 1, 2,.. . , r-1, 
where 33X c 332 c ' ' ' is an increasing sequence of cr-algebras of events. 
When all the inequalities are replaced by equalities then {Ur} is called a 
martingale. [In each case the r — 1 conditions (9.1) are automatically 
satisfied if they hold for the particular value k = r — l.J Recall also that if 
{XJ is a sequence of independent, random variables with E(Xfc) — 0; then 
the partial sums Sn form a martingale; furthermore, if the variances exist, 
{S^} is a submartingale. 
Theorem 1. (Kolmogorov's inequality for positive submartingales.) Let 
Ux, . . . , Un be positive variables. Suppose thai the submartingate condition 
* The contents of this section will not be used in the sequel. 
242 LAWS OF LARGE NUMBERS. APPLICATIONS IN ANALYSIS VII.9 
(9.1) is satisfied for r <, n. Then for t > 0 
(9.2) Pfmax U» > t) ? r1 E(UJ. 
If {U*} is an arbitrary martingale then the variables |Ut| form a sub- 
martingale. (See the lemma in VI, 12.) It follows that theorem I entails the 
important 
Corollary. (Kolmogorov's inequality for martingales.) If Ult...,\Jn 
constitute a martingale then for t > 0 
(9-3) pfmax |UJ > t) <, r1 E(|UJ). 
Proof of theorem 1. We repeat literally the proof of Kolmogorov's in- 
equality in V,8(e) letting S* = Vk. The assumption that the S,. are sums of 
independent random variables was used only to establish the inequality 
V,(8.16) which now reads 
(9.4) ^ ^ 
Now \A} is 93,-measurable, and therefore 
(9.5) E(UJ^ | »,) = I^CU, | SB,) ^ Uy1^ 
[See V,(I0.9)]. Taking expectations we get (9.4). > 
We turn to a generalization of the infinite convolution theorem of section 8, 
although it leads only to a special case of the general martingale convergence 
theorem. Indeed, Doob has shown that the following theorem remains valid 
if the condition E(S?) < oo is replaced by the weaker requirement that 
EflSJ) remain bounded. The proof of the general theorem is intricate, 
however, and our version is given because of the great importance of the 
theorem and the simplicity of the proof. (For a generalization see problem 
13.) 
Theorem 2. (Martingale convergence theorem.) Let {Sn} be an infinite 
martingale with E(S?) < C < oo for all n. There exists a random variable 
S such that Sn -+ S with probability one. Furthermore E(S J = E(S) for 
all n. 
Proof. We repeat the proof of theorem 2 in section 8. The assumption that 
the Sk are sums of independent variables was used only in (8.3) to prove 
that 
(9.6) 
Now we know from VI,12 that the martingale property of {Sn} implies that 
VI1.9 GENERALIZED MARTINGALES 243 
E(Sn | Sm) = Sm for n > m. By the fundamental property V,A0.9) of 
conditional expectations this implies 
(9.7) E(SnSm | S J = S2m) n > m. 
Taking expectations and recalling the formula V,A0.10) for iterated 
expectations we conclude that E(SnSm) = E(S^) and hence 
E(S*-S2m) = E(S2) - E(S2m), : n>m. 
But by the lemma of VI,12 the variables S* form a submartingale, and so 
the sequence (E(S*)} is monotonically increasing. By assumption it is 
bounded, and hence it has a finite limit. This implies the truth of (9.6). 
The martingale properly implies that E(Sn) is independent of n, and the 
identity E(S) = E(SW) follows from the boundedness of the sequence 
} [example VIII,l(e)]. > 
As an immediate corollary we obtain the following analogue to theorem 3 
of section 8. 
Theorem 3. Let {X,,} be a sequence of random variables such that 
(9.8) E(Xn | SB^) = 0 
for all n. If bx<b2< > oo and 
(9.9) 2 ^(XJ) < °°> 
then with probability one 
(9.10) Xl + 'V + X^a 
Proof. It is easily seen that the variables 
(9.11) Un= " 
form a martingale and that E(IP) is bounded by the series in (9.9). The 
preceding theorem therefore guarantees the almost sure convergence of 
{Un}, and by Kronecker's lemma this implies the assertion (9.10). > 
Examples, (a) Polya's urn scheme was treated in examples VI, 
and above in 4(<z). If Yn is the proportion of black balls at the nth. trial 
it was shown that {Yrt} is a martingale and we see now that a limit Y = 
lim Yrt exists with probability one. On the other hand, the probability of 
a black ball at the nth trial is obtained by randomization of the binomial 
distribution. Thus, if Sn is the tola! number of black balls drawn in the 
first n trials the distribution of n~lSn tends to the beta distribution F 
fv>und in example 4(a). It follows that the limit variable Y has the beta 
distribution F. 
244 LAWS OF LARGE NUMBERS. APPLICATIONS IN ANALYSIS VII. 10 
(b) Branching processes. In the branching process described in 1; XH,5 
the population size Xn in the nth generation has the expectation E(X J = 
= pn [seel; XII,D.9)]. Given that the (n— l)th generation consisted of v 
individuals, the (conditional) expectation of Xn becomes /nv, and this 
independently of the size of the preceding generations. Thus, if we put 
Sw = XJ/nn the sequence {SJ forms a martingale. It is not difficult to 
establish that if E(Xj[) < co then E(S*) remains bounded (see problem 
7 of 1; XII,6). We thus have the striking result that Sw converges with 
probability one to a limit S^. This implies, in particular, that the distri- 
bution of Sw tends to the distribution of S^. These results are due to 
T. E. Harris. 
(c) Harmonic functions. For clarity we describe a specific example, 
although the following argument applies to more general Markov chains 
and concordant functions {example VI,12(c)]. 
Let D denote the unit disk of points x = (xlt x2) such that x\ + x\ <; 1. 
For any point x e D let Cx be the largest circle centered at x and contained 
in D. -We consider a Markov process {Yn} in D defined as follows. 
Given that Yn = x the variable Yn+1 is uniformly distributed on the circle 
Cx; the initial position Yo = y is assumed known. The transition prob- 
abilities are given by a stochastic kernel K which for fixed x is concentrated 
on Cx and reduces there to the uniform distribution. A function u in D 
is concordant if u(x) equals the average of the value of u on Cx. Consider 
now a harmonic function u that is continuous On the closed disk D. Then 
{w(YJ} is a bounded martingale and hence Z = lim «(YJ exists with 
probability one. Since the coordinate variables xf are harmonic functions 
it follows that with probability one Yn tends to a limit Y e D. It is easily 
seen that the process cannot converge to an interior point of D, and hence 
with probability one Yn tends to a point Y on the boundary of D. 
An extension of arguments of this sort is used for the study of asymptotic 
properties of Markov processes, and also to prove general theorems con- 
cerning harmonic functions, such as Fatou's theorem concerning the 
existence almost everywhere of radial boundary values.15 > 
10. PROBLEMS FOR SOLUTION 
1. If u is bounded and continuous on 0, oo tnen as n 
» /n+k\ tk (?k 
uniformly in every finite interval. 
15 M. Brelot and J. L. Doob, Ann. Inst. Fourier, vol. 13 A963), pp. 395-^15. 
VII. 10 PROBLEMS FOR SOLUTION 245 
Hint: Remember the "negative binomial" distribution of 1; VI,8. No calcu- 
lation necessary. 
2. If u has a continuous derivative u , the derivative B'nu of Bn u tends 
uniformly to u'. 
3. Bernstein polynomials in ft2. If u(x,y) is continuous in the triangle x > o, 
y > 0, x + y <, 1, then uniformly 
i • ~n) j\k\(n-j-kY. 
4. A function u continuous in 0,1 can be uniformly approximated by even 
polynomials. If u@) = 0 the same is true for odd polynomials.16 
5. If n is continuous in the interval 0, oo and w(a>) exists, it can be approxi- 
mated uniformly by linear combinations of e~nx. 
6. For the three moment sequences given below find the probabilities />?"> of 
C.5). Find the corresponding distribution F using the limit relation C.11). 
(a) fin = pn @ < p < 1), F) nn — , (c) nn —- . 
n t l n -t l 
7.17 Let p be a polynomial of degree v. Show that A71/? vanishes identically 
h 
when n > v. Conclude that Bn>P is a polynomial of degree <> (despite its 
formal appearance as polynomial of degree n > v). 
8. When F has a density, F.4) can be derived by integration from F.6). 
9. Law of large numbers for stationary sequences. Let {Xk} (k =0, ±1, 
±2, . . .) be a stationary sequence and define the X? by truncation as in G.2). 
If E(Xfc) = 0 and E(X^X^) -* 0 as n -+ oo then 
Pin-1 |Xj + • • • + XJ > e} — 0. 
10. Let the Xk be independent and define the X? by truncation as in G.2). 
Let an -*¦ 0 and suppose that 
2 P{|X,| > sn} - 0, a? 2 E(X*2> ~* °- 
Prove that 
p(k - 
11. (To theorem 1 of section 7). Show that a{t) -*¦ 0 implies t(/)->0. 
Prove that t(x) — |tBx) < e for x sufficiently large. Apply this inequality 
successively to x = /, 2t, At, ... to conclude that t(/) < 2e. 
12. {Direct proof of the strong law of large numbers.) With the notations used 
in the proof of theorem 1 in section 8 put Zr = max \S'k\ for 2r < k < 2r+1. 
16 A famous theorem due to H. Ch. Miintz asserts that uniform approximation is possible 
in terms of linear combinations of 1, fni, zn-, ... iff 2 n^1 diverges. 
17 The use of this result leads to a considerable simplification of the classical solution 
of the moment problem (for example in the book of Shohai andTaiiiarkin). 
246 LAWS OF LARGE NUMBERS. APPLICATIONS IN ANALYSIS VII. 10 
Using Kolmogorov's inequality show that 
and that this implies the strong law of large numbers. (This proof avoids reference 
to theorems 2 and 3 of section 8.) 
13. {Convergence theorem for submartingales.) Prove that theorem 2 of section 
8 applies also to submartingales {Uk} provided Ufc > 0 for all k. 
14. Generalize the variant of Chebyshev's inequality in the example of V,7(a) to 
martingales.16 
18 A. W. Marshall, A one-sided analog of Kolmogorovs inequality, Ann. Math. Statist., 
vol. 31 A960) pp. 483-487. 
CHAPTER VIII 
The Basic Limit Theorems 
The main results of this chapter are found in sections 1,3, and 6. Sections 
4, 5, and 7 may be considered as sources of interesting examples. These are 
chosen because of their importance in other contexts. 
The last two sections are devoted to regularly varying functions in the 
sense of Karamata. This interesting theory steadily gains in importance, 
but it is not accessible in textboolcs and has not been adapted to distribution 
functions. A tremendous amount of disconnected calculation in probability 
can be saved by exploiting the asymptotic relations in section 9. They are of 
a technical nature in contrast to the simple section 8. 
1. CONVERGENCE OF MEASURES 
The following theory is independent of the number of dimensions. For 
convenience of expression the text refers to one-dimensional distributions, 
but with the conventions of 111,5 the formulas apply without change in higher 
dimensions. 
Two examples are typical for the phenomena,with which we have to cope. 
Examples, (a) Consider an arbitrary probability distribution F and 
put FJx) = F{x—n~x). At a point x at which F is continuous we have 
Fn(x)-> F(x), but at points of discontinuity Fn(x) -*¦ F(x—). We shall 
nevertheless agree to say that the sequence {Fn} converges to F. 
(b) This time we put Fn(x) = F(x+ri) where F is a continuous distri- 
bution function. Now Fn(x)->\ for all x: a limit exists, but is not a 
probability distribution* function. Here Fn{I) —*- 0 for every bounded 
interval, but not when / coincides with the whole line. 
(c) Let Fn{x) = F(x+(-\)nn). Then F2n(z)—1 whereas F2n+1(x)-+0. 
Accordingly, the distribution functions as such do not converge, but neverthe- 
less Fn{I} —*¦ 0 for every bounded interval. > 
247 
248 THE BASIC LIMIT THEOREMS VIII. 1 
Basic Notions and Notations 
It will be necessary to distinguish three classes of continuous functions. 
In one dimension1 C( — co, co) is the class of all bounded continuous 
functions; C[— oo, oo] is the subclass of functions With finite limits w(— oo) 
and w(oo); finally, Co(—oo, oo) is the subclass of functions "vanishing at 
infinity," thatjs, where w(±oo) = 0. 
We shall say that / is an interval of continuity for the probability distri- 
bution F if / is open and its endpoints are not atoms.2 The whole line 
counts as an interval of continuity. Throughout this section we use the 
abbreviations 
J*+oo , /*+oo 
u(x)Fn{dx}, E(u)= u(x)F{dx}. 
— 00 J— 00 
, Throughout this section Fn will stand for a proper probability distri- 
bution, but F will be permitted to be defective (that is, its total niass may 
be <1; see the definition in V,l). ». 
Definition.3 The sequence {Fn} converges to the (possibly defective) 
distribution F if 
A.2) 
for every bounded interval of continuity of F. In this case we write Fn-> F 
or F = lim Fn. 
The convergence is called proper if F is not defective. 
For stylistic clarity we speak sometimes of improper convergence to 
indicate that the limit F is defective. 
For ease of reference we record two simple criteria for proper convergence. 
Criterion 1. The convergence Fw -»¦ F is proper iff to each e > 0 there 
correspond numbers a and N such that Fn{—a, a} > 1 — e for n > N. 
1 For the analogue in higher dimensions note that in one dimension C[—oo, oo] is 
simply the class of continuous functions on the compactified line obtained by adding 
±oo to ft1. For C[— oo, oo] in 3l2 both axes are so extended, and this requires the 
existence of limits u(x, ±oo) and «(±op,x) for each number x. In itself this class is not 
ver) interesting, but distribution functions belong to it. For Co(—oo, oo) it is required 
that «(x, ±oo) = «(±oo,x) = 0. 
2 In higher dimensions it is required that the boundary of / has probability zero. 
3 For readers interested in general measure theory we remark the following. The 
definitions and the theorems of this section apply without change to bounded measures in 
arbitrary locally compact spaces provided "interval of continuity" is replaced by "open 
set whose boundary has measure zero." To bounded intervals there correspond subsets 
of compacts sets. Finally, Co is the class of conditions functions vanishing at infinity, 
that is, uE Co iff u is continuous and \u\ < e outside some compact set. The other 
classes play no role in this section. 
VIII. 1 CONVERGENCE OF MEASURES 249 
Proof. Without loss of generality we may suppose that —a, a is an 
interval of continuity for the limit F. The condition is sufficient because it 
implies that F{—a, a} > 1 — e, and hence F cannot be defective. Con- 
versely, if F is a probability distribution we may choose a so large that 
F{—a, a} > 1 — ?e. Then Fn{—a, a} > 1 — e for n sufficiently large, 
and hence the condition is necessary. ^ 
Criterion 2. A sequence {Fn} of probability distributions converges to a 
proper probability distribution Fiff(\ .2) holds for every bounded or unbounded 
interval I which is an interval of continuity for F. 
(This implies that in the case of proper convergence Fn(x) -> F(x) at 
every point of continuity of F.) 
Proof. We may suppose that Fn->F with F possibly defective. 
Obviously F is proper iff 0.2) holds for /= — oo, oo, and hence the 
condition of the criterion is sufficient. Assume then that F is a proper 
probability distribution. For x > —a the interval — oo, x is the union of 
-co, a and a, x. Using the preceding criterion it is therefore seen that for 
a and n sufficiently large Fn{—oo,x} differs from F{—oo, x} by less 
than 3e. A similar argument applies to x, oo and we conclude that A.2) 
holds for all semi-infinite intervals. »» 
We have defined convergence by A.2), but the next theorem shows that we 
could have used A.3) as defining relation. 
Theorem 1. (i) In order that Fn-* F it is necessary and sufficient that* 
A.3) E»-+E(u) for all u e C0(-oo, oo)v 
(ii) If the convergence is proper then En(w) —*• E(w) for all bounded con- 
tinuous functions. 
Proof, {a) It is convenient to begin with the assertion concerning proper 
convergence. Assume then that F is a probability distribution and Fn -¦>• F. 
Let u be a continuous function such that \u(x)\ < M for all x. Let A be 
an interval of continuity for F so large that F{A) > 1 — e. For the 
complement A.' we have then Fn{A'\ < 2e for all n sufficiently large. 
Since u is uniformly continuous in finite intervals it is possible to partition 
A by intervals Ix, . . . , In so small that within each, u oscillates by 'ess 
4 If En(w) -* E(w) for a certain class of functions one says that Fr converges 10 F 
"'weakly with respect to that class." Thus convergence in the sense of definition 1 is equiv- 
alent to weak convergence with respect to Co( — <x>, co). 
250 THE BASIC LIMIT THEOREMS VIII. 1 
than e. These 4 may be chosen so that they are intervals of continuity for 
F. Within A we can approximate u by a step function a assuming a 
constant value in each Ik and such that \u(x) — a{x)\ < e for all x e A. 
In the complement A' we put a{x) = 0. Then \u{x)—a{x)\ < M for 
x e A' and 
A.4) |E(«)-E(a)| ^ ei^} + M/^4'} ^ e + Me. 
Similarly for n sufficiently large 
A.5) |E» - E»| < €Fn{A} + MFn{A') <* « + 
Now Ew(cr) is a finite linear combination of values Fn{Ik} tending to 
F{Ik). It follows that En(a) -* E(ct) and so for n sufficiently large 
A.6) |E(a) - E»| < €. 
Combining the last three inequalities we get 
A.7) |E(ii) - En(«)| ^ jE(ii) - E(<r)| + |E(a) - En(<r)| + 
+ |En(<r) - En(«)| < 3(M + l)e 
and as e is arbitrary this implies that Ew(w) —> E(w). 
This argument breaks down in the case of improper convergence because 
then Fn{A'} need not be small. However, in this case we consider only 
functions «eC0(-co,co) and the interval A may be chosen so large that 
\u{x)\ < € for xgA'. Then \u(x) — a(x)\ < e for all x, and the in- 
equalities A.4)—A.5) hold in the sharper form.with the right side replaced by 
€. Thus A.2) implies A.3). 
(b) We prove5 that Ew(w) -> E(w) implies Fn -> F. Let / be an interval 
of continuity of F of length L. Denote by Is a concentric interval of 
length L + 6 where 6 is chosen so small that F{IS} < F{I} + e. Let u 
be a continuous function which within / assumes the constant value 1, 
which vanishes outside Is, and for which 0 < u(x) < 1 everywhere. 
Then Ew(w) > Fn{I) and E(w) < F{I6) < F{I) + e. But for n sufficiently 
large we have Ew(w) < E(w) + e, and so 
Fn{l) < En(ii) < E(«) + € < F{IS) + e 
Using a similar argument with Is replaced by an interval of length L — <5 
we get the reversed inequality Fn{I) > F{I) — 2e, and so Fn -> /" as 
asserted. > 
5 It would be simpler to apply the proof of theorem 2, but the proof of the text is more 
intuitive. 
VIII. 1 CONVERGENCE OF MEASURES 251 
It is desirable to have a criterion for convergence which does not pre- 
suppose the knowledge of the limit. This is furnished by 
Theorem 2. In order that the sequence {Fn} of probability distributions 
converges to a (possibly defective) limit distribution it is necessary and sufficient 
that for each «eC0(-co,co) the sequence ofexpectations, Ew(w), tends to a 
finite limit. 
Proof.6 The necessity is covered by theorem 1. For the proof of sufficiency 
we anticipate the selection theorem 1 of section 6. (It is elementary, but it is 
preferable to discuss it together with related topics.) 
According to this theorem it is always possible to find a subsequence {Fn) 
converging to a possibly defective limit <I>. Denote by E*(w) the expectation 
of u with respect to <D. Let u e Co(— oo, oo). Then EWfc(w)-+ E*(w) by 
theorem 1. But EWfc(w) ->lim Ew(w) as well. Hence E*(w) = limEw(w) and 
u was arbitrary in Co(—oo, oo). Another application of theorem 1 gives 
Fn -> Q> as required. ». 
Examples, (d) Convergence of moments. If the distributions Fn are 
concentrated on 0, 1 the definition of u outside this interval is immaterial 
and in the wording of the theorem it suffices to assume u continuous in 
0, 1. Every such function can be approximated uniformly by polynomials 
(see VII,2) and hence the theorem may be restated as follows. A sequence 
of distributions Fn concentrated on 0,1 converges to a limit F iff for each 
k the sequence of moments En(Xk) converges to a number fik. In this case 
fik = E(X*) is the Arth moment of F and the convergence is proper because 
fio=\. (SeeVII,3.) 
(e) Convergence of moments (continued). In general the expectations of 
Fn need not converge even if Fn->F properly.- For example, if Fn 
attributes weight n~x to n2 and weight 1 — n~x to the origin, then {Fn} 
converges to the distribution concentrated at the origin, but En(X)—> oo. 
We have however, the following useful criterion. If Fn-+ F and for some 
p > 0 the expectations En(|X|p) remain bounded, then F is a proper 
probability distribution. Indeed, the contribution of the region \x\ > a to 
Ew(IXi") is > ap(\— Fn{—a, a}) and this quantity can remain <M only 
if 1 — F{—a, a} < a~pM. Since a may be chosen arbitrarily large F must 
6 The theorem is trivial if one assumes the Riesz representation theorem (see note I, 
in V,l). Indeed, lim Ew(«) defines a linear functional and according to that theorem the 
limit is the expectation of u with respect to some F. 
252 THE BASIC LIMIT THEOREMS VIII.2 
be proper. A slight sharpening of this argument shows that the absolute 
moments En(|X|a) of order <x< p converge to E(|X|a). 
(/) Convergence of densities. If the probability distributions Fn have 
densities /„ the latter need not converge even if Fn -> F and F has a 
continuous density. As an example let fn(x) = 1 — cos Irnrx for 0 < x < 1 
and fn(x) = 0 elsewhere. Here Fn converges to the uniform distribution 
with density f(x) = 1 for 0 < x < 1, but fn does not converge to /. 
On the other hand, if fn-+f und f is a probability density then Fn-+ F 
where F is the proper distribution with density /. Indeed, Fatou's lemma 
[IV.B.9)] implies that lim inf Fn{I) > F{I) for every interval / of con- 
tinuity. If the inequality sign prevailed for some / it would hold a fortiori 
for every bigger interval, in particular for — oo, oo. This being impossible 
A.2) holds. > 
In dealing with functions ut such as sin tx or v(t+x) depending on a 
parameter t, it is often useful to know that for n sufficiently large the 
relation |En(uf) — E(wt)| < e holds simultaneously for all t. We prove 
that this is so if the family of functions ut is equicontinuous, that is, if to 
each e > 0 there corresponds a 6 independent of t such that 
\ut(x2) ~ ut(xi)\ < € whenever |x2—xx\ < <S. 
Corollary. Suppose that Fn-+ F properly. Let {ut} be a family of 
equicontinuous functions depending on the parameter t and such that 
\ut\ < M < oo for some M and all t. Then En(wt) -* E(wt) uniformly in t. 
Proof. The proof in theorem 2 that En(w) —*¦ E(w) depended on partition- 
ing the interval A into intervals within each of which u varies by less than e. 
In the present situation this partition may be chosen independently of t and 
the assertion becomes obvious. > 
Example, (g) Let ut(x) = u(tx) where u is a differentiate function 
with \u'(x)\ < 1- By the mean value theorem 
k02)-Wt0i)! < \t\ ¦ ta-*il, 
and so the family is equicontinuous provided t is restricted to a finite 
interval —a, a.. Therefore En(«() —*¦ E(ut) uniformly in every finite 
/-interval. > 
2. SPECIAL PROPERTIES 
According to definition 1 to V,2 two distributions U and V are of the 
same type if they differ only by location and scale parameters, that is, if 
B.1) V(x) = U(Ax+B), A>0 
We now show thai convergence is a property of types in the sense that a 
VIII.2 SPECIAL PROPERTIES 253 
change of location parameters does not affect the type of the limit distribution. 
It is this fact which makes it legitimate to speak of an "asymptotically normal 
sequence" without specifying the appropriate parameters. More precisely 
we prove 
Lemma 1. Let U and V be two probability distributions neither of which 
is concentrated at one point. If for a sequence {Fn} of probability distributions 
and constants an > 0 cmd <xw > 0 
B.2) Fn(anx+bn) -* U(x), Fn(*nx+Pn) -> V(x) 
at all points of continuity, then 
B.3) \ 5b _* A > o, Pn~bn _^B 
and B.1) is true. Conversely, if B.3) holds then each of the two relations B.2) 
implies the other and B.1). 
Proof. For reasons of symmetry we may assume that the first relation in 
B.2) holds. To simplify notations we put Gn(x) = Fn(anx+bn) and also 
Pn — <*-n\an anc* an = (^n~^n)lan- Assume then that Gn -> U. If 
B.4) P»-+A, on-+B 
then obviously 
B.5) Gn{Pnx+on)-»V(x) 
with V(x) = U(Ax + B). We have to prove that B.5) cannot take place 
unless B.4) holds. 
Since V is not concentrated at one point there exist at least two values 
x and x" such that the sequences {/>„#' + <Jn} and {pnx" + an} remain 
bounded. This implies the boundedness of the sequences {/>„} and {an}, 
and hence it is possible to find a sequence of integers nk such that pWfc —*¦ A 
and ank -> B. But then V(x) = if {Ax + B), and hence A > 0, for other- 
wise F cannot be a probability distribution. It follows that the limits A 
and B are the same for all subsequences, and so B.4) i's true. > 
Example. The lemma breaks down if V is concentrated at one point. 
Thus if />„-* oo and an = (—l)w the condition B.4) is not satisfied but 
B.5) holds with V concentrated at the origin. > 
Two types of sequences {Fn} of probability distributions occur so 
frequently that they deserve names. For notational clarity we state the 
definitions formally in terms of random variables Xn, but the notions 
really refer only to their distributions Fn. The definitions are therefore 
meaningful without reference to any probability space. 
Definition 1. Xn converges in probability to zero if 
B.6) P{|XJ>e}->0 
254 THE BASIC LIMIT THEOREMS VIII.3 
for any € > 0. We indicate this by Xn -> 0. 
By extension Xn —>¦ X means the same as Xw — X —>¦ 0. 
Note that B.6) holds iff the distributions Fn tend to the distribution 
concentrated at the origin. In general, however, Fn-+ F implies nothing 
about the convergence of Xl5 X2,. .. . For example, if the X, are independ- 
ent with a common distribution F then Fn->F but the sequence {XJ does 
not converge in probability. 
The following simple lemma is of frequent use but not always mentioned 
explicitly. (For example, the truncation method used in 1; X depends 
implicitly on it.) 
Lemma 2. Denote the disTributions of Xn and Yn by Fn and Gn. Suppose 
that Xn — Yn —^> 0 and Gn -+ G. Then also Fn->G. 
In particular, if Xw —> X then Fn -> F where F is the distribution of X. 
Proof. If Xn < x then either Yn<x + e or Xn — Yn < — e. The 
probability of the latter event tends to 0 and hence Fn(x) < Gn(x+e) + e 
for all n sufficiently large. The same argument leads to an analogous 
inequality in the opposite direction. > 
Definition 2. The sequence {Xw} is stochastically bounded if for each 
e > 0 there exists an a such that for all n sufficiently large 
B.7) P{|XJ > a) < €. . 
This notion applies equally to distributions in higher dimensions and 
vector variables Xw. 
A properly convergent sequence is obviously stochastically bounded 
whereas improper convergence excludes stochastic boundedness. We 
have therefore the trite but useful criterion: If the distributions Fn converge, 
then the limit F is a proper distribution iff {Fn} is stochastically bounded. 
If {XB} and {Yn} are stochastically bounded, so is {XB + YB}. Indeed, 
the event |XW+YJ > 2a cannot occur unless either {XJ > a or |YJ > a 
and therefore 
B.8) l 
3. DISTRIBUTIONS AS OPERATORS 
The convolution U = F-ku of a point function u and a probability 
distribution F was defined in V,4. If we define a family of functions ut 
by ut(x) = u(t—x) we can express the value U(t) as an expectation 
u(t-y)F{dy} = E(ut). 
_ -co 
VIII. 1 DISTRIBUTIONS AS OPERATORS 255 
We use this to derive a criterion for proper convergence. It is based on the 
class C[— oo, oo] of continuous functions with limits «(±oo) because 
such functions are uniformly continuous. 
Theorem 1. A sequence of probability distributions FH converges properly 
to a probability distribution F iff for each u e C[— oo, oo] the convolutions 
Un = Fn + u converge uniformly to a limit U. In this case U = Fj*u. 
Proof. The condition is necessary because the uniform continuity of. u 
implies that the family {ut} is equicontinuous and so by the last corollary 
Un-+ F*ku uniformly. Conversely, the condition of the theorem entails 
the convergence of the expectations Ew(«). We saw in section 1 that this 
implies the convergence Fn -*¦ F, but it remains to show that F is proper. 
For this purpose we use criterion 1 of section 1. 
If u increases monotonically from 0 to 1 the same will be true of each Un. 
Because of the uniform convergence there exists an N such that 
Wn(x) — UN(x)\ < € for n> N and all x. Choose a so large that 
UN(—a) < e. Now UN is defined by a convolution of the form C.1); 
restricting the interval of integration to — oo < y. < —2a we see that for 
n > N 
2e > Un(-a) > u{a)Fn{-2a). 
Since u increases to unity it follows that for n and a sufficiently large 
Fn(—a) will be as small as we please. For reasons of symmetry the same 
argument applies to \ — F(a), and hence F is proper by virtue of 
criterion 1. 
To illustrate the power of our last result we derive an important theorem 
of analysis whose proof becomes particularly simple in the present prob- 
abilistic setting. (For a typical application see problem 10.) 
Example, (a) General approximation theorems. With an arbitrary prob- 
ability distribution G we associate the family of distributions Gh differing 
from it only by a scale parameter: Gh(x) = G(x/h). As h —*¦ 0 the distri- 
butions Gh tend to the distribution concentrated at the origin, and hence by 
the preceding theorem Gh-ku-+u for each «eC[-oo, oo], the con- 
vergence being uniform.7 
7 For a direct verification note that 
r+oo 
Gh • «(/) - «@ = [u(t-y) - u(t)]G{dylh}. 
J — 00 
To given e>0 there exists a <5 such that within each interval of length 2<5 the oscillation 
of u is less than e. The contribution of the interval \y\ <, 6 to the integral is then <«, 
and the contribution of \y\ > 6 tends to 0 because Gh attributes to \y\ > d a mass 
tending to 0 as h-* 0. 
256 THE BASIC LIMIT THEOREMS VIII.3 
If G has a density g, the values of Gh * u are given by 
C-2) Gh*u{t) 
When g has a bounded derivative the same is true of Gh and C.2) may be 
differentiated under the integral. Taking for g the normal density we get 
the following ^ 
Approximation lemma. To each u ? C[—oo, oo] there exists an infinitely 
differentiable v ? C[— oo, oo] such that \u(x) — v(x)\ < e for all x. ^ 
In the present context it is desirable to replace the clumsy convolution 
symbol * by a simpler notation emphasizing that in C.1) the distribution 
F serves as an operator sending u into U. This operator will be denoted by 
the German letter 5 and we agree that U = %u means exactly the same as 
U = F-k u. The true advantage of this apparent pedantry will become 
visible only when other types of operators appear in the same context. It 
will then be convenient to see at a glance whether a distribution plays its 
original probabilistic role or serves merely as an analytic operator (even 
though this fine distinction may lead to schizophrenia among the distribu- 
tions themselves). With this explanation we introduce the 
Notational convention. With each probability distribution F we associate 
the operator 5 from C[— oo, oo] to itself which associates with the function 
uthe transform %u =¦ F^k u. As far as possible distributions and the associated 
operators will be denoted by corresponding Latin and German letters. 
As usual in operator notation 5©w denotes the result of % operating on 
Em, and so <5© denotes the operator associated with the convolution F*kG 
of two probability distributions. In particular, 5" is the operator associated 
with Fn *, the n-fold convolution of F with itself. 
Example, (b) If Ha denotes the atomic distribution concentrated at a, 
then ?a is the translation operator %>au{x) = u(x—a). In particular, &0 
serves as the identity operator: §oh = u. 
We now define the norm \\ u [| of the bounded function u by 
C.3) Ml = sup |u(*)| 
With this notation the statement "mm converges uniformly to w" simplifies 
to ||wn — tt|| —v 0. Note that the norm satisfies the easily verified triangle 
inequality \\u 4- v\\ < \\u\\ + ||i>||. 
An operator T is called bounded if there exists a constant a such thai 
II Tu|| < a - |jwl|. The smallest number with this property is called the norm 
VIII.3 DISTRIBUTIONS AS OPERATORS 257 
of T and is denoted by \\T\\. With these notations the principal properties 
of the linear operators associated with distribution functions are: 
They are positive, that, is, u > 0 implies %u > 0. They have norm.l, 
which implies 
C.4) 15" II < ||u||. 
Finally, they commute, that is, 5© = ©5- 
Definition.8 If $„ and 5 are operators associated with probability 
distributions Fn and F we write ^n -> 5 iff 
C.5) II^-^I-^O 
for each u ? C[— oo, oo]. 
In other words, %n -> 5 if Fnic u-> F+ u uniformly. Theorem 1 
may now be restated as follows. 
Theorem la. Fn -+ F properly iff %n -*. %. 
The next lemma is basic. It has the form of an algebraic inequality and 
illustrates the suggestive power of the new notation. 
Lemma 1. For operators associated with probability distributions 
C.6) llfofoM - @1@,u|| < ||&« - Exu\\ + ||8flM - ®lM||. 
Proof. The operator on the left equals Ei~©1)^2 + ($2—©2)©! and 
C.6) follows from the triangle inequality and the fact that g2 and ©j have 
norms < 1. Notice that this proof applies also to defective probability 
distributions. > 
An immediate consequence of C.6) is 
Theorem 2. Let the sequences {Fn} and {(/„} of probability distributions 
converge properly to F and G respectively. Then 
C.7) Fn*Gn->F*G. 
(The convergence is proper by the definition of Fit: G. The theorem is false 
if F or G is defective. See problem 9.)- 
As a second application we prove that theorem 1 remains valid if the class 
of functions u is restricted to the particularly pleasing functions with 
derivatives of all orders. In this way we obtain the more flexible 
8 In Banach space terminology C.5) is described as strong convergence. Note that it 
does not imply \\%n — ft || -* 0. For example, if Fn is concentrated at l//i and g is the 
identity operator, then $„«(*) — du(x) = u(x-n-1) — u(x) and C.5) is true but 
||3n — 2f|| = 2, because there exist functions |t>| <; 1 such that v@) — 1 and 
v(-n~x) = -1. 
258 THE BASIC LIMIT THEOREMS VIII.4 
Criterion 3. Let the Fn be probability distributions. If for each infinitely 
differentiate* v E C[— oo, oo] the sequence {^nv} converges uniformly, 
then there exists a proper probability distribution F such that Fn -> F. 
Proof. It was shown in example (a) that to given u e C[— oo, oo] and 
e > 0 there exists an infinitely differentiable v such that \\u — v\\ < e. 
By the triangle inequality 
C.8) \\%nu-%mu\\ < \\%nu-%nv\ + \\dnv-%mv\\ + |lafm»-3fm«||. 
The first and last terms on the right are <e, and by assumption the middle 
term is <e for all n,m sufficiently large. Thus {$„«} converges uniformly 
and Fn-> F by theorem 1. > 
With En and E defined in A.1) the same argument yields 
Criterion 2. Let Fn and F be proper probability distributions. If 
En(v) —v E(v) for each infinitely differentiable v vanishing at infinity then 
Fn->F. 
The basic inequality C.6) extends by induction to convolutions with 
more than two terms; for ease of reference we record the obvious result in 
Lemma 2. Let U = fa ¦ • • %n and 93 = ©x • • • ©n where the ^ and 
E3 are associated with prcbability distributions. Then 
C.9) 
3=1 
In particular 
C.10) ||gf*tt - ©n«|| < n • || 5« - 
(For applications see problems 14-15.) 
4. THE CENTRAL LIMIT THEOREM 
The central limit theorem establishes conditions under which sums of 
independent random variables are asymptotically normaliy distributed. 
Its role and meaning has been partly explained in 1; X,l and we have 
applied it on several occasions [last in example VI,ll(g)]. It occupies a 
place of honor in probability theory acquired by its age and by the fruitful 
role which it played in the development of the theory and 'still plays in 
applications. It is therefore appropriate to use the central limit theorem 
as a test case to compare the scope of the various tools at our disposal. 
For this reason we shall give several proofs. A more systematic treatment 
(including necessary and sufficient conditions) will be found in chapters IX, 
By this is meant that all derivatives exist and belong to C[ — oc, co]. 
VIII.4 THE CENTRAL LIMIT THEOREM 259 
XV, and XVI. The present discussion sidetracks us from the development 
of our main theme; its purpose is to illustrate the advantages of the operator 
terminology by a striking and significant example. Also, many readers will 
welcome an easy access to the central limit theorem in its simplest setting. 
At the cost of some repetitions we begin by a special case. 
Theorem 1. {Identical distributions in ft1.) Let Xl9 X2, . . . be mutually 
independent random variables with a common distribution F. Assume 
D.1) E(X,) = 0, Var(X,)=l. 
As n —v oo the distribution of the normalized sums 
D-2) S: = (X1+---+XJ/V« _ 
tends to the normal distribution 91 with density n(x) = e~ix2/\/2tt. 
In purely analytical terms: for a distribution F with zero expectation 
and unit variance 
D.3) 
For the proof we require the following 
Lemma. If %n is the operator associated with Fn(x) = F(xy/n) then for 
each u e C[—oo, oo] with three bounded derivatives 
D.4) n[<$nu-u]-+\u" 
uniformly on the line. 
Proof. Since E(X?) = 1 we can define a proper probability distribution 
Ffby 
D.5) F#n {dy} = ny2 Fn{dy} = nyz F{Jn dy}. 
The change of variables \jn y = s shows that Ff tends to the distribution 
concentrated at the origin. In view of D.1) we have for the difference of 
the two sides in D.4) 
n[dnu(x)-u(x)] - \u"{x) = 
D.6) p)() &{) ] 
y2 
The Taylor development of the numerator shows that for \y\ < e the 
integrand is dominated by | \y\ • ||«w|l < e ' ll«wll» and for a11 V bv II - 
Since F# tends to concentrate near the origin it follows that for n sufficiently 
large the quantity is in absolute value less than e(||u"|| + ||u"||), and so the 
left side tends uniformly to zero. > 
260 THE BASIC LIMIT THEOREMS VIII.4 
Proof of theorem 1. Denote by © and ©„, respectively, the operators 
associated with the normal distributions yi(x) and yi(z\/n ). Then by the 
basic inequality C.10) 
D.7) 
< n ||gfn u — u\ 
By the preceding lemma the right side tends to zero and hence 5? -> © 
by criterion 1 of section 3. >¦ 
Example, (a) Central limit theorem with infinite variances. It is of 
methodological interest to note that the proof of theorem 1 applies without 
change to certain distributions without variance, provided appropriate 
norming constants are chosen. For example, if the Xk have a density such 
that f{x) = 2 |*r_Mog \x\ for \x\ > 1 and f(x) = 0 for \x\ < 1, then 
(X^- • '+Xn)l(y/2n log«) has a normal limit distribution. (The proof 
requires only obvious changes.) Necessary and sufficient conditions for a 
normal limit are given in IX,7 and XVII,5. > 
The method of proof is of wide applicability. Problem 16 may serve as 
a good exercise. Here we use the method to prove the central limit theorem 
in more general settings. The following theorem refers formally to two 
dimensions but is valid in %r. 
Theorem 2. (Multivariate case). Let {Xn} stand for a sequence of mutually 
independent two-dimensional random variables with a common distribution 
F. Suppose that the expectations are zero and that the covariance matrix 
is given by 
D.8) C=l al pa^\ 
\paxa2 a2 )¦ 
As «->oo the distribution of (XXH +XJ/^ tenfo t0 the bivariate 
normal distribution with zero expectation and covariance matrix C. 
Proof. The proof requires no essential change if the matrix notation of 
111,5 is used. Since subscripts are already overtaxed we denote the points 
of the plane by row vectors x = (x{1), xl2)). Then u(x) denotes a function 
of the two variables and we denote its partial derivatives by subscripts. 
Thus u' = {ux, u2) is a row vector, and u" = (uik) is a symmetric two by 
two matrix. With this notation the Taylor expansion takes on the form 
D.9) u{x-y) = u{x) - yu'(x) + \yu'\x)yT + • • • 
VIII.4 THE CENTRAL LIMIT THEOREM 261 
where yT is the transpose of y, namely the column vector with com- 
ponents yll), yiz). In analogy with D.5) we define a proper probability 
distribution by 
= nq{y)F{y]ndy} where 
a\ a\ 
As in the last proof F# tends to the probability distribution concentrated 
at the origin. To D.6) there corresponds the identity 
D.10) f u(x-y)-u(x) + yu'jx) - \yu"{x)yT # 
J 
f u(x-y)-u(x) + yu'jx) - \yu"{x)yT # 
Jtt2 ^(y) 
where10 
D.11) m(x) = E(yu"(x)yT) = uu(z)o* + 2u12(x)palP2 + u22{x)a\. 
(Here E denotes expectations with respect to F.) In view of D.9) the 
integrand tends to zero and as in the preceding lemma it is seen that 
n[$nu—u]-+m uniformly, and the proof of the theorem requires no 
change. > 
Example, (b) Random walks in d dimensions. Let Xx, X2, . . . be independ- 
ent random vectors with a common distribution that may be described as 
follows. The Xk have a random direction in the sense introduced in 
1,10, and the length L is a random variable with E(L2) = 1. For reasons 
of symmetry the covariance matrix C is the diagonal matrix with elements 
or2 = ljd. The distribution of the normalized sum SJ\fn tends to the 
normal distribution with covariance matrix C. The distribution of the 
squared length of the vector SJ->Jn therefore tends to the distribution of 
the sum of squares of independent normal variables. It was shown in 11,3 
that this limit has the density 
This result shows the influence of the number of dimensions and applies, 
in particular, to the random flight example I,I0(e). 
(c) Random dispersal of populations. As an empirical application of the 
foregoing example consider the spread of a population of oak trees in 
prehistoric times. If new plants were due only to seeds dropped by mature 
trees, then seedlings would be located near mature trees and the distance of 
an nth generation tree from its progenitor would be approximately normally 
10 Obviously m(x) is the trace (sum of the diagonal elements) of the product Cu" 
This is true in all dimensions. [In one dimension m(x) — z" 
262 THE BASIC LIMIT THEOREMS VIII.4 
distributed. Under these conditions the area covered by the descendants of a 
tree would be roughly proportional to the age of the tree. Observations 
show that the actual development is inconsistent with this hypothesis. 
Biologists conclude that the actual dispersal was strongly influenced by birds 
carrying the seeds long distances.11 > 
We turn to a generalization of theorem 1 to variable distributions The 
conditions give the impression that they are introduced artifically with the 
sole purpose of making the same proof work. Actually it turns out that the 
conditions are also necessary for the validity of the central limit theorem 
with the classical norming used in D.17). (See XV,6.) 
Theorem 3. (Lindeberg).12 Let Xx, X2, . . . be mutually independent 
one-dimensional random variables With distributions Fx, F2,. . . such that 
D.13) 
and put 
D.14) s* = <r2 + • • • + al 
Assume that for each t > 0 
D.15) s~2\ f y 
*=1 J\v\>tsn 
or, what amounts to the same, that 
D.16) s;2i f y2Fk{dy}-+L 
*=l J\v\<tsn 
Then the distribution of the normalized sum 
D.17) S: = (X1+-- 
tends to the normal distribution 91 with zero expectation and unit variance. 
The Lindeberg condition D.15) guarantees that the individual variances 
cr* are small as compared to their sum szn in the sense that for given e > 0 
11 J. G. Skellam, Biometrika, vol. 38 A951) pp. 196-218. 
12 J. W. Lindeberg, Math. Zeit., vol. 15 A922) pp. 211-235. Special cases and variants 
had been known before, but Lindeberg gave the first general form containing theorem 1. 
The necessity of Lindeberg's condition with the classical norming was proved by Feller, 
Ibid., vol. 40 A935). (See XV,6.) 
Lindeberg's method appeared intricate and was in practice replaced by the method of 
characteristic functions developed by P. Levy. That streamlined modern techniques 
permit presenting Lindeberg's method in a simple and intuitive manner was shown by 
H. F. Trotter, Archiv. der Mathematik, vol. 9 A959) pp. 226-234. Proofs of this section 
utilize Trotter's idea. 
VIII.4 THE CENTRAL LIMIT THEOREM 263 
and all n sufficiently large 
D-18) **<«« k=\,...,n. 
In fact, obviously ak/sk is less than t2 plus the left side in D.15), and 
taking t = \* we see that D.15) implies D.18). 
Theorem 3 generalizes to higher dimensions in the way indicated by 
theorem 2. See also problems 17-20. 
Proof. To each distribution Fk we make correspond a normal distri- 
bution Gk with zero expectation and the same variance a\. The distribution 
Fk(zsn) of Xk/sn now depends on both k and n, and we denote the 
associated operator by 3f*,n« Similarly Ekn is associated with the normal 
distribution Gk(zsn). By C.9) it suffices to prove that 
D.19) ZJI <**.„« -©*.„« II -0 
for every ue C[—oo, oo] with three bounded derivatives. We proceed as 
in theorem 1, but D.6) is now replaced by the n relations 
y 
-(,) _, n. ^ dy} 
± J 
Splitting the interval of integration into \y\ < c and \y\ > e and using 
the same estimates as in D.6) we obtain 
D.21) I! &.„!« - u - & u" 
II 2 
||«'"|| 4 + Kll • f 2/2 F4{5B dy). 
5n J\v\<e 
The Lindeberg condition D.15) with t = e now guarantees that for n 
sufficiently large 
D.22) 2 
For our normal distributions Gk the Lindeberg condition D.15) is satisfied 
as a simple consequence of D.18), and therefore the inequality D.22) remains 
valid with <ftktn replaced by (&kn. Adding these two inequalities we obtain 
D.19), and this concludes the proof. > 
Examples, (d) Uniform distributions. Let X* be uniformly distributed 
[with density 1/B^)] between — ak and ak. Then a2 = ?a*. It is easily 
264 THE BASIC LIMIT THEOREMS VIII.4 
seen that the conditions of the theorem are satisfied if the ak remain 
bounded and a\ + • • • + a\-+ oo; indeed, in this case the sum D.15) 
vanishes identically for all n sufficiently large. On the other hand, if 
]Tfl2 < co then sn remains bounded and D.15) cannot hold: in this case 
the central limit theorem does not apply. (Instead we get an example of an 
infinite convolution to be studied in section 5.) 
A less obvious case where the central limit theorem does not hold is 
a\ = 2k. Then 3s2n = 2n+1 -2<2a\ and obviously the left side of 
D.15) is > \ if, say, t < To~o- These examples show that D.15) serves to 
insure that the individual Xk will be asymptotically negligible: the prob- 
ability that any term Xk will be of the same order of magnitude as the sum 
Sn must tend to zero. 
(e) Bounded variables. Assume that the Xk are uniformly bounded, 
that is, that all the distributions Fk are carried by some finite interval 
—a, a. The Lindeberg condition D.15) is then satisfied iff sn —*¦ oo. 
(/) Let F be a probability distribution with zero expectation and unit 
variance. Choose a sequence of positive numbers ak and put Fn(x) = 
= F(x/an) (so that Fk has variance cr?). The Lindeberg condition is satisfied 
iff sn -»- oo and ojsn —*¦ 0. Indeed, we know that these conditions are 
necessary. On the other hand, the left side in D.15) reduces to 
x2 F{dx}. 
\x\<tsn/ok 
Under the stated conditions sjak tends to oo uniformly in k = 1, . .. , n, 
and so for n sufficiently large all the integrals appearing in the sum will be 
<e. This means that the sum is <es* and so D.15) is true. > 
It is of methodological interest to observe that the same method of proof 
works even for certain sequences of random variables without expectations, 
but the norming factors are, of course, different. We shall return to this 
problem in XV,6 where we shall also further analyze the nature of the 
Lindeberg condition. (See problems 19-20.) 
We conclude this excursion by a version of the central limit theorem for 
random sums. The idea is as follows. If in theorem 1 we replace the fixed 
number n of terms by a Poisson variable N with expectation n it is plausible 
that the distribution of SN will still tend to 91. Similar situations arise in 
statistics and physics when the number of observations is not fixed in 
advance. 
We consider only sums of the form SN = Xx + • • • + XN where the 
X3 and N are mutually independent random variables. We suppose that 
the Xj have a common distribution F with zero expectation and variance 
1. Using the notation of section 2 we have 
VIII. 5 INFINITE CONVOLUTIONS 265 
Theorem 4.13 (Random sums.) Let Nx, N2,. .. be positive integral-valued 
random variables such that 
D.23) n-W^l. 
Then the distribution of SNJy/n tends to 91. 
The interesting feature is that S^jVn is not normalized to unit variance. In fact, 
the theorem applies to cases with E(Nn) = <x> and even when expectations exist, D.23) 
does not imply that «-1E(Nn) -* 1. Normalization to unit variance may be impossible, 
and when possible it complicates the proof. 
Proof. To avoid double subscripts we write P{Nn = k} = ak with the 
understanding that the ak depend on n. The operator associated with 
SNn is given by the formal power series ^ fl*5*- As in the proof of theorem 
1 let 5« t>e the operator associated with F(zy/ri). Since F7* -+ 91 it suffices 
to prove that 
D-24) J>*Sf>-5>-0 
uniformly for each u e C[— oo, oo] with three bounded derivatives. 
Using the obvious factoring and the basic inequality C.9) itis seen that 
D.25) Haft, - 8fsmi < ns!*-"iii - «n < \k - n\ • us,« - ui:. 
Because of D.23) the coefficients ak with \k — n\ > en add to less than e 
provided n is sufficiently large. For such n the norm of the left side in 
D.24) is 
D.26) < Jak liafjii - gjjull < 2e • ||u|| + 2C • n \\%nu - u\\. 
We saw in the proof of the lemma the right side is < 2e ||w|| + 3c ||w"| for 
all n sufficiently large, and so D.24) holds uniformly. > 
*5. INFINITE CONVOLUTIONS 
The following theocem is given for its intrinsic interest and because it 
is a good example for the working of our criteria. Stronger versions are 
found in VII.8, IX,9r and XV1U0. 
13 For generalizations to mutually dependent X3- sec P. Billingsley, Limit theorems for 
randomly selected partial sums, Ann. Math. Statist., vol. 33 A963) pp. 85-92. When 
D.23) is dropped one gets limit theorems of a novel form. See H. E. Robbins, The 
asymptotic distribution of the sum of a random number of random variables, Bull. Amer. 
Math. Soc. vol. 54 A948) pp. 1151-1161. 
For generalizations of the central limit theorem to other types of dependent variables 
the reader is referred to the book by Loeve. (For exchangeable variables, see problem 21.) 
¦ This section is not used in the sequel. 
266 THE BASIC LIMIT THEOREMS' VIII.5 
We denote by Xlf X2,... mutually independent variables with distribu- 
tions Flt F2, It is assumed that E(X,) = 0 and a\ = E(XJ) exist. 
Theorem. If a2 = 2 of < oo the distributions14 Gn of the partial sums 
Xj + • • • + Xn tend to a probability distribution G with zero expectation and 
variance a2. 
Proof. To establish the existence of a proper limit G it suffices (theorem 
1 of section 3) to show that for infinitely differentiable ueC[—co, oo] 
the sequence of functions $1^2 "' * 5nw converges uniformly as n -*¦ 00. 
Now for n > m by the obvious factorization 
E-1) 
Since E(X,.) = 0 we have the identity 
- u(x) = (+CC 
E.2) &*(*) - u(x) = ([u(x-y) - u(x) + *«'(*)] 
By the second-order Taylor expansion the integrand is in absolute value 
<; ||w*|| • yz and therefore ||gti/ — i/|| < or^ . ||w"||. By the Basic inequality 
C.9) the quantity E.1) is therefore ^«+1 -\ 4- a\) • ||«*|| and thus 
there exists a proper distribution G such that Gn -* G. Since 0rn has 
variance a\ + • • • -f a\ the second moment of G exists and is. <,a2. 
By the criterion of example \{e) this implies that G has zero expectation. 
Finally, G is the convolution of Gn and the limit distribution of 
Xn+1 -f • • • + Xn+J:, and hence the variance of G cannot be smaller than 
that of Gn. This concludes the proof. > 
Examples, (a) In example I,ll(c) a random choice of a point between 
0 and 1 is effected by a succession of coin * i?sings. In the present terminology 
this means representing the uniform distribution as an infinite convolution. 
Example 1,11 (d)' shows that the infinite convolution of the corresponding 
even-numbered terms is a singular distribution. (See XVII, 10.) 
(b) Let the Yk be independent with E(Y*) = 0 and E(Y|) = 1. Then 
the distributions of the partial sums of 2 ^Y,. converge if ]? b* < 00. 
This fact was exploited in the discussion of normal stochastic processes in 
111,7. 
(c) Application to birth processes. Let Xw be a positive variable with 
density Xne~Kt- Then E(Xn) ~ %/Var (XJ = A and in case m = 
bs ^ %ZX < ^ our theorem applies to the centered variables Xn — A. 
This observation leads to a probabilistic interpretation of the divergent pure 
birth process described in 1; XVII,3-4. A "particle" moves by successive 
14 It was shown in VH,8 that the random variables ?»„ themselves converge to a limit. 
VIII.6 SELECTION THEOREMS 
jumps, the sojourn times Xly X2,... being independent exponentially 
distributed variables. Here Sn = Xx + + Xn represents the epoch of 
the nth jump. If lim E(Sn) = m < oo, the distribution of Sn tends,to a 
proper limit G. Then G(t) is the probability that infinitely many jumps will 
occur before epoch /. 
(d) For applications to shot noise, trunking problems, etc., see problem 
22. : > 
6. SELECTION THEOREMS 
A standard method of proving the convergence of a numerical sequence 
consists in proving first the existence of at least one point of accumulation 
and then its uniqueness. A similar procedure is applicable to distributions, 
the analogue to-a point of accumulation being provided by the following im- 
portant theorem usually ascribed to Helly. As for all theorems of this section, 
it is independent of the number of dimensions. (A special case was used in 1; 
Theorem 1. (i) Every sequence {Fk} of probability distributions in %r 
possesses a subsequence Fni, Fnt,.. . that converges {properly or improperly) 
to a limit F. 
(ii) In order that all such limits be proper it is necessary and sufficient 
that {Fn} be stochastically bounded. (See definition 2 of section 2.) 
(iii) In order that Fn-+ F it is necessary and sufficient that the limit of 
every convergent subsequence equals F. 
The proof is based on the following 
Lemma. Let ax, a2,. . . be an arbitrary sequence of points. Every sequence 
{un} of numerical functions contains a subsequence unx, un2, . . . that converges 
at all points a5 (possibly to ±00).*" 
Proof. We use G. Cantor's "diagonal method." It is possible to find a 
sequence vx,v2,. . . such that the sequence of values uV]c(ax) converges. To 
avoid multiple indices we put w^1* = wVfc so that {w*1*} is a subsequence of 
{un} and converges at the particular point ax. Out of this subsequence we 
extract a further subsequence u[2), uB2),.. . ihat converges at the point a2. 
Proceeding by induction we construct for each 77 a sequence u[n), uBn),. . . 
converging at an and contained in the preceding sequence. Consider now 
the diagonal sequence u{xx), u{22), u{33),.... Except for its first n — 1 terms 
this sequence is contained in the nth sequence u[n) ¦; u{2n),. . . and hence it 
converges at an. This being true for each n, the diagonal sequence {w^n)} 
converges at all points ax, a2,. .. and the lemma is proved. > 
Proof of theorem 1. (i) Choose for {a,} a sequence that is everywhere 
dense, and choose a subsequence {FnJ that converges at each point a,-. 
268 THE BASIC LIMIT THEOREMS VIII.6 
Denote the limits by G(a,). For any point x not belonging to the set {a,} 
we define G(x) as the greatest lower iound of all G{a,) with a, > x. The 
function G thus defined increases from 0 to 1, but it need not be right- 
continuous: we can only assert that G(x) lies between the limits G(x+) 
and G(x—). However, it is possible to redefine G at the points of dis- 
continuity so as to obtain a right-continuous function F which agrees with 
G at all points of continuity. Let x be such a point. There exist two points 
0, < x < a}- such that 
F.1) G{as)-G(ad<e * G{a%) ? F(x) ? G(at). 
The Fn being monotone we have FnJjat) <, Fnjfx) < F^a,). Letting 
k-*- oo we see from F.1) that no limit point of the sequence {Fnt(x)} can 
differ from F(x) by more than e, and so Fn(x) -> F(x) at all points of 
continuity. 
¦ i 
(ii) Next we recall that a convergent sequence of distributions converges 
properly iff it is stochastically bounded. Given (i) the remaining assertions 
are therefore almost tautological. > 
, The selection theorem is extremely important. The following famous 
theorem in number theory may give an idea of its amazing power and 
may also serve as a reminder that our probabilistic terminology must not 
be allowed to obscure the much wider scope of the theory developed. 
Examples, (a) An equidistribution theorem in number theory.15 Let a be 
an irrational number and an the fractional part of net. Denote by Nn(x) 
the number of terms among alt a2,. . . , <xn that are < x. Then ?J~Wn(a:) -*¦ x 
for all 0 < x < 1. 
Proof. We consider distributions and functions on the circle of unit 
length; in other words, additions of coordinates are reduced modulo 1. 
(The idea was explained in 11,8. The convenient tool of distribution functions 
becomes meaningless on the circle, but distributions in the sense of measures 
are meaningful.) Let Fn. be the atomic distribution concentrated on the n 
points a, 2a, ...,«a- and assigning probability lfn to each. By the 
selection theorem there exists a sequence nx, n2,. .. such that Fnje'-+ F, 
where F is a proper probability distribution (the circle being bounded). 
Taking convolutions with an arbitrary continuous function u we get 
F.2) - [M(x-a) -f u(z-2a) + ¦¦ • • + u(z-nka.)] -*> v(z). 
15 Usually attributed to H. Wcyl although discovered independently by Bohl and by 
Sierpiriski. See G. H. Hardy and E. M. Wright, Theory of numbers, Oxford, 1945, pp. 
378-381, to appreciate the difficulties of the proof when the theorem is considered in a 
non-probabilistic setting. 
VIII.6 SELECTION THEOREMS 269 
Now it is obvious that replacing x by x—a. does not affect the asymptotic 
behavior of the left side, and hence v(x) = v{x—a) for all x. This in turn 
implies v(x) = v{x—ka) for k = 1, 2, By the corollary to lemma 2 
in V,4 the points a, 2a,... lie everywhere dense, and hence v = const. 
We have thus shown that for each continuous u the convolution F+u 
is a constant. It follows that F must attribute the same value to intervals of 
equal length, and so F{I] equals the length of the interval /. The impos- 
sibility of other limits proves that the whole sequence {Fn} converges to this 
distribution, and this proves the theorem. We call F the uniform distribution 
on the circle. 
{b) Convergence of moments. Let Fn and F be probability distributions 
with finite moments of all orders, which we denote by (xkn) and fjih, 
respectively. We know from VII,3 that different distribution functions can 
have .the same moment sequence and it is therefore not always possible from 
the behavior of fx{kn) to conclude that Fn -> F. However, // F is the only 
distribution with the moments fj.^ (jl2-> . . • and if /j.(kn) -*¦ pk for k = 1,2,... 
then Fn -*¦ F. In fact, the result of example \{e) shows that every convergent 
subsequence of {Fn} converges to F. 
(c) Separability. For brevity call a distribution rational if it is concentrated 
at finitely many rational points and-attributes a rational weight to each. An 
arbitrary distribution F is the limit of a sequence {Fn} of rational distri- 
butions, and we may choose Fn with zero expectation since this can be 
achieved by the addition of an atom and adjustment of the weights by 
arbitrarily small amounts. But there are only denumerably many rational 
distributions anil they may be ordered into a simple sequence Glt G2, 
Thus there exists a sequence {Gn} of distributions with zero expectations and 
finite variances such that every distribution F is the limit of some subsequence 
Theorem 1 was formulated in the form most useful for probability but is 
unnecessarily restrictive. The proof depended on the fact that a sequence 
{Fn} of monotone functions with Fn(— oo) — 0, Fn(oo) = 1 contains a 
convergent subsequence. Now this remains true also when the condition 
Fn(oo) = 1 is replaced by the less stringent requirement that the numerical 
sequence {Fn(x)} be bounded for each fixed x. The limit F will then be finite 
but possibly unbounded; the induced measure will be finite on intervals — oo, x, 
but-possibly infinite for — oo, oo. A similar relaxation is possible for — oo 
and we are led to the following generalization of theorem 1, in which the 
symbol nn~> fx is used in the obvious sense that the relation holds in 
finite intervals. 
270 THE BASIC LIMIT THEOREMS VIII.7 
Theorem 2. Let {/un} be a sequence of measures such that the numerical 
sequence fj.n{—x, x) is bounded for each x. There exists a measure /u 
and a sequence nu n2, . . ¦ such that fj.njc —> //,. 
Variants of the selection theorem hold for many classes of functions 
Particularly useful is the following theorem, usually called after either 
Ascoli or Arzela. 
Theorem 3. Let {«„} be an equicontinuous16 sequence of functions 
\un\ 5^ 1 • There exists a subsequence {unj converging to a continuous limit 
u. The convergence is uniform in every finite interval. 
Proof. Choose again a dense sequence of points a, and a subsequence 
{unk} converging at each a/, denote the limit by u{a,). Then 
F.3) \unr{x)-nnt{x)\ < \unr(x)-unr(aj)\ + K,(*)-"„,(*,)! 
By assumption the last term tends to 0. Because of the assumed equicontinuity 
there exists for each x a point a} such that 
F.4) \un{x) - un{a,)\ < e 
for all n, and finitely many such a, suffice for any finite interval /. It 
follows that the right side in F.3) will be <3e for all r and s sufficiently 
large uniformly in /. Thus u(x) = lim un(x) exists, and because of F.4) 
we have \u(x) — w(a,)| < «r which implies the continuity of w. > 
*7. ERGODIC THEOREMS FOR MARKOV CHAINS 
Let AT be a stochastic kernel concentrated on a finite or infinite interval 
Q.. (By definition 1 of VJ,1I this means: AT is a function of two variables, 
a point x and a set F, which for fixed F reduces to a Baire function of 
x and for fixed xeQ. a probability distribution concentrated on Q.) In 
higher dimensions the interval Q may be replaced by more general regions 
and the theory requires no change. 
It was shown in VI,11 that there exist Markov chains (Xo, Xlt. . .) with 
transition probabilities K. The distribution y0 of the initial variable Xo 
may be chosen arbitrarily and the distributions of Xlf X2,... are then 
16 That is, to each c > 0 there corresponds a 6 > 0 such that |x' — x"\ <6 implies 
\un{x') - Wn(x*)| < c for all n. 
• This material is treated because of its importance and as a striking example for the 
use of the selection theorems. It is not used explicitly in the sequel. 
VIII.7 ERGODIC THEOREMS FOR MARKOV CHAINS 271 
given recursively by 
G-1) yn(r) = (yn_1{dx)K(x,r). 
Ja 
In particular, if y0 is concentrated at a point x0 then yn{F) = Kin)(x0, F) 
coincides with the transition probability from x0 to F. 
Definition 1. A measure a is strictly positive in Q. if a{/} > 0 for each 
open interval I <= Q. The kernel K is strictly positive if K{x, /) > 0 for 
each x and each open interval in Q. 
Definition 2. The kernel is ergodic if there exists a strictly positive prob- 
ability distribution a such that 'yn—*- a independently of the initial probability 
distribution y0. 
This amounts to saying that 
G.2) 
for each interval of continuity for a. The definition is the same as in the 
discrete case A; XV); its meaning has been discussed and clarified by 
examples in VI,11. 
The most general stochastic kernels are,subject to various pathologies, 
and we wish to restrict the theory to kernels depending in a continuous 
manner on x. The simplest way of expressing this is by considering the 
transformations on continuous functions induced by K. Given a function 
u which is bounded and continuous in the underlying interval Q, we define 
w0 = u and, by induction, 
G.3) un(x) 
This transformation on functions is dual to the transformation G.1) on 
measures. Note that in both cases throughout this section indices serve to 
indicate the effect of a transformation induced by K. 
The regularity property that we wish to impose on K is, roughly speaking, 
that ux should not be worse than w0. The following definition expresses 
exactly our needs but looks formal. The examples will show that it is 
trivially satisfied in typical situations. 
Definition 3. The kernel K is regular if the family of transforms uk is 
equicontinuous11 whenever u0 is uniformly continuous in O.. 
17 See footnote16. Our "regularity" in analogous to "complete continuity" as used in 
Hilbert space theory. 
272 THE BASIC LIMIT THEOREMS VIII.7 
Examples, (a) Convolutions. If F is a probability distribution the 
convolutions 
J c 
represent a special case of the transformation G.3). (With self-explanatory 
notations in this case K(x, /) = F{I— x}. This transformation is regular 
because, w0 being uniformly continuous, there exists a d such that 
I*' — x"\ < d implies \uo(x) — uo(x")\ < e and by induction this entails 
\un{x) - un(x")\ <C e for all n. 
(b) Let Q be the unit interval and let K be defined by a density k which 
is continuous in the closed unit square. Then K is regular. Indeed 
G.4) \un{x') - un{x")\ <? \\k(x\ y) - k{x", y)\ • lii^Cy)! dy. 
Jo 
By induction it is seen that if |wo| < M also \un\ < M for all n. Because 
of the uniform continuity of k there exists a 6 such that 
\k(x', y) - k(x", y)\ < e/M whenever }x' - x"\ < <5, 
and then \un(x) — un(x")\ < e independently of n. y 
The condition of strict positivity in the following theorems is unnecessarily 
restrictive. Its main function is to eliminate the nuisance of decomposable 
and periodic chains with which we had to cope in 1; XV. 
Theorem 1. Every strictly positive regular kernel K on a bounded closed 
interval Q. is ergodic. 
This theorem fails when Q, is unbounded, for the limit in G.2) can be 
identically zero. A universal criterion may be formulated in terms of 
stationary measures. We recall that a measure a is called stationary for 
K if <x.x = a2 = • • • = a, that is, if all its transforms G.1) are identical. 
Theorem 2. A strictly positive regular kernel K is ergodic iff it possesses 
a strictly positive stationary probability distribution a. 
Proof of theorem 1. Let v0 be a continuous function and vx its transform 
G.3). The proof depends on the obvious fact that for a strictly positive 
kernel K the maximum of the transform vx is strictly less than the maximum 
of v0 except if v0 is a constant. 
Consider now the sequence of transforms un of a continuous function 
u0. Since Q, is closed, u0 is uniformly continuous on Q. and hence the 
sequence {un} is equicontinuous. By theorem 3 of section 6 there exists 
therefore a subsequence {wnfc} converging uniformly to a continuous 
VIII.7 ERGODIC THEOREMS FOR MARKOV CHAINS 273 
function v0. Then wWfc+1 converges to the transform vx of v0. Now the 
numerical sequence of the maxima mn of un is monotone, and hence mn 
converges to a limit m. Because of the uniform convergence both v0 and vx 
have the maximum m, and hence vo(z) = m for all x. This limit being 
independent of the subsequence {wnj we conclude that wn->m uniformly. 
Let y0 be an arbitrary probability distribution on Q. and denote by En 
expectations with respect to its transform yn defined in G.1). A comparison 
of G.1) and G.3) shows that 
E>0) = Eo(«n) — E0(m) = m. 
The convergence of En(w0) for arbitrary continuous u0 implies the existence 
of a probability measure a such that yn -> a. (See theorem 2 of section 
1; the convergence is proper since the distributions yn are concentrated on 
a finite interval.) From G.1) it follows that a is stationary for K. The strict 
positivity of a is an immediate consequence of the strict positivity of K. > 
Proof of theorem 2. Denote by E expectations with respect to the given 
stationary distribution a. For an arbitrary w0 e C[— oo, oo], and its 
transforms uk we have on account of the stationarity E(w0) = Efa,} = • • •. 
Furthermore, E(\uk\) decreases with k and so lim Eflw*)) = m exists. 
As in the preceding proof we choose a subsequence such that un]c -> v0. 
Then wnfc+1 -*- vx, where vx is the transform of v0. By bounded convergence 
this entails E(unk) -> E(u0) and E(KJ)-*E(j%i). Thus 
E(Vl) = E(v0) = E(w0) and Ed^l) = E(|wo|) = m. 
In view of the strict positivity of K the last equality implies that, the con- 
tinuous function v0 cannot change signs. When E(w0) = 0 we have 
therefore vo(z) = 0 identically. It follows that for arbitrary initial uQ we 
have vQ(x) = E(w0) for all x. This proves that un(x) --> E(m0) which is the 
same as K{n)(x, V) -> a(F) at all intervals of continuity. > 
We now apply this theory to convolutions on the circle of circumference 1, 
that is, to transformations of the form 
G.5) un+l(x) » I un(x-y) F{dy\ 
Jo 
where F is a probability distribution on the circle and addition is module 
1. [See I[,8 and example 6(a).] This transformation may be written in the 
form G.3) with Q. = 0, 1 and Kin\x,T) = Fn *{x - V}. Theorem I 
applies directly if F is strictly positive, but we prove the following more 
general analogue to the central limit theorem. 
274 THE BASIC LIMIT THEOREMS VIII.7 
Theorem 3.18 Let F be a probability distribution on the circle and suppose 
that it is not concentrated on the vertices of a regular polygon. Then Fn* 
tends to the distribution with constant density. 
Proof. It suffices to show that for an arbitrary continuous function u0 
the transforms un tend to a constant m (depending on u0). Indeed, as 
the second part of the proof of theorem 1 shows, this implies that Fn* 
converges to a probability distribution a on the circle, and since a * w0 
is constant for every continuous function u0 it follows that a coincides 
with the uniform distribution. 
To show that un-*-m we use the first part of the proof of theorem 1 
except that we require a new proof for the proposition that'the maximum 
of the transform vx of a continuous function v0 is strictly less than the 
maximum of v0 except if v0 is a constant. To prove the theorem it suffices 
therefore to establish the following proposition. If v0 is a continuous function 
such that v0 <? m and vo(z) < m for all x of an internal I of length 
A > 0, then there exists an r such that vr(z) < m for all x. 
Since rotations do not affect the maxima there is no loss of generality 
in assuming that 0 is a point of increase of F. If b is another point of 
increase then 0,b,2b,. . . , rb are points of increase of jpr*, and it is 
possible to choose b and r such that every interval of length A contains 
at least one ampng these points (see lemma 1 and the corollary in V,4a). 
By definition 
G.6) vr(x) 
To every point x it is possible to find a point y of increase of Fr* such 
that x — y is contained in /. Then vo(x—y) < m, and hence vr(x) < m. 
Since x is arbitrary this proves the assertion. > 
Note. The proof is easily adapted to show that if F is concentrated on 
the vertices of a regular polygon with one vertex at 0, then Fn* tends to an 
atomic distribution with atoms of equal weight. Convergence need not take 
place if 0 is not among the atoms. 
Example, (c) Let F be concentrated on the two irrational points a 
and a + \. Then Fn* is concentrated on the two points na and na + ?, 
and convergence is impossible. > 
18 For the analogue on the open line see problems 23 and 24. For generalizations to 
variable distributions see P. Levy, Bull. Soc. Math. France, vol. 67 A939) pp. 1-41; 
A. Dvoretzky and J. Wolfowitz, Duke Math. J., vol. 18 A951) pp. 501-507. 
VIII.8 REGULAR VARIATION 275 
8. REGULAR VARIATION 
The notion of regular variation (introduced by J. Karamata in 1930) 
proved fruitful in many connections, and finds an ever increasing number of 
applications in probability theory. The reason for this is partly explained 
in the next lemma, which is basic despite its simplicity. The examples of 
this section contain interesting probabilistic results, and problem 29-30 
contains a basic result concerning stable distributions which follow from the 
lemma in an elementary way. (See also problem 31.) 
We have frequently to deal with monotone functions U obtained from 
a probability distribution F by integrating yv F{dy) over 0, x or x, oo. 
[See, for example, D.5), D.15), D.16).] The usual changes of parameters 
lead from such a function U to the family of functions of the form at U(tx), 
and we have to investigate their asymptotic behavior as /-> oo. If a limit 
ip(x) exists, it suffices to consider norming factors of the form at = y>(l)/?/(/) 
provided. ip(l) > 0. The next lemma is therefore wider in scope than 
appears at first sight. It shows that the class of possible limits is surprisingly 
restricted. 
Lemma 1. Let U be a positive monotone function on 0, co such that 
(8.1) ^^ -*- f(x) <, oo t -* oo 
v 17@ 
at a dense set A of points. Then 
(8.2) fix) = x» 
where —oo <? p <? oo. 
The senseless symbol a;00 is introduced only to avoid exceptions. It is, of 
course, to be interpreted as oo for x > 1 and as 0 for x < 1. Similarly 
a?~°° is oo or 0 according as x < 1 or x > 1. (See problem 25.) 
Proof. The identity 
U(txxx2) = U(txxx2) U(tx2) 
( ' 1/@ U(tx2) 17@ 
shows that if in (8.1) a finite positive limit exists for x = xx and x = x2, 
then also for x = xxx2, and 
(8.4) v(xix2) = V>(xi) W(xd- 
Suppose first that ip(xx) = oo for some point xv Then by induction 
y>(x») = oo and y>(x~n) = 0 for all n. Since ip is monotone this implies 
that either ip(x) — a;00 or ip{x) = a;-00. It remains to prove the lemma for 
finite valued y>. (See problem 25.) Because of the assumed monotonicity 
we may define %p everywhere by right-continuity, in which case (8.4) holds at 
276 THE BASIC LIMIT THEOREMS YIII.8 
all points xlt x2. Now (8.4) differs only notationally from the equation which 
we have used repeatedly to characterize the exponential distribution. In fact, 
letting x = e* and yj(e*) = w(|) the relation (8.4) is transformed into 
w(?i+?2) = "(?i) "(?2)- We know from I; XVH,6 that all solutions that are 
bounded in finite intervals are of the form w(?) = ep*. This, Iiowever, is 
the same as y>(x) = xp. ^ 
A function U satisfying the conditions of lemma 1 with a. finite p will 
be said to vary regularly at infinity, and this definition will be extended to 
non-monotone functions. If we put 
(8.5) U{x) = x* L{x) 
the ratio U(tx)jU(t) will approach x* iff 
(8.6) SE^i. 
for every x > 0. Functions with this property are said to vary slowly, and 
thus the transformation (8.5) reduces regular variation to slow variation. 
It is convenient to use this fact for a formal definition of regular variation. 
Definition. A positive {not necessarily monotone) function L defined on 
0, 00 varies slowly at infinity iff (8.6) is true. 
A function U varies regularly with exponent p (— 00 < p < 00) iff it is 
of the form (8.5) with L slowly varying. 
This definition extends to regular variation at the origin'. U varies 
regularly at 0 iff Uixr1) varies regularly at 00. Thus no new theory is 
required for this notion. 
The property of regular variation depends only on the behavior at infinity 
and it is therefore not necessary that L(x) be positive, or even defined, for 
all x > 0. 
Example, (a) All powers of |loga;| vary slowly both at 0 and at 00. 
Similarly, a function approaching a. positive limit varies slowly. 
The function A + x2)p varies regularly at 00 with exponent 2p. 
e* does not vary regularly at infinity, but it satisfies the conditions of 
lemma 1 with p = 00. Finally, 2 + sin x does not satisfy (8.1). > 
For ease of reference we rephrase lemma 1 in the form of a 
Theorem. A monotone function U varies regularly at infinity iff (S.I) holds 
on a dense set and the limit v> is finite and positive in some interval.19 
19 The notion of regular variation may be generalized as follows: Instead of postulating 
the existence of a limit in (8.1) we require only that every sequence {/„} tending to infinity 
contains a subsequence such that U(tn]x)lU(tn)) tends to finite positive limit. We say 
then that U varies dotninatedly. See the end of the problem section 10. 
VIII.8 REGULAR VARIATION 277 
This theorem carries over to non-monotone functions except that it must 
be assumed that convergence takes place at all points. 
The following lemma should serve to develop a feeling for regular 
variation. It is an immediate consequence of the general form (9.9) of slowing 
varying functions. 
Lemma 2. If L varies slowly at infinity then 
(8.7) x~< < L{x) < xe 
for any fixed c > 0 and all x sufficiently large. 
The passage to the limit in (8.6) is uniform infinite intervals 0 < a < x < b. 
We conclude this survey by a frequently used criterion. 
Lemma 3. Suppose that 
> I, and an—>-oo. 
If U is a monotone function such that 
(8.8) lim Xn U(anx) = x{x) <, oo 
exists on a dense set, and % is finite and positive in some interval, then U 
varies regularly and x(x) = cxp where — oo < p < oo. 
Proof. We may assume that #(l) = I and that (8.8) is true for x = I 
(because this can be achieved by a trivial change of scale). For given t 
define n as the smallest integer such that an+1 > t. Then an <, t < an+1 
and for a non-decreasing U 
(8 9) U(?l»X) < < 
U(an+1) ~ U(t) ~, U(an) ' 
for a non-increasing U the reversed inequalities hold. Since An U(an) -*¦ I 
the extreme members tend to x(x) at eac^ point where (8.8) holds. The 
assertion is therefore contained in the last theorem. > 
To illustrate typical applications we derive first a limit theorem due to 
R. A. Fisher and B. V. Gnedenko, and next a new result. 
Example, (b) Distribution of maxima. Let the variables Xk be mutually 
independent and have a common distribution F. Put 
X; = max [Xl5 . . ., X J. 
We ask whether there exist scale factors an such that the variables X*jan 
have a limit distribution G. We exclude two cases on account of their 
278 THE BASIC LIMIT THEOREMS VIII.8 
triviality. If F has a largest point of increase f then the distribution of 
X* trivially tends to the distribution concentrated at ?. On the other hand, 
it is always possible to choose scale factors an increasing so rapidly that 
X*/an tends to 0 in probability. The remaining cases are covered by the 
following proposition. > 
Let F(x) < 1 for all x. In order that with appropriate scale factors an 
the distributions Gn of X*/an tend to a distribution G not concentrated at 0 
it is necessary and sufficient that 1 — F varies regularly with an exponent 
p < 0. In this case, 
(8.10) G(x) = 
for x>0 and G{x) = 0 for x < 0. (Clearly c > 0.) 
Proof. If a limit distribution G exists we have 
(8.11) F»(anx) -» G(x) 
at all points of continuity. Passing to logarithms and remembering that 
log(l— z)~ — z as z->0 we get 
(8.12) n[l-F(anx)}-+-logG(x). 
Since 0 < G(x) < 1 in some interval the last lemma guarantees the regular 
variation of I — F. Conversely, if 1 — F varies regularly it is possible 
to determine an such that n[\ —F(an)] -+¦ 1, and in this case the left side in 
(8.12) tends to x". (See problem 26.) > 
Example, (c) Convolutions. From the definition (8.6) it is obvious that the 
sum of two slowly varying functions is again slowly varying. We now prove 
the following > 
Proposition. If Ft and F2 are two distribution Junctions such that as 
X—*- 00 
(8.13) l- 
with L{ slowly Varying, then the convolution G = FX*F2 has a regularly 
varying tail such that 
(8.14) 1 - G(x) ~ x-p(L1(x)+L2(a;)). 
Proof. Let Xx and X2 be independent random variables with distributions 
Fx and F2. Put t' « A + d)t > t. The event Xx + X2 > t occurs 
whenever one of the variables is >t' and the other >—St. As t—*-oo the 
probability of the latter contingency tends to 1, and hence for any e > 0 
and t sufficiently large 
(8.15) 1 - G@ ? [A -F&')) + A -F2(O)]A - e). 
VIII.9 ASYMPTOTIC PROPERTIES OF REGULARLY VARYING FUNCTIONS 279 
On the other hand, if we put t" = (l—d)t with 0 < <5 < ? then the 
event X: + X2 > t cannot occur unless either one of the variables exceeds 
t", or else both are >dt. In view of (8.13) it is clear that the probability of 
the latter contingency is asymptotically negligible compared with the prob- 
ability that X; > t", and this implies that for t sufficiently large 
(8.16) 1 - G{t) < [{\-Fx{t")) + (l- 
Since <5 and e can be chosen arbitarily small the two inequalities (8.15) 
and (8.16) together entail the assertion (8.14). >. 
By induction on r one gets the interesting 
Corollary. If 1 - F(x) ~ x-"L{x) then 1 - Fr*(x) ~ rx-"L{x). 
When applicable, this theorem20 supplements the central limit theorem 
by providing information concerning the tails. (For applications to stable 
distributions see problems 29 and 30. For a related theorem concerning 
the compound Poisson distribution see problem 31.) 
*9. ASYMPTOTIC PROPERTIES OF REGULARLY 
V. ^YING FUNCTIONS 
The purpose of this section is to investigate the relations between the 
tails and the truncated moments of distributions with regularly varying 
tails. The main result is that if 1 — F(x) and F(—x) vary regularly so do 
all the truncated moments. This is asserted by theorem 2, which contains 
more than what we shall need for the theory of stable distributions. It 
could be proved directly, but it may also be considered a corollary to theorem 
1 which embodies Karamata's21 striking characterization of regular variation. 
It seems therefore best to give a complete exposition of the theory in par- 
ticular since the arguments can now be significantly simplified.22 
We introduce the formal abbreviations 
(9.1) Zv{x) = f V Z(y) dy, Zfc) = f V Z(y) dy. 
JO Jx 
* This section is used only for the theory of stable distributions, but the use of theorem 2 
would simplify many lengthy calculations in the literature. 
20 Special cases were noticed by S. Port. 
21 J. Karamata, Sur un mode de croissance reguliere, Mathematica (Cluj), vol. 4 A930) 
pp. 38-53. Despite frequent references to this paper, no newer exposition seems to exist. 
For recent generalizations and applications to Tauberian theorems see W. Feller, One-sided 
analogues of Karamattfs regular variation, in the Karamata memorial volume A968) of 
L* Enseignement Mathematique. 
22 Although new, our proof of theorem 1 uses Karamata's ideas. 
280 THE BASIC LIMIT THEOREMS VIII.9 
It will now be shown that in the case of a regularly varying Z these functions 
are asymptotically related to Z just as in the simple case Z{x) = xa. 
The asymptotic behavior of Zp at infinity is not affected by the behavior 
of Z near the origin. Without loss of generality we may therefore assume 
that Z vanishes identically in some neighborhood of 0 and so the integral 
defining Zp will be meaningful for all p. 
Lemma. Let Z > 0 vary slowly. The integrals in (9.1) converge at oo 
for p < — 1, diverge for p > — 1. 
If p ^> — 1 then Zp varies regularly with exponent p + 1. If p < —\ 
then Z* varies regularly with exponent p + 1, and this remains true for 
p -f l =0 if Z*x exists. 
Proof. For given positive x and c choose rj such that for y^rj 
(9.2) A -€)Z{y) < Z{xy) < A + c)Z{y). 
Assume that the integrals in (9.1) converge. From 
/•oo 
(9.3) Z*p(tx) = x^1\ y*Z(xy)dy 
it follows for t > rj that 
(l-€)x^Z*v(t) ? Z*p(tx) < A+c)*^1 Z*p(t). 
Since € is arbitrary we conclude that as t —>- oo 
,9.4) ^ 
z*v(t) 
This proves the regular variation of Z*. Furthermore, -since Z* is a 
decreasing function it follows that p -f 1 < 0. Thus the integrals in (9.1) 
cannot converge unless p < — 1. 
Assume then that these integrals diverge. Then for t > rj 
ZP(tx) = Zp{tjx) + x^1 f V Z(xy) dy 
Jn 
and hence 
(l-6)^+iZv{t) < Zp(tx) - Zp(rjx) < (l+€)x^Zp(t). 
On dividing by Zp(t) and letting t —> oo \ye conclude as above that 
y p g 
Zp(tx)/Zp(t) tends to a:»+1. In case of divergence therefore Zp varies 
regularly, and divergence is possible only when p ^> — 1. > 
Ths next theorem shows that regular variation of Z ensures that of Zv 
and Z*; the converse is also true except if Zv or Z* vary slowly. Further- 
more we get a useful criterion for the regular variation of these functions. 
VIII.9 ASYMPTOTIC PROPERTIES OF REGULARLY VARYING FUNCTIONS 281 
Parts (a) and (b) of the theorem treat the functions Z* and Zp, respectively. 
They are parallel in all respects, but only part (a) is used extensively in prob- 
ability theory. 
Theorem 1. (a) If Z varies regularly with exponent y and Z* exists, then 
where X = —(p+y+1) ^ 0. 
Conversely, if (9.5) holds with X > 0, then Z and Z* vary regularly with 
exponents y = —X — p — 1 and —X, respectively. If (9.5) holds with 
X — 0 then Z* varies slowly {but nothing can be said about Z). 
(b) If Z varies regularly with exponent y and if p >. — y — 1 then 
(9.6) 
with X = p + y + 1. 
Conversely, if (9.6) holds with X > 0 then Z and Zp vary regularly with 
exponents X — p — 1 and X, respectively. If (9.6) holds with X = 0 then 
Zv varies slowly. 
Proof. The proofs are identical for both parts, and we conduct it for 
part (a). Put 
l y 
The numerator on the left is the negative derivative of the denominator, and 
hence we get for x > 1 
% y 
Suppose now that Z varies regularly with exponent y. By the preceding 
lemma Z* varies regularly with exponent X = y + p + I and so the two 
sides in (9.7) vary regularly with exponent —1. Thus rj is a slowly varying 
function. As / -*¦ co the last integrand in (9.8) therefore tends to s~l. 
Unfortunately we do not know that rj is bounded, and so we can only 
assert that the lower limit of the integral is ^>log x by virtue of Fatou's 
theorem [see IV,B.9)]. But because of thue regular variation of Z* the left 
side tends to X log x, and so 
lim sup //(/) < X. 
But this implies the boundedness of ^ and hence we may choose a sequence 
t _> oo such that rj(tn) —>- c < oo. Because of the slow variation this 
282 THE BASIC LIMIT THEOREMS VIII.9 
implies that r}{tns)-+c for all s, and the convergence is bounded. Thus 
the right side in (9.8) approaches c log x, and hence c = A. It follows that 
the limit c is independent of the sequence {/„}, and so tj(t)->X. This 
proves that (9.5) is true. 
The converse is easier. Suppose rj(t) —»- X > 0. The two sides in (9.8) 
then approach X log x, and hence the ratio Z*(t)jZ*(tx) approaches x}- 
as asserted. If X > 0 this together with (9.5) proves that Z varies regularly 
with exponent —?. — p — 1. ^ 
Although we shall not use it we mention the following interesting 
Corollary. A function Z varies slowly iff it is of the form 
(9.9) Z(x) = a(x) exp (J" ^ dy) 
yjhere e(x) -*¦ 0 and a(x) ->c< oo ay x->oo. 
Proof. It is easily verified that the right side represents a slowly varying 
function. Conversely, assume that Z varies slowly. Using (9.6) with 
p = y =. 0 we get 
Z(t) 1 + 
with e(f) -> 0. On the left the numerator is the derivative of the denominator, 
and by integration we get 
Z0(x) = Z0(l) • x exp(J" ^ dt} 
which is equivalent to (9.9) because Z(x) ~ Z0(x)x~x by (9.6). > 
We proceed to apply theorem 1 to the truncated moment functions of a 
probability distribution F. We can consider each tail separately or else 
combine them by considering F(x) — F(—x) instead of F. It suffices 
therefore to study distributions F concentrated on 0, oo. For such a 
distribution we define the truncated moment functions C/f and Vn by 
(9.10) 
) = f V F{dy}, Vfa) = ^y" F{dy}. 
Jo Jx 
It will be understood th?* the second integral converges while the first integral 
tends to oo as x-*- oo. This requires that I > 0 and — oo < r\ < I. 
In particular, Vo = 1 — F is the tail of the distribution F. 
We prove a generalization of part (a) of theorem 1; part (b) generalizes 
in like manner. 
VIII.9 ASYMPTOTIC PROPERTIES OF REGULARLY VARYING FUNCTIONS 283 
Theorem 2.23. Suppose that ?/?(oo) = oo. 
(i) If either C/f or Vn varies regularly then there exists a limit 
fV(O 
(9.11) lim n— =c, 0 <, c < oo. 
We write this limit uniquely in the form 
(9.12) c = ^ , r, ? a <C ? 
a — 7] 
with a = r] if c = oo. 
(ii) Conversely, if (9.11) w frwe wi7A 0 < c < oo f/ze/j automatically 
a ]> 0 crn</ rAere exwte a slowly varying function L such that 
(9.13) Uc(a;) ^ n) 
wAere the sign ^ indicates that the ratio of the two sides tends to 1. 
(iii) The statement remains true when c = 0 or c — oo, provided the sign 
—' is interpreted in the obvious manner. 
For example, if (9.11) holds with c = 0 then a = ? and U^ varies 
slowly, but about Vn we know only that Vn{x) = o{x^~^L{x)). In this case 
Vn need not vary regularly (see problem 31). However, slow variation is the 
only case in which regular variation of one of the functions C/? or Vn does 
not imply regular variation of the others. 
Proof, (i) We write Vn in the form 
(9.14) Vn{x) = fV* Ujdy).. 
Jx 
Integrating by parts between x and t > x we get 
vn(x) - vn(t) = - 
The last two terms on the right are positive and therefore the integral must 
converge as / —>- oo. Because of the monotonicity of C/? this implies that 
y*~c C/4@ -*¦ 0 and hence 
(9.15) Vn{x) = -a^ ?/;(*) + (C-i?) f °°y""<~1 l/«( 
Jx 
or 
(9.16) 
a^4t7c(a;) J* 
23 For a generalization see problems 34 and 35. 
284 THE BASIC LIMIT THEOREMS VIII. 10 
Assume now that UQ varies regularly. Since C/f(oo) = oo the exponent 
is necessarily <? and we denote it by ? — a. (Since the integral in (9.16) 
converges we have necessarily a ^ tj.) The relation (9.5) with Z = U^ 
and p = r\ — ? — 1 asserts that the right side in (9.16) tends to 
if X ^ 0 and to uo if A = 0. We have thus shown that if t/? varies 
regularly with exponent ? — a, then (9.11) holds with c given by (9.12) 
and >0. 
Assume then that Vn varies regularly. Its exponent is <r) and we 
denote it by r\ — a. We use the same argument except that (9.15) is replaced 
by the analogous relation 
(9.17) 
Ujx) = -*'-" Vn(x) + a-r,) fV-" Vn{y) dy. 
Jo 
An application of (9.6) with Z = Vn and p = ? — r\ — I now shows that 
(9.11) holds with c given by (9.12) where a ;> 0. 
(ii) To prove the converse, assume (9.11) and write c in the form (9.12). 
Suppose first that 0 < c < oo. From (9.16) we see then that 
(9.18) ^*»7 
f00 
From theorem 1 (a) it follows directly that t/? varies regularly with exponent 
? — a > 0, and (9.11) then implies that Vn varies regularly with exponent 
r\ —-a. It follows that U^ and Vn can be written in the form (9.13) where 
a ^ 0. 
If c = 0 the same argument shows that t/? varies slowiy, but (9.11) does 
not permit the conclusion that Vn varies regularly. 
Finally, if (9.11) holds with c = oo we conclude from (9.18) that 
(9.19) ^0 
0, 
V^-"-1 V,{y) dy 
Jo 
and by theorem 1 (b) this implies that Vn varies slow.ly. > 
10. PROBLEMS FOR SOLUTION 
1. Alternative definition of convergence. Let Fn and F be probability dis- 
tributions. Show that Fn-+F (properly) iff for given e > 0, h > 0 and / there 
exists an N(e, h,t) such that for n > N(e, h, t) 
A0.1) F(t -h)- e < Fn(t) < F(t + h) + e. 
VIII. 10 PROBLEMS FOR SOLUTION 285 
2. Improper convergence. If F is a defective distribution then A0.1) implies 
that Fn -* F improperly. The converse is not true. Show that proper convergence 
may be defined by requiring that A0.1) holds for n ^ N(c,h), independently 
of t. 
3. Let {F«} converge properly to a limit that is not concentrated at one point. 
The sequence {Fn(anz +bn)} converges to the distribution concentrated at the 
origin iff an -» oo, bn = o(an). 
4. Let X2, X2,. .. be independent random variables with a common distribution 
F and Sn = X2 + • • • + Xn. Let the variables a~xSn — bn have a proper limit 
distribution U not concentrated at one point. Ifan>0 then 
an -> oo, fln/fln-i -* 1- 
[Hint: . Using theorem 2 of section 3 show that a2jan approaches a finite limit. 
It suffices to consider symmetric distributions.] (The limit distribution is stable, 
see yi.l.) 
5. Let {un} be a sequence of bounded monotone functions converging pointwise 
to a bounded continuous limit (which is automatically monotone). Prove that the 
convergence is uniform. [Hint: Partition the axis into subintervals within each 
of which it varies by less than <¦.] 
6. Let Fn be concentrated at rr1 and u(x) = sin (a;2). Then Fnic u -*• u 
pointwise, but not uniformly. 
7. (a) If the joint distribution of (Xn, Yn) converges to that of (X, Y), then 
the distribution of X* + Yn tends to that of X + Y. 
(b) Show that theorem 2 gf section 3 is a special case. 
(c) The conclusion does not hold in general if it is only known that the marginal 
distributions for X* and Yn converge. 
8. Let Fn-*F with F defective. If aeC0(-oo, oo) then Fn*«->F*« 
uniformly in every finite interval. (This generalizes theorem 1 of section 3.) 
9. If Fn -*¦ F improperly it is not necessarily true that FnJc Fn -*¦ F-Ar F. 
Example. Let Fn have atoms of weight | at the points — n, 0, and n. 
10. In the plane every continuous function vanishing at infinity can be approxi- 
mated uniformly by finite linear combinations 2 ck<pk(x)ysk(y) with infinitely 
differentiable <pk and y>k. 
[Hint: Use example 3(a) choosing Gk(x, y) = 9lfc(^)9lfc(y) where 91 is the 
normal density.] 
Metrics. A function p is called a distance function for probability distributions 
if p{F, G) is defined for every pair F, G of probability distributions and has the 
following three properties: P(F, G) ^ 0 and p(F, G) = 0 iff F = G; next 
p(F,G) = p(G,F); and finally, p satisfies the triangle inequality 
p(Fi, F2) < 9{FX, G) + P(F2, G). 
11. P. Levy metric. For two proper distributions F and G define p(F, G) as 
the infimum of all h > 0 such that 
A0.2) F{x-h) -h <, G(x) <F(x +h) +h 
for all x. Verify that p is a distance function. Show that Fn -* F properly iff 
P(Fn, F) - 0. 
286 THE BASIC LIMIT THEOREMS VIII. 10 
12. Distance "in variation" Put P(F, G) = sup \\%u - (gull where «6C0 
and ||ii|| = 1. Show that P is a distance function.24 If F and G are atomic 
and attribute weights pk and qk to the point ak, then 
A0.3) ,,(F, G) = 
If F and G have densities / and g 
A0.4) />(F, G) = f °° \f(x) -g(x)\ dx. 
J—ao 
[Hint: It suffices to prove A0.4) for continuous / and g. The general case follows 
by approximation.] 
13. Continuation. Show that P(Fn, G)-+0 implies proper convergence 
Fn -*¦ G. To see that the converse is false consider the normal distribution 
functions 3l(nx) and the distribution Fn concentrated at n~x. 
14. Continuation. If U = Fxi< • • *Fn and V = Gj * • • • * Gn show that 
A0.5) P(U, V) ? 2 P(Fk, Gk). 
This extends the basic inequality C.9). [Hint: Use C.9) and a test function u 
such that ||2Ui-Sii|| is close to P(U, V).] 
15. Approximation by the Poisson distribution.25 Let F attribute weight p to 
the point 1 and q = 1 — p to the point 0. If G is the Poisson distribution with 
expectation, p show that p(F, G) ^ f/^, where P is the distance defined in A0.3). 
Conclude: If F is the distribution of the number of successes in n Bernoulli 
trials with probabilities plt... ,/?n and if G is the Poisson distribution with 
expectation px + • • • + pn then P(F, G) <, $(pf+- • • +p2). 
16. The law of large numbers of VII,7 states that if the Xk are independent 
and identically distributed, and if E(Xk) * 0, then (X2 H t-XJ/n —^»- 0. Prove 
this by the method used for theorem 1 in section 4. 
17. The Lindeberg condition D.15) is satisfied if <tk = E(|Xf+*|) exists for 
some d > 0 and 04 + • • • + a,, «= o(s^+d) (Lidpunov's condition). 
18. Let Fk be symmetric and 1 — Fk(x) = fa-*-1'* for x > 1. Show that 
the Lindeberg condition D.15) is not satisfied. 
19. Let Xjt - ±1 with probability |A -kr2) and X* - ±k with probability 
\k~2. By simple truncation prove that SJ'^n behaves asymptotically in the same 
way as if Xk = ±1 with probability ?. Thus the distribution of Sn/v^» tends 
to 31 but Var(Sn/V«) -> 2. 
20. Construct variants of the preceding problem where E(X|) = °o and yet 
the distribution of SjVn tends to 31. 
24 The definition can be extended to differences of arbitrary finite measures and defines 
the "norm topology" for mea ares. Problem 13 shows that the resulting notion of con- 
vergence is not natural for probability theory. 
2r Suggested by inequalities in L. LeCam, An approximation theorem for the Poisson 
binomial distribution. Pacific J. Math., vol. 10 A960) pp. 1181-1197. 
VIII. 10 PROBLEMS FOR SOLUTION 287 
21.26 Central limit theorem for exchangeable variables. For fixed 6 let Fe be a 
distribution with- zero expectation and variance o2@). A value 0 is chosen 
according to the probability distribution G and one considers mutually in- 
dependent variables X^ with the common distribution Fe. If a2 is the expectation 
of a2 with respect to G show that the distribution of SJ(aVn) tends to the 
distribution 
9l(ax((o(d))G{dd}. 
? 
It is not normal unless G is concentrated at one point. 
22. Shot noise in vacuum tubes, etc. Consider the stochastic process of example 
VI,3(/r) with discretized time parameter. Assuming that at epoch kh an arrival 
occurs with probability 0J1 show that the intensity of the current in the discrete 
model is given by an infinite convolution. The passage to the limit h -*¦ 0 leads to 
Camphell's theorem VI,C.4). 
Do the same for the busy-trunkline example VI,3(/). Generalize the model to 
the situation where the after-effect at epoch kh is a random variable assuming 
the values 1,2,... with probabilities px,p2,.... 
23. The sequence {Fn*} is never stochastically bounded. [Hint: It suffices to 
consider symmetric distributions. Also, one may suppose that F has infinite 
tails, for otherwise Fn* ->0 by the central limit theorem. Use V, E.10).] 
Note. It will be shown in example XV,3(a) that Fn* -» 0. 
24. Continuation. It is nevertheless possible that for every x 
lim sup Fn*(x) = 1, lim inf Fn*(x) = 0. 
In fact, it is possible to choose two extremely rapidly increasing sequences of 
integers ak and nk such that 
2kak * 
[Hint: Consider the distribution P{X = ( —1)*^*} =pk. With an appropriate 
choice of the constants there is an overwhelming probability that about 2k among 
the terms Xlf. . . , X^ will equal ( — 1)*^ and none will exceed ak in absolute 
value. Then for k even Snjc > ak — nkak_1. Show that 
nk = Bk) I, pk — _ , ak — (nk)k 
will do.] 
25. In the'proof of lemma 1 of section 8 it suffices to assume that the set A is 
dense in some open interval. 
26. Distribution of maxima. Let Xx,... , X« be independent with the common 
distribution F and X* = max (X2,... , XJ. Let Gn be the distribution of 
26 J. R. Blum, H. Chernoff, M. Rosenblatt, and H. Teicher, Central limit theorems for 
interchangeable processes, Canadian J. Math., vol. 10 A958) pp. 222-229. 
288 THE BASIC LIMIT THEOREMS VIII. 10 
(a) If F(x) = 1 — e~x and an = n then Gn tends to the distribution concen- 
trated at the point 0. Show directly that no choice of an leads to more discrim- 
inating results. j 
F) If F is the Cauchy distribution with density — % and an = njn, 
then Gn(x) '-+ e-*'1 for x > 0. <l + x * 
27. If X and Y have a common distribution F such that 1 — F(x) ~ x~pL{x) 
with L slowly varying, then P{X > 11 X + Y > /} -+\ as t -* oo. Roughly 
speaking, a large value for the sum is likely to be due to the contribution of one of 
the two variables.27 
28. Let v > 0 and a > 0 on 0, oo and suppose that 
lim [a(t)v(tx) + b(t)x] = z(x) 
t-+ao 
exists and depends continuously on x. For fixed x0 > 0 prove that 
xox x 
varies regularly. Conclude that either z(x) = ex* or z(x) = ex -f cxx log x, 
provided only that v itself does not vary regularly [in which case z(x) = cx« + cxa:]. 
29. Let G be a symmetric stable distribution, that is, Gr*(crx) = G(x) (see 
VI, 1). From the last corollary in section 8 conclude that 1 — G{x) — x-*L(x) 
with a <2 unless r [1 —G(<vc)] t-»-0 in which case G is the normal distribution. 
[Hint: The sequence r[l—C?(cr^)] remains bounded by the symmetrization 
inequality V,E.13). The remainder is easy.] 
30. Generalize to unsymmetric stable distributions. 
31. Let {Xn} be a sequence of mutually independent positive random variables 
with a common distribution F concentrated on 0, oo.^Let N be a Poisson 
variable. The random sum SN = Xx + • • ¦ + XN has the compound Poisson 
distribution 
U = e~ey-Fn*. 
** n\ 
Let L vary slowly at infinity. Prove that 
// 1 - F(x) — x-e L(x) then 1 - U(x) ~ car" L(x). 
[Hint: Obviously P{SN > x} exceeds the probability that exactly one among 
the components X5 > x, that is 
1 - U(x) ^ C[l - F(t)]e~cli-F(t)x 
On the other hand, for sufficiently large x the event SN > x cannot occur unless 
either one among the components X^. > A — e)x, or at least two exceed xi, 
or finally N > x*; The probability of the second contingency is o(l — F(x)), 
while the probability of N > log x tends to 0 more rapidly than any power of x.] 
32. Let F be atomic with weight proportional to rrx2~in at the point 2". 
Show that Uz, as defined in (9.10), is slowly varying and t/2(oo) = oo, but that 
1 — F does not vary regularly. 
[Hint: For the last statement it suffices to consider the magnitude of the jumps.] 
27 Tlie phenomenon as such seems to have been noticed first by B. Mandelbrot. 
VIII.10 PROBLEMS FOR SOLUTION 289 
Note. The remaining problems refer Co a generalization of the notion of regular 
variation.28 A convenient starting point is provided by the following 
Definition. A monotone function u varies dominatedly at infinity if'the ratios 
uBx)/u(x) remain bounded away from 0 and oo. 
33. Show that a non-decreasing function u varies dominatedly iff there exist 
constants A,p, and t0 such that 
A0.5) ~j < Ax*, t >t0, x > 1. 
For non-increasing u the same criterion applies with -c > 1 replaced by x < 1. 
34. (Generalization of theorem 2 in section 9.) Define U^ and Vn as in (9.10) 
(which requires that — <x> < v < 0- Put R(t) = t^~nVn(t)jU^(t). 
Show that U^ varies dominatedly iff lim sup/?(/) < oo. Similarly Vn varies 
dominatedly iff lim inf R(t) > 0. 
35. (Continuation.) More precisely: If R(t) <, M for t > t0, then 
A0.6) ??x)<(M + l)xv, x>l, t>tQ 
with p = (? — rj)Mj{M + 1). Conversely, A0.6) with p < ? — n implies 
C - v - p 
These statements remain true if R is replaced by its reciprocal R~l and at 
the same time the ratio U^(tx)/U^(t) is replaced by Vn{t)j Vn(tx). 
36. Prove the following criterion: If there exists a number s > 1 such that 
lim inf U^{st)jU^{t) > 1 then Vn varies dominatedly. Similarly, if lim inf Vn{tls)j 
Vn{t) > lthen ?/? varies dominatedly. 
28 For further results and details see W. Feller, One-sided analogues of Karamatd's 
regular variation, in the Karamata Memorial volume of PEnseignement Mathematique, 
vol. 15 A969), pp. 107-121. See also W. Feller, On regular variation and local limit theorems, 
Proc. Fifth Berkeley Symposium Math. Statistics and Probability, vol. 2, part 1, pp. 
373-388 A965-66). 
CHAPTER IX 
Infinitely Divisible 
Distributions and Semi-Groups 
The purpose of this chapter is to show that the basic theorems concerning 
infinitely divisible distributions, processes with independent increments, and 
stable distributions and their domains of attraction can be derived by a 
natural extension of the argument used to prove the central limit theorem. 
The theory will be developed anew and amplified by methods of Fourier analysis, 
and for this reason the present outline is limited to the basic facts. The 
interest in the chapter is largely methodological, to tie the present topics to 
the general theory of Markov processes; when applicable, the methods of 
Fourier analysis lead to sharper results. To provide easy access to important 
facts some theorems are proved twice. Thus the general structure theorem is 
first proved for semi-groups of distributions with variances. In this way 
sections 1-4 present a self-contained exposition of basic facts. 
The semi-group operators in this chapter are convolutions. Other semi- 
groups will be considered independently in the next chapter by new methods. 
1. ORIENTATION 
The limit theorems of this chapter are a natural extension of the central 
limit theorem,, and ihe infinitely divisible distributions are closely related 
to the normal distribution. To see this it is worthwhile to repeat the proof 
of theorem 1 in VIII,4 in a slightly different setting. 
We consider this time an arbitrary triangular array {Xk „} where for 
each n the n variables1 X1#n, . . . , Xn#n are independent and have a 
1 Triangular arrays were defined in VI,3. It should be borne in mind that we are really 
dealing with distribution functions Fk*n; the random variables Xfr#B serve merely to 
simplify notations. Accordingly, the variables of different rows need not be related in 
any way (and need not be defined on the same probability space). 
290 
IX. 1 ORIENTATION 291 
common distribution Fn. For the row sums we write Sn = Xln + • • • + Xn n- 
In chapter VIII we dealt with the special case where Xkn = X^a and 
Fn(x) — F(anx)- There the row sums were denoted by S*. 
Throughout this chapter we use the operational notation* of VIII,3. Thus 
gn is the operator associated with Fn and g? is associated with the 
distribution of Srt. /Finally, \\u\\ denotes the upper bound of the continuous 
function |«|. 
Example, (a) Central limit theorem. Suppose that there exist numbers 
en -*• 0 such that 
A.1) IXx^Ke., E(X1.n) = 0, n 
For a function u with three bounded derivatives we have the identity 
A.2) »[».«(,)-«(«)] = I"" "(»-ir)-«(«) +»»»'(») . ny* p{dy). 
J-<» y 
The finite measure nyz Fn{dy) converges by assumption to the probability 
distribution concentrated at the origin. The fraction under the integral is a 
continuous function of y and differs from \u"(x) by less than en \\u'"\\. 
Thus 
A.3) «[&*«-«] ~>K 
uniformly in x. 
Suppose now that {©„} is a second sequence of operators such that 
n[(Snu—u] tends uniformly to \u'. Then 
A.4) n(%nu-®nii)'+0 
uniformly. By the basic inequality VIII,C.10) (which will be used constantly 
in the sequel) 
A.5) HSS «-©>!! <«!&,«-©»«!!, 
and the right side tends to zero in consequence of A.4). As we have seen 
in the proof of theorem 1 in VIII,4, we may choose for En the operator 
associated with the symmetric normal distribution with variance \jn. Then 
©? = ©x and hence 5?->-©1. We have thus proved that the distribution 
of Sn tends to the normal distribution 91. > 
In scrutinizing the structure of this proof it is seen that the form of the 
right side in A.3) played no role. Suppose we had an array such that 
(uniformly) 
A.6) /i[gnw-«]- 
where 31 is an arbitrary, but fixed, operator. Our argument permits us 
292 ' INFINITELY DIVISIBLE DISTRIBUTIONS AND SEMI-GROUPS IX. 1 
to compare any two arrays satisfying A.6) and to conclude that their row 
sums behave asymptotically in the same way. If for one such array the 
distributions of Sn tend to a limit G then the same will be true for all our 
arrays. We shall prove that this is always the case. 
Example, (b) /• >isson distribution. Suppose Xj „ equals 1 with prob- 
ability pn, and 0 with probability 1 — pn. If npn-+ a 
A.7) n[%nu(z)-u(x)] = npn[u(x-])-u(x)]-**[u(x-l)-u{x)]. 
This time we take for En the operator associated with the Poisson distribution 
with expectation a/w. An easy calculation shows that also n[&nu—u] tends 
to the right side in A.7) and we conclude as before that gf?w "*" ®i- Thus the 
distribution of Sn tends to the Poisson distribution with expectation a. 
[The right side in A.7) illustrates one possible form for the operator % in 
A.6). For another example of a simple triangular array see problem 2.] > 
In our two examples we were fortunate in knowing the limit distribution 
in advance. In general the triangular array as such will serve to define the 
limit and in this way we shall derive new distribution functions. This 
procedure was used in 1; VI to define the Poisson distribution as a limit of 
binomial distributions. 
We recall from VI,3 that the limit distributions of the sums Sn are called 
infinitely divisible. We shall show that such a limit distribution exists when- 
ever a relation of the form A.6) holds, and that this condition is also necessary. 
Another approach to the problem depends on the study of the measures 
nyz Fn{dy}. In both examples a limit measure existed; in example (a) it was 
concentrated at the origin, in (b) at the point 1. In general, the relation A.6) 
is intimately connected with the existence of a measure O such that 
nyz Fn{dy] —> ?l{dy), and infinitely divisible distributions will be characterized 
either by the operator 91 or the measure O (which may be unbounded). 
A third approach to the problem starts from the solution of the con- 
volution equation 
A.8) Q.*Qt = Q»t 
in which Qt is^a probability distribution depending on the parameter / > 0. 
Example, (c) The normal and the Poisson distributions satisfy A.8) with 
t proportional to the variance. The gamma distributions of 11,B.2) have 
the convolution property 11,B.3), which is a special case of A.8). The same 
is true of the analogous convolution properties derived for the Cauchy 
distribution 11,D.5) and the one-sided stable distribution of 11,D.7). 
For a triangular array with Fn = Ql/n the relation A.6)'states that as t 
runs through \, \, . . . . One should expect that A.6) will hold for an 
arbitrary approach / -> 0+. 
IX.2 CONVOLUTION SEMI-GROUPS 293 
Now A.8) is the basic equation for processes with stationary independent 
increments (VI,4) and is closely connected with semi-group theory. In this 
context % appears as a "generator." Ft turns out that this theory provides the 
easiest access to limit theorems and to infinitely divisible distributions, and 
hence we begin with it. 
2. CONVOLUTION SEMI-GROUPS 
For / > O'let Qt be a probability distribution in ft1 satisfying A.8) and 
?}(/) the associated operator, that is, 
B.1) W)u{z)=\ u{x-y)Qi{dy). 
J —00 
Then A.8) is equivalent to 
B.2) 
A family of operators satisfying B.2) is called a semi-group. [It fails to be a 
group because in genera] ?}(/) ha% no inverse.] The operators of a semi- 
group may be of an arbitrary nature and it is convenient to have a word to 
indicate our requirement that ?}(f) be associated with a probability 
distribution. 
Definition 1. A convolution semi-group {Q@} (where t > 0) is a family 
of operators associated with probability distributions and satisfying B.2). 
We take Co[—oo, oo] as domain of definition. The operators Q(t) are 
transition operators, that is, 0 <C u <, 1 implies 0 < Q(t)u < 1 and we 
have Q(/)l =1. 
We shall have to deal with operators [such as d}\dx% in A.3)] which are 
not defined for all continuous functions. For our present purposes it is 
fortunately possible to avoid tedious discussions of the precise domain of 
definition of such operators since we need consider only the class of functions 
u such that u e C[—oo, oo] and u has derivatives of all orders belonging to 
C[—oo, oo]. Such functions are called infinitely differentiable,2 and their 
class is denoted by C°°. For the present we consider only operators 31 
defined for all u e C00 and such that 3l« 6 C00, and so all occurring operators 
may be taken as operators from C°° to C00. For operators associated with 
probability distributions we saw in VIII,3 that gn -»¦ gf iff %nu-+ %u for 
u 6 C00. We now extend this definition of convergence consistently to arbitrary 
operators. 
2 The class C00 is introduced only to avoid a new term. It could be replaced by the 
class of functions with (say) four bounded derivatives, or (simpler still) by the class of 
all linear combinations of normal distribution functions with arbitrary expectations and 
variances. 
294 INFINITELY DIVISIBLE DISTRIBUTIONS AND SEMI-GROUPS . IX.2 
Definition 2. Let 9In and 91 be operators frem C* to C*. We say tliat 
9ln converges to 91, in symbols %n —> 91, // 
B.3) \Wnu-Ku\\-+0 
for each u e C°° 
Now B.3). states that 9ln« -> 9lw uniformly. Conversely, if for each 
weC00 the sequence {9lnu} converges uniformly to a limit v e C00 an 
operator 91 is defined by 9l« = v, and clearly 9ln —> 91. 
Definition 3. The convolution semi-group {?!(/)} is continuous if 
B.4) O(A)—1. 
w/jere 1 is the identity operator. In this case we put ?}@) = 1. . 
Since ||Q(O«II < ll«ll we get from the definition B.2) for h>0 
B.5) 
For A sufficiently small the left side will be < € independently of /, and in 
this sense a continuous convolution semi-group is uniformly continuous. 
Definition 4. An operator 91 from C* to C00 is said to generate the 
convolution semi-greupu{?l(t)} if as h —*¦ 0+ 
B.6) 
We say, equivalently, that 91 is the generator.3 
More explicitly, whenever the limit exists the operator 91 is defined by 
B.7) rir[u(x-y)-u(x))Qt{dy}-^^u(z). 
J — 00 
Obviously a semi-group with a generator is automatically continuous. 
It will be shown that all continuous convolution semi-groups possess gener- 
ators, but this is by no means obvious. 
Formally Bi6) defines 91 as derivative of &(t) at / = 0. Its existence 
implies differentiability at t > 0 since 
as //->0+ and similarly for h—>0—. 
The following examples will be used in the sequel. 
3 Since we restrict the domain of definition of 21 to C00 our terminology departs 
slightly from canonical usage as developed in E. Hille and R. S. Phillips A957). 
IX.2 CONVOLUTION SEMI-GROUPS 295 
Examples, (a) Compound Poisson semi-groups. Let 
B.9) Qt = e-'tJ^F** 
be a compound Poisson distribution. Here 
B.10) ?l(h)u — u = (e~"h— l)u + a/ie"" 5«4-— 52« + ' * ' • 
L 2! J 
Dividing by h wesee that B.6) holds with 91 = a(g—1). Thus the compound 
Poisson semi-group B.9) is generated by a(g—1) and we shall indicate its 
elements by the abbreviation Q(t) = eai%~1H. 
1 (b) Translations. Denote by Ta the distribution concentrated at a and 
by X(a) the associated operator. For fixed /? > 0 the semi-group property 
Tps * ?>« = 7>(t+i) holds and ?(/?/) u(x) = u(x-pt). The graph of 
%(fit)u is obtained by a translation from that of u and we speak of a 
translation semi-group. The generator is given by —ft — . Note that this 
dx 
generator is the limit as h->0 of the generator a(g — 1) when a = /?//i 
and F is concentrated at h. Now a($f—1) is a difference operator, and the 
passage to the limit was studied in VII,5. It is suggestive to indicate this 
semi-group by %(t) = exp (~0f — I. ¦ 
(c) Addition of generators. Let 9lx and 9l2 generate the convolution 
semi-groups {?}x@} and {?^@)- Then 9lx + 9l2 generates the convolution 
semi-group of operators ?i(t) = ?*x@ C^C*)• [Such Q(t) is associated with 
the convolution of the distributions associated with Qx@ and ^(O\ see 
theorem 2 of VItI,3.] The assertion is obvious from the simple rearrangement 
B.11) 
Translated semi-groups. As a special case we get the rule: if 91 
generates the semi-group of operators Q(t) associated with the distributions 
Qt, then 91 — ft d/dx generates a semi-group {Q#(t)} such that 
¦Q*(x)-.Qt(z-0t). 
(e) Normal semi-groups. Let Qt stand for the normal distribution with 
zero expectation and variance ct. As already mentioned, these Qt deter- 
mine a semi-group, and we seek its generator as defined by B.7). By Taylor's 
formula 
B.12) u(x - y) - u(x) = -y u\x) + \y* u\x) - \tf u'"(x - By). 
296 INFINITELY DIVISIBLE DISTRIBUTIONS AND SEMI-GROUPS IX.3 
The third absolute moment of Qt is proportional to t%, and we see thus 
that for functions with three bounded derivatives the limit in B.7) exists and 
equals \cu"{x). We express" this by saying that W = Icd2[dx2. > 
(For further examples see problems 3-5.) 
Note on the Fokker-Planck equation. Consider the family of functions defined by 
v(t, x) = ?}(/)/(*). The relation B.8) states that for smooth / 
dv 
C13) Jt = fe. 
This is the Fokker-Planck equation of the process, and v is its unique solution satisfying 
the initial condition v@,x) = f(x). Equation B.13) describes the process, and unnecessary 
complications are introduced by the traditional attempts to replace B.13) by an equation 
for the transition probabilities Qt themselves. Consider, for example, a translated com- 
d 
pound Poisson semi-group generated by % = aE—1) — /*—. The Fokker-Planck 
ax 
equation B.13) holds whenever the initial function fix) = v@,x) has a continuous 
derivative. Its formal analogue for the transition probabilities is given by 
B.14) —- = —/? — — <x.Qt + <x.F-k Qt 
dt ox 
This equation makes sense only if Q has a density and is therefore not applicable to dis- 
crete processes. The usual reliance on B.14) instead of B.13) only causes complications. 
3. PREPARATORY LEMMAS 
In this section we collect a few simple lemmas on which the whole theory 
depends. Despite its simplicity the following inequality is basic. 
Lemma 1. If % and 21# generate the convolution semi-groups {?}(*)} 
\, respectively, then for all t > 0 
C-1) 
Proof. From the semi-group property and the basic inequality A.5) 
we get for r =^ 1,2, ... 
u [t\ ^itlt\ 
||?}@« — & @«!l < r CI|~|m — Q l-|u 
W W 
C.2) „ 
Q(l/r) - 1 ., Q#(r/r) - 1 
= I 
tfr tjr 
As r ~> oo the rightrside tends to the right side in C.1) and so this inequality 
is true. > 
Corollary. Distinct convolution semi-groups cannot have the same generator. 
IX.3 
PREPARATORY LEMMAS 
297 
Lemma 2. {Convergence.) For each n let %n generate the convolution 
semi-group {?>„(»}. 
If 3tn-*¦ 31, then 31 generates a convolution semi-group {?i@}, and 
Qn@ -v Q(r) for-each t > 0. 
Proof. For each / > 0 the sequence {?}„(*)«} converges uniformly, 
since by C.1) 
C.3) 
\\Qn(t)u - 
< t \\Knu - %mu\\. 
By criterion 1 of VIII,3 there exists therefore an operator Q(t) associated 
with a probability distribution such that Qn(t) -*¦ ?}(*)• Then 
C.4) &n(S+ *) = &n(S) &n@ ~* &G0 ^@ 
(by theorem 2 of VIII.3) and so {Q@) is a convolution semi-group. To 
show that it is generated by 31 note that 
u — 
0.@-1. 
u - 
The first term on the right tends to ||3tnw — 3Im|| as t -*¦ 0. Letting m -*¦ co 
in C.3) we see that the second term is < ||3I« — 3Inw||. For fixed n the 
upper limit of the left side is therefore <2 \%u — 3Inw|| which can be made 
arbitrarily small by choosing n sufficiently large. > 
The next lemma makes it at least plausible that every continuous con- 
volution semi-group has a generator: 
Lemma 3. Let {&@} be a continuous convolution semi-group. If for 
some sequence tx, t2, . . - , -*¦ 0 . 
C.5) 
then 31 generates the semi-group. 
Proof. Call the left side 31*. As was shown in example 2(a) this 31* 
generates a compound Poisson semi-group and by the last lemma there 
exists a semi-group {Q#@} generated by 31. To show that Q#@ = Q@ 
we proceed as in C.2) to obtain 
C.6) 
\\?L(rtk)u - &*(rtk)u\\ ^ rtk 
3I,w - 
u 
Let k -+¦ oo and r-*-'co so that rtk -* t. The right side tends to 0 and the 
left to ||Q@« - ?#@«ll by virtue of B.5). > 
298 INFINITELY DIVISIBLE DISTRIBUTIONS AND SEMI-GROUPS IX.4 
These are the lemmas that will be of immediate use. The next is recorded 
here because it is merely a variant of lemma 2 and the proof is nearly the same. 
We shall use only the special case vn = n and 7 = 1, which will serve as the 
connecting link between triangular arrays and convolution semi-groups. 
Lemma 4. For each n let $n be the operator associated with the probability 
distribution Fn. If 
C.7) «Bf«-1)-*«, 
then % generates a convolution semi-group {Q@}- If n -*¦ oo and —-*¦ t 
then n 
C.8) 5vn"- 
In particular, gf? —* &0)- The lemma remains true if n is restricted to a 
sequence n1} w2,.... 
Proof. The left side in C.7) generates a compound Poisson semi-group 
[example 2(a)] and so 91 is a generator by lemma 2. By the basic inequality 
A.5) 
C-9) \\%l»u-Q(vJn)u\\ <, vjn \\n[%nu-u] - n[&(l/n)u-u]\\ 
and for we C00 each of the terms within the norm signs tends uniformly 
to 9tw. > 
4. FINITE VARIANCES 
Semi-groups of distributions with finite variances are of special importance 
and their theory is so simple that it deserves a special treatment. Many 
readers will not be interested in the more complicated general semi-groups, 
and for others this section may provide an interesting introductory example. 
We consider a convolution semi-group {?}(/)} and denote the associated 
probability distributions by Qt. Suppose that Qt has a finite Variance 
or2(f). Because of the semi-group property o2(s+t) ==¦ o2(s) + a\t) and 
the only positive solution of this equation4 is of the form o2(t) = ct. 
Suppose that Qt is centered to zero expectation. The second moment 
then induces a. probability distribution Q.t defined by 
D.1) &t{dy} = - 
ct 
4 Theequation q>(s+t) = <p(s) + <p(t) is called theHamel equation. Putting u(t) = e<Pu'- 
one gets u(s+t) = u(s)u(t) in which form the equation was encountered several times 
and is treated in 1; XVII ,6. The expectation of Qt is also a solution of the Hamel equation; 
it is therefore either of the form mt or exceedingly weird. See section 5a. 
IX.4 Finite variances 299 
By the selection theorem there exists a sequence {/„} tending to 0 such that 
as t runs through it Q.t tends to a possibly defective distribution Q.. 
Since Qt is centered to zero expectation we have the identity 
J-oo 
X2 
the integrand being (for fixed x) a continuous function of y assuming at 
the origin the value \u"(x). At infinity the integrand and its derivative 
vanish. This implies that as t runs through {/„}, and consequently 
Q.t -*¦ Q., the integral in D.2) tends uniformly to the analogous integral with 
respect to the limit distribution Q. According to lemma 3 of section 3 this 
means that our semi-group has a generator 51 given by 
J"+o° u(x—v} — u(x) 4- v u'(x\ 
-^ ^ V T y K } O\dy). 
-oo y 
This representation of 51 is unique because for functions of the form 
D.4) u(x) = 1 + 
one gets 
D.5) 21m@) = c I - 
J-oo 1 + yL 
The knowledge of 9Iw for all u e C00 therefore uniquely determines the 
measure (I +y2)~x 0.{dy} and hence Q itself. 
In consequence of this uniqueness the limit distribution Q. is independent 
of the sequence {tk} and hence O.t{dy) -*¦ Q{dy} for any approach t -*¦ 0. 
We shall show that H is a proper probability distribution and that every 
operator of the form D.3) is a generator. The proof depends on two special 
case,s contained in the following 
Examples, (a) Normal semi-groups. When Q. is the probability distri- 
bution concentrated at the origin D.3) reduces to %u(x) = \cu"(x). We saw 
in example 2(e) that this % generates a semi-group of normal distributions 
with zero expectations and variances ct. It is easy to verify directly that the 
distributions Qt tend to the probability distribution concentrated at the 
origin. 
(b) Compound Poisson semi-groups. Let F be a probability distribution 
concentrated on the intervals \x\ > r\ having expectation ml and variance 
m2. The distributions Qt of the compound Poisson semi-group of example 
2(a) have expectations am^ and variances <xm2t. The semi-group is gener- 
ated by aBr — 1). In accordance with example 2(d) the same Qt but 
300 INFINITELY DIVISIBLE DISTRIBUTIONS AND SEMI-GROUPS IX.5 
centered to zero expectations form a semi-group generated by 
that is 
D.6) 2lu(z) = f [u{x-y) - u(x) - y «'(*)] F{dy}. 
J\v\>n 
With the change of notations Q{dy] = y2 F{dy}/m2 and am2 = c this 
reduces to D.3). Conversely, if D is a probability distribution concentrated 
on |a;| > 0, then D.3) may be rewritten in the rnrm D.6) and hence such % 
generates a compound Poisson distribution whose distributions Qt have 
zero expectation arid variances m2t = ct. 
We are now in a position to formulate the basic 
Theorem. Let Qt have zero expectation and variance ct. The convolution 
semi-group (?}@) then has a generator % of the formD.3) where Q is a 
proper probability distribution. The representation D.3) is unique. Con- 
versely, every operator of this form generates a convolution semi-group of 
distributions with zero expectation and variance ct. 
Proof. We have shown the existence of a generator of the form D.3) 
but have piroved only that Q. has a total mass co <^ 1. It remains to prove 
that if Q. has a mass co then the operator % of D.3) generates a semi-group 
such that Qt has zero expectation and a variance <. coot. 
Let %n be the operator obtained from D.3) by deleting the intervals 
0 < \y\ < V from the domain of integration. Denote the masses attributed 
by Q to the origin and to \y\ > r\ respectively by m and oor It follows from 
the preceding examples that 91^ is the sum of two operators, of which the 
first generates a normal semi-group with variances cmt, and the second 
generates a compound Poisson semi-group with variances ccoj. By the 
addition rule of example 2(c) the operator %n itself generates a semi-group 
with variances** c(m+co^)t. Being the limit of %n as ?? —> 0 the operator 
91 itself is the generator of a semi-group. The variances of the associated 
distributions lie between the variances c(m+co,.)t corresponding to 51,, 
and their limit coot. This proves that 21 indeed generates a semi-group with 
variances coot. > 
5. THE MAIN THEOREMS 
In this section {Q@} stands for an arbitrary continuous convolution 
semi-group, and the associated distribution functions are again denoted by 
Qt. By analogy with D.1) we define a new measure Q.t by 
The novel feature is that, in the absence of a second moment of Qt, the 
measure Qt need not be finite on the whole line. However, Q.t{I} is finite 
IX.5 THE MAIN THEOREMS 301 
for every finite interval /. Furthermore, since Qt{dy} = ty~2 Qt{dy} it 
follows that y~2 is integrable wkh respect to Qt over any domain excluding 
a neighborhood of the origin. We shall see that as t -> 0 the measures Qt 
will converge to a measure Q with similar properties and this Q will deter- 
mine the generator of the semi-group. It is therefore convenient to introduce 
the 
Definition. A measure O. on the real line will be called canonical if 
Q.{I} < oo for finite intervals I and if the integrals 
E.2) 
y>+(*) = f V2 Q{dy}, yr{-x) = f 
Jx J— 
2 
oo 
converge for each x > 0. 
(For definiteness we take the intervals of integration closed.) 
We proceed to show that the theory of the preceding section carries over 
except that we have to deal with canonical measures rather than with prob- 
ability distributions and that in the absence of expectations we must resort 
to an artificial centering. We define the truncation function rs as the con- 
tinuous monotone function such that 
E.3) rs(x) = x when \x\ < s, rs(x) = ±s when \x\ > s 
where s > 0 is arbitrary, but fixed. 
In analogy with D.2) we now have the identity 
E.4) ?W^L M(I) _ p «(*-?)-»(*)-r,(y)»(*) a<{dy} 
t J-oo y 
where 
C+ao C+oo 
E.5) bt = rs(y)y-2 Qt{dy} = r1 rs(y) Qt{dy}. 
J — CO J — 00 
The integrand in E.4) is again (for fixed x) a bounded continuous function 
assuming at the origin the value \u"(x). It will be noted that the special 
choice E.3) for the truncation function is not important: we could choose 
for t any bounded continuous function provided it is near the origin twice 
continuously differentiable with r@) = t"@) = 0 and r'@) = 1. 
We have now a setup similar to the one in the preceding section, and we 
derive a similar theorem. The integral in E.4) makes sense with Qt replaced 
by any canonical measure, and we define an operator 2t(r) by 
= 
y2 
The superscript r serves to indicate the dependence on the truncation 
function t. A change of rs for of the point s in the definition E.3)] 
302 INFINITELY DIVISIBLE DISTRIBUTIONS AND SEMI-GROUPS IX.5 
amounts to adding a term bdldx to 9I(r), and so the family of operatore 
E.7) % = 2T(r) + bdldx 
is independent of the choice of ~s. 
Theorem 1. A continuous convolution semi-group {?}(/)} hbs a generator 
21, and 21 is of the forth determined by E.6)-E.7) where Q is a canonical 
measure. 
. Conversely, every operator 21 of this form generates a continuous convolution 
semi-group (Q@)- *Thp measure D. is unique, r .id as t-+0 
E.8) i\{l} - &{!} 
for infinite intervals5 I and 
E.9) r-*[l -Q,(*)] - y>+(x), t-i5it(-x) — yr(-x) 
for x > 0. 
Proof. We return for the moment to the notations of section 1. For each 
n we consider the sum Sn = Xln -f • • • -f- Xnn of n mutually independent 
variables with the common distribution Q1/n. Then Sn has the distribution 
Qlt and h'ence Sn remains stochastically bounded (definition 2 of VIII,2). 
We now anticipate lemma 1 of section 7 according to which this implies 
that for each finite interval / the measures O.1/n{I) remain bounded, and 
that to each e > 0 there corresponds a number a.> 0 such that 
E-10) »[1 - Q1/n(a) + Qlln{-a)] < e 
for all n. (The lemma is quite simple, but its proof is postponed in order 
not to interrupt the argument.) 
By the selection theorem there exists a measure Q. and a sequence of 
integers . nk such that as f:1 runs through it, Qt{I] ->I2{/} for finite 
intervals. The contribution of / to the integral in E.4) then converges to the 
corresponding contribution o/ / to the integral E.6) defining 9I(r). Re- 
membering that y~2 O.t{dy) = Qt{dy) it is seen from E.10) that the contri- 
bution of \y\ > a to these integrals is uniformly small if a is chosen large 
enough. We conclude that Q is a canonical measure and that as f-1 runs 
through {nk} the integral in E.4) converges uniformly to the integral in 
E.6). Furthermore, under the present conditions the quantity bt of E.5) 
remains bounded and hence there is no loss of generality in supposing that the 
sequence {nk} was picked so that as t~x runs through it bt converges 
.to a number »i». Then as t'1 runs through {nk} 
E.11) t 
5 Here and in the sequel it is understood convergence is required only for intervals and 
points of continuity. 
IX.5 THE MAIN THEOREMS 303 
the convergence being uniform. By lemma 3 of section 3 this means that the 
semi-group (Q(f)} is generated by $t, and so E.11) holds for any approach 
We have thus shown that a generator % exists and can be expressed in the 
form E.6)-E.7) in terms of a canonical measure Q. As in the.preceding 
section the uniqueness of Q in this representation follows from the fact 
that for functions of the form D.4) the value 9Iw@) is given by D.5) (with 
c now absorbed in Q). 
The uniqueness of Q. implies that E.8) holds for an arbitrary approach 
?->0. Furthermore, E.10) guarantees that the quantities in E.9) are uni- 
formly small for x sufficiently large. These quantities were defined by E.2) 
and the analogous relations with .Q. replaced by Qt. This shows that E.9) 
is a consequence of E.8). 
It remains to show that the measure Q can be chosen arbitrarily. The 
proof is the same as in the case of finite variances. As in example 4(b) it is 
seen that if Q is concentrated on \y\ > r\ > 0 the operator 51 of E.6)- 
E.7) generates a compound Poisson semi-group with modified centering (but 
without finite expectations). Thus % can again be represented as a limit of 
generators and is therefore a generator. »> 
Example. Cauchy semi-groups. The distributions Qt with density 
¦n^t^+x2)-1 form a semi-group. It is easily verified that the limits in E.9) 
are given by y)+(x) = y>~(—x) = nz'1, and nQ. coincides with the Lebesgue 
measure or ordinary length. > 
The following theorem embodies various important characterizations of 
infinitely divisible distributions. The proof of part (v) is postponed to section 
7. (For an alternative direct proof see problem II.) This part admits of 
further generalizations to triangular arrays with variable distributions. 
(See section 9. The full theory will be developed in chapter XVII.) For the 
history of the theory see VI,3. 
Theorem 2. The following classes of probability distributions are identical: 
(i) Infinitely divisible distributions. 
(ii) Distributions associated with continuous convolution semi-groups (that is, 
distributions of increments in processes with stationary independent increments). 
(iii) Limits of sequences of compound Poisson distributions. 
(iv) Limits of sequences of infinitely divisible distributions. 
(v) Limit distributions of row sums in triangular arrays {Xk n} where the 
variables Xk_n of the nth row have a common distribution. 
Proof. Let {?}(/)} be a continuous convolution semi-group. It wa? 
shown in example 2(a) that for fixed h > 0 the operator %h = [Sl(h)~\}jh 
generates a compound Poisson semi-group of operators Qh(t). As h-+0 
304 INFINITELY DIVISIBLE DISTRIBUTIONS AND SEMI-GROUPS IX.5 
the generators ^lh converge to the generator 91, and hence QA@ -> ?5@ 
by lemma 2 of section 3. Thus Qt is the limit of compound Poisson distri- 
butions, and so the class (ii) is contained in (iii). The class (iii) is trivially 
contained in (iv). 
For each n let G{n) be an infinitely divisible distribution. By definition 
G(n) is the distribution of the sum of n independent identically distributed 
random variables, and in this way the sequence {G{n)} gives rise to a 
triangular array as described under (v). Thus the class (iv) is contained in (v). 
In section 7 it will be shown that (v) is contained in (ii), and so the classes 
(ii)-(v) are identical. Finally, the class of ail infinitely divisible distributions 
is a subclass of (iv) and contains (ii). > 
According to the last theorem every infinitely divisible distribution F 
appears as a distribution Qt of an appropriate convolution semi-group. 
The value of the parameter / can be fixed arbitrarily by an appropriate 
change of scale on the f-axis. However, there exists only one semi-group 
{?}(/)} to which an infinitely divisible distribution F belongs. This amounts 
to saying that the representation F — F^* of F as an «-fold convolution is 
unique. This assertion is plausible, but requires proof. As a matter of fact, 
in itr, Fourier theoretic version the uniqueness becomes obvious, whereas in 
the present context the proof would detract from the main topic without 
being illuminating. For this reason we desist for once from proving the state- 
ment within both frameworks. 
Application to stochastic processes. Let X(t) be the variable of a stochastic 
process with stationary independent increments (VI,4) and let us interpret 
Qt as the distribution of the increment X(r+j) — X(s). Consider a time 
interval s, s + 1 of unit length and subdivide it by the points s = 
= s0 < ^ < • • • < sn = s + 1 into subintervals of length «~J. Then 
/C) - Xfo.0 > x) = 1 - g1<w(z) and so n[l-Qlin(x)] equals the 
expected number of intervals sk_x, sk with increment >x. A.s n —*¦ oo this 
expected number tends to ip+(x). For simplicity of discussion suppose 
that the limits X(/+) and X(f-) exist for all / and that X@ lies between 
them. Let sk_ltsk be the interval of our partition containing /. For n 
sufficiently large the increment X(^) — X(sk_j) will be close to the jump 
X(H-) — X(/—) and it is intuitively clear that the limit y+{x) represents the 
expected number of epochs t per unit time at which X(/ + ) — X(t~) > x. 
The argument may be justified rigorously but we shall not enter into details. 
It follows from this result that the expected number of discontinuities is 
zero only if y)+(x) = 0 and yr(—x) = 0 for all x > 0. In this case O is 
concentrated at the origin, that is, the increments X(/+j) — X(s) are 
normally distributed. For such a process the paths are continuous with 
iX.6 example: stable semi-groups 305 
probability one (theorem of P. Levy and N. Wiener) and so the paths are 
continuous with probability 1 iff the process is normal. 
As a second illustration consider the compound Poisson process B.8). 
The expected number of jumps per unit time is a, and the probability of a 
jump exceeding x > 0 is 1 — F(x). Thus a[l— F(x)] is the expected 
number of jumps >x in full agreement with our intuitive argument. 
*5a. Discontinuous Semi-groups 
It is natural to ask whether there exist discontinuous semi-groups. The question is of 
no practical importance but the answer has some curiousity value: Every convolution 
semi-group {?}(*)} differs merely by centering from a continuous semi-group {Q #(/)}. In 
particular, // the distributions Qt are symmetric the semi-group is necessarily continuous. 
In the general case there exists a function cp such that the distributions Q# defined by 
Qt(x+<p(t)) are associated with a continuous semi-group. The function cp must obviously 
satisfy 
E.12) <p(t+s) = fp(t) + <p(s). 
This is the famous Hamel equation whose only continuous solution is of the form ct 
(see footnote 4 to section 4). In fact, the only Baire function satisfying E.12) is linear. 
The other solutions are weird indeed; for example, a non-linear solution assumes in every 
interval arbitrarily large and arbitrarily small values, and it is impossible to represent it 
analytically by limiting processes. In short, it is fair to ask in what precise sense it "exists." 
To return to earth, consider an arbitrary convolution semi-group {?}(/)} and me 
triangular array {Xk n} associated with the distributions Qi\n. The row sums Sn have 
the common distribution C?i and hence we can use the last lemma to extract a sequence 
n1,n2,. . , such that as n runs through it n[Q(l/n) — l] -*3l# where 31 # is the generator 
of a continuous semi-group {?}#(/)}. We may choose the n^ of the form 2V. The 
inequality C.2) now shows that Q(/) = Q#(r) for all t that are multiples of ljnk for 
arbitrarily large k, that is, for all / of the form / = a2~v with a and v integers. Thus 
there exists always a continuous semi-group {?}#(/)} such that D.(t) = !Q.#(t) for all t 
of a dense set 2. 
We are now in a position to prove the initial proposition, Choose en > 0 such that 
t + en is in 2. Then 
E.13) Q#(f+«n) = &(t + en) = Q@Q(en). 
As en-—0 the left side tends to Q#(t) and hence it suffices to sho./ that if Q(en) -* % 
then the distribution F is concentrated at a single point. Choose points hn in 2 such 
that 0 < <rn < hn and hn -*0. Then Q#(AB) = -Q(//n-en)Q(en). The left side tends 
to the identity operator, and so F can indeed have only one point of increase. 
6. EXAMPLE: STABLE SEMI-GROUPS 
A semi-group {Q(/)} is called stable if its distributions are of.the form 
F.1) Qt(x) = 
where /< > 0 and /5, are constants depending continuously on t, and 
G is a fixed distribution. Obviously. G is a stable distribution as defined 
306 INFINITELY DIVISIBLE DISTRIBUTIONS AND SEMI-GROUPS IX.6 
in VI,I. The theory of stable semi-groups is here developed principally as an 
illustration for the results of the last section and to put on record the form 
of their generators. In an indirect way the results of this section are derived 
independently in section 8. 
Because of the assumed continuity of Xt and fit the semi-group (if it 
exists) is continuous. As t -> 0 the distribution Qt tends to the distribution 
concentrated at the origin, and hence Xt-+ oo and fit-*0. The first 
relation in E.14) takes on the form 
F.2) r-*[l - G(Xt(x-pj)] — y>+(x), x > 0. 
Since 0t -* 0 and G is monotone this relation remains valid also when fit 
is dropped, and then F.2) may be rewritten in the form 
F.3) V (). 
1 - G(Xt) * K J 
(Here we assume that 1 is a point, of continuity for y>+, which can be 
achieved by a change of scale.) Now F.3) is a particular case of the relation 
VIII,(8.1) defining regular variation. We conclude that either \p+ vanishes 
identically, or else the tail 1 — G varies regularly at infinity and 
F.4) y>+(x) = c+z~*, z>0, c+ > 0. 
On the positive half-axis the measure Q. has therefore the density ac+ar*-1. 
We conclude that 0 < a < 2 because Q attributes finite masses to finite 
neighborhoods of the origin, and y>+(x)—*-0 as x —*- oo. For similar reasons 
either ip~ vanishes identically or else 'yr(x) = c~ \x\~a for x < 0. The 
exponent a is the same for the two tails, because also the tail sum 
1 — G(x) + G(—x) varies regularly. 
The functions rp+ and ip~ determine the measure D. up to a possible 
atom at the origin. We shall see that such an atom cannot exist unless both 
V+ and y>~ vanish identically and Q is concentrated at the origin. 
The generator ^ is given by E.7). In the present case it is convenient to 
write it in the form 
F.5) « = c+2C + c-%; + bd/dx, 
where the operators 51+ and 2l~ describe the contributions of the two half 
axes and are defined as follows. 
If 0 < a < 1 
F.6) %t u(x) = fW-y) - u(x)]y—x dy. 
Jo 
If L < a < 2 
F.7) 
u(x) = r[u(x-y) - u(x) - y u'{x)}y-*~l dy, 
Jo 
IX.6 example: stable semi-groups 307 
and finally if a = 1 
F.8) 21+ u(z) = 
u{x-y) - u(x) - 
1" is defined by the analogous integral over —oo,0 with y~a~l replaced by 
]y\-*-1. The centering in F.6) and F.7) differs front that in the canonical 
form E.6), but the difference is absorbed in the term bdjdx in F.5). An 
atom of Q at the origin would add a' term ydtjdx1 to the generator %. It 
was shown in example 4(a) that this term by itself generates a semi-group of 
normal distributions. 
Theorem, (a) When b = 0 and 0 < a < 1 or 1 < a < 2 the operator 
F.5) generates a strictly stable semicgroup^bf the form 
F.9) Qt(x) = G(*r1/a); 
when a = 1 and b = 0 // generates a stable semi-group of the form 
F.10) Qt{x) = G(xr> - (c+-C-) log /). 
(b) A stable semi-group is either generated by F.5) or else it is a semi-group 
of normal distributions. 
[We saw in 2(b) that bdjdx generates a translation semi-group and to 
obtain the semi-group generated by F.5) with b -^ 0 it suffices in F.9) and 
F.10) to replace x by x -f bt.] 
Proof, (a) A change of scale changes the distribution Q, of a semi-group 
into distributions defined by Qf(x) == Qt(xjp). These form a new semi- 
group (Q#(/)}. If we put v(x) = u(px) it is seen fron\the definition of a 
convolution that Q#(t)u(x) = Q(t)v(x/p). For the generators this means 
that to find W#u(z) we have simply to calculate 'Hd(x) and replace x by 
xjp. The substitution y ~ z/'p in F.7) and F.8) shows thai for the corre- 
sponding generators Wf = /oal2la. How pas#a is obviously the generator of 
the semi-group {Q(p*/)}, and from the uniqueness of generators we con- 
clude therefore that QtU-jp) = Qtpa(x). Letting 0' ~ Qx and p — t~i/x we 
get F.9). 
A similar argument applies in the case a = 1. except that when the 
substitution y = zjp is used in F.8) the centerin.2 function gives rise to an 
additional term of the form (c* — c~){p log p) u(r\, and this leads to F.10). 
(b) To measure Q concentrated at the origin there corresponds a normal 
semi-group. We saw that the generator of any olher stable semigroup is of 
the form ^Ia + yd2/dx2. As shown in example 2(c) the distributions of the 
corresponding semi-group would be the convolutions of our stable Qt with 
normal distributions with variance lyt and it is clear that such a semi- 
group cannot be stable. > 
308 INFINITELY DIVISIBLE DISTRIBUTIONS AND SEMI-GROUPS IX.7 
The theorem asserts that the function A.t occurring in the definition of 
stable semi-groups is of the form Xt = t~1/a. Considering F.2) and its 
analogue for the negative half-axis we get the 
Corollary. If 0 < a < 2, c+ > 0, c~ > 0 (but c+ + c> 0) there 
exists exactly one stable distribution function G such that as x —> oo 
F.11) xa[l-G(x)]-+c+, x*G(-z)-+c~. 
The normal distribution is the only remaining stable distribution [and satisfies 
FA I) with a = 2 and c+ = c~ = 0]. 
The assertion within brackets will be proved in section 8. 
7. TRIANGULAR ARRAYS WITH IDENTICAL DISTRIBUTIONS 
For each n let Xln,... , Xnn be mutually independent random 
variables with a common distribution Fn. We are interested in the possible 
limit distributions of the sums Sn = XIn + • • • + Xnw, but it is useful 
to begin by investigating a necessary condition for the existence of a limit 
distribution, namely the requirement that the sequence {Sn} be stochastically 
bounded. We recall from VII1,2 that {Snj is said to be stochastically bounded 
if to each e > 0 there corresponds an a such that P{|Sn| > a} < e 
for all n. Very roughly speaking this means that no probability mass flows 
out to infinity. Obviously this is a necessary condition for the existence of 
a proper limit distribution. 
We shall rely heavily on truncation. It is most convenient to use once more 
the truncation function ts introduced in E.3) in order to avoid discontin- 
uities. ts is the continuous monotone function such that ts(x) = x when 
\x\ < s and rs{x) = ±-s when |a;| ^> s. With this truncation function 
we put 
G.1) Xfcn = Ts(Xfcn), Xfcn = Xk „ = Xk „ + Xk n. 
The new variables depend on the parameter s even though our notation does 
not emphasizejt. The row sums of the triangular arrays {X^n} and {X?>n} 
will be denoted by S; and S*. Thus Sw = S; + SJ. The'variables X'kin 
are bounded, and for their expectation we write 
G.2) ft, = E(X;.n). 
(/?„ is, of course, independent of k.) Finally we introduce ihe analogue 
Lo the measures Clt of section 5, namely the measure Ow defined by 
G.3) <t>n{dx) = nx* Fn{dx). 
On{/} is finite for finite intervals /, but the whole line may receive .an 
infinite mass. 
IX.7 TRIANGULAR ARRAYS WITH IDENTICAL DISTRIBUTIONS 309 
It is plausible that {SJ cannot remain stochastically bounded unless the 
individual components become small in the sense that 
G-4) P{|Xft.J > e}-* 0, „-*«>, 
for every e > 0. (The left side is independent of k.) Arrays with this 
property are called null-arrays. We shall see that only null-arrays can have 
stochastically bounded row sums, but for the time being we introduce G.4) 
as a starting assumption. 
The "necessary" part of the following lemma was used in the proof of 
theorem 1 in section 5; there the condition G.4) was fulfilled because the 
semi-group was continuous. 
Lemma. {Compactness.) In order that the row sums Sn of a null-array 
{Xj. „} remain stochastically bounded it is necessary and sufficient 
(i) that On{/} remains bounded for every finite interval /, and 
(ii) that for large x the tail sums 
G-5) Tn(x) = n[\ - Fn(x) + Fn(-x)) 
are uniformly small. 
In other words, it is required that to each e > 0 there corresponds a / 
such that Tn(x) < e for x > /. (Note that Tn is a decreasing function.) 
Proof. In the special case of symmetric distributions Fn the necessity 
of condition (ii) is apparent from the inequality 
G.6) P{|SJ >a}> |A - exp (-Tn(a))) 
[see V,E.10)]. For arbitrary Fn we apply the familiar symmetrization. 
Tdgether with Sn the symmetrized variables °Sn also remain stochastically 
bounded, and therefore condition (ii) applies to the tails °Tn of the 
symmetrized distributions. But for a null-array it is clear that for each <5 > 0 
ultimately °Tn(a) > \Tn{a + d), and so condition (ii) is necessary in all 
cases. 
Assuming that condition (ii) is satisfied, the truncation point s can be 
chosen so large that Tn(s) < .1 for all n. The number of terms among 
X" ,. . . , X" that are different from 0 is then a binomial random variable 
with expectation and variance less than I. It is therefore possible to pick 
numbers N and c such that with probability arbitrarily close to 1 fewer 
than N among the variables X"k n will be different from 0 and all of them 
<c. This means that the sums S'^ remain stochastically bounded, and 
under these circumstances {Sn} is stochastically bounded iff {S^} is. 
It remains to show that condition (i) is necessary and sufficient for the 
stochastic boundedness of {S,',}. 
310 INFINITELY DIVISIBLE DISTRIBUTIONS AND SEMI-GROUPS IX.7 
Put a\ = Var (Sn). If on -> oo the central limit theorem of example l(a) 
applied to the variables (X^ n—pn)/an shows that for large n the distribution 
of S^ will be approximately normal with variance ^->-oo and hence 
P{S^e/}—*0 for any finite interval /. The same argument applies to 
subsequences and proves that {S^} cannot remain stochastically bounded 
unless Var (S^) remains bounded. But in this case Chebyshev's inequality 
shows that {S^ — nftn} is stochastically bounded, and the same will be true 
of S^ iff {«/?„} is bounded. Now /?„ -> 0 because {Xk_„} is a null-array, 
and hence the boundedness of «/5n implies thai Var (S^) ~ E(S^2). 
We have thus shown that the boundedness of E(S^2) is a necessary 
condition for the stochastic boundedness of {S^}, and by Chebyshev's 
inequality this condition is also sufficient. But 
G.7) E(S;2) = <Dn{^l} + s2 Tn(s) 
and hence under the present circumstances the condition (i) is equivalent to 
the condition that E(S^2) remains bounded. > 
The assumption that {Xk „} is a null-array was used only in connection 
with the symmetrization and could be omitted for arrays with symmetric 
distributions Fn. However, the boundedness of <?„{/} implies the E(X*2n) = 
= O(n~l), and one concludes easily from the lemma that an array with 
stochastically bounded row sums and symmetric distributions is necessarily 
a null-array. By symmetrization it follows thai in the general case there exist 
numbers (j.n (for example, the medians of Xk'n) such that {Xk n — fj,n) is 
a null-array. In other words, an appropriate centering will produce a null- 
array, and in this sense only null-arrays are of interest. 
Example. Let the Xkn have normal distributions with expectation 
/5n and variance tr1. Then Sn — «/5n has the standard normal distribution 
but since the /$„ are arbitrary, {Sn} need not be stochastically bounded. 
This illustrates the importance of centering. > 
For theoretical purposes it would be possible to center the array so that 
ftn = 0 for all «, but the resulting criterion would be difficult to use in 
concrete situations. With arbitrary centerings the criteria involve non-lfnear 
terms and become unwieldy. We shall cover this case in full generality in 
XVIIj. Here we shall strike a compromise: we shall require only that 
G.8) ft = o(E(X;2n)), n - oo. 
This condition seems to be satisfied in all cases occurring in practice. In 
any case, it is so mild that it is usually easy to satisfy it by an appropriate 
centering whereas the more stringent requirement that /5n = 0 may require 
complicated calculations. 
IX.7 TRIANGULAR ARRAYS WITH IDENTICAL DISTRIBUTIONS 311 
Theorem. Let {Xk'n} be a null-array such that G.8) holds. 
In order that there exist centering constants bn such that the distributions of 
Sn — bn tend to a proper limit it is necessary and sufficient that there exist a 
canonical measure Q. such that 
G.9) (D 
for every finite interval and that for x > 0 
G.10) n[\ -Fn(x)] - V+(*). nFn(-x) — yr(-x). 
In this case the distribution of Sn — nfin tends to the distribution Qx 
associated with the convolution semi-group generated by the operator *& 
defined by 
J-oo y 
Proof. We observe first that with arbitrary bn the conditions (i) and (ii) 
of the lemma are necessary for {Sn — bn) to be stochastically bounded. The 
proof is the same with the simplification that the relation E(S^2)'--' Var (S^) 
is now a consequence of G.8) whereas before we had to derive it from the 
boundedness of S^. 
Assume then the conditions of the lemma satisfied. By the selection 
theorem there exists a sequence {nk} such that as n runs through it G.9) 
holds for finite intervals. For a finite interval 0 < a < x < b we have 
G-12) n[Fn(b). - Fn(a)} = 
and G.9) entails that also this quantity tends to a limit. Condition (ii) 
assures us that «[1 — Fn{b)] will be less than an arbitrary e > 0 provided 
only that b is sufficiently large. It follows that for 0 < a < b < oo the 
integral in G.12) converges to the analogous integral with respect to il. 
Thus Q is a canonical measure, and G.10) is true as n runs through {nk}. 
We know that the operator 31 of G.11) defines a semi-group {&(/)} of 
convolution operators. Let E,, be the operator induced by the distribution 
Gn of Xk%n — f}nt. namely G,,(z) = Fn(x+fin). It was shown in section 1 
that to show that the distribution of S7lk — wfc/?W/k tends to the distrbution 
Ql associated with 53A) it suffices to show that as n runs through \nk\- 
Now 
[u(x + f!n-y) - u(x-y)\ Fv{dy). 
-co 
312 INFINITELY DIVISIBLE DISTRIBUTIONS AND SEMI-GROUPS IX.8 
We express u(x+fin~y) using Taylor's formula-to-second-order terms. 
Since /?„ -> 0 it follows from G.8) and the boundedness of <&n{I} that 
nftl -> 0. As also u is bounded it is seen that 
JP+OO 
[u{x-y) - u(x) + rs(y) u'(x))nFn{dy} -\- en(x) 
— 00 
where en is a quantity tending uniformly to zero. The integral may be 
rewritten in the form G.11) except that the integration is with respect to On 
rather than Q. As was shown repeatedly, the limit relations G.9)-G.10) 
imply that the integral in G.15) converges to that in G.11) and so G.13) 
holds as n runs through- {nk}. Finally, the uniqueness of the semi-group 
containing Qx shows that our limit relations must be true for an arbitrary 
approach n -> oo, and this concludes the proof. > 
8. DOMAINS OF ATTRACTION 
In this section Xx, X2,... are independent variables with a common 
distribution F. By definition 2 of VI, 1 the distribution F belongs to the 
domain of attraction of G if there exist constants an > 0 and bn such that 
the distribution of a~1(X1+- ¦ -+Xn) — bn tends to G, whfere G is a 
proper distribution not concentrated at a point. Despite preliminary results 
in VI, 1 and in section 6 we here develop the theory from scratch. (In XVII,5 
the theory will be developed independently and in greater detail.) 
Throughout this section we use the notation 
(8-1) U(x) = f y2 F{dyl x > 0. 
J-x 
We recall from the theory of regular variation in VIII,8 that a positive 
function L defined on 0, oo varies slowly (at oo) if for x > 0 
(8.2) -——->1, s->oo. 
Theorem 1. A distribution F belongs to the domain of attraction of some 
distribution G iff there exists a slowly varying L such that 
(8.3) 
with 0 < a <; 2, and when a < 2 
(84) 1 ~ Hz) F(-x) 
1 - F(x) + F(-x) P' 4 - F(x) + F(-x) 
IX. 8 DOMAINS OF ATTRACTION 313 
When a = 2 condition (S3) alone is sufficient provided F is not concentrated 
at one point.* 
We shall see that (8.3) with a = 2 implies convergence to the normal 
distribution. This covers distributions with finite variance, but also many 
distributions with unbounded slowly varying U [see example VIII,4(a)]. 
Using theorem 2 of VIII,9 with 1 = 2 and r\ = 0 it is seen that the 
relation (8.3) is fully equivalent to1 
(8 5) x\\-F(x)+F(-x)} 2-a 
U(x) a 
in the sense that the two relations imply each other. 
When 0 < a < 2 we can rewrite (8.5) in the form 
(8.6) 1 - F(z) + F(-x) ~ ^JZ_? x-a L(X^ 
a 
and conversely (8.6) implies (8.3) and (8.5). This leads us to a reformulation 
of the theorem which is more intuitive inasmuch as it describes the behavior 
of the individual tails. (For other alternatives see problem 17.) 
Theorem la. (Alternative form), (i) A distribution F belongs to the 
domain of attraction of the normal distribution iff U varies slowly. 
(ii) It belongs to some other domain of attraction iff (8.6) and (8.4) hold 
for some 0 < a < 2. 
Proof. We shall apply the theorem of section 7 to the array of variables 
Xkn = Xk/an with distributions Fn(x) = F(anx). The row sums of the 
array {Xk „} are given by 
Obviously an «-»¦ oo and hence {Xfc „} is a null-array. To show that the 
condition G.8) is satisfied we put 
(8.7) v(z) = [*yF{dy) 
J-x 
6 For distributions with finite variance, U varies slowly except when F is concentrated 
at the origin. In all other cases (8.3) and (8.4) remain unchanged if F(x) is replaced by 
F(x+b). 
7 Condition (8.4) requires a similar relation for each tail separately:. 
x2[\-F(x)] 2-a x2F(-x) . 2-a 
(*} U(x) V/? a ' U(x) *q a 
When' a = 2 these relations follow from (8.5), which explains the absence of.a secvad 
condition when a = 2. Theorem 1 could have been formulated more" concisely *but 
more artificially) as follows: F belongs to some domain of attraction iff (*) is true with 
0 < a ^ 2, p > 0, q > 0, p + q = 1. 
314 INFINITELY DIVISIBLE DISTRIBUTIONS AND SEMI-GROUPS IX.8 
and note that G.8) is certainly true if 
(8.8) v?(x) = o(U(x)). 
Now if U(x) —> co as x -*- oo it is clear that v(x) = o(x U(x)) and U(x) = 
= o(x~2), and therefore (8.8) holds. The case of a bounded function V is 
of no interest since we know that the central limit' theorem applies to 
variables with finite variances. However, even with bounded functions V 
the relation (8.8) holds provided that the distribution F is centered to zero 
expectation. 
Condition (i) of the last theorem requires8 that for x > 0 
(8.9) n,a~2 U(anx) - Q(-z, z}, n - oo, 
while (ii) reduces to 
(8.10) n[\-F(anx]-^y+(x), nF(-anx)^ y>~(-x) 
[for the notation see E.2)]. It is easily seen that9 an+1jan -> 1. According to 
lemma 3 of VfII,8 it follows therefore-from (8.9) that U varies regularly, 
and the limit on the right is proportional to a power of x. Following a custom 
established by P. Levy we denote this power by 2 — a. Thus 
(8.11) Q{-x, x} = Cx2-*, x>0. 
The'left side being a non-decreasing function of x and bounded near the 
origin, we have 0 < a < 2. It follows that V is indeed of the form 
asserted in (8.3). 
Again, the same lemma 3 of ViIJ,8 assures us that the limits in (8.10) are 
either identically zero, or else proportional to a power of x. Now (8.5) shows 
that the only possible power is x~a; in fact, when a = 2 both limits are 
identically zero, whereas for a < 2 the limits are necessarily of the form 
Ax~a and Bx~a .where A > 0 and B > 0, but A + B > 0. It follows 
that the conditions of the theorem are necessary. 
Assuming (8.3) to be true*it is possible to construct a sequence {an} such 
that 
(8.12) na-2U(an)-+l. 
For example, one may take for an.the lower bound of all / such that 
nt~2 U{t) < 1. Then (8.3) guarantees that for x > 0 
(8.13) na~2 U(anx) -> z2~a. 
8 As usual, it is tacitiy understood that convergence is required only at points of continuity. 
9 For symmetric distributions this follows from the fact that (Xj + • • • + Xn)/an and 
(X1 + ¦ ¦ ¦ + Xw)/o,1 + 1 have the same limit distribution. For arbitrary F the assertion 
follows by syinmetrization. 
IX.8 DOMAINS OF ATTRACTION 315 
Thus condition G.9) is satisfied for intervals of the form / = {—x, x). In 
case a = 2 the limit measure Q is concentrated at the origin and therefore 
G.9) is automatically satisfied for all finite intervals; in this case it follows 
from (8.5) that also condition G.10) is satisfied with y>+ and tp~ identically 
zero. When a < 2 the relations (8.3)-(8.5) together imply that as z-> oo 
(8.14) 1 - F(x) ~ p 1^-2 z~a L(x), F(-x) ~ q ^=-2 z~« L(z) 
a a 
provided p > 0 and q > 0. (In the contrary case the symbol ~ is to be 
replaced by "little oh" and there is no essential change.) It follows that 
condition G.10) holds, and this in turn implies that G.9) applies to arbitrary 
intervals at a positive distance from the origin. >¦ 
It is noteworthy that all the results of section 7 are implicitly contained in 
the present theorem and its proof. The proof leads also to other valuable 
information. First we have the obvious 
Corollary. If a = 2 the limit distribution is normal, and otherwise it is a 
stable distribution satisfying F.11). In either case it is determined up to 
arbitrary scale parameters. 
We saw also that (8.12) leads to a possible sequence of norming factors 
an. It is easily seen that another sequence {a'n} will do iff the ratios a'Jan 
tend to a positive limit. 
Under the conditions of theorem 1 we have established the existence of a 
limit distribution for Sn — nj3n where [with v defined in (8.7)] 
(8.15) 0n = E(Xfc,n) = a~l v(san) + S[l-F(san)-F(-san)]. 
We now proceed to prove the pleasant fact that the centering constants /?„ 
are really unnecessary except when a = 1: 
When a < 1 we apply theorem 2 of VIII,9 with ? = 1 and 7] = 0 
separately to the two half-axes to find that as a; 
(8.16) u(z) — x[i-F(x)-F(~z)]. 
1 — a 
From this and (8.10) it follows that nfin tends to a finite limit and therefore 
plays no essential role. 
When a > 1 the same theorem 2 of VIII,9 with ? = 2 and rj = 1 
shows that F has an expectation, and we naturally center F to zero 
expectation. The domain of integration in the integral (8.7) for v may then 
be replaced by \y\ > x and it is found that (8.16) holds without change. 
Thus the distributions of Sn tend to a limit which is again centered to zero 
316 INFINITELY DIVISIBLE DISTRIBUTIONS AND SEMI-GROUPS IX.9 
expectation. Similarly, when a < 1 the limit distribution is centered so as 
to be strictly stable. We have thus proved 
Theorem .2. Suppose that F satisfies the conditions of theorem 1. If 
a < 1 then Fn *(anx) -> G(x) where G is a strictly stable distribution 
satisfying F.11). If a1 > 1 the same is true provided F is centered to zero 
expectation. 
(For the centering when a=l see XVII,5. Concerning the moments of 
F see problem 16.) 
9. VARIABLE DISTRIBUTIONS. THE THREE-SERIES 
THEOREM 
We turn very briefly to general triangular arrays* {Xk „} where the 
variables10 Xx „, . . . , Xnn of the nth row are mutually independent, but 
have arbitrary distributions Fkn. To preserve the character of our limit 
theorems we consider only null-arrays: it is required that for arbi'rary 
t] > 0 and e > 0 and n sufficiently large 
(9.1) PflX^J > r,} < e, k=l,...,n. 
The theory developed in section 7 carries over with the sole change that 
expressions like n Var (X^ n) are replaced by the corresponding sums. In 
particular, only infinitely divisible distributions occur as limit distributions of 
row sums of null-arrays. The verification may be left to the reader as a 
matter of routine. 
We proceed to discuss some interesting special cases. The notations are 
the same as in section 7, but in the following it does not matter which type of 
truncation is used; it is perhaps simplest to define the truncated variables 
by X'kin = XiiB when 1Xi>B| < s and X'kn = 0 otherwise. Here the 
truncation level s is arbitrary. 
The first theorem is a variant of the compactness lemma a"nd is equivalent 
to it. 
Theorem 1. {Law of large numbers) Let Sn stand jor the row sums of a 
null-array. In order that there exist constants bn such that11 Sn — bn —>¦ 0 
it is necessary and sufficient that for each fj > 0 and each truncation level s 
In this case one may take bn = ^ E(X^ n), 
k 
10 Concerning the number of variables in the nth row see problem 10 in VII,13. 
11 We recall from VIII.2 that Zn converges in probability to O.if P{|Zn| > e} -* 0 
for every e > 0. 
IX.9 VARIABLE DISTRIBUTIONS. THE THREE-SERIES THEOREM 317 
As an application we prove the following theorem which was already 
discussed in VIII,5. 
Theorem 2. {Infinite convolutions.) Let Yl5 Y2, . . . be independent 
random variables with distributions GX,G2,.... In order that the distributions 
Gxic G2- -ic Gn of the sums Tn = Yx + • • • + Yn tend to a proper limit 
distribution G it is necessary and sufficient that for each s > 0 
(9.3) 2 P{IY*I >*}<«. 2Var(Y^)<oo 
and 
(9.4) 2E(YD - m. 
Proof. For a given increasing sequence of integers vx, v2,.. . and 
k = 1,;. . . , n put Xjfc>n = YVn+k. The distributions Gx ic ' • • it Gn con- 
verge iff all triangular arrays of this type obey the law of large numbers with 
centering constants bn =0. From theorem 1 it is clear that the conditions 
(9.3) and (9.4) are necessary and sufficient for this. *¦ 
Theorem 2 may be reformulated more strikingly as follows. 
Theorem 3. (Kolmogorov's "three-series theorem".) The series ^Yk 
converges with probability one if (9.3) and (9.4) hold, and with probability 
zero otherwise. 
Proof. Assume (9.3) and (9.4). By theorem 2 of VII,8 the second con- 
dition in (9.3) guarantees that ]? [Y^—E(Yp] converges with probability 
one, and then (9.4) implies the same for ]? Y^. By the Borel-Cantelli lemma 
(see 1; VIII,3) the first condition in (9.3) entails that with probability one 
only finitely many Yk differ from Y'k, and so 2 Y* converges with 
probability one. 
To prove the necessity of our conditions recall from IV,6 that the prob- 
ability of convergence is either zero or one. In the latter case'the distribution 
of the partial sums must converge, and so (9.3) and (9.4) hold. >> 
Processes with Non-stationary Increments 
The semi-group theory developed in this chapter is the tool particularly adapted to 
processes with stationary independent increments. Without the condition of stationarity 
the increment X(/) — X(x) will have a distribution depending on the two parameters / 
and t, and we have to deal with a two-parametric family of operators Q(t, /), 0 < r < t 
satisfying the convolution equation 
(9.5) Q(r, j)?}(j, /) = Q(r, /), t < s < t 
Are the distributions associated with such operators infinitely divisible? We can partition 
t, / into n intervals tk_x,tk and consider the variables X(tk) — X(tk_l), but to apply 
318 INFINITELY DIVISIBLE DISTRIBUTIONS AND SEMI-GROUPS IX. 10 
the theory of triangular arrays we require the condition (9.1) amounting to a uniform 
continuity of the distributions in their dependence on the two time parameters. But 
?}(t,/) need not depend continuously on /. In fact, the partial sums of a sequence 
Xlf X2,. . . of independent random variables represent a process with independent incre- 
ments where all changes occur at integer-valued epochs and so the process is basically 
discontinuous. In a certain sense, however, this is the only type of essential discontinuity. 
The qualification "essential" is necessary, for it was shown in section 5a that even with 
ordinary semi-groups artificial centering can produce mischief which, though inconse- 
quential, requires caution in formulations. For simplicity we s'tick therefore to symmetric 
distributions and prove 
Lemma. If the distributions associated with Q(t,Y) are symmetric, a one-sided limit 
Q(t, /—) exists for each t. 
Proof. Let r < t1 < t2 <-• • • and tn -* t. The sequence of distributions associated 
with ?}(t, /„) is stochastically bounded and so there exists a convergent subsequence. 
Dropping double subscripts we may suppose that ?}(t, /„) -*¦ U where U is associated 
with a proper distribution U. It follows easily thaf ?}(/„, tn+1) ~* 1 and this implies 
&('n> sn) — 1 for any sequence of epochs such that /„ < sn < tn+v In view of (9.5) this 
means that Q(t, sn) -* U, and so the limit U is independent of the sequence {/„}, and 
the lemma is proved. >- 
Following Paul Levy, we say that a. fixed discontinuity occurs at / if the two limits 
Q(t, / + ) and Q(t, —1) are different. It follows readily from theorem 2 that the set of 
fixed discontinuities is countable. Using symmetrization it follows also in the general case 
that, except for at most denumerably many epochs, discontinuities are due only to centering 
(and are removable by an adequate centering). The contribution &d(r, t) of all fixed 
discontinuities to Q(t, /) is an infinite convolution and it is possible to decompose the 
process into a discrete and a continuous part. For the triangular arrays arising from con- 
tinuous processes it is not difficult to see (using theorem 2) that the uniformity condition 
(9.1) is automatically satisfied and we-reach the conclusion that the distributions associated 
with continuous processesMre infinitely divisible. P. Levy has shown that the sample function 
of such processes are well-behaved in the sense that with probability one right and left 
limits exist at every epoch /. 
10. PROBLEMS FOR SOLUTION 
1. In Example 1 (a) show that ]? X? n—>1 as n — qo! (Hint: Use variances.) 
2. In an ordinary symmetric random walk let T be the epoch of the first 
passage through +1. In other words, T is a random variable such that 
Consider a triangular array in which Xkn has the same distribution as T//r- 
Using the elementary methods of section 1 show by direct calculation that 
00 u(x - y) — u(x) 
7=T dy- 
1 Z* 
2v J 
Conclude that the distribution of the row sums tends to the stable distribution 
Fx defined in 11,D.7) with the convolution property 11,D.9). Interpret the result 
IX. 10 PROBLEMS FOR SOLUTION 319 
in terms of a random walk in which the steps are ±ljn and the times between 
successive steps l/n2. 
[Hint: The series defining ^nu(x) can be approximated by an integral.] 
3. Consider the gamma distributions of 11,B.2) for the parameter value a = 1. 
Show that the convolution property 11,B.3) implies that they form a semi-group 
with a generator given by 
„ , , f °° u(x-y) - u(x) 
Uu(x) = e-vdy. 
jo y 
Discuss the absence of a centering term. 
4. The one-sided'stable distributions of 11,D.7) enjoy the convolution property 
11,D.9) and therefore form a semi-group. Show that the generator is given by the 
right side of (*) in problem 2. 
5. Let the distributions Qt of a semi-group be concentrated on the integers 
and denote the weight of k by qk(t). Show that 
Uu(x) - -q'@) u(x) + 2Y@) u(x - k). 
Compare with the canonical form E.9). Interpret in this light the generating 
functions for infinitely divisible distributions obtained in 1; XII-,2. 
6. Generalize the notions of section 2 to semi-groups with defective distributions. 
Show that if U generates {?}(/)} then U — c\ generates {<?-<*?}(/)}. 
7. The notation e'(8f-D for the compound Poisson semi-group tempts one to 
write in general ?1@ = ^u. For the normal semi-group this leads to the formal 
operational equation 
Show that it is valid whenever the Taylor series of u converges for all x and the 
series on the right converges for t > 0. (Hint: start from the Taylor series for the 
convolution of u and a normal distribution. Use the moments of the normal 
distribution.) 
8. The distributions of a semi-group have finite expectations iff -j r-- is 
integrable with respect to the measure Q appearing in the generator. + 'x' 
9. Show directly that if n[%n— 1] -*U, the operator U is necessarily of the 
form of a generator. [Use the method of section 4 considering functions of the 
form D.4) but do not use semi-group theory. The intention is to derive the general 
form of a generator without first proving its existence.] 
10. Let Fk attach probabilities \ to the two points ±nk. Then 
generates a semi-group such that Q2t(x) = Qt(xM), but Qt is not stable. (P. Levy.) 
11. (A direct proof that the limit distributions of row sums of triangular arrays 
are infinitely divisible.) Let {Xkn} be a triangular array with identically distributed 
variables. The stochastic boundedness of Sn implies the same for the partial sums 
xi.n + "'" + Xm „, where m stands for the largest integer <Lnjr. Using the 
320 INFINITELY DIVISIBLE DISTRIBUTIONS AND SEMI-GROUPS IX. 10 
selection theorem show that this implies that the limit distribution G of Sn is 
the /--fold convolution of a distribution Gr. 
12. For any distribution F and smooth u 
C + oo X2 
11C - i)«ll ? ioo(ii«n + ||«"||) TT72F{dx} + llu'1 
J —00 
13. To the triangular array {Xk „} with distributions Fn there corresponds 
another array {XJ#n} with compound Poisson distributions 
Show that n[$n —gff] -*Q whenever {Sn} is stochastically bounded. This shows 
that the row sums Sn and S # are asymptotically equivalent. Since the distribution 
of S # is associated with <?ln5n- M this yields a second method for deriving the main 
theorems of section 7. This method can be used also for arrays with variable 
distributions. {Hint; Use problem 12.) 
14. With the notations of section 5 put Mn = max [Xln, . . . , XnJ. If Sw 
has a limit distribution show that y>+(x) = -lim log P{Mn < x). 
15. If Sn has a limit distribution so do the row sums of the array formed by 
the squares X? n. 
16. {Domains of attraction.) Let F belong to the domain of attraction of a 
stable distribution with index a. Using theorem 2 of VIII,9 show that F possesses 
absolute moments of all orders <a. If a < 2 no moments of order >a exist. 
The last statement is false when a = 2. 
17. {Continuation.) In section 8 the theory was based on the truncated second 
moment function, but this was done only for reasons of tradition. Theorem 2 of 
VIII,9 permits us to replace y2 in (8.1) by \y\p with other exponents p, and for 
each p to replace (8.3) and (8.5) by equivalent relations. 
18. Let Xj, X2, ... be independent variables with a common distribution F. 
If 1 — F{x) + F{ —x) varies slowly, deduce from the compactness lemma that 
a sequence SnJak + bk can have no proper limit distribution G except G 
concentrated at a point. (This may be expressed by saying that F belongs to no 
domain of partial attraction. See XVII ,9.) Hint: Use symmetrization. 
CHAPTER X 
Markov Processes and Semi-Groups 
This chapter starts out with an elementary survey of the most common 
types of Markov processes—or rather, of the basic equations governing 
their transition probabilities. From this we pass to Bochner's notion of 
subordination of processes and to the treatment of Markov processes by 
semi-groups. The so-called exponential formula of semi-group theory is the 
connecting link between these topics. The existence of generators will be 
proved only in chapter XIII by the theory of resolvents. In theory the present 
exposition might have covered the processes and semi-groups of the preceding 
chapter as a special case, but the methods and uses are so different that the 
following theory is self-contained and independent of chapter IX. The results 
will be amplified in chapter XIII, but the theory of Markov processes is not 
used for the remaining topics in this book. 
This chapter is largely in the nature of a survey, and no attempt is made at 
either generality or completeness.1 Specifically, we shall not discuss properties 
of the sample functions, and throughout this chapter the existence of the 
processes will be taken for granted. Our interest centers entirely on the 
analytical properties of the transition probabilities and of the defining 
operators. 
The theory of the induced semi-groups of transformations wili be treated 
in fair generality in sections 8-9. In the earlier sections the basic space is an 
interval on the line or the whole line although parts of the theory apply more 
generally. To avoid special symbols it is therefore agreed that when no 
limits are indicated integrals are taken over a fixed set Q serving as the basic 
space. 
1 The semi-group treatment of Markov processes is described in greater detail in Dynkin 
A965) and Loeve A963). Yosida A966) contains a succinct introduction to the analytic 
theory of semi-groups and their applications to diffusion and to ergodic theory. 
321 
322 MARKOV PROCESSES AND SEMI-GROUPS X.I 
1. THE PSEUDO-POISSON TYPE 
Throughout this chapter we limit our discussion to Markov processes with 
stationary transition probabilities Qt defined by 
A.1) Qt(*, H = P{X(r+r) e r | X(t) = x} 
and supposed to be independent of t. (See VI,11.) 
A simple generalization of the compound Poisson process leads to an 
important class of such processes from which all others can be derived by 
approximation. The theory of semi-groups hinges on an analytical counter- 
part to this situation (section 10). 
Let N(/) denote the variable of an ordinary Poisson process. In VI,4 
the compound Poisson process was introduced by considering the random 
sums SN(t) where So, Sx,. . . are the partial sums of a sequence of independ- 
ent identically distributed random variables. The pseudo-Poisson process is 
defined in like manner except that now So, Sx, . . . are the variables of 
Markov chain with transition probabilities given by a stochastic kernel K 
(see VI, 11). The variables X(r) = SN(j) define a new stochastic process 
which can be described formally as follows. 
Between the jumps of the Poisson process the typical sample path remains 
constant. A transition from x to T can occur in 0, 1,2, .. . steps, and hence 
A.2) Qt(x, V) = e-at f (-^ KM(x, T), t > 0. 
n=0 
This generalizes the compound Poisson distribution VI,D.2) and reduces to 
it in the special case when Q is the whole line and Sn is. the sum of n 
independent random variables with a common distribution jF. 
The composition rule 
A-3) Qt+r(z> T) = (&(*, dy) QT(y, T) 
(/, t > 0) analogous to VI,D.1) is easily verified analytically.2 It is called 
the Chapman-Kolmogorov equation and states that a transition from x at 
epoch 0 to F at epoch t + r occurs via a point y at epoch r and that the 
subsequent change is independent of the past.3 [See 1; XVII,9 and also 
Examples, (a) Particles under collision. Let a particle travel at uniform 
speed through homogeneous matter occasionally scoring a collision. Each 
2 Approximating Qt and QT by their partial sums with n terms shows that the right 
side in A.3) is <, the left side but ;> the nth partial sum of Qi+T. 
3 It is sometimes claimed that A.3) is a law either of nature or of compound prob- 
abilities, but it is not true for non-Markovian processes. See 1; XV,13. 
X.I THE PSEUDO-POISSON TYPE 323 
collision produces a change of energy regulated by a stochastic kernel K. 
The transition probabilities for the energy X(/) are of the form A.2) if 
the number of collisions obeys a Poisson process. This will be the case 
under the now familiar assumptions concerning homogeneity of space and 
lack of memory. 
It is usually assumed that the fraction of energy lost at each collision is 
independent of the initial amount, which means that K(x, dy) = V{dy\x) 
where K is a probability distribution concentrated on 0,1. For later 
applications we consider the special case where V(x) = x*. Then K has a 
density given by 
A.4) k(x, y) = AaTV~\ 0 < y < x. 
For X = 1 this implies that the fraction of energy lost is uniformly dis- 
tributed.* The iterated kernels &(n) were calculated in VI,A1.5). Sub- 
stituting into A.2) it is seen that Qt has an atom of weight e~"at at the origin 
(accounting for the event of no collision) and for 0 < y. < x the density 
A-5) qt(x, y) = e-'tyffat f Ix{2j<xtX log (x/y) ) 
zVlog (x[y) 
where Ix is the Bessel function defined in 11,G.1). [See examples 2(a) and 
(b) The energy loss of fast particles by ipnization.5 An instructive 
variant of the last example is obtained by considering the extreme case of a 
particle whose energy may be considered infinitely large. The energy losses 
at successive collisions are then independent random variables with a common 
distribution V concentrated on 0, oo. If X(/) is the total energy loss 
within the time interval 0,/ then X(/) is the variable of a compound Poisson 
process. Its transition probabilities are given by A.2) with K{n) replaced by 
the convolutions Vn* 
(c) Changes in direction. Instead of the energy of a particle, we may 
consider the direction in which it travels and derive a model analogous to 
example (a). The main difference is that a direction in 3i3 is determined by 
two variables, and so the density kernel k now depends on four real 
variables. 
4 This assumption is used by W. Heitler and L. Janossy, Absorption of meson producing 
nucleons, Proc. Physical Soc, Series A, vol. 62 A949) pp. 374-385, where the Fokker- 
Planck equation A.8) is derived (but not solved). 
5 Title of a paper by L. Landau, J. Physics, USSR, vol. 8 A944) pp. 201-205. Landau 
uses a different terminology, but his assumptions are identical with ours and he derives 
the forward equation A.6). 
324 MARKOV PROCESSES AND SEMI-GROUPS X.2 
(d) The randomized random walk of example 11,7F) represents a pseudo- 
Poisson process restricted to the integers. For a fixed integer x the kernel 
K attributes weight \ to the two points x ± 1. ^ 
From A.2) one gets easily 
A.6) dQt(? FJ = -<*&(*, T) + <z(Qt(x, dz) K(z, T). 
This is Kolmogorov' s forward equation which will be discussed in a more 
general setting in section 3 where it will be shown in the next section that A.2) 
is its only solution satisfying the obvious probabilistic requirements. The 
equation A.6) takes on a more familiar form when K has a density k. At the 
point x the distribution Qt has an atom of weight e~at, which is the prob- 
ability of no change; except for this atom Qt has a density qt satisfying 
the equation 
A.6a) dJj&Jl = ^q^x, |) + Jqt(x, z) k(z, $) dz. 
If ju0 is the probability distribution at epoch 0 the distribution at epoch 
t is given by 
A,7) 
and A.6) implies 
A.8) B-^JP- = -m{F} + aj^{dz} K(z, 
This version of A.6) is known to physicists as the Fokker-Planck (or 
continuity) equation. Its nature will be analyzed in section 3. When K and 
the initial distribution /u0 have densities, then jut also has a density mt, 
and the Fokker-Planck equation reduces to 
A.8a) ^p = -amt(?) + a (mt(z) k(z, ?) dz 
2. A VARIANT: LINEAR INCREMENTS 
A simple variant of our process occurs in physics, queuing theory, and 
other applications. The assumptions concerning the jumps remain the 
same but between jumps X(t) varies linearly at a rate c. This means that 
X(/) — ct is the variable of the described pseudo-Poisson process; if 
Qt stands for the transition probabilities of the new process, then 
Qt(x, F + ct) must satisfy A.6). The resulting equation for Qt is of an 
X.2 a variant: linear increments 325 
unfamiliar form, but if differentiate densities exist they satisfy familiar 
equations. For mt we have to replace ? in A.80) by ? -f ct. With the 
change of variables y — ? + ct We get the Fokker-Planck equation.6 
dmt(y) dmt(y) C 
B.1) dt = -c -^ ccmt(y) + aj mt(z) fc(z, y) dz. 
The analogue to A.6a) is obtained similarly by adding the term — c dqjdy 
to the right side. 
In connection with semi-group theory we shall cast the Fokker-Planck 
equation in a more flexible form quite independent of the unnatural differ- 
entiability conditions. [See example 10F).] 
Examples, (a) Particles under collision. In the physical literature example 
l(fl) occurs usually in a modified form where it is assumed that between 
collisions energy is dissipated at a constant rate dye to absorption or friction. 
The model of B.1) fits this situation if the energy loss is proportional with mt 
standing for the probability density of the'energy at epoch /. 
In other situations physicists assume that between collisions energy is 
dissipated at a rate proportional to the instantaneous energy. In this case 
the logarithm of the energy decreases at a constant rate and an equation of 
the form B.1) now governs the probability density for the logarithm of the 
energy. 
(b) Stellar radiation.7 In this model the variable / stands for distance 
and X(/) for the intensity of a light ray traveling through space. It is 
assumed that (within the equatorial plane) each element of volume radiates 
at a constant rate and hence X(/) increases linearly. But the space also 
contains absorbing dark clouds which we treat as a Poissonian ensemble of 
points. On meeting a cloud each ray experiences a chance-determined loss 
and we have the exact situation that led us to B.1). It is plausible (and it 
6 Many special cases of the Fokker-Planck equation A.8) have been discovered in- 
dependently, and much fuss has been made about the generalization B.1). The general 
notion of Fokker-Planck equations was developed by Kolmogorov in his celebrated 
paper, Ober die analytischen Methoden in der Wahrscheinlichkeitsrechnung, Math. Ann., 
vol. 104 A931) pp. 415-458. In it Kolmogorov mentions the possibility of adding an 
arbitrary diffusion term 
dzmt dmt 
yC 
to the right side, and B.1) is merely a special case of this. Even the first existence theorems 
covered the general equation in the non-stationary case. [Feller, Math. Ann., vol. 113 
A936).] 
7 The physical assumptions are taken from V. A. Ambarzumian, On the brightness 
fluctuations in the Milky Way, Doklady Akad. Nauk SSSR, vol. 44 A944) pp. 223-226, 
where a version of B.1) is derived by an indirect approach. 
326 MARKOV PROCESSES AND SEMI-GROUPS X.3 
can be proved) that the density mt of X(/) approaches a steady state density 
m which is independent of / and satisfies B.1) with the left side replaced by 0. 
Ambarzumian assumes specifically that the loss of intensity at an individual 
passage through a cloud is regulated by the transition kernel A.4). In this 
case an explicit solution is available in A.5) but it is of minor interest. More 
important (and easily verified) is the fact that B.1) has a time-independent 
(or steady state) solution, namely the gamma density 
- 
c) 
This result shows that pertinent information can be derived from B.1) 
even without rinding explicit solutions. For example, it is readily verified 
by direct integration that the steady state solution has expectation 
c[a(l — /lc)]-1 where /lc is the expectation of the absorption distribution V. 
(c) The ruin problems of VI,5 represent the special case where & is a 
convolution kernel. The variable of these processes is obtained by adding 
— ct to the variable of a compound Poisson process. Analogous ruin 
problems can be formulated for arbitrary pseudo-Poissonian processes, 
and they lead to B.1). > 
3. JUMP PROCESSES 
In the pseudo-Poisson process the waiting time to the next jump has a 
fixed exponential distribution with expectation I/a. A natural generalization 
consists in permitting this distribution to depend on the present value of 
the path function X(/). [In example \{a) this amounts to assuming that 
the probability of scoring a hit depends on the energy of the particle.] The 
Markovian character of the process requires that the distribution be expon- 
ential, but its expectation can depend on the present value of X(/). Accord- 
ingly, we start from the following 
Basic postulates. Given that X(t) — x, the waiting time to the next jump 
has an exponential distribution with expectation l/a(x) and is independent 
of the past history. The probability that the following jump leads to a point in 
V equals K(x,T). 
In analytical terms these postulates lead to an integral equation for the 
transition probabilities QtQc, F) of the process (assuming that such a 
process does in fact e*ist). Consider a fixed point x and a fixed set F not 
containing x. The event (X(/) e F} cannot occur unless the first jump 
from x has occurred at some epoch s < /. Given this, the conditional 
probability of (X(/) e F} is obtained by integrating K(x, dy)Qt_?y, F) over 
the set Q. of all possible y. Now the epoch of the first jump is a random 
X.3 JUMP PROCESSES 327 
variable with exponential density aL(z)er*w. Integrating with respect to 
it we get the probability of {X(t) e T} in the form 
C.1a) Qt(x, T) = a(x) [*-<•>• ds f K{x, dy) Qt_s(y, F). 
Jo Jn 
For a set F containing x we must add the probability that no jump occurs 
before / and thus we get 
C.1b) Qt(x, T) = e-*ixU + a(x) P<Ta(a:)Vs f K(x, dy) Qt_s(y, T). 
Jo Jn 
These two equations, valid for x ?T and x eT, respectively, are the 
analytic equivalent of the basic postulates. They simplify by the change of 
the variable of integration to r = t — s. Differentiation with respect to t 
then reduces the two equations to the same form, and the pair is replaced 
by the single integro-differential equation 
C.2) ^ \ 
at Jn 
This is Kolmogorov's backward equation, which serves as point of departure 
for the analytical development because it avoids the annoyance of 
distinguishing between two cases. 
The backward equation C.2) admits of a simple intuitive interpretation 
which may serve to reformulate the basic postulates in more practical terms. 
In terms of difference ratios C,2) is equivalent to 
C.3) Qt+h(x, D = [1 -oi(x)h] Qt(x, T) + <x(x)h f K(x, dy) Qt(y, T) + o(h). 
Jn 
For an intuitive interpretation of this relation consider the change within 
the time interval 0, t+h as the result of the change within the initial short 
interval 0, h and the subsequent interval h, t+h of duration t. Evidently 
then C.3) states that if X@) = x, the probability of one jump within 0, h 
is <x(x)h + o(h); and the probability of more than one jump is o(h); 
finally, if a jump does occur within 0, h, the conditional probabilities of 
the possible transitions are given by K(x, dy). These three postulates lead to 
C.3) and hence to C.2). In essence they repeat the basic postulates.8 
From a probabilistic point of view the backward equation is somewhat 
artificial inasmuch as in it the terminal state V plays the role of a parameter, 
8 The differentiability of Qt with respect to / and the fact that the probability of more 
than one jump is o(h) are now stated as new postulates, whereas they are implied by the 
original more sophisticated formulation. 
328 MARKOV PROCESSES AND SEMI-GROUPS X.3 
and C.2) describes the dependence of Qt(x, F) on the initial position x. 
Offhand it would seem more natural to derive an equation for Qt+h by 
splitting up the interval fr, t+h into a long initial interval 0, / and the 
short terminal interval /, t+h. Instead of C.3) we get then formally 
C.4) Qt+h(x, F) = f Qt(x, dz)[l -<x(z)h) + ( Qt(x, dz) <x(z)h K(z, F) + o{h) 
d h 
and hence 
C.5) BQt(*' F) = - f Qt(x, dz) a(z) + f Qt(x, dz) a(z) K(z, F). 
at Jr Jn 
This is Kolmogorov's forward equation (in special cases known to physicists 
as the continuity or Fokker-Planck equation). It reduces to A.6) when a is 
independent of z. 
The formal character of the derivation was emphasized because the 
forward equation is really not implied by our basic postulates. This is 
because the term o(h) in C.3) depends on z and since z appears as variable 
of integration in C.4), the term o(h) should have appeared under the 
integral sign. But then the problem arises as to whether the integrals in 
C.5) converge and whether the passage to the limit h —*- 0 is legitimate. 
[No such problems occurred in connection with the backward equation 
because the initial value x was fixed and the integral in C.2) exists in con- 
sequence of the boundedness of Qt.] 
It is possible to justify the forward equation by adding to our basic 
postulates an appropriate condition on the error term in C.3), but such a 
derivation would lose its intuitive appeal and, besides, it seems impossible 
to formulate conditions which will cover all typical cases occurring in 
practice. Once unbounded functions a are admitted, the existence of the 
integrals in C.5) is in doubt and the equation cannot be justified a priori. 
On the other hand, the backward equation is a necessary consequence of 
the basic assumptions, and it is therefore best to use it as a starting point 
and to investigate the extent to which the forward equation can be derived 
from it. 
A solution of the backward equation is easily constructed using successive 
approximations with a simple probabilistic significance. Denote by 
Q\n)(x, F) the probability of a transition from X@) = x to X(/) e F with 
at most n jumps. A transition without jumps is possible only if xeT, 
and since the sojourn time at x has an exponential distribution we have 
C.6) Q{t°\x, F) = e-*ix)t Ki0)(x, F) 
(where Ki0)(x, F) equals 1 or 0 according as x is, or is not, contained 
in F). Suppose next that the first jump occurs at epochs s < t and leads 
X.3 jump processes 329 
from x to y. Summing over all possible s and y we get [as in C.1)] the 
recursion formula 
C.7) Q\n+1)(x, F) = Q?>(z, F) + jV«(*>'a(*) ds(K(x, dy) Q^t(y, F) 
valid for n — 0, 1,... . Obviously Q(t0) <, Q1^ and hence by induction 
It follows that for every pair x, F, the limit 
C.8) Ql^ix, T) = lim «2in)(z, F) 
n-»oo 
exists, but conceivably it could be infinite. We show that actually 
C.9) Q^XxtCl)^! 
[which implies Q\m\x, F) <, 1 for all sets in Q]. It suffices fe prove that 
C.10) Q\n\x 
for all n. This is trivially true for n = 0 and we proceed by. induction: 
Assuming C.10) for some fixed n we get from C.7) (recalling that K is 
stochastic) -. 
C.il) . Q\n+1\x, Q) ^ e-aix)t +. fVa(*)fa(a:) ds = 1, 
Jo 
and hence C.10) is true for all «. 
From C.7) it follows by monotone convergence that Q^ satisfies the 
backward equations in the original integral version C.1) [and hence also in 
the integro-differential version C.2)]. For any other positive solution Qt of 
C.1) it is clear that Qt ^ Q\o); comparing C.1) with C.7) we conclude that 
Qt > Q\n) for all n, and hence Qt ^ Q{*K For this reason Q^ is called 
the minimal solution of the backward equations; C.9) shows that Q}^1 is 
stochastic or substochastic. 
Tt follows from C.8) that Q\x){x, F) is the probability of a passage from 
x to F in finitely many steps. Accordingly, with a substochastic solution 
the defect 1 — Q^ix, Q) represents the probability that, from x as 
starting point, infinitely many jumps will occur within time /. We know 
from 1; XVII,4 and example VIII;5(c) that this phenomenon occurs in 
certain pure birth processes, and hence substochastic solutions exist. But 
they are the exception rather than the rule. In particular, if the coefficient 
a.(x) is bounded the minimal solution is strictly stochastic, that is, 
C.12) Ql^ix, Q) =1, t > 0. 
Indeed, if on(x) < a < oo for all x we show by induction that 
C.13) Q(tn)(x 
330 MARKOV PROCESSES AND SEMI-GROUPS X.3 
for all n and / > 0. This is trivially true for n = 0. Assume C.12) for 
some n, and note that the right side is a decreasing function, say /(/). 
Consider C.7) with F = Q.. In consequence of C.13) the inner integral is 
^>/@ which does not depend on the variable of integration. Integrating 
a(x)e~a(x)s it is then seen that C.13) holds with n replaced by n + 1. 
Finally we note that in the strictly stochastic case C.12) the solution Q[co) 
is unique. In fact, because of the minimal character of Q\*>) any othei 
acceptable solution would satisfy 
1 > Qt(x, Cl) = Qt(x, F) + Qt(x, Q - F) 
C-14) > Q(r\x, F) + $«>(*, Q - F) = Q\*\x, Cl) = 1 
which is impossible unless the equality sign prevails in both places. We have 
thus proved the 
Theorem. The backward equations admit of a minimal solution Q\K>) 
defined by C.8) and corresponding to a process in which transitions from x to 
F occur only with finitely many jumps. It is stochastic or substochastic. 
In the substochastic case the defect 1 — Q[K)(x, Q.) accounts for the prob- 
ability of the event that infinitely many jumps occur within time /, in which 
case the minimal process terminates. 
In the strictly stochastic case C.12) the minimal solution is the unique 
probabilistic solution of the backward equation. This case arises whenever the 
coefficient a(x) is bounded. 
The discovery of defective solutions came as a shocking surprise during 
the early stages of the theory in the 193O's, but it has given impetus to 
research leading to a unified theory of Markov processes. Processes i» which 
transitions from x to F are possible after the occurrence of infinitely many 
jumps are the analogue to diffusion processes with boundary conditions 
and therefore not as pathological as they appeared at the beginning. The 
possibility of infinitely many jumps also explains the difficulties in deriving 
the forward equations directly.9 The backward equations were derived from 
the assumption that, given the present state x, the next jump occurs after an 
exponentially distributed waiting time with expectation l/oc(x). The forward 
equations depend on the state just prior to epoch /, and therefore depend 
on the whole space Q. In particular, it is not easy to express directly the 
requirement that there exist a last jump prior to epoch /. 
However, it will be shown in the Appendix that the minimal solution ?<0O) 
automatically satisfies the forward equations and is minimal also for the latter. 
It follows, in particular, that if a is bounded, Q\m) represents the unique 
Compare the analogous discussion in 1; XVII,9. 
X.3 JUMP PROCESSES 331 
solution of the forward equation. When Q{tm) is substochastic there exist 
various processes involving transitions through infinitely many jumps and 
satisfying the backward equations. The transition probabilities Qt of such 
processes may, but need not, satisfy the forward equations. This surprising 
fact shows that the forward equations may be satisfied in situations where 
the derivation from C.4) breaks down. 
In conclusion we note again that (in contrast to the process in section 2 
and to diffusion processes involving derivatives with respect to x) the pure 
jump process does not depend on the nature of the underlying space: our 
formulas apply to any set Q on which a stochastic kernel K is defined. 
Example. Denumerable sample spaces. If the random variables X(t) 
are positive and integral-valued the underlying sample space consists of 
the integers 1, 2, .... It suffices now to know the transition probabilities 
Pik@ from one integer to another; all other transition probabilities are 
obtained by summation over k. The theory of Markovian processes on the 
integers was outlined in 1; XVII,9 where, however, also non-stationary 
transition probabilities were considered. To restrict that theory to the 
stationary case the coefficients ct and probabilities pik must be assumed 
independent of /. The assumptions are then identical with the present ones 
and the two systems of Kolmogorov equations derived in 1; XVII,9 are 
easily seen to be the special cases of C.2) and C.5) [replacing a(i) by ct 
and K(i,J) by p{j\. The divergent birth process of 1; XVII,4 is an example 
of a process with infinitely many jumps within a finite time interval. We 
shall return to this process in XIV,7 to present the possibility of a passage 
from i to j involving infinitely many jumps. > 
Appendix.10 Ths minimal solution for the forward equation. The construction of the minimal 
solution Qifx) for the backward equation can be adapted to the forward equation. We 
indicate briefly how this can be done and how one can verify that the two solutions are 
in fact identical. Details will be left to the reader. 
Let 
Jr 
C.15) 
* Jr 
To construct a solution of the forward equation we define Q\o) by C.6) and put 
C.16) <2<*+1) (*, r) = Q[°\x, T) + ITg&Cz, dy) a(y) K#(y, T). 
10 In XIV,7 the theory is developed by means of Laplace transforms. (For simplicity 
only countable spaces are treated, but the argument applies generally without essential 
change.) The direct method of the text is less elegant, but has the advantage that it applies 
also to non-stationary processes with transition probabilities depending on the time param- 
eter. In this general form the theory was developed by Feller, Trans. Amer. Math. Soc. 
vol. 48 A940) pp. 488-515 [erratum vol. 58, p. 474]. 
332 MARKOV PROCESSES AND SEMI-GROUPS X.4 
This defines the probabilities for transitions in at most n + 1 steps in terms of the last 
jump, just as C.7) refers to the first jump. Repeating the proof of the last theorem it is 
seen that Qitco) = Iim Q\n) is the minimal solution of the forward equations. 
Although we used the same letters, the two recursion formulas C.7) and C.16) are 
independent, and it is by no means clear that the resulting kernels are identical. To show 
that this is so we put P\n) = Q[n) — Q{tn~1], which corresponds to transitions in exactly 
n steps. Then C.16) reduces to 
t 
C-17) P\»+l)(z, T) = f |j»t(»J(*, dy) <x{y) K#tyt T). 
o 
We indicate this by the shorthand notation i><n+i> = i»<n)$I. For the recursion formula 
C.7) we write similarly P^n+l) = 5Bi»<n). The starting i»<0) is the same in either case 
[defined by C.6)]. We now prove that the two recursion formulas lead to the same result. 
More precisely: the P[n) defined by i><»+i> =i>(«>2t satisfy also i»<n+1) = 93i»jn). We 
proceed by induction. Assume the assertion to be true for all n ^ r. Then 
and thus the induction hypothesis holds also for n = r + 1. 
We have thus proved that the minimal solution is common to the backward and forward 
equations. 
4. DIFFUSION PROCESSES IN ft1 
Having considered processes in which all changes occur by jumps we turn 
to the other extreme where the sample functions are (with probability 
one) continuous. Their theory is parallel to that developed in the last 
section, but the basic equations require more sophisticated analysis. We 
shall therefore be satisfied with a derivation of the backward equation and 
with a brief summary concerning the minimal solution and other problems. 
The prototype for diffusion processes is the Brownian motion (or Wiener 
process). This is the process with independent normally distributed incre- 
ments. Its transition probabilities have densities qt(xt y) given by the normal 
density with expectation x and variance at, where a > 0 is a constant. 
These densities satisfy the standard diffusion equation, 
tA ,v dg*(*» y) 1 
D.1) —— = - a 
2 a 
-2 
It will now be shown that other transition probabilities are governed by 
related partial differential equations. The object of this derivation is merely 
to give an idea concerning the types of processes and the problems involved 
and thus to serve as a first introduction; for this reason we shall not strive at 
generality or completeness. 
From the nature of the normal distribution it is evident that in Brownian 
motion the increments during a short time interval of duration t have the 
X.4 DIFFUSION PROCESSES IN ft1 333 
following properties: (i) for fixed d > 0 the probability of a displacement 
exceeding d is o(t); (ii) the expected value of the displacement is zero; 
(iii) its variance is at. We retain the first condition, but adapt the others 
to an inhomogeneous medium; that is, we let a depend on x and permit a 
non-zero mean, displacement. Under such circumstances the expectation 
and the variance of the displacement will not be strictJy proportional to 
/ and we can postulate only that given X(t) = x the displacement 
X(/+t) — X(t) has an expectation b(x)t + o(t) and variance a(x) + o(t). 
Moments do not necessarily exist, but in view of the first condition it is 
natural to consider truncated moments. These considerations lead us to the 
following 
Postulates11 for the transition probabilities Qt. For every d > 0 as t -> 0 
D.2) r1 f Qt(x, dy) -> 0 
J\v-x\>6 
D.3) r1!" (y-x)Qt(x,dy)-+b(x) 
J\v—x]<d 
D-4) rl\ (y-xJQt(x>dy)-+a(x). 
Note that if D.2) holds for all d > 0, the asymptotic behavior of the 
quantities in D.3) and D.4) is independent of d; it is then permissible in 
the last two relations to replace d by 1. 
The first condition makes large displacements improbable and was 
introduced in 1936 in the hope that it is necessary and sufficient for the 
continuity of the sample functions.12 It was named in honor of Lindeberg 
because of its similarity to his condition in the central limit theorem. It can 
be shown that under mild regularity conditions on the transition probabilities 
the existence of the limits in D.3) and D.4) is really a consequence of D.2). 
We shall not discuss such details because we are not at this juncture interested 
in developing a systematic theory.13 Our modest aim is to explain the nature 
11 The original derivation of D.1) from probabilistic assumptions is due to Einstein. 
The first systematic derivation of the backward equation D.6) and forward equation E.2) 
was given in Kolmogorov's famous paper of 1931 (see section 2). The improved postulates 
of the text are due to Feller A936), who gave the first existence proof and investigated the 
relation between the two equations. 
12 This conjecture was verified by D. Ray. 
13 Modern semi-group theory enabled the author to derive the most general backward 
equation (generator) for Markov processes satisfying a Lindeberg type condition. The 
classical differential operators are replaced by a modernized version, in which a "natural 
scale" takes over the role of the coefficient b^ and a "speed measure" the role of a. The 
study of such processes was the object of fruitful research by E. B. Dynkin and his school on 
one hand, by K. Ito and H. P. McKean on the other. The whoie theory is developed 
in the books by these authors quoted in the bibliography. 
334 MARKOV PROCESSES AND SEMI-GROUPS X.4 
and the empirical meaning of the diffusion equations in the simplest situation. 
For this purpose we show how certain differential equations can be derived 
formally from D.2)-D.4), but we shall not discuss under what conditions 
there exist solutions to these equations.14 The coefficients a and b may be 
therefore assumed as bounded continuous functions and a(x) > 0. 
We take as our basic space a finite or infinite interval / on the line and 
continue the convention that when no limits are indicated, the integration 
is over the interval /. To simplify writing and to prepare for the applications 
of semi-group theory we introduce the transformations. 
D.5) u(t, z) = ht(x, dy) uo(y) 
changing (for fixed t) a bounded continuous "initial function" u0 into a 
function15 with values u(t, x). 
Clearly the knowledge of the left side in D.5) for all initial uQ uniquely 
determines Qt. It will now be shown that under mild regularity conditions 
u must satisfy the backward equation 
,. , du 1 d2u , du 
D.6) — = - a h b — , 
Bt 2 dx2 dx 
generalizing the standard diffusion equation D.1). We seek a function u 
satisfying it and such that u(t, x) —*- uo(x) as t -> 0. In case of uniqueness 
this solution is necessarily of the form D.5) and Qt is called the Green 
function of the equation. Cases of non-uniqueness will be discussed in the 
next section. 
To derive the backward equation D.6) we start from the identity 
D.7) u{s+1, x) = (qs(x, dy) u(t, y), s, t > 0 
x) = (qs(x 
which is an immediate consequence, of the Chapman-Kolmogorov equation 
A.3). From it we get for h > 0 
D.8) "V't»,~, ^,~, = ± Qk(^ dy)[u{tf y)_u{u x)] 
h h J 
We now suppose that the transition probabilities Qt are sufficiently regular 
to ensure that in D.5) the transform u has two bounded continuous deriva- 
tives with respect to x, at least when u0 is infinitely differentiate. To given 
14 For the treatment of diffusion equations by Laplace transforms see XIV,5. 
15 In terms of the stochastic process; u(t, z) is the conditional expectation of wo(X(f)) 
on the hypothesis that X@) = x. 
X.4 DIFFUSION PROCESSES IN 311 335 
€ > 0 and fixed x there corresponds then by Taylor's formula a d > 0 
such that 
D.9) 
, X / \ , » du(t, X) 
u(t, y) - u(t, x) - (y-x) -\—-' 
< e\y-x\ 
for all |y — x\ < d. With this d consider in D.8) separately the contribu- 
tions of the domains \y — x\ > 6 and \y — x\ < 6. The former tends to 
0 in consequence of D.2) and of the boundedness of u. Owing to the 
conditions D.3) and D.4) it is clear from D.9) that for sufficiently small h 
the contribution of \y — x\ <? 6 differs from the right.side in D.6) by less 
than € ¦ a(x). Since e is arbitrary, this means that as h -> 0 the right side 
in D.8) tends to that of D.6). Accordingly, at ieast a right-sfided derivative 
dujdt exists and is given by D.6). The principal result of the theory may be 
summarized roughly as follows. If the transition probabilities of a Markov 
process satisfy the continuity condition D.2) the process is determined by the 
two coefficients b and a. This sounds theoretical, but in practical situations 
the coefficients b and a are given a priori from their empirical meaning and 
the nature of the process. 
To explain the meaning of b and a consider the increment X(r-f t) — X(t) 
over a short time interval assuming that X(t) = x. If the moments in D.3) 
and D.4) were complete, this increment would have the conditional expecta- 
tion b(x)t -f o(t) and the conditional variance a(x)t — b2(x)t2 + o{t) = 
= a{x)t 4- o(t). Thus b(x) is a measure for the local average rate of 
displacement (which may be zero for reasons of symmetry), and a{x) for the 
variance. For want of a better word we shall refer to b as the infinitesimal 
velocity (or drift) and a as the infinitesimal variance. 
The following examples illustrate the way in which these coefficients are 
determined in concrete situations. 
Examples, (a) Brownian motion. If the x-axis is assumed homogeneous 
and symmetric, a(x) must be independent of x and b(x) must vanish. We 
are thus led to the classical diffusion equation D.1). 
(b) The Ornstein-Uhlenbeck process is obtained by subjecting the particles 
of a Brownian motion to an elastic force. Analytically this means a drift 
towards the origin of a magnitude proportional to the distance, that is, 
b{x) = px. As this does not affect the infinitesimal variance, a{x) remains 
a constant, say 1. The backward equation takes r>n the form 
D,o) ^ 
dt 2 
336 MARKOV PROCESSES AND SEMI-GROUPS X.4 
It is fortunately easy to solve this equation. Indeed, the change of variables 
v(t, z) = u(t, ze) 
reduces it to 
D.11) e^JL = \ll 
dt 2dz* 
and the further change of variables 
D.12) 
T = 
changes D.11) into the standard diffusion equation D.1). It follows that 
the transition densities qt(z, y) of the Ornstein-Uhlenbeck process coincide 
with the normal density centered at ze~pt and with variance t given by D.1-2). 
It was shown in example III,8(e) that the Ornstein-Uhlenbeck process 
determined by D.10) and an initial normal distribution is the only normal 
Markovian process with stationary transition probabilities. (Brownian 
motion is included as the special case p = 0.) 
(c) Diffusion in genetics. Consider a population with distinct generations 
and a constant size N. (A cornfield represents a typical example.) There 
are 2N genes and each belongs to one of two genotypes. We denote by 
Xn the proportion of genes of type A. If selection advantages and mutations 
are disregarded, the genes in the (/7+l)st generation may be taken as a random 
sample of size 2N of the genes in the nth generation. The Xn process is 
then Markovian, 0 < Xn < 1, and given that Xn = z the distribution of 
2NXn+1 is binomial with mean 2Nz and variance 2Nz{\ —z). The change 
per generation has expectation 0 and variance proportional to z{\ —z). 
Suppose now that we look over a tremendous number of generations and 
introduce a time scale on which the development appears continuous. 
In this approximation we deal with a Markov process whose transition 
probabilities satisfy our basic conditions with b{z) = 0 and a(z) propor- 
tional to z(i — z). The proportionality factor depends on the unit of time 
scale and may be normalized to 1. Then D.6) takes on the form 
DA3) 
ot 
and this time the process is restricted to the finite interval 0, 1. Selection 
and mutation pressures would cause a drift and lead to an equation D.13) 
with a first-order term added. The resulting model is mathematically 
equivalent to the models developed by R. A. Fisher and S. Wright although 
their arguments were of a different nature. The genetical implications are 
somewhat dubious because of the assumption of Constant population size, 
X.5 THE FORWARD EQUATION. BOUNDARY CONDITIONS 337 
the effect of which is not generally appreciated. The correct description15 
depends on an equation in two space variables (gene frequency and population 
size). 
(d) Population growth. We wish to describe the growth of a large population 
in which the individuals are stochastically independent and the reproduction 
rate does not depend on the population size. For a very large population the 
process is approximately continuous, that is, governed by a diffusion 
equation. The independence of the individuals implies that the infinitesimal 
velocity and variance must be proportional to the population size. Thus the 
process is governed by the backward equation D.6} with a*= olx and b = fix. 
The constants a and /? depend on the choice of the units of time and 
population size, and with appropriate units of measurement it is possible to 
achieve that a = 1 and /? = 1, —1, or 0 (depending on the net rate of 
growth). 
In 1; XVII,E.7) the same population growth is described by a discrete 
model. Given X(t) = n it was assumed that the probabilities of the 
contingencies X(/+t) = /7+1, n — I, and n differ from- Ant, /unt, and 
1 — (X+/j)nt, respectively, by terms o{tz), and'so the infinitesimal velocity 
and variance are {X—/x)n and (X+/.i)n. the diffusion process is obtained 
by a simple passage to the limit, and it can be shown that its transition 
probabilities represent the limit of the transition probabilities for the discrete 
model. 
Similar approximations of discrete processes by diffusion processes are 
often practical; the passage from ordinary random walks to diffusion 
processes described in 1; XIV,6 provides a typical example. [Continued: 
in example 5(a).] > 
5. THE FORWARD EQUATION. BOUNDARY CONDITIONS 
In this section we assume for simplicity that the transition probabilities 
Qt have probability densities qt given by a stochastic density kernel 
The transformation D.5) and the ensuing backward equation D.6) describe 
the transition probabilities in their dependence on the initial point x. From 
a probabilistic point of view it appears more natural to keep the initial point 
x fixed and to consider qt{x,y) as a function of the terminal point y. 
From this point of view the transformation D.5) should be replaced by 
E.1) v(s, y) = vo{z) qs(z, y) dx 
15 W. Feller, Proc. Second Berkeley Symposium on Math. Statist, and Probability, 
1951, pp. 227-246. 
338 MARKOV PROCESSES AND SEMI-GROUPS X.5 
Here v0 is an arbitrary probability density. From the stochastic character 
of qs it follows that for arbitrary fixed s > 0 the transform v is again a 
probability density. In other words, whereas the transformation D.5) 
operated on continuous functions, the new transformation changes probability 
densities into new densities. 
In the preceding section we were able by probabilistic arguments to show 
that, the transform D.5) satisfies the backward equation D.6). Even though 
the new transformation is more natural from a probabilistic point of view, 
a similar direct derivation of the forward equation is impossible. However, 
the general theory of adjoint partial differential equations make it plausible 
that (under sufficient regularity conditions) v should satisfy the equation 
17 
,, ~> Ms, y) Id2 d 
E.2) — =-- [a(y) v(s, y)] - — [b(y) v(s, y)). 
os 2 dy By 
17 Here is an informal sketch of the derivation of E.2). From the Chapman-Kolomogorov 
equation A.3) for the transition probabilities it follows that 
v(s, y) u{t, y) dy 
depends only on the sum s + f. Accordingly, 
f dv(s>y) < ^ f , 
We now express dujdt in accordance with the backward equation D.6) and apply the 
obvious integrations by parts to the resulting integral. If R(s, y) stands for the right 
side in E.2) we conclude that (*) equals 
J 
R(s,y)u(t,y) dy 
plus a quantity depending only on the values of u, v and their derivatives at the boundaries 
(or at infinity). Under appropriate conditions these boundary terms may be neglected, 
and in this case the passage to the limit r -»¦ 0 leads to the identity 
If this is to be valid for arbitrary w0 the expression within brackets must vanish, that is 
E.2) must hold. 
This argument is justified in most situations of practical interest and accordingly the 
forward equation E.2) is generally valid. However, in the so-called return processes the 
boundary terms which we have neglected actually play a role. The transition probabilities 
of such processes therefore satisfy the backward equation D.6), but not E.2); the correct 
forward equation is in this case an equation of a different form. 
It is also noteworthy that E.2) is meaningless unless a and b are differentiable whereas 
no such restriction applies to the backward equation. The true forward equation can be 
written down also when a and b are not differentiate, but it involves the generalized 
differential operators mentioned in footnote 13 of section 4. 
X.5 THE FORWARD EQUATION. BOUNDARY CONDITIONS 339 
In probability theory this equation is known as the forward or Fokker-Planck 
equation. 
Before proceeding let us illustrate the kind of information that can be 
derived from E.2) more easily than from the backward equation. 
Example, (a) Population growth. Example A(d) leads to the forward 
equation 
fK i\ dt;(s, y) d2y v(s, y) dy v(s, y) 
{>*) Z~= a —T~i P 
{•>¦-*) Z.a TiP z• 
ds Cy . dy 
It can be proved that for a given initial density v0 there exists a unique 
solution. Although explicit formulas are hard to come by, much relevant 
information can be obtained directly from the equation. For example, to 
calculate the expected population size. M(s) multiply E.3) by y and 
integrate with respect to y from 0 to oo. On the left we get the derivative 
M'{s). Using integration by parts and assuming that v vanishes at infinity 
faster than l/yz it is seen that the right side equals fiM(s). Thus 
M'(s) = pM(s) 
and hence M(s) is proportional to e&s. Similar formal manipulations show 
that the variance is proportional to 2a/j~V5(e^ — l). [Compare the analogous 
result in the discrete case, formulas E.10)and (I0.9)of 1; XV[[.] Admittedly 
the manipulations require justification, but the result has at least heuristic 
value and could not be obtained from the backward equation without 
explicit calculation of qt. > 
The connection between the forward and backward equations is similar to 
that described in the case of jump processes in section 3. We give a brief 
summary without proof. 
Consider the backward equation D.6) in an open interval x1,z2 which 
may be finite or infinite. We assume, of course, ~a > 0 and that the co- 
efficients a and b are sufficiently regular for E.2) to make sense. Under 
these conditions there exists a unique minimal solution Qt such that D.5) 
yields a solution of the backward equation D.6). The catch is that for fixed 
/ and x the kernel Ot(x,Y) may represent a defective distribution. Under 
any circumstances Qt -possesses densities and the function r of E.1) 
satisfies the forward equation. In fact, this solution is again minimal in the 
obvious sense made precise in section 3. To this extent the forward equation 
is a consequence of the backward equation. However, these equations deter- 
mine the process uniquely only when the minimal solution is not defective. 
In all other cases the nature of the process is determined by additional 
boundary conditions. 
The nature of boundary conditions is best understood by analogy with 
340 MARKOV PROCESSES AND SEMI-GROUPS X.5 
the simple random walk on 0, oo discussed in 1; XIV. Various conventions 
can be in effect when the origin is reached for the first time. In the ruin 
problem the process stops; in this case the origin is said to act as an absorbing 
barrier. On the other hand, when the origin acts as reflecting barrier, the 
particle is returned instantaneously to the position 1 and the process 
continues forever. The point is that boundary conditions appear iff a 
boundary point can be reached. The event "the boundary point x2 has 
been reached before epoch /" is well defined in diffusion processes because 
of the continuity of the path functions. It is closely related to the event 
"infinitely many jumps have occurred before /" in jump processes. 
In some diffusion processes with probability one no boundary point 
is ever reached. Such is the Brownian motion [example 4(a)). Then the 
minimal solution stands for a proper probability distribution and no other 
solutions exist. In all other situations the minimal solution regulates the 
process until a boundary is reached. It corresponds to absorbing barriers, 
that is, it describes a process that stops when a boundary point is reached. 
This is the most important type of process not only because all other 
processes are extensions of it, but even more because all first-passage 
probabilities can be calculated by imposing artificial absorbing barriers. 
The method is generally applicable but will be explained by the simplest 
example. (It was used implicitly in random walks and elsewhere, for example 
in problem 18 of 1; XVII,10.) 
In the following examples we limit our attention to the simple equation 
E.4). More general diffucion equations will be treated by the method of 
Laplace transforms in XIV,5. 
Examples, (b) One absorbing barrier. First-passage times. Consider 
Brownian motion on 0, °o with an absorbing barrier at the origin. More 
precisely, a Brownian motion starting at the point x > 0 at epoch 0 is 
stopped at the epoch of the first arrival at the origin. Because of symmetry 
both the backward and the forward equation take on the form of the classical 
diffusion equation 
E4) ; ^1^ 
dt 2dx2 
The appropriate boundary condition is qt@, y) — 0 for all /, just as in the 
case of random walks. (The assertion can be justified either by the passage 
to the limit in 1; XIV,6 or from fhe minimal character of the solution.) 
For a given w0 we see^ a solution of E.4) defined for / > 0, a> >; 0 and 
such that w@, x) = uo(x) and u(t, 0) = 0. Its construction depends on 
the method of images due to Lord Kelvin.18 We extend u0 to the left half- 
line by wo(— x) = —uo(x) and-solve E.4) with this initial condition in 
18 
See problem 15-18 in 1; XIV,9 for the same method applied to difference equations. 
X.5 THE FORWARD EQUATION. BOUNDARY CONDITIONS 341 
— oo, oo. For reasons of symmetry the solution satisfies the condition 
u(t, 0) = 0, and restricting x again to 0, oo we have the desired solution. 
It is given by the integral of uo(y) qt(x, y) over 0, oo where 
Thus qt represents the transition densities of our process (t > 0, x > 0, 
y > 0). It is easily seen that qt is, for fixed y, a solution E.4) satisfying the 
boundary condition qt@, y) = 0. [For a more systematic derivation see 
example XIV,5(a).] 
Integrating over y one gets the total probability mass at epoch / 
E.6) 
where 9t stands for the standard normal distribution. In other words, 
E.6) is the probability that a path starting from x > 0 does not reach the 
origin before epoch t. In this sense E.6) represents the distribution of 
first-passage times in a free Brownian motion. Note that E.6) may be charac- 
terized as the solution of the differential equation E.4) defined for x > 0 
and satisfying the initial condition w@, x) = 1 together with the boundary 
condition u(t, 0) = 0. 
[One recognizes in E.6) the stable distribution with exponent a = \; 
the same result was found in YI,2 by a passage to the limit from random 
walks.] 
(c) Two absorbing barriers. Consider now a Brownian motion impeded 
by two absorbing barriers at 0 and a > 0. This means that for fixed 
0 < y < a the transition densities qt should satisfy the differential equation 
E.4) together with the boundary conditions qt@, y) = qt(a, y) = 0. 
It is easily verified that the solution is given by19 
4t(x> V) = 
E.7) _ _J_ 
(- 
l 2/ / HV 
where 0 < x, y < a. Indeed, the series is manifestly convergent, and the 
obvious cancellation of terms shows that qt@, y) = qt(a, y) = 0 for all 
/ > 0 and 0 < y < a. [The Laplace transform of E.7) is given in XIV,E.17).] 
19 The construction depends on successive approximations by repeated reflections. In 
E.5) we have a solution of the differential equation satisfying the boundary condition at 
0, but not a. A reflection at a leads to a four-term solution satisfying the boundary 
condition at a, but not at 0. Alternating reflections at 0 and a lead in the limit to E.7). 
The analogous solution for random walks is given in 1; XIV,(9.1), and E.7) could be 
derived from it by the passage to the limit described in 1; XIV,6. 
342 MARKOV PROCESSES AND SEMI-GROUPS X.5 
Integrating E.7) over 0 < y < a we get the total probability mass at 
epoch t in the form 
2ka + a — x\ _ 
E.8) xa«, *) - | [ni 
This is the probability that a particle starting at x will not be absorbed 
before epoch t. 
The function Xa is a solution of the diffusion equation E.4) tending to 1 
as / -*- 0 and satisfying the boundary conditions Aa(t, 0) = Ao(/, a) = 0. 
This solution can be obtained also by a routine application of the method of 
Fourier series in the form20 
E-9) ^-i 
We nave thus obtained two very different representations21 for the same 
function Aa. This is fortunate because the series in E.8) converges reasonably 
only when / is small-, whereas E.9) is applicable for large /. 
For an alternative interpretation of Aa consider the position X(t) of a 
particle in tree Brownian motion starting at the origin. To say that during 
the time interval 0, / the particle remained within — \a, \a amounts to 
saying that in a process with absorbing barriers at ±\a and starting at 0 
no absorption took place before epoch t. Thus Aa(/, \a) equals the prob- 
ability that in an unrestricted Brownian motion starting at the origin \X(s)\ < \a 
for all s in the interval 0 < s < /. 
(d) Application to limit theorems and Kolmogorov-Smirnov tests. Let 
Y,, Y2, . . . be independent random variables with a common distribution 
and suppose that E(Y,) = 0 and E(Y2) = 1. Put Sn = Yx + • • • + Yn 
and Tn = max [|SX|,. . . , |SJ]. In view of the central limit theorem it is 
plausible that the asymptotic behavior of Tn will be nearly the same as 
in the case where the Y,- are normal variables, and in the latter case the 
normed sum SJs/n is comparable to the variable of a Brownian motion 
20 The analogous formula for random walks is derived in 1; XIV,5 where, however, 
the boundary conditions are .Aa(/, 0) = 1 and Xa(t, a) = 0. 
21 The identity between E.8) and E$) serves as a standard example for the Poisson 
summation formula [see XIX,E.10)J. It has acquired historical luster, having been dis- 
covered originally in connection with Jacobi's theory of transformations of th^ta functions. 
See Satz 277 in E. Landau, Vkrteilung der Primzahlen, 1909. 
X.5 THE FORWARD EQUATION. BOUNDARY CONDITIONS 343 
at epoch kfn (k = 0, 1, . . . , n). The probability that this Brownian motion 
remains constrained to the interval ( — \ay \a) was shown to equal ka(l, \d). 
Our plausibility argument would therefore lead us to the conjecture that as 
n —*¦ oo 
E.10) P{Tn <z}~+L(z) 
where L(z) = A22(l, z) is obtained from E.8) and E.9): 
L(z) = 2 f 
E-11) 
2 ^7 
7T w=0 2/1 + 
This conjecture was proved in 1946 by P. Erdos and M. Kac, and the 
underlying idea has since become known as invariance principle. It states, 
roughly speaking, that the asymptotic distribution of certain functions of 
random variables is insensitive to changes of the distributions of these 
variables and may be obtained by considering an appropriate approximating 
stochastic process. This method has been perfected by M. Donsker, P. 
Billingsley, Yu. V. P »hc ov, and others, and has become a powerful tool 
for proving limit theoic»»;3. 
For similar reasons the distribution E.11) plays a prominent part also in 
the vast literature on non-parametric tests of the type discussed in I,12.22 
(e) Reflecting barriers. By analogy with the ordinary random walk we 
dqJO, y) 
define a reflecting barrier at the origin by the boundary condition -^-r = 
dy 
= 0 for a reflecting barrier at the origin is imposed by analogy with random 
walks. It is readily verified that the solution for the interval 0, 00 is given by 
E.5) with the minus sign replaced by plus. The formal derivation by the 
method of images is the same, except that one puts uo(—z) = uo(z). The 
solution for 0, a with reflecting barriers at both 0 and a is obtained 
similarly by changing the minus sign to a plus in E.7). (An alternative 
expression obtained by Fourier expansions or the Poisson summation 
formula is given in problem 11 of XIX,9.) 
It should be noted that in the case of reflecting barriers qt is a proper 
probability density. > 
22 The topic is relatively new, and yet the starting point of the much used identity E.W) 
seems already to have fallen into oblivion. [A. Renyi, On the distribution function L(z). 
Selected Translations in Math. Statist, and Probability, vol. 4 A963) pp. 219-224. Renyi's 
supposedly new proof depends on the classical argument involving theta functions, thus 
obscuring the simple probabilistic meaning of E.11).] 
344 MARKOV PROCESSES AND SEMI-GROUPS X.6 
6. DIFFUSION IN HIGHER DIMENSIONS 
It is easy to generalize the foregoing theory to two dimensions. To avoid 
the nuisance of subscripts we denote the coordinate variables by (X(/), Y(/)) 
and the values of the transition densities by qt(x, y; ?, y\)\ here z, y is the 
initial point and qt is a density in (?, rj). The postulates are as in section 
4 except that the infinitesimal velocity b(x) is replaced by a vector, and the 
variance a(x) by a covariance matrix. Instead of D.6) we get for the 
backward diffusion equation 
du ~d2u ,' 5V , tfu . du ". du 
r; = flu ~2 + 2a12 —— + a22 —¦ + bx — + b2 — 
ot ox ox oy oy ox oy 
the coefficients depending on x and y. In the case of two-dimensional 
Brownian motion we require rotational symmetry, and up to an irrelevant 
nor.ming constant we must have 
iHlr. + Sl- 
dt 2\_dz2 dy2} 
The corresponding transition densities are normal with variance /, centered 
at (x, y). The obvious factoring of this density shows that X(/) and Y(/) 
are stochastically independent. 
The most jnteresting variable in this processes the distance RG) from 
the origin (R2 — X2 + Y2). It is intuitively obvious that R(f) is the 
variable of a one-dimensional diffusion process and it is intere"sting to 
compare the various ways of getting at the diffusion equation for this process- 
In polar coordinates our normal transition densities for F.2) take on the form 
F.3) 
p 
2-nt \ It 
) 
(with x = r cos a, etc.). Given the position r, a at epoch 0, the marginal 
density of R(/). is obtained by integrating F.3) with respect to 6. The 
parameter a drops out and we get23 for the transition densities of the R(t) 
process 
F.4) wt(r, p) = - 
where /0 is the Bessel function defined in 11,G.1). Here r stands for the 
initial position at epoch 0. From the derivation it is clear that for fixed p the 
23 The integral is well known. For a routine verification expand e008 e into a power 
series in cos 6. 
346 MARKOV PROCESSES AND SEMI-GROUPS X.7 
with a variety of other distributions, each of which leads to an analogue of 
the exponential formula. ^ 
The variables X(T(r)) form a new stochastic process which need not be 
Markovian. For the process to be Markovian it is obviously necessary that 
the Pt satisfy the Chapman-Kolmogorov equation 
G-3) Ps+t(x, F) = Ps(x, dy) Pt(y, F). 
J — 00 
This means that the distribution of X<T(/-f s)) is obtained by integration of 
Pt(y, F) with respect to the distribution of X(T(j)) and so 
G.4) Pt(y, D = P{X(T(/+j)) e I11 X(T(j)) = y) 
by the definition of conditional probabilities. A similar calculation of higher 
order transition probabilities shows that G.3) suffices to ensure the Markovian 
character of the derived process {X(T(/))}. 
We wish now to find the distributions Ut that lead to solutions Pt of 
G.3). A direct attack on this problem leads to considerable difficulties, but 
these can be avoided by first considering the simple special case where the 
variables T(/) are restricted to the multiples of a fixed number h > 0. For 
the distribution of T(t) we write 
G.5) P{T(r) = nh) "=*»(/). 
Given that X@) = x the variable X(T(t)) has the distribution 
G-6) Pt(^^) 
fc=O 
Since the kernels {Qt} satisfy the Chapman-Kolmogorov equation we have 
G.7) f */>,(*, dy) Pt\y, V) = 2 a?s) ak(j) • Q{j+t)?x, F) 
i.k 
and it is seen that the kernels Pt satisfy the Chapman-Kolmogorov equation 
G.3) iff 
G.8) ao{s) an(t) + a^ a^{t) + • • • + an(s) ao(t) = an(s+t) 
for all s > 0 and / > 0. This relation holds if (T(r)} is a process with 
stationary independent increments, and the most general solution of G.8) 
was found in 1; XII,2. 
This result leads to the conjecture that in general G.3) will be satisfied 
whenever the T(/) are the variables of a process with stationary independent 
X.7 SUBORDINATED PROCESSES 347 
increments, that is, whenever the distributions Ut satisfy24 
G-9) Vi+,{x) 
\ 
o- 
We verify this conjecture by a passage to the limit.25 We represent Ut as the 
limit of a sequence of arithmetic distributions of the type just considered: 
U\v) is concentrated on the multiples of a number hv, and the weights that it 
attaches to the points nhv satisfy a relation of the form G.8). For each v 
we get thus a kernel P{tv) corresponding to G.6) that satisfies the Chapman- 
Kolmogorov equation G.3). To show that also the kernel Pt of G.1) 
satisfies this equation it suffices therefore to show that P{tv) —>¦ Pt, or, what 
amounts to the same, that 
/*+O0 /'-TOO 
G-10) P\"\x, dy)f(y) -J Pt(z, dy)f(y) 
J —00 J — OC 
for every continuous function /~°° vanishing at infinity. If we put 
G-11) F(t,x) = \ Qt(x,dy)f(y) 
J — 00 
G.10) may be rewritten in the form 
G.12) f^Fis, x) U{tv){ds} — ^Fis, x) Ut{ds). 
Jo Jo 
This relation holds certainly if F is continuous, and this imposes only an 
extremely mild regularity condition on the Qt. We have thus proved the 
following basic result: 
Let {X@} be a Markov process with continuous transition probabilities 
Qt and (T(/)} a process with non-negative independent increments. Then 
{X(T(/))} is a Markovian process with transition probabilities Pt given by 
A.1). This process is said to be subordinate2* to (X(/)} using the operational 
time T(/). The process (T(f)} is called the directing process. 
The most interesting special case arises when also the X(t) process has 
independent increments. In this case the transition probabilities depend only 
on the differences P — x and may be replaced by the equivalent distribution 
24 The most general solution of G.9) will be found by means of Laplace transforms in 
XIII.7. It can be obtained also from the general theory of infinitely divisible distributions. 
28 A direct verification requires analytic skill. Our procedure shows once again that a 
naive approach is sometimes most powerful. 
26 The notion of subordinated semi-groups was introduced by S. Bochner in 1949. For a 
high-level systematic approach see E. Nelson, A functional calculus using singular Laplace 
integrals, Trans. Amer. Math. Soc, 88 A958), pp.,400-413. 
348 MARKOV PROCESSES AND SEMI-GROUPS X.7 
functions. Then G.7) takes on the simpler form 
G.13) Pt{x) = ("qAx) Ut{ds}. 
Jo 
All our examples are of this type. 
Examples, (b) The Cauchy process is subordinated to Brownian motion. 
Let (X(/)} be the Brownian motion (Wiener process) with transition 
densities given by qt{x) = B7rt)~h-^x2lt. For (T(/)} we take the stable 
process with exponent ? with transition densities given by 
ut(x) = -J-=_ e-^l*. 
V 2n six3 
The distribution G.13) has then a density given by 
G.14) pt(x) = -L s-v-<.«+««)/<i.) ds = 
and thus our subordination procedure leads to a Cauchy process. 
This result may be interpreted in terms of two independent Brownian 
motions X(/) and Y(/) as follows. 
It was shown in example VT,2(e) that Ut may be interpreted as the 
distribution of the waiting time for the epoch at whicli the Y(^)-process for 
the first time attains the value t, > 0. Accordingly, a Cauchy process Z(t) 
may be realized by considering the value of the X-process at the epoch T(t) 
when Y(s) first attains the value t. [For another connection of the Cauchy 
process with hitting times in Brownian motion see example VI,2(/).] 
(c) Stable processes. The last example generalizes easily to arbitrary 
strictly stable processes (X(/)} and (T(/)} with exponents, a and ft, 
respectively. Here a < 2, but since T(/) must be positive we have 
necessarily /5 < 1. The transition probabilities Qt and Ut are of the form 
Qt(x) = Q(xt~lla) and Ut(x) = Uixt^1) where Q and U are fixed stable 
distributions. We show that the subordinated process X(T(/)) is stable with 
exponent a/5. This assertion is equivalent to the relation PM(x) = Pt{x-xlaP). 
In view of the given form of Qs and Ut this relation follows trivially from 
G.13) by the substitution s = yXxlp. 
[Our result is essentially equivalent to the product formula derived in 
example \l,2{h). When X(/) > 0 the formula can be restated in terms of 
Laplace transforms as in XIII,7(e). For the Fourier version see problem 9 
inXVII,12.] 
(d) Compound Poisson process directed by gamma process. Let Qt be 
the compound Poisson distribution generated by the probability distribution 
F and let Ut have the gamma density er'x*-1!^). Then G.13) takes on 
X.7 SUBORDINATED PROCESSES 345 
transition probabilities wt satisfy F.2) in polar coordinates; that is to say 
F.5) 
dt 2\dr2 r dr) 
This is the backward equation for the R(/) process and is obtained from F.2) 
simply by requiring rotational symmetry. 
Equation F.5) shows that the R@ process has an infinitesimal velocity 
l/Br). The existence of a drift away from the origin can be understood if 
one considers a plane Brownian motion starting at the point r of the x-axis. 
For reasons of symmetry its abscissa at epoch h > 0 is equally likely to be 
>r or <r. In the first case certainly R(A) > r, but this relation can occur 
also in the second case. Thus the relation R(h) > r has probability >?, 
and on the average R is bound to increase. 
The same derivation of transition probabilities applies to three dimensions 
with one essential simplification: the Jacobian p in F.3) is now replaced 
by p2 sin 6, and an elementary integration is possible. Instead of F.4) we 
get for the transition densities of the R(/) process in three dimensions 
(Again r stands foF the initial position at epoch 0.) 
7. SUBORDINATED PROCESSES 
From a Markov process (X(/)} with stationary transition probabilities 
Qt(x, V) it is possible to derive a variety of new processes by introducing 
what may i>e called a randomized operational time. Suppose that to each 
/ > 0 there corresponds a random variable T(/) with distribution Ut. 
A new stochastic kernel Pt may then be defined by 
G.7) Pt{x, T) = [™Qs{x, T) Ut{ds}. 
Jo- 
This represents the distribution of X(T(/)) given that X@) = 0. 
Example, (a) If T(/) has a Poisson distribution with expectation a/ 
A.2) 
7j=o n I 
These Pt are the transition probabilities of a pseudo-Poisson process. It 
will be shown in section 9 that the randomization by Poisson distributions 
leads to the so-called exponential formula which is basic for the theory of 
Markov semi-groups. We shall now see that similar results are obtained 
X.8 MARKOV PROCESSES AND SEMI-GROUPS 349 
the form 
G-15) Pt* 
71=0 
where 
S 
G.16) «„(,) = f V ^ ¦ e~° S~ ds = Zm. 2— 
Jo n\ 
s 2 
nlT(t) 
It is easily verified that the probabilities an(t) have the infinitely divisible 
generating function 2an@?" = B—?)"*• 
(e) Gamma process directed by the Poisson process. Let us now consider 
the same distributions but with reversed roles. The operational time is then 
integral-valued and 0 has weight e~*. It follows that the resulting distribution 
has an atom of weight e~* at the origin. The continuous part has the density 
G-17) | e- f^~ • e~*'- = e- 
n=i (n—1)! n\ 
where Ix is the Bessel function of 11,G.1). It follows that this distribution is 
infinitely divisible, but a direct verification is not easy. *¦ 
8. MARKOV PROCESSES AND SEMI-GROUPS 
Chapter VIII revealed the advantages of treating probability distributions 
as operators on continuous functions. The advantages of the operator 
approach to stochastic kernels are even greater, and the theory of semi-groups 
leads to a unified theory of Markov processes not attainable by other 
methods. Given a stochastic kernel K in %x and a bounded continuous 
function u the relation 
r-i-oo 
J — 00 
(8.1) U(z) = \ K{x,dy)u{y) 
J — 00 
defines a new function. Little generality is lost in assuming that the transform 
U is again continuous and we could proceed to study properties of the kernel 
K in terms of the induced transformation u -+¦ U on continuous functions. 
There are two main reasons for a more general setup. First, transformations 
of the form (8.1) make sense in arbitrary spaces, and it would be exceedingly 
uneconomical to develop a theory which does not cover the simplest and 
most important special case, namely processes with a denumerable state 
space [where (8.1) reduces to a matrix transformation]. Second, even in a 
theory restricted to continuous functions on the line various types of boundary 
conditions compel one to introduce special classes of continuous functions. 
On the other hand, the greater generality is bought at no expense. Readers 
so inclined are urged to ignore the generality and refer all theorems to one 
(or several) of the following typical situations, (i) The underlying space 2 
350 MARKOV PROCESSES AND SEMI-GROUPS X.8 
is the real line and ?? the class of bounded continuous functions vanishing at 
infinity, (ii) the space S is a finite closed interval / in 'Ji1 or ft2 and & 
the class of continuous functions on it. (iii) 2 consists of the integers 
and & of bounded sequences. In this case it is best to think of sequences 
as column vectors and of transformations as matrices. 
As in chapter VIII the norm of a bounded real function u is defined by 
||i/1| = sup \u(x)\. A sequence of functions un converges uniformly to u 
iff \\un -wll-0. 
From now on Jz? will denote a family of real functions on some set 2 
with the following properties: (i) If ux and u2 belong to JSP then every 
linear combination cxux + c2u2 e <g. (ii) If une?> and ||kb —k||-»-0 
then we ??'. (iii) If u e ?P then also u+ and u~ belong to & (where 
u = u+ — u~ is the usual decomposition of u into its positive and negative 
parts.) In other words, J? is closed under linear combinations, uniform 
limits, and absolute values. The first two properties make S? a Banach 
space, the last a lattice. 
The following definitions are standard. A linear transformation T 
is an endomorphism on S? if each u e S? has an image Tu e S? such that 
|| Tu || <m ||w|| where m is a constant independent of u. The smallest 
constant with this property is called the norm || T\\ of T. The'transformation 
T is positive if u > 0 implies T« ^ 0. In this case —Tur < Tu < Tu+. 
A contraction is a positive operator T with -\\T\\ < 1. If the constant 
function 1 belongs to ?? and T is a positive operator such that T\ = 1, 
then T is called a transition operator. (It is automatically a contraction.) 
Given two endomorphisms S and T on Jf, their product ST is the 
endomorphism mapping w into S(Tu). Obviously \\ST\\ <, \\S\\ • ||r||. In 
general ST ^ TS, in contrast to the particular class of convolution operators 
of VIII,3 which commuted with each other. 
We are seriously interested only in transformations of the form (8.1) where 
AT is a stochastic, or at least substochastic, kernel. Operators of this form 
are contractions or transition operators and they also enjoy the 
Monotone convergence property: ifun > 0 and un\ u {with un and u in 
then Tun -*¦ Tu pointwise. 
In practically all situations contractions with this property are of the form 
(8.1). Two examples will illustrate this point. 
Examples, (a) Let 2 stand for the real line and S? — C for the family 
of all bounded continuous functions on it. Let Co c= ?P the subclass of 
functions vanishing at ± oo. If T is a contraction on ^f then for u e Co 
the value Tu(x) of Tu at a fixed x is a positive linear functional on S?'. 
By the F. Riesz representation theorem there exists a possibly defective 
X.8 MARKOV PROCESSES AND SEMI-GROUPS 351 
probability distribution F such that Tu(x) is the expectation of u with 
respect to F. Since F depends on x we write K(x, F) for F(F). Then 
for «eC0 
r-f-oo 
(8.2) Tu(x) = \ K(x,dy)u(y), 
J — 00 
and when T has the monotone convergence property this relation auto- 
matically extends to all bounded continuous functions. 
For fixed x, as 0 function of F, the kernel A" is a measure. If F is an 
open interval and {un} an increasing sequence of continuous functions 
such that un(x) -*• 1 if x e F and un(z) -*• 0 otherwise, then K(x, F) = lim 
Tun(x) by the basic properties of integrals. Since Tun is continuous it 
follows that for fixed F the kernel K is a Baire function of x and therefore 
K has all the .properties required of stochastic or substochastic kernels. 
The same situation prevails when the line is replaced by an interval, or %n. 
(b) Let 2 be the set of integers, and ?? the set of numerical sequences 
u — {xn) with II" II = SUP \xn\- K ptj stands for a stochastic or substochastic 
matrix we define a transformation T such that the zth component of Tu 
is given by 
(8.3) (H = Ia-A, 
Evidently T is a contraction operator enjoying the monotone convergence 
property; if the matrix is strictly stochastic, then r is a transition 
operator. > 
These examples are typical and it is actually difficult to find contractions 
not induced by a stochastic kernel. Anyhow, we are justified to proceed 
with the general theory of contractions with the assurance that applications to 
probabilistically significant problems will be obvious. (In fact, we shall 
never have to go beyond the scope of these examples.) 
The transition probabilities of a Markov process form a one-parameter 
family of kernels satisfying the Chapman-Kolmogorov equation 
(8.4) QU*> T) =JQs(x, dy) Qt{y, F) 
(s > 0, / > 0), the integration extending over the underlying space. Each 
individual kernel induces a transition operator ?}(/) defined by 
(8.5) ?@ u(x) = \Qt(x, dy) u(y). 
Obviously then (8.4) is equivalent with 
(8.6) a(j+o = a(j) a@, s>o,t>o. 
352 MARKOV PROCESSES AND SEMI-GROUPS X.8 
A family of endomorphisms with this property is a semi-group. Clearly 
?l(s) ?}(/) = ?}(/) Q(^), that is, the elements of a semi-group commute 
with each other. 
A sequence of endomorphisms Tn on ?? is said to converge™ to the endo- 
morphism T iff || Tnu — Tu || -*• 0 for each u e ??. In this case we write 
T -+¦ T 
From now on we concentrate on semi-groups of contraction operators 
and impose a regularity condition on them. Denote again by 1 the identity 
operator, 1w = u. 
Definition. A semi-group of contraction operators ?} (/) will be called 
continuous76 if ?1@) = 1 and &(h) -*¦ 1 as h-+0+. 
If 0 <t' <t" we have 
(8.7) ||Q(/> - ?>(/>|| < \\W-*> - " 
For continuous semi-groups there exists a 6 > 0 such that the right side is 
<€ for t" — t' < 6. Thus not only is it true that ?>(/) -+¦ ?>(/„) as / -*¦ tQ, 
but (8.7) shows that ?}(/)" is a uniformly continuous function of / for each 
fixed m.29 
The transformation (8.1) is, of course, the same as D.5) and served as starting point for 
the derivation of the backward equation for diffusion processes. Now a family of Markovian 
transition probabilities induces also a semi-group of transformations of measures such that 
the measure /* is transformed into a measure T(t)n attributing to the set F the mass 
(8.8) T(t)fx{T) = M{dx} Qt(x, T). 
When the Qt have a density kernel qx this transformation is the same as E.1) and was 
used for the forward equation. Probability theory being concerned primarily with measures, 
rather than functions, the question arises, why we do not start from the semi-group 
{T(t)} rather than ?}(/)? The answer is interesting and throws new light on the intricate 
relationship between the backward and forward equations. 
The reason is that (as evidenced by the above examples) with the usual setup the con- 
tinuous semi-groups of contractions on the function space & come from transition 
probabilities: studying our semi-groups Q(t) is in practice the same as studying Markovian 
27 This mode of convergence was introduced in ViII.3 and is called strong. It does 
not imply that \\Tn — T\\ -*0 (which type of convergence is called uniform). A weaker 
type of convergence is defined by the requirement that Tnu(x) -¦ Tu(x) for each x, but 
not necessarily uniformly. See problem 6 in VIII,10. 
28 We use this word as abbreviation fjor the standard term "strongly continuous at the 
origin." 
29 There exist semi-groups such that d(h) tends to an operator T •? t, but they are 
pathological. For an example define an endomorphism T by 
Tu(x) = J«@;[l+cosaO + J«Gr)[l-cosxJ 
and put ?}(/) = T for all / ;> 0. 
X.9 THE "EXPONENTIAL FORMULA" OF SEMI-GROUP THEORY 353 
transition probabilities. For semi-groups of measures this is not true. There exist analytically 
very reasonable contraction semi-groups that are not induced by Markov processes. To 
get an example consider any Markovian semi-group of the form (8.8) on the line assuming 
only that an absolutely continuous n is transformed info an absolutely continuous T(t)p 
[for example, let T(t) be the convolution with a normal distribution with variance /]. 
If /* = fie + /*, is the decomposition of fi into its absolutely continuous and singular 
parts define, a new semi-group {S(t)} by 
(8.9) S(t)n = T(t)fic + fis. 
This semi-group is continuous and S@) = 1, but it is not difficult to see that it is not 
connected with any system of transition probabilities and that it is probabilistically 
meaningless. 
9. THE "EXPONENTIAL FORMULA" OF 
SEMI-GROUP THEORY 
The pseudo-Poisson processes of section 1 are by far the simplest Markov 
processes, and it will now be shown that practically all Markov processes 
represent limiting forms of pesudo-Poisson processes.30 An abstract version 
of the theorem plays a fundamental role in semi-group theory, and we shall 
now see that it is really a consequence of the law of large numbers. 
If T is the operator induced by the stochastic kernel K, the operator 
?}(/) induced by the pseudo-Poisson distribution A.2) takes on the form 
(9.1) 
n 
the series being defined as the limit of the partial sums. These operators 
form a semi-group by virtue of the Chapman-Kolmogorov equation A.3). 
It is better, however, to start afresh and to prove the assertion for arbitrary 
contractions T. 
Theorem 1. If T is a contraction on <?', the operators (9.1) form a con- 
tinuous semi-group of contractions. If T is a transition operator so is ?}(/)• 
Proof. Obviously ?>(/) is positive and ||Q(/)|| < e-at+atim < 1. The 
semi-group property is easily verified from the formal product of the series 
for ?l(s) and &(/) (see footnote 1 to section 1). The relation &(/*) -> 1 
is obvious from (9.1). > 
We shall abbreviate (9.1) to 
(9.2) ?1@ = e*t{T~n. 
30 The special case where the JQ(r) are convolution operators is treated in chapter IX. 
354 MARKOV PROCESSES AND SEMI-GROUPS X.9 
Semi-groups of contractions of this form will be called pseudo-Poiss'onian31 
and we shall say that {?i@} is generated by ol(T— 1). 
Consider now an arbitrary continuous semi-group of contractions ?}(/)• 
It behaves in many respects just as a real-valued continuous function and the 
approximation theory developed in chapter VII using the law of large 
numbers carries over without serious change. We show in particular that the 
procedure of example VII, 1F) leads to an important formula of general 
semi-group theory. 
For fixed h > 0 we define the operators 
(9.3) Sfl(t) = e-t/? 
n=o n! 
which could be described as obtained by randomization of the parameter 
t in ?i@- Comparing with (9.1) is it seen that the ?ih(t) form a pseudo- 
Poissonian semi-group generated by [?}(/*) —1]/h. We now prove that 
(9.4) 
Because of the importance of this result we formulate it as 
Theorem 2. Every continuous semi-group of contractions Q@ is the 
limit (9.4) of the pseudo-Poisson semi-group {Q/,@} generated by the 
endomorphism hr1 [?}(/*) — 1 ]. 
Proof. The starting point is the identity 
(9.5) Qh(t)u - ?l(r)u = *-*»f 25_H! [Q(nfc)tt-?)(r)uJ. 
n=0 /I! 
Choose 6 such that ||?l(s)u — u\\ < e for 0 < s < d. In view of (8.7) 
we have then 
(9.6) \\S(nh)u -?l@u|| <* for \nh - t\ < rjt. 
The Poisson distribution appearing in (9.5) has expectation and variance 
equal to tfh. The contribution of the terms with \nh — t\ ^ 6 can be 
estimated using Chebyshev's inequality, and we find that 
It follows that (9.4) holds uniformly in finite /-intervals. *" 
Equivalent variants of this section are obtained by letting other infinitely divisible dis- 
tributions take over the role of the Poisson distribution. We know from 1; XII,2 that the 
31 There exist contraction semi-groups of the form ets where S is an endomorphism not 
of the form a.{T — I). Such are the semi-groups associated with the solutions of the jump 
processes of section 3 if <x(x) remains bounded. 
X.9 THE "EXPONENTIAL FORMULA OF SEMI-GROUP THEORY 355 
generating function of an infinitely divisible distribution {«„(/)} concentrated on the 
integers n ^ 0 is of the form 
(9-7) f ««W = exp (/«t/7(O- 
o 
where 
Suppose that the distribution {«„(/)} has expectation bt and a finite variance ct. Replac- 
ing in (9.3) the Poisson distribution by {«„(//?)} leads to the operator 
(9.9) 
As in the preceding proof a simple application of the law of large numbers shows that 
&/,(/) -*?}(') as h -> 0. In this way we get a substitute "exponential formula" in which 
{«„(/)} takes over the role of the Poisson distribution.32 
To see the probabilistic content and the possible generalizations of this argument, denote 
by X(/) the variables of the Markov process with the semi-group (Q(/)}, and by T(/) 
the variables of the process with independent increments subject to {«„(/)}. The operators 
(9.9) correspond to the transition probabilities for the variables XlATj — j 1. In other 
(-&) 
words, we have introduced a particular subordinated process; the law of large numbers 
for the T-process makes it plausible that as A-*0 the distributions of the new process 
tend to those of the initial Markov process. This approximation procedure is by no means 
restricted to integral valued variables T(/). Indeed, we may take for (T(/)} an arbitrary 
process with positive independent increments such that E(T(/)) = bt and that variances 
exist. 
The given Markov process (X(/)} thus appears as the limit of the subordinated Markov 
processes with variables X 
MA))- 
The point is that the approximating semi-groups may be of a much simpler structure 
than the original one. In fact, the semi-group of the operators ?}/,(/) of (9.9) is of the simple 
pseudo-Poisson type. To see this put 
(9.10) O # =* /»(W)) = 2 Ptfrn(h) = f />nO(nA). 
This is a mixture of transition operators and therefore itself a transition operator. A 
comparison of (9.7) and (9.9) now shows that formally 
(9.11) QA(r) = exp^-(.G# -1) 
n 
which is indeed of the form (9.2). It is not difficult to justify (9.11) by elementary methods, 
but we shall see that it is really only a special case of a formula for the generators of 
subordinated semi-groups (see example XIII,9(b)). 
32 This was pointed out by K. L. Chung (see VII.5). 
356 MARKOV PROCESSES AND SEMI-GROUPS X.10 
10. GENERATORS. THE BACKWARD EQUATION 
Consider a pseudo-Poisson semi-group {Q@} of contractions generated 
by the operator $1 = aG*— 1). This operator being an endomorphism, 
= v is defined for all u e S?, and 
A0.1) Qih) * u -» v, 
h 
It would be pleasant if the same were true of all semi-groups, but this is 
too much to expect. For example, for the semi-group associated with 
Brownian motion the diffusion equation D.1) implies that for twice con- 
tinuously differentiable u the left side in A0.1) tends to \u", but no limit 
exists when u is not differentiable. The diffusion equation nevertheless 
determines the process uniquely because a semi-group is determined by its 
action on twice differentiable functions. We must therefore not expect that 
A0.1) will hold for all functions w, but for all practical purposes it will 
suffice if it holds for sufficiently many functions. With this in mind we 
introduce the 
Definition. If for some elements u,v in S? the relation A0.1) holds (in the 
sense of uniform convergence) we put v = $lw. The operator so defined is 
called the generator** of the semi-group {?}(/)}. 
Premultiplying A0.1) by ?}(/) we see that it implies 
h 
Thus, if %u exists then all functions Q(t)u are in the domain of the $1 and 
This relation is essentially the same as the backward equation for Markov 
processes. In fact, with the notations of section 4 we should put 
u(t, x) = Q@ uo(x), 
where w0 is the initial function. Then A0.3) becomes 
. m, x). 
dt 
33 The treatment of convolution semi-groups in chapter IX restricts the consideration to 
infinitely differentiable functions with the result that all generators are defined on the same 
domain. No such convenient device is applicable for general semi-groups. 
X.10 GENERATORS. THE BACKWARD EQUATION 357 
This is the familiar backward equation, but it must be interpreted properly. 
The transition probabilities of the diffusion processes in section 4 are so 
smooth that the backward equation is satisfied for all continuous initial 
functions w0. This is not necessarily so in general. 
Examples, (a) Translations. Let & consist of the continuous functions 
on the line vanishing at infinity and put Zi{t)u{x) = u(x+t). Obviously 
A0.1) holds iff u possesses a continuous derivative u vanishing at infinity, 
and in this case %u = u. 
Formally the backward equation A0.4) reduces to 
do j) f* - f*. 
ot ox 
The formal solution reducing for / = 0 to a given initial w0 would be 
given by u(t, x) = uo(t-\-x). But this is a true solution only if w0 is differ- 
entiable. 
(b) As in section 2 consider a pseudo-Poisson process with variables X@ 
and another process defined by X#(/) = X(/) — ct. The correspondingM 
semi-groups are in the obvious relationship that the value of ?}#(/)w at x 
equals the value of Q(/)w at x -f- ct. For the generators this implies 
A0.6) U*=U-c — 
dx 
and so the domain of $I# is restricted to differentiate functions. The 
backward equation is satisfied whenever the initial function w0 has a 
continuous derivative but not for arbitrary functions. In particular, the 
transition probabilities themselves need not satisfy the backward equation. 
This explains the difficulties of the old-fashioned theories [discussed in 
connection with IX,B.14)] and also why we had to introduce unnatural 
regularity assumptions to derive the forward equation B.1). *¦ 
The usefulness of the notion of generator is due to the fact that for each 
continuous semi-group of contractions the generator defines the semi-group 
uniquely. A simple proof of this theorem will be given in XIII,9. 
This theorem enables us to handle backward equations without un- 
necessary restrictions and greatly simplifies their derivation. Thus the 
most general form of diffusion operators alluded to in footnote 12 of section 
4 could not have been derived without the a priori knowledge that a generator 
does in fact exist. 
CHAPTER XI 
Renewal Theory 
Renewal processes were introduced in VI,6 and illustrated in VI,7. We 
now begin with the general theory of the so-called renewal equation, which 
occurs frequently in various connections. A striking example for the 
applicability of the general renewal theorem is supplied by the limit theorem 
of section 8. Sections 6 and 7 contain an improved and generalized version 
of some asymptotic estimates originally derived laboriously by deep analytic 
methods. This illustrates the economy of thought and tools to be achieved 
by a general theoretical approach to hard individual problems. For a 
treatment of renewal problems by Laplace transforms see XIV,1-3. 
Many papers and much ingenuity have been spent on the elusive prpblem 
of freeing the renewal theorem of the condition that the variables be positive. 
In view of this impressive history a new and greatly simplified proof of the 
general theorem is incorporated in section 9. 
1. THE RENEWAL THEOREM 
A F be a distribution concentrated1 on 0, oo, that is, we suppose 
F@) = 0. We do not require the existence of an expectation, but because 
of the assumed positivity we can safely write 
= rVF{dy} = r[l- 
Jo Jo 
A.1) 
where ju < oo. When - ju = oo we agree to interpret the symbol /m~1 as 0. 
In this section we investigate the asymptotic behavior as x —*- oo of the 
function 
A.2) 17-ff 
-* 
1 No essential changes occur if one permits an atom of weight p < 1 at the origin. 
(See problem 1.) 
358 
XT.l THE RENEWAL THEOREM 359 
It will be seen presently that this problem is intimately connected with the 
asymptotic behavior of the solution Z of the renewal equation 
A.3) Z(x) = z(x) -H \'z{x-y) F{dy), x > 0. 
Jo 
For definiteness we take the interval of integration closed, but in the present 
context it will be understood that z and Z vanish on the negative half-axis; 
the limits of integration may then be replaced by — oo and oo, and the 
renewal equation may be~written in the form of the convolution equation 
A.4) Z = z + F*Z. 
(A similar remark applies to all convolutions in the sequel.) 
The probabilistic meaning of U and probabilistic applications of the 
renewal equation were discussed at some length in VI,6-7. For the present 
we shall therefore proceed purely analytically. However, it should be borne 
in mind that in a renewal process U(x) equals the expected number of 
renewal epochs in 0, x, the origin counting as a renewal epoch. Accordingly, 
U should be interpreted as a measure concentrated on 0, oo, the interval 
i 
/ = a, b carrying the mass U{I} = U(b) — U(a). The origin is an atom of 
unit weight contributed by the zeroth term in the series A.2). 
The following lemma merely restates theorem 1 of VI,6 but a new proof 
is given to render the present section self-contained. 
Lemma. U(x) < oo for all x. If z is bounded the function Z defined by 
A.5) Z(x) = I *z{x-y) U{dy}, x>0 
Jo 
is the unique solution of the renewal equation A.3) that is bounded on finite 
intervals. 
[With the convention that z(x) = Z{x) = 0 for x < 0 we may write 
A.5) in the form Z = ?/**.] 
Proof. Put Un = F°* + • • + Fn* and choose positive numbers 
t and rj such that 1 — F{t) > rj. Then 
A.6) 
f[l - F(x-y)] Un{dy} = 1 - F("+1)V), * > 0 
Jo 
and hence r}[Un{x) — Un{x— t)] < 1. Letting n -»• cc we conclude that 
U{I} < rj'1 for every interval / of length <t. Since an arbitrary interval 
of length a is the union of at most 1 + a\x intervals of length t it follows 
360 RENEWAL THEORY XI. 1 
that 
A.7) U{x)-U(x-a)<!Ca 
where Ca = (a + t)/(t?^). Thus U{I) is uniformly bounded for all intervals 
/ of a given length. 
Now Zn = Un + z satisfies Zn+1 — z + F+ Zn. Letting n-+ oq one 
sees that the integral in A.5) makes sense and that Z is a solution of A.3). 
To prove its uniqueness note that the difference of two solutions would 
satisfy V = F+ V, and therefore also 
(z) = Fv(x- 
Jo 
A.8) V(z) = Fv(x-y)r*{dy}, x>0 
for r=l,2, .... But Fr*(x)—>-0 as r-+oo and since V is supposed 
bounded in 0, a; this implies V{x) — 0 for all x > 0. »¦ 
The formulation of the renewal theorem is encumbered by the special 
role played by distributions concentrated on the multiples of a number X. 
According to definition 3 of V,2, such a distribution is called arithmetic, 
and the largest X such that F is concentrated on X, 2X,"... is called'the 
span of F. In this case the measure U is purely atomic, and we denote by 
Mn the weight of nX. The renewal theorem of 1; XIII,11Estates that un-^Xjfx. 
The following theorem2 generalizes this result to arbitrary distributions 
concentrated on 0, oo. The case of arithmetic F is repeated for complete- 
ness. (We recall the convention that fir1 = 0 if fi = oo.) 
Renewal theorem {first form). If F is not arithmetic 
A.9) 
for every h > 0. If F is arithmetic the same is true when h is a multiple 
of the span X. 
Before proving the theorem we reformulate it in terms of the asymptotic 
behavior of the solutions A.5) of the renewal equation. Since the given 
function z may be decomposed into its positive and negative parts we may 
suppose that z ^> 0. For definiteness we suppose at first that the distribution 
F is non-arithmetic and has an expectation ju < oo. 
2 The discrete case was proved in 1949 by P. Erdos, W. Feller and H. Pollard. Their 
proof was immediately generalized by D. Blackwell. The present proof is new. For a 
generalization to distributions not concentrated on 0, oo see section 9. A far reaching 
generalization in another direction is contained in Y. S. Chow and H. E. Robbins, A 
renewal theorem for random variables' which are dependent or non-identically distributed. 
Ann. Math. Statist., vol. 34 A963), pp. 390-401. 
XI. 1 THE RENEWAL THEOREM 361 
If z(x) = 1 for 0 <; a <; x < b < oo and z(ar) = 0 for all other x 
we get from A.5) 
A.10) Z(t) = U(t -a)- U(t -b)-+{b - a)l/Lc, t -* oo. 
This result generalizes immediately to finite step functions: Let Ilt...,Ir 
be non-overlapping intervals on the positive half-axis of lengths Lu . . . , Lr. 
If z assumes the value ak in Ik and vanishes outside the* union of the 4, 
then clearly 
A.11) 2@ -+[T1J.akLk = [Tl rz(x) dx. 
Jfc=l JO 
Now the classical Riemann integral of a function z is defined in terms of 
approximating finite step functions, and it is therefore plausible that the 
limit relation A.11) should hold whenever z is Riemann integrable. To 
make this point clear we recall the definition of the Riemann integral of z 
over a finite interval 0 < x < a. It suffices to consider partitions into sub- 
intervals of equal length h — ajn. Let mk be the largest, and mk the 
smallest number such that 
A.-12) m* < *(*) ^ m* >r {k-\)h<x<kh. 
The obvious dependence of mk and mk on h should be kept in mind. The 
tower and upper Riemann sums for the given span h are defined by 
A.13) , - <r = h2,mk, d = h^mk. 
As h —*- 0 both a and 5 approach finite limits. If a — a -*¦ 0 these 
limits are the same, and the Riemann integral of z is defined by this common 
limit. Every bounded function that is continuous except for jumps is 
integrable in this sense. 
When it comes to integrals over 0, oo the classical definition introduces 
an avoidable complication. To make the class of integrable functions as 
extensive as possible the integral over 0, oo is conventionally defined as the 
limit of integrals over 0, a. A continuous non-negative function z is 
integrable in this sense iff the area between its graph and the a>axis is finite. 
Unfortunately this does not preclude the effective oscillation of z{x) and 
oo as x —> oo. (See example a.) It is obviously not reasonable to assume 
that the solution Z will tend to a finite limit if the given function z oscillates 
in a wild iftanner. In other words, the sophisticated standard definition 
makes too many functions integrable, and for our purposes it is preferable 
to proceed in the naive manner by extending the original definition also to 
infinite intervals. For want of an established term we speak of a direct 
integration in contrast to the indirect procedure involving a passage to the 
limit from finite intervals. 
362 RENEWAL THEORY XI. 1 
Definition. A function z > 0 is called directly Riemann integrable if the 
upper and lower Riemann sums defined in A.12)-A.13) are finite and tend to 
the same limit as h—*-0. 
This definition makes no distinction between finite and infinite intervals. 
It is easily seen that z is directly integrable over 0, oo if it is integrable over 
every finite interval 0, a and if a <-oo for some h. (Then automatically 
a < oo for all h.) It is this last property that excludes wild oscillations. 
We may restate the definition in terms of approximating step functions. 
For fixed h > 0 put zk{x) = 1 when (k — \)h <; x < kh and zk{x) = 0 
elsewhere. Then 
A.14) 2 = ? mjfcZjt and 
z = 
are two finite step functions and z <; z < z. The integral of z is the common 
limit as h —*¦ 0 of the integrals of these step functions. Denote by Zk the 
solution of the renewal equation corresponding to zk. The solutions corre- 
sponding to z and 2 are then given by 
A.15) Z = ^ 2,^ and Z = 
By the renewal theorem Zk{x)-+h\iA for each fixed /:. Furthermore, A.7) 
assures us that Zk(x) <? CA for all & and a;. The remainders of the series in 
A.15) therefore tend uniformly to 0 and we conclude that 
A.16) Z{x) -> o\fx, Z{x) -> ff/^ (x -> oo). 
But Z. <LZ <iZ and hence all limit values of Z(x) lie between a//x and 
ff/yU. If z is directly Riemann integrable it follows that 
Jo 
0-17) Z{x)-+n-l\ z{y)dy, x^co. 
Jo 
So far we have assumed that F is non-arithmetic and /u < oo. The 
argument applies without change when ju = oo if fi~l is interpreted as 0. 
If F is arithmetic with span X the solution Z of A.5) is of the form 
A.18) Z(x) = ^z{x- kX)uk 
where uk -*¦ Xjfx. One concludes easily that for fixed x 
A.19) Z{x + nX)^Xirly?z{x+jX), n -> oo. 
provided the series converges, which is certainly the case if z is -directly 
integrable. 
We have derived A.17) and A.19) from the renewal theorem, but these 
relations contain the renewal theorem as a special case when z reduces to 
the indicator of an interval 0, h. We have thus proved the following 
XI. I THE RENEWAL THEOREM 363 
Renewal theorem. {Alternative form)? If z is directly Riemann integrable 
the solution Z of the renewal equation satisfies A.17) if F is non-arithmetic, 
and A.19) if F is arithmetic with span X. 
One may ask whether the condition of direct integrability may be dropped 
at Jeast for continuous functions z tending to 0 at infinity. The following 
examples show that this is riot so. Example 3(b) will show similarly that the 
renewal theorem may .fail for an unbounded function z even when it 
vanishes outside a finite interval. Improper Riemann integrals are therefore 
not usable, in renewal theory and direct integrability appears as the natural 
basis. 
Examples, (a) A continuous function z may be unbounded and yet Riemann 
integrable over 0, oo. To see this let z(n) = an for n = 1, 2, . . . and let z 
vanish identically outside the union of the intervals \x — n\ < hn < \; 
between n and n ± hn let z vary linearly with x. The graph of z then 
consists of a sequence of triangles of areas anhn, and hence z is Riemann 
integrable iff ^anhn < oo. This does not preclude that an -> oo. 
(b) To explore the role of direct integrability in the renewal theorem it 
suffices to consider arithmetic distributions F. Thus we may suppose that 
the measure U is concentrated on the integers and that the weight un 
carried by the point n tends to the limit /jr1 > 0. For any positive integer 
n we have then 
Z(n) = un 2@) + «„_! 2A) + ' • • + uoz(n). 
Now choose for z the function of the preceding example with an — i\ 
then Z(n) ^ nf/r1, and so Z is not even bounded. The same is obviously 
true if an tends to 0 sufficiently slowly, and thus we get an example of a 
continuous integrable function z such that z{x) ->• 0, but Z{x) does not 
remain bounded. > 
3 It is hoped that this form and the preceding discussion will end the sorry confusion now 
prevailing in the'literature. The most widely used reference is the report by W. L. Smith, 
Renewal theory and its ramifications, in the J. Roy. Stat. Soc. (Series B), vol. 20 A958), 
pp. 243-302. Since its appearance Smith's "key renewal theorem" has in practice replaced 
all previously used versions (which were not always correct). The key theorem proves A 17) 
under the superfluous assumption that z be monotone. Smith's proof (of 1954) is based 
on Wiener's deep Tauberian theorems, and the 1958 report gives the impression that this 
tortuous procedure is simpler than a direct reduction to the first form of the renewal 
theorem. Also, the condition that z be bounded was inadvertently omitted in the report. 
[Concerning its necessity see example 3(b).] 
364 RENEWAL THEORY XI .2 
2. PROOF OF THE RENEWAL THEOREM 
For arithmetic distributions F the renewal theorem was proved in 1; 
XIII,11 and we suppose therefore F non-arithmetic. For the proof we 
require two lemmas. (The first reappears in a stronger form in the corollary 
in section 9.) 
Lemma L Let t be a bounded uniformly continuous function such that 
?(*) <. t@) for -oo < * < oo. // 
B-1) Ux) = [\x-y)F{dy} 
Jo 
then ?(z) = ?@) identically. 
Proof. Taking convolutions with F we conclude from B.1) by induction 
that 
3 = r«*- 
Jo 
B.2) ?(*) = ?(* - y) F^{dy), r = 1, 2, . . . . 
The integrand is <?@), and for x = 0 the equality is therefore possible 
only if ?( — y) — ?@) for every y that is a point of increase of Fr*. By 
lemma 2 of V,4a the set 2 formed by such points is asymptotically dense at 
infinity, and in view of the uniform continuity of ? this implies that 
?(—*/) -*- ?@) as y —> oo. Now as r'-*> oo the mass of Fr* tends to be 
concentrated at oo. For large r the integral in B.2) therefore depends 
essentially only on large values of y, and for such values ?(z <- y) is close 
to ?@). Letting r^oo we conclude therefore from B.2) that t,{x) = ?@) 
as asserted. > 
Lemma 2. Le/ z 6e a continuous function vanishing outside 0,h. The 
corresponding solution Z of the renewal equation, is uniformly continuous and 
for every a 
B.3) Z{x + a) - Z(x) ^0, x -+ oo. 
Proof. The differences z{% + 6) — z(x) vanish outside an interval of 
length h + 26 and therefore by A.5) and A.7) 
B.4) \2{x + 6) - Z{x)\ < Ch+26 max \z(x +6)- z(x)\. 
This shows that if z is uniformly continuous the same is true of Z. 
Suppose now that « has a continuous derivative z'. Then z exists and 
satisfies the renewal equation 
B.5) Z\x) = z'(x) + \XZ\x - y) F{dy). 
Jo 
XI.2 PROOF OF THE RENEWAL THEOREM 365 
Thus Z' is bounded and uniformly continuous. Let 
B.6) lim supZ'(x) = rj, 
and choose a sequence such that Z'{tn) -*> rj. The family of functions ?n 
defined by 
B.7) U*)=Z'{tn+z) 
is equicontinuous and 
fx+t 
B.8) u*) = *'('.+*) + U* ~ y) F{dy). 
Jo 
Hence there exists a subsequence Such that ?n converges to a limit ?. 
It follows from B.8) that this limit satisfies the conditions of lemma 1 and 
therefore ?'(*) = ?'(<>) = »? for all x. 
Thus Z'(/nr 4- x) -+ rj or 
B-9) 
This being true for every a and Z being bounded it follows that rj = 0. 
The same argument applies to the lower limit and proves that Z'(x) -*> 0. 
We have thus proved the lemma for continuously differentiable z. But an 
arbitrary continuous z can be approximated by a continuously differentiable 
function zx vanishing outside 0, h. Let Zx be the corresponding solution 
of the renewal equation. Then 
\z -zx\<e implies \Z - Zx\ < Che, 
and thus \Z{x+a) — Z(x)\ < BCA + l)e for all x sufficiently large. Thus 
B.3)'holds for arbitrary continuous z. > 
The conclusion of the proof is now easy. If / is the interval a <, x <C /? 
we denote by / + / the interval a + r <>x <; /? + /. We know from A.9) 
that U{I + /} remains bounded for every finite interval /. By the selection 
theorem 2 of VIII,6 there exists therefore a sequerice tk -> oo and a measure 
V such that 
B.10) U{tk + dy}-+V{dy}. 
The measure V is finite on finite intervals, but is not concentrated on 0, co. 
Now let z be a continuous function vanishing outside the finite interval 
0, a. For the corresponding solution Z of the renewal equation we have 
then 
B.11) Z{tk + x) = (az(-s) U{tk + x + ds} -> \z(-s) V{x + ds}. 
Jo Jo 
From the preceding lemma it follows that the family of measures V{x 4- ds} 
366 RENEWAL THEORY XI.3 
is independent ofz, and hence V{I) must be proportional to the length of 
/. Thus B.10) may be put in the form 
B.12) U(tk) - U{tk-h)-+yh. 
This is the same as the assertion A.9) of the renewal theorem except that 
the factor rj is replaced by the unknown y and that / is restricted to the 
sequence {tk}. However, our derivation of the alternative form of the 
renewal theorem remains valid and thus 
'2.13) Z(tk)-+y\ z(y)dy 
Jo 
whenever z is directly integrable. 
The function z = 1 — F is monotone, and its integral equals ju. The 
corresponding solution Z reduces to the constant 1. If// < oo the function 
z is directly integrabje, and B.13) states that y/u ¦= 1. When fx = oo we 
truncate z and conclude from B.13) that y~x exceeds the integral of z 
over an arbitrary interval 0, a. Thus // = oo implies y = 0. Hence the 
limit in B.12) is independent of the sequence {tk}, and B.12) reduces to the 
assertion A.9) of the renewal theorem. *> 
*3. REFINEMENTS 
In this section we show how regularity properties of the distribution F 
may lead to sharper forms of the renewal theorem. The results are not 
exciting in themselves, but they are useful in many applications. 
Theorem 1. If F is non-arithmetic with expectation u and variance 
a2, then 
C.1) '0 < 
The renewal theorem itself states only that U(t)~ tj/u, and the estimate 
C.1) is much sharper. It is applicable even when no variance exists with the 
right side replaced by oo. [The analogue for arithmetic distributions is given 
byl; 
Proof. Put 
C.2) Z(t) = U(t) - 
It is easily verified that this is the solution of the renewal equation correspond- 
ing to 
C.3) z(t) = -. \[l-F{y)\dy. 
* This section should be omitted at the first reading. 
XI.3 REFINEMENTS 367 
Integrating by parts we get 
J%°° If00 T2 -U II2 
z{t)dt = ±\ y2 F{dy) = a-±lL . 
o 2/u Jo Ifx 
Being monotone z is directly integrable, and the alternative form of the 
renewal theorem asserts that C.1) is true. * 
Next we turn to smoothness properties of the renewal function U. If F 
has a density / the renewal equation for U takes on the form 
C-5) V(x) = 1 + (*U(z-y)f(y) dy. 
Jo 
If / is continuous a formal differentiation would indicate that U should 
have a derivative u satisfying the equation 
C.6) u(x) = f{x) + (*u(x-y)f(y) dy. 
Jo 
This is a renewal equation of the standard type, and we know that it has a 
unique solution whenever / is bounded (not necessarily continuous). It is 
easily verified that the function U defined by 
C.7) U(t) = 1 + fu(y) dy, t > 0. 
Jo 
satisfies C.5) and hence the solution u of C.6) is indeed a density for U. 
As a corollary to the alternative form of the renewal theorem we get thus 
Theorem 2. If F has a directly integrable density f, then U has a density 
u such that u(t)-^- fir1. 
Densities that are not directly integrable will hardly occur in practice but certain con- 
clusions are possible even for them. In fact, consider the density 
¦ f fit-I 
Jo 
C.8) /2(r)= f(t-y)f(y)dy 
Jo 
of Fie F. In general /2 will behave much better than /. For example, if /< M we get 
for reasons of symmetry 
C.9) /2(O < 2M[1 - FdOl 
If ju < oo the right side is a monotone integrable function and this implies that /2 is 
directly integrable. Now u — f is the solution of the renewal equation with z =/2, and 
we have thus 
Theorem 2a. If F has a bounded density f and a finite expectation //, then 
C.10) u(t) - f(t)-* ju-\ 
This result is curious because it shows that if the oscillations of / are wild, u will 
oscillate in a manner to compensate them. (For related results see problems 7-8.) 
368 RENEWAL THEORY XI.4 
The condition that / be bounded is essential. We illustrate this by an example which also 
throws new light on the condition of direct integrability in the renewal theorem. 
Examples, (a) Let G be the probability distribution concentrated on 0, 1 and defined 
by 
C.1D G(x) = : rrr, 0 < x <, 1. 
It has a density that is continuous in the open interval, but since x~1G(x) — oo as x ->- 0 
the density is unbounded near the origin. The sum of n independent random variables 
with the distribution G is certainly <x if each component is <x/n, and hence 
C.12) G^*(*) > (G{x/n))n. 
It follows that for each n the density of Gn* is unbounded near the origin. 
' Now put F(x) = G(x - 1). Then Fn*(x) = Gn*(x - n), and hence Fn* has a 
density which vanishes tor x < n, is continuous for x > n, but unbounded near n. The 
density u of the renewal function U = ^ Fn* is therefore unboundediotheneighborhood 
of every integer n > 0. 
(b) The density u of the preceding example satisfies C.6) which agrees with the standard 
renewal equation A.3) with z = /. This is an integrable function vanishing for x > 2 and 
continuous except at the point 1. The fact that the solution Z — u is unbounded near 
every integer shows that the renewal theorem breaks down if z is not properly Riemann 
integrable (bounded), even when z is concentrated on a finite interval. > 
4. PERSISTENT RENEWAL PROCESSES 
The renewal theorem will now be used to derive various limit theorems 
for the renewal processes introduced in VI,6. We are concerned with a 
sequence of mutually independent random variables T1? T2, . . . , the 
interarrival times, with a common distribution F. In this section we assume 
that F is a proper distribution and F@) = 0. In addition to the Tk there 
may be defined a non-negative variable So with a proper distribution Fo. 
We put 
D.1) S, = S0 + T1 + --- + Tn. 
The variables Sn are called renewal epochs. The renewal process {Sn} is 
called pure if So = Q and delayed otherwise. 
We adhere to the notation ?/=*]? Fn* introduced in A.2). The expected 
I—I 
number of renewal epochs in 0, t equals 
D.2) V(t) =!*>{$„ <t}=F0*U 
n=0 
For h > 0 we have therefore4 
-t+k 
J-t+k 
[U(l+h-y)- U(t-y))F0{dy}. 
0 
4 We recall from section 1 that the intervals of integration are taken closed; the limits of 
integration may therefore be replaced by — oo and oo. 
XI.4 PERSISTENT RENEWAL PROCESSES 369 
If F is not arithmetic the integrand tends to fx~xh as t -> oo, and thus the 
basic theorem extends also to delayed processes: if F is non-arithmetic 
the expected number of renewal epochs within t, t+h tends to' fjrxh. This 
statement contains two equidistribution theorems; first, the renewal rate 
tends to a constant, and second, this constant rate is independent of the 
initial distribution. In this sense we have an analogue to the ergodic theorems 
for Markov chains in 1; XV. 
If fx < oo it follows that V(t) ~ yrH as / —»¦ oo. It is natural to ask 
whether Fo can be chosen as to get the identity V{t) = yrxt, meaning a 
constant renewal rate. Now V satisfies the renewal equation 
D.4) v = F0 + F*V 
and thus V(t) = fi~xt iff 
D-5) F0(t) = -~- \\t-y)F{dy}. 
/x /u Jo 
Integration by parts shows this to be the same as 
D-6) F0(t) = - P[l - F(y)} dy. 
J 
This Fo is a probability distribution and so the answer is affirmative: with 
the initial distribution D.6) the renewal rate is constant. V{t) = ju~xt. 
The distribution D.6) appears also as the limit distribution of the residual 
waiting times, or hitting probabilities. To given t > 0 there corresponds a 
chance-dependent subscript Nr such that 
D.7) SN( < / < SN(+1. 
In the terminology introduced in VI,7 the variable SN _hl — t is called 
residual waiting time at epoch /. We denote by //(/, ?) The. probability 
that it is <;?. In other words, //(/, ?) is the probability that the first 
__i 
renewal epoch following epoch t lies within /, f-f ?, or that ihe k^d t 
be overshot by an amount <?. This event occurs if some renewal epoch Sn 
equals x < r and the following interarrival lime lies between r — x and 
/ — x + ?. In the case of a pure renewal process we get, summing over 
x and n, 
D.8) H(t, f) =, I u{dj:}[F(t-z + 
Jo 
Th'.s integral contains ? as a free parameter but is of ihe standard form 
U ~k z with z(t) = F(t+?) — F(t), which function is directly integrable.5 
5 For n? < x < (n + 1)! we have z(x) < F((n+2)$) — F(n?), and the series with 
these terms is obviously convergent. 
370 RENEWAL THEORY XI.4 
Assume F non-arithmetic. Since 
D.9) f%(/) dt = ["{[I - F(t)] - [1 - /•(' + ?)]) dt = f*(J - F(s)) ds 
Jo Jo Jo 
we have the limit theorem 
D.10) lim H(t, f) = fT1 P[1 - F(s)] <fr. 
f -»oo Jo 
(It is easily verified that this is true also for the delayed process regardless 
of the initial distribution Fo.) This limit theorem is remarkable in several 
respects. As the following discussion shows, it is closely connected with the 
inspection paradox of VI,7 and the waiting time paradox in 1,4. 
When fx < oo the limit distribution D.10) agrees with. D.6) and thus 
if fx < co the residual waiting time has a proper limit distribution which 
coincides with the distribution attaining a uniform renewal rate. In this 
pattern we recognize one more the tendency towards a "steady state." 
The limit distribution of D.10) has a finite expectation only if F has a 
variance. This indicates that, roughly speaking, the entrance probabilities 
behave worse than F. Indeed, when /u == oo we have 
D.11) H(t, ?)-*0 
for all ?: the probability tends to 1 that the level t will be overshot by an 
arbitrarily large amount ?. (For the case of regularly varying tails more 
precise information is derived in XIV,3.) 
Examples, (a) Superposition of renewal processes. Given n renewal proc- 
esses, a new process can be formed by combining all their renewal epochs 
into one sequence. In general the new process is not a renewal process, 
but it is easy to calculate the waiting time W for the first renewal following 
epoch 0. We shall show that under fairly general conditions the distribution 
of W is approximately exponential and so the combined process is close to a 
Poisson process. This result explains why many processes (such as the in- 
coming traffic at a telephone exchange) are of the Poisson type. 
Consider n mutually independent renewal processes induced by the 
distributions of their interarrival times by Fx,. . . , Fn with expectations 
/*!,..., /v Put 
D.12) l + ...+l = I. 
We require, roughly speaking, that the renewal epochs of each individual 
renewal process are extremely rare so that the cumulative effect is due to 
many small causes. To express this we assume that for fixed k and y the 
probabilities Fk(y) are small and /uk large—an assumption that becomes 
meaningful in the form of a limit theorem. 
XI.4 PERSISTENT RENEWAL PROCESSES 371 
Consider the "steady state" situation where the processes have been 
going on for a long time. For the waiting time W* to the nearest renewal 
epoch in the kth process we have then approximately 
D.13) P{W, ^ t} * i j\l - Fk(y)) dy**-. 
[The last approximation is justified by the smallness of Fk(y).] The waiting 
time W in the cumulative process is the smallest among the waiting 
times Wj. and hence 
D.14) P{W > t} * (l - -\ ¦ ¦¦ ¦ ({ - ±\ w e-*: 
This estimate is easily made precise, and under the indicated conditions 
the exponential distribution emerges as the limit distribution as n —»¦ oo. 
(b) Hitting probabilities in random walks. For a sequence of independent 
random variables X1? X2, ... let 
Yn = X> + • • • + Xn. 
For positive Xk the random walk {Yn} reduces to a renewal process, but 
we consider arbitrary Xk. Assume the random walk to be persistent so 
that for each t > 0 with certainty Yw > / for some n. If N is the smallest 
index for which this is true YN is called the point of first entry into t, oo. 
The variable YN — t is the amount by which the / level is overshot at the 
first entry and corresponds to the residual waiting time in renewal processes. 
We put again P{YN <?/-!-?} = H(t, ?), and show how the limit theorem 
for residual waiting times applies to this distribution. 
Define Sj as the point of first entry into 0, oo and, by induction, Sn+1 
as the point of first entry into Sn5 oo. The sequence Sx, S2,. .. coincides 
with the ladder heights introduced in VI,8 and forms a renewal pro.cess: 
the differences Sn+1 — Sn are evidently mutually independent and have the 
same distribution as So. Thus YN — / is actually the residual waiting 
time in the renewal process {SJ, and so D.10) applies. > 
By the method used to derive D.10) it can be shown that the spent waiting 
time t — SN has the same limit distribution. Fcr the length Lt = SN(+1 — SN< 
of the interarrival time containing the epoch t we get 
D.15) P{L, < |} = f U{dz}[F($) - F(t-x)] 
Jt-t 
and hence 
D.16) lim P{L< ? ?} = [T1 \\f& - F(y)] dy = /*"* [x F(dx). 
(-.» JO JO 
372 RENEWAL THEORY XL5 
The curious implications of this formula were discussed in connection with 
the inspection paradox in VI,7 and the waiting time paradox in 1,4. 
It is easily seen that the three families of random variables / - SN/, SN/+1 - /, and L 
form Markov processes with stationary transition probabilities. Our three limit theorems 
therefore represent examples for ergodic theorems for Markov processes (See also 
XIV.3.) 
5. THE NUMBER N« OF RENEWAL EPOCHS 
For simplicity we consider a pure renewal process so that the rth renewal 
epoch is the sum of r independent variables with / as distribution. The 
origin counts as renewal epoch. We denote by N( the number of renewal 
epochs within 0, t. The event {N, > r) occurs iff the rth renewal epoch 
¦ i—i 
falls within 0, t, and hence 
E.1) P{N( > r) = Fr*(t). 
Obviously Nt > 1. It follows that 
E.2) E(N<) =2P{N( > r] = U(t). 
(For higher moments see problem 13.) 
The variable Nt occurs also in sequential sampling. Suppose that a 
sampling {Tn} is to continue until the sum of the observations for the first 
time exceeds t. Then Nt represents the total number of trials. . Many 
tedious calculations might have been saved by the use of the estimate C.1) 
provided by the refined renewal theorem. 
If F has expectation fx and variance a2 the asymptotic behavior of the 
distribution of Nt is determined by the fact that FT* is asymptotically 
normally distributed. The neqessary calculations can be found in 1; XIII,6 
and do not depend on the arithmetic character of F. We have therefore the 
general . 
Central limit theorem for Nt. If F has expectation fX and variance a2 
then for large t the number Nt of renewal epochs is approximately normally 
distributed with expectation tfT1 and variance ta2fj,~3. 
Example, (a) Type I counters. The incoming particles constitute a 
Poisson process. A particle reaching the counter when it is free is registered 
but locks the counter for a. fixed duration ?. Particles reaching the counter 
during a locked period have no effect whatever. For simplicity we start the 
process at an epoch when a new particle reaches a free counter. We have 
then two renewal processes. The primary process—the incoming traffic—is a 
XI.5 THE NUMBER Nt OF RENEWAL EPOCHS 373 
Poisson process, that is, its interarrival times have an exponential distribution 
1 — e~ct with expectation c~l and variance c~2. The successive registrations 
form a secondary renewal process in which the interarrival times represent 
the sum of f plus an exponential random variable. The waiting time 
between registrations has therefore expectation ? + c and variance c~2. 
Thus the number of registrations within the time interval 0. / is approximately 
normally distributed with expectation ^(l+c^) and variance tc(l+c?)~3. 
The discrepancy between these quantities shows that the registrations 
are not Poisson distributed. In the early days it was not understood that the 
registration process differs essentially from the primary process, and the 
observations led some physicists to the erroneous conclusion that cosmic ray 
showers do not conform to the Poisson pattern of "perfect randomness." > 
A limit distribution for Nt exists iff F belongs to some domain of attraction. These 
domains of attraction are characterized in IX,8 and XVII,5 and it follows that Nt has a 
proper limit distribution iff 
E.3) 1 - F(x) ~ x~« L{x), x— .o 
where L is slowly varying and 0 < a < 2. The limit distribution for Nt is easily obtained 
and reveals the paradoxical properties of fluctuations. The behavior is radically different 
for a < 1 and a > 1. 
Consider the case 0 < a < 1. If ar is chosen so that 
E.4) . ¦ r{\ - F(aT)] 
a. 
then Fr*(a1x) —- Ga{x) where Ga is the one-sided stable distribution satisfying the con- 
dition x'[l - Ga(x)] -> B - a)/a as x -* co. (Cf. IX,6 and XVII.5 as well as XIII,6.) 
Let r and t increase in such a manner that / ~ a^. On account of the slow variation 
of L we get then from E.3) and E.4) 
2 - a x-1 
E.5) . r 
a I - F(t) 
whence from E.1) 
E.6) pJ[1 - 1^ \ 
This is an analogue to the central limit theorem. The special case a = i is covered in 1; 
XIII,6. The surprising feature is conveyed by the norming factor 1 — F(t) in E.6). Very 
roughly 1 - F(t) is cf the order of magnitude t-<* and so the probable order of magnitude 
of N( is of the order t'; the density of the renewal epochs must decrease radically (which 
agrees with the asymptotic behavior of the hitting probabilities). 
When 1 < a < 2 the distribution F has an expectation ft < >x and the same type of 
calculation shows that 
f t - Mt)x\ 
E.7) PN(> \-Gx(x) 
J/4 
where 
E.8) 
satisfies 
RENEWAL 
./[! - F(Ht))] 
THEORY 
2 - a 
-+ u. 
XI.6 
In this case the expected number of renewal epochs increases linearly, but the norming 
X(t) indicates that the fluctuations about the expectation are extremely violent. 
6. TERMINATING (TRANSIENT) PROCESSES 
The general theory of renewal processes with a defective distribution F 
reduces almost to a triviality. The corresponding renewal equation, however^ 
frequently appears under diverse disguises with accidental features obscuring 
the general background. A clear understanding of the basic facts will avoid 
cumbersome argument in individual applications. In particular, the 
asymptotic estimate of theorem 2 will yield results previously derived by 
special adaptations of the famous Wiener-Hopf techniques. 
To avoid notational confusion we replace the underlying distribution 
F by L. Accordingly, in this section L stands for a defective distribution 
with L@) = 0 and L(oo) = Lw < 1. It serves as distribution of the 
(defective) interarrival times Tfc, the defect 1 — Lw representing the 
probability of a termination. The origin of the time axis counts as renewal 
epoch number zero, and Sn = T1 -f • • • -f- Tn is the nth. renewal epoch; 
it is a defective variable with distribution Ln* whose total mass equals 
Ln*(co) = ??,. The defect \ — L% is the probability of extinction before 
the nth. renewal epoch; We put again 
F.1) C/ = |lw*. 
As in the persistent process, U(t) equals the expected number of renewal 
epochs within 0,/, this time, however, the expected number of renewal 
epochs ever occurring is finite, namely 
F.2) C/(oo) = —L— . 
The probability that the nth renewal epoch Sr is the last and <x equals 
(i -L^) Ln*(x). We have thus 
Theorem I. A transient renewal process commencing at the origin terminates 
with probability one. The epoch of termination M {that is, the maximum 
attained by the sequence 0, Sl5 S2, . . .) has the proper distribution 
F.3) P{M < x) = A -LJ U(x). 
The probability that the «th renewal epoch is the last equals (l—L^L*, 
and so the number of renewal epochs has a geometric distribution. 
XI.6 TERMINATING (TRANSIENT) PROCESSES 375 
It is possible to couch these results in terms of the (defective) renewal 
equation 
F.4)' Z(t) = z@ + \'z(t-y) L{dy}, 
Jo 
but with a defective L the theory is trite. Assuming again that z(x) = 0 for 
x < 0 the unique solution is given by 
F.5) Z(t) = \\t-y) U{dy} 
Jo 
and evidently 
F.6) 
L 
oo 
whenever z(/)—>-z(oo) as/.—>-oo. 
Examples, (a) The event {M <; t) occurs if the process terminates with 
So, or else if Tx assumes some positive value y < t and the residual 
process attains an age <, t — y. Thus Z(t) = P{M < /} satisfies the 
renewal equation 
F.7) 7@ = 1 - L, 
This is equivalent to {p.5). 
{b) Calculation of moments. The last equation represents the proper 
distribution Z as the sum of two defective distributions, namely a con- 
volution and the distribution with a single atom at the origin. To calculate 
the expectation of Z put 
F.8) EL = xL{dx) 
Jo 
and similarly for other distributions, whether defective or not. Since L is 
defective the convolution* in F.7) has expectation L^ ¦ Ez + EL. Thus 
Ez = EJ(\ — La,). For the more general equation F.4) we get in like 
manner. 
Ez + EL 
F.9) Ez = ' L . 
Higher moments can be .calculated by the same method. > 
Asymptotic estimates 
In applications z(t) usually tends to a limit z(oo), and in this case 2{t) 
tends to the limit Z(oo) given by F.6). It is frequently important to obtain 
asymptotic estimates for the difference Z(oo) — Z(/), This can be achieved 
by a method of wide applicability in the theory of random walks [see the 
376 RENEWAL THEORY XI.6 
associated random walks in example XH,4F)]. It depends on the (usually 
harmless) assumption that there exists a number k such that 
F.10) ^V» L{dy) = 1. 
Jo 
This root k is obviously unique and, the distribution L being defective, 
k > 0. We now define a proper probability distribution 1/ by 
F.11) L#{dy] = eKVL{dy} 
and associate with each function / a new function /# defined by 
/#(*) = eK*f(x). 
A glance at F.4) shows that the renewal equation 
FJ2) Z#{t) = z#(t) + C'Z^it-y) L#{dy) 
Jo 
holds. Now if Z#(t)-+a 5*0 then Z(t)~ge-Kt. Accordingly, if z# is 
directly integrable [in which case z(oo) = 0] the renewal theorem implies 
that 
F.13) e- j \ 
fi" Jo , 
where 
F.14) p* = {* eft/L{dy) 
Jo 
In F.13) one has a good estimate for Z(/) for large /. 
With a slight modification this procedure applies also when z(co) ^ 0. 
Put 
zx{t) = z 
It is easily verified that the difference Z(oo) — Z(t) satisfies the standard 
renewal equation F.4) with z replaced by zx. A simple integration by parts 
shows that the integral of zf(x) = zl(x)eKX is given by the right side in F.15). 
Applying F.13) to Zx we get therefore 
Theorem 2. If F.10) holds then the solution of the renewal equation satisfies 
F.15) {j,#eKt[Z(oo) - Z{t)\ — ^ + r°°e^[z(oo) - z{x)\ dx 
K Jo 
provided fx jt oo an^ zx w directly integrable. 
XI.7 DIVERSE APPLICATIONS 377 
For the particular case F.7) we get 
F.16) P{M > /} ~ * ~ Lg° e-Kt. 
The next section will show the surprising power of this estimate. In 
particular, example (b) will show that our simple method sometimes leads 
quickly to results that used to require deep and laborious methods. 
The case Loo > 1. If ?<» > 1 there exists a constant k < 0 such that F.10) is true 
and the transformation described reduces the integral equation F.4) to F.12). The 
renewal theorem therefore leads to precise estimates of the asymptotic behavior of Z(t)eKt 
[The discrete case is covered in theorem 1 of 1; XIII,10. For applications in demography 
see example 1; XII,10(<?).] 
7. DIVERSE APPLICATIONS 
As has been pointed out already the theory of the last section may be 
applied to problems which are conceptually not directly related to renewal 
processes. In this section we give two independent examples. 
(a) Cramer's estimates for ruin. It was shown in VI,5 that the ruin 
problem in compound Poisson processes and problems connected with 
storage facilities, scheduling of patients, etc., depends on a probability 
distribution R+ concentrated on 0, oo, and satisfying the integro-differential 
equation 
G.1) /?¦(«).= - /?(*)'- - \R(z-x) F{dx) 
c c Jo 
where F is a proper distribution.6 Integrating G.1) over 0, / and per- 
forming the obvious integration by parts one gets 
G.2) R(t) - R@) =r- f R(t-x)[\ - F(x)] dx. 
c Jo 
Here R@) is an unknown constant, but otherwise G.2) is a renewal equation 
with a defective distribution L with density a/c [1 — F(x)]. Denoting the 
expectation of F by a the mass of L equals Lx = ac/ujc . [The process is 
meaningful only if LK < 1 for otherwise R(i) = 0 for all t.\ Note that 
G.2) is a special case of F.4) and that i?(oo) = 1. Recalling F.6) we con- 
clude that 
G.3) R@) = 1 - 
6 This is the special case of VI,E.4) when F is concentrated on 0, oo. It will betreated 
by Laplace transforms in XIV,2F). The general situation will be taken up in XII,5(</). 
378 RENEWAL THEORY XI.7 
and with this value the integral equation G.2) reduces to the form F.7) for 
the distribution of the lifetime M of a terminating process with interarrival 
time distribution L. From F.16) it follows that if there exists a constant 
K such that 
- f 
c Jo 
G.4) - f Vx[l - F(x)] dx = 1 
c Jo 
and 
r oo 
G.5) ilu = - eKXx[\ - F(x)) dx < oo 
c Jo 
then as t —*- oo 
G.6) i-R(t)—Ui-^y*. 
k/jT \ c } 
This is Cramer's famous estimate originally derived by deep complex 
variable methods. The moments of R may be calculated as indicated in 
example 6(b). 
(b) Gaps in Poisson processes. In VI,7 we derived a renewal equation for 
the distribution V of the waiting time for the first gap of length >| in a 
renewal process. When the latter is a Poisson process the interarrival times 
have an exponential distribution, and the renewal equation VI,G.1) is of 
the standard form V = z + F* L with 
L{x) = 1 - e~cx, z(x) = 0 for x < | 
G.7) 
L(x) = 1 - e~cS, z(x) = e~eS for x > |. 
Since z(oo) = 1 — L^ the solution V is a proper distribution as required 
by the problem. 
The moments of our waiting time W are easily calculated by the method 
described in example 6(b). We get 
e«- 
1 
G.8) E(W) = , Var (W) = 
c2 
If we interpret W as the waiting time for a pedestrian to cross a stream 
of traffic these formulas reveal the effect of an increasing traffic rate. The 
average number of cars during a crossing time is c|. Taking c| = 1,2 
we get E(W) «s 1.72? and E(W) «* 3.2|, respectively. The variance 
increases from about P to 6?2. [For explicit solutions and connection 
with covering theorems see example XIV,2(a).] The asymptotic estimate 
F.14) applies. If c? > 1 the determining equation F.10) reduces to 
G.9) ce{K-c)* = k, 0<k<c 
and by a routine calculation we get from F.14) 
G.10) 1 - V(t) ~ t ~ kI° e 
1 | 
~Kt 
XI.8 EXISTENCE OF LIMITS IN STOCHASTIC PROCESSES 379 
8. EXISTENCE OF LIMITS IN STOCHASTIC PROCESSES 
Perhaps the most striking proof of the power of the renewal theorem is 
that it enables us without effort to derive the existence of a "steady state" 
in a huge class of stochastic processes. About the process itself we need 
assume only that the probabilities in question are well defined; otherwise 
the theorem is purely analytic.7 
Consider a stochastic process with denumerably many states EQ, Eu . . . 
and denote by Pk(t) the probability of Ek at epoch / > 0. The following 
theorem depends on the existence of "recurrent events," that is, of epochs 
at which the process starts from scratch. More precisely, we assume that 
with probability one there exists an epoch Sx such that the continuation of 
the process beyond Sx is a probabilistic replica of the whole process 
commencing at epoch 0. This implies the existence of further epochs 
S2, S3, . . . with the same property. The sequence {Sn} forms a persistent 
renewal process, and we assume that the mean recurrence time /t = E(Sx) 
is finite. We denote by Pk{t) the conditional probability of the state Ek at 
epoch t + s given that Si = s. It is assumed that these probabilities are 
independent of s. Under these conditions we prove the important 
Theorem 
(8.1) UmPk(t) = pk 
t— oo 
exists wiih pk>0 and %pk = 1. 
Proof. Let qk(t) be the probability of the joint event that Sx > / and 
that at epoch / the system is in state Ek. Then 
(8.2) fqk(t) = 1 - F(t) 
where F is the distribution of the recurrence times S,l+1 — Sn. By hypothesis 
(8.3) Pk(t) = qk(t) + I'p^t-y) F{dy). 
Jo 
The function qk is directly integrable since it is dominated by the monotone 
integrable function 1 — F. Therefore 
1 f00 • 
(8.4) lim/\@ = - <7*@<fr 
/-¦oo //Jo 
by the second renewal theorem. Integration of (8.2) shows that these limits 
add to unity, and the theorem is proved. > 
7 For more sophisticated results see V. E. BeneS, A "renewal" limit theorem for general 
stochastic processes, Ann. Math. Statist., vol. 33 A962) pp. 98-113, or his book A963). 
380 RENEWAL THEORY XI.9 
It is noteworthy that the existence of the limit (8.1) has been established 
without indication of a way to compute them. 
Note. If Fo is a proper distribution, then (8.1) implies that 
f 
(8.5) f'k(t~V) Foidy) ~+ Pk as *¦ —*¦ °°- 
Jo 
Thus the theorem also covers the case of a delayed renewal process {Sn} 
in which So has distribution Fo. 
Examples, (a) Queuing theory. Consider an installation (telephone ex- 
change, post office, or part of a computer) consisting of one or more "servers," 
and let the state Ek signify that there are k "customers" in the installation. 
In most models the process starts from scratch whenever an arriving customer 
finds the system in state Eo; in this case our limit theorem holds iff such an 
epoch occurs with probability one and the expectations are finite. 
(b) Two-stage renewal process. Suppose that there are two possible states 
Ex, E2. Initially the system is in Ex. The successive sojourn times in Ex 
are random variables X, with a common distribution Fx. They alternate 
with sojourn times Y, in E2, having a common distribution F2. Assuming, 
as usual, independence of all the variables we have an imbedded renewal 
process with interarrival distribution F — F±i^ F2. Suppose E(X,) = 
= u^ < oo and E(Y3) = ju2 < oo. Clearly qx{t) — 1 — Fx(t) and therefore 
as / —*¦ oo the probabilities of Ek tend to the limits 
(8.6) i@ , 2() 
This argument generalizes easily to multi-stage systems. 
(c) The differential equations of 1; XVII correspond to stochastic 
processes in which the successive returns to any state form a renewal process 
of the required type. Our theorem therefore guarantees the existence of limit 
probabilities. Their explicit form can be determined easily from the differ- 
ential equations with the derivatives replaced by zero. [See, for example, 1; 
XVII,G.3). We shall return to this point more systematically in XIV,9. 
The same argument applies to the semi-Markov process described in problem 
14 of XIV,10.] > 
*9. RENEWAL THEORY ON THE WHOLE LINE 
In this section the renewal theory will be generalized to distributions 
that are not concentrated on a half-line. To avoid trivialities we assume 
Not used in the sequel. 
XI.9 RENEWAL THEORY ON THE WHOLE LINE 381 
that F{ — oo, 0) > 0 and F{0, 00} > 0 and that F is non-arithmetic. The 
modifications necessary for arithmetic distributions will be obvious by 
analogy with section 1. 
We recall from VI, 10 that the distribution F is transient iff 
(9.1) ?/{'} =!>"*{/} 
is finite for all finite intervals. Otherwise U{I) = 00 for every interval and 
F is called persistent. For transient distributions the question imposes 
itself: do the renewal theorems of section 1 carry over? This problem has 
intrigued many mathematicians, perhaps less because of its intrinsic 
importance than because of its unsuspected difficulties. Thus the renewal 
theorem was generalized step by step to various special classes of transient 
distributions by Blackwell, Chung, Chung and Pollard, Chung and Wolfo- 
witz, Karlin, and Smith, but the general theorem was proved only in 1961 
by Feller and Orey using probabilistic and Fourier analytic tools. The 
following proof is considerably simpler and more elementary. In fact, 
when F has a finite expectation the proof given in section 2 carries over 
without change. (For renewal theory in the plane see problem 20.) 
For the following it must be recalled that a distribution with an expectation 
ju 5«s 0 is transient (theorem 4 of VI,10). As usual, /+ / denotes the 
interval obtained by translating / through /. 
General renewal theorem. If F has an expectation /u > 0 then for every 
finite interval I of length h > 0 
(9.2) U{I + /}'—-¦ t^oo 
(9.3) U{I + t}^0 /—.-oo. 
(b) If F is transient and without expectation then ?/{/+/}—*0 as 
t -> ±00 for every finite interval I. 
From now on it is understood that F is transient and that z is a con- 
tinuous function vanishing outside the finite interval —h < x < h where 
z > 0. 
Before proceeding to the proof we recall a few facts proved in VI,10. 
The convolution Z = U' it z is well definad by 
*+oo 
(9.4) Z(x) = z(x-y) U{dy) 
J—00 
because the effective domain of integration is finite. According to theorem 
2 of VI,10 this Z is a continuous function satisfying the renewal equation 
(9.5) Z = z + F*Z, 
and it assumes its maximum at a point ? at which z(?) > 0. 
382 RENEWAL THEORY XI.9 
Put Un = F°* + • • • + Fn*. Every non-negative solution Z of (9.5) 
satisfies Z >z = UQ~k z, and hence by induction Z > Unic z. Tt follows 
that the solution (9.4) is minimal in the sense that Z±>Z for any other non- 
negative solution Zi. Since ZY — Z + const, is again a solution, it follows 
that 
(9.6) lim infZ(x) = 0, x-+±co. 
Lemma 1. For every constant a 
(9.7) Z(x + a) - Z(x) -+ 0 x -* ± oo. 
Proof. The proof is identical with the proof of lemma 2 in section 2. 
There we used the fact that a bounded uniformly continuous solution of the 
convolution equation 
(9.8) ? = F* I 
attaining its maximum at x — 0 reduces to a constant. This remains true 
also for distributions not concentrated on 0, oo, and the proof is actually 
simpler because now the set ]T formed by the points of increase of F, 
F2*, ... is everywhere dense. > 
Although we shall not use it explicitly, we mention the following interesting 
Corollary.8 Every bounded continuous solution of (9.8) reduces to a constant. 
Proof. If $ is uniformly continuous the proof of lemma 1 applies without change. 
Now if G is an arbitrary probability distribution then ?x = G -k ? is again a solution 
of (9.8). We may choose G such that ?± has a bounded derivative and is therefore 
uniformly continuous. In particular, ihc convolution of ? with an arbitrary normal 
distribution reduces to a constant. Letting the variance of G tend to zero we see that ? 
itself reduces to a constant. *¦ 
Proof of the renewal theorem when expectations exist. When 0 < ju < oo 
the proof in section 2 applies with one trite change. In the final relation B.13) 
we used the trial function z = 1 — F for which the solution Z reduced 
to the constant 1. Now we use instead 
(9.9) 2 = F°* - F. 
For it Un • z = 'F°* — F(n+l>*, and since ju > 0 it /s clear that Z = F°*. 
It should be noticed that this proof applies also if /u = +od in the 
obvious sense that the integral of x F{dx) diverges over 0, oo, but converges 
over —oo,0. > 
8 This corollary holds for distributions on arbitrary groups. See G. Choquet and J. 
Deny, C. R. Acad. Sci. Paris, vol. 250 A960) pp. 799-801. 
XI.9 RENEWAL THEORY ON THE WHOLE LINE 383 
When no expectation exists the proof requires a more delicate analysis. 
The next lemma shows that it suffices to prove the assertion for one tail. 
Lemma 2. Suppose that no expectation exists and that 
(9.10) 
Then also 
(9.11) 
Proof. We use the result of example 4(b) concerning hitting probabilities in 
the random walk governed by F. Denote by H{t, |) the probability that 
the first entry into /, oo takes place between / and / + ?. Relative to 
/ + x the interval / + / occupies the same position as / — x relative to / 
and hence 
(9.12) U{I + t] = f°°#(f, </?) U{I - ?}. 
Jo 
Considering the first step in the random walk one sees that 
(9.13) 1 -H@, |)^1 -F(tj), 
We know already that the assertion is true if/u<oo or ^ = —oo, that is, 
if the right side is integrable over 0, 00. Otherwise H has an infinite 
expectation, and hence H(t, ?)-*-0 as / —> 00 for every ?. For large / 
therefore only large values of ? play an effective role and for them U{I — ?} 
is small. Thus (9.11) is an immediate consequence of (9.10) and (9.12). > 
Lemma 3. Suppose Z(x) < m and choose p > 0 such that p' — 
— 1 — pm > 0. To given e > 0 there exists an se such that for s > se 
either 
(9.14) 2{s) < e 
or else 
(9.15) Z(s + x)>p Z(s) Z(x) for all x. 
Proof. Because of the uniform continuity of Z and lemma 1 we can 
choose se such that 
(9.16) Z(s + x) - Z(x) > -ep' for s > se and \x\ < h. 
Put 
(9.17) Vs{x) = Z(s + x) -p Z(s) Z(x), v3(x) = z(s + x) - p Z(s)z(x). 
Vs satisfies the renewal equation Vs = vs + F* Vs and from the remark 
i 
s 
preceding lemma 1 it follows that if Vs assumes negative values, then it. 
384 RENEWAL THEORY XI.9 
assumes a minimum at a point ? where v(?) < 0, and hence \g\ < h. 
In vifew of (9.16) we have then if s > s€ 
(9.18) Va{i) > ~ep' + Z(s)[\ - p Z(f)] > p'[Z(s) - c]. 
Accordingly, either (9.14) holds, or else Vf assumes no negative values in 
which case (9.15) is true. > 
Lemma 4. Let 
(9.19) lim sup Z(x) = ?y x -+ ± oo. 
Then also 
(-9.20) lim sup [Z(x) + Z(—x)] ~ r\ x -> oo. 
Proof. Choose a such that Z(o) < E; this is possible in consequence of 
(9.6). By the preceding lemma we have for sufficiently large s either 
(9.21) p Z(s) Z(a~s) < Z(a) < S, 
or else Z(s) < e. Since e is arbitrary, the inequality (9.21) will hold in 
ajiy case for all s sufficiently large. In view.of lemma 1 this implies9 
(9.22) Z(j)Z(-j)-*0. 
Thus for large x either Z(x) or Z(—x) is small, and since Z > 0 it is 
clear that (9.19) implies (9.20). > 
Proof of the theorem. Assume rj > 0 because otherwise there is nothing 
to be proved. Consider the convolutions of Z and z with the uniform 
distribution 0, /, namely 
(9.23) Wt(x) = - fX Z{y) dy, wt(x) = - f" z{y) dy. 
t Jx-t t Jx-t 
Our next goal is to show that as t — > oo one of the relations 
(9.24) Wt{t) = - [z{y) dy-+rj or Wt@) = - f^y) dy - rj 
t Jo t J-t 
must take place. 
Because of (9.7) the upper bounds for Z(x) and Wt{x) (with / fixed) are 
the same, and hence the maximum of Wt is >rj. On the other hand, Wt 
satisfies the renewal equation (9.5) with z replaced by wt. As noted before, 
9 It is easily seen that (9.22) is equivalent to U{I + t) U{I — t] -* 0. If p{l] stands for 
the probability that the random walk {Sn} governed by F enters /, then (9.22) is also 
equivalent to p{I + t} p{l — ti -*¦ 0. If this were false the probability of coming near the 
origin after a visit to / + t wo61d not tend to 0, and F could not be transient. 
XI. 10 PROBLEMS FOR SOLUTION 385 
this implies that the maximum of Wt is attained at a point where wt is 
positive, that is, between —h and t + h. Now for \t <, x < / 
(9.25) Wt(x) = - P Z{y) dy + - f' \z(y) + Z(-y)] dy. 
t Jt-x t JO 
The combined length of the two intervals of integration is x and 50 it follows 
from (9.20) that for / sufficiently large W't(x) < rj(xjt) + e. thus if 
Wt(x) > r] the point x must be close to / and Wt(t) close to r\. If the 
maximum of Wt is attained at a point a; <; \t a similar argument shows 
that x must be close to 0 and Wt@) close to r\. 
We have now proved that for large / either Wt{t) or ^@) is close to rj. 
But a glance at (9.23) shows that in view of (9.20) 
(9.26) lim sup [Wt(t) + Wt@)] = lim sup r1 [\z{y) + Z(-y)] 42/ < 1?. 
Jo 
Because of the continuity of the two functions therefore either Wt(t) -> rj 
and Wt@) -> 0, or else these relations hold with the limits interchanged. 
For reasons of symmetry we may assume that Wt(t) -> rj, that is 
rt pt/2 
(9.27) Wt(t) = r1 Z(y) dy = r1 [Z(il+y) + Z(|f - y)\ dy - 77. 
Jo Jo 
It follows that for arbitrarily large t there exist values of x such that both 
Z(x) and Z{t — x) are close to r\. By lemma 3 this implies that for large / 
the values of Z(/) are bounded away from 0, and therefore Z(—/)->0 in 
consequence of (9.22). Thus U{I — /}->0 as /-»-oo, and in view of 
lemma 2 this accomplishes the proof. p. 
10. PROBLEMS FOR SOLUTION 
(See also problems 12-20 in VI,13.) 
1. Dropping the assumption F@) =0 amounts to replacing F by the dis- 
tribution F# = pH0 + qF where HQ is concentrated at the origin and p +q = 1. 
Then U is replaced by U# = U/q. Show that this is a probabilistically obvious 
consequence of the definition and verify the assertion formally (a) by calculating 
the convolutions, (ft) from the renewal equation. 
2. If F is the uniform distribution in 0, 1 show that 
n (t- k)k 
U{t) = J (_i)V-fc ., for n <t < n + l. 
fc=0 
This formula is frequently rediscovered in queuing theory, but it reveals little about 
the nature of U. The asymptotic formula 0 < U(t) — 2t-+% is much more 
interesting. It is an immediate consequence of C.1). 
3. Suppose that z > 0 and that \z'\ is integrable over 0, co. Show that z is 
directly integrable. 
386 RENEWAL THEORY XI. 10 
Note: In the next three problems it is understood that Z and Zx are the 
solutions of the standard renewal equation A.3) corresponding to z and zv 
4. If z -* oo yand z1>—- z as x — co show that Zx ~ Z. 
5. If zx is the integral of z and z^O) = 0? then Zx is the integral of Z. 
Conclude that if z = a;" then Z ~ xn/(n/u) provided /< < oo. 
6. (Generalization.) If 2X = G ^f z (where G is a measure finite on finite 
intervals) then also Zx = G + Z. For G(a;) = x<* with fl > 0 conclude: 
// z(x) r^ x?-1 then Z(x) ^ xa/(a/i). 
7. Gb theorem 2 of section 3). Denote by fr the density of Fr* and put 
v = u —/—••• — fr- Show that if fr is bounded, then v(x) -+. l//<. In particular, 
if /-*0 then « -*'l//<. 
8. If F2* has a directly integrable density then V = U - 1 - F has a density 
y tending to l//<. 
9. From D.4) show that Z(t) = V(t) - V(t - h) satisfies the standard renewal 
equation with z(t) = F0(t) - F0(t - h). Derive the result V(t) - V(t - /?) -W1//1 
directly from the renewal theorem. 
10. Joint distribution for the residual and spent waiting times. With the notation 
D.7) prove that as / -> oo 
1 f30 
P{/ - SN< > x, SNt+1 -t>y}^fM-\ [1 - F(s)} ds. 
"Jx'-v 
(Hint: Derive a renewal equation for the left side.) 
11. Steady-state properties. Consider a delayed renewal process with initial 
distribution Fo given by D.6). The probability H(t, ?) that a renewal epoch 
Sw occurs between / and / + ? satisfies the renewal equation 
H(t, I) = Fo(/ + ?) - F0(t) 
Conclude without calculations that H(t, ?) = F0@ identically. 
12. Maximal observed lifetime. In the standard persistent renewal process let 
V(t, I) be the probability that the maximal interarrival time observed up to 
epoch / had a duration > ?. Show that 
V(t, 0 = 1- 
f? 
V(t-y 
Jo 
Discuss the character of the solution. 
13. For the number Nt of renewal epochs defined in section 5 show that 
E(N?) = f BA: + 1)F** (/) = 2U±U(t) - U(t). 
Ar»O 
Using an integration by parts conclude from this and the renewal theorem that 
E(Nt2) = \ [u(x) dx + a-zt + o(t) 
P Jo ' 
and hence Var (Nt) ^ (a'2j/iz)t in accordance with the estimate in the central limit 
theorem. (Note: This method applies also to arithmetic distributions and is 
preferable to the derivation bf the same result outlined in problem 23 of 1; XIII,12.) 
XI. 10 PROBLEMS FOR SOLUTION 387 
14. If F is proper and a a constant, reduce the integro-differential equation 
Z' =aZ - aZ* F to Z(t) = Z@) + a f Z(t-x)[\ - F(x)] dx: 
Jo 
15. Generalized type II counters. The incoming particles constitute a Poisson 
process. The/th arriving particle locks the counter for a duration T, and annuls 
the aftereffect (if any) of its predecessors. The T, are^ independent of each other 
and of the Poisson process and have the common distribution G. If Y is the 
duration of a locked interval and Z{t) = P{Y > t), show that Y is a proper 
variable and 
Jo 
Z(t) = [1 - (/(/)]<?-«' + I Z(t-x) ¦ [1 - G(x)\ce~** dx. 
Show that this renewal process is terminating if and only if G has an expectation 
/l< < or1. Discuss the applicability of the asymptotic estimates of section 6. 
16. Effect of a traffic island. [Example 7F).] A two-way traffic moves in two 
independent lanes, representing Poisson processes with equal densities. The 
expected time required to effect a crossing is 2?, and formulas G.10) apply with 
this-change. A traffic island, however, has the effect that the total crossing time 
is the sum of two independent variables with expectations and variances given in 
G.10). Discuss the practical effect. 
17. Arrivals at a counter constitute a persistent renewal process with distribution 
F. After each registration the counter is locked for a fixed duration ? during 
which all arrivals are without effect. Show that the distribution of the time from 
the end of a locked period to the next arrival is given by 
f 
Jo 
[F($+t-y) -F(S-y)]U{dy}. 
If F is exponential so is this distribution. 
18. Non-linear renewal. A particle has an exponential lifetime at the expiration 
of which it has probability pk to produce k independent replicas acting in the 
same manner (k = 0, 1, . . .). The probability F(t) that the whole process stops 
before epoch t satisfies the equation 
00 ft 
F(t) = po(\ -<?-«*) + J />* ae-»l*-x)Fk(x) dx. 
(No genera! method for handling such equations is known.) 
19. Let F be an arbitrary distribution in &1 with expectation /u > 0 and 
finite second moment m2. Show that 
m2 
where, as usual, x+ denotes the positive part of x. Hint: If Z(t) stands for the 
388 RENEWAL THEORY XL 10 
left side then Z satisfies renewal equation with 
F(x) dx, t < 0 
— 00 
A -F(x)) dx, t > 0 
t 
20, Renewal theorem in 3t2. Let the distribution of the pair (X, Y) be concen- 
trated on the positive quadrant. Let / be the interval 0 < x, y <, 1. For an 
arbitrary vector a denote by / + a the interval obtained by translating / through 
a. Lemma 1 of section 9 generalizes as follows. For any fixed vectors a and b 
U{I + a + /b} - U{I + /b} - 0 
as / -*¦ oo. 
(a) Taking this for granted show that the renewal theorem for the marginal 
distributions implies that U{I + tb) -> 0. 
F) Show that the proof of lemma carries over trivially.10 
10 A more appropriate formulation of renewal problems in the plane has been introduced 
recently by P. J. Bickel and J. A. Yahav, [Renewal theory in the plane, Ann. Math. Statist., 
vol. 36 A965) pp. 946-955]. They consider the expected number of visits to the region 
between circles of radii r and r + a, and let r -*¦ oo. 
CHAPTER XII 
Random Walks 
This chapter treats random-walk problems with emphasis on combinatorial 
methods and the systematic use of ladder variables. Some of the results 
will be derived anew and supplemented in chapter XVIII by Fourier methods. 
(Other aspects of random walks were covered in VI, 10.) In the main our 
attention will be restricted to two central topics. First, it will be shown that 
the curipus results derived in 1; III for fluctuations in coin tossing have a 
much wider validity and that essentially the same methods are applicable. 
The second topic is connected with first passages and ruin problems. It has 
become fashionable to relate such topics to the famous Wiener-Hopf theory, 
but the connections are not as close as they are usually made to appear. 
They will be discussed in sections 3a and XVIII,4. 
E. Sparre Andersen's discovery in 1949 of the power of combinatorial 
methods in fluctuation theory put the whole theory of random walks into a 
new light. Since then progress has been extremely rapid, stimulated also by 
the unexpected discovery of the close connection between random walks and 
queuing problems.1 
The literature is vast and bewildering. The theory presented in the follow- 
ing pages is so elementary and simple that the newcomer would never suspect 
how difficult the problems used to be before their natural setting was under- 
stood. For example, the elementary asymptotic estimates in section 5 cover 
a variety of practical results obtained previously by deep methods and 
sometimes with great ingenuity. 
Sections,6-8 are nearly independent of the first part. It is hardly necessary 
to say that our treatment is one-sided and neglects interesting aspects of 
random walks such as connections with potential theory and group theory.2 
1The first such connection seems to have been pointed out by D. V. Lindley in 1952. 
He derived an integral equation which would now be considered of the Wiener-Hopf type. 
2 For other aspects see Spitzer's book A964), although it is limited to arithmetic distri- 
butions. For combinatorial methods applicable to higher dimensions see C. Hobby and 
R. Pyke, Combinatorial results in multidimensional fluctuation theory, Ann. Math. Statist., 
vol. 34 A963) pp. 402-404. 
389 
390 RANDOM WALKS IN ft1 XII. 1 
1. tfASIC CONCEPTS AND NOTATIONS 
Throughout this chapter Xl9 X2, . . . are independent random variables 
with a common distribution F not concentrated on a half-axis. , [For distri- 
butions with F@) = 0 or F@) = 1 the topic is covered by renewal theory] 
The induced random walk is the sequence of random variables 
(i-i) so = o, sB = x1 + --- + xI,.- 
Sometimes we consider a section (Xm, . . . , Xk) of the given sequence 
{X;}; its partial sums 0, Sm—S,-, . . . , Sfc—S}- will be called a section of 
the random walk. The subscripts are treated in the usual manner as a time 
parameter. Thus an epoch n is said to divide the whole random walk 
into a preceding and a residual section. Because So = 0 the random walk 
is said to start at the origin. By adding a constant a to all terms we obtain 
a random walk starting at a. Thus Sw, Sn+1, ... is a random walk induced 
by F and starting at Sn. 
Orientation. Looking at the graph of a random walk one notices as a 
striking feature the points where Sn reaches a record value, that is, where 
Sn exceeds all previously attained values So, . . . , Sn_x. These are the 
ladder points according to the terminology introduced in VI,8. (See fig. 1 
in that section.) The theoretical importance of ladder points derives from 
the fact that the sections between them are probabilistic replicas of each other, 
and therefore important conclusions concerning the random walk can be 
derived from a study of the first ladder point. 
In volume 1 we have studied repeatedly random walks in which the Xk 
assume the values +1 and —1 with probabilities p and q respectively. 
In such walks each record value exceeds the preceding one by +1, and the 
successive ladder points represent simply the first passages through 1, 2, .... 
In the present terminology we would say that the ladder heights are known 
in advance, and only the waiting times between successive ladder points 
require study. These are independent random variables with the same 
distribution as the first passage time through +1. The generating function 
of this distribution was found in 1; XI,C.6) and is giVen by 
A.2) [1 -Vl -4pqs*]lBqs) 
where *j denotes the positive root [see also 1; XIV ,4; for explicit formulas 
see 1; Xl,3(d) and 1; XIV,5]. When p < q the first passage t'mes are 
defective random variables since the probability that a positive value will 
ever be attained equals p\q. 
The same record value may be repeated several times before a new record 
value is reached. Points of such relative maxima are called weak ladder 
points. [In the simple binomial random walk the first weak ladder point is 
either A, 1) or else it is of the form Br, 0).] 
XII. 1 BASIC CONCEPTS AND NOTATIONS 391 
After these preliminary remarks we proceed to a formal introduction of 
ladder variables repeating in part what was said in VI,8. The definition 
depends on an inequality, and there exist therefore four types of ladder 
variables corresponding to the four possibilities <, <, >, >. This leads 
to a twofold classification to be described by the self-explanatory terms 
ascending and descending, strict and weak. The ascending and descending 
variables are related by the familiar symmetry between plus and minus, or 
maxima and minima. The distinction between strict and weak variables, 
however, puts a burden on description and notation. The simplest way out 
is to consider only continuous distributions F, for then the strict and weak 
variables are the same with probability one. Beginners are advised to 
proceed in this way and not to distinguish between strict and weak ladder 
variables, but this distinction is unavoidable for the general theory on one 
hand, and for examples such as the coin-tossing game on the other. 
To introduce the necessary notations and conventions we consider the 
ascending strict ladder variables. We shall then show that the theory of 
weak ladder variables follows as a simple corollary of the theory of strict 
variables. Descending ladder variables require no new theory. We shall 
therefore take the ascending strict ladder variables as typical, and when no 
danger of corlfusion arises, we shall drop the qualifications "ascending" and 
"strict." 
Ascending strict ladder variables. Consider the sequence of points (n, Sn) 
for n = 1, 2,.. . (the origin is. excluded). The first strict ascending ladder 
point {3~x, ^fi) is the first term in this sequence for which Sn > 0. In other 
words, 3~x is the epoch of the first entry into the (strictly) positive half-axis 
defined by 
A.3) {Fx = n} = {S, < 0, . . . , S^ < 0, Sn > 0}, 
and J^i = S^. The variable $~x is called first ladder epoch, JF± the first 
ladder height. These variables remain undefined if the event A.3) does not 
take place, and hence both variables are possibly defective.3 
For the joint distribution of (&~lf JfJ we write 
A.4) 
The marginal distributions are given by 
A.5) T{^i = «} = Hn(co), n =1,2, ... 
A.6) P{JT1 < x} = f,Hn(x) = H{x). 
The two variables have the same defect, namely 1 — H(co) > 0. 
3 Problems 3-6 provide illustrative exercises accessible without general theory. 
392 RANDOM WALKS IN ft1 XII. 1 
The section of the random walk following the first ladder epoch is a 
probabilistic replica of the whole random walk. Its first ladder point is the 
second point of the whole random walk with the property that 
A.7) Sn > So,. .. , Sn > S./l_1; . 
it will be called the second ladder point of the entire random walk. It is 
of the form {S'1 + &~tt 3^x + JP2) where the pairs (&~lt JfJ and 
(&~t,-Jt?t) are independent and identically distributed. (See'also VI,8.) 
Proceeding in this way we define the third, fourth,. . . ladder points of our 
random walk. Thus a point (n, Sn) is an ascending ladder point if it satisfies 
A.7). The rth ladder point (if it exists) is of the form {Srx H \- Tr% 
jtfx 4. ... 4. jtf>r) where the pairs (&~h, Jf k) are mutually independent 
and have the common distribution A.4). (See fig. 1 in VI,8.) 
For economy of notation no new letters will be introduced for the sums 
^ + h $~r and jTx + • • • + jfr. They form'(possibly terminating) 
renewal processes with llinterarrival times" fT^ and J^h. In the random 
walk, of course, only &~k is of the nature of a time variable. The ladder 
points themselves form a two-dimensional renewal process. 
We shall denote by 
A.8) 
00 
n=0 
the renewal measure^ for the ladder height process. (Here H°* = y>0.) Its 
1 
improper distribution function given by ip(x) = y>{—co, x} vanishes 
when x < 0, while for a; positive ip(x) equals one plus the expected number 
of ladder points in the strip 0,x (no limitation on time). We know from VI,6 
and ?1,1 that %p{x) < co for all x and in the case of defective ladder 
variables 
A.9) v(°o)|() 
n=0 1 — H(OO) 
Finally we introduce the notation y0 for the atomic distribution with unit 
mass at the origin; thus for any interval / 
A.10) y*0{/}¦'= 1 // xel, wo{I}=O otherwise. 
Ascending weak ladder variables. The point («, Sn) is a weak (ascending) 
ladder point iff Sn ;> Sk for k = 0, 1,... , n. The theories of strict and 
weak ladder variables run parallel, and we shall systematically use the 
same letters, indicating weak variables by bars: thus &~x is the small- 
est index n such that Sj < 0,... , Sn_! < 0, but Sn ;> 0. As was 
mentioned before, the tedious distinction between strict and weak 
XII. 1 BASIC CONCEPTS AND NOTATIONS 393 
variables becomes unnecessary when the distribution F is continuous. 
Even in the general situation it is easy to express the distribution R of 
weak ladder heights in terms of the distribution H, and this will enable 
us to confine our attention to the single distribution defined in A.6). 
The first weak ladder point is identical with the first strict ladder point 
except if the random walk returns to the origin having passed only through 
negative values; in this case 3rifx = 0 and we put ? = P{jFr1 = 0}. Thus 
00 
(i.ii) ? = 2p(Si < o,,.., s^ < 0,-s, = o}. 
(The event cannot occur if Xx > 0 and hence 0 <. ? .< 1.) With probability 
1 — ? the first strict ladder point coincides with the first weak ladder point 
and hence 
A.12) #=?v>0+(l-?)/f. ' 
In words, the distribution of the first weak ladder height is a mixture of the 
distribution H and the atomic distribution concentrated at the origin. 
Example. In the simple binomial random walk the first weak ladder 
height equals 1 iff the first step leads to +1. If the first step leads to —1 the 
(conditional) probability of a return to 0 equals 1 if p > q, and pfq 
otherwise. In the first case ? = q, in the second ? = p. The possible 
ladder heights are 1 and 0, and they have probabilities p and q if p <q, 
. while both probabilities equal p when p <q. In the latter case the ladder 
height is a defective variable. > 
The probability that prior to the first entry into 0, oo the random walk 
returns to the origin exactly k times equals ?*A— ?). The expected number 
of such returns is 1/A — ?) and this is also the expected multiplicity of each 
weak ladder height prior to the appearance of the next strict ladder points! 
Therefore 
A.13) y> = ~. V- 
(See problem 7.) The simplicity of these relations enables us to avoid 
explicit use of the distribution R. 
Descending ladder variables. The strict and weak descending ladder variables 
are defined by symmetry, that is, by changing > into <. On the rare 
occasions where a special notation will be required we shall denote descending 
order by the superscript minus. Thus the first strict descending ladder point 
is (&~l , 3^~), and so on. 
It will be seen presently that the probabilities P{Jf x = 0} and P{J^! = 0) 
394 RANDOM WALKS IN 511 XII.2 
are identical because 
A.14) P{S1>0,...,Sn_1>0,Sn = 0} = 
P{S! < 0,. . . , S.., < 0, Sn = 0}. 
It follows that the analogue to A.12) and A.13) for the descending ladder 
variables depend on the same quantity ?. 
2. DUALITY. TYPES OF RANDOM WALKS 
The amazing properties of the fluctuations in coin tossing were derived in 
1; III by simple combinatorial arguments depending on taking the variables 
(X1(.. . , Xn) in reverse order. The same device will now lead to important 
results of great generality. 
For fixed n we introduce n new variables by X* = Xn,. . . , X* = Xx. 
Their partial sums are given by SJ = Sn — Sn_k where k = 0,. .. , n. 
The joint distributions of (So,. . . , Sn) and (SJ,. . . , SJ) being the 
same, the correspondence Xfc-»-Xj maps any event A defined by 
(So,.. . , Sn) into an event A* of equal probability. The mapping is easy 
to visualize because the graphs of @, S2,.. . , SJ and @, S*',..., SJ) 
are rotations of each other through 180 degrees. 
Example, (a) If S2 < 0,... , Sn_! < 0 but Sn = 0, then 
S? > 0, . . . , S*_, > 0 and SJ = 0. 
This proves the validity of the relation A.14) used in the preceding section. >> 
We now apply the reversal procedure to the event Sn > So, . .. , Sn > Sn_j 
defining a (ascending strict) ladder point. The dual relations are SJ > S*_k 
for k = 1, .. . , n. But S* > S*_k is the same as Sk > 0, and hence we 
have for every finite interval / <=¦ 0, oo 
B.1) P{Sn > S, for j = 0,. v , n-l and Snel} = ' 
= P{S, > 0 for j = 1, .. . , n and Sn e/}. 
The left side is the probability that there exists a ladder point with abscissa 
n and ordinate, in /. The right side is the probability of the event that a visit 
to / at epoch n takes place without prior visit to the closed half-line 
— oo,0. 
Consider then the result of summing B.1) over all n. On the left we get 
%p{I) by the definition A.8) of the renewal measure ip. On the right we get 
the expected number of visits to the interval / prior to the first entry into 
— oo, 0. It is finite because y>{I} < oo. We have thus proved the basic 
XH.2 DUALITY. TYPES OF RANDOM WALKS 395 
Duality lemma.. The renewal measure tp admits of two interpretations. For 
every finite interval I <=¦ 0, oo the value ip{I) equals 
(a) the expected number or ladder points in I; 
(b) the expected number, of visits Sne/ such that Sk > 0 for 
k = 1,2,... ,ri. 
This simple lemma will enable us to prove in an elementary way theorems 
that would otherwise require deep analytic methods. In its analytic formu- 
lation the lemma does not seem exciting, but it has immediate consequences 
that are most surprising and contrary to naive intuitions- 
Example, (b) Simple random walk. In the random walk of the example 
in section 1 there exists a ladder point with ordinate k iff the event {Sn = k} 
occurs for some n, and we saw that the probability for this is 1 or (plq)k- 
according as p > q or p < q. By the duality lemma this means that in a 
symmetric random walk the expected number of .visits to k ^ 1 prior to the 
first return to the origin equals 1 for all k. The fantastic nature of this result 
appears clearer in the coin-tossing terminology. The assertion is that on the 
average Peter's accumulatedgain passes once through every value k, however. 
large, before reaching the zero level for the first time. This statement usually 
arouses incredulity, but it can be verified by direct calculation (problem 2). 
(Our old result that the waiting time for the first return to 0 has infinite 
expectation follows by summation over k.) > 
In the symmetric binomial random walk (coin tossing) each of the values 
±1 is attained with probability one, but the expected waiting time for each 
of these events is infinite. The next theorem shows that this is not a peculiarity 
of tHe coin tossing game since a similar statement is true for all random walks 
in which both positive and negative values are assumed with probability one: 
Theorem 1. There exist only two types of random walks. 
(i) The, oscillating type. Both the ascending and the descending renewal 
processes are persistent, Sn oscillates with probability 1 between — oo and 
co, and 
B.2) EGT0 = oo, E(«!Tr) = «. 
(ii) Drift to — oo, (say). The ascending renewal process is terminating, 
the descending one proper. With probability one Sn drifts to -oo and 
reaches a finite maximum M > 0. The relations B.5) and B.7) are true. 
[Walks of type (ii) are obviously transient, but type (i) includes both 
persistent and transient walks. See end of VI,10.] 
396 RANDOM WALKS IN 311 ' XII.2 
Proof. The identity B.1) holds also when the strict inequalities are 
replaced by weak ones. For / = 0, oo it reduces to 
B.3) P{Sn>Sfc for 0<k<n} = P{Sk>0 
for 0 < k < n} = 1 - P{^ < «}• 
The left side equals the probability that («, Sn) be a weak ascending ladder 
point. These probabilities add to y>(oo) < oo, and in view of A.13) we have 
therefore 
B.4) ' -1- tfoo) = |[1 - P{^r < n}]. 
1 — ? n=0 
When the descending ladder process is defective the terms of the series are 
bounded away from zero and the series diverges. In this case tp(oo) = oo 
which means that the ascending process is persistent. We have thus an 
analytic proof for the intuitively obvious fact that it is impossible that both 
the ascending and th6 descending ladder processes terminate. 
If 3~~ is proper B.4) reduces to 
B<5) v A' l -rv ' A-0A- 
with the obvious interpretation when//(oo) = 1. It follows that E(^~~) < oo 
iff //(oo) < 1, that is, iff the ascending variable &~x is defective. Thus 
either one of these variables is defective, or else B.2) holds. 
If E(^~~) < oo the ascending renewal process is terminating. With 
probability one. there occurs a last ladder point, and so 
B.6) M = max {So, Slf...}. 
is finite. Given that the «th ladder point occurred, the probability that it is 
the last equals 1 — //(oo), and so [see XI,F.3)] 
A7) P{M <v} = [1 - tf(oo)] f Hn\x) = [1 - H(oo)] tp(x). * 
In the following theorem we agree to write E(X) = + oo if the defining 
integral diverges only at +oo or, what amounts to the same, if P{X < t} is 
integrable over — oo, 0. 
Theorem 2. (i) If E(XX) = 0, then Jf1 and Fx are proper* and 
=oo. 
4Theorem 4 of VI,10 contains the. stronger result that the random walk is persistent 
whenever E(XX) = 0. 
XII.2 DUALITY. TYPES OF RANDOM WALKS 397 
(ii) If E(XX) is finite and positive, then ,2f x and J~x are proper, have 
finite expectations, and 
B.8) E(Jf,) = Ei.rj E(XX) 
The random walk drifts to + oo. 
(iii) If E(XX) = + oo then Epf x) = oo a«rf the random walk drifts to 
(iv) Otherwise either the random walk drifts to —oo (in which case S"x and 
\ are defective), or else E(^f1) = oo. 
The identity B.8) was discovered by A. Wald in a more general setting to 
be discussed in XVIII,2. The following proof is based on the strong law of 
large numbers of VII,8. Purely analytic proofs will be given in due course. 
(See theorem 3 of section 7 and problems 9-11 as well as XVIII,4.) 
Proof. If n coincides with the kth ladder epoch we have the identity 
<2.9) 5* = 
n 
We now observe that the strong law of large numbers applies also if E(XJ = 
= + oo as can be seen by the obvious truncation. 
(i) Let E(Xx) = 0. As k -^ oo the left side in B.9) tends to 0. It follows 
that the denominator tends to infinity. This implies that ^Tx is proper and 
EG^j) = oo. 
(ii) If 0 < E(XX) < oo the strong law of large numbers implies that the 
random walk drifts to oo. In view of B.5) this means that S'1 is proper and 
E(^"i) < oo. Numerator and denominator in B.9) therefore tend to finite 
limits, and B.8) now follows from the converse to the law of large numbers 
(theorem 4 in VII,8). 
(iii) If E(Xj) = -f bo the same argument shows that E(J#\) — oo. 
(iv) In the remaining cases we show that if Jfx is proper and E(J^X) < oo 
the random walk drifts to — oo. Considering the first step in the random 
walk it is clear that for z > 0 
B,10) P{J^1 > x) > PiXj. > x). 
If J>t1 is improper the random walk drifts to -co. If it is proper the 
integral of the left side extended over 0, co equals Ep?\). If E(Jf\) < co 
it follows that E(XX) is finite or — oo. The case E(XX) > 0 has been taken 
care of, and if E(XJ < 0 (or — oo) the random walk drifts to — oo. k 
It follows from B.10) and the analogous inequality for x < 0 that if both 
and 3^~ are proper and have finite expectations, then P{iXi| > x) is 
398 RANDOM WALKS IN ft1 XIL3 
integrable and hence E(XX) exists (lemma 2 of V,6). With E(Xi) 5* 0 
one of the ladder variables would be defective, and hence we have the 
Corollary. If both <ffx and 2f?\ are proper and have finite expectations, 
then E(XX) = 0. 
The converse is not true. However, if E(XX) = 0 and E(X*) < 00 then 
3fc\ and Jf ~ have finite expectations. (See problem 10. A more precise 
result is contained in theorem 1 of XVII1,5 where SN and SN are the first 
entry variables with distributions ?Fx and ti?) 
3. DISTRIBUTION OF LADDER HEIGHTS. 
WIENER-HOPF FACTORIZATION 
The calculation of the ladder height distributions H and H seems at 
first to present a formidable problem, and was originally considered in this 
light. The duality lemma leads to a simple solution, however. The idea is 
1 
that the first entry into, say, — 00, 0 should be considered together with the 
section of the random walk prior to this first entry. We are thus led to the 
study of the modified random walk {Sn} which terminates at the epoch of 
1 
the first entry into — 00, 0. We denote by yn the' defective probability 
distribution of the position at epoch n in this restricted random walk; that 
is, for an arbitrary interval / and n = 1, 2, .. . we put 
C.1) Vn{I} = P{SX > 0, . . . , Sn > 0, Sn g /}. 
(Note that this implies yn{— oo, 0} = 0.) As before, ip0 is the probability 
distribution concentrated at the origin. Now it was shown in B.1) that 
y)n{I} equals the probability that (n, Sn) be a ladder point with Sn e /. 
Summing over n we get therefore 
C.2) y{I} = !>„{/} 
where ip is the renewal function introduced in A.8). In other words, for an 
interval in the open positive half-axis y{I} is the expected number of 
(strict ascending) ladder points with ordinate in /. For / in the negative 
half-axis we now define f{I} = 0. It follows that the series in C.2) con- 
verges for every bounded interval / (though not necessarily for / = 0, oo). 
It is this unexpected result that renders the following theory so incredibly 
simple. 
1 
Studying the hrst entry into — oo, 0 means studying the weak descending 
ladder process, and with the notations of section 1 the point of first entry 
is Jf-f its distribution R~. For typographical convenience, however, we 
XII.3 LADDER HEIGHTS. WEINER-HOPF FACTORIZATION 399 
replace H~ by p and denote by pn{I} the probability that the first entry to 
— co, 0 takes place at epoch n and within the interval /. Formally for 
n= 1,2, ... 
C.3) Pn{I} = P{SX > 0, . . . , Sn_x > 0, Sn < 0, Sn G /}. 
(This implies pn{0, 00} = 0. The term p0 remains undefined.) This time 
the series 
C-4)- p{I) =j>«M 
obviously converges and represents the possibly defective distribution of 
the point of the first entry. (In other words, p{I} = J^~{I}.) 
It is easy to derive recurrence relations for yn and pn. Indeed, given the 
position y of Sn the (conditional) probability that Sn+1 e / equals 
F{I — y}, where / — y is the translate of / through — y. Thus 
C.5a) 
Pn+1{/} = r^idy} F{I - y) if I cz _oo,d 
Jo- 
y}n+1{I} = rWn{dy}F{I-y} if I <= 0~Z> 
Jo- 
(the origin contributing only when n = 0). For bounded intervals / the 
duality lemma assures the convergence' of 2 VnU)> an<^ 2 PnU) always 
converges to a number < 1. We have thus series representations for p 
and ip. It is clear that these sums satisfy 
C.6a) p{I} = Iy>{dy} F{I - y} if I cz _oo, 0 
Jo 
o- 
C.66) y>{I} = r^dy} F{I - y) if 
Jo- 
with the proviso that C.6b) is restricted to bounded intervals /. We shall 
see that in practice the relations C.6) are more useful than the theoretical 
series representations for p and ip. It is sometimes convenient to replace 
the interval function p and y by the equivalent point functions 
p(x) = p{—co, x} and ip{x) = tp{— oc, x). 
C'early C.6a) is equivalent to 
C.7a) p(x) = [Xy>{dy)F(x-y), x < 0. 
Jo- 
From C.6b) we get for x > 0 
(~{dy} [F(x-y) - F(- 
Jo- 
400 RANDOM WALKS IN .ft1 XTI.3 
Taking into account C.7a) we see thus that C.6b) is equivalent to 
(*-y), x > 0. 
To simplify notations we introduce the convolution 
C.8) V*Ff 
f 
Since y> is concentrated on 0, oo the value y * F{I} equals the sum of the 
two integrals in C.6) and is therefore finite. As y has a unit atom at the 
origin we can combine the two relations C.6) into the single convolution 
equation 
C.9) P + y> = y>0 + y>1c.F. 
In view of the fact that p and y) — yH are concentrated on — oo, 0 and 
0, oo, respectively, the relation C.9) is fully equivalent to the pair C.6). 
We shall use C.9) as an integral equation determining the unknown 
measures p and y>. A great many conclusions of theoretical importance 
can be derived directly from C.9). We list the most remarkable such theorem 
under the heading of an example in order to indicate that it will not be used 
in the sequel and that we embark on a digression. 
Examples, (a) Wiener-Hopf type factorization. It follows from the 
definition A.7) of tp that it satisfies the renewal equation 
C.10) V> = V>0 + y^^- 
Using this relation we show that C.9) may be rewritten in the equivalent form 
C.11) F=H+ p-H* p. 
Indeed, convolving C.9) with H we get 
+Ap ->Wo = H- F+y-k.F. 
Subtracting this from C.9) yields C.11). Conversely, convolving C.11) 
with y) we get 
y-k F= y — y0 + V* P - (v - Vo)* P = V - Vo + P 
which is the same as C.9). 
The identity C.11) is remarkable in that it represents an arbitrary prob- 
ability distribution F in terms of two (possibly defective) distributions H 
and p concentrated on 0, oo and — oo, 0, respectively. The first interval 
is open to the second closed, but this asymmetry can be remedied by express- 
1 
ing the first entrance probabilities p into — oo, 0 by the first entrance 
XII.3 LADDER HEIGHTS. WEINER-HOPF FACTORIZATION 4Q1 
probabilities H~ into — oo, 0. The relationship between these prob- 
abilities is given by the analogue to A.12) for x < 0, namely 
with 'C defined by A.11) [which relation holds also with the inequalities 
reversed; see example 2(a)]. Substituting into C.11) we get after a trite 
rearrangement 
X3.12) Vo - F = (I -0 [y,0 -H}* [y>0 - H-]. 
Of course, for a continuous distribution F the relations C.11) and C.12) 
are identical. 
Various versions of this formula have been discovered independently by 
different methods and have caused much excitement. • For a different variant 
see problem 19, and for the Fourier analytic equivalent see XVIII,3. The 
connection with the Wiener-Hopf techniques is discussed in section 3a. 
Wald's identity B.8) is an easv consequence of C.11). (See problem 11 as 
well as XVIII,2.) 
(b) Explicit expressions for H and H~ are usually difficult to.come by. 
An interesting distribution for which the calculations are particularly simple 
was found in example VI,8F). If F is the convolution of two exponentials 
concentrated on 0, oo and — oo, 0, respectively, then it has a density of the 
form 
x > 0. 
a + 
We suppose b < a, so that E(X) > 0. Then H and H~ have densities 
given by be~bx and beax. Here Hie H~ = {bja)F, and C.11) is trivially 
true. »> 
(For further explicit examples see problem 9.) 
We now turn to the consideration of C.9) as an integral equation for the 
unknown measures p and ip. It will be shown that in the present context 
the solution is unique. For brevity we agree to say that a pair (p, y) is 
probabilistically possible if p is a possibly defective probability distribution 
concentrated on — co, 0, and y: — ip0 a measure concentrated on 0, co 
such that for each bounded interval / the measures y>{[ -f t} remain 
bounded. (The last condition follows from the renewal theorems since 
Hn*.) 
Theorem 1. The convolution equation C.9) [or, equivalently, the pair 
C.6)] admits of exactly one probabilistically possible solution (p, yj). 
402 RANDOM WALKS IN ft1 XII.3 
This implies that p is the distribution of the point of first entry into 
— oo, 0 and v> = 2 #"* where H is the distribution of the point of first 
entry into 0, oo. 
Proof. Let p# and ip# be two non-negative measures satisfying C.6), 
and y)# > yH. From C.6b) we get by induction that y# > y0 + • • • + \pn 
for every n, and hence our solution ip is minimal in the sense that for any 
other solution y)# with a unit atom at the origin y#{I} > y>{I} for all 
intervals. In other words, d = y# — ip is a measure. From C.6a) it is 
now seen that the same is true of y = p# — p. Since both (p, yi) and 
(p#,y#) satisfy C.9) we have 
C.13) d + y = d*F. 
Let / be a fixed finite interval and put z(t) = d{I + t). Two cases are 
possible. If p is a proper distribution the fact that p# ^> p implies that 
p# = p and hence y = 0. Then z is a bounded solution of the convolution 
equation z = F-kz and hence by induction 
<t-y)F**{dy} 
— 00 
for all n. Now z > 0 and z(t) = 0 for every t such that / + t is con- 
tained in the negative half-axis. For such t it is clear from C.14) that 
z(t — y) = 0 for every y which is a point of increase for some Fn*. By 
lemma 2 of V,4a the set of such y is everywhere dense and we conclude that 
2 vanishes identically. 
For a defective p we know only that y > 0, and then C.13) implies only 
that z < F* z. In this case C.14) holds with the equality sign replaced by 
<. But the random walk then drifts to oo and so the mass of Fn* tends 
to concentrate near oo. Again, z(t—y) = 0 for all sufficiently large y 
and so z must vanish identically. Thus y# = y as asserted. > 
3a. THE WIENER-HOPF INTEGRAL EQUATION 
To explain the connection between the integral equation C.9) and the 
standard Wiener-Hopf equation it is best to begin by a probabilistic problem 
where the latter occurs. 
Example, (c) Distribution of maxima. For simplicity let us assume that 
the distribution F has a density / ana a negative expectation. The random 
walk {Sn} drifts to -co and a finite-valued random variable 
C.15) M = max [0, Sx, S2,. . .] 
XII.3a THE WEINER-HOPF INTEGRAL EQUATION 403 
is denned with probability one. We propose to calculate its probability 
distribution M{x) = P{M < x) which is by definition concentrated on 
0, oo. The event {M <? x) occurs iff 
Xj = y < x and max [0, X2, X2+X3, . . .] < x—y. 
Summing over all possible y we get 
C.16) M(x) = [" M(x-y)f(y) dy, x>0 
«'—0O 
which is the same as 
C.17) M(x) = \M(s)f(x-s) ds, x > 0 
Jo 
On the other hand, we know from B.7) that M(x) = [1 — H(co)] y(x). 
We saw that y satisfies integral equation C.7Z>) where under the present 
conditions p@) =1. A simple integration by parts now shows that C.76) 
and C.17) are actually identical. > 
The standard form of the Wiener-Hopf integral equation is represented 
by C.17) and our example illustrates the way in which it can occur in prob- 
ability theory. General references to the Wiener-Hopf techniques are 
misleading, however, because the restriction to positive functions and 
measures changes (and simplifies) the nature of the problem. 
The ingenious method5 used by N. Wiener and E. Hopf to treat C.17) 
has attracted wide attention and has been adapted to various probabilistic 
problems, for example, by H. Cramer for asymptotic estimates for prob- 
abilities of ruin. The method involves a formidable analytic apparatus and 
hence the ease with which these estimates are obtained from die present 
approach is almost disquieting. The deeper reason car, be understood as 
follows. The equation C.17) represents, at best, only one of the two 
equations C.7), and when p@) < 1 even less. Taken by it^lf C.17) is 
much more difficult to handle than the pair C.7). For example, the unique- 
ness theorem fails for C.17) even if only probability distributions are admitted. 
In fact, the basic idea of the Wiener-Hopf technique consists in introducing 
an auxiliary function which in the general theory lacks any oarticular 
meaning. This tour de force in effect replaces the individual equation 
C.17) by a pair equivalent to C.7) but the uniqueness is lost. We proceeded 
in the opposite direction, starting from th'; obvious recursion system C.5) 
for the probabilities connected with the two inseparable problems: the 
<c Dating back to 1931. A huge literature followed the first presentation in book form: 
E. Hopf, Mathematical problems of radiative equilibrium, Cambridge tracts, No. 31, 1934. 
404 RANDOM WALKS IN ft1 XII. 
first entry to — oo, 0 and the random walk restricted to x > 0 prior to 
this first entry. In this way we derived the integral equation C.9) from 
the cnown solution, and the uniqueness of the probabilistic solution was 
easy to establish. The convergence proof, the properties of the solutions, 
as well as the connection between the distribution M of the maxima and 
the renewal measure y depend on the duality lemma. 
The possibility of attacking the Wiener-Hopf equation C.17) using the 
duality principle was noticed by F. Spitzer.6 The usual way of connecting 
the Wiener-Hopf theory with probabilistic problems starts from formulas 
related to (9.3) below in their Fourier version to which we shall return in 
chapter XVIII. There exists now a huge literature relating Wiener-Hopf 
techniques to probabilistic problems and extending the scope of combi- 
natorial methods. Most of this literature uses Fourier techniques.7 
4. EXAMPLES 
Explicit formulas for the first entry distributions are in general difficult 
to obtain. By a stroke of good fortune there is a remarkable exception to 
this rule, discussed in example (a). At first sight the distribution F of this 
example appears artificial, but the type turns up frequently in connection 
with Poisson processes, queuing theory; ruin problems, etc. Considering 
the extreme simplicity of our general results it is unbelievable how much 
ingenuity and analytical skill has been spent (often repeatedly) on individual 
special cases. 
Example (c) exhibits (in a rather pedestrian fashion) the complete cal- 
culations in the case of an arithmetic F with rational generating functions. 
The calculations are given because the same method is used for rational 
Laplace or Fourier transforms. Another example is found in problems 3-6. 
Example (b) deals with a general relationship of independent interest. 
We adhere to the notations of the preceding section. Thus H and p 
1 
are the distributions of the point of first entry into 0, oo and — oo, 0 
respectively. (In other words, H and p are the distributions of the first 
strict ascending and the first weak descending ladder heights.) Finally, 
y) = 2 Hn* is the renewal function corresponding to H. Our main tool 
is the equation C.7a) stating that for x < 0 the distribution of the first 
6 The Wiener-Hopf equation whose kernel is a probability density, Duke Math. J., vol. 
24 A957) pp 327-343. 
7 A meaningful short survey of the literature is impossible on account of the unsettled 
state of affairs and because the methodology of many papers suffers under the influence of 
accidents of historical developments. Generalizations beyond probability theory are 
illustrated by G. Baxter, An operator identity, Pacific J. Math., vol. 4 A958) pp. 649-663. 
XII.4 EXAMPLES 405 
entry into — oo, 0 is given by 
D-1) !**)=[* w{dy}F(x-y). 
Jo- 
Examples, (a) Distributions with one exponential tail occur more frequently 
than might be expected. For example, in the random walk of example VI,8F) 
and in the corresponding queuing process VT,9(e) both tails are exponential. 
Suppose, by way of introduction, that the left tail of F is exponential, that is, 
Fix) = qe"* for x < 0. Whatever y is, D.1) shows that p{x) = Ce"* 
for x < 0 where C is a constant. Having made this discovery we inter- 
change the role of the two half-axes (partly to facilitate reference to our 
formulas, partly with a view to the most important applications in queuing 
theory). Assume then that 
D.2) F(x)=l-pe-a* for x>0 
without any conditions imposed for x < 0. To avoid unnecessary compli- 
cations we assume that F has a finite expectation /x and that F is con- 
tinuous. It follows from the preliminary remark that the ladder height 
distribution H has a density proportional to e~ax. We now distinguish 
two cases. . , 
(i) If /x > 0 the distribution H is proper and hence for x > 0 
D.3) H(x) = 1 - e-as, y(x) = 1 + ax. 
[The latter follows trivially from y = 2 Hn* °r the renewal equation 
C.10).] From D.1) we get 
D.4) p(x) = F(x) + a | F(s) ds, x<0,. 
J— 00 
and thus we have explicit expressions for all desired probabilities. An easy 
calculation shows that 
D.5) p@) = 1 - a/x. 
This is a special case of B.8) because A — p(O))-1 = EG^) by virtue of 
B.5). 
(ii) If [x < 0, the relations D.3) and D.4) still represent a solution of the 
integral equation C.9), but because of D.5) it is probabilistically impossible 
when n < 0. For the correct solution we know that H has a density 
h{x) = (a—K)e~ax where 0 < k < a because H is defective. An easy 
calculation shows that y>'(x) = {a.—K)e~KX for x > 0. The unknown 
constant k is obtained from the condition that p@) =1. A routine cal- 
culation shows that k must be the unique positive root of the equation D.6). 
Given the root of this transcendental equation we have again explicit 
formulas for H, p, and y. 
406 RANDOM WALKS IN ft1 XII.4 
The reader will easily verify that the same theory applies when the variables 
Xl5 X2,. . . of the random walk are integral-valued and the distribution F 
has a geometric right tail, that is, if F attributes to the integer k > 0 the 
weight qPk. 
(b) Associated random walks. Suppose that F has an expectation ft ^ 0 
and that there exists a number x j? 0 such that . - 
D-6) 
f 
—00 
Given an arbitrary measure y on the line we associate with it a new 
measure ay defined by 
D.7) ' ay{dy) = e*" y{dy}. 
,The measure aF associated with F is again a proper probability distribution 
and we say that the random walks generated by aF and F are associated 
with each other} It is easily seen that the «-fold convolution of aF with 
itself is associated with Fn* so that the notation aFn* is unambiguous. 
Furthermore, the recursion formulas C.5) show that the transforms apn 
and aipn have the same probabilistic meaning in the new random walk, as 
pn and Vn in the old one. It follows generally that the transforms ap, aH, 
aip, etc., have the obvious meaning for the random,walk associated with aF. 
[This can be seen also directly from the integral equation C.9).] 
The integral 
J'+OO 
—00 
evtF{dy} 
exists for all t between 0 and k and in this interval <p may be differentiated 
indefinitely. The second derivative being positive, <p is a convex function. 
If <p'(k) exists, the fact that <p@) = (p(k) implies that <p'@) and <p'(k) 
have opposite signs. The random walks induced by F and aF have therefore 
drifts in opposite directions. (This remains obviously, true even in the 
exceptional case that aF has no finite expectation.) 
We have thus devised a widely applicable method of translating facts 
about a random walk with ju < 0 into results for a random walk with 
positive expectation, and vice versa. 
If ju < 0 the ladder height distribution H is defective, but aH is a 
proper distribution. This means that 
D.8) I™ eKV H{dy) = I. 
Jo 
8 This notion was used by A. Khintchine, A. Wald, and others but was never fully 
exploited. The transformation D.7) was used for renewal theory in XI,6 and (in a form 
disguised by the use of generating functions) in theorem l(iii) of 1; XIII, 10 and will be 
used for Laplace transforms in XIII,A.8). The equation D.6) serves also in the Wiener- 
Hopf theory. 
XII.4 EXAMPLES 407 
The power of the method of associated random walks derives largely from 
this remark. Indeed, we know from XI,6 that excellent asymptotic estimates 
are available for the ascending ladder process if one knows the root of the 
equation D.8). These estimates would be illusory if they required a knowledge 
of H, but we see now that the roots of the two equations D.6) and D.8) are 
identical. 
(c) Bounded arithmetic distributions. Let a and b be positive integers 
and let F be an arithmetic distribution with span 1 and jumps fkzXk — 
= — b, . . . , a. The measures ip and p are also concentrated on integers 
and we denote their jumps at k by ipk and pk, respectively. The first 
1 
entry into — oo, 0 occurs at an integer > — b and so pk = 0 for k < — b. 
We introduce the generating functions 
D.9) d>(s) =• | fks\ T(s) = I yks\ R(s) = ? Pksk. 
They differ from those of 1; XI in that <?> and R involve also negative 
powers of s, but it is clear that the basic properties and rules remain 
unchanged. In particular, // = <I>'A) is the expectation of F. To a con- 
volution of distributions there corresponds the product of the generating 
functions, and so the basic integral equation C.9) is equivalent to 
Y + R = 1 + ^ or 
The numerator and denominator are polynomials of degrees b and 
a + b, respectively. The power series on the left is regular,, for |s| < 1, 
and so all roots of the denominator located within the unit circle must 
cancel against roots of the numerator. We proceed to show that this require- 
ment uniquely determines R and T. 
For concreteness suppose // = 0. (For // ^ 0 see problems 12-13.) 
Then s = 1 is a double root of the equation O(s) = 1. For |s| =*= 1 we 
have |O(j)| < 0A) = 1 where the inequality sign holds only if sk = 1 for 
every 'k such that fk > 0. As the distribution F is assumed to have span 1 
this is true only if s = 1, and hence no other roots of O(s) = I are located 
on the unit circle itself. To discover how. many roots are located in the 
interior we consider the polynomial of degree a + b defined by 
P(s) = sb[®(s) - q), q>\. 
For \s\ = 1 we have \P(s)\ > q - 1 > 0 and 
\P{s) + qsb\ = |O(j)| < 1 <q\sb\. 
408 RANDOM WALKS IN 311 XII.5 
By Rouche's theorem9 this implies that the polynomials P(s) and qsb have 
the same number of zeros located inside the unit circle. It follows that P 
has exactly b roots with \s\ < 1 and a roots with |s\ > 1. Now P@) = 0 
and P(i) = 1 — q < 0 while P(s) > 0 for large s, and so P has two 
real roots s' < 1 < s". As q -> 1 the roots of P tend to the roots of 
<I>(j) = 1. Thus we conclude finally that the denominator in D.10) has 1 
as a double root, and furthermore b — 1 roots sx, . . . , sb_x with |^| < 1 
and a — 1 roots <rl5 . . . , aa_x with \<Jj\ > 1. Then the denominator is 
of the form 
D.11) s\®{s)-\) = Cis-ms- 
The roots s1}'.. . , sb_1 must cancel against those of the numerator, and 
since the' coefficients ipn remain bounded the same is true of one root 
s = 1. This'determines *F up to a multiplicative constant. But by definition 
= 1 and hence we have the desired explicit formula 
D.12) 
Expansion into partial fractions leads to explicit expressions for ipn, the 
great advantage of this method being that the knowledge of the dominant 
root leads to reasonable asymptotic estimates (see 1; XI,4). . 
For the generating function R of the first entrance probabilities pk we 
get from D.10) and D.12) - 
[The coefficient C is cJefined in D.11) and depends only on the given 
distribution {fk}.] Again a partial fraction expansion leads to asymptotic 
estimates. (Continued in problems 12-15.) 
5. APPLICATIONS 
It was shown in VI,9 that a basic problem of queuing theory consists 
in finding the distribution M of 
E.1) M = max [Q; Sx,...] 
in a random walk with variables X* such that fj, = E(X^.) < 0. Examples 
VI,9(ar) to (c) show that the same problem turns up in other contexts, 
for example in connection with ruin problems in compound Poisson 
9 See, for example, E. Hille, Analytic function theory, vol. I, section 9.2. (Ginn and Co., 
1959.) 
XII. 5 APPLICATIONS 
409 
processes. In this case, as well as in queuing theory, the underlying distri- 
bution is of the form 
E.2) F=A*h 
where A is concentrated on 0, oo and B on — oo, 0. We suppose that A 
and B have finite expectations a and — b, so that F has expectation 
fx = a — b. We suppose also that F is continuous so as to avoid the tedious 
distinction between strict and weak ladder variables. 
As in the preceding twcf sections we denote the ascending and descending 
ladder height distributions by H and p respectively. (For consistency we 
should write H~ for p.) In other words, H and p are the distributions 
of the point of first entry into 0, oo and — oo, 0 (and also into corre- 
sponding closed intervals). It was shown in example 3(c) and in B.7) that 
(/> < 0 
E.3) M(x) = -^ = [1 - ff(oo)] | Hn\x). 
(co) 
o 
Example 4 (a) contains an explicit formula10 for this distribution if one of the 
tails of F is exponential, that is, 
E.4) F(x) = 1 - pe-ax for x > 0, 
or else F(x) = qeax for x < 0. 
By extreme good luck the condition E.4) holds if F is of the form E.2) with 
E.5) A(x) = I - e~ax for x > 0. 
Then 
E.6), 
Our simple results are therefore applicable in queuing theory whenever 
either the incoming traffic is Poissonian or the service time is exponential. 
Furthermore, the ruin problem in compound Poisson processes is covered 
by the present conditions. There exists an immense applied literature 
treating special problems under various assumptions on the distribution B, 
sometimes, as in ruin problems, in a disguised form. As it turns out, greater 
generality and much greater simplicity can be achieved by using only the 
10 Another explicit formula is contained in example 4(c) for the case of an arUnm *ic 
distribution F \Vith finitely many atoms. This explicit formula is too unwieldy i e 
practical, but an expansion into partial fractions leads to good asymptotic estimates if the 
dominant root of the denominator is known. The same method applies to Fourier transforms 
whenever the characteristic function of F is rational. .This remark covers many special 
cases treated in the literature. 
410 RANDOM WALKS IN ft1 XII.5 
condition E.4) instead of the combination E.5) and E.2). We see here a 
prime example of the economy of thought inherent in a general Theory where 
one's view is not obscured by accidents of special cases. 
Examples, (a) The Khintchine-Pollaczek formula. Suppose that F is of 
the form E.2) with A given by E.5) and /u = I/a — b > 0. The random 
walk drifts to oo and we have to replace the maximum in E.1) by the 
minimum. This means replacing H in E.3) by the distribution p given in 
D.4). A simple integration'by parts shows that for x < 0 
E.7) p(x) = y.(X B(y)dy. 
J— oo 
Hence p@) = v.b and so for x < 0 
E.8) P{min (So, Sx, . . .) < x) = A - afc) f pn*(x). 
o 
This is the celebrated Khintchine-Pollaczek formula, which has been 
rediscovered time and again in special situations, invariably using Laplace 
transforms [which method is inapplicable for the more general distributions of 
the form E.4)]. We return to it in problems 10-11 of XVIII,7. 
(b) The dual case. Consider the same distributions as in the last example 
but with [x < 0. As was shown in the second part of example 4(a) in this case 
E.9) P{max (So, S^ . . .) < x) = - w(x) = 1 - (l - -)e 
,—KX 
a \ ay 
where k is the unique positive root of the "characteristic equation" D.6). 
[This result can be obtained also by the method of: associated random walks 
recalling the fact that when /j, > 0 one has y(x) = 1 + ccx for x > 0.] 
In queuing theory E.9) implies that at a server with exponential servicing 
times the distribution of the waiting times tends to an exponential limit. 
(c) Asymptotic estimates. The method of associated random walk 
described in section 46 leads easily to useful estimates for the tail of the 
distribution 
E.10) M(x) = P{max (So, Sl5 . . .) < x). 
The following simple method replaces many complicated calculations used 
for special problems in the applied literature. It represents a special case of 
the general theory in XI,6, but for convenience the following exposition is 
self-contained. 
The distribution M is given by E.3). In it yj stands for the renewal 
measure corresponding to the defective distribution H. To the associated 
proper distribution aH there corresponds the renewal measure atp given by 
ayj{dx} = eKXy{dx}, with k given by D.6) or D.8). Hence E.3) may be 
XII.5 APPLICATIONS 411 
rewritten in the form 
('5.11) M{dx) = [l-/f(oo)]e-** • 
By the basic renewal theorem of XI, 1 the renewal measure atp is asymptotically 
uniformly distributed with a density /5 where 
E.12) jg = [*x<**H{dx}. 
Jo 
Integrating E.11) between t and oo we see therefore that as t -*¦ oo 
-*t 
E.13) 1 - M(t) ~ * ~(> e 
provided only that /? < oo. [Otherwise 1 — M(t) = o(e~Kt).] 
The constant /? depends on the distribution H which is usually not known 
explicitly, but the exponent k depends only on the given distribution F. 
At worst therefore E.13) represents an estimate involving an unknown 
factor, and even this result is not easily obtained by other methods. The 
next example illustrates important applications. 
{d) Cramer's estimate for probabilities of ruin. We now apply the preceding 
result to example (a). Here the drift is toward -f-oo, and hence the roles of 
the positive and negative half-axes must be interchanged. This means that 
k < 0, and the distribution H is to be replaced by the distribution E.7) 
of the first entry into — oo, 0. Thus E.12) takes on the form 
E.14) 0 = «|0 e~Mv \V\ B(y) dy. 
J 
—00 
We saw that p@) = cd)y and so E.13) is equivalent to the statement that as 
x —*¦ oo 
E.15) P{min (So, Sx,.. .) < ^ 
\x\p 
This formula has many applications. In queuing theory the left side represents 
the limit distribution for the waiting time of the nth customer (see the theorem 
in VI,9). In exagnple V\,9{d) it was shown that the basic ruin problem of 
VI,5 may be reduced to this queuing problem. With different notations the 
same problem is treated in example XI,7(a).n It is therefore not surprising 
that the estimate E.15) has been derived repeatedly under special circum- 
stances, but the problem becomes simpler in its natural general setting. 
11 To our defective (Jistribution p, which is concentrated on — oo, 0, there corresponds 
in XI,7 the defective distribution L with density (<x/c)(l — Fix)) concentrated on 0, oo. 
412 RANDOM WALKS IN 311 XII.6 
From the point of view of the general theory E.15) is equivalent to a famous 
estimate for the probability of ruin in the theory of risk due to H. Cramer.12 > 
6. A COMBINATORIAL LEMMA 
The distribution of ladder epochs depends on a simple combinatorial 
lemma, and the probabilistic part of the argument will appear clearer if we 
isolate this lemma. 
Let xx, . . . ,xn be n numbers, and consider their partial sums 
s0 = 0, . . . , sn = x1 + • • • + xn. 
We say that v > 0 is a ladder index if sv > s0, . . . , sv > 5v_l5 that is, if 
sv exceeds all preceding partial sums. There are n ladder indices if all x^ 
are positive, whereas there are none if all xv are negative. 
Consider the n cyclical reorderings (ar1} . . . , xn), (x2, . . . ,xn,x1),.. . , 
(xn, a?i, . . . , zn_i) and number them from 0 to n — 1. The partial sums 
s[v) in the arrangement number v are given by 
F..) 4"- 
Sn — Sv + h-n+v for k = T1 — V+ 1, . . . , W. 
Lemma 1. Suppose sn > 0. Denote by r the number of cyclical re- 
arrangements in which n is a ladder index. Then r > 1, and in each such 
cyclical arrangement there are exactly r ladder indices. 
Examples. For (—1, —1, —1,0, 1, 10) we have r = 1: the given order 
is the only one in which the last partial sum is maximal. For (—1,4, 7, 1) 
we have r = 3; the permutations number 0, 2, and 3 yield 3 ladder indices 
each. > 
Proof. Choose v so that sv is maximal, and if there are several such 
indices choose v as small as possible. In other words, 
F.2) sv j> sl5 . . . , sv j> sv_i, sv > sv+1, • • • » sv > s 
n. 
It is then seen from F.1) that in the vth permutation the last partial sum 
is strictly maximal and so n is a ladder index. Thus r > 1. Without loss 
of generality we- now suppose that n is a ladder index in the original 
arrangement, that is, sn > s3 for all j. The quantities in the first line in 
F.1) are then <sn, and the second line in F.1) shows that n is a ladder 
index also in the vth permutation iff sv > su . . . , sv > sv_x, that is, 
iff v is a ladder index in the original arrangement. Thus the number of 
*2 For a newer derivation by Wiener-Hopf techniques in the complex plane see Cramer's 
paper cited in VI,5. Our E.15) is Cramer's E7). 
XII.7 DISTRIBUTION OF LADDER EPOCHS 413 
permutations in which n is a ladder index equals the number of ladder 
indices, and the lemma is proved. ^ 
Weak ladder indices are denned analogously except that the strict 
inequality > is replaced by >. The preceding argument applies to them 
and leads to 
Lemma 2. If sn > 0, lemma 1 applies also to weak ladder indices. 
7. DISTRIBUTION OF LADDER EPOCHS 
In the preceding sections we have focused our attention on the ladder 
height, but now we turn to the ladder epochs. Let 
a.w rn = pis, < o,..., s,.! < o, sn > o}. 
This is the probability that the first entry into 0, oo occurs at the «th step, 
and so {rn} is the (possibly defective) distribution of the first ladder epoch 
We introduce its generating function 
00 
G.2) r(s) = ZTnSn, 0 <, s <> 1. 
n=l 
The following remarkable theorem shows that the distribution {tJ is 
completely determined by the probabilities P{Sn > 0} and vice versa. It 
was discovered by E. Sparre Andersen whose ingenious but extremely com- 
plicated proof was gradually simplified by several authors. We derive it as a 
simple corollary to our combinatorial lemma. [A stronger version is 
contained in (9.3) and will be treated by Fourier methods in chapter XVIII.] 
Theorem 1: 
G.3) log —l—- = I - P{Sn > 0}. 
1 — t(s) n=i n 
Note: The theorem and its proof remain valid if in G.1) and G.3) the signs 
> and < are replaced by > and <, respectively. In this case {rn} 
stands for the distribution of the first weak ladder epoch. 
Proof. For each sample point consider the n cyclical permutations 
(Xv! . . . , Xn, Xx, . . . , Xv-1) and denote the corresponding partial sums by 
Sqv>, • • • > SLV>- Fix an integer r and define n random variables Y(v) as 
follows: \'"v) =1 if n is the rth ladder index for (S[v), . . . , S^v)) and 
Y(v) =0 otherwise. To v = 1 there corresponds the unpermuted sequence 
(So, . . . , Sn) and hence 
G.4) P{YA)= l} = rlr) 
414 RANDOM WALKS IN ft1 XII.7 
where {V^} is the distribution of the rth ladder epoch. This epoch is the 
sum of r independent random variables distributed as $~x, and hence 
T^r) is the coefficient of sn in the rth power rr(s). 
For reasons of symmetry the variables Y(v) have a common distribution; 
since they assume only the values 0 and 1 we conclude from G.4) that 
i 
ac\ _(r) TVV*1^ — TYVd'-L.. . . L V(")\ 
n 
By our last lemma the sum YA) + • •» + Y(n) can assume only the values 
0 or r, and hence 
G.6) - r{nr) = - P{Y@) + • • • + Y(n) = r}. 
r n 
For fixed n and r = 0, 1, . . . the events on the right are mutually exclusive 
and their union is the event {Sn > 0}. Summing over r we get therefore 
G.7) I 1 rj' = - P{Sn > 0} 
r=i r. n 
On multiplying by sn and summing over n we obtain 
G.8) | - r(s) = 2- P{Sn > 0} 
r=i r «=i n 
which is the same as the assertion G.3). > 
Corollary. If F is continuous and symmetric, then 
G.9) t(s)= 1-Vl -s. 
Proof. All the probabilities occurring in G.3) equal and so the right side 
equals log A/v 1 — s ). f>* 
It is of interest to generalize this result assuming only that 
G.10) P{Sn>0}-*. 
Such is the case whenever the distribution of SJan tends to the normal distribution ?R. 
We shall assume a tfifle more than G.10), namely that the series 
G.11) f -[P{Sn>0}-*] = c 
n=l n 
converges (not necessarily absolutely). It will be shown in XYIH,5 that G.11) holds when- 
ever F has zero expectation and a finite variance. 
The following th» op m is given not only because of its intrinsic interest, but also as an 
illustration for the use of refined Tauberian theorems. 
XII.7 DISTRIBUTION OF LADDER EPOCHS 415 
Theorem la. IfG.U) holds then 
G.12) ii 
Thus when F has zero expectation and a finite variance the distribution {rn} is very 
similar to the one encountered in the binomial random walk. 
Proof. From G.3) we see that as s -*-1 
00 
G.13). * * 
It follows that 
G.14) LL*>. 
— s 
On the left one recognizes the generating function of the probabilities in G.12). These 
decrease monotonically, and hence G.12) is true by the last part of the Tauberian theorem 
5 in XIII.5. > 
Theorem 2. The random walk drifts to —oo iff 
G.15; 
l Tl 
This criterion remains valid13 with {Sn > 0} replaced by {Sn > 0}. 
Proof. Drift to — oo takes place -iff the ascending ladder processes are 
terminating that is, iff the distribution of 3~x is defective. This is the same 
as tA) < 1, and in this case the two sides in G.3) remain bounded as 
s—>-l. The condition G.15) is seen to be necessary and sufficient. The 
same argument applied to weak ladder epochs justifies the concluding 
assertion. > 
We know that drift to — oo takes place if F has an expectation ix < 0, but it is not 
analytically obvious that /j. < 0 implies G.15). The verification of this fact provides an 
excellent technical exercise of methodological- interest. (See problem 16.) 
This theorem has surprising implications. 
Examples, (a) Let F be a strictly stable distribution with F@) = d < f. 
Intuitively one would expect a drift toward oo, but in fact the random walk 
is of the oscillating type. Indeed, the series G.15) reduces to A — d)^/* 
and diverges. Thus 3~x is proper. But the same argument applies to the 
negative half-axis and shows that also the descending ladder variable 3~~ 
is proper. 
(b) Let F stand for the symmetric Cauchy distribution and consider the 
randoniwalk generated by the variables X^ - Xn + 1. The median of the 
13 We shall see that ]T n~1P{Sn = 0} < oo under any circumstances [see 9{c)\. 
416 RANDOM WALKS IN Si1 XII.7 
sums S^ = Sn + n lies at n, and intuitively one should expect a strong 
drift to oo. Actually the probabilities P{S^ > 0} are again independent of 
77, and as in the preceding example we conclude that the random walk is of 
the oscillating type. > 
Theorem 3. The ladder epoch 3TX has a finite expectation {and is proper) 
iff the random walk drifts to oo. In this case 
G.16) log E(jrx) = log 2 krk = | ± P{Sn < 0}. 
n=i n 
(The series diverges in all other cases). 
Proof. Subtracting the two sides of G.3) from log(l— s)'1 we get for 
0< s < 1 
G.17) log i-=^-} = f - [1 - P{Sn > 0}]. 
1 — s n=i n 
As s —>¦ 1 the left side converges iff &~x is proper and has a finite expectation. 
The right side tends to the right side in G.16), and by theorem 2 this series 
converges iff the random walk drifts to oo. > 
In conclusion we show that the generating function occurring in theorem 1 
has an alternative probabilistic interpretation which will lead directly to the 
amazing arc sine laws. 
Theorem 4. The generating function of the probabilities 
G.18) Pn = P{SX > 0, S2 > 0, . . . , Sn > 0} 
is given by 
G.19) pis) = —^— 
1 - t(s) 
that is, 
G-20) log pis) - 2 ~ P{Sn > 0}. 
n=l n 
For reasons of symmetry the probabilities 
have the generating function q given by 
oo 
G-22) log q(s) = 2 - P{Sn < 0}. 
n=i n 
(Cf. problem 21.) 
XII.8 THE ARC SINE LAWS 417 
Proof. We use the duality lemma of section 2. From the theory of 
recurrent events it is clear that G.19) is the generating function of the 
probabilities pn that n is a ladder epoch, that is 
G.23) Pn = P{Sn > So, .,. ', SB > S^}. 
Reversing the order of the variables X,- we get the dual interpretation G.18). 
[This is really contained in B.1) when 7 = 0, oo.] >. 
8. THE ARC SINE LAWS 
One of the surprising features of the chance fluctuations in coin tossing 
finds its expression in the two arc sine laws A; 111,4 and 8). One of them 
implies that the number of positive terms in the sequence S1}. . . , Sn is 
more likely to be relatively close to 0 or n than to n/2 as one would naively 
expect. The second implies the same for the position of the maximal term. 
We show now that these .laws are valid for arbitrary symmetric and for many 
other distributions. This discovery proyes the general relevance and applic- 
ability of the discussions of 1; III. 
In the following we have to cope with the nuisance that the maximum may 
be assumed repeatedly and that partial sums may vanish. These possibilities 
can be disregarded if F is continuous, for then the probability is zero that 
any two partial sums are equal. (Readers are advised to consider only this 
case.) For the general theory we agree to consider the index of the first 
maximum, that is, the index k such that 
n. 
(8.1) Sfc > So,. . . , Sfc- > Sk_1, Sk > Sk+1, . . . , SA > S 
Here n is fixed and k runs through the values 0, 1, . . . , n. The event (8.1) 
must occur for some k < n, and so we may define the (proper) random 
variable Kn as the index of the first maximum, that is, the index where 
(8.1) occurs. Here (So = 0). 
The event (8.1) requires the simultaneous realization of the two events 
{Sk > So, ¦ . . , St > S,_x} and {S*+1 - S, < 0,. . . , Sn - S, < 0}. The 
first involves only X1} . .. , Xk, the second, only Xk+l,.. . , Xn, and hence 
the two events are independent. But these are the events occurring in the last 
theorem and so we have proved 
Lemma 1. For all k, n 
(8.2) P{Kn = k) = Pkqn_k. 
Suppose now that P{Sn > 0} = P{Sn < 0} = \ for all n. The right 
sides in G.15) and G.17) then reduce to \ log A— s)~x, and hence 
p(s) = q{s) = J~ 
418 RANDOM WALKS IN R1 XII.8 
Thus 
and this may be writs.. - the more pleasing form 
This expression was used in 1; 111,D.1) to define the discrete arc sine distri- 
bution which was found in 1; III,4 and 8 to govern various random variables 
connected with coin tossing; its limiting form was derived in 1; 111,D.4). 
Using in particular the arc sine law of 1; 111,8/ we can state 
Theorem 1. If F is symmetric and continuous, the probability distribution 
of Kn (the index of the first maximum in So, Sl5 . . . , Sn) is the same as in 
the coin tossing game. It is given by (8.3) or (8.4). For fixed 0 < a < 1 as 
n —*- oo 
(8.5) P{Kn < na} -v 2 - arc sin 
77 
The limit distribution has the density l/|>r\/a(l —a)] which is unbounded 
at the endpoints 0 and 1 and has its minimum at the midpoint ?. This shows 
that the reduced maximum KJn is much more likely to be close to 0 or 1 
than to \. For a fuller discussion see 1; 111,4 and 8. For an alternative form 
of the theorem see problem 22. 
This theorem can be generalized just as theorem 1 of the preceding section. 
Theorem la.14 If the series 
(8-6) V - [P{Sn > 0} - ?] = c 
n-111 
converges, then as n —*¦ oo and n — k —*• oo 
(8.7) F0C-*>~HP"""^-i-. 
and hence the ar&sine law (8.5) holds. 
It will be shown in XVIII,5 that the series (8.6) converges whenever F has zero expecta- 
tion and finite variance. The arc sine laws therefore hold for such distributions. 
Proof. From G.20) and the elementary theorem of Abel on power series we conclude 
that as s —<¦ 1 
(8.8) log (p(s)Vl - s ) = T - [P{Sn > 0} - J] - c, 
14 This theorem was proved by laborious calculations by Sparre-Andersen. The remark 
that the Tauberian theorem removes all trouble, is due to Spitzer. For a generalization 
see 9.d. 
XI1.8 THE ARC SINE LAWS 419 
and so 
(8.9) p(s)~e*-(l-sy-* 
By the definition G.18) the pn decrease monotonically, and hence the last part of the 
Tauberian theorem 5 of XIII,5 implies that coefficients of the two power series in (8.9) 
exhibit the same asymptotic behavior. Thus 
(8.10) Pn~ec\*\(-Vn, n-*oo 
For qn we get the same relation with c replaced by —c, and hence the assertion (8.7) 
follows from (8.2). The derivation of the arc sine law depends only on the asymptotic 
relation (8.7) and not on the identity (8.4). >¦ 
Theorem 1 and its proof carry over to arbitrary strictly stable distributions. If 
P{Sn > 0} = 8 is independent of n we get from G.20) and G.22) 
(8.11) p(s) = A -*ra, q(s) = A - j)*-i 
and hence 
(8-12) P{Kn = k} =pkqn_k = (-W75) («!*) * 
The limit theorem (8.5) holds with the arc sine distribution on the right replaced by the 
distribution with density 
sin tt<5 1 
(8.13) j— -, 0<*<l. 
77- X1 °A—X)° 
Theorem la carries over to distributions belonging to the domain of attraction of a stable 
distribution. 
In 1; III we had to prove the two arc sine laws separately, but the next 
theorem shows that they are equivalent. Theorem 2 (for continuous 
distributions) was the point of departure of the investigations by E. Sparre- 
Andersen introducing the new approach to fluctuation theory. The original 
proof was exceedingly intricate. Several proofs are now in existence, but the 
following seems simplest. 
Theorem 2. The number Hn of strictly positive terms among Sl5 . . . , Sn 
has the same distribution (8.2) as Kn, the index of the first maximal term in 
So = 0, Sl5 . . . , Sn. 
(See problem 23.) 
This theorem will be reduced to a purely combinatorial lemma. Let 
xl5 . . . , xn be n arbitrary (not necessarily distinct) real numbers and put 
(8.14) s0 = 0, sk = xx + • • - + x 
k. 
The maximum among s0,. . . , sn may be assumed repeatedly and we must 
therefore distinguish between the index of the first and the last maximal term. 
Consider now the n\ permutations xix,. . . , xin (some of which may have 
420 RANDOM WALKS IN Oi1 XII.8 
the same outer appearance). With each we associate the sequence of its 
n + 1 partial sums 0, xi%, . . . , xi% + • • • + xin. 
Example, (a) Let xx = x2 = 1 and x3 = x4 = — 1. Only 6 rearrange- 
ments (xix, . . . , xit) are. distinguishable, but each represents four permu- 
tations of the subscripts. In the arrangement A,1,-1,-1) three partial 
sums are strictly positive, and the (unique) maximum occurs at the third 
place. In the arrangement (—1,-1,1,1) no partial sum is positive, but 
the last is zero. The first maximum has index 0, the last index 4. > 
Theorem 2 will be shown to be a- simple consequence of 
Lemma 2. Let r be an integer 0 < r < n. The number Ar of permutations 
with exactly r strictly positive partial sums is the same as the number Br 
of permutations in which the first maximum among these partial sums occurs 
at the place r. 
(See problem 24.) 
Proof.15 We proceed by induction. The assertion is true for n'— 1 
since x1 > 0 implies Ax == Bx = 1 and Ao ='2?0 = 0 while xx < 0 
implies Ax = Bx = 0 and Ao = Bo = 1. Assume the lemma true when n 
is replaced by n — 1 ^ 1. Denote by A(k) and Bj.k) the numbers corre- 
sponding to Ar and Br when the «-tuple (xu .. . , xn) is replaced by the 
(n — l)-tuple obtained by omitting xk. The induction. hypothesis then 
states that A(rk) = B(rk) for * 1 <, k <, h and r = 0, ...,«- 1. This is 
true also for r = n since trivially A^ — B™ =0. 
(a) Suppose xx + • • • + xn <, 0. The n\ permutations of (xlf . . . , xn) 
are obtained by choosing the element xk at the last place and" permuting the 
remaining n — 1 elements. The nth partial sum being .^0, it is clear that 
the number of positive partial sums and the index of the first maximal term 
depend only on the first n — 1 elements. Thus 
(8.15) 4 
and hence Ar^= Br. by the induction hypothesis. 
15 The following proof is due to Mr. A. W. Joseph of Birmingham (England). Its 
extreme simplicity comes almost as a shock if one remembers that in 1949 Sparre 
Andersen's discovery of theorem 2 was a sensation greeted with incredulity, and the original 
proof was of an extraordinary mtricacy and complexity. A .reduction to the purely com- 
binatorial lemma 2 and an elementary proof of the latter was given by the author. (See 
the first edition of the present book.) Joseph's proof is not only simpler, but is the first 
constructive proof establishing a one-to-one correspondence between the two types of 
permutations. Our discussion of this aspect in lemma 3 exploits an idea of Mr. M. T. L. 
Bizley of London (England). The author is grateful to Messrs. Joseph and Bizley foi 
permission to use their unpublished results (communicated when the typescript was alread] 
at the printers). 
XII.8 THE ARC SINE LAWS 421 
(b) Assume xx + • • • + xn > 0. The nth partial sum is then positive 
and the preceding argument shows that now 
(8.16) . Ar = 
To obtain an analogous recursion formula for B,, consider the arrangements 
(xk, xjx, . . . ,Xjni) starting with xkr The nth partial sum being positive 
the maximal terms of the partial sums have positive subscripts. Clearly the 
first maximum occurs at the place r A <, r <; ri) iff the first maximum of 
the partial sums for {xit, ... , x,ni) occurs at the place r — 1. Thus 
(8.17) Br= " 
A comparison of (8.16) and (8.17) shows again that Ar = Br, and this 
completes the proof. . >. 
We shall presently see that this argument yields further results, but first 
we return to the 
Proof of theorem 1. We proceed as in the proof of theorem 1 in section 7. 
Consider the n\ permutations (xix, . .. , x{j and number them so that the 
natural order (xl5... , xn) counts as number one. For a fixed integer 
0 < f < n define Y00 = 1 if the permutation number v has exactly r 
positive partial sums, and Y(v> = 0 otherwise. For reasons of symmetry the 
n\ random variables have a common distribution, and hence 
(8.18) P{Hn =* r) = P{YA) = 1} = E(YA)) = - 2 E(Y(v)). 
n! 
Similarly . 
(8.19) P{Kn = r} = — 2 E(Z(V)) 
n! 
where Z(v) = 1 if in the permutation number v the first maximal partial 
sum has index' r, and Zi?) = 0 otherwise. By the last^lemma the sums 
2 Y(>>) and ^Z{v) are identical, and hence the ^probabilities in (8.18), and 
(8.19) are the same. . >. 
Note on Sparre Andersen transformations. It follows from lemma 2 that 
there exists a transformation such that each «-tuple (xu . . . , xn) of real 
numbers is mapped into a rearrangement {xit,.. . , xin) in such a way that: 
(i) if exactly r @ <C r <, n) among the partial sums sk in (8.14) are strictly 
positive, then a maximum of the partial sums of (xti,. . . , x{-n) occurs for 
the first time with index r> and (/*) the transformation is invertible (or 
one-to-one). Such transformations will be called after E. Sparre Andersen 
even though he was concerned with independent random variables without 
422 RANDOM WALKS IN %x XII.8 
being aware of the possibility to reduce uieorem 2 to the purely com- 
binatorial lemma 2. A perusal of the proof of lemma 2 reveals that it 
contains implicitly a prescription for a construction of a Sparre Andersen 
transformation. The procedure is recursive, the first step being given by the 
rule: if sn < 0 leave the w-tuple (xlf . . . , xn) unchanged, but if sn > 0 
replace it by the cyclical rearrangement (xn, xx,. . . , xn_1). The next step 
consists in applying the same rule to the (n — l)-tuple ($x,.. . , #n_i). The 
desired rearrangement (xix, .. . , x{) is obtained after n — 1 steps. 
Examples, (b) Let (xx,. . . , z6) = (—1, 2, — 1, 1, 1, —2). No change 
occurs at the first step while the second leads to A, —1, 2, —1, 1, —2). 
As s4 = 1 the third step yields A, 1, —1,2, —1, —2), and the fourth 
step introduces no change because s3 = 0. Since s2 = 1 the final step 
leads to the arrangement A, 1,2, —1, —1, —2). The unique maximum of 
the partial sums occurs at the third place, and in the original arrangement 
exactly three partial sums are positive. 
(c) Suppose that x, < 0 for all j. The initial and the final arrangement of 
the Xj are identical. No partial sum is positive, and s0 = 0 represents a 
maximum (which is repeated if x1 = 0). > 
It is preferable to replace the recursive construction by a direct description 
of the final result. We give it in the following lemma; because of its intrinsic 
interest a new proof is given which is independent of the preceding lemma. 
(See also problem 24.) 
Lemma 3. Let (xlf . . . , xn) be an n-tuple of real numbers such that the 
partial sums sVl, . . . , sVr are positive and all others negative or zero: here 
vi > ^2 > '¦' ' > vr > 0- Write down xVi, . . . , xVr followed by the remaining 
Xj in their original order. {If all partial sums are <0 then r = 0 and the 
order remains unchanged.) Among the maxima of the partial sums in the new 
arrangement the first occurs at the rth place, and the transformation thus 
defined is one-to-one. 
Proof. Denote the' new arrangement by (?lf. .. , ?n) and its partial sums 
by a0, . . . , ov To every subscript j <, n there corresponds a unique 
subscript 'k such that ?3; = xk. In particular, ?u . . . , ?r agree with 
xVi, . . . , xVt in that order. 
Consider first a j such that sk < 0. Tt is clear from the construction that 
j > k and the elements ?/-jh-i; •••>?/ represent a permutation of xlt]. . . , 
xk. Thus o-j = cr3_fc 4- sk < a,-*, and so cr, cannot be the first maximal 
partial sum. 
If r = 0 it follows that the first maximum among the a, is assumed for 
j = 0. When r > 0 the first maximum occurs at one among the places 
0, 1-, . . . , r, and we show that only r is possible. Indeed, if / < r it 
XII.9 MISCELLANEOUS COMPLEMENTS 423 
follows from the construction that the v; elements ?,, . . . , fj_1+F. 
coincide with some rearrangement of (xl,...fxv). Thus <t;-_1+v = 
= Gj-i + sVj > O}-\, and hence no maximum can occur at the place /' — 1. 
To complete the proof of the lemma it remains to show that the trans- 
formation is one-to-one. As a matter of fact, the inverse to fi, ...,?„ may 
be constructed by the following recursive rule. If all ct; < 0 leave the 
arrangement unchanged. Otherwise let k be the largest subscript such that 
ak > 0. Replace (f1?.. . , fn) by (f2, ...,&, &, ffc+1, . . . , fB), and 
apply the same procedure to the (k— l)-tuple (?2,. . . , ?fc). >> 
. Note on exchangeable variables. It should be noticed that the proof does not depend on 
the independence of the variables X,- but only the identity of the joint distribution for each 
of the n\ arrangements (Xtl,. . . , X,n). In other words, theorem 2 remains valid for ev^ery 
n-tuple of exchangeable variables (VII,4) -although naturally the common distribution of 
Kn and IIn will depend on the joint distribution of the X3. As an interesting example let 
Xx, X2,... be independent with a common distribution F, and put Yk = Xk — SJn 
(where k == 1,... , n). The variables Yj,. .. , Yn are exchangeable and their partial 
sums are 
(8.20) Zk = Sk - kSJn, k = 1, .. . , n - 1. 
With reference to the graph of (So, Sj,..., S^) we can describe 2^ as the vertical distance 
of the vertex St from the chord joining the origin to the endpoint (n, Sn). 
We now soppose that F is continuous (in order to avoid the necessity of distinguishing 
between the first and the last maximum). With probability 1 there is a unique maximum 
among the terms 0, 2X,..., ?„_!• To the cyclical rearrangement (Y2, . . ., Yn, Yx) 
there correspond the partial sums 0, 22 — 2j,. .. , 2n_] — Zx, —^lt and it is clear that 
the location of the maximum has moved one place ahead in cyclical order. (If the original 
maximum was at the zero place then 2^ < 0 for k = 1,..., n—1, and the new maximum 
is at the place n — 1.) In the n cyclical permutations the maximum is therefore assumed 
exactly once at each place, and its position is uniformly distributed over 0, 1,... , n—1. 
We nave thus the following theorem due to Sparre-Andersen and related to the theorem 
3ofl; 111,9 in coin tossing. 
Theorem 3. In any random walk with continuous F and for any n the number of vertices 
among S1,...,Sn_1 that lie above the chord from @,0) to («, Sn) is uniformly distributed 
over 0,1,..., n — 1. 
(The same is true of the index of the vertex with greatest maximal distance.) 
9. MISCELLANEOUS COMPLEMENTS 
(a) Joint Distributions 
The argument leading to theorem 1 of section 7 requires only notational 
changes to yield the joint distribution of the ladder variables. Adapting the 
notation of section 1, let / be an interval in 0, oo and denote by N(nr){I} 
the probability that n be the rth ladder epoch and Sn e /. Put 
(9.1) H{I,s}= 
424 RANDOM WALKS IN 511 XII.9 
It is seen by induction for fixed s 
The argument leading to G.3) now yields without difficulty the following 
result due to G. Baxter which reduces to G.3) when / = 0, oo. 
Theorem. For I c 0, oo and 0 < s < 1 
(9-3) f l-fr*{r,s}= i} 
r=i r n=i n 
A simpler and more tractable form will be derived in XVIII,3. 
(b) A Mortality Interpretation for Generating Functions 
The • following interpretation may help intuition and simplify formal 
calculations. For fixed s with 0 < s < 1 consider the defective random 
walk which at each step has probability 1 — s to terminate and otherwise 
subject to the distribution sF. Now sn Fn*{I} is the probability of a position 
in / at time n, the defect 1 — sn representing the probability of a prior 
termination. All considerations carry over without change, except that all 
distributions become defective. In particular, in our random wafk with 
mortality, (9.1) is simply the first ladder height distribution, and (9.2) the 
analogue to Hr* of sections 2-3- The generating function t(s) now 
equals the probability that a ladder index will occur. 
(c) The Recurrent Event 
(9-4) {Sx < 0,. . . , S^ < 0, Sn = 0} 
represents a return to the origin without previous visits to the right half-axis. 
It was considered in section 1 in the definition of weak ladder variables. 
Denote by con the probability of the first occurrence of the event (9.4) at 
epoch n, that isj 
(9.5) co,, = P{SX < 0, . . . , Sn_x < 0, Sn = 0}. 
If co(s) = 2 °JrsT-> tnen <°r is tne generating function for the rth occurrence 
and so 1/[1 — oj(s)] is the generating function for the probabilities (9.4). 
A simplified version of the proof of G.3) leads to the basic identity 
(9.6) iOg 
— a>(s) 
Comparing this with G.3), G.16), G.22),, etc., one sees how easy it is to 
pass from weak to strict ladder variables and vice versa. Formula (9.6) 
XII. 10 PROBLEMS FOR SOLUTION 425 
confirms also the remark of section 1 that the probabilities of (9.4) remain 
unchanged if all inequalities are reversed. 
(d) Generalization to Arbitrary Intervals 
The theory of section 3 generalizes with trite notational changes to the 
situation in which 0, oo is replaced by an arbitrary interval A, and — oo, 0 
by the complement A'. In particular, the Wiener-Hopf integral equation 
remains unchanged. The reader is invited to work out the details; they 
are fully developed in XVII 1,1. (See also problem 15.) 
10. PROBLEMS FOR SOLUTION 
1. In the binomial random walk [example 2(b)] let ek be the expected number 
of indices n > 0 such that Sn = k, Sx > 0, . . . , Sn_x > 0 (visits to k preceding 
the first negative value). Denote by / the probability of ever reaching —1, 
that is, / = 1 if q >p and /' = q/p otherwise. Taking the point A, 1) as new 
origin prove that e0 = 1 + pfe0 and ek = p(ek_i+fek) for k > 1. Conclude 
that for k > 0 
ek = p-1 if p > q, ek = (p/q^q'1 if p <, q. 
2. Continuation. For k > 1 let ak be the expected number of indices n > 1 
such that Sn = k, Sx > 0, . . . , Sn_.x > 0 (visits to h preceding the first return 
to the origin). Show that a} = pek_x and hence 
ak = 1 if p > q, ak = (p/q)k if p < q. 
This gives a direct proof of the paradoxical result of example 2(b). 
Note. The following problems 3-6 may serve as introduction to the problems 
of this chapter and can be solved before studying it. They present also examples 
for explicit solutions of the basic integral equations. Furthermore, they illustrate 
the power and elegance of generating functions [try to solve equation A) directly!]. 
3. The variables Xk of a random walk have a common arithmetic distribution 
attaching probabilities /i,/2, ... to the integers 1, 2, ... and q to —1 (where 
q +fx +/2 + • • • =1). Denote by Xr (r = 1, 2,. . .) the probability that the 
first positive term of the sequence SX,S2, . . . assumes the value r. (In other 
words, {Ar} is the distribution of the first ladder height.) Show that: 
{a) The lr satisfy the recurrence relations 
A) ^r=/,+ 
(b) The generating functions satisfy 
B) ^).1 
Ax<7 + qs 1 — 1 
(c) If E(Xk) = n =/'d) - q > 0, the/e exists a unique root, 0 < a < 1, of 
the equation 
<3) f(s)+qls = l. 
426 RANDOM WALKS IN III1 XI 1.10 
From the fact that A must be monotone and <1 in 0, 1 conclude that 
D) 
a s — a 
This is equivalent to 
E) K = \L 
(d) If E(Xk) <0 the appropriate solution is obtained letting Xl = (\—q)!q 
in BV Then D) and E) hold with cr = 1. 
4. Adapt the preceding problem to weak ladder heights. In other words, 
instead of Ar consider the probability that the first non-negative term of Slt S2,. . . 
assumes the value r (r = 0, 1,. . .)• Show that A) and D) are replaced by 
Da) y(s) - 1 - ? 
— a 
5. In the random walk of problem 3 (but without using this problem) let x 
be the probability that Sn < 0 for some n. Show that x satisfies the equation 
C) and hence x = a. 
6. Continuation. Show that the probability that Sn < 0 for some n > 0 is 
q + f(a) = 1 - q(a-l-\). Verify that A'(l) = [xa[q(\ -a)], which is a special 
case of relation B.8) (or Wald's equation). 
7. Derive A.13) by straight calculation from A.12). 
8. Hitting probabilities. For t > 0 and ? > 0 denote by G(t, f) the probability 
that the first sum Sn exceeding / will be </ +. f. Prove that C satisfies the 
integral equation 
C7(/, f) = F(t + ?) - F(t) + [G(t-y, !-)F{dy}.r 
J-a 
In case of non-uniqueness, G is the minimal solution. The ladder height dis- 
tribution H is uniquely determined by H{?) = C@, f). 
9. Let if be a continuous probability distribution concentrated on 0, °o and 
H~ a possibly defective continuous distribution concentrated on — °o, 0. Suppose 
that 
F) h + h- - h*h- = f; 
is a probability distribution. From the uniqueness theorem in section 3 it follows 
that, H and H~ are the distributions of the points of first entry Jf and &~ 
in the random walk generated by F. In this way it is possible to find distributions 
F admitting of an explicit representation of the form F). Such is the case if 
0 < q <, 1 and H and H~ have densities defined by either 
(a) be~hx for x > 0, q for -1 < x < 0 
or 
(b) b~x for 0 < x < b, q for -1 < x < 0 
XI1.10 PROBLEMS FOR SOLUTION 427 
In case (a) the distribution has a density given by 
(b - q)e~bx + qe~^x+^ for x > 0, and ^-b(x+1> for -1 < x < 0. 
In case (b) if 6 > 1 the density of F is given by qb~\\ + x) for —1 < x < 0, 
by ^Zr1 for 0 < x < Z» - 1, and by qb~\b - x) for Z> - 1 < x < b. In either 
case F has 0 expectation iff q = 1. 
10. From C.11) conclude: If H and if" are proper and have variances then 
E(Xi) =0 and E(X») = -2E(jr1)E(Jf^). 
11. Analytic proof of Wold's relation B.8). From C.11) conclude 
1 - F(x) = [1 -P@)][l -#(*)] + f°%{<fo}[/f(*.-y) - #(*)] 
for .» > 0 and a; < 0, respectively. Conclude that F has a positive expectation 
n iff if has a finite expectation v and p@) < 1. Conclude by integration ovtr 
-co, oo that /i = [1 — p@)]i>, which is equivalent to B.8). 
12. To example 4(c). If n > 0 the denominator has a positive root j0 < 1, 
exactly Z» — 1 complex roots in |j| < s0, and a — 1 complex roots in \s[ > 1 
The situation for n < 0 is described by changing s into 1/j. 
13. The generating function of the ascending ladder height distribution ii 
example 4(c) is given by 
For descending ladder heights change s/(fk into skjs. 
14. To example 4(c). Suppose that the X,- assume the values —2, —1,0, 1,2 
each with probability i. Show that the ascending ladder height distribution is 
given by 
x 1 + V~5 x 2 
For the vfeaA: heights Ao = TVG- V~5 ), Ar = ^A + ^5 ), A2 = l. 
15. In example 4(c) denote by y|n) the probability that the first n steps do not 
lead out of the interval —B, A and that the nth. step leads to the position < 
(Thus y>l%] =0 for k > A and k < —B. As usual y?0) equals 1 when k = ^ 
and 0 otherwise.) Let y^- = 2 Vkn) be the expected number of visits to k i 
to leaving —B, A. Show that 
and that for A: > .^4 and k < -B 
A 
Pic = 2 Vv/*-v 
| 
is the probability that the first exit from the interval —B, A leads to the point I 
[This problem is important in sequential analysis. It illustrates the situation 
described in 9(d).] 
428 RANDOM WALKS IN ft1 XII. 10 
16. Theorem 2 of section 7 implies that if /c < 0 then ]?/f^PfS,, > 0} < oo. 
Fill in the following direct proof. It suffices to show (Chebyshev) that 
F{dy) < co, ^ ^ fV2 
F{dy) < co, ^ ^ fV2 
\y\>n 
The first is obvious. To prove the second relation write the integral as a sum of n 
integrals over k — 1 < \y\ < k, k = 1,. . . , n. Reverse the order of summation 
and conclude that the whole series is <2E(|X|). 
17. For the coin-tossing game show that 
2 t 
2 t P{SW = 0} = log 
1 + 
Hint: Easy by observing that the left side may be written as the integral on 
[A -x2)~l - I]*-1 from 0 to s. 
18. Suppose that the random walk is transient, that is U{I} — ^ Fn*{I} < °o 
for every bounded interval. In the notation of section 3 put O = ]?™ pn*. Prove 
the truth of the renewal equation 
U = O + U+H. 
If yr is the analogue of y> for the negative half-line then y>~ = A — ?)$ as in 
A.13). 
19. Conclude that 
and show this to be equivalent to the Wiener-Hopf decomposition C.12). 
20. Derive Wald's identity E(?r1) = E(.5r1)E(X1) directly from the renewal 
equation in problem 18. 
21. To theorem 4 of section 7. The probabilities />* = P{Si > 0, . . . , Sn > 0} 
and q* = P^ < 0, . . . , Sn < 0} have generating functions given by 
logp*(s) = 2 - P{Sn ;> 0} and \oga*n = ? - P{Sn < 0}. 
n=l n n=ln 
22. On the last maximum: Instead ol the variable Kn of Section 8 consider the 
index K* of the last maximum of the partial sums So, . .. , Sn. With the notations 
of the preceding problem, prove that 
P{K*n = k} = p*a*n_k. 
23. Alternative form of theorem 2, section 8. The number n* of non-negative 
terms among So, . . . , Sn has the same distribution as the variable K* of the 
preceding problem. Prove this by applying theorem 2 to (—Xn — Xn-1,. . . , — X2). 
24. The combinatorial lemma 2 of section 8 remains valid if the first maximum 
of So, . . . , Sn is replaced by the last maximum, and the number of positive partial 
sums by the number of non-negative terms in Sl5. .. , Sn (excluding So = 0). 
The proof is the same except for the obvious changes of the inequalities. In like 
manner lemma 3 carries over. 
CHAPTER XIII 
Laplace Transforms. 
Tauberian Theorems. Resolvents 
The Laplace transforms are a powerful practical tool, but at the same time 
their theory is of intrinsic value and opens the door to other theories such as 
semi-groups. The theorem on completely monotone functions and the basic 
Tauberian theorem have rightly been considered pearls of hard analysis. 
(Although the present proofs are simple and elementary, the pioneer work 
in this direction required originality and power.) Resolvents (sections 9-10) 
are basic for semi-group theory. 
As this chapter must cover diverse needs, a serious effort has been made 
to keep the various parts as independent of each other as the subject permits, 
and to make it possible to skip over details. Chapter XIV may serve for 
collateral reading and to provide examples. The rc/iaining part of this book 
is entirely independent of the present chapter. 
Despite the frequent appearance of regularly varying functions only the 
quite elementary theorem 1 of VIII,8 is used. 
1. DEFINITIONS. THE CONTINUITY THEOREM 
Definition 1. If F is a proper or defective probability distribution con- 
centrated on 0,oo, the Laplace transform <p of F is the function defined for 
A > 0 by 
A.1) <p(A) = re-XxF{dx}. 
Jo 
Here and in the sequel it is understood that the interval of integration is 
closed (and may be replaced by — oo, oo). Whenever we speak of the 
Laplace transform of a distribution F it is tacitly understood that F is 
concentrated on 0, oo. As usual we stretch the language and speak of "the 
429 
430 LAPLACE TRANSFORMS. TAUBERIAN THEOREMS XIII. 1 
Laplace transform of the random variable X," meaning the transform 
of its distribution. With the usual notation for expectations we have then 
A.2) 7^) = E(^X). 
Example, (a) Let X assume the values 0, 1, ... with probabilities 
po,'p1, . . . . Then <p{X) = ]Fpne~n?L whereas the generating function is 
P(s) = >Zpl)s"- Thus <p(Z) = P(e-x) and the Laplace transform differs 
from the generating function only by the change of variable s = e~x. This 
explains the close analogy between the properties of Laplace transforms and 
generating functions. 
(b) The gamma distribution with density /a(z) = (x<x~1IF((x.))e~x has the 
transform 
A.3) <p.{X) = -7- f Vu+1)* a* dx = -i— , a > 0. 
r(a)Jo (v*+l)a 
r(a)Jo 
The next theorem shows that a distribution is recognizable by its transform; 
without this the usefulness of Laplace transforms would be limited. 
Theorem 1. (Uniqueness.) Distinct probability distributions have distinct 
Laplace transforms. 
First proof. In VI11,F.4) we have an explicit inversion formula which 
permits us to calculate F when its transform is known. This formula will be 
derived afresh in section 4. 
Second proof. Put y = e~x. As x goes from 0 to 00 the variable y goes 
from 1 to 0. We now define a probability distribution G concentrated on 
j 
0, 1 by letting G{y) = 1 — F(x) at points of continuity. Then 
A.4) <p(A) = f V" F{dx} = f V G{dy} 
Jo Jo 
as is obvious from the fact that the Riemann sums ]T e~**k[F(zk+1) — F(zk)] 
coincide with the Riemann sums ^,yk[G{yk) — G(yk+l)} when yk = e~Xk. 
We know from VII,3 that the distribution G is uniquely determined by its 
moments, and these are given by cp(k). Thus the knowledge of <p(l), 
9?B), . . . determines G, and hence F. This result is stronger than the 
assertion of the theorem.1 > 
The following basic result is a simple consequence of theorem 1. 
1 More generally, a completely monotone function is uniquely determined by its values 
at a sequence {«„} of points such that ^ °n* diverges. However, if the series converges 
there exist two distinct completely monotone functions agreeing at all points an. For an 
elementary proof of this famous theorem see W. Feller, On Miintz' theorem and completely 
monotone functions, Amer. Math. Monthly, vol. 75 A968), pp. 342-350. 
XIII. 1 DEFINITIONS. THE CONTINUITY THEOREM 431 
Theorem 2. {Continuity theorem.) For n = 1,2,... let Fn be a probability 
distribution with transform <pn. 
If Fn-+ F where F is a possibly defective distribution with transform <p 
then <pn(k) -+ <p{X) for A >0. 
Conversely, if the sequence {<pn(X)} converges for each A > 0 to a limit 
<p(X), then <p is the transform of a possibly defective distribution F, and 
F -*- F 
The limit F is not defective iff <p(X) —> 1 as A —*¦ 0. 
Proof. The first part is contained in the basic convergence theorem of 
VIII,1. For the second part we use the selection theorem 1 of VIII,6. Let 
{Fnk} be a subsequence converging to the possibly defective distribution F. 
By the first part of the theorem the transforms converge to the Laplace 
transform of F. It follows that F is the unique distribution with Laplace 
transform <p, and so all convergent subsequences converge to the same limit 
F. This implies the convergence of Fn to F. The last assertion of the theorem 
is clear by inspection of A.1). > 
For clarity of exposition we shall as far as possible reserve the letter F 
for probability distributions, but instead of A.1) we may consider more 
general integrals of the form 
A.5) (o{\) = f V*8 U{dx}. 
Jo 
where U is a measure attributing a finite mass ?/{/} to the finite interval /, 
but may attribute an infinite mass to the positive half axis. As usual, we 
describe this measure conveniently in terms of its improper distribution 
function defined by U(x) = U{0,x }. In the important special case where 
U is defined as the integral of a function u > 0 the integral A.5) reduces to 
oo 
A.6) co(X) = e~kx u(x) dx 
Jo 
Examples, (c) If u(x) = x* with a > -1, then o>(A) = r(a+l)/Aa+1 for 
all A > 0. 
(d) If u(x) = e"* then w(A) = lJ(X-a) for A > a > 0, but the integral 
A.6) diverges for A <? a. 
(e) If u(x) = e*2 the integral \jl.6) diverges everywhere. 
(/) By differentiation we get from A.1) 
A.7) -<P'Q) = f ™e~Xxx F{dx) 
Jo 
and this is an integral of the form A.5) with U{dx} =t x F{dx}. This example 
illustrates how integrals of the form A.5) arise naturally in connection with 
proper probability distributions. > 
432 LAPLACE TRANSFORMS. TAUBERIAN THEOREMS XIII. 1 
We shall be interested principally in measures U derived by simple 
operations from probability distributions, and the integral in A.5) will 
generally converge for all X > 0. However, nothing is gained by excluding 
measures for which convergence takes place only for some X. Now (o(a) < oo 
implies co(X) < oo for all X > a, and so the values of X for which the 
integral in A.5) converges fill an interval a, oo. 
Definition 2. Let U be a measure concentrated on 0, oo. If the integral 
in A.5) converges for X > a, then the function co defined for X > a is called 
the Laplace transform of U. 
If U has a density u, the Laplace transform A.6) of U is also called the 
ordinary Laplace transform of u. 
The last convention is introduced merely tor convenience. To be systematic one should 
consider more general integrals of the form 
f°° 
A.8) e-** v(x) U{dx) 
J 
Jo 
and call them "Laplace transform of v with respect to the measure (/." Then A.6) would 
be the "transform of u with respect to Lebesgue measure" (or ordinary length). This 
would have the theoretical advantage that one could consider functions u and v of variable 
signs. For the purposes of this book it is simplest and least confusing to associate Laplace 
transforms only with measures, and we shall do so.2 
If U is a measure such that the integral in A.5) converges for X = a, 
then for all X > 0 
A.9) co(X+a) = f VAx • e~ax U{dx) = f VAa: U# {dx} 
Jo Jo 
is the Laplace transform of the bounded measure U#{dx} — e~ax U{dx}, 
and ai{X+a)lco{a) is the transform of a probability distribution. In this 
way every theorem concerning transforms of probability distributions 
automatically generalizes to a wider class of measures. Because the graph 
of the new transform co(X+a) is obtained by translation of the graph of w 
we shall refer to this extremely useful method as the translation principle. 
For example, since U is uniquely determined by ?/#, and U# by (o{X+a) 
for X > 0, we can generalize theorem 1 as follows. 
Theorem la. A measure U is uniquely determined by the values of its Laplace 
transform A.5) in some interval a < X < oo. 
2 The terminology is not well established', and in the literature the term "Laplace trans- 
form of F" may refer either to A.1) or to B.6). We would describe B.6) as the "ordinary 
Laplace transform of the distribution function F," but texts treating principally such 
transforms would drop the determinative "ordinary." To avoid ambiguities in such cases 
the transform A.1) is then called the Laplace-Stieltjes transform. 
XIII. 1 DEFINITIONS. THE CONTINUITY THEOREM 433 
Corollary. A continuous function u is uniquely determined by the values 
of its ordinary Laplace transform A.6) in some interval a < X < co. 
Proof. The transform determines uniquely the integral U of u, and two 
distinct continuous3 functions cannot have identical integrals. > 
[An explicit formula for u in terms of co is given in VII,F.6).] 
The continuity theorem generalizes similarly to sequences of arbitrary 
measures U7I with Laplace transforms. The fact that Un has a Laplace 
transform impiies that Un{I} < co for finite intervals /. We recall from 
VIII, 1 and VII1.6 that a sequence of such .measures is said to converge to 
a measure U iff Un{/}—>• U{I) < oo for every finite interval of continuity 
oft/. 
Theorem 2z. {Extended continuity theorem.) For n = 1, 2, . . . let Un be 
a measure with Laplace transform con. If o>n(A) —*- co(A) for X > a, then co 
is the Laplace transform of a measure U and Un-> U. 
Conversely, if Un-+ U ad the sequence {ft>n(a)} is bounded, then 
o)n{X) -> oj(X) for I > a. 
Proof, {a) Assume that Un ~> U and that ojn(a) < A. If / > 0 is a 
point of continuity of U then 
A.10) f e~Uia)x Ujdx} -* \e-u+aux U{dx) 
Jo Jo 
and the left side differs from ion(X+a) by at most 
A.11) j 'e~Uia)x Un{dx) < Ae~Xt 
which can be made <e by choosing t suificiently large. This means that 
the upper and lower limits of o>„(?>+a) differ by less than an arbitrary e, 
and hence for every a > 0 the sequence {con(X-\-a)} converges to a finite 
limit. 
(b) Assume then that o>n(X) -*¦ oj(A) for ?. > a. For'fixed ?^> a the 
function oj.FI(X + X0);!o)n(X0) is the Laplace transform of the probability 
distribution U*{dx\ = (\j\<i,O.n))e~-}'<)X UJcU). By the continuity theorem 
therefore U',f conwrgei. to a possibly defective distributicn UM, and this 
implies that U„ converges to a measure U such that U{dx) = 
¦J#(dx). ^ > 
The following exaniple shows the necessity of the condition that {o>n(a)} 
remain bounded. 
:! The same argument shows that in general u is determined up to values on an arbitrary 
set of measure zero. 
434 LAPLACE TRANSFORMS. TAUBERIAN THEOREMS XIII.2 
Example, (g) Let Un attach weight en* to the point n, and zero to the 
complement. Since Un{0, n) = 0 we have Un -> 0, but con(X) ~ 
= eMn-X) -^ oo for all A > 0. > 
One speaks sometimes of the bilateral transform of a distribution F with two tails, 
namely 
A.12) 
J—be 
but this function need not exist for any. A ^ 0. If it exists, y(—X) is often called the 
moment generating function, but in reality it is the generating function of the sequence 
{t*n/nl} where fin is the /zth moment. 
2. ELEMENTARY PROPERTIES 
In this section we list the most frequently used properties of the Laplace 
transforms; the parallel to> generating functions is conspicuous. 
(i) Convolutions. Let F and G be probability distributions and U their 
convolution, that is, 
B.1) U{x) = \XG{x-y)F{dy}. 
Jo 
The corresponding Laplace transforms obey the multiplication rule 
B.2) co = <py. 
This is equivalent to the assertion that for independent random variables 
E(e~"A(X+Y)) = E(e~xx) E(e~XY), which is a special case of the multiplication 
rule for expectations.4 
If F and G have densities / and g, then U has a density u given by 
B.3) 
u(x) = (Xg(x- 
Jo 
and the multiplication rule B.2) applies to the "ordinary" Laplace trans- 
forms A.6) of /, g, and u. 
We now show that the multiplication rule can be extended as follows. 
Let F and G be arbitrary measures with Laplace transforms q> and y 
converging for X > 0. The convolution U has then a Laplace transform <x> 
given by B.2). This implies in particular that the multiplication rule applies 
to the "ordinary" transforms of any two integrable functions / and g and 
their convolution B.3). 
4 The converse is false: two variables may be dependent and yet such that the distribution 
of their sum is given by the convolution formula. [See II,4(e) and problem 1 of 111,9.] 
XIII.2 ELEMENTARY PROPERTIES 435 
To prove the assertion we introduce the finite measures Fn obtained by 
truncation of F as follows: for x <? n we put Fn(x) = F(x), but for 
x > n we let Fn(x) = F(n). Define Gn. similarly by truncating G. For 
x < n the convolution Un = Fn* Gn does not differ from U, and hence 
not only Fn~> F and Gn -> G, but also Un -+ U. For the corresponding 
Laplace transforms we have con = <pnyn and letting n —>¦ oo we get the 
assertion co = <py. 
Examples, (a) Gamma distributions. In example \{b) the familiar con- 
volution rule flx*fp = f^p is mirrored in the obvious relation cp^cpp = (p^+p. 
(b) Powers. To wa(-c) = xx~1/r(a) there corresponds the ordinary 
Laplace transform coa(A) = A~a. It follows that the convolution B.3) of 
wa and Up is given by ua+p. The preceding example follows from this by the 
translation principle since yJJ) = coa(^+l). 
(c) If a > 0 then e~aXco(X) is the Laplace transform of the measure with 
distribution function U(x—a). This is obvious from the definition, but may 
be considered also as a special case of the convolution theorem inasmuch as 
e~a* is the transform of the distribution concentrated at the point a. > 
(ii) Derivatives and moments. If F is a probability distribution and 
<p its Laplace transform A.1), then <p possesses derivatives of all orders given 
by 
B.4) (-1)" (p{nK*) = \e~Xxxn F{dx) 
Jo 
(as always, X > 0). The differentiation under the integral is permissible 
since the new integrand is bounded and continuous. 
It follows in particular that F possesses a finite nth moment iff a finite 
limit <p(n)@) exists. For a random variable X we can therefore write 
B.5) E(X) = -9/@), E(X2) = <A0) 
with the obvious conventions in case of divergence. The differentiation 
rule B.4) remains valid for arbitrary measures F. 
(iii) Integration by parts leads from A.1) to 
/•OO 
</.o; e r\x)ax— .. ^ ~. 
Jo / 
For probability distributions it is sometimes preferable to rewrite B.6) in 
terms of the tail 
f " 
Jo 
B.7) 
Jo 
This corresponds to formula 1; XI,A.6) for generating functions. 
436 LAPLACE TRANSFORMS. TAUBERIAN THEOREMS XIII.3 
(iv) Change of scale. From A.2) we have E(e~axX) = y{aX) for each 
fixed a > 0, and so y{aX) is the transform of the distribution F{dxja) [with 
distribution function F(xla)]. This relation is in constant use. 
Example, (d) Law of large numbers. Let Xlf X2, . . . be independent 
random variables with a common Laplace transform cp. Suppose E(X/) = /u. 
The Laplace transform of the sum Xx + • • • + Xn is <pn, and hence the 
transform of the average [Xx-|-- • -+XJ/n is given by 9?n(^/«). Near the 
origin <p{X) = 1 — pX + o{X) [see B.5)] and so as n —»- 00 
B.8) 
\nj \ n / 
But e~MX is the transform of the distribution concentrated at ju, and so 
the distribution of [Xx+- • +Xn]/« tends to this limit. This is the weak law 
of large numbers in the Khintchine version, which does not require the 
existence of a variance. True, the proof applies directly only to positive 
variables, but it illustrates the elegance of Laplace transform methods. > 
3. EXAMPLES 
(a) Uniform distribution. Let F stand for the uniform distribution 
concentrated on 0, 1. Its Laplace transform is given by <p(X) = {\—e~x)jX. 
Using the binomial expansion it is seen that the w-fold convolution Fn* has 
the transform 
„—A.k'i— n 
As X~n is the transform.corresponding to U{x) — xnjn! example 2(c) shows 
that e~n<k~n corresponds to {x—lc)\\n\ where x+ denotes the function 
that equals 0 for x <? 0 and x for x ^ 0. Thus 
C.2) Fn*(x) = i i(-l)*("V*-*)J.- 
n \ fc=o \k/ 
This formula was derived by direct calculation in 1,(9.5) and by a passage 
to the limit in problem 20 of 1; XI. 
(b) Stable distributions with exponent ?, The distribution function 
C.3) G(x) = 2[1 -9t(l/>/?)], . * > 0 
(where 9t is the standard normal distribution) has the Laplace transform 
C.4) y(Q = e-VT>. 
This can be verified by elementary calculations, but they are tedious and we 
XIII. 3 examples 437 
prefer to derive C.4) from the limit theorem 3 in 1; 111,7 in which the 
distribution G was first encountered. Consider a simple symmetric random 
walk (coin tossing), and denote by T the epoch of the first return to the 
origin. The cited limit theorem states that G is the limit distribution of the 
normalized sums 0^ +• •-+Tn)/«2, where Tj^, . . are independent 
random variables distributed like T. According to 1; XI,C.14) the 
generating function of T is given by f(s) = 1 — Vl-s2, and therefore 
C.5) y{X) = lim [1—VI -e~2X/n]n = lim 1 - 
L n J 
We have mentioned several times that G is a stable distribution, but 
again the direct computational verification is laborious. Now obviously 
yn{\) = y(n2X) which is the same as Gn*(x) = G(n~2x) and proves the 
stability without effort. 
(c) Power series and mixtures. Let F be a probability distribution with 
Laplace transform q?(A). We have repeatedly encountered distributions 
of the form 
00 
C.6) G 
fc=0 
where {pk} is a probability distribution. If P(s) = 2/V* stands for the 
generating function of {pk}, the Laplace transform of G is ob/iously given 
by 
C-7) 7(A)=I.Pfc/(A) = P(^(A)). 
This principle can be extended to arbitrary power series with positive coeffi- 
cients. We turn to specific applications. 
(d) BesseI function densities. In example II,7(c) we saw that for r = 1, 
2, ... the density 
C.8) vr(x) = e- - IT{x) 
x 
corresponds to a distribution of the form C.6) where F is exponential with 
<p(A) = 1/(A+1), and {pk} is the distribution of the first-passage epoch 
through the point /• > 0 in an ordinary symmetric random walk. The 
generating function of this distribution is 
C.9) P(s) = 
i-Vi- 
2\r 
S 1 
[seel; XI,C.6)]. Substituting s = A + A) we conclude that the ordinary 
Laplace transform of the probability density C.8) is given by 
C.10) [A+ 1 -V(A+lJ-l]r. 
438 LAPLACE TRANSFORMS. TAUBERIAN THEOREMS XIII.3 
That vr is a probability density and C.10) its transform has been proved 
only for r = 1, 2,. . . . However, the statement is true5 for all r > 0. It 
is of probabilistic interest because it implies the convolution formula 
vr *vs = vr+s and thus the infinite divisibility of vr. (See section 7.) 
(e) Another Bessel density. In C.6) choose for F the exponential distri- 
bution with <p(l) = 1/(^+1) and for {pk} the Poisson distribution with 
P(s) = e~t+ts. It is easy to calculate G explicitly, but fortunately this task 
was already accomplished in example II,7(a). We saw there that the density 
C.11) w,(x) = < , 
defined in 11,G.2) is the convolution of our distribution G with a gamma 
density /i>p+i. It follows that the ordinary Laplace transform of wp is 
the product of our y with the transform of /i>p+i, namely 
Accordingly, the probability density C.11) has the Laplace transform 
C.12) - 
J (A + 1 
) 
For / = 1 we see using the translation rule A.9) that \Jxp Ip{2\Jx) has the 
ordinary transform ^~p~1el/A. 
(/) Mixtures of exponential densities. Let the density / be of the form 
C.13) f{x) = ip^e-'**, pk>0, 
*=i jt=i 
where for definileness we assume 0 < at < • • • < an. The corresponding 
Laplace transform is given by 
C.14) p() %pk 
*=i A. + ak P(A) 
where P is a polynomial of degree n with roots — ak, and Q is a poly- 
nomial of degree n — 1. Conversely, for any polynomial Q of degree 
/* — 1 the ratio Q{X)jP{X) admits of a partial fraction expansion of the form 
C.14) with , 
[see 1; XI,D.5)]. For C.14) to correspond to a mixture C.13) it is necessary 
and sufficient that pr > 0 and that Q@)/P@) = 1. From the graph of P 
it is clear that P'(—ar) and P'{—ar+1) are of opposite signs, and hence 
the same must be true of Q(—ar) and Q{—ar+1). In other words, it is 
necessary that Q has a root — br between — ar and — ar+1. But as Q 
5 This result is due to H. Weber. The extremely difficult analytic proof is now replaced 
by an elementary proof in J. Soc. Industr, Appl. Math., vol. 14 A966) pp. a64--875. 
XIII.4 COMPLETELY MONOTONE FUNCTIONS 439 
cannot have more than n — 1 roots — bT we conclude that these must satisfy 
C.16) 0 < ax < bx < a2 < b2 < ¦ ¦ ¦ < bn_r < an. 
This guarantees that all pr are of the same sign, and we reach the following 
conclusion: Let P and Q be polynomials of degree n and n — 1, 
respectively, and 0(O)/P(O) = 1. In order that Q{X)IP{X) be the Laplace 
transform of a mixture C.13) of exponential densities it is necessary and 
sufficient that the roots —ar of P and —br of Q de distinct and {with proper 
numbering) satisfy C.16). >- 
4. COMPLETELY MONOTONE FUNCTIONS. 
INVERSION FORMULAS 
As we saw insVII,2 a function / in 0, V is a generating function of a 
positive sequence {fn} iff / is absolutely monotone, that is, if / possesses 
positive derivatives f{n) of all orders. An analogous theorem holds for 
Laplace transforms, except that now the derivatives alternate in sign. 
Definition 1. A function cp on 0, oo is completely monotone if it possesses 
derivatives <p(n) of all orders and 
D.1) (-l)Vn)W>0, *>0. 
As X —> 0 the values (p{n){X) approach finite or infinite limits which we 
denote by (p(n)@). Typical examples are l/X and 1/A + ^). 
The following beautiful theorem due to S. Bernstein A928) was the 
starting point of much research, and the proof has been simplified by siages. 
We are able to give an extreme ly simple proof because the spade work was 
laid by the characterization of generating functions derived in theorem 2 of 
VII,2 as a consequence of the law of large numbers. 
Theorem 1. A function <p on 0, oo is the Laplace transform of a prob- 
ability distribution F, iff it is completely monotone, and (p@) = 1. 
We shall prove a version of this theorem which appears more general in 
form, but can actually be derived from the restricted version by an appeal 
to the translation principle explained in connection with A.9). 
Theorem la. The function cp on 0, oo is completely monotone iff it is of 
the form 
D.2) r 0 
where F is nor a necessarily finite measure on 0, oo. 
440 LAPLACE TRANSFORMS. TAUBERIAN THEOREMS XI11.4 
(By our initial convention the interval of integration is closed: a possible 
atom of F at the origin has the effect that 97@0) > 0.) 
Proof. The necessity of the condition follows by formal differentiation 
as in B.4). Assuming 9? to be completely monotone consider (p{a—as) 
for fixed a > 0 and 0 < s < 1 as a function of s. Its derivatives are 
evidently positive and by theorem 2 of VII ,2 the Taylor expansion 
D.3) (p{a - as) = ; 
n=0 n\ 
is valid for 0 <; s < 1. Thus 
00 
D.4) (pa{X) = <p(a-ae ) = 
; 
n=o n! 
is the Laplace transform of an arithmetic measure attributing mass 
{—a)n<p(n){a)jn\ to the point n\a (where n = 0, 1,. ..). Now (pa{X)-+ (p{X) 
as a —*¦ 00. By the extended continuity theorem there exists therefore a 
measure F such that Fa-+ F and 9? is its Laplace transform. > 
We have not only proved theorem la, but the relation Fa-+ F may be 
restated in the form of the important 
Theorem 2. {Inversion formula) If (A.2) holds for X > 0, then at all points 
of continuity* 
D.5) F{x) = lim J 
a-* 00 n<a* K ! 
This formula is of great theoretical interest and permits various conclusions. 
The following boundedness criterion may serve as an example of particular 
interest for semi-group theory. (See problem 13.) 
Corollary. For <p to be of the form 
D.6) (p(A) = \e-Xxf(x)dx where 0<f<C 
Jo 
it is necessary and sufficient that 
(-a)n(p{n)(a) C 
D.7) 0 < y <, - 
n! a 
for all a > 0. 
6 The inversion formula D.5) was derived in VII,F.4) as a direct consequence of the law 
of large numbers. In VII,F.6) we have an analogous inversion formula for integrals of the 
form D.6) with continuous f (not necessarily positive). 
XIII.4 COMPLETELY MONOTONE FUNCTIONS 441 
Proof. Differentiating D.6) under the integral we get D.7) [see B.4)]. 
Conversely, D.7) implies that <p is completely monotone and hence the 
transform of a measure F. Substituting from D.7) into D.5) we conclude that 
F{x2) - F(Xl) <, C(xt-zJ 
for any pair xl < x2. This means that F has bounded difference ratios 
and hence F is the integral of a function /< C (see V,3). > 
Theorem 1 leads to simple tests that a given function is the Laplace trans- 
form of a probability distribution. The standard technique is illustrated by 
the proof of 
Criterion 1. If <p and \p are completely monotone so is their product yip. 
Proof. We show by induction that the derivatives of cpip alternate in 
sign. Assume that for every pair <p, y of completely monotone functions 
the first n derivatives of <pip alternate in sign. As —<p' and — y>' are com- 
pletely monotone the induction hypothesis applies to the products — (prip 
and —cptp', and we conclude from — {<py))' = — <p'y — (pip' that in fact the 
first n + 1 derivatives of (py> alternate in sign. Since the hypothesis is 
trivially true ior n = 1 the criterion is proved. > 
The same proof yields the useful 
Criterion 2. If <p is completely monotone and y> a positive function with a 
completely monotone derivative then <p{y>) is completely monotone. {In 
particular-, e~v is completely monotone.) 
Typical applications are given in section 6 and in the following example, 
which occurs frequently in the literature with unnecessary complications. 
Example, (a) An equation occurring in branching processes. Let <p be 
the Laplace transform of a probability distribution F with expectation 
0 < fi < oo, and let c > 0. We prove that the equation 
D.8) m = <p(A+c-cp(X)) 
has a unique root fi(X) < 1 and /? is the Laplace transform of a distribution 
B which is proper iff juc < 1, defective otherwise. 
(See XIV,4 for applications and references.) 
Proof. Consider the equation 
D.9) (p(X+c-cs) - s = 0 
for fixed X > 0 and 0 <; s <> 1. The left side is a convex function which 
assumes a negative value at s = 1 and a positive value at s = 0. It follows 
that there exists a unique root. 
442 LAPLACE TRANSFORMS. TAUBERIAN THEOREMS XIII.5 
To prove that the root /?(A) is a Laplace transform put ft = 0 and 
recursively ft+1 = <p(A+c—eft,). Then ft < ft < 1 and since 9? is 
decreasing this implies ft < ft < 1, and by induction ft < ft+1 < 1. 
The limit of the bounded monotone sequence {ft,} satisfies D.8) and hence 
/? = lim ft. Now ft(A) = cp(?.-\-c) is completely monotone and criterion 
2 shows recursively that ft, ft, ... are completely monotone. By the 
continuity theorem the same is true of the limit ft and hence /? is the 
Laplace transform of a measure B. Since ft A) < 1 for all A the total 
mass of B is /?@) < 1. It remains to decide under what conditions /?@) = 1. 
By construction 5 = /?@) is the smallest root of the equation 
D.10) <p(c-cs) - s = 0. 
Considered as a function of s the left side is convex; it is positive for s = 0 
and vanishes for s = 1. A second root s < 1 exists therefore iff at s = 1 
the derivative is positive, that is iff — cg/@) > 1. Otherwise /?@) = 1 and 
/? is the Laplace transform of a proper probability distribution B. Hence 
B is proper iff — c<p'@) = cju < 1. >- 
5. TAUBERIAN THEOREMS 
Let U be a measure concentrated on 0, 00 and such that its Laplace 
transform 
E.1) co(A) - TV"* U{dx} 
Jo 
exists for A > 0. It will be convenient to describe the measure U in terms 
of its improper distribution function defined for x > 0 by U{0, x}. We 
shall see that under fairly general conditions the behavior of a> near the 
origin uniquely determines the asymptotic behavior of U(x) as x —> oo and 
vice versa. Historically any relation describing the asymptotic behavior of 
U in terms of a> is called a Tauberian theorem, whereas theorems describing 
the behavior of a> in terms of U are usually called Abelian. We shall make 
no distinction between these two classes because our relations will be 
symmetric. 
To avoid unsightly formulas involving reciprocals we introduce two 
positive variables / and r related by 
E.2) tr = 1. 
Then r -> 0 when / -> oo. 
To understand the background of the Tauberian theorems note that 
for fixed / the change of variables x = ty in E.1) shows that oj{tX) is the 
Laplace transform corresponding to the improper distribution function 
XIII.5 TAUBERIAN THEOREMS 443 
U{ty). Since co decreases it is possible to find a sequence ru r2,.. . -+o 
such that as r runs through it 
E.3) S^-yW 
with y(X) finite at least for X > 1. By the extended continuity theorem the 
limit y is the Laplace transform of a measure G and as / runs through the 
reciprocals tk = l/rfc 
E.4) 
at all points of continuity of G. For x = 1 it is seen that the asymptotic 
behavior of U(t) as / —»> oo is intimately connected with the behavior of 
In principle we could formulate this fact as an all-embracing Tauberian 
theorem, but it would be too clumsy for practical use. To achieve reasonable 
simplicity we consider only the case where E.3) is valid for any approach 
t —>- 0, that is, when co varies regularly at 0. The elementary lemma7 1 of 
VII 1,8 states that the limit y is necessarily of the form y{X) = X~p with 
p > 0. The corresponding measure is given by G(x) — xpjT{p+\), and 
E.4) implies that U varies regularly and the exponents of co and U are 
the same in absolute value. We formulate this important result together 
with its converse in 
Theorem 1. Let U be a measure with a Laplace transform a> defined for 
X > 0. Then each of the relations 
E.5) 
o>(t) X" 
and 
E.6) Hau,,, 
implies the other as well as 
E.7) to{T)~U{t)T{P+\). 
Proof, (a) Assume E.5). The left side is the Laplace transform corre- 
sponding to U(tx)/(o(T), and by the extended continuity theorem this implies 
E.8) 
co(r) 
For i=l we get E.7), and substituting this back into E.8) we get E.6). 
7 This lemma is used only to justify the otherwise artificial form of the relations E.5) and 
E.6). The theory of regular variation is not used in this section [except for the side remark 
that E.18) implies E.16)]. 
444 LAPLACE TRANSFORMS. TAUBERIAN THEOREMS XIII.5 
(b) Assume E.6). Taking Laplace transforms we get 
E9) w(T^IVfl) 
U(t) ~* Xp 
provided the extended continuity theorem is applicable, that is, provided 
the left-side remains bounded for some X. As under (a) it is seen that E.9) 
implies E.7) and E.5), and to prove the theorem it suffices to verify that 
co{r)jU{t) remains bounded. 
On partitioning the domain of integration by the points t, It, 4/, ... 
it is clear that 
-«-l 
E.10) w(t)<2<T2 UBnt). 
0 
In view of E.7) there exists a t0 such that UBt) < 2P+1 U(t) for / > t0. 
Repeated application of this inequality yields 
(Z 11\ ^il2 ^ V ?w(p+l) -2" 
C/@ ~ o 
and so the left side indeed remains bounded as i —*- oo, > 
Examples, (a) U(x) ~ log2 x as a; -> oo iff w(^) '^ log2 X as ^ —> 0. 
Similarly t/(a;) ~ yjx iff co(^) ^^ \\j-n\X. 
(b) Let F be a probability distribution with Laplace transform <p. The 
measure U{dx} = x F{dx) has the transform —<p. Hence if —<p'(X) ~ ftX~<) 
as X —*¦ oo then 
l/(a;) = y F{dy} ~ ^ **] <x ^P, * -* °o 
and vice versa. This generalizes the differentiation rule B.4) which is con- 
tained in E.7) for p = 0. > 
It is sometimes useful to know to what extent the theorem remains valid 
in the limit p -> oo. We state the result in the form of a 
Corollary. If for some a > 1 as t —*¦ oo 
E12) eilher <™}^0 or 
«.(t) 1/@ 
then 
E.13) f 
Proof. The first relation in E.12) implies that a>(rX)lio(r) -> 0 for X > a 
and by the extended continuity theorem U(tx)la>(T) -*> 0 for all x > 0. The 
XHI-5 TAUBERIAN THEOREMS 445 
second relation in E.12) entails E.13) because 
Jo 
at 
e~x/t U{dx} > e~a U(ta). 
Jn applications it is more convenient to express theorem 1 in terms of slow 
variation. We recall that a positive function L defined on 0, co varies 
slowJy at co if for every fixed x 
^>-.. 
? 
L varies slowly at 0 if this relation holds as / —*¦ 0, that is, if L(l/x) varies 
slowly at oo. Evidently U satisfies E.6) iff U(x)/xf> varies slowly at oo 
and similarly E.5) holds iff Xpco{X) varies slowly at 0. Consequently 
theorem 1 may be rephrased as follows. 
Theorem 2. If L is slowly varying at infinity and 0 < p < oo, then each 
of the relations 
E.15) o>(t)~t-"lAV t —0, 
and 
E.16) U(t)~—- tpL(t), t-*ao 
r(p+l) W 
implies the other. 
Theorem 2 has a glorious history. The implication E.16) -* E.15) (from the measure 
to the transform) is called.an Abelian theorem; the converse E.15) ->¦ E.16) (from trans- 
form to measure), a Tauberian theorem. In the usual setup, the two theorems are entirely 
separated, the Tauberian part causing the trouble. In a famous paper G. H. Hardy and 
J. E. Littlewood treated the case eo(A) ~ X~p by difficult calculations. In 1930, J. Karamata 
created a sensation by a simplified proof for this special case. (This proof is still found in 
texts on complex variables and Laplace transforms.) Soon afterwards he introduced the 
class of regularly varying functions and proved theorem 2; the.proof was too complicated 
for textbooks, however. The notion of slow variation was introduced by R. Schmidt about 
1925 in the same connection. Our proof simplifies and unifies the theory and leads to the 
little-known, but useful, corollary. 
A great advantage of our proof is that it applies without change when the 
roles of infinity and zero are interchanged, that is, if t-*- oo while /—>-0. 
In this way we get the dual theorem connecting the behavior of co at infinity 
with that of U at the origin. [It will not be used in this book except to 
derive F.2).] 
Theorem 3. The last two theorems and the corollary remain valid when 
the roles of the origin and infinity are interchanged, that is, for r —> oo and 
446 LAPLACE TRANSFORMS. TAUBERIAN THEOREMS XIII. 5 
Theorem 2 represents the main result of this section, but for completeness 
we derive two useful complements. First of all, when U has a density 
U' = u it is desirable to obtain estimates ^br u. This problem cannot be 
treated in full generality, because a well-behaved distribution U can have an 
extremely ill-behaved density u. In most applications, however, the density 
u will be ultimately monotone, that is, monotone in some interval x0, oo. 
For such densities we have 
Theorem 4.8 Let 0 < p < oo. If U has an ultimately monotone derivative 
u then as X —*¦ 0 and x—* oo, respectively, 
E.17) 
^-d-\ iff 
(For a formally stronger version see problem 16.) 
Proof. The assertion is an immediate consequence of theorem 2 and the 
following 
Lemma. Suppose that U has an ultimately monotone density u. If 
E.16) holds with p > 0 then 
E.18) u(x) ~ pU(x)/x, x -> oo. 
[Conversely, E.18) implies E.16) even if u is not monotone. This is 
contained in VIII,(9.6) with Z = u and p = 0.] 
Proof. For 0 < a < b 
E 19) U{tb) ~ U(ta) = C U-^ dv 
1/@ Ja 1/@ 
As t —*- od the left side tends to bp — ap. For sufficiently large t the inte- 
grand is monotone, and then E.16) implies that it remains bounded as 
t —*- oo. By the selection theorem of VIII,6 there exists therefore a sequence 
hi h> .••—*• °° such that as t runs through it 
E.20) 
at all points of continuity. It follows that the integral of y> over a, b equals 
bp — cf, and so y>(y) = pyp~x. This limit being independent of the sequence 
{tk} the relation E.20) is true for an arbitrary approach t—** oo, and for 
y = 1 it reduces to E.18). > 
8 This includes the famous Tauberian theorem of E. Landau. Our proof serves as a 
new example of how the selection theorem obviates analytical intricacies. 
XIII. 5 TAUBERIAN THEOREMS 447 
Example, (c) For a probability distribution F with characteristic 
function q> we have [see B.7)] 
E.21) f VA*[1 - F{x)) dx = [\- 
Jo 
Since 1 — F is monotone each of the relations 
E.22) 1 - <p(X) ~ P-'HI/X) and 1 - F(x) 
(p > 0) implies the other. The next section will illustrate the usefulness of 
this observation. ^ 
The use of this theorem is illustrated in the next section. In conclusion 
we show how theorem 2 leads to a Tauberian theorem for power series. 
[It is used in XII,(8.10) and in XVII,5.] 
Theorem 5. Let qn>0 and suppose that 
E-23) fiE) = f qnsn 
n=0 
converges for 0 <, s < 1. If L varies slowly at infinity and 0 < p < oo 
then each of the two relations 
E.24) e(^-X_L(-J_), 
and 
E.25) q0 + qi + • • • + ^n_x -^ -——- n" L{n\ n-+co 
r(+i) 
implies the other. 
Furthermore, if the sequence {qn} is monotonic and 0 < p < oo, 
E.24) w equivalent to 
E.26) <?„ - ^7- n"-1 L(n), n - 00. 
U) 
Proof. Let C/ be the measure with density u defined by 
E.27) u(x) = qn for n < x < n + 1. 
The left side in E.25) equals U(n). The Laplace transform co of ?/ is 
given by 
E.28) cu(A) = l^f^ | ' {^ > 
f | *„* = ^ Q{e). 
A. n=0 /I 
It is thus seen that the relations E.24) and E.25) are equivalent to E.15) 
and E.16), respectively, a.xl hence they imply each other by virtue of theorem 
2. Similarly, E.26) is an immediate consequence of theorem 4. > 
448 LAPLACE TRANSFORMS. TAUBERIAN THEOREMS XIII.6 
Example. {d) Let qn = np~l log" n where p > 0 and a is arbitrary. The 
sequence {qri} is ultimately monotone and so E.24) holds with L(t) = 
= V(p) log" /. > 
*6. STABLE DISTRIBUTIONS 
To show the usefulness of the Tauberian theorems we now derive the most 
general stable distributions concentrated on 0, oo and give a complete 
characterization of their domains of attraction. The proofs are straight- 
forward and of remarkable simplicity when- compared with the methods 
required for distributions not concentrated on 0, oo. 
Theorem I. For fixed 0 < a < 1 the function ya(A) = e~f is the Laplace 
transform of a distribution Ga with the following properties: 
Ga is stable; more precisely, if Xl5 . . . , Xn are independent variables with 
the distribution Ga, then (X,-f-- • • + Xn)///l/a has again the distribution Ga. 
F.1) x*[] - Ga(x)] -> —1— , x -* co, 
1A-a) 
F.2) e*~*Ga{x) -> 0, x-+0. 
Proof. The function ya is completely monotone by the second criterion 
of section 4, because e~x is completely monotone and Xa has a completely 
monotone derivative. Since ya{0) = 1, the measure Ga with Laplace 
transform ya has total mass 1. The asserted stability property is obvious 
since >•?(/*) = ya(nllaX). 
F.1) is a special case of E.22), and F.2) is an immediate consequence of 
theorem 3 and the corollary to theorem 1 of the preceding section. > 
Theorem 2. Suppose that F is a probability distribution concentrated on 
0, oo such that 
F.3) Fn*(anx) — G{x) 
{at points of continuity) wherry G is a proper distribution not concentrated at 
a single point. Then 
(a) There exists a function L that varies slowly at infinity9 and a constant 
cf. with 0 < a < 1 such that 
F.4) 1 - F{x) ~ ^^ z - oo. 
* Except for F.2) the results of this section are derived independently in chapters IX 
and XVII. Stable distributions were introduced in VI,1. 
9 That is. L satisfies E,14). The norming factor r(l—a) in F.4) is a matter of con- 
venience and affects only notations 
XIII.7 INFINITELY DIVISIBLE DISTRIBUTIONS 449 
(b) Conversely, if F is of the form F.4) // is possible to choose an such 
that 
F.5) 
and in this case F.3) holds with /7 = Gz. 
This implies that the possible limits G in F.3) differ only by scale factors 
from some Ga. It ft* lows, in particular, that there are no other stable dis- 
I 
I 
tributions concentrated on 0, go. (See lemma 1 of VIII,2.) 
Proof. If cp and y are the Laplace transforms of F and G, then F.3) is 
equivalent to 
F.6) — n log <p(X/an) -*¦ —log y(X). 
By the simple theorem of VIII,8 this implies that —log <p varies regularly 
at the origin, that is 
F.7) 
with L varying slowly at infinity and a > 0. From F.6) then — log 
= CX7. Since G is not concentrated at a single point we have 0 < a < 1. 
Now F.7) implies 
F.8) 
In view of E.22) the two relations F.4) and F.8) imply each other. 
Accordingly, F.4) is necessary for F.1) to hold. 
For the converse part we start from F.4) which was just shown to imply 
F.8). For fixed n define ari as the lower bound of all x such that 
n[\ — F(x)] 1/FA — a). Then F.5) holds. Using this and the slow variation 
of L we conclude from F.8) that 
F.9) 1 - <p(Alan) - X'a?L@ 
It follows that the left side in F.6) tends to ?*, and this concludes the proof. > 
(See problem 26 for the influence of the maximal term.) 
*7. INFINITELY DIVISIBLE DISTRIBUTIONS 
According to the definition in VI,3 a probability distribution U with 
Laplace transform a» is infinitely divisible iff for n =-1, 2, . . . the positive 
«th root o>n = (»1/n is the Laplace transform of a probability distribution. 
Not used in the sequel. 
450 LAPLACE TRANSFORMS. TAUBERIAN THEOREMS XIII.7 
Theorem 1. The function io is the Laplace transform of an infinitely 
divisible probability distribution iff (» = e ~v> where y> has a completely 
monotone derivative and y>@) = 0. 
Proof. Using the criterion 2 of section 4, it is seen that when y>@) = 0 
and y>' is completely monotone then <wn = e~vln is the Laplace transform 
of a probability distribution. The condition is therefore sufficient. 
To prove the necessity of the condition assume that con = e~v/ri is, for 
each n, the Laplace transform of a probability distribution and put 
G.1) V>ntt) = n[\ -a>na)l 
Then ipn -> xp and the derivative y>'n = —nco'n is completely monotone. 
By the mean value theorem rpn{?.) = hp'n(Q\) > hp'J}), and since yn-+f 
this implies that the sequence {y)'n(X)} is bounded for each fixed X > 0. 
It is therefore possible to find a convergent subsequence, and the limit is 
automatically completely monotone by the extended continuity theorem. 
Thus y> is an integral of a completely monotone function, and this completes 
the proof. > 
An alternative form of this theorem is as follows. 
Theorem 2. The function co is the Laplace transform of an infinitely 
divisible distribution iff it is of the form co = e~w where 
J*00 i _ p~x* 
— P{dx) 
0 X 
and P is a measure such that 
G.3) x-1 P{dx] < oo. 
Proof. In view of the representation theorem for completely monotone 
functions .the conditions of theorem 1 may be restated to the effect that 
we must have ^@) = 0 and 
G.4) V'(*) = [ne-XxP{dx) 
Jo 
where P is a measure. Truncating the integral at a changes the equality 
sign into >, and this implies that 
e—T>'ix} 
0 X 
for each a > 0 (the integrand being bounded). It follows that G.2) makes 
sense and the condition G.3) is satisfied. Formal differentiation now shows 
that G.2) represents the integral of G.4) vanishing at zero. > 
[See problems 17-23 and example 9(a).] 
XIII.7 INFINITELY DIVISIBLE DISTRIBUTIONS 451 
Examples, (a) The compound Poisson distribution 
G.6) u = e-ef-Fn* 
0 nl 
has the Laplace transform e~e+e9 and G.2) is true with P{dx} = ex F{dx). 
{b) The gamma density xa~x e-x\T{a) has transform to(X) = 1/(A + 1)°. 
Here 
f °° 1 - e~Xx 
G.7) ip(X) = a e x dx 
Jo x 
because y'{X) = 
(c) Stable distributions. For the transform w(A) = e~x<* of section 6 we 
have w{X) = A* and 
-e~Xx 
G.8) Xa = —-— r — dx 
V r(l-a)Jo x*+x 
as is again seen by differentiation. 
(d) Bessel functions. Consider the density vr of example 3(d) with 
Laplace transform C.10). It is obvious from the form of the latter that vr 
is the «-fold convolution of vrjn with itself, and hence infinitely divisible. 
Formal differentiation shows that in this case y)'{k) = r/V(A+lJ — 1 and 
it is easily shown (see problem 6) that this y>' is of the form G.4) with 
P{dx} = re~x I0(x) dx. 
(e) Subordination. It is easily seen from the criteria in section 4 that 
if yvand y>2 are positive functions with completely monotone derivatives, 
the composite function ip{X) = y)x(y>2(fy) has the same property. The 
corresponding infinitely divisible distribution is of special interest. To 
find it, denote by Q\l) the probability distribution with Laplace transform 
e-tviW (where /=1,2), and put 
t{x) = r 
Jo 
G.9) U 
(The distribution Ut is thus obtained by randomization of the parameter 
s in Q[2).) The Laplace transform of Ut is 
G.10) cot(X) = 
Readers of X,7 will recognize in G.9) the subordination of processes: 
Ut is subordinated to Q\2) by the directing process Q\l). It is seen with 
what e^se we get the Laplace transforms of the new process although only 
for the special case that the distributions Q\2) are concentrated on the 
positive half-axis. 
452 LAPLACE TRANSFORMS. TAUBERIAN THEOREMS XIII.8 
A special case deserves attention: if v'i(A) = Aa and y>2(A) = Xp then 
xp(A) = /."P. Thus a stable c/.-process directed by a stable ^-process leads to 
a stable ^-process. Readers should verify that this statement in substance 
repeats the assertion of problem 10. For a more general proposition see 
example VI,".G?). 
(/) Every mixture of exponential distributions is infinitely divisible.10 The 
most general such distribution has a density of the form 
G.11) f(x) = 
Jo 
where U is a probability distribution. In the special case where U is 
concentrated on finitely many points 0 < ax < • • • < an it was shown in 
example 3(/) that the Laplace transform is of the form 
G.12) (p(X) = C • X + bl • • • A + bn~1 l-— 
A + ax A + an_r X + an 
with ak < bk < ak+1. Now 
A + bk 1 1 bk — ak 
_ L 
dl 
is the product of two completely monotone functions, and therefore itself 
completely monotone. It follows that each factor in G.12) is infinitely 
divisible and therefore the same is true of (p. For general mixtures the 
assertion follows by a simple passage to the limit (see problems 20-23). > 
*8. HIGHER DIMENSIONS 
The generalization to higher dimensions is obvious: not even the definition 
A.1) requires a change if x is interpreted as column matrix (xx, . . . , xn) 
and A as row matrix (Al5 . . . , An). Then 
Xx = Xxxx + • • • + Xnx 
nxn 
is the inner product of A and x. Within probability theory the use of 
multidimensional transforms is comparatively restricted. 
Examples, (a) Resolvent equation. Let / be a continuous function in one 
dimension with ordinary Laplace transform <p(A). Consider the function 
10 This surprising observation is due to F. W. Steutel, Ann. Math. Statist., vol. 40 A969), 
pp. 1130-1131 and vol. 38 A967), pp. 1303-1305. 
* Not used in the sequel. 
XIII.8 HIGHER DIMENSIONS 453 
f(s+t) of the two variables s, t. Its +wo-dimensional transform is given by 
(8.1) o)(k,v) = \ e~Xs-vtf{s+t)dsdt. 
After the change of variables s + t = x and — s + / = y the integral 
reduces to 
A — v 
(8.2) w(A, v) = - 
l-v 
We shall encounter this relation in more dignified surroundings as the basic 
resolvent equation for semi-groups [see A0.5) and the concluding remarks 
to section 10]. 
(b) Mittag-Leffler functions. This example illustrates the use of higher 
dimensions as a technical tool for evaluating simple transforms. We shall 
prove the following proposition: 
If F is. stable with Laplace transform e~x<x, the distribution 
(8.3) Gt(x) = 1 - F{ttf>*)y x > 0, 
(t fixed) has as Laplace transform the Mittag-Leffler function 
(8.4) f {~X) t 
= 1X1+fox) 
k* 
This result is of considerable interest because in various limit theorems the 
distribution G appears in company with F [see, for example, XI,E.6)]. A 
direct calculation seems difficult, but it is easy to proceed as follows. First 
keep x fixed and take t as variable. The ordinary Laplace transform yt{v) 
(with v as variable) of Gt(x) is obviously A —e~v<Xx)lv. Except for the 
norming factor v this is a distribution function in x, and its Laplace 
transform is evidently 
(8.5) 
v* 
This, then, is the bivariate transform of (8.3). In theory it could have been 
calculated by taking first the transform with respect to x, then t, and so 
(8.5) is the transform with respect to t of the transform which we seek. 
But expanding (8.5) into a geometric series one sees that (8.5) is in fact the 
transform of (8.4) and thus the proposition is proved. 
454 LAPLACE TRANSFORMS. TAUBERIAN THEOREMS XIII.9 
The Mittag-Leffler function (8.4) is a generalization of the exponential to 
which it reduces when a = 1. »> 
9. LAPLACE TRANSFORMS FOR SEMI-GROUPS 
The notion of Laplace integrals can be generalized to abstract-valued 
functions and integrals,11 but we shall consider only Laplace transforms of 
semi-groups of transformations associated with Markov processes.12 We 
return to the basic conventions and notations of X,8. 
Let H be a space (for example, the line, an interval, or the integers), and 
i? a Banach space ofbounded functions on it with the norm \\u\\ = sup \u(z)\. 
It will be assumed that if u e 1? then also |w| e &. Let {?l@> t > 0} be a 
continuous semi-group of contractions on =??. In other words we assume 
that for uey there exists a function &(t)u e S? and that ?}(/) has 
the following properties: 0 < u < 1 implies 0 < &(t)u < 1; furthermore 
Q(t+s) = Q(/)Q(i), and Q(/?)-> Q@) = 1, the identity operator.13 
We begin by defining integration. Given an arbitrary probability distri- 
bution F on 0, oo we want to define a contraction operator E from =?? 
to =?\ to be denoted by 
(9.1) E = r&(s) F{ ds}, 
Jo 
such that 
= r°°Q(/+s) F{ds}. 
Jo 
(9.2) 
(The dependence of E on the distribution F should be kept in mind.) 
For a semi-group associated with a Markov process with transition 
probabilities Qt{x, T) this operator E will be induced by the stochastic or 
substochastic kernel 
(9.3) 
f"&(*, H F{ds}. 
Jo 
A natural (almost trivial) definition of the operator E presents itself if 
F is atomic and a simple limiting procedure leads to the desired definition as 
follows. 
11 A fruitful theory covering transforms of the form (9.6) was developed by S. Bochner, 
Completely monotone functions in partially ordered spaces, Duke Math. J., vol. 9 A942) 
519-526. For a generalization permitting an arbitrary family of operators see the book by 
E. Hille and R. S. Phillips A957). 
12 The construction of the minimal solution in XIV,7 may serve as a typical example 
for the present methods. 
13 Recall from X,8 that strong convergence Tn-*T of endomorphisms means 
|| 7> — Tu\\ -*0 for all // G ?. Our "continuity" is an abbreviation for "strong 
continuity for t > 0." 
XIII9 LAPLACE TRANSFORMS FOR SEMI-GROUPS 455 
Let pj > 0 and px + • • • + pr = 1. The linear combination 
(9-4) E = PiZl{Q + • • > + pr{&tT) 
is again a contraction and may be interpreted as the expectation of 
with respect to the probability distribution attaching weight />, to tj. 
This defines (9.1) for the special case of finite discrete distributions, and 
(9.2) is true. The general expectation (9.1) is defined by a passage to the 
limit just as a Riemann integral: partition 0, oo into intervals Ix,. . . , /„, 
choose tjElj, and form the Riemann sum ^ &(tk) F{Ik} which is a con- 
traction. In view of the uniform continuity property X,(8.7) the familiar 
convergence proof works without change. This defines (9.1) as a special 
case of a Bochner integral. 
If the semi-group consists of transition operators, that is, if Q(t)l = 1 
for all t, then ?1 = 1. The notation (9.1) will be used for E, and for the 
function Ew we shall use the usual symbol 
= r&(s)w • F{ds} 
Jo 
(9.5) Ew 
(although it would be logically more consistent, to write vv outside the 
integral). The value Ew{x) at a given point x is the ordinary expectation 
with respect to F of the numerical function ?}(•$) w(x). 
In the special case F{ds} = e~x* ds the operator E is called the Laplace 
integral of the semi-group, or resolvent. It will be denoted by 
(9.6) 9?(A) = e~Xi Q(s) ds, A > 0. 
J 
In view of (9.2) the- resolvent operators *K(A) commute with the operators 
?}(/) of the semi-group. In order that A9?(AI = 1 it is necessary and 
sufficient that Q@1 = 1 for all t, and thus the contraction A9?(A) is a 
transition operator iff all &(s) are transition operators. 
Lemma. The knowledge of 9?(A)w for all X > 0 and w e J? uniquely 
determines the semi-group. 
Proof. The value 
= f 
Jo 
w(x) = f V"Q@ w(x) ¦ dt 
Jo 
at a given point x is the ordinary Laplac transform of the numerical function 
of t defined by ?)(/) w{x). This function being continuous, it is uniquely 
determined by its Laplace transform (see the corollary in section 1). Thus 
is uniquely determined for all t and all weSf. > 
456 LAPLACE TRANSFORMS. TAUBERJAN THEOREMS XIII.9 
The Laplace transform (9.6) leads to a simple characterization of the 
infinitesimal generator % of the semi-group. By the definition of this 
operator in X,10 we have 
(9.7, 
if S2tw exists (that is, if the norm of the difference of the two sides tends to 
zero). 
Theorem 1. For fixed X > 0 
(9.8) u = 9?(A)w 
iff u is in the domain of % and 
(9.9) \u - %u = w. 
Proof, (i) Define u by (9.8). Referring to the property (9.2) of expec- 
tations we have 
ne/j) — 1 if00, i r°° , 
(9.10) ^^ u = - \ e-Xs&(s+h)w • ds-- e~*s&(s)w • ds. 
h h Jo h Jo 
The change of variable s + h = t in the first integral reduces this to 
(9.11) ' ^ ~ u = -—:z- e~u?i(t)w • dt ^""CKOw • dt 
h h Jo h Jo 
PXh i i Ch 
= (u-X^w) - - ^-'X&iOw-w) dt. 
h h Jo 
Since ||?}(f)w—w\\ —> 0 as t—*-0 the second term on the right tends 
in norm to 0, and the whol,e right side therefore tends to X{u—X~xw). 
Thus (9.9) is true. 
(ii) Conversely, assume that %u exists, that is, (9.7) holds. Since 
is a contraction commuting with the semi-group, (9.7) implies 
(9.12) 
But we have just seen that the left side tends to A9?(A)« — u, and the 
resulting identity exhibits u as the Laplace transform of the function w in 
(9.9). > 
Corollary 1. Forgiven \veJ? there exists exactly one solution u of (9.9). 
Corollary 2. Two distinct semi-groups cannot have the same generator %. 
XIH.9 LAPLACE TRANSFORMS FOR SEMI-GROUPS 457 
Proof. The knowledge of the generator ?} permits us to find the Laplace 
transform 9?(A)w for all w e ^ and by the above lemma this uniquely 
determines all operators of the semi-group. > 
It is tempting to derive Tauberian theorems analogous to those of section 
5, but we shall be satisfied with the rather primitive 
Theorem 2. As A —»¦ oo 
(9.13) A9?(A)->1. 
Proof. For arbitrary weif we have 
(9.14) ||A<R(A)w - w\\ < r 
Jo 
- w\\ • Xe~>Adt. 
.As A —> oo the probability distribution with density Xe~Xi tends to the 
distribution concentrated at the origin. The integrand is bounded and tends 
to 0 as t-+0, and so the integral tends to 0 and (9.13) is true. *¦ 
Corollary 3. The generator % has a domain which is dense in ??. 
Proof. It follows from (9.13) that every weif is the strong limit of a 
sequence of elements A9?(A)w, and by theorem 1 these elements are in the 
domain of %. ^ 
Examples, (a) Infinitely divisible semi-groups. Let U be the infinitely 
divisible distribution with Laplace transform w = e~v described in G.2). 
The distributions Ut with Laplace transforms 
(9.15) f V** Ut{dx) = e~tvU) = exp l-t f °° )—±H P{dx}\ 
Jo ¦ \ Jo x i 
are again infinitely divisible, and the associated convolution operators 11@ 
form a semi-group. To find its generator14 choose a bounded continuously 
differentiable function v. Then clearly 
(,.,«) mtzA <x) _, f • *--» - «»). i y Ut{dy}. 
t Jo y t 
Differentiation of (9.15) shows that the measure t~xyUt{dy) has the 
transform ip'{X)e-ttpa) which tends to y'{X) as t -*- 0. But y>' is the trans- 
form of the measure P, and so our measures tend to P. Since the fraction 
under the last integral is (for fixed x) a bounded continuous function of y, 
14 This derivation is given for purposes of illustration. The generator is already known 
from chapter IX and can be obtained by a passage to the limit from compound Poisson 
distributions. 
458 LAPLACE TRANSFORMS. TAUBERIAN THEOREMS XIII. 10 
we get 
(9.17) «,<-) = f" <X~V) ~ "(X) P{dy) 
Jo y 
and have thus an interpretation of the measure P in the canonical repre- 
sentation of infinitely divisible distributions. 
(b) Subordinated semi-groups. Let {?}(*)} stand foran arbitrary Markovian 
semi-group, and let Ut be the infinitely divisible distribution of the preceding 
example. As explained in X,7 a new Markovian semigroup {?}*@} maybe 
obtained by randomization of the parameter t. In the present notation 
(9.18) Q*@ = f °°QE) Ut{ds}. 
Jo 
Putting for abbreviation 
(9.19) V(s, x) = QE) "' * v(x) 
s 
we have 
(9.20) . =-^ -»(*)= V(s,x)--sUt{ds}. 
t Jo t 
For a function v in the domain of % and for x fixed the function V is 
continuous everywhere including the origin since V(s, x) —>¦ %v(x) as 
5 -»> 0. We saw in the last example that t^s Ut{ds} -+ P{ds} if t -+ 0. Thus 
the right side in (9.20) tends to a limit and hence %*v exists and is given by 
(9.21) %*v(x) = (K K(s, x) P{JS}. 
Jo 
The conclusion is that the domains of 51 and %* coincide, and 
(9.22) «• = f" QE) ~ 1 P{^5} 
Jo s 
in the sense that (9.21) holds for v in the domain of %. > 
10. THE HILLE-YOSIDA THEOREM 
The famous and exceedingly useful Hille-Yosida theorem characterizes 
generators of arbitrary semi-groups of transformations, but we shall 
specialize it to our contraction semi-groups. The theorem asserts that the 
properties of generators found in the last section represent not only necessary 
but also sufficient conditions. 
Theorem 1. (Hille- Yosida.) An operator % with domain ???' c jgf is the 
generator of a continuous semi-group of contractions Q@ on 3? (with 
?}@) = 1) iff it has the following properties. 
XIII. 10 THE HILLE-YOSIDA THEOREM 459 
(i) The equation 
A0.1) Xu-%u = w, A>0, 
has for each w e JO? exactly one solution u; 
(ii) if 0 < w <, 1 then 0 <, Xu <, 1; 
(iii) //?<? domain 3" of % is dense in S?. 
We know already that every generator possesses these properties, and 
so the conditions are necessary. Furthermore* if the solution u is denoted 
by « = 9?(A)w, we know that 9?(A) coincides with the Laplace transform 
(9.6). Accordingly the conditions of the theorem may be restated as follows. 
(i') The operator 9?(A) satisfies the identity 
A0.2) A9?(A) - 519?(A) = 1. 
The domain of 9?(A) is =??; the range coincides with the domain =??' of % 
(ii') The operator A9?(A) is a contraction. 
(iii') The range of 9?(A) is dense in =??. 
From theorem 2 in section 9 we know that 9?(A) must satisfy the further 
condition 
A0.3) A9t(A)-*1, A -*oo. 
This implies that every u is the limit of its own transforms and hence that 
the range 3" of 9? (A) is dense. It follows that A0.3) can serve as replace- 
ment for (iii'), and thus the three conditions of the theorem are fully equivalent 
to the set (i'),(iK), A0.3). 
We now suppose that we are given a family of operators 9? (A) with these 
properties and proceed to construct the desired semi-group as the limit of a 
family of pseudo-Poisson semi-groups. The construction depends on 
Lemma 1. If w is in domain J?" of % then 
A0.4) ra(A)w = 9?(AJlw. 
The operators 9?(A) and 9?(v) commute and satisfy the resolvent equation 
A0.5) 9?(A) - 9?(v) = (v-A)9?(A)9?(v). 
Proof. Put v = %u. Since both u and w are in the domain 3" of 
% it follows from A0.1) that the same is true of v and 
Thus v = 9?(A)Ww which is the same as A0.4). 
Next, define z as the unique solution of vz — %z = w. Subtracting this 
from A0.1) we get after a trite rearrangement 
k(u-z) -%(u-z) = (v-A)z, 
460 LAPLACE TRANSFORMS. TAUBERIAN THEOREMS XII1.10 
which is the same as A0.5). The symmetry of this identity implies that the 
operators commute. * 
For the construction of our semi-group we recall from theorem 1 of X,9 
that to an arbitrary contraction T and a > 0 there corresponds a semi- 
group of contractions defined by 
A0.6) ea'(r-1) = e-fltf(-^ Tn. 
n=0 " • 
The generator of this semi-group is a(T— 1), which is an endomorphism. 
We apply this result to 7= Awa). Put for abbreviation 
A0.7) %x = A[A9?(A) - 1] = A2nR(A), &x(t) = em\ 
These operators defined for A > 0 commute with each other, and for fixed 
A the operator . %x generates the quasi-Poissonian semi-group of con- 
tractions ?^@- 
It follows from A0.4) that %xu ->• %u for all u in the domain <?' o(the 
given operator %. We can forget about the special definition of %x and 
consider the remaining assertion of the Hille-Yosida theorem as a special 
case of a more general limit theorem which is useful in itself. In it A may 
be restripted to the sequence of integers. 
Approximation lemma 2. Let {?^@) be a family of pseudo-Poissonian 
semi-groups commuting with each other and generated by the endomorphisms 
If %xu-+%u for all u of a dense set ??'', then 
A0.8) QA@->Q@» A-^oo, 
where {?l(t)} is a semi-group of contractions whose generator agrees with 
% for all ue&". 
Furthermore, for u e 3" 
. A0.9) \\Q(t)u - &x(t)u\\ < t \\%u - %xu\\. 
Proof. For two commuting contractions we have the identity 
Sn - Tn = E"-1+- • • + Tn-1)(S-T) 
and hence 
A0.10) ||SBtt-rBw|| < n ||5«-rw||. 
Applied to operators Zix{tfn) this inequality yields after a trite rearrangement 
A0:11) . \\&x(t)a - &v(t)u\\ < t 
Letting n ¦-> oo we get 
tin t/n 
A0.12)- IIQa@« - Q,@«IL < t \\%xu - %vu\\. 
XIH10 THE HILLE-YOSIDA THEOREM 461 
This shows that for «ei" the sequence. {&x(t)u} is uniformly convergent 
as A->oo. Since 2?' is dense in 2? this uniform convergence extends to 
all u, and if we denote the limit by Q(t)u we have a contraction ?)(/) 
for which A0.8) is true. The semi-group property is obvious. Also, letting 
v -*- oo in A0.12) we get A0.9). Rewriting the left side as in A0.11) we have 
A0.13) 
, .@ 
Choose A large enough to render the right side <e. For sufficiently 
small t the second difference ratio on the left differs in norm from %xu 
by less than e, and hence from %u by less than 3e. Thus for weif" 
A0.14) 
t 
and this concludes the proof. > 
Examples. Diffusion. Let =?? be the family of continuous functions on 
the line vanishing at ±oo. To use familiar notations we replace A by h~2 
and let h -> 0. Define the difference operator VA by 
A0.15) V(») 
This is of the form h~\T — 1) where T is a transition operator, and hence 
V7l generates a semi-group etv* of transition operators (a Markovian semi- 
group). The operators V7l commute with each other, arid for functions 
with three bounded derivatives Vhu -> \u uniformly. The lemma implies 
the existence of a limiting semi-group {?}(/)} generated by an operator % 
such that ?}« = \u" at least when u is sufficiently smooth. 
In this particular case we know that {Q@} is the semi-group of con- 
volutions with normal distributions of variance t and we have not obtained 
new information. The example reveals nevertheless how easy it can be 
(sometimes) to establish the existence of semi-group with given generators. 
The argument applies, for example, to more general differential operators 
and also to boundary conditions. (See problems 24, 25.) > 
Note on the resolvent and complete monotonicity. The Hille-Yosida theorem emphasizes 
properties of the generator %, but it is possible to reformulate the theorem so as to obtain 
a characterization of the family (9?(A)}. 
Theorem 2. (Alternative form of the Hille- Yosida theorem.) In order that a family 
(^H(A); A > 0} of endomorphisms be the resolvent of a semi-group {Q@} of contractions it is 
necessary and sufficient (a) that the resolvent equation 
A0.16) 
be satisfied, (b) that A9?(A) be a contraction, and (c) that >3{(X) —• 1 as A •— oo. 
462 LAPLACE TRANSFORMS. TAUBERIAN THEOREMS XIII. 10 
Proof. A0.16) is identical with A0.5), while conditions (b) and (c) appear above as (ii') 
and A0.3). All three conditions are therefore necessary. 
Assuming the conditions to hold we define an operator $1 as follows. Choose some 
v > 0 and define Sf' as the range of sJi{v), that is: «e/ iff u = 9?(?')w for some 
w e X'. For such u we put %u = hi — w. This defines an operator $1 with domain Se' 
and satisfying the identity 
A0.17) v<K{v) 
We show that this identity extends to all A, that is 
A0.18) A9?(A) - «A) = 1. 
The left side may be rewritten in the form 
A0.19) (A-y)tf(A) + (v-<&W(l). 
Using A0.16) and the fact that (v—%)"J{(v) = 1 we get 
A0.20) (»'-W(A) = 1 + (v-A)tt(A). 
Using A0.19) the identity A0.18) follows. It shows that all conditions of the Hille-Yosida 
theorem are satisfied, and this accomplishes the proof. ^ 
The preceding theorem shows that the whole semi-group theory hinges on the resolvent 
equation A0.16), and it is therefore interesting to explore its meaning in terms of ordinary 
Laplace transforms of functions. It is clear from A0.16) that 9{(A) depends continuously 
on A in the sense that ^(v) -+ 'iH(A) as v -+ A. However, we can go a step farther and 
define a derivative 5K'(A) by 
A0.21) 1H'(A) = lim 
The same procedure now shows that the right side has a derivative given by — 29l(A)!iR'(A). 
Proceeding by induction it is seen that 9?(A) has derivatives 5H(n)(A) of all orders and 
A0.22) (-1)b«(b)(A) = /j!«b+1(^). 
Let now u be an arbitrary function in & such that 0 < u < 1. Choose an arbitrary 
point x and put co(A) = 5R(A)m(x). The right side in A0.22) is a positive operator of norm 
<n!/An+1 and therefore co is completely monotone and jco(n)(A)J < n\/Xn+1. From the 
corollary in section 4 it follows now that co is the ordinary Laplace transform with values 
lying between 0 and 1. If we denote this function by Q(r) u(x), this defines Q(t) as a 
contraction operator. Comparing the resolvent equation A0.16) with (8.2) it is now clear 
that it implies the semi-group property 
A0.23) Q(H-jOw(x) = Q(r)Q(y)if(z). 
We see thus that the essential features of the semi-group theory could have been derived 
from A0.16) using only the classical Lapiace transforms of ordinary functions. In par- 
ticular, the resolvent equation turns out to be merely an abstract paraphrasing of the 
elementary example 8(a). 
To emphasize further that the present abstract theory merely paraphrases the theorems 
concerning ordinary Laplace transforms we prove an inversion formula. 
Theorem 3. For fixed t > 0 as A -+ oo 
A0.24) T—TT rt^W 
(fl1)! 
U PROBLEMS FOR SOLUTION 463 
Proof. From the definition (9.6) of &(x> as a Laplace transform of ?}(/) it follows 
that 
A0.25) 
Jo 
The left side of A0.24) is the integral of ?l(s) with respect to the density [ 
t(n—1)!] n which has expected value / and variance t2jn. As n -*¦ oo this measure tends to 
the distribution concentrated at /, and because of the continuity of ?}(•*), this implies 
A0.24) just as in the case of functions [formula A0.24) is the same as VII,A.6)]. > 
11. PROBLEMS FOR SOLUTION 
1. Let FQ be the geometric distribution attributing weight qpn t© the point 
nq (n = 0, 1 ,...)• As q -? 0 show that FQ tends to the exponential distribution 
1 — e~x and that its Laplace-transform tends to 1/(A + 1). 
2. Show that the ordinary Laplace transforms of cos a; and sin a; are X/(X2 + l) 
and 1/(A2 + 1). Conclude that A +a~2)e~x(l — cos ax) is a probability density 
with Laplace transform A +a2)(X + l)-1[(X + lJ + a2Yx. Hint: Use eix = 
= cos x + i sin x or, alternatively, two successive integrations by parts. 
3. Let co be the transform of a measure U. Then to is integrable over 0, 1 
and 1, oo iff \jx is integrable with respect to U over 1, oo and 0, 1, respectively., 
4. Parseval relation. If X and Y are independent random variables with dis- 
tributions F and C, and transforms q> and y, the transform of XY is 
f °°9>(^) G{dy) = r 
Jo Jo 
F{dy). 
5. Let F be a distribution with transform q>. .If a > 6 then q>(A+a)/q>(a) 
is the transform of the distribution e-aa: F{dx}/g>(a). For fixed / > 0 conclude 
from example 3F) that15 exp [— t^ll + a2 + at] is the transform of an infinitely 
/ r 1/ / ,-\2"i 
divisible distribution with density , exp — •= —— — aVx\ . 
J Vl-rrx3 ^l2\Vx )} 
6. From the definition 11,G.1) show that the ordinary Laplace transform of IQ(x) 
is co0(Z) = 1/VA2~^~1 for X > 1. |~Recall the identity B"\ = [~^(-4)".l 
7. Continuation. Show that /„' = Ix, and hence that ¦ Ix has the ordinary 
Laplace transform cox{X) = coQ{X) R{X) where R(X) = X - V X2 - 1. 
8. Continuation. Show that 2l'n = In_1 + In+l for n = 1, 2,. . . and hence 
by induction that /„ has the ordinary transform con(X) = co0(X) Rn(X). 
9. From example 3(e) conclude by integration that e1/x — 1 is the ordinary 
transform of I1BVx)/Vx. 
10. Let X and Y be independent random variables with Laplace transforms 
V and e~x<t, respectively. Then YX1/* has the Laplace transform (p(Xa). 
15 This formula occurs in applications and has been derived repeatedly by lengthy cal- 
culations. 
464 LAPLACE TRANSFORMS. TAUBERIAN THEOREMS XIII. 11 
11. The density / of a probability distribution is completely monotone iff it is 
a mixture of exponential densities [that is, if it is of the form G.11)]. Hint: Use 
problem 3. 
12. Verify the inversion formula D.5) by direct calculation in the special cases 
i(X) 1/(A + 1) and (k) \ 
13. Show that the corollary in section 4 remains valid if / and <p(n) are replaced 
by their absolute values. 
14. Assuming e~xIn{x) monotone at infinity, conclude from problem 8 that 
1 
e XIn(x) r-^—-== X -+ 00. 
15. Suppose that 1 - <p{k) ~ Xx~p L(X) as X -+ 0 where P > 0. Using 
example 5(c) show that 1 — Fn*(x)~ nxP-1 L(\/x)/r(p) as x -v oo. [Compare 
this with example VIII,8(c).] 
16. In theorem 4 of section 5 it suffices that u(x)~ v(x) where v is ultimately 
monotone. 
17. Every infinitely divisible distribution is the limit of compound Poisson 
distributions. 
18. If in the canonical representation G.2) for infinitely divisible distributions 
P(x) ~ xcL(x) as x — oo with 0 < c < 1, prove that 1 - F(x) ~ (c/1 - c)^1 L{x). 
[Continued in example XVII,4(d).] 
19. Let P be the generating function of an infinitely divisible integral-valued 
random variable and y the Laplace transform of a probability distribution. 
Prove that P(cp) is infinitely divisible. 
20. The infinitely divisible Laplace transforms g>n converge to the Laplace 
transform y of a probability distribution iff the corresponding measure Pn in 
the canonical representation G.2) converges to P. Hence: the limit of a sequence 
of infinitely divisible distributions is itself infinitely divisible. 
21. Let Fn be a mixture G.11) of exponential distributions corresponding to a 
mixing distribution Un. The sequence {Fn} converges to a probability distribution 
F iff the ?/„' converge to a probability distribution U. In this case F is a mixture 
corresponding to U. 
22. A probability distribution with a completely monotone density is infinitely 
divisible. Hint: Use problems 11 and 21 as well as example 7(/). 
23. Every mixture of geometric distributions is infinitely divisible. Hint: Follow 
the pattern of example 7(/). 
24. Diffusion with an absorbing barrier. In the example of section 10 restrict 
x to x > 0 and when x — h < 0 put^x— h) = 0 in the definition of VA. Show 
that the convergence proof goes through if & is the space of continuous functions 
with w(oo) = 0, w@) = 0, but not if the last condition is dropped. The resulting 
semi-group is given in example X,5(b). 
25. Reflecting barriers. In the example of section 10 restrict x to x > 0 and 
when x — h < 0 put u(x—h) =u(x+h) in the definition of VA. Then S7hu 
converges for every u with three bounded derivatives such that w'@) = 0. The 
domain JSf' of 51 is restricted by this boundary condition. The semi-group is 
described in example X,5(e). 
XIH. 11 PROBLEMS FOR SOLUTION 465 
26. The influence of the maximal term in the convergence to stable distributions. 
Let Xx, X2,... be independent variables with the common distribution F satis- 
fying F.4), that is, belonging to the domain of attraction of the stable distribution 
Ga. Put Sn *= Xx H + Xn and Mn = max [Xlf... , XJ. Prove that the 
ratio Sn/Mn has a Laplace transform eon(A) converging to16 
1 + a A -e-xt)r* dt 
Hence E(Sn/Mn) - 1/A -a). 
Hint: Evaluating the integral over the region X,- <, Xx one gets 
\n-l 
= ne~x 
I C°F{dx}( | °V*v/*F{dy}\ 
Substitute y = tx and then x = ans where an satisfies F.5). The inner integral 
is easily seen to be 
, 1 ~ F(QnS) 1 f' ^ F{andt) (\\ s 
where ^(A) stands for the denominator in (*). Thus 
<xds 
I 
16 This result and its analogue for stable distributions with exponent a > 0 was derived 
by D. A. Darling in terms of characteristic functions. See Trans. Amer. Math. Soc., vol. 
73 A952) pp. 95-107. 
CHAPTER XIV 
Applications of 
Laplace Transforms 
This chapter can serve as collateral reading to chapter XIII. It covers 
several independent topics ranging from practical problems (sections 1, 2, 
4, 5) to the general existence theorem in section 7. The limit theorem of 
section 3 illustrates the power of the methods developed in connection with 
regular variation. The last section serves to describe techniques for the 
analysis of asymptotic properties and first-passage times in Markov processes. 
1. THE RENEWAL EQUATION: THEORY 
For the probabilistic background the reader is referred to VI,6~7. Although 
the whole of chapter XI was devoted to renewal theory, we give here an 
independent and much less sophisticated approach. A comparison of the 
methods and results is interesting. Given the rudiments of the theory of 
Laplace transforms, the present approach is simpler and more straightfor- 
ward, but the precise result of the basic renewal theorem is at present not 
obtainable by Laplace transforms. On the other hand, Laplace transforms 
lead more easily to the limit theorems of section 3 and to explicit solutions 
of the type discussed in section 2. 
The object of the present study is the integral equation 
A.1) K@ = G@ + rV(i-«) F{dx} 
Jo 
in which F and G are given monotone right continuous functions vanishing 
for / < 0. We consider them as improper distribution functions of measures 
and suppose F is not concentrated at the origin and that their Laplace 
transforms 
A.2) qffl - PV' F{dt), y(X) « PY* G{dt) 
Jo Jo 
466 
XIV. 1 THE RENEWAL EQUATION: THEORY 467 
exist for X > 0. As in the preceding chapter all intervals of integration 
are taken closed. It will be shown that there exists exactly one solution V; 
it is an improper distribution function whose Laplace transform xp exists 
for all X > 0i If G has a density g, then V has a density v satisfying 
the integral equation 
A.3) 
(tv(t-x 
Jo 
obtained by differentiation from A.1). 
Recalling the convolution rule we get for the Laplace transform ip of 
the distribution V (or the ordinary transform of its density) xp = y + ip<p, 
wnence formally 
A.4) 
To show that this formal solution is the Laplace transform of a measure 
(or density) we distinguish three cases (of which only the first two are 
probabilistically significant). 
Case (a). F is a probability distribution, not concentrated at the origin. 
Then <p@) = 1 and <p(X) < 1 for X > 0. Accordingly 
A.5) to = = I fn 
1 — <p o 
converges for X > 0. Obviously a> is completely monotone and therefore 
the Laplace transform of a measure U (theorem 1 of XIII ,4). Now 
xp = my is the Laplace transform of the convolution V = U*k G, that is 
A.6) V(t) = [G{t-x) U{dx}. 
Jo 
Finally, if G has a density g then V possesses a density v = U-kg. We 
have thus proved the existence and the uniqueness of the desired solution of 
our integral equations. 
The asymptotic behavior of V at infinity is described by the Tauberian 
theorem 2 of XIII,4. Consider the typical case where G(oo) < oo and F 
has a finite expectation fx. Near the origin y(X) ~ (jT'1G{cc)X-x which 
implies that 
A.7) V(t)~ p-xG(c6)-t, /-•¦oo. 
The renewal theorems in XI, 1 yield the more precise result that 
V(t+h) - V(t)-»frlG(cc)h, 
but this cannot be derived from Tauberian theorems. [These lead to better 
results when F has no expectation; (section 3).] 
468 APPLICATIONS OF LAPLACE TRANSFORMS XIV.2 
Case (b). F is a defective distribution, F(ao) < 1. Assume for simplicity 
that also <7(oo) < oo. The preceding argument applies with the notable 
simplification that <p@) = F(oo) < 1 and so w@) < oo: the measure V 
is now bounded. 
Case (c). The last case is F(oo) > 1. For small values of A the de- 
nominator in A.4) is negative, and for such values w(A) cannot be a 
Laplace transform. Fortunately this fact causes no trouble. To avoid 
trivialities assume that F has no atom at the origin so that <p{X) -»- 0 as 
A -*¦ oo. In this case there exists a unique root k > 0 of the equation 
<p(t<) = 1 and the argument under (a) applies without change for A > k. 
In other words, there exists a unique solution V, but its Laplace transform 
a) converges only for A > k. For such values co is still given by A.4).1 
2. RENEWAL-TYPE EQUATIONS: EXAMPLES 
(a) Waiting times for gaps in a Poisson process. Let V> be the distribution 
of the waiting time to the completion of the first gap of length ? in a 
Poisson process with parameter c (that is, in a renewal process with 
exponential interarrival times). This problem was treated analytically in 
example XI ,7(b). Empirical interpretations (delay of a pedestrian or car 
trying to cross a stream of traffic, locked times in type II Geiger counters, 
etc.) are given in VI,7. We proceed to set up the renewal equation afresh. 
The waiting time commencing at epoch 0 necessarily exceeds ?. It 
terminates before t > ? if no arrival occurs before epoch ? (probability 
e~ci) or else if the first arrival occurs at an epoch x < ? and the residual 
waiting time is < t — x. Because of the inherent lack of memory the 
probability V(t) of a waiting time <t is therefore 
B.1) V(t) = e~ct + \ V(t-x)-e~cxcdx 
Jo 
for f |> ? and V{t) = 0 for t < ?. Despite its strange appearance B.1) 
is a renewal equation of the standard type A.1) in which F has the density 
f(x) = ce~ex concentrated on 0 < x < ?, while G is concentrated at the 
point ?. Thus 
B.2) <p{X) = 2 
c + A 
1 The solution V is of the form V{dx) = eKXV#{dx) where V# is the solution [with 
Laplace transform xp#(X) = y(A+«)] of a standard renewal equation A.1) with F 
replaced by the proper probability distribution F#{dx) = e~KX F{dx} and G by 
G#{dx} = e~K*G{d.c}. 
XIV.2 RENEWAL-TYPE EQUATIONS. EXAMPLES 469 
and hence the transform xp of V is given by 
B.3) ( 
The expressions XI,G.8) for the expectation and variance are obtained 
from this by simple differentiations2 and the same is true of the higher 
moments. 
It is instructive to derive from B.3) an explicit formula for the solution. For reasons that 
will become apparent we switch to the tail 1 — V(t) of the distribution. Its ordinary 
Laplace transform is [1— v(A)]/A [see XIII,B.7)] which admits of an expansion into a 
geometric series 
The expression within braces differs from the Laplace transform A — e~A)/A of the uniform 
distribution merely by a scale factor ? and by the change from A to A + c. As was 
observed repeatedly, this change corresponds to a multiplication of the densities by e~ct. 
Thus 
oo 
a.5) 
where /"* is the density of the /i-fold convolution of the uniform distribution with itself. 
Using 1,(9.6) we get finally 
B.6) 1 - 
The relation to covering theorems is interesting. As was shown in 1,(9.9) the inner sum 
represents (for /, ? fixed) the probability that n — 1 points chosen at random in 0, / 
partition this interval into n parts each of which is <, ?. Now the waiting time exceeds / 
iff every subinterval of 0, / contains at least one arrival and so B.6) states that if in a 
Poisson process exactly n — 1 arrivals occur in 0, / their conditional distribution is uniform. 
If one starts from this fact one can take B.6) as a consequence of the covering theorem; 
alternatively, B.6) represents a new proof of the covering theorem by randomization. 
(b) Ruin problem in compound Poisson processes. As a second illustrative 
example we treat the integro-differential equation 
B.7) R\t) = (a/c)i?(r) - (a/c) f R(f-*) F{dx} 
Jo 
in which F is a probability distribution with finite expectation /u. This 
equation was derived in VI,5, where its relevance for collective risk theory, 
storage problems, etc., is discussed. Its solution and asymptotic properties 
are derived by different methods in example XI,7(a). 
2 It saves labor first to clear the denominator to avoid tedious differentiations of fractions. 
470 APPLICATIONS OF LAPLACE TRANSFORMS XIV.3 
The problem is to find' a probability distribution R satisfying B.7). 
This equation is related to the renewal equation and can be treated in the same 
way. Taking ordinary Laplace transforms and noticing that 
B.8) P{X) = f VAx R{x) dx = A f V"K'(aO dx + X~xR@) 
Jo Jo 
we get 
B.9) p{X) = 
where <p is the Laplace transform of F. Recalling that [1 —^(A)]/A is the 
ordinary Laplace transform of 1 — T(x) we note that the first fraction on 
the right is of the form A.4) and hence the Laplace-Stieltjes transform of a 
measure .R. The factor 1 /A' indicates an integration, and hence />(A) is the 
ordinary Laplace transform of the improper distribution function R(x) [as 
indicated in B.8)].- Since R(x) -> 1 as x -> oo it follows from theorem 4 
in XIII,5 that />(A)->1 as A->0, From B.9) we get therefore for the 
unknown constant R@) 
B.10) J?@) = 1 - (a/c)/i. 
Accordingly, our problem admits of a unique solution if cufi < c and admits 
of no solution if cnjn > c. This result was to be anticipated from the prob- 
abilistic setup. 
Formula B.9) appears also in queuing theory under the name Khintchine-Pollaczek 
formula [see example XII,5(a)]. Many papers derive explicit expressions in special cases. 
In the case of the pure Poisson process, F is concentrated at the point 1, and y(A) = e~x. 
The expression for p is now almost the same as in B.3) and the same method leads easily 
to the explicit solution 
^=( 
Although of no practical use, this formula is interesting because of the presence of positive 
exponents which must cancel out in curious ways. It has been known in connection with 
collective risk3 theory since 1934 but was repeatedly rediscovered. 
3. LIMIT THEOREMS INVOLVING ARC SINE 
DISTRIBUTIONS 
It has become customary to refer to distributions concentrated on 0, 1 
with density 
7T 
3 An explicit solution for ruin before epoch / is given by R. Pyke, The supremum and 
infimum of the Poisson process, Ann. :v4ath. Statist., vol. 30 A959) pp. 568-576. 
XIV.3 LIMIT THEOREMS WITH ARC SINE DISTRIBUTIONS 471 
as "generalized arc sine distributions" although they are special beta distri- 
butions. The special case a = ? corresponds to the distribution function 
2tt~x arc sin \lx which plays an important role in the fluctuation theory for 
random walks. An increasing number of investigations are concerned with 
limit distributions related to qa, and their intricate calculations make the 
occurrence of qa seem rather mysterious. The deeper reason lies in the 
intimate connection of qa to distribution functions with regularly varying 
tails, that is, distributions of the form 
C.2) 1 - F(x) = x-* L(x), 0 < a < 1 
where, L(tx)lL(t) -> 1 as t -> oo. For such functions the renewal theorem 
may be supplemented to the effect that the renewal function U = ^ ^n* 
satisfies 
C.3) 17@ —, *->oo. 
r(l-a)r(l+a)L@ 
In other words, if F varies regularly, so does U. It is known (but not obvious) 
that the constant in C.3) equals (sin <rra)/7ra and so C.3) may be rewritten 
in the form 
C.4) [1 — F(x)] U(x) -*¦ , z->oo. 
7T<X 
Lemma. If F is of the form C.2) then C.4) holds. 
Proof. By the Tauberian theorem 4 of XIII,5 
1 - <p(A)~r(l-a)AaL(l/A) /L->0. 
The Laplace transform of U is 2 <pn = 1/A — <p) and C.3) is true by virtue 
of theorem 2 of XIII,5. > 
Consider now a sequence of positive independent variables Xfc. with the 
common distribution F and their partial sums Sn = Xx + • • • + Xn. 
For fixed / > 0 denote by N, the chance-dependent index for which 
C.5) SN(<*< 
We are interested in the two subintervals 
Y« = /-SN( and Z< = 
They were introduced in VI,7 as "spent waiting time" and "residual waiting 
time" at epoch t. The interest attached to these variables was explained in 
various connections, and in XI,4 it was proved that as f -*¦ oo the variables 
Yt and Zt have a common proper limit distribution iff F has a finite 
expectation. Otherwise, however, P{Yt<z}->0 for each fixed x > 0, 
472 APPLICATIONS OF LAPLACE TRANSFORMS XIV.3 
and similarly for Zt. The following interesting theorem emerges as a by- 
product, of our results, but the original proof presented formidable analytical 
difficulties.4 
Theorem. If C.2) is true, then the normed variable YJt has the limit 
density qa of C.1), and ZJt has the limit density given by5 
11 c\ t \ sm ira 1 ^ „ 
C.6) pa(x) = — , x > 0. 
*(l+) 
tt 
Proof. The inequality tx1 < Yt < tx2 occurs iff Sn = ty and 
for some combination n, y such that 1 — x2 < y < 1 — xv Summing 
over all n and possible y we get 
C.7) F{tx, < Y, < tx2) = P ''[I - F{t{\-y))} U{t dy) 
Jl-Xj 
and hence using C.4) 
C.8) Pfo < Yt < tx2} ~S°a ri i - mi-y)). uijdyl 
. 1T0L Jl-x, I— F@ ' 17@ ' 
Now U(ty)jU(t) -*¦ ya and so the measure U{t dy}fU{t) tends to the 
measure with density ay" while the first factor approaches A— y)~*. 
Because of the monotonicity the approach is uniform, and so 
C.9) F{tXl <Yt< tx2} 
7T 
which proves the first assertion. For P{Zt > ts} we get the same integral 
between the limits 0 and 1/A +s) and by differentiation one gets C.6). > 
It is a remarkable fact that the density qa becomes infinite near the 
endpoints 0 and 1. The most probable values for YJt are therefore near 
0 and 1. 
It is easy to amend our argument to obtain converses to the lemma and 
the theorem. The condition C.2) is then seen to be necessary for the 
existence of a limit distribution for YJt. On the other hand, C.2) character- 
izes the domain of attraction of stable distributions, and this explains the 
frequent occurrence of qa in connection with such distributions. 
4 E. B. Dynkin, Some limit theorems for sums of independent random variables with infinite 
mathematical expectations. See Selected Trans, in Math. Statist, and Probability, vol. 1 
A961) IMS-AMS, pp. 171-189, 
5 Since SN<+i = Zt + / the distribution of Zj/Sn^ is obtained from C.6) by the 
change of variable x = y/(l—y). It is thus seen that also 7iJStit+x has the limit.density qa. 
XIV.4 BUSY PERIODS AND RELATED BRANCHING PROCESSES 473 
4. BUSY PERIODS AND RELATED BRANCHING PROCESSES 
It was shown in example XIII,4(a) that, if q> is the Laplace transform 
of a probability distribution F with expectation /u, the equation 
D.1) fi(X) = <p{X + c - 
possesses a unique solution fi: furthermore /? is the Laplace transform of a 
distribution B which is proper if cjjl < 1 and defective otherwise. This 
simple and elegant theory is being applied with increasing frequency and it is 
therefore worthwhile to explain the probabilistic background of D.1) and its 
applications. 
The derivation of D.1) and similar equations is simple if one gets used 
to expressing probabilistic relations directly in terms of Laplace trans- 
forms. A typical situation is as follows. Consider a random sum SN = 
= Xx + • • • + XN where the X, are independent with Laplace transform 
y(A), and N is an independent variable with generating function P(s). 
The Laplace transform of SN is obviously P(y(X)) [see example XIII,3(c)]. 
For a Poisson variable N this Laplace transform is of the form e~a{l~nX)^. 
As we have seen repeatedly, in applications the parameter a is often taken 
as a random variable subject to a distribution U. Adapting the terminology 
of distribution functions we can then say that e~x[l~v{X)] is the conditional 
Laplace transform of SN given the value a of the parameter. The absolute 
Laplace transform is obtained by integration with respect to U. Due to the 
peculiar form of the integrand the result is obviously w(l— y(A)) where co 
stands for the Laplace transform of U. 
Examples, (a) Busy periods.6 Customers (or calls) arrive at a server 
(or trunkline) in accordance with a Poisson process at a rate c. The 
successive service times are supposed to be independent variables with 
the common distribution F. Suppose that at epoch 0 a customer arrives 
and the server is free. His service time commences immediately: the 
customers arriving during his service time join a queue, and the service 
times continue without interruption as long as a queue exists. By busy 
period is meant the interval from 0 to the first epoch when the server again 
6 That D.1) governs the busy periods was pointed out by D. G. Kendall, Some problems 
in the theory of queues, J. Roy. Statist. Soc. (B), vol. 13 A951) pp. 151-185. The elegant 
reduction to branching processes was contributed by I.'J. Good. Equation D.1) is equivalent 
to 
J't (rx)n 
e~cx —— Bn*(t-x) F{dx) 
o "l 
which is frequently referred to as Takacs' integral equation. The intrinsic simplicity of the 
theory is not always understood. 
474 APPLICATIONS OF LAPLACE TRANSFORMS XIV.4 
becomes free. Its duration is a random variable and we denote by B and (} 
its distribution and Laplace transform, respectively. 
In the terminology of branching processes the customer initiating the busy 
period is the "ancestor," the customers arriving during his service time are 
his direct descendents, and so on. Given that the progenitor departs at 
epoch x the number N of his direct descendants is a Poisson variable with 
expectation ex. Denote by X, the total service time of the yth direct 
descendant and all of his progeny. Although these service times are not 
necessarily consecutive their total duration has clearly the same distribution 
as the busy period. The total service time required by all (direct and indirect) 
descendants is therefore SN = Xx + • • • + XN where the X, have the La- 
place transform /? and all the variables are independent. For the busy period 
we have to add the service time x of the ancestor himself. Accordingly, 
given the length of the ancestor's service time the busy period x -f SN has 
the (conditional) Laplace transform e~xU+c~cpU)l. The parameter x has 
the distribution F and integration with respect to a; yields D.1). 
If B is defective the defect 1 — 2?(oo) represents the probability of a 
never-ending busy period (congestion). The condition c\i < 1 expresses 
that the expected total service time of customers arriving per time unit must 
not exceed unity. It is easy from D.1) to calculate the expectation and 
variance of B. 
In the special case of exponential service times F{t) = 1 — e~ai and 
9?(A) = <x/(A+a). In this case D.1) reduces to a quadratic equation one of 
whose roots is unbounded at infinity. The solution /? therefore agrees with 
the other root, namely 
~X + o + c _ j /A + aJ- c\2 j" 
- 2Vac V I 2Vic / 
This Laplace transform occurs in example XIII,3(c). Taking into account 
the changed scale parameter and the translation principle we find that the 
corresponding density is given by 
D.3) Voc/c e-^+^x-1 I&yJaLC x). 
The same result will be derived by another method in example 6F); it was 
used in example VI,9(e). 
(b) Delays in traffic.1 Suppose that cars passing a given point of the 
road conform to a Poisson process at a rate c. Let the traffic be stopped 
(by a red light or otherwise) for a duration 6. When traffic is resumed K 
cars will wait in line, where K is a Poisson variable with parameter cd. 
7 This example is inspired by J. D. C. Little's treatment of the number of cars delayed. 
[Operations Res., vol. 9 A961) pp. 39-52.] 
XIV. 5 DIFFUSION PROCESSES 475 
Because the rth car in the line cannot move before the r — 1 cars ahead of it, 
each car in the line causes a delay for all following cars. It is natural to 
assume that the several delays are independent random variables with a 
common distribution F. For the duration of a waiting line newly arriving 
cars are compelled to join the line, thus contributing to the total delay. The 
situation is the same as in the preceding example except that we have K 
"ancestors." The total delay caused by each individual car and its direct and 
indirect descendants has the Laplace transform p satisfying D.1), and the 
total "busy period"—the interval from the resumption of traffic to the 
first epoch where no car stands waiting—has the Laplace transform 
It is easy to calculate the expected delay and one can use this result for the 
discussion of the effect of successive traffic lights, etc. (See problems 
6, 7.) > 
5. DIFFUSION PROCESSES 
In the one-dimensional Brownian motion the transition probabilities are 
normal and. the first passage times have a stable distribution with index \ 
[see example VI,2(e)]. Being in possession of these explicit formulas we must 
not expect new information from the use of Laplace transforms. The reason 
for starting afresh from the diffusion equation is that the method is instructive 
and applicable to the most general diffusion equation (except that no explicit 
solutions can be expected when the coefficients are arbitrary). To simplify 
writing we take it for granted that the transition probabilities ,Qt have 
densities qt (although the method to be outlined would lead to this result 
without special assumptions). 
We begin with the special case of Brownian motion. For a given bounded 
continuous function / put 
J*+oo 
qfa V)Av) 
— 00 
Our starting point is the fact derived in example X,4(a) that (at least for / 
sufficiently smooth) u will satisfy the diffusion equation 
du(t, x) _^ 1 d2u(t, x) 
K ' dt 2 dx* 
with the initial condition w(f, x) ->/(z) as t -> 0. In terms of the ordinary 
Laplace transform 
E.3) cox(x) = I e~xtu(t, x) dt 
Jo 
476 APPLICATIONS OF LAPLACE TRANSFORMS XIV.5 
we conclude from E.2) that8 
E.4) Xojx - Wx =f 
and from E.1) that 
J'+oo 
Kx(x, s)f(s) ds 
— ao 
where Kx(x, y) is the ordinary Laplace transform of qt(x, y). In the theory 
of differential equations Kx is called the Green function of E.4). We shall 
show that 
E.6) K,{x,y) 
The truth of this formula can be verified by checking that E.5) represents the 
desired solution of the differential equation E.4), but this does not explain 
how the formula was found. 
We propose to derive E.6) by a probabilistic argument applicable to more 
general equations and leading to explicit expressions for the basic first 
passage times. (Problem 9.) We take it as known that the path variables 
X(/) depend continuously on t. Let X@) = x and denote by F{t\x,y) the 
probability that the point y will be reached before epoch /. We call F 
the distribution of the first-passage epoch from x to y and denote its 
Laplace transform by <px(x, y). 
For x < y < z the event X(t) = z takes place iff a first passage through 
y occurs at some epoch r < / and is followed by a transition from y to z 
within time t — r. Thus qt(x, z) represents the convolution of F{t, x, y) 
and qt(y, z), whence 
E.7) Kx(x, z) = <px(x, y) Kx(y, z), x < y < z. 
Fix a point y and choose for / a function concentrated on y, oo. Multiply 
E.7) by f(z) and integrate with respect to z. In view of E.5) the result is 
E.8) cox(x) = cpx(x, y) (ox(y), x>y, 
while E.4) requires that for y fixed <px(x, y) satisfy the differential equation 
E.9) X<px - \d-^ = 0, x<y. 
1 ox 
A solution which is bounded at — oo is necessarily of the form Cxe 
8 Readers of the sections on semi-groups will notice that we are concerned with a 
Markovian semi-group generated by the differential operator % = \d2\dx2. The differential 
equation E.4) is a special case of the basic equation XIII,A0.1) occurring in the Hille- 
Yosida theorem. 
XIV.5 DIFFUSION PROCESSES 477 
Since E.8) shows that yx(x, y)-+l as z -> y, we have yx(x, y) = 
provided x < y. A similar argument applies when x > y and it is clear that 
for reasons of symmetry the Laplace transform of the first-passage time from 
x to y is given by 
E.10) <Px(x, V) = e- 
Letting z = y in E.7) we see therefore that 
and since K must depend symmetrically on x and y it follows that 
K(y, y) reduces to a constant Cx depending only on A. We have thus 
determined Kx up to a multiplicative constant Cxi that V2AQ = 1 
follows easily from the fact that to /= 1 there corresponds the solution 
wx(x) = I/A. This proves the truth of E.6). 
The following examples show how to calculate the probability that a point 
yx > x will be reached before another point y2 < x. At the same time they 
illustrate the treatment of boundary conditions. 
Examples, (a) One absorbing barrier. The Brownian motion on 0, oo 
with an absorbing barrier at the origin is obtained by stopping an ordinary 
Brownian motion with X@) = x > 0 when it reaches the origin. We 
denote its transition densities by cffha(x, y) and adapt similarly the other 
notations. 
In the unrestricted Brownian motion the probability density of a passage 
from x > 0 to y > 0 with an intermediate passage through 0 is the con- 
volution of the first passage from ? to 0 and qt@, y). The corresponding 
Laplace transform is <px(x, 0) Kx@, y) and hence we must have 
E.11) Kf %x, y) = Kx(x, y) - <px(x, 0) Kx@, y), 
where x > 0, y > 0. This is equivalent to 
E.12) Kx {x, y) = [e" — e J/V2/ 
or 
E.13) q?b3(x, y) = qt(x,.y) - qt(x, -y) 
in agreement with the solution X,E.5) obtained by the reflection principle. 
The argument leading to E.7) applies without change to the absorbing 
barrier process and we conclude from E.12) that for 0 < x < y 
E.14) <p?\x, y) = -^ 
V 2X 
e ¦ — x — e~ 
v 
478 APPLICATIONS OF LAPLACE TRANSFORMS XIV.5 
This9 is the Laplace transform of the probability that in an unrestricted 
Brownian motion with X@) = x the point y > x is reached before epoch t 
and before a passage through the origin. Letting A —>- 0 we conclude that 
the probability that y will be reached before the origin equals xfy. just as in 
the symmetric Bernoulli random walk (see the ruin problem in 1; XIV,2). 
(Continued in problem 8.) 
(b) Two absorbing barriers. Consider now a Brownian motion starting 
at a point x in 0, 1 and terminating when either 0 or 1 is reached. It is 
easiest to derive this process from the preceding absorbing barrier process 
by introducing an additional absorbing barrier at 1 so that the reasoning 
leading to E.11) applies without change. The transition densities qf(x,y) 
of the new process have therefore the Laplace transform Kf given by 
E.15) K*(xt y) = Kfi&y) - <P?X*, 1) *Sb V, V) 
with x and y restricted to 0, 1. [Note that the boundary conditions 
$ . V) = Kf A, y) arc satisfied.] Simple arithmetic shows that 
E.16) K«(x,y) = e- ±±^1 
Expanding 1/[1 — e~2^u] into a geometric series, one is led to the alternative 
representation 
E.17) K#(X, y) = ~j= f 
v/2/l n=—uc 
which is equivalent to the solution X,E.7) obtained by the reflection 
principle. > 
The same argument applies to the more general diffusion equation 
3 _ ia(x) ^ + b(x) M^} _ a > 0, 
in a finite or infinite interval. Instead of E.4) we get 
E.19) Acox-\aco'x-bco'x=f 
and the solution is again of the form E.5) with a Green function Kk of 
the form E.7) where <px(x, y) is the transform of the first-passage density 
from x to y > x. For fixed y, this function must satisfy the differential 
equation corresponding to E.9^ namely 
E.20) X<px - \ay"x - b<p'x = 0. 
9 For y fixed, <p|bs represents the solution of the differential equation E.9) which 
reduces to 0 when x = 0 and to 1 when x — y. In this form the result applies to arbitrary 
triples of points a < x < b and a > x > b and to more general differential equations. 
XIV.6 BIRTH-AND-DEATH PROCESSES AND RANDOM WALKS 479 
It must be bounded at the left endpoint and <px(y, y) = 1. These conditions 
determine <px uniquely except if E.20) possesses a bounded solution, in 
which case (as in the above examples) appropriate boundary conditions 
must be imposed. (See problems 9, 10.) 
6. BIRTH-AND-DEATH PROCESSES AND RANDOM 
WALKS 
In this section we explore the connection between the birth-and-death 
processes of 1; XVII,5 and the randomized random walk of 11,7. The main 
purpose is to illustrate the techniques involving Laplace transforms and the 
proper use of boundary conditions. 
Consider a simple random walk starting at the origin in which the individual 
steps equal 1 or —1 with respective probabilities p and q. The times between 
successive steps are supposed to be independent random variables with an 
exponential distribution with expectation 1/c. The probability Pn(t) of 
the position n at epoch / was found in 11,G.7), but we start afresh from a 
new angle. To derive an equation for Pn(t) we argue as follows. The position 
n^O at epoch / is possible only if a jump has occurred before /. Given that 
the first jump occurred at / — x and led to 1, the (conditional) probability 
of the position n at epoch / is Pn-x(x). Thus for n = ±1, ±2, .... 
F A a) Pn(t) = [ce-^-^pP^ix) + qPn+1(x)] dx. 
Jo 
For n = 0 the term e~ct must be added to account for the possibility of no 
jump up to epoch /. Thus 
F.16) P0(t) = e~ct + I ce-^-^lpP.^x) + qP^x)] dx. 
Jo 
Accordingly, the Pn must satisfy the infinite .system of convolution 
equations F.1). A simple differentiation leads to the infinite system of 
differential equations10 
F.2) P'n{t) = -cPn(t) + cpP^it) + cqPn+1(t) 
together with the initial conditions P0@) = 1, Pn@) = 0 for n 5^ 0. 
The two systems F.1) and F.2) are equivalent, but the latter has the 
formal advantage that the special role of n = 0 is noticeable only in the 
initial conditions. For the use of Laplace transforms it does not matter 
where we start. 
10 They are a special case of the equations 1; XVII,E.2) for general birth-and-death 
processes and may be derived in like manner. 
480 APPLICATIONS OF LAPLACE TRANSFORMS XIV.6 
We pass to Laplace transforms putting 
F.3) 7rn(A) = f V" Pn(t) du 
Jo 
Since convolutions correspond to multiplication of Laplace transforms and 
e~cx has the transform l/(c+A) the system F.1) is equivalent to 
F.4a) 7rn(A) = -—- [ptt^A) + qirn+1(k)]t 
c + A 
F.46) 7ro(A) = —j— + -—- \pir_iik) + qirx(X)l 
c + A c + A 
[The same result could have been obtained from F.2) since 
which follows on integration by parts.] 
The system of linear equations F.4) is of the type encountered in connection 
with random walks in 1; XIV, and we solve it by the same method. The 
quadratic equation 
F.5) cqs2 - (c + X)s + cp = 0 
has the roots 
F.6) sx = ^ '- ?*• and ax = (pjq)sx\ 
2cq 
It is easily verified that with arbitrary constants Ax, Bx the linear com- 
binations 7rn(A) = Axs^ +'Bxox% satisfy F.4a) for n = I, 2, . . . , and the 
coefficients can be chosen so as to yield the correct values for tto(A) and 
tt^A). Given n0 and -nx it is possible from F.4a) to calculate recursively 
;r2, tt3, . . . , and so for n > 0 eyery solution is of the form 7rri(A) 
= ,4^ + Bx<fl. Now ^ -> 0 but ax -> oo as A -> oo. As our 7rn(A) remain 
bounded at infinity we must have Bx = 0, and hence 
F.7a) TTn{X) = 7ro(A)s2, n = 0, 1, 2, .... 
For n <> 0 we get analogously 
F.1b) ttw(A) = ir,(AK = (p/4)w7r0(A)sIn, n = 0, -1, -2, .... 
Substituting into F.46) we get finally 
F.8) 7 l 
- 4czpq 
and so all irn(X) are uniquely determined. 
XIV.6 BIRTH-AND-DEATH PROCESSES AND RANDOM WALKS 481 
Much information can be extracted from these Laplace transforms without 
knowledge of explicit formulas for the solution itself. For example, since 
multiplication of Laplace transforms corresponds to convolutions, the form 
F.7) suggests that for n > 0 the probabilities Pn are of the form Pn = 
__ pn* ^ p^ where F is a (possibly defective) probability distribution with 
transform sx. That this is so can be seen probabilistically as follows. If at 
epoch / the random walk is at the point n, the first passage through n 
must have occurred at some epoch t < t. In this case the (conditional) 
probability of being at epoch / again at n equals P0(t—r). Thus Pn is 
the convolution of Po and the distribution Fn of the first passage time 
through n. Again, this first passage time is the sum of n identically dis- 
tributed independent random variables, namely the waiting times between 
successive passages through 1, 2,.... This explains the fonn F.7) and shows 
at the same time that snx is (for n > 0) the transform of the distribution Fn 
of the first passage time through n. This distribution is defective unless 
p = q = |, for only in this case is s0 = 1. 
In the present case we are fortunately able to invert the transforms sx. 
It was shown in example Xlll,3(d) that (A - </P - 1 )r is (for A > 1) 
the ordinary Laplace transform of (rjx)Ir(x). Changing A into \\2c\jpq 
merely changes a scale factor, and replacing A by A + c reflects multipli- 
cation of the density by e~cx. It follows that snx (with n > 0) is the ordinary 
Laplace transform of a distribution Fn with density 
F.9) /„(') 
This is the density of the first passage time through n > 0. This fact was 
established by direct methods in 11,G.13) [and so the present argument may 
be viewed as a new derivation of the Laplace transform of x~l In(x)]. 
An explicit expression for the probabilities Pn(t) can be obtained similarly. 
In problem 8 of XIII,11 we found the Laplace transform of /n, and the 
adjustment of parameters just described leads directly to the explicit formulas 
F.10) Pn(t) = J<MTe-ctInBcJmt), « = 0, ±1,±2, .... 
Again, this result was derived by direct methods in 11,G.7). 
As we have seen in 1; XVII,7 various trunking and servicing problems 
lead to the same system of differential equations F.2) except that n is 
restricted to n > 0 and that a different equation corresponds to the 
boundary state n = 0. Two examples will show how the present method 
operates in such cases. 
Examples, (a) Single-server queues. We consider a single server in which 
newly arriving customers join the queue if the server is busy. The state of the 
system is given by the number n > 0 of customers in the queue including 
the customer being served. The interarrival times and the service" times are 
482 APPLICATIONS OF LAPLACE TRANSFORMS XIV.6 
mutually independent and have the exponential densities Xe~u and jue-^, 
respectively. This is a special case of the multi-server example (b) in 1; 
XVII,7, but we derive the differential equations afresh in order to elucidate 
the intimate connection with our present random walk model. 
Suppose that at present there are n > 1 customers in the queue. The 
next change of state will be +1 if it is due to a new arrival, and —1 if it is 
due to the termination of the present service time. The waiting time T for 
this change is the smaller of the waiting times for these two contingencies 
and so P{T > /} = e~ct where we put c = A + ju. When a change occurs 
it is +1 with probability p = A/c, and — l.with probability q = n\c. In 
other words, as long as the queue lasts our process conforms to our random 
walk model, and hence the differential equations F.2) hold for n > 1. 
However, when no service is going on, a change can be caused only by new 
arrivals, and so for n = 0 the differential equation takes on the form 
F.11) Pi(t) = -cpP0(t) + cqP.it). 
We solve these differential equations assuming that originally the server 
was free, that is P0@) = 1. For n > 1 the Laplace transforms 
again satisfy the equations F.4a), but for n = 0 we get from F.11) 
F.12) {cP+X)ttq{X) = 1 + 
As in the general random walk we get irn{K) = TTQ{X)snx for n > 1, but in 
view of F.12) 
F.13) (^ Lii 
cp + a — cqsx 
Thus 
We found that sn is the Laplace transform of the distribution Fn with 
density F.9); the factor I/A corresponds to integration, and so for n > 0 
F.14) Pn{t) + Pn+1(t) + • • ' = Fn(t) 
where Fn is the distribution with density F.9). For n = 0 the left side is, 
of course, unity. 
(b) Fluctuations during a busy period. We consider the same server, 
but only during a busy period. In other words, it is assumed that at epoch 
0 a customer arrives at the empty server, and we let the process terminate 
when the server becomes empty. Analytically this implies that n is now 
restricted to n > 1, and the initial condition is Px@) = 1. Nothing changes 
in the differential equations F.2) for n ^r 2, but in the absence of a zero 
state the term cpP0(t) drops out in equation number one. Thus the Laplace 
XIV.7 THE KOLMOGOROV DIFFERENTIAL EQUATIONS 483 
transforms -rrn{X) satisfy FAd) for n > 2 and 
F.15) (A+c)^) = 1 +cqirt(X). 
As before we get irn{X) = ^(A)^ for n > 2, but tt^X) is to be 
determined from F.15). A routine calculation shows that v\{X) = sj(cp), 
and hence nn(X) = s\J{cp). Using the preceding example we have thus the 
final result that Pn{t) = fn(t)l(cp) with /„ given by F.9). 
To ensure that the busy period has a finite duration we assume that 
p < q. Denote the duration of the busy period by T. Then P{T > /} = 
= ^@ = 2^@- Now P'{t) = — cqP^t) as can be seen summing the 
differential equations, or probabilistically as follows. Neglecting events of 
negligible probabilities the busy time terminates between / and t + h iff 
at epoch t there is only one customer in the queue and his service terminates 
within the next time interval of duration h. The two conditions have 
probabilities Px(r) and cqh + o(h), and so the density of T satisfies the 
condition —P'(t) = cqP^t). Accordingly, the duration of the busy period 
has the density 
F.16) -P'(t) = Jqjpt-1 h{2c-JJq t)e 
~ct 
This result was derived by a different method in example 4(a) and was used 
in the queuing process VI,9(e). See problem 13. > 
7. THE KOLMOGOROV DIFFERENTIAL EQUATIONS11 
We return to the Markovian processes restricted to the integers 1, 2, .... 
The Kolmogorov differential equations were derived in 1; XVII,9 and 
again in X,3. This section contains an independent treatment by means of 
Laplace transforms. To render the exposition self-contained we give a 
new derivation of the basic equations, this time in the form of convolution 
equations. 
The basic assumption is that if X(t) = / at some epoch t, the value 
X(/) will remain constant for an interval t < / < t+T whose duration 
has the exponential density cte~CiX; the probability of a jump to jis then pti. 
Given that X@) = / the probability Pik{t) that X(/) = k ^ i can now be 
11 The theory developed in this section applies without essential change to the general 
jump processes of X,3. It is a good exercise to reformulate the proofs in terms of the 
probabilities themselves without using Laplace transforms. Some elegance is lost, but the 
theory then easily generalizes to the non-stationary case where the coefficients Cj and p^ 
depend on t. In this form the theory is developed (for general jump processes) in W. Feller, 
Trans. Amer. Math. Soc, vol. 48 A940), pp. 488-515 [erratum vol. 58, p. 474]. 
For a probabilistic treatment based on the study of sample paths see Chung A967). 
For generalizations to semi-Markov processes see problem 14. 
484 APPLICATIONS OF LAPLACE TRANSFORMS XIV. 7 
calculated by summing over all possible epochs and results of the first jump: 
G.1a) Pik(t) = 2 Cc.e-^p^P^t-x) dx (k * /). 
>=a Jo 
For k = i v/ must add a term accounting for the possibility of no jump: 
G.1&) Pait) = e-'S + f [c.e-^p^P^t-x) dx 
These equations can be unified by introducing the Kronecker symbol dik 
which equals 1 or 0 according as k = i or k ^ i. 
The backward equations G.1) are our point of departure;12 given arbitrary 
Ci > 0 and a stochastic matrix p = (pik) we seek stochastic matrices 
P@ = (PM) satisfying G.1). 
Alternatively, if we suppose that any finite time interval contains only, 
finitely many jumps we can-modify the argument by considering the epoch x 
of the last jump preceding t. The probability of a jump from j to k has 
density ]T Pij(x)cjpikt while the probability of no jump between x and / 
equals e~°i(t~x). Instead of G.1) we get the forward equations 
Pik(t) = 6ike-^ + ('zP 
Jo i=i 
G.2) Pik(t) = 6ike-^ + (zPttxicp^^ dx. 
J 
As will be seen, however, there exist processes with infinitely many jumps 
satisfying the backward equations, and hence the forward equations are not 
implied by the basic assumptions underlying the process. This phenomenon 
was discussed in X,3 and also in 1; XVII,9. 
In terms of the Laplace transforms 
G.3) KM) = I™e~xt Pik(t) dt 
Jo 
the backward equations G.1) take on the form 
G.4) n_ 
A + C4 A 
Ci 
We now switch to a more convenient matrix notation. (The rules of matrix 
12 The change of variables y = t — x makes differentiation easy, and it is seen that the 
convolution equations G.1) are equivalent to the system of differential equations 
P'ik(t) - -CiPik(O + et 
together with the initial conditions />,,@) = 1 and P,fc@) = 0 for k ^ i. This system 
agrees with 1; XVII,(9.14), except that there the coefficients ct and pti depend on time, 
and hence Pik is a function of two epochs t and / rather than of the duration t — r. 
XIV.7 THE KOLMOGOROV DIFFERENTIAL EQUATIONS 485 
calculus apply equally to infinite matrices with non-negative elements.) We 
introduce the matrices U(X) = (U.ik(X)) and similarly P{t) = (Pik(t))'t 
P = (Pik)> anc* tne diagonal matrix c with elements cv By 1 we denote 
the column vector all of whose elements equal 1. The row sums of a matrix 
A are then given by Al. Finally, / is the identity matrix. 
It is then clear from G.4) that the backward equations G.1) are trans- 
formed into 
G.5) (A+c)n(A) = 
and the forward equations into 
G.6) ri(A)(A+c) = /+ri(A)cp. 
To construct the minimal solution we put recursively 
G.7) (A+c) n<0)(A) = /, (A+c) nW(A) = / + cp n<">(A). 
For the row sums of AIl<Tl)(A) we introduce the notation 
G.8) AII<B>(AI = 1 - ?<n)(A). 
Substituting into G.7) and remembering that pi = 1 it is seen that 
G.9) 
Since ?<0) ^ 0 it follows that ?<W)(A) > 0 for all n, and so the matrices 
AFI<W)(A) are substochastic. Their elements are non-decreasing functions 
of n and therefore there exists a finite limit 
G.10) n<0O)(A)=limn<n)(A) 
>00 
and An<cp)(A) is substochastic or stochastic. 
Obviously n<c0)(A) satisfies the backward equation G.5) and for any 
other non-negative solution II(A) one has trivially II(A) > n<0)(A), and 
by induction U(X) ^ n<n>(A) for all n. Thus 
G.11) n(A) > 
Less obvious is that n<00)(A) satisfies also the forward equation G.6). 
To show it we prove by induction that 
G.12) n<*>(A)(A+c) = 
This is true for n = 1. Assuming the truth of G.12), substitution into G.7) 
leads to 
G.13) (A+c)n<w+1'(A)(A+c) = A/ + c + '[/.+ cpn<"-1'(A)]cp. 
The expression within brackets equals (A+c)Il<w)(A). Premultiplication of 
486 APPLICATIONS OF LAPLACE TRANSFORMS XIV.7 
G.13) by (A+c)-1 yields G.12) with n replaced by n + 1. This relation 
is therefore true for all n, and hence IT(CO)(A) satisfies the forward equation. 
Repeating the argument that led to G.11) we see equally that any lion- 
negative solution of the forward equations G.6) satisfies II (A) > n@0)(A). 
For this reason n(co)(A) is called the minimal solution. 
We have thus proved 
Theorem 1. There exists a matrix n@0)(A) ;> 0 with row sums ^ X~x 
satisfying both G.5) and G.6) and such that for every non-negative solution 
of either G.5) or G.6) the inequality G.11) holds. 
Theorem 2. The minimal solution is the Laplace transform of a family of 
substochastic or stochastic matrices P(t) satisfying the Chapman-Kolmogorov 
equation 
G.14) P(s+t)=P(s)P(t) 
and both the backward and forward equations G.1)-G.2). Either all matrices 
P(t) and An@0)(A) (/ > 0, A > 0) are strictly stochastic or none is. 
Proof. We drop the superscript oo and write II(A) for n@0)(A). From 
the definition G.7) it is clear that n^A) is the transform of a ^positive 
function P\^ which is the convolution of finitely many exponential distri- 
butions. Because of G.8) the row sums of P(n)(/) form a monotone sequence 
bounded by 1 and so it follows that II (A) is the transform of a matrix P(t) 
which is substochastic or stochastic. From G.5)—G.6) it is clear that P(t) 
satisfies the original forward and backward equations. These, imply that 
P(t) depends continuously on /. It follows that if the fth row sum is < 1 
for some / the /th row sum of I1(A) is < A for all A and conversely. 
To restate G.14) in terms of Laplace transforms multiply if by e~Xt~"* and 
integrate over s and /. The right side leads to the matrix product II (A) II (v), 
and the left side is easily evaluated by the substitution x = t + s, y = 
= —t + s. The result is 
v — A 
conversely G.15) implies G.14). [This argument was used in example 
XIII,8(o).] 
To prove G.15) consider the matrix equation 
G.16) (A+c)Q = A + cpQ. 
If A and Q are non-negative then obviously Q ;> (A+c)-1 A = U{0)(X)A 
and by induction Q > Uln)(X)A for all n. Thus Q ^ Yl{X)A. Now U(v) 
satisfies G.16) with A = I + (A—v)Il(v) and hence for A > v 
G.17) n(v) > n(A) 
XIV.7 THE KOLMOGOROV DIFFERENTIAL EQUATIONS 487 
On the other hand, the right-hand member satisfies the forward equation 
G.6) with A replaced by v. It follows that it is > U(v) and thus the 
equality sign holds in G.17). This concludes the proof.13 > 
To see whether the matrix AI1(OO)(A) is strictly stochastic14 we return to 
the relations G.8) and G.9). Since the elements ^nl(X) are non-increasing 
functions of n there exists a limit ?(A) = lim ?(n)(A) such that 
G.18) An(oo)(A)l = 1 - 
and 
G.19) (A+c)?(A) = cp^(A), 0 <> ?(A) <, 1. 
On the other hand, we have 
G.20) (A+c)?@>(A) = cl = cpl 
and therefore ?@)(A) ^ ?(A).for any vector ?(A) satisfying G.19). From 
G.9) it follows by induction that ?(n)(A) > ?(A) for all n, and so the vector 
?(A) in G.18) represents the maximal vector satisfying G.19). We have thus 
Theorem 3. The row defects of the minimal solution are represented by 
the well-defined maximal vector ?(A) satisfying G.19). 
Thus An@0)(A) is strictly stochastic iff G.19) implies ?(A) = 0. 
Corollary 1. If ct ^ M < oo for all i the minimal solution is strictly 
stochastic (so that neither the forward nor the backward equations possess 
other admissible solutions). 
Proof. Since c/(A-j-c) is an increasing function of c it follows from G.19) 
by induction that 
for all n, and hence ?(A) = 0. 
If A (A) is a matrix of elements of the form ?;(A) rjk(X) with arbitrary rjk(X) then 
+ A(X) is again a solution of the backward equation G.5). It is always possible 
to choose A (A) so as to obtain admissible matrices P(t) satisfying the Chapman- 
Kolmogorov equation. The procedure is illustrated in the next section. The corresponding 
13 G.15) is the resolvent equation for the family of contractions AIT(A) on the Banach 
space of bounded column vectors. We saw in XIII,10 that it holds iff the range of these 
transformations is independent of A, and minimal character guarantees this. (In terms 
of boundary theory the range is characterized by the vanishing of the vectors at the "active 
exit boundary.") 
14 Warning: A formal multiplication of the forward equations by the column vector 1 
would seem to lead to the identity AI1(AI = 1, but the series involved may diverge. 
The procedure is legitimate if the cf are bounded (corollary 1). 
488 APPLICATIONS OF LAPLACE TRANSFORMS XIV.8 
processes are characterized by transitions involving infinitely many iumps in a finite time 
interval. Curiously enough, the forward equations may be satisfied even though their 
interpretation in terms of a last jump is false. 
These are the main results. We conclude with a criterion that is useful 
in applications and interesting because its proof introduces notions of 
potential theory; the kernel F of G.25) is a typical potential. 
We assume c{ > 0 and rewrite G.19) in the form 
G.22) 
Multiplying by p* and adding over k = 0,. .. ,n— 1 we get 
G.23) ?(A) + X s 
*=o 
This implies that pn?(A) depends monotonically on n and so pn?(A) 
where x is the minimal column vector satisfying15 
G.24) px = x, 
Now define a matrix (with possibly infinite elements) by 
00 
,-1 
G.25) r= 
*=1 
Letting n -> oo in G.23) we get 
G.26) ?(A) + AF?(A) = x, 
which implies in particular that ?k(X) = 0 for each k such that F^ = oo. 
This is the case if A: is a persistent state for the Markov chain with matrix 
p and hence we have 
Corollary 2. The minimal solution is strictly stochastic (and hence unique) 
whenever the discrete Markov chain with matrix p has only persistent states. 
8. EXAMPLE: THE PURE BIRTH PROCESS 
Instead of pursuing the general theory we consider in detail processes in 
which only transitions /—*¦/+ 1 are possible, for they furnish good 
illustrations for the types of processes arising from non-uniqueness. To 
avoid trivalities we suppose ct > 0 for all /. By definition piti+i = 1 
whence pik = 0 for all other combinations. The backward and forward 
equations now reduce to 
(8.D 
15 It is not difficult to see that x is independent of A and AII(oo)(A) — z — 
XIV.8 example: the pure birth process 489 
and 
(8.2) (A+c4)ni4(A) - c^n, M(A) = dik, 
where 6^. equals 1 for i = k and 0 otherwise. We put for abbreviation 
ct + A ct + A 
p{ is the Laplace transform of the (exponential) sojourn time distribution 
at z, and r.t is the ordinary Laplace transform of the probability that this 
sojourn time extends beyond /. The dependence of rs and py on A should 
be borne in mind. 
(a) The minimal solution. It is easily verified that 
PiPi+i''' Pk-xrk f°r k > i 
(8.4) nifc(A)= * 
0 for k < i 
is the minimal solution for both (8.1) and (8.2). It reflects the fact that 
transitions from / to k < z are impossible, and thaf the epoch of the 
arrival at k > z is the sum of the k independent sojourn times at z', z+1,.. . , 
k-\. 
Let Pik(t) stand for the transition probabilities of the process defined by 
(8.4). We prove the following important result derived by other methods in 
1; XVII,4. 
Lemma. If 
(8.5) 
then 
(8.6) 
for all i and t > 0. Otherwise (8.6) is false for all i. 
Proof. Note that Xrk = 1 — pk, whence 
(8.7) A[IUA) + • • • + nM+n(*A)] = 1 - Pi - • • Pi+n. 
Thus (8.6) holds iff for all A > 0 
(8.8) PiPi+1 •••/>„ — 0 
Now if cn -»¦ oo then pn ^* e^6" and Jience in this case (8.5) is necessary 
and sufficient for (8.8). On the other hand, if cn does not tend to infinity 
there exists a number q < 1 such that pn < q for infinitely many n, and 
hence both (8.8) and (8.5) hold. > 
490 APPLICATIONS OF LAPLACE TRANSFORMS XIV. 8 
In the case where the series in (8.5) diverges there are no surprises: the 
cn determine uniquely a birth process satisfying the basic postulates from 
which we started. From now on we assume therefore that 2 c~n1 < °°- 
The defect 1 — StPifc(/) is the probability that by epoch / the system 
has passed through all states, or has "arrived at the boundary oo." The epoch 
of the arrival is the sum of the sojourn times at z, z-j-1,. . .. The series 
converges with probability one because the sum of the mean sojourn times 
l/cn converges. 
In a process starting at i the lifetime of the process up to the epoch of the 
arrival at oo has the Laplace transform 
(8-9) ft = lim PiPi+1 • • • Pi+n 
n->oo 
and the ft satisfy the equations G.19), namely, 
(8-10) (A+ct)ft = c,ft+1. 
For the row sums we get from (8.7) 
(8.11) A X IIiJfc(A) = 1 - ft. 
k-i 
(b) Return processes. Starting from the process (8.4) new processes may 
be defined as follows. Choose numbers qi such that qi ^> 0, ^qt *= 1. 
We stipulate that on arrival at oo with probability1* qt the state of the system 
passes instantaneously to i. The original process now starts afresh until a 
second arrival at oo takes place. The time elapsed between the two arrivals 
at oo is a random variable with I-aplace transform 
(8.12) r(A)«2^. 
The Markovian character of the process requires that oh the second arrival 
at oo the process recommences in the same manner. We mew describe the 
transition probabilities /*?*(/) of the new process in terms of its Laplace 
transforms II^Aj. The probability of a transition from i at epoch 0 to 
k at epoch t without an intervening passage through oo has the transform 
(8.4). The probability to reach k after exactly one passage through oo has 
therefore the Laplace transform ft 2* ftllftCA)* and the epoch of the second 
arrival at oo has transform ftr(A). Considering further returns we see in 
this way that we must have 
(8.13) 
1 - t(A) 
16 Variants of the return processes are obtained by letting S?t < 1; on arrival at 
the process terminates with probability 1—2 qt. 
XIV. 9 ERGODIC LIMITS AND FIRST-PASSAGE TIMES 491 
where [1—rtf)]-1 = 2 t"(X) counts the number of passages through oo. 
A trite calculation using (8.11) shows that the row sums in (8.13) equal I/A, 
and so the n™*(A) -are the transforms of a strictly stochastic matrix of 
transition probabilities Pnt(t). 
It is easily verified that the new process satisfies the backward equations 
(8.1) but not the forward equations (8.2). This is as should be: the postulates 
leading to the forward equations are violated since no last jump need exist. 
(c) The bilateral birth process. To obtain a process satisfying 1x>th the 
forward and the backward equations we modify the birth process by letting 
the states of the system run through 0, ± 1, ±2,.... Otherwise the con- 
ventions remain the same: the constants ci > 0 are defined for all integers, 
and transitions from i are possible only to / + 1. We assume again that 
V \\cn < oo, the summation now extending from — oo to oo. 
Nothing changes for the minimal solution which is still given by (8.4). 
The limit 
(814) r\k = lim Wik{X) = rkPk_lPk_2pk_3 • ¦ ¦ 
i-*—ao 
exists and may be interpreted as the transform of "the probability P_ooifc(/) 
of a transition from — oo at epoch 0 to k at epoch /." With this starting 
point the process will run through all states from — oo to oo and "arrive at 
oo" at a*i epoch with Laplace transform ?_«, = lim ?n. We now define 
n—>— oo 
a new process as follows. It starts as the process corresponding to the 
minimal solution (8.4) but on reaching oo it recommences at — oo, and in this 
way the process continues forever. By the construction used in (b) we get 
for the transition probabilities 
(8.15) n?k(k) = Uik(X) + *iVk . 
1 - *.„ 
It is easily verified that the Ufk satisfy both the backward and the forward 
equations (8.1) and (8.2). The process satisfies the hypotheses leading to the 
backward equations, but not those for the forward equations. 
9. CALCULATION OF ERGODIC LIMITS AND OF 
FIRST-PASSAGE TIMES 
As can be. expected, the behavior as t —*¦ oo of the transition probabilities 
Pij(t) of Markov processes on integers is similar to that of higher transition 
probabilities in discrete chains with the pleasing simplification, however, 
that the nuisance of periodic chains disappears. Theorem 1 establishes this 
fact as a simple consequence of the ergodic theorem of 1; XV. Our main 
concern will then be to calculate the limits for the general processes of section 
492 APPLICATIONS OF LAPLACE TRANSFORMS XIV.9 
7 and to show how first passage times can be found. The methods used are of 
wide applicability. 
Theorem 1. Suppose that for the family of stochastic matrices P(f) 
(9.1) P(s+t) = P(s)P(t) 
and P(t)-+I as /-*-0. If no Pik vanishes17 identically, then as /-*-oo 
(9.2) Pik(t)-+uk 
where either uk = 0 for all k or else 
C9.3) uk > 0, J uk = 1, 
and 
(9-4) Iu,Pik(t) = uk. 
i - 
The second alternative occurs whenever there exists a probability vector 
{ux, u2,. . .) satisfying (9.4) for some t > 0. In this case (9.4) holds for all 
t > 0, and the probability vector u is unique. 
(As expfained in 1; XVII,6 the impdrtant feature is that the limits do 
not depend on /, which indicates that the influence of the initial conditions 
is asymptotically negligible.) . 
Proof. For a fixed E > 0 consider the discrete Markov chain with 
matrix PF) and higher transition probabilities given by P"(d) = P(n6). 
If all elements Pik(n6) are ultimately positive the chain is irreducible and 
aperiodic, and by the ergodic theorem 1; XV.,7 the assertions are true 
for / restricted to the sequence 6, 26,36,.... Since two rationals have 
infinitely many multiples in common the limit as n -*¦ oo of Pik(n6) is the 
same for all rational 6. To finish the proof it suffices to show that Pik(t) 
is a uniformly continuous function of / and is positive for large /. Now by 
(9.1) 
(9.5) Pu{s) Pik(t) <, Pik(s+t) <, Pik{t) + [1 - Pti(s)] 
[the first inequality is trivial, the second follows from the fact tjiat the terms 
Pit(s) with j 9* i add up to 1— Pit(s)]. For s sufficiently small we have 
1 — € <, Pu(s) <, 1 and so (9.5) shows the uniform continuity of Pik. It 
follows froin (9.5) also that if Ptt(/) > 0 then Pik(t+s) > 0 in some 
5-interval of fixed length and hence Ptt is either identically zero or ultimately 
positive. > 
17 This condition is introduped only to avoid trivialities that may be circumvented by 
restrictions to appropriate sets of states. It is not difficult to see that our conditions imply 
strict positivity of Pik(t) for all /. 
XIV.9 ERGODIC LIMITS AND FIRST-PASSAGE TIMES 493 
We now apply this result to the minimal solution of section 7 assuming 
that it is strictly stochastic, and hence unique. In matrix notation (9.4) reads 
uP(t) = u; for the corresponding ordinary Laplace transform this implies 
(9.6) . uXU(X)^u. 
If a vector u satisfies (9.6) for some particular value X > 0 the resolvent 
equation G.15) entails the truth of (9.6) for all X > 0, and hence the 
truth of (9.4) for all / > 0. Introducing (9.6) into the forward equation 
G.6) we get 
(9.7) «cp = we; 
the components ukck are finite though possibly unbounded. On the other 
hand, if u is a probability vector satisfying (9.7) it follows by induction 
from G.12) that uXIHn>(X) ^ u for all n, and hence uXU(X) <> u. But the 
matrix XU (X) being strictly stochastic the sums of the components on either 
side must be equal and hence (9.6) is true. We have thus 
Theorem 2. If the minimal solution is strictly stochastic {and hence wiique) 
the relations (9.2) hold with uk > 0 iff there exists a probability vector u 
such that (9.7) holds. 
This implies in particular that the solution u of (9.7) is unique. 
Probabilistic interpretation. To fix ideas consider the simplest case where 
the discrete chain with transition probabilities ptj is ergodic. In other words, 
we assume that there exists a strictly positive probability vector a = 
= (ax, a2, . . .) such that ap = a and p^} -*- a.k as n ->- oo. It is then clear 
that,// a = ^cf.kc~l < uj, ~'the probability vector with components 
uk =¦ ct.kc~xlo satisfies (9.7) whereas no solution exists if a = oo. 
Now it is intuitively obvious that the transitions in our process are +he 
same as in the discrete Markov chain with matrix p, but their timing is 
different. For an orientation consider a particular state and label it with the 
index 0. The successive sojourn times at 0 alternate with off times during 
which the system is at states j > 0. The number of visits to the state j is 
regulated by p, their duration depends on c,. In the discrete Markov chain 
the long-run frequencies of j and 0 are in the ratio a;/a0 and hence clJccq 
should be the expected number of visits to j during an off interval. The 
expected duration of each visit being 1/c, we conclude that in the long run 
the prohabiHties of the states j and 0 should stand in the proportion 
11 oi Uj-.Uq. 
This argument can be made rigorous even in the case where /»,-,-(/)->0. According to 
a theorem of C.Derman mentioned in 1; XV,11, if p induces an irreducible and persistent 
chain there exists a vector a such that ap = a and a is unique up to a multiplicative 
constant; here ctk > 0, but the series ^ aA may diverge. Even in this case the ratio 
494 APPLICATIONS OF LAPLACE TRANSFORMS XIV.9 
ayrocj, have the relative frequency interpretation given above and the argument holds 
generally. If X**0*1 ^ °° t^ien (9-2)-(9.4) are true with uk proportional to oc^1, and 
otherwise P(t) -*-0 as t —*¦ co. The interesting feature is that the limits uk may be positive 
even if the discrete chain has only null states. 
The existence of the limits P<fc(oo) can be obtained also by a renewal 
argument intimately connected with the recurrence times. 'To show how the 
distribution of recurrence and first passage times may be calculated we number 
the states 0, 1, 2,... aad use 0 as pivotal state. Consider a new process 
which coincides with the original process up to the random epoch of the first 
visit to 0 but with the state fixed at 0 forever after. In other words, the new 
process is obtained from the old one by making 0 an absorbing state. Denote 
the transition probabilities of the modified process by °Pik(t). Then 
°Poo(O = 1. In terms of the original process °Pi0(t) is the probability of a 
first passage from i ^ 0 to 0 before epoch t, and °Pik(t) gives the prob- 
ability of a transition from iVO to k ^ 0 without intermediate passage 
through 0. It is probabilistically clear that the matrix °P(t) should satisfy 
the same backward and forward equations as P(/) except that c0 is replaced 
byO. We now proceed the inverse way: we modify the backward and forward 
equations by changing c0 to 0 and show that the unique solution of this 
absorbing-state process has the predicted properties. 
If | is the vector represented by the zeroth column of 11D), the backward 
equations show that the vector 
(9.8) a+c-cp)? = r] 
has components 1, 0, 0,.... Now the backward equations for °Ti(X) are 
obtained on replacing cQ by 0, and so it ? stands for the zeroth column of 
°U(X) the vector (9.8) has components r\x = rj2 = • • • = 0, but r]0 = p 5* 0. 
It follows that the vector with components tjk = nA0(A) — />°nfc0(A) satisfies 
(9.8) with ?7 =^ 0, and as XU(X) is strictly stochastic this implies ?* = 0 
for all k (theorem 3 of section 7). Since °U00(X) = 1JA we have therefore 
for k ^ 0 
(9.9) ru*) = x>nM{xynw{X). 
Referring to the first equation in (9.8) we see also that 
+ ^ 
+ ^ 
X + c0 X + c0 
(9.9) and (9.10) are renewal equations with obvious probabilistic interpreta- 
tion. In fact, let the process start at k > 0. Then °Uk0 is the ordinary 
Laplace transform of the probability 0PM(/) that the first entry to 0 occurs 
before /, and hence A°I1A;O(A) is the Laplace transform of the distribution 
Fk of the epoch of first entry to 0. Thus (9.9) states that P*^/) is. the con- 
volution of Fk and Poo; the event X(/) = 0 takes place iff the first entry 
XIV. 10 PROBLEMS FOR SOLUTION 495 
occurs at some epoch x < / and / — x time units later the system is again 
atO. 
Similarly, ^,poj^°Ti.iO represents the distribution Fo of an off time, that 
is, the interval between two consecutive sojourn times at 0. The factor 
of noo(A) on the right in (9.10) therefore represents the waiting time for a 
first return to 0 if the system is initially at 0. (This is also the distribution 
of a complete period = sojourn time plus off time.) The renewal equation 
(9.10) expresses P00(t) as the sum of the probability that the sojourn time 
at 0 extends beyond / and the probability of X(/) = 0 after a first return 
at epoch x < /. If 0 is persistent (9.10) implies by the renewal theorem that 
(9.11) P00(oo) 
where /z is the expected duration of an off time and c~l + ju is the expected 
duration of a complete cycle. 
16. PROBLEMS FOR SOLUTION 
1. In the renewal equation A.3) let F\t) =g(j) = e^t^/Tip). Then 
do..) L 
By the method of partial fractions show that for integral18 p 
A0.2) v{t) - I 
P 
where ak = ?-**/» and 1* =» — 1. 
2. A server has Poisson incoming traffic with parameter a and a holding time 
distribution G with Laplace transform y. Let H(j) be the probability that 
the duration of a holding time does not exceed / and that no new call arrives during 
it. Show that H is a defective distribution with Laplace-Stiettjes transform 
(*) 
3. L&st calls. Suppose that the server of the preceding example is free at epoch 
0. Denote by U(j) the probability that up to epoch / all arriving calls find the 
server free. Derive a renewal equation for U and conclude that the ordinary Laplace 
transform to of U satisfies the linear equation 
The expected waiting time for the first call arriving during a busy period is 
a + a"i[l -y 
18 The roots of the denominator are the same for p — n and p — nj2, but the solutions 
are entirely different. This shows that the popular "expansion according to the roots of the 
denominator" requires caution when tp is an irrational function. 
496 APPLICATIONS OF LAPLACE TRANSFORMS XIV. 10 
Hint: Use the preceding problem. 
4. (Continuation.) Solve the preceding problem by the method described in 
problem 17 of VI, 13 considering U as the distribution of the total lifetime of a 
delayed terminating renewal process. 
5. If F has expectation fi and variance a2 and if c/j < 1, the solution of the 
busy-period equation D.1) has variance (a2 +c/«3)/(l — en). 
6. In example 4(b) the generating function of the total numbers of cars delayed 
is g^W*)-1] where 
A0.3) y>(s) = s<p(c - cy>(s)). 
7. Show that if <p is the Laplace transform of a proper distribution the solution 
V of A0.3) is the generating function of a possibly defective distribution. The 
latter is proper iff F has an expectation /x < 1/c. 
8. In the absorbing barrier process of example 5{a) denote by F(t, x)> the 
probability that (starting from x) absorption will take place within time t. (Thus 
F stands for the distribution of the total lifetime of the process.) Show that the 
ordinary Laplace transform of 1 —F is given by the integral of K&bs(x, y) over 
0 <jy < oo. Conclude that the Laplace-Stieltjes transform of F is given by 
e~V2i. x^ jn agreement with the fact that it must satisfy the differential equation E.9). 
9. Starting from E.7) show that the Green function of the general diffusion 
equation E.19) in any interval is necessarily of the form 
W(y) 
H0.4) Kx(x,y) 
where f x an(^ Vz are solutions of the homogeneous equation 
(*) \<p - \a<p' - by' = 0 
bounded, respectively, at the left and the right boundary. If (*) has no bounded 
solution then §x and r\x are determined up to arbitrary multiplicative constants 
which can be absorbed in W. (Otherwise appropriate boundary conditions must 
be imposed.) 
Show that cox defined by A0.4) and E.5) satisfies the differential equation 
E.19) iff W is the Wronskian 
A0.5) W(y) = U ^ 
The solutions f A and r\x are necessarily monotonic, and hence W(y) j* 0. 
10. Continuation. For x < y the first-passage epoch from x to y has the 
Laplace transform $x(x)l$x(y). For x > y it is given by rjx(x)lr)x(y). 
11. Show that the method described in section 5 for diffusion processes applies 
equally to. a general birth-and-death process}9 
12. Adjust example 6(a) to the case of a > 1 channels. {Explicit calculations 
of the a constants are messy and not recommended.) 
19 For details and boundary conditions see W. Feller, The birth and death process as 
diffusion process, Journal Mathematiques Pures Appliquees, vol. 38 A959), pp. 301-345. 
XIV. 10 PROBLEMS FOR SOLUTION 497 
13. In example 6(b) show directly from the differential equations that the busy 
time has expectation -: - and variance 
v c(q-p) 
14. Semi-Markov processes. A semi-Markov process on 1,2,... differs from 
a Markov process in that the sojourn times may depend on the terminal state- 
given that the state / was entered at r the probability that the sojourn time ends 
before r + t by a jump to A: is File(t). Then ^k Fik(t) gives the distribution of 
the sojourn time and pik == Fik(co) is the probability of a jump to k. Denote by 
Pik(t) the probability of \k at epoch t + t given that / was entered at epoch t. 
Derive an analogue to the Kolmogorov backward equations. With self-explanatory 
notations the transformed version is given by 
where y(X) is the diagonal matrix with elements [1 — ^k <pik(X)]IA. For 
Fik(t) =/>ijfc(l — e~eit) this reduces to the backward equations G.5). The con- 
struction of the minimal solution of section 7 goes through.20 
20 For details see W. Feller, On semi-Markov processes, Proc. National Acad. of Sciences, 
vol. 51 A964) pp. 653-659. Semi-Markov processes were introduced by P. Levy and W. L. 
Smith, and were investigated in particular by R. Pyke. 
CHAPTER XV 
Characteristic Functions 
This chapter develops the elements of the theory of characteristic functions 
and is entirely independent of chapters VI, VII, IX-XIV. A refined Fourier 
analysis is deferred to chapter XIX. 
1. DEFINITION. BASIC PROPERTIES 
The generating function of a non-negative integral-valued random vari- 
able X is the function defined for 0 < s <; 1 by E^), the expectation of 
sx. As was shown in chapter XIII, the change of variable s — e~x makes 
this useful tool available for the study of arbitrary non-negative random 
variables. The usefulness of these transforms derives largely from the multi- 
plicative property s**" = sxsv and e~X{x+v) = e-Xxe~Xv. Now this property 
is shared by the exponential function with a purely imaginary argument, 
that is, by the function defined for real x by 
A.1) e*x =s cos t,x -f i sin t,x 
where ? is a real constant and i2 = —1. This function being bounded, 
its expectation exists under any circumstances. The use of E(e'fX) as a 
substitute for generating functions provides a powerful and universally 
applicable tool, but it is bought at the price of introducing complex-valued 
functions and random variables. Note, however, that the independent 
variable remains restricted to the real line, (or, later on, to &*")• 
By a complex-valued function w = u + iv is meant the pair of real 
functions u and v defined for real x. The expectation E(w) is merely an 
abbreviation for E(«) + z'E(y). We write, as usual, w = u — iv for the 
conjugate function, and |w| for the absolute value (that is, |w|2 = ww = 
= u2 -f v2). The elementary properties of expectation remain valid, and 
only the mean value theorem requires comment: */ |w| <? a then |E(w>)| <, a. 
In fact, by Schwarz inequality 
A.2) • |E0v)|2 = (E(u)J + (E(v)J < ECi/2) + E0>2) = E(|w|2) < a*. 
498 
XV. 1 DEFINITION. BASIC PROPERTIES 499 
Two complex-valued random variables W, = U, + i\j are called 
independent iff the pairs (Ul5 VJ and (U2,V2) are independent. That the 
multiplicative property E(W1W2) = EOVJ ECWa) holds as usual is seen 
by decomposition into real and imaginary parts. (This formula illustrates 
the advantage of the complex notation.) With these preparations we define 
an analogue to generating functions as follows. 
Definition. Let X be a random variable with probability distribution F. 
The characteristic Junction of F (or of X) is the function <p defined for real 
C by 
A.3) <p(Q « P V- F{dx} - u@ + iv(Q 
J—00 
where 
cos ix ¦ F{dz}, v@ = sin ?s • F{dx). 
— 00 J— 00 
For distributions F with a xiensity /, of course, 
Jr+oo 
I ««./(*) dx. 
—oo 
Terniaological note. In the accepted terminology of Fourier analysis <p 
is the Fourier-Stieltjes transform of F. Such transforms are defined for all 
bounded measures and the term "characteristic function*' emphasizes that 
the measure has unit mass. (No other measures have characteristic functions.) 
On the other hand, integrals of the form (l.S) occur in many connections 
and we shall say that A.5) defines the ordinary Fourier transform of /. The 
characteristic (Unction of F is the ordinary Fourier transform of the density 
/ (when the latter exists), but the term Fourier transform applies also to 
other functions. > 
For ease of reference we list some basic properties of characteristic 
functions. 
LtwMia 1. Let ip a* u + iv be the characteristic function of a random 
variable X with distribution F. Then 
(a) <p is continuous. 
(b) f@) - 1 and \<p(Q\ ? 1 for all {. 
(c) oX 4- b has the characteristic function 
A.6) E 
In particular, <p = u — iv is the characteristic function of —X. 
(d) u is even and v is odd. The characteristic function is real iff F is 
symmetric. 
500 CHARACTERISTIC FUNCTIONS XV. 1 
(<?) For all ? 
A.7) 0<l -wB0<4(l-w@). 
(For variants see problems 1-3.) 
Proof, (a) Note that |<?i?x| = 1 and hence 
A.8) |ew*+*> _ e<c*| =1^-11. 
The right side is independent of x and is arbitarily small for h sufficiently 
close to 0. Thus <p is, in fact, uniformly continuous. Property (b) is 
obvious from the mean, value theorem, and (c) requires no comment. For 
the proof of (d) we anticipate the fact that distinct.distributjons have distinct 
characteristic functions. Now <p is real iff <p = y, that is, if X and —X 
have the same characteristic function. But then X and —X have the same 
distribution, and so F is symmetric. Finally, to prove (e) consider the 
elementary trigonometric relation 
A.9) 1 - cos 2?r =* 2A -cos2 ?a?) < 4A -cos ?x) 
valid because 0 < 1 + cos tpc < 2. Taking expectations we get A.7). > 
Consider now two random variables Xu X2 with distributions Flt F2 and 
characteristic functions <plt <p2. If Xx and X2 are independent, the multi- 
plicative property of the exponential entails 
A.10) E 
This simple result is used frequently and we record it therefore as 
Lemma 2. The convolution Fx *k F2 has the characteristic function q>i<p2. 
In other words: to the sum Xx + X2 of two independent random variables 
there corresponds the product <fi(p2 of their characteristic functions.1 
If X2 has the same distribution as Xlt then the sum Xx — X2 represents 
the symmetrized variable (see V,5). We have therefore the 
Corollary. \<p\2 is the characteristic function of the symmetrized distri- 
bution °F. 
The following lemma gives a characterization of arithmetic distributions. 
Lemma 3. If X 5^ 0 the following three statements are equivalent: 
(b) (p has period A, that is <p{t+nX) = <p(?) for all I and n. 
1 The converse is false, for it was shown in II,4(c) and again in problem 1 of 111,9 that 
in some exceptional cases the sum of,two dependent variables may have the distribution 
F1 -k F2, and consequently the characteristic function <Pi<p2. 
XV. 1 DEFINITION. BASIC PROPERTIES 501 
(c) All points of increase ofF are among 0, ±h, ±2h, . . . where 
h = fk 
Proof. If (c) is true and F attributes weight pn to nh then 
This function has period 2tt/A, and so (c) implies (b), which in turn is 
stronger than (a). 
Conversely, if (a) holds, the expectation of the non-negative function 
1 — cos Xx vanishes, and this is possible only if 1 — cos he = 0 at every 
point x that is a point of increase for F. Thus F is concentrated on the 
multiples of 2tt/X, and hence (c) is true. 
Technically this lemma covers the extreme case of a distribution F 
concentrated at the origin. Then <p(?) = 1 for all ?, and so every number 
is a period of op. In general, if X is a period of 99 the same is true of all 
multiples ± X, ±2A, . . . , but for a non-constant periodic function y 
there exists a smallest positive period, and this is called the true period. 
Similarly, for an arithmetic F there exists a largest positive h for which 
property (c) holds, and this is called the.span of F. It follows from lemma 
3 that the span h and the period X are related by Xh — 2tt. Thus unless 
either <p(?) # 1 for all ? # 0, or <p(?) = 1 identically, there exists a small- 
est X > 0 such that ^(A) = 1 but <p(Q #1 for 0< ? < A. 
All this can be restated in a form of more general appearance. Instead 
of (p(X) = 1 assume only that \<p(X)\ = 1. There exists then a real b such 
that (p(X) = eiW, and we can apply the preceding result to the variable 
X — b with characteristic function <p(C)e~ibX which equals 1 at ? = X. 
Every period of this characteristic function is automatically a period of l^l, 
and we have thus proved 
Lemma 4. There exist only the following three possibilities: 
(a) |<KOI<1 for all ?#0. 
(b) \<P(%)\ = * and l<K0l < 1 /or 0 < ? < A. In this case \tp\ has period 
X and there exists a real number b such that F(x+b) is arithmetic with span 
h = 2tt/X. 
(c) |<K?)I = ! for alt ?• In tnJS case <P(Q — eihi and F is concentrated 
at the point b. 
Example. Let F be concentrated on 0 and 1 attributing probability 
\ to each. Then F is arithmetic with span 1, and its characteristic function 
9?(?) = (l+ea)f2 has period 2tt. The distribution F(x+\) is concentrated 
on ±|. It has span \ and its characteristic function cos ?/2 has period 
4tt. > 
502 CHARACTERISTIC FUNCTIONS XV.2 
2. SPECIAL DISTRIBUTIONS. MIXTURES 
For ease of reference we give a table of the characteristic functions of ten 
common densities and describe the method of deriving them. 
Notes to table 1. (I) Normal density. If one is not afraid of complex 
integration the result is obvious by the substitution y = x — /?. To prove 
the formula in the real domain use differentiation and integration by parts to 
obtain <p'(Q=-t<p(Q. Since <p@) = 1 it follows that log <p(Q = -K2, 
as asserted. 
B)-C) Uniform densities. The calculation in B) and C) is obvious. 
The two distributions differ only by location parameters, and the relation 
between the characteristic functions illustrates the rule A:6). 
D) Triangular density. Direct calculation is easy using integration by 
parts. Alternatively, observe that our triangular density is the convolution 
of the uniform density in — \a < x < \a with itself and in view of C) its 
characteristic function is therefore (— • sin ---I. 
E) This is obtained by application of the inversion formula C.5) to the 
triangular density D). See also problem 4.. This formula is of great im- 
portance because many Fourier-analytic proofs depend on the use of a 
characteristic function vanishing outside a finite interval. 
F) Gamma densities. Use the substituti6n y = x(l —/?)• If one prefers to 
stay in the real domain expand e** into a power series. For the characteristic 
function one gets in this way 
f 
«-• n! 1X0 
which is the binomial series for A—/?)-'. For the special case t= 1 
(exponential distribution) the calculation can be performed in the real by 
repeated integration by parts. The same is true (by recursion) for all integral 
values of t. 
G) The bilateral exponential is obtained by symmetrizatioa from the 
exponential distribution, and so the characteristic function follows from 
F) with t = 1. A direct verification is easy using repeated integrations by 
parts. 
(8) Cauchy distribution. Again the formula follows from the preceding 
one by the use of the inversion formula 0-5). The direct verification of this 
formula is a standard exercise in the calculus of residues. 
(9) Bessel density. This is the Fourier version of the Laplace transform 
derived in XI 11,3 (</), and may be proved in the same way. 
A0) Hyperbolic cosine. The corresponding distribution function is 
F(x) = 1 — 2tt~x arc tan e~x. Formula 10 is of no importance, but it has a 
XV.2 
SPECIAL DISTRIBUTIONS. MIXTURES 
503 
Table 1 
No. 
Name 
Density 
Interval 
Characteristic Function 
1 
2 
3 
4 
6 
7 
8 
10 
Normal 
Uniform 
Uniform 
Triangular 
Gamma 
Bilateral 
exponential 
Cauchy 
Bessel 
Hyperbolic 
cosine0 
V27T 
1 
a 
1 
la 
a 
1 1 — cos ax 
it ax* 
T(t) 
~* I It(x) 
X 
1 
¦n cosh a; 
— oo < x < oo 
0 < a; < a 
1*1 <a 
1*1 <fl 
— oo < a; < oo 
* > 0, t > 0 
— oo < a; < oo 
— oo < a; < oo 
t >0 
a; > 0, r > 0 
— oo < a; < oo 
. "*. 
r*1 
_ 1 
2 
sina? 
1 — cos at, 
- for K| ^a 
a 
0 for U| >a 
1 
- ay 
r+T2 
1 
cosh (wC/2) 
a cosh x = b(e* +e~x). 
curiosity value in that it exhibits a "self-reciprocalpair": the density and 
its characteristic function differ only by scale parameters. (The normal 
density is the prime example for this phenomenon.) To calculate the 
characteristic function expand the density into the geometric series 
i 
Applying number 7 to the individual term one gets the canonical partial 
fraction expansion for the characteristic function. &> 
(For further examples see problems 5-3.) 
504 
CHARACTERISTIC FUNCTIONS 
XV.2 
Returning to the general theory, we give a method of constructing new 
characteristic functions out of given ones. The principle is extremely 
simple, but example (b) will show that it can be exploited to avoid lengthy 
calculations. 
Lemma. Let F0,Flt... be probability distributions with characteristic 
functions <p0, <plt. . . . If pk ;> 0 and 2/>* = * the mixture 
B.1) V^lPtF, 
is a probability distribution with characteristic function 
B.2) co = J Pk<pk. 
Examples, (a) Random sums. Let Xlt X2,... be independent random 
variables with a common distribution F and characteristic function q>. 
Let N be an integral-valued random variable with generating function 
P(s) = X^*5* and independent of the Xy. The random sum Xx -f • • • -f XN 
has then the distribution B.1) with Fk — Fk*, and the corresponding 
characteristic function is 
B.3) co@ - ?(?($)- 
The most noteworthy special case is that of the compound Poisson dis- 
tribution. Here pk = e-'tkjk\ and 
B.4) a>(?) = e-'+"(c). 
The ordinary Poisson distribution represents the special case where F is 
concentrated at the point 1, that is, when ?>(?) = €&. 
a\ at 
Figure 1. Illustrating example 
XV.2a SOME UNEXPECTED PHENOMEN 505 
(b) Convex polygons. From number 5 in table 1 we know that 
1 — |?| for |?| < 1 
B.5) <p@ = 
0 for |?| ? 1 
is a characteristic function. If alt... , an are arbitrary positive numbers, 
the mixture 
B.6) 
is an even characteristic function whose graph in 0, oo is a convex polygon 
(fig. 1). In fact, without loss of generality assume ax < a2 < • • • < an. 
In the interval 0 < ? < ax the graph of eo is a segment of a line with slope 
(Pi Pn\ 
1- • •¦* -\ )• Between ax and a2 the term px\ax drops out, and so 
al an' 
on, until between.an_x and an the graph coincides with a segment of slope 
—pjan. In 0, oo the graph is therefore a polygon consisting of n finite 
segments with decreasing slopes and the segment an, oo of the ?-axis. It is 
easily seen that every polygon with these properties may be represented in 
the form B.6) (the n sides intercepting the co-axis at the points pn, 
Pn+Pn-u • • • >/*n+ ''' +/*i = !)• We conclude that every even function 
eo ^ 0 with co@) — 1 whose graph in 0, oo is a convex polygon is a 
characteristic function. 
A simple passage ,to the limit will lead to the famous Polya criterion 
[example 3(b)] and reveals its natural source. Even the present special 
criterion leads to surprising and noteworthy results. 
2a. SOME UNEXPECTED PHENOMENA 
We digress somewhat to introduce certain special types of characteristic functions with 
surprising and interesting properties. We begin by a preliminaty remark concerning 
arithmetic distributions. 
Suppose that the distribution G is concentrated on the multiples rtn/L of some fixed 
number tt/L > 0, the point nir/L carrying probability pn; here n = 0, ± 1 The 
characteristic function y is given by 
+ 00 
and has period 2L. It is usually not easy to find simple explicit expressions for y, whereas 
it is easy to express the probabilities pT in "terms of the characteristic function y. Indeed, 
multiply B.7) by e~irtX>lL. The probability pn appears as the coefficient of the periodic 
function *«<«-*">*C/? whose integral over —L,L vanishes except when n = r. It follows 
that 
1 f i ¦ 
O o\ pT— — I y(i)e~XT*UL </? r = 0, ± I 
J-L 
506 CHARACTERISTIC FUNCTIONS XV.2a 
-L -1 0 1 L 2L 3L 4L 
Figure 2. Illustrating example (c). 
We now anticipate the following criterion of theorem 1 in XIX,4. Let y be a continuous 
function with period L > 0 and normed by y@) = 1. Then y is o characteristic function 
iff all the numbers pr in B.8) are ^0. In this case.{/?r} is automatically a probability 
distribution and B.7) holds. 
Example, (c) Choose Z, > 1 arbitrary and let y be the function with period 2L 
which for |?j ^ L agrees with the characteristic function 93 of B.5). Then y is the 
characteristic function of an arithmetic distribution concentrated on the multiples of n/L. 
In fact, for reasons of symmetry B.8) reduces to 
\ CL 
B.9) 
\ CL 1 fL 
- y@ cos mt/Ldt = - A-0 cos 
^ Jo L Jo 
and a simple integration by parts shows that 
B.10) p0 = 1/B1), /v = Ltt-2 A -cos m/L) ^ U, r ^ 0. 
We have thus obtained a whole family of periodic characteristic functions whose graphs 
consist of aperiodic repetition of a right triangle with bases 2nL — 1 < x <2nL + 1 and 
intermittent sections of the ?-axis (see fig. 2). (We shall return to this example in the more 
general setting of Poisson's summation formula in XIX,5.) p- 
Curiosities, (i) Two distinct characteristic functions can coincide within a finite interval 
—a, a. This obvious corollary to examples (b) or (c) shows a marked contrast between 
characteristic functions and generating functions (or Laplace transforms). 
(ii) The relation F -k Fx = F ¦*• F2 between three probability distributions does not imply1 
that Fx = Fz. Indeed, with y denned by B.5) we have 9»9>j = q>y2 for any two character- 
istic functions that coincide within the interval —1,1. In particular, we have <p* = <py 
for each of the periodic characteristic functions of example (c). 
(iii) Even more surprising is that there exist two real characteristic functions <Pj such that 
\<pz\ = 9>x > 0 everywhere. In fact, consider the characteristic function y of example 
(c) with L = 1. Its graph is shown by the heavy polygonal line in fig. 3. We saw that the 
corresponding distribution attributes to the origin weight ?. Eliminating this atom and 
doubling all the other weights yields a distribution with characteristic function 2y — 1. fts 
graph is given by a polygonal Hne oscillating between ±1 whose slopes are ±2. It follows 
that 2y(l?) — 1 is a characteristic functidn whose graph is obtained from that of y by 
2 Statisticians and astronomers sometimes ask whether a given distribution has a normal 
component. This problem makes sense because the characteristic function of a normal 
distribution 9IU has no zeros and therefore 5Ra * Fx = 5Ra * F2 does imply q>j = <p2 and 
hence, by the uniqueness theorem, Fx — F2, 
XV;3 
UNIQUENESS. INVERSION FORMULAS 
507 
Figure 3. Illustrating curiosity (Hi). 
mirroring every second triangle along the f-axis (fig. 3). Thus y(O and 2y(if) — 1 are 
two real characteristic functions that differ only by their signs. (For a similar construction 
relating to fig. 2 see problem 9.) 
3. UNIQUENESS. INVERSION FORMULAS 
Let F and G be two distributioas with characteristic functions <p and y. 
Then 
C.1) <r*V@ - I +V1-1* F{dx}. 
J—00 
Integrating with respect to G{dQ one gets 
C.2) f" V*W0 G{dQ = P"V(*-0 F{dz}. 
This identity is known as the Parseval relation (which, however, can be 
written in many equivalent forms; we shall return to it in chapter XIX). 
We shafl use only the special case where G = 9la is the normal distribution 
with density an(ax). Its characteristic function is given by y(?) = 
— >/27rn(?/a), and so C.2) takes on the form 
(^^A F{d~) 
a f 
f+V«V@ 
J—* 
C.3) 
which is the same as 
C.4) 
= f+V«V@ 
Ztt J-oo 
J—oo 
= P" i n( 
J-oo a \ 
Surprisingly many conclusions can be drawn from this identity. To begin 
with, the right side is the density of the convolution 9ta* F of F with a 
normal distribution of zero expectation and variance a2. Thus the knowledge 
508 CHARACTERISTIC FUNCTIONS XV.3 
of (p enables us in principle to calculate the distributions 9ia * F for all a. 
But 9ta has variance a2, and hence $lairF-+F as a-+0. It follows 
that the knowledge of cp uniquely determines the distribution F. We have 
thus the important 
Theorem 1. Distinct probability distributions have distinct characteristic 
functions. 
Suppose then that we are given a sequence of probability distributions Fn 
with characteristic functions <pn, such that <pn(Q —*> <p(O for all ?. By 
the selection theorem of VIII,6 there exists a sequence {nk} and a possibly 
defective distribution F such that Fnk —> F. We apply C.4) to the pair 
(<Pnk, Fnk) anc* let k -+ co. In the limit we get again the identity'C.4) [the 
left side by bounded convergence, the right side because the integrand 
n((/ — x)d) vanishes at infinity]. But we have seen that for given <p the 
identity C.4) determines F uniquely, and hence the limit F is the same for 
all convergent subsequences {Fnk}. We have thus the 
Lemma. Let Fn be a probability distribution with characteristic function 
9V If <Pn@ ~* ViQ for °H I then there exists a possibly defective distribution 
F such that Fn-+F. 
Example, (a) Let U be a probability distribution with a real, non- 
negative characteristic function to. Let Fn = Un* so that <pn@ — <*>"(?)• 
Then (pn@ —>¦ 0 except at the points where a>(?) = 1, and by lemma 4 of 
section 1, this set consists of all points of the form ±«A, where X > 0 is a 
fixed number. It follows that the left side in C.4) is identically zero, and so 
?/"*—>-0. By symmetrization we conclude that Gn*—>-0 for any prob- 
ability distribution G not concentrated at zero. > 
The next theorem states essentially that the limit F is defective iff the 
limit <p is discontinuous at the origin. 
Theorem 2. (Continuity theorem.) In order that a sequence {Fn} of 
probability distributions converges properly to a probability distribution F it 
is necessary and sufficient that the sequence {<pn} of their characteristic 
functions converges pointwise to a limit <p, and that <p is continuous in some 
neighborhood of the origin. 
In this case <p is the characteristir function of F. {Hence <p is continuous 
everywhere and the convergence yn-+ <p is uniform in every finite interval.) 
Proof, (a) Assume that i%, —> F where F is a proper probability distri- 
bution. By the corollary in VIII, 1 the characteristic functions <pn converge 
to the characteristic function <p of F, and the convergence is uniform in 
finite intervals. 
XV.3 UNIQUENESS. INVERSION FORMULAS 509 
(b) Assume <?„(?) ^> <p(Q for all ?. By the preceding lemma the limit 
F = lim Fn exists and the identity C.3) applies. The left side is the 
expectation of the bounded function e-VyiQ with respect to a normal 
distribution with zero expectation and variance a~\ As a —> oo this 
distribution concentrates near the origin and so the left side tends to <p@) 
whenever <p is continuous in some neighborhood of the origin. But since 
= 1 we have <p@) = 1. On the other hand, ^J2rnxi(x) <; 1 for all 
x, and so the right side is <F{ — co, oo}. Thus F{— oo, oo} > 1, and hence 
F is proper. ». 
Corollary. A continuous function which is the pointwise limit of a sequence 
of characteristic functions is itself a characteristic function. 
Example, (b) Pnlya's criterion.. Let co be a real even function with co@) ~ 1 
and a graph that is convex in 0, oo. Then co is a characteristic function. 
Indeed, we saw in example 2(b) that the assertion is true when the graph is a 
convex polygon. Now the inscribed polygons to a concave curve are convex, 
and hence the general assertion is an immediate consequence of the corollary. 
The criterion (together with a tricky proof) had a surprise value in the early 
days. G. Polya used it in 1920 to prove that *rlcl" for 0 < a < 1 is a 
characteristic function of a stable distribution. (Cauchy is sajjd to have been 
aware of this fact, but gave no proof.) Actually e~^a is a characteristic 
function even for 1 < a < 2, but the criterion breaks down. > 
We defer to chapter XIX a full use of the method developed for the proof 
of theorem 1. We use it here, however, to derive an important theorem 
which was used in numbers 5 and 8 of the table in section 2. For abbreviation 
we write <p e L iff |9?| is integrable over — co, oo. 
Theorem 3. (Fourier inversion.) Let cp be the characteristic function of 
the distribution F and suppose that <peL. Then F has a bounded continuous 
density f given by 
C.5) /(^) = -L e-«*<pU)dt. 
2tt J-oo 
Proof. Denote the right side in C.4) by fa(t). Then fa is the density 
of the convolution Fa = 9ta * F of F with the normal distribution 9ta 
of zero expectation and variance a2. As already mentioned, this implies 
that Fa-> F as a -»- 0. From the representation on the left it is clear that 
/a(/) -+f(t) boundedly, where / is the bounded continuous function defined 
in C.5). Thus for every bounded interval / 
C.6) Fa{I} = $ fait) dt -]/(*) dx. 
510 CHARACTERISTIC FUNCTIONS XV.3 
But if / is an interval of continuity for F the leftmost member tends to 
F{I}, and so / is indeed the density of F. ^ 
Corollary. If <p > 0 then <peL iff the corresponding distribution F has a 
bounded density. 
Proof. By the last theorem the integrability of <p entails that F has a 
bounded continuous density. Conversely, if F has a density / < M we 
get from C.4) for t ~ 0 
+« 
C.7) ^- ?>@e-*a; dC = -=±— 
The integrand on the left is ^ 0 and if <p were not integrable the integral 
would tend to oo as a —> 0. »¦ 
Examples, (c) Plancherel identity. Let the distribution F have a density 
/ and characteristic function <p. Then \<p\* € L iff /2 6 L and in this case 
C.8) p/Ty)yf p 
J—oo 27T J— oo 
Indeed, \<p\* is the characteristic function of the symmetrized distribution 
°F. If \<p\2eL it follows that the density 
C.9) 
of °F is bounded and continuous. The left side in C.8) equals ^"@), and 
the inversion formula C.5) applied to °f shows the same is true of the right 
side. Conversely, if /* e L an application of Schwarz' inequality to C.9) 
shows that °f is bounded, and hence \<p\* e L by the last corollary. We 
shall return to the relation C.8) in XIX,7. 
(d) Continuity theorem for densities. Let <pn and <p be integrable 
characteristic functions such that 
C.10) 
J— 00 
By the last corollary the corresponding distributions FH and F have 
bounded continuous densities /„ and /, respectively. From the inversion 
formula C.5) we see that 
P 
oo 
Therefore /„ —>/ uniformly. (See also problem 12.) 
(e) Inversion formula for distribution functions. Let F be a distribution 
with characteristic function <p, and let /i > 0 be arbitrary, but fixed. We 
XV.4 REGULARITY PROPERTIES 511 
prove that 
F(x+h:>-F(«).j.p 
« 2 J 
whenever the integrand is integrable (for example, if it is 0(l/?2), that is, 
if I9KOI = PO/0 as ?-v °o). Indeed, the left side is the density of the 
convolution of F with the uniform distribution concentrated on —h, 0; by 
the product rule the factor of e~Kx under the integral is the characteristic 
function of this convolution. Thus C.11) represents but a special case 
of the general inversion formula C.5). »- 
Note on so-called inversion formulas. Formula C.11) is applicable only when \<p(?)IC\ 
is integrable near infinity, but trite variations of this formula are generally applicable. 
For example, let Fa again denote the convolution of F with the symmetric normal dis- 
tribution with variance a2. Then by C.11) 
C,2) 
The statement that if x and x + h are points of continuity of F the right side tends to 
[F(x+h) — F(x)]/h as o->0 is a typical "inversion theorem." An infinite variety of 
equivalent formulas may be written down. The traditional form consists in replacing in 
C.12) the normal distribution by the uniform distribution in —/, / and letting t-— oo. 
By force of tradition such inversion formulas remain a popular topic even though they have 
lost much of their importance; their derivation from the Dirichlet integral detracts from 
the logical structure of the theory. 
From distributions with integrable characteristic functions we turn to 
lattice distributions. Let F attribute weight pk to the point b + kh, where 
Pk^.Q anc* 2/** = I- The characteristic function cp is then given by 
+ 00 
— 00 
We suppose h > 0. 
Theorem 4. If <p is a characteristic function of the form C.13) then 
C.14) pr = — I"'* <p(Qe-i(b+rhK d?. 
2tT J-ir/h 
Proof. The integrand is a series in which the factor of ph equals ei(k~r)hli. 
Its integral equals 0 or 2-n\h according as k ^ r or k = r, and so C.14) 
is true. > 
4. REGULARITY PROPERTIES 
The main result of this section may be summarized roughly to the effect 
tha.t the smaller the tails of a distribution F, the smoother is its characteristic 
function y, conversely, the smoother F, the better will <p behave at 
512 CHARACTERISTIC FUNCTIONS XV.4 
infinity. (Lemmas 2 and 4.) Most estimates connected with characteristic 
functions depend on an appraisal of the error committed in approximating 
eil by finitely many terms of its Taylor expansion. The next lemma states 
that this error is dominated by the first omitted term. 
Lemma I.3 For n = 1, 2, . .. and t > 0 
D.1) 
e« _ i _ H 
it (if)"-1 
Proof. Denote the expression within the absolute value signs by pn(t). 
Then 
D.2) Pl(t) = i j> dx, 
whence \px(t)\ ^ t. Furthermore for n > 1 
D.3) /»»@-*J^.-i(*)tf*, 
and D.1) now follows by induction. > 
In the sequel F is an arbitrary distribution function, and <p its character- 
istic function. For the moments and absolute moments of F (when they 
exist) we write 
D.4) mn - f+V F{dx}, Mn - (*~\*\* F{dx). 
J—to J— oo 
Lemma 2. 7/" Afn < oo, the nth derivative of <p exists and is a continuous 
function given by 
D.5) <pin)(Q - in r^eVx" F{dx). 
J—ao- 
Proof. The difference ratios of <p are given by 
r 
J— 
> 
D.6) «{»»> «o . r> 
/t J—oo h 
According to the last lemma the integrand is dominated by \x\ and so for 
n = 1 the assertion D.5) follows by dominated convergence. The general 
case follows by induction. • > 
Corollary. //" m2 < op then 
D.7) 9/@) - imx, ?>"@) - - 
3 The same proof shows that when the Taylor development for either sin / or cos / 
is stopped after finitely many terms, the error is of the same sign, and smaller in absolute 
value, than the first omitted term. For example, 1 — cos f S f */2. 
XV.4 
REGULARITY PROPERTIES 
513 
The converse* of the last relation is also true: If <p"@) exists, then 
m2 < oo. 
Proof. Denoting the real part of 9? by k we have 
D.8) I^^?L^.,W 
Proof. The existence of u"@) implies that u exists near the origin and 
is continuous there. In particular, w'@) = 0 because u is even. By. the 
mean value theorem there exists a 6 such that 0 < 6 < 1 and 
D.9) 
u(h) - 1 
/l2 
u'{Bh) 
h 
< 
u'@h) 
eh 
As h -> 0 the right side tends to t/"@). But the fraction under the integral 
in D.8) tends to ?, and so the integral approaches 00 if m2 = 00. For a 
generalization see problem 15. > 
Examples, (a) A non-constant function ip such that ip"@) — 0 cannot 
be a characteristic function, since the corresponding distribution would 
have a vanishing second moment. For example, e~l?|a is not a characteristic 
function when a > 2. 
(b) The weak law of large numbers. Let Xl5 X2, . . . be independent random 
variables with E(X,-) = 0 and the common characteristic function <p. Put 
Sn = \x + • ¦ • + Xn. The average SJn has the characteristic function 
9?"(?/az). Now near the origin <p(h) = 1 + o(h), and hence <p(C(n) = 
= 1 + o(\jri) as n^-co. Taking logarithms we see, therefore, that 
<pn(XJn) —»- 1. By the continuity theorem 2 of section 3 this implies that the 
distribution of SJn tends to the distribution concentrated at the origin. 
This is the weak law of large numbers. The simple and straightforward 
nature of the proof is typical for characteristic functions; a variant will lead 
to the central limit theorem.5 > 
Lemma 3. (Riemann-Lebesgue.) If g is integrable and 
&* g(x) dx, 
-00 
hen 
as 
4 The argument does not apply to the first derivative. The long outstanding problem 
of finding conditions for the existence of q>'@) is solved in section XVII,2a.' 
5 It was shown in VII,7 that the weak law of large numbers can hold even when the 
variables X,- have no expectations The proof of the text shows the existence of a derivative 
r/@) is a sufficient condition. It is actually also necessary (see section XVII,2o). 
514 CHARACTERISTIC FUNCTIONS XV.4 
Proof. The assertion is easily verified for finite step functions g. For an 
arbitrary integrable function g and e > 0 there exists by the mean 
approximation theorem of IV,2 a finite step function gx such that 
D.11) \g(x) - gl(x)\ dx < e. 
J— 00 
The transform D.10) yx of gx vanishes at infinity, and in consequence of 
the last two relations we have |y(?) — y^Ol < * for all ?. Accordingly 
l^iCOl < 2c fox all sufficiently large |?|, and as e is arbitrary this means 
that yi(?)-*0 as ?-»-±«>. ^ 
As a simple corollary we get 
Lemma 4. If F has a density f, then <?{Q-*-0 as ? ->- ± oo. •//" / has 
integrate derivatives /',... ,/<">, then |p(?)| = o(|C|-n) a5 |CI -^ oo. 
Proof. The first assertion is contained in lemma 3. If /' is integrable, 
an integration by parts shews that 
D.12) *@«^ f+Vy'(*)*r, 
and hence If @1 .«¦ oOCI). and so on. ^> 
Appendix: The Taylor development of Characteristic Functions 
The inequality D.1) may be rewritten in the form 
D-13) ¦ n o,-iO| - .1 
From this we get using D.S) 
D.14) 
9(C+0 - 
(/I—1): 
If Mn < «» this inequality is valid for arbitrary ? and / and provides an upper bound 
for the difference between ? and the first terms of its Taylor development. In the special 
case when F is concentrated at the point 1 the inequality D.14) reduces to D.1). 
Suppose now that all moments exist and that 
D.15) Hm sup - M\/n = X < oo. 
Stirling's formula for n\ then shows trivially that for (/( < 1/CA) the right side in D.14) 
tends to zero as n -*¦ oo, and so the Taylor series for <p converges in some interval about 
?. It follows that <p is analytic in a neighborhood of any point of the real axis, and hence 
completely determined by its power series about the origin. But <pm>@) = {i)nmn, and 
thus <p is completely determined by the moments mn of F. Accordingly, // D.15) holds 
then F is uniquely determined by its moments, and q> is analytic in a neighborhood of the 
XV.5 CENTRAL LIMIT THEOREM FOR EQUAL COMPONENTS 515 
real axis. This uniqueness criterion is weaker than Carlcman's sufficient condition 
V M~lln = oo mentioned in VII,C.14), but the two criteria are not very far apart. (For 
an example of a distribution not determined by its moments see VII,3.) 
5. THE CENTRAL LIMIT THEOREM FOR EQUAL 
COMPONENTS 
Work connected with the central limit theorem has greatly influenced 
^he development and sharpening of the tools now generally used in prob- 
ability theory, and a comparison of different proofs is therefore illuminating. 
Until recently the method of characteristic functions (first used by P. Levy) 
was incomparably simpler than the direct approach devised by Lindeberg 
(not to mention ether approaches). The streamlined modern version of the 
latter (presented in VIII,4) is not more complicated and has, besides, other 
merits. On the'other hand, the method of characteristic functions leads to 
refinements which are at present not attainable by direct methods. Among 
these are the local limit theorem in this section as well as the error estimates 
and asymptotic expansions developed in the next chapter. We separate the 
case of variables with a common distribution, partly because of its impor- 
tance, and partly to explain the essence of the method in the simplest situation. 
Throughout this section Xl5 X2, . . . are mutually independent variables 
with the common distribution F and characteristic function <p. We suppose 
E.1) E(X,) = 0, E(X°) = 1 
and put Sn = Xx + • • • + Xn. 
Theorem61. The distribution of SJ-yJn tends to the normal distribution 91. 
By virtue of the continuity theorem 2 in section 3 the assertion is equivalent 
to the statement that as n —> oo 
E.2) <pnaiJn)-+e-& for all ?. 
Proof. By lemma 2 of the preceding section has a continuous second 
derivative, and hence by Taylor's formula 
E.3) (fix) = <p@) + x<p'{0) + |*V@) + o(*?h x — 0- 
Choose I arbitrary and let x = ?/« to conclude that 
<5-4) "(^)-1-sJi?+ 0® 
Taking nth powers we get E.2). 
6 The existence of a variance is not necessary for the asymptotic normality of !&„. For 
the necessary and sufficient conditions see corollary 1 in XVII,5. 
516 CHARACTERISTIC FUNCTIONS XV.5 
It is natural to expect that when F possesses a density /, the density of 
SJyJn should tend to the normal density n. This is not always true, but 
the exceptions are fortunately rather "pathological." The following theorem 
covers the situations occurring in common practice. 
Theorem 2. If \q>\ is integrable, then SJyJn has a density fn which tends 
uniformly to the normal density n. 
Proof. The fourier inversion formula C.5) holds both for /„ and n, 
and therefore 
E.5) \fn{x) - n(*)| < ±- fC 
Z7T J- 
"oo / T \ 1 
a>nl-^p) — e~& dt. 
\y/n) 
The right side is independent of x and we have to show that it tends to 0 as 
n —*- oo. In view of E.3) it is possible to choose 6 > 0 such that 
E.6) MQ\?e-*t for |?| < 6. 
We now split the integral into three parts and prove that each is < e for n 
sufficiently large. A) As we have seen in the last proof, within a fixed 
interval — a <. ? ^ a the integrand tends uniformly to zero and so the 
contribution of —a, a tends to zero. B) For a < |?[ < d\fn the integrand 
is < 2e~&* and so the contribution of this interval is < e if a is chosen 
sufficiently large. C) We know from lemma 4 of section 1 that 1^@1 < 1 
for ?5*6, and from lemma 3 of the last section that <p(Q-+O as |?| -»- 00. 
It follows that the maximum of J9?(?)l foY |?| ^ 6 equals a number rj_<\. 
The contribution of the intervals |?| > d^lri to the integral. E.5) is then less 
than 
E.7) V^f ?(M <% + [ 
? 
-00 
The first integral .equals the integral of yfn \q>\, and so the quantity E.7) 
tends to zero. > 
Actually the proof yields7 the somewhat stronger result that if \<p\rG L for some integer 
r then fn-*-n uniformly. Oa the other hand, the corollary to theorem 3.3 shows that if 
no \<p\T is integrable, then every fn is unbounded. Because 'of their curiosity value we 
insert examples showing that such pathologies can in fact occur. 
Examples, (a) For x >.O and p ^ 1 put 
E.8) up(x) 
¦ * log^p x 
7 The only change is that in E.7) the factor r\*~x is replaced by rf*~ry and q> by <pr. 
XV.5 CENTRAL LIMIT THEOREM FOR EQUAL COMPONENTS 517 
Let g be a density concentrated on 0~l such that g(x) > uv{x) in some interval 0, h. 
There exists an interval 0, 6 in which uv decreases monotonically, and within this interval 
rx 
uv(x—y)u 
Jo 
E.9) g2*(x) ? uv(x-y) up(V) dy>x u2Jx) = u2v(x). 
By induction it follows that for n = 2k there exists an interval 0, hn in which gn* > unp, 
and hence gn*(x) -*¦« as x-*0+. Thus no convolution gn* is bounded. 
(b) A variant of the preceding example exhibits the same pathology in a more radical 
form. Let v be the density obtained by symmetrization of g and put 
E.10) f(x) = ±[t>(s 
Then / is an even probability density concentrated on —2, 2, and we may suppose that 
it has unit variance. The analysis of the last example shows that v is continuous except 
at the origin, where it is unbounded. The same statement is true of all convolutions vn* 
Now /2n*(x) is a linear combination of values v2n*(x+k) with k = 0, ±1, ±2,..., 
±n, and is therefore unbounded at all these points. The density of the normalized sum 
S2J^2n of variables X/ with the density / is given by f2n(x) = V2n/2n*(xV'2/i). It is 
continuous except at the 2n + 1 points of the form k/V2n (k = 0, ±1,... , ±/i), 
where it is unbounded. Since to every rational point /.there correspond infinitely many 
pairs k, n such that k/V2n = t it follows that the distribution of SjVn tends to % but 
the densities fn do not converge at any rational point, and the sequence {fn} is unbounded 
in every interval. ' 
(c) See problem 20. > 
To round off the picture we turn to lattice distributions, that is, we suppose 
that the variables X, are restricted to values of the form b, b±h, b±2h,... . 
We assume that h is the span of the distribution F>. that is, h is the 
largest positive number with the stated property. Lemma 4 in section 1 states 
that \<p\ has period Infh, and hence J^l is not integrable. Theorem 2, 
however, has a perfect analogue for the weights of the atoms of the (distri- 
bution of SJyfn. All these atoms are among the points of the form x = 
(nb+kh)ly/n, where k = 0, ±1, ±2,.... For such x we put 
E.11) r«v-,--i ,- 
and we leave pn(x) undefined for all other x. In E.12) therefore x is 
restricted to the smallest lattice containing all atoms of SJvn. 
Theorem 3. If F is a lattice distribution with span h, then as n —>¦ oo 
E.12) ^(x)_n(^o 
uniformly in x. 
518 CHARACTERISTIC FUNCTIONS XV.6 
Proof. By C.14) 
In 1 rVrnrM / Y \ 
E.13) ±.pn{x)== _ n(±\e-<*dl 
h 2tt J-Vn,/h tyn/ 
Using again the Fourier inversion formula C.5) for the normal density n 
we see that the left side in E.12) is dominated by 
E.14). f "_"* <pn(-^)-e-k2 
J-y/nvlh \y/n/ 
It was shown in the proof of theorem 2 that the first integral tends to zero. 
The second integral trivially tends to zero and this completes the proof. »- 
6. THE LINDEBERG CONDITIONS 
We consider now a sequence of independent variables Xk such that 
F.1) E(X*) = 0, E(Xj) = oJ. 
We denote the distribution of Xk by Fk, its characteristic function by <pk, 
and as usual we put Sn = Xx + • • • + Xn and j* = Var (SJ. Thus 
F.2) 52, = a\ + • • • + a*. 
We say that the Lindeberg condition is satisfied if 
F.3) 4-2 f *2Ffc{rfz}->0, n—oo, 
for each fixed / > 0. Roughly speaking, this condition requires that the 
variance crf be due mainly to masses in an interval whose length is small 
in comparison with sn. It is clear that <y\Js2n is less than t2 plus the left 
side in F.3) and, / being arbitrary, F.3) implies that for arbitrary e > 0 
and n sufficiently large 
F.4) — ?e,- fc = l,...,n. 
This, of course, implies that sn-+ oo. 
The ratio crjsn may be taken as a measure for the contribution of the 
component Xn to the weighted sum SJsn and so F.10) may be described 
as stating that asymptotically Sn/.yn is the sum of "many individually 
negligible components.*' The Lindeberg condition was introduced in VIII, 
, D.15) and the following theorem coincides with theorem 3 of VIII,4. Each 
proof has its advantages. The present one permits us to prove that the 
Lindeberg conditions are, in a certain sense, necessary; it leads also to the 
XV.6 THE LINDEBERG CONDITIONS 519 
asymptotic expansions in chapter XVI, and to convergence theorems for 
densities (problem 28). 
Theorem 1. If the Lindeberg condition F.3) holds, the distribution of the 
normalized sums SJsn tends to the standard normal distribution 5R. 
Proof. Choose ? > 0 arbitrary, but fixed. We have to show that 
6-5) naisn) ¦ ¦' VnUK)-* e-K 
Since 9^@) = 0 and \<pl(x)\ ^ «r* for all x it follows from the two-term 
Taylor expansion and F.4) that for n sufficiently large 
We show that if this is true F.5) is equivalent to 
In fact, we saw in B.4) that e*k~x is the characteristic function of a compound 
Poisson distribution and therefore I***! <[ 1. Now for any complex 
numbers such that \ak\ <^ 1 and \bk\ <, 1 
F.8) > k-'«n-*i--tJ 
as can be seen by induction from the identity 
For any S > 0 we have \e* — 1 — z\ < d \z\ if \z\ is sufficiently small, 
and hence we get from F.6) for large n 
Since S is arbitrary this means that the left side tends to zero and hence 
F.5) holds iff F.7) is true. 
Now F.7) may be rewritten in the form 
F.10) 2 r[>/s" - 1 - & + ^]Fk{dx} -> 0 
J L S 2sJ 
*-l J-«e 
From the basic inequality D.1) it follows that for \x\ <^ tsn the integrand is 
dominated -by \xt/sn\* < tt*x*lsl and for \x\ > tsn by x*?/s2n. The left 
520 CHARACTERISTIC FUNCTIONS XV.6 
side in F.10) is therefore in absolute value 
F-11) </?3 + ?V2 [ -a*Fk{dx}. 
lJ|| 
In consequence of the Lindeberg condition F.3) the second term tends to 
zero, and as / can be chosen arbitrarily small it follows that F.10) is true. * . 
Illustrative examples are given in VIII,4, in problems 17-20 of VIII, 10, 
and in problems 26-27 below. 
The next theorem contains a partial converse to theorem 1. 
Theorem 2. Suppose that sn -*> oo and ojsn-+0. Then the Lindeberg 
condition F.3) is necessary for the convergence of the distribution of Sjsn 
to 5ft. 
Warning. We shall presently see that even when the Lindeberg condition 
fails the distribution of SJsn can tend to a normal distribution with 
variance < 1. 
Proof. We begin by showing that F.4) holds. By assumption there exists 
a v such that ajsn < e for n > v. For v < k < n we have then 
<Jklsn S aklsk < €> and the v ratios ok(sn with k <? v tend to 0 because 
Assume then that the distribution of SJsn tends to 5ft, that is, assume 
F.5). We saw in the preceding proof that when F.4) holds, this relation 
implies F.10). Since cos z — I + \z2 > 0 the real part of the integrand is 
non-negative and so the real part of the left side is 
F.12) 
> X f (?t - 2)^W ^ (*?2 - 2'~2) -t 2 f *¦*¦*{<&}• 
Thus for arbitrary ? and / the right side tends to zero, and hence F.3) 
is true. > 
The condition ajsn —*- 0 is not strictly necessary as can be seen in the 
special case where all the distributions Fk are normal: the. ak may then be 
chosen arbitrarily and yet the distribution of Sn/^n coincides with 5ft. (See 
also problem 27.) However, the condition ajsn is a-natural way to ensure 
that the influence of the individual terms Xk is in the limit negligible ? and 
without this condition there is a radical change in the character of the 
problem. Even if ajsn —*¦ 0 and sn —»¦ oo the Lindeberg condition is not 
necessary in order that there exist some norming constants an such that 
the distribution of Sjan tends to 5ft. The following example will clarify 
the situation. 
XV.7 CHARACTERISTIC FUNCTIONS IN HIGHER DIMENSIONS 521 
Example. Let {Xn} be a sequence of variables satisfying the conditions 
of theorem 1 including the norming F.1). Let the X^ be independent of 
each other and of the Xk and such that 
F.13) 2P{X;^0}< oo. 
n=l 
Put %n = Xn + X'n and denote the partial sums of {X^} and {%n} by 
S^ and Sn. By the first Borel-Cantelli lemma with probability one ojily 
finitely many X^ will differ from 0, and hence with probability one S^ = 
= O(sn). It follows easily that the distributions of &Jsn and SJsn have 
the same asymptotic behavior. Thus the distribution of &Jsn tends to 9t 
even though si is not the variance of §„; in fact, the %n need not have a 
finite expectation. If E(Sn) = 0 arid E(§2) = jjj < oo, the distribution of 
?jsn converges only if sjsn tends to a limit p. In this case the limit 
distribution is normal with a variance p < 1. > 
This example shows that the partial sums Sn can be asymptotically 
normally distributed even when the components Xn have no expectations, 
and also that the variances are not always the appropriate norming constants. 
We shall not pursue this topic here for two reasons. First, the whole theory 
will be covered in chapteF XVII. More importantly, generalizations of the 
above theorems provide excellent exercises, and problems 29-32 are designed 
to lead by easy stages to necessary and sufficient conditions for the central 
limit theorem. 
7. CHARACTERISTIC FUNCTIONS IN 
HIGHER DIMENSIONS 
The theory of characteristic functions in higher dimensions is so closely 
parallel to the theory in ft1 that a systematic exposition appears unnecessary. 
To describe the basic ideas and notations it suffices to consider the case of 
two dimensions. Then X stands for a pair of two real random variables Xx 
and X2 with a given joint probability distribution F. We treat X »s a row 
vector with components Xx and X2; similarly, in F(x) the variable x 
should be interpreted as row vector with components xl5 x2. On the other 
hand the variable ? of the corresponding characteristic function stands for a 
column vector ? = (?l5 ?2). This convention has the advantage that z? 
now denotes the inner product xt, = (llx1 -f ^2x%. The characteristic function 
cp of X {or of F) is defined by 
G.1) 
This definition is formally the same as in one dimension, but the exponent 
has a new interpretation and the integration is with respect to a bivariate 
distribution. 
522 CHARACTERISTIC FUNCTIONS XV.7 
The main properties of bivariate characteristic functions are self-evident. 
For example, the choice ?2 = 0 reduces the inner product x( to z^, 
and hence 9?(?i,0) represents the characteristic function of the {marginal) 
distribution of Xx. For any fixed choice of the parameters ?l5 ?2 the linear 
combination ?XXX -f ?2X2 is a {one-dimensional) random variable and its 
characteristic function is given by 
G.2) E^iW,')^ 
here ?x and ?2 are fixed and X serves as independent variable. In particular, 
the characteristic function of the sum Xx + X2 is given by <p{X, A). In 
this manner the bivariate characteristic function yields the univariate 
characteristic function of all linear combinations ?XXX + ?2X2. Conversely 
if we know the distributions of all such combinations, we can calculate all 
expressions 9?(A?1} A?2), and hence ,the bivariate characteristic function.8 
The next example shows the usefulness and flexibility of this approach. 
It uses the notations introduced in 111,5. 
Example, {a) Multivariate normal characteristic functions. Let X = 
= (Xl5 X2) (thought of as a row vector!) have a non-degenerate normal 
distribution. For tue sake of simplicity we suppose that E(X) = 0 and denote 
the covariance matrix E(XrX) by C. Its elements are ckk = Var (Xfc) and 
c12 = c21 = Cov (Xl5 X2). For fixed t,x and ?2 the linear combination 
?X = ^XXX + ^2X2 has zero expectation and variance 
G.3) a2 = 
With X as independent variable, the characteristic function of the variable 
X? = ?iXx + ^2X2 is therefore given by e~^"i>?. Accordingly, the bivariate 
characteristic function of X = (Xl5 X2) is given by 
G.4) y@ = e-*T°l. 
This formula holds also in r dimensions except that then t,TCt, is a quadratic 
form in r variables ?1}1. . . , t,r. Thus G.4) represents the characteristic 
function of the r-dimensional normal distribution with zero expectation and 
covariance matrix C. 
It is occasionally desirable to change both pairs (Xl5X2) and (?l5 ?2) 
to polar coordinates, that is, to introduce new variables by 
G.5) Xj — R cos 0, X2 = R sin 0, ?x = p cos a, ?2 = p sin a. 
8 This proves incidentally that a probability distribution in SI2 is uniquely determined 
by the probabilities of all half-planes. This fact (noted by H. Cramer and H. Wold) does not 
seem to be accessible by elementary methods. For an application to moments see problem 
21. 
XV.7 CHARACTERISTIC FUNCTIONS IN HIGHER DIMENSIONS 523 
(For such transformations, see 111,1.) Then 
G.6) 9?(O = E(et"Rcos@-a)) 
but it must be borne in mind that this is not the characteristic function of the 
pair (R,0); the latter is given by E(ei(?lR+?20)). 
Examples, (b) Rotational symmetry. When the pair (X1} X2) represents 
a "vector issued in a random direction" (see 1,10) the joint distribution of 
(R, 0) factors into the distribution G of R and the uniform distribution 
over — rr < 6 < n. The expectation in G.6) is then independent of a and 
takes on the form 
G.7) tp?xt ?2) = B7T)-1 
JO 
The change of variable cos 0 = x reduces the inner integral to that discussed 
in problem 6, and thus 
G.8) rtk,&) = rjo(Pr)G{dr} (p = 
Jo 
where 
G.9) J0(x) = h{ix) = 
A:=0 
(The Bessel function /0 was introduced in 11,7.) 
A unit vector in a random direction has tHe distribution G concentrated 
at the point 1. Thus /,J(v5 + l\) is the characteristic function of the 
resultant of n independent unit vectors issued in random directions. This 
result was derived by Raleigh in connection with random flights. 
(c) We consider the special case where (X1} X2) has a bivariate density 
/ given by 
G.10) f(xr, x2) = (inrWe-', r = V*x2 + x\ 
where a is a positive constant. Then G.8) takes on the form9 
i. ?2) = a2 
Jo 
Vrl 
G.11) ?(?i. ?2) = a2 re-arJ0(pr)r dr = A + pVrl 
J 
(d) Rotational symmetry in 3i3. Example (b) carries over to three dimen- 
sions except that we have now two polar angles: the geographic longitude 
Substituting for Jo its expansion G.9) one gets 
which is the binomial series for 
524 CHARACTERISTIC FUNCTIONS XV.7 
a) and the polar distance 6. The inner integral in G.7) takes on the form 
G.12) 
-1- I dco \ eirp cos0 sin 6 dd = \ j {eirp cose 4 e~irp cosfl) sin Q dO 
4tt J—it Jo Jo 
where p2 = t\ + t\ + t,\. The substitution cos d = x reduces this 
expression to (rp)-1 sin rp, and so G.8) has the analogue 
(* CO * 
G.13) cpa,, U, ?3) = !HLi!? G{dr). 
Jo rp 
In particular, for a unit random vector the integral reduces to p'1 sin p. 
Letting ?2 = ?3 we see that the characteristic function of the Xx-component 
of a unit random vector is given by ?~x sin ?x. We have thus a new proof for 
the fact established in 1,10 that this component is distributed uniformly over 
-1, 1. > 
It may be left to the reader to verify that the main theorems concerning 
characteristic functions in one dimension carry over without essential 
change. The Fourier inversion theorem in %2 states that if 9? is (absolutely) 
integrable over the entire plane, then X has a bounded continuous density 
given by 
G.14) x^-^jfr* 
Example, (e) Bivariate Cauchy distribution: When the inversion formula 
G.14) is applied to the density / in example (c) it is seen upon division by 
/@,0)=B7r)-1a2 that 
G.15) 
represents the characteristic function of a bivariate density g defined by. 
G.16) g.(*i,*2)= ° 
x{ 
It follows that this density shares the main properties of the ordinary Cauchy 
density. In particular, it is strictly stable: if XA),. . . , X(n) are mutually 
independent vector variables with the density G.16), their average 
(XA) + -•-+X(n))/rt has the same density. > 
XV.8 TWO CHARACTERIZATIONS OF NORMAL DISTRIBUTION 525 
*8. TWO CHARACTERIZATIONS OF THE 
NORMAL DISTRIBUTION 
We begin by a famous theorem conjectured by P. Levy and proved in 
1936 by H. Cramer. Unfortunately its proof depends on analytic function 
theory and is therefore not quite in line with our treatment of characteristic 
functions. 
Theorem 1. Let Xx and X2 be independent random variables whose sum is 
normally distributed. Then both \x and X2 have normal distributions. 
In other words, the normal distribution cannot be decomposed except 
in the trivial manner. The proof will be based on the following lemma of 
some independent interest. 
Lemma. Let F be a probability distribution such that 
(8.1) f(n) = f+ V*2 F{dx] < oo 
J— CO 
for some r\ > 0. The characteristic function <p ¦ is then an entire function 
{definedfor all complex ?). If <??(?) j? 0 for all complex ?, then F is normal. 
Proof of the lemma. For all complex ? and real x, r\ one has 
\xt\ <. 7]2x2 + ?7~2|?|2 and so the integral defining <p converges for all 
complex ? and 
(8.2) |p(OI <e«~W'f(V). 
This means that 9? is an entire function of order <C 2, and if such a 
function has no zeros, then log q>(?) is quadratic polynomial.10 Hence, 
9?(?) = e~^2+ibt' where a and b are (possibly complex) numbers. But <p 
is a characteristic function and hence —i(p'{0) equals the expectation, and 
— 9?"@) the second moment of the distribution. It follows that b is real 
and a > 0, and so F is indeed normal. > 
Proof of theorem 1. Without loss of generality we may assume the variables 
Xt and X2 centered so that the origin is a median for each. Then 
(8.3) P{|XX + X2| > /} > JPflXJ > /}. 
Now the usual integration by parts [see V,6] shows that 
(8.4) f{ri) < rfTx ¦ J*\\ - F(x) + F(-x)] dx, 
Jo 
and therefore the functions fk corresponding to Xfc satisfy the inequalities 
* This section treats special topics and is used only in problem 27. 
10 See, for example, E. Hille, Analytic function theory, Boston, 1962, vol. II, p. 199 
(Hadamard's factorization theorem). 
526 CHARACTERISTIC FUNCTIONS XV.9 
fk(v) < 2f(v) < °o- Since <px(t) <p2(?) = e-la?+ibt neither- <px nor y2 
can have a zero, and so Xx and X2 are normal. > 
We .turn to a proof of the following characterization of the normal distri- 
bution enunciated and discussed in 111*4. 
Theorem 2. Let Xx and X2 be independent variables gnd 
(8.5) Y1 = a11X1 + al2X2, Y2 = a2lXl + a22X2. 
If also Y1 and Y2 are independent of each other then either all four variables 
are normal, or else the transformation (8.5) is trivial in the sense that either 
Y1 = aX1 and Y2 = bX2 or Y1 = aX2 and Y2 = bXv 
Proof. For the special case of variables X3- with continuous densities 
the theorem was proved in 111,4. The proof depended on the general solution 
of the functional equation 111,D.4), and we shall now show that an equation 
of the same type is satisfied by the characteristic functions q>, of the variables 
Xj. We show first that it suffices to consider real characteristic functions. 
This argument illustrates the usefulness of theorem 1. 
(a) Reduction to symmetric distributions. Introduce a pair of variables 
X~ and X~ that are independent of each other and of the X;-, and distrib- 
uted as —Xx and —X2, respectively. The linear transformation (8.5) 
changes the symmetrized variables °Xi = Xy + X3~ into a pair ^Yi, °Y2) 
of symmetric independent variables. If the theorem is true for such variables 
then °Xj is normal, and by theorem 1 this implies that also X,- is normal. 
(b) The functional equation. Because of the assumed independence of 
Yx and Y2 the bivariate characteristic function of (Yl5 Y2) must factor: 
(8.6) E(et(?iYi+?isY2)) = E(y<iYi)E(<?^Yi0. 
Substituting from (8.5) we see that this relation implies the following 
identity for the characteristic functions of X1 and X2 
(8.7) 
This identity coincides with 111,D.4) except that the roles of a12 and a21 are 
interchanged. By assumption the 99, are real and continuous, and as in 
111,4 it is seen that all ajk may be assumed to be different from zero. By 
the lemma of 111,4 therefore $?,•(?) = e~a^, and so the X, are normal. > 
9. PROBLEMS FOR SOLUTION 
1. From the inequality A.7) conclude (without calculations) that for every 
characteristic function q> 
(9.1) 
XV.. PROBLEMS FOR SOLUTION 527 
2. If ip = u + iv is a characteristic function show that 
(9.2) 
This in turn impjies 
(9-3) 
Hint: For (9.2) use Schwarz' inequality, for (9.3) consider characteristic functions 
of the form eictlq>(Q. 
3. With the same notations 
(9.4) 
The inequality A.7) is contained herein when ?2 = ~?i- 
4. From elementary formulas prove (without explicit integrations) that the char- 
acteristic function <p of the density A/tt)[A - cos x^x2] differs only by a constant 
factor from 2 \t\ - |? + 1| - K —11- Conclude that ?(?) = 1 - \t\ for |?| ^ 1. 
5. From the characteristic function of the density Jae!*! derive a new char- 
acteristic function by simple differentiation with respect to a. Use the result 
to show that the convolution of the given distribution with itself has density 
lae~a\x\(l+a \x\). 
6. Let / be the density concentrated on —1,1 and defined by 
(9.5) /W-WTT3- 
Show that its characteristic function is given by 
(9.6) vU) = 2 j-jr W* " 'oCtf- 
Note that /0(?) = /0(/?) where I0 is the Bessel function denned in 11,G.1). 
Hint: Expand ei!*x into a power series. The coefficient of ?n is given by an integral, 
and (9.6) can be verified by induction on n using an integration by parts. 
7. The arc sine distribution with density l/[nVx(l —x)] concentrated on 0, 1 
has the characteristic function e*S/2./0(?/2). Hint: Reduce to the preceding problem. 
8. Using the entry 10 of the table in section 2 show that 2ittx ¦ (sinha;) is a 
density with characteristic function 2/[l + cosh (tt?)]. Hint: Use problem 6 
of 11,9. 
9. Let yL stand for the characteristic function with period 2L described in 
example 2(c). Show that 2y2L — yL is again a characteristic function of an 
arithmetic distribution. Its graph is obtained from fig. 2 by reflecting every second 
triangle about the ?-axis. 
10.11 Let X and Y be independent random variables with distributions F 
and G, and characteristic functions y and y, respectively. Show that the 
product XY has the characteristic function 
/•oo /*oo 
(9.7) Y{fr)F{dx}=\ <p(tx)G{dx}. 
J — oo J — oo 
11 Combining (9.7) with the theorem in the footnote to example V,9F) one gets the follow- 
ing criterion due to A. Khintchine. A function co is the characteristic function of a ummodal 
distribution iff co({,) = Q q>(?x) dx where <p is a characteristic function. 
528 CHARACTERISTIC FUNCTIONS XV.9 
11. If {q>n} is a sequence of characteristic functions such that q>n@ -*¦ 1 for 
-6 < C < <5, then q>H(Q — 1 for all ?. 
12. Let g be an even density with a strictly positive characteristic function y. 
Then 
(9.8) 
is a probability density with characteristic function 
,n <w ,v\ 2y@ — y(?+a) — y(?—a) 
(9 9) yfl@ = -^ ——- . 
2[1 — y(a)] 
As a -+ oo we have ya-+y but not ga -+g. This shows that in the continuity 
theorem for densities the condition C.10) is essential. 
13. If y is a real characteristic function and y ^ 0, there exist even densities 
gn with strictly positive characteristic functions yn such that yn -*¦ y. Hint: 
Consider mixtures A — e)G + eF and convolutions. 
14. If y is a characteristic function such that y ^ 9 and y(a) j? 1, then 
(9.9) defines a characteristic function. Hint: Use the preceding two problems. 
15. Generalization of the converse to D.7). Considering the distributions 
(\lm2k)x2k F{dx) (when they exist) prove by induction: the distribution F possesses 
a finite moment m2r iff the 2rth derivative of the characteristic function y exists 
at the origin. 
16. Let / be a probability density with a positive and integrable characteristic 
function. Then / has a unique maximum at the origin. If a second derivative 
/" exists, then 
(9.10) 
analogous expansions hold for the first 2r terms of the Taylor development. 
[Note that / is even and hence /<2*+1> @) = 0.] 
17. Let q> be a real characteristic function with continuous second derivative 
<p". Then [unless <p@ = 1 for ail ?] 
(9.11) 
is a characteristic function belonging to an even density /2 defined for x > 0 by 
(9.12) 
Generalize to higher moments. 
18. Let / be an even density with characteristic function <p. For x > 0 put 
f (s) ds 
(9.13) <?() 
C ^ 
<?(*) = 
Then ^ is again an even density and its characteristic function is 
i n 
(9.14) y@ = j 9»(J)*- 
XV.9 . PROBLEMS FOR SOLUTION 529 
19. Let y be a characteristic function such that limsup|y(?)l =1 as ?-*¦<». 
The corresponding distribution F is purely singular (with respect to Lebesgue 
measure). 
20. Suppose that ck > 0, 2^ = 1 but %ck2k = oo. Let u be an even 
continuous density concentrated on -1, 1 and let co be its characteristic function. 
Then 
(9.15) f{x) = ? c*2* «B*s) 
defines a density that is continuous except at the origin and has the characteristic 
function 
(9.16) <KO =2cM2-k0- 
Show that \y\n is not integrable for any n. Hint: For x ^ 0 the series in (9.15) 
is finite. Use the trivial inequality (]? ckpk)n > ]? c?/?? valid for pk > 0. 
21. Moment problem in 3l2. Let Xj^ and X2 be two random variables with a 
joint distribution F. Put Ak = EflXJ*) + E(|X2|fc). Show that F is uniquely 
determined by its moments if lim sup k~l A\ik < co. Hint: As pointed but in the 
footnote 8 to section 7 it suffices to prove that the distributions of all linear 
combinations axXx + a2X2 are uniquely determined. Use the criterion D.15). 
22. Degenerate bivariate distributions. If <p is a univariate characteristic function 
and ax, a2 arbitrary constants, show that <piaxt,x-\-att,^ as a function of ix, ?2 
represents the bivariate characteristic function of a pair (Xx, X2) such that identically 
a2Xx = fliX^ Formulate the converse. Consider the special case a2 = 0. 
23. Let X, Y, U be mutually independent random variables with characteristic 
functions (p,y,a>. Show that the product <Ksi) y(X<t) ^(^1 + ^2) represents the 
bivariate characteristic function of the pair (U 4- X, U + Y). Hint: Use a tri- 
variate characteristic function. 
Examples and complements to the central limit theorem 
24. Prove the central limit theorem 4 of VIJI.4 for random sums by the method 
of characteristic functions. 
25. Let Xk have the density e~xxak-1i'r(ak) where ak —¦ co. The variance of 
Sn is s% = iax+- • -+an). Show that the Lindeberg condition is satisfied if 
26. Let P{Xifc = ±1} = (A: - l)/2? and ?{Xfc = ± Vk} = \jlk. Show that 
there do not exist norming constants an such that the distribution of SJan 
tends to 9?. Hint: Pass to exponentials using 
C\ 
27. If the distribution of S^/sn tends to 31, but ajsn —p > 0 then the 
distribution of XJsn tends to a normal distribution with variance pL. Hint: Sy 
the Cramer-Levy theorem in section 8 if 5R = UJz V, then both 0 and V are 
normal. Use convergent subsequences for the distributions of Xn/jv and Sn^Jsn. 
530 CHARACTERISTIC FUNCTIONS XV.9 
28. {Central limit theorem for densities.) Show that theorem 2 of section 5 
generalizes to sequences with variable densities fk provided sufficient uniformity 
conditions are imposed on the characteristic functions. It suffices, for example, 
that the third absolute moments remain bounded and that the fk have derivatives 
such that \f'k\ < M for all k. . 
29. {Central limit theorem for triangular arrays?) For each n let Xln,... , X,, n 
be n independent variables with distributions Fkn. Let Tn = Xt n +' • • • +Xnn. 
Suppose that E<Xfcn) = 0 and E(T2) = 1, and'that 
(9.17) 
-i J|x 
for each t > 0. Show that the distribution of Tn tends to SR. Hint: Adapt the 
proof of theorem 1 in section 6/ 
Note. The Lindeberg theorem represents the special case Xfcn = Xk/sn and 
Tn = SJsn. Then (9.17) reduces to the Lindeberg condition F.3). For triangular 
arrays see VI,3. 
30. (Truncation.) Let {Xfc} be a sequence of independent variables with 
symmetric distributions. For each n and k <, n let Xfc n be the variable obtained 
by truncating Xfc at ±a«. Suppose that 2*=1 P{|Xfc| > an} -*0 and that (9.17) 
holds. Show that the distribution of Sn/an tends to SR. 
31. (Generalized central limit theorem.) Suppose the distributions Fk are 
symmetric and that lor every t > 0 
(9.18) 
Prove that the distribution of SJa,, tends to 5R (a) using the last two problems, 
(b) directly, by adapting the proof of theorem 1 in section 6.12 
32. (Continuation.) The condition of symmetry may be replaced by the weaker 
¦xmdition 
(9.19) I If *Fk{dx} 
I If * 
0. 
33. In'order that there exist norming constants an for which the conditions 
(9.18) are satisfied it is necessary and sufficient that there exists a sequence of 
numbers tn -+ oo such that 
k 
In this case one can take 
2 f Fk{dx}-+09 f,i I . f Fk{dx) - oo. 
"lJ\X\<tn n fc-1 J\X\<tn 
< - 2 f ^ 
*=1 J\x\<tn 
(This criterion usually can be applied without difficulty.) 
12 Theorem 2 generalizes similarly but requires a different proof. 
CHAPTER XVI' 
Expansions Related to 
the Central Limit Theorem 
The topics of this chapter are highly technical and may be divided into 
two classes. One problem is to obtain estimates for the error in the central 
limit theorem and to improve on this result by providing asymptotic expan- 
sions. A problem of an entirely different nature is to supplement the central 
limit theorem for large values of the independent variable, where the 
classical formulation becomes empty. 
In order to facilitate access to important theorems, and to explain the 
basic ideas, we separate the case of identically distributed variables. Section 
7 on large deviations is independent of the first five sections. The theory 
developed in these sections depends essentially on two techniques: direct 
estimation of absolutely convergent Fourier integrals, and smoothing methods. 
At the cost of some repetitions and some loss of elegance we separate the two 
main, ideas by first treating expansions for densities. 
The chapter culminates in the Berry-Esseen theorem of section 5. The 
smoothing method described in section 3 was first used by A. C. Berry in 
the proof of this theorem. An endless variety of smoothing procedures 
are in general use. In fact, the long and glorious history of the subject 
matter of this chapter has the unfortunate effect that accidents of historical 
development continue to influence the treatment of individual topics. 
The resulting diversity of tools and abundance of ad hoc methods has rendered 
the field proverbial for its messiness. The following systematic exploitation 
of Berry's method and of modern inequalities fortunately permits an amazing 
unification and simplification of the whole theory.1 
* This chapter treats special topics and should be omitted at first reading. 
1The best-known introduction to the asymptotic expansions is H. Cramer A962). It 
contains the expansion theorems of sections 2 and 4 for equally distributed variables and a 
slightly sharper version of the theorems of section 7. The first rigorous treatmenl of the 
expansion theorems is due to Cramer, but his methods are no longer useful. Gnedenko 
and Kolmogorov A954) treat the material of sections 1-5. 
531 
532 EXPANSIONS RELATED TO CENTRAL LIMIT THEOREM XVI. 1 
1. NOTATIONS 
Except in the last section (which deals with unequal components) we 
shall denote by F a one-dimensional probability distribution with char- 
acteristic function cp. When the &th moment exists, it will be denoted by: 
A-1) 
J— 00 
We suppose fxx = 0 and put, as usual, ju2 = a2. For the normalized «-fold 
convolution we write Fn. Thus 
A.2) Fn{x) = F**(xoMrn). 
When a density of Fn exists we shall denote it by ~fn. 
Except in section 6 (concerned with large deviations) we shall have to 
deal with functions of the form 
(i.3) «(*)=- r 
2tt J- 
and the obvious estimate 
A.4) W(x)\^ 
J-oo 
Both u and v will be integrable. If u is a probability density, then v is 
its characteristic function. To simplify expressions we introduce the 
Convention. The function v in A.3) will be called the Fourier transform 
of u and the right side of A.4) will be called the Fourier norm of u. 
As always, the normal density is denoted by 
A.5) n(x) - -~ e-**\ 
Its Fourier transform is the characteristic function e~^\ By repeated 
differentiation we get therefore the identity 
A.6) -^ n(x) = f f V^-fO^ di 
valid for k = 1,2,.... Obviously the left side is of the form 
A.7) ¦^-n(x) = (-l)kHk(x)n(x) 
where Hk is a polynomial of degree k. The Hk are called Hermite 
XVI.2 EXPANSIONS FOR DENSITIES 533 
polynomials.2 In particular, 
A.8) Hx(x) = x, H2(x) = x*-l, H3(x) = x*-3x. 
The characteristic property of Hk is, then, that Hk(x)n(x) has the Fourier 
transform (/?)* e~&2. 
2. EXPANSIONS FOR DENSITIES 
The central limit theorem 2 of XV,5 for densities can be strengthened 
considerably when some higher moments /xk exist. The important assump- 
tion is that3 
J*+OO 
00 
for some v > 1. The proof given in XV,5 may be summarized roughly 
as follows. The difference un=fn—n has the Fourier transform 
B.2) Pn@ = fpp -e~* . 
The integral of \vn\ tends to zero for two reasons. Given an arbitrarily 
small but fixed d > 0 the contribution of the intervals |?| > do\ln tends 
to zero because of B.1). Within |?| < do\/n the integrand vn is small by 
virtue of the behavior of <p near the origin. The latter conclusion depends 
only on the fact that /j.x = 0 and /j..2 =. a2. When higher moments exist we 
can use more terms in the Taylor development for <p and thus obtain more 
precise information concerning the speed of convergence /„ -> n. Un- 
fortunately the problem becomes notationally involved when more than three 
terms are involved, and we therefore separate out the simplest and most 
important special case. 
Theorem V. Suppose that jj.z exists and that \cp\> is integrable for some 
v>\. Then fn exists for n > v and as n -> oo 
B.3) fn(x) - n(x) - —f-^ (x3-2x) n(x) = oi-j=) 
uniformly in x. 
Proof. Ey the Fourier inversion theorem of XV,3 the iert side in B.?) 
e.ists :"or n > v and has the Fourier norm 
B.4) 
i 
in 1 
7'" 
\ 
- Sometimes Chebyshev-Hermite polynoi-nials. The terminology is not unique. Various 
nofining factors are in use and frequently e"-c' replaces our e~^-xZ. 
¦' Concerning tl.is condition i,ee examples XV.5(a~b) and problem 20 in XV.9. 
534 
EXPANSIONS RELATED TO CENTRAL LIMIT THEOREM 
XVI.2 
Choose <5 > 0 arbitrary, but fixed. Since <pn is the characteristic function 
of a density we have \<p(Q\ < 1 for |?| j± 0 and q>@-*0 as |?| -> co 
(lemmas 4 of XV,1 and 3 of XV,4). There exists therefore a number 
qd< 1 such that \<p(Q\ <Hd for |?| ;> d. The contribution of the intervals 
|?| > doyjn to the integral in B.4) is then 
B.5) 
J-oo 
to? 
and this tends to zero more rapidly than any power of 1/n. 
With the abbreviation4 
B.6) 
we have therefore 
B.7) 
= log 
exp 
) - l - 
+ 
The integrand will be estimated using the following general scheme 
B.8) 
\e' - 1—^1 - Ke'- 
where y ^ max (|a|, |^f|). (That this inequality is valid for arbitrary real 
or complex a and p becomes evident on replacing e" and e* by their 
power series.) 
The function ip is thrice differentiable and ip@) = y>'@) = v"@) = 0 
while v'"@) = izfj.%. Since rpm is continuous it is possible to find a neighbor- 
hood |?| < d of the origin in which ip'" varies by less than e. From the 
three-term Taylor expansion we conclude that 
B-9) tytt) ~ itod 
Here we choose d so small that also 
for \t\ < d. 
for 
< d. 
With this choice of d it is seen using B.8) that the integrand in B.7) is less 
than 
B.11) ^( ^i 
and as e is arbitrarv we have Nn = o(l/vn ) and so B.3) is true. 
4 All logarithms of complex numbers used in the sequel are defined by the Taylor series 
log A +z) = ^ (—z)nln valid for \z \ < i. No other values of z will occur. 
XV1.2 
EXPANSIONS FOR DENSITIES 
535 
The same argument leads to higher-order expansions, but their terms cannot be expressed 
by simple explicit formulas. We therefore postpone the explicit construction of the poly- 
nomials involved. 
Theorem 2. Suppose that the moments fi3,. . ., fir exist and that \q>\v is integrable for 
some v ^ 1. Then fn exists for n ^ v and as n -*¦ oo 
B.12) fH(x) - n(x) - n(x) ? rr*™Pk(x) = o(n~l 
k=3 
uniformly in x. Here Pk is a real polynomial depending only on 
n and r {or otherwise on F). 
The first two terms are given by 
B.13) Pz = ^Hz, P4 = 
fik but not on 
3 ' 24a4 4' 
where Hk stands for the Hermite polynomial defined in A.7). The expansion B.12) is 
called (or used to be called) the Edgeworth expansion for fr 
Proof. We adhere to the notation B.6). If p is a polynomial with real coefficients 
p1,p2>- • • then 
B.14) /« - n - nLPkHk 
has the Fourier norm 
B.15) 
The theorem will be proved by exhibiting appropriate polynomials p. (Their dependence 
on n is not stressed in order not to encumber the notations.) 
We begin by estimating the integrand. The procedure is as in the last proof except that 
we use the Taylor approximation for y> up to and including the term of degree r. This 
approximation will be denoted by ?2 y>r (?)• Thus y>r is a polynomial of degree r — 2 
with yr@) = 0; it is uniquely determined by the property that 
We now put 
B.16) 
Then P(iQ is a polynomial with real coefficients depending on n. For fixed ?, on the 
other hand, p is a polynomial in 1/V/i whose coefficients can be calculated explicitly as 
polynomials in /.<!, /*2,. . . , /nr. As in the last proof it is obvious that for fixed 6 > 0 the 
contribution of |?| > 6<rVn to the integral in B.15) tends to zero more rapidly than any 
power of l//i, and thus we are concerned only with the integrand for |?| < daVn- To 
estimate it we use [instead of B.8)] the inequality 
B.17) 
e*-l - 
r—2 
\e* - e»\ 
- 1 - 
r-2 
- 01 + 
101 
r-l 
valid when |<x| < y and |/5| < y. 
536 EXPANSIONS RELATED TO CENTRAL LIMIT THEOREM XVI.3 
By analogy to B.9) we now determine <5 such that for |?| < <5 
B.18) |V(?) - ?V(?)| <. eor\t\r. 
The coefficient of ? in y>r being /V3/6, we can suppose ithat for |?| < <5 also 
B.19) |Vr(?)| < a |?| < Jtr2 
provided a > 1 + |^3|. Finally we require that for |?| < d 
B.20) |V(?)| <±cr2?2. ' 
For |?| < daVn the integrand in B.15) is then less than 
B.21) e-H 
As e is arbitrary we have Nn = o(n~$r+1). 
We have now found real coefficients pk depending on n such that the left side in B.14) 
is ofa"*7) uniformly in x. For fixed ? the left side is a polynomial in 1/V/i. Rearrang- 
ing it according to ascending powers of 1/V« we get an expression of the form postulated 
in the theorem except that the summation extends beyond r. But the terms involving 
powers l/nk with k > \r — 1 can be dropped, and we get then the desired expansion 
B.12). > 
The explicit definition of the polynomials Pk is thus as follows. A polynomial y>r of degree 
r — 2 is uniquely determined by the Taylor formula 
B.22) log tp(X) = ?2[-?cr2+w(?)] + o(|?|r) 
valid near the origin. Rearrange B.16) according to powers of l/Vn. Denote the coefficient 
of n~$k+1 by qk(iQ- Then Pk is the polynomial such that n(x) Pk(x) has the inverse Fourier 
transform e~-Pqk(i?). 
3. SMOOTHING 
Every expansion for the densities fn leads by integration to an analogous 
expansion for the distributions Fn, but this simple procedure is not available 
when the integrability condition B.1) fails. To cope with this situation we 
shall proceed indirectly (following A. C. Berry). To estimate the discrepancy 
Fn — 31 or a similar function A we shall use the Fourier methods of the 
last section to estimate an approximation TA to A, and then appraise the 
error TA — A by direct methods. In this section we develop the basic 
tools for this procedure. 
Let VT be the probability distribution with density 
C.1) , ' ' 1|-C0!Tl 
and characteristic function «jt. For |?| < T we have 
C.2) coT@ = 1 - j?! , 
XVI.3 SMOOTHING 537 
but this explicit form is of no importance. What matters is that wT(?) 
vanishes for |?| > T, for this circumstance will eliminate all questions of 
convergence. 
We shall be interested in bounds for Fn — 31 and, more generally, for 
functions of the form An = Fn — Gn. Such functions will be approximated 
by their convolutions with VT and we put generically TA = VT + A. In 
other words, given any function A we define 
J'+oo 
A(t-x)vT(x)dx. 
—oo 
If A is bounded and continuous, then TA -+• A as J-> oo. Our main 
problem is to estimate the maximum of |A| in terms of the maximum of 
ITA|. 
Lemma 1. Let F be a probability distribution and G a function such that 
G(-oo) = 0, G(oo) = 1, and \G'(x)\ < m < oo. 
Put 
C.4) A(x) = F(x) - G(x) 
and 
C.5) y] = sup |A(x)|, y]T = sup |TA(x)|. 
X X 
Then 
C-6) Vt>-t — • 
2 ttT 
Proof. The function A vanishes at infinity and the one-sided limits 
A(x+) and A(x—) exist everywhere, and so it is clear that at some point x0 
either |A(xo+)| = rj or |A(x0—)| = rj. We may assume A(x0) = r\. As 
F does not decrease and G grows at a rate < m this implies 
C 7) A(zo+j) >rj - ms for s > 0. 
Putting 
C.8) h = — , / = x0 + h, x = h - s, 
2m 
we have then 
C.9) A(/-x)>- + mx for \x\ < h. 
We now estimate the convolution integral in C.3) using C.9) and the bound 
A(/—x) > —r] for |x| > h. The contribution of the linear term vanishes 
for reasons of symmetry; since the density rT attributes to \x\ > h amass 
< 4/G7 77?) we get 
C.10) 
4 1 4 -n &n V 
! 
rJT > TA(x0) >l|i_— _^-— «;!_ — = ---— 
2L Thj tTH 2 Trln L 7T- 
nThj tt 
538 EXPANSIONS RELATED TO CENTRAL LIMIT THEOREM XVI.4 
In our applications G will have a derivative g coinciding either with the 
normal density n or with one of the finite expansions described in the last 
section. In every case g will have a Fourier transform y with two con- 
tinuous derivatives such that y@)f= 1 and y'@) = 0. Obviously then 
the convolution Tg = VT~k g. has the Fourier transform ya>T. Similarly, 
by the Fourier inversion theorem of XV,3 the product (pcoT is the Fourier 
transform of the density Tf of VT it F. In other words, 
C.11) Tf(x) - Tg(x) = ± T e- 
2-n J-t 
Integrating with respect to x we obtain 
C.12) TA(*) 
2-ttJ-t -it, 
No integration constant appears because both sides tend to 0 as |x| —> oo, 
the left because F(x) — G(x) —>- 0, the right by the Riemann-Lebesgue 
lemma 4 of XV,4. Note that <p@) = y@) = 1 and <p'@) = /@) = 0; 
hence the integrand is a continuous function vanishing at the origin, and so no 
problem of convergence arises. 
From C.12) we get an upper bound for r/T which, combined with C.6), 
yields an upper bound for 77, namely 
,, , 24m 
C.13) \F(x) - G(x)\ < - 
t 
It J-T 
I 
ttT 
As this inequality will be the basis for all estimates in the next two sections 
we recapitulate the conditions of its validity. 
Lemma 2. Let F be a probability distribution with vanishing expectation 
and characteristic function <p. Suppose that F — G vanishes at ±00 and 
that G has a derivative g such that \g\ Km. Finally, suppose that g has a 
continuously differentiable Fourier transform y such that y@) = 1 and 
y'@) = 0. Then C.13) holds for all x and T > 0. 
We shall give two independent applications of this inequality: In the 
next section we derive integrated versions of the expansion theorems of 
section 2. In section 5 we derive the famous Berry-Esseen bound for the 
discrepancy Fn — 9t. 
4. EXPANSIONS FOR DISTRIBUTIONS 
From the expansion B.3) for densities we get by simple integration 
D.1) Fn(x) - 3l(x) - J±- (l-*2)n(z) = o(±\. 
6<7\/n \yjnj 
XVI.4 EXPANSIONS FOR DISTRIBUTIONS 539 
For this expansion to hold it is not necessary that F has a density. In fact, 
we shall now prove that D.1) holds for all distributions with the sole exception 
of lattice distributions (that is, when F is concentrated on the set of points 
of the form b ± nh). For a lattice distribution the inversion formula 
XV, E.12) shows that the largest jump of Fn is of the order of magnitude 
1/Vrt, and hence D.1) cannot be true of any lattice distribution. However, 
even foi fattice distributions the following theorem applies with a minor 
amendment. For convenience we separate the two cases. 
Theorem 1. If F is not a lattice distribution and if the third moment fi3 
exists, then D.1) holds uniformly for all z. 
Proof. Put 
D.2) & 2 
Then G satisfies the conditions of the last lemma with 
D.3) 
We use the inequality C.13) with T=a\ln where the constant a is 
chosen so large that 24 |G'(a;)| < ea for all x. Then 
r{X) - 
\Fn (x) - <7(*)| - ' ' ' aVn 
€ 
"I" ~~7=l' 
As the domain of integration is finite we can use the argument of section 2 
even when \<p\ is not integrable over the whole line. We partition the 
interval of integration into two parts. First, since F is not a lattice distri- 
bution the maximum of |9>(?)I f°r ^ ^ l?l ^ aa ls strictly less than 1 
owing to lemma 4 of XV,1. As in section 2 it follows that the contribution 
of |?| > do\ln tends to zero faster than any power of Ijn. Second, by 
the estimate B.11) for |?| <, da>fn the integrand in D.4) is 
\y/n 
and so for large n the right side in D.4) is < lOOOe/Vw. Since e is 
arbitrary this concludes the proof. > 
This argument breaks down for lattice distributions because their character- 
istic functions are periodic (and so the contribution of |?| > dayn does 
not tend to zero). The theorem can nevertheless be saved by a natural 
540 EXPANSIONS RELATED TO CENTRAL LIMIT THEOREM XVI.4 
reformulation which takes into account the lattice character. The distri- 
bution function F is a stepfunction, but we shall approximate it by a 
continuous distribution function F# with polygonal graph. 
Definition. Let F be concentrated on the lattice of points b ± nh, tut 
on no sublattice (that is, h is the span of F). 
The polygonal approximant F# to F is the distribution function with a 
polygonal graph with vertices at the midpoints b ± (n+\)h lying on the graph 
ofF. 
Thus 
D.5) F#(x) = F(x) if x=b± (n + \)h 
D.6) F#(x) = \[F{x) + F(x-)} if x = b±nh. 
Now Fn is a lattice distribution with span 
<4-7> *" - ifn' 
and hence for large n the polygonal approximant Ff is very close to Fn. 
Theorem 2.5 For lattice distributions the expansion D.1) holds with Fn 
replaced by its polygonal approximant F#. 
In particular, D.1) is true at all midpoints of the lattice for Fn (with span hn), 
yjhile at the points of the lattice D.1) holds with Fn(x) replaced by 
\[Fn(x) + Fn(x~)}. 
Proof. The approximant F# is easily seen to be identical with the con- 
volution" of F with the uniform distribution over — \h < x < \h. Accord- 
ingly, F# is the convolution of Fn with the uniform distribution over 
— \hn < x < \hn, and we denote by G# the convolution of this distribution 
with G, that is 
D.8) G#(x) = h~l f "/2G(x-y) dy. 
J-hn/2 
If M denotes the maximum of \G"\ it follows from the two-term Taylor 
expansion of G about the point x that 
D.9) |G#(x) - G(x)\ < \Mhl = 0A/*), 
and to prove the theorem it suffices therefore to show that 
D.10) \F${x) - G#(x)\ = o(l/Vn). 
0 Instead of replacing Fn by F% one can expand F* — Fn into a Fourier series and 
add it to the right side in D.1). In this way one arrives formally at a form of the theorem 
proved by Esseen by intricate formal calculations. See, for example, the book by Gnedenko 
and Kolmogorov. 
XVI.4 EXPANSIONS FOR DISTRIBUTIONS 541 
Since, taking convolutions corresponds to multiplying the transforms, we 
conclude from D.4) that 
D.11) 
\con@\ 
where <on(?) = (sin \hnQj(\hnQ is the characteristic function of the uniform 
distribution. The estimates used for D.4) apply except that a new argument is 
required to show that 
D.i2) P \nai°n)«>na)\ r1 di = \ \ala 
<pn{y) sin & 
2 
l\\ 
dy = o(±). 
By Lemma 4 of XV, 1 the characteristic function <p has period 2v/h, and the 
same is obviously true of |sin \hy\. It suffices therefore to prove that 
D.13) 
But this is trivially true because within a neighborhood of the origin 
\<p(y)\ < e-*ay2 while outside this neighborhood \<p(y)\ is bounded away 
from 0 and hence the integrand in D.13) decreases faster than any power of 
n. The integral is therefore actually O(\/n). > 
We turn to higher-order expansions. The proof of D.1) differs from the proof of B.3) 
only by the smoothing, which accounts for the finite limits in the integral D.4). The same 
smoothing can be applied to the higher expansions B.12), but it is obvious that to achieve an 
error term of the order of magnitude /i"*1"*1 we shall have to take T^an^1. Here one 
difficulty arises. The proof of D.1) depended on the fact that the maximum of |?>(?/(oV n))\ 
in 8aVn < i?| < T is less than one. For non-lattice distributions this is always true when 
T = aVn, but not necessarily when T increases as some higher power of n. For higher- 
order expansions we are therefore compelled to introduce the assumption that 
D.14) Iimsupi9>(?)l < 1 
which for non-lattice distributions implies that the maximum q3 of \q>(Q\ for |?| > 6 is 
less than 1. With this additional assumption the method of proof given in detail for D.1) 
applies without change to the expansions B.12) and leads to 
Theorem 3. 7/D.14) holds and the moments fia,. . ., fiT exist, then as n -> oo, 
D.15) Fn{x) - 3l(x) - n(x) 
uniformly in x. Here Rk is a polynomial depending only on ju1, . . . , fxT but not on n and 
r (or otherwise on F). 
The expansion D.15) is simply the integrated version of B.12) and the polynomials R^ 
are related to those in B.12) by 
d 
D.16) n(x) Pk(x) = — n(x) Rk(x). 
542 EXPANSIONS RELATED TO CENTRAL LIMIT THEOREM XVI.5 
There is therefore no need to repeat their construction. The condition D.14) is satisfied by 
every non-singular F. 
D.15) is called the Edgeworth expansion of F. If F has moments of all orders, one is 
tempted to let r ->- oo, but the resulting infinite series need not converge for any n. 
(Cramer showed that it converges for all n iff e**2 is integrable with respect to F.) The 
formal Edgeworth series should not be confused with the Hermite polynomial expansion 
D.17) Fnix) - 9i(x) = 2 ck Hk(x)e~l*2 
which is convergent whenever F has a finite expectation, but is without deeper prob- 
abilistic meaning. For example even if it is possible to expand each Fn into a series of 
the form D.17) the coefficients are not indicative of the speed of convergence Fn -»¦ 91. 
5. THE BERRY-ESSEEN THEOREMS6 
The following important theorem was discovered (with radically different 
proofs) by A. C. Berry A941) and C. G. Essen A942). 
Theorem 1. Let the Xk be independent variables with a common distribution 
F such that 
E.1) E(Xfc) = 0, E(X{) = a2 > 0, E(|Xfc|3) = p < oo, 
and let Fn stand for the distribution of the normalized sum 
Then for all x and n 
E.2) \Fn(x)- 3K*)| <~T-p- 
ayn 
The striking feature of the inequality is that it depends only on the first 
three moments. The expansion D.1) provides a better asymptotic estimate, 
but the speed of convergence depends on more delicate properties of the under- 
lying distribution. The factor 3 on the right could be replaced by a better 
upper bound C but no attempt is made in our setup to achieve optimal 
results.7 
6 This section uses the smoothing inequality C.13) (with G standing for the normal 
distribution) but is otherwise independent of the preceding sections. 
7 Berry gives a bound C <, 1.88, but his calculations were found in error. Esseen 
gives C <[ 7.59. Unpublished calculations are reported to yield 'C <? 2.9 (Esseen 1956) 
and C <; 2.05 (D. L. Wallace 1958). Our streamlined method yields a remarkably good 
bound even though it avoids the usual messy numerical calculations. No substantial im- 
provement can be expected without improving the error term 24m/rr in C.13). 
XVI.5 
THE BERRY-ESSEEN THEOREMS 
543 
Proof. The proof will be based on the smoothing inequality C.13) with 
F = Fn and G = 9t. For T we choose 
E.3) 
3 p 
the last inequality being a consequence of the moment inequality a3 < p. 
[See V,8(c).] Since the normal density n has a maximum m < f we get 
E.4) 
— 
To appraise the integrand we note that the familiar expansion for <xn — /?n 
leads to the inequality 
E.5) |aw - 0"| ^ «|a - l| • yn~x if |a| < y, |/S| < y. 
We use this with a = <p(^l<yy/n) and /? = e~^2/n. From the inequality 
XV,D.1) for e" we have 
•oo 
E.6) 
and hence 
E.7) 
We conclude that for |?| <> T 
(eitx - 1 - 
if 
E.8) 
2 
6a n 
< e~^ 
Since a3 < p the assertion of the theorem is trivially true for y/n <? 3 
and hence we may assume n > 10. Then 
E.9) iK^W^r1 ^ s"*1. 
and the right side may serve for the bound yn~x in E.5). Noting that 
er* - I + x ^ ^«2 for a: > 0 we get from E.6) 
E.10) n 
-I — e 
^ficr + c. 
6<j\/n $n 
Since v« > 3 it follows from E.5) and E.9) that the integrand in E.4) is 
E.H) 
\C\ 
544 EXPANSIONS RELATED TO CENTRAL LIMIT THEOREM XVI.5 
This function is integrable over — oo < ? < oo, and simple integrations 
by parts now show that 
E.12) ttT\Fn(x) - 3l(x)| < fV^ + f + 10. 
Since \l-n <| the right side is < -4* < 4^ ancj s0 E2) is true. > 
The theorem and its proof can be generalized to sequences {Xk} with vary- 
ing distributions as follows. 
Theorem 2.8 Let the Xk be independent variables such that 
E.13) P(Xfc) = 0, P(Xj) = al P(|X3J) = Pk 
Put 
/c 1 a\ 2 2. ,2 
E.14) sn = ax + • • • + <rn, rn = Pl + • • • + Pn 
and denote by Fn the distribution of the normalized sum (X^- • - + Xn)/.yn. 
Then for all z and n 
d 15^ IF (r\ — ^(r\\ <" f> — 
Proof. If o)k stands for the characteristic function of Xk the starting 
inequality E.4) is now replaced by 
E.16) tt \Fn(x) - 3l(x)| < 
J-T 
This time we choose 
sj \sj 
E.17) T = l--- 
Instead of E.5) we now use 
n . 
E.18) let! • • • <xn - Pi • ' • Pn\ < 2^1' ' * Yk-i*k-PkY*+i '"Yn 
valid if |afc| < yk and \Pk\ < yk. This inequality will be applied to 
E.19) a, = <okaisn), pk = e-W/-V \Q < T. 
By analogy with E.8) we have 
E-20) Ka/O| < 1 - \^C + f\\tf < exp (- ? + ff) 
2 s* 6S; \ 2s; 6s; / 
provided okT < sny/2. To obtain a bound yk applicable for all k we change 
8 Due (with an entirely different proof) to Esseen. 
XVI.5 THE BERRY-ESSEEN THEOREMS 545 
the coefficient ¦? to f and put 
Obviously \pk\ < yk, and from E.20) also |ocfc| < yk for k such that 
okT <, lsn. But from the moment inequality ph > a\ it follows that yk > 1 
if okT > %sn, and hence |afc| < yk for all k. 
The theorem is trivially true when the right side in E.15) is > 1, that is, if 
fj^n > i- Accordingly we assume from now on that rjs\ < \ or T > -V6-. 
The minimum value of yk is assumed for- some k such that 
n < 4/3 T < i, and hence yfc > e^2/32 for all k. Thus finally 
E-22) \yx ¦ ¦ ¦ yk_xf3k+1 • • • yn\ ? exp ?2 - ± 
\ 2 
By analogy with E.10) we get 
n r T 
f< TX\ VU a I ^ ' n \Y\ $_, b4 
1 J.jLJ) 7 OCfc — Pvl ^» C "t" 
*=i 6sn 8sn *=i 
To appraise the last sum we recall that a\ <, p| <, r$ • pk whence 
E.24) 
These inequalities show that the integrand in E.16) is 
E.25) 
and hence finally 
E.26) ttT \Fn{x) - 9(l(x)| ^ ffV27r + tt • 64 + 9.6. 
The right side is <167r/3, and thus E.26) implies E.15). > 
Recently much attention has been paid to generalizations of the Berry- 
Esseen theorem to variables without third moment; the upper bound is 
then replaced by a fractional moment or some related quantity. The first 
step in this direction was taken by M. L. Katz A963). The usual calculations 
are messy, and no attempt has been made to develop unified methods applic- 
able to the several variants. Our proof was developed for this purpose and 
can be restated so as to cover a much wider range. Indeed, the third moment 
occurs in the proof only because of the use of the inequality 
\eitx _ 1 _ itx _|_ |,2x2j ^ i |te|3> 
Actually it would have sufficed to use this estimate in some finite interval 
\x\ < a and to be otherwise satisfied with the bound t2z2. In this way one 
obtains the following theorem obtained by different methods and with an 
unspecified constant by L. V. Osipov and V. V. Petrov. 
546 EXPANSIONS RELATED TO CENTRAL LIMIT THEOREM XVI.5 
Theorem 3. Assume the conditions of theorem 2 except that no third 
moments need exist. Then for arbitrary rk > 0 
E.27) \Fn(x) - g&OOl < 6(S;3 f |z|3 F{dx) + s;2 f z2 F{<fc}V 
Simple truncation methods permit one to extend this result to variables 
without moments.9 
6. EXPANSIONS IN THE CASE OF VARYING COMPONENTS 
The theory of sections 2 and 4 is easily generalized to sequences {Xk} of 
independent variables with varying distributions Uk. In fact, our notations 
and arguments were intended to prepare for this task and were therefore 
not always the simplest. 
Let E(Xk) = 0 and E(X{) = a\. As usual we put si = a\ + • • • + <r*. 
To preserve continuity we let Fn again stand for the distribution of the 
normalized sum (Xx+- • -+Xn)/sn. 
To fix ideas, let us consider the one-term expansion D.1). The left side 
has now the obvious analogue 
..(n) 
F.1) Dn(x) = Fn(x) - ?L 
where 
F.2) Mn) 
In the case of equal components it was shown that DJx) = o{\j\Jn). 
Now Dn is the sum of various error terms which, in the present situation 
need not be of comparable magnitude. In fact, if the Xk have fourth 
moments it can be shown that under mild further conditions 
F.3) \Dn(x)\ = O(n25-6) + O(ns-*). 
Here either of the two terms can preponderate depending on the behavior of 
the sequence ns~2 which may fluctuate between 0 and oo. In theory it 
would be possible to find universal bounds for the error,10 but these would 
9 For details see W. Feljer, On the Berry-Esseen theorem, Zs. Wahrscheinlichkeitstheorie 
verw. Gebiete, vol. 10 A968) pp. 261-268. It is surprising that the unified general method 
actually simplifies the argument even in the classical case and, moreover, leads without 
effort to better numerical estimates. 
10 For example, in Cramer's basic theory the bound is of the form 
Dn(x) = 
which may be worse than F.3). 
XVI.6 EXPANSIONS IN THE CASE OF VARYING COMPONENTS 547 
be messy and too pessimistic in individual cases arising in practice. It is 
therefore more prudent to consider only sequences {Xk} with some typical 
pattern of behavior, but to keep the proofs so flexible as to be applicable in 
various situations. 
For a typical pattern we shall consider sequences such that the ratios s2/n 
remain bounded between two positive constants.11 We show that under mild 
additional restrictions the expansion D.1) remains valid and its proof requires 
no change. In other situations the error term may take on different forms. 
For example, if s2n — o(n) it can be said only that \Dn(x)\ = o(n/sl). 
However, the proof is adaptable to this situation. 
The proof of D.1) depended on taking the Fourier transform of Dn(x). 
If cok denotes the characteristic function of Xk this transform may be 
written in the form 
F 4) envnin/sn) _ e-k2 _ nv"n 
6 
2 _ nv 
6s3 
where 
6sn 
F-5) vnU) = n-1J>g a>»@ 
Now this is exactly the same form as used in the proof of D.1), except that 
there yn(?) = log (p(?) was independent of n. Let us now see how this 
dependence on n influences the proof. Only two properties of v were 
used. 
(a) We used the continuity of the third derivative <p'" to find an interval 
|?| < 6 within which v™ varies by less than e. To assure that this <5 can 
be chosen independent of n we have now to assume some uniformity 
condition concerning the derivatives co"k' near the origin. To avoid uninterest- 
ing technical discussions we shall suppose that the moments E(X?) exist 
and remain bounded. Then the cog have uniformly bounded derivatives 
and the same is true of v™. 
(b) The pfoof of D.1) depended on the fact that \<pn(Q\ = o(l/V/i) 
uniformly for all ? > <5. The analogue now would be 
F.6) K(?) • • • o>B@l = o(\[y/n) uniformly in ? > <5 > 0. 
This condition eliminates the possibility that all Xk have lattice distri- 
butions with the same span, in which case the product in F.6) would be 
periodic function of ?. Otherwise this condition is so mild as to be trivially 
satisfied in most cases. For example, if the X* have densities each factor 
\(ok\ remains bounded away from 1, and the left side in F.6) decreases faster 
11 Then F.3) gives \Dn(x)\ = O(l//i) which is sharper than the bound oiM^n) obtained 
in D.1). The improvement is due to the assumption that fourth moments exist. 
548 EXPANSIONS RELATED TO CENTRAL LIMIT THEOREM XVI.6 
than any power of \Jn unless |a>n(?)|-> 1; that is, unless the Xn tend to be 
concentrated at one point. Thus in general the stronger condition 
F.7) K@ • • • @,@1 = o(n~a) uniformly in ? > d 
will be satisfied and easily verified for all a > 0. 
Under our two additional assumptions the proof of D.1) goes through 
without change and we have thus 
Theorem 1. Suppose that with some positive constants 
F.8) en < si < Cn, E(X*) < M 
for all n and that F.6) holds. Then \Dn(x)\ = o(l/V«) uniformly for all x. 
As mentioned before, the proof applies equally to other situations. For 
example, suppose that 
F.9) slfn-+O but szjn -> oo. 
The proof of D.1) carries through with T= asPJn, and since T = o(sn) 
the condition F.6) becomes unnecessary. In this way one arrives at the 
following variant. 
Theorem la. If F.9) holds and the E(X?) are uniformly bounded then 
\Dn (x)\ = o(nfs3n) uniformly in x. 
The other theorems of sections 2 and 4 generalize in like manner. For 
example, the proof of theorem 3 in section 4 leads without essential change 
to the following general expansion theorem;12 
Theorem 2. Suppose that 
F.10) 0 < c < E(|X|V) < C < oo, v = 1, . . . , r + 1, 
and that F.7) holds with a = r + 1. Then the asymptotic expansion D.15) 
holds uniformly in x. 
The polynomials Rj depend on the moments occurring in F.10) but for 
fixed x the sequence {R,(x)} is bounded. 
7. LARGE DEVIATIONS13 
We begin again by considering our general problem in the special case of 
variables with a common distribution F such that E(Xk) = 0 and 
12With slightly milder uniformity conditions this theorem is contained in Cramer's 
pioneer work. Cramer's methods, however, are now obsolete. 
1S This section is entirely independent of the preceding sections in this chapter. 
XVI.7 LARGE DEVIATIONS 549 
E(X|) = a2. As before, _Fn stands for the distribution of the normalized 
sums (Xx + - • •+Xn)/orV«. Then Fn tends to the normal distribution SR. 
This information is valuable for moderate values of x, but for large x both 
Fn{x) and 9R(x) are close to unity and the statement of the central limit 
theorem becomes empty. Similarly most of our expansions and approxima- 
tions become redundant: One needs an estimate of the relative error in 
approximating 1 — Fn by 1 — 5R. Many times we would like to use the 
relation 
G.1) l^IM 
1 - 
in situations where both x and n tend to infinity. This relation cannot be 
true generally since for the symmetric binomial distribution the numerator 
vanishes for all x > y/n. We shall show, however that G.1) is true if x 
varies with n in such a way that xn~^ -> 0 provided that the integral 
Jf+OO 
I *• F{dx] 
— 00 
exists for all ? in some interval |?| < ?0. [This amounts to saying that the 
characteristic function <p(?) = /(/?) is analytic in a neighborhood of the^ 
origin, but it is preferable to deal with the real function /.] 
Theorem 1. If the integral G.2) converges in some interval about the origin, 
and if x varies with n in such a way that x-> oo and x = o(n^), then 
G.1) is true. 
Changing x into — x we obtain the dual theorem for the left tail. The 
theorem is presumably general enough to cover "all situations of practical 
interest," but the method of proof will lead to much stronger results. 
For the proof we switch from / to its logarithm. In a neighborhood of 
the origin 
G.3) y@ = log/@ = Z 7? ?* 
defines an analytic function. The coefficient ipk depends"only on the moments 
ju1, . . . , /uk of the distribution F and is called the semi-invariant of order k 
of F. In general y)x = fix, ip2 = o2, ...... In the present case fix = 0 and 
therefore y^ = 0, ip2 = cr2, Wz = /> • • • • 
The proof is based on the technique of associated distributions.14 With 
the distribution F we associate the new probability distribution V such that 
G.4) V{dx] = e-v(s)esx F{dx], 
14 It was employed in renewal theory XI ,6 and for random walks XII ,4. 
550 EXPANSIONS RELATED TO CENTRAL LIMIT THEOREM XVI.7 
where the parameter s is chosen within the interval of convergence of ip. 
The function 
plays for V the same role as does / for the original distribution F. In 
particular, it follows by differentiation of G.5) that V has expectation 
ip'(s) and variance ip"(s). 
The idea of the proof can now be explained roughly as follows: It is 
readily seen from either G.4) or G.5) that the distributions Fn* and Vn* 
again stand in the relationship G.4) except that the norming constant e~v{s) 
is replaced by e~nxp{s). Inverting this relation we get 
G.6) 1 - Fn{x) = 1 - Fn*(xayfh) = enxf{s) [* e-*vVv*{dy}. 
Jxov n 
In view of the central limit theorem it seems natural here to replace Vn* 
by the corresponding normal distribution with expectation ntp'(s) and 
variance mp"(s). The relative error committed in this approximation will be 
small if the lower limit of the integral is close to the expectation of V*, 
that is, if x is close to tp'(s)yjnlo. In this way one can derive good approxima- 
tions to 1 — Fn(x) for certain large values of x, and G.1) is among them. 
Proof. In a neighborhood of the origin ip is an analytic function with a 
power series of the form 
G.7) v(s) = 1 + JorV -f l^3 + • • • . 
ip is a convex function with y>'@) = 0, and hence increases for s > 0. The 
relation 
G.8) y/mp'is) = ax s > 0, x > 0, 
therefore establishes a one-to-one correspondence between the variables 
s and x as long as s and x/y/n are restricted to a suitable neighborhood 
of the origin. Each variable may be considered as an analytic function of the 
other, and clearly 
G.9) s XT if ^ 
We now proceed in two steps: 
(a) We begin by calculating the quantity As obtained on replacing Vn* 
in G.6) by the normal distribution having the* same expectation nip'js) and 
the same variance ntp"(s). The standard substitution y = nip'(s) + t\imp"(s) 
yields 
G.10) A, = 
Jo 
XVI.7 LARGE DEVIATIONS 551 
Completing the square in the exponent we get 
G.11) As = exp (n[y(s) - sy>'(s) + fry E)]) • [I - W(sjny>"(s))]. 
The exponent and its first two derivatives vanish at the origin and so its 
power series starts with cubic terms. Thus 
G.12) A, = [1 - 9ft(sVV(s)>] • [1 H- O(ns*)], s -> 0. 
If ns* -> 0 or, what amounts to the same, if x = o(nty we may rewrite 
G.12) in the form 
G.13) As=[l- 
where we put for abbreviation 
G.14) x = ss/ny}"(s). 
It remains to show that in G.13) we may replace x by x. The power series 
for (x—x)/n is independent of n and a trite calculation shows that it starts 
with cubic terms. Accordingly, 
G.15) \x — x\ = O(V«53) = O(xzjn). 
From 1; VII,A.8) we know that as f-> oo 
G.16) —^ t. 
1-.3K0. 
Integrating between x and x we get for x —»- oo 
— O(t • Irr r\\ — f)(T*lrt\ 
1 - 
G.17) log 
and hence 
G.18) 
Substituting into G.13) we get finally if x -> oo so that x = o(n%) 
G.19) 
(b) If S^s denotes the normal distribution with expectation nip'is) and 
variance ntp"(s) then >4S stands for the right side in G.6) when Vn* is 
replaced by 9ts. We now proceed to appraise the error committed by this 
replacement. By the Berry-Esseen theorem (section 5) 
G.20) \Vn*(y) - %(y)\ < 
for all y, where Ms denotes the third absolute moment of the distribution 
552 EXPANSIONS RELATED TO CENTRAL LIMIT THEOREM XVI.7 
V. After a simple integration by parts it is therefore seen that 
G.21) 
\l-Fn(x)-As\ < ~± enV(S)L-S,'(S) + s f°° e-svdy\ = 
O yjn L Jnip'is) J 
But by G.11) 
G.22) As = en 
x 
and hence the right side in G.21) is As • O(x/V/7). Thus 
G-23) 1 - Fn(x) = A.\ 
In combination with G.19) this not oniy proves the theorem, but also the 
stronger 
Corollary. If x -> oo so that x = o(nl) then 
G.24) 
1 - 
We have indirectly derived a much further-going result applicable whenevei x varies 
with n in such a way that x -* oo but x = 0(Vn). Indeed, by G.23) we have then 
1 — Fn(x) ~ As with A, given by G.11). Here the argument of 3? is x, but G.18) 
shows that x may be replaced by x. We get therefore the general approximation formula 
G.25) 1 - Fn(x) = exp (n[y(*)-V(*) + iv'2(*)])[l -9l(x)] • [1 +O(x/V/«)]. 
The exponent is a power series in s commencing with the term of third order. As in G.8) 
we now define an analytic function s of the variable z by y>'(s) = oz. With this function 
we define a power series A such that 
G.26) z2 A(z) = Al23 + V + V(*) - 5y'(j) + iv'2(^). 
In terms of this series we have 
Iheorem 2.15 If in theorem 1 the condition x = o(/j») is replaced by x = o( V n), then 
15 The use of the transformation G.4) in connection with the central limit theorem seems 
due to F. Esscher A932). The present theorem is due to H. Cramer A938), and was 
generalized to variable components by Feller A943). For newer results in this case see 
V. V. Petrov, Uspekhi Matem. Nauk, vol. 9 A954) (in Russian), and W. Richter, Local 
limit theorems for large deviations, Theory of Probability and Its Applications (transl.), 
vol. 2 A957) pp. 206-220. The latter author treats densities rather than distributions. For 
a different approach leading to approximations of the form 1 — Fn(x) = exp [v(x)+o(v(x))] 
see W. Feller, Zs. Wahrscheinlichkeitstheorie verw. Gebiete vol. 14 A969), pp. 1-20. 
XVI.7 LARGE DEVIATIONS 553 
In particular, if x = o(«*), only the first term in the power series matters, and we get 
G.28) ! ~ ffi? ~ 
For an increase such that x =» o(n&) we get 
and so on. Note that the right sides may tend to 0 or oo, and hence these formulas do not 
imply an asymptotic equivalence of 1 — Fn(x) and 1 — 9l(x). Such an equivalence exists 
bnly if x = o(n*) [or, in the case of a vanishing third moment, if x = o(n*)]. Under 
any circumstances we have the following interesting 
Corollary. If x = o( Vn) then for any « > 0 ultimately 
G.30) exp (- A + c)**/2) < 1 - /"„ (x)< exp (- A - «)*2/2). 
The preceding theory may be generalized to cover partial sums of random 
variables Xk with varying distributions and characteristic functions cok. 
The procedure may be illustrated by the following generalization of theorem 
1 in which the uniformity conditions are unnecessarily severe. In it Fn 
stands again for the distribution of the normalized sum (Xx + • •; + Xn)/sn. 
Theorem 3. Suppose that there exists an interval —a, a in which all the 
characteristic functions a>k are analytic, and that 
G.31) E(|XJ3) <? Mai, 
where M is independent of n. If sn and x tend to oo so that x = 0E*), then 
G.32) 1 - Fn(x) _^ { 
with an error Oix^jsn). 
The proof is the same except that ip is now replaced by the real-valued 
analytic function ipn defined for —a < s < a by 
G-33) y>n(s) = ~ ^og cok(—is). 
n *=i 
In the formal calculations now xsn replaces xasfn. The basic equation G.8) 
takes on the form y'n(s) = xsjn. 
CH APTE R X VII 
Infinitely Divisible Distributions 
This chapter presents the core of the now classical limit theorems of 
probability theory—the reservoir created by the contributions of innumerably 
many individual streams and developments. The most economical treatment 
of the subject would start from the theory of triangular arrays developed in 
section 7, but once more we begin by a discussion of simple special cases in 
order to facilitate access to various important topics. 
The notions of infinite divisibility, stability, etc., and their intuitive meaning 
were discussed in chapter VI. The main results of the present chapter were 
derived in a different form, and by different methods, in chapter IX, but the 
present chapter provides more detailed information. It is self-contained, 
and may be studied as a sequel to chapter XV (characteristic functions) 
independently of the preceding chapters. 
1. INFINITELY DIVISIBLE DISTRIBUTIONS 
We continue the practice of using descriptive terms interchangeably for 
distributions and their characteristic functions. With this understanding the 
definition of infinite divisibility given in VI ,3 may be rephrased as follows. 
Definition. A characteristic function a> is infinitely divisible iff for each n 
there exists a characteristic function con such that 
A.1) a>l = a>. 
We shall presently see that infinite divisibility can be characterized by 
other striking properties which explain why the notion plays an important 
role in probability theory. 
Note concerning roots and logarithms of characteristic functions. It is 
tempting to refer to a>n in A.1) as the nth root of co, but to make this 
meaningful we have to show that this root is essentially unique. To discuss 
554 
XVII. 1 INFINITELY DIVISIBLE DISTRIBUTIONS 555 
the indeterminacy of roots and logarithms in the complex domain it is con- 
venient to start from the polar representation a =¦ rei0 of the complex 
number o^O. The positive number, r is uniquely determined, but the 
argument 0 is determined only up to multiples of 2n. In principle this 
indeterminancy is inherited by log a — log r + id and by c&n — rx/neiein 
(here ri/n stands for the positive root, and logr for the familiar real 
logarithm). Nevertheless, in any interval |?| < ?0 in which a>(?) 5* 0 the 
characteristic function co admits of a unique polar representation co@ = 
= r(Oe'*(C> silch that 0 is continuous and 0@) — 0. In such an interval we 
can write without ambiguity tog co(?) = log r(?) + j'0(?) and co1/7l(?) = 
— r1/7l(Qet*(f)/n; these determinations are the only ones that render logco 
and a>1/n continuous functions that are real at the point ? = 0. In this sense 
logco and a>1/n are uniquely determined in any interval |?| < ?0 free of 
zeros of co. We shall use the symbols log co and <o1/n only in this sense, 
but it must be borne in mind that this definition breaks down1 as soon as 
ft>(?0) = 0. > 
Let F be an arbitrary probability distribution and 99. its characteristic 
function. We recall from XV,B.4) that F generates the family of compound 
Poisson distributions 
with characteristic functions ec(<f>~1]'. Here c > 0 is arbitrary. Obviously 
co = ec«p-v is infinitely divisible (the root co1/n being of the same form 
with c replaced by c[n). The normal and the Cauchy distribution show that 
an infinitely divisible distribution need not be of the compound Poisson type, 
but we shall now show that every infinitely divisible distribution is the limit 
of a sequence of compound Poisson distributions. Basic for the whole 
theory is 
Theorem 1. Let {<pn} be a sequence-of characteristic functions. In order 
that there exist a continuous limit 
A.3) 
it is necessary and sufficient that 
A.4) «fo« 
with ip continuous. In this case 
A.5) 10 = 
1 At the end of XV,2 (as well as in problem 9 of XV.9) we found pairs of real character- 
istic functions such that <p\ = <p\. This shows that in the presence of zeros even real 
characteristic functions may possess two real roots that are again characteristic functions. 
556 INFINITELY DIVISIBLE DISTRIBUTIONS XVII. 1 
Proof. We recall from the continuity theorem in XV,3 that if a sequence 
of characteristic functions converges to a continuous function, the latter 
represents a characteristic function and the convergence is automatically 
uniform in every finite interval. 
(a) We begin with the easy part of the theorem. Assume A.4) where y 
is continuous. This implies that <pn(?) —> 1 for every ?, and the convergence 
is automatically uniform in finite intervals. This means that in any interval 
l?l < ?1 we have |1 — <pn(?)\ < 1 for all n sufficiently large. For such n 
we conclude then from the Taylor expansion for log(l— z) that 
A.6) n log cpn(l) = n log [1 - [1-^@] = 
= -n[l - <pn@] - 1 [1 - <pna)f . 
Because of A.4) the first term on the right tends to — y(?)> and since 
<pn(s) —> 1 this implies that all other terms tend to zero. Thus n log <pn —»- — tp 
or <p% —> e~v, as asserted. 
ib) The converse is equally simple if it is known that the limit at in A.3) 
has no zeros. Indeed, consider an arbitrary finite interval |?| <, ?v In it the 
convergence in A.3) is uniform and the absence of zeros of co implies that 
also 9?n(?) 5^ 0 for | ?| ^ Ci and all n sufficiently large. We can therefore 
pass to logarithms and conclude that n log <pn —>- log at, and hence 
l°g <Pn —»• 0- This implies that <pn(?) —> 1 for each fixed ?,, and the con- 
vergence is automatically uniform in every finite interval. As under (a) 
therefore we conclude that the expansion A.6) is valid, and since 
this implies that 
A.7) 
where 0A) stands for a quantity that tends to 0 as n —> bo. By assumption 
the left side tends to logo>@, and hence obviously n[\ — <pn]-+- — log ft> 
as asserted. 
To validate this argument we have to show that co(?) cannot vanish for 
any ?. For that purpose we can replace <a and <pn by the characteristic 
functions |a>|2 and l9?n|2, respectively, and hence it suffices to consider the 
special case of A.3) where all yn are real and <pn ^> 0. Let then |?| <, Ci 
be an interval in which cd(Q > 0. Within this interval — n log <pn(?) is 
positive and remains bounded. On the other hand, for |?j <, ?x, the expan- 
sion A.6) is valid, and since all terms are of the same sign it follows that 
n[l — <pn(C)] remains bounded for all |?| <, Ci- But by the basic inequality 
XV,A.7) for characteristic functions 
XVII. 1 INFINITELY DIVISIBLE DISTRIBUTIONS 557 
and so n[l — <pn(OJ remains bounded for all |?| <, 2?x. It follows that this 
interval can contain no zero of co. But then the initial argument applies 
to this interval and leads to the conclusion that co(?) > 0 for all |?| <? 4?x. 
Continued doubling shows that w(?) > 0 for all ?, and this concludes the 
proof. >. 
Theorem 1 has many consequences. On multiplying A.4) by / > 0 it is 
seen that this relation is equivalent to 
A.8) , e*51-^^'?' =«/(?)• 
The left side represents a characteristic function of the compound Poisson 
type: and therefore e*^0 is a characteristic function for every t > 0. We 
conclude in particular that co = e^ is necessarily infinitely divisible. In 
other words, every characteristic function co appearing as the limit of a 
sequence {9?"} of characteristic functions is infinitely divisible. This may be 
regarded as a widening of the definition of infinite divisibility in that it 
replaces the identity A.1) by the more general limit relation A.3). It will 
be seen in section 7 that this result may be further extended to more genera!, 
triangular arrays, but we record our preliminary result in the form of 
Theorem 2. A characteristic function co is infinitely divisible iff there exists 
a sequence {yn} of characteristic functions such that <p*—*co. 
In this case co1 is a characteristic function for every f > 0, and co(?) 5^ 0 
for all I 
Corollary. A continuous limit of a sequence {coj of infinitely divisible 
characteristic functions is itself infinitely divisible. 
Proof. By assumption <pn = coj/n is again a characteristic function, and 
so the relation wn —> co may be rewritten in the form cpnn —> co. > 
Every compound Poisson distribution is infinitely divisible, and theorem 1 
tells us that every infinitely divisible distribution can be represented as a 
limit of a sequence of compound Poisson distributions [see A.8) with / = 1]. 
In this way we get a new characterization of infinite divisibility. 
Theorem 3. The class of infinitely divisible distributions coincides .with the 
class of limit distributions of compound Poisson distributions. 
Application to processes with independent increments. As explained in VI,3 such processes 
can be described by a family (X(/)} of random variables with the property that for any 
partition tQ < tx < • ¦ • < tn the increments X(tk) — X(/fc_a) represent n mutually 
independent variables. The increments are stationary if the distribution of X(s+t) — X(s) 
depends only on the length / of the interval, but not on its position on the time axis. In 
this case X(s+t) — X(s) is the sum of n independent variables distributed as 
X(s + t/n) - X(s), and hence the distribution of X(s + t) — X(s) is infinitely divisible. 
558 INFINITELY DIVISIBLE DISTRIBUTIONS XVII.2 
Conversely, every family of infinitely divisible distributions with characteristic functions 
of the form eW can regulate a process with independent stationary increments. The results 
concerning triangular arrays in section 7 will generalize this result to processes with non- 
stationary independent increments. The increment X(/ + s) — X(s) is then the sum of 
increments XUk+i) — X(tk) and these are mutually independent random variables. The 
theorem of section 7 then applies provided the process is continuous in the sense that 
X(/ + h) — X(/) tends in probability to zero as h -»• 0. For such processes the distribution 
of the increments X(t + s) — X(t) are infinitely divisible. (Discontinuous processes of this 
type exist, but the discontinuities are of a trite nature and, in a certain sense, removable. 
See the discussion in IX,5a and IX,9.) 
Compound Poisson processes admit of a particularly simple probabilistic interpretation 
(see VI,3 and IX,5) and the fact that every infinitely divisible distribution appears as limit 
of compound Poisson distributions helps to understand the nature of the more general 
processes with independent increments. 
2. CANONICAL FORMS. THE MAIN LIMIT THEOREM 
We saw that to find the most general form of infinitely divisible character- 
istic functions o> = ev it suffices to determine the general form of possible 
limits of sequences of characteristic functions exp cn(?n— 1) of the com- 
pound Poisson type. For various applications it is desirable to state the 
problem more generally by permitting arbitrary centerings, and hence we 
seek the possible limits of characteristic functions of the form con = eVn, 
where we put for abbreviation 
B.1) yaO = cn[q>M - l - ipn& 
The con are infinitely divisible, and the same is therefore true of their 
continuous limits. 
Our problem is to find conditions under which there exists a continuous 
limit 
B.2) v@ = lim V 
It is understood that <pn is the characteristic function of a probability 
distribution Fn, the cn are positive constants, and the centering constants 
(}n are real. 
For distributions with expectations the natural centering is to zero expec- 
tation, and whenever possible we shall choose /?„ accordingly. However, we 
need'a universally applicable centering with similar properties. As it turns 
out, the simplest such centering is obtained by the requirement that for ? = 1 
the value of y>n be real! If un and vn stand for the real and imaginary part 
of (pn our condition requires that 
B.3) fin = yn(l).= f+"sin * Fn{dx}. 
J—ao 
XVII.2 CANONICAL FORMS. THE MAIN LIMIT THEOREM 559 
This shows that our centering is always possible. With it 
B.4) y,n@ = cn f+V* -1-it sin x] Fn{dx}. 
J—co 
Near the origin the integrand behaves like — ??***, just as is the case with the 
more familiar centering to zero expectation. The usefulness of the centering 
B.3) is due largely to the following 
Lemma. Let {cn} and {<pn} be given. If there exist centering constants 
fln such that rpn tends to a continuous limit y>, then B3) will achieve the same 
goal. 
Proof. Define rpn by B.1) with arbitrary /Sn, and suppose that yn-*y. 
If b denotes the imaginary part of y(\) we conclude for ? = 1 that 
B.5) cn(vn(l)-Cn)-+b. 
Multiplying by /? and subtracting, from rpn —*- \p we see that 
B.6) cn[<pna) - 1 - ivn(l)C] - y(Q - ibC 
and this proves the assertion. > 
We begin.by treating our convergence problem in a special case in which the 
solution is particularly simple. Suppose that the functions tpn and tp are 
twice continuously differentiable (which means that the corresponding 
distributions have variances; see XV,4). Suppose that not only y>n —>¦ tp, 
but also tp"n —> ip". In view of B.1) this means that 
B.7) 
n f+>V Fn{dx) - - v/'@ 
J— oc 
By assumption cnx2 Fn{dx) defines a finite measure, and we denote its 
total mass by fin. For ? = 0 we see from B.7) that jun -*- —ip"@). On 
dividing B.7) by /un we get on the left the characteristic function of a proper 
probability distribution, and as n -»¦ oo it tends to y"@/v"@). It follows 
that y>"'(O/tp" @) is the characteristic function of a probability distribution, 
and hence 
B.8) - 
= f+°VCx M{dx) 
J—co 
where M is a finite measure. From this we obtain y> by repeated integration. 
Bearing in mind that ip@) — 0 and that with our centering condition 
must be real, we get 
X 
*{">• 
560 INFINITELY DIVISIBLE DISTRIBUTIONS XVII.2 
This integral makes sense, the integra id being a bounded continuous 
function assuming at the origin the value — ??2. 
Under our differentiability conditions the limit xp is necessarily of the 
form B.9). We show next that with an arbitrarily chosen finite measure M 
the integral B.9) defines an infinitely divisible characteristic function ev. 
However, we can go a step further. For the integral to make sense it is not 
necessary that the measure M be finite. It suffices that M attributes finite 
masses to. finite intervals and that M{—z, %} increases sufficiently slowly 
for the integrals 
B.10) M+(z) = f V2 M{dy), M-(-z) = f V2 M{dy) 
Jx J—oo 
to converge for all z > 0. (For definiteness we take the intervals of inte- 
gration closed.) Measures defined by the densities \z\v dz with 0 < p < 1 
are typical examples. We show that if M has these properties B.9) defines 
an infinitely divisible characteristic function, and that all such characteristic 
functions are obtained in this manner. For this reason it is convenient to 
introduce a special term for our measures. 
Definition 1. A measure M will be called canonical if it attributes finite 
masses to finite intervals and the integrals B.10) converge for some {and 
therefore all) x > 0. 
Lemma 2. If M is a canonical measure and rp defined by B.9) then ev is 
an infinitely divisible characteristic function. 
Proof. We consider two important special cases. 
(a) Suppose that M is concentrated at the origin and attributes mass 
m > 0 to it. Then ip(Q = — m?2/2, and so ev is a normal characteristic 
function with variance m~l. 
(b) Suppose that M is concentrated on \z\ > r\ where r\ > 0. In this 
case B.9) may be rewritten in a simpler form. Indeed, z~2 M{dz] now 
defines a finite measure with total mass ju = M+(rj) + M~{—rj). Accord- 
ingly, x~z M{dz}[/bi.= F{dz) defines a probability measure with character- 
istic function <p, and obviously y(?) = piviQ ~ 1 ~ &?}, where b is a 
real constant. Thus in this case ev is the characteristic function of the 
compound Poisson type, and hence infinitely divisible. 
(c) In the general case, let m > 0 be the mass attributed by M to the 
origin, and put 
Then 
B.12) y>(?) = - ^ ?2 + lim v,@ 
XVII.2 CANONICAL FORMS. THE MAIN LIMIT THEOREM 561 
We saw that evVC) is the characteristic function of an infinitely divisible 
distribution Ur If m > 0 the addition of — ra?2/2 to y,@ corresponds 
to a convolution of Un with a normal distribution. Thus B.12) represents 
ev as the limit of a sequence of infinitely divisible characteristic functions, 
and hence ev is infinitely divisible as asserted. > 
We show next that the representation B.9) is unique in the sense that distinct 
canonical measures give rise to distinct integrals. 
Lemma 3. The representation B.9) of yj is unique. 
Proof. In the special case of a finite measure M it i^ clear that the second 
derivative y>" exists and that — y"@ coincides with the expectation of 
e** with respect to M. The uniqueness theorem for characteristic functions 
guarantees that M is uniquely determined by y", and hence by ip. 
This argument can be adapted to unbounded .canonical measures, but it 
is necessary to replace the second derivative by an operation with a similar 
effect and applicable to arbitrary continuous functions. Such operations can 
be chosen in various ways (see problems 1-3). We choose the operation 
that transforms tp into the function tp* defined by 
B.13) . v*@ = ?.@ -± TrpiC + s) ds, ¦ 
2/j J-h' 
where h > 0 is arbitrary, but fixed. For the function tp defined by B.9) 
we get ' . •. ¦ ". 
° 
B.14) ,y>*(?) == S e«x ¦ K{z) M{dx) 
«/—oo 
where we put for abbreviation 
" ¦. sin xh 
B.15) K(x) = z 
xh 
This is a strictly positive continuous function assuming at the origin the value 
/r/6 and as ^-*-±°c we have K{x) ^ x~z. The measure M* defined by 
M*{dx) — K(x) M{dx) is therefore finite, and B.14) states that y>* is (ts 
Fourier transform. By the uniqueness theorem for characteristic functions 
the knowledge of ip* uniquely determines the measure M*. But then 
M{dx) — K~l{x) M*{dz} is uniquely determined and so the knowledge of 
tp enables us to calculate the corresponding canonical measure (cf. problem 
3). ' ^ 
Our next goal should be to pro1- z that lemma 2 riescrihes the totality of'a!i 
infinitely divisible characteristic functions, but to do this we must first solve 
the convergence problem described at the beginning of this section. We put 
it now in tfte following slightly-more general form: Let {Mn\ be a sequerxe 
562 INFINITELY DIVISIBLE DISTRIBUTIONS XVII.2 
of canonical measures and 
i , HS1 nXMn{dx} + ibn? 
-co X 
where 6W is real. We ask for the necessary and sufficient conditions that 
Wn —*¦ V where y> is a continuous function. Note that functions y>n defined 
by B.1) or B.4) are a special case of B.16) with 
B.17) Mn{dx) = cnx* Fn{dx) 
Assume y>n -*¦ y, the limit y> being continuous. The transforms defined 
by B.13) then satisfy y>* -> y>*, that is 
B.18) 
f+K>K(*) Mn{dx) - y,* @, 
J—00 
where J? is the strictly positive continuous function defined by B.15). On 
the left we recognize the Fourier transform of a finite measure with total mass 
B.19) fin = \K(x) Mn{dx). 
/—00 
Clearly fin->-y>*@). It is easily seen that /*n-+0 would imply tp(Q = 0 
for all ?, and hence we may suppose y> *@) = p > 0. Then the measures 
M* defined by 
B.20) M* {dx} = - K(x) Mn{dx) 
are probability measures, and B.18) states that their characteristic functions 
tend to the continuous function y*@/v*@)- I* follows that 
B.21) M*n-+M* 
where M* is the probability distribution with characteristic function 
V>*(QIV>it(O)' But Vn may be written in the form 
B.22) VJLQ = /i J^ J*1-1'-*™* K~\x) Ml{dx) + ibnl 
OO 
The integrand is a bounded continuous function of x, ana so B.21) implies 
that the integral converges. It follows that bn-+b and our limit y is of the 
form 
B.23) «0 = /• \+" e*'-1-iisinX K-W M\dX) + tbl. 
J-oo X 
Since A/* is a probability measure it is clear that the measure M defined by 
B.24) M{dx) - * 
XVII.2 CANONICAL FORMS. THE MAIN LIMIT THEOREM 563 
is canonical, and 
B.25) y@ = f+" «*' " t - '"* Sin X M{dx] + i6?. 
J—oo X 
This shows that, except for the irrelevant centering term ibt,, all our limits 
are of the form described by lemma 2. As already remarked, functions ipn 
defined by B.1) are a special case of B.16), and hence we have solved the 
convergence problem formulated at the beginning of this section. We state 
the result as 
Theorem 1. The class of infinitely divisible characteristic functions is identical 
with the class of functions of the form ev with xp defined by B.25) in terms ofa 
canonical measure M and a real number b. 
In other words, except for the arbitrary centering there is a one-to-one 
correspondence between canonical measures and infinitely divisible 
distributions. 
In the preceding section we have emphasized that the conditions 
M*^>-M* and bn—>b are necessary for the relation y>rt—*-y>. Actually 
we have also shown the sufficiency of these conditions, since xpn could be 
written in the form B.22) which makes it obvious that xpn tends to the limit 
defined by B.23). We have thus found a useful limit theorem, but it is 
desirable to express the condition M*-+M* in terms of the canonical 
measures Mn and M. The relationship between Mn and M* is defined 
B.22). In finite intervals K remains bounded away from 0 and oo, and so 
for every finite / the relations M*{I}^M*{I} and Mn{I} -> M{I) 
imply each other. As x —>- oo the behavior of K is nearly the same as that 
of x~2, and hence M*{x, oo} ~ M+{x) where M+ stands for the integral 
occurring in the definition B.10) of canonical measures. Thus proper 
convergence M* —*- M* is fully equivalent to the conditions 
B.26) Mn{I} - M{I} 
for all finite intervals of continuity for M, and 
B.27) M\{x) -> M+(x), M-(x) -> M~{x) 
at all points x > 0 of continuity. In the special case of canonical measures 
of the form Mn{dx) = cnx2 Fn{dx] (with Fn a probability distribution) 
these relations take on the form 
B.28) c.JV Fn{dx} - 
and 
B.29) cn[l - Fn(x)\ - M+(x)t cnFn(-x) - M'(-x). 
If / = a,b is a finite interval of continuity not containing the origin, then 
564 INFINITELY DIVISIBLE DISTRIBUTIONS XVII.2 
B.28) obviously implies cnFn{I} -> M+(a) - M+(b) and so B.29) may be 
taken as an extension of B.28) to semi-infinite intervals. An equivalent 
condition is that no masses flow out to infinity in the sense that to each e > 0 
there corresponds a t such that 
B-30) cn[l - Fn(r) + Fn(-r)] < e 
at least for all n sufficiently large. In the presence of B.28) the conditions 
B.29) and B.30) imply each other. [Note that the left side of B.30) is a 
decreasing function of t.] Sequences of canonical measures Mn{dx) = 
= cnz2 Fn{dx) will occur so frequently that it is desirable to introduce a 
convenient term for reference. 
Definition 2. A sequence {Mn} of canonical measures is said to converge 
properly to the canonical measure M if the. conditions B.26) and B.27) are 
satisfied. We write Mn —*- M iff this is the case. 
With this terminology we can restate our finding concerning the con- 
vergence y>n —»- y as follows. 
Theorem 2. Let Mn be a canonical measure and xpn be defined by B.16). 
In order that ipn tends to a continuous limit ip it is necessary and sufficient 
that there exist a canonical measure M such that Mn —*- M, and that 
bn —*- b. In this case ip is given by B.25). ' 
In the following we shall use this theorem only in the special case 
B.31) V 
where <pn is the characteristic function of a probability distribution Fn. 
Our conditions then take on the form 
B.32) cnx*Fn{dz}-+M{dx}, cn{pn-bn)^b 
where we put again 
B.33) /?„= sin z-Fn{dz}. 
J—OO 
By virtue of theorem 1 of section 1 our conditions apply not only to sequences 
of compound Poisson distributions, but also to'more general sequences of 
the form {#;}. 
Note on other canonical representations. The measure M is not the one encountered in 
the literature. In his pioneer work P. Levy used the measure A defined outside the origin 
by A{dx} = x~2M{dx} and which represents the limit of nFn{dx). It is finite on intervals 
\x\ > 6 > 0 but unbounded near the origin. It does not take into account the atom of M 
at the origin, if any. In terms of this measure B.9) takes on the form 
B.34) v@ = -i<72?2 + iK + l»m f [e«* - 1 + /{ sin x] A{dx}. 
J 
XVII.2a DERIVATIVES OF CHARACTERISTIC FUNCTIONS 565 
This is (except for a different choice of the centering function) P. Levy's original canonical 
representation. Its main drawback is that it requires many words for a "full description of 
all required properties of the measure A. 
Khintchine introduced the bounded measure K denned by K{dx] = A +x2)~x M{dx} 
This bounded measure may be chosen arbitrarily, and Khintchine's canonical represen- 
tation is given by 
B.35) *(?) = ibl + J \e«* - 1 - j-L-J -L- K{dx}. 
It is easiest to describe since it avoids unbounded measures. This advantage is counter- 
balanced by the fact that the artificial nature of the measure K complicates many argu- 
ments unnecessarily. Stable distributions and the example 3(/) illustrate this difficulty. 
2a. DERIVATIVES OF CHARACTERISTIC FUNCTIONS 
Let F be a probability distribution with' characteristic function q>. It was shown in 
XV,4 that if F has an expectation p then <p has a derivative <p with g/@) = ijti. The 
converse is false. The differentiability of <p is closely connected with the law of large numbers 
for a sequence {Xn} of independent random variables with the common distribution F, 
and hence many studies were concerned with conditions on F that will ensure the existence 
of <p''. This problem was solved by E. J. G. Pitman in 1956 following a partial answer by 
A. Zygmund A947) who had still to impose smoothness conditions on q>. In view of the 
formidable difficulties of a direct attack on the problem it is interesting to see that its 
solution follows as a simple corollary of the last theorem. 
Theorem. Each of the following three conditions implies the other two. 
(i) /@) = />. 
(ii) As / -*¦ oo. 
B.36) t[\ - F(t)+F(t)]-+O, I xF{dx}-+ft. 
(iii) The average (Xx + - • -+Xn)/« tends in probability to /.i. 
Proof. The real part of <p being even, the derivative <p'@) is necessarily purely imagin- 
ary. To see the connection between our limit theorem and the relation <p @) = j> it is 
best to write the latter in the form 
B.37) t[<p(?lt) — 1] -*¦ i/ut,, /->oo. 
If / runs through a sequence {cn} this becomes a special case of B.31) with <pn(Q = 
= <p(?lcn) and Fn(x) = F{cnx). Thus theorem 2 asserts that B.37) holds iff 
B.38) tx2 F{t dx} — 0, t\ sin a; F{t dx} -> /i. 
J-co 
(a) Assume B.36). An integration by parts shows that for arbitrary a > 0 
B.39) / x2 F{tdx} < 4 tx[\ - F(tx) + F(-tx)] dx. 
J-a Jo 
As / -* oo the integrand tends to 0, and hence the same is true of the integral. Since 
566 INFINITELY DIVISIBLE DISTRIBUTIONS XVII.3 
\t sin x/t — x\ < Cxz it follows easily that B.38) is true and this entails B.37). Conversely 
B.38) dearly implies B.37). Thus conditions (i) and (ii) are equivalent. 
(b) According to theorem 1 of section 1 we have 9>n(?//») -*¦ e*"C iff 
U.40, n[<pU/n) - 1]-+ifi?. 
In other words, the law of large numbers applies iff B.37) holds when / runs through the 
sequence of positive integers. Since the convergence of characteristic functions is auto- 
matically uniform in finite interva s it is clear that B.40) implies B.37) and so the con- 
ditions (i) and (iii) are equivalent. 
That B.36) represents the necessary and sufficient conditions for the law of large numbers 
(iii) was shown by different methods in theorem 1 of VII,7. > 
3. EXAMPLES AND SPECIAL PROPERTIES 
We list a few special distributions and turn then to properties such as 
existence of moments and positivity. They are listed as "examples" partly 
for clarity of exposition, partly to emphasize that the individual items are not 
connected. None of the material of this section is used in the sequel. 
Further examples are found in problems 6, 7, and 19. 
Examples, (a) Normal distribution. If M is concentrated at the origin 
and attributes weight a2 to it, then B.25) leads to y>(?) = — \^ and e* 
is normal with zero expectation and variance a2. 
(b) Poisson distribution. The standard Poisson distribution with expectation 
a has the characteristic function co = ev with y)(?) = a(eiC—1). We 
change the location parameters so as to obtain a distribution concentrated 
at points of the form —b + nh. This changes the exponent into. />(?) = 
= a.(e^h— 1) — ib?, which is a special case of B:25) with M concentrated 
at the single point h. The property that the measure M is concentrated at a 
single point is therefore characteristic of the normal and the Poisson distri- 
butions with arbitrary scale parameters. Convolutions of finitely many such 
distributions correspond to canonical measures with finitely many atoms. 
The most general measure M may be obtained as the limit of a sequence of 
such measures and so all infinitely divisible distributions are limits of con- 
volutions of finitely many Poisson and normal distributions. 
(c) Randomized random walks. In 11,G.7) we encountered the family of 
arithmetic distributions attributing to r = 0, ±1, ±2,. . . probability 
C.1) ar(t) = J(^je-%Bjyq t); 
here the parameters p,q, t are positive, .p + q = 1, and Ir is the Bessel 
function defined in 11,G.1). The fact that {ar} satisfies the Chapman- 
Kolmogorov equation shows that it is infinitely divisible. Its characteristic 
function co = ev is easily calculated because it differs from the Schlomilch 
XVII.3 EXAMPLES AND SPECIAL PROPERTIES 567 
expansion 11,G.8) merely by the change of variable u = \Jp\q e~*. The result 
C-2) yiO =-t + t(pe«+qe-«) 
shows that {ar(t)} is the distribution of the difference of two independent 
Poisson variables with expectations pt and qt. The canonical measure is 
concentrated at the points ±1. 
(d) Gamma distributions. The distribution with density 
gt(x) = e-** 
for x > 0 has the characteristic function yt@ = A—/?)"' which is clearly 
infinitely divisible. To put it into the canonical form note that 
C.3) (log ytU)y = it(l - iO = it f "e*" dx. 
Jo 
Integration shows that 
- -1 e~* dx. 
o x 
Thus the canonical measure M is defined by the density txe~x for x > 0. 
Here no centering term is necessary since the integral converges without it. 
(e) Hyperbolic cosine density. We saw in XV ,2 that the density f(x) = 
= 1/TTCOsha; has the characteristic function eo(?) = 1/cosh (tt?/2). To show 
that it is infinitely divisible we note that (log to)" = — Gr2/4)w2. Now a>z 
is the characteristic function of the density /2* which was calculated in 
probiem 6 of 11,9. Thus 
C-5) -? log «@ = - pV- -^-^ dx. 
Since (log w)' vanishes at the origin we get 
Jf+oo A& _ \ _ 
L 
L * 
The canonical measure therefore has density x]{e* — e~*). For reasons of 
symmetry the contribution of the term it,x vanishes, and since the integral 
converges without it, this term may be omitted from the numerator. 
(/) P. Levy's example. The function 
C.7) y@ = 2 +f 2-*[cos 
*—00 
is of the form B.9) with M symmetric and attributing weight 2k to the 
points ±2* with k = 0, ± 1 ±2,, (The series converges because 
1 — cos 2*? "w 22*~1?2 as ?-»-— oo.) The characteristic function o>.= ev 
568 INFINITELY DIVISIBLE DISTRIBUTIONS XVII.3 
has the curious property that w2(C) = <oB?)> ^nd hence a>2*(?) = <oBkQ. 
For stability (in the sense introduced in VI,1) one should have con(?) = 
= co(anQ forall n, but this requirement is satisfied only for n — 2, 4, 8, • • • • 
In the terminology of section 9 this co belongs to its own domain of partial 
attraction, but it does not have a domain of attraction. (See problem 10.) 
(g) One-sided stable densities. We proceed to calculate the characteristic 
functions corresponding to the canonical measures concentrated on 0, oo 
such that 
C.8) M&x} = Car2 0 < a < 2, C> 0. 
This example is of great importance because it will turn out that from it we 
may derive the general form of stable characteristic functions. 
(i) If 0 < a < 1 we consider the characteristic function coa — ev* with 
i0 
This differs from the canonical form B.9) by the omission of the centering 
term, which is dispensable since the integral converges without it. To 
evaluate the integral we suppose ? > 0, and consider it as the limit as 
of 
C.10) 
J^oo -u-»;)x _ i ' 1 /• 
doc = - (X - /?) 
o s*+1 a, Jo 
a 
(for the characteristic function of gamma densities see XV,2). Now 
where d is the argument of X — /?, that is, tan 6 = — ?/A. Obviously 
B-^-Tr/2 as X -> 0+, and hence (A-i?)" -> per*"'*'. We write the final 
result in the form 
C.11) V/0-=r-C-?&=2^"'V ^>0. 
(a-l)a 
For ^<0 one gets y>a(Q as the conjugate of ipa(—I). 
(ii) When 1 < a < 2 we put 
C.12) vJLQ = C| J+1 liX dx. 
This differs from the canonical form B.9) by the more convenient centering 
to zero expectation. An integration by parts reduces the exponent in the 
XVII.3 EXAMPLES AND SPECIAL PROPERTIES 569 
denominator and enables us to use the preceding result. A routine cal- 
culation shows that ipa is again given by C.11). (The real part is again 
negative, because now cos rt\2 < 0.) 
(iii) When a = 1 we use the standard form 
ck\x\ m ^ f °° e*x - 1 - it, sin x , 
C.13) MO = C —2 dx. 
We know from XV,2 that A —cos x)j{ttx2) is a probability density, and hence 
the real part of v>i@ equals — |7r?. For the imaginary part we get 
(i ia\ f00 sin lx - I sin x J f f00 sin t,x r f00 sin x 1 
C.14) dx = hm\\ —f-dx-n ——dx\ 
JO Z e-0 LJe X* Je X* J 
When ? > 0 the substitution t,x = y reduces the first integral to the form 
of the second, and the whole reduces to 
a ie\ rv r^smxj r\- fC sin ey dy Y\ y 
C.15) -Umi —-dx=-l\\m\ ^-•-2- = -? log ?. 
f-o Jc x e-o Ji ey y 
Thus finally 
C.16) VlQ = C(-M-/C log 0, ? > 0. 
Of course, y>i(—0 is the conjugate to v'iCO- 
When a 5^ 1 the characteristic function co = evct enjoys the property 
that con(?) = w(n1/atO- This means that co is strictly stable according to 
the definition of VI,1: The sum of independent variables Xx, . . . , Xn with 
characteristic function w has the same distribution as nllaXv When a = 1 
we have con(?) = o)(nC)e~iZ log n, and hence the distribution of the sum 
differs from that of nxl*Xx by its centering. Thus xpx is stable in the wide 
sense. 
[For various properties of, and examples for, stable distributions see 
VI,1-2. Additional properties will be derived in section 5. In section 4c we 
shall see that when a < 1 the distribution is concentrated on the positive 
half-axis. This is not true for a # 1.] 
(h) General stable densities. To each yja of the preceding example there 
corresponds an analogous characteristic function induced by the canonical 
measure with the same density, but concentrated on the negative half-axis. 
To obtain these characteristic functions we have merely to change / into —i 
in our formulas. One can derive more general stable characteristic functions 
by taking linear combinations of the two extreme cases, that is, using a 
canonical measure M such that for x >.0 
C.17) M{0, x} = CPx--\ M{-x, 0} = Cqx~~\ 
Here p ]> 0, q > 0 and p + q — 1. From what was said it is clear that the 
570 INFINITELY DIVISIBLE DISTRIBUTIONS XVII.4 
corresponding characteristic function to = ev is given by 
C.18) v@ = \C\m C ^^Us S ± i(p-q) sin ^1 
a(a-l)L 2 2J 
if 0 < a < 1 or 1 < a < 2, while for a = 1 
C.19) v(Q = -|?l -C[\<n ± /(/>-?) log |?|]; 
here the upper sign applies when ? > 0, the lower for ? < 0. Note that 
for a = 2 we get y>(?) = — Up + <?)?2> that is, the normal distribution. 
It corresponds to a measure M concentrated at the origin. 
It will be shown in section 5 that (neglecting arbitrary centerings) these 
formulas yield the most general stable characteristic functions. In particular, 
all symmetric stable distributions have characteristic functions of the form 
e-alcl* with a > 0. > 
4. SPECIAL PROPERTIES 
In this section a> = ew stands for an infinitely divisible characteristic 
function with y> given in the standard form 
-oo 
X 
where M is a canonical measure and b a real constant. By the definition 
of canonical measures the integral 
D.2) 
M+(x) = ry 
Jx 
converges for all x > 0, and a similar statement holds for x < 0. 
The probability distribution with characteristic function to will be denoted 
by U. 
(a) Existence of moments. It was shown in XV,4 that the second moment 
of U is finite iff co is twice differentiable, that is, iff ip" exists. The same 
argument shows that this is the case iff the measure M is finite. In other 
words, for a second moment of U to exist it is necessary and sufficient that the 
measure M be finite. 
A similar reasoning (see problem 15 of XV,9) shows more generally that 
for any integer k ^> 1 the 2fcth moment of U exists iff M has a moment of 
order 2k — 2. 
(b) Decompositions. Every representation of M = Mx + Af2 of M as 
the sum of two measures induces in an obvious manner a factorization 
co = eVle v'2 of a> into two infinitely divisible characteristic functions. If 
XVI 1.4 SPECIAL PROPERTIES 571 
M is concentrated at a single point the same is true of Mx and Mz; in 
other words, if co is normal or Poisson,2 the same will be true of the two 
factors eVl and ev*. But any other infinitely divisible co can be split into 
two essentially different components. In particular, any non-normal stable 
characteristic function can be factorized into non-stable infinitely divisible 
characteristic functions. 
A particularly useful decomposition co = eVlev* is obtained by represent- 
ing M as a sum of two measures concentrated on the intervals \x\ < rj 
and \x\ > 7], respectively. For the latter we express M in terms of the 
measure N defined by N{dx) = ar2 M{dx). Thus we write 
D.3) 
where 
~^ M{dx) 
.*|<if X 
D.5) 
and the difference b — /S accounts for the changed centering terms in D.4) 
and D.5). 
Note that e*2 is the characteristic function of a compound Poisson distri- 
bution generated by a probability distribution F such that F{dx} = c N{dx}, 
or 
D.6) 1 - F{x) = cM+(x), x>0. 
The function eVl is infinitely differentiable. We see thus that every infinitely 
divisible distribution if is the convolution of a distribution Ux possessing 
moments of all orders and a compound Poisson distribution U2 generated by 
a probability distribution F with tails proportional to M+ and M~. It 
follows in particular that U possesses a Arth moment iff the Arth moment of 
F exists. 
(c) Positive variables. We proceed to prove that U is concentrated on 
07°o iff3 
e- i P{dx} 
0 X 
ibt, 
2 By theorem 1 of XV,8 the normal characteristic function does not admit of any factor- 
ization into non-normal characteristic functions. An analogous statement holds for the 
Poisson distribution (Raikov's theorem). 
3 This remark was made by P. Levy and is also an immediate consequence of the Laplace 
transform version XIII,G.2). It is interesting that without probabilistic arguments a formal 
verification of the assertion is cumbersome (see G. Baxter and J. M. Shapiro, Sankhya, 
vol. 22.) 
572 INFINITELY DIVISIBLE DISTRIBUTIONS XVII.4 
where b^>0 and P is a measure such that (l+x)-1 is integrable with 
respect to P. (In the original notation of D.1) we have P{dx) — x-1 
M{dx}.) 
Assume U concentrated on [0, oo) and consider the decomposition 
described by D.3)-D.5). The origin is a point of increase for the compound 
Poisson distribution U2. The distribution Ux has zero expectation, and 
therefore some point of increase s < 0. It follows that s + /? is a point of 
increase for U, and hence /? ;> 0. The same argument shows that Ux can 
have no normal component, and therefore the contribution of Ux must 
tend to 0 as rj —*¦ 0. Finally, if / is a point of increase for the probability 
distribution F generating Uz, then nt is a point of increase for U2 itself. 
It follows that F, and hence N, are concentrated on the positive half-axis, 
and so the integral in D.5) actually extends only over x > r\. The integrand 
vanishes at the origin, and therefore in the passage to the limit r\ —>¦ 0 the 
measure N need not remain bounded. However, for x > r\ we can switch 
from the measure N to P{dx] = x N{dx) (which is the same as a; M{dx}). 
Near the origin the new integrand (et?x—1 xr1 is bounded away from 0, 
and hence P must assign finite values to neighborhoods of the origin. 
In this way we obtain the representation D.7). 
Conversely, if ip is defined by D.7) then our argument shows ev to be 
the limit of characteristic functions of compound Poisson distributions 
concentrated on 0, oo. The same is therefore true of the limit distribution U. 
(d) Asymptotic behavior. The result concerning the existence of moments 
appears to indicate that the asymptotic behavior of the distribution function 
U as x -> ± oo depends only on the behavior of the canonical measure M 
near ±oo or, what amounts to the same, on the asymptotic behavior of the 
functions M+ and M~. Rather than attempting to prove this conjecture 
in the greatest possible generality we consider a typical situation. 
Suppose that M+ varies regularly at oo, that is, 
D.8) M+(x) = x~^L{x) 
where ? > 0 and L is slowly varying. Then 
D.9) 1 - U(x) ~ M+(x), x -+oo. 
Proof. Let S be a random variable having U for distribution function. 
Consider the canonical measure M as a sum Mx + Mz + Mz of three 
I- 
measures concentrated on the intervals 1, oo, —1,1, and — oo, — 1. As 
shown under (b), this induces a representation S = Xx + X2 + X8 + /? as a 
sum of three independent random variables such that: Xx has a compound 
Poisson distribution C/x generated by a probability distribution F concen- 
trated on 1, oo and defined by D.6); the canonical measure corresponding 
XVII.4 SPECIAL PROPERTIES 573 
to X2 is concentrated on —1,1; finally X3 is defined as Xx except that 
-co, 1 takes over the role of 1, oo. It is not difficult to show that 
D.10) P{XX > x) ~ M+(x), x — oo 
(see theorem 2 of VIII ,9). To prove the assertion D.9) it suffices therefore 
to show that 
D.11) P{S> x}~P{X1>x}, Z-+00. 
In this connection the centering constant /S plays no role and we assume 
/? = 0. Then for every e > 0 
D.12) P{S > x) > P{XX > A +*M • P{X2 + X3 > -ex}. 
On the other hand, since X3 < 0 
D.13) P{S > x) <> P{XX >A - e)x} + P{X2 > ex}. 
As x —»> co the last probability in D.12) tends to 1, while the last probability 
in D.13) decreases faster than any power x~a because X2 has moments of 
all orders. Thus D.12) and D.13) imply the truth of D.11). 
(e) Subordination. If ev is infinitely divisible, so is esv for every s > 0. 
By randomization of the parameter s we obtain a new characteristic function 
of the form 
D.14) cp(Q = [^ esv{°G{ds} 
Jo 
where G is an arbitrary probability distribution concentrated on 0, oo. 
The characteristic function q> need not be infinitely divisible, but it is easily 
verified that if G — Gxic G2 is the convolution of two probability distribu- 
tions then (with obvious notations) cp = cp^. It follows that if G is 
infinitely divisible then D.14) defines an infinitely divisible characteristic 
function. 
This result has a simple probabilistic interpretation: Let (X(/)} stand for 
the variables of a process with independent increments such that X(t) has 
the characteristic function eiv. If T is a positive variable with distribution 
G then q> may be interpreted as the characteristic function of the composite 
random variable X(T). Suppose then that G is infinitely divisible with 
characteristic function ey. We may envisage a second process (T(?)} with 
independent increments such that T(/) has the characteristic function ety. 
For each / > 0 we get a new variable X(T(/)), and these are again the 
variables of a process with independent increments.4 Thus T(/) serves as 
4 If G has the Laplace transform <?-p<*> then T@ corresponds to the Laplace trans- 
form e-<pU> and it is easily verified that the characteristic function of X(T(r)) is given by 
e-tp(-v). 
574 INFINITELY DIVISIBLE DISTRIBUTIONS XVII.5 
operational time. In the terminology of X,7 the new process (X(TF))} is 
obtained by subordination, with (T(f)} as directing process. We have now 
found a purely analytic proof that the subordination process always leads to 
infinitely divisible distributions. 
5. STABLE DISTRIBUTIONS AND THEIR 
DOMAINS OF ATTRACTION 
Let {Xn} be a sequence of mutually independent random variables with a 
common distribution F, and put Sn = X^ • • • +Xn. Let U be a 
distribution not concentrated at one point. According to the terminology 
introduced in VI, 1 we say that F belongs to the domain of attraction of U 
iff there exist constants an > 0 and bn such that the distribution of 
a~xSn — nbn tends to U. The exclusion of limit distributions concentrated 
at a single point serves to eliminate the trivial situation where bn-+b 
while an increases so rapidly that <ar~1Sn tends in probability to zero. 
We wish to rephrase the definition in terms of the characteristic functions 
<p and w of the distributions F and U. According to lemma 4 of XV, 1 
the distribution U is concentrated at one point iff |w(?)l = 1 for all ?. 
Accordingly, q> belongs to the domain of attraction of the characteristic 
function a> if \oj\ is not identically one, and there exist constants an > 0 
and bn such that 
E.1) 
It was shown in VI,1 that the limit oj is necessarily stable, but we shall now 
develop the whole theory anew as a simple consequence of the basic limit 
theorem of section 2. In conformity with the notations used there we put 
E.2) <pn(Q = <pttlan)e-ib>*, Fn(x) = F(an(x+bn)). 
According to theorem 1 of section 1 the relation E.1) holds iff 
E-3) n[<pna)-l]-+y>a) 
for all ?, where a> = ey>. 
Consider first the special case of a symmetric F. Then bn — 0. We know 
from theorem 1 of section 2 that E.3) implies the existence of a canonical 
measure M such that nx2 Fn{dx) —*- M{dx). To express this we introduce 
the truncated moment function 
E.4) !*(*) = (* V2 F{dy}, x>0. 
J—x 
Then at all points of continuity 
E.5) -2 n{anx) 
XVII.5 stable distributions; domains of attraction 575 
and 
E.6) 
where 
E.7) 
Jx 
The relation ?>(?/an)-»-l implies an->- oo, and therefore Sn/an and 
SJ«n+i have the same limit distribution U. It follows that the ratio an+jan 
tends to 1, and hence lemma 3 of VIII,8 applies to E.5). We conclude that 
I* varies regularly and the canonical measure M is of the form 
E.8) M{=xTx] = Car2"", x > 0 
with a <? 2. (The exponent is denoted by 2 — a in conformity with a 
usage introduced by P. Levy.) If a = 2 the measure M is concentrated at 
the origin. The convergence of the integral in E.7) requires that a > 0; for 
0 < a < 2 we find 
E.9) M+(x) = C ^-^ x~", x>0. 
a 
A similar argument applies to unsymmetric distributions F, but instead of 
E.6) we get the less appealing relations 
E.10) n[\ - F(an(x+bn))] - 
and an analogous modification applies to E.5). However, the fact that 
9>n(?)-*l and an->-co implies ?n->0, and so E.10) is actually fully 
equivalent to E.6) and the analogous relation for the left tail. 
We see thus that E.6) holds whenever F belongs to a domain of attraction 
In view of lemma 3 of VIII,8 this'means that either M+ vanishes identically, 
or else the tail 1 — F varies regularly and M+(x) = Ax~a. Then E.7) 
shows that on the positive half-axis the measure M has the density Aolx1-*. 
The same argument applies to the left tail and also to the tail sum;' the 
exponent a must therefore be common to the two tails. 
If both tails vanish identically M is concentrated at the origin. In no 
other case can M have an atom at the origin. This is so because the canonical 
measure °M corresponding to the symmetrized distribution °U is the sum of 
M and its mirror image with respect to the origin, and we saw that °M is 
either atomless or concentrated at the origin. Accordingly, when a < 2 
the canonical measure M is uniquely determined by its densities on the two 
half-axes and these are proportional to la;I-". For intervals — y,x con- 
taining the origin we have therefore 
E. 
576 INFINITELY DIVISIBLE DISTRIBUTIONS XVI1.5 
where 0 < a < 2, C > 0, and p -f q — 1 • For a = 2 the measure is 
concentrated at the origin. In accordance with E.7) this is equivalent to 
E.12) M+(z) = Cp —— x~\ M-(-z) = Cq =—^ x~\ 
a a 
The characteristic function corresponding to these measures are given by 
C.18) and C.19). They show clearly that our distributions are stable in 
the sense thai Un * differs from U only by location parameters. This means 
that each stable distribution belongs to its own domain of attraction, and we 
have therefore solved the problem of finding all distributions possessing a 
domain of attraction. We record this in 
Theorem 1. A distribution possesses a domain of attraction iff it i» stable. 
(i) The class of stable distributions coincides with the class of infinitely 
divisible distributions with canonical measures given by E.11). 
(ii) The corresponding characteristic functions are of the form co(?) — 
= eu@+M With y, defined by C.18)-C.19), and 0 < a < 2. 
(iii) As x—>co the tails of the corresponding distribution U satisfy 
E.13) x*[[-U.(x)] -> Cp ^—^ , x*U(-x) -* Cq 2 * 
a a 
The last statement is a direct corollary of E.6) if one remembers that U 
belongs to its own domain of attraction with norming constants given by 
an = nlla. [Alternatively, E.13) represents a special case of the result obtained 
in 4{d).\ ...' 
Mote that each of the three descriptions in the theorem determines U 
uniquely up to an arbitrary centering. 
, Before returning to the investigation of.the conditions under which a 
distribution F belongs to the domain of attraction of a stable distribution 
we recall a basic result concerning regular variation. According to the 
definition in VJII,8 a function L varies slowly at infinity if for each fixed 
x > 0 
E.14) , o. 
L(t) 
In this case we have for arbitrary <5 > 0 and all x sufficiently large 
E.15) z~* < L(x) < x*. 
A function fi varies regularly if it is of the form /u(x) = xpL(x). We con- 
sider in particular the truncated moment function // defined by E.4). 
Applying theorem 2 of VIII,9 with ? = 2 and rj — 0 to the distribution 
function on 0, oo defined by F{x) — F(—x), we obtain the following 
XVII.5 stable distributions; domains of attraction 577 
important result: 
If fi varies regularly with exponent 2 — a (where 0 < a < 2) then 
M*) a 
Conversely, if E.16) is true with a < 2, ///e« // and the tailsum 
1 - F(z) + F(-z) 
fflry regularly with exponents 2 — ex. and — a, respectively. If E.16) 
n/7/; a = 2 ?//ew // varies slowly. 
In deriving E.7) we saw that for a symmetric F to belong to a domain of 
attraction it is necessary that the truncated moment function fi varies 
regularly: 
E.17) fx(x) ~ x2-* L(x), x -+ oo, 
where L varies slowly. We shall now see that this is true also for un- 
symmetric distributions. When a = 2 this condition turns out to be 
sufficient, but when a < 2 the canonical measure E.11) attributes to the 
positive and negative half-axes weights in the proportion p:q and it turns 
out the two tails of F must be similarly balanced. 
We are now in a position to prove the basic 
Theorem 2. (a) In order that a distribution F belong to some domain of 
attraction it is necessary that the truncated moment function [x varies 
regularly with an exponent 2 — a @ < a < 2). [That is, E.17) holds.] 
(b) If"a = 2, this condition is also sufficient provided F is not concentrated 
at one point. 
(c) If E.17) holds with 0 < a < 2 then F belongs to some domain of 
attraction iff the tails are balanced so that as x -v oo 
1 - F(x) F(-x) 
1 - F(x) + F(-x) 1 - F(x) + F(-x) 
Note that nothing is assumed concerning the centering of F. The theorem 
therefore implies that E.17) either holds with an arbitrary centering or with 
none. The truth of this is easily verified directly except that when F is 
concentrated at a single point / the left side in E.17) vanishes identically for 
the centering at t, and varies regularly for all other centerings. 
The theorem was formulated so as to cover also convergence to the normal 
distributions. When a < 2 it appears more natural to express the main 
condition in terms of the tailsum of F rather than oo. The following 
corollaries restate the theorem in equivalent forms. 
578 INFINITELY DIVISIBLE DISTRIBUTIONS XVII.5 
Corollary 1. A distribution F not concentrated at one point belongs to the 
domain of attraction of the normal distribution iff ju varies slowly. 
This is the case iff E.16) holds with a = 2. 
Needless to say, fj, varies slowly whenever F has a finite variance. 
Corollary 2. A distribuition F belongs to the domain of attraction of a stable 
distribution with exponent a < 2 iff its tails satisfy the balancing condition 
E.18) and the tailsum varies regularly with exponent a. 
The latter condition is fully equivalent to E.16). 
Proof, (a) Necessity. Suppose that the canonical measure of the limit 
distribution U is given by E.11). In the process of deriving this relation 
we saw that a distribution belonging to the domain of attraction of U 
satisfies E.6) and its analogue for the left tail, and so 
E.19) /ill - F{anx) + F(-anx)} -* M+(x) + M~(-x.) 
Assume first a < 2, so that the right side is not identically zero. As 
already mentioned, lemma 3 of VIII,8 then guarantees that the tailsum 
1 — F(x) + F(—x) varies regularly with exponent —a. But then E.16) 
holds and so fi varies regularly with exponent 2 — a. The balancing con- 
dition E.18) is now an immediate consequence of E.6). 
There remains the case a = 2. The left side in E.19) then tends to zero, 
and thus the probability that |Xfc| > an for some k < n tends to zero. In 
order that SJan does not tend in probability to zero it is therefore necessary 
that the sum of the truncated second moments of X,^ be bounded away 
from 0. But 
E.20) ^ -co 
1 - HaJ + F(-a,) 
and hence E.16) holds with a = 2. This implies the slow variation of ju, 
and so our conditions are necessary. 
(b) Sufficiency. We shall not only prove that our conditions are sufficient, 
but shall at the same time specify norming constants an and bn that will 
guarantee convergence to a prescribed stable distribution. This is done in 
theorem 3. > 
The formulation of theorem 3 assumes knowledge of the fact that distri- 
butions in any domain of attraction with a. > 1 possess expectations. In 
the proof we shall require additional information concerning the truncated 
first moments. It is natural to formulate these results in a more general 
setting, although we shall require only the special case {I — 1. 
Lemma. A distribution F belonging to a domain of attraction with index 
a possesses absolute moments mp of all orders /? < a. If ex. < 2 no moments 
of order fi > a exist. 
XVII.5 stable distributions; domains of attraction 579 
More precisely, if ft < a then as t -»¦ oo 
E.21) 
\x\>t CL — ft 
while for a < 2 a/i*/ /? > a 
E.22) f |af F{dz} ~ -iL_ ^ _ F(t) + F(__t)]. 
J|*|<t p — ex. 
(Note that in each case the integral is a regularly varying function with 
exponent ft — a.) 
Proof. The relations E.21) and E.22) represent the general form E^15) 
and are direct consequences of theorem 2 in VIII,9 applied to the distribution 
defined on 0, oo by F(x) + F(-x). For E.21) set ?.= 2 and rj = ft and 
rj = 0. > 
It is implicit in the proof of theorem 2 that the norming constants an must 
satisfy the condition 
E.23) iUC. 
a; 
If fi varies regularly [satisfies E.17)] such an exist: one may define an 
as the lower bound of all x for which nx-^fi{x) < C. Because of the regular 
variation we have then for x > 0 
E.24) 
This means that the mass attributed by the measure nz2F{andx} to any 
symmetric interval — x, x tends to M{—x,x}. In view of E.16) the relation 
E.24) automatically entails the analogous relation E.19) for the tailsum of F. 
When a = 2 the right side is identically zero; when a < 2 the balancing 
condition E.18) guarantees that also the individual tails satisfy the required 
conditions 
E.25) «[1 - F(anx)] -> Cp ^^ x~\ nF{-anx) -* Cq ^^ aT", 
a . a 
the right sides being identical with M+(x) and M~(x). [Incidentally, when 
a < 2 the relations E.25) in turn imply E.24).] 
We have thus shown that the measures nx% F{an dx) tend properly to the 
canonical measure M. By theorem 2 of section 2 this implies that 
E.26) 
f+0° #* -l-il sin x % f+0° <#» -I-it sin x , x 
. nx~ F{an dx} _^ _ M{dx}. 
J—oo X" J— oo X" 
From this it is now easy to derive 
580 INFINITELY DIVISIBLE DISTRIBUTIONS XVII.5 
Theorem 3. Let U be the stable distribution determined (including centering) 
by the characteristic function C.18) if a ^ 1 or C.1'9) if a = 1. 
Let the distribution F satisfy the conditions of theorem 2, and let {an} 
satisfy E.23). 
(i) If 0 < a < 1 then (pn(ijan) -+ <o(Q = e^. 
(ii) If 1 < a < 2 the same is true provided F is centered to zero 
expectation. 
(iii) If ex. = I then 
E.27) (<pttl<>n)e-iK<)n - 
'where 
E.28) fcn = fsin — F{dx). 
J-oo an 
We have thus the pleasing result that when a < 1 no centering procedure 
is required, while for a > 1 the natural centering to zero expectation suffices. 
Proof, (i) Let a < 1. The integral defining y(Q in C.18) differs from the 
right side in E.26) in that the term /? sin x is missing. We show that these 
terms may be omitted also in E.26) so that 
E.29) e—7-± • nx* F{an dx) - e—^- • M{dx}. 
J— oo X" J— oo X" 
Outside a neighborhood of the origin the integrand is continuous, and since 
nz* F{an dx} -+ M{dx) the relation E.29) holds if an interval \x\ < S is 
cut out of the domain of integration. It suffices therefore to show that the 
contribution of \x\ < d to the integral on the left can be made arbitrarily 
small by choosing d sufficiently small. Now this contribution is dominated 
by 
E.30) n\ \x\ F{an dx} = f f \y\F{dy), 
J\x\<6 anj\v\<an6 
and E.22) with /? = 1 shows that the right side is ~B — <x/(l — 
which tends to \zero with 6. 
Thus E.29) holds. It can be rewritten in the form n[<p(Clan) — 1] 
and by theorem 1 of section 1 this is equivalent to the assertion 
j 
(ii) Let a > 1. The argument used under (i) carries over except that the 
modified version of E.26) now takes the form 
e- \-^ nx* F{an dx} - | e- ±-h* M{dx). 
-oo X J-oo X 
To justify it we have to show that the contribution of |a:| > / to the integral 
XVII.6 STABLE DENSITIES 581 
on the left can be made arbitrarily small by choosing / sufficiently large. 
This follows directly from E.21). 
(iii) Let a = 1. No modification is required in E.26), but to show that 
this relation is equivalent to the assertion E.27) it is necessary to prove that 
for fixed ? 
E.32) 9 
or, what amounts to the same, that 
E.33) 
For -/? < 1 the absolute moment mfi of F is finite. From the obvious 
inequality \eil - 1| < 2 |/|' we conclude that \<p(Uan) - 1| <2mfi \t\fia~^ 
and so the left side in E.33) is O(na~2fi). But the defining relation E.23) 
shows that n = O(a\+f) for every <= > 0, and so E.33) is true. *¦ 
Concluding remark. The domain of attraction of the normal distribution must not be 
confused with the notion of the "domain of normal attraction of a stable distribution U 
with exponent aa introduced by B. V. Gnedenko. A distribution F is said to belong to 
this domain if it belongs to the domain of attraction of U with norming coefficients 
an == /i1/*. The delimitation of this domain originally posed a serious problem, but 
within the present setup the solution is furnished by the condition E.23) on the norming 
constants. A distribution F belongs to the "normal" domain of attraction of U iff 
a^[l —F(x)]-*Cp and x*F(—x) ->¦ Cq as x->-oo. Here C>0 is a constant. (Note, 
incidentally, that in this terminology the normal distribution possesses a domain of non- 
normal attraction.) 
*6. STABLE DENSITIES 
It seems impossible to express stable densities in a closed form, but 
series expansions were given independently by Feller A952) and H. Berg- 
strom A953). They contain implicitly results discovered later by more compli- 
cated methods, and they provide a good example for the use of the Fourier 
inversion formula (although complex integration is used). We shall not 
consider the exponent a = 1. 
For ? > 0 we can put the stable characteristic functions in the form 
e~a^a, where a is a complex constant. Its absolute value affects only a scale 
parameter so that we are free to let a have a unit modulus and write 
a — eivyl2 with y real., Thus we put 
where in ± the upper sign prevails for ? > 0, the lower for ? < 0. [See 
the canonical form C.18).] The ratio of the real and imaginary parts are 
* This section treats a special topic and should be omitted at first reading. 
582 INFINITELY DIVISIBLE DISTRIBUTIONS XVII.6 
subject to inequalities evident in C.18); with the present notations for ev 
to be stable it is necessary and sufficient that 
,, ,. ... • i/ 0 < a < 1 
F-2> M*2-« // Ka<2. 
Since ev is absolutely integrable the corresponding distribution has a 
density. It will be denoted by p(x; a, y),t and we proceed to calculate it 
from the Fourier inversion formula XV,C.5). Knowing that p is real and 
that rp{— 0 is the conjugate of ip(?) we get 
{6.3) p(x; a, y) = tT1 Re fVto« 
Jo 
It suffices to calculate this function for x > 0 since 
F.4) P(~x; a, y) = p(x; a, -y). 
(a) The case a < 1. Consider the integrand as a function of the complex 
variable ?. When x > 0 and Im ? -»¦ — oo the integrand tends to 0 owing 
to the dominance of the linear term in the exponent. This enables one to 
move the path of integration to the negative imaginary axis, which amounts 
to using the substitution ? = (t/x)e-iiT and proceeding as if all coefficients 
were real. The new integrand is of the form e'1'01*. The exponential expan- 
sion for e~ct<x and the familiar gamma integral lead without further artifice to 
F.5) p(-x;a,y)=.Re— Z^T, M-*aexp i - (y-a) I. 
(b) The case 1 < a < 2. The use of the formal substitution 
can be justified as in the case a < 1. The new integrand is of the form 
e-t-cta t*-l-i. Expanding e~ct<x into an exponential series we get 
F.6) 
1 D / .7ry\ *r((« + l)/a)/ . f .-ny~\\n 
= — Reexpl— i~L\^ \ — ix exp — i — \\ 
a7r \ 2a/n=o n! \ L 2a J/ 
Changing the summation index « to /b — 1 and using the familiar recursion 
formula T(^+l) = s F(s) leads to 
,,„ t , 1 o .^r(l+fc/a)/ f .77, .-]\* 
F.7) p(-x;cc,y) = — ReiJt ' i-xexp -i — (y-a) . 
ttx jt=i /c! \ L 2a J/ 
We have thus proved 
XVII.7 TRIANGULAR ARRAYS 583 
Lemma 1. For x > 0 and 0 < a < 1 
F.8) p(x; a, y) = ±- f E(fca-H) (_*-*)* sin *? ( 
For a; > 0 am/ 1*< a < 2 
F.9) p(x; a, y) =* — 2 ' (-*) sin — (y-a). 
ira;*-i fe! 2a 
values for x < 0 are groin &y F.4). 
Note that F.8) provides asymptcrtic estimates for x -»¦ oo. A curious 
by-product of these formulas is as follows: 
Lemma 2. //" ? < a < I o/k/ a; > 0 then 
y* = a(y+l) — I. 
A trite check shows that y* falls within the range prescribed by F.2). 
The identity F.10) was first noticed.(with a complicated proof) by V. M. 
Zolotarev. 
7. TRIANGULAR ARRAYS 
The notion of a triangular array was explained in VI,3 as follows. For 
each n we are given finitely many, say rn, 'independent random variables 
Xfcin ' (fc = 1 > 2, . . . , rw) with distributions Fkn and characteristic functions 
<pk n. We form the row sum Sn = X, n + • • • + Xr n, and denote its 
distribution and it characteristic function by Un and a>n, respectively. 
For reasons explained in VI,3 we are interested primarily in arrays where the 
influence of individual components is asymptotically negligible. To ensure 
this we imposed the condition VI,C.2) that the variables Xfcn tend in 
probability to zero uniformly in k = 1,. .. , rn. In terms of characteristic 
functions this means that given e > 0 and ?0 > 0 one has for all n 
sufficiently large 
G.1) |1 - <p*.n@\ < « for m < to, k = 1, . . . , rn. 
Such an array is called a null array. 
In effect sections 1 and 2 are concerned with triangular arrays in which the 
distributions Fkn do not depend on k, and such arrays are automatically 
null arrays. The condition G.1) enables us to use the theory developed in the 
first two sections. In particular, it will be now shown that the main result 
carries over to arbitrary .null-arrays: if the distributions of the row-sums Sn 
584 INFINITELY DIVISIBLE DISTRIBUTIONS XVII.7 
tend to a limit, the latter is infinitely divisible.5 We shall find precise criteria 
for the convergence to a specified infinitely divisible distribution. 
For the reader's convenience we recall that a measure M is canonical if it 
attributes finite masses M{I) to finite intervals and is such that the integrals 
G.2) M+(x) = fV2 M{dy}, M~(-x) = f V2 M{dy) 
Jx J—oo 
exist for each x > 0. (Definition 1 of section 1.) 
To simplify notations we introduce a measure Mn defined by 
Tn 
Jfc=l 
This is the analogue to the measures nx2 Fn{dx} in the preceding sections. By 
analogy to G.2) we put for x > 0 
G.4) M+n(x) = 2 
Extensive use will be made of truncated variables, but the standard trun- 
cation procedure will be modified slightly in order to avoid trite complications 
resulting from the use of discontinuous functions. The modified procedure 
will replace the random variable X by the truncated variable t(x) where r 
is the continuous monotone function such that 
G.5) t{x) = x for \x\ <> a, r(x) = ±a for \x\ < a 
[obviously t(—x) = — t(x)]. For the expectations of the truncated variables 
we write 
ifc=l Jfc=l 
Theoretically it would be possible to center the XktJl in such a way that 
all & n vanish. This would simplify arguments, but the resulting criterion 
would not be directly applicable in many concrete situations. However, it is 
usually possible to center the XkiJl so as to render the /3fcn small enough 
that Bn —*¦ 0. In this case the conditions of the following theorem reduce to 
the condition Mn -»¦ M familiar from the preceding sections. In the general 
case we still have MTI{I} -> M{I) for intervals / at a positive distance from 
the origin, but neighborhoods of the origin are affected by Bn. (t^q choice 
of the truncation point a has no effect.) 
5 Concerning the implications of this result for processes with independent but non- 
stationary increments, see the concluding remarks to section 1. 
XVII.7 TRIANGULAR ARRAYS 585 
Theorem. Le/ {Xfc n} be a null array. If it is possible to find constants bn 
such that the distributions of Sn — bn tend to a limit distribution U, the bn 
o/G.6) will do.6 The limit distribution U is infinitely divisible. 
In order that convergence takes place to a limit U with canonical measure 
M it is necessary and sufficient that at all points of continuity x > 6 
G.7) M\{x) -> M+(x), M-(-x)-*M-(x) 
and that for some s > 0 
G.8) Mn{^ss} - Bn - 
In this case the distribution of Sn — bn tends to the distribution with character- 
istic function o) — ev defined by 
G.9) v<0 = P SX ~ X ~ ilT{*-} M{dx). 
J X 
X 
[The condition G.8) will automatically hold at all points of continuity.] 
Proof. We proceed by steps. 
(a) Suppose first that all variables Xk n are symmetric so that the distri- 
butions of Sn must converge without preliminary centering. The character- 
istic functions <pk n are real, and in view of G.1) the Taylor expansion 
G.10) -log^i 
nolds for arbitrary ? provided only that n is sufficiently large. The question 
is whether 
G-11) 2>g9>*.*@-Y<Q. 
All the terms of the expansion in G.10) are positive, and hence G.11) requires 
that the sum of the linear terms remains bounded. In view of G.1) this implies 
that the contribution of the higher-order terms is asymptotically negligible 
and we conclude that G.11) holds iff 
G.12) 2[%.»@-i]-v<0. 
The left side may be written in the form rn {<pn — 1 ] where <pn is the character- 
istic function of the arithmetic mean of the distributions Fk n. We are thus 
6 The theorem remains valid also with standard truncation, that is, if r is replaced by the 
truncation function vanishing outside \x\ > a. To avoid notational complications it is 
then necessary to assume that there are no atoms of M at ±a. [Part (b) of the proof gets 
more involved since it may not be possible to find 0 such that E(t(X + 0) = 0.] 
7 A distribution concentrated at a single point is infinitely divisible, the corresponding 
canonical measure being identically zero. 
586 INFINITELY DIVISIBLE DISTRIBUTIONS XVII.7 
concerned with a special case of theorem 2 of section 2 and conclude that a 
relation of the form G.12) holds iff there exists a canonical measure M such 
that Mn -> M. When Bn = 0 the conditions G.7)-G.8) are equivalent to 
Mn-*- M because for an interval / at a positive distance from the origin 
the relation Mn{I) -> M{I) is implied by G.7). This proves the theorem 
for symmetric distributions. 
(b) Suppose next that (lkn = 0 for all k and n. We shall prove that 
G.11) cannot take place unless 
G.13) 
that is, unless the sum on the left remains bounded. In this case G.11) and 
G.12) are again equivalent, and the concluding argument used under (a) 
again reduces the assertion to theorem 2 of section 2. It is true that this 
theorem refers to a centering b'n using sin x instead of t(x), but this is 
compensated by the corresponding change for the limit distribution since 
fi 1A\ L L' f+C° T(X) ~ Sm X *f f J 1 f+C° T(X) — Sm Z i/fj 1 
G.14) bn - b'n = 2 - Mn{dx] — 2 M{dx). 
J—oo X J— oo iC 
To derive G.13) from G.11) we start from the identity 
J'+OO 
[ei;x - 1 - iWx)]FkJdx] 
— 00 
valid because /?fc n = 0. For |a:| < a the integrand equals eiix — 1 — z'&c 
and is dominated by K2a;2- Since |r(a:)| < a it follows that 
G.16) 
To show that M+(a) must remain bounded we consider the array {°Xk „} 
obtained by symmetrization of {Xfcn}. The condition G.1) for null arrays 
implies that for n sufficiently large the probability of the event 
exceeds | for all k < rn. Thus \M+n(a) < °M+(a), and we know that the 
latter quantity remains bounded if convergence takes place. We conclude 
that in case of convergence M+(a) + M~(—a) remains bounded, and hence 
G.17) J>*..@ - H2 = Mn(^}.' €„(?), 
where en stands for a quantity tending to zero. On the other hand, the real 
part of the integrand in G.15) does not change sign. For |*| < a and ? 
XVII.7 
TRIANGULAR ARRAYS 
587 
sufficiently small it is in absolute value >K2;c2> anc* hence 
G.18) 
The last two inequalities show that the left side in G.11) cannot remain 
bounded unless Mn{—a,a} remains bounded, and in this case G.13) is 
implied by G.16). 
(c) We turn finally to an arbitrary null-array {X*.,,,}. Since E(T(Xfc>n—0)) 
is a continuous monotone function of 0 going from a to —a there exists 
a unique value dkn such that the variable Yfcn = Xfcn — 6kn satisfies the 
condition E(r(Yfc>n)) = 0. Clearly {Ykn} is a null-array and hence the 
theorem applies to it. 
We have thus found the general form-of the possible.limit distributions, 
but the conditions for convergence are expressed in terms of the measure 
Nn{dx) = 2 z2Fk,n{Qk.n + dx) of the artificially centered distributions of the 
Yfcn. In other words, we have proved the theorem with Mn replaced by Nn 
in G.7) and G.8), and Bn replaced by 0. 
To eliminate the centering constants dkn we recall that they tend uniformly 
to 0 and so ultimately 
It follows that the condition G.7) applies interchangeably to both arrays. 
Before turning to condition G.8) we show that the_arrays {Yfcn} and 
{Xfcn — fikin} have the same limit distribution, that is, 
G.19) 
Jfc=l 
Let Zfc>n = r(Yfc>n) — r(Xkn) + 6kn. From the definition of r it is 
clear that Zfcn vanishes unless Xfcjn | > a — ldkn\ and even there 
|Zfcin| < |0t,B| -»-0. Condition G.7) therefore guarantees that 
.20) 2. IM^fc n)l = / \P+ ¦» — 0i- «l ~"*" 0- 
fc=l 
and this is stronger than G.19). 
Finally, we turn to condition G.8). We use the sign ^ to indicate that the 
difference of the two sides tends to 0 as n —*- oo. When G.7) and G.20) hold 
it is easily seen that 
G.21) 
n{-a,a}-Bn**r± \ 
Jfc=l J\ 
\ ( 
\x\<a 
^..W 
\v\<a 
y2FkJdy+dkJ^Nn{-a,a}, 
588 INFINITELY DIVISIBLE DISTRIBUTIONS XVII.8 
and tlius G.8) is equivalent to the corresponding condition for the array 
{**.„}• > 
Example. The role of centering. For k = 1, . . . , n let Xk n be normally 
distributed with expectation. n~l and variance n~~.. With the centering to 
zero expectations the limit distribution exists and is normal. But with the 
centering constants /?*. n — n~% we have Bn ~2\/«—>• °o. It follows that 
Mn{—a, a}—>~ co. This example shows that the non-linear form of the 
theorem is unavoidable if arbitrary centerings are permitted. It shows also 
that in this case it does not suffice to consider the linear term in the expansion 
G.10) for Iog9?fc>n. > 
For further results see problems 17 and 18. 
f8. THE CLASS L 
As an illustration of the power of the last theorem we give a simple proof 
of a theorem discovered by P. Levy. We are once more concerned with partial 
sums Sn = Xj + • • • + Xn of a sequence of mutually independent random 
variables but, in contrast to section 5, the distribution Fn of Xn is permitted 
to depend on n. We put S* = (Sn — bn)/an and wish to characterize the 
possible limit distributions of {S*}, under the assumption that 
(8.1) <i.-°o, ^ 
an 
The first condition eliminates convergent series ]T Xk which are treated in 
section 10. Situations avoided by the second condition are best illustrated 
by the 
Example. Let Xn have an exponential distribution with expectation nl. 
Put an = nl and bn = 0. Obviously the distribution of S* tends to the 
exponential distribution with expectation 1, but the convergence is due 
entirely to the preponderance of the term Xn. > 
Following Khintchine it is usual to say that a distribution belongs to the 
class L if it is the limit distribution of a sequence {S*} satisfying the con- 
ditions (8.1). 
•In this formulation it is not clear that all distributions of the class L are 
infinitely divisible, but we shall prove this as a consequence of the 
Lemma. A characteristic function co belongs to the class L iff for each 
0 < s < 1 the ratio (o(Qja)(s^) is a characteristic function. 
t This section treats a special topic. 
XVII.8 THE CLASS L 589 
Proof, (a) Necessity. Denote the characteristic function of SJ by 
con and let n> m. The variable S* is the sum of (am(an)S*n and a variable 
depending only on XTO+1,. . . , XB. Therefore 
(8-2) con@ = aj&ajaj ' <PmM) 
where <pm>n is a characteristic function. Now let n —>• oo and m—>~co in 
such a way that ajan^s < 1. [This is possible on account of (8.1).] 
The left side tends to w(?) and the first factor on the right tends to co(sQ 
because the convergence of characteristic functions is uniform in finite 
intervals. (Theorem 2 of XV,3.) We conclude first that co has no zeros. 
In fact, since <pmn remains bounded co(?0) = 0 would imply co(s?0) — 0, 
and hence w(s*?0) for all k > 0, whereas actually «($*?„)—»> 1. Accord- 
ingly, the ratio (o(?)[co(sO appears as the continuous limit of the character- 
istic functions (pm>TI, and is therefore a characteristic function. 
(b) Sufficiency. The above argument shows that o has no zeros, and 
hence we have the identity 
(8.3) m(n0 = 
Under the conditions of the lemma the factor co(k?)lco((k —1H is the 
characteristic function of a random variable Xfc .and hence co(?) is the 
characteristic function of (X1 + - • -+Xn)//7. > 
We have not only proved the theorem but have found that co is the 
characteristic function of the nth row sum in a triangular array. The 
condition G.1) for null arrays is trivially satisfied, and hence co is infinitely 
divisible. To find the canonicalmeasure M (determining co we note that 
the ratio co(?,)lco(s?,) is infinitely divisible as can be seen from the factoriz- 
ation (8.3). The canonical measure N determining a)(?)/ft>(s?) is related 
to M by the identity 
(8.4) N{dx) = M{dx) - s2M{s~1 dx). 
In terms of the functions M+ and M~ this relation reads 
(8.5) N+(x) = M+(x) - M+(xls), N~(-x) = M~{-x) - h?-{-xjs). 
We have shown that if the canonical measure M determines a character- 
istic function co of class L, then the functions Ar+ and N~ defined in 
(8.5) must be monotone for each 0 < s < 1. Conversely, if this is true then 
(8.4) defines a canonical measure determining m(?)/<u(.s?)- We have thus 
proved the 
Theorem. A characteristic function co belongs to the class L iff it is 
infinitely divisible and its determining canonical measure M is such that the 
two functions in (8.5) are monotone for every fixed 0 < s < 1. 
590 INFINITELY DIVISIBLE DISTRIBUTIONS XVII.9 
Note. It is easily verified that the functions are monotone iff .M+(ex) 
and M~(~ex) are convex functions. 
*9. PARTIAL ATTRACTION. "UNIVERSAL LAWS" 
As we have seen, a distribution F need not belong to any domain of 
attraction, and the question arises whether there exist general patterns in 
the asymptotic behavior of the sequence {Fn*} of its successive convolutions. 
The sad answer is that practically every imaginable behavior occurs and no 
general regularity properties are discernible. We describe a few of the 
possibilities principally for their curiosity value. 
The characteristic function <p is said to belong to the domain of partial 
attraction of y iff there exist norming constants ar, br and a sequence of 
integers nr—>-co such that 
(9.D 
Here it is understood that \y\ is not identically 1, that is, the corresponding 
distribution is not concentrated at one point. Thus (9.1) generalize? the notion 
of domains of attraction by considering limits of subsequences. 
The Jimit y is necessarily infinitely divisible by virtue of theorem 2 of 
section 1. The following examples will show that both extremes are possible: 
there exist distributions that belong to no domain of partial attraction and others 
that belong to the domain of partial attraction of every infinitely divisible 
distribution. 
Examples, (a) Example 3(/) exhibits a characteristic function <p which 
is not stable but belongs to its own domain of partial attraction. 
(Jb) A symmetric distribution with slowly varying tails belongs to no domain 
of partial attraction. Suppose that L(x) = 1 — Fix) + F(—x) varies 
slowly at infinity. By theorem 2 of VIII,9 in this case 
(9.z) U(x) = j V F{dy] = o(x2L(x)), x -* oo. 
By the theorem of section 7, for F to belong to some domain of partial 
attraction it is necessary that as n runs through an appropriate sequence 
n[\ — F(anx) -f F{— anx)] and na~2 U(anx) converge at all points of 
continuity. The first condition requires that n L(an) ^ 1, the second that 
n L(an) -* oo. 
(c) An infinitely divisible y need not belong to its own domain of partial 
attraction. Indeed, it follows from theorem 1 of section 1 that if <p belongs 
to the domain of attraction of y so does the characteristic function e*1, 
* This section treats special topics. 
XVII.9 PARTIAL ATTRACTION. "UNIVERSAL LAWS" 591 
which is infinitely divisible. The last example shows that ?~x need not 
belong to any domain of partial attraction. 
(d) As a preparation to the oddities in the subsequent examples we prove 
the following proposition. Consider an arbitrary sequence of infinitely 
divisible characteristic functions a)r = eVr with bounded exponents. Put 
00 
(9.3) 
It is possible to choose the constants ak > 0 and integers nk such that as 
r -*¦ oo 
(9-4) nMK) ~ Vr@ -* 0 
for all I 
Proof. Choose for {nk} a monotone sequence of integers increasing so 
rapidly that «fc/«fc_i > 2k max |^fc|. The left side in (9.4) is then dominated by 
I "* 
(9.5) «r2Wfl*WI + I 2 
Jfc=-1 fc-r+l 
We choose the coefficients ar recursively as follows. Put a1 = 1. Given 
«!,... ,0^-1 choose ar so large that the quantity (9.5) is <\jr for all 
|?| < r. This is possible because the first sum depends continuously on ? 
and vanishes for ? = 0. 
(e) Every infinitely divisible characteristic function co = ev possesses a 
domain of partial attraction. Indeed, we know that <o is the limit of a 
sequence of characteristic functions a>k = eVk of the compound Poisson 
type. Define A by (9.3) and put <p = ex. Then <p is a characteristic function 
and (9.4) states that 
(9.6) lim <pn'(ilar) = lim ***«> = a>@- 
(f) Variants. Let ea and ep be two infinitely divisible characteristic 
functions and choose the terms in (9.3) such that y)^ —*¦ a and y'afc+i —*¦ fl- 
it follows from (9.4) easily that if a sequence v^dla^) converges, the limit 
is necessarily a linear combination of a and /?. In other words, e* belongs 
to the domain of partial attraction of all characteristic functions of the form 
eJ"x"H, and to no.others. This example generalizes easily. In the terminology 
of convex sets it shows that a distribution F may belong to the domains of 
partial attraction of all distributions in the convex hull of n prescribed 
infinitely divisible distributions. 
(g) Given a sequence of infinitely divisible characteristic functions e*1, 
e**, . . . there exists a (p — ex belonging to the domain of partial attraction 
of each of them. Partition the integers into infinitely many subsequences. 
(For example, let the nth subsequence contain all those integers that are 
592 INFINITELY DIVISIBLE DISTRIBUTIONS XVII. 10 
divisible by 2" but not by 2n.) We can then choose the yr in example (d) 
such that y;r->an when r runs through the «th subsequence. With this 
choice (9.4) shows that <p = ex has the desired property. 
{h) Dobliri's "universal laws." It is possible that <p belongs to the domain 
of partial attraction of every infinitely divisible o>. Indeed, it is obvious 
that if (p belongs to the domain of partial attraction of a^, co2, . . . and 
a>n —> co, then cp belongs also to the domain of partial attraction of a>. 
Now there exist only countably many infinitely divisible characteristic 
functions whose canonical measures are concentrated at finitely many 
rational points and have only rational weights. We can therefore order these 
functions in a simple sequence e, eaz, .... Then every infinitely divisible 
co is the limit of a subsequence of {eak}. The characteristic function cp of the 
last example belongs to the domain of partial attraction of each at, and 
therefore also of w. 
{Note. The last result was obtained by W. Doblin in a masterly study in 1940, following 
previous work by A. Khintchine in 1937. The technical difficulties presented by the problem 
at that time were formidable. The phenomenon of example (b) was discovered in special 
cases by B. V. Gnedenko, A. Khintchine, and P. Levy. It is interesting to observe the 
complications encountered in a special example when the underlying phenomenon of regular 
variation is not properly understood.] 
*10. INFINITE CONVOLUTIONS 
Let Xl5 Xj,, . . . be independent random variables with characteristic 
functions <p1? <p.z, . . . . As in G.5) we denote by r the monotone continuous 
truncation function defined by t(x) = x for \x\ < a and t(x) = ±a for 
\x\ > a. The basic theorem on infinite convolutions states that the distributions 
of the partial sums X1 -\- ¦ ¦ ¦ -\- Xn converge to a probability distribution U 
iff 
A0.1) t Var(r(Xfc)) < co, |p{|X*| > a} < oo 
and 
A0.2) 
A-=l 
where b is a number. 
The special case of finite variance was treated in VIII,5 together with 
examples and applications. In full generality the theorem appears in IX,9 
where the result is also extended by proving the convergence of the series 
2 Xri (the "three-series theorem")- The theorem was shown to be a simple 
corollary to the basic theorems concerning triangular arrays, and it is not 
This section treats a special topic. 
XVII. 11 HIGHER DIMENSIONS 593 
necessary to repeat the argument.8 We shall therefore be satisfied with 
examples illustrating the use of characteristic functions. 
Examples, (a) Factorization of the uniform distribution. Let Xk = ±2~k 
with probability J. It was shown in example 1,11 (c) informally that 2X* 
may be interpreted as "a number chosen at random between 1 and —1." 
This amounts to the assertion that the characteristic function (sin ?)/? of 
the uniform distribution is the infinite product of the characteristic functions 
cos (?/2*). For an analytic proof we start from the identity 
nt\*\ sin? ? ? ? sin 
A0.3) = cos - • cos z • • • cos 
C 2 4 2n ?/2n 
which is proved by induction using the formula sin 2a = 2 sin a cos a. 
As n —*¦ oo the last factor tends to 1 uniformly in every finite interval. 
Note that the product of the even-numbered terms again corresponds to a 
sum of independent random variables. We know from example l,l\(d) 
that this sum has a singular distribution of the Cantor type.9 
(See problems 5,7, and 19.) 
(b) Let Yk have density \e~^ with characteristic function 1/A + ?2). 
Then 2 Ykjk converges. For the characteristic function we get the canonical 
product representation for 7r?/sinh tt? where sinh denotes the hyperbolic 
sine. Using problem 8 in XV,9 we find that the density of ]? Yfc/& is given by 
1/B + e* + e~x) = l/4(cosh (z/2)J. 
11. HIGHER DIMENSIONS 
The theory developed in this chapter carries over without essential changes 
to higher dimensions, and we shall not give all the details. In the canonical 
form for infinitely divisible distributions it is best to separate the normal 
component and consider only canonical measures without atom at the origin. 
The formulas then require no change provided t,x is interpreted as an inner 
product in the manner described in XV,7. For definiteness we spell out the 
formula in two dimensions. 
A measure without an atom at the origin is canonical if it attributes finite 
masses to finite intervals and if 1/A H-x^+o:^) is integrable with respect 
8 It is a good exercise to verify directly that the conditions A0.1)—A0.2) assure that the 
products ?>i • • • 9>n converge uniformly in every finite interval. The necessity of the con- 
ditions is less obvious, but follows easily on observing that the triangular array whose nth 
row is XB, XB+1,... , Xrl+rf| must satisfy the conditions of the theorem of section 7 with 
M = 0. 
9 G. Choquet gave a charming geometric proof applicable to more general infinite con- 
volutions. It is given in A. Tortrat, J. Math. Pures. Appl., vol. 39 A960) pp. 231-273. 
594 INFINITELY DIVISIBLE DISTRIBUTIONS XVII. 11 
to it, and if it has no atom at the origin. Choose an appropriate centering 
function in one dimension, say r(x) = sin x or the one defined in G.5). Put 
A1.1) 
J 
the integral extending over the whole plane. Then co = ev is an infinitely 
divisible bivariate characteristic function. The most general infinitely divisible 
characteristic function is obtained by multiplication by a normal characteristic 
function. 
A reformulation in polar coordinates may render the situation more 
intuitive. Put 
A1.2) Ci — P cos <p, ?2 = P sin ?¦> z = rcosd, y = rsin6. 
Define the canonical measure in polar coordinates as follows,. For each 
6 with — 77 < 6 < 77 choose a one-dimensional canonical measure Ag 
concentrated on 0, oo; furthermore, choose a finite measure W on 
—77 < 0 <, it (the circle). Then M may be defined by randomization of 
the parameter 6, and (with a trite change in centering) A1.1) may be.recast 
in the form 
A1.3) 
V(^i, U) = W{dd} ; 
J-«r Jo+ r 
(This fonn permits one to absorb the normal component by adding an atom 
at the origin to Ag.) 
Example. Stable distributions. By analogy with one dimension we put 
Ag{dr} — r~*+1 dr. One could add an arbitrary factor Cg, but this would 
merely change the measure W. As we have seen in example 3{g), with this 
measure A1.3) takes on the form 
A1.4) ?&, ?2) = -CP*j*}cos?(<P-W(l T tan |) W[dO), 
where the upper or lower sign prevails according as <p — 6 > 0 or 
(p _ 0 < o. This shows that e"' is a strictly stable characteristic function, 
and as in section 5 one sees that there are no others. However, just as in one 
dimension, the exponent a = 1 leads to characteristic functions that are 
stable only in the wide sense and have a logarithmic term in the exponent. 
When a = 1 and W is the uniform distribution we get the characteristic 
function e~ap of the symmetric Cauchy distribution in 3iz [see example 
XV,7(e) and problems 21-23]. > 
XVII. 12 PROBLEMS FOR SOLUTION 595 
12. PROBLEMS FOR SOLUTION 
1. It was shown in section 2 that if y is the logarithm of an infinitely divisible 
characteristic function, then 
A2.1) HO -Yh[ v{l-s)dx = x@ 
is a real multiple of a characteristic function. Prove the converse: Suppose that y 
is a continuous function such that x@U@) is a characteristic function for every 
choice of h > 0. Then y differs only by a linear function from the logarithm 
of an infinitely divisible characteristic function. Furthermore, y is such a logarithm 
if it satisfies the further conditions y@) = 0 and y( —0 = y(?)- 
[Hint: Prove that the solutions of the homogeneous equation (with x = 0) 
are linear.] 
2. Show that problem 1 and the argument of section 2 remain valid if in A2.1) 
or B.13) the uniform distribution is replaced by a distribution concentrated at the 
points: 
A2.2) y(?) - Uv(C+h) + v(C-h)] = *(?)• 
However, there arises a slight complication from the fact that the density corre- 
sponding to x is not strictly positive. 
3. Generalization. Let R be an arbitrary even probability distribution with 
finite variance. If ev is an infinitely divisible characteristic function and 
A2.3) x=V-Rrkv, 
then x(Olx@) is a characteristic function. The argument of section 2 goes through 
using A2.3) instead of B.13). 
In particular, if R has the density \e~\x\ one is led directly to Khintchine's 
normal form for y. (See the concluding note to section 2.) However, some care 
is required by the fact that y is unbounded. 
4. If co is an infinitely divisible characteristic function then there exist constants 
a and b such that |log co(Q\ < a + b? for all ?. 
5. Shot noise in vacuum tubes. In example VI,3(A) we considered a triangular 
array in which XkiTl had the characteristic function 
Tk.niO = 1 +a/r[e«/<*»>-l], 
where h = n~K Show that the characteristic functions of S,, ='X1>ri + • • • + Xn n 
tend to ev where . - 
ev is the characteristic function of the random variable X@, and by differentiation 
one gets CampbeWs theorem VI,C.4). 
6. Let U = ^ Xn/// wliere the variables X^. are independent and have the 
common density \e~W. Show that10 U is infinitely divisible with the canonical 
10 The characteristic function co is defined by an infinite product which happens to be 
the well-known canonical product of In ||/l5! ^! 
596 INFINITELY DIVISIBLE DISTRIBUTIONS XVII.12 
e-M 
measure M{dx} = \x\ __ - dx. [No calculations beyond summing a geometric 
series are required.] 
7. Let P(s) = 2.pksk where pk > 0 and Zpk = 1. Assume P@) > 0 and that 
P(s) 
log ——• is a power series with positive coefficients. If <p is the characteristic 
function of an arbitrary distribution F show that P(<p) is an infinitely divisible 
characteristic function. Find its canonical measure M in terms of Fn*. 
Special case of interest: IfO<;a<6<l then • -^ is an infinitely 
1 — a \ — by 
divisible characteristic function. (See also problem 19.) 
8. Continuation. Interpret P(<p) in terms of randomization and subordinated 
processes using the fact that P is the generating function of an infinitely divisible 
integral-valued random variable. 
9. Let X be stable with characteristic function e~lda @ < a <, 2) and let Y 
be independent of X. If Y is positive with a distribution G (concentrated on 
0, oo show that the characteristic function of XYl/a is given by 
f 
Jo 
lav G{dy). 
Conclude,: If .X and Y are independent strictly stable variables with exponents 
a and ft and if Y > 0, then XY1'" is strictly stable with exponent a/J. 
10. Let co bea characteristic function such that a>2(?) = <o(aQ and w3(?) = 
= a>(J>Q. Then eo is stable. 
[Example 3(/) shows that the first relation does not suffice. The exponents 2, 3 
may be replaced by any two relatively prime integers.] 
11. Show that the simple lemma 3 of VIII,8 applies (not only to monotone 
functions but also) to logarithms of characteristic functions. Conclude that if 
(on(Q = o>(qnQ for all n then log a>(?) = At* for ? > 0, where A is a complex 
constant. 
12. Continuation. Using the result of problem 28 in VIII, 10 show directly 
that if to is a stable characteristic function then for ? > 0 either log eo(?) = 
= At? + ibt, or else log eo(?) = At, + ibt, log t, with b real. 
13. Let F be carried by 0, oo and 1 — F(x) = x-*L(x) with 0 < a < 1 and 
L slowly varying at infinity. Prove that 1 — q>(?)r+j At*L(l/Q as ?-*0 + 
14. Continuation. From the results of section 5 prove the converse, and also 
that A = T(l -a)e-»™/2. 
15. Continuation. By induction on k prove: in order that 1 — F{x) ~ ax~aL(x) 
as x -*- oo with L slowly varying and k < a. < k + 1 it is necessary and sufficient 
that as ? -»¦ 0 + 
Then automatically A = — aT(k— a)e~»?™. 
16. Formulate the weak law for triangular arrays as a special case of the general 
theorem of section 7. 
XVII. 12 PROBLEMS FOR SOLUTION 597 
17. Let {Xk n} be a null array whose row sums have a limit distribution deter- 
mined by the canonical measure M. Show that for x > 0 
P{max [X1<71,. .. , Xrn.J <, x) -*e 
Formulate a converse. 
18. Let {Xk p) be a null array of symmetric variables whose row sums have a 
limit distribution determined by the canonical measure M with- an atom of 
weight a2 at the origin. Show that the distribution of Sf = ^X|>71 — o2 con- 
verges to a distribution determined by a measure M# without atom at the origin 
and such that M# (x) = 2M+(Vx) for x > 0. 
19. Let 0 < r, < 1 and Ilr, < oo. For arbitrary real ai the infinite product 
converges and represents an infinitely divisible characteristic function. (Hint: 
Each factor is infinitely divisible by problem 7.) 
20. Use the method of example 9(d) to construct a distribution F such that 
lim sup Fn*(x) = 1 and Iim inf Fn*(x) = 0 at all points. 
21. In A1.4) let W stand for the uniform distribution. Then 
and ev is a symmetric stable distribution. 
22. In A1.4) let W attribute weight \ to each of the four points 0, it, %n, -\tt. 
Then A1.4) represents the bivariate characteristic function of two independent 
one-dimensional stable variables. 
23. In A1.4) let W be concentrated on the two points a and a + it. Then 
A1.4) represents a degenerate characteristic function of a pair such that 
X1 sin a — X2 cos o = 0. 
More generally, any discrete W leads to a convolution of degenerate distributions. 
Explain A1.4) by a limiting process. 
CHAPTER XVIII 
Applications of Fourier Methods 
to Random Walks 
To a large extent this chapter treats topics already covered in chapter 
XII, for which reason applications are kept to a minimum. A serious 
attempt has been made to make it self-contained and accessible with a 
minimum of previous knowledge except the Fourier analysis of chapter XV. 
The theory is entirely independent of the last two chapters. Section 6 is 
independent of the preceding ones. 
1. THE BASIC IDENTITY 
Throughout this chapter X1} X2, . .. are mutually independent random 
variables with a common distribution F and characteristic function (p. As 
usual we put So = 0 and Sn = X1 + • • • + X,,; the sequence {S,,} 
constitutes the random walk generated by F. 
Let A be an arbitrary set on the line and A' its complement. (In most 
applications A' will be a finite or infinite interval.) If / is a subset (interval) 
of A' and if 
A.1) S1e>4,....,SII_1e>4,SIle/_ (/<= O 
we say that the set A' is entered {for the first time) at epoch n and at a point 
of I. Since A' need not be entered at all the epoch N of the entry is a 
possibly defective random variable, and the same is true of the point SN of 
first entry. For the joint distribution of the pair (N, SN) we write 
A.2) P{N = n, SN e /} = ffn{I}, n=l,2,.... 
Thus Hn{I) is the probability of the event A.1), but the distribution A.2) 
is defined for all sets / on the line by the convention that Hn{I) = 0 if 
/ c A. The probabilities A.2) will be called hitting probabilities. Their 
598 
XVIII. 1 THE BASIC IDENTITY 599 
study is intimately connected with the study of the random walk prior 
to the first entry into A', that is, the random walk restricted to A. For 
/<= A and «= 1,2,... put 
A.3) Gn{I) = P{S1 e A,... , S^ eA,SneI}; 
in words, this is the probability that at epoch n the set / <= A is visited and 
up to epoch* h no entry into A' took place. We extend this definition to 
all sets on the line by letting Gn{I} = 0 if / <= A'. 
A-4) G 
The variable N is not defective iff this quantity tends to 0 as n —*- oo. 
Considering the position Sn of the random walk at epochs n = 1,2,... 
it is obvious that for / <= A1 
whereas for I <=¦ A 
We now agree to let Go stand for the probability distribution concentrated 
at ihe origin. Then the relations A.5) hold for n = 0, 1, 2, . . . and deter- 
mine recursively all the probabilities Hn and Gn. The two relations can be 
combined in one. Given an arbitrary set / on the line we split it into the 
components I A' and IA and apply A.5) to these components. Recalling 
that Hn and Gn are concentrated, respectively, on A' and A we get 
d-6) Hn+1{I} + Gn+1{I} = (Gn{dy}F{I-y} 
Ja 
for n = 0, 1, . . . and arbitrary /. 
The special case A = 0, co was treated in XII,3, the relation XII,C.5) 
being the same as the present A.5). We could retrace our steps and derive 
an integral equation of the Wiener-Hopf type analogous to XII,C.9) and 
again possessing.only one probabilistically possible solution (though the 
uniqueness is not absolute). It is preferable, however, to rely this time on the 
powerful method of Fourier analysis. 
We are concerned with the distribution of the pair (N, SN). Since N is 
integral-valued we use generating functions for N and characteristic 
functions for SN. Accordingly we put 
A.7) X(s, 0 = f sn f eiixHn{dx}, y(s, 0 = f sn f e*'Gn{dx). 
n=i Ja' «=o Ja 
(The zero terms of the two series equal 0 and 1, respectively.) These series 
converge at least for \s\ < 1, but usually in a wicfer interval. 
600 APPLYING FOURIER METHODS TO RANDOM WALKS XVIII. 1 
The effective domains of integration are inserted for clarity, but the limits 
of integration may be given as well as — oo and +00. In particular, the 
integral in A.6) is an ordinary convolution. On taking Fourier-Stieltjes 
transforms the relation A.6) therefore takes on the form 
0-8) Xn+i(Q + yn+i@ = yn@ 
Multiplying by sn+1 and adding over n = 0, 1, .. . we get 
X(s, 0 + y(s, 0-\= 
for all s for which the series in A.7) converge. We have thus established 
the basic identity 
A.9) 1 -x = y[\-scp}. 
(For an alternative proof see problem 6.) 
In principle % and y can be calculated recursively from A.5), and the 
identity A.9) appears at first glance redundant. In reality direct calculations 
are rarely feasible, but much valuable information can be extracted directly 
from A.9). 
Example. Let F stand for the bilateral exponential distribution with 
density ?e~lxl and characteristic function <??(?)= 1/0 +?2)> and let 
A = —a, a. For x > a we get from A.5a) 
A.10) Hn+1{x, 00} = Hn+1{-co, -x] = | Gn{dy}e-(x~v) = cne 
with cn independent of x. It follows that the point SN of first entry into 
\x\ > a is independent of the epoch of this entry and has a density pro- 
portional to e~M (for \x\ > a). This result accords intuitively with the 
lack of memory of the exponential distribution described in chapter I. The 
independence means that the joint characteristic function % must factor, 
and from the form of the density for SN we conclude that 
T eiaX> 
A.11) *(s, 0 = \P{s)\ * 
+ j 
l — /? 1 + 
where P is the generating function of the epoch N of the first entry into 
\x\ > a. [The proportionality factor is deduced from the fact that 
A direct calculation of P(s) would be cumbersome, but an explicit 
expression can be easily deduced from A.9). In fact, with our form of the 
characteristic function the right side in A.9) vanishes for ? = db/V 1 — s, 
XVIII.2 FINITE INTERVALS. WALD'S APPROXIMATION 601 
and so for this value x(s> ?) must reduce to 1. Thus 
A.12) P(s) = 
Ll + y/1 - S 1 - ./I - Sj 
From this it follows that the epoch N of the first entry into \x\ > a has 
expectation 1 + a + \a*. 
(For further examples see problems 1-5.) > 
*2. FINITE INTERVALS. WALD'S APPROXIMATION 
Theorem. Let A = —a,b be a finite interval containing the origin and 
let (N, SN) be the hitting point for the complement A'. 
The variables N and SN are proper. The generating function 
B.1) fy p{n >«} = fy on{,4} 
71=0 71=0 
converges for some1 s > 1 and hence N has moments of all orders. The 
hitting point SN has an expectation iff the random-walk distribution F has 
an expectation /u, in which case 
B.2) E(SN) = fi - E(N). 
The identity B.2) was first discussed by A. Wald. In the special case 
A = 0, oo it reduces to XII,B.8). 
Proof. As was already pointed out, Gn{A} and P{N > n) are different 
notations for the probability that the random walk lasts for more than n 
steps, and so the two sides in B.1) are identical. 
Choose an integer r such that P{|Sr| < a + b} = r\ < 1. The event 
{N > n + r} cannot occur unless 
N > n and |XB+l + • • • + Xn+r| < a + b 
(These two events are independent because {N > n) depends only on the 
variables X,, . . . , Xn. Since Xn+1 + • • • + Xn+r has the same distribution 
as Sr we conclude that 
P{N > n + r} < P{N > n}rj. 
Hence by induction 
B.3) P{N > kr) <> r)k, 
* This section is included because of its importance in statistics; it should be omitted at 
first reading. 
1 This is known to statisticians as C. Stein's lemma. For an alternative proof see 
problem 8. 
602 APPLYING FOURIER METHODS TO RANDOM WALKS XVIII.2 
which shows that the sequence P{N > n) decreases at least as fast as a 
geometric sequence with ratio rj1/r. It follows that N is a proper variable 
and that the series in B.1) converge at least for \s\ < r\-yT. This proves the 
first assertion. 
It follows also that A.9) is meaningful for \s\ < r)~1/r. For 5 = 1 we 
conclude 
B.4) 1 -xo,o = v(uov-<pa)i 
But 2A,?) is the characteristic function of SN, and the fact that ^A,0) = 
= 1 shows that SN is proper. 
The event |SN| > t + a + b cannot occur unless for some n one has 
N > n — 1 and |XJ > /. As already remarked, these two events are 
independent, and since the XB are identically distributed we conclude that 
P{|SN| > t + a + b) 
I P{N > n - 1} • P{|X,| > /} = E(N) 
71=1 
The expectation fi = E(X2) exists iff the right side is integrable over 0, 00. 
In this case the same is true of the left side, and then E(SN) exists. On the 
other hand, 
P{|SN| > /} > P{|X,| > / + a + b) 
because the occurrence of the event on the right implies SN = Xv Thus the 
existence of E(SN) implies the existence of /u = E(X2). When these 
expectations exist we can differentiate B.4) to obtain 
B.5) i E(SN) = 2*22°> = 9'@) y(l, 0) = if, E(N). > 
ac, 
We proceed.now to derive a variant of the basic identity A.9) known as 
Wald's identity. To avoid the use of imaginary arguments we put 
B.6) /(A) = f+V* F{dx). 
J—co 
Suppose that this integral converges in some interval — Xo < X < Xx 
about the origin. The characteristic function is then given by <p{iX) =f(X), 
this function being analytic in a complex neighborhood of each X in the given 
interval. Wald's identity is obtained formally from A.9) letting ? = iX and 
s = \j(p{iX). For these particular values the right side vanishes and hence 
%(s, 0=1. In view of the definition of % this relation may be restated in 
probabilistic terms as follows. 
XVIII.2 FINITE INTERVALS. WALD'S APPROXIMATION 603 
Wald's lemma.2 If the integral {2.6) converges for —Xo < X < klt then in 
this interval 
B.7) 
Proof. We repeat the argument leading to A.9). As the measures Gn are 
concentrated on a finite interval their Fourier transforms %n converge for 
all ? in the complex plane. By assumption <p{iX) = f{X) exists, and hence 
it is seen that the Fourier version A.8) of A.6) is valid for ? = iX. On 
multiplication by /""^(A) this relation takes on the form 
B.8) f-n-*(X) Xn+1{iX) =/(A)"« yn{iX) -f-n 
If f~nW 7n(iX) —>¦ 0 the right sides add to unity due to the obvious can- 
cellation of terms. In this case summation of B.8) leads to the assertion B.7) 
and hence it suffices to show that 
B.9) f- 
Now if f(rj) < oo 
Gn{A} < P{-a <Sn<b}< eia+b)M- Pe-"x F1* {dx} 
J—a 
< e(a+b)M • fn(fj). 
Thus B.9) is true if f(X) > f{rf). As we are free to choose i\ this proves 
B.7) for all X excepting values where / assumes its minimum. But being 
convex f has at most one minimum, and at it B.7) follows by continuity, k 
Example. Estimates concerning N. Wald was led to his lemma from 
problems in sequential analysis where it was required to find approximations 
to the distribution of the epoch N of the first exit from A, as well as 
estimates for the probabilities that this exit takes place to the right or left of 
this interval. Wald's method is a generalization of the procedure described 
in 1; XIV,8 for arithmetic distributions with finitely .many jumps. (There it 
is also shown how strict inequalities can be obtained.) Put 
B.10) pk = P{N=A:, SN>b}, qk = P{N=kT SN^-a} 
and write for the corresponding generating functions P(s) and Q(s). 
(Then P + Q is the generating function for N.) Suppose now that a and 
b are large in comparison with the expectation and variance of F. The 
2 Wald used B.7) in connection with sequential analysis, This was before 1945 and 
before the general random walks were systematically explored. It is therefore natural 
that his conditions were severe and his methods difficult, but unfortunately they still 
influence the statistical literature. The argument of the text utilizes an idea of H. D. Miller 
A961). 
604 APPLYING FOURIER METHODS TO RANDOM WALKS XVIII.3 
hitting point SN is then likely to be relatively close to either b or —a. If 
these were the only possible values of SN the identity B.7) would take on 
the form 
B.11) P(lim)e-Xb + Q(lim)eXa = 1, 
and one expects naturally that under the stated assumptions B.11) will 
be satisfied at least approximately. The function f is convex and it is 
usually possible to find an interval s0 < s < st such that in it the equation 
B.12) sf{X) = 1 
admits of two roots Xx{s) and ?^{s) depending continuously on s. Sub- 
stituting into B.11) we get two linear equations for the generating functions 
P and Q, and thus we get (at least approximately) the distribution of N 
and the probabilities for an exodus to the right and left. > 
3. THE WIENER-HOPF FACTORIZATION 
In this section we derive by purely analytical methods various consequences 
of the basic identity A.9). It turns out that they contain, in a more flexible 
and sharper form, many of the results derived in chapter XII by combinatorial 
methods. This may produce the false impression of a superiority of the Fourier 
methods, but in reality it is the interplay of the two methods that characterizes 
the recent progress of the theory' Each method leads to results which seem 
inaccessible to the other. (For examples in one direction see section 5; the 
arc sine law for the number of positive partial sums as well as generalizations 
of the whole theory to exchangeable variables illustrate advantages of the 
combinatorial approach.) 
From now on N and SN will denote the epoch and the point of first 
entry into the open half-line 0, oo. Their joint distribution 
is given by 
C.1) Hn{I) = P{S, < 0,.. . , Sn.t < 0, Sn e /}, / <= 0^> 
with the understanding that Ho = 0 and that Hn is concentrated on 0, oo. 
Instead of the bivariate characteristic function we introduce as before 
the more convenient combination of generating and characteristic function 
C.2) x(s> 0 - E(s?sn), 
namely, 
C.3) 
XVI11.3 THE WIENER-HOPF FACTORIZATION 605 
(The integration is over the open half-axis, but nothing changes if the lower 
limit is replaced by — oo.) For brevity we shall refer to % as "the transform" 
of the sequence of measures Hn. 
For the epoch and point of first entry into the open negative half-axis we 
write N~ and SN~; then {H^} and %~ denote the corresponding distribu- 
tion and transform. 
When the underlying distribution F is discontinuous we must distinguish 
between first entries into open and closed half-axes. It is therefore necessary 
to consider the event of a return to the origin through negative values. Its 
probability distribution {/„} is given by 
C.4) /„ - P{S1 < 0, . . . , S,., < 0, Sn = 0}, n > 1, 
and we put f(s) — 2 fns7i- ** wiH be seen presently that the right side 
n=l 
in C.4) remains unchanged if all the inequalities are reversed. Clearly 
Ifn <, P(X! < 0} < 1. ' 
With these notations we can now formulate the basic 
Wiener-Hopf factorization theorem. For \s\ < 1 one has the identity 
C.5) 1 - s<p@ - [1 -f(s)] • [1 -x(s, 0] • [1 ~X~(s, 01 
The proof will lead to explicit expressions for / and % which we state 
in the form of separate lemmas.3 
Lemma 1. For 0 ^ s < 1 
C.6) log i— = | S- f °> F**{dx). 
1 ( Q n J 
An analogous formula for %~ follows by symmetry. 
Lemma 2. For 0 <, s < 1 
C-7) log—^—i 
I — f{s) n=i n 
Since no inequalities enter the right side it follows that C.4) remains valid 
with all inequalities reversed. This result was obtained in example XII,2(a) 
as a consequence of the duality principle. 
The remarkable feature of the factorization C.5) is that it represents an 
arbitrary characteristic function cp in terms of two (possibly defective) 
3 For ? = 0 lemma 1 reduces to theorem 1 of XII,7. The generalized version XII,(9.3) 
is equivalent to lemma 1, but is clumsy by comparison. Lemma 2 restates XII,(9.6). It is 
due to G. Baxter. A greatly simplified (but still rather difficult) proof was given by F. 
Spitzer, Trans. Amer. Math. Soc., vol. 94 A960), pp. 150-169. 
606 APPLYING FOURIER METHODS TO RANDOM WALKS XVIII.3 
distributions concentrated on the two half-axes. Lemma 1 shows that this 
representation is unique. 
The proof is straightforward for continuous distributions, but for the 
general case we require the analogue to lemma 1 for the entrance probabilities 
into the closed interval 0, oo. These will be denoted by Rn{I}, that is, 
C.8) Rn{I) = P(Sx < 0,. . . , S^ < 0, Sn e /} 
for any interval / in 0, oo. Of course, RQ = 0 and Rn{— oo, 0} = 0. 
Lemma 3. For 0 < s < 1 the transform p of {Rn} is given by 
C.9) log- 1—T = | S- PV-F"*{</*}. 
1 - p(s, 0 «-i « Jo- 
Proof. We start from the basic identity A.9) applied to A = 0, oo. With 
our present notation the entrance probabilities are Rn rather than Hn, and 
so A.9) reads 
C-10) 1 - p{s, 0 = y(s, 0[l - sp(Q). 
Here y is the transform of the sequence of probabilities Gn defined on 
— oo, 0 by 
C.11) Gn{I) = P{SX < 0,. . . , Sn_x < 0, SB < 0, Sn e /}, 
that is 
C.12) y(s, 0 - 1 = 15" f°>* GB{^}. 
n=l J—oo 
For fixed |j| < 1 the functions 1 — S(p(t) and 1 — #(j, ?) can have no 
zeros, and hence (see XVII, 1) their logarithms are uniquely defined as 
continuous functions of ? vanishing at the origin. We can therefore rewrite 
C.10) in the form 
C.13) log 1—— = log i— + log y(s, 0 
1 - s<p(Q 1 - p(s, 0 
or 
oo _n /*-(-oo oo _n oo / i \n 
C.14) 2 - ««¦ Fn*{dx} = 2 - Pn(s, o + 2L-iL [K*; 0-i]n- 
n=l /J J—» n=l /J n=l /J 
Consider this relation for a fixed value 0 < s < 1. Then />n(j, 0 *s tne 
characteristic function of a defective probability distribution concentrated on 
0, oo, and hence the first series on the right is the Fourier-Stieltjes transform 
of a finite measure concentrated on 0, oo. Similarly, C.12) shows that 
p(s, 0—1 istne Fourier-Stieltjes transform of a finite measure concentrated 
XVIII.3 THE WIENER-HOPF FACTORIZATION 607 
on —oo,0. The same is therefore true of [p(s, ?)— l]n» and so the last 
series is the transform of the difference of two measures on — oo, 0. It 
follows that, when restricted to sets in 0, oo, the first two series in C.14) 
represent the same finite measure, namely ^ (sn/n)Fn*. The assertion C.9) 
restates this fact in terms of the corresponding transforms. > 
Proof of lemma 2. This lemma is contained in lemma 3 inasmuch as the 
two sides in C.7) are the weights of the atoms at the origin of the measures 
whose transforms appear in C.9). This is obviously true of the right sides. 
As for the left sides, by.the definition C.8) the atom of Rn at the origin has 
weight /„. Thus f(s) is the weight attributed to the origin by the measure 
2 snRn with transform %(s, ?). The measure with transform 2 Pn(s> Qln 
therefore attributes to the origin the weight ^fn(s)ln = log A — 
Proof of lemma 1. We may proceed in two ways. 
(i) Lemma 1 is the analogue of lemma 3 for the open half-axis and exactly 
the same proof applies. If both lemmas are considered known we may 
subtract C.6) from C.9) to conclude that 
C.15) p(s, 0 =f(s) + V-f(s)lx(s, 0- 
[This identity states that the first entry into 0, oo can be a return to the 
origin through negative values and that, when such a return does not take 
place, the (conditional) distribution of the point of first entry into 0, oo 
reduces to the distribution {Hn} of the first entry into 0, oo.] 
(ii) Alternatively we may prove C.15) directly from the definitions C.1) 
and C.8) of Hn and Rn. [For that it suffices in C.8) to consider the last 
index k < n for which Sk = 0 and take (Ar, 0) as new origin.] Substituting 
C.15) into C.9) we get lemma 1 as a corollary to lemmas 2 and 3. > 
Proof of the factorization theorem. Adding the identities of lemmas 1-2 
and the analogue of lemma 1 relating to — oo, 0, we get C.5) in its logarithmic 
form. That C.5) holds also for s = 1 follows by continuity. 
Corollary. 
C.16) y(s, 0 = * 
Proof. In view of C.13) and lemma 3 
C.17) yE,0 = exp (f -f° e*'F"*{dx}), 
and by lemma 1 the right sides in C.16) and C.17^ are identical. 
608 APPLYING FOURIER METHODS TO RANDOM WALKS XVIII.3 
Examples, (a) Binomial random walk. Let 
P{X1=1}=/? and P{X1 = -1} =q. 
The first entries into the two half-axes necessarily take place at ± 1, and hence 
C.18) X(s, 0 = P{s)e«, x~(s, 0 = Q(s)e-« 
where P and Q are the generating functions of the epochs of first entry. 
The two sides of the factorization formula C.5) are therefore linear com- 
binations of three exponentials eik{ with k = 0, ±1. Equating the coeffi- 
cients one gets the three equations 
V-f(sW+P(s)Q(s)} = 1, [l-f(s)]P(s) = sp,. 
C.19) V 
This leads to a quadratic equation for 1 —f(s), and the condition /@) = 0 
implies that / is given by 
C.20) 
The generating functions P and Q now follow from C.19). If p > q we 
have /(I) = q and hence Q{\) < 1. In this way the factorization theorem 
leads directly to the first passage and recurrence time distributions found by 
other methods in 1; XI and 1; XIV. 
(b) Finite arithmetic distributions. In principle the same method applies 
if F is concentrated on the integers between —a and b. The transforms % 
and x~ together with / are now determined by a + b + 1 equations, but 
explicit solutions are hard to come by [see example Xll,4(c)]. 
(c) Let F be the convolution of exponential distributions concentrated on 
the two half-axes, that is, let 
C.21) p(l) =-Z—•-±- a>0, b>0. 
a + it, b — it, 
Because of the continuity of F we have f{s) = 0 identically. 1 he left 
side in the factorization formula C.5) has a pole at t = —ib, but x~(s> 0 
is regular around any point t, with negative imaginary part. (This is so 
because. %~ ls ^e transform of a measure concentrated on — oo, 0.) It 
follows that x must be of the form x(s*'O = (p—it,)'1 U(s,0, with U 
regular for all t,. One may therefore surmise that U will be independent of 
t,, that is, that x an<^ X~ w^ be of the form 
C.22) XV, 0 = 7 Ty * X (s, Q = 
b — it, a + it, 
XVIII.4 IMPLICATIONS AND APPLICATIONS 609 
For this to be so we must have 
C.23) l- 
iO \ b-iU 
Clearing the denominators and equating the coefficients we find that P{s) = 
= Q(s) and that P(s) satisfies a quadratic equation. The condition 
P@) = 0 eliminates one of the two roots, and we find finally 
C.24) P(s) = Q(s) = \[a + b- 
Assume a > b. Then P(l) = b, and hence P{s)jb and Q{s)ja are 
generating functions of a proper and a defective probability distribution. 
The function % defined in C.22) is therefore the transform of a pair (N, SN) 
such that SN is independent of N and has the characteristic function 
bKb—it,). A similar statement holds for x~, and because of the uniqueness 
of the factorization P(s)/b and Q{s)ja are indeed the generating functions 
of the epochs N and N~ of first entries. [That SN and SN- are expon- 
entially distributed was found also in example XII,4(a). Recall from example 
VI,9(e) that distributions of the form C.21) play an important role in queueing 
theory.] »¦ 
For further examples see problems 9-11. 
4. IMPLICATIONS AND APPLICATIONS 
We proceed to analyze the preceding section from a probabilistic point of 
view and to relate it to certain results derived in chapter XII. 
(i) The duality principle. We begin by showing that the corollary C.16) 
is equivalent to 
Lemma 1. For any interval I in 0, co 
D.1) P{SX < S,, . . . , SH_X < SB, Sn ? /} = 
This fact was derived in XII,B.1) by considering the variables Xx, . . . ,Xn 
in reverse order. Viewed in this way the lemma appears almost self-evident, 
but we saw that many important relations are simple consequences of it. 
In the Fourier analytic treatment it plays no role, but it is remarkable that it 
comes as a byproduct of a purely analytic theory.4 [For a reminder of the 
4 Our Fourier analytic arguments are rather elementary, but historically the original 
Wiener-Kopf theory served as point of departure. Most of the literature therefore uses 
deep complex variable techniques which are really out of place in probability theory 
because even the original Wiener-Hopf techniques simplify greatly by a restriction to 
positive kernels. See the discussion in XII,3a. 
610 APPLYING FOURIER METHODS TO RANDOM WALKS XVIII.4 
fantastic consequences of lemma 1 concerning fluctuations the reader may 
consult example XII,2F).] 
Proof. The corollary C.16) refers to the negative half-axis and for a direct 
comparison all inequalities in D.1) should therefore be reversed. The prob- 
ability on the right in D.1) then coincides with the probability (?„{/} 
introduced in C.11), and y(s, ?) is simply the corresponding transform. To 
prove the lemma we have therefore to show that [1 — %{s, ?)]-1 is the 
transform of the sequence of probabilities appearing on the left side in D.1). 
Now %(s, ?) was defined as the transform of the distribution of the point 
{N, SN) of first entry into 0, oo, and hence xr is the transform of the rth 
ladder point (Nr, SN). It follows that 
D.2) 
is the transform of the sequence of probabilities that n be a ladder epoch and 
Sn 6 /. But these are the probabilities appearing on the left in D.1), and this 
concludes the proof. > 
(ii) The epoch N of the first entry into 0, oo has the generating function 
t given by r(s) = %(s, 0). Thus by C.6) 
D.3) log ~— = | s- P{SB > 0}. 
1 — t(s) n=i n 
This formula was derived by combinatorial methods in XII,7 where various 
consequences were discussed. For example, letting s -> 1 in D.3) it is seen 
that the variable N is proper iff the series 2 ""^{Sn > 0} diverges; in 
case of convergence the random walk drifts to — bo. On adding 
log A — s) = —2 snjn to D.3) and letting s -*¦ 1 one finds that 
D.4): log E(N) = log r'(l) = | - P{SB < 0} 
provided only that N is proper. But we have just observed that the last 
series converges iff the random walk drifts to oo, and hence we have 
Lemma 2. A necessary and sufficient condition that N be proper and 
E(N) < oo is that the random walk drifts to oo. 
This result was derived by different methods in XII,2. For further pro- 
perties of the distribution of N the reader is referred to XII ,7. 
(iii) On the expectation ofthetpoint S^ of first entry. With the methods of 
chapter XII not much could be said about the distribution of SN, but now 
we get the characteristic function of SN by setting s = 1 in C.6). However, 
XVIII.4 IMPLICATIONS AND APPLICATIONS 611 
it is preferable to derive some pertinent information directly from the 
factorization formula. 
lemma 3. If both SN and SN- are proper and have finite expectations 
then F has zero expectation and a variance a2 given by 
D.5) \a2 = - [1 -/(I)] • E(SN) • E(SN-). 
Theorem l.of the next section shows that the converse is also true. The 
surprising implication is that the existence of a second moment of F is- 
necessary to ensure a finite expectation for SN. 
Proof. For s = 1 we get from C.5) 
D.6) 
As ? —»- 0 the fractions on the right tend to the derivatives of the characteristic 
functions % and x~, that is, to *E(SN) and *E(SN-). The left side has 
therefore a finite limit a2, which means that <p'@) = 0 and 9?"@) = \a2. 
It follows that a2 is the variance of F (see the corollary in XV,4). > 
We turn to the case of a drift toward oo. It follows from lemmas 1-2 of 
section 3, together with D.5) that in this case as s -> 1 and ? -> 0 
D.7) [l-f(s)]-1 • [1 - xis&r1 - exp /f -P{SB < 0}) = E(N) < oo. 
\n=l n J 
Now by the factorization theorem 
D 8) x(h 0 - l .^ y@ - l l 
I I [l-/d)]-[l-f(U)]" 
Letting ? -> 0 we get the important result that 
D.9) E(SN) = E(X,) • E(N) 
provided E(SN) and E(XX) exist (the latter is positive because of the 
assumed drift to oo). 
We can go a step further. Our argument shows that the left side in D.8) tends to a finite 
limit iff 9/@) — i/i exists. Now it was shown in XVII,2a that this is the case iff our random 
walk obeys the generalized weak law of iarge numbers, namely iff 
D.10) ^SB-^>/. 
( >- signifying convergence in probability). It was shown also that for positive variables 
this implies that n coincides with their expectation. Thus in D.8) the left side approaches 
612 APPLYING FOURIER METHODS TO RANDOM WALKS XVIII. 5 
a limit iff E(N) < oo, and the right iff ?>'@) exists. We have thus 
Lemma 4. When the random walk drifts to oo, then E(N) < oo iff there exists a number 
H > 0 such that D.10) holds. 
In XII,8 we could only show that the existence of E(XX) suffices, and even for this weaker 
result we required the strong law of large numbers together with its converse. 
5. TWO DEEPER THEOREMS 
To illustrate the use of more refined methods we derive two theorems of 
independent interest. The first refines lemma 3 of the preceding section; 
Jhe second has applications in queueing theory. The proofs depend on deep 
Tauberian theorems, and the second uses Laplace transforms. 
Theorem X. If F has zero expectation and variance a2 the series 
E.1) f - [P{SB > 0} - « = c 
n=l 
converges at least conditionally, and 
E.2) 
This theorem is due to F. Spitzer. The convergence of the series played a 
role in theorems la of XII,7 and 8. 
Proof. Differentiating C.6) with respect to ? and setting ? == 0 one gets 
E.3) -i ¦ d-^^ = f - Vx F»*{dx} • exp [- | S- P{SB > 0}]. 
at, n=i n Jo L n-i'/i J 
Both series converge absolutely for \s\ < 1 since the coefficients of sn 
remain bounded. Indeed, by the central limit theorem the moments of" 
order <,2 of Sjayfn tend to the corresponding moments of the normal 
distribution, which means that as n —*• oo 
E.4) jf.F- 
Accordingly, by the easy part of theorem 5 of XIII,5 as s-+-1 
oo cn _ oo _n _. , 
E.5) 2 2-FJ*{d*}~^=2 4=~4U-sr}- 
n=l n t yjLTT n=l y/n y/2 
The left side in E.3) tends to E(SN) which may be finite or infinite, but 
cannot be zero. Combining E.3) and E.5) we get therefore 
E.6) E(SN) = -~ lim exp [ | S- (i - P{SB > 0})]. 
XVIII.5 TWO DEEPER THEOREMS 613 
The exponent tends to a finite number or to +00. The same argument 
applies to N~, that is, to the exponent with Sn > 0 replaced by Sn < 0. 
But the sum of the two exponents equals 2 (s ln) P{Sn = 0} and remains 
bounded as s—> 1. It follows that the exponent in E.6) remains bounded, 
and hence tends to a finite limit —c. Since its coefficients are oirr1) this 
implies5 that for s = 1 the series converges to — c. This concludes the proof. 
> 
Next we consider random walks with a drift to — 00 and put 
E.7) MB = max {0, Slf .. ., SB}. 
It may be recalled from VI,9 that in applications to queueing theory Mn 
represents the waiting time of the nth customer. However, the proof of the 
following limit theorem is perhaps more interesting than the theorem itself. 
Theorem 2. If the random walk drifts to — 00 the distributions Un of 
Mn tend to a limit distribution U with characteristic function co given by 
E.8) ©@ = exp 
n=i njo 
Note that 2 n~1'P{Sn > 0} < 00 in consequence of D.3), and so the 
series in E.8) converges absolutely for all ? with positive imaginary part. 
Proof. Let a>n denote the characteristic function of Un. We begin by 
showing that for \s\ < 1 
E.9) f sncon{Q = -*— exp [ i s- f V* ~ 1) Fn*{dx}]. 
n=0 1 — S Ln-1 tl JO J 
The event {Mv e /} occurs iff the following two conditions are satisfied. 
First, for some 0 < n < v the point in, Sn) is a ladder point with Sn 6 /; 
second, Sfc — Sn < 0 for all n <k <v. The first condition involves only 
Xl5. . . , Xn, the second only Xn+1,. . . , Xv. The two events are therefore 
independent and so 
E.10) P{MV 6 /} = aobv + • • • + a A 
where 
E.11) an = P{SX < SB, . . . , S^ < SB, SB 6 /}, bn = P{N > n). 
The probabilities an occur on the left in D.1), and we saw that their trans- 
form is given by [1 — %{s, Q]. The generating function of {&„} is given by 
[1— t(s)]/A— s) with t defined in D.3). In view of the convolution property 
5 By the elementary (original) theorem of Tauber. See, for example, E. C. Titchmarsh, 
Theory of Functions, 2nd ed., Oxford 1939, p. 10. 
614 APPLYING FOURIER METHODS TO RANDOM WALKS XVIII.6 
E.10) the product of these functions represents the transform of the prob- 
abilities P{Mn e/}, and E.9) merely records this fact. 
We have already noticed that the exponents in E.8) and E.9) are regular 
for all ? with positive imaginary part. For X > 0 we may therefore put 
? = iX which leads us to the Laplace transforms 
E.12) a>n(iX) = f V* Un{dx} = X re~x* Un(x) dx. 
Jo Jo 
From the monotone character of the sequence of maxima Mn it follows that 
for fixed x the sequence {Un(x)} decreases, and hence for fixed X the 
Laplace transforms a)n(iX) form a decreasing sequence. In view of E.9) 
we have as s -> 1 
E.13) 
and by the last part of the Tauberian theorem 5 of XIII,5 this implies that 
oiJjX) r-> io(iX). This implies the asserted convergence Un-+ U. > 
6. CRITERIA FOR PERSISTENCY 
The material of this section is independent of the preceding theory. It 
is devoted to the method developed by K. L. Chung and W. H. J. Fuchs 
A950) to decide whether a random walk is persistent or transient. Despite 
the criteria and methods developed in chapters VI and XII the Fourier- 
analytic method preserves its methodological and historical interest and is 
at present the only method applicable in higher dimensions. In the following 
F stands for a one-dimensional distribution with characteristic function 
= u@ + HO- 
For 0 < s < 1 we introduce the finite measure 
F.1) 
00 
nirn* 
n=0 
According to the theory developed in VI, 10 the distribution F is transient iff 
for some open interval / about the origin US{I} remains bounded as s —>• 1; 
in this case US{I} remains bounded for every open interval /. Non-transient 
distributions are called persistent. 
Criterion. The distribution F is transient iff for some a > 0 
F-2) 
f 2 
Jo A— su) + s 
remains bounded as s —>• 1 from below. 
(It will be seen that in the contrary case the integral tends to co.) 
XVIII.6 CRITERIA FOR PERSISTENCY 615 
Proof, (i) Assume that the integral F.2) remains bounded for some fixed 
a > 0. The Parseval relation XV,C.2) applied to Fn* and a triangular 
density (number 4 in XV,2) reads 
I ^~^Fn*{dx) = - A - —) <pn< 
—* ax a j—a\ a / 
Multiplying by sn and adding over n we get 
F.4) ^ ^ "' "" ^ "'1~S^ 
2 Ca I l\ 1 — sm 
a Jo \ a/ (l-s«J + sV 
(because the real part of 9? is even and the imaginary part is odd). Let / 
stand for the interval \x\ < 2fa. For a; e/ the integrand on the left is >? 
and so US{I) remains bounded. The condition of the theorem is therefore 
sufficient. 
(ii) To prove the necessity of the condition we use Parseval's relation with 
the distribution number 5 in XV,2 which has" the characteristic function 
1 - \C\fa for |?| < a. This replaces F.4) by 
P (i - a) om=i 
04 A—SW) + ST 
For a transient F the left side remains bounded, and so the integral F.2) 
remains bounded. > 
As an application we prove a lemma which was proved by different methods 
in theorem 4 of VI, 10. For further examples see problems 13-16. 
Lemma I.6 A probability distribution with vanishing expectation is persistent. 
Proof. The characteristic function has a derivative vanishing at the origin, 
and hence we can choose a so small that 
0 ? 1 - tf(Q ? e?, for 0<,i<a. 
Then 1 — su(?) <, 1 — s + e? and using the inequality 2 \xy\ <, x2 + y2 
it is seen that the integral in F.2) is 
ae 77- 
> — 
6e 
6 The fact that E(X,) = 0 implies persistency was first established by Chung and Fuchs. 
It is interesting to reflect that in 1950 this presented a serious problem and many attempts 
to solve it had ended in failures. Attention on this problem was focused by the surprise 
discovery of the unfavorable "fair" random walk in which P{Sn > n/log/i} -*¦ 1. See 1; 
X,3 and problem 15 in 1; X,8. For a related phenomenon see the footnote to problem 13. 
616 APPLYING FOURIER METHODS TO RANDOM WALKS XVIII.7 
The right side can be made arbitrarily large, and so the integral F.2) tends 
to co. > 
The passage to the limit involved in the criterion is rather delicate, and it is therefore 
useful to have the simpler sufficient conditions stated in the following 
Corollary. The probability distribution F is persistent if 
for every a > 0, and transient if for some a > 0 
Proof. The integrand in F.2) is decreased when 1 — su in the numerator is replaced by 
1 — «, and sv in the denominator by v. Then F.6) follows by monotone convergence. 
Similarly, the integrand in F.2) increases when the term A2 is dropped, and then F.7) 
follows by monotone convergence. ^- 
These criteria apply without change in higher dimensions except that then u and v 
become functions of several variables ?,-, and the integrals are extended over spheres 
centered at the origin. In this way we prove the following criteria. 
L6mma 2. A truly two-dimensional probability distribution with zero expectations and 
finite variances is persistent. 
Proof. The characteristic function is twice continuously differentiable, and from the 
two-term Taylor expansion it is seen that in a neighborhood of the origin the integrand of 
F.6) is >?/(?2+?f). The integral corresponding to F.6) therefore diverges. > 
Lemma 3. Every truly three-dimensional distribution is transient. 
Proof. On considering the Taylor expansion of cos (x1^1+x2^2+x3^ in some x- 
neighborhood of the origin one sees that for a'ny characteristic function there exists a 
neighborhood of the origin in which 
1 - 
The three-dimensional analogue to F.7) is therefore dominated by an integral of 
(?f+?i+?|)~l extended over a neighborhood of the origin, and in three dimensions this 
integral converges. > 
7. PROBLEMS FOR SOLUTION 
1. Do the example of section 1 for the case of an unsymmetric interval —a,b. 
(Derive two linear equations for two generating functions corresponding to the 
two boundaries. Explicit solutions are messy.) 
Problems 2-5 refer to a symmetric binomial random walk, that is, <p@ — cos ?. 
The notations are those of section 1. 
2. Let A consist of the two points 0, 1. Show by elementary considerations 
that X{s, 0 - j^ ^ er* +? **\ and y(s, 4) = T^W1 +\ eA' Verify 
A.9). 
XVIII.7 PROBLEMS FOR SOLUTION 
3. If in the preceding problem the roles of A and A' are interchanged one gets 
c , r i _ VTZ^ "T1 
x(s, o - 5««+ *A ~ Vl ~j2}' 7(J' ° H* ~ -—r~^ J • 
Interpret probabilistically. 
4. If A' consists of the origin alone % depends only on s and y must be the 
sum of two power series in e& and e~«C, respectively. Using this information 
derive % and y directly from A.9). 
5. If A' consists of the origin alone one has x = sq> and y = 1. 
6. Alternative proof of the identity A.9). With the notations of section 1 show that 
n r 
:=1 JA' 
(*) *¦"*{/}-! f Hk{dy)Fi»-»*{I-y}+Gn{I} 
(a) by a direct probabilistic argument, and (b) by induction. Show that (*) is 
equivalent to A.9). 
7. In the case of a (not necessarily symmetric) binomial random walk Wald's 
approximation in section 2 leads to a rigorous solution. Show that B.12) reduces 
to a quadratic equation for t = e~x and that one is led to the solution known 
from 1; XJV,D.H). Specifically, Q(s) agrees with Ug except that the latter 
b 
refers to a basic interval 0, a rather than —a, b, and to a starting point z. 
8. As in section 2 let Gn{I} be the probability that Sn 6 / C A and that no 
exit from A = —a, b has taken place previously. Show that if two distributions 
F and F# agree within the interval \x\ < a + b they lead to the same probabilities 
Gn. Use this and an appropriate truncation for an alternative proof that the series 
B.1) converges for some s > 1. 
9. Random walks in which the distribution F is concentrated on finitely many 
integers were treated in example XII,4(c). Show that the formulas derived there 
contain implicitly the Wiener-Hopf factorization fo"r 1 — <p. 
10. (Khintchine-Pollaczek formula.) Let F be the convolution of an exponential 
with expectation ,1/a concentrated on 0, oo and a distribution B concentrated 
on —oo,0. Denote the characteristic function of B by ?, its expectation by 
—b < 0. We suppose that the expectation a~l — b of F is positive. Then 
a 
Note: This formula plays an important role in queueing theory. For alternative 
treatments see examples XII,5(a-6) and XIV,2F). 
11. {Continuation.) If ab > 1 show that there exists a unique positive number 
k between 0 and a such that 
— /k) = a — 
Prove that x~(L 0 = ° "".* ~ "^' • Hint: Apply problem 10 to the associated 
I ^ K 
random walk with characteristic function ay(Q = <p(t — /*). Recall that 
*"(!, 0 = aX~(h t - i*). [See example-XII,4F).] 
618 APPLYING FOURIER METHODS TO RANDOM WALKS XVIII.7 
12. Let Un = max [0, S^ . .., Sn] and Vn = Sn - UB. By a very slight 
change of the argument used for E.9) show that the bivariate characteristic function 
of the pair (Un, Vn) is the coefficient of sn in7 
i 
13.8 Suppose that in a neighborhood of the origin |1 - ?>(?)l <A-\?\. Then 
F is persistent unless it has an expectation n ^ 0. 
J*« fi/C 
d?\ x2 F{dx). Substitute ? = 1// 
o J-i/f 
and interchange the order of integration to see that this integral diverges unless 
H exists. 
14. Using the criterion F.7) show that if t~x~p I x2 F{dx) -*¦ oc for some 
J-t 
P > 0 as / -> oo the distribution F is transient.9 
15. The distribution with characteristic function <p(Q = e~x^\jn\ cos (/t!?) 
is transient. 
Hint: Use F.7) and the change of variable ? = A/n!)/. 
16. The unsymmetric stable distributions with characteristic exponent a = 1 are 
transient, but the Cauchy distribution is persistent. 
'First derived analytically by F. Spitzer, Trans. Amer. Math. Soc., vol. 82 A956) 
pp. 323-339. 
8 This problem commands theoretical interest. It applies whenever <p has a derivative 
at the origin. We saw in XVII,2a that this is possible even without F having an expectation, 
and that in this case the weak law of large numbers applies nevertheless. Thus we get 
examples of random walks in which for each sufficiently large n there is an overwhelming 
probability that Sn > A — €)nfi with n > 0, and yet there is no drift to a>: the random 
walk is persistent. 
9 This shows that under slight regularity conditions F is transient whenever an absolute 
moment of order p < 1 diverges. The intricacies of the problem without any regularity 
conditions are shown in L. A. Shepp, Bull. Amer. Math. Soc., vol. 70 A964) pp. 540-542. 
CHAPTER XIX 
Harmonic Analysis 
This chapter supplements the theory of characteristic functions presented 
in chapter XV and gives applications to stochastic processes and integrals. 
The discussion of Poisson's summation formula in section 5 is practically 
independent of the remainder. The whole theory is independent of chapters 
XVI-XVIII. 
1. THE PARSEVAL RELATION 
Let U be a probability distribution with characteristic function 
J"+oo 
e** U{dx}. 
— oo 
Integrating this relation with respect to some other probability distribution 
F we get 
J»+oo r+oo 
a,(?) F{dQ = cp(x) U{dx], 
— 00 J— 00 
where q> is the characteristic function of F. This is one form of the Parseval 
relation from which the basic results of XV,3 were derived. Surprisingly 
enough, a wealth of new information can be obtained by rewriting Parseval's 
formula in equivalent forms and considering special cases. A simple example 
of independent interest may illustrate this method, which will be used 
repeatedly. 
Example. The formula 
A.3) f" V*V9 F{dQ = r°V*) U{a + dx] 
J—00 J—00 
differs from A.2) only notationally. We apply the special case where F 
is the uniform distribution in — t, t and <p(x) = sin txjtx. This function 
does not exceed 1 in absolute value and as t -*¦ oo it tends to 0 at all points 
619 
620 HARMONIC ANALYSIS XIX.2 
x ?? 0. By bounded convergence we get therefore 
A.4) U(a) - U(a-) = lim - f V*V?) <*?• 
t-*ao It J—t 
This fonnula makes it possible to decide whether a is a point of continuity 
and to find the weight of the atom at a, if any. The most interesting result 
is obtained by applying A.4) to the symmetrized distribution °U with 
characteristic function \a>\2. If pltp2t... are the weights of the atoms of 
U then °U has an atom of weight }T/>| at the origin (problem 11 in V,12) 
and so 
A,5) 
f f 
2t J-t 
This formula shows, in particular, that the characteristic functions of 
continuous distributions are, on the average, small. > 
A versatile and useful variant of the Parsevai formula A.2) is as follows. 
If A and B are arbitrary probability distributions with characteristic functions 
a and C, respectively, then 
+ 00 +00 
A.6) !!co(s-tyA{ds} B{dt) = f a(z)pX^) U{dx} 
— 00 —00 
where /? is the conjugate of p\ For a direct verification it suffices to integrate 
J-+0O 
eil-t)xU{dx} 
— 00 
with respect to A and B. This argument produces the erroneous impression 
that A.6) is more general than A.2), whereas the relation A.6) is in reality 
the special case of the Parsevai relation A.2) corresponding to F = A*k~B 
where ~B is the distribution with characteristic function /? [that is, 
~B(x) = 1 — B(—x) at all points of continuity].. Indeed, F has the 
characteristic function q>.= a/?, and so the right sides in A.2) and A.6) 
are identical. That the left sides differ only notationally is best seen using 
two independent random variables X and Y with distributions A and B, 
respectively. The left side in A.6) represents the direct definition of the 
expectation E(o>(X—Y)), whereas the left side in A.2) expresses this 
expectation in terms of the distribution F of X — Y. 
(We return to Parseval's formula in section 7.) 
2. POSITIVE DEFINITE FUNCTIONS 
An important theorem due to S. Bochner A932) makes it possible to 
describe the class of characteristic functions by intrinsic properties. The 
following simple criterion will point the way. 
XIX.2 POSITIVE DEFINITE FUNCTIONS 621 
Lemma 1. Let co be a bounded continuous {complex-valued) function that 
is integrable1 over —00,00. Define u by 
1 f+0° 
B.1) «(*) = ;r 
-00 
In order that @ be a characteristic function it is necessary and sufficient that 
co@) = 1 and that u(x) ;> 0 for all x. In this case u is the probability 
density corresponding to 10. 
Proof. The Fourier inversion formula XV,C.5) shows that the conditions 
are necessary. Now choose an arbitrary even density / with integrable 
characteristic function 9? > 0. Multiply B.1) by cp{tx)eiax and integrate 
with respect to x. Since the inversion formula XV,C.5) applies to the pair 
/, <p the result is 
B.2) P°°u(x) cp(tx)eia* 
J-00 
The right side is the expectation of a> with respect to a probability distri- 
bution, and hence it is bounded by the maximum of |<x>|. For the particular 
value a = 0 the integrand on the left is non-negative and tends to u(x) 
as t -> 0. The boundedness of the integral therefore implies that u is 
integrable. Letting t -*¦ 0 in B.2) we get therefore 
J'+oo 
u(x)eiax dx = 
—0 
-00 
(the left side by bounded convergence, the right side because the prcbability 
distribution involved tends to the distribution concentrated at the point a). 
For a = Q we see that u is a probability density, and to is indeed its 
characteristic function. > 
The integrability condition of the lemma looks more restrictive than it is. 
In fact, by the continuity theorem a continuous function a> is characteristic 
iff a>(^)e~^2 is a characteristic function for every fixed « > 0. It follows 
that a bounded continuous function with <x>@) = 1 is characteristic iff for 
all x and « > 0 
J 
e-*xoj(X)e-erdt, > 0. 
— 00 
This criterion is perfectly general, but it is not easy to apply in individual 
situations; moreover, the arbitrary choice of the convergence factor <r^2 is a 
drawback. For this reason we restate the criterion in a form in which the 
condition is sharpened. 
1 As elsewhere this means absolute integrability. 
622 HARMONIC ANALYSIS XIX. 2 
Lemma 2. A bounded continuous function a> is characteristic iff &>@) = 1 
and if for every probability distribution A and all x 
J'+oo 
e-«*a>a) °A{dQ > 0 
— 00 
where °A = A*k ~A is the distribution obtained by syrnmetrization. 
Proof, (a) Necessity. If a is the characteristic function of A tnen °A has 
the characteristic function |a|2 and the necessity of B.5) is implicit in the 
Parseval relation A.3). 
(b) Sufficiency. It was shown in B.4) that the condition is sufficient if A 
is restricted to normal distributions with arbitrary variances. > 
We have seen that B.5) may be rewritten in the form A.6) with B = A. 
In particular, if A is concentrated at finitely many points tlf t2^. . . ,tn 
with corresponding weights px,p2, . . . ,pn, then B.5) takes on the form 
If this inequality is valid for all choices of ti and pi then B.5) is satisfied 
for all discrete distributions A with finitely many atoms. As every distri- 
bution is the limit of a sequence of such discrete distributions the condition 
B.6) is necessary and sufficient. With the change of notation zi = p}e~ixti 
it takes on the form 
B.7) J.oa(tj-t>)zfrt>0. 
For the final formulation of our criterion we introduce a frequently used term. 
Definition. A complex-valued function a> of the real variable t is called 
positive definite iff B.7) holds for every choice of finitely many real numbers 
tly . . . , tn and complex numbers zx, . . . , zn. 
Theorem. (Bochner.) A continuous function co is the characteristic function 
of a probability distribution iff it is positive definite and a>@) = 1. 
Proof. We have shown that the condition is necessary, and also that it is 
sufficient when . co is bounded. The proof is completed by the next lemma 
which shows that all positive definite functions are bounded. > 
Lemma 3. For any positive definite to 
B.8) ft>@) > 0, KOI < o)@) 
B.9) «(-/) = oj{t). 
XIX. 3 STATIONARY PROCESSES 623 
Proof. We use B.7) with n = 2 letting t2 = 0 and z2 = 1. Dropping 
the unnecessary subscripts we get 
B.10) w@)[l + |2|2] + <o(fJ + co(-t)z > 0. 
For 2 = 0 it is seen that o>@) > 0. For positive z we get B.9) and it 
follows that to = 0 if w@) = 0. Finally, if <o@) ^ 0 and 2 = -w@/ 
w@) then B.10) reduces to |co(O|2 < oj2@). '> 
3. STATIONARY PROCESSES 
The last theorem has important consequences for stochastic processes 
with stationary covariances. By this is meant a family of random variables 
{XJ defined for — 00 < t < 00 and having covariances such that 
C.1) Cov (Xs+t, Xs) = P(t) 
is independent of s. So far we have considered only real random variables, 
but now the notations will become simpler and more symmetric if we admit 
complex-valued random variables. A complex random variable is, of course, 
merely a pair of real variables written in the form X = U + iV and nothing 
need be assumed concerning the joint distribution of. U and V. The variable 
X = U — iV is called the conjugate of X and the product XX takes over 
the role of X2 in the real theory. This necessitates a slight unsymmetry in the 
definition of variances and covariances: 
Definition. For complex random variables vAth 
E(X) = E(Y) = 0 
we define 
C.2) Cov (X, Y) = E(XY). 
Then Var (X) = E(|X|2) > 0, but Cov (Y, X) is the conjugate of 
Cov (X, Y). 
Theorem. Let {XJ be a family of random variables such that 
C-3) P(t) = E(Xt+sXs) 
is a continuous function2 independent of s. Then p is positive definite, that is, 
C.4) p{t) = 
where R is a measure on the real line with lotal mass p@). 
2 Continuity is important: for mutually independent variables X, one has pi-tf == 0 
except when / = 0, and this covariance function is not of the form C.4). See problem 4. 
624 HARMONIC ANALYSIS XIX. 3 
If the variables Xt are real the measure R is symmetric and 
r+oo 
C.5) P(t) = cos Xt R{dX}. 
J—ao 
Proof. Choose arbitrary real points tx, . . ., tn and complex constants 
«!,..., zn. Then 
C 6) I />(';- tkyzfr = 2 E(X{Ztk)Zjrk = 
EB XXJQ E(C X|2) > 0 
and so C.4) is true by the criterion of the last section. When p is real the 
relation Cf4) holds also for the mirrored measure obtained by changing 
x to —x, and because of the uniqueness R is symmetric. > 
The measure R is called the spectral measure3 of the process; the set 
formed by its points of increase is called the spectrum of {XJ. In most 
applications the variables are centered so that E(Xt) = 0, in which case 
p(t) = Cov (Xt+S, Xs). For this reason p is usually referred to as the 
covariance function of the process. Actually the centering of Xt has no 
influence on the properties of the process with which we shall be concerned. 
Examples, (a) Let Zx,. . ., Zn be mutually uncorrelated random 
variables with zero expectation and variances of,. . . , a\. Put 
C.7) X, = Z^'+•••+. Zne""' 
with Xlt. . . , Xn real. Then 
C.8) P(t) = ajV'^ +•-.• + o&x*' 
and so R is concentrated at the n points Xx,. . . , Xn. We shall see that 
the most general stationary process may be treated as a limiting case of this 
example. ¦ , ¦ 
If the process C.7) is real it can be put into the form 
C.9) Xt = Uj cos *!/+•'• -+Ur cos XTt +• Vx sin Xxt+- • -+Vr sin Xrt 
where the U, and Vy are real uncorrelated random variables and 
E(U?) = E(V^) = o?. . 
A typical example occurs in 111,G.23). The corresponding covariances are 
p(t) = of cos Xxt + • ¦• • + al cos Xrt. 
(b) Markovian processes. If the variables Xt are normal and the process 
is Markovian, then p(t) = *ra|<l [see 111,(8.14)]. The spectral measure is 
proportional to a Cauchy density. 
3 In communication engineering, also called the "power spectrum. 
XIX.3 STATIONARY PROCESSES 625 
(c) Let Xt = ZeitY where Y and Z are independent real random 
variables, E(Z) = 0. Then 
p(t) = E(Zl)E(eitv) 
which shows that the spectral measure R is given by the probability 
distribution of Y multiplied by the factor E(ZZ). > 
Theoretically it matters little whether a process is described in terms of its 
covariance function p or, equivalently, in terms of the corresponding 
spectral measure R, but in practice the description in terms of the spectral 
measure R is usually simpler and preferable. In applications to com- 
munication engineering the spectral analysis has technical advantages in 
instrumentation and measurement, but we shall not dwell on this point. 
Of greater importance from our point of view is that linear operations (often 
called "filters") on the variables Xt are more readily described in terms of 
R than of p. 
Example, (d) Linear operations. As the simplest example consider the 
family of random variables Yt defined by 
C.10) 
where the ck and rk are constants (jk real) and the sum is finite. The 
covariance function of Yt is given by the double sum 
C.11) PY(t) = 
Substituting into C.4) one finds 
J*+oo 
— oo 
This shows that the spectral measure RY is determined by 
C.12) RY{dA) = U cje-iTiX\z R{dX}. 
In contrast to C.11) this relationship admits of an intuitive interpretation: 
the "frequency" X is affected by a "frequency response factor" /(a) which 
depends on the given transformation C.10). 
This example is of much wider applicability than appears at first sight 
because integrals and derivatives are limits of sums of the form C.10) and 
therefore a similar remark applies to them. Fo)- example, if Xt serves as 
input to a standard electric circuit, the output Y4 can be represented by 
integrals involving Xt; the spectra! measure RY is again expressible by R 
and a frequency response. The latter depends on the characteristics of the 
network, and our result can be used in two directions; namely, to describe 
the output process and also to consiruct networks which will yield an output 
with certain prescribed properties. > 
626 HARMONIC ANALYSIS XIX.4 
We turn to the converse of our theorem and show that given an arbitrary 
measure R on the line there exists a stationary process {X J with spectral 
measure R. Since the mapping Xt-+aXt changes R into a~R there is no 
loss of generality in assuming that R is a probability measure. We take 
the A-axis equipped with the probability measure R as sample space and 
denote by Xt the random variable defined by Xt(A) = em. Then 
*+oo 
C.13) P(t) = E(X<+SXS) = eia R{dA}, 
J—oa 
and so the spectral measure of our process is given by R. We have thus 
constructed an explicit model of a stationary process with the prescribed 
spectral measure. That such a model is possible with the real line as sample 
space is surprising and gratifying. We shall return to it in section 8. 
It is easy to modify the model so as to obtain variables with zero expectation. Let Y 
be a random variable that is independent of all the Xt and assumes the values ±1 each 
with probability J. Put X't = YX,. Then E(Xj) = 0 and E(X;+SX,) = E(Y2) E(X1+SXS). 
Thus {X't} is a stationary process with zero expectations, and C.13) represents its true 
covariance function. 
4. FOURIER SERIES 
An arithmetic distribution attributing probability cpn to the point n 
has the characteristic function 
D.1) rfQ =I<pneir* 
— 00 
with period 2n. The probabilities <pn can be expressed by the inversion 
formula 
D.2) yk = ±- (" 
2.7T J—ff 
which is easily verified from.D.1) [see XV,C.14)]. 
We now start from an arbitrary function cp with period 2tt and define 
<Pk by D-2). Our problem is to decide whether <p is a characteristic function, 
that is, whether {<??„} is a probability distribution. The method depends on 
investigating the behavior of the family of functions f. defined for 0 < r < 1 
by 
+ 00 
D.3) /,(?)== 2 9V 
— 00 
Despite its simplicity the same argument will yield important results con- 
cerning Fourier series and characteristic functions of distributions con- 
centrated on finite intervals. 
XIX.4 FOURIER SERIES 627 
In what follows it is best to interpret the basic interval — n, n as a circle 
(that is, to identify the points n and —n). For an integrable y the number 
<pk will be called the kth Fourier coefficient of cp. The series occurring in 
D.1) is the corresponding 'formal Fourier series." It need not converge, 
but the sequence {(pn} being bounded, the series D.3) converges to a con- 
tinuous (even differentiable) function fr. When r—*-1 it is possible for fr 
to tend to a limit y> even when the series in D.1) diverges. In this case one 
says that the series is "Abel summable" to ip. 
Examples, (a) Let <pn = 1 for n = 0, 1, 2,. . . , but <pn = 0 for n < 0. 
Each term of the series in D.1) has absolute value 1, and so the series cannot 
converge for any value ?. On the other hand, the right side in D.3) reduces 
to a geometric series which converges to 
D-4) MO = ~T? • 
1 — ret; 
As r -*¦ 1 a limit exists at all points except ? = 0. 
(b) An important special case of D.3) is represented by the functions 
1 +w 
D.5) /r@ 
27T-OO 
obtained when <pn = 1/Bn) for all n. The contribution of the terms 
n > 0 was evaluated in D.4). For reasons of symmetry we get 
D.6) ± J 
) r + 
1 — reu 1 — re 
or 
D.7) . Pr(t) ± ^^ 
2n 1 + r — 2r cos t 
This function is of constant use in the theory of harmonic functions where 
pT{t—Q is called the "Poissen kernel." For reference we state its main 
property in the next lemma. 
• Lemma. For fixed 0 < r •< 1 the function pr is the density of a prob- 
ability distribution Pr on the circle. As r —*¦ 1 the latter tends to the probability 
distribution concentrated at the origin. 
Proof. Obviously pr > 0. That the integral of pr over — n, it equals 
one is evident from D.5) because for n -^ 0 the integral of 'eint vanishes. 
For S < t < 7T the denominator in D.7) is bounded away from zero. As 
r—*-\ it follows that in every open interval excluding the origin p-r{t)-*~Q 
boundedly as r —> 1, and so Pr has a limit distribution concentrated at the 
origin. > 
628 HARMONIC ANALYSIS XIX.4 
Theorem 1. A continuous function cp with period 2n is a characteristic 
function iff its Fourier coefficients D.2) satisfy <ph > 0 and <p@) = 1. In 
this case <p is represented by the uniformly convergent Fourier series D.1). 
[In other words, a formal Fourier series with non-negative coefficients <pk 
converges to a continuous function iff J cplz < oo. In this case D.1) holds.] 
Proof. In view of D.2) and D.5) the function fr of D.3) may be put into 
the form 
D-8) frtt) = \'V(t) ¦ Prtt-t) dt. 
J—v 
On the right we recognize the convolution of y and the probability distri- 
bution PT, and we conclude 
T, 
D.9) 
Furthermore, if m is an Upper bound for \cp\ then by D.8) 
+ 00 
D.10) W)=2<Pnrinl< m. 
— oo 
The terms of the series being non-negative it follows for r —*¦ 1 that 
2 Vn ^ m- Therefore 2 Vn6™^ converges uniformly and it is evident from 
D.3) that fr(X) tends to this value. Thus D.1) is true, and this concludes the 
proof. > 
Note that D.9) is a direct consequence of the convergence properties of 
convolutions and hence independent of the positivity of the coefficients cpn. 
As a by-product we thus have 
Theorem 2.4 If cp is continuous with period 2nt' then D.9) holds uniformly 
in 
(For generalizations to discontinuous functions see corollary 2 and 
problems 6-8.) 
Corollary 1. (Fejer.) A continuous periodic function <p is the uniform 
limit of a sequence of trigonometric polynomials. 
4 The theorem may be restated as follows: The Fourier series of a continuous periodic 
function <p is Abel summable to <p. The theorem (and the method of proof) apply equally 
to other methods of summability. 
The phenomenon was first discovered by L. Fejer using Cesaro summability (see problem 
9) at a time when divergent series still seemed mysterious. The discovery therefore came as 
a sensation, and for historical reasons texts still use Cesaro summability although the Abel 
method is more convenient and unifies proofs. 
XIX.5 THE POISSON SUMMATION FORMULA 629 
In other words, given e > 0 there exist numbers a_N,. . . , aN such that 
N 
D-11) <K0~ 2 aneir 
for all ?. 
Proof. For arbitrary N and 0 < r < 1 
D.12) 
N 
- 2 1 
n=-N 
~ fr(Q\ 
The claim is that we can choose r so close to 1 that the first term on the 
right will be <e/2 for all ?. Having chosen r we can choose N so 
large that the last series in D.12) adds to < e/2. Then D.11) holds with 
<*n = 9Vlnl- > 
The following result is mentioned for completeness only. It is actually 
contained in lemma 1 of section 6. 
Corollary 2. Two integrable periodic functions with identical Fourier 
coefficients differ at most on a set of measure zero {that is, their indefinite 
integrals are the same). 
Proof. For an integrable periodic cp with Fourier coefficients cpn put 
D.13) <t>(*) = (*[<p(t)-(po]dt. 
J—n 
This <5 is a continuous periodic function, and an integration by parts shows 
that for « 7^ 0 its «th Fourier coefficient equals — icpjn. 
The relations D.9) and D.3) together show that a continuous function cp 
is uniquely determined by its Fourier coefficients <pn. The coefficients cpn 
with n t& 0 therefore determine cp up to an additive constant. For an 
arbitrary integrable cp it follows that its Fourier coefficients determine the 
integral <I>, and hence cp is determined up to values on a set of measure 
zero. > 
*5. THE POISSOISf SUMMATION'FORMULA 
In this section <p stands for a characteristic function such that \<p\ is 
integrable over the whole line. By the Fourier inversion formula XV,C.5) 
this implies the existence of a. continuous density f. By the Riemann-Lebesgue 
lemma 3 of XV,4 both / and cp vanish at infinity. If <p tends to zero 
sufficiently fast it is possible to use it to construct periodic functions by a 
method that may be described roughly as wrapping the ?-axis around a 
* This section treats important special topics. It is not used in the sequel, and it is 
independent of the preceding sections except that it uses theorem 1 of section 4. 
630 HARMONIC ANALYSIS XIX.5 
circle of length 2A. The new function may be presented in the form 
+ 00 
E.1) wit) = I <p(t+2kX). 
(The sum of a doubly infinite series 2?loo <*k is here defined as l\m^=_N ak 
when this limit exists.) In case of convergence the function y is obviously 
periodic with period 2A. In its simplest probabilistic form the Poisson 
summation formula asserts that whenever y> is continuous v>(?)/v>@) is. a 
characteristic function of an arithmetic probability distribution with atoms of 
weight proportional to firnrfX) at the points mrfX. (Here « = 0, ±1, 
'±2,. . ..) At first sight this result may appear as a mere curiosity, but it 
contains the famous sampling theorem of communication theory and many 
special cases of considerable interest. 
Poisson summation formula.5 Suppose that the characteristic function y 
is absolutely integrable, and hence the corresponding probability density f 
continuous. Then 
+ 00 +00 
E.2) - ^ '" ' ~' "x 
7 
— 00 A —00 
provided the series on the left converges to a continuous function ip 
For ? = 0 this implies that 
+ 00 
E.3) !<pBkX) = GTlXIf(n7rlX) 
— 00 
is a positive number A, and so ip(X)jA is a characteristic function. 
Proof. It suffices to show that the right side in E.2) is the formal Fourier 
series of the periodic function y> on the left, that is 
E.4) 
J-x 
In fact, these Fourier coefficients are non-negative, and y> was assumed 
continuous; by theorem 1 of the preceding section the Fourier series there- 
fore converges to ip, and so E.2) is true. 
The contribution of the kth. term of the series E.1) for y) to the left side 
in E.4) equals 
i fX i rBk+l)X 
E.5) — (p(C+2M)e-in{ir/XK dC = — (p{s)e-in{itlx)s ds 
2A J-x 2X JBk-Dx 
5 The identity E.2) is usually established under a variety of subsidiary conditions. Our 
simple formulation, as well as the greatly simplified proof, are made possible by a systematic 
exploitation of the positivity of /. For .variants of the theorem and its proof see problems 
12 and 13. 
XIX.5 THE POISSON SUMMATION FORMULA 631 
and is in absolute value less than 
JHZk+VH 
\cp(s)\ ds. 
l2k-l)X 
The intervals Bk — 1)A < s < Bk + 1)A cover the real axis without 
overlap, and so the quantities E.6) add up to the integral of \cp\ which is 
finite. Summing E.5) over —N<k<N and letting N->oo we get 
therefore by dominated convergence 
E.7) ± y>(Oe-in{"nK dt, = - cp(s) e-inMx)s ds. 
2/ J-a 2X J-oo 
The right side equals Gr/A)/(«7r/A) by the Fourier inversion theorem and this 
concludes the proof. > 
The most interesting special case arises when y vanishes identically for 
|?| > a where a < h The infinite series in E.1) reduces to a single term, 
and y) is simply the periodic continuation of y with period 2a. Then E.2) 
holds. In E.3) the left side reduces to 1 which shows that ip is the character- 
istic function of a probability distribution. We have thus the 
Corollary. If a characteristic function vanishes for 11\ ^> a then all its 
periodic continuations with period 2 A > 2a are again characteristic functions. 
This corollary is actually somewhat sharper6 than the "sampling theorem" 
as usually stated in texts on communication engineering and variously 
ascribed to H. Nyquist or C. Shannon. 
"Sampling theorem." A probability density f whose characteristic function 
cp vanishes outside —a, a is uniquely determined1 by the values Gr!X)f(n7rjX) 
6 Usually unnecessary conditions are introduced because the proofs rely on standard 
Fourier theory which neglects the positivity of / germane to probability theory. 
7 An explicit expression for f(x) may be derived as follows. For \t,\ < A we have 
and hence by the Fourier inversion formula 
Now y> is given by the right side in E.2), and a trite integration leads to the final formula 
This expansion is sometimes referred to as "cardinal series." It k^. muny applications. 
[See theorem 16 in J. M. Whittaker, Interpolator)'function theory, Cambridge Tracts No. 33 
J935. For an analogue in higher dimensions see D. P. Peterscn and D. Vliddleton, 
Information and Control, vol. 5 A962) pp. 279-323.] 
632 HARMONIC ANALYSIS XIX.5 
for any fixed a > a. {Here n=0, ±1,....) (These values induce a 
probability distribution whose characteristic function is the periodic con- 
tinuation of (p with period 2a.) 
Examples, (a) Consider the density f(x) = A— cos x)I(ttx2) with the 
characteristic function <p(?) = 1 — |?| vanishing for |?| > 1. For X = 1 
and ? = 1 we get from E.3) remembering that /@) = l/Brr) 
E>8) \ + ± 
2 7T 
The periodic continuation of cp with period 2A = 2 is graphed in figure 
2 of XV,2, a continuation with period A > 2 in figure 3. 
(b) For a simple example for E.2) see problem 11. > 
As usual in similar situatiens, formula E.2) may be rewritten in a form 
that looks more general. Indeed, applying E.2) to the density f(x+s) we 
get the alternative form of the Poisson summation formula 
E-9) 2K?+U)<T 
-co A —\ / 
Examples, (c) Applying E.9) to the normal density and using only the 
special value ? = X one gets 
E.10) +f e-^k+1)^cosBk + l)h = - +f (-l)*h(B/c+1Or + s). 
-00 A —00 \ / / 
This is a famous formula from the theory of theta functions which was 
proved in X,5 by more elementary methods. In fact, differentiation with 
respect to x shows that the identity of X E.8) and X E.9) is equivalent to 
E.10) with a = {?r\d)\lt and .s = xjyjt. 
(d) For the derisity f(x) = 7r~1(l+a;2)"~1 with characteristic function 
<p(t) = e-M we get from E.2) for ? = 0 
.2 2 
e* — e~* n=-oo a2 + nV 
This is the partial fraction decomposition for the hyperbolic cotangent. 
(e) Densities on the circle of length 2-n may be obtained by wrapping the 
real axis around the circle as described in 11,8. To a given density / on the 
line there corresponds on the circle the density given by the series 
^/Bttw + s). From E.9) with ? = 0 we get a new representation .of this 
density in terms of the original characteristic function. In the special case 
XIX.6 POSITIVE DEFINITE SEQUENCES 633 
/ = n we get the analogue to the normal density on the unit circle in the form 
1 +00 /1 \ 1 +00 
E.12) -4=]>>P (_-L(s+2w7rJ = ±- 2>-ll 
j2t -00 \ 2t / 2iT -00 
The second representation shows clearly that the convolution of two normal 
densities with parameters tx and t2 is a normal density with parameter 
h + h. > 
6. POSITIVE DEFINITE SEQUENCES 
This section is concerned with probability distributions on a finite interval; 
for definiteness its length will be taken to be 2rt. As in section 4 we identify 
the two endpoints of the interval and interpret the latter as a circle of unit 
radius. Thus we consider F as a. probability distribution on the unit circle 
and define its Fourier coefficients by 
F.1). yk = ^~ \\~ikiF{dt\ /c = 0, ±1, .... 
2rt J->r 
Note that <pk = cp_k. It will now be shown that the coefficients <pk 
uniquely determine the distribution. Allowing for a trivial change of scale 
the assertion is equivalent to the following: A distribution concentrated on 
—X, X is uniquely determined by the knowledge of the values <p(mr/X) assumed 
by its characteristic function at the multiples of ttJX. The assertion represents 
the dual to the sampling theorem of the preceding section according to which 
a characteristic function vanishing outside —X, X is uniquely determined by 
the values f(rnrjX) of the density. 
Theorem 1. A distribution F on the circle is uniquely determined by its 
Fourier coefficients cpk. 
Proof. As in D.3) we put for 0 < r < 1 
F.2) fr@ = 
— 00 
The trite calculation that led to D.8) shows that now 
F.3) 
where pT stands for the Poisson kernel defined in D.7). We know that pr is 
a probability density, and we denote the corresponding distribution PT. 
Then fr is the density of the convolution Pr~k F which tends to F as 
r -> 1, and so F is actually calculable in terms of fT. > 
634 HARMONIC ANALYSIS XIX.6 
Lemma 1. Let {cpn} be an arbitrary bounded sequence of complex numbers. 
In order that there exists a measure F on the circle with Fourier coefficients 
cpn it is necessary and sufficient that for each r< 1 the function f defined in 
F.2) be non-negative. 
Proof. The necessity is obvious from F.3) and the strict positivity of pr. 
Multiply F.2) by e~ik!> and integrate to obtain 
F.4) 
2 77 J-* 
For the particular value k = 0 it is seen that cp0 > 0 and without loss of 
generality we may assume that cp0 = 1/Btt). With this norming fr is the 
density of a probability distribution Fr on the circle, and F.5) states that 
9V1*1 is the kih. Fourier coefficient of Fr. By the selection theorem it is 
possible to let r —>¦ 1 in such a manner that Fr converges to a probability 
distribution F. From F.5) it is obvious that <pk satisfies F.1), and this 
completes the proof. >¦ 
Note that this lemma is stronger than corollary 2 in section 4. We proceed 
as in section 2 and derive a counterpart to Bochner's theorem; it is due to 
G. Herglotz. 
Definition. A sequence {(pk) is called positive definite if for every choice 
of finitely many complex numbers zly . . . , zn 
F.5) 2 <Pi-t zFk > 0. 
Lemma 2. // {(pn) is positive definite then y>0>0 and \<pn\ < <pQ. 
Proof. The proof of lemma 3 of section 2 applies. (See also problem 14.) 
Theorem 2. A sequence {(pn) represents the Fourier coefficients of a 
measure F on the circle iff it is positive definite. 
Proof, (a) A trite calculation shows that if the yk are given by F.1) the 
left side in F.5) equals the integral of A/2^) |J e~ij%\2 with respect to F. 
The condition is therefore necessary. 
(b) To show its sufficiency choose zk = rkem for k > 0 and zk = 0 for 
k < 0. With this sequence the sum in F.5) takes on the form 
00 00 +00-00 +00 
F-6) 2 t» ||tt * I 
fc=0 
and by the definition F.2) the last sum equals fr(t)- It is true that the in- 
equality F.5) was postulated only for finite sequences {zn}, but a simple 
XIX. 7 L2 theory 635 
passage to the limit shows that it applies also to our infinite sequence, and we 
have thus proved that fr(t) ^ 0. By lemma 1 this implies that the <pn are 
indeed the Fourier coefficients of a measure on the unit circle. >> 
From this criterion we derive an analogue to the theorem of section 3: 
Theorem 3. Let {Xn} be a sequence of random variables defined on some 
probability space such that 
F-7) Pn = E(Xn+X) 
is independent of v. Then there exists a unique measure R on the circle 
—77,77 such that pn is its nth Fourier coefficient. 
Proof. Clearly 
F-8) 2 Pi^fr = 2 E(X,2iX^) = E(|2 XjZj\2) 
which shows that the sequence {pn} is positive definite. *> 
The converse is also true: to any measure on the circle there exists a 
sequence {Xn} such that F.7) yields its Fourier coefficients. This can be 
seen by the construction used at the end of section 3, but we shall return to 
this point in section 8. 
Examples, (a) Let the Xn be real identically distributed independent 
variables with E(Xre) = /u and Var (Xn) = a2. Then pQ = a2 + fx2 and 
pk = ju2 for all k # 0. The spectral measure is the sum of an atom of 
weight /a2 at the origin plus a uniform distribution with density o-2/Bt7). 
(b) The construction in section 4 shows that the density defined for fixed 
r < 1 and 0 by pr(t—6) has Fourier coefficients pn = rMein0. 
(c) Markov processes. It was shown in 111,8 that stationary Markov 
sequences of real normal variables have covariances of the form pn = r|re| 
with 0 < r < 1. A similar argument shows that the covariances of arbitrary 
complex stationary Markov sequences are of the* form rMeiTI°. When 
r < 1 the spectral measure has density pr(t — O); when r = 1 it is con- 
centrated at the point 6. > 
7. L2 THEORY 
For purposes of probability theory it was necessary to introduce character- 
istic functions as transforms of measures, but other approaches to harmonic 
analysis are equally natural. In particular, it is possible to define Fourier 
transforms of functions (rather than measures) and the Fourier inversion 
formula makes it plausible that greater symmetry can be achieved in this way. 
It turns out that the greatest simplicity and elegance is attained when only 
636 HARMONIC ANALYSIS XIX.7 
square integrable functions are admitted. This theory will now be developed 
for its intrinsic interest and because it is extensively used in the harmonic 
analysis of stochastic processes. 
For a complex-valued function u of the real variable x we define the 
norm \\u\\ > 0 by 
— 
G.1) IM|0= \u(x)\2dx. 
J—00 
Two functions differing only on a set of measure zero will be considered 
identical. (In other words, we are actually dealing with equivalence classes 
of functions, but indulge in a usual harmless abuse of language.) With this 
convention \\u\\ = 0 iff u = 0. The class of all functions with finite norm 
will be denoted by L2. The distance of two functions u, v in L2 is defined 
by ||u — i>||. With this definition L2 is a metric space and a sequence of 
functions un in L2 converges in this metric to u iff \\un — u\\ —*- 0. This 
convergence8 will be indicated by u = l.i.m. un or un 1^1^. u. It is also called 
"convergence in the mean square". {un} is a Cauchy sequence iff 
IIun — um\\ —*" 0 as n, m —*- go. 
We mention without proof that the metric space L2 is complete in the sense 
that every Cauchy sequence {un} possesses a unique limit u e L2. 
Examples, (a) A function u in L2 is integrable over every finite interval 
because \u(x)\ < \u(z)\2 + 1 at all points. The statement is not true for 
infinite intervals since A+l-zl)-1 is in L2 but not integrable. 
(b) Every bounded integrable function is in L2 because \u\ < M implies 
\u\2 < M \u\. The statement is false for unbounded functions since x~- is 
integrable over 0, 1, but not square integrable. > 
The inner product (w, v) of two functions is defined by 
J'+oo 
u(x) v(x) dx. 
—oo 
It exists for every pair of functions in L2 since by Schwarz' inequality 
G.3) P \uv\ dx < \\u\\ ¦ \\v\\. 
J-co 
In particular, (w, u) = ||m||2. With this definition of the inner product V- 
8 Pointwise convergence of un to a limit v does not imply that un '¦'»¦ > v [see example 
IV,2(e)]. However, if it is known that if also u = l.i.m. un exists, th^n u = v. In fact, 
by Fatou's lemrm 
\u(x) - v(x)\2 dx < !im \u(x) - un(x)\2 dx = 0. 
J —OO J—00 
XIX.7 L2 theory 637 
becomes a Hilbert space. The analogy of the inner product G.2) with the 
covariance of two random variables with zero expectations is manifest and 
will be exploited later on. 
After these preparations we turn to our main object, namely to define 
transforms of the form 
1 C+a0 
= -~ u{x)e«*dx. 
yJlTT J-OO 
G.4) 
When u is a probability density, u differs by the factor V277- from the 
characteristic function and to avoid confusion u will be called the Plancherel 
transform of u. The definition G.4) applies only to integrable functions 
u but we shall extend it to all U. The following examples may facilitate an 
understanding of the procedure and of the nature of the generalized transform. 
Examples, (c) Any function u in L2 is integrable over finite intervals, 
and hence we may define the truncated transforms 
Note that u{n) is the true Plancherel transform of the function u{n) defined 
by u{n){x) = u(x) for |a;| < n and u{n){x) = 0 for all other x. As n -> go 
the values u{n)@ need not converge for any particular ?, but we shall 
show that {uln)} is a Cauchy sequence and hence there exists an element 
u of L2 such that u = l.i.m. u{n). This u will be defined to be the 
Plancherel transform of u even though the integral in G.4) need not con- 
verge. The particular mode of truncation plays no role, and the same u 
might have been obtained by taking any other sequence of integrable functions 
u{n) converging in the mean to u. 
(d) If u stands for the uniform density in —1,1 then its Plancherel 
transform u = sin x\{x\J2tt) is not integrable. However, u is in L2 and 
we shall see that its Plancherel transform coincides with the original density 
u. In this way we get a generalization of the Fourier inversion formula 
XV,C.5) applicable to densities whose characteristic functions are not 
integrable. > 
We proceed to the definition of the general Plancherel transform. For any 
integrable function u the transform u is defined by G.4). Such u is con- 
tinuous (by the principle of dominated convergence) and |w| is bounded by 
Btt)~- times the integral of \u\. In general u is not integrable. For brevity 
we now agree to call a function u "good11 if it is bounded and continuous, 
and u as well as u is integrable. Then also u is good, and both u and 
u belong to L2 [see example (b)]. 
—= u@e ** ilQ dC = u(x)n[ — 
y/ZTT J-ao J-oo \ € / € 
638 HARMONIC ANALYSIS XIX.7 
First we show that the inversion formula 
1 C+oo 
G-5) W@ = ~= (KQe-*'dl 
yJ2.TT J— 00 
holds for good functions. We repeat the argument used for the inversion 
formula in XV,3. Multiply G.4) by (lly/ln^-**-***? and integrate to 
obtain 
G.6) 
where n denotes the standard normal density. As e —*- 0 the integral on 
the left tends to the integral in G.5) while the convolution on the right tends 
to u{t). Thus G.5) holds for good functions. 
Now let. v be another good function. Multiply G.5) by the conjugate 
v{t) and integrate over — oo < / < oo. The left side equals the inner pro- 
duct (u, v) and after interchanging the order of integration the right side 
reduces to (u, v). Thus good functions satisfy the identity 
G.7) (w, v) = (u, v) 
which will be referred to as the Parseval relation for U. For v = u it 
reduces to 
G.8) || M|| = ||M||. 
It follows that the distance of two transforms u and v is the same as tlie 
distance between u and v. We express this by saying that among good 
functions the Plancherel transform is an isometry. 
Next we show that the relations G.7) and G.8) remain valid for arbitrary 
integrable functions u and v belonging to L2. The transforms are not 
necessarily integrable, but G.8) implies that they belong to L2 [compare 
examples (a) and (d)]. 
First we observe that an integrable function w with two integrable 
derivatives is necessarily good; indeed, from lemma 4 of XV,4 one concludes 
that \w(?)\ =¦ o(t,~2) as ?—»¦ ±oo, and so w is certainly integrable. 
Suppose now that u is bounded and integrable (and hence in L2). By 
the mean approximation theorem of IV,2 it is possible to find a sequence of 
good functions un such that 
J*+oo 
\u(x) - un{x)\ dx - 0. 
— 00 
If \u\ < M these un may be chosen such that also \un\ < M. Then un 
tends in the mean to u because \\u — uj\2 cannot exceed 2A/ times the 
integral in G.9). The isometry G.8) for good functions therefore guarantees 
XIX.7 L2 THEORY 639 
that {uj is a Cauchy sequence. On the other hand, ?„(?) -> u{l) for every 
fixed ? because |w(?) — wn(?)l cannot exceed the integral in G.9). But, as 
pointed out in the last footnote, the pointwise convergence of the elements of 
a Cauchy sequence entails the convergence of the sequence itself, and thus 
we have 
G.10) m = l.i.m. un. 
\pplying G.7) to the pair un and v and letting n —*¦ go we see now that 
G.7) remains valid whenever »w is bounded and integrable while v is good. 
Another such passage to the limit shows that G.7) remains valid for any pair 
of bounded integrable functions. 
It remains to show that G.7) is valid also for unbounded functions u 
and v provided they are integrable and belong to L2. For the proof we 
repeat the preceding argument with the sole change that the approximating 
functions un are now defined by truncation: un{x) = u{x) if \u(x)\ < n 
and un(x) = 0 for all other x. Then G.10) holds and un-^>u, and the 
proof applies without further change. 
We are now ready for the final step, namely to extend the definition of the 
Plancherel transform to the whole of L2. As shown in example (c), every 
function u in L2 is the limit of a Cauchy sequence of integrable functions 
un in L2. We have just shown that the transforms iin defined by G.4) 
form a Cauchy sequence, and we now define u as the limit of this Cauchy 
sequence. Since two Cauchy sequences may be combined into one the limit 
u is independent of the choice of the approximating sequence {«„}. Also, if 
u happens to be integrable we may take un = u for all n, and thus it is 
seen that the new definition is consistent with G.4) whenever u is. integrable. 
To summarize: 
A Plancherel transform u is defined for every u in L2; for integrable u 
it is given by G.4), and in general by the rule that 
G.11) // u = l.i.m. un then u = l.i.m. un. 
The Parseval relation G.7) and the isometry G.8) apply generally. The mapping 
u-+ii is one-to-one, the transform of u being given by u(—x). 
The last statement is a version of the Fourier inversion formula G.5) 
applicable when u or u are not integrable so that the integrals in G.4) and 
G.5) are not defined in the usual sense. This complete symmetry in the 
relationship between the original functions and their transforms represents 
the main advantage of Fourier theory in Hilbert spaces. 
The theory as outlined is widely used in prediction theory. As an example 
of a probabilistic application we mention' a criterion usually ascribed to A. 
640 HARMONIC ANALYSIS XIX.7 
Khintchine although it appears in the classical work of N. Wiener. For 
reasons of historical tradition even newer texts fail to notice that it is really 
merely a special case of the Parseval formula and requires no elaborate proof. 
Wiener-Khintchine criterion. In order that a function <p be the characteristic 
function of a probability density f it is necessary and sufficient that there exist 
a function u such that \\u\\2 = 1 and 
G.12) <p(X) = I u(x) u{x+X) dx 
J~ 00 
In this case f = \\u\\2. 
Proof. For fixed A put v(x) = u{x+X). Then i>(?) = fi(?)e-"t. The 
Parseval relation G.7) reduces to 
/*+oo /*+oo 
G.13) |w(a;)|2 eiXx dx = u(x) u{x+X) dx. 
J—00 J—00 
and since ||w||2 = 1 the left side represents the characteristic function of a 
probability density. Conversely, given a probability density / it is possible 
to choose u such that /= \u\2, and then G.12) holds. The choice of u is 
not unique. (One problem of prediction theory concerns the possibility of 
choosing u vanishing on a half-line.) > 
The L2 theory for Fourier integrals carries over to Fourier series. The 
functions are now defined on the circle, but to the Fourier (or Plancherel) 
transforms there correspond now sequences of Fourier coefficients. Except 
for this formal difference the two theories run parallel, and a brief summary 
will suffice. 
To our L2 there corresponds now the Hilbert space L2(—tt, tt) of 
square integrable functions on the circle. The norm and inner product are 
now defined by 
G.14) Mi2 = ±- [\u(x)\2 dx, (u, v) = -±- \\{x) 55) dx 
/.TT J—TT /TT J—TT 
it being understood that the integrals extend over the whole circle (the points 
—tt and tt being identified). The role of "good functions" is played by 
finite trigonometric polynomials of the form 
G.15) u(x) = Zuneinx, 
whose Fourier coefficients un are given by 
.10) un — u\x)e ax. 
/TT J—!T 
To a good function there corresponds the finite sequence {«„} of its coeffi- 
cients, and, conversely, to every finite sequence of complex numbers there 
XIX.8 STOCHASTIC PROCESSES AND INTEGRALS 641 
corresponds a good function. The relations G.16) and G.15) define the 
Fourier transform u = {un) and its inverse. Formal multiplication and 
integration shows that for two good functions 
G.17) J \ 
We now consider the Hilbert space § of infinite sequences u = {«„}, 
v = {vn}, etc., with norm and inner product defined by 
G-18) ll«il=2KI2, (M) = 2«»Iv 
This space enjoys properties analogous to L2; in particular, finite sequences 
are dense in the whole space. It follows that there exists a one-to-one 
correspondence between the sequences u = {«„} in ?> and the functions 
I 
u in L2(—77-, 77-). To each sequence {un} such that ]? [un\2 < oo there 
corresponds a square integrable function u with Fourier coefficients un and 
conversely. The mapping u <-> {un} is again an isometry, and the Parseval 
relation {u, v) = (u, v) holds. The Fourier series need not converge but the 
partial sums 2-n ukeikx form a sequence of continuous functions that 
converges to u in the L2 metric. The same statement is true of other 
continuous approximations. Thus ]? ukrMeik* tends to u as r->l. 
As above, we consider the special case of the Parseval relation represented 
by 
G.19) ~ f u{x) v{x)e-™* dx = 2 uk+nTk. 
Ltt J—!t k 
Choosing v = u one sees again that a sequence {<pn} represents the Fourier 
I 
coefficients of a probability density on —it, it iff it is of the form 
G.20) <Pn = 2w^- where 2 \uk\2 = 1. 
A covariance of this form occurs in 111,G.4). (See also problem 17.) 
8. STOCHASTIC PROCESSES AND INTEGRALS 
For notational simplicity we refer in this section to sequences {Xn} of 
random variables, but it will be evident that the exposition applies to families 
depending on a continuous time parameter with the sole change that the 
spectral measure is not confined to a finite interval and that series are replaced 
by integrals. 
Let, then, {Xn} stand for a doubly infinite sequence of random variables 
defined on some probability space S and having finite second moments. 
The sequence is assumed stationary in the restricted sense that 
vXv) = pn 
642 HARMONIC ANALYSIS XIX.8 
is independent of v. According to theorem 3 of section 6 there exists a unique 
measure R on the circle — tt, tt such that 
(8-1) Pn = 7~ ^e-in*R{dx}, „= 0, ±1, .... 
We shall now elaborate on the idea mentioned in section 3 that the circle 
equipped with the spectral measure R may be used to construct a concrete 
representation for the stochastic process {XJ (at least for all properties 
depending only on second moments). The description uses Hilbert space 
terminology, and we shall consider two Hilbert spaces. 
{a) The space L\. We construct a space of functions on the circle by 
literal repetition of the definition of L2 in section 7, except thai the line 
is replaced by the circle — tt, tt and Lebesgue measure by the measure R. 
The norm and the inner product of (complex-valued) functions on the circle 
are defined by 
(8.2) N|2 = ± [\u{x)\* R{dx}, (u, v) = — \\{x) J(?) R{dx}, 
ITT J-tt . 2tT J-tt 
respectively. The basic convention now is that two functions are considered 
identical if they differ only on a set of R-measure zero. The impact of this 
convention is serious. If R is concentrated on the two points 0 and 1 then a 
"function" (in our sense) is completely determined by its values at these 
two points. For example, sin rnrx is the zero function. Even in such radical 
cases no harm is done in using the customary formulas for continuous functions 
and in referring to their graphs. Thus reference to a "step function" is always 
meaningful and it simplifies the language. 
The Hilbert space L^ consists of all functions on the circle with finite norm. 
If \\un — u\\ -> 0 the sequence {un} is said to converge to u in our metric 
(or in mean square with respect to the weight distribution R). The Hilbert 
space L\ is a complete metric space in which the continuous functions are 
dense. (For definitions see section 7.) 
(b) The Hilbert space § spanned by {Xn}. Denote by §0 the family of 
random variables with finite second moments defined in the arbitrary, 
but fixed, sample space S. By Schwarz's inequality E(UV) exists for any 
pair of such variables, and it is natural to generalize (8.2) from the circle 
to the sample space o using the underlying probability measure instead of 
R. We accordingly agree again to identify two random variables if they differ 
only on a set of probability zero and define inner products by E(UV); the 
norm of U is the positive root of E(Ufj). With these conventions ?>0 again 
becomes a Hilbert space; it is a complete metric space in which a sequence of 
random variables Un is said to converge to U if E(jUn—U|2) —>-0. 
XIX.8 STOCHASTIC PROCESSES AND INTEGRALS 643 
In dealing with a sequence {Xn} one is usually interested only in random 
variables that are functions of the Xk and in many connections one considers 
only linear functions. This restricts the consideration to finite linear com- 
binations ^ akXk and limits of sequences of such finite linear combinations. 
Random variables of this kind form a subspace § of §0, called the Hilbert 
space spanned by the Xk. In it inner products, norms, and convergence are 
defined as just described and § is a complete metric space. 
In the present context the expectations E(XJ play no role whatever, 
but as "covariance" sounds better than "inner product" we introduce the 
usual convention that E(XJ = 0. The sole purpose of this is to establish pn 
as a covariance, and no centering is necessary if one agrees to call E(XY) 
the covariance of X and Y. We come now to the crucial point, namely that 
for our purposes the intuitively simple space L\ may serve as concrete model 
for 9). Indeed, by definition the covariance pj_k = Cov (X,, Xk) of any 
pair equals the inner product of the functions eijx and eikx in L\. It 
follows that the covariance of two finite linear combinations U = 2 ai^i 
and V = ^ bkXk equals the inner product of the corresponding linear 
combinations u = ]? ateiix and v = ]? b^***. By the very definition of 
convergence in the two spaces this mapping now extends to all random 
variables. We have thus the important result that the mapping Xk<-> eikx 
induces a one-to-one correspondence between the random variables in ?> and 
the functions in LR, and this correspondence, preserves inner products and 
norms (and hence limits). In technical language the two spaces are isometric.9 
We are in a position to study § and {Xn} referring explicitly only to the 
concrete space LR. This procedure has theoretical advantages in addition 
to being an aid to intuition. Since functions on the circle are a familiar object 
it is relatively easy to discover sequences {u{n)} of functions in L% with 
desirable structural properties. To u(n) there corresponds a random variable 
Zn on the original sample space S; if the Fourier coefficients of u(n) are 
known it is possible to represent Zn explicitly as a limit of finite linear 
9 Readers acquainted with Hilbert space theory should note the connection with the 
standard spectral theorem for unitary operators. The linear operator which maps § into 
itself in such a way that Xn —> Xn+1 is called a shift operator, and LR serves as a model 
in which the action of this shift operator becomes multiplication by eix. Conversely, given 
an arbitrary unitary operator T on a Hilbert space <r>0 and an arbitrary element X0G §0, 
the sequence of elements Xn = TnX0 may be treated as a stationary sequence and T as 
the shift operator on the subspace 9) spanned by this sequence. If Xo can be chosen such 
that § = §0 we have obtained the standard spectral theorem for T except that we have 
a concrete representation of the "resolution of the identity" based on the choice of Xq. 
If § c §0, then §0 is the direct sum of two invariant subspaces, and the presentation 
applies to each of them. By a simple change of notations one derives the general spectral 
representation, including the theory of multiplicity for the spectrum. 
644 HARMONIC ANALYSIS XIX.8 
combinations of the variables Xk. If the joint distributions of the Xk are 
normal the same is true for those of the Zn. 
In practice this procedure is usually reversed. Given a complicated process 
{XjJ our aim is to express it in terms of the variables Zn of a simpler process. 
A practical way to achieve this is to proceed in L\ rather than in the original 
space. A few examples will explain this better than a theoretical discourse. 
Examples, (a) Representation of {Xn} by independent variables. As 
elsewhere in this section {Xn} stands for a given process with covariances pn 
and spectral measure R defined by (8.1). In our mapping Xn corresponds 
jto the function einx on the circle. We show now that for certain functions y 
the random variables corresponding to einxjy{x) are uncorrelated. We con- 
sider only the situation when the spectral measure R has an ordinary density 
r. For simplicity10 r will be assumed strictly positive and continuous. 
Choose a function y such that 
(8.3) \y(x)\* = r(x). 
The Fourier series of y converges in the L2 norm as explained in section 7. 
Denoting the Fourier coefficients of y by yk we have ]? \yk\2 < oo and by 
the Parseval relation G.20) 
+ 00 
(8-4) pn = 
Consider now the doubly infinite sequence of functions w(n) defined by 
(8.5) uM(x) = e— . 
y{x) 
Substituting into (8.2) it is seen that 
(8.6) ||w(n)|| = 1, (uln),ulm)) = 0 
for m 7& n. For the random variables Zn corresponding to the functions 
u(n) this implies that they are uncorrelated and of unit variance. In particular, 
if the Xk are normal the Zk are mutually independent. 
It is interesting that the space spanned by the variables Xfc contains a 
stationary sequence {Zn} of uncorrelated variables. An explicit expression 
of Zn in terms of the Xk can be obtained from the Fourier expansion of the 
function u(n), but it is more profitable to proceed in the opposite direction: 
the structure of {ZJ being simpler than that of {XJ it is preferable to 
express the Xk in terms of the Zn. Now 
AT -tfciB AT 
(8.7) 2 Yn u(n+k\x) = ^— 
2 
=~N 
10 The restriction is not used except to avoid trite explanations of what is meant by 
r(x)jy(x) when r(x) = y(x) = 0 and of how series converge. 
XIX.8 STOCHASTIC PROCESSES AND INTEGRALS 645 
The sum on the right is a section of the Fourier series for y and tends in the 
Hilbert space metric to y. It follows that the quantity (8.7) tends to eikx. 
In our mapping u{n+k) corresponds to Zn+k and so the series ? yn 
converges, and we can write 
+ 00 
(8-8) x, = 2 ynzn+k. 
n=— oo 
We have thus obtained an explicit representation of Xk as a "moving average" 
in a stationary sequence of uncorrelated variables Zk. 
The representation (8.8) is evidently not unique. The natural question 
arises whether it is possible to express Xk solely by the variables Zk, 
Zfc-i, Zfc_2,. . . (representing the "past"), that is, whether the function y 
in (8.3) can be chosen such that yn = 0 for n > 1. This problem is 
fundamental in prediction theory, but lies outside the scope of the present 
volume. For typical examples see 111,G.5), and problem 18. 
(b) The associated process with uncorrelated increments. For each / with 
—77- < / < 77- define yt by 
1 for x < t 
(8-9) yt{x) = 
0 for x> t 
and denote by Yt the corresponding random variable in §. The increments 
Yt — Ys for non-overlapping intervals have obviously covariance 0; 
furthermore Var (Yt) = R{—tt, /}. Thus {YJ is a process with uncorrelated 
increments and variances given by R. If the Xt are normal, the increments 
of the Yt process are actually independent. 
With every stationary sequence {Xk} there is in this way associated a 
process with uncorrelated increments. An explicit expression of Yt in 
terms of the Xk is obtainable in the standard way by expanding the function 
yt in (8.9) into a Fourier series. Once more it is preferable to proceed in the 
opposite direction. This will be done in the next example. 
(c) Stochastic integrals. The representation of a random variable U 
in terms of the X^ depends (as we have seen) on the Fourier expansion of 
the function corresponding to U. By contrast, the following representation in 
terms of the variables Yt is almost too simple for comfort. It refers to the 
graph of the function, and for simplicity we assume the latter continuous. 
Consider first a step function w, that is, a function of the form 
(8.10) w = axytl + a2(yt2-ytl) + • • • + an(yr-ytn_l) 
where the ai are constants and — it < tx < t2 < • • • < tn_x < n. The 
associated random variable W is obtained on replacing in this expression 
each yt. by Yt.. Now an arbitrary continuous function w can be approxi- 
mated uniformly by tep functions w{n) of the form (8.10). Uniform con- 
vergence of W{n) to w implies the convergence in the norm of L-R and 
646 HARMONIC ANALYSIS XIX.8 
hence also the convergence of the corresponding random variables W(n) to 
W. This gives us a prescription for finding the image W of an arbitrary 
continuous function w by a simple limiting procedure: approximate w 
by step functions of the form (8.10) and replace yt by Yt.. Remember 
that (8.10) is a function, and not a number, just as the limit w of w(n) is a 
function rather than a number. But (8.10) looks like a Riemann sum and 
our procedure is formally reminiscent of the definition of the Riemann 
integral. It has therefore become standard practice to use the notation 
(8.11) W 
= (\ 
J—IT 
to indicate the described limiting process. The random variable (8.11) is 
called the stochastic integral of the continuous function w. The name is 
arbitrary and the notation mere shorthand for the limiting procedure which 
we have rigorously defined. By definition the function eint corresponds to 
the random variable Xn and hence we can write 
(8.12) 
J—i 
This is the basic spectral representation of the arbitrary stationary sequence 
{Xn} in terms of the associated process with uncorrelated increments. 
The notation for stochastic integrals is, perhaps, more suggestive than 
logical but we are not concerned with this usage. Our aim was to show that 
this useful concept and the important representation (8.12) are easily 
established by means of Fourier analysis. This illustrates the power of the 
canonical mapping used in this section and first introduced by Cramer. ^ 
The theory depends only on the second moments, of {Xn} and is in practice 
.applicable only when these moments are truly significant. Such is the case 
when the process is normal, because normal distributions are completely 
determined by their covariances. In other applications one may trust that the 
process is "not too far off a normal process" just as the oldest regression 
analysis trusted in a universal applicability of methods developed for normal 
variables. Unfortunately the mere existence of a beautiful theory in no way 
justifies this trust. In example C.c) the sample functions of the process are 
strictly periodic. The future of an individual path is completely determined 
by the data for a full period, but the prediction theory based on the jL2 
theory takes no account of this fact and identifies all processes with the same 
spectral measure. One who observes the-sequence 1,-1,1,-1,... going 
on since time immemorial can safely predict the next observation, but jL2 
methods will lead him to predict the miraculous occurrence of 0. This 
example shows that the L2 methods are not universally applicable, but they 
are the ideal tool for treating normal processes. 
XIX.9 PROBLEMS FOR SOLUTION 647 
9. PROBLEMS FOR SOLUTION 
1. Curious characteristic junctions. Let -rk(x) = 1 — \x\jh for \x\ <, h and 
rh(x) = 0 for |x| ^ h. Put 
(9.1) a(x) = J «nrk(x-n). 
n=-oo 
When the an are real and h = \ the graph of a is the polygonal line with vertices 
(n, an). When h < \ the graph of a consists of segments of the z-axis and the 
sloping sides of isosceles triangles with vertices (n, an). 
Using the criterion of lemma 1 in section 2 show that a(?)/a@) is a characteristic 
function if the an are real and such that a_n = an and \ax\ + \a2\ + ¦ • • <. \aQ. 
2. (Generalization.) The last statement remains true when rk is replaced by an 
arbitrary even integrable characteristic function. (Using the characteristic functions 
exhibited in fig. 1 of XV,2 you may construct characteristic functions with ex- 
ceedingly weird polygonal graphs.) 
3. (Continuation.) The preceding result is a special case of the following: Let 
t be an even integrable characteristic function; if the An are real, and the an 
complex constants with ^ \an\ < oo, then 
(9.2) a@ ^akra-h) 
is a characteristic function iff a@) = 1 and ]T ake~i}-& > 0 for all ?. (It suffices 
actually that the last series be Abel summable to a positive function.) 
4. The covariance function p defined in C.3) is continuous everywhere iff it 
is continuous at the origin. This is the case iff E(Xt —X,,J -*¦ 0 as / -»> 0. 
5. (Difference ratios and derivatives). Let {Xj be a stationary process with 
p(t) = E(Xt+sXs) and spectral measure R. For h > 0 a new process is defined 
by XJ*> - (Xt+h - Xt)/h. 
(a) Show that the spectral measure Rlh) of the new process is given by 
Rih){dx} = 2/r2[l -cos hx]R{dx}. The covariances p{h)(t) tend to a limit as h ->0 
iff a continuous second derivative p'(t) exists, that is, if the measure x2 R{dx} 
is finite. 
(b) In the latter case E(|X<C) - X<*>|2) ->0 as e — 0 and 6 ->0. 
Note: In the Hilbert space terminology of section 7 this means that for fixed 
/ as cn -+0 the sequence {X<€n)} is Cauchy, and hence a derivative X't = l.i.m.X'^ 
exists. 
6. (See theorem 2 of section 4.) If q> is continuous except for a jump at the origin, 
then frCO -*¦ ^@ uniformly outside a neighborhood of the origin and 
i) 
7. Continuation. If <p is the difference between two monotone functions then 
/r@-|[^+) ~ ?(?-)] at all points. 
8. A bounded periodic function <p with non-negative Fourier coefficients <pn 
is necessarily continuous and ^?;n < oo. The example <pn = I In shows that 
this is false if <p is only supposed to be integrable. Hint: Use the main idea of the 
proof of theorem 1 in section 4. 
648 HARMONIC ANALYSIS XIX.9 
9. Cesaro summability. Replace D.3) by 
MO - 2 «wif* 
where an - 1 - |n|BW + I)-1 for |rz| ^ IN and an = 0 for |n| > 2N. Show that 
the theory of section 4 goes through with pT(t) replaced by 
n a\ 1 sin2 (N+pt 
2N + 1 sin2 |f 
which is again a probability density. 
10. Continuation. Show, more generally, that the theory goes through if the 
an are the Fourier coefficients of a symmetrized probability density on the circle. 
11. Use the Poisson summation formula E.2) to show that 
where n stands for the standard normal density. [This is the solution of the 
reflecting barrier problem in example X,5(e).] 
12. In the Poisson summation formula E.2) the condition that ]T <?(? + 2kX) 
converges to a continuous function y> may be replaced by the condition that 
^f(nnjX) < 00. Hint: y> is under any circumstances an integrable function. 
Use corollary 2 in section 4. 
13. Alternative derivation of the Poisson summation formula. Let <p be the 
characteristic function of an arbitrary probability distribution F. If pT stands 
for the Poisson kernel of D.5) and D.7) show (without further calculations) that 
for 0 <> r < 1 
1 +oo p+00 
<93) Jn I <ptt+n)r\n\ein* = e**pT(x+X) F{dx). 
n=—oo J— 00 
Hence the left side is a characteristic function. Letting r -*¦ 1 conclude that if F 
has a density / then 
(9.4) ^L ^ 
n=—oo k=—oo 
whenever 2/(~^+^7r) < °°- Show that (9.4) is equivalent to the general 
version E.9) of the summation formula. 
Note: The result may be restated to the effect that the left side in (9.4) is Abel 
summable to the right side whenever the latter is continuous. 
14. The sequence {<pn} is positive definite iff {9>nnnl} is positive definite for 
every 0 < r < 1. Necessary and sufficient is that ]? q>nr\n\einX > 0 for all A 
and 0 < r < 1. 
15. From problems 1 and 14 derive (without calculations) the following theorem 
(observed by L. Shepp): Let {<pn} be positive definite and denote by a the piecewise 
linear function with vertices at (n, <pn). Then a is positive definite. 
16. Let <p be a characteristic function and denote by a the piecewise linear 
function with vertices (n} <p(ri)). Then a is a characteristic function. (This merely 
paraphrases problem 15 and is a special case of problem 1 for h = 1.) Use the 
XIX.9 PROBLEMS FOR SOLUTION 649 
other cases of problem 1 to describe other curious characteristic functions obtainable 
from (p. 
17. If r < 1 the covariances Pn = rMein9 of Markov sequences satisfy G.20) 
with uk = Vl - r2rV*8 for k ^ 0 and uk ¦-= 0 for k < 0. Find alternative 
representations. 
18. Continuation. Let {Xn} be Markovian with covariances pn = H"leinfl. 
If T < 1 one has X^ = Vl - r2 2?_0 rk^k9Xn_k where the Zfc are uncorrelated. 
If r = 1 one has Xn - ^n9X^. 
Answers to Problems 
CHAPTER I 
(ii) ^g-*«-*>» for 
(iii) j e-'W, all x (iv) cwr**, x >0 
(v) «A +;_^e—* 
1 e-a»-ax* 
2. (i) ^-57= f°r 1*1- < 1 (ii) i for 1 < t < 5 
(iii) if1 "t) for |x!<2 (ivI~l for °<x<2 
(v) I -Jar* + J^ for |x| < 1 (vi) ± + $x* + -L^"* for |x| < 1. 
3. (i) h-^l-e-^) for 0 <x < h, and h-^e^-De-** for x > A. 
(ii) A-^l -e-a<*+*») for -A < x < 0, and A-Hl-e-f*)^"* for x > 0. 
4. (i) A/3 if h <, 1 and 1/C V?) if A ^ 1. 
(ii) V^t el*(l -K(Vo/2)). 
5. (i) 1 -x-1 for x > 1; (ii) x2(x + l). 
7. P{Z ^ x} = 1 - <?-«* for x < t and =1 for x > t. 
10. F) The platoon together with cars directly ahead or trailing form a sample in 
which the smallest element is at the last place, the next smallest at the second. 
m-i /n+k — l\ 
16. p = J I 7 J-*-*. For m = 1, * = 2 one gets /> = ?. 
18. /rf"-1 - (n-l)/n. 
651 
652 ANSWERS TO PROBLEMS 
Jo Jx 
(ii) The density is It - t2 for 0 < / <, 1 and B-/J for 1 < / < 2 
(iii) The density is 2/2 for 0 < / < 1 and again B-tf for 1 < / < 2. 
20. 2 I x(l — x) dx = \. Two out of six permutations produce an intersection. 
Jo 
21. Xn: 4 log— for x < {; X12 and X21: 4 log 2 for x<{, 4 log— for 
\<x<\\ X22: 4Iog4x for \ < x < \, 4 log- for \ < x < 1. The 
expectations are A» tg » tV X 
2 2 1 
27. Distributions - arc sin ?z and \x*; densities =^ and i» for 
0 < x < 2. n n V4 -x* 
28. 2rTx arc sin ^x. 
1 2 1 + vl — x2 
30. (a) log- , (b) - log where 0 < x < 1. 
X IT X 
31. - f sin2fl*/0=i r Vl -y2 <fy = 2w-1[arc sin 
Jo<cos 0<z "" Jo 
where 0 < x < 1. 
35. The substitution s = F(y) reduces the integral to the corresponding integral 
for the uniform distribution. Note that F(m + x) «* ? + f(m)x for small x. 
CHAPTER II 
4- g*g{*) =?e-H(l+|z|) 
10. (a) A/i(e~*<— e~"t)(fi — X) as a convolution of two exponentials, (b) Using 
(a) one gets Ae~A'. 
12. For a person arriving at random the density is 1 — \t2 for 0 < / < 1 and 
|B-02 for all 1 < / < 2. The expectation equals -^. 
--1\ /m +/x + n -j - k - 1\ I (m +n + n + y*-l\ 
/y+A:+y-l\ i 
' \ J )\ m~J )l\ m 
• CHAPTER HI 
7. (a) "^"'"'arid 1 - e~x - e~* + e-*-*-*** for x > 0, y > 0. 
1 2a a2 
Var (Y | X) = ^^ji + (i +axf ~ A +axL ' 
ANSWERS TO PROBLEMS 
8. If / has 
Var (X) 
expectation ju and variance a2 then E(X) = E(Y) = ?ju, 
() = Var (Y) = \o* + -^\ Cov (X, Y) = |a* - -&»\ 
9. Density 2x2 in unit square. In n variables (n — l)!X2Xf' • -Xj}if. 
10. %e-{x+v) for y > x > 0 .and \e~v+2x for y > x, x < 0. Interchange 
y when y <x. 
y , g x and 
0<x<i. 
where 0<x<?<y<l and the domain of integration satisfies the con- 
ditions that 2x < s < \ and also 1— 2y<j<l— y. 
12. Bivariate nonnal with variances m, n and covariance ^m\tu Conditional 
... . tn . . n — m ....... 
density has expectation — t and variance m • as is clear intuitively. 
13. Xf + • • • + X* has the gamma density /1/2,»/2 [see 11,B.2)]. From C.1) 
therefore 
For m = 2,« = 4 we get example 3(a). 
15. (a) 4aw/ when * + y < 1, x > 0, y > 0 
qxy — 4(x+y—IJ when a? + y > 1, 0<x, y<l 
4a;B-x-y) when y > 1, x + y < 2, x > 0 
4yB-x-y) when x > 1, x + y < 2, y > 0. 
2A -x-yJ for 0 < x, y < 1, x + $ < 1 
2'A -xJ for x > 0, y < 0, x + y > 0. 
2A +yJ for x > 0, y < 0, x + y < 0. 
For x < 0 by symmetry. 
17. f "/(/>)/> </p I V(Vr2 + P2-2rPcos 6) dd. 
Jo Jo 
20. (a) Xn = U cos \tm + V sin \nn 
(b) U + V(-l)n 
(c) U cos ?7771 + V sin \nn + W. 
21. (a) Var (Y,^) - Var (Y«) = Var (C«) - 2 Cov (Yn, C«) + 1 whence 
F) a2 - 2oLoP + 1=0 
- a)<\ - q»)/p wher. 
654 ANSWERS TO PROBLEMS 
CHAPTER VI 
11. Not necessarily. It is necessary that n\\ —F(cri)] -*0. 
12; For x > 0 the densities are given by 1 and ?A —e*). 
13.-qU(Xj = 1 —qe-***. The cumber of renewal epochs is always geometrically 
distributed. 
19. Z = z + F-kZ where z(t) = 1 - e~ct for t <, f, z(t) = z(f) for / ^ ? and 
) * for / > ?. 
20. K = ^ + B * K where A{dx] = [1 -G(x)]F{iic} and B{dx} = 
23. The arc sine density g(y) = - 
-y) 
CHAPTER VH 
6. {a) J" 1 f*A -p)"~k with F concentrated at p. 
F) -i-: , density /(*) = 1. 
Some Books on Cognate Subjects 
A. INTRODUCTORY TEXTS 
Krickeberg, K. [1965], Probability Theory. (Translated from the German 1963) 
Addison-Wesley, Reading, Mass. 230 pp. 
Loeve, M. [1963], Probability Theory. 3rd ed. Van .Nostrand, Princeton, N.J. 
685 pp. 
Neveu, J. [1965], Mathematical Foundations of the Calculus of Probability. (Trans- 
lated from the French 1964.) Holden Day, San Francisco, Calif. 233 pp. 
B. SPECIFIC SUBJECTS 
Bochner, S. [1955], Harmonic Analysis and the Theory of Probability. Univ. of 
California Press. 176 pp. 
Grenander, U. [1963], Probabilities on Algebraic Structures. John Wiley, New 
York. 218 pp. 
Lukacs, E. [1960], Characteristic Functions. Griffin, London. 216 pp. 
Lukacs, E. and R. G. Laha [1964], Applications of Characteristic Functions. Griffin, 
London. 202 pp. 
C. STOCHASTIC PROCESSES WITH EMPHASIS ON THEORY 
Chung, K. L. [1967], Markov Chains with Stationary Transition Probabilities. 
2nd ed. Springer, Berlin. 301 pp. 
Dynkin,E. B. [1965], Markov Processes. Two vols. (Translation from the Russian 
1963) Springer, Berlin. 174 pp. 365 + 271 pp. 
Ito, K. and H. P. McKean Jr. [1965]. Diffusion Proceeds and Their Sample Paths. 
Springer, Berlin. 321 pp. . 
Kemperman, J. H. B. [1961], The Passage Problem for a Station ry Markov Chain. 
University of Chicago Press. 127 pp. 
Levy, Paul [1965], Processus Stochastiques et Mouvement Brownien. 2nd ed. 
Gauthier-Villars,- Paris. 438 pp. 
Spitzer, Frank [1964], Principles of Random Walk. Van Nostrand, Princeton. 
406 pp. 
Skorokhod, A. V. [1965], Studies in the Theory of Random Processes. (Translation 
from the Russian 1961.) Addison-Wesley, Reading, Mass. 199 pp. 
655 
656 SOME BOOKS ON COGNATE SUBJECTS 
Yaglom, A. M. [1962], Stationary Random Functions. (Translation from the 
Russian.) Prentice-Hall, Englewood Cliffs, N.J. 235 pp. 
D. STOCHASTIC PROCESSES WITH EMPHASIS ON 
APPLICATIONS OR EXAMPLES 
Barucha-Reid, A. T. [1960], Elements of the Theory of Stochastic Processes and Their 
Applications. McGraw-Hill, New York. 468 pp. 
BeneS, V. E. [1963], General Stochastic Processes in the Theory of Queues. Addison- 
Wesley, Reading, Mass. 88 pp. 
Grenander, U. and M. Rosenblatt [1957], Statistical Analysis of Stationary Time 
Series. John Wiley, New York. 300 pp. 
Kliinichine, A. Y. [1960], Mathematical Methods in the Theory ofQueueing. (Trans- 
lation from the Russian.) Griffin, London. 120 pp. 
Prabhu, N. U. [1965], Stochastic Processes. Macmillan, New York. 233 pp. 
[1965], Queues and Inventories. John Wiley, New York. 275 pp. 
Riordan, J'. [1962], Stochastic Service Systems. John Wiley, New York. 139 pp. 
Wax, N. (editor) [1954], Selected Papers on Noise and Stochastic Processes. Dover, 
New York. 337 pp. 
E. BOOKS OF HISTORICAL INTEREST 
Cramer, H. [1962], Random Variables and Probability Distributions. 2nd ed. (The 
first appeared in 1937.) Cambridge Tracts. 119 pp. 
Doob, J. L. [1953], Stochastic Processes. John Wiley, New York. 654 pp. 
Gnedenko, B. V. and A. N. Kolmogorov [1954], Limit Distributions for Sums of 
Independent Random Variables. (Translated from the Russian 1949) Addison- 
Wesley, Reading, Mass. 264 pp. 
Kolmogorov, A. N. [1950], Foundations of the Theory of Probability. Chelsea Press, 
New York. 70 pp. (The German original appeared in 1933.) 
Levy, P. [1925], Calcul des Probabilites. Gauthier-Villars, Paris. 350 pp. 
Levy, P. [1937 and 1954], Theorie de VAddition des Variables Aleatoires. Gauthier- 
Villars, Paris. 384 pp. 
F. SEMI-GROUPS AND GENERAL ANALYSIS 
Hille, E. and R. S. Phillips [1957], Functional Analysis and Semi-groups. (Revised 
edition.) Amer. Math. Soc. 808 pp. 
Karlin, S. and W. Studden [1966], Tchebycheff Systems: With Applications in 
Analysis and Statistics. Interscience, New York. 586 pp. 
Yosida, K. [1965], Functional Analysis. Springer, Berlin. 458 pp. 
Index 
ABELIAN theorems, 418, 445 
ABEL's integral equation, 33 
ABEL summability, 627, 628, 648 
Absolute continuity, 139, 140 
Absolutely fair, 210 
Absolutely monotone functions, 223-224, 
439, 441 
Absolute probabilities, 207-209 
Absorbing barriers, 340-342, 464, 477-479 
Absorption (physical), 25, 31, 323 
Accidents, 57, 181 
Additive set functions, 107 
countably = sigma, 108, 119, 129-130 
Age, see Duration 
Algebra of sets, 112-113, 116 
generation of, 163 
AMBARZUMIAN, V. A., 206, 325 
Anomalous numbers, 63 
Arc sine distributions, 50 
and limit theorems, 470-473 
in random walks, 417-423 
Arithmetic distributions, 138, 407-408;see 
also lattice distributions 
Arrays, null, 177-178, 585 
triangular, 177-178, 216, 308-312, 583- 
588, 596 
ARZELA-ASCOLI theorem, 270 
Ascending ladder variables, strict, 391 
weak, 392-393 
Associated random walks, 406 
Astronomy, applications to, 33, 172, 173- 
174,206,215,325-326 
Asymptotic behavior, 521* 572 
Asymptotic estimates, 375-377, 410-411 
Asymptotic properties, of regularly varying 
functions, 279-284 
Asymptotically dense, 147 
Asymptotically negligible, 178 
Asymptotically unbiased estimators, 220 
Atomic measures, 137-138 
Atoms (of measures), 137 
under convolutions, 147, 149,-166 
Attraction, see Domain of attraction 
Auto-regressive process, 89, 96 
BACHELIER process, see BROWNIAN 
motion 
Backward equations, 357 
for diffusion, 334, 336-337, 344 
for jump processes, 327-328; 484-487 
of KOLMOGOROV, 327-328 
minimal solution of, 329-331, 322, .486 
for semi-groups, 352, 356-357 
for semi-MARKOV processes, 497 
Bad luck, 15-17 
BAIRE functions, 104-106, 109, 114, 130, 
305,351 
BANACH space, 257, 350, 487, see also 
HILBERT space 
Barriers, see Absorbing barriers; Boundary 
conditions; and Reflecting barriers 
BARUCHA-REID, A.T., 656 
BAXTER, G., 404, 424, 571, 605 
BAYES, T., 56 
BENES, V.E., 379, 656 
BENFORD, F., 63 
BERGSTROM, H., 581 
BERNOUILLI trials, 1, 141-142 
BERNSTEIN, S., 79, 439 
BERNSTEIN polynomials, 222-223 
in R2, 245 
BERRY, A. C., 531, 536, 542 
BERRY-ESSEEN theorems, 538, 542-546, 
551 
BESSEL functions, 58-61, 523 
characteristic function of, 503 
and infinite divisibility, 177, 451, 566 
and LAPLACE transforms, 437, 438, 479- 
482 
and related distributions, 58-61, 149,166 
657 
658 
INDEX 
in stochastic processes, 58-61, 323 
BESSEL density, 502 
Beta density, 50 
in renewal, 471 
Beta integral, 47 
BICKEL, P. J., 388 
Bilateral exponential, 49-50, 502 
characteristic function of, 503 
Bilateral LAPLACE transform, 434 
BILLINGSLEY, P., 39, 265, 343 
Binary orbits, 33 
Binomial random walks, 608-609 
Birth processes, 41, 266-267, 488-491 
bilaterial, 491 
Birth-and^death processes, 479-483, 496 
busy periods in, 482-483 
and diffusion, 496 
see also Randomized random walks 
Bivariate normal density, 70 
BIZLEY, M. T. L., 420 
BLACKWELL, D., 360, 381 
BLUM, J. R., 287 
BOCHNER, S., 321, 347, 454, 620, 622, 
634, 655 
BOCHNER integral, 455 
BOHL, 268 
BOREL algebra, 113, 119 
measurable functions, 116 
BOREL set, 114,116, 117,123, 125,130 
approximation of, 115, 124 
convention for, 127 
BOREL-CANTELLI, 105, 317 
BOTTS.T.A., 103 
BOUDREAU, P. E., 196 
Boundary conditions, 337-343, 477 
BOURBAKI, N., 103 
Branching processes, 244, 441, 474-475 
BRELOT, M., 244 
BROWNIAN motion, 99,181, 322-349, 
475-479 
continuity of paths in, 181, 332 
with elastic force, 335 
first-passage times in, 174-175, 340, 476 
one absorbing barrier, 340, 477 
in Rrt 175,344 
subordination of CAUCHY process to, 34fl 
two absorbing barriers, 341, 478 
BUFFON's needle problem, 61-62 
BUHLMANN,H.,229 
Busei, 22, 55,188 
Busy period*, 194-195,198-20Q, 473-474, 
482-483 
CAMPBELL'S theorem, 179, 287, 595 
Canonical measures, 560-565 
CANTELLI, see BOREL-CANTELLI 
CANTOR, G., 267 
CANTOR distribution, 35-36,141, 593 
convolutions of, 146 
CANTOR'S diagonal method, 267-268 
Cap and cup, 104 
CARLEMAN, T., 227, 515 
CAUCHY, A., 172, 509 
CAUCHY distribution, 50, 64,173, 502 
bivariate, 524 
in BROWNIAN motion, 175, 348 
inRr, 70-71, 73, 100, 524, 594* 
and random walks, 204, 618 
and stability, 173, 555 
CAUCHY semi-groups, 303 
Centering, 45, 137, 584-588 
in infinitely divisible distributions, 559 
Central limit theorem. 258-265,287, 291. 
529-530 
applications of, 209, 529-530 
for densities, 533-536 
for equal components, 515-518 
expansions related to, 531-553 
with infinite variances, 260 
and large deviations, 548-553 
in renewal, 372 
CESARO summability, 628, 648 
Chains, random, 206-207 
strength of, 9 
CHANDRASEKHAR, S., 32 
CHAPMAN-KOLMOGOROV equations, 60 
334,338,346-347,353,566 
continuous time, 322, 486-488 
discrete time, 98 
and semi-groups, 351 
CHAPMAN-KOLMOGOROV identity, 98 
Characteristic exponents of R, 170 
Characteristic functions, 498-526 
derivatives of, 565-566 
factorization of, 506, 593, 631 
finitely divisible, 557 
infinitely divisible, 554-557, 560-564 
logarithms and roots of, 554-555 
periodic; 511, 626, 630, 631 
inKr, 521-524 
TAYLOR development of, 514-515 
see also POISSON summation 
INDEX 
659 
CHEBYSHEV-HERMITE polynomials, 533 
CHEBYSHEV's inequality, 151-152, 310, 
354 
generalized, 234 
for martingales, 246 
Checkerboard partitions, 133 
CHERNOFF, H., 287 
Chi-square density, 48 
CHOQUET, G., 382, 593 
CHOW, Y. S., 360 
CHUNG, K. L., 105,167, 231, 355, 381, 
483,614,615,655 
Circles, covering theorem for, 28-29 
densities on, 632-633 
distributions on, 29, 61-64, 627 
equidistribution on, 268, 273 
probability distribution on, 274 
Coin-tossing, 210, 211-212, 405, 417 
and random choice, 35 
Coincidences, 217 
Collisions of particles, 206, 322-323, 325 
Compactness of triangular arrays, 309 
Complete monotonicity, 224-227, 439-442, 
450,464 
abstract, 454 
Completion of measures, 126 
Composition of kernels, 206 
Compound POISSON processes, 180-181, 
305,326 
and ruin problems, 182-184, 198, 469-470 
and semi-groups, 295, 299-300 
_^nd subordination, 348-349 
Concentration, 4, 45,137 
Concave functions, 153 
Concordant functions, 210-211, 244 
Conditional distributions and expectations, 
71-74,156-159,160-165 
Conditional probability, 157 
Contagion, 57-58 
Continuity of semi-groups, 353; see also 
Fixed discontinuities 
Continuity theorem, 431-433, 508 
characteristic function of, 508-509, 510 
for densities, 510 
and LAPLACE transforms, 433 
and quasi-characteristic functions of, 557 
and semi-groups, 460 
Contractions, 350, 456 
Convergence, of densities, 252 
dominated, 111 
in the mean square, 636 
of measures, 247-252, 267-270, 284-285 
notations and principles of, 248-251 
of moments, 251-252, 269 
in norm = strong, 257, 352 
of operators, 257, 285, 352 
in probability, 253-254 
Convex functions, 153-155 
of martingales, 214-215 
Convex polygons, 505 
Convolutions, 143-148, 272, 278 
on circles, 64,143, 273-274 
and covering theorems, 26-29 
definition of, 7, 8 
of densities, 7-8, 46, 71 
infinite, 265-267, 317, 592-593 
and LAPLACE transforms, 434-435 
of singular distributions, 146 
Convolution semi-groups, 293-296 
Coordinate variables, 4, 68 
Correlation, 68 
Countably many intervals, 108 
Counters for particles, 372; see also 
GEIGER counters; Queues 
Covariance, 68 
matrix, 82-83 
of processes, 88-94, 623-626, 643-646 
Covering theorems, 76, 216, 469 
and convolutions, 26-29 
CRAMER, H., 182, 403, 522. 531, 542, 
546, 548, 552', 646,.656 
CRAMER'S estimate for ruin^ 182, 377-378, 
403,411-412 
CRAMER-LEVY theorem, 525 
Cup, 104 
Dams, 195 
DARLING, D. Ai, 465 
Decision functions, 213 
Decompositions, 570-571 
Defective distributionl, 115,129, 130. 205 
in renewal, 187 
de FINETTI, B., 179, 230 
Degenerate distributions, 83, 87 
Degenerate processes, 90-91 
Delayed renewal processes, 187, 368 
Delays in traffic, 190, 380, 387, 474-475, 
496 
Densities, 3-6, 49-53, 66-71,138-143 
notations and conventions for, 45-46 
Denumberable sample spac , 331-332 
DENY.J., 382 
660 
INDEX 
Derivatives and LAPLACE transforms, 435 
DERMAN, C, 493 
Descending ladder variables, 393-394 
Differences, notation for, 221-222 
Differential equations, KOLMOGOROV, 
483-488 
Diffusion processes, and birth-and-death 
processes, 496 
with elastic force, 335-336 
in genetics, 336-337 
in higher dimensions, 344-345 
in Rr, 332-337, 344-345, 436, 461, 464, 
475-479, 496 
Digits, distribution of, 34, 63-64 
Directing process, 347 
Directional changes of particles, 323 
Directions, random, 29-33 
Directly RIEMANN integrable functions, 
362-363 
DIRICHLET integral, 511 
Discontinuities,-318 
Discontinuous semi-groups, 305 
Discrepancies, see Empirical distributions 
Discrete distributions, 55-58 
Distance function for distributions, 285 
Distribution functions, definition of, 3 
DOBLIN.W., 173, 592 
DOBLIN's "Universal laws," 590-592 
Domain of attraction, 172 
criteria for, 312-316, 320, 448, 574-581 
normal, 581 
partial, 320, 568, 590-592 
and stable distributions, 574-581 
Dominated convergence, 111 
DONSKER, M. F., 39,343 
DOOB, J. LY, 103,164, 210, 244, 656 
Drift, in diffusion, 335 
in random walks, 397, 610-611 
Duality, 394-398, 609-610 
Duplication formula, 64 
Duration, of birth processes, 490 
of busy period, 473^474, 482-483 
of dead period, 190 
of diffusion, 341-342 
of renewal process, 187, 216, 374-377 
estimates for, 377 
DVORETZKY, A., 274 
DYNKIN, E. B., 321, 333, 472, 655 
Economics, stable distributions in, 175 
EDGEWORTH expansion, 535, 542 
EINSTEIN, Albert Jr., 182, 333 
Elastic forces, 335-336 
Electric transmission lines, 208-209 
Empirical distributions, 36-39 
Empirical interpretations, 22 
Endomorphisms, 350 
Energy losses, 25, 323;see also Collisions 
Ensembles, of circles and spheres, 9-10 
random, of points in space, 9 
Entrance probability, see Hitting points 
Equicontinuity, 252, 270 
Equidistribution theorem, 268 
Equivalent functions, 125, 636, 642 
ERD5S, P., 343, 360 
Ergodic limits, 491-493 
Ergodic stochastic kernels, 271 
Ergodic theorems, 270-274, 491 
ESSCHER.F., 552 
ESSEEN, G., 540, 542, 544, 545; see also 
BERRY-ESSEEN 
Estimator, 41 
Exchangeable variables, 228-230, 423 
central limit theorem for, 287 
Expansions, and the.central limit theorem 
531-553 
for. distributions, 538-542 
involving varying components, 546-548 
Expectations, 117-118, 133 
conditional, 162-165 
definition of, 5 
Explicit expressions, 193 
Exponential distributions, 1, 8-21, 39-43, 
74-77 
bilateral, 49, 148 
bivariate, 100 
as limits, 24,43, 370 
reduction to, 25-26 
and uniform distributions, 44, 74-77 
see also Gamma distributions 
Exponential formula of semi-groups, 231, 
353-355 
Extension theorem, 118-121, 123 
Fair, absolutely, 210 
FATOU's theorem, 110-111, 636 
for boundary values, 244 
FEJER.L.,628 
FELLER, W., 53, 61, 179, 194, 262, 279, 
289,325, 331,333, 337, 360, 381, 
430,483, 496,497, 546, 552, 581 
Filter*, 88, 625 
INDEX 
661 
FINETTI, B. de., 42,179, 230 
Finite arithmetic distributions, 608-609 
First entry, see Hitting points; ladder 
variables 
First passages, birth-dnd-death processes, 
I 60-61,481,494-495 
'in diffusion, 174-175, 340-341, 475-476 
and MARKOV chains, 492 
First return, 391, 424, 495 
FISHER, R. A., 77, 277, 336 
FISHER's Z-statistic, 49 
Fishing, 182 
Fixed discontinuities, 324-326, 328 
FOKKER-PLANCK equation, 296, 323, 
324, 325, 328 
Forward equations, 337-343, 484 
and diffusion, 337-343,475-479 
and jump processes, 324,328 
of KOLMOGOROV, 324, 328 
minimal solution for, 329,331-332, 
485-488 
and semi-groups, 352 
FOURIER analysis, applications of to random 
walks, 598-616 
FOURIER coefficients, 628, 634, 647-648 
FOURIER inversions, 509-510, 511, 639 
FOURIER series, 626-629, 641 
FOURIER transforms, 499, 532 
FOURIER-STIELTJES transform, 499, 606 
Fractional parts, 148, 268 
FRECHET's maximal distribution, 166 
Free paths, 10-11 
Frequency functions, definition of, 8 
FUBINI's theorem, 111, 122, 144 
FUCHS.W. HJ.,614, 615 
Functional, linear, 120 
GALTON, F., 73 
Gamma distributions, 11, 47-48, 176, 435, 
502, 503 
alternate name for, 48 
approximation by, 220 
infinitely divisible, 176, 451, 567 
limit of order stt^istics, 24 
randomized, 58-59 
subordination, 336-337 
Gamma functions, 47 
Gamma process, direction of by POISSON 
process, 349 
direction ofTOISSON process by, 348- 
349 
Gaps, large, 188, 378, 468 
small, 217 
GEARY, R. C, 86 
GEIGER counters, 189, 190, 372-373, 387, 
468 
Generating functions, characterization of, 
223 
mortality interpretation for, 424 
Generation, of algebras, 163 
of exponentially distributed variables, 44 
Generators, 294-296, 356-357, 456-457, 
476 
Genetics, diffusion in, 336-337 
GILBERT, E.N., 217 
GNEDENKO, B. V., 38, 39, 277, 531, 540, 
581,592,656 
GNEDENKO-KOROLJUK theorem, 43 
GOOD, I. J., 473 
Gravitational fields, 173-174, 215 
Green function, 334,476, 496 
GREENWOOD, M., 57 
GRENANDER, U., 77, 655, 656 
GRIFFIN, J. S.Jr., 196 
Grouping of data, 4-5 
Growth, logistic, 52 
GUMBEL.E.J., 165 
HADAMARD's factorization theorem, 525 
HALMOS.P. R., 103, 213 
HAMEL equation, 298, 305 
HARDY, G. H., 155, 268, 445 
Harmonic analysis, 619-646 
Harmonic functions, 244 
HARRIS, T. E., 244 
HAUSDORFF, F., 226 
Height distribution, 73 
HEITLER.W.,323 
HELLY, E., 267 
HENNEQUIN, E., 103 
HERGLOTZ, G., 634 
HERMITE polynomials, 523-533, 535 
expansion of, 542 
HEWITT, E., 124,229 
HEYDE, C C, 227 
Hidden periodicities, 76 
HILBERT spaces, 271, 637, 639-643, 645 
HILLE, E., 231, 294, 408, 454, 525, 656 
HILLE-YOSIDA theorem, 458-463, 476 
Hitting points, in random walks, 426, 598- 
599 
in renewal, 188r 371-372,426 
662 
INDEX 
see also Ladder variables 
HOBBY, C, 389 
HOLDER'S inequality, 155 
HOLTSMARK distribution, 172, 173-174, 
215 
HOPF, E., 403; see also WIENER-HOPF 
HUNT, G., 210 
HUYGENS' principle, 51 
Hyperbolic functions and densities, .502- 
503, 527, 567, 632-633 
IBRAGIMOV, I. A., 167 
Idle servers, 200 
Images, method of, 340 
Imbedded renewal processes, 191-193, 379 
Improper, see Proper distributions 
Increase, point of, 137,147 
Independence of random variables, 6, 69, 71, 
121-125, 134 
and complex variables, 498 
criterion for, 136 
Independent increments, 95-96, 179-182, 
293,304,'317, 645 
and discontinuities, 305, 317-318 
and subordination, 347 
Index of the first maximum, 417 
Indicators, 104 
Induced partitions, 22 
Induced random walks, 390 
Inequalities, 152-156 
of HOLDER, 155 
of JENSEN, 153-154 
of KOLMOGOROV, 156 
moment, 155 
of SCHWARZ, 152-153 
Infinite convolutions, 265-267, 317, 592- 
593 
Infinite differentiability, 256, 293 
Infinitely divisible distributions, 176-179, 
292-293, 449-452, 554-595 
in Rr, 593, 596 
and semi-groups, 290-318, 457-458 
special properties of, 570-574 
Infinitesimal generators, 456 
Infinitesimal speed and variance, 335 
Inner products, 636 
Inspection paradox, 187, 372 
Insurance, see Risk theory 
Integrals, stochastic, 645-646 
Integration by parts, 150-151 
and LAPLACE transforms, 435-436 
Interval functions, 106-112, 128, 129 
Interval of continuity, 248 
Invariance principle, 343 
Inventories, 195-196 
Inversion formulas, 140, 221-222, 638 
characteristic functions for, 510-511, 524 
for LAPLACE transforms, 232-234, 440- 
441,462 
and moment problems, 227 
Ionization, 323 
Isometry, 638 
ITO, K., 333, 655 
JACOBI's theta functions, 342, 345, 632 
JANOSSY, L., 323 
JENSEN'S inequality, 153-154, 214 
Joint distributions, 68, 423-425 
JORDAN decomposition, 138, 142 
JOSEPH, A. W., 420 
Jump processes, 326-332 
with infinitely many jumps, 331, 484 
KAC, M., 79,196, 343 
KARAMATA, J., 173, 247, 275, 279, 445 
KARLIN, S., 228, 381, 656 
KATZ, M. L., 545 
KELVIN; Lord, 340 
KEMPERMAN, J. H. B., 655 
KENDALL, D. G., 194, 231, 473 
Kernels, stochastic, 159, 205, 270-272 
KHINTCHINE A., 137,173,179, 406, 527, 
565,588,592,639-640,656 
KHINTCHINE-POLLACZEK formula, 410, 
470,617 
KHINTCHINE's criterion, 639-640 
KHINTCHINE's law of large numbers, 235, 
436 
KHINTCHINE's unimodality criterion, 158, 
527 
KIEFER, J., 200 
KINGMANJ. F.C., 184 
KOLMOGOROV, A. N., 39,123,124, 179, 
325,333, 531, 540, 656; see also 
CHAPMAN-KOLMOGOROV 
KOLMOGOROV-SMIRNOV theorem, 39, 
342-343 
KOLMOGOROV's backward equation, 327 
328 
KOLMOGOROV's differential equations, 
331,483-488 
KOLMOGOROV's forward equation, 
INDE* 
663 
324,328 
KOLMOGOROV's inequality, 156, 246 
for martingales, 242 
for positive submartingales, 241-242 
KOLMOGOROV's three-series theorem, 317 
KOROLJUK, V. S., 33, 38, 39, 43; see also 
GNEDENKO-KOROLJUK 
KRICKEBERG, K., 103, 655 
KRONECKER delta kerne}, 206, 484 
KRONECKER's lemma, 239, 243 
Ladder epochs, distribution of, 413-417 
Ladder heights, 191, 398-400 
Ladder indices, 412-413 
Ladder points, 390 
Ladder variables, ascending, strict, 391 
weak, 392-393 
descending, 393-394 
LAHA, R. G,, 655 
LAMPERTI.A., 189 
LANDAU, E., 342, 446 
LANDAU, L., 323 
LAPLACE-STIELTJES transform, 432, 
470, 495, 496 
LAPLACE transforms, 232-233, 429-458 
applications of, 466-495 
and convolutions, 434-435 
and derivatives, 435 
elementary properties of, 434-436 
examples of, 436-439 
and integration by parts, 435-436 
inversion formulas for, 232-234 
and moments, 435 
in Rr, 452-454 
and random walks, 614 
for semi-groups, 454-458 
LAPLACE's second law, 50 
Last come first served, 190 
Lattice distributions, 138 
central limit theorem tor, 517-518, 540 
characteristic functions for, 511 
see.also POISSON's summation formula 
Lattices (algebraic), 350 
Law of large numbers, 219-246, 286, 
436,513 
converse, 241 
for identically distributed variables, 234- 
237 
KHINTCHINE'slaw, 235 
for stationary sequences, 245 
strong law, 237-241 
for triangular arrays, 316-317, 596 
weak law, 235-236 
LEBESQUE completion, 126 
LEBESQUE decomposition theorem, 142 
LEBESQUE measure, 33-36, 126 
LEBESQUE-NIKODYM theorem, 140; see 
also RIEMANN-LEBESQUE theorem 
LEBESQUE-STIELTJES integral, 110, 119. 
121,131-132 
LE CAM, L., 286 
LEFFLER, see MITTAG-LEFFLER 
LEGENDRE's duplication formula, 64 
Length of random chains, 206-207 
LEVY, P., 173, 179, 181, 210, 262, 274, 
285, 305, 314, 318, 497, 515, 525, 
565, 567-568, 571, 575, 588, 592, 
655, 656; see also CRAMER-LEVY 
theorem 
LEVY-PARETO distribution, 172;xee also 
PARETO distribution 
LEVY'S canonical measure, 564 
LEVY'S examples, 215, 319, 567-568 
LEVY'S metrk, 285 
Lifetime, see Duration; Recurrence time 
Light, absorption of, 31 
HUYGENS' principle of, 51 
intensity of, 25 
in stellar system's, 206, 325-326 
transmission of through matter, 25, 31, 43 
Likelihood ratios, 211 
Limit theorems, 24, 342-343 
and arc sine distributions, 470-473 
basic, 247-288 
and queues, 380 
LINDEBERG.J.W., 515 
LINDEBERG conditions, 262, 263, 286, 
518-521,530 
in diffusion, 333 
LINDLEY, D. V., 194, 89 
Linear functional, 120 
Linear increments,in jump processes, 324- 
326 
Linear operators on stochastic processes, 
625-626 
LITTLE, J.D. C, 474 
LITTLEWOOD, J. E., 155, 445 
LJAPUNOV's condition, 286 
Locally compact spaces, 120, 123, 248 
Locked period, 189 
LOEVE, MM 103, 104, 229, .265, 321, 655 
Logarithmic distribution, 63 
664 
INDEX 
Logistic distribution and growth, 52-53 
Lost calls, 190, 495. 
Luck, persistence of, 15-17 
LUKACS, E., 86, 655 
LUNDBERG, F., 182 . 
MC KEAN, H. P. Jr., 333, 655 
MCSHANE, E.J., 103 
MANDELBROT, B., 175, 288 
Marginal distribution, 67, 134, 157 
normal, 100 
prescribed, 165 
MARKOV, A., 228 
MARKOV processes with continuous 
time. 96,321-345,624-625 
in countable spaces, 483-488 
and ergodic theorems, 369, 491-492 
and semi-groups, 349-357, 454-458 
see also Birth-and-death processes; Semi- 
MARKOV processes 
MARKOV processes with discrete time, 94- 
99,101-102,205-209,217 
and ergodic theorems, 270-274 
and martingales, 244 
and spectral aspects, 635, 649 
MARKOV property, 8-9 
strong, 20 
MARSHALL, A. W., 246 
Martingales, 209-215, 241-244 
inequalities for, 241-242, 246 
Matrix calculus, 82-83, 484-485 
Maximal partial sums, 198, 402, 408^412, 
419-423 
estimate of, 412 
see also Duration 
Maximal recurrence time, 189, 386 
Maximal row sums, 320, 597 
Maximal term, 172, 277, 287, 465; see also 
Order statistics; Record values 
MAXWELL distribution, 32, 48, 78-79 
Mean approximation theorem, 111-112 
Mean square convergence, 636 
Mean value theorem, 109 
Measurability, 113-115 
Measure space, 115 
Median, 17,137 
Metrics for distributions, 285; see also 
BANACH space; HILBERT spaces 
Microscopes, 31 
MIDDLETON, D., 631 
Milky Way brightness, 325 
MILLER, H. D., 603 
MILLS, H. D., 101 
Minimal solutions, and diffusions, 339 
and jump process, 329-331 
of KOLMOGOROV differential equations, 
485-488 
and semi-MARKOV processes, 497 
and WIENER-HOPF equation, 402 
Mirror experiment, 51 
MITTAG-LEFFLER function, 453-454 
Mixtures, 53-55, 73, 159, 167 
and transforms, 437, 504 
Moments, 5, 136,151,570 
convergence of, 251-252, 269 
and derivatives, 435 
generating function, 434 
HAUSDORFF moment problems, 224- 
228,245 
inequalities, 155 
in renewal, 375 
uniqueness problem, 227, 233, 514-515 
in Rr, 529 
Monotone convergence principle, 110 
property of, 350-352 
Monotone functions, 275-277 
Monotone sequences, 105 
Mortality, random walks with, 424 
Moving average processes, 88-89, 645 
Multivariate normal characteristic functions, 
522-523 
MUNTZ, H. Ch., 245 
Natural scale in diffusion, 333 
Nearest neighbors, 10 
Needle problems, BUFFON's, 61-62 
NELSON, E., 100, 347 
NEUMANN, J.V., 44 
NEUMANN, K., identity of, 60 
NEVEU.J., 103,655 
NEWELL, G. F., 40 
NEYMANJ., 182 
NIKODYM.jee RADON-NIKODYM; 
LEBESQUE-NIKODYM 
Noise, see Shot effect 
Nonlinear renewals,y387 
Norm,256, 350, 636, 642 
topology, 286 
Normal distributions, 46, 64, 87, 173, 503, 
566 
bivariate, 70, 72, 101 
characterization of, 77-80, 85, 525-526 
INDEX 
665 
degenerate, 87 
domain of attraction, 313, 577-578 
marginal, 99-100 
MARKOVIAN, 94 
inRr, 83-87,522 
"Normal" domain of attraction, 581 
Normal semi-groups, 299, 307, 319 
Normal stochastic processes, 87-94, 641-646 
Nucleons, 323 
Null arrays, 177-178, 583-588 
Null sets, 125-126, 140 
NYQUIST, N., 631 
Operational time, 181, 345 
Operators associated with distributions, 
254-258 
Optional stopping, 213 
Orbits, binary, 33 
Order relation in Rr, 82, 132 
Order statistics, 18, 20-21, 100 
application to estimations, 41 
and limit theorems, 24, 43 
OREY, S., 381 
ORNSTEIN-UHLENBECK process, 99, 
335-336 
Oscillating random walks, 204, 395 
OSIPOV, L. V., 545 
Paradoxes, 11-14, 23, 187 
PARETO distribution, 50, 175; see also 
LEVY-PARETO 
PARSEVAL relation, 463, 507, 615, 619- 
620,638,641,644 
as KHINTCHINE criterion, 639-640 
Partial attraction, 320, 568, 590-592 
Partial ordering, 82, 132 
Particles, collisions of, 206, 322-323, 325 
counters for, 372 
directional changes of, 323 
energy, losses of, 323,325 
splitting of, 25, 42,100 
Patients, scheduling of, 183 
BEARSON, K., 48 
PEARSON'S system of distributions, 48, 
50 
Pedestrians, 189, 37S, 387 
Periodograms, 76 
Persistency, see Transient distributions 
Petersburg game, 236 
PETERSEN,D.P.,631 
PETROV,V.V.,545,552 
PHILLIPS, R. S., 231, 294, 454, 656 
P1NKHAM, R. S., 63 
PITMAN, E.J.G., 565 
PLANCHEREL theorem, 510 
PLANCHEREL transform, 637-640 
PLANCK, see FOKKER-PLANCK 
Platpon formation, 40 
POINCARE, H., 62 
POlNCARE's roulette problem, 62-63 
Point functions, 128 
POISSON distributions, 566 
approximations by, 286 
compound, 555, 557-558 
difference of, 149, 166, 567 
POISSON ensembles, 15; see also 
Gravitational fields 
POISSON kernel, 627, 648 
POISSON processes, 12, 14-15 
direction of by gamma process, 348-349 
direction of gamma process by, 349 
gaps in, 378 
as limit in renewal, 370 
supremum in, 183 
see also Compound POISSON processes; 
Pseudo-POISSON processes 
POISSON's summation formula, 63, 343, 
629-633, 648 
Polar coordinates, 68 
POLLACZEK, F., 198; see also 
KHINTCHINE-POLLACZEK 
POLLAK, H. O., 217 
POLLARD, H., 360,381 
POLYA.G., 155, 172,182 
POLYA distribution, 57 
POLYA's criterion, 505, 509 
POLYA's urn, 210, 229-230, 243 
Polymers, 206 
Population growth, 337 
logistic, 52-53 
random dispersal of, 261-262 
PORT.-S. C, 279 
Positive definite functions,-620-623 
Positive definite matrices, 81 
Positive definite sequences, 633-635 
Positive variables, 571-572 
Potentials, 488 
Power spectrum, 624 
PRABHU, N. U., 656 
Probability density, see Densities 
Probability distributions, 130 
r, 127-165 
666 
INDEX 
Probability measures and spaces, 103-- 
116,133 
Processes with independent increments, 
179-184 
Product measures and spaces, 121-123 
PROHOROV, Yu. V.. 39, 343 
Projection, 67 
of random vectors, 30, 22, 33 
Proper distributions, 13C 
Proper convergence, 248, 285 
Pseudo-POISSON processes, 322-324, 345 
and exponential formula, 354 
with linear increments, 324-325 
and semi-groups, 354-357, 459 
Pure birth process, 488-491 
PYKE, R., 183, 389, 470, 497 
Quasi-stable distributions, 173 
Queues, 54-55, 65,196-197, 208, 481-482 
joint distribution for residual and spent 
waiting times, 386 
and limit theorems, 380 
one-server, 194-195 
and paradoxes, 12-14 
parallel, 17, 18,41 
for shuttle trains, 196-108 
see also Busy periods 
Queuing processes, 194-200, 208, 380, 410 
Radiation, stellar, 206 
RADON-NIKODYM derivative, 139 
RADON-NIKODYM theorem, 139, 140, 141 
RAIKOV's theorem, 571 
Random chains, 206-207 
Random choice, 2, 21-25, 69 
and coin tossing, 35 
see eho Covering theorems 
Random directions, 29-33, 42, 43, 142 
addition 01,31-33, 207,523 
Random dispersal, 261-262 
Random flights, 32r33 
Random partitions, 22*23, 74-75", see also 
Covering theorems 
Random splittings, 25-26, 42, 100 
Random sums, 54, 159, 167, 504 
central limit theorems for, 265, 530 
characteristic function for, 504 
Random variables, 4, 68, 116-118, 131 
complex, 499 
Random vectors, 31, 33, 107-108 
Random walks in R1, 190-193, 200-204, 
389-425, 598-616 
associated, 406 
empirically distributed, 38 
simple ( = Bernouilli), 213-214, 318, 393, 
395, 425, 437 
see also Hitting points; Ladder variables 
Randomization, 53-64 
and exchangeable variables, 228 
and semi-groups, 231, 355 
and subordination, 345-349 
Randomized random walks, 58-61, 479-483, 
566-567 
Random walks in Rr, 32-33 
central limit theorem for, 261 
Ratios, 16-17, 24-25, 54 
RAY, D., 333 
RAYLEIGH, Lord, 32-33, 523 
Record values, 15-16, 40; see also 
Order statistics 
Rectangular density, 21, 50 
Recurrence time, 184 
maximal, 189,386 
observed, 13, 187 
Recursive procedures, 26 
Reflecting barriers, 340, 343, 464 
Reflection principle, 175, 477, 478 
Regeneration epochs, 184 
Registrations, 191, 373 
Regression, 72, 86 
Regular stochastic kernels, 271-273 
Regular variation, 275-284, 288, 289 
Reliability theory, 52 
Renewal epochs, 184, 372-374 
Renewal equation, 185-187, 359, 366-368, 
385-3S8 
theory of, 466-468 
Renewal processes, 12, 184-187, 216, 358- 
388 
applications of; 377-378 
imbedded, 191-193 
nonlinear, 387 
superposition of, 370-371 
transient, 374-377 
two-stage, 380 
Renewal theorems, 35R-372 
proof of, 364-366 
on the whole line, 201, 380-385, 42F 
RENYI, A., 343 
Reservoirs, 182, 183, 195,seealso Storage 
and inventories 
Residual waiting time, 188 
limit theorem for, 369, 370, 386, 471-472 
Resolution of identity, 643 
INDEX 
667 
Resolvents, 429, 452-453, 455, 487 
and complete monotonicity, 461 
Resultant of random vectors, 31, 146, 523 
Returns to the origin, 424 
RICHTER, W., 552 
RJEMANN integrable, directly, 362-363 
RIEMANN-LEBESQUE theorem, 513-514, 
538,629 
RIESZ, F., representation theorem of, 120, 
134,251 
RIESZ, M., 231 
RIORDAN.J.,656 
Risk theory, 182-183; fee also Ruin problems 
ROBBINS, H.E., 99, 265, 360 
ROSENBLATT, M., 77, 287, 656 
Rotational symmetry, 523-524 
Rotations, 78, 84,101 
ROUCHE's theorem, 408 
Roulette, 22, 62 
Rounding errors, 22, 62 
Row vectors, 83 
ROYDEN.H. L., 228 
Ruin problems, 198, 326 
in compound POISSON processes, 182- 
184,469-470 
estimates for, 377-378, 411-412 
Runs, see Record values 
RVAGEVA, E. L., 39 
Sample extremes and median, 18 
Sample mean and variance, 86-87 
Sampling theorem, 631-632, 633 
SANKHYA, 571 
SAVAGE, L. J., 124, 229 
Scale parameters, 45,134 
Scheduling of patients, 183 
SCHELLING, H. v., 208 
SCHLOMILCH's formula, 60, 566-567 
SCHMIDT, R., 445 
SCHWARZ' inequality, 152-153,166, 498, 
527,642 
Second moments, 5 
Selection theorems, 267-270 
Self-reciprocal functions, 503 
Self-renewing aggregates, 187' 
Semi-groups,.231-232 
CAUCHY, 303 
compound POISSON, 299-300 
convolution, 293-296 
discontinuous, 305 
exponential formula of, 353-355 
with finite variances, 298-300 
generators of, 356-357 
and infinitely divisible distributions, 290- 
318 
and LAPLACE transforms, 454-458 
and MARKOV processes, 349-353 
normal, 299 
stable, 305-308 
Semi-MARKOV processes, 483, 497 
Semi-martingale,'214 
Separability, 269 
Servers, see Queues; Queuing 
Servo-stochastic process, 101-102 
Servomechanisms, 101-102 
Set functions, 106-107 
Sets, algebras of, 112-113 
null, 125-126 
SHANNON, C, 631 
SHAPIRO, J.M., 571 
SHEPP, L., 64,158, 618, 648 
Shift operator, 643 
SHOHAT, J.A., 228, 245 
Shot effect, 178-179, 287, 595 
Shuttle trains, 196 
SIERPINSKI, W., 268 
Sigma additive, 108 
Sigma algebras, 112-113 
Sigma finite measures, 115 
Significance test, 76-77 
Significant digits, 63 
Simple conditional distributions, 156-159 
Simple functions, 131 
Singular distributions, 36, 107-108.141 
convolutions of, 146, 593 
SKELLAM, J. G., 262 
SKITOVIC, V. P., 79 
SKOROKHOD, A. V., 39, 655 
Slow variation, see Regular variation 
SMIRNOV, N. V., 39,43; see also 
KOLMOGOROV-SMIRNOV 
SMITH, W. L., 363, 381, 497 
Smoothing, 88,144, 536-538 
SNEDECOR's density, 49 
Spaces, 115-125 
Span, 138 
SPARRE-ANDERSEN, E;, 389, 413, 418- 
423 
SPARRE-ANDERSEN transformations, 
421-422 
Special densities, 45-53 
Spectral measure, 624, 647 
668 
INDEX 
prescribed, 626 
Spectral theorem for unitary operators, 643 
Speed measure, 333 
Spent waiting time, 188, 386 
limit theorems for, 371, 471 
Spheres, passage through, 30-31 
uniform distribution on, 69 
SPITZER, F., 198, 389, 404, 418, 605, 
612,618,655 
Splittings, random, 25-26 
Stable distributions, 169-176, 574-581 
densities of, 581-583 
positive, 448-449 
products of, 176, 452 
inU ,594,597 
in renewal, 373 
strictly stable, 170 
subordination, 348, 451 
symmetric, 172 
see also CAUCHY distribution; Domain 
of attraction 
Stable distributions of order V4, 52, 64, 173, 
319,436-437 
Stable semi-groups, 305-308 
Stars, gravitational field of, 173-174, 215 
neighbors of, 10 
radiation from, 206, 325-326 
Stationary increments, 180, 293 
Stationary measure and probability, 208, 
272 
Stationary processes, 87-94, 101, 623-626 
law of large numbers for, 245 
see also Ergodic theorems; Steady state 
Stationary sequences, 97-98 
Statistic, 18 
Steady-state, 14, 207-208, 326 
in renewal, 370,386 
STEIN, C, 601 
Step functions, 108 
STEUTEL, F. W., 452 
STEVENS, W.L., 77 
STIELTJES, 228; see also LEBESQUE- 
.STIELTJES integrals; LAPLACE- 
STIELTJES transform 
STIRLING'S formula, 47 
Stochastic boundedness, 254, 309 
Stochastic difference equation, 90 
Stochastic integrals, 645^-646 
Stochastic kernels, J 59, 205, 270-272, 349, 
351 
Stochastic processes, 304-305» 641-646 
limits in, 379-380 
StM\es in rivers, 182 
Storage and inventories, 195-196; see also 
Resetvcirs 
Stratification, ,57 
Strength, tensile,« 
Strict sense stable, 170 
Strong continuity of senu-gioups, 454 
Strong convergence, 257, 350 
Strong law of large numbers, 237-241 
STUDDEN, W., 228,656 
Student's density, 49 
Submartingale, 214, 241-242 
Subordination of processes, 345-349, 355, 
451-452,458,573-574,596 
Substochastic kernels, 205 
Superposition of renewal processes. 370 
Support, 45 
Symmetric CAUCHY distribution, in R , 
70-71 
inU ,71 
Symmetric distributions, 172, 526 
Symmetric events, 124 
Symmetrically dependent variables = Ex- 
changeable variables, 228-230 
Symmetrization, 148-150 
Symmetrization inequalities, 149 
Systems, impossibility of, 213 
Tail events, 124 
TAKACS' integral equation, 473 
TAMARKIN, J. D., 228, 245 
TANNER, J.C., 189 
TAUBERIAN theorems, 363, 442-448 
applications of, 418, 419, 448. 467, 471, 
612-614 
TAYLOR development, 512, 514-515, 528 
TAYLOR expansion, 585, 616 
TAYLOR formula, 312 
generalized, 230-231 
TAYLOR series, 534, 536 
TEICHER.H., 287 
Telephone calls, 57,179 
lost calls,#190, 495-496 
see also Busy periods 
Tensile strength, 9 
Terminating ( « Transient) renewal processes, 
186-187,374-377 
Theta functions, 343, 632 
Three-series theorem, 317 
TITCHMARSH, E. C, 613 
INDEX 
669 
TORTRAT, A., 103,593 
Total variation, 286 
Traffic problems, 40,189, 378, 387, 474- 
475,496 
Trains, queues for, 196-198 
Transient distributions, 614*616 
Transient random walks, 201-204 
criteria for, 203, 415, 614-616, 618 
renewal equation and theorem for, 200, 
384, 428 
Transient ( = Terminating) renewal process, 
186-187,374-377 
Transition operators, 350 
Translation operator, 231-232, 256 
Translation principle, 432 
Translation semi-groups, 295, 357 
Transmission lines, 208-209 
Trees, growth of seeds of, 261-262 
Triangle inequality, 256 
Triangular arrays, 177-178, 216, 308-312, 
583-588, 596 
Triangular density, 26, 50, 502, 503 
TROTTER, H. F., 262 
Trunklines, see Telephone calls 
Type of distribution, 45,137 
UGA herit, 217 
UHLENBECK, see ORNSTEIN-UHLENBECK 
Unbiased estimator, 220 
Uniform distribution, 21-29, 50, 502, 503; 
see also Random choice 
Unimodal distributions, 158, 527 
convolutions of, 164 
Unitary operators in HILBERT space, 643 
"Universal laws" of DOBLIN, 590-592 
Vacuum tube, 178-179, 287, 595 
Vanishing at infinity, 248 
Variable distributions, 316-318 
Variables, independent, sequences of, 
123-125 
random, 116-118 
Variance, 5, 46,136 
conditional, 72 
infinitesimal, 335 
Variation, total, 286 
Vector notation, 82 
Velocities, MAXWELL distribution for, 
32,48,78-79 
Violins, loudness of, 31 
Visibility, 10 
Visits in random walks, 191, 202 
Von NEUMANN, J., 44 
von SCHELLING, H., 208 
Waiting lines, see Queues 
Waiting time paradoxes, 11-14, 372 
Waiting times, 17-20 
WALD, A., 228,397,406 
WALD's identity and approximations, 401, 
426,427,428,601-604 
WALLACE, D.L., 542 
WALSH, J.W., 51 
WAX, N., 656 
Weak convergence, 251 
WEBER, H., 438 
WEIBULL distributions, 52 
WEIERSTRASS approximation theorem, 
223 
WEYL, H., 268 
WHITTAKER, J. M., 631 
WIENER, N., 305, 363, 403, 640 
WIENER process, see BROWNIAN motion 
WIENER-BACHELIER process, 181 
WIENER-HOPF factorization, 389, 400-40^ 
428,604-609,617 
WIENER-HOPF integral equation, 402-404, 
406, 425 
WIENER-HOPF techniques, 412 
WIENER-KHINTCHINE criterion, 640 
WILKS, S. S., 40 
WINTNER, A., 167 
WOLD, H., 522 
WOLFOWITZ, J., 200, 274, 331 
WRIGHT, E.M., 268 
WRIGHT, S., 336 
YAGLOM, A. M., 656 
YAHAV, J. A., 388 
YOS1DA, K., 321, 656; see also HILLE- 
YOSIDA theorem 
YULE.G.U., 57 
Zero-or-one laws, 124 
ZOLOTAREV, V. M., 583 
ZYGMUND, A., 565 
