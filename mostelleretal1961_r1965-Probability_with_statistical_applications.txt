PROBABILITY 
WITH STATISTICAL 
APPLICATIONS 
FREDERICK MOSTELLER 
Harvard University 
ROBERT E. K. ROURKE 
Kent School 
GEORGE B. THOMAS, JR. 
3Iassachusetts Institute of Technology 
ADDISON-WESLEY PUBLISHING COMPANY, INC. 
READING MASSACHUSETTS 
This book is in the 
ADDISON-WESLEY SERIES IN STATISTICS 
Copyright � 1961 
ADDISON-WESLEY PUBLISHING COMPANY, INC. 
Printed in the United States of .truerf ca 
ALL RIGHTS RESERVED. THIS BOOK, OR PARTS THEREOF, 
MAY NOT BE REPRODUCED IN ANY FORM 
WITHOUT WRITTEN PERMISSION OF THE PUBLI,HERS. 
Library of Congress Catalog Card No. 61-9151 
Second Printing- January 1965 
ADDISON-WESLEY PUBLIbHING COMPANY, INC. 
Headquarters: 
Reading, Massachusetts 01867 
School Division 02ce: 
3220 Porter Drive, Palo Alto, California 94304 
Sales 02ce: 
411 Elm Street, Dallas, Texas 75202 
Sales Oce: 
10-15 Chitty Strect, London W 1, England 
To 
A. W. Tuc 
and 
S. S. WILKS 
PREFACE 
In writing this text, the authors have kept before them two basic ob- 
jectives: the first is to acquaint the reader with the theory of probability-- 
the mathematics of uncertainty; the second is to illustrate some applica- 
tions of probability to statistical theory. Chapter i elaborates these 
objectives, explains current interpretations of probability, and illustrates 
how probability and statistical theory are applied to important practical 
and scientific problems. 
The reader may expect to gain three things from this book: first, an 
understanding of the kinds of regularity that occur amid random fluctua- 
tions; second, experience in associating probabilistic mathematical models 
with physical phenomena; and third, the ability to use these mathematical 
models to interpret the physical phenomena and to predict, with ap- 
propriate measures of uncertainty, the outcomes of related experiments. 
The level of mathematics required for an understanding of the material 
is that of a second course in high-school algebra. No knowledge of calculus 
is assumed, but readers having a background in calculus will recognize 
that some topics (for example: probabilities as areas, and least squares) 
could be handled by methods different from those presented here. 
As distinctive features, we think our treatment has the following: 
1. The first four chapters give, in a readily accessible form, a brief 
course in elementary probability theory for finite sample spaces. 
2. Chapter 5 offers one of the few available elementary introductions 
to random variables, their distributions, and properties of their distribu- 
tions. 
3. The key position of the normal distribution both in probability and 
in statistics has led to the inclusion in Chapter 6 of an intuitive intro- 
duction to continuous random variables. This, together with the study 
of joint distributions of two or more discrete random variables, lays a 
foundation for the study of distributions of functions of several random 
variables. Without the normal distribution very little can be done ana- 
lytically or numerically about computing approximate numerical prob- 
abilities for functions of several independent random variables. With it, 
the theory of sampling in Chapter 9 offers an immediate and major 
application, whose results are routinely used in statistical infcrence in 
every field. 
4. The properties of binomial probability distributions are studied in 
detail as a means of introducing, and applying, the central limit theorem. 
The development, given in the last half of Chapter 7, is new. 
5. In addition to applications of classical statistical infercncc, we in- 
clude some simple cxamples of modcrn Bayesian inference. 
ix 
x PREFACE 
6. Our treatment of least squares is unusual because it depends only 
on high-school algebra. A welcome by-product is that the calculation 
of the residual sum of squares seems easier with this approach than with 
that of traditional calculus. We also include a discussion of models of 
regression not usually found in elementary texts. 
The authors have tried to introduce each new concept through examples, 
and additional examples are given after each important definition or 
theorem. Readers who desire a faster pace may scan some of these ex- 
amples rapidly and concentrate their attention on the numbered defini- 
tions, theorems, and corollaries. However, mastery of the theory will 
usually be increased by studying the illustrative examples, and by working 
exercises in the lists appearing at the ends of most sections. 
The six end-pages at the very front and back of the book contain a 
summary of most of the key formulas, a glossary of symbols, and two 
brief tables of the normal distribution (each the inverse of the other). 
The binomial tables at the back of the book include values of p 
less than, equal to, and greater than �. The consequent doubling of the 
size of the tables is justified, we believe, by the added convenience. 
By a suitable selection of topics, the text can be used for a one-semester, 
or a one-quarter, course. The taste and experience of the instructor are 
the best guides to such selections. 
A reader studying the book xvithout the gindance of an instructor 
should generally follow the order of presentation of the text. A reasonable 
minimal goal is to complete Section 7-4; and if pressed for time the 
reader may omit Sections 3-8 through 3-12, 4-4 through 4-6, 5-5 and 5-7, 
and 6-2 through 6-4. Such omissions leave out many useful and 
portant topics, and make it difficult to understand those parts of Chap- 
ters 7, 8, and 9 that refer to the normal distribution. However, Chapter 10 
is still accessible if preceded by Sections 5-5, 6-1, and the paragraph in 
Section 9-4 defining the sample correlation coefficient; Chapter 10 does 
not depend upon Chapters 7 or 8. 
A reader particularly interested in the statistical applications (accept- 
ance sampling, hypothesis testing, estimation) should include Sections 
6-4, 7-7, and most of Chapter 8. 
Theorems, corollaries, and important definitions are numbered sequen- 
tially, by chapters. For example, 6-1 Definition, 6-2 Definition, 6-3 Deft- 
nition, 6-4 Theorem, 6-5 Corollary are the first three definitions and the 
first theorem and first corollary in Chapter 6. The second theorem of 
Chapter 6 is number 6-6 Theorem. This numbering system, with the 
number on the left, set in boldface type, is intended to make it easier to 
look up a reference. In the body of the text, however, we refer to Defini- 
tion 6-3, to Corollary 6-5, and so on. 
PREFACE xi 
The authors are indebted to many for help, criticism, and encourage- 
ment in connection with the xvriting of this book. Their experiences as 
members of the Commission on Mathematics of the College Entrance 
Examination Board have been especially valuable. In particular, they 
have used the evaluations and experiences associated with the experi- 
mental text Introductory Probability and Statistical Inference, also known 
as "the grey book." The authors assisted in the writing of that text, and 
they recall with pleasure the collaboration of Edwin C. Douglas, Richard 
S. Pieters, Donald E. Richmond, and Samuel S. Wilks. Parts of the 
present work have been based on, or quoted from, material copyrighted 
by the College Entrance Examination Board, and this has been done by 
permission. This permission does not imply any responsibility for, or 
endorsement of, this work by the College Entrance Examination Board 
or the Commission on Mathematics. 
Much of the material in this text was used as the basis of the nationally 
televised NBC Continental Classroom course in Probability and Statistics, 
first presented early in 1961. 
We gratefully express our appreciation to: 
W. G. Cochran, A. P. Dempster, G. E. Noether, and John W. Pratt for 
critically reading and discussing parts of the manuscript, and for many 
helpful conversations and suggestions. 
Rita Chartrand, Mary 5cQuillin, and Jane Thomas for much patient 
and skillful work on the production of the manuscript. 
Cleo �outz for typing, computations, tables and graphs, and efficient 
organization and management. 
Laurence Herbst, for his work on the binomial tables. 
Linda Alger, Keewhan Choi, Robert M. Elashoff, Miles Davis, and 
Joseph I. Naus for their assistance with problem solutions. 
William and Gale 5osteller for the card-shuffling data of the first-ace 
example in Chapter 1. 
Biometrika Trustees for permission to publish Chart I--Confidence 
Limits. 
The Free Press, Glencoe, Illinois, and The Rand Corporation for per- 
mission to publish a page of random digits from the book A Millioa 
Raadom Digits with 100,000 Normal Deviates, copyright 1955 by The 
Rand Corporation. 
The Macmillan Company of Canada, for permission to use some prob- 
lems from An Advanced Course in Algebra by Norman Miller and lobert 
E. K. Rourke. 
F.M. 
R.E.K.R. 
G.B.T., Jr. 
CONTENTS 
CHAPTER 1. PROBABILITY AND STATISTICS. THE STUDY OF VARIABILITY 1 
1-1 Probability and statistics . 1 
1-2 Interpretations of probability. 2 
1-3 Illustrations of probabilistic models 3 
1-4 Applications of statistics 6 
1-5 The empirical study of variability 7 
1-6 Do probabilities grow? 17 
CHAPTER 2. PERMUTATIONS COMBINATIONS AND THE BINOMIAL 
THEORL`hX . 19 
2--1 Permutations' the multiplication principle . 19 
2-2 Formulas for permutations 27 
2-3 Combinations 33 
2-4 Permutations of things that arc not all different 42 
2-5 The binomial theorem . 48 
CHARTER 3. A FIRST LOOK AT PROBABILITY: EQUALLY LIKnLY 
0UTCO.hiES 55 
3-1 Introduction. Some experiments 55 
3-2 A sample space of an experiment 63 
3-3 Probabilities in a finite sample space 67 
3-4 Events and sets 73 
3-5 Mutually exclusive events 75 
3-6 Independent events 81 
3-7 Conditional probability 85 
3-8 Sample spaces with many elements 93 
3-9 Random drawings 100 
3-10 Random numbers 104 
3-11 Use of tables of random digits 106 
3-12 Conclusion I l0 
CHARTER 4. GENERAL THLORY OF PROBABILITY FOR FINITE SAMPLE 
Srxcs 112 
4-1 Introduction 112 
*-4-2 Sample space and probability 116 
4-3 Indcpcmlcnt events 123 
4-4 Conditional probability 131 
4-5 Using the product rule to asqgn probabilities in a sample space 137 
4-6 Baycs' theorem 143 
xiii 
xiv CONTENTS 
CHAPTER 5. NUMBERS DETnRMINED BY EXPERIMENTS. RANDOM 
VARIABLES 155 
5-1 Random variables and their probability functions 155 
5-2 Mathematical expectation of a random variable: population mean 167 
5-3 Mean of a function of a random variable 174 
5-4 Variability l$1 
5-5 Average and variance in a sample 194 
5-6 Chebyshev's theorem for a probability distribution 202 
5-7 Chebyshev's theorem for a frequency distribution of measure- 
ments 207 
CHAPTER 6. JOINT DISTRIBUTIONS AND CONTINUOUS DISTRIBUTIONS 209 
6-1 Joint probability function of two random variables 209 
6-2 Probabihty graphs for continuous random variables: probabilities 
represented by areas 221 
6-3 Cumulative probability graphs 224 
6-4 The normal curve and the normal probability distribution 230 
CHAPTER 7. REPEATED TRIALS WITH TWO TYPES OF OUTCOMES: 
THE BINOMIAL DISTRIBUTION . 241 
7-1 Examples of binomial experiments 241 
7-2 Extension of the binomial experiment to n trials 250 
7-3 Expected value of a binomial random variable. 256 
7-4 Binomial probability tables 258 
7-5 Properties of the binomial distribution 262 
7-6 Tools for studying the limit of the bnomial distribution . 268 
7-7 Areas for binomial distributions tend to areas under the normal as 
n grows--the central limit theorem for the binomial 275 
CHAPTER 8. SOME STATISTICAL APPLICATIONS OF PROBABILITY 285 
$-1 Estimation and the testing of hypotheses 285 
8-2 Estimating p, the binomial probability of success . 286 
8-3 Conservative confidence limits for p xxith large n 295 
8-4 Bayesian approach when prior information is available 299 
8-5 Testing of a binomial statistical hypothesis 301 
8-6 Bayesian inference wth personal probabilities 310 
CItAPTER 9 TItEORY OF SAMPLING. rARIANCE$ OF SUMS AND OF 
AwAm.:s 314 
9-1 Calculation of the distribution of a sum 314 
9-2 The variance of the distribution of the sum of two independent 
random variables 318 
9-3 Variance of the sum and of the average of several variables . 325 
CONTENTS XV 
9-4 Covariance and correlation 335 
9-5 Sampling without replaecmcnt from a finite population 349 
CHARTER 10. LEAST SQUARES, CURVE-FITTING, AND REGRESSION 360 
10--1 Curve-fitting 360 
10-2 The method of least squares 366 
10-3 Fitting a line through the origin . 375 
10-4 Fitting a general line 379 
10-5 Precision of estimation and the correlation coefficient 386 
10-6 Models for rcgression 391 
ArrENDIX I. COLLECTIONa OF OBJECTS: SETS 403 
I--1 The notion of a set 403 
I-2 Two ways of specifying sets 405 
I-3 Universal set and subsets 411 
I-4 Operations xxth sets 413 
APPENDIX II. SUMMATIONS AND SUBSCRIPTS 417 
II-1 Subscripts and the summation symbol,  417 
II-2 Theorems about summations . 422 
APPENDIX III. A THEOREM ON INDEPENDENCE 427 
TABLE I. 2500 random digits 430 
TABLE II. Values of n! and log n! 431 
TABLE III. Normal curve areas 432 
TABLE IV. Three-place tables of the binomial distribution 433 
CHART I. Chart for 95% confidence limits 453 
ANSWERS TO EVEN-NUMBERED EXERCISES. 455 
BIBLIOGRAPHY . 471 
INDEX 473 
CHAPTER 
1 
PROBABILITY AND 
STATISTICS. THE STUDY 
OF VARIABILITY 
This chapter provides initial answers to questions such as: What is 
probability? ]Vhat is statistics? How are they used today? The aim is 
to introduce some background ideas in probability and statistics, and to 
transmit a feeling for the questions posed and the answers given in these 
subjects. Algebraic skills are not involved here. Consequently: 
(1) We discuss the nature and role of mathematical models, and the 
relation of these models to the real world. 
(2) We exhibit and discuss probability models, which are special kinds of 
mathematical models. 
(3) We offer opportunities for personal experience with the fluctuations 
and regularities of experiments involving chance. 
At a first reading, one should not expect full understanding of all the 
ideas in this chapter. Something less will set the stage for the mathematical 
work to come. 
1-1. PROBABILITY AND STATISTICS 
The wealth and variety of applications of the theory of probability at- 
tract many students. Some find beauty in the extensive mathematical 
structure that emerges from a few assumptions and definitions; others, 
both the practical and the philosophical, enjoy discussing the meanings 
that may be attached to probabilistic statements. Still others admire the 
order that emerges from seeming chaos--toss a penny once, and no one 
knows whether it will fall heads or tails; toss two tons of pennies, and we 
all know that one ton will fall heads, the other tails. 
1 
2 THE STUDY OF VARIABILITY [CHAr. 1 
We are used to the notion that the idealized triangles of plane geometry 
can be used as mathematical representations, or mathematical models, of 
physical triangles in the real world. In a similar way, we build mathemati- 
cal models for probabilistic problems and develop consequences of them. 
For example, a tossed coin has probability � of coming up "heads"; and 
ve shall develop for coin tosses a probability model that gives the proba- 
bility that when n coins are tossed, exactly x fall heads and n -- x fall 
tails. In particular, the probability that all land heads is (�)n. The theory 
and its consequences apply to idealized coins, and ve hope it applies to 
real coins when they are tossed. In Section 1-3, we give brief descriptions 
of a variety of problems in the real world that are studied by probabilistic 
models. 
The field of statistical inference leans heavily upon the theory of proba- 
bility, but supplements it. When data are gathered, we may use statistical 
theory to help choose among alternative mathematical models. For a 
given tommy, consider drawing a sample of families to estimate the fraction 
of homes with color television sets. The theory of probability tells us, for 
a given fraction owning sets in the community, what the ownership frac- 
tion in the sample is likely to be. But statistical inference uses the sample 
result to estimate the fraction in the town who own sets. In this example, 
probability theory deduces from the known content of the population the 
probable content of the sample, while statistical inference infers the content 
of the population from the observed content of the sample. [ore generally, 
the theory of probability deduces from a mathematical model the proper- 
ties of a physical process, while statistical inference infers the properties 
of the model from observed data. 
The field of statistics includes more than statistical inference. In general, 
statistics is the art and science of gathering, aalyzing, and makig in- 
fereces from data. Some parts of statistics are not mathematical, while 
other parts are. Although wc have tried to separate probability from 
statistics, statisticians must work on problems in probability as well as 
in statistics. 
1-2. INTERPRETATIONS OF PROBABILITY 
At the mathematical level, there is hardly any disagreement about the 
foundations of probability or about its mathematical consequences. The 
foundation in set theory was laid in 1933 by the great Russian probabllist, 
A. Kolmogorov, still an active research worker in 1960. At the level of 
interpretation and use, there are two extreme positions that are often 
adopted and, of course, many positions in bctxvecn. 
The objective position is, at present, the most popular. This position 
holds that probability is applicable only to events that can be repeated 
1-3] ILLUSTRATIONS OF PROBABILISTIC MODELS 3 
over and over under much the same conditions. Thus the objectivist is 
happy to talk about probabilities in connection with the tossing of a coin 
or the manufacture of a mass-produced item. tie can readily think of many 
light bulbs being produced, and of the probability of a good light bulb as 
the long-run ratio of number of good bulbs to the total number produced. 
But he draws the line at unique events. For example, he would not care 
to talk about the probability that Romulus founded Rome, or that Chile 
and Argentina would unite to become a single country in the next ten years. 
Thus a large class of problems is set aside by the objectivist as not ap- 
propriate for the application of probability, because there is no long-run 
ratio in view. Furthermore, the objectivist likes to make interpretations 
only from repeated events, and prefers not to bring other kinds of evidence 
into his inferences. 
The other school of thought is solnetimes called personalistic. The 
personalist regards probability as a measure of personal belief in a par- 
ticular proposition, such as the proposition that it xvill rain tomorrow. 
This school of thought believes that different "reasonable" individuals 
may differ in their degrees of belief, even when offered the same evidence; 
and so their personal probabilities for the same event may differ. The 
personalist will apply probability to all the problems an objectivist studies, 
and to many more. For example, at least in principle, the personalist 
xvould take the Romulus question in his stride. The personalist also has 
available some additional techniques; in particular, he may have more 
use for Bayes' Theorem, treated in Chapter 5, than the objectivist. On the 
other hand, when the amount of data is large, the objectivist and the 
personalist usually get similar answers. 
A beginner would be unxvise to try to decide at once where he fits in with 
respect to these tx;o views. Furthermore, the last word is never said on 
such matters because new schools of thought arise. But the distinction 
between probability as a long-run relative frequency and probability as 
a measure of degree of belief is one that he may wish to reconsider from 
time to time as he understands the issues better. 
1-3. ILLUSTRATIONS OF PROBABILISTIC MODELS 
Games o chance loom large in the early history of probability, and even 
today they provide instructive problems for both the beginner and the 
expert. About 1654, the Chevalier de 3,ldr6, an amateur mathematician, 
consulted Blaise Pascal (mathematician, scientist, and theologian) about 
the solution of generalizations of the following problem.* 
* You may enjoy reading Oystein Ore, "Pascal and the invention of probability 
theory," American Mathematical Monthly, Vol. 67, No. 5, May, 1960, pp. 409-419. 
4 THE STUDY OF VARIABILITY ' [CHAP. 1 
Problem of points. On each play of a game, one of two players scores a 
point, and the two players have equal chances of making the point. Three 
points are required to win. If the players must end the game when one 
has 2 points and the other 1, how should the stakes be divided? Pascal 
said the stakes should be split 3 to 1, in favor of the man who was ahead. 
What do you think? 
Pascal engaged in a profitable correspondence with Fermat, another 
great mathematician, on this and other problems in probability, and be- 
tween them they developed many results, some of which are presented 
in Chapters 2, 3, and 4. 
Another early gambling problem is that of duration of play. We men- 
tion it because it has evolved and developed through the years, and in this 
evolution has become of value to both scientists and industrialists. 
Duration of play. Two players in a fair game have as fortunes m and n 
units, and the stake on each play is one unit each. Each player has an 
equal chance of vinning a play. If they play until one player is ruined, 
how long will they play, and what is the chance that the player starting 
with m units wins? The probability that the game lasts t trials is difficult 
to compute; the probability that the player with fortune m wins is 
m/(m - n). 
This problem is a forerunner of that of the random motion of a physical 
particle which is absorbed when it strikes a barrier--one of the many 
kinds of "random-walk" problems studied by physicists. To show the 
relation between the problems, suppose that a particle starts at the origin, 
O, and in each unit of time moves one unit to the right or one unit to the 
left, the direction being randomly determined. Erect barriers m units to 
the right and n units to the left of the origin, and suppose the particle 
stops when it strikes a barrier. The position of the particle after t units 
of time corresponds exactly to the amount of money won after t plays. 
For each of the foregoing problems, the probability model consists, 
essentially, of a fair coin, a rule for assigning points for plays, and a rule 
for deciding the winner of the game. A few examples of problems requiring 
probability models for their solution may help give the flavor of applica- 
tions that are made today. The answers to the questions raised are either 
beyond the scope of this book, or require an extensive specialized develop- 
ment. Such models are developed by applied mathematicians, statisti- 
cians, physicists, biologists, or other scientists. 
Queueing theory. People arrive at random times at a counter to be served 
by an attendant, lining up in a queue if others are waiting. Given in- 
formation about the rate of arrival and the length of time an attendant 
requires to serve each customer, how much of the time is the attendant 
idle? How much of the time is the queue more than 10 persons long? 
What would be the effect of adding another attendant? If people are not 
1-3] ILLUSTRATIONS OF PROBABILISTIC MODELS 5 
allowed to wait in line but must go elsewhere, what percentage of arrivals 
go unserved ? Variations of this problem are of interest in the maintenance 
of a battery of machines, in deciding how many toll booths to provide at 
the entrance to a throughway, in considering equipment needed for tele- 
phone lines and for high-speed computers, and even in the construction 
and control of dams. 
Inherita?,ce i, biology. The lendelian theory of heredity in its simplest 
form requires little more probability than that presented in this text; 
but, of course, the theory has gone far beyond endel. Suppose parents 
are classified on the basis of one pair of genes, and that d represents a 
dominant gene, and r represents a recessive gene. Then a parent with 
genes dd is pure dominant, dr is hybrid, and rr is pure recessive. The pure 
dominant and the hybrid are alike in appearance. Offspring receive one 
gene from each parent, and are classified the same way, dd, dr, or rr. 
The following table gives, for a simple case, the proportions of offspring 
of each type, for a given type of parents. Typical problems are: Knowing 
the proportions of the types of parents, what can we say of the composition 
of the population of offspring after 1, 2, . . . , n generations? If the model 
is modified so that dominant characteristics are favored in some way, do 
recessives die out? 
Parents Offspring 
dd dr rr 
dd dd 1 
1 
dd dr  
dd rr 1 
1 1 
dr dr   
! 
dr rr  
rr rr i 
Theory of epidemics. Suppose an infectious disease is spread by contact, 
that a susceptible person has a chance of catching it with each contact 
with an infected person, but that one becomes immune after having had 
the disease and can no longer transmit it. Then the mathematical theories 
of epidemics describe thc progrcss of an epidemic in terms of the numbers 
susceptible, infected, and immune through time. Typical questions are: 
How many susceptibles will be left when the number of infected is zero 
(epidemic over) ? How long will the epidemic last? For a city of given size, 
what is the probability that the disease will die out? 
Naturally, in this book we cannot expect to study such difficult problems 
in full generality, but we can lay a foundation for their study. 
6 THE STUDY OF VARIABILITY' [CHAP. 1 
1-4. APPLICATIONS OF STATISTICS 
We have already indicated that statistics deals, in part, with the analysis 
of data stemming from probability models, and that statisticians may 
also develop probability models like those in Section 1-3. A few examples 
of applications of statistics in other fields may interest the student. 
Screening of drugs. A pharmaceutical house tests hundreds of new 
medications, trying to find one that vill be safe, and superior to the 
standard treatment of a disease. People vary in their responses to a 
medication, and so do the animals on which medications are initially 
tested. This variation introduces a probabilistic aspect to the problem. 
Usually, testing is done in stages: most medications are eliminated at an 
initial stage based on a small number of subjects. If a medication looks 
promising it is carried on to a later stage where a more elaborate and 
severe test is made. One problem is to choose the sizes and the severities 
of the experiments at the successive stages so that good new medications 
are unlikely to be discarded, but so that poorer medications do not receive 
expensive investigations. 
Field tests. The addition of fertilizer increases crop yield. The farmer's 
profit depends on yield, costs, and sale price. Agricultural experiment 
stations help the farme;r by carrying out field trials designed to measure 
the additional yield, say of corn, for given amounts of nitrogen fertilizer. 
These trials produce curves that relate yield to amount of fertilizer, and 
farmers use these curves together with cost information and anticipated 
sales price to decide on the amount of fertilizer to use. The efficient design 
of the field trials is part of the vork of the agricultural statistician. 
Sample surveys. The use of sample surveys is not restricted to opinion 
polls. Surveys are also used by large companies to assess their inventories 
or their book value. Surveys are taken to determine xvhat and hoxv much 
mathematics is available in colleges. You may have seen figures in nexvs- 
papers estimating the number of unemployed; these come from a periodic 
governmental sample survey. To find and correct errors in the U.S. 
Census, the Census Bureau uses special sample surveys. That surveys are 
widely used instead of a complete census is partly a matter of cost in time 
and money, but partly a matter of quality. A more thorough and careful 
job can be done on a sample than on a large population. 
Many problems can be treated only by sampling methods: breaking 
strength of steel rods, life testing of vacuum tubes, and, in general, destruc- 
tive test situations. Other problems have infinite populations; there is 
no end to the number of measurements the Bureau of Standards can take 
on its platinum-iridium standard meter bars. 
Genetics and radiation. The development and testing of atomic bombs 
has led to extensive experiments on the genetic effects of radiation in insects 
and mammals. Mutation, a suddenly produced variation in the character 
1-5] THE EMPIRICAL STUDY OF ViRIABILITY 7 
of offspring, is sometimes produced by radiation. IPor example, fruit flies 
are exposed to radiation of different kinds and in different doses, and 
mutations in offspring are observed. Several sites on the fly are possible 
places for mutations. Here are typical questions: Are the different sites 
equally likely to mutate? Is the frequency of mutation proportional to 
dose? Do kinds of radiation differ in their effect? Statistical studies of 
the effects of radiation on humans are still carried on at Hiroshima and 
7Nagasaki. 
Geology. Large boulders are left scattered by a glacier. From the dis- 
tribution of the angles that the long axes of the boulders make with the 
7North, it is desired to estimate the direction of the path of the glacier. 
Other examples of statistical applications will be found throughout 
the text. 
1-5. THE EMPIRICAL STUDY OF VARIABILITY 
As we have seen, probability and statistics deal with the fluctuations 
and the regulartries in processes that have random or chance elements. 
Although we all experience such variability every day in traffic flow, in 
time taken to brush our teeth, in our on changing weight, in our ex- 
penditures for necessities, in our time used for study, and in our games 
and races, we rarely study variability systematically. Thus �e have im- 
pressions about variability, but usually no data. 
Better personal experience of probability processes can be acquired by 
doing a few experiments of a simple sort, keeping records of the results, and 
analyzing them. Some of the results will be much as expected, others a 
bit surprising. In the rest of this chapter, we study the results of some 
simple experiments that you can do, and we suggest additional ones so 
that you can gain experience with probability models and variability. You 
are asked to write dowr your initial thoughtful guess about the outcome 
in each example without peeking ahead, so that you will gain experience 
in such estimates, and so that you will honestly know whether the result 
is as you expected or not. When you are seriously wrong, you should ask 
yourself what features of the problem you did not take into account. You 
should understand that even professional mathematicians cannot solve 
all the mathematical examples without hard work, and that some cannot 
be solved without empirical data. 
First-ace problem. An ordinary deck of 52 playing cards containing four 
aces is shuffled thoroughly, and we count from the top the number of cards 
down to and including the first ace, and record the count. The process 
is repeated. 
(a) What is the average count? (Without reading further write down 
your thoughtful guess.) 
THE STUDY OF VARIABILITY ' [CHAP. 1 
TABLE 1--1 
COUNT (NUMBER OF CARDS TO AND INCLUDING THE FIRST ACE) 
FOR EACH OF 100 SHUFFLES. 
Shuffle First 20 Second 20 Third 20 Fourth 20 Fifth 20 
number counts counts counts counts counts 
1 5 17 5 7 27 
2 4 8 5 15 18 
3 29 19 8 17 11 
4 3 18 4 16 2 
5 24 20 i 9 1 
6 3 13 5 11 28 
7 3 2 24 10 17 
8 22 2 1 i 9 
9 5 19 15 21 3 
10 16 9 2 i 4 
11 i 7 3 4 17 
12 i i 18 15 7 
13 5 3 22 4 2 
14 23 6 25 26 5 
15 16 2 13 8 9 
16 6 15 11 13 6 
17 26 3 2 13 3 
18 10 4 5 11 22 
19 i 12 3 i 13 
20 32 5 22 29 1 
Total 235 185 194 232 205 
Average 11.75 9.25 9.70 11.60 10.25 
What is the probability that the first ace is on card 1 ? 2? ... 52? 
series of three dots, as used here, stands for all the whole numbers be- 
the numbers immediately preceding and following the dots.) 
Within what number of cards will we find the first ace half the time ? 
(Write down your thoughtful guess.) 
Discussion. Parts (b) and (c) of this problem are treated theoretically 
Chapter 3, but here we study the matter empirically. 
Table 1-1, we list in order in columns of 20 the results of 100 shuffles 
this experiment. We observe that the counts vary considerably, from 
32. They might have varied more--from 1 to 49--because all four 
could be clustered on the bottom of the deck. Furthermore, as you 
down a column the numbers change without much rhyme or reason. 
call such changes sampling fluctuations or sampling variation. If the 
1-5] THE EMPIRICAL STUDY OF VARIABILITY 9 
TABLE 1--2 
OBSERVED FREQUENCY DISTRIBUTION OF COUNTS AND 
THEORETICAL FREQUENCIES OF COUNTS FOR FIRST-ACE PROBLEM. 
Number of Theoretical Number of Theoretical 
Count Count 
times observed frequencies times observed frequencies 
i 11 7.7 21 i i 7 
2 7 72 22 4 15 
3 9 6.8 23 i 1.3 
4 6 6.4 24 2 i 2 
5 9 6 0 25 i 1.1 
6 3 5.6 26 2 1.0 
7 3 5.2 27 i 8 
8 3 4.9 28 i .7 
9 4 4.6 29 2 .7 
10 2 4.2 30 o .6 
11 4 3.9 31 0 .5 
12 1 3.6 32 1 4 
13 5 3.4 33 0 .4 
14 0 3.1 34 0 .3 
15 4 2 9 35 0 .3 
16 3 2 6 36 0 .2 
17 4 2.4 37 0 .2 
]8 3 2 2 38 0 1 
19 2 2 0 39 0 .1 
20 i 1.8 40-49 0 3 
100 99.9 
shuffling is thoroughly done, knowledge of one count is no help in predict- 
what the next xvill yield. 
That there is order in this chaos is suggested by the stability of the 
column totals and column averages. The averages vary only from 9.25 to 
11.75. The changes in the average are much less than the changes from one 
count to the next. 
We summarize these data in a frequency distribution in Table 1-2, by 
obtaining the number of times each count occurred in our 100 shuffles. For 
example, the count i occurs 11 times in Table 1-1. We also give the 
theoretical frequency, computed from a probability model for this problem. 
We defer the calculation of such theoretical frequencies to Chapter 3, but 
here we can compute the probability that the count is 1. There are 4 aces 
out of 52 cards that can be on top of the deck, and any one of these yields 
count of 1. So it is natural to say that the probability that an ace is on 
10 THE STUDY OF VARIABILITY, [CHAP. 1 
top is  or x-. Since we had 100 shuffles, the theoretical frequency is 
oo  7.7. (The symbol  means "is approximately equal to.") Of 
course we cannot have 7.7 counts of 1; that is the long-run rate. per 100 
counts if thousands of trials are made. We call it the theoretical or expected 
frequency, and discuss it in Chapter 5. The theoretical frequencies of 
counts from 40 to 49 are each less than 0.05, so their sum, 0.3, is reported. 
We observe that the frequencies for theoretical counts do not match 
those of the observed counts exactly, but that they have the same general 
trend, i.e., they decrease as the size of the count increases. The dis- 
crepancies you see between the observed and theoretical frequencies are 
part of the experience this chapter can give. There are txvo sources for 
such discrepancies--sampling variation and failure of the theoretical proba- 
bility model to fit the facts of real-life shuffling and counting. The counting 
was carefully checked, but it is harder to check the shuffling. 
The authors have made a statistical study of these counts, and they 
find o evidence of disagreement between the theoretical model and the 
actual data. The study is not presented here. 
Let us return to our first question' 
(a) What is the average count? 
Our total for the five sets is 
235 + 185 -}- 194 ,4- 232 -- 205 = 1051, 
so the average for 100 counts is 10.51. This is close to the theoretical 
value, as we now show, using considcratious of symmetry. But the reader 
needs to take the argument partly on faith. 
The 4 aces break the rest of the pack into 5 parts, as show in Table 1-3. 
Any part may have from 0 to 48 cards in it. It seems reasonable (and it can 
be proved) that all 5 parts have the same long-run average count. In 
Table 1-3, we show opposite each part its average count for 20 new 
shufflings. 
TABLE 1--3. AVERAGE COUT. 
Observed average 
for 20 shufflings 
Part 1: cards before the first ace 9.75 
Part 2: cards between the first ace and the second 12.55 
Part 3: cards between the second ace and the third 6 65 
Part 4: cards between the third ace and the fourth 9.40 
Part 5: cards after the fourth ace 9.10 
1-5] THE EMPIRICAL STUDY OF VARIABILITY ] 
If it is true that the 5 parts have the sanhe long-run average, then 
4s 9.6 is the theoretical count for a part. When we counted to the 
5 -- 
first ace we included the ace, so the expected count including the first 
ace is 9.6 q- 1 ---- 10..6. This theoretical number is very near our average, 
10.51, for 100 hands. However, agreement this close is unexpected. 
Application of large-sample theory shows that one-third of the repetitions 
of the 100-hand experiment would produce averages that deviate more 
than 0.85 from 10.6, and that 5% of the repetitions would be more than 
1.7 from it. 
The theoretical numbers of Table 1-2, when divided by 100, answer 
question (b)' 
(b) What is the probability that the first ace is on card 17 2? ... 52? 
Our final question was 
(c) Within what number of cards will we find the first ace half the time? 
Table 1-2 shows that 51 times in 100 trials we observed the first ace at 
a count of 8 or less. So we could use the number 8 as an estimate of the 
answer to (c). Since we are also give the expected frequencies, we can 
add them starting from a count of 1 and continuing until the sum of the 
frequencies is 50% or more. Doing so, we find the theoretical answer to 
be 9; the total theoretical frequency for counts of 9 or less is 54.4%. We 
call this count 9 the median count, to distinguish it from the meaa count 
of 10.6. 
We next consider an example of an entirely different nature. 
Distribution of word-length. What is the average length of xvords, mea- 
sured in letters, used in sports reporting? (Write down a thoughtful guess.) 
Solution. We show in Table 1-4 the results for a sample of 50 words 
from one newspaper article on baseball. Naturally, a more extensive 
sample would be needed for firm conclusions. 
TABLE 1--4. DISTRIBUTION OF VORD-LENGTtt IN SPORTS ARTICLE. 
Length in letters Frequency Length in letters Frequency 
i i 7 5 
2 6 8 2 
3 12 9 3 
4 7 10 0 
5 7 11 1 
6 5 12 1 
Total 50 
12 THE STUDY OF VARIABILITY ' [CHAP. 1 
TABLE 1--5. FREQUENCY DISTRIBUTION OF LAST 
DIGITS FROM 100 TELEPHONE NUMBERS. 
Digit Frequency 
0 11 
1 13 
2 ll 
3 11 
4 10 
5 5 
6 7 
7 14 
8 8 
9 10 
100 
The sum of the lengths for the 50 words can be obtained by multiplying 
each length by its frequency and adding these products to obtain a sum 
of 243. The average length is 243/50  4.9. We observe that three-letter 
words are most frequent, and that about half the words are 1, 2, 3, or 4 
letters long. 
Last digits of phone numbers. From a telephone book, find the frequency 
distribution of the last digits for 100 phone numbers. (Write down your 
guess for the frequency distribution.) 
Solution. Many people expect the digits 0, 1, . . ., 9 to be about equally 
frequent. Table 1-5 gives the results for one sample of 100. We observe 
that the digits are about equally frequent, as people expect. Of course, 
this is only one sample of 100. 
Distribution of .first digits. Find the frequency distribution of first (left- 
most) digits in counts of votes for a given candidate by some unit of popu- 
lation, such as state, county, or precinct (or in physical measurements 
such as areas of states, heights of mountains, or the first significant digits 
in physical constants). In the number 345, 3 is the first significant digit, as 
it is also in 0.00345. 
Solution. Most people guess that the digits 1, 2,..., 9 are about equally 
frequent. Write down your guess. Table 1-6 gives the first digits of counts 
of votes for Eisenhower in counties in Illinois in the 1956 presidential 
1-5] THE EMPIRICAL STUDY OF VARIABILITY 13 
TABLE 1--6. I2REQUENCY DISTRIBUTION OF FIRST 
DIGITS FOR VOTING STATISTICS. 
1 24 
2 14 
3 11 
4 16 
5 11 
6 
7 5 
8 4 
9 5 
102 
election. We observe that l's are quite frequent, and that the low numbers 
are much more frequent than the high ones. Note that the digits 7, 8, 
and 9 together, instead of representing � of the total or 34, show only 14. 
A number of scientific papers have set up probabfiity models to explain 
this phenomenon--unexpected for most of us. The high frequency for 
the low numbers is said to have been first pointed out by a man who 
observed that the early pages of a well-used table of logarithms were much 
dirtier than the late pages. He decided on this evidence that first digits 
were most frequently small, and counts on a large variety of measures 
have borne him out. 
Random walk. Suppose a man stands facing north and tosses a coin to 
decide whether to take one step north or one step south. Suppose he con- 
tinues tossing and stepping in this manner for 25 steps. 
(a) On the average how far is he from his starting point? (�Vritc dovn 
a thoughtful guess.) 
(b) On how many steps is he on the north side of his starting point; 
on how many is he on the south side? 
(c) Itow often does he return to the starting point during the walk? 
(Write down your guess.) 
Discussion. These are difficult mathematical problems, but we can 
simulate the experiment by tossing a coin 25 times and counting steps 
north and south. (Alternatively, we could use last digits from telephone 
numbers, using odd numbers to represent a step north and even numbers 
for south. Or we could use the random digits given in Table I at the back 
THE STUDY OF VARIABILITY, [CHAP. 1 
TABLE 1--7 
RESULTS FOR 10 RANDOM WALKS OF 25 STEPS. 
1N T AND S INDICATE ])q'ORTH AND SOUTH. 
Final Times on Times on Times 
Walk 
position north side south side at origin 
I 1S 16 5 4 
2 7N 19 3 3 
3 5N 20 2 3 
4 9N 24 0 1 
5 1N 4 15 6 
6 5S 6 11 8 
7 5S 7 12 6 
8 3S 2 19 4 
9 ]]N 25 0 0 
10 3N 9 15 1 
Sum of distances 50 Totals 132 82 36 
of the book in the same manner.) Table 1-7 shows the results for 10 walks 
of 25 steps. 
The column totals show that the average distance from the origin is 
50/10 = 5. A theoretical answer from advanced work is about 4. (For 
n steps, the theoretical mean distance is about 0.8v/-, for large n.) 
We note that there is considerable imbalance between time spent on the 
north and on the south. But the symmetry of north and south and of 
heads and tails shows us that in the long run, over many walks, half the 
time will be spent on each side of the starting point. The imbalance of 
an average of 13.2 stops on the north versus 8.2 on the south lnUSt therefore 
be due to large sampling fiuctuatious. Note that oil walk 9 all 25 stops 
were on the north; on walk 4, 24 out of 25 were on the north, and on xvalk 
8, 19 were on the south. [Vc seem to have discovered that instead of each 
walk being split about equally--about half on the north, and half on the 
south--a very substantial fraction of the time is likely to be spent on one 
side in any one walk. This surprising result is not a feature of the smallness 
of the total number of steps taken nor of an unusual sample. It is a general 
feature of this kind of random-xvalk problem. 
Finally, the average number of returns to the origin was observed to 
be 36/10 = 3.6. Advanced theory gives about 3.0 as the theoretical 
mean. 
Random digits. You may like to see the magnitudes of departures from 
expected frequencies observed in a table of random numbers, entitled 
1--5] THE EMPIRICAL STUDY OF VARIABILITY 15 
TABLE 1--8. FREQUENCIES OF RANDOM DIGITS. 
Frequencws in first Frequencws in a 
Digit block of 50,000 million digits 
0 4923 99,802 
i 5013 100,050 
2 4916 100,641 
3 4951 100,311 
4 5109 100,094 
5 4993 100,214 
6 5055 99,942 
7 5080 99,559 
8 4986 100,107 
9 ] 4974 99,280 
A Million Random Digits, made by The Rand Corporation and published 
by The Free Press, Glencoe, Illinois. The second column in Table 1-3 
gives the frequencies of the digits 0, 1, ..., 9 in the first block of 50,000 
random digits in the table; the expected frequency for each digit is, 
of course, 50,000/10 = 5000. The third colunto gives the frequencies for 
the million digits, the expected frequencies being each 100,000. 
Large-sample theory suggests that about  of the observed frequencies 
for the 50,000 blocks should be within 67 of the expected frequency, and 
that about  of the observed frequencies for the million digits should be 
within 300 of the expected frequencies. In both instances, 6 digits have 
frequencies within the interval where  (or 6.7 digits) are expected, so the 
agreement is close. 
EXERCISES FOR SECTION 1-5 
1. Obtain a deck of ordinary playing cards and, after thorough shuffling, 
count the number of cards down to and including thc first ace; record the count 
for five shuffles. Get the average count for the five shuffles and compare it with 
the theoretical value of 10.6. 
2. For each of five shuffles of an ordinary deck of playing cards, record the 
counts of the cards before the first ace, betwecn the first ace and thc second, 
and so on, as in Table 1-3. Then get the averages for each part as in the final 
column of Table 1-3, and compare the results with 9.6. 
3. Obtain a frequency distribution for the lengths of the first 50 words in a 
sports article in a newspaper, and compare the mean word-length ith that 
obtained from Table 1-4. 
4. Open a residential telephone book to any page, and obtain the frequency 
distribution of the last digit for 25 telephone numbers. Find the average value 
of the last digit and compare it with 4.5, the theoretical value if all digits are 
equally likely. 
16 THE STUDY OF VARIABILITY � [CHAP. 1 
5. From an almanac, or other source, obtain the distribution of leftmost digits 
of areas of states of the United States of America (or populations) and compare 
the distribution of digits with that of Table 1-6. 
6. From a chemical or physical handbook, obtain the frequency distribution 
of the first significant digits of 50 physical constants. 
7. Use the random digits of Table I at the back of the book to carry out 4 
random walks of size 25, like those described in the text. Use your data to 
answer the three questions in the text. 
8. Use the random numbers of Table I at the back of the book to make 10 
random walks of length 10'steps each, and use these results to answer the three 
random-walk questions in the text (for walks of length 10). 
9. Obtain the frequency distribution of the digits 0, 1,..., 9 of the 50 random 
numbers in the first 5 columns and first 10 rows of the random digit Table I at 
the back of the book. 
10. Split an ordinary pack of playing cards into two packs, the reds and the 
blacks. Lay out the reds in a row (in order A, 2, 3, ..., 10, J, Q, K of diamonds, 
then A, 2, 3, ..., K of hearts). Shuffle the blacks and lay them out beneath 
the reds. Then count the number of times the value of a black card matches 
that of the red. Repeat 5 times, and obtain the average number of matches for 
the 5 shuffles. Make a thoughtful guess at the theoretical average number 
of matches. 
11. Open a novel to a pagc near the middle, and choose the first 10 full lines 
of text. Record the number of e's in each line, and get the average number of 
e's per line. Use the letter count from one line as a base, and estimate the percent 
of letters that are e's. 
12. Consider the duration-of-play problem, Section 1-3, with m = 3, n = 2. 
By flipping a coin (or using random numbers, Table I) and scoring a point for 
the player starting with m units when a head appears, and one for the other 
player when a tail appears, play 10 games, recording the total tosses required 
for each game, and the winner. (a) Find the average number of tosses per 
game. (b) Compute the fraction of games won by the player starting with m 
units, and compare it with 0.6. 
13. Refer to the problem of points, Section 1-3. Use a coin toss (or random 
numbers, Table I) to simulate the finish of the game 20 times. Compare the 
number of times the player with 2 points won to the remaining number. Are the 
numbers approximately in the ratio 3 to 1 as Pascal thought? 
14. Record the number of rolls of a die before a 6 appears. Repeat the experi- 
ment 10 times, and obtain the average number of rolls required. 
15. Record the total number of rolls of a die required before every face has 
appeared. Repeat the experiment 5 times, and obtain the average number 
of rolls. 
16. Simphfied epidemic. An infectious disease has a one-day infectious period, 
and after that day the patient is immune. Six hermits (numbered 1, 2, 3, 4, 5, 6) 
live on an is]and, and if one has the disease he randomly visits another hermit 
for help during his infectious period. If the visited hermit has not had the disease, 
he catches it and is infectious the next day. Assume hermit 1 has the disease 
today, and the rest have not had it. Throw a die to choose which hermit he 
1-6] Do PROBABILITIES GROW? 17 
visits (ignore face 1). That hermit is infectious tomorrow. Then throw again 
to see whom he visits, and so on. Continue the process until a sick hermit visits 
an immune one and the disease dies out. Repeat the experiment 5 times and 
find the average number who get the disease. 
17. Server problem. In a unit of time there zs a 50:50 chance that a customer 
appears at a counter to be served. If others are ahead of him at the counter he 
lines up in the queue, otherwise the server serves him and takes 2 units of time 
to complete the service. In the 10th unit, what is the average number in the 
queue if the process starts with no customers at the counter? Use a coin to carry 
out the experiment 5 times and get the average number. Also get the average 
number served at the dose of the 10th unit of time. Example (a, stands for a 
customer who arrived in the ith time interval): 
Time unit 1 2 3 4 5 6 7 8 9 10 
Arrivals al a2 -- -- -- a6 a7 a8 a9 a10 
Being 
served al al a2 a2 a6 a6 a7 a7 a8 
In line al al, a2 a2 a2  -- a6 a6, a7 a7, as a7, as, a9 as, a9, a10 
Total served: 4; number in queue in 10th unit: 3. 
1-6. DO PROBABILITIES GROW? 
Most people correctly believe that vhen a fair coin is tossed many times, 
the fraction of heads will be close to �. Some feel that a logical consequence 
is that, after 10 heads have appeared in a row, the probability of a tail 
is larger than before. This view stems from a misapprehension about the 
way the "law of averages" works for coins. Since the coin has neither 
memory, conscience, nor force of its own, it can scarcely change its proba- 
bility. The great probabilist Feller puts the explanation succinctly. He 
says that the law of averages works by swamping rather than by com- 
pensation. Thus, if a set of tosses started with 10 heads, the 10 will be 
largely swamped after 1000 tosses, and negligible after a million. 
One reason for believing that probabilities grow is that in some problems 
they do. Can you think of such a problem? In the first-ace problem, if 
we have dealt 30 cards without an ace, the probability of an ace on the 
next card is large, 4 � 
2, and after 48 cards without an ace, the probability 
of an ace is 1. This growth happens because we draw without replacement 
from the pack, and the composition of the population has changed. But 
when a coin is tossed, there is no sense in which we have used up a head 
from a finite pool of heads. The model of drawing without replacement is 
the wrong one for coins. 
18 THE STUDY OF VARIABILITY [CHAP. 1 
In some problems, superficially like the fair-coin problem, probabilities 
change from time to time. At the start of the season, a pitcher may not 
be in good physical condition, and his probability of throwing a strike 
may be low. But later he may improve. Still later a small injury may 
plague him. For this pitcher, one may well believe that the probability 
of throwing a strike will wax and wane with time. But simple forms of 
the law of averages are not readily applicable to such a complicated 
process. 
CHAPTER 
PERMUTATIONS, 
COMBINATIONS, AND THE 
BINOMIAL THEOREM 
2-1. PERMUTATIONS: THE MULTIPLICATION PRINCIPLE 
For generations, people have been intrigued by problems requiring them 
to find the number of ways of arranging a set of objects. In how many 
ways can 12 people line up at a theater box office? How many automobile 
license plates can be made using 2 letters followed by 3 digits? In how 
many ways can a person take a walk of 9 blocks, if he always walks 5 
blocks west and 4 blocks north? Although such questions are fascinating 
and challenging in their own right, we shall consider them for an addi- 
tional reason: we often need answers to such questions in the study of 
probability. 
We wish to discover a general principle that will enable us to find the 
number of possible arrangements of sets of objects. To this end, let us 
consider an example. 
EXaMP,E 1. In how many ways can 3 books, denoted by A, B, and C, 
be arranged in order on a shelf? 
Solution 1. One way to solve this problem is to list the possible arrange- 
ments and count them. A tree graph (Fig. 2-1) provides an organized 
way of listing the arrangements so that none is missed. 
The initial point, or origin, is denoted by 0. If we follow all possible 
branches from 0 to the right-hand edge of the tree, we get the 6 possible 
arrangements listed in the column on the extreme right. Note that the 
tree diagram takes order into account. Thus ABC and ACB count as 
different ar,'angements of the 3 books because they are in different orders. 
Order is the essence of such arrangements; a change in order yields a different 
arrangement. 
19 
20 PERMUTATIONS, COMBINATIONS, BINOMIAL THEOREM [CHAP. 2 
Book in Book in Book n Possible 
1st space 2nd space 3rd space arrangenents 
.t ' B C ABC 
C B .tCB 
/ A C BAC 
Ongm 0 B  
  C A BCA 
 .1 B ' CAB 
C  B A ' CBA 
Fro. 2-1. Tree for arrangements of 3 books. 
Solution 2. A more convenient solution to this example is suggested by 
a further study of the tree diagram The reasoning is as follows: 
The problem requires us to fill 3 spaces, which can be represented as 
I 
In the first space, we can put A or B or C. Hence the first space can be 
filled in three ways: 
(This is indicated on the tree graph by the 3 branches from 0 that end at 
the column headed Book in 1st space.) For each of the 3 ways of filling 
the first space, we have 2 ways of filling the ,second space, because either 
of the 2 remaining books can be used: 
Thus, we can fill the first 2 spaces in 3 X 2, or 6, ways. (Note that 6 
branches of the tree end at the column headed Book in 2nd space.) For 
each of the 6 ways of filling the first 2 spaces, we have one way of filling 
the third space, becau tuffy one book remains. Therefore, we can fill 
the 3 spaces in 6 X 1, or 6, ways. (Note that 6 branches of the tree end 
at the column headed Book in $rd space.) We can indicate the number 
of ways of filling each of the 3 spaces thus: 
.12111 
And, as indicated by the tree, we can obtain the total number of arrange- 
2-1] PERMUTATIONS: THE MULTIPLICATION PRINCIPLE 21 
merits by multiplication: 
3X2X1----6. 
To this point, we have used the word "arrangements" to describe order- 
ings of objects that result from operations such as that of placing books in 
a line. "Arrangement" is a common word that is ifformally descriptive. 
But we are dealing vith special kinds of arrangements: we are concerned 
with arrangements, or orderings, of objects in a line, not with other kinds 
of arrangements, such as those of flowers in a vase. Since we refer to a 
special kind of arrangement, we need, for more precise description, a 
special word. This special word is permutation. 
Each of the six arrangements in the foregoing example is called a per- 
mutation of the three books. We say that there are six permutations of 
the three books, taken three at a time, or all together. 
2-1 Definition. Permutation. A permutation of a number of objects is 
any arrangement of these objects in a definite order. 
To "permute" a set of objects means to arrange them in a definite order. 
EMPLE 2. If at least 3 copies each of book A, book B, and book C 
are available, in how many distinguishable ways can we arrange 3 of the 
books on a shelf? (Regard the copies as indistinguishable.) 
Solution. With at least 3 copies of each book available, we can now have 
arrangements like AAA or ABA. Because the copies are indistinguishable 
in appearance, even though they are composed of different molecules, 
one arrangement of 3 copies of book A is indistinguishable from any other 
arrangement of those copies, or of any other 3 copies of book A. However, 
the arrangements ABA and AAB are distinguishable. By reasoning simi- 
lar to that used in Example 1, we can show that each of the 3 spaces can 
now be filled in 3 ways. The choices are indicated thus: 
[ 3 [ 3 ] 3 ] 
As before, the total number of permutations is found by multiplication: 
3X3X3=27. 
A short cut in counting. When the number of objects in a set is large, 
the number of permutations of the objects cannot, without great labor, 
be found by listing and counting. Fortunately, the method of reasoning 
suggested by the tree graph and used in Examples I and 2 can be extended, 
and used to provide a convenient general method for dealing xvith problems 
in permutations. 
22 PERMUTATIONS, COMBINATIONS, BINOMIAL THEOREM [CHAP. 2 
2-2 The multiplication principle. If an operation can be performed in 
n ways and, after it is performed in any one of these ways, a 
second operation can be performed in n2 ways and, after it is 
performed in any one of these ways, a third operation can be per- 
formed in n3 ways, and so on for lc operations, then the/c operations 
can be performed together in 
n X n2 X n3 X ... X nk ways. (1) 
A note on notation. We have used subscripts on the letter n, along vith 
three dots, to indicate a set of variables of arbitrary length. This device 
may seem complicated, but some such method is necessary. All the letters 
in the English alphabet would denote only 26 variables, but the subscripts 
and the three dots enable us to denote any finite number. 
Observe the special function of the three dots. They indicate that we 
are to begin with the factor n and write additional factors until we reach 
the/cth factor, nk. The dots do not imply that/c is greater than 3. If, for 
example, lc ---- 2, then expression (1) becomes 
nl X n2, 
and if k = 1, the expression means simply n. 
The need for subscripts becomes apparent if we try to get along without 
them when the number of variables is large, or indefinite. If we denote a 
set of variables of arbitrary number by 
a, b, c, . . . , h, 
a little thought shows that this notation implies eight variables, not an 
arbitrary number. When we become famihar with the use of subscripts, 
we appreciate their convenience and usefulness. (See Appendix II.) 
A trce diagram to illustrate the mdEplcatio priciple. The tree in 
Fig. 2-2 illustrates the multiplication principle for n = 2, n2 ---= 3, and 
n3 = 2. The total number of paths along branches of the tree, from the 
origin 0 to the right-hand edge of the diagram, is 
n:t X n2 X n3 = 2 X 3 X 2. 
Examples such as the foregoing make the multiplication principle in- 
tuitively evident. We shall in future accept its truth, and use it freely as 
a short cut in counting the number of permntations of sets of objects. 
ote that the multiplication principle takes order into account. 
PERMUTATIONS: THE MULTIPLICATION PRINCIPLE 23 
1 
1 2 ---- 2 
1 
2 
1 
 2 
2 
FIG. 2-2. Tree illustrating multiplication principle. 
EMPLE 3. How many license plates can be made using 2 letters 
followed by a 3-digit number? 
Sol,tion. There are 5 spaces to fill. The first space can be filled with 
any one of 26 letters, and so in 26 ways. After the first space has been 
filled in any one of these ways, the second space can be filled in 26 ways 
(repetitions of a letter allowed). Similarly, the third space can be filled in 
9 ways (zero not allowed), the fourth space in 10 ways, and the fifth space 
in 10 ways (zero and repetitions of a digit allowed). By the multiplication 
principle, the answer is 
26 � 26 � 9 � 10 � 10 ---- 608,400. 
EXAMPLE 4. In planning a round trip from Chicago to Southampton 
by way of New York, a traveler decides to travel between Chicago and 
New York by air and between New York and Southampton by steamship. 
If there are 6 airlines operating between Chicago and New York, and 4 
steamship lines operating between New York and Southampton, in how 
many ways can the round trip be made without traveling over any line 
twice ? 
Solution. The trip from Chicago to New York can be made in 6 ways; 
after it has been made in any one of these ways, the trip from New York 
to Southampton can be made in 4 ways. Then the trip from Southampton 
to New York can be made in 3 ways, after which the trip from New York 
24 PERMUTATIONS, COMBINATIONS, BINOMIAL THEOREM [CHAP. 2 
to Chicago can be made in 5 ways. By the multiplication principle, the 
number of possible ways of making the round trip is 
6 X4 X 3 X 5 = 360. 
EXAMPLE 5. Given the digits 1, 2, 3, 4, and 5, find how many 4-digit 
numbers can be formed from them (a) if no digit may be repeated, (b) if 
repetitions of a digit are allowed, and (c) if the number must be odd, with- 
out any repeated digit. 
Solution. (a) No repetitions. There are 4 places to fill. The first place 
can be filled with any one of the 5 digits, and so in 5 ways. Then, since 
no digit may be used more than once, the second place can be filled with 
any one of the remaining digits, and so in 4 ways. Similarly, the third 
place can be filled in 3 ways, and the fourth place in 2 ways. From the 
multiplication principle, it follows that the number of 4-digit numbers is 
5X4X3X2---- 120. 
(b) Repetitions allowed. If repetitions of a digit are allowed, each of the 
4 places can be filled with any one of the given 5 digits, and so in 5 ways. 
The number of 4-digit riumbers, with repetitions allowed, is therefore 
5 X 5 X 5 X 5---- 625. 
(c) Odd, without repetitions. If the number must be odd, the final digit 
has to be 1 or 3 or 5. Therefore the fourth place can be filled in 3 ways. 
After this has been done in any one of these ways, the remaining places 
can be filled in 4 ways, 3 ways, and 2 ways, respectively, since no digit 
may be used more than once. The number of odd, 4-digit numbers, without 
repeated digits, is 
4X3X2X3----72. 
NOTE. We filled the fourth place first. If some operation must be per- 
formed in a special way, it is usually advisable to do it .first. However, for 
nonspecial operations, the order in time of the space filling is often arbi- 
trary. Thus in part (c) of the foregoing example, once the fourth space is 
filled, it doesn't matter vhich of the three remaining spaces is filled next. 
Similarly, in Example 1, it doesn't matter which space on the shelf is 
filled first. We can put a book in the middle space, then put a book to its 
left, and then one to its right. The multiplication principle still applies 
and gives the same answer as before. We think of the first operation as 
that of placing a book in the middle space; the second, as that of placing 
a book in the leftmost space; and the third, as that of placing a book in 
the rightmost space. It helps us in analyzing the problem if we think 
2-1] PERMUTATIONS: THE MULTIPLICATION PRINCIPLE 25 
of performing a definite sequence of operations one after another, even 
though we might do things in a different order. In fact, the three books 
can all be put on the shelf at the same time, rather than one after another; 
but such a way of looking at the problem provides no insight into its 
solution, whereas the one-book-after-another approach does. 
EXAMPLE 6. Bill's Pizza Palace offers pepper, onion, sausage, mush- 
rooms, and anchovies as toppings for the plain cheese base of the pizzas. 
How many different pizzas can be made? 
Solution. There are 5 ingredients. In adding a topping to the base, 
we deal with the available ingredients one at a time. The pepper can be 
dealt with in 2 ways--take it or leave it. After the pepper has been dealt 
with, we can dispose of the onion in 2 ways--take it or leave it. Similarly, 
each of the 5 toppings can be dealt with in 2 ways. Therefore, there are 
25, or 32, possible pizzas, including the plain pizza and the one with 
everything. 
The addztion principle. Consider two operations, one of which can be 
performed in m ways and the other in n ways. Then the multiplication 
principle says: If, after the first operation is performed in any one of the 
m ways, the second operation can be performed in n ways, the two opera- 
tions can be performed together in mn ways. In short, the multiplication 
principle is concerned with situations where we can perform the first 
operation and then perform the second. 
A different situation is faced if we wish to perform the first operation or 
the second operation, not both. Consider the following example. 
EXtMPLE 7. Three different flag are available. In how many ways can 
a signal with at least 2 flags be arranged on a flagpole, if the order of the 
flags on the flagpole counts? 
Soltdion. As our first operation, let us arrange 2 flags on the flagpole 
(Fig. 2-3). By the multiplication principle, this can be done in 3 X 2, or 
o I'"'Pl o N 
FI. 2-3. Signals using 2 flags out of 3. 
26 PERMUTATIONS, COMBINATIONS, BINOMIAL THEOREM [CHAP. 2 
6, ways. As our second operation, let us arrange 3 flags on the flagpole. 
This can be done in 3 X 2 X 1, or 6, ways. 
7Now we have tuffy one signal to arrange, and this signal may be a 
two-flag signal or a three-flag signal, but not both together. It is a ques- 
tion of performing the first operation or the second, not the first operation 
and then the second. The operations are mutually exclusive: they cannot 
both occur together. The total number of signals is therefore 
6 .6 - 12. 
2-3 lhe cddition principle. If two operations are mutually exclusive, 
and the first can be done in m ways and the second in n ways, then 
one operation or the other can be done in m d- n ways. 
This principle is readily generalized to include any finite number of 
operations The statement is left as an exercise. 
EXERCISES FOR SECTION 2-I 
Use the multiplicktion principle to solve the following exercises. 
1. In how many ways can eight people line up at a theater box office? 
2. How many 5-digit numbers can be formed from the integers 1, 2, 4, 6, 7, 8, 
if no integer can be used more than once? How many of these numbers xill be 
even? How many odd? 
3. If the call letters of a broadcasting station must begin with the letter W, 
how many different stations could be designated by using only 3 letters, with 
repetitions of a letter allowed? How many by using 4 letters, without repe- 
titions ? 
4. In low many ways can 3 letters be mailed in 6 mail boxes, if each letter 
must be mailed in a different box? If the letters are not necessarily mailed in 
different boxes, how many ways are there of posting them ? 
5. There are 7 seats available in a sedan. In how many ways can 7 persons 
be seated for a journey if only 3 are able to drive? [Hint. See note following 
Example 5(e).] 
6. A passenger train has 9 coaehes. In how many ways can 4 people be 
assigned to coaches if they must ride in different coaches? 
7. In how many ways can 6 students be seated in a classroom with 30 desks? 
8. Twelve boys try out for the basketball team. Two can play only at center, 
four only as right or left guard, and the rest can play only as right or left forward. 
In how many ways could the coach assign a team ? 
9. ttow many nulnbers, each with at least 3 digits, can be formed from the 
5 digits l, 2, 3, 4, 5, if no digit may be used more than once? 
10. In how many ways can 5 boys and 5 girls bc seated alternately in a row 
of l0 chairs, numbered from 1 to 10, if a boy always occupies chair number one? 
2-2] FORMULAS FOR PERMUTATIONS 27 
11. In how many ways can 3 different presents, A, B, and C, be given to any 
3 of 15 persons? If a specified person must receive A, and if no person is to 
receive more than one present, in how many ways can the presents be dis- 
tributed ? 
12. In how many ways ean a selection of at least one book be made from 8 
different books? (Hint: See Example 6 of the text.) 
13. Given 4 flags of different eelors, how many different signals can be made 
by arranging them on a vertical mast, if at least 2 flags must be used for each 
signal ? 
14. An encyclopedia consists of nine volumes numbered 1 to 9. In how many 
ways can the nine volumes be arranged together on a shelf so that some or all 
of the volumes are out of order ? 
15. How many 5-digit numbers can be formed? How many of these begin 
with 2 and end with 4? How many do not contain the digit 5? How many are 
divisible by 5 ? 
16. How many different parties of 2 or more can be formed from 9 people? 
17. Five boys compete in a race. In how many ways can the first two places 
be taken? 
18. (a) How many subsets, including the empw and universal sets, can be 
formed from a set of 10 different objects? (b) From a set of n different objects? 
19. How many ordered pairs of symbols (x, y) can be formed if x can be re- 
placed byaor b or c, and y can be replaced by 1 or 2 or3 or 4? Drawatree 
diagram exhibiting the set of possible ordered pairs (x, y). 
20. How many permutations are there of n different objects, taken r at a 
time, with repetitions allowed? (It is assumed that there are at least r copies 
of each of the n objects available.) 
21. On stepping off a train, a man finds that he has a nickel, a dime, a quarter, 
and a half-do]lar in his pocket. In how many ways can he give the porter a tip? 
2-2. FORMULAS FOR PERMUTATIONS 
The multiplication principle provides a general method for finding the 
number of permutations of sets of objects. For some important types of 
problems, this method can be shortened by means of some convenient 
symbols and formulas that we now introduce. 
The factorial symbol. As we have seen in Section 2-1, the multiplication 
principle enables us tc establish facts such as the following: 
(1) 7 people can be arranged in a line in 
7X6X5X4X3X2X 1 ways; 
(2) 20 books can be arranged on a shelf in 
20 X 19 X 18 X ''' X 3 X 2 X 1 ways; 
28 PERMUTATIONS, COMBINATIONS, BINOMIAL THEOREM [CHAP. 2 
(3) n objects can be arranged in a line in 
n(n -- 1)(n -- 2)...3 X 2 X 1 ways; 
and so on. Once again, note that the dots do not imply that n is greater 
than 3. The dots indicate that we are to begin with the integer n and 
continue to multiply factors, each of which is one less than its predecessor, 
until I is reached. 
Problems such as the three foregoing may lead to very large numbers 
or very long sequences of factors. For convenience, therefore, we intro- 
duce a special symbol. 
2-4 Definition. n factorial. The product of all vhole numbers from 1 
to n is called n factorial, and denoted by n !. 
Thus, 
n! = n(n - 1)(n - 2) . . . 3 x 2 x1= n x (n -1)!. 
In particular, we have 
ll= 1, 
2! - 2 X 1 = 2 X 1! = 2, 
3! = 3 X 2 X 1 = 3 X 2!-- 6, 
4! = 4 X 3 X 2 X 1 = 4 X 3! = 24, 
5!---- 5X4X3X2X 1 = 5X4! = 120. 
Proceeding in this way, we can make the table of n! shown in Table 2-1, 
or the more extensive Table II in the back of the book. Table II also gives 
log n !. 
TABLE 2-1 
n I 2 3 4 5 6 7 8 9 10 
n! I 2 6 24 120 720 5040 40,320 362,880 3,628,800 
The task soon becomes laborious, because the factorials increase in size at 
a fantastic rate. The number of pernmtations of the letters of the alphabet, 
26!, is greater than 4 X 1026. 
The factoffal symbol provides a useful notation for representing large 
numbers of the type encountered in the study of permutations and re- 
lated topics. 
2-2] FORMULAS FOR PERMUTATIONS 29 
Note that 20!  20 X 19!, 
100! ---- 100 X 99!, 
(nq-1)!= (nq-1) xn!. 
EXAMPLE 1. From the multiplication principle, we can show (cf. Section 
2-1) that 50 people can form a line in 
50 X 49 X 48 X ... X 3 X 2 X 1 ---- 50! ways. 
2-5 Theorem. Permutations of n things, all together. The number of 
permutations of a set of n different objects, taken all together, is n!. 
Proof. The proof is a direct application of the multiplication principle. 
For, we have n spaces to fill. The first space can be filled with any one of 
the n objects, and so in n ways. After this has been done in any one of 
these vays, the second space can be filled vith any one of the remaining 
objects, and so in n -- i ways. Similarly, the third space can be filled in 
n -- 2 ways, the fourth space in n -- 3 vays, and so on. Therefore, by 
the multiplication principle, the number of ways of filling the n spaces is 
n(n -- 1)(n -- 2)...3 X 2 X i ---- n!. 
The number of permutations of n different objects, taken all together, 
is denoted by P. Therefore we have 
,P,---- n!. -[ [] 
We now consider permutations of n different objects in which some, 
but not necessarily all, of the objects are used. 
EXAMPLE 2. In how many ways can 3 books be chosen from 7 different 
books and arranged in 3 spaces on a bookshelf? 
Solution. The first space can be filled with any one of the 7 books, and 
so in 7 ways. After this has been done in any one of these ways, the second 
space can be filled in 6 ways. Similarly, the third space can be filled in 
5 ways. By the multiplication principle, the 3 spaces can be filled in 
7 X6 X 5 ways. 
FactoriM symbols can also be used to denote the product 7 X 6 X 5. 
For, 
7 X6X5X4X3X2X 1 
7X6X5= 
4X3X2X 1 4! 
30 PERMUTATIONS, COMBINATIONS, BINOMIAL THEOREM [CHAP. 2 
The number of permutations of 7 objects, taken 3 at a time, is denoted 
by7Pa, and its value is 7 X 6 X 5. Thus, 
7 
7Pa---- 7X 6X 5---- 4' 
NOTE. To evaluate 7Pa, we "begin with 7 and proceed for 3 factors." 
2-6 Definition. ,Pr. An arrangement of r objects, taken from a set of 
n objects, is called a permutation of the n objects, taken r at a time. 
The total number of such permutations is denoted by Pr, r _< n. 
2-7 Theorem. Permutations of n things, r at a time. The number of 
pernmtations of a set of n different objects, taken r at a time, with- 
out repetitions, is 
n! 
- (n -- r)! 
Proof. Once again, the proof is an application of the multiplication 
principle. Suppose that we have r spaces to fill and n objects from which 
to choose. The first space can be filled with any one of the n objects, and 
so in n ways. After this has been done in any one of these ways, there 
remain n -- 1 objects, any one of which can be put in the second space. 
Thus the second space can be filled in n -- 1 ways. Similarly, the third 
space can be filled in n -- 2 ways, the fourth space in , -- 3 ways, and 
so on. The pattern shows that the tenth space can be filled in n -- 9 
ways, the twenty-fifth space in n -- 24 ways and, in general, the rth 
space in n -- (r- 1) ways. From the nmltiplication principle, the r 
spaces can be filled in 
1 ---- n(n -- 1)(n -- 2)... (n -- r q- 1) ways. (1) 
The right-hand member of formula (1) consists of r factors. It takes 
another convenient form if we multiply by (n -- r)!/(n -- r)!, because 
then we can write 
= ,9!' [] (2) 
2-2] FORMULAS FOR PERMUTATIONS 31 
Formula (1) is defined for r _ n. Formulas (1) and (2) agree if r  n. 
But if r  n, formula (2) gives 
nP = 0-' 
By defining 0! to be 1, we make formula (2) hold also for the case r = n. 
The result is then identical witl that of Theorem 2-5. Moreover, if n = 1, 
the formula 
n] = n X (,z -- 1)t (3) 
becomes 1  = 1 (0). 
Hence, by defining 
we make fornmla (3) hold for n = 1. 
EMPL 3. How many 5-letter words can be formed from the letters 
of the word equations? (A "word," in this sense, means any arrangement 
of letters. It does not need to be a word in some language.) 
Sohdiom The problem is that of finding the numbcr of permutations of 
9 letters, taken 5 at a time. This number is 
P = 9 x 8 x 7 x 6 x 5 = 15,120. 
EMPUE 4. How many permutations are there of 5 cards, taken from 
a bridge deck of 52 different cards? 
Solution. From Theorem 2-7, the number is 
52P5 = 52 X 51 X 50 X 49 X 48 = 311,875,200. 
NOTE. Order counts here. Thus, 2, 3, 4, 5, 6 of hearts differs from 
2, 4, 3, 5, 6 of hearts. 
ExeuE 5. How many words can be formed from the letters of the 
word hyperbola, taken all together? In how many of these words will the 
letters h and y occur together? In how many will the letters h and y not 
occur together? 
Solution. From Theorem 2-5, thc number of ways of arranging 9 differ- 
ent letters, all together, is 9! or 362,880. Therefore, the rcquired number 
is 9! if there are no restrictions. If the letters h and y must occur together, 
it is a good idea to consider them as one letter, hy. We now have 8 differcnt 
32 PERMUTATIONS COMBINATIONS BINOMIAI THEOREM [CHAP. 2 
letters to be arranged all together. This gives 8! arrangements. However, 
in each of these arrangements, the order hy may be changed to yh, so that 
each of the 8! arrangements gives rise to two arrangements that satisfy 
the given restriction. Hence, the total number of words in which the 
letters h and y occur together is 2(8!) ---- 80,640. 
The number of words in which the letters h and y do not occur together 
is the difference 362,880 -- 80,640 ---- 282,240. 
:For solving problems involving arrangements of objects in a set, the 
reader now has available the multiplication and addition principles and 
some formulas. He must not expect all problems to yield to the direct 
application of a formula. :Flexibility is the key to the situation. Special 
problems may require formulas, the multiplication or addition principle, 
some special device, or a combination of these methods. 
EXERCISES FOR SECTION 2-2 
Note. A "word," as used in these exercises, means any arrangement of letters. 
1. Evaluate the following: 9P3, ,P, 7P7, P2. 
2. Compute Po and .interpret it. 
3. How many words can be formed from the letters of the word fragments 
(a) taken all at a time, (b) taken 8 at a time, (c) taken 4 at a time? 
4. A student has 4 examinations to write and there are 10 examination 
periods available. How many possible arrangements are there of his examination 
program? 
5. A musical concert is to consist of 3 songs and 2 violin selections. In how 
many ways can the program be arranged so that the concert begins and ends 
with a song, and neither violin selection follows immediately after the other? 
6. Prove that the number of 3-letter words that can be formed from the letters 
of the word background is the same as the number of words that can be made 
by rearranging the letters of the word ground. 
7. How many automobile license plates bearing 5-digit numbers can be made 
if no license number starts with 07 If letters of the alphabet are used in place of 
the first digit and the next digit is not 0, how many plates can be made? 
8. A passenger train consists of 2 baggage cars, 4 day coaches, and 3 parlor 
cars. In how many ways can the train be made up if the 2 baggage cars must 
come in front, and the 3 parlor cars must come in the rear? 
9. If there are 3 roads from town A to town B, and 4 roads from town B to 
town C, in hov many ways can one make a trip from A to C by way of B, and 
return from C to A by way of B? 
10. In how many ways can 7 books be arranged on a shelf (a) if 2 specified 
books must always be side by side, and (b) if these 2 books must not be side 
by side? 
11. In geometry, polygons are commonly labeled by placing letters at their 
vertices. How many ways are there of labeling a triangle with letters of the 
2-3] �OMm^TONS 33 
alphabet? How many ways are there of labeling a pentagon? A deeagon? (Do 
not multiply out the answers.) 
12. How many 5-letter words can be made from 10 different letters (a) f any 
letter may be repeated any number of times, (b) if repetitions of a letter are not 
allowed? (e) In how many of the words of (a) ill repeated letters actually 
occur? 
13. How many 3-letter words can you make from the letters in your last 
name, if the words must begin and end with different consonants and have a 
vowel in the middle? 
2-3. COMBINATIONS 
In order to study the distinction between a permutation and a com- 
bination, we shall consider an example. 
ExMP, 1. In how many ways can a reader select 3 books, without 
regard to their order, from a set of 4 different books denoted by A, B, C, 
and D? 
Solution. We have seen that the number of permutations of 4 different 
books, taken 3 at a time, is 
4P3  4 x 3 x 2: 24. 
In these permutations, or arrangements, the order of the books counts. 
An entirely different problem arises if we wish to make a selection of 
3 books from A, B, C, and D without taking order into account. There 
are then only 4 possible selections: 
ABC, ABD, ACD, BCD. (1) 
For example, we do not list ACB because the selection ACB is the same 
selection as ABC, since order does not count. 
The word "selection" is a good, everyday word that describes the out- 
come of the operation we are considering. However, since we are dealing 
with a special kind of selection that is not concerned with the order of 
objects, we need a special word. (A similar situation occurred in connec- 
tion with the word "arrangement" as used to describe a permutation.) 
Each selection in the list (1) is called a combination of the 4 books taken 
3 at a time. The total number of such combinations is denoted by 
4C, orby (), 
34 PERMUTATIONS, COMBINATIONS, BINOMIAL THEOREM [CHAP. 2 
each of which is read "number of combinations of 4 things taken 3 at a 
time." The symbol (]) has no bar in the middle; it is not a fraction. By 
counting items in the list (1), we see that 
The foregoing example underlines the difference between a permutation 
and a combination: 
In a permutation, order counts; 
in a combination, order does not count. 
Practical considerations. Ordinarily, we must decide from the nature of 
the problem whether permutations or combinations are involved. The 
decision hinges on the answer to the question: Does order count or doesn't 
it? For example, if we are arranging 3 books on a shelf, it is natural to 
regard ABC and ACB as different arrangements, and to take order into 
consideration; permutations are involved. But if we are selecting 3 books 
for weekend reading, ABC and ACB are regarded as the same selection; 
order does not count, .and combinations are involved. Likewise two men, 
X and Y, can line up in 2 ways: XY or YX. But these two men can form 
a committee of two in only one way, because XY and YX yield the same 
committee. Order counts in a line-up; order does not count in a com- 
mittee of the usual type, unless it matters which member is chairman. 
Subsets 4 a given set. The language of sets can be used in discussing 
Example 1. We talk about subsets of 3 elements that can be formed 
from the set 
For brevity, we sometimes e11 a subset of 3 elements  3-subset. Thus we 
sy that the number of 3-subsets in the given 4-set is 4. 
2-8 Definition. Combinations. A combination is a selection of objects 
considered without regard to their order. A subset of r objects 
selected without regard to their order from a set of n different 
objects is called a combination of the n objects, taken r at a time. 
The total number of such combinations is denoted by 
nC, orby (;), where r  n. 
Alternatively, we say that the number of r-subsets in a given n-set is 
,C, or (;'). We must nmv find out how to evaluate these symbols. 
2-3] COMBIXATIONS 35 
E;ahtation of Cr, or (). Consider (1), the list of possible selections of 
3 books from 4. By rearranging, we get 6 permutations from each of the 
4 selections 
ABC, ABD, ACD, BCD, 
since each 3-subset can be arranged in 3! vays. This operation yields a 
total of 4(3 !) or 24 permutations, as listed in Table 2-2. 
TABLE 2--2 
4C3 AND 4P3 . EACH COMBINATION HAS 3!  6 PERMUTATIONS. 
Combinations Permutations 
ABC ABC, ACB, BAC, BCA, CAB, CBA 
ABD ABD, ADB, BAD, BDA, DAB, DBA 
ACD ACD, ADC, CAD, CDA, DAC, DCA 
BCD BCD, BDC, CBD, CDB, DBC, DCB 
It is evident that all 24 permutations of the 4 books, taken 3 at a time, 
are obtained by thus rearranging the combinations. In other words, 
(number of combinations) X 3! = (number of permutations). 
Or, in symbols: 
4C3 X 3!  
Thus 
A generalization of the foregoing reasoning enables us to evaluate nC. 
or (�). 
2-9 Theorem. Combinations of n things, r at a time. The number of 
combinations of a set of n different objects, taken r at a time, is 
() n! 
Cr = ---- r!(n -- r)l' (2) 
36 PERMUTATIONS, COMBINATIONS, BINOMIAL THEOREM [CHAP. 2 
Proof. Each combination of r objects can be arranged in r! ways, and 
therefore gives rise to r! permutations. Hence, r! permutations of each of 
the Cr combinations yield Cr X r! permutations. Moreover, the number 
C X r! is the total number of permutations, since each permutation of 
r objects arises from some combination of r objects. Therefore, 
n! 
C x r!---- Pr (n -- r)! 
Or, dividing by r!, we get 
(nr) [] 
C---- ---- r!(n--r)!' 
By direct application of formula (2), we obtain 
100 100 X 99 
ooC2 -- 2!98! -- 1 X 2 -- 4950, 
n! 
() -- l!(n -- 1)!- n' 
(n n) n! 
-- n!(n -- n)! - O! = 1. 
2-10 Corollary. The number of combinations of n things taken n -- r 
at a time is the same as the nfimber taken r at a time: 
(n- r)- (n- r)!r]-- ()' (3) 
Proof. The denominator of the middle term of Eq. (3) can be rearranged 
to give (�). [] 
Discussion. That the number of combinations of n objects taken 
(n -- r) at a time is the same as the number taken r at a time is not 
surprising. For whenever we select r objects from the n, we leave (n -- r) 
objects behind. Thus, 
(95) = () and (550) = (50. 
\45/ 
2-3] COMBINATIONS 37 
EX-iMrLE 2. In how many ways can a hand of 13 cards be selected 
from a standard bridge deck of 52 cards? 
Solution. The number of ways of selecting 13 cards from a deck of 52 
different cards is given by formula (2): 
13 -- l!q!- 635,013,559,600. 
EMPLE 3. In how many ways can a committee of 3 be chosen from 
4 married couples (a) if all are equally eligible, (b) if the committee must 
consist of 2 women and 1 man, (c) if a husband and wife cannot both 
serve on the same committee? 
Soltion. (a) In a committee order does not count, so the problem is 
that of selecting 3 people from 8 in all possible ways. From formula (2), 
the total number is 
-- 3]- 12 3-- 56. 
(b) The 2 xvomen can be selected in () or 6 ways, and after they have 
been selected in any one of these ways, the 1 man can be selecd in () 
or 4 ways. Hence, by the multiplication principle of Section 2-1, the 
number of ways of selecting 2 women and 1 man is 
(c) If a husband and wife cannot both serve on the committee, then 
3 couples must be represented on the committee. Three couples can be 
selected from 4 in () ways. After the 3 couples have been selected, two 
choices can be made from the first couple (husband or wife), two from 
the second couple, and two from the third couple. By the multiplication 
principle, the total number of committees is 
Alternatively, there are 4 ways to select a couple, and 6 ways to select 
the remaining member or, in all, 6  4  24 ways to select a committee 
with a couple. Subtracting 24 from the total ways, 56, gives 32 com- 
mittees without a couple. Frequently, counting unwanted cases and 
taking complements is easier than a direct count. 
38 PERMUTATIONS, COMBINATIONS, BINOMIAL' THEOREM [CHAP. 2 
EXAMPLE 4. In hov many vays can a selection of one or more books 
be made from 5 identical algebra books and 4 identical geometry books? 
Solution. Let us first deal vith the algebra books. We can select 1 or 
2 or 3 or 4 or 5 or none of them. Hence the algebra books can be dealt 
vith in 6 ways. After dealing vith them in any one of these 6 ways, we can 
similarly deal with the geometry books in 5 ways. By the multiplication 
principle, we can make a selection from both kinds of books in 6 X 5, 
or 30 vays. These 30 ways include the case in which we take no algebra 
book and no geometry book. If ve must take at least one book, then the 
number of selections is 30 -- 1 ---- 29. 
2-11 Theorem. Pascal's Rule. 
r ---- r-- 1 - ' for 1 _ r_ n. 
Proof. The formula may be proved by substituting factorial symbols 
and simplifying. Another method of proof depends on the meanings of 
the symbols, as follows. 
The number of selections of r objects that can be made from a given 
set of n - 1 objects, xvithout restrictions, is (r+l). Consider some speci- 
fied object in the given set. If this specified object is included in the 
selection, the remaining r -- 1 objects can be selected from the remaining 
n objects in (r_l) xvays. If the specified object is not included, the r 
objects must be selected from the remaining n objects, and this can be 
done in () ways. The total number of selections is obtained by adding 
the number in xvhich the specified object occurs to the number in which 
the specified object does not occur, since no other cases are possible. 
Therefore, the total number is 
NOTE. The foregoing proof is an application of the addition principle, 
not of the multiplication principle. The problem presents us with txvo 
operations, either of which is admissible separately, but not both simul- 
taneously. It is a question of either this operation or that operation. Such 
operations are mutztally exclusive; they cannot both occur together. Each 
operation forms the basis of a separate problem, and the final result is 
obtained by addition, not by multiplication. 
2-3] COMBINATIONS 39 
TABLE 2--3. PASCAL'S TRIANGLE FOR 0  r  n  10. 
= l r n. 
r r i r ' -- -- 
n 0 1 2 3 4 5 6 7 8 9 10 
0 1 
1 1 1 
2 1 2 1 
3 1 3 3 1 
4 1 4 6 4 1 
5 1 5 10 10 5 1 
6 1 6 15 20 15 6 1 
7 1 7 21 35 35 21 7 1 
8 1 8 28 56 70 56 28 8 1 
9 1 9 36 84 126 126 84 36 9 1 
10 1 10 45 120 210 252 210 120 45 10 i 
Pascal's rule gives a simple way of building a table of values of (), 
known as Pascal's triangle. Table 2-3 shows the part of Pascal's triangle 
for values of n from 0 through 10. The rows of the table correspond to 
values of n; the columns, to values of r. The first and last entries in each 
row are 1 because () ---- () ---- 1. The entry other than the first or last 
in each row is the sum of the entry immediately above it and the entry 
to the left of that one, by Pascal's rule. Thus, for example, the entry for 
n ---- 3, r ---- 2 appearing in the fourth row and third column of the body 
of the table is the sum of the entry in the third row and third column and 
that in the third rmv and second column because (s) = () _}_ (). 
EXERCrSES FOR SECTION 2-3 
1. Evaluate the following: (), 
2. Show that () = 1, and interpret it in terms of selections. 
3. Solve the following equations for 
(a) = 45; (b) (nl) -- 60; (C) 8 = 12 ' 
40 PERMUTATIONS, COMBINATIONS, BINOMIAL THEOREM [CHAP. 2 
4. In how many ways can a committee of 5 bc chosen from $ people? 
5. A contractor needs 4 carpenters and 10 apply for the jobs. In how many 
ways can he pick out 4? 
6. In how many ways can a selection of fruit be made from 7 plums, :i lemons, 
and 9 oranges? (Assume that the 7 plums are indistinguishable. Likewise for 
the lemons and for the oranges.) 
7. How many selections of 1 or more letters can be made from 2 A's, 5 B's, 
and 9 C's? 
8. Ten points are taken on the circumference of a circle. How many chords 
can be drawn by joining them in all possible ways? With these 10 points as 
vertices, how many triangles can be drawn? How many hexagons? 
9. In how many ways can a selecton of 4 books be made from 97 If a certain 
book must be chosen, in how many ways can the selection be made? In how 
many ways can it be made if a certain book must be left? 
10. A company of 20 men is to bc dvided into 3 sections so that there are 3 
men in the first, 5 in the second, and 12 in the third. In how many ways can 
this bc done? (Don't multiply out.) 
11. Find the number of ways in which at least one book can be selected from 
4 identical cook books and $ identical novels. 
12. Write a symbol for the number of combinations of 20 objects taken 4 at 
a time, and for the number of combinations of 100 objects taken 98 at a time. 
Compute the numerical value of each symbol, and find which is the greater. 
13. A pack of playing cards contains 52 different cards. If a hand is made 
up of 5 cards, use the factoffal notation to express the number of possible hands. 
(Disregard order in the hands.) 
14. In how many ways can 2 booksellers divide between them 300 copies of 
one book, 200 copies of another, and 100 copies of a third, if neither bookseller 
is to get all the books? (Don't multiply out.) 
15. A bridge deck of cards is made up of 13 spades, 13 hearts, 13 diamonds, 
and 13 clubs. How many different hands can be formed if each hand contains 
5 spades, 4 hearts, 2 diamonds, and 2 clubs? (Don't multiply out.) 
16. Six candidates contest an election for two similar offices. If a voter may 
mark his ballot either for one or for two candidates, in how many ways can he 
cast his vote? 
17. How many 5-letter words, each consisting of 3 consonants and 2 vowels, 
can bc formed from the letters of the word equations? 
18. Twenty persons are to travel in a double-decker bus that can carry 12 
passengers inside and $ outside. If 4 of the persons will not travel inside, and 
5 will not travel outside, in how many ways can the passengers be seated (a) if 
the arrangement of the passengers inside, or outside, is not considered, and 
(b) if the arrangement inside and outside is considered? 
19. In how many ways can 4 persons be selected from 5 married couples 
(a) if the selection must consist of 2 women and 2 men, and (b) if a husband and 
wife cannot both bc selected? 
20. Verify that the entries for n = 4 iu Table 2-3 satisfy the conditions 
() = (44) = i and (4r) = () + (r31) for 1 _ r _ 3. 
21. Write out the entries that would bc in the rows for n = 11 and n -- 12 
in Pascal's triangle, Table 2-3. 
2--3] COMBINATIONS 41 
EXERCISES ON PERMUTATIONS AND COMBINATIONS 
A. 
1. In how many ways can a man choose 3 gifts from l0 different articles? 
2. A railway has 50 stations. If the names of the point of departure and the 
destination are printcd on cach ticket, how many different kinds of single tickets 
must be printed? How many kinds are needed if each ticket may be used in 
either direction between two towns? 
3. In how many ways can 15 different objects be divided among A, B, and 
C, if A must receive 2 objects, B must receive 3 objects, and C must receive 
10 objects? 
4. Given 20 points, no three of which are in a straight line, find the number 
of straight lines that can be drawn by joining pairs of these points. 
5. Given 4 non-coplanar points in space, how many planes can be determined 
by selecting triples of these points? 
6. A ring of 8 boys is to be enlarged by the addition of 5 girls. In how many 
ways can this be done if no two girls are to stand beside each other? (Note that 
order counts here because people are distinguishable.) 
7. A town council is made up of a mayor and 6 aldermen. How many different 
committees of 4 can be formed, (a) if the mayor is on each committee, and (b) 
if the mayor is on no committee? 
8. How many 4-letter words can be made from the letters of the word zephyr? 
How many of these words will not contain the letter r? How many will contain 
r? How many will begin with z and end with r? 
9. In how many ways can a coach choose a team of 5 from 10 boys (a) if 2 
specified boys must be included, and (b) if there are no restrictions? 
10. A man has 8 different pairs of gloves. In how many ways can he select 
a right-hand glove and a left-hand glove that do not match? 
11. A 35-mm colored slide is mounted in a 2" X 2" square holdcr. How many 
wrong ways are there of inserting the mounted slide into a projector? 
12. In how many numbcrs betwecn 1000 and 9999 inclusive does the digit 
3 occur? 
13. How many words, each of 2 vowels and 2 consonants, can be formed 
from the letters of the xord involute? 
14. How many quadrilaterals can be formed, each having as its vertices 4 of 
the vertices of a given regular polygon of 20 sides, if no 2 of the selcctcd 4 arc 
opposite vertices of the given polygon ? 
15. Prove (;') q- (r_nl) - (nr+l) (Pascal's Rule) by using factorials to replace 
the symbols on the left, and simplifying. 
16. There are 10 chairs in a row. In how many ways can 2 persons be seated? 
In how many of these ways will the 2 persons be sitting in adjacent chairs? In 
how many will they have at least one chair between them? 
17. How many diagonals has a 20-sided polygon? How many sides has a 
polygon with 35 diagonals? 
42 PERMUTATIONS, COMBINATIONS, BINOMIAL ,THEOREM [CHAP. 2 
15. Four jobs of one kind can be held by women only, 5 jobs of another 
kind by men only, and 3 jobs of a third kind by either men or women. In how 
many ways can these jobs be filled from 18 applicants of whom $ are women 
and 10 are men? 
19. Fifteen points lie in a plane in such a way that 5 of the points are on one 
straight line and, apart from these, no 3 points are collinear. Find the total 
number of straight lines that can be obtained by joining pairs of the 15 points. 
20. How many 6-digit numbers can be formed from the digits 1, 2, 3, 4, 5, 6, 
7, 8, 9 xf each number has 3 odd and 3 even digits and no two digits are alike? 
21. In how many ways can one assign to 2 soldiers different 3-digit numbers? 
In how many ways can this bc done if the 3-digit numbers are composed of even 
digits only (zero being considered even)? 
22. In a set of 10 examinations, 2 are in mathematics. In how many different 
orders can the examinations be given if those in mathematics are not consecutive? 
- 23. A stamp collector has 8 different Canadian stamps and 10 different 
United States stamps. Find the number of ways in whmh he can select 3 
Canadian stamps and 3 United States stamps and arrange them in 6 numbered 
spaces in his stamp album. 
24. A symphony is recorded on 4 discs, both sides of each disc being used. 
In how many ways can the 8 sides be played on a phonograph so that some 
part of the symphony is played out of its correct order? 
25. A railway coach has 10 scats facing backward and 10 facing forward. 
In how many ways can 8 passengers be seated, if 2 refuse to ride facing forward 
and 3 refuse to ride facing backward? 
26. From a company of 20 soldiers, a squad of 3 men is chosen each night. 
For how many consecutive nights could a squad go on duty wthout two of the 
squads being identical? In how many of these squads would a given soldier 
serve ? 
27. Find the number of ways in which 8 persons can be assigned to 2 different 
rooms, if each room must have at least 3 persons in it. 
2-4. PERMUTATIONS OF THINGS THAT ARE NOT ALL DIFFERENT 
In Sections 2-1 through 2-3, we considered arrangements of sets of 
objects that were different from each other. How will the number of 
possible permutations be affected if some objects in the given set are 
alike? A little thought will doubtless convince you that if some of the 
objects in a set cannot be distinguished from others, the number of pos- 
sible permutations is decreased. For example, the letters A, B, and C 
yield 3! or six 3-letter words; but the letters A, A, A yield only one 3- 
letter word. 
EXAMPLE 1. In how many ways can the letters of the word assess be 
arranged, all at a time? 
Solution. The problem would be easy if the four s's were different from 
one another. For we know that there are 6! permutations of 6 different 
2-4] PERMUTATIONS OF TILINGS NOT ALL DIFFERENT 43 
letters taken all together. We shall relate this familiar problem (letters 
all different) to our new problem (letters not all different) by making the 
four s's temporarily different, as described in the following. 
Let the unknown total number of permutations of the letters of the 
word assess be x. Now consider any one of these permutations; for example, 
s s s s a e. 
In this arrangement, if we replace the four s's by 
Sl, S2 S3, S4, 
the original arrangement gives rise to 4! arrangements by permuting the 
four s's with subscripts (now different) vithout disturbing the other letters. 
In the same way, each of the original x permutations gives rise to 4! 
permutations. Thus the total number of permutations is x(4!). Since the 
6 letters 
81, 82, 83, 84, a, e 
are now all different, x(4!) is the number of permutations of 6 different 
letters, taken all together. Therefore, 
x(4!) ---- 6! 
or 
6 
4 
Recall that a similar type of reasoning was used to evaluate (). We 
can at once generalize this reasoning to show that the number of per- 
mutations of a set of n objects, taken all together, where r of the objects 
are alike and the rest are different, is n!/r!. Repeated applications of this 
principle yield the following theorem. 
2-12 lheotem. Permutations of objects that are not all different. Given 
a set of n objects having n elements alike of one kind, and ns elements alike of another kind, and n3 elements alike of a third 
kind, and so on for / kinds of objects; then the number of 
permutations of the n objects, taken all together, is 
n] 
nl!n2!'' 'nk!' (1) 
where nx - ns - '" - nk ---- n. 
44 PERMUTATIONS, COMBINATIONS BINOMIAL ,THEOREM [CHAP. 2 
2-13 Corollary. Permutations for two kinds of objects. If a set of n 
objects consists of r elements of one kind and n -- r elements of 
another, then the number of permutations of the n objects, taken 
all together, is 
r!(n -- r) ] n-- r 
Proof. (a) The proof follows at once from Theorem 2-12, when we set 
n = r and n2 = n -- r. Alternatively, this corollary can be proved 
as follows. 
(b) Suppose there are r A's and n -- r B's to be arranged in order. We 
think of n blank spaces to be filled, r with A's, and the rest with B's. The 
number of ways of selecting the r spaces for the A's is (r ) and, after this 
has been done, the A's can be arranged in the spaces in just one way. 
Next, the B's can be arranged in the remaining n -- r spaces in just 
(-) X 1, or 1, way. Hence the total number of arrangements is (r). [] 
REMXRK. The numlcr of permutations of n objects, r alike of one kind 
and n -- r alike of another kind, is equal to the number of combinations 
of n different objects (the n blank spaces), taken r at a time. The fore- 
going proof shows why this particular equivalence bctvcen permutations 
and combinations occurs. 
EXAMPLE 2. HOW many arrangements can be made of the letters of the 
word Mississippi, taken all together? 
Solution. We have 11 letters in all, with one m, four i's, four s's, and 
two p's {m iiii ssss pp}. Thus, n = 1, n2 ---- 4, ns = 4, and n4 = 2. 
By Theorem 2-12, the total number of permutations of the 11 letters, 
taken all together, is 
11! 
-- 34,650. 
1!41412! 
EXAMPLE 3. How many arrangements can be made from the letters of 
the word equations, provided that the vowels must always remain in the 
order e, u, a, i, o? 
Solution 1. Since the order of the vowels relative to one another cannot 
be changed, it follows that the vowels cannot be permuted among them- 
selves and so for the purposes of this problem may be considered as 
2-4] PERMUTATIONS OF THINGS NOT ALL DIFFERENT 45 
identical. Hence the problem is that of finding the number of permuta- 
tions of 9 letters, taken all together, where 5 of the letters are identical. 
By Theorem 2-12, the number is 
9! 
-- 3024. 
5!(1!)4 
Solution 2. The 9 letters are to be arranged to fill 9 spaces. The spaces 
for the vovels can be selected in () ways and, once they are selected, the 
vowels can be arranged in them, in the order e, u, a, i, o, in just one way. 
The consonants can be arranged in the remaining 4 spaces in 4! ways. 
Hence the total number of arrangements is 
X 1X 4!---- 5!4i X 4!-- 5!-- 3024. 
]7X-iMPLE 4. Given n q-r letters, of which n are A's and r are B's, 
how many different sequences can be formed from the A's and B's, if 
each sequence must contain all n A's? 
Solution. There are r q- i mutually exclusive cases because we may 
have n A's and no B's, or n A's and one B, or n A's and two B's, and so 
on. The r q- i alternative cases are listed in Table 2-4, together with the 
number of sequences to which each case gives rise. For each case, the 
number of sequences is calculated by formula (2) of Corollary 2-13. 
Since the cases are mutually exclusive, we get the total number of 
sequences by applying the addition principle. The number is 
q- 2 q-'" q- r '[] 
TABLE 2--4 
Mutually exclusive cases Number of sequences 
n A's, no B's (8) 
n A's, one B 
n A's, two B's rn+2 
\2! 
n A's, three B's (n?) 
n A's, r B's (?) 
46 PERMUTATIONS, COMBINATIONS, BINOMIAL THEOREM [CHAP. 2 
REMARK. This sum equals' ("+?), as can be shown by successive 
applications of Pascal's Rule (Theorem 2-11, Section 2-3). For we have 
1 o + 1 = 1 
2 + 3 3 
and so on. Finally, the rth step gives 
[] 
EXERCISES FOR SECTION 2-4 
1. Find the number of arrangements of the letters of the xxord cowmittee, 
using all the letters in each arrangement. 
2. How many different numbers can be obtained by arranging the digits 
2233344455, all together, in all possible xvays � 
3. How many permutations can be made using the letters of the word institu- 
tion, taken all at a time? How many of these begin with t and end with s? 
4. In how many ways can 13 different cards be arranged in a row, if a certain 
10 of them must always be in a specified order relative to each other? 
5. Find the number of ways in which 6 plus signs and 4 minus signs can be 
arranged in a row. 
6. Find the number of ways in which nine 3's and six 5's can be placed in a 
row so that no two 5's come together. 
7. How many different numbers can be obtained by arranging the digits 
123456789, all at a time, if the even digits must always remain in ascending 
order and the odd digits likewise? 
8. Find the number of arrangements of the letters of the word engineering, 
taken all together. In hoxv many of these are three e's together? In how many 
are exactly two e's together? 
9. A class consists of 12 girls and 10 boys. In how many xx ays can the class 
form a line, if the girls always remain in ascending order of height, and the 
boys likewise? 
10. In how many ways can one take a walk for 9 blocks, if he ahvays walks 
5 blocks west and 4 blocks north ? 
2-4] PERMUTATIONS OF THINGS NOT ALL DIFFERENT 47 
11. How many numbers greater than 3,000,000 can be formed froin the 
digits 1, 1, 1, 2, 2, 3, 3? 
12. In hov many ways can 5 rcd balls, 4 black balls, and 4 vhite balls be 
placed in a row so that the balls at the ends of the roxv are of the same color? 
REVIEW EXERCISES 
1. A rat runs a branching maze, so constructed that he first must choose one 
of a pair of doors, beyond each of these he must choose one of 3 doors, and 
beyond each of these he must choose one of 4 doors. After passing through a 
door he cannot return. Itow many paths are there from start to finish? 
2. A metallurgist, studying alloys, xxants to study the effect of 3 different 
temperatures, 6 different heating times, and 4 different amounts of a copper 
compound. One experiment has one level for each variable. How many different 
experiments must he pelforln if every triple of temperature level, heating time, 
and amount of copper is to be represented? 
3. A dial safe has 100 positions on its dial and 3 settings are required for a 
combination. Howeve L no setting can be fewer than 10 positions from the 
immediately preceding setting. How many combinations are there? 
4. A keymaker has 12 types of blanks. Each blank has 5 different positions 
where metal can be removed and there are 3 cutting depths at each position 
except the first, which has only 2. Hoxv many possible keys are there? 
5. A soil chemist has 6 different treatments to study, and he can apply 3 
different treatments, simultaneously, in a single experiment. How many experi- 
ments must he do to exhaust all triples of trcatmcnts? 
6. If the soil chemist in the preceding exercise cannot have triples n which 
treatments A and B appear simultaneously, hoxv many experiments are there? 
7. A computing machine is used for the study of problen-solving. It has 10 
different steps it can use, and it does not use one it has previously used in the 
same attempt to solve a problem. The problems it solves require 4 different 
steps taken in the correct order. What is the largest number of 4-step attempts 
the computer may have to make before it solves a given problem ? 
8. An experimenter studying problem-solving has designed a problem whose 
correct solution requires 6 steps taken in order. There are two steps of type A, 
two of type B, and two of type C. He has made a list of the possible orders 
AABBCC, AABCBC, ..., CCBBAA 
and he has 88 such orders in his list. Has he found them all? 
9. In the World Series, the American League team A, and the National 
League team N, play until one team wins 4 games. If the sequence of winners 
is designated by letters (NAAAA means National League won the first game 
and lost the next 4), how many different sequences of winners are possible? 
48 PERMUTATIONS, COMBINATIONS, BINOMIAL THEOREM [CHAP. 2 
2-5. THE BINOMIAL THEOREM 
Expansions of positive, integral powers of the binomial (a -q- x), such as 
(a -q- x) 2 = a 2 -q- 2ax -q- x 2, 
(a -q- x) a = a a -q- 3a2x -q- 3ax 2 -q- x a, 
are of frequent use in algebra. Moreover, expansions of this kind are 
important for our future studies in this book, and are related to results 
already obtained in this chapter. We are presently interested in finding 
a law or formula by which such expansions can be readily obtained. 
Of course, we can always obtain them by ordinary multiplication. But 
the process soon becomes laborious. After we have shown by multiplica- 
tion that 
(a -q- x) 4 = a 4 -q- 4aax -q- 6a2x 2 -q- 4ax a -q- x 4, 
we may well begin to wonder if there isn't some better method of getting 
the result. If we study the expansions for (a 4- x) 2, (a -q- x) a, and (a -q- x) 4, 
we soon note that part of the solution is easily guessed. The terms in the 
expansions, apart from their coeJcients, van be written by inspection. For 
example, the expansion of (a 4- x) 5 has 5 q- 1, or 6 terms; without their 
coefficients, the terms are 
a 5, a4x a3x 2, a2x 3, ax 4, x 5. 
Note that each of these terms is the product of 5 factors, where each 
factor is a or x. We say that such terms are of degree 5 in a and x. (In 
general, the degree of a term in a and x equals the number of factors a 
or x that the term contains.) 
Proceeding along these lines, we see that the expansion of (a q-x)  
has n -q- 1 terms; without their coefficients, the terms are 
n an--Ix, a--23:2, n--r r n 
a, ..., a x, ..., x, 
where each term is of degree n in a and x. But there remains the question: 
How do we find the coefficients of these terms? Why, in the expansion of 
(a -q- x)4, do we have 4a3x and 6a2x27 To answer such questions, let us 
examine the multiplication process. 
Prodzxts of distinct binomials. The use of subscripts may help to il- 
luminate what is going on when we multiply binomials. Consider the 
following products: 
2--5] THE BINOMIAL THEOREM 49 
(a + x)�2 + x2) = aa + ax + .a + 
(a + )( + x)( + ) 
The foregoing expansions illustrate three principles of multiplication: 
(1) the degree, in a and x, of each term in a product equals the number 
of factors multiplied; 
(2) the terms are obtained by selectig exactly one letter from each of 
the factors (note the subscripts on the right-hand sides); 
(3) the expansion consists of the sum of such terms obtained in all pos- 
sible ways. 
In these products of dintinct bnomials, if we drop subscripts and collect 
like terms, we obtain the expansions of (a-]-x) 2 and (a + x) 3, respec- 
tively. Note that principles (2) and (3) are the keys to the discovery of 
the coefficients. If, for example, we can select two a's and one x in 3 
different ways, as shown by the boxes, then the term ax will occur 3 times 
and so have the coefficient 3 in the expansion. 
Let us apply the three principles above to obtain the expansion of 
(a -]- x) 4. Since 
(a + )4 = (a + )(a + )(a + )(a + ), 
the terms of the expansion are of degree 4 in a and x. The possibilities 
are listed once more for reference: 
a 4, a3x, a2x 2, a3c3, 4. 
Each term is obtained by selecting exactly one letter from each of the 4 
factors. To get the first term, a 4, we select no x's and four a's from the 
4 factors. Because this can be done in () or 1 way, a 4 occurs only once 
in the expansion, and its coefficient is 1. To get the second term, a3x, 
we select an x from one of the factors and three a's from the remaining 
three factors. This can be done in () or 4 ways. Thus the term ax occurs 
4 times and has the coefficient 4. Similarly, the term ax  is obtained in 
() or 6 vays; the term ax 3 in () or 4 ways; and the term x 4 in (4 4 ) or 1 
50 PERMUTATIONS, COMBINATIONS, BINOMIAL THEOREM [CHAP. 2 
xvay. These last three terms therefore have the coefficients 6, 4, and 1, 
respectively. The complete expansion is the sum of all these terms' 
(a q- x) 4= ()a4q - (41)a3x q-(24 ) a2x2q- ()ax3q- ()x 4 
 a 4 q- 4ax q- 6a2x 2 q- 4ax  q- x 4. 
We now proceed to generalize the foregoing reasoning. 
2-14 Theorem. The binomial theorem. If n is a positive integer, then 
(aq-x)*---- (;)a'q-()a'-xq-()a'-2x2q- ... 
(1) 
q- (rn) an--rx r q- � � � q- (nn) x n. 
Proof. Each term in the expansion of (a q- x) n is of degree n in the 
variables a and x. Thus, if we ignore the coefficients, each term has the 
general pattern 
a x, where r = 0,1,2,3,...,n. 
The term an--rz r 1S obtained by selecting x from r of the factors and a from 
the remaining n -- r factors. This selection can be made in (�) ways. 
(Cf. Theorem 2-9, Section 2-3.) Hence the term an--rx r occurs (�) t;mes 
and its coefficicnl; is (�). Therefore the complete general term is 
a x, 
and the expansion is that shown in (1). In summation notation (see 
Appendix II), this expansion may be written 
(aq-x) n  (rn) n-r r 
---- a x . [] 
rO 
The coefficients (') arc often called biomial coecicts. 
2-5] THE BINOMIAL THEOREM 51 
Binomial formula with expanded coecients. Since 
= 1, = n, 2 -- 2] ' 
() n(n -- 1)(n -- 2) 
= ..  etc., 
the binomial expansion (1) may also be written as 
 n a_x n(n -- 1) a,_2x2 x,. 
(a+x) =a +., + 2! ' +''' + (2) 
EXAMPLE 1. Expand (2 d- x)4 
Solutioz*. Use formula (2), to obtain 
4 (2a)x d- 4.3 4.3 � 2 4.3 � 2. 1 X4 
(2 d-x) 4 = 24d- l2(22)x2d- 1 2 3 (2)xa -1 1.2.3.4 
= 16d-32xd-24x2d-8xad-x 4. 
EXAMPLE 2. Expand (1 -- 2x2) s. 
Solution. From formula (2), we obtain 
(1 -- 2x 2) 5---- (1 d- (--2x 2))5 
5 5.4 (__2x2)2 d- 5.4.3 (__2x2)3 
= 1 d-(--2x 2) d- 1--Y2 123 
5 � 4.3 � 2 (__2x2)4 + (__2x2)s 
1234 
---- 1 -- 10x2d-40x 4 -- 80x6d-80x s -- 32x � 
NOTE. The signs of the terms in this expansion alternate because of 
the negative sign of the second term of the binomial (1 -- 2x2). For the 
same reason, the signs in the formal expansion of (a -- x)  alternate. 
52 PERMUTATIONS, COMBINATIONS, BINOMIAL THEOREM [CHAP. 2 
EXAMPLE 3. Prove that () q-' () q- () q- ... q- () = 2 . 
Solution. Since formula (1) is valid for all values of a and x, we may 
seta = x---- 1. This gives 
which proves the proposition. [] 
EXAMPLE 4. If nx is near zero, prove that 
(1 q-x) n  1 q-nx. 
Solution. The proof follows at once from formula (2) if we set a ---- 1 
and ignore terms with x 2, x a, and higher powers. These neglected terms are 
n(n-- 1) 2 n(n-- 1)(n-- 2) 
2! x , 3! x , and so on. 
Their absolute values are respectively less than the absolute values of 
n2x 2 n3. 3 
2---?' 3---. ' and so on, 
because 
n(n-- 1) < n 2, n(n-- 1)(n-- 2) < n a, and so on. 
If nx is near zero, these higher powers of nx are small compared with 
the first power. Therefore, we have 
(1 q-x) n  1 q-nx. I [] (3) 
In Exercise 39 of this section, you are asked to show that when Inxl < 1 
the approximation (3) is in error by an amount that is less than (nx)2. 
This estimate of the error shows that the nearer nx is to zero, the better 
the approximation. For example, consider 
(1 q- 0.003) 2� 
2--5] THE BINOMIAL THEOREM 53 
Since x ---- 0.003 and n ---- 20, it follows that 
(nx) 2 = (0.06) 2 = 0.0036. 
Consequently, formula (3) gives a two-decimal approximation for 1.0032�: 
1 q- 20(0.003) ---- 1.06. 
For nx near zero, the formula 
(1 q-x) n  1 q-nx 
holds for nonintegral values of n as well as for integral values. For example, 
(1 q-x)/2  1 q- �x, if x is near zero. (4) 
EXAMPLE 5. Find , approximately. 
Solution. Transforming the radical and using (4), we get 
4 = 2 1 = 2(1 + 0.005) 1/2  2(1 + 0.0025) = 2.005. 
EXERCISES FOR SECTION 2-5 
1. In the expansion of (p q- q)?, what is thc dcgrec of each term in p and q? 
Write, in ascending powers of q, the terms of this expansion without their co- 
efficients. What is the general form of these partial tcrms? 
2. How many terms are there in the expansion of (rn q- n)��? Write the 
first three terms of this expansion, without their coefficients, in ascending 
powers of n. 
3. In the expansion m ascending powers of x of (1 q- x)���, what is (a) the 
200th term? (b) the coefficient of the 375th term? (c) the coefficient of x6257 
4. In the expansion of (a q- x)oo, write, in unsimplified form, the 50th term. 
What is the 20th term? What is the term that contains x6�? 
5. Expand (qq-p)4. Ifp = �andq = �, find thc values of the terms. 
Use the binomial formula to expand the following: 
6. (a) (1 q- b)5; (b) 1.015 7. (a) (1- b)5; (b) 0.985 
8. (1 q- p)7 9. (1- 3a) 4 10. (1- x2) 5 
11. (2 q- 4m) 4 12. (x q- y)6 13. (3 -- 6c) 5 
14. (�a q- 1) 4 15. (p q- q)7 16. (a q- ax) 6 
17. (1 -- xa) 5 18. (x 2-- xa)  19. (x-- y)5 
54 PERMUTATIONS COMBINATIONS BINOMIAL THEOREM [CHAP. 
Expand each of the following to 3,terms: 
20. (p-]-q)5O 21. (x- y)1OO 22. (1- a2) 4� 23. (2x- 3y) 8 
Use the binomial formula to find approximations for the following: 
24. 1.002 TM 25. 0.9972� 26. 1.0045 27. 0.9998 TM 
28. x/9.09 29. /-24 30. x/ 31.  
32. 600 /4 33. 1/3120 
Find approximations for the following, given that Ix[ is small: 
34.  -- 2x 35. 1  6x 36.  8x 37. 8 -- 24x 
38. If n is even, use formula (1) to prove that 
39. If [nx] 4 1, prove that the approximation formula 
(lx)  = lnx, 
is in error by an amount les than (nx) . (Hint. l/r]  1/2 -, r  1.) 
40. How many different terms are there in the expansion of (a  b  c): 
forn = 1?lorn = 27 forn = 3?forgeneraln? 
CHAPTER 
3 
A FIRST LOOK AT 
PROBABILITY: EQUALLY 
LIKELY OUTCOMES 
3-1. INTRODUCTION. SOME EXPERIMENTS 
One of the earliest notions in probability was that of equally likely 
cases, or equally likely outcomes. The words "equally likely" are meant 
to convey the notion of "equally probable." The dea is essentially an 
intuitive one. For example, if a coin is tossed it seems reasonable to 
assume that the coin is just as likely to fall "heads" as to fall "tails": 
the two outcomes, heads and tails, are considered to be equally likely. 
In other words, we say that heads and tails have eqml charces. 
A "die" (plural, dice) is a homogeneous cube whose faces are marked 
with dots as follows: 
If a die is thrown, most people find it reasonable to assume that when 
the die stops, one face is as likely to be on top as another: the six faces 
have equal chances. Thus, the throw of a die gives rise to six "equally 
likely" outcomes: the upper face shows one dot, or two dots, or three dots, 
and so on. We say that each of these outcomes has "one chance in six" 
of occurring. 
Cards in an ordinary bridge deck are made so as to be indistinguishable 
from one another when they are placed face down. Suppose that we shuffle 
such a deck and draw one card. We then say that each card has the 
same chance of being drawn as every other card: one chance in fifty-two. 
What makes us feel, in the tossing of a coin, that the two sdes are 
equally likely, that in the throwing of a die the six faces are equally 
55 
56 EQUALLY LIKELY OUTCOMES [CHAP. 3 
likely, and that in the drawing' of a card the fifty-tvo cards are equally 
likely? It is the symmetry and homogeneity of the coin and the die, 
ad the similarity of the cards. If the die were heavily loaded on one 
side, then the opposite side would be more likely to appear on top, and 
the six faces would not be equally likely. Experience with ordinary coins, 
dice, and cards confirms the notion that heads and tails appear about 
equally often when coins are tossed, that one face of a die appears on top 
about as often as another when dice are thrown, and that one card ap- 
pears about as often as another when cards are drawn. Thus both reason- 
ing based on symmetry and similarity, and experience with actual physical 
objects support the idea of equally likely outcomes for such experiments. 
J. E. Kerrich* designed a coin-tossing experiment while he was interned 
during World War II. In ten sets of 1000 tosses, he found that the num- 
bers of heads were 
502, 511, 497, 529, 504, 476, 507, 528, 504, 529. 
We see that these numbers cluster around 500, although none is exactly 
500. To check that theory fits the facts, it is important to examine the 
results of such experiments with physical objects and to compare these 
results with theory. It is even more important to experience at first 
hand the relation between mathematical theory and events in the real 
vorld. For this reason, we have suggested and we shall continue to suggest 
experiments for you to perform. 
To appreciate the importance of experimental verification as well as 
clear reasoning, consider the experiment of tossing two coins. It is said 
that the mathematician D'Alembert reasoned that there are three pos- 
sible ways the coins can fall: (a) both heads, (b) one head and one tail, 
and (c) both tails. He thought these three outcomes were equally likely. 
If that were true, we should expect each of them to occur about � of the 
time in actual experiments. Before reading further, you are urged to do 
Exercise 1 at the end of this section. 
Table 3-1 shows another analysis of the two-coin problem. This analy- 
sis leads to four possible outcomes instead of three. If these four cases 
were equally likely, we should expect one head and one tail to occur 
about twice'as often as two heads (or two tails), because cases 2 and 3 
both contribute to the "one head and one tail" count. 
Would reason alone enable us to decide which analysis of the two-coin 
problem is correct? Possibly so. But intelligent people have disagreed, 
* J. E. Kerrich, An Experimental Itroduction to the Theory of Probability. 
Belgisk Import Co. Copenhagen. 
3-1] INTRODUCTION. SOME EXPERIMENTS 57 
TABLE 3--1. POSSIBLE VAYS 2 COINS CAN LAND. 
Case First coin Second coin 
1 head head 
� head tail 
3 tail head 
4 tail tail 
some choosing the "three equally likely cases" and others the "four equally 
likely cases." As we learn more about the theory of probability, we shall 
discover that the "four equally likely cases" corresponds to the assump- 
tion that the outcome on either coin is independent of the outcome on 
the other--an assumption that most experts nowadays believe fits the 
facts for coins. 
Four major uses of the "equally likely cases" approach in probability 
are � 
(1) To describe a physical experiment such as the tossing of a coin. 
(2) To offer a "baseline" for comparison with empirical results. We 
may not believe that a phenomenon can be described by "equally likely 
cases," but we see where such an assumption leads, and then we check 
the consequences experimentally. For instance, the experiment of tossing 
2 coins 50 times furnishes evidence for deciding in favor of three or 
of four equally likely cases. 
(3) To give a satisfactory approximation to a process where we know 
very well that the cases are not equally likely. For example, even though 
more boys than girls are born, we often assume that every new baby 
has an equal chance of being a boy or a grl, and for many purposes this 
assumption is adequate. 
(4) To achieve random sampling in sample surveys and other experi- 
mental investigations. The analysis of data from random samples is a 
relatively straightforward matter, but the analysis of data from samples 
not based on randomness requires difficult judgments. 
The idea of an experiment. We have used the word experiment to 
describe such things as the throwing of one or more coins, the tossing of 
a die, or the drawing of a card from a bridge deck. Many more serious 
experiments are associated with every medical and scientific research 
campaign, such as the search for a polio vaccine, the study of the cause 
58 EQUALLY LIKELY OUTCOMES [CHAP. 3 
and cure of cancer, the studies of'genetics by Gregor Mendel,* the psycho- 
logical experiments of Pavlov, and the synthesis of penicillin by Sheehan. 
In the experiments of such campaigns, it is difficult to imagine all possible 
outcomes and it is not to be expected that these outcomes would be 
equally likely. 
Nearly all serious experiments produce observations or measurements 
whose interpretation may well use an application of probability or of 
statistical reasoning. We shall use the word "experiment" to describe any 
act that can be repeated under given conditions. Usually the exact result 
of the act cannot be predicted with certainty. We focus attention on 
experiments that have only a finite number of possible outcomes and. 
in this chapter, we usually deal with outcomes that are equally likely. 
For example, selecting a sample of 3 persons from a population of 30 
persons is an "experiment" that may result in the choice of any one of 
the (%0) or 4060 different combinations of 30 people, selected 3 at a time. 
To select such a sample "at random" means that each combination has 
i chance in 4060 of being selected. 
Probability: a measure of chance. Although we used the word loosely 
earlier, we now introduce the word "probability" as a technical word 
that we shall define and use when deahng Sth experiments involving 
"chance." 
The experiments discussed to this point suggest a method of assigning 
numbers to the chances of certain events. Consider, for example, the 
throwing of a coin. In everyday language, we say that "the coin has 
i chance in 2 of falling heads"; in technical language, we say that "the 
probability of heads is �." In symbols, we write 
P (head) -- �. 
Similarly, in the tossing of a die, the face xvith six dots has 1 chance in 
6 of landing on top; thus, we say that "the probability of 6 dots on top 
is ." In symbols' 
P (6 dots on top) = . 
Likewise, when xve draw a card front a shuffled bridge deck, we have 
P (ace of hearts) = 22; 
and when we select at raildom a sample of 3 people from a population of 
* See "Mathenmties of Heredity" by Gregor Mendel, World of Mathematics, 
vol. 2, pp. 937-949. 
3-1] INTRODUCTION. SOME EXPERIMENTS 59 
30, the probability of selecting three specified persons 21, B, and C is 
P (A, B, and C) ---- 
4060' 
In general, we use 
"P ( .... )" 
to denote "the probability of .... ," where the blank may be filled with 
the name of any outcome. 
As a further example, consider a 10-ticket draw for a prize. A name 
is written on each of l0 tickets, the tickets are then thoroughly mixed in 
a bag, and one ticket is drawn at random. The person whose name appears 
on the ticket so drawn is the winner. 
Now if John's name appears on just one ticket, his chance of winning 
the prize is 1 in 10, since all outcomes in the drawing are equally likely. 
Therefore we say 
P (John wins) 
Similarly, if John's name appears on 7 tickets, his chances of winning 
are 7 in 10, and 
P (John wins) -- o. 
The general idea is that of separating from the whole set of equally 
likely outcomes the special subset of "favorable" outcomes. Then we 
assign the probability of a favorable outcome by the following rule' 
number of favorable outcomes 
P (favorable outcome) = 
number of possible outcomes 
This method of assigning to a favorable outcome a measure, or number, 
called its probability has an immediate consequence; for if there are no 
favorable outcomes in the set of possible outcomes, then 
P (favorable outcome) = 0; 
and, if all possible outcomes are favorable, then 
P (favorable outcome) = 1. 
It follows that 
0 _ P (favorable outcome) _ 1. 
This is consistent with our intuitive feeling. If John's name is on no 
60 EQUALLY LIKELY OUTCOMES [CHAP. 3 
ticket, he has no chance of winring and P (John wins) = 0; ff his name 
is on all the tickets, his winning is a sure thing and P (John wins) = 1. 
The number assigned to the probability of a favorable outcome also 
measures what we feel would be the long-run proportion of occurrences of 
the outcome in many repetitions of the experiment. 
EXAMrLE 1. Four faces of an ordinary six-sided die are painted red, 
and the other two faces are painted green. If the die is rolled once, what 
is the probability that the top face is (a) red, (b) green? 
Solution. Since the six faces are equally likely, the probability that the 
top face is red is given by 
number of favorable outcomes 4 2 
number of possible outcomes ----  = ' 
Thus we have 
P (red) = - 
Similarly, 
P (green) = _2 _ 1 
6 -- 3' 
Note that the answer is not �. The two outcomes "red" and "green" 
are not equally likely. 
The idea of an event. Sometimes it is convenient to regard a set of 
outcomes as corresponding to a single event. In Example 1, we may think 
of any one of the 4 red faces landing on top (4 outcomes) as corresponding 
to the event "top face red." The foregoing method of assigning proba- 
bilities can be defined in terms of events. 
3-1 Definition. Probabihty of an event. If an experiment can result in 
any one of n different, equally likely outcomes, and if exactly rn 
of these outcomes correspond to event A, then the probability of 
event A is 
P(n) = m. () 
REMARK. We denote the event not-A by A. It follows at once from 
the foregoing definition that the probability of event not-A is given by 
p(.) _ n -- m_ 1 m_ 1 -- P(A). (2) 
3-1] INTRODUCTION. SOME EXPERIMENTS 61 
Odds. The relative chances for A and for z are often expressed in 
terms of the odds in fad, or of .4: 
odds in favor of .4 -- P(A) m/n rn 
P(J_) -- (n -- rn)/n --n -- rn (3) 
Thus for the die with 4 red faces and 2 green faces, the odds in favor of 
red are I or . If we know the probabilities of the events A and J_, 
Eq. (3) gives the odds in favor of A. Sometimes these are also called the 
odds against . 
On the other hand, if we know that the odds in favor of A are a/b, 
then 
P(A) -- a b 
a q- b' P(A) .... (4) 
a-+-b 
EXAMrLE 2. A card is drawn at random from an ordinary bridge deck. 
Find the probability and the odds that the card is an "honor" (that is, 
an ace, king, queen, jack, or ten). 
Solution. The number of possible outcomes is 52. There are 5 honor 
cards in each of the 4 suits--a total of 20 honor cards. We may regard 
the 20 different outcomes in which an honor card is drawn as corresponding 
to the event "honor card drawn." If the cards are well shuffled, and 
one card is drawn at random from the deck, we assume that each of the 
52 cards is equally likely to appear. Hence 
rn 20 5 
P (honor card) -- -- -- 
n 52 13 
The odds in favor of an honor card are 5 to 8, since 
5 8 5/13 5 
P (non-honor card): 1 13 -- 13 and 8/13 -- ' 
Alternatively, the odds against an honor card are 8 to 5. We get the 
same results by observing that 
number of honor cards 
odds in favor of honor card --- 
number of non-honor cards 
20 5 
32 8 
02 EQUALLY LIKELY OUTCOMES [CHAP. 3 
and then computing � 
5 5 
P (honor card) -- 5 q- 8 -- 13 
EXERCISES FOR SECTION 3-1 
1. Perform the following experiment. Toss 2 coins 50 times, keeping a careful 
tally of the possible outcomes--two heads, one head and one tail, two tails. 
Compute the proportion of occurrence for each of the three outcomes. Do your 
results appear to support the reasoning of D'Alembert that the three outcomes 
are equally likely? 
2. If a die is thrown, ;vhat is the probability that the upper face shows 3? 
More than 3? Less than 3? An even number? An odd number? 
3. In a throw of two ordinary dice, what is the probability that the numbers 
on their upper faces add up to 3? to 4? to 117 
4. When two coins are tossed, what is the probability that both show heads? 
That they show one head and one tail? 
5. From a class of 40 students with 25 girls, one student is chosen by lot. 
What is the probability that a boy is choscn? 
6. A bag contains 10 white marbles and 8 black marbles. If one marble is 
drawn from the bag at random, what is the probability that it is black? That 
it is white? 
7. In a family of three children, what is the probability that all three are boys? 
Assume that boys and girls have an equal chance of being born. 
8. Toss a coin 100 times, keeping a record of the number of heads; or use 
the results of Exercise 1 for 50 tosses of 2 coins. Compute the relative fre- 
quency of heads. How does your result compare with the probability of getting 
a head on one toss of a single coin? 
9. One card is drawn at random from a well-shuffled bridge deck of 52 cards. 
What is the probability of drawing a heart? An ace? A black card? 
10. What is wrong with the following procedure? To find the probability 
that an American citizen chosen at random was born in a given state, divide 
the number of favorable cases (1) by the total number of states (50), and obtain 
the answer oo. 
11. A letter is chosen at random from the word equations. What is the proba- 
bility that the letter is a vowel? A consonant? The letter m? 
12. A person holds a ticket in a lottery that offers 10 prizes and sells 120 
tickets. What is thc probability that the person will win a prize? That he will 
not win a prize? 
13. Ten balls, numbered 1 to 10, are placed m a bag and two of the balls are 
drawn at random. What is the probability that balls numbered 3 and 7 are 
dra;vn ? 
14. The numbers 1 to 9 inclusive are written on slips of paper, and the slips 
are placed m a bag and thoroughly mixed. One slip is drawn from the bag 
at random. What is the probability that the number on the slip is odd? Even? 
Prime? (Note. Wc do not count the number 1 as a prime.) A multiple of 3? 
3-2] A SAMPLE SPACE OF AN IgXPERIMENT 63 
15. Five balls, numbered 1 to 5, are placed in a bag, mixed, and drawn out, 
one at a time. What is the probability that the balls are drawn in the order 
1, 2, 3, 4, 5? 
16. A six-volume cncyclopedla is placed at random on a bookshelf. What is 
the probability that one or more of the volumes is out of its correct order? 
17. A 3-dlgit number is forlned by randomly choosing three of the digits 
1, 2, 3, 4, 5, without repetition. What is the probability that the nulnbcr is even? 
Odd? A multiple of 5? 
3-2. A SAMPLE SPACE OF AN EXPERIMENT 
We have discussed the notion of an experiment and examined a number 
of simple experiments with equally likely outcomes. We now introduce 
an important related idea: "a sample space of an experiment." We ap- 
proach this idea with the aid of an illustrative example. 
Consider the experiment of tossing two coins: a dime and a quarter. 
How shall we list the possible outcomes of this experiment? It may be 
done in a number of ways, and the particular method preferred depends 
upon what our interest is centered on. Suppose, for example, that we are 
interested in whether each coin falls heads (H) or tails (T). Then the set 
S = {HH, HT, TH, TT} (1) 
provides a list that represents the possible outcomes of one toss, if xve 
understand that the first letter in a pair designates the outcome for the 
dime, and the second letter that for the quarter. Thus HT means that 
the dime fell heads and the quarter fell tails. Every outcome of the 
experiment corresponds to exactly one element of the set (1). 
Alternatively, we may be interested only in the number of heads or 
tails that appear. If we agree to denote a heads and b tails by the ordered 
pair (a, b), then the set 
& = o), O, (0,2)} (2) 
lists all possible outcomes of the experiment. And every outcome of the 
experiment corresponds to exactly one element of the set (2). 
Again, we may be concerned only with whether the coins fall ahke (A) 
or different (D). We could then list all possible outcomes xvith the set 
= {.4, o}. 
As before, every outcome of the experiment corresponds to exactly one 
element of the set (3). 
64 EQUALLY LIKELY OUTCOMES [CHAP. 3 
Hence each of the sets (1), '(2), and (3) provides a list that includes 
all possible outcomes of the experiment. Each such set is called '% sample 
space of the experiment"; that's why we talk about "a" sample space of 
an experiment, rather than "the" sample space. More than one sample 
space can be used to list the possible outcomes. 
NOTE. S iS a more fundamental sample space than SI or S2 because it 
offers more information. If we know which element of S occurs, we can 
tell which outcomes occur in SI and S2; but the reverse is not always true. 
3-2 Definitions. Sample space; sample point. A sample space of an 
experiment is a set S of elements such that any outcome of the 
experiment corresponds to exactly one element in the set. An 
element in a sample space is called a sample point. 
EXAMPLE 1. Three-child families. To study the distribution of boys 
and girls in families having three children, a survey of such families is 
made. What is a sample space for the experiment of drawing one family 
from a population of three-child families? 
Solution. Let B stand for "boy," and G stand for "girl." If we use a 
triple of letters to represent the oldest, the second, and the youngest 
child, in that order, then the following set is a sample space for a single 
family: 
{BBB, BBG, BGB, GBB, BGG, GBG, GGB, GGG}. 
The triple GBB, for instance, represents the outcome "oldest child is a 
girl, second and third are boys." Another sample space is obtained by 
listing the number of boys in families with three children: 
{0, 1, 2, 3}. 
Another useful way of listing the possibilities in this example is the 
"tree" method shown in Fig. 3-1. 
EXAMPLE 2. The numbers 1, 2, 3, and 4 are written separately on four 
slips of paper. The slips are then put into a hat and stirred. A blindfolded 
person draws two slips from the hat, one after the other, without replace- 
ment. Describe a sample space for the experiment. 
Solution. We may consider that each outcome of the experiment is 
represented by an ordered pair of numbers (x, y), where x is the number 
on the first slip and y is the Humber on the second. The restrictions on 
3-2] A SAMPLE SPACE OF AN EXPERIMENT 65 
x and y are as follows' 
1 _< x _< 4, 1 _< y _< 4, .r  y. 
Table 3-2 shows a sample space. 
l'irst clnhl Second cl.ld Third child Sample 
Boy  BI313 
t;irl  Boy  BGB 
 Gil  BGG 
O 
- Bo'  G.1  GBG 
 Boy -- GGB 
Fro. 3-1. Tree for three-child families. 
TABLE 3--.29. AMPLE SPACE FOR 2 NUMBERED SLIPS. 
y: number on second slip 
 1 2 3 4 
x: number 1 (1, 2) (1, 3) (1, 4) 
on first 2 (2, 1) (2, 3) (2, 4) 
slip 
3 I (3, l) (3, 2) (3, 4) 
4 (4, 1) (, 2) (4, 3) 
EXERCISES FOR SECTION 3-2 
1. A coin is tossed and then a die is thrown. List a sample space for this 
experiment. Illustrate with a tree graph. 
2. Three coins are tossed. List two sample spaces for this experiment. 
3. Two letters are randomly chosen, one after another, from the word tack. 
List a sample spac('. 
4. A boy has in his pocket a penny, a nickel, a dime, and a quarter. Hc takes 
two coins out of his pocket, one after the othcr. List a samplc space. Illustrate 
with a tree graph. 
06 EQUALLY LIKELY OUTCOMES [CHAP. 3 
5. Suppose you plan to make a survey of families having two children. You 
want to record the sex of each child, in the order of their births. For example, 
if the first child is a boy and the second a glrl, you record (boy, girl). This is 
one point in the sample space. List all the sample points. 
6. If the survey in Exercise 5 is undertaken for families having four hildren, 
list an appropriate sample space. How many sample points does it have? How 
many of these points correspond to families having 3 boys and 1 girl? How many 
correspond to families in which the first child is a girl? 
7. Two dice, one black and one red, are tossed and the numbers of dots on 
their upper faces are noted. List a sample space for the experiment. (Note. A 
tabular arrangement is convenient.) 
8. An engineer's ruler has a cross section that is an equilateral triangle. Two 
such rulers, one red and one green, have their faces numbered 1, 2, and 3. The 
rulers are tossed onto the floor and the numbers on the bottom faces are read 
when they come to rest. Set up a table for the sample space of outcomes. 
9. An experiment consists of selecting 3 radios from a lot of 25 and testing 
them. The test shows that a radio is defective (D), or nondefective (N). List 
a sample space for this experiment. 
10. From five different books, A, B, C, D, and E, three are selected. List a 
suitable sample space of outcomes. In your sample space: (a) How many sample 
points correspond to a selection including A ? (b) How many correspond to a 
selection without A? (e) How many correspond to a selection including both 
B and C? (d) How man correspond to a selection including either D or E? 
11. Two marbles, one red and one blue, are to be placed in two boxes numbered 
1 and 2. List an appropriate sample space (a) if one box may be left empty; 
(b) if neither box may be left empty. 
12. A letter is chosen at random from the ord ground. Which of the follow- 
ing sets are acceptable as sample spaces for the experiment and which are not? 
(a) {g, r, o, , n, d}; (b) {vowel, g, r, n, d}; 
(e) {r, o, u, n, d}; (d) {vowel, consonant}; 
(e) {consonant, u}. 
13. A bag contains a number of marbles, identical in every way except that 
some are red, some white, and some blue. Two marbles are drawn, one after 
the other, without replacement. What is a sample space for this experiment? 
How many points of the sample space correspond to drawing two marbles of 
the same color? How many points correspond to drawing marbles of different 
colors? How many correspond to drawing a red and a blue marble? How many 
correspond to drawing a red or a blue marble? 
14. In the sample space of Exercise 7, how many sample points correspond 
to a total of more than 10 dots? To a total of less than 5 dots? To an even 
total? To tlxe black (lie shoxxing more than 5 dots? To the red die showing less 
than 3 ad the black (lie showing lnore than 5? To the red die showing an even 
number or the black (lie showing 3? 
3-3] PROBABILITIES IN A FINITE SAMPLE SPACE 67 
3-3. PROBABILITIES IN A FINITE SAMPLE SPACE 
When an experiment is performed, we may want to knoxv the probabili- 
ties of various outcomes or events associated xvith the experiment. Often 
such probabilities can be computed by setting up a sample space of equally 
likely outcomes and counting the sample points. The following examples 
illustrate the method. Since we shall be making considerable use of these 
examples throughout the remainder of this chapter, they should be studied 
with special care. 
EXAMPLE 1. TWO dice. An experiment consists of throxving two ordinary 
six-sided dice and observing the numbers of dots on their upper faces. 
Discussion. For the purposes of this experiment, we assume that the 
dice are distinguishable: one die is red, and the other die is clear. It 
would serve our purpose just as well to throw a single die twice, the first 
throw corresponding to the red die, the second throw corresponding to 
the clear die. Table 3-3 shows a sample space that lists the possible out- 
comes of the experiment. 
In the rest of the chapter, Table 3-3 is referred to repeatedly, so in- 
serting a bookmark here may speed your reading. 
TABLE 3--3. SAMPLE SPACE FOR TWO-DICE EXPERIMENT. 
Clear die outcome (c) 
r 1 2 3 4 5 6 
1 (1, 1) (], 2) (1, 3) (1, 4) (1, 5) (1, 6) 
Red die 2 (2, 1) (2, 2) (2, 3) (2, 4) (2, 5) (2, 6) 
outcome 
(r) 3 (3, 1) (3, 2) (3, 3) (3, 4) (3, 5) (3, 6) 
4 (4, 1) (4, 2) (4, 3) (4, 4) (4, 5) (4, 6) 
5 (5, 1) (5, 2) (5, 3) (5, 4) (5, 5) (5, 6) 
6 (6, 1) (6, 2) (6, 3) (6, 4) (6, 5) (6, 6) 
Each row of the table corresponds to a fixed value of r, the outcome 
for the red die; and each column corresponds to a fixed value of c, the 
outcome for the clear die. For instance, the entry (2, 4) in the second 
row and the fourth column represents the event "red die shows 2, clear 
68 EQUALLY LIKELY OUTCOMES [CHAP. 3 
die shows 4." The entire samlJle space, S, is the set of ordered pairs 
(r, c) with r and c each taking the values 1, 2, 3, 4, 5, or 6' 
6}. 
Even without making the list, we can see from (1) that there are 6 X 6, 
or 36, possible outcomes of the experiment. We assume that the dice 
are well balanced and fairly thrown, so that these outcomes are equally 
likely to occur. We therefore attach probability  to each point in the 
sample space. 
Once the sample space of the experiment has been set up and proba- 
bilities assigned to the sample points, xve can answer questions such as 
the following: 
(1) What is the probability of throwing a double? 
(2) What is the probability that the number on the clear die is at 
least 3 greater than the number on the red die? 
(3) What is the probability that the sum r -]- c is 10? 
When an event is described by a verbal expression (for example, 
"throwing a double"), we often find it helpful to translate the verbal 
expression into an algebraic condition such as "c ---- r." Then we focus 
attention on the subset of the sample space whose members satisfy this 
algebraic condition, which is usually an equation or an inequahty. Count- 
TABLE 3--4. EVENTS AND PROBABILITIES FOR THE 
TXVO-DICE EXPERIMENT. 
Question Verbal Algebram Solution set 
number description condition (subset of S) Probability 
1 thrmving a (1, 1) (2, 2), (3, 3) 6 1 
CT   __ 
double (4, 4), (5, 5), (6, 6) 36 6 
clear score 
2 at least 3 (1, 4), (1, 5), (1, 6), 6 1 
c_r-]-3 - 
greater than (2, 5), (2, 6), (3, 6) 36 6 
red score 
3 sum 3 1 
I equallO c-]-r = 10 (4,6), (5, 5), (6, 4) - 
36 12 
3-3] PROBABILITIES IN A FINITE SAMPLE SPACE 69 
ing does the rest. For example, consider the three questions just posed in 
connection with the two-dice experiment. We list, in Table 3-4, the verbal 
descriptions, the corresponding algebraic conditions, the solution sets, and 
the required probabilities. 
REd,ARK. The foregoing procedure is useful for finding the probabilities 
of various outcomes of an experiment. We list the steps in the method: 
(1) Set p a sample space S of all possible o,tcomes. The sample space 
may be listed as in Table 3-3, or it may be indicated by the set-builder 
notation as in Eq. (1). 
(2) Assign probabilities to the elements of the sample space (sample 
points). In a sample space of n equally likely outcomes, we assign proba- 
bility 1/n to each sample point. The sum of the probabilities of all the 
sample points in a given sample space must equal 1. 
(3) To obtain the probability of an e'ent E, add the probabilities assigned 
to the elements of the sbsct of S that corresponds to �. Since the empty set 
has no elements, its probabilily is zero. (For "empty set," see Appendix 
I-3, ust before Theorem I-3.) 
E.rLE 2. For a chronic disease, there are five standard ameliorative 
treatments: a, b, c, d, and e. A doctor has resources for conducting a 
,omparative study of three of these treatments. If he chooses the three 
treatments for study at random froin the five, what is the probability that 
(a) treatment a will be chosen, (b) treatments a and b will be chosen, 
(c) at least one of a and b will be chosen? 
Selection. In Table 3-5, we list the (,), or 10, possible selections of the 
5 treatments, taken 3 at a time. For reference, the sample points are 
numbered. 
TABLE 3--5.  FOR STUDY OF TREATMENTS. 
1 2 3 4 5 6 7 8__ 9 . 10 
abc abd abe acd ace ade bcd bce bde cde 
Next, we assign probabfilty 6 to each ample point, since we assume 
that all 10 selections are equally likely. Then the probability that treat- 
meet a is chosen is 6, because there are 6 selections corresponding to the 
event "treatment a is chosen." 
Similarly, the probability that treatments a and b are both chosen is  
because there are exactly 3 selections containing both a and b. Finally, 
70 EQUALLY LIKELY OUTCOMES [CHAP. 3 
the probability that at least on of the treatments a and b is chosen is 
9 . 
X-6, only the tenth sample point contains neither a nor b. 
REMARK. Alternatively, we can reach the foregoing conclusions by 
using the facts of Chapter 2. For example, there are () ways of coosing 
three treatments from five without restrictions, and () ways of choosing 
them if treatment a must be included. Therefore, 
m 6 
P(a chosen): --: -- 
The other cases can be dealt with similarly. 
EXERCISES FOR SECTION 3-3 
Exercises 1 through 7 refer to the two-dice experiment. Consult the sample 
space in Table 3-3. 
1. What is the probability of not throwing a double? 
2. What is the probability that the number on one die is double the number 
on the other? 
3. What is the probability that one die gives a 5 and the other die a number 
less than 5? 
4. What is the probability that the clear die gives a number less than 3 and 
the red die a number greater than 37 
5. Evaluate: (a) P(r q- c = 6) (b) P(r q- c = 8) (c) P(r q- c < 5) 
(d) P(r q- c > 9) (e) P(r >_ c q- 4) 
6. Give algebraic descriptions of the following verbally described events: 
(a) not throwing a double, (b) red die shows two less than clear die, (c) clear 
die shows number at least 2 greater than red (tie, (d) number on red die twice 
that on clear die. 
7. Give verbal descriptions of the following algebraically described events: 
(a) r = 3c (b) r-- c = 1 (c) r  c 
(d) rq-c > 8 (e) c = r 2 (f) r >_ c 
8. For the sample space of Exercise 4 at the end of Section 3-2, answer the 
following: (a) What is the probability that both coins are silver? (b) What is 
the probability that the value of the coins selected is less than 20 cents? Less 
than 15 cents? More than 15 cents? A prirne number? A number divisible 
by 107 
9. In Exercise 6, Section 3-2, assume that all points in the sample space have 
the same pobability. What is the probability that in a family of four children 
the first two are girls? What is the probability that three are boys and one is 
a girl? That there are two boys and two girls? 
3-3] PROBABILITIES IN A FINITE SAMPLE SPACE 71 
10. In Exercise 8, Section 3-2, assume that all points of the sample space 
have equal probabilities. Let r denote the number on the rcd ruler, and g the 
number on the green. Evaluate: (a) P(r = q), (b) P(r q- q > 3), (e) P(r > q), 
(d) P(r  a), (e) P(r = a2). 
11. In the ancient Indian game of Tong, two players simultaneously show 
their right hands to each othcr, exhibiting either one or two or thrce extended 
fingers. If each player is equally likely to extend one, two, or three fingers, 
what is the probability that thc total number of fingers extended is even? 
Odd? Greater than 4? Less than 2? Prime? (Note. Set up a sample space as 
a first step.) 
12. Two rods, one black and one white, have square cross sections. Each 
rod has its faces numbered 1, 2, 3, and 4. The rods are rolled on the floor, and 
the numbcrs on their upper faces are read after they come to rest. Set up a 
table for a sample space of outcomes. If b is the number on the upper face of 
the black rod, and w that on the uppcr face of the white rod, evaluatc: 
(a) P(b q-w = 5) (b) P(b = w) (e) P(b > w q- 1) 
(d) P (black 1 or 3 and white 2 or 4) (e) P (sum of numbers evcn) 
(f) P (larger number shown is a 4) 
13. Repeat the experiment of Exercise 8 at the end of Section 3-2 for three 
engineer's rulers. (The third ruler is blue.) What is the probability that exactly 
one of the rulers shows a 2? That exactly two rulers show a 2? That all threc 
rulers show a 2? That the sum of the numbers shown is at least 7? 
14. Suppose that you have a black rod from Exercise 12 and a red engineer's 
ruler from Exercise 13. Rod and ruler are rolled on the floor, and the number 
on the top face of the rod and that on the bottom face of the ruler are noted. 
Set up a sample space and find the probability that the number on the black 
rod is greater than that on the red ruler. What is the probability that both 
numbers are the same? That the sum of the numbers is prime? 
15. In the sample space of Exercise 10 at the end of Section 3-2 assume that 
all points are equally likely. What is the probability that B will be included in 
the selection? That both A and B will be included? That either A or B will 
be included? That the selection will be C', D, and E? 
REVIEW EXERCISES FOR SECTIONS 3-1, 3-2, AND 3-3 
1. The numbers from 1 through 15 are painted on 15 balls, one number per 
ball. If one of these balls is drawn at random, what is the probability that the 
number on it is: (a) Diwsble by 5? (b) Even  (c) Odd? (d) A perfcct square? 
(e) A 2-digit numbcr? (f) A prime numbcr? (g) A prime number that is 2 
morc than another prime? 
2. A bag contains 5 times as many red marbles as black marbles (idcntical 
except for color). One marble is drawn at random. What is the probability that 
it is rcd? 
72 EQUALLY LIKELY OUTCOMES [CHAP. 3 
3. A regular icosahedron is a symmetrical solid with 20 faces. Some of the 
faces of such a solid are painted red and the rest are painted blue. If, when 
the icosahedron is thrown onto the floor, the probability of a red face landing 
on the bottom is 4 times the probability of a blue face, how many faces are 
painted red? 
4. A poll is taken among 70 residents of a suburb of Boston on the question 
of an ordinance to prohibit motorboats on the upper Mystic Lake. The results 
of the poll are tabulated as follows: 
Own Own Own 
Own 
motorboat sailboat motorboat Totals 
neither 
only only and sailboat 
Favor 
0 7 1 18 26 
ordinance 
Oppose 20 2 3 5 30 
ordinance 
No opinion 0 1 1 12 14 
Totals 20 [ 10 , 5 ,, 35 70 
If one of the 70 persons is chosen at random, what is the probability that he: 
(a) Favors the ordinance? (b) Opposes the ordinance? (c) Favors the ordinance 
or has no opinion on it? (d) Owns a boat? (e) Owns a sailboat? (f) Owns a 
motorboat ? 
5. A committee of two persons is to be selected from three men (Archer, 
Baker, Connor) and two women (Davis and Eads). Describe two different 
sample spaces for the experiment. 
6. You ask a friend to "think of a number." Describe a sample space for the 
experiment. 
7. A teacher asks each member of his class to count the number of pencils 
that he (or she) has brought to class. Describe a sample space for the experiment. 
8. Cards are dealt one after another from an ordinary bridge deck until the 
first ace appears. Describe two diffcrent sample spaces for the experiment. 
9. A coin is tossed repeatedly until a head first appears, or untfi tails appear 
four times in succession. Describe a sample space for the experiment. 
10. A dic is thrown until a "2" appears. Describe a sample space for the 
experiment. 
11. You ask each of 25 different people to tell you their birthdays. Describe 
a sample space for the experiment. 
12. A plant breeder crosses two parcut strains, each possessing a gene pair 
of type aA. Each parent contributes one-half of this gone pair (either a or A) 
to the offspring, where the txxo halves are combined. Describe a sample space 
for the genetic type of the offspring. 
3-4] EVENTS AD SETS 73 
13. Suppose that m places are to be filled from n candidates, here n _ 
(a) How many ways are there of selecting m candidates from among the 
available? (b) In how many of tile different ways of part (a) is a particular 
candidate A included in tile selection? (c) Assuming all selections of  from 
among the n are equally likely, what is the probability that candidate A is 
included? Discuss your result. Does it seem to be reasonable? 
14. Txxo eards are drawn from an ordinmy bridge deck, one after the other, 
and without eplaeement. (a) If order counts, how many different ordered pars 
(z, j) are there in a sample space of the expemnent, xxhere x denotes the 
card drawn and !! denotes the second card? (b) What is the pobability of each 
point in this sample space? (e) What is the pobability that the first card is an 
ace and the second card is a jack? (d) That one of the eards is an ace and the 
other is a jack? 
3-4. EVENTS AND SETS 
As already indicated in Sections 3-1 and 3-3, when used as a technical 
word an "credit" is a subset of a sample space S of an experiment. We have 
seen that subsets, or "events," may be described either verbally or by 
algebraic equations and inequalities. Such descriptions define subsets 
of S corresponding to the "events" under consideration. 
A note o, "or" and "ad." In everyday English, expressions of the form 
"A or B" use the word "or" in two different ways: 
(1) in the excl,siz'e sense, which connotes "A or B but not both" (for 
example, a coin falls "heads or tails"); 
(2) in the zclu.sire ense, which connotes "A or B or both" (for example, 
consider the statement: "I may visit France or Italy this summer."). 
Ordinarily, the context is a sufficient guide to the intended meaning. 
ttowever, when we use the expression "A or B" in referring to eents, 
the meaning is never in doubt because we alwa?1s use the inclusive "or"; 
in other words, "A or B" means "A or B or both." 
The foregoing usage agrees with the definition of A t_J B, that is, the 
tnion of sets A and B. For, 
A U B is the set of all points belonging to A or to B or to both. 
Thus, in this book, "the probability of A or B" always means 
P(,4  ). 
The idea of simultaneous membership in two sets is connoted in our 
use of "and" when we talk about evenIs. Thus if events A and B are 
74 EQUALLY LiI(ELY OUTCOMES [CHAP. 3 
subsets of a sample space, then'"A and B" is their intersection; that is, 
the event "A and B" contains those sample points that belong to both 
A and B. For, 
the intersection of A and B, A C B, is the set of all elements 
belonging to both A and B. 
When we use the verbal description "the probability of A and B," we 
P(n a S). 
Vc shall illustrate these ideas with further examples based on Table 3-3. 
]XAMPLE 1. In the two-dice experiment, what is the probability that 
r<3orc<27 
Solution. For the outcome r _ 3, the red die must show either 1 or 
2 or 3. The corresponding set A consists of the 18 points in the first 
three rows of Table 3-3. For c _ 2, the clear die must show either 1 or 2, 
and the corresponding set B consists of the 12 points n the first two col- 
umns. The points in the union of A and B correspond to the event 
r _ 3 or c _ 2. To find the number of points in A LJ B, we must not 
add the number in A to the number in B, because there are 6 points that 
are in both sets and we must not count these twice. The correct count 
of points n A [_J B is 
lS- 12 -- 6 = 24. (1) 
Therefore the probability of r < 3 or c < 2 is 2 2 
-- -- -6, or , 
We notice in the above calculation that 18 is the number of points in 
A, 12 is the number in B, and 6 is the number in their intersection A (3 B. 
Dividing all terms of Eq. (1) by 36, we get 
18 12 6 24 
3 + 3-- 3-= 3' (2) 
Thus, in this example, we nmy say that 
P(A) + P(B) -- P(n a S) = P(A U S); (3) 
for, 
__ 12 
xs P(B) -- , 
P(A) -- , 
-- 36 -- 36' 
In the next section, we show that Eq. (3) is true in general. 
3-5] MUTUALLY EXCLUSIVE EVENTS 75 
EXERCISES FOR SECTION 3-4 
Exercises 1 through 7 refer to the sample space for the two-dice experiment 
of Table 3-3. 
1. What is the probability that r > 2 or c > 3? 
2. What is the probability that r > 2 and 3c > 3? 
3. What is the probability that r < 2 or c < 4? 
4. What is the probability that r < 2 and c < 4? 
5. What is the probability that r - c = 5 or r - c = 7? 
6. What is the probability that r - c = 5 and r - c = 7? 
7. If A is the event "r is greater than 4" and B is the event "c is greater 
than 2," prove that 
P(A) ,x_p(B) -- P(AfB) = P(A UB). 
Exercises 8 through 14 refer to the sample space for Example 2, Section 3-3 
(Table 3-5). 
8. What is the probability that treatments a or c are chosen? 
9. What is the probability that treatments a and c are chosen? 
10. What is the probability that a and b or b and c are chosen? 
11. What is the probability that a and b and b and c are chosen? 
12. What is the probability that e is chosen or that b and c are chosen? 
13. What is the probability that e is chosen and that also b and c are chosen? 
14. If A is the event "a is chosen" and B is the event "b is chosen," show 
that 
P(A) d- P(B) -- P(A f B) = P(A U B). 
3-5. MUTUALLY EXCLUSIVE EVENTS 
If two events cannot happen at the same time, they are said to be 
mzdually excltst,e. The computation of probabilities is especially simple 
when an event consists of other mutually exclusive events. 
EXAMPLE 1. In the two-dice example of Section 3-3, what is the 
probability that the sum r -]- c is 7 or 107 
Solution (Refer to Table 3-3.) There are 6 sample points with 
r- c---- 7, and 3 withr - c---- 10. Since the corresponding sets do not 
overlap, there are 9 points with sum 7 or 10. Hence the probability is 
9 1 
a-, or . 
In set language, if A is the set of points with r -]- c = 7, and B is the 
set with r -]- c ---- 10, then, for this example, xve have 
P(A) + P(B) = P(A U B). 
76 EQUALLY LIKELY OUTCOMES [CHAP. 3 
The equality follows because � 
P(A) %, P(B) a and P(A U B)  
Equation (1) is like Eq. (3) of the previous section, with P(A f B) ---- O. 
:3-3 Definition. Mutually exclusive events. If two events have no points 
n common, they are called mutually exclusive, or disjoint. And 
 events are mutually exclusive if no two of them have any points 
in common (Fig. 3-2). 
We no a consequence of this definition: he ierseeio of wo or more 
mtduall exhswe events is he emp seL 
Fro. 3-2. Graph of mutually exclusive or disjoint subsets. 
3-4 Theorem. Probability of A U B. If A and B are events in a finite 
sample space S (Fig. 3-3), then 
P(A U B) = P(A) d- P(B) -- P(A  B). (2) 
Proof. The probability of A tO B is the sum of the probabilities of the 
points in A tO B. Now P(A) (- P(B) is the sum of the probabilities of 
points in A plus the sum of the probabilities of points in B. Therefore 
P(A) (-P(B) includes the probabilities of points in the intersection 
A ( B twice. If we subtract this probability P(A N B) once, we shall 
have the sum of the probabilities of all points in A tO B, each taken just 
once. Hence 
P(A tO B) = P(A) + P(B) -- P(A Fq B). [] (3) 
3-5 Corollary. If A and B are disjoint, then 
P(.4 tO B) = P(.4 ) + P(B). [ (4) 
3-5] MUT...� EXC.USIV VETS 77 
Equation (4) follows at once from Eq. (3) since, if A and B are dis- 
joint, A ( B ---- , the empty set, and P(A  B) ---- P(5) ---- O. 
Fro. 3-3. Events in S. 
3-6 Corollary. Several disjoint e,enls. Let .t, .12, As,..., A,, be 
mutually exclusive eyelets. Then 
In words, the probability of A or A or... or A,, is the a,m of 6heir 
probabilities, provided the events re mutually exclusive. 
Proof. (Cf. Fig. 34.) The probability of the union of A, A, and 
so on, is the um of the probabilities of its points. The sum 
P(A) P(A) 4 .... � 
is the sum of the protbilities of the points in A, plus the sum for A, 
and so on. Since the sets do not overlap, ths sum includes the proba- 
bilities of the points in the union, once nd only once for each point.  
S 
FIG. 3-4. Disjoint sets. 
3-7 Definition. Partili(m. If the events A, A2, ..., Am are mutually 
exclusive and exhaustive (i.e., thei union contains all the sample 
points of S), then we say that the m events form a partition of 
the sample space S into m subsets. 
78 EQUALLY LIKELY OUTCOMES [CHAP. 3 
For example, Fig. 3-5 illustrates a partition of S into 8 subsets' 
S ---- A1 LJ A2 CJ Aa CJ A4 CJ As CJ A6 LJ A7 LJ As. 
FiG. 3-5. A partition of S. 
3-8 Theorem. Probabzlities under a partition. If A , A 2, . . . , A , 
form a partition of a finite sample space S, then 
P(A) +-P(A) +... 4-P(A,) = 1. 
Proof. From Corollary 3-6, we have 
P(A1) - P(A2) - -'- - P(Am) = P(A [J A2 [J ''' [J Am) 
= y(s) 
EXAMPLE 2. A high-school basketball coach has available three com- 
plimentary tckets to a professional basketball game. He decides to give 
the tickets to three players chosen at random from the five players of 
his first string team: Art (a), Bob (b), Chuck (c), Dick (d), and Ed (e). 
What is the probability that both Art and Bob are chosen, or both Chuck 
and Ed are chosen, or Bob, Chuck, and Dick are chosen? 
olutzom A sample space COlmists of the (a), or 10, possible selections 
of the 5 players, taken 3 at a time: 
S ={abc, abet, abe, acd, ace, ade, bcd, bce, bde, cde}. 
3--5] MUTUALLY EXCLUSIVE EVENTS 79 
We now select subsets of S that correspond to the three events in which 
we are interested. These subsets and their verbal descriptions are tabulated 
as follows: 
Verbal description Event 
Art and I]ob are chosen A = {abe, abd, abe} 
Chuck and Ed are chosen A2 = {ace, bee, cde} 
Bob, Chuck, and Dick are chosen .13 = {bed} 
Since the three players are chosen at rag:dom, we assign to each sample 
point of S probability 1-!6. Then, because the events A, A2, and A3 
are mutually exclusive, we have 
 03-k0.3-k0.1=0.7. 
Ex.w 3. In the two-dice example of Section 3-3, what is the proba- 
bility of not getting a double? 
Sohdion. There are 6 points in Table 3-3 that correspond to the event 
"throwing a double." Denote this set by A. Then 
The desired probability of not getting a double is 
The cvent "getting a double" and the event "not getting a double" 
m'e mutually exclusive. They are also said to be "complementary." 
The two events, getting a double and not getting a double, together 
exhaust all possible outcomes. 
3-9 Definifim Complementary events. An event A and the event A, 
consisting of all points of the same sample spacc not in A, are 
called complementary events. 
Thus any event A and its complementary event A are mutually ex- 
clusive, and their union is the whole sample space. In other xvords, events 
A and  form a partition of S into two subsets. 
80 EQUALLY LIKELY OUTCOMES [CHAP. 3 
3-10. Theorem. Complementar. y events. If A and A are complementary 
events, then 
Proof. Since A and . are disjoint, formula (4) gives 
P(n u = P(n) + 
Since A U  is the entire sample space S, it follows that 
P(A U ) = P(S) = 1. 
Therefore 
= 
This formula was obtained in Section 3-1 as a consequence of the 
defiuition of probability of an event for sample spaces vith equally likely 
outcomes. The present proof is also valid for more general sample spaces 
that will e studied in Chapter 4. 
EXERCISES FOR SECTION 3-5 
1. A die is rolled. Let E be the event "die shows 4," and F be the event 
"die shows even number." Are events E and F mutually exclusive? 
2. A die is rolled. Let E be the event "die shos even number," and F be 
the event "die shows odd number." Are events E and F complementary? 
Are they mutually exclusive? 
3. What is the probability of throing a one or a two or a three with a single 
fair dic? 
4. If the probability that .1 wins a game is 0.6, what is the probability that 
A loses? Are the two events "A wins" and ". loses" mutually exclusive? 
Are they complementary? (What about a tie?) 
5. Two coins are tossed. E is the event "getting two heads," and F is the 
event "getting two tails." Are events E and F mutually exclusive? Are they 
complementary? Evaluate P(E  F). 
Exercises 6 through 16 refer to the to-dicc experiment of Example 1 in 
Section 3-3. Find the probability that: 
6. The sum of the spots is not 11. 
7. The two dice show only 3 or 4 or both. 
8. Neither 3 nor 4 appears. 
9. Each die shows 3 or more spots. 
3-6] INDEPENDENT EVENTS 81 
10. At least one die shows fewer than 3 spots. 
11. Both dice show fewer than 3 spots. 
12. Only one die shows fewer than 3 spots. 
13. r - c is even or r - c is odd. 
14. -� = 4orr- � = 11. 
15. r_< 2-be. 
16. rc. 
17. Refer to Example 2 of this section. If E is the event "Dick and Ed are 
chosen," F is the event "Bob and Chuck are chosen," and G is the event "Chuck, 
Dick and Ed are chosen," find P(E U F U G). 
1S. If the probability of Jim's winning a race is � and the probability of 
Tom's winning is x 
X, what is the probabdity that either Jinx or Tom will win 
if the}' are in the same race? 
19. Three coins are tossed. Find the probability of getting (a) no heads, 
(b) at least one head. 
20. The integers 1, 2, 3, ..., 20 are written on slips of paper which are placed 
in a bowl and thoroughly mixed. A slip is drawn from the bowl at random. 
What is the probability that the number on the slip is either prime or divisible 
by 3? 
3-6. INDEPENDENT EVENTS 
The present discussion introduces the notion, and leads to the definition, 
of independent events. When we say, in everyday language, that two 
events "have nothing to do with each other," we are describing what, 
in technical language, are called "independent events." We begin with an 
illustrative example. 
EmaMLE 1. In the two-dice experiment of Section 3-3, what is the 
probability that r _< 3 and c >_ 5? 
Solution. The event that concerns us requires that two conditions be 
satisfied simultaneously. If A is the set of points with r _< 3 and B 
is the set with c > 5, then we want to know the number of points that 
these sets have in common--in short, their intersection, A 91 B. This 
intersection is the 3 X 2 array of points in the first three rmvs and the 
last two columns of Table 3-3. Itence we have 
P(A 91 B)  -- - 
-- 36 -- 6' 
By counting, we find that 
P(A) s __ x and P(B) x2  
-- 36 --  -- 36 -- 3, 
82 EQUALLY LIKELY OUTCOMES [CHAP. 3 
since A has 18 points and B has 12. Using these probabilities and the 
answer for P(A ( B), ve verify that, for this example, 
P(A N B) = P(A) � P(B). (1) 
This multiplication formula, (1), agrees with the results obtained by an 
intuitive approach to the problem. For, consider a very long series of 
throws of the two dice. We expect to find r _ 3 in about half of these 
throws. Let us restrict our attention to this half of the throws. Of these 
throws, how many have c _ 5? Since what happens on the red die does 
not affect the clear die, it seems reasonable that about � of the throws 
with r < 3 will also have c > 5. Thus the fraction of throws with both 
r _ 3 and c _ 5 is about � of �, or -. 
Is formula (1) true in general? The answer is "no," as we shall see 
in Example 4. When formula (1) holds, the events A and B are called 
independent events; otherwise they are called dependent events. Our intu- 
ition suggests that the fall of the red die is independent of the fall of the 
clear die. For it seems evident that the fall of the red die has nothing 
to do with the fall of the clear die, and our everyday usage of the word 
"independent" implies just that. Moreover, it turns out that when two 
everyday events "have. nothing to do with each other," the probability 
that both events occur is obtained by the multiplication of their separate 
probabilities, as we have just seen in Example 1. 
:For technical purposes, however, we need a definition that frees us from 
the vagueness of the expression "have nothing to do with each other." 
Such a technical definition, suggested by the results of problems similar 
to Example 1, is now given. 
3-1 1 Definition. Independent events. Events A and B are independent 
if and only if 
P(A ( B) ---- P(A) � P(B). (1) 
The foregoing definition provides us with a clean-cut meaning for 
"independent events"; if two events A and B do not satisfy Eq. (1), 
the events are dependent. 
3-12 Theorem. If A and B are independent events with nonzero 
probabilities, then sets A and B have a common sample point. 
Proof. Let � represent the empty set. Either A Cl B----� or 
A clB  �. If A clB = �, then P(A clB) = 0, and from (1) it 
3-6] INDEPENDENT EVENTS 
follows that P(A) ---- 0 or P(B) ---- O. Since this contradicts the hypoth- 
esis of the theorem, it follows that .4 N B  b. [] 
EMPLE 2. Independent e,cnts: cois. Tvo coins are tossed. Show that 
event "head on first coin" and event "coins fall alike" are independent. 
oluton. A sample space for the experiment is 
S = {HH, HT, TH, TT}. 
Let event A be "hed on first coin" and event B be "coins fall alike." 
Since the four outcomes in S re equally likely, we assign to ech the 
probability . Therefore we have 
A  {UU, HT}, P(A)    , 
B  {UU, TT}, P(B)  - , 
A aB = {UU}, P(A aB) = . 
Hence it follows that 
P(A  B) = P(A) . P(B), 
and events A and B are independent, by Definition 3-11. 
In the following examples, we exhibit first a case in which the definition 
of independence is satisfied, and second a case in which events do not 
satisfy the definition of independence. 
EMP 3. Independent evts: dice. In the two-dice experiment of 
Section 3-3, what is the probability that the red die shows even and the 
clear die shows odd? 
Solution. Let us count points in the sample space S (see Table 
There are 18 points (3 rows) with r even and 18 points (3 columns) with 
c odd. These two 18-point sets have 9 points in commonsthe 9 points 
where the three rows intersect the three columns. Hence there are 9 
points with r even and c odd, and we have 
P (r even and c odd)  --  
--36--4' 
Since 
P (r even) xs 
and 
P (c odd)  s 
the events "r even" and "c odd" re independent, by Definition 3-11. 
84 EQUALLY LIKELY OUTCOMES [CHAP. 3 
EXAMPLE 4. Dependent eveits: dice. In the two-dice experiment of 
Section 3-3, what is the probability that the sum on the two dice is 11 
(r -]- c  11) and, at the same time, r  57 
Sohdion. There are two points in Table 3-3 with r  c  f]' (5, 6) 
and (6, 5). If we denote this set of two points by , then 
P(E) =  8. 
6 
Let F bethe set defined by r  5. Then Fhas 30 points and 
0  
P(F) -- a6 -- ' 
Since the simultaneous event E and F has only the single point (6, 5), 
we have 
- &. 
Since  is not equal to the product of  and , E and F are dependent 
events, by Definition 3-11. 
REMARK. When three or more events are independent, the probability 
of their simultaneous occurrence is the product of their probabilities. Thus, 
for example, if , F, and  are independent, then 
P(E  F  G) = P(E) � P(F) � P(G). (2) 
WARNING 
There is a danger of confusing mutually exclusive events with 
independent events. A source of this confusion is the common 
expression "have nothing to do with each other." This expres- 
sion provides a useful description of independence when applied 
to everyday events. But when applied mistakenly to sets, it 
suggests nonoverlapping; and nonoverlapping sets are mutually 
exclusive and are not independent. Indeed, in dealing with 
independent events A and B in a smnple space, we know that 
the sets A and B must have a point in common if both A and 
B have nonzero probabilities. (Cf. Theorem 3-12.) 
EXERCISES FOR SECTION 3-6 
1. In the two-dice experiment of Example l, Section 3-3, show that the event 
"r  4" and the event "c  3" are independent. 
2. Three coins are tossed. Show that the event "heads on the first coin" and 
the event "tails on the last two" are independent. Show that the event "two 
coins heads" and the event "three coins heads" are dependent. 
3-7] COXI)lTIONAL PP, O BAB ILIT� 85 
3. If a coin is thrown four times, what is the probability that it will fall 
heads on the first throw, tails on the next two throws, and heads on the fourth 
throw ? 
4. A pair of dice is tossed txxiee. What is the probability that, on the second 
toss, each de shows spots different from those it showed on the first toss? 
sume indetcndence of the outcomes of the two tosses. 
5. i die is tossed three times. What is the probability that the first toss will 
show odd, the second toss even, and the third toss a six? Assume independence 
of the outcomes of the three tosses. 
6. In a ecrtain school, examination results showed that 10% of the students 
failed nathematics, 12% failed English, and 2% falcd both mathematics and 
English. A student is selected at random from the sehool roll. Are the event 
"student faled mathematics" and the event "student failed English" inde- 
pendent? 
7. If E is any event in sample space S, show that E and S are independent. 
Are E and � independent ? 
S. A bag contains 5 black marbles, 4 red marbles, and 3 xxhitc marbles. 
Three marbles are drawn in succession, each marble being replaced before the 
next one i drawn. What is the probability that the first marble is black, the 
second red, and the third white? 
3-7. CONDITIONAL PROBABILITY 
Often we deal with probabilities for part rather than all of a sample 
space. The probability that a person randomly selected from a population 
has blue eyes differs from the probability of blue eyes for a person randomly 
selected from the blondes in this population. For a set of students about 
to take a mathematics course, the probability that a randomly selected 
one will get an honor grade is lower than the probability for those who 
made honor grades in their last txvo mathematics courses. The chance 
of a serious fire in the next year in a warehouse selected at random from 
those in a large city differs from that in the subpopulation consisting only 
of fireproofed warehouses. Each of these examples focuses attention on 
the probability of an event in a subset of the original sample space, and 
emphasizes that the probability in the subset may differ from that in the 
whole space. The subpopulations are defined by extra conditions beyond 
those for the xvhole population, and probabilities associated with events 
in these subpopulations are called conditioal probabilities. 
To introduce the idea of conditional probability, wc discuss the follow- 
ing example, based on the two-dice experiment of Section 3-3. Refer 
to Table 3-3. 
EXAMPLE 1. Given that r -c < 4, find the probability that r = 1. 
86 EQUALLY LIKELY OUTCOMES [CHAP. 3 
Discussion. First, we need soYne idea of xvhat such a probability means. 
Among all throws of tvo dice, some produce a sum r - c that is less 
than 4, and others do not. We ignore all that do not and obtain a reduced 
sample space, S , consisting of three points: 
Since these three outcomes were equally likely in the original sample 
space S, we assign them equal probabilities in the reduced sample space S . 
Since they are the only points in S , we assign to each of them probability 
 The event defined by r = 1 consists of the two points 
(1, 1) and (1, 2). 
Therefore the probability of r = 1 in the reduced sample space S' is 2 
We call this result the conditional probability that r: 1, given that 
r-c< 4. 
To study conditional probability further, consider, in the original sample 
spaceS, the sets that correspond tot +c < 4 and tot---- 1. For con- 
venience, these are tab'ulated in Table 3-6. 
TABLE 3--6 
Condition Event 
r-t-c < 4 B = {(1,1), (1,2), (2,1)} 
r = i A = {(1,1), (1,2), (1,3), (1,4), (1,5), (1, 6)} 
Since we want to know the chances of A given B, we naturally are 
interested in the event A N B corresponding to the set of points that 
are simultaneously in A and B. We have: 
A N = 2)}. 
What are the probabilities of events A, B, and A  B in the original 
sample space S? They are 
P(A) s P(B)  P(A (q B)  
-- 36, -- 36, -- 36' 
3-7] coxm.mXL PROBABILITY 87 
(Note that we purposely do not reduce these fractions to lowest terms. 
Sometimes such reduction obscures the pattern that we hope to discover.) 
The usual notation for "the probability of A, given B" is 
P(AIB). 
The vertical bar is read "given." 
In the foregoing example, our first solution led us to the result 
P(AIm) = 3. 
We now observe that, in this example, we also have 
P(A C B) = P(B). P(AIB), (1) 
since 
36 -- 36 3 
Formula (1) suggests an altcrnative way of getting P(A]B). For Eq. (1) 
implies 
p(AiB) = P(A c B). 
P(B) 
It is also interesting to consider the result obtained by interchanging 
A and B on both sides of Eq. (1). The event A  B is the same as the 
event B  A. Thus, for this example, we wonder if it is also true that 
P(A  B) = P(B a A) = P(A). P(BIA ). (2) 
(See Exercise 2 at the end of this section.) 
EXPLE 2. Two a's and two b's are arranged in ordcr. All arrange- 
mcnts are equally likely. Given that the last letter, in ordcr, is b, find 
the probability that the two a's are together. 
Solution. A sample space of possiblc orders of the four letters is as 
follows: 
S = {aabb, abab, abba, baab, baba, bbaa}. 
Consider a reduced sample space, B, whose elements have b as the last 
letter: 
B = laabb, ahab, baab}. 
88 EQUALLY LIKELY OUTCOMES [CHAP. 3 
Since we want to know the pr)bability that b is the last letter and that 
the two a's are together, we look for the points in B that contain aa. 
The required points are the intersection of B and A, where A is the set 
of all points that include aa: 
A : {aabb, baab, bbaa}. 
Therefore 
A ('1 B = {aabb, baab}. 
If we treat all points of B as equally likely and as constituting a reduced 
sample space, we assign probability � to each of them. Since two points 
of B contain aa, it follows that 
Is Eq. (1) also satisfied in this example? In the original sample space S, 
we have 
Hence, in this example,, 
a = 
since 
6 -- 6 13 
In Exercise 2 at the end of this section, you are asked to verify that 
Eq. (2) also holds for this example. 
IEMAaK. Example 2 can be solved by making direct use of the ideas of 
Chapter 2. For if b is in the last position, the remaining three letters 
a, a, b can be arranged in 3!/2! or 3 ways. Of these three arrangements, 
only 2 (aabb and baab) have the two a's togethel-. Since the three possible 
arrangements with b in the last place are equally likely, we have 
_P (a's togetherlb in last place) ---- 2 
The foregoing examples, and others like them, lead us to adopt the 
following defiuitions. 
3-13 Definitions. Conditioal probability and reduced sample space. 
The conditional probability of A, given B, is denoted by P(A lB), 
and is defined by the equation 
3-7] CONDITIONAL PROBABILITY 89 
P(A]B) -- P(A C B) 
P(,B) ' if P(B)  O. (3) 
The reduced sample space is B, the given event. 
All probabilities are referred to some sample space, and P(A) is an 
abbreviation for P(AIS), where S is the whole sample space. But the S 
is ordinarily dropped as understood. When some subset of S, such as B, 
is known to contain all the outcomes of the experiment, theix we need to 
be explicit ,and write P(A,B). In particular, P(B[B) ---- 1. 
RE'LiRa. 1. The probabilities in the fraction on the right side of Eq. (3) 
are probabilities of the events in the original sample space S. Of course, 
we get the same result if we first convert to the reduced sample space, 
which is B. For we then increase the total probability in B to 1; and, if 
we compute the probabilities on the right side of Eq. (3) in sample space B, 
the denominator is 1 while the numerator is the probability of A  B 
in B. 
ItEAR 2. The restriction P(B)  0 in Eq. (3) means that the given 
event B must not have probability zero. In finite sample spaces of equally 
likely outcomes, there must be a nonzero probability for B before it is 
useful to talk about the probability of A, given B. 
In more advanced work with infinite sample spaces, events of probability 
zero can occur, and conditional probabilities, given such events, can be 
sensibly interpreted. 
RELRK 3. Equations (3) and (1) are essentially the same. We get 
(1) by multiplying both sides of (3) by P(B). Conversely, assuming 
P(B)  O, we get (3) by dividing both sides of (1) by P(B). 
RELR 4. Even if we know P(A) and P(B), there isn't any formula 
for computing P(A 91 B) from them unless A and B are independent. 
We must treat A, B, and A 1 B as three individual sets in S. It may 
not be a trivial matter to construct A 1 B from the separate sets A and 
B unless the sets are small and we can list all the elements. 
WARNINGS 
(1) P(A I B) is rarely the same as P(A CI B). Indeed, the cou- 
ditiona} probability of A, given B, may be entirely different 
from JO(A) or from P(A 1 B). 
(2) AIB is not a symbol for a set. 
90 EQUALLY LIKELY OUTCOMES [CHAP. 3 
For example, in connection with warning (1), consider sample space S 
of the two-dice experiment. (Cf. Table 3-3.) Let A denote the event 
"red die even," and let /3 denote the event "clear die 2." By counting 
sample points in S, we see that 
P(A ( B) = a _ 2_ 
-- 12 
and 
EMPUE 3. In the two-dice experiment (ef. Table 3-3), if r  c = 11, 
what is the probability that the dear die shows 47 
,ol,tzo. The probability is zero; it is impossible to have c = 4 if 
r+c= 11. 
EX.MrLE 4. In the two-dice experiment of Section 3-3, given that the 
red dic shows 4, what is the probability that the dear die shows a number 
greater than 47 
Solution. Let B be the event described by r = 4, and A the event 
described by c > 4. Then B contains 6 sample points (ef. Table 3-3), 
A contains 12, and their intersection A  B contains 2 points, (4, 5) 
and (4, 6). 
Therefore we have 
-- 36, 
whence Eq. (3) gives 
= = 
Note that we obtain the result more directly by counting: 2 equally 
likely cases (c  5 or 6) out of 6 equally likely cases in B yield a proba- 
bility of , or . 
Since A has 12 sample points, we also have 
P(A) 
Thus, i this example, the conditional probability of A, given B, is the 
same as the probability of A. In other words, the iormation that the 
red die shows 4 does not change the probability that the clear die will 
show 5 or 6. The later probability is , regardless of the outcome on 
the red die. 
The foregoing example illustrates a general theoreln. 
3-7] CONDITION tL PROBABILITY 
3-14 Theorem. Conditional probability of idependent events If A 
and B are independent events having nonzero probabilities, then 
P(AIB)= P(A) and P(BIA) = P(B). (4) 
Proof. Since A and B are independent, and since A N B ---- B  A, 
we have from Eq. (2), Section 3-6, 
Since neither P(A) nor P(B) is zero, we may use them as divisors. 
Equation (3) gives 
P(AI) P(A m P(A). P(B): P(A). 
= p() - p() 
The proof that P(B[A) = P(B) is left as an exercise. 
EXERCISES FOR SECTION 3-7 
1. In the two-dice example of Table 3-3, given that r - c _ 10, find the 
probability that r = 5. Given that r - c = 8, find the probability that c _ 4. 
2. For Example 2, show that 
P(A CI B) = P(B  A) = P(A). P(BIA). 
3. Five-digit numbers are formed by permuting the digits 44433. All 
rangements are equally likely. Given that a number is even, what is the proba- 
bihty that the two 3's are together? 
4. Two dine are tossed. If the first die shows 5, what is the probability that 
the second die shows even? 
Exercises 5 through 16 are based on the following data. Six boys (Joe, Sam, 
Tom, Dick, Harry, and Pete) form a club. They decide to select from their 
humbin a committee of three. The selection process is to be by lot, so that all 
twenty possible committees are equally probable. 
5. Verify that the number of committees of 3 that can be selected from 
6 boys is 20. 
6. Describe a process of selecting such a committee by lot. 
7. Set up a sample space of 20 points to represent the 20 possible committees. 
92 EQUALLY LIKELY OUTCOMES [CHAP. 3 
8. What is the probability that Sam is on the committee? That Sam is not 
on the committee? 
9. What is the probabihty that Sam is on the committee and Tom is not? 
10. What is the probability that neither Sam nor Tom is on the committee? 
11. Given that Sam is on the committee, what is the probability that Tom 
is also on it? 
12. What is the probability that Tom, Dick, and Harry arc not all on the 
committee ? 
13. Given that Tom and Dick arc on the committee, what is the probability 
that Harry is not? 
14. What is the pobability that Joc or Pete o both are on the committee? 
15. Suppose that Joc and Sam are brothers, Tom and Dick are brothers, 
and Harry and Pete are bothcrs. What is the probability that the committee 
has two brothers on it? 
16. Instead, suppose that Joe, Sam, and Pete are the only brothers. What 
is the probability that the committee has no two or more brothers on it? 
17. If p is the probability that an event will happen in one trial, show that 
the probability of its happening n each of n independent trials is p. 
18. If 2 perons are chosen from 10 and all choiee are equally likely, what is 
the probability that two specified persons will both be chosen? That they will 
not both be chosen? That neither of them will bc chosen? 
19. Of 100,000 persor/s living at age 20, statistics show that 47,73 will be 
alive at 70. What is the probability that a person aged 20 fil hve to be 70? 
That he will die before he is 70? 
20. The probability that J il] die within the next 20 years is 0.025, and 
that B will die within the next 20 years is 0.030. liThat is the probability that 
both J and B will die within the next 20 years? That A will die and B will 
not die? That neither A nor B will (lie? 
21. Seven persons form a line at random. liThat is the probability that two 
specified persons arc next to each other? That these persons ae not next to 
each other? 
22. A basketball player has a probability of � of scoring on a free throw. 
How many frec throws would hc have to take n order to make his probability 
of scoring one or more times at least 0.99? 
23. If p is the probabfiity that an event will happen in one trial, show that 
the probability that it will happen at least once in n independent trials is 
1 -- (1 
24. A bag contains 3 white marbles and 4 black ones. In succession, three 
persons each draw a marble, without replacing it in the bag. The first person 
who draws a white marble sins. What are the respective chances of the person 
draing first, the person draing second, and the person drawing third? (They 
continue until someone wins.) 
25. If you stop 3 people at random on the street, what is the probability 
that all ere born on Friday? That two were born on Friday and the other 
on Tuesday? That none were born on Monday? 
3--8] SAMPLE SPACES ,V1TI! MANY ELEMENTS 93 
26. Two numbers arc selected at random from 1, 2, 3, . .., 10. What is the 
probability that the sum of the two numbcr is even? 
27. A_ buyer will accept a lot of 10 tadres if a amplc of 2, picked at random, 
contains no defcctiw's. What is the probability that hc will accept a lot of 10 
if it contains 4 defectives? 
2S. A comnuttcc of 3 is ohostin from a group of 20 people. What is the proba- 
bility that a specified member of thc groul) xxill be on the committee? That this 
specified member xxill not be on the committee? 
29..\ committee of 4 is chosen at random from 5 married couples. What 
is the probabihty that the committee will not include a busband and xxife? 
30. Seven-digit numbms are formcd by permuting the digts 1, 2, 3, 4, 5, 6, 7. 
If all pcrnmtations are equally likely, what is the probability that, in a permuta- 
tion selected at random, the odd digits will occur in ascending order? 
3-8. SAMPLE SPACES WITH MANY ELEMENTS 
When the number of elements in a sample space is very large, it is 
inconvenient to make a list. However, even without a list, the methods 
of counting developed in Chapter 2 may enable us to calculate proba- 
bilities for sample spaces with equally likely outcomes. The following 
examples illustrate the methods. 
EMr,E 1. The first ace. An ordinary bridge deck of 52 eards is 
thoroughly shuffled. The cards are then dealt face up, one at a time, 
until an ace appears. What is the probability that the first ace appears 
(a) at the fifth card? (b) at the kth card? (c) at the kth card or sooner? 
Solutiom (a) There are several possible sample spaces for this experi- 
ment. We choose one as follows. Once the cards are shuffled and in posi- 
tion in the deck, the only feature of each card that concerns us is whether 
it is an ace (A) or a non-ace (N). There are 48 non-aces and 4 aces, so 
we consider all possible arrangements of 48 N's and 4 A's in 52 numbered 
positions. There are 
4814! 
permutations of 48 N's and 4 A's, and each of these is a point in our 
sample space S. We assume that all points are equally likely and assign 
probability 1/(s42) to each point. 
Consider now an event E in sample space S, where E is described as 
"the first ace appears at the fifth card." If the first ace is in fifth place, 
then the first five symbols of every sample point in E are 
NNNNA, 
.04 EQUALLY LIKELY OUTCOMES [CHAP. 3 
in that order. The number of points in E is, therefore, the number of 
ways of arranging the remaining 44 N's and 3 A's in the remaining 47 
places. This number is 
44!3! 
Hence 
p()_ (V) 6,25 0.060. 
(2) 270,725 
(b) Similarly, if the first ace appears in the kth place in the row, then 
the remaining 3 aces and 48 -- (k -- 1) or 49 -- k non-aces can be 
arranged in the last 52 -- k positions in ($2-k) ways. Hence, 
P (first ace in kth place) -- , k: 1, 2, . .., 49. 
(c) Denote by F the event "first ace at kth card or sooner." Then 
the complementary event F is the event "4 aces after kth card." The 
first k symbols of every sample point in F are all N's. Therefore the 
number of sample points in  is the number of ways of arranging 4 A's 
and 48 -- k N's in the remaining 52 -- k places. This number is 
(52--k)! _ (52--k). 
(48 - k)!4! 4 
Hence 
P(F) -- , 
and 
_ (-k) 
P(F) =  -P(F) -- 1 --,  = , ' 48. 
If k  9, we obtain 
P(F) = 1 -- 0.46 = 0.54. 
Thus there is a better than even chance that the first ace appears at or 
before the ninth card. 
3-8] SAMPLE SPACES V'ITH MANY ELEMENTS 05 
REMARK. The foregoing example affords an instance in which the so- 
called "maturation of chance" operates. In the light of this example, the 
student may wish to re-read the discussion in Section 1-6. 
E.'OrLE 2. The birthdory problem. There are k people in a rooln. What 
is the probability that at least two of these people have the same birth- 
day, that is, have their birthdays ou the same day and lnonth of the year? 
What is the sinailest value of k such that the probability is i- or better 
that at least two of the people have the same birthday? (Write down 
your guess.) 
Sohtlion. We shall neglect February 29 and deal with a 36a-day year. 
There are 365 possibilities for each person's birthday, and hence 365  
possibilities for the birthdays of k people. Thus our sample space S 
has 365 i points, each of which is an ordered k-tuple 
(3:1, -T2, .['3, � � � , 'Tk), 
where x represents the birthday of a first person, x_9 represents the birth- 
day of a second person,..., and x represents the birthday of the kth 
person. We assume that all of the 365  possible outcomes are equally 
likely, and assign to each sample point probability 1/365 . 
Consider now an event E in sample space S, where E is described thus: 
"no two of the k people have the same birthday." Under this restriction, 
the birthday of a first person has 365 possible values, that of a second 
person 364 possible values, that of a third person 363 possible values, ..., 
and that of the kth person 365- (k- 1), or 365- k q-1 possible 
values. Therefore, by the multiplication principle, the number of possible 
sets of k birthdays with no two birthdays alike is 
365 � 364 � 363 ..- (365 -- k  1), 
and this number is the numbel' of sample points in F. 
It follows that 
365. 364. 363..- (36;5 -- k q- 1). 
P(E) = 365 
Finally, 
P (at leaat 2 birthdays are the same) ---- 1 -- P(E). 
96 EQUALLY LIKELY OUTCOMES [CHAP. 3 
Probabilities for specific valdes of ]c yield some rather startling informa- 
tion. Some results are given in Table 3-7. With as few as 23 people in 
the room, there is a better than even chance that two people have identical 
birthdays ! 
TABLE 3--7 
Number of people in 5 10 20 23 30 40 60 
room 
Probability that at 
least two birthdays 0.027 0.117 0.411 0.507 0.706 0.891 0.994 
are the same 
REMARK. Although in examples such as the foregoing no list is made, 
it is a good idea to think carefully about the nature of the individual 
points in the sample space, their number, and the subset that corresponds 
to a particular event whose probability is desired. 
At times, it is advisable to split an event A into simpler subsets that 
are mutually exclusive, such as 
A, A2, Aa, ..., Ak. 
In such a case, we have 
A ---- A1 U A U Aa U''' U Ak, 
and (since these subsets are disjoint) 
P(A) ---- P(A1) -+- P(A2) -+- ... -+- P(A). (1) 
As Example 3 illustrates, it sometimes happens that the subsets have 
equal probabilities; if so, Eq. (1) becomes 
= 
EXAMPLE 3. A small boy is playing with a set of 10 colored cubes and 
3 empty boxes. If he puts the 10 cubea into the 3 boxes at random, what 
is the probability that he puts 3 cubes in one box, 3 in another box, and 
4 in the third box? 
Solution. Imagine that the boxes have been lettered a, b, c to ellable 
us to tell them apart. To help construct a sample space for the experi- 
3--8] SAMPLE SPACES V,'ITH MANY ELEMENTS 97 
merit, let us watch the boy perform the experiment and write down, in 
order, the letters of the boxes as he puts ill the cubes, one after another. 
The result is a string of 10 letters; for example, 
bbcaaaccba. (2) 
The particular sequence (2) corresponds to first cube in box b, second 
cube in box b, third cube in box c, fourth, fifth, and sixth cubes in box a, 
and so on. Thus the points of our sample space S [of which sequence (2) 
is one example] consist of all possible sequences of 10 letters, where each 
letter in the sequence may be a, or b, or c. From the multiplication prin- 
ciple of Chapter 2, we find that the number of points in this sample space is 
n = 3 X 3 X 3 X ... X 3 ---- 3 l� 
We assign probability 1/n to each sample point, since the boy puts the 
cubes into the boxes at random. 
We next consider the event A described by saying that 3 cubes go into 
one box, 3 into another, and 4: into the third. Let us split this event 
into three lnutually exclusive and exhaustive subsets, as follows: 
A x is the event "3 cubes in box a, 3 cubes in box b, 
and 4 cubes in box c"; 
A 2 is the event "3 cubes in box a, 4 cubes in box b, 
and 3 cubes in box c"; 
A a is the event "4 cubes in box a, 3 cubes in box b, 
and 3 cubes in box c." 
Since no two of thee events can occur simultaneously, they are disjoint. 
Hence 
P(A) = P(Ax) 4- P(A2) 4- P(A3). 
Now let us focus attention on one of these subsets, say A 1. The points 
of S that are in A1 have 3 a's, 3 b's, and 4 c's arranged in some order. 
By Theorem 2-12, Section 2-4, the total number of possible different 
arrangements of 3 a's, 3 b's, and 4 c's is 
10! . (3) 
3!3!4! 
98 EQUALLY LIKELY OUTCOMES [CHAP. 3 
Therefore the probability of A{ is this number times l/n: 
10! 1 
P(A) -- 3!3!4! X 3 -' 
Finally, it is clear that formula (3) also gives the number of points in 
A2 or in As. Therefore the three events A, A2, and As have equal 
probabilities, and 
10 ! 1 1400 
P(A) ---- 3P(A) - 3 X 3!3!4 X3 lo -- 3s  0.213. 
EXAMPLE 4. Sampling problem. A school staff consists of 30 teachers: 
20 women and 10 men. A random sample of 5 teachers is drawn for the 
discussion of school problems. What is thc probability that the sample 
(a) is composed entirely of women, (b) has exactly 2 men? 
Solution. (a) The number of possible equally likely samples is 
(350) , or 142,506. 
A sample composed entirely of women can be selected in 
(250) , or 15,504 ways. 
Thus, 15,504 points of S correspond to the event "sample composed en- 
tirely of women." Hence 
P (5 women) -- (250) -- 15,504 . 0.109. 
(o) 142,506 
(b) A sample composed of exactly 2 men and 3 women can be selected in 
Therefore 
1 o 
P (2 men and 3 women) -- (2)(a _ 51,300 . 0.360. 
(350) 142,506 
NOTE. The following generalization of the foregoing example is im- 
portant. Suppose that we have a group of n objects, m A's and w 's 
3--8] SAMPLE SPACES VITH MANY ELEMENTS 99 
(m q- w = n). From the n objects, we choose a sample of r. What is 
the probability that the sample contains exactly x A's? The data are 
collected in Table 3-8. 
TABLE 3-8 
A A Totals 
In sample x r -- x r 
Not in sample m -- x w--rq-x n-- r 
Totals m w n 
We have (r ) possible samples. Of these, (')(r-x) have exactly x A's. 
Therefore 
P(x A's) = 
The formula just calculated tells how the probability is distributed 
among the possible 2-by-2 tables represented by Table 3-8. Each value 
of x gives a different table. The distribution of probabilities for such a 
set of 2-by-2 tables has a name: the hjpergeometric distribution. 
EXERCISES FOR SECTION 3-8 
1. Refer to Example 1 of this section. What is the probability that the first 
aee appears at the 4th eard? At the 47th card? 
2. In the first-ace problem (Example 1, Section 3-8), what is the probability 
that the first ace appears at the 5th card or sooner ? At the 49th card or sooner? 
3. Refer to the birthday problem (Example 2, Section 3-8). Forty people 
are in a room. What is the probability that at least two of them have the same 
birthday? Ten people are accosted at random on the street and their birthdays 
noted. What is the probabihty that at least two of them have the same birth- 
day? Estimate the probability that at least two members of the Urnted States 
Senate have identical birthdays. (See Table 3-7.) 
4. There are k people in a room. What is the probability that at least two of 
them have the same birthmonth? (Assume that all birthmonths are equally 
likely.) 
5. Repeat Exercise 4 for the ease in which k = 5. 
6. Eight commuters drive their ears to the city each day and park at one of 
three parking tots. If the tots are seteeted at random, what is the probability 
100 EQUALLY LIKELY OUTCOMES [CHAP. 3 
that, on a given day, there will b6 5 of these 8 cars in one parking lot, 2 in an- 
other, and ] in the third? 
7. From a lot of 20 radios a sample of 3 is randomly selected for inspection. 
If there are 6 defective radios in the lot, what is the probability that the sample 
(a) is composed entirely of defectives, (b) is composed entirely of nondefectives, 
(c) is composed of one defective and two nondefectives? 
8. A class is made up of 35 students, 20 girls and ]5 boys. It is decided to 
distribute 4 complimentary tickets by lot to 4 members of the class. What is 
the probability that (a) the tickets go to 4 girls, (b) the tickets go to 2 boys 
and 2 girls? 
9. Show that 
4 d- 3 = , k = 1,2,...,48. 
[Hight. Use Pascal's Rule to combine (52-k) and (52-k), etc.] 
10. There were 33 different presidents of the United States from 1789 to 
1960. Before looking up their birthdays, what are the odds in favor of your 
finding that some pair of them had the same birthday? Now look in the lVorld 
.41manac, or elsewhere, and determine the facts. 
11. Visitors to historical sites often read inscriptions on tombstones. If a 
visitor selects a random ample of 30 tombstones, what is the probability of 
finding two dates of death that are the same month and day? Compare the 
dates of death of deceased United States pesidents. Are any two of them the 
same month and day? 
3-9. RANDOM DRAWINGS 
IH the early part of this chapter, we have seeH that certain physical 
considerations--fair tossing of symmetrical objects such as coins or dice, 
or thorough shuffling of identical cards xvith blindfold drawing--make it 
reasonable to assign equal probabilities to the points of the sample space. 
The fair tossing, the thorough shuffling, and the blindfold drawing are 
physical processes that we use in trying to achieve what is called "random- 
ness,"*; that is, in trying to give all outcomes equal chances or, mathe- 
matically, to give all points of the sample space equal probabilities. 
In addition to the simple experiments presented above, there are more 
serious experiments where it is desirable to assign some chosen set of 
probabilities over the sample space of outcomes, and to make the outcomes 
* The expressions "random," "at random," and "randomness" are not ex- 
clusively used for situations where equal probabilities are desired, but in every- 
day language this is usually what is meant. We use these expressions with the 
everyday meaning unless the text specifically states otherwise. 
3 9] RANDOM DXWXCS 101 
occur in accordance with these assigned probabilities. In some of these 
experiments, such as the famous drawing of draft uumbers in the fall of 
1940, only one trial of the experiment may be nmde. In others, the ex- 
periment may be repeated ninny times, as in thc simulation of the random- 
walk problem in Chapter 1. 
In this section and in the next two, we give examples of such cxperi- 
ments. We discuss ways of achieving desired probabihtics, whether equal 
or unequal, by physical processes, and traps to be avoided in attempting 
to produce probabilities by these processes. In Section 3-11, we present 
some physically sound processes for approximating required probability 
assiguments. 
ExAPLE 1. Door prize. At a school dance, a door prize is offered to 
the couple holding the winning ticket. The sample space consists of the 
tc ticket numbers. The dance committee wants to give all couples an 
equal chance at the door prize, and therefore wishes each ticket to have 
the probability 1/k of being drawn. The tickets are placed in a bowl 
and stirred. Then a blindfolded person reaches into the bowl and draws 
out the winning ticket. 
Criticism. For the purpose at hand, the procedure seems adequate. 
If k is large, say 100 or more, the physical stirring is probably rather 
ineffective, because slips of paper in a boM are very dicult to stir 
thoroughly. One might wonder whether early, middle, and late arriving 
couples had equal chances, but an investigation of such a question by 
many repeated trials at successive dances is inappropriate. Each ticket 
may not have probability 1/k of being drawn, but we can only guess about 
the direction of bias, if one exists. 
EkMPLE 2. Selective Service numbers. During World War II, it became 
necessary to choose an order in which to draft men into military service. 
Each man in a Selective Service District was given a number from 1 to 
9000. (The significance of the number 9000 is that it was larger than 
the number of men in any one Selective Service District.) 
Each number was placed in an opaque capsule, and the capsules were 
put into a boM and stirred. The capsules were then drawn, one at a time, 
from the boM until the supply of capsules was exhausted. A sample space 
of this experiment is the set of 9000 permutations of the numbers. No 
doubt the intention was to make each permutation have probability 
1/9000L After the stirring, high ocials drew the numbers from the bowl, 
and the early numbers were announced by radio as they were drawn. 
There is some question as to the effectiveness of the stirring. The 
resulting sequence of numbers had surprising properties, and some scien- 
tific papers have been written to prove that the drawing was not random. 
102 EQUALLY LIKELY OUTCOMES [CHAP. 3 
But from our previous discussicOn of the physical conditions under which 
we assign equal probabilities to sample points, we recognize that the real 
issue is whether or not the stirring was thorough. Only thorough stirring 
gives us confidence in the assignment of equal probabilities. 
Here is the frequency distribution of the first 50 numbers drawn: 
Numbers between Frequency 
1--1000 5 
1001--2000 0 
2001--3000 3 
3001--4000 1 
4001--5000 7 
5001--6000 8 
6001--7000 11 
7001--8000 7 
8001--9000 8 
Total 50 
Note that there are rather few numbers between 1 and 4000. We 
expect about  X 50, or about 22, as opposed to the 9 observed. The 
actual finding is consistent xvith the notion that the capsules were in 
layers and not thoroughly stirred. It is also remarkable that among the 
first 50 drawn, all 5 numbers below 2000 were between 100 and 199. Of 
course, every set of 50 numbers drawn from 9000 would be remarkable 
in some way. It is the correspondence between the special remarkablehess 
of these numbers and the special kind of outcome that we expect from 
inadequate stirring that raises doubts about the assignment of equal 
probabilities to the sample space. Better evidence would be a first-hand 
knowledge of the original mixing process. The moral, as every cook 
knows, is that thorough mixing is not as casy as it sounds. 
EXAMPLE 3. -3Iedical experiment. A doctor proposes a new treatment 
for a certain disease. It is desired to compare the new treatment with 
the old. Of 20 patients available for the study, half will be given the new 
treatment and half the old. The 20 patients are grouped into 10 pairs, 
each pair consisting of two patients who have the discase in a similar 
state of advancement. The doctor plans to give one patient in each pair 
the new treatment, and the other patient the old. (This helps guard against 
the possibility that the half chosen for the new drug will be mainly severe 
cases or mainly light cases.) 
3-9] RANDOM DRXWCS 103 
So far so good. But how does the doctor pick from a pair the patient 
who is to receive the new treatlnent? One might think that the doctor's 
choice is imlnaterial, but he knows that the matching of pairs, though 
carefully done, is not perfect, and that he may, froin his knowlcdge of 
the patients, subconsciously choose for the ncw trcatment the patient 
who has the better chance of recovery from the disease. This would 
systematically bias the test in favor of the new drug How can the doctor 
defend the experilnent from this kind of bias? 
The sample space consists of the 21� ---- 1024 possible ways of choosing 
one patient from each of the 10 pairs. Since there is no medical reason 
for preferring some of these choices to others, the doctor wants each to 
have probability 1 What physical process can he use to make his 
1--G' 
choices? One way is to list the 1024 choices on slips of paper and, after 
thorough mixing, draw one of the slips. Alternatively, he could arbitrarily 
assign to one patient of each pair the letter H and to the other, T, and for 
each pair toss a coin. If the coin falls heads, patient H gets the new 
drug; if it falls tails, patient T gets the new drug. 
Empirically checking or soh,i?,g a probability problem. Sometimes, after 
working out a complicated problem in applied probability theory, the 
worker has some uneasiness in his mind about the accuracy of his solution. 
If the problem does not provide suitable specml cases to give an adequate 
mathematical check, he may turn to random sampling as a check. 
We illustrate with an easy problem. Two distinct numbers are drawn 
in order from the integers 1, 2, 3, ... , 10, all ordered pairs being equally 
likely. What is the probability that the larger number of the ordered 
pair exceeds 5? After you have worked out your answer, check it by 
actually drawing two cards from 10 properly numbered cards, and then 
repeating this experiment a large number of times. Compare the empirical 
results with your theoretical answer. 
Some probability problems are so complicated and mathematically in- 
tractable that to obtain numerical answers to a single problem, thousands 
of repetitions of an experiment are executed on high-speed computers. 
The numerical answer obtained from averaging, or otherwise analyzing, 
the many results is the one used for practical work. This technique is 
called the Monte Carlo method. 
Random drawings for solving nonprobabzlzstzc problems. The Monte 
Carlo method is not reserved for problems in probability. An applied 
mathematician often finds it convenient to transform a nonprobabilistic 
problem into a probabilistic one. He then uses experimental methods 
not unlike those we have described. In other words, he uses empirical 
probabilistic methods to solve nonprobabilistic problems. 
104 EQUALLY LIKELY OUTCOMES [CHAP. 3 
1 
0 
FG. 3-6. Irregularly shaped region .t, whose area is to be determined by 
a probabilistic process. 
The following example illustrates this type of thinking. Suppose that 
we wish to approximate the area contained in the irregularly shaped 
region A of Fig. 3-6. Assume that it is possible to drop a point "at 
random" into the unit square. By "at random" we mean that every 
rectangular region of ara p in the square has probability p of having the 
point fall in it. Thus the region of unknoxvn area A has probability A 
that the point falls inside it. 
When a point is dropped into the square, it either falls into the special 
region A, or it does not. Envisage dropping hundreds of points at random 
into the unit square. The fraction of points that fall inside A is a good 
estimate of its area. A rough and ready method of performing a suitable 
experiment is as follows. Draw the figure so that the unit square has sides 
of 2 inches, and then stand 8 or 10 feet away and throw darts at it. Only 
count throws where the dart hits the square. The area A is estimated 
by the ratio of the number of hits in .4 to the number of hits in the square. 
3-10. RANDOM NUMBERS 
After one has tossed coins, drawn cards, thrown dice, and so on, for a 
large number of times, he begins to wish for faster and better methods 
of performing mathematically equivalent experiments. If we wish to 
draw 500 sets of three cards from a pack, the shuffling is slow and tire- 
some, and fatigue leads to poor shuffling and lack of randomness. Cards, 
marbles, and slips of paper are all very well if there are only a few to 
handle. If there are hundreds or, as often happens, thousands, the task 
gets out of hand. As a result of such considerations--slowness of handling 
3-10] RANDOM NUMBERS 105 
TABLE 3--9 
BRI]gF TABLE OF RAXDOM NUMBERS.* 
Columns 
Rows 1-5 6-10 
1 22719 92549 
2 17618 88357 
3 25267 35973 
4 88594 69428 
5 60482 33679 
6 30753 19458 
7 60551 24788 
8 35612 09972 
9 43713 18448 
10 73998 97374 
* Reprinted by permission of the publisher, The Free Press of Glencoo, 
Illinois, from A Million Random Digits with 100,000 Normal Deviates, copyright 
1955, by The Rand Corporation. 
physical objects and lack of randomness--random numbers were invented 
to provide a basis for mathematical experiments to simulate physical ones. 
What are random numbers? Random numbers are formed from ordi- 
nary digits successively generated by a random process. The series of 
digits may be of almost any length desired. Published tables of random 
numbers have up to 1,000,000 digits. 
Construction of a table of random digits. Most tables of random digits 
are constructed by setting up a sample space consisting of the ten digits 
0, 1, 2, 3, 4, 5, 6, 7, 8, 9. Some physical process is devised that gives 
good positive evidence that each of these digits has probability �6 of 
occurring on each trial and that the separate trials are independent. Then 
the process is set in motion, and thousands of digits are generated and 
written down in the order in which they occur. Table 3-9 is a short table 
of random digits generated by such a process; Table I at the back of the 
book is a larger sample. One vay to generate such digits is to roll a die 
and toss a coin, but ignore the two ordered pairs where the 6 appears on 
the die. Label 
(H, ), (H, 2), (H, 3), (H, 4), (H, ), (T, ), (T, 2), (T, 3), (T, 4), (T, 5) 
106 EQUALLY LIKELY OUTCOMES [CHAP. 3 
with the digits � 
0, 1, 2, 3, 4, 5, 6, 7, 8, 9, respectively. 
If the coin and die are true, the probabilities are o for each digit. 
Persons wishing to make random drawings use these tables as described 
in Section 3-11. Thus they use the random process behind the table, 
instead of a process of their own devising. The tables are speedy to use, 
and are based on a better physical process than one we are likely to con- 
struct in a few minutes for ourselves. 
It must be emphasized again that a set of digits is not in and of itself 
random or not random. Thus, if one writes the digits 825 and the digits 
999, there is no reason to say that the first set is random and the second, 
not. We call a set of numbers random if the following conditions hold: 
(1) There are known probabilities in the sample space for those num- 
bers; and 
(2) there is a physical process that generated the successive numbers 
with good assurance that each element of the sample space had the proba- 
bility assigned to it. 
The words "random digits" are an abbreviation for "randomly generated 
digits," where each digit has probability 1 
y. 
3-11. USE OF TABLES OF RANDOM DIGITS 
Random-digit tables have a great variety of uses. 5Ve shall offer a fexv 
examples by way of illustration. 
To begin drawing random numbers from a random-digit table is some- 
times an awkward matter. However, if you own your own table, the 
difficulty is easily overcome. Merely start at the beginning of the table 
and continue systematically until you have used as ninny digits as your 
problem requires. Then check off the used digits, and start the next 
problem with the next digit. 
EXAMPLE l. In an earlier example, we wished to draw pairs of distinct 
numbers with equal probabilities from the 10 digits 1, 2,..., 10. In 
a random-digit table, it is often convenient to consider the digit "0" as 
"10"; we shall do so in this example. 
Let us start by sampling at the top left of Table 3-9 with the five 
digits 22719. Since the first digit is 2, in our first salnple of txvo numbers 
the first number is 2. Reading across, we see that the next digit is 2. We 
ignore it, since the numbers in each of our ordered pairs must be distinct. 
The next digit is 7. Therefore, our first pair of numbers is (2, 7). Continu- 
3-11] USE OF TABLES OF RANDOM DIGITS 107 
ing, we lote that the next digit in the table is 1, so we record 1 for the 
first number of our second ordered pail'. The next digit in the table is 9, 
so our second pail' is (1, 9). 
Our line of five digits is nov exhausted, so we proceed to the second row, 
consisting of the digits 17618. The first of these digits is 1 and the second 
is 7, so our next pair is (1, 7). ]Ve continue in this mannel- until we get 
as many pairs as we please. 
EXAMPLE 2. Medical problem. The doctor designing the medical experi- 
ment with 20 patients in pairs (Example 3, Section 3-9) might proceed 
as follows. He first lists his 10 pairs of patients in order in two columns: 
Jones Smith 
Johnson Williams 
Hoffman Wood 
Ross Farlow 
Zanetti Wilson 
Suppose that he has used Table 3-9 through the first two sets of five 
digits. Then he begins with column 1, row 3, and proceeds to choose for 
the new treatment the patient in the first column if the digit is 0, 1, 2, 3, 4; 
otherwise, he chooses the patient in the second column. The random 
digits in the table are 25267. Therefore, in the first five pairs, Jones, 
Williams, Hoffman, Farlow, and Wilson are to be given the new treatment. 
EXAMPLE 3. Four boat owners. Suppose that four boys own a boat in 
shares, with Joe having a 10% share, Bill a 20% share, Tom a 30% share, 
and Sam a 40% share. On the Fourth of July, they all want to use the 
boat and they agree to draw lots for it. Sam and Tom argue that since 
they own larger shares, they should have better chances of winning in 
the draw. They want their chances to be equal to their fractions of 
ownership. 
Essentially what is desired is a sample space of four points J, B, T, 
and S to which are assigned probabilities 0.1, 0.2, 0.3, and 0.4, respectively. 
With a table of random digits, the task is readily accomplished as follows. 
Blindfold one of the boys and let him jab a pencil at the table of random 
digits. .Note the digit in the table nearest to the pencil point. If this 
digit is 0, Joe gets the boat; if it is 1 or 2, Bill gets the boat; if it is 3, 
4, or 5, Tom gets the boat; and if it is 6, 7, 8, or 9, Sam gets the boat. 
EXAMPLE 4. Not using all digits. Suppose that we wish a distribution 
over a sample space of points A B, and C with probabilities {, , and 3 
108 EQUALLY LIKELY OUTCOMES [CHAP. 3 
respectively. A convenient technique is to make the following corre- 
spondence: 
Digit in table Point in sample space 
0 A 
1 B 
2 B 
3 C 
4 C 
5 C 
6, 7, 8, 9 (ignore) No point 
Thus we ignore the digits 6, 7, 8, 9 when they occur in the table. The 
only random digits of interest are 0, 1, 2, 3, 4, 5. They have total proba- 
bility i and are equally likely, so each has probability . 
EX^MPLE 5. Obtaining finer probabilities. Suppose that xve require 
samples from a sample space of four points A, B, C, D with probabilities 
0.11, 0.25, 0.34, 0.30, respectively. Instead of looking at the sample space 
of random digits with 10 equally likely points, xve could consider two 
successive digits in the table as one of the 100 equally likely two-digit 
numbers00, 01, 02, 03,...,10, 11,...,20,...,99. Each of these has 
probability xo of occurring. (Why?) 
We then set up a correspondence as follows: 
Two-digit random number Point in sample space 
00-10 A 
11-35 B 
36-69 C 
70-99 D 
With two-digit numbers, most vorkers find it easier to read dovn a 
column than across a row. For example, if we start in row 4, columns 1 
and 2, ve read the random digits 88, 60, 30, 60, . . . , so our sample points 
are D, C, B, C, . . . 
EXAMPLE 6. Drawitg a sample fron a list. Suppose that, for survey 
purposes, we wish to draw a sample of 200 students from the 800 students 
3-11] -sE OF TABLES OF RANDOM DIGITS 109 
of a school. One way is to assign to each student one of the three-digit 
mmbers 001, 002,..., 800. Then enter the random-digit table and ex- 
amine successive three-digit numbers. If, in Table 3-9, we start with 
colunms 6, 7, 8 and row i and read down, the first three numbers ob- 
tained are 925, 883, and 359. 
Each three-digit number is either the number of a student in the list, 
or it is not. If not, ignore the number, and proceed to the next. If the 
three-digit number belongs to a student in the list, then we check that 
student's name for the sample unless he has previously been checked, 
in which case we ignore that number and proceed to the next. The process 
is continued until 200 students have been checked. These students con- 
stitute the random sample from the population of 800 students. 
EXERCISES FOR SECTIONS 3-9, 3-10, 
AND 3-11 
1. Describe a ptLx sical process fm randomly choosing 2 persons from a goup 
of 10. Set up a sample space and assign probabihties to it. 
2. Write a description of a random drawing problem of your own, set up a 
sample space and assign probabilities to it. Then describe a phystcal process 
for carrying out the drawings. 
3. Compute the probability for the problem described on page 103 and execute 
the experiment. 
4. On a rectangular coordinate system, draw a square with vertices (0, 0), 
(0, 1), (1, 1), and (1, 0). With center at (0, 0) and radius I unit, draw a 
quarter circle within the square. How can you use the Monte Carlo method, 
a table of random digits, and the foregoing figure to estimate the value of r? 
5. Take a random sample of 30 pages of this book. Record whether or not 
each page of the sample has a figure or a table on it. Estimate the fraction of 
pages in the book that have figures or tables. 
6. In Example 6 of Section 3-11, what is the sample space and the probability 
attached to each sample point? 
7. For phoning in connection with a TV program, it is desired to draw three 
names at random from a large telephone book (excluding the yellow pages). 
How would you draw the three names? 
8. Suggest a method of using the random-digit table to obtain selective serv- 
ice numbers. (See Example 2, Section 3-9.) 
9. Refer to the example of the four boat oxxnets (Example 3, Section 3-11). 
Hoxv would you modify the scheme if there were three boat owners with shares 
of 20%, 30%, and 50%, respectively? 
10. How would you modify Example 4 of Section 3-11 if the sample points 
were A, B, C, and D, with probabilities 0.2, 0.3, 0.4, and 0.1, respectively? 
11. Modify Example 5, Section 3-11, to accommodate five sample points 
A, B, C, D, and E with probabilities 0.23, 0.32, 0.35, 0.06, and 0.04, respectively. 
110 EQUALLY LIKELY OUTCOMES [CHAP. 3 
3-12. CONCLUSION 
In this chapter, the purpose of the intuitive approach to probability 
and statistics was threefold: 
(1) to gain an intuitive feeling for probabilities and some notion of 
how to work with them; 
(2) to make plausible the assumptions that we shall make later in the 
more formal mathematical treatment; and 
(3) to become familiar with concepts and notations to be used later. 
By now we should realize that a mathematical theory does not always 
work out perfectly when applied to real-life situations, and that its value 
depends upon finding the conditions, if any, under which the theory is 
a close approximation to real life. Thus, in thinking about probabilities 
associated with the faces of a die, our mathematical die is a perfect 
homogeneous cube. Each face has probability  of appearing. A brand 
new physical die bought from a reliable manufacturer is a close approxi- 
mation to our theoretical cube. We expect the true probabilities for the 
physical die to be extremely close to, but not exactly equal to, . A worn 
die might have probabilities rather far from . 
However, if we do not know the probabilities, all is not lost. It is one 
function of probability theory to state what the frequencies of various 
outcomes are when the initial probabilities are known. But it is also the 
function of statistics to make inferences about the values of the true 
probabilities on the basis of experimental results when the true proba- 
bilities are unknown. 
All that is lost, as we know less and less about a die, are the values 
of the probabilities associated with its faces;we do not lose the mathematical 
theorj or the laws. Later, we shall develop a more complete theory of 
probability for unknown probabilities and unequally likely events. From 
such theories, we can develop statistical methods for important problems. 
It is fortunate that these methods work for initially unknown proba- 
bilities, because in most scientific and engineering work the probabilities 
are not known, but must be estimated from observations. When we come 
to real-life situations, we rarely assume that the ideal probabilities, ob- 
tained from counting possibilities, represent the physical situation. For 
example, we assume that a production process has some true, but unknown, 
probability of turning out a defective light bulb. We take observations 
and use them to estimate the unknown probability. 
In practical work, idealized probabilities such as those obtained from 
counting are often treated as hypotheses that are available for a test. 
One might have the idealized notion that as many males are born as 
3-12] CONCLTSiON 111 
females, that is, that the probability of a male birth is �. After looking 
at the records for the United States in the years 1935-1952, one would 
soon be convinced that, consistently, more boys than girls are born. In 
1950, there were 1,823,555 boys and 1,730,594 girls* born. We might 
then estimate that the probability of a male birth is about 1,823,555/ 
3,554,149, or about 0.513. And we would abandon the notion that the 
true probability is 0.5, except as a rough approximation. 
The rest of this book is devoted to a more formal development of the 
ideas of the theory of probability and statistics. 
* The ll'orld Almatac--1956, New York World Telegram, 1956, p. 302. 
CHAPTER 
4 
GENERAL THEORY OF 
PROBABILITY FOR 
FINITE SAMPLE SPACES 
4-1. INTRODUCTION 
In Chapter 3 ve discovered some general results in sample spaces vith 
equally likely outcomes. For example, we found that 
P(A U B) = P(A) -]- P(B) -- P(A 91 B), (1) 
for events A and B. 
In this chapter, we adopt a set of axioms and definitions that can be 
applied even when outcomes in a sample space are not equally likely. 
The axioms are reasonable and sufiScient for proving general results like 
Eq. (1). But before stating these axioms, we consider a simple experiment 
to illustrate why some axioms, or assumptions, are needed. The experi- 
ment has these properties: 
(1) there are exactly two outcomes; 
(2) each outcome has a definite probability whose value is between 
zero and one; 
(3) the outcomes are not equally likely; 
(4) there is no obvious way to assign probabilities to the two outcomes. 
Thunbtack experinent. Imagine that an ordinary thumbtack is tossed 
or dropped onto a hard surface, where it bounces before conting to rest. 
When the thumbtack comes to rest, it points up (U) or down (D), as in 
Fig. 4-1. 
112 
4-1] INTRODUCTION 
FIG. 4-1. Thumbtack. 
These are the two possible outcomes of the experiment, just as head (H) 
and tail (/') are the two possible ways a coin can land. Each time the 
thumbtack is tossed, it seems reasonable to suppose that the outcomes 
U and D have fixed probabilities P(U) = p and P(D) = 1 -- p. But 
we cannot say, just by looking at the thumbtack, exactly what number' 
between 0 and 1 is equal to p. In particular, there is no reason to 
believe that p = �, because the two cases U and D need not be equally 
likely. 
How might we get sonhe idea of the value of P(U)? Let us specify 
conditions and toss a thuinbtack 50 times, say, then record tile data and 
calculate the proportion of times the thumbtack falls U. This proportion 
is not P(U): it is an estimate of P(U). We cannot hope to get the proba- 
bility exactly from such experiments. Even fol' apparently symmetrical 
coins the proportion that actually falls heads in a sequence of 50 tosses 
may not be �. But if we specify that the coin be a thin fiat disc, not out of 
shape like a bent bottle cap, and that it be given a vigorous toss into the 
air, with a spinning motion that turns it ovel' and over many times before 
it lands, then it seems reasonable that the two cases, head and tail, are 
equally likely; P(H) = P(T) = - If thumbtack tossing ever becomes 
2' 
a popular indoor sport, some physicist or statistician will no doubt develop 
a theory that predicts quite well, for a few given dimensions, materials, 
and tossing specifications, the pl-obability that the tack falls U. The fact 
that we can alerive at reasonable theoretical probabilities for cards, dice, 
and coins, and cannot do so easily for a thumbtack does not lessen for us 
the reality of the probability P(U). 
Suppose our thumbtacks fall U about 40 times out of 100. We would 
estimate P(U) to be 0.4. Xow if 0.4 were the true probability, we could 
apply to this number the ideas we worked out earlier fol' coins and dice. 
Instead, let us suppose there is a true but unknown value for P(U), 
say p. 
ExMP,z 1. Two tosses. If we toss the thumbtack twice and its proba- 
bility of falling U on a single toss is p, what is the probability that it 
falls U both times? 
1 14 PROBABILITY FOR FINITE SAMPLE SPACES [CHAP. 4 
Solution. We assume the tosses are independent. Our sample space of 
ordered pairs is represented thus: 
Second toss 
U D 
First U (U, U) (U, D) 
toss O (O, U) (O, D) 
Let event A be U on the first trial, and event B be U on the second trial. 
The point (U, U), vhose probability we want, comprises the event A FIB. 
Hence the probability that the thumbtack falls U both times is P(A ('1 B). 
From our experience with sample spaces of equally likely outcomes, xve 
know that in such spaces the probability of the intersection of independent 
events is the product of the individual probabilities. So we might assume 
this to be true more generally and assign probability 
P(A) . P(B) = p2 
to the point (U, U). Another line of reasoning that leads to the same 
result is the following. Consider a long sequence of pairs of tosses of a 
thumbtack. In this sequence, the proportion vhere the first toss of a pair 
results in U is approximately p. And approximately the proportion p of 
lhese fall U on the second toss as well. Hence we expect the long-run 
proportion that fall (U, U) to be about p'. Thus both lines of reasoning 
suggest that we assign probability p2 to the outcome (U, U); and we do so: 
P({(u, u)}) = p2. (2) 
Notation. The parentheses and braces in Eq. (2) are used in the following 
ways:the inside parentheses, in (U, U), are used in the same way we use 
parentheses to designate a point, say (3, 4), in coordinate geometry. Thus 
(U, U) is a sample point. Next, the braces, in {(U, U)}, indicate a set 
whose only clement is the point (U, U). Finally, the outer parentheses 
are used as they are, for example, in P(E), denoting probability of a set E. 
Itowever, the weird collection of braces and parentheses in Eq. (2) is 
almost too frightening to live with, so ve shall adopt the logically less 
accurate, but typographically more pleasing, notation P(U, U), and 
write simply 
P(U, U) = 
4-1] XTIODUCTO 1 15 
Similarly, we assign to the outcome (U, D) the probability 
P(U, D) = pq, 
where 
q----1--p 
is the probability that a thumbtack lands "point down." 
If, for example, we assume that 0.4 is the true probability p  P(U), 
then 
P(U, U) = (0.4)(0.4) -- 0.16. 
Similar considerations would give us 
P(U, D) = P(U) . P(D) 
= (0.4)(0.6) ---- 0.24, 
and P(D, D) ---- (0.6)(0.6) ---- 0.36. 
EXERCISES FOR SECTION 4-1 
1. In the thumbtack example, find the probabilities of the four possible out- 
comes on two tosses, assuming that P(U) = 0.3. 
2. Use the results of Exercise 1 to find the probability that (a) at least one 
toss falls U; (b) the second toss falls D; (c) the second toss falls D, given that 
the first toss falls U [compare your answer with the answer to part (b)]; (d) 
both tosses fall alike. 
3. ttow would you assign the probability that two tosses of a thumbtack fall 
U, given that both fall alike? 
4. Suppose a thumbtack, with P(U) = p and P(D) = q = 1 -- p, is inde- 
pendently tossed three hines in succession. List a sample space for the possible 
outcomes of this experiment. Assign probabilities to its points. 
5. In Exercise 4, take P(U) = p = 0.4, and find the probabfiity that the 
thumbtack fell U twice and D once in the three tosses. 
6. A thumbtack with P(U) -- p -- 0.2 is tossed four times. What probabili- 
ties would you assign to the following outcomes? 
(a) UUUD (b) UUDU (c) UDUU (d) DUUU (e) UUDD 
(f) UDUD (g) DUUD (h) UDDU (i) DUDU (j) DDUU 
(k) U three times and D once (1) U twice nd D twice 
7. Suppose that the length of the shaft of the thumbtack in Fig. 4-1 varies 
from 0 to some large positive value L. What would you guess P(U) to be when 
the length of the shaft is 07 When it is L? Discuss. 
8. Do the probabilities you assigned to the sample points in Exercise 4 add 
up to 1, as they should? 
116 PiOBABILITY FOR FINITE SAMPLE SPACES [CHAP. 4 
4-2. SAMPLE SPACE AND PR)BABILITY 
In this section, we develop the axioms of probability in relation to the 
familiar notion of a sample space of an experiment. A sample space, we 
recall, is a set of elements such that any performance of the experiment 
produces a result that corresponds to exactly one element in the set. ]Ve 
restrict attention to finite sample spaces, i.e., those with a finite number 
of sample points. In a finite sample space, every set of sample points is 
called an event. An elementary event contains exactly one sample point.* 
If a performance of the experiment produces a result that corresponds 
to a point in the subset E, we say that the event E occurs. The empty set 
is also an event, but it never occurs, since no sample points are in it. 
The next example illustrates events in a sample space of 4 sample points. 
]XAMPLE 1. Bond issue for new school. A survey is made in connection 
with the planning for a new high school. Each of 100 voters is asked two 
questions: 
(1) Do you favor a bond isme to finance the building of the school? 
(2) Do you own property in the school district? 
Discttssion. Each voter in the survey belongs to one of the following 
four categories: 
el: favors issue and owns property, 
e2: favors issue and does not own property, 
es' opposes issue and owns property, 
e4: opposes issue and does not own property. 
The experiment of surveying 100 voters and classifying them is the 
same as 100 performances of the simpler experiment of asking just one 
voter and classifying him. The set 
S = es, ca, e4} 
is an appropriate sample space for this single-voter experiment, since 
each performance must result in exactly one of these four possibilities. 
This sample space S also provides a scheme for tallying the results of the 
100-voter experiment. 
* So different elementary events are always mztually exclusit, e. All the ele- 
mentary events together form a partition of the sample space. 
4-2] SAMPLE SPACE AND PROBABILITY 117 
The nonempty subsets of S are 
1�1}, {�1,�2}, {e2, �4}, {�1, �3, e4}, 
l�2}, {�1, �3}, {�3, �4}, {�2, �3, �4}, 
lea}, {�1,�4}, {�1, e2, Ca}, {�1, �2, ea,�4}. 
l�4}, {�2, 3}, {�1, e2, �4}. 
Each of these subsets is an event. Technically, the empty set is also an 
event, though trivial. The subsets 
E1 = {el}, E2 = {e2}, Ea = {ca}, and J4 
contain just one sample point apiece;they are the elementary a,ents. Every 
event, other than the empty set, is the union of one or more distinct 
elementary events. These events can also be described verbally; for 
example, "favors the bond issue" describes the event E1 U E2 ---- {el, e2}; 
and "owns property or opposes the issue" describes {el, es, e4}. The 
event {el, e, e, e4/ is the entire sample space S; it may be described by 
"person in the survey." 
Later we shall want to see how we might attach probabilities to the sets 
in this sample space and continue the example. But we delay this in order 
to introduce the general idea of assigning probabilities to more general 
sample spaces. 
NOTE. In set theory, a logical distinction is made between a set E1 that 
contains a single point el, and the point itself. We have made the distinc- 
tion above by writing E1 --- {el/, to indicate that E1 is the set whose 
only element is the point el. This permits us to write the probability of 
E1 as P(E1) rather than as P({el }). However, we shall not always make 
such distinctions, and may write this probability simply as P(el), without 
the inner braces. This usage is an abbreviation. 
Probability. Given a sample space S, we need to assign probabilities to 
its events. We assume that the sample space has a finite number n of 
sample points: 
= 
To each event in S we assign a number, called its probability. We now adopt 
the following axioms, or postulates, about these probabilities. 
115 PROBABILITY FOR FINITE SAMPLE SPACES [CHAP. 4 
AXIOMS FOR PROBABIIZITY IN FINITE SAMPLE SPACES 
AXIOM I. Positiveness. The probability assigned to each event is positive 
or zero. 
AXIOM II. Certainty. The probability of the entire sample space is 1. 
AXIOM III. Unions. If A and B are mutually exclusive events, then 
P(A U B) = P(A) + P(B). 
We call the first of these the positivehess postulate because probabilities 
are never negative; they are either positive or zero. For most purposes, 
events with zero probability, in a finite sample space, can be deleted. 
The second postulate is called the certainty postulate because it says, 
in effect, that the probability of an event that is bound to occur is 1. The 
entire sample space is just such a certain event because it contains all 
possible outcomes of the experiment. 
The third postulate, concerning the probability of the union of two 
mutually exclusive events, permits us to focus attention on the elementary 
events when we are assigning probabilities. For, as the next theorem shows, 
as soon as we know the probabilities of the elementary events, the proba- 
bilities of all other events are uniquely determined by Axiom III. 
4-1 Theorem. Let A be an event in a finite sample space S. If A is the 
empty set, then P(A) = 0. If A is nonempty, then P(A) is the 
sum of the probabilities of the elementary events whose union is A. 
Proof. First, suppose A = �, the empty set. In Axiom III, take 
A = � and B = S, the entire sample space. Then A and B are mutually 
exclusive, because  is empty, so 
P(4, u S) ---- P(4,) ,-+- P(S). (1) 
Also, since S is the entire sample space, the union of  and S is S: 
�os=s. 
Hence 
P(� U S) = P(S). (1') 
Subtracting Eq. (1') from (1), we have 
0 = P(�), 
and therefore P(A) ----- 0 if A =- �. 
4-2] SAMPLE SPACE AND PROBABILITY 119 
1Next, suppose .4 is nonempty and is the union of m distinct elementary 
events /1, 12,'.., Jm, where ],.' = {el}, i = 1, 2,..., m. For the 
purpose of the present proof, we assume that the sample points have been 
labeled in such a way that the m pothis in A are the first m points of lhe 
sample space This simplifies the notation without afrocling the validity 
of the proof. 
If m  1, A  E andP(`4)  P(E). If m  2, A  E UE is 
the union of two mutually exclusive evenis, because E and E are distinct 
elementary events. Axiom III gives the result 
P(A) = P(l) + P(2). (2) 
If m: 3, then A = (El U E) U Es and, again by Axiom III, 
P(`4) = u + 
Application of Eq. (2) leads to 
P(.4) ---- P(/'i) n,_ p(F2) __ P(Ea). 
The extension to values of m > 3 is readily made by mathematical 
induction. Ve assume the theorem is true for m -- 1 elementary events 
and write A as the union of Em and E1 U E U ''' U Era--1. When we 
apply xiom III, we get 
= P(E1) + P(2) + ''' + P(Em-1) + 
the desired extension.  
In some applications, we feel that the  sample points are equally likely 
to occur, and then we assign to each elementary event the probability 1In. 
But in many applications, the elementary events have uncqusl probabili- 
ties. In the school-bond example, xxe would assign to each category a 
probability equal to the proportion of voters in the school district who are 
in that category, if the proportions were known. Thus, if it were known 
that 40% favor the issue md own property, 20% fsvor the issue and do 
not own property, 30% oppose the issue and own property, md 10% 
oppose the issue and do not own property, we would assign probabilities 
to the elementary events as follows: 
P(E) = 0.4; P(E2) --= 0.2, P(Fa) --= 0.3, P(F4) ---- 0.1. 
120 PROBABILITY FOR iN1TE SAMPLE SPACES [CHAP. 4 
Ve also arrange this information in the form of a two-by-two array 
giving the sample space and associated probabilities shown in Table 4-1. 
The purpose of this arrangement is to focus attention on the two attributes 
or characteristics that the survey is designed to study' namely, the state 
of property ownership, for one, and the attitude toward the bond issue 
for the other. Each person in the survey either does or does not own prop- 
erty in the district, and either does or does not favor the bond issue. When 
we provide for a "yes" or "no" answer to each of the two survey questions, 
we get the four categories described earlier. Such a two-by-two table is 
often used to study a possible relationship between two characteristics 
or attributes. 
TABLE 4--1 
Owns property, Does not own 
0 property, 0 
Favors bond issue, F El, Pl ---- 0 4 E2, p2 = 0 2 
Opposes bond issue, Y Ea, p = 0 3 E4, p4 = 0 1 
EXAMPLE 2. If the' probabilities of the elementary events are those 
given in Table 4-1, what is the probability that a voter selected at random 
(a) is in favor of the bond issue? (b) favors the bond issue or owns prop- 
erty in the district? (c) opposes the bond issue or does not own property 
in the district? 
Solution. Every event in the sample space can be expressed in terms 
of the events 0 and F and their complements 0 and F, where 
F  /e, e2/ ----  U  corresponds to "favors bond issue," 
and 
0 ---- /e, e/ ---- E1 U  corresponds to "owns property in the district." 
The events in question have these probabilities: 
P (person favors bond issue) = P(F) = P(E1) -- P(E) 
---- 0.4 h- 0.2 ---- 0.6, 
P (favors bonds or owns property) ---- P(F U O) 
 0.4 4- 0.2 -]- 0.3 ---- 0.9, 
4-2] SAMPLE SPACE AND PROBABILITY 121 
P (opposes bonds or does not own property) = P(F U O) 
= P(E2) -P(Es) - P(E4) 
---- 0.2 h- 0.3 h- 0.1 = 0.6. 
Theorems. In Section 3-5, we proved the following theorems. (Note that 
their proofs, as given earlier, did not assume that the elementary events 
in S were equally likely. Those proofs are valid in any finite sample space, 
so we need not repeat them here. Alternatively, they can easily be proved 
directly from the axioms.) 
4-2 Theorem. A or B (or both). 
P(A U B) = P(A) + P(B) -- P(A  B). (3) 
4-3 Theorem. 3[utually exclusive events. If A, A2, ...,Areare mu- 
tually exclusive, then 
P(A1  n .-.  n,,) = P(n) + P(A) +... + P(.am). (4) 
4-4 Theorem. omplemetary evegs. 
P(A) = x -- P(A). (5) 
In the bond example, we used elementary events to find the probability 
that a voter is in favor of the bond issue or owns property in the district' 
P(F  o) = p(ff) + P(ff) + p(ff) = 0.9. 
The only sample points not in F U 0 are those that belong neither to 
F nor to O; they therefore belong to F  0. Hence the complement of 
F U 0 is F  0 and, by Theorem 4-4, 
P(F u O) = 1-- P(F m ) 
= 1--0.1=0.9. 
A third method uses Eq. (3)' 
P(F  o) = P(F) + a(o) - P(F a o) 
= 0.6 + 0.7 -- 0.4 = 0.9. 
ote that we would get the absurd result 1.3 if we forgot to subtract 
P(F  O) from P(F) + P(O), because the events F and 0 are ot mutually 
exclusive. 
122 PROBABILITY FOR FINITE SAMPLE SPACES [CHAP. 4 
EXERCISE FOR SECTION 4-2 
1. Let A and B be events in a sample space S, such that 
P(A) = 0.4, P(B) = 0.3, P(A n B) = 0.2. 
Find the probabilities of: 
(a) A UB (b)  (c)  (d) AB (e) A U (f) J- U 
2. In the two-dice example of Chapter 3, Table 3-3, the sample space is 
S = {(r, c): r and c are integers from 1 through 6}. 
Let A be the event described by r _ 3 and B the event described by c >_ 4. 
Find the probabilities of: 
(a) A (b) B (c) A AB (d) A U B 
(e)  (f) (g) u (h) n 
3. In Exercise 2 above, describe each of the following events in different 
mathematical symbols or in words: 
(a) A n B (b) A U B (c) XU  (d) _7_ n  
4. Color blindness. Assume that 5% of males and 1% of females are color- 
blind. Assume furthermore that 50% of the population is male and 50% female. 
A person is to be selected at random from this population, and that person's 
sex and state of vision (color-blind or not) are to be recorded. List a sample 
space for the experiment of sampling one person. Assign probabilities to the 
elements of the sample space. What is the probability that (a) the person is 
male and color-blind? (b) the person is female and color-blind? (c) the person 
is color-blind? [Genetic theory suggests that if p is the proportion of color-blind 
males, p2 is the proportion of color-blind females, so �% rather than 1% may be 
a more realistic figure for females in this example.] 
5. The truc odds in favor of three events that are mutually exclusive, and 
whose union is the sample space, arc in the ratio 3 to 2 to 1. Find the probabili- 
ties of the three events. 
6. A sample space is composed of n mutually exclusive events, of which n -- 1 
have identical probabilities and the elnaining one has probability as large as 
r q- i of the others. Find the probabilities of the two kinds of events. 
7. The probabilities of the nmtually exchlsivc events .4 and B are related 
as P(B) = [P(A)] 2, and A U B = S, the sample space. Find P(A), (a) exactly, 
and (b) to two decimals. 
8. The event C is twice as likely as A, and B is as likely as A and C together. 
The events arc mutually exclusive and together they exhaust the sample space. 
Find their probabilities. 
4--3] INDEPENDENT EVENTS 123 
9. If one letter is choscn at random from the wold Soot and one letter from 
the word toot, what is the probability that the two lettcrs are the samc? (First, 
set up a sample spacc and assign pobabfiities to its elements.) 
10. Two letters are drawn at random, without rcp]acement, from the word 
?n�ncnto. What is thc probabfiity that some arrangement of the two lctters 
spells ?he? 
11. Terry is batting in a ball gamc with no one on base. Assume the possible 
outcomes of this experiment, and the associatcd probabilities, to be as follows: 
P (striking out) = 0.35, P (base on balls) = 0.21, 
P (flying out) = 0.17, P (grounding out) = 0.10, 
P (getting extra-basc hit) = 0.04, P (being hit by the pitcher) = 0.01, 
P (getting a single) = 0.12. 
Find the following probabilities: 
(a) P (getting at least to first base safely), 
(b) P (having to hurry toward first base), 
(c) P (getting a hit), 
(d) P (getting put out). 
12. In the school-bond example (Table 4-1), if all the voters who do not own 
property in the district join the property owners who arc opposed to the bond 
issue in voting against it, and evcryone votes, how will the vote on the bond issue 
turn out? 
13. Prove the theorem: If A is a subset of B and P(B) = 0, then P(A) = O. 
14. The statement "A implies B" means that every occurrence of A is also 
an occurrence of B. Explain why this is the same as saying that A is a subset 
of B in the sample space. Prove that if A implies B, then P(A) _ P(B). 
4-3. INDEPENDENT EVENTS 
The definitions of independence and dependence in Section 3-6 also 
apply in sample spaces where the outcomes are not equally likely. 
EXMUE 1. In the bond-issue example, show that F and 0 are de- 
pendent. 
Solution. Recalling that 
F = 0 = ? a 0 = 
xve have 
P(F) = 0.6, P(O) = 0.7, P(F c O) = 0.4 
and 
P(F C O)  P(F) � P(O), 
since 
0.4  0.6 X 0.7 = 0.42. 
124 PROBABILITY FOR FINITE SAMPLE SPACES [CHAP. 4 
EXAMPLE 2. A thumbtack with probability P(U) = 0.4 is tossed twice. 
If E is the event "first toss lands up" and F is the event "second toss 
lands up," show that the following pairs of events are independent: 
(a) E and F (b) E and  (c)  and F (d) ' and  
Solutton. The possible outcomes and associated probabilities were 
discussed in Section 4-1. We reorganize the data here for reference. 
Outcome of second toss 
F F 
(U) (D) Row sums 
Outcome of E (U) 0.16 0 24 0.40 
first toss 
E (D) 0 24 0 36 0.60 
Column sums 0.40 0.60 1.00 
If E is the event "first oss lands up" and F the event "second toss lands 
up," then 
P(E) = P(F) = 0.4 
and 
P(E N F) ---- P(U, U) ---- 0.16 = P(E) . P(F), 
so the events E and F are independent. It is also easy to verify that E and 
 are independent, as are  and F, and  and ' 
P(E c ) ---- P(U, D) = 0.24 = P(E) � P(), 
P(E c F) = P(D, U) = 0.24 = P(E). P(F), 
P( c ) = P(D, D) = 0.36 = P() � P(). 
We now restate the formal definition of independence, and prove a 
theorem suggested by the last example. 
4-5 Definition. Independent events. Two events E and F are inde- 
pendent if and only if 
P(E rq F) ---- P(E) . P(F). [ (1) 
4-3] INDEPENDENT EVENTS 125 
4-6 Theorem. lndependet e,ents. Let E and F be independent events 
in a sample space S. Then E and  are independent, as are J and 
F, and  and . 
Proof. Consider the two-wy array in Table 4-2. We shall show that 
the entries in this table correctly give the probabihties of the corresponding 
compound events. 
TABLE 4--2. INDEPENDENT EVENTS. 
F  Row sums 
E P(E) � P(F) P(E). p(7) P(E) 
 P(). P(F) p(). p()7) p(') 
Column sums P (F) P(F) i 
From the assumption that E and F are independent, Eq. (1) says that 
P(E (q F) ---- P(E) . P(F), 
and the entry in the upper left corner, corresponding to P(E  F), is 
correct. Next, the row sums and column sums must be P(E), P(E), P(F), 
and P(), as shown in Table 4-3. 
TABLE 4-3 
F 
E P(E). P(F) P(E) 
P(F) P(F) i 
From the row sum P(E), we see that 
P(E  F) = P(E) -- P(E) . P(F) = P(E) . [1 -- P(F)] 
= P(E) � P(F), (2) 
which shows that E and F are independent. 
126 PROBABILITY FOR FINITE SAMPLE SPACES [CHAP. 4 
Similarly, ' and F are independent and 
P( N F) ---- P() . P(F), (3) 
but we leave the proof as an exercise. Likewise E and F are independent' 
P(  Y) ---- P() -- P(  F) ---- P() -- P() . P(F) 
---- P(E). [1 -- P(F)]---- P(E). P(F). (4) 
Therefore, if E and F are independent, the probabilities of the compound 
events are those shown in the eelis of Table 4-2. [] 
Note that the probability entered in any of the four main eelis of Table 
4-2 is just the product of the corresponding row and column probabilities. 
This property of probabilities of independent events is very easy to cheek 
when the probabilities are set up in a two-by-two table of this kind. If 
one entry can be filled in by multiplication of its row sum and its column 
sum, so can all the others. 
EXAMrnE 3. Are the events E and F independent if the probabilities 
are as shown in the following table? 
F P 
E 0 04 0.06 O. 10 
.' O. 08 O. 82 0 90 
O. 12 0.88 1.00 
The answer is "no," because 
0.04  (0.12). (0.10) ---- 0.012. 
We also observe that every entry is different from the product of its row 
sum and its colunto sum. 
IEMARK. In the bond-issue example, we have discussed ownership 
status (person owns or does not own property) and attitude toward the 
bond issue ("for" or "against"). If the events "owns property" and "for 
bond issue" were independent, theu Theorem 4-6 would imply inde- 
pendence between such other pairs of events as "owns property" and 
"against bond issue," and so on. It would then be convenient to speak 
of independence of "ownership status" and "attitude on bond issue." So, 
4-3] INDEPEWDENT EVENTS 127 
in general, when independence works for one cell of a 2-by-2 table, we say 
that the characteristic, or label, associated with the rows is independent 
of that associated with the columns. And indeed, even in a larger table 
of m rows and n colunms, if the probability in every cell is the product of 
its row total and colunto total, we continue to say that the row label is 
independent of the column label. 
Irdepedence of three or more eerts. If we study three or more events 
we may represent them by El, E=,... , E,,. (Ill this discussion, the 
are not elementary events.) It is natural to say that these m events are 
independent provided the probability of their intersection is equal to the 
product of their probabilities: 
P(E1 (q E2 (q ... (q E) = P(E1) . P(E2) . . . P(E,O. (5) 
But if m > 3, Eq. (5) alone is not sufficient to guarantee the truth of the 
equations that we get by replacing some of these events by their comple- 
ments on both sides of Eq. (5), as is the case when m = 2. To achieve this 
desired goal, we need to require complete independence. 
4-7 Definition. Complete indepezdezce. The m events are said to be 
completely independent if and only if every combination of these 
events, taken any number at a time, is independent. 
When m = 3, complete independence of El, E2, Ea means that the 
following equations are satisfied: 
P(E (q E2 (q Ea) = P(E1) ' P(E2) ' P(Ea), 
P(E1  E2) = P(E1)' P(E2), 
P(E1 CI Es) - P(E1) ' P(E3), 
P(E2 (q Es) ---- P(E2) � P(E3). 
And if equations (6) are satisfied, so is any equation we get by replacing 
an event by its complement on both sides of one of the original equations. 
For instance, 
ZY) = P(E1). P(TZ). (7) 
Or we may replace any two, or three, events by their complements on both 
sides of the equation and get a true result. 
RE:Vlr:. It might be supposed that 3 events are independent if every 
pair of them is independent. However, such pairwise independence does 
not imply independence of the three events, as the following example shows. 
128 PROBABILITY FOR FINITE SAMPLE SPACES [CHAP. 4 
EX^MPLE 4. Three pairwise independent events that are not independent. 
Discussion. Two coins are tossed. If �2 is the event "head on first coin," 
�2 the event "head on second coin," and �3 the event "the coins match; 
both are heads or both tails," then 
1 
P(E)---- P(E2)---- P(Ea)-  
and 
P(E1  E) = P(E,  Es) = P(E  Es) = k. 
Hence the events are independent in pairs. But 
P(E  E  E3) = k  P(E1)' P(E2)' P(E3), 
so they re not independent when tken ll together. 
Exxr 5. Independently, a coin is tossed,  crd is drown from  
deck, and a die is thrown. Wht is the probability that we observe a hed 
on the coin, n ace from the deck, and  five on the die? 
Solution. 
P (hed) 1 P (ce) 1 P (5 on die) = . 
--  -- 13 
i 1 
P (head and ce and 5) =  X  X  -- 15 6. 
Exxr 6. Flawless shoes. In  shoe factory, uppers, soles, nd heels 
re mnufctured separately nd randomly assemblcd into single shoes. 
:Five percent of the uppers, four percent of the soles, and one percent 
of the heels hve flaws. What percent of the pirs of shocs are flawless 
in these three prs? 
Solution. Let U, S, nd H stand for unfiawed upper, sole, and heel, 
respectively, nd , , and  stand for the flawed parts. For  single shoe, 
P(U) = - o.05 = 0.95, P(S) = - 0.0 = 0.96, 
P(H) = 1 -- 0.0 = 0.99, 
P(U  S  H) = 0.95 x 0.96 x 0.99  0.903. 
This is the probability that one shoc is unfiawed. Assuming that pairs are 
also randomly sscmblcd, wc would have 
P (both shocs unflawed) = P (left and right unfiawed) 
= P (left unfiawed) � P (right unfiawed) 
 0.903 X 0.903 
 0.815. 
4-3] INDEPEXDENT EVENTS 129 
EX^MPLE 7. Light bulbs. Light bulbs are produced by a sequence of 
machine operations. When the machine is in good working order, it 
produces one defective bulb per thousand. The outcomes for successive 
bulbs are independent. What is the probability that the next two bulbs 
produeed are nondefeetive? 
Solution. Let � be "first bulb nondefeetive," F be "second bulb non- 
defeetive." 
P(E) = P(F) = i -- 0.001 = 0.999, 
P(E F3F) = 0.999 X 0.999 = (1 -- 0.001) 2 = 1 -- 2(0.001) = 0.998. 
After this seerion, when we speak of independence, we shall mean com- 
plete indepetdence. 
EXERCISES FOR SECTION 4-3 
1. If two events E and F are mutually exclusive and have probabilities 
different from zelo, prove that they are dependent. 
2. Give examples of events E and F like those desclibed in Exercise l, based 
upon the two-dice example, Table 3-3. 
3. Prove that if E and F are independent, then  and F are also independent. 
4. Prove that if P(E FI F)  P(E) � P(F), then 
P(' f3 F)  P(). P(F), P(E a F)  P(E) � P(F), 
P(  .)  P() � p(.). 
5. Three ordinary dice are thron. Assuming the outcomes on the dice are 
completely independent, what is the probability that the sum of the numbers 
on the top faces is five? 
6. A certain automatic machine makes bolts and fills boxes with them. If 
1 box in 100 has at least one defective bolt in it and the outcomes are inde- 
pendent, what is the probability that each of the next 3 boxes has one or more 
defective bolts? That all have no defective bolts? 
7. The probability that a man is hospitalized during the next month is 0.01. 
If we consider th:ee men who are strangers to each other, what is the pobability 
that duling the next month exactly one of them goes to the hospital? 
8. There are three traffic lights spaced several miles apart on a highway 
between toxxns A and B. The cycles of the three lights ale one minute each. 
The three lights show glcen 30, 40, and 50 seconds, respectively. Assuming 
that a car strictly observes traffic-light regulations, what is the probability that 
the car makes the trip from A to B without being stopped by any of these three 
traffic lights? That the car will be stopped by exactly one light? By exactly 
to lights? By all three? (Assume that this is the only car on the road from 
A to B.) 
]30 PROBABILITY FOR FINITE SAMPLE SPACES [CHAP. 
9. Two ordinary dice are independently thrown and the outcomes on the 
top faces are observed. Show that the events 
El: first die shows an even number, 
E2: second die shows an odd number, 
E3: sum of the results is odd 
are pairwise independent, but not completely independent. 
10. Let S = {e, e2, e3, e4, es, e6} be the sample space of an experiment. 
Suppose the probabilities of the elementary events are 
1 13 
pl : , P2 : 6, P3 -- 16' P4 = g, P5 ---- P6 ---- 1-, 
where Pi = P([e]). Let E -- [el, e4], F = [el, e2, e5], G -- [el, e2, e3]. 
Show that E, F, and G are independent, but not completely independent. 
11. In matches between two teams, teams A, B, and C score points in games, 
independently of whom they play, according to the following probability table' 
Points 
Team 0. 1 2 3 4 5 
A 0.5 0.5 
B 02 0.8 
 0.2 
(7 0.8 I 
The team with the most points wins. Show that P(A beats B), P(B beats C), 
and P(C beats A) are all greater than �. That is, A usually beats B, B usually 
beats C, and C usually beats A. Thus the relation "usually beats" need not be 
transitive. 
ADDITIONAL EXERCISES FOR SECTION 4-3 
World Series Exercises 
In a World's Series, teams A and B play until one teain has won 4 games. 
Let p be the probability that team A wins any individual game played with B. 
Then q = I -- p is the probability that B wins. Use this information to answer 
the questions in Exercises 1 through 9: 
1. What is the plobabihty that A wins the first 4 games? That B wins the 
first 4 gaines? That the selics ends at 4 games? [Ans: 
2. What is the probability that A wins the selies in the 5th game? That the 
selies ends at 5 games? [Ans: 4ptq, 4pq(p a q_ q3)] 
4-4] CONDITIONAL PROBABILITY 131 
3. What is the probability that A wins the series in the 6th game? That the 
series ends at 6 games? [Ans: 10p4q 2, 10p2q2(p 2 - q2)] 
4. What is the probability that A wins the series in the 7th game? That the 
series ends at 7 games? [Ans: 20p4q 3, 20p3q 3] 
5. Using the results of Exercises 1 through 4, construct a sample space for the 
experiment of playing a World's Series and assign probabilities to the sample 
points. What is the probability that team A wins the series? That team B 
wins? (Express the answer for B's winning in two different ways.) [Ans: 
P(A wins) = p4(1 + 4q ,:- 10q 2 + 20q3)] 
6. In Eercise 5, suppose p -- 2, q = �, so that team A is "twice as good" 
as team B.* Is A's chance of winning the series also tviee the probability that 
B wins? If not, what are the odds in favor of A's winning the series? [Ans: 
P(A wins) = lSOS P(B wins) = 1--;  4.77:1] 
7. If, in Exercise 5, p -- q -- �, what is the probability that the series ends 
in 4 games? 5? 6? 7? [Ans.' 2 
16' 16' 16 
8. In Exercise 5, assume that p = 2. What is the probability that the series 
� 153 216 200 160] 
ends in4games?5?6? 7? [Ans: 72, 72, 72, 
9. In Exercise 8, with p = 2, which is more likely, that the series is over be- 
fore the 6th game, or that it is not over then? What are the relative odds? 
4-4. CONDITIONAL PROBABILITY 
In Section 3-7, conditional probabilities were studied for sample spaces 
whose sample points have equal probabilities. In this section we extend 
the notion of conditional probability to more general sample spaces. In 
the remaining two sections of this chapter, we study two classes of appli- 
cations of conditional probability: 
(a) its use in assigning probabilities in a sample space, 
(b) its use in modifying our "degree of belief" in various alternative 
hypotheses as a result of experimental evidence. 
EXAMrLE 1. An irregular tetrahedron is tossed into the air. The four 
faces, numbered 1, 2, 3, 4, have corresponding probabilities 0.1, 0.2, 0.3, 
0.4 of being on the bottom when the tetrahedron comes to rest. Given 
that face 1 or face 2 is down, what is the probability that it is face 1 ? 
Solution. Since we are given that face 1 or face 2 is down, we can ignore 
the other two possibilities and consider a reduced sample space consisting 
solely of the outcomes face 1 down and face 2 down. The probability of 
face 2 is twice that of face 1. Hence, in a large number of performances 
of the experiment resulting in one of these two faces on the bottom, we 
* Historical results suggest that these figures approximate the relative strengths 
of the teams in actual play. 
132 PROBABILITY FOR FINITE SAMPLE SPACES [CHAP. 4 
expect face 1 to be down about - of the time and face 2 to be down about 
- of the time. Therefore, 
1 0.1 
P (face 1[face 1 or face 2) =  = 0.1 d- 0.2' 
The result has the form 
P(AIn or B) -- + p(B), 
where A and B are the mutually exclusive events "face 1 down" and "face 
2 down." 
EXAMPLE 2. Color blindness. Assume that 5% of males and 1% of 
females are color-blind, and that males and females each form 50% of 
the population. A researcher studying color blindness selects a color-blind 
person at random. What is the probability that the person so selected is 
(a) male, (b) female? 
Solution. The given data provide us with the probabilities shown in 
Table 4-4. For instance, 5% of 50% of the population, or 2.5%, is both 
male and color-blind, so 47.5% is male and not color-blind. Similarly, 
1% of 50%, or � of 1%, is female and color-blind, and 49.5% is female 
and not color-blind. A sample of 1000 persons having exactly these per- 
centages would have 25 color-blind males and 5 color-blind females; a 
total of 30 color-blind persons. Since males are o of this group and fe- 
males are 6o, it seems reasonable to say that the probability of selecting 
a male is 2s and the probability of selecting a female is 
TABLE 4-4. COLOR BLINDNESS. 
Color-blind, Normal color vision, 
' N Row sums 
Male M 025 .475 .500 
Female F .005 .495 .500 
Column sums 030 .970 1. 000 
We write the conditional probability of the event "person selected is 
male," given the event "person is color-blind," as 
P (male]color-blind)= P(M]C)= 2s_ s 
4-4] CONDITIONAL PROBABILITY 133 
Note that 25 is also the same as 
0.025 _ P(M r3 C) 
0.030 P(C) ' 
and we have, for this example, 
P�[C) = P( n C). 
P(c) () 
Similarly, 
P(F[C) P(F n C) 0.005  
= P(c) = o.o3-- = ' 
Since the axioms do not treat conditional probability, we require a 
definition. 
4-8 Definition. Conditional probability. The conditional probability of 
an event A, given B, is denoted by P(AIB) and is defined by 
(2) 
xvhere A, B, and A r3 B are events in a sample space S, and 
P()  O. 
IEMARK. If we multiply both sides of Eq. (2) by P(B), we get 
P(A r3 B) = P(B) � P(AIB). (3) 
Order is not important in A F B, because 
A r3B = B r3A. 
Hence xve also have 
P(B r3 A) = P(B) . P(AIB), (4) 
and 
P(A C B) ---- P(A) . P(BIA). (5) 
134 PROBABILITY FOR FINITE SAMPLE SPACES [CHAP. 4 
Equation (5) is used in Section 4-5 to assign probabilities. It can also 
be extended to three or more events. For example, the probability of the 
joint occurrence of three events A, B, and C is 
P(A N B a C) = P(A) � P(BIA) � P(C]A a B). (6) 
Another way of looking at Eq. (2) may be helpful. Consider a sample 
space S and the events, A, B, and their intersection A  B (shown shaded 
in Fig. 4-2). If we are given B, we ignore all other possible outcomes in S, 
and think of B as constituting a new, reduced, sample space S*. (See 
Fig. 4-2.) If we were to assign to points of S* the same probabilities they 
Fid. 4-2. New sample space. S* = B. 
had in S, these would add up only to P(B). We wish that the total proba- 
bility in the new sample space S*, which is just B, were one. We achieve 
this desired goal by enlarging all probabilities p of points in B by multi- 
plying each probability by the constant factor 1/P(B). When we assign 
these new probabilities to points of B, 
P(B) ' (7) 
and sum both sides of Eq. (7) over all values of i corresponding to sample 
points in B = S*, we find that the total probability in S* is 
Z P P(B) 
Z - - 
as desired. 
Finally, to get the probability of any event A, given B, we add the * 
probabilities of the points of A that are in the reduced sample space 
S* = B. These are the points in the intersection A  B; we denote their 
probabilities in S by , , ..., m. Then, summing both sides of Eq. (7) 
for values of  from 1 through m, we get 
P(AIB) = pt = = P(B) 
4-4] CONDITIONAL PROBABILITY 135 
1kote that the conditional probability of A, given B, is proportional to 
the probability of A ('l B, the proportionality factor being lc - lIP(B), 
just as it is in Eq. (7) for individual points. 
EXAMPLE 3. A coin is tossed until a head appears, or until it has been 
tossed three times. Given that the head does not occur on the first toss, 
what is the probability that the coin is tossed three times? 
Solution. A sample space is given by 
with associated probabilities 
P(H) = , P(TH) = , P(TTH) P(TTT) = 1 
These add to 1. Let B be the given event, "no head on first toss." Then 
nd 
1 
P(B) =  +  + s-- . 
Next, let A be the event "coin is tossed three times." Then 
n = {TTH, TTT}, P(A) = , 
A  B = A, P(A  B) 1 
Hence 
P(AIU) P(A 
= P(B) 
EMru 4. The integers from 1 through n are ssigned probabilities 
proportional to their sizes. () Find the probabilities. (b) Find the con- 
ditional probability of 1, given that 1 or n occurs. 
Solution. () The total probability must be 1, and the probability of 
any integer i from i through n is proportional to i: 
P(i) = k X i;  = 1,2,...,n. 
Then 
n 
136 PROBABILITY FOR FINITE SAMPLE SPACES [CHAP. 4 
But 
n(n + l) 
1 -]- 2 -]- 3 -]- � � � -]- n = 
2 
Hence 
2 
k-- 
n(n + 
and 
2i 
P(i) = n(n-]- 1) 
(b) P(I 1 or n) P(1 FI 1 or n) P(1) 
-- P(1 or n) = P(1) d- P(n) 
kx1 1 
kx ld-kxn lq-n 
Note that we did not need to know the value of the proportionality factor 
k to solve part (b). 
EXE. RCISES FOR SECTION 4-4 
1. If A and B are mutually exclusive and P(B) is not zero, what can you say 
about P(AIB)? Interpret your result. 
2. If A always occurs when B does, then every sample point in B is also in 
A; B is a subset of A. What can you say about P(AIB ) in such circumstances? 
Interpret your result. 
3. If A and B are independent and P(B)  0, what can you say about 
P(AIB)? Does this seem reasonable? 
4. In Example 1, what is the probability that face 3 is down, given that face 
4 is not down? 
5. In Example 2, a person is selected at random from the population of people 
with normal color vision. What is the probability that the selected person is 
(a) male? (b) female? 
6. In Example 3, givcn that the coin was tosscd at most two times, what is 
thc probability that it was tosscd exactly twice? 
7. A coin is tossed until a head first appears, or until it has been tossed 4 
timcs. Given that a head did not appear on either of the first two tosses, find 
the probabdity that (a) the coin was tossed 4 times, and (b) it was tossed just 
3 timcs. 
8. ByEq.(3),ifP(B)  O, thenP(A FIB) = P(B).P(AIB ). IfP(B) = O, 
P(AIB ) is undefined. But P(A r3 B) = P(B) � P(AIB) is still true in some 
sense. Why? 
9. In a high-school class of 180 students, all of whom took both English and 
History, 15 failed History, 10 failed English, and 5 failed both. Find the prob- 
ability that a student chosen at random from this class failed History and passed 
English. Find the probability that hc failed English and passed History. 
4-5] usxo Tp, Pro)ucT uuLp 137 
10. Twenty boys went on a picnic. Five got sunburned, 8 got bitten by 
mosquitoes, and 10 got home without mishap. What is 
sunburned boy was ignored by the mosquitoes? What is 
a bitten boy  as also burned ? 
11. An insurance company finds that about one check in a thousand is drawn 
on insufficient funds, and that sucl cheeks are invariably postdated. The 
company also finds that bou[ oue cleck in one hundred drawn on sufficient 
funds is postdated. If a postdated cheek is eeeived, what is the pobabili[y 
tim[ it comes fiom a ousttuner having insufficient funds? 
12. In the baseball problem, Exercise 11 of Section 4-2, find the theoretical 
bafing average. 
13. Suppose 2 bad light bulbs get mixed up i[h 10 good ones, and that you 
start testing the bulbs, one by one, until you have found both defectives. What 
is the probability that you will find the last defective on the 711 testing? 
14. (a) If, in the two-dice expmimen[ of Table 3-3, 
least one die has fewer than 3 spots showing, what is 
other die has 3 or more spots 9 (b) If we are given that r  c + 2, what is 
the probabih[y that r + c = 107 
1. If a family having 4 ehil&en is known [o have at least 1 boy, what is 
he probability ha it has exactly 2 boys? (Assume that boys and girls have 
an equal chance of being born.) What additional uns[a[ed assumptions are 
5 ou making � 
16. The sample space consists of the integers from 1 [o 2n, which are assigned 
probabilities proportional to their logarithms. (a) Find the probabihfies. 
(b) Show that the conditional probability of the integel 2, given that an even 
integer occurs, is 
log 2 
n log 2 + log (nl) 
17. Prove: If A and B are mutually exclusive and P(.t U B) is not zero, then 
P(A) 
+ 
Which examp}es in he ex il}usrae applications of his result? 
4-5. USING THE PRODUCT RULE TO ASSIGN PROBABILITIES IN A 
SAMPLE SPACE 
We now illustrate hmv the rule 
P(A n B) = P(B n A) = P(I 
is used in assigning probabilities when A and B are not assumed to be 
independent. 
EXAMPLE 1. Jimmy likes to go shopping with his mother because he 
can sometimes get her to buy him a toy. The probability that she takes 
138 PROBABILITY FOR FINITE SAMPLE SPACES [CHAP. 4 
him along on her shopping trip this afternoon is 0.4, and if she does, the 
probability that he gets a toy is 0.8. What is the probability that she takes 
him shopping and buys him a toy ? 
Solution. 
P (shopping and toy) ---- P (shopping) � P (toylshopping) 
: 0.4 X 0.8 : 0.32. 
EXAMPLE 2. A magazine advertiser estimates that the probability that 
his ad will be read by a subscriber is 0.4, and that if it is read, the proba- 
bility that the reader will buy his product is 0.01. Using these estimates, 
find the probability that a subscriber will read the ad and buy the product. 
Solution. 
P (read ad and buy product) = P (read ad) � P (buy productlread ad) 
= 0.4 X 0.01 = 0.004. 
EXAMPLE 3(a). Drawing without replacement. An urn contains 5 black 
balls and 10 red balls. Two balls are drawn at random, one after the other, 
without replacement. St up a sample space for the possible outcomes of 
the experiment, with appropriate probabilities. 
Solution. As a sample space for the experiment, we take 
s= {(B,B), (B,R), (R,B), (R, 
where, for example, (B, R) means "first ball black, second ball red." 
Since balls are drawn at random, all balls in the urn at any drawing are 
equally likely to be drawn. For both balls to be black, the first one drawn 
must be black (p = ), and the second one drawn must also be black 
(P2 ---- 44)' Therefore 
P(B, B) ---- P (lst ball black) � P (2nd ball blacklist ball black) 
__ 5 4 2 
-- 14 -- 21' 
Likewise, 
P(B, R): P (1st ball black) � P (2nd ball red[lst ball black) 
5 10 -- 5 
---- -  14 21 
P(R, B)  P (1st ball red) � P (2nd ball blacklist ball red) 
__ 1   5 
-- 15 14 -- 21 
4-5] USING THE PRODUCT RULE 139 
P(R, R) = P (1st ball red) � P (2nd ball red]lst ball red) 
__ 10 9 __ 9 
14 -- 21' 
The results are summarized in Table 4-5(a). 
TABLE 4-5(a). DRAWINGS WITHOUT REPLACEMENT. 
Second ball 
Ro? 
B R totals 
B 2_ __ 
First 21 21 3 
5 9 2 
ball R 
x 2 1 
Column totals 
Note that � X �  2r; the outcome for the second ball is not inde- 
pendent of the outcome for the first ball. 
EXAMPLE 3(b). Drawing with replacement. If the sampling in Example 
3(a) is done with replacement (we put the first ball back before drawing 
the second ball), then the probability of a black ball on the second drawing 
is independent of the outcome on the first: 
P (2nd ball black) ---- P (lst ball black) =  -- i 
The probabilities of sample points in this new experiment are shown in 
Table 4-5(b). Each cell entry is the product of the corresponding row 
total and column total. 
TABLE 4-5(b). DRAWINGS WITH REPLACEMENT. 
Second ball 
Row 
B R totals 
B i a 
First 9 9 3 
ball R 2 4 2 
Column totals � I  i 
140 PROBABILITY FOR FINITE SAMPLE SPACES [CHAP. 4 
EXAMPLE 4. TWO urns. An 3rdinary die is thrown once. If a 1 or 6 
appears, a ball is then drawn from urn I, otherwise a ball is drawn from 
urn II. Urn I contains 3 red balls, 2 white balls, 1 blue ball. Urn II con- 
tains 4 white balls, 2 blue balls, and no red balls. Set up a sample space 
for the possible outcomes of the experiment and find the probability (a) 
that a white ball is drawn, and (b) that urn I was used, given that a white 
ball was drawn. 
Soluhon. The experimental conditions imply the following probabilities: 
P(I)- 1 P(/[I) = �, P(W[I) �, P(B[I) = -, 
P(II) = , P(/[II) O, P(W[II) , P(B[II) = 1 
Using these data, we construct a sample space showing the urn used 
and the color of ball drawn. Table 4-6 shows probabilities of the possible 
outcomes of the experiment. Hence,/)(IF) = , and 
P(IIW) P(I f W) 1/9 1 
---- P(W) ---- 5/ = ' 
The chance of urn I, given that a white ball was drawn, is no longer 1 
in 3; now it is only 1 in 5. 
TABLE 4--6. Two UR.XS. 
Color of ball 
R W B Row sums 
I 1 
Urn 
II 0 4 2 2 
1 5 5 1 
Column sums g  1-' 
EXAMPLE 5. Bridge and pinochle cards. In the card room of a men's 
club, there are 5 ordinary bridge decks and 3 pinochle decks, all having 
similar construction and designs. One of these 8 decks is chosen at random, 
and a card is randomly drawn from it. If the card is the jack of hearts, 
what is the probability that it came from a pinochle deck? From a bridge 
deck? Pinochle decks contain 48 cards; two each of 9, 10, jack, queen, 
king, and ace in the four suits clubs, diamonds, hearts, and spades. 
4--5] USING THE PRODUCT RULE 141 
Solution. Since the experiment consists of first choosing a deck of cards, 
and then choosing a card from that deck, the sample space that we think 
of is a set of ordered pairs (x, y), with 
x  bridge, if a bridge deck is drawn, 
z  pinochle, if a pinochle deck is drawn, 
and   a designation of a card. 
The probability of drawing a jack of hearts from a pinochle deck is 
from a bridge deck, the probability is . Now to answer the question: 
P(pinochlejack of hearts)  P(pinochle  jack of hearts) 
P(jack of hearts) 
and 
P(]ack of hearts) = P(pinochle  jack of hearts) 
 P(bridge  jack of hear) 
= P(pinochle) � P(jack of hearts[pinochle) 
+ P(bridge) � P(jack of hearts  bridge) 
3 2 5 1 23 23 
= X +X 5 = 83= 8(8)(13) 
Therefore 
aX- 13 
P(pinochle]jack of hearts) = s 4s = __ 
a 23 
Similarly, 
5 
P(bridge]jack of hearts) =  X  _ 1. 
* 23 
8(8)(3) 
Alternatively, since "bridge deck" and "pinochle deck" are complementary 
events, we could have computed the o from i  * Table 4-7 shows 
 23. 
the foregoing computations in another form. 
TABLE 4--7. BRIDGE AND PINOCHLE. 
Jack of hearts Other Row sums 
5 
Bridge  X 2  X   
3 
Pinochle s  X -  X  s 
48 -- 
5 6 23 so9 1 
Column sums 8 X 5 q- 8 X 4 - 832 sa2 
142 PROBABILITY FOR FINITE SAMPLE SPACES [CHAP. 
P(pinochleljack of hearts) ---- s 4s 
23 -- 23, 
832 
P(bridgeljack of hearts) ---- 1 -- 
We note that the information that the card drawn was the jack of 
hearts has changed the chances for a pinochle deck from , which is less 
than , to o which is greater than . Naturally, if a 2, 3, 4, 5, 6, 7, 
or 8 had been drawn, we would know that it came from a bridge deck. 
Question: What is the probability that such a card would be drawn? 
EXERCISES FOR SECTION 4-5 
1. In the two-urn example, Table 4-6, verify the probabilities of the fol- 
lowing events: 
(a) II (3 IV (b) I (3 B (c) I (3 R (d) II (3 B 
2. In the two-urn example, Table 4-6, find the following conditional proba- 
bilities: 
(a) P(IIIW) (b) P(IIR) (c) P(IIIR) (d) P(IIB) (e) P(IIIB) 
3. Suppose that the urn in Example 3, Table 4-5(a), contains b black balls 
and r red balls. Set up a sample space and assign probabilities to points in it, 
assuming that two balls are drawn, one after the other, without replacement. 
Using these probabilities compute: 
(a) P (2nd ball red]lst ball black), (b) P (2nd ball red), 
(c) P (lst ball red[2nd ball red). 
4. Repeat Exercise 3, assuming that the sample is drawn with replacement. 
5. (This problem should be worked before going on to Section 4-6.) In a 
certain factory, machine A produces 30% of the output, machine B produces 
25%, and machine C produces the rest. One percent of the output of machine 
A is defective, as is 1.2% of B's output, and 2% of C's. In a day's run, the three 
machines produce 10,000 items. What is the probability that one item drawn 
at random from these 10,000 is defective? If it is defective, what is the probabil- 
ity that it was produced by A? by B? by C? 
6. Suppose P(E) = 0.3, P(F) = 0.2, and P(E [3 F) = 0.4. Make a two- 
by-two table showing probabilities of E F/ F,  F1 F, E F/ F, E F/ Y. What are 
the following probabilities equal to? 
(a) P(E 91 F) (b) P(EIF) (c) P(FIE) (d) 
(e) P(E [3 Y) (f) P( [3 P) 
4--6] B.A_YIgS' TH EORE1V[ 143 
?. In rolling a die repeatedly, what is the probability that a ! appears for the 
first time on the 4th roll? 
$. In dealing cmds from a bridge deck, what is the probability that the first 
spade occurs at the 5th card? 
9. In Example 1, suppose that the probability that Jimmy's mother will 
buy him a toy when she does not take him with her is 0.2. If the other proba- 
bilities are unchanged, what is the probabfiity that she gets him a toy when 
she goes shopping? 
10. In Example 2, make the further assumption that the probability that a 
nonsubsmber ill read the ad is 0.002, and that if a nonsubscriber reads the ad, 
the probability that he ill buy the product is 0.00S What is the probability 
that a randomly chosen pro'son will read the ad and buy the product? Assume 
there is one chance in 20 that a person is a subscriber. 
11. From twelve tickets numbered from 1 thlough 12, two tickets are drawn, 
one after the other, without replacement. What is the probabfity that (a) both 
numbers are even? (b) both numbers are odd? (c) the first number is even and 
the second number is odd? (d) one number is even and the other number 
is odd ? 
12. In preparation for an examination, a student has been given two sets of 
questions to study, with 5 questions in each set. At the time of the examina- 
tion, he knows the answms to all of the questions in the first set, and to 4 of 
the 5 questions in the second set. If the examination consists of 2 questions, 
2 chosen at random from one set, and 1 chosen at random from the othm set, and 
the examiner tosses a coin to decide which set to take the two problems from 
what is the probability that the student will be able to answer all of the ques- 
tions? That he can answer only 2 of them? 
12. Suppose, in Exercise 12, that there me 10 questions in each set and, at 
the time of the examination, the student knows the answers to 9 questions in 
the first set and $ questions in the second set. If the other conditions of Exercise 
12 are unchanged, what is the probability that the student can answer all 2 
questions on the examination? That he can answer none of the questions? 
14. In the bridge-and-pinochle example, suppose that thee were equal 
numbers of bridge decks and pinochle decks in the card room. If a deck is 
selected at random and a card is chosen at random from that deck, and the 
card is the 10 of spades, what is the probability that it came from a pinochle 
deck? From a bridge deck? 
15. Answer the questions of Exercise 14 assuming, however, that there ale 
$ pinochle decks and 4 bridge decks in the card room. 
4-6. BAYES' THEOREM 
At the start of the experiment in the bridge-and-pinochle example at the 
end of Section 4-5, the probabilities of drawing a bridge deck or a pinochle 
deck were  and , in that order. These probabilities measure the chances 
that a bridge deck, or a pinochle deck, will be used, and are often called 
a priori, or prior, probabilities. They are probabilities prior to any informa- 
tion that the experiment may yield. 
144 PROBABILITY FOR FINITE SAMPLE SPACES [CHAP. 4 
Suppose now that we know the conditions of the experiment, and that 
we are allowed to see only the card that was drawn. If a prize is offered for 
correctly guessing the kind of deck the card came from, should we always 
guess "bridge" on the ground that its prior probability was  while that 
of "pinochle" was only ? Obviously not, for if a face card is drawn, it is 
more likely to have come from a pinochle deck. We are therefore interested 
in the conditional probabilities of "bridge" and "pinochle," given the 
designation of the card that was drawn. These are called the a posterJori, 
or posterior, probabilities because they are the probabilities after the result 
of the experiment is known; or at least after we know the designation of 
the card that was drawn. 
Table 4-8 shows the posterior probabilities of "bridge" and "pinochle" 
for each possible card drawn. It also shows the prior probabilities, for 
comparison. We notice here that only the outcome "9 through ace" would 
change our guess from "bridge" to "pinochle." If 2 through 8 is drawn, 
the probability is 1 that it came from a bridge deck. 
TABLE 4--8. iRIOR AND POSTERIOR PROBABILiTiES. 
Posterior, given card was: 
Prior 
2 through 8 9 through ace 
lO 
Bridge  1 2--5 
3 [ 13 
Pinochle I   0 2--5 
EXAMPLE 1. TWO urns. In the two-urn example, Example 4 of Section 
4-5, find the posterior probabilities of urns I and II, given that the first 
ball drawn is replaced and, after mixing, a second ball is drawn from the 
same urn as the first ball, and that both balls are white. 
Solution. We forego listing the sample space S consisting of sample 
points like (I; R, R), (I; R, W), and so on to (II; B, B), where (I; R, W) 
means "urn I is used, the first ball drawn is red, and the second ball drawn 
is white," and so on. But we have such a sample space in mind, and we 
asslgr probabilities to its points in accord with the laws of conditional 
probability. For example, since the first ball is replaced before the second 
is drawn, 
P(I; R, W) ---- P(I). P(R[I) � P(W[I) 
__ I 1 
- 
4-6] BAYES' THEOREM 145 
The purpose of this example is to introduce the notation used in the 
general Bayes' Theorem, so we do not leap at once to the numerical 
solution. 
Let E be the event "two white balls are drawn": 
E = {(I; w, w), (II; w, w)}. 
Also, let H1 be the event "urn I was used," and H2 the event "urn II was 
used." Note that H1 and H2 are mutually exclusive and that their union 
is S. We want the conditional probabilities 
P(u11E) and 
The formula for conditional probability tells us that 
p(HiE ) _-- P(H ( E) 
P(E) ' (1) 
and we can write a similar equation with H2 in place of H. It is easy to 
compute P(H ffl E) and P(H  E); their values are 
- (2a) 
P(H1 FE)= P(I;IF, W)---- �'6 6 -- 7, 
P(H F E) = P(II' W, IV) = -.-. - -- s__ (2b) 
, 6--27' 
Moreover, 
" (3) 
P(E) = P(H1 CI E) q- P(H2 r3 E) -- 27, 
since E must occur either with H or with H2, and it cannot occur simul- 
taneously with both. If we now substitute from Eqs. (2) and (3) into 
Eq. (1), we get 
P(H1 r3 E) 1/27 1 
P(Hi[E) ---- P(H1 (3 E) q- P(H2 r3 E) -- 9/27 --  = 0.11, 
P(H2 r3 E) 8/27 8 = 0.89. 
P(tI21E) ---- P(H1 CI E) q- P(H2 r3 E) -- 9/27 -- 9 
Note hoxv the evidence provided by the outcome "both balls white" is 
reflected in the high posterior probability of urn II, where white balls 
predominate. 
Bayes' Theorem, which generalizes the results of examples like the fore- 
going, can be used in scientific work in the following way. Suppose there 
are several mutually exclusive and exhaustive hypotheses H, H2, . .., H 
to account for a phenomenon that is subject to test by experiment. Before 
146 PROBABILITY FOR FINITE SAMPLE SPACES [CHAP. 4 
a particular experiment begins, it may be very hard to assign probabilities, 
i.e., prior probabilities, to these hypotheses. An experimenter might assign 
probabilities that are in some way proportional to the "intensity of belief" 
he has in the various hypotheses. (Another investigator might assign quite 
different probabilities.) An experiment is performed, with the aim of 
discovering evidence to modify these prior probabilities. Such evidence 
may even assign such low posterior probabilities to some of the hypotheses 
as to eliminate them from further consideration, just as the drawing of an 
eight eliminates the pinochle deck. 
Each new experiment can begin with a priori probabilities of the re- 
maining hypotheses proportional to the a posterJori probabilities that 
resulted from the previous experiments. In this way, scientific evidence 
accumulates and modifies our beliefs, weakening our intensity of belief 
in some hypotheses, strengthening it in others. And the more evidence 
that accumulates, the less does it matter what the original a priori proba- 
bilities were, provided they were all tenable and that no possible hypothesis 
was assigned prior probability 1 or zero. 
4-9 Bayes' theorem. Let H, Ha, � � �, H be mutually exclusive events 
whose union is the sample space S of an experiment. Let E be an 
arbitrary event of S such that P(E)  0. Then 
P(H1 n E) 
P(HiIE) ---- P(H1 n E) q- P(Ha n E) q- ... q- P(H, n E) ' (4) 
and similar results hold for Ha, H3, and so on. 
Proof. The proof will be given for the case n = 3. Figure 4-3 and 
Tables 4-9 and 4-10 illustrate this case. The three hypotheses Hi, Ha, 
and H3 are mutually exclusive and exhaustive; their union is S. The part 
of E that is in Hi is Hi n E, the part in H2 is Ha n E, and the part in 
4-3. Partitioningwhcrcn = 3: S = Hi YHa U H3, E = (Hi n E) 
n E) u n E). 
4-6] BAYES' THEOREM 147 
H3 is H3 l E. The entire event E is the union of these three mutually 
exclusive events--similarly for the complementary event , which we 
include for completeness but which plays no part in the proof. 
TABLE 4--9. PARTITION OF SAMPLE SPACE. 
Event 
Hypothesis E  Unions of rows 
H H ffl E tt ffl  H 
H H  E H   H 
H3 H3 CI t H3 CI  H3 
Unions of columns E ' S 
Table 4-10 gives the probabilities of the joint events in the cells of 
Table 4-9. 
TABLE 4--10. PROBABILITIES FOR TABLE 4--9. 
Event 
Hypothesis E E Row sums 
H P(H CI E) P(H CI E) P(H) 
H2 P(H2 (3 E) P(H2 (3 E) P(H2) 
H P(H CI E) P(H CI E) P(H) 
Column sums P(E) P(E) 1 
Since the H's are mutually exclusive and exhaustive, the first column 
sum is P(E)' 
P(E) = P(H (h E) + P(H  E) + P(H3 E). (5) 
By the law of conditional probability, 
p(HiE ) _ P(H tn E) 
P(E) 
P(H1 ffl E) 
P(H1 Cl E) q- P(H2 Cl E) q- P(Ha (3 E) 
148 PROBABILITY FOR FINITE SAMPLE SPACES [CHAP. 4 
This completes the proof for n = 3. The proof for n---- 2, or n >_ 4, 
follows the same pattern and leads to Eq. (4) in each case. [] 
EXAMPLE 2. (See Exercise 5, Section 4-5.) In a factory, machine 21 
produces 30% of the output, machine B produces 25%, and machine C 
produces the remaining 45%. One percent of the output of machine 21 
is defective, as is 1.2% of B's output, and 2% of C's. In a day's run, the 
three machines produce 10,000 items. An item drawn at random from a 
day's output is defective. What is the probability that it was produced 
by A? by B? by C? 
Solution. You have already applied Bayes' Theorem if you solved this 
exercise in Section 4-5. The connection is made by taking E, Hi, H2, and 
H3 to be the following events: 
E: defective item, 
Hi' item produced by machine .4, 
H2' item produced by machine B, 
H3' item produced by machine C. 
Then P(H IE) is the probability that the item was produced by A, given 
that it was defective. P(H N E) is the probability of the event "produced 
by A and defective," with similar meanings for P(Hs  E) and P(H  E). 
The data give the following probabilities for an item selected at random 
from the total day's production: 
P(Hi) = 0.30, P(EIH) = 0.010, 
P(H) ---- 0.25, P(E[H) ---- 0.012, 
P(H) = 0.45, P(EIH) = 0.020. 
From these data we may compute 
P(H  E) = P(H) . P(EIHi) = 0.003 
P(H q E) = P(H) . P(EIH2) = 0.003 
P(H q E): P(H3) . P(EIH) = 0.009. 
Total: P(E) = 0.015. 
Before an item is drawn from the population and examined, the proba- 
bilities of its having been produced by machines A, B, and C are 0.30, 
0.25, and 0.45 in that order. Bayes' Theorem is useful in telling us how 
4-6] BAYmS' THEOREM 149 
these probabilities are modified when we have the additional information 
that the item drawn was defective. The new probabilities are 
P(H f3 E) 0.003 0.20, 
P(Hi[E) = P(E) -- 0.0115- 
P(H,E) = P(H2 c E) _ 0.003 _ 0.20, 
- P(E) 0.0m 
P(H3 C E) 0.009 0.60. 
P(Ha[E): P(E) -- 0.015- 
We summarize these results: : Machine 
' A B C 
A priori probability 
(before information that item is defective) 0.30 0.25 0.45 
A posteriori probability 
(after information that item is defective) 0.20 0.20 0.60 
This example illustrates one of the chief applications of Bayes' Theorem. 
We start with a set of prior probabilities associated with the possibilities 
Hi, //2, and so on. Next, we perform an experiment and observe that 
event E has occurred. Then we use this information to modify the set 
of prior probabilities, replacing 
P(H) by P(HIE) , 
P(H2) by P(HslE) , 
and so on, with the help of Eq. (4). 
RMAlm 1. In order to compute P(H C E) we use the a priori proba- 
bility P(H) and the conditional probability of E, given//1, because 
P(H (3 E) = P(H) . P(EIH). (6a) 
Similarly for the other "hypotheses," H2 and so on, 
P(Hi ffl J) : ]'(Hi)' P(EIH). (6b) 
150 PROBABILITY FOR FINITE SAMPLE SPACES [CHAP. 4 
If we use Eqs. (6a,b) to evaluate the numerator and denominator in 
Eq. (4), we also have 
P(H1)' P(E[Hi) 
P(H1]E) = P(H1). P(EIH1) 4- P(H2)' P(E]H2)  ... -3- P(H,O ' 
(7) 
The left side of Eq. (7) is the a posterJori probability of Hi, given E; on 
the right side appear the a priori probabihties of H, H,..., H, together 
with the probabilities of E given H1, of E given H, and so on. 
REMARk: 2. In the bridge-pinochle example, the a priori odds are 5 to 
3 in favor of selecting a bridge deck. The a posteriori odds, given that a 
jack of hearts was drawn, are 13 to 10 in favor of a pinochle deck. The 
result in Bayes' Theorem can always, as here, be expressed in terms of 
odds. The a priori odds are proportional to the a priori probabilities 
P(H O, P(H), � �., P(H,O. 
The a posteriori odds are proportional to the numerators of Eq. (4) and 
the other equations like it, since they all have the same denominator. 
Hence the a posteriori odds are proportional to 
P(H N E), P(H  E), ..., P(H,  E). 
EXERCISES FOR SECTION 4-6 
1. Sixty percent of the students in a school are boys. Eighty percent of the 
boys and 75% of the girls have activity tickets for all the school activities. A 
ticket is found and turned in to the school's lost and found department. What 
is the probability that it belongs to a girl? To a boy? 
2. Three girls, Alice, Betty, and Charlotte, wash the family dishes. Since 
Alice is the oldest, she does the job 40% of the time. Betty and Charlotte 
share the other 60% equally. The probability that at least one dish will be 
broken when Alice is washing them is 0.02; for Betty and Charlotte the proba- 
bilities are 0.03 and 0.02. The parents don't know who is washing the dishes, 
but one night they hear one break. What is the probability that Alice was 
washing? Betty? Charlotte? 
3. An experiment consists of throwing a three-sided die and then, depending 
upon the outcome of the throw, selecting a ball from one of two urns. If the 
die falls "1 or 2," the ball is drawn from an urn containing i red ball and 4 
black balls; if the die falls "3" the ball is drawn from an urn with 3 red and 
2 black balls. You didn't see the die thrown, but you observed that a red ball 
was drawn. What is the pobability that it came from the first urn? From 
the second ? 
4-6] nAYES' THEOREM 151 
4. Suppose, in the two-urn problem of Examplc 1, the first ball is not re- 
placed, and a second ball is drawn from the same urn as thc first. If both balls 
are white, what is the probability that urn I was used? urn II? 
5. (Continuation.) In Exercise 4 above, suppose that 2 blue balls are drawn, 
without replacement. What are the a posteriori probabilities of the two urns? 
6. (Continuation.) In Exercise 5, suppose that the filst ball is replaced before 
the second is drawn. Find the a posterJori plobabilities of the urns if both balls 
are blue. 
7. A fair coin is tossed and if it falls "heads" we dlaw a ball froIn uln I; 
if "tails," flora Uln II. Urn I contains 3 red balls and I white ball. Urn II contains 
I red ball and 3 white balls. What ale the a priori and a posteriori probabilities 
of the two urns, assuming (a) that a red ball is drawn, (b) that a white ball is 
draw n ? 
S. Solve Exercise 7 under the modified assumptions that the a priori proba- 
bilities are 0.1 for the first urn and 0.9 for the second. 
9. In Exercise $, suppose the experiment continues for n draxvings, the ball 
being replaced and the contents of each urn thoroughly mixed before thc next 
aliawing. If all n balls drawn are red, what are the a posteriori probabilities of 
the urns? FOl what value, or values, of n are these a posterJori probabilities 
approximately equal? What happens to these probabilities if n is very large? 
What is your interpretation of this result? 
10. Assume that i coin in 10,000,000 has two heads; the rest are legitimate. 
If a coin, chosen at randoln, is tossed 10 times and comcs up "heads" every 
time, what is the probability that it is two-headed? 
11. (Continuation) In Exercise 10, suppose the coin falls "heads" n times in 
a row. How large must n be to make the odds approximately even that the 
coin is two-headed? 
12. A commuter who works in Boston must either go through the Sumner 
tunnel or across the Mystic River bridge to get home. He varies his route, 
choosing the tunnel with probability �, the bridge with probability . If he 
goes by tunnel, he gets home by 6 o'clock 75% of the time; if he goes by bridge, 
he gets home by 6 o'clock only 70% of the time, but he likes the scenery better 
that way. If he gets home after 6 o'clock, what is the probability that he used 
the bridge? 
13. An automobile insurance company classifies drivers as class A (good 
risks), class B (medium risks), and class C (poor risks). They believe that 
class A risks constitute 30% of the dlivers who apply to them for insurance, 
class B 50%, and class C 20%. The probability that a class A driver will have 
one or more accidents in any 12-month period is 0.01, for a class B driver the 
probability is 0.03, and for a class C driver it is 0.10. The company sells Mr. 
Jones an insurance policy and within 12 months he has an accident. What is 
the probability that he is a class A risk? Class B? Class C? 
14. (Continuation.) If a policyholder, in Exercise 13, goes n years without 
an accident, and years are independent, what are the odds that he belongs to 
class A? Class B? Class C? 
15. In a factory, machine A produces 40% of the output and machine B 
produces 60%. On the average, 9 items in 1000 produccd by A are defective 
152 PROBABILITY FOR FINITF SAMPLE S,PACES [CHAP. 4 
and 1 item in 250 produced by B is dcfcctive. An item drawn at random from 
a day's output lS defective. What is the probability that it was produced by A ? 
by B? 
16. Friends of yours play two games about equally often. One game is played 
with one die, the other with two (lice. The score in either game is the number 
of dots on the top face, or faces. You hear the score of a throw announced 
as 2. What is the chance they are playing the one-die game? 
17. Answer the question of Excrcise 16 if the announced scorc is 6. If it is 7. 
If it is 1. 
15. Under hypothesis H1 a rare event E has the very small probability p 
of occurring, xvhile under a second hypothesis, He, its probability is pC. (a) If 
the two hypotheses are cqually likely, and are the only ones, and E occurs, 
find P(HllE). Interprct. (b) Suppose that, instead of E,  occurs. Find 
P(Hi[), compare it with P(H1), and comment on the value of one ' observation. 
19. Events A, A2, . .., Am are mutually exclusive, exhaustive, and equally 
likely a priori hypotheses. The conditional probability of E, given A,, is 
i 
P(EIA,) = -; i = 1, 2, . . . , n. 
n 
If, in two independent trials, EE occurs, find P(AIEE ). Evaluate for i = n, 
n = 10. [You may use the formulas 
i = n(r q- 1)/2, * = n(n q- 1)(2n q- 1)/6.] 
MISCELLANEOUS EXERCISES FOR CHAPTER 4 
For Exercises 1 through 3, use the following data. A box contains 5 books. 
A boy randomly takes out one book and then replaces it. Itc does this 5 times. 
1. What is the probability that he has had every book out of the box? 
2. What is the probability that hc has taken exactly 4 different books from 
the box? 
3. What is the probability that thc number of different books he takes out 
of the box is exactly 3? Exactly 2? Exactly 17 
In Exercises 4 through 7, 4 dce ale thrown, and we want to know the proba- 
bility that: 
4. All four dice show the same number. 
5. No two arc alike. 
6. Two are alikc of one kind and two are alikc of anothcr kind. 
7. Two are alike and the other two differ from these and from each other. 
EXERCISES 153 
In Exercises 8 through 15, use the following information about a game played 
with two regular dice. A player throws two dicc, and if he scorcs 7 or 11, he 
wins. If he scores 2, 3, or 12, he loses. But lf he scores 4, 5, 6, 8, 9, or 10, he 
throws the dice again, and keeps on throwing until hc gets a 7, m which casc 
he loses, or he gets the score that he got on his first throw, in which case he 
wins. Find the probability that: 
8. tie loses on the first throw. 
9. He wins on the first throw. 
10. He scores 4 on the first throw, and goes on to in. 
11. He scores 5 on the first throw, and goes on to win. (�ote. Throws other 
than 5 and 7 can be ignored, once the 5 is thrown.) 
12. He scores 8 on the first throw, and wins. 
13. He scores 9 on the first throw, and wins. 
14. He scores 10 on the first throw, and wins. 
15. He wins. 
In Exercises 16 through 19, assume that 4 cards are drawn, without replace- 
ment, from an ordinary bridge deck. What is the probability that: 
16. All 4 suits are represented. 
17. Exactly 3 suits are represented. 
18. All cards are from the same suit. 
19. Exactly 2 suits are represented. 
20. Draw 4 cards froin a shuffled pack, then put thcm back and repeat until 
25 hands are drawn. Record the numbers of diffcrent suits represented in each 
hand. Compare the experimental relative frequencies with the theoretical 
results you got in Eercises 16 through 19. 
In Exercises 21 through 26, assume that 5 calds are drawn, without replace- 
ment, from a pinochle deck. Find the probability that the number of red cards 
in the hand is: 
21. 5 22. 4 23. 3 24. 2 25. i 26. 0 
27. Use a pinochle deck (or remove the four 2's from a bridge deck), and 
draw a hand of 5 cards, without leplacement. Record the number of red 
cards. Replace the 5 cards that were drawn, shuffle the pack, and repeat the 
experiment until 25 hands have been drawn and the rcsults recorded. Compare 
the observed relative frequencies with the theoretical probabilities of Exercises 
21 through 26. 
In Exercises 28 through 31, use the folloing information. Three students 
.1, B, C, have equal claims for an award. They decide that each will toss a coin, 
and that the man whose coin falls unlike the other two wins. (The "odd man" 
wins.) If all three coins fall alike, they toss again. 
28. Describe a sample space for the result of the first toss of the three coins, 
and assign probabilities to its elements. What is the probability that A wins 
154 PROBABILITY FOR FINITE SAMPLE SPACES [CHAP. 4 
on the first toss? That B does? That C does? That there is no winner on 
first toss? 
29. Given that thee is a winner on the first toss, what is the probability 
that it is A ? 
30. What is the probability that no winner is decided in the first 2 tosses? 
In the first n tosses? 
31. Given that no winner is decided in the first n tosses, what is the probability 
that .1 wins on the next toss? 
CHAPTER 
NUMBERS DETERMINED 
BY EXPERIMENTS. 
RANDOM VARIABLES 
5-1. RANDOM VARIABLES AND THEIR PROBABILITY FUNCTIONS 
This chapter introduces two important new concepts: random variable 
and probability function. The idea of a sample space is familiar, and we 
use examples based on this idea to show how random variables and their 
probability functions arise. The examples point the way to general defini- 
tions, and we then go on to study some properties of random variables. 
E.PLE 1. Three coins are tossed. How many fall "heads"? 
Discussion. The answer is a number determined by the outcome of 
the experiment. The number may be 0, 1, 2, or 3. Although we cannot 
predict the outcome exactly, we can say what the possibilities and proba- 
bilities are. A sample space for the experiment is shown in the first column 
of Table 5-1. The second column shows the number of heads for each 
TABLE 5--1. THREE COINS. 
Sample Number of 
point heads Probability 
HHH 3 
HHT 2 
1 
HTH 2 
1 
THH 2 
THT 1 
TTH 1 i 
TTT , 0  
155 
150 RANDOM VARIABLES [CHAP. 5 
sample point, and the third column shows the probabilities of the sample 
points. 
The information about the possible numbers of heads, and their proba- 
bilities, is collected in Table 5-2. The probability of getting exactly 2 heads 
is found by adding the probabilities of HHT, HTH, THH, and similarly 
for other possibilities. 
TABLE 5--2. THREE COINS. PROBABILITY FUNCTION FOR 
NUMBER OF HEADS. 
Probability     
No. of heads 0 1 3 
If we let the variable X represent the number of heads, then Table 5-2 
shows the possible values that X can have, and the probability of each 
value. This set of ordered pairs, each of the form 
(number of heads, probability of that number), 
is the probability function of X. Since the value of X is a number de- 
termined by the outcome of an experiment, X is called a random variable. 
It may seem a bit awkward at first, but we often wish to distinguish 
between a random variable X and one of its values. To help us make 
such a distinction we use the capital letter X for the random variable and 
the small letter x for one of its values. And we use f(x) (read "f at x") 
for the probability that the random variable X takes on the value x: 
f(x) = = 
Thus, in the three-coin experiment, 
f(0) = P(X = 0) = -, f(2) = P(X = 2) = , 
f(1) = P(X = 1) = , f(3) = P(X = 3) = . 
:NOTE. In this example, the vahles of f(x) are 
1X     
, 3X 3X 1X 
The coefficients 1, 3, 3, 1 are the binomial coefficients () for x = O, 1, 2, 3. 
Consequently all values of f(x) are given by the following formulas: 
5--1] PROBABILITY FUNCTIONS OF RANDOM VARIABLES 157 
_ 3, 
x!(3 -- x)!  , x = 0, 1, 2, 3. 
EXAMPLE 2. Sums. A three-sided die is made from an engineer's ruler 
by painting the numbers 1, 2, and 3 on the three long faces. If such a 
die is thrown twice, what is the probability function of the random vari- 
able X, where X is the sum of the two face-down digits? 
Soldion. All pairs of faces are equally likely. The sample space of 
outcomes can be convmxiently listed in a square array, as in Table 5-3. 
In that table (2, 1), for example, indicates that 2 was the outcome of 
the first throw and that 1 was the outcome of the second throw. We enter 
the value of X below each outcome pair. 
TABLE 5--3. SAMPLE SPACE FOR 2 THROWS OF THREE-SIDED DIE. 
Outcome of second throw 
I 2 3 
(1, 1) (1, 2) (1, 3) 
I 
2 3 4 
Outcome 
of (2, 1) (2, 2) (2, 3) 
first 2 3 4 5 
throw , 
3 (3, 1) (3, 2) (3, 3) 
4 5 6 
Because each cell has probability -}, the probability function of X, 
the sum of the numbers on the two bottom faces, is obtained by counting 
the number of ways each sum can happen and dividing by 9. The result 
is the probability function in the following table: 
Probability, f(x)  2 a 2 _ 
Sum, x 6 
You may have noticed that in this example X is the sum of two other 
random variables, the outcome on the first throw and the outcome on the 
second throw, which might be labeled U and V, respectively. Thus 
x=u+v. 
158 RANDOM VARIABLES [CHAP. 5 
TABLE 5--4. iUMBERS OF DIVISORS OF INTEGERS 1 THROUGH 10. 
Integer 1 2 3 4 5 6 7 8 9 10 
No. of divisors 1 2 2 3 2 4 2 4 3 4 
EXAMPLE 3. We select one of the integers 1 through 10 at random 
and count its divisors, or factors. What is the probability that it has 
exactly 2 divisors? Exactly 1 divisor? Exactly 4 divisors? More than 4? 
Solution. We first explain the terminology, which is conventional in the 
theory of numbers. We say that one integer is a divisor, or factor, of a 
second integer if the second is a whole number times the first. Thus the 
divisors of 6 are 1, 2, 3, and 6; and the divisors of 7 are 1 and 7. Here 
we are interested only in positive divisors. 
The first row of Table 5-4 is a sample space for the experiment of select- 
ing an integer from 1 through 10 at random. Let X be the number of 
divisors of the selected integer. The second row shows the value of the 
random variable for each sample point. Each integer has probability 0.1 
of being drawn because the expression "at random" means that the 
integers 1 through 10 are equally likely. 
Next, we combine cses according to the number of divisors and add 
their probabilities, 0.1 for each sample point, thus obtaining Table 5-5. 
We let x stand for any one of the possible numbers of divisors, and f(x) 
for the probability that X takes the value x. Thus with the value x = 2, 
xve associate the probability f(2) = P(X ---- 2) = 0.4. 
A graph of the points with coordinates x and f(x) is shown in Fig. 5-1. 
This graph represents the probability function of the random variable X, 
where X is the "number of divisors of an integer from 1 through 10 chosen 
at random." In the graph, vertical bars with lengths proportional to the 
probabilities are added to guide the eye. The actual graph consists of 
just the four points indicated by the dots at the tops of these bars. 
TABLE 5--5. iXUMBERS OF DIVISORS AND THEIR PROBABILITIES 
(FOR INTEGER SELECTED AT RANDOM FROM 1 THROUGH 10) 
P obability, f(x) 0.1 0.4 0.2 0.3 
No. of divisors, x 3 4 
Using Table 5-5, we can easily answer the four original questions about 
the number of divisors: 
= 2) = re) = 0.4, 
p(x = 1) = f(1) = o.1, 
P(X---- 4) ----f(4) = 0.3, 
5--1] PROBABILITY FUNCTIONS OF RANDOM VARIABLES 159 
0.4 0 4 
03  0.3, 
0.2[ 02 ! 
I 
O1 0.1 
I I = x 
0 1 2 3 4 
FIG. 5-1. Graph of probability function of number of divisors of intcgcrs 
from 1 through 10. 
and, since no integer from 1 through 10 has more than 4 divisors, 
P(X > 4) = O. 
With these examples to guide us, we now formulate the following general 
definitions. 
5-1 Definitions. (1) Random variable. A variable whose value is a 
number determined by the outcome of an experiment is called a 
random variable. 
(2) Probability function. Let X be a random variable with possible 
values x, x2, . . . , xt and associated probabilities f(xO, f(x2), � � � , 
f(xt). Then the set f whose elements are the ordered pairs 
(xi, f(xO), i. 1,2,...,t, 
is called the probability function of X. 
Thus the number of divisors of an integer chosen at random from 1 
through 10 is a random variable with t = 4 possible values: 
x : 1, x2 ---- 2, xa: 3, X4 = 4. 
The associated probabilities are those given in Table 5-5. The probability 
function for this example is the set of ordered pairs of numbers repre- 
sented by dots in the graph of Fig. 5-1: 
f = {(1, 0.1), (2, 0.4), (3, 0.2), (4, 0.3)}. 
160 ^)or V^mABLES [C^P. 5 
In this book, we rarely list probability functions as sets of ordered pairs. 
It is more convenient, and equally valid, to show the probability function 
by means of a formula for f(x), or by means of a table like Table 5-5. 
Probabilities are assigned to events (sets of sample points), and the 
assignment of a probability to each of the possible events is called the 
probability distribution over the sample space. In finite sample spaces 
with n elementary events, there are 2  possible sets; hence if n is lrge, 
it is inconvenient to list the probabilities for 2  sets. Instead, we usually 
give the probability function, which lists  probability for ech of the 
n elementary events. Thus the probability function is one wy of sum 
mrizing the probability distribution. In discussing probabilities generally, 
it is common to spek of the probbiliW distribution interchangeably with 
the probability function. 
Cox. A rndom wrible is like ny other vrible except that 
we my know more bout the rndom wrible, nmely the probability 
that it tkes ny one of its possible wlues. 
Exrn 4. Matching historical events and dates. A student is to 
match three historical events (bttle of Lexington nd Concord, Coltim- 
bus's discovery of America, bttle of Hstings) with three dtes (1775, 
1492, 1066). If he guesses, with no knowledge of the correct nswers, 
wht is the probability function of the number of nswers he gets right? 
Soldion. A sample space S for the experiment of giving the student 
this test could be the following six permutations of the three dates: 
el' 1066, 1492, 1775 e4: 1492, 1775, 1066 
e2' 1066, 1775, 1492 es: 1775, 1066, 1492 
ca' 1492, 1066, 1775 e6: 1775, 1492, 1066 
If he answers strictly by guessing, then each permutation has probability  
Xext, we associate with each element of S the number of correct answers 
X that it provides. With no loss of generality, we may assume that the 
events are listed in the order (1) battle of Hastings, (2) Columbus's 
discovery of America, and (3) battle of Lexington and Concord. If the 
student chooses e as his answer, he gets all 3 right. If he chooses e2, ca, 
or e6, he gets 1 right. If he chooses e4 or es, he gets 0 right. Table 5-6 
shows the sample space of permutations e, e2,..., ee and the number 
of correct answers in each. Table 5-7 organizes the data in a form that 
shows the probability function of the random variable X (---- "number 
of correct answers"). 
5--1] PROBiBILITY FUNCTIONS OF RkNDOM VARIABLES 161 
TABLE 5--6. {ATCHING DATES WITH HISTORICAL EVENTS. 
Permutation Probabihty Numbcr of 
�, of e, correct answers, 
1 3 
1 1 
�2  
i 1 
�3  
1 0 
�4  
1 0 
�5  
1 1 
�6  
TABLE 5--7. ])ROBABILITY FUNCTION. 
Probabfiity, /(x)    
Nmnber of correct 
answers, x 0 1 3 
3 3 
6-  
2 2 
I I 
I I 
I_ I I I 
0   
Fro. 5-2. Graph of probabfiity function for matching example. 
REMARlZ. A random variable is often defined to be a function that 
assigns a real number to each sample point. Table 5-6 illustrates this 
idea; the random variable X may be thought of as a function defined 
on the domain 
{el, e2, e3, e4, es, e6}, 
with the values (see the third column of Table 5-6)' 
X(e) = 3, X(e2) = 1, X(ea) = 1, 
X(e4) = 0, X(es) -- 0, X(e6) = 1. 
From this point of view, the random variable X is the set whose elements 
are the six ordered pairs 
(el, 3), (e2, 1), (ca, 1), (e4, 0), (es, 0), (e6, 1). 
1(2 ADOM V^mSBLES [C^. 5 
The values of X are the possible numbers of correct answers: 3, 1, 0. 
Given a sample point representing the outcome of the experiment, the 
value of X is determined. Given only the sample space, we know the 
possible values of X and their probabilities, the probability function of 
Table 5-7. 
or our present purposes, it is sufficient to consider a random variable 
as a variable whose value is a number determined by the outcome of 
an experiment. 
Idea of a run. If you toss a coin 9 times and it comes up 
THHHHHHH T, 
in that order, you may wonder if something in the construction of the 
coin, or in the way it was tossed, caused so few runs (only 3 in this example). 
Any unbroken secluence of like letters is called a run, even though the 
secluence has only 1 letter (length 1), as at the beginning and end of the 
foregoing example. The middle run of H's has length 7. Some statistical 
tests for randomness are based on runs. For instance, in the next example 
a large number of runs might suggest that people visiting the soda fountain 
prefer not to sit side by side. 
ExsME 5. Runs of two kznds of elements. A small soda fountain 
has 5 seats in a row, 3 of which are occupied. Assuming that all seating 
arrangements of 3 persons are eclually likely, find the probability function 
of the number of runs of occupied seats (0) and empty seats (E). (Example: 
0 E E 0 0 has three runs, as indicated by the underlining.) 
Solution. The experiment is to have 3 people come into the soda foun- 
tain, when all 5 seats are empty, and sit down. A sample space for this 
experiment is a list of all possible arrangements of three O's and two 
E's, corresponding to the three occupied seats and two empty seats. 
The number of sample points is 
since that is the number of permutations of 5 things, of which three are 
O's and two are �'s. If people choose seats at random, each sample point 
has probability  
The random variable of interest to us in this experiment is the number 
of runs of O's and E's in the sample point that represents the seating 
arrangement. A list of sample points together with the number of runs 
ill each is given below. We are concerned only with "occupied" or "empty," 
not with the different persons seated. Thus the seating arrangement 
5--1] PROBABILITY FUNCTIONS OF RANDOM VARIABLES 163 
designated 0 0 0 EE means that seats numbered 1 through 3 are oc- 
cupied, seats 4 and 5 are empty. 
Seating Rms Seating Runs 
OOOEE 2 OEEO0 3 
OOEOE 4 EOOOE 3 
OOEEO 3 EOOEO 4 
OEOOE 4 EOEO0 4 
OEOEO 5 EEOO0 2 
Counting in this list the number of ways to get each possible number of 
runs, we obtain the probability function of the random variable X, which 
denotes the number of runs (see Table 5-8). 
TABLE 5--S. THE PROBABILITY FUNCTION OF THE 
NUMBER OF RUNS OF FIVE ELEMENTS, THREE OF ONE KIND AND 
TWO OF ANOTHER. 
Pobability, f(x) 0.2 0.3 0.4 0.1 
No. of runs, x -- 3 4 5 
Idea of turning points. The next example deals with a topic used in 
studying economic time series, such as daily stock market averages or 
weekly production of automobiles. A time series is a set of observations 
or measurements arranged in the order in which they were made. If 
there is no trend, these measurements should fluctuate about a mean 
value, some above and some below. If they continually increase or de- 
crease or follow some cyclical pattern, it may be possible to predict the 
future behavior of the series. 
If among three successive numerical measurements the middle one 
is the least or the greatest of the three, it is called a turning point of the 
sequence. Thus in the sequence 
3, 5, 4, 7 
the numbers 5 and 4 are turning points because 5 is the greatest of 3, 
5, 4 and 4 is the least of 5, 4, 7. In random fluctuations there are more 
likely to be many turning points in the successive measurements than 
there would be if the measurements were in general increasing or de- 
creasing. 
ExAMrLE 6. Turning points. If all permutations of four different meas- 
urements are equally likely, what is the probability function of the random 
variable X, where X is the number of turning points? 
164 RANDOM VARIABLES [CHAr. 5 
Sohdion. For the purpose of counting the number of turning points, 
there is no loss of generality if we replace the measurements, in order 
of increasing magnitude, by the numbers 1, 2, 3, and 4. Then any sequence 
of four different measurements provides some permutation of the four 
numbers 1, 2, 3, 4. Thus 1423 indicates that the smallest measurement 
is first, the largest second, and so on. And this permutation has two 
turning points: 4 and 2. We list the 4! permutations, together with the 
numbers of turning points, in Table 5-9. 
TABLE 5--9 
I)ERMUTATIONS OF 1 2 3 4 AND NUMBERS OF TURNING POINTS. 
Pcrmu- Turning Pcrmu- Turning Permu- Turning Permu- Turning 
tation points tation points tation points tation points 
1234 0 2134 1 3124 1 4123 1 
1243 1 2143 2 3142 2 4132 2 
1324 2 2314 2 3214 I 4213 1 
1342 1 2341 1 3241 2 4231 2 
1423 2 2413 2 3412 2 4312 1 
1432 1 2431 1 3421 1 4321 0 
Counting up the freluency of each number of turning points, we get 
the probability function shown in Table 5-10. 
Thus we get no turning points only 1-% of the time, one turning point 
about half the time: and two turning points nearly half the time. 
The foregoing examples illustrate the steps in constructing the proba- 
bility function of a random variable. 
1. Construct for the given experiment the sample space of 
possible outcomes, along with their associated probabilities. 
2. List the value of the random variable that corresponds 
to each sample point. 
3. List the possible values x, x2, ..., xt of the random 
variable, and list the associated probabilities f(Xl), f(x2), � . . , 
f(xt). (Compute the probability of xi by adding together the 
probabilities of all sample points that correspond to xi.) 
Then the sct of ordered pairs 
i = 1, 2,..., t 
is the probability function of the random variable. The proba- 
bility function is usually displayed either as a table, like 
Tables 5-2, 5-5, 5-7, 5-8, 5-10, or as a formula for f(x). 
5--1] PROBABILITY FUNCTIONS OF RANDOM VARIABLES 165 
TABLE 5-10 
Probability, f(x) 2    o 
Numbcr of 
tm'ning points, x 0 1 2 
Firther conmet on notation. The expression P(X ---- x) denotes the 
probability that the random variable X takes the value x. Usually we 
shall introduce the probability function and write f(xi) for the probability 
P(X----xi). Sometimes it is convenient to abbreviate P(X----xi) to 
P(x) when no confusion would develop. If we need to talk about more 
than one random variable, we may introduce such other letters as Y or 
Z, having respective values y or z and probability functions g or h. Thus 
we might have f(x) ---- P(X ---- x), g(y) - P(Y ---- Yi), and h(z) ---- 
P(Z = 
EXERCISES FOR SECTION 5-1 
1. An ordinary sx-sidcd die is thrown once. Find the probability function of 
the number of dots appealing on the top face. Graph the probability function. 
2. Throw an ordinary die 50 times and record the numbers of times it falls 
with 1, 2,..., 6 dots up. (Or use your random numbers, Table I, to simulate 
this experiment.) Dividc thcse numbers by 50 to convcrt them to relative 
frequencies and plot the graph of (x, r(x)), where x -- l, 2, .... 6 and r(x) equals 
the observed relative fequcncy. Compare with thc graph of the probabihty 
function in Exercise I above. 
3. Suppose a number is sclcctcd at random from the integers 1 through 20. 
Let x be the number of ts divisors. Construct the probability function of X and 
graph it. What is the probability that there will bc 4 or more divisors? 
4. A coin is tossed 3 times. Lct X be the number of runs in the sequence of 
outcomes: first toss, sccond toss, third toss. Find the probability function of X 
and construct its graph. What values of X are most probable? 
5. Do a "turning points" example for the case of 3 measurements. What num- 
ber of turning points has the greatest probability? Lcast? 
6. Two ordinary six-sided dice are thrown (see Table 3-3). Find the prob- 
ability function for the total score on their top faces and graph it. Do you think 
the points on the graph lic on any smplc crave, or curves? Discuss. 
7. (Continuation.) A white die and a red die are thrown at the same timc and 
the difference R -- IV is obscrvcd, where R is thc number on top of the red die 
and IV that for the white. Find the probability function of this difference and 
sketch its graph. What values of R -- IV are most probable? Least? Compare 
this probability function and its graph with thosc obtained in Exercise 6 above. 
Comment. 
8. The three-sided engineer's ruler of Example 2 is thrown 3 times. Find the 
probability function of the sum of the 3 face-down digits. Sketch and discuss 
166 RANDOM VARIABLES [CHAP. 5 
ts graph. [Hint. Let X bc the sum on the first two throws, Y the score on the 
third throw, and eonsidcr pairs of values of X and Y as constituting the sample 
space; then form the sum X q- Y. Use the probability function of X given in 
Example 2.] 
9. Usc the table of random numbers at the back of the book and record a 
scquence of 25 numbers. The rcsults correspond to random sampling, with re~ 
plaecment, from the digits 0, 1,..., 9. However, interpret "0" as "10," and 
beside each numbcr record the number of its divisors. Compute the relative 
frcquencies of thc various numbers of divisors observed, and plot the graph of 
relative fiequeneies against number of divisors. Compare with the probability 
function of Example 3 and its graph. 
10. Throw an ordinary die. Record, in order, the number on top, then the 
number on the side nearest you (the "front"), next the one on the bottom, and 
finally the one on thc side farthest from you. Repeat this operation 10 tmes, 
eaeh time recording a sequence of 4 numbers according to the method deseribed. 
Next, eomputc the number of turning points in each sequence and plot their 
rclativc frequencies on a graph. Compare with the graph of the probability 
function of tinning points in Example 6 and eomment. 
11. Graph the probability function of the number of runs in Example 5. 
12. Simulate the seating arrangement experiment of Example 5 as follows: 
shuffle 3 red eards and 2 black eards from a bridge deck, then deal them out one 
at a time. Record R for red and B for black. Regard reds as "occupied" and 
blacks as "empty." Repeat thc operahon 25 times, shuffling the eards well before 
cach deal. Compute the number of runs obtained in each sequence of 5, and the 
relative frcquencies of various numbers of runs obtained in the 25 sequences. 
Plot, and compare with the graph of the probability function of Example 5. 
13. Supposc 4 coins are tossed. If � is the number of tails, find the probability 
function of � and graph it. 
14. Repeat Exercise 13 when X  is the number of heads minus the number 
of tails. 
15. From a lot of 10 TV sets containing 4 defeetives, a sample of 3 sets is 
drawn at random without replacement. Let � be the number of defeetives in 
the sample. (a) Describe a sample space for this experiment. (b) How many 
points are there in your sample spaee? (c) Tabulate the probability function 
of �. (d) Graph this probability function. 
16. Four pcople take counts of the number of students in a lecture room, 
and thc esults are 51, 52, 52, 53. If all permutations of these counts are equally 
likely, what is thc probability function of the random variable �, where � is 
the numbcr of turning points? 
17. Given the following probability function: 
f(x) 0 c 2c 2c c 2 2c 2 7c 2 q- c 
(a) Find c. (b) Evaluate P(X >_ 5) and P(X < 3). (e) If P(X_< k) > �, 
find the minimum value of k. 
5-2] POPULATION MEAN 167 
5-2. MATHEMATICAL EXPECTATION OF A RANDOM VARIABLE: 
POPULATION MEAN 
In this section, we introduce the concept of the mca valxe of a random 
variable. It is closely related to the notion of the arithmetic mean, or 
average. 
EXAMPLE 1. A bowl contains 300 tags; 150 are numbered 1, 100 are 
numbered 2, and 50 are numbered 3. A tag is drawn at random from the 
bowl, its number X is recorded, the tag is returned to the bowl, and the 
tags are thoroughly mixed. This process is repeated 500 times. What is 
the arithmetic average of the values of the random variable X that are 
thus recorded? 
Solution. Let n of the tags that were drawn have the number 1, n2 
have 2, and as have 3. Then the arithmetic average 7 (read "x bar") is 
1X n q-2X nq-3X ns 
.7 = (1) 
n + n2 + n 
The numerator of this expression is a "weighted sum" of l's, 2's, and 
3's, each "weighted" by a factor nl, n2, or ns that is equal to the number 
of times the given number is drawn. This average (1) can also be expressed 
in terms of the proportio,s n/n, n2/n, n/n, with 
n = 500 =  q- 2 q- ha, 
to yield 
.7---- 1 X n--!q-2x n--2q-3 xn--s' (2) 
The expression (2) exhibits the average as another weighted sum of 
l's, 2's, and 3's; here the three numbers are weighted by their relative 
proportions. Of course, ve don't know the exact values of these propor- 
tions (unless we actually perform the experiment), so we can't say in ad- 
vance just what the average produced by this particular experiment is. 
But since the probability of drawing a 1 is f(1) ---- �, of drawing a 2 is 
f(2) ---- �, and of drawing a 3 is f(3) = , we might suppose that the 
proportions n/n, n2/n, and na/n are approximately equal to f(1), f(2), 
and f(3), in that order. Thus, for a value of n as large as 500, we might 
expect an average near 
I 5 
1 X f(1) q- 2 X f(2) -1-3 X f(3) = 1 X �q-2 X �q-3 X 6 -- a. 
168 RXN)OM VXRXLS , [CXP. 5 
The arithmetic mean of all the tags in the bowl is also 
1 X 150 q-2X 100q-3X 50 500 5 
150 + 100 + 50 300 
For a small sample of tags, we would not expect the sample average 
necessarily to be near this population mean, but for a large sample most 
people do expect it, and it usually is near. 
EXAMP, 2. The number of divisors of an integer from 1 through 10, 
chosen at random, is a random variable X. What is its expected value? 
Solution. Table 5-5 provides the probability function for the number 
of divisors X. We use this probability function to compute the "popula- 
tion mean" in the way indicated in Example 1 above. The result is 
1 X 0.1 q-2 X 0.4q- 3 X 0.2 q- 4 X 0..3 = 2.7. 
This is the average result that we might expect from a large number of 
performances of the experiment. Of course, no number has 2.7 divisors; 
moreover, on just one performance of the experiment, the most likely 
number of divisors is 2, since that has the highest probability. 
The foregoing examples lead us to the following definitions. 
5-2 Definition. Sample average. Let X be a random variable whose 
possible values are x, x2, ..., xt. Suppose that a sample of n 
observations produces nl values of X that are equal to x, n2 that 
are equal to x2, . .., nt that are equal to 
Frequencies n n2 ... nt Total' n 
Values of X  ... 
Then the average value of X for this sample is 
 = xn q- xan2 q- ''' q- xtnt  xn 
nl q- n2 q- ''' q- nt  ni 
or 
= - (3) 
n 
(See Appendix II for a discussion of the summation symbol, .) 
5-2] POPULXTION MEXN 169 
The set of ordered pairs (.r, rig), i ----- 1, 2, ..., t, displayed as a table 
in Definition 5-2 is called the frequency distributzon of the sample values. 
In dealing with samples, the frequency distribution plays an important 
role, just as the probability function does in dealing with populations. 
5-3 Definition. Mathematical expectation: population mean. Let X be 
a random variable with probability function as follows: 
Pobability, f(x) f(x) f(x) ... f(x) 
Value of X, x Xl X2 � � � Xt 
The mathematical expectation of X, denoted by E(X), is defined 
to be 
E(X) ---- xlf(Xl) -- x2f(x2) - ''' - xtf(xt), 
or 
t 
E(X) =  xf(xO. (4) 
E(X) is also called the mean of X, or the population mean. 
REMARK. The mean is also abbreviated/ (read "mew" and spelled "mu"), 
the Greek letter for "m," the first letter of the word "mean." Sometimes 
several random variables X, Y, ... are being studied together. We may 
use these letters as subscripts on/ to indicate the means. Thus we would 
write 
x = E(X) and /z ---- E(Y). 
When only one random variable is being considered, the subscript is usually 
omitted. 
We may express the result of Eq. (4) in words: 
To compute the mean of a random variable, multiply each pos- 
sible value of the variable by its probability and add these products. 
Equations (3) and (4) are not identical, but they are similar. In par- 
ticular, the proportions ni/n in Eq. (3) vary from one sample to another, 
and it is only a coincidence if ni/n is equal to the probability f(xO. How- 
ever, it is true that 
n 
170 RANDOII VARIABLES [CHAP. 5 
and therefore 
These approximations are usually better when n is large, and, of course, 
become equalities if the sample coincides with the entire population. 
EXAMPLE 3. One die is thrown. What is the mathematical expectation 
of the number of dots on the top face? 
Solution. Let the random variable X denote the number of dots on the 
top face of the die. The possible values are 1, 2, . . . , 6, each with proba- 
bility 3. Hence, by Eq. (4), 
= 
-- 1X.q-2X{.q-3X{-q-4X{-q-5X{-q-6X x 2x-- 3.5. 
EXAMPLE 4. What is the mathematical expectation of the number 
of runs when 3 things of one kind and 2 things of another kind are arranged 
at random in a row? 
Solution. Here the random variable X is the number of runs whose 
probability function is given in Table 5-8. Using it, we compute 
t = E(X)  2 X 0.2 q-3 X 0.3 q-4 X 0.4 q-5 X 0.1 = 3.4. 
IEMARK. Note that in this example, again, the mathematical expecta- 
tion of the number of runs is 3.4, not an integer, and not any value that 
the random variable could actually have. The same is true of the mathe- 
matical expectation of the number of dots on the top face of the die in 
Example 3 and of the number of divisors in Example 2. We mention this 
because the term "mathematical expectation" is often abbreviated "ex- 
pectation." The examples show tha this "expectation" is not something 
we "expect" in the ordinary sense of the word, except that the long-run average 
over repeated experiments is likely to be close to it. Again, the term "ex- 
pected value" is sometimes used as a synonym for "mathematical expecta- 
tion," but there should be no implication that this value is frequent, 
highly probable, or even possible. It is merely the weighted mean of the 
possible valucs, each weighted by its probability. 
EXMr,E 5. According to an American experience mortality table, 
the probability that a 25-year-old man will survive one year is 0.992, and 
that hc will die within a year is 0.008. An insurance company offers to 
sell such a man a $1000 one-year term life insurance policy for a premium 
of $10. What is the company's expected gain? 
5-2] POPULXTON MFaN 171 
Solution The "gain," X, is a random variable that may take the value 
+$10 (if the man lives) or --$990 (if he dies). The probability function 
is as follows: 
f(:r) 0.992 0.008 
x -q-10 --990 
and 
/ ---- E(X) = 10 X 0.992 -- 990 X 0.008: 2. 
It is important that the expected gain (before administrative expenses 
and taxes) be positive in order to enable the insurance company to stay 
in business and to build up reserves to pay its beneficiaries and policy- 
holders. 
EXAMPLE 6. One-armed bandit. A simplified slot machine has 2 dials. 
Each dial has 3 kinds of pictures on it, identified as "apples," "bells," and 
"cherries." The machine is rigged so that the 2 dials operate independ- 
ently, and after they are spun, each comes to rest with 1 of the 3 pictures 
showing in a window on the front of the machine. The probabilities of 
the possible outcomes, for each dial, are 
Outcomes Bells Cherries Apples 
Probabilities 0.4 0.5 0.1 
Each play costs five cents. A play consists of pulling a lever that spins the 
dials, resulting in one of the 9 possible combinations of 2 pictures, 1 on 
each dial. The machine pays off as follows: 
for 2 apples, 50 for 2 cherries, 5 
for 2 bells, 10t for anything else it pays nothing 
Find the mathematical expectation of net profit (in money) to a person 
who plays once. 
Solution. The random variable X here equals the number of cents won. 
Table 5-11 shows a sample space of possible outcomes, their probabilities, 
and the corresponding profit in cents. The three entries in the upper 
left corner, 
(a, a): 45 
0.01 
mean that the outcome "two apples" has an associated profit of 45 cents, 
and occurs with probability 0.01. 
172 RANDOM VARIABLES [CHAP. 5 
TABLE 5--11. ONE-ARMED BANDIT. 
Second dial 
Apples 0.1 Bells 0.4 Cherries 0.5 
Apples (a, a): 45 (a, b): --5 (a, c): --5 
0.1 0.01 0.04 0.05 
First Bells (b, a): --5 (b, b): 5 (b, c): --5 
dial 0.4 0.04 0.16 0.20 
Cherries (c, a): --5 (c, b): --5 (c, c): 0 
0.5 0.05 0.20 0.25 
The expected value of the random variable X, the profit on one play, is 
tx ---- E(X) = 45 X 0.01 q- 5 X 0.16 q- 0 X 0.25 -- 5 X 0.58 
---- .45 q- .80 -- 2.90 ---- --1.65 (cents). 
In ten plays, the expected loss is 16.5 cents; in 100 plays, $1.65. 
EXERCISES FOR SECTION 5-2 
1. In Example 2, Section 5-1, what is the expected sum of the 2 face-down 
digits in the experiment with the engineer's ruler? 
2. In Example 4, Section 5-1, what is the expected number of correct answers? 
3. In Example 5, Section 5-1, what is the expected number of runs? 
4. From a bag of 7 marbles, 5 red and 2 blue, 3 marbles are drawn at random 
without replacement. Check that the expected number of blue marbles is 3 X . 
5. For Exercise 17, Section 5-1, compute the expected value of X. 
6. For Exercise 15, Section 5-1, find the expected number of defective TV 
sets. 
7. Refer to Example 6, Section 5-2: one-armed bandit. Find the expected 
number of cents plofit for one play on a two-independent-dial slot machine, given 
the following data for one dial: 
Outcomes Bells Cherries Apples 
Pobabilities 0.3 0.6 0.1 
Payoffs: for 2 apples, 25 for 2 cherries, 5 
for 2 bells, 10t for anything else, zero 
5-2] POPULATO '' ME,a- ' 173 
8. Problem of points. To decide who wins a $4 prize, A and B play the follow- 
ing game. A coin is tossed. If the coin falls heads A gets a point; if it falls tails, 
B gets a point. The first person to get three points wins. After 3 tosses, A has 
2 points and B has 1. Make a sample space for the rest of the game. Let X be 
A's winnings. What is the expected value of A's winnings when he has 2 points 
and B has 17 
9. In a lottery, 100 tickets arc sold at 25 cents cach. There are 4 cash prizes, 
worth $10, $3, $2, and $1, respcetively. What is the expected nct gain for a 
purchaser of two tickets? 
10. Four identical light bulbs are temporarily rcmoved from their sockets 
and placed in a box. The bulbs are then taken at random from the box and put 
back in the sockets. What is the expected number of bulbs that will be rcplaeed 
in their original sockets? 
11. Calculate the expected number of "heads" when n coins are tossed together 
if (a) n = 1, (b) n -- 2, (c) n = 3, (d) n = 4. What do you predict for the 
answer for an arbitrary valuc of n v Can you pmvc it? 
12. Find the expected value of the sum of the numbers of dots on the top faces 
of two ordinary cubreal dice, on one throw. 
13. Roulette. A roulette wheel has 38 equally spaced openings numbered 00, 
0, 1, 2, 3, ..., 35, 36. A gambler may bet $1 on any number. The croupier 
spins the roulette wheel and drops a small ball onto it while it is spinning. If 
the ball comes to rest on the number the gambler has bet on, he receives $35 in 
addition to his bet of $1, but otherwise he loses his $1. Find the mathematical 
expectation of his gain. 
14. Find the expected number of turning points in a scries of 4 different meas- 
urements. (See Table 5-9.) 
15. The number of accidents that occur at a particular intersection between 
4:30 and 6:30 p.m. on Fridays is 0, 1, 2, or 3, with corresponding probabilities 
0.94, 0.03, 0.02, 0.01. Find the expected number of accidents during the period 
in question. During 100 such periods. 
16. Player A pays B $1, and 3 unbiased dice are rolled fairly. A receives $2 
from B if 1 ace appears, $4 if 2 aces appear, and $8 if 3 aces appear; otherwise 
he gets nothing. Is this a fair game? (That is, do A and B have the same ex- 
pectation of gain?) If not, how much should A receive from B when 3 aces 
appear, to make the game fair? 
17. In the World Series, suppose one team is stronger than the other and has 
probability ] of winrang each game, independent of the outcomes of any other 
games. Under these assumptions, it is possible to show that the probabilities 
that the series ends in 4, 5, 6 or 7 games respectively are about .21, .30, .27, or 
.22. Find the expected number of games in the series, undcr these assumptions. 
18. The probability that a man aged 50 will live another year is 0.988. How 
large a premium should the insurance company charge him for a $1000 term 
life insurance policy for one year (not including insurance company charges for 
administration, profit, etc.)? 
19. In one play of the game called "chuck-a-luck," the player wins an amount 
15, 10, 5, or --5 cents (--5 means he loses 5 cents), with probabilities 
s ,s and 2s respectively. (Cf. Wallis and Roberts, Statistics, a new ap- 
16' 216' 2'-' 
174 RANDOM VARIABLES [CHAP. 5 
proach, The Free Press, 1956, p. 332.) Find the mathematical expectation of 
the player's gain (a) on one play of thc game, (b) on 100 plays. 
20. A sample of 4 balls is drawn without replacement from an urn containing 
3 red and 5 white balls. If the sample contains 2 or more red balls, the player 
receives one dollar; otherwise he loses fifty cents. What is the mathematical 
expectation of his gain? (First set up an appropriate sample space for the ex- 
periment.) 
21. A fail' coin is tossed until the first time a tail comes up or until three 
heads occur. Write out a sample space for this experiment and assign prob- 
abilities to its elements. Find the expected number of tosses in one performance 
of the experiment. 
22. A farmel' estimates that during the coming year his hens will produce 
10,000 dozen eggs. lie further estimates that, after taking into account his 
various costs and the seasonal price fluctuations, he may gain as much as 6 cents 
per dozen, or lose as much as 2 cents per dozen, and that the probabilities as- 
sociated with these possibilities are as follows: 
Gain (in cents per doz.) 6 4 2 0-06 --2 
0-T5- -- 
Probability 0.20 0.04 
What does he estimate as his expected gain (a) in cents per dozen, and (b) on the 
10,000 dozen? 
23. The possible values of a random variable � are the integers from n through 
n  m. If these possibilities are equally likely, find E(,). 
24. The random variable X has values 0 and n with probabilities (n -- 1)/n 
and l/n, in that order. Find E(�). Describe the graph of the probability func- 
tion of X (a) forn = 5, (b) forn = 20, (c) for n = 1000, (d) for n  "infin- 
ity." What is the limit of E(�) as n  "infinity"? 
25. The possible values of a random variable X are the integers 1, 2, 3, . .., 
n and P(X = x) = cx for some constant c. (The probability function has a 
triangular shaped graph.) Show that c = 2/n(n  1). Find E(�). Is E(X)  
n when n is large? Discuss and intexpxet the esult. 
5-3. MEAN OF A FUNCTION OF A RANDOM VARIABLE 
Suppose that X is a random variable, a variable xvhose value is a number 
determined by the outcome of an experiment. If the value of X is increased 
by 5, the result is again a number determined by the outcome of that 
experiment: a number that is a value of the new random variable Y q- 5. 
Or, if the value of X is squared, the result is a value of the random variable 
X 2. In this section, xve study random variables that are related to X: 
variables such as aX, X q- c, a" q- c, X '2, and (X -- c) 2, xvhere a and c 
are constants. Each of these random variables has a probability function, 
xvhich we can get from the probability function of X, and each has a 
mean. In the next example, we show how these means are computed 
5-3] MEAN OF A FUNCTION OF A RANDOM VARIABLE 175 
directly from the probability function of X without going through the 
intermediate step of finding the probability function of the related random 
variable. 
EXAm'LE 1. The random variable X has probability function as 
follows' 
Probability, f(x) 0. 0.3 0.5 
Values of X, x 0 1 
Compute the following me,ns: E(X), E(2X), E(X q- 1), E(2X q- 1), 
E(X2), and E[(X -- 0.3)2]. 
Solution. (a) E(X) = --1 X 0.2q-0 X 0.3q- 1 X 0.5 = 0.3. Thus, 
the mean of X is 0.3. 
(b) The possible values of 2X, and their probabilities, are as follows' 
Probabilities 0.2 0.3 0.5 
Values of 2X 2 
Note that P(2X = --2) is the same asP(X= --1), and so on. If we 
multiply each possible value of 2X by its probability and add these prod- 
ucts, we get the mean, or expected value, of 2X: 
/X2x ----- E(2X) = --2 X 0.2 q- 0 X 0.3 q- 2 X 0.5 
= 0.6 = 2E(x). 
Doubling every number doubles the mean. 
(c) X+l = �(X + 1) 
= (-+)x0.2+(0+)x0.3+(+)x0.5 
= .3 = E(x) + . 
Obviously, if we add 1 to every number, the mean is increased by 1. 
(d) E(2X+ ) = (--2+ ) X 0.2+ (0+ ) X 0.3 + (2+ ) X 0.5 
= 1.6 = 2E(X) q- 1. 
Doubling every number and adding 1 doubles the mean and adds 1 
to the result. 
(e) .(X 2) = (--1) 2 X 0.2 q- (0) 2 X 0.3 q- (1) 2 X 0.5 
= 0.7  [/(X)] 2. 
Note that the mean of the square is not the square of the mean. 
176 RANDOM VARIABLES [CHAP. 5 
(f) E[(x - 0.32)] = (-1.3) 2 < 0.2 + (-0.3) 2 x 0.3 + (0.7) 2 x 0.5 
= 0.61 = s(xh - [E(x)] 2 
There is something special about 0.3 in this example; it is the mean of X. 
The common feature in all these examples is this: we have computed 
the mean of a function of X by substituting in the formula for the function 
the possible values of X (in these cases --1, 0, 1), multiplying the results 
by the probabilities of these values of X (here 0.2, 0.3, 0.5), and adding 
the products. We formalize this procedure in the following definition. 
5-4 Definition. Mean of a function. Let X be a random variable whose 
probability function is as follows: 
Probability, fix) f(x) f(x2) ... f(xt) 
Values of X, x x x2 . .. x 
Let H be a function of X. Then the mean, or expected value, of 
the new random variable H(X) is given by 
E[H(X)] ---- H(zOf(xO - H(x2)f(x2) -''' - H(xt)f(xt), (1) 
or 
E[H(X)] =  H(xl)f(x,). (2) 
IEMARK. The mean of the nmv random variable Y = H(X) can be 
computed from the probability function of Y by multiplying each possible 
value of Y by its probability and adding these products. In Example 1 (b) 
above, we have illustrated this for Y ---- 2X. The possible values of Y are 
y = 2Xl = 2 X (--1) = --2, 
Y2 = 2x2 = 2 X 0 = 0, 
Y3 = 2x3 = 2 X 1 ---- 2, 
and their probabilities are 
P(Y---- y) ---- P(Y = --2) = P(X---- --1) ---- f(xO, 
P(Y---- Y2) ---- P(Y = O) = P(X---- O) = f(x2), 
P(Y---- Ya) ---- P(Y = 2) = P(X = 1) = f(xa). 
5-3] ME'N OF A FUNCT1OX OF x_ RANDOM VARlkBLE 177 
Hence we find, for that example, 
E(Y) = yP(Y = yl) -- y2P(Y ---- Y2) + YaP(Y = Y3) 
= yf(x) + 7/2f(.c2) + Yaf(xa) 
= 2xf(x) + 2.c2f(x2) + 2.caf(.ra), 
which corresponds to the result given by Eq. (1). Note that we do not 
2.cif(2.rx) + 2.r2.f(2.r2) + 2raf(2xa), 
because the probability that 2X takes the value 2.r is the same as the 
probability that X takes the value x, and this is f(.r), not f(2.r). 
Sometimes two or more values of X yield the same value of the new 
random variable H(X). For example, both X = --1 and X = +1 yield 
the value X 2= +1 in Example l(e) above. More generally, suppose 
that Y = H(X) takes the value y for m distinct values of X, say for 
X = Xl, X2,...,Xm. Then 
y : S(.v) = S(r2) ..... S(x). (3) 
The corresponding contribution to the mean of Y is 
y � P(Y = y). 
But 
P(Y: yx) = f(x) + f(.r2) + ... + f(x), 
so that 
y . P(Y = y) = yf(x) + yf(x2 + '" + yf(x), 
and when we take Eq. (3) into account, we see that 
y � P(Y = yx) = S(x)f(xl) + S(x2)f(x2) + ... + S(x)f(x). (4) 
Similarly, if another set of values of X corresponds to Y2, a third 
to Ya, and so on, we can group the terms on the right side of Eq. (1) into 
terms corresponding to 
YP(Y = Yl) + Y2(Y = Y2 + .... E(Y). 
Thus Eq. (1) allows us to use the probability function of X to get the 
same result that we would get by computing the mean of Y  H(X) 
from the probability function of Y. 
Example 1 has illustrated some results that we now state as theorems, 
since they are true in general. We also provide algebraic proofs. 
178 RANDOM VARIABLES [CHAP. 5 
5-5 Theorem. Let X be a raniom variable. Then 
E(aX 4. b) = aE(X) 4. b, (5) 
for any numerical constants a and b. 
Proof. Suppose the probability function of X is 
i= 
Then, by Definition 5-4, 
E(aX 4. b) ---- (ax 4. b)f(xl) 4. (ax2 4. b)f(x2) 4. ... 4. (axt 4. b)f(xt). 
We expand the right side of this equation, factor out a and b, and get 
E(aX 4. b) = a[xf(x) 4. xf(x) 4. ... 4. xtf(xt)] 
4- b[f(x) 4- 4- ... 4- 
The first of the bracketed expressions is E(X) ---- xif(x), and the 
second is 1, since f(x) = 1. Therefore we have the desired result 
E(aX 4. b) ---- aE(X) 4. b. [] 
5-6 Corollary. Let X be a random variable with mean E(X)  I. 
Then E(X -- I) ---- O. 
Proof. Take a ---- 1, b = --/ in Theorem 5-5' 
E(X-- i) ---- E(X) --  =  --  = O. [] (6) 
REMARr:. The expected value of X -- c is often called the first moment 
of X, taken about c. The reason for this terminology is that E(X -- c) 
is, by Definition 5-4, 
(Xl -- �)f(xl) 4- (x2 -- �)f(x2) _1 .... 4- (Xt -- �)f(xt), (7) 
and this has the following physical interpretation. If we imagine a light 
but rigid bar with weights (in some system of units) equal to f(x) at 
5-3] MEAN OF A FUNCTION OF A RANDOM VARIABLE 179 
x, f(x2) at x2, and so on, f(xt) at xt, then formula (7) represents the sum 
of products of these weights each multiplied by the length of the "lever 
arm" from c to that weight. (See Fig. 5-3.) 
x x 2 c xt 
Fro. 5-3. Moment, about c, of wcghts f(zl), f(z), � � �, f(xt). 
In physics, expression (7) is called the first moment of the system of 
weights about c. If c is at the center of mass of the system the first moment 
is zero, and there is no tendency for the system to rotate about a support 
placed at that point. Thus children on a seesaw can balance by placing 
the support directly under the center of mass. Equation (6) tells us that 
if we place the fulcrum at the mean  = E(X), the first moment about 
 is zero. Conversely, if E(X -- c) = O, then c = E(X); the mean is 
the only point about which the first moment is zero. This is one sense in 
which the mean is used to represent the "location," or the "middle," of the 
domain of the probability function. It isn't always at an equal distance 
from the ends of the domain--just as the point of balance of a seesaw 
isn't always halfway between the two children. If one child is much heavier 
than the other, the support must be nearer that child Likewise, if lots of 
the probability is piled up near one end of the domain of a random vari- 
able, the mean is usually near that end. 
EXERCISES FOR SECTION 5-3 
For Exercises 1 through 4, use the following data: 
Probability, fix) 0.2 0.1 0.3 0___ 0.1 
Values of X, x ' ---- --1 0 2 
1. Compute E(X). 
2. Find the probabihty table for the function 3X- 1 and then compute 
E(3X -- 1). Compare your answer with 3E(X) -- 1. 
3. Find the probability table for the function 2X - 3. Compute E(2X - 3) 
and compare your answer with 2E(X) - 3. 
180 RANDOM VARIABLES [CHAP. 5 
4. Find the probability function for X 2 and compute E(X2). 
5. Find the probability function for X 2 -[- 1, and compute E(X  - 1). 
For Exercises 6 through 9, use the following data: 
Probability, /(x) 0.2 0.3 0.2 0l 0.1 
Values of X, x 2 3 5 
6. Find E(X). 
7. Compute, as easily as possible, (a) E(3X- 7), (b) E(X- 2.7), and 
(c) 
8. Compute E(X2). 
9. Compute E(X- 2.7) 2 and then show that your answer is equal to 
ST. PETERSBURG PARADOX EXERCISES 
Description of play. A player tosses a coin until it falls tails, or until he 
has tossed n times without a tail. Let X be the number of heads in one play 
of the game. 
1. Find the probability function of X for n = 2. 
2. Repeat Exercise 1 for'n = 3. 
3. Repeat Exercise 1 for n = 4. 
4. Repeat Exercise 1 for a general value of n. 
Description of payoff. In the game described above, the number of dollars 
the player receives is the random variable Y = 2 x. 
5. For Exercise 1 find E(Y). 
6. For Exercise 2 find E(Y). 
7. For Exercise 3 find E(Y). 
8. For Exercise 4 find E(Y). [Hint. Recall that p q- p2 _ . . . _ pn-1 is the 
sum of a geometric ptogression.] 
9. Discuss the behavior of E(Y) as n grows large. 
10. Lift the restriction to n trials, and consider the expected payoff when the 
player tosses until he gets a tail. (This is the original St. Petersburg problem, 
and it is satisfactory to say that the expectation is infinite.) 
Fair game. Recall that a game between two persons is said to be fair if the 
expected value to both persons is zero. Our player plays against a bank (or 
gambling house), tossing until he gets a tail, with payoff Y = 2 x dollars, as 
before. 
11. If the game is to be fair, and if the bank has unlimited resources, how much 
should the player pay the bank for one play of the game? 
12. Suppose the bank has only 220 dollars ($1,048,476), xhat should the 
player pay the bank to make it a fair game? 
13. Do Exercise 12 if the bank has 4 X 10 TM dollars, about the size of the 
national debt of the United States of America in 1960. 
5-4] V,RI,BILT� 181 
NOT:. Although it is amusing to see the modest payments required to pla a 
fair game against a bank with astronomical esources, the result in Exercise 11 is 
somewhat shocking. The importance of that result is not its literal interpreta- 
tion, i.e., that no one can pay an infinite amomt. Rather, consideration of this 
and similar problems led people to calize that expected dollar value i not the 
only measure of orth, because  re,an will not invest a 1,qge amount of money 
in an enterprise with an even larger expected value if the probability that he 
gets his money back is tiny. Most of s would not care to risk $10,000 for a 
1/10,000 chance at a tax-flee billion, even though the expected value of the 
proposition is S90,000. Economists introduced the notion of utility to explain 
such behavior. 
5-4. VARIABILITY 
We recall that the probability function of a random variable X tells 
us the possible values that X can have and the probabilities of those 
values. For many practical purposes, it is convenient to have a quick 
summary of the information that the probability function furnishes. The 
mean, or expected value E(X), is one such summary; it tells where the 
center of mass of the probability function is located. Thus the mean number 
of dots on the top face of a die is 3.5, the mean number of divisors of an 
integer from 1 through 10 is 2.7, the mean number of heads when two 
coins are tossed is 1. The mean is usefzd in giving us a quick pictre of 
the long-run a'erage result when an experiment is performed over and over. 
But it tells us nothing about how outcomes spread out from one per- 
formance of the experiment to another. We shall now consider various 
alternative ways of measuring such variability, or spread, and then 
introduce the two most commonly used measures of spread, the standard 
deviation and the variance. (Either of these measures determines the 
other because the variance is the square of the standard deviation.) 
Idea of spread, or variability. To gain some experience with the idea of 
variability, we consider six random variables Xn, X, ..., Xr whose 
probability graphs are shown in order A, B, ..., F in Fig. 5-4. These 
probability functions are symmetrical about the value x ---- 0; their means 
are all equal to zero. We consider various ways of measuring their spreads 
about this common mean. 
The first measure of variability, or spread, that suggests itself is the 
range, defined as follows. Consider those values of X that have proba- 
bilities greater than zero. Then the range of X is the largest of these 
values minus the smallest. In examples A, B, C the range is 2; in examples 
D and E the range is 4; and in example F the range is 6. We might prefer 
a measure of variability that distinguishes among A, B, and C; particu- 
larly so since we wish to measure variability around the mean, and 
has  of its probability concentrated at the mean, while Xc has none. 
182 RANDOM VARIABLES [CHAr. 5 
6 
I 1 
A I 
--1 0 1 
1 I 1 
I I I 
I I I 
I I I 
B --1 0 1 
I 1 
I 
I I 
I I 
I I 
i i i 
C --1 o 1 
1 I 1 I 1 
I I I I 
D I i i i i 
--2 --1 0 1 2 
I 1 
I I 
I I 
I I 
I I 
E I I I I I 
--2 --1 0 1 2 
1 I I 1 
xl 4T l iI 
I I I I 
F I I I I I I I 
--3 --2 --1 0 1 2 3 
Fro. 5-4. Graphs of six probability functions, illustrating variability. 
5-4] VARIABILITY 183 
How would we compare the spread about the mean of the random 
variables XA and XB? First let us consider XA, the outcome on an 
experiment A. In 1200 performances of the experiment, we expect 0 as 
the outcome about 900 times, 71 about 150 times, and --1 about 150 
times. By contrast, experiment B would yield about 400 zeros, .t00 plus 
ones, and 400 minus ones; results that seem to jump around more than the 
results of experiment A. Thus it seems reasonable that any measure of 
variability that is proposed should say that B is more variable than A. 
The comparison between B and C is less obvious. However, since we 
use the mean as a measure of location of the probability distribution, we 
shall measure variability abo,t the mean. Now B gives a result equal to 
the mean, 0, about � of the time, and a result I unit away from the mean 
about  of the time. By contrast, C always gives a result that is 1 unit 
away from the mean. Hence C seems more variable than B, when variabil- 
ity is measured about the mean. 
Clearly, E seems more variable than C, and F more variable than E, but 
D and C are harder to compare. 
Let us try to compare C and D. In C, the outcome is always either 71 
or --1, and hence is I unit away from the mean, 0. In D, on the other 
hand, the outcome is at the mean about � of the time, is 1 unit away 
from the mean about  of the time, and is 2 units away from the mean 
the remaining  of the time. Hence the mathematical expectation of 
these "distances away from the mean" is 
2 __ 6 
ox�+x{+2x-, 
which is slightly greater than the corresponding value for C. Thus, by 
this line of reasoning, D is more variable than C. 
The line of reasoning applied to D in the previous paragraph introduces 
the mathematical expectation of the absolute distance of X from its mean 
as a measure of variability. Thus, applying Eq. (1) of Definition 5-4 
to the function 
H(X) = ]XD -- I[, 
we have 
([xD - ,[) = [- - ol.- + [- - o[. { + [o - o[. { 
+ I - o[.- + [ - o[. � 
= (�) +  (-) + 0(-) +  ({) + (-) = �. 
184 tANDOM VAmSB,ES [CAV. 5 
The mean absolute deviation is defined as follows: 
5-7 Definition. Mean absolute deviation. Let X be a random variable 
with mean 
S(X) = ,. 
Then the mean absolute deviation of X, about/, is the expected 
value of 
mean absolute deviation of X ---- E([X -- /x[). (1) 
In the third column of Table 5-12 we exhibit the mean absolute devia- 
tions for the examples shown in Fig. 5-4. 
Although the mean absolute deviation gives a sensible measure of 
variability, it is not mathematically tractable The absolute values 
are hard to combine algebraically, so the first thing that suggests 
itself is to remove them. But the ordinary mean deviation is zero, by 
Corollary 5-6. 
The big advantage of the absolute values of the deviations is that they 
all count in the same direction; since none is negative, they can't cancel 
each other. Another function that has this useful feature is the squared 
deviation, (X- )2. And this turns out to be much more tractable 
mathematically. As we become acquainted with properties of the variance, 
which uses the squares of deviations from the mean to measure variability, 
we shall see that there are two fundamental reasons for using it rather 
than some other measure: 
(1) Additivity. The variance of the sum of two independent random 
variables is the sum of their variances, and even when the two variables 
are dependent the variability of their sum has a simple formula. 
(2) Central limit theorem. The limiting behavior of a random variable 
that is the sum of a large number of independent random variables de- 
pends upon the variances of these random variables. 
Of course, it isn't just the biggest squared deviation that counts, but 
rather the weighted mean of all the squared deviations, each weighted 
according to its probability. Statisticians call this mean squared deviation, 
E[(X -- /)2], the "variance," and sometimes denote it by Var (X). 
For example D, the computation of the variance goes as follows, since 
5--4] VARIABILITY 185 
1 1 1 1 1 
Probability, f(x) 
Values of X, x --2 --1 0 1 2 
Values of X--it, x-- 0 --2 --1 0 1 2 
Values of (X -- t) 2, x 2 4 1 0 1 4 
Values of x2f(x) 4 
Vat (X) 
Hence, for D, the variance is 2. A similar computation for distribution 
C, which you are asked to perform in Exercise 2, shows that its variance is 1. 
5-8 Definition. lrzance. Let X be a random variable xvith mean 
E(X) = /. The variance of X, denoted by Vat (X), is defined by 
t 
Wr (x) = [(x- )] =  (.- )f(,). (2) 
In words, the variance of X is the mean squared deviation of X from 
its mean 
One final adjustment is necessary to get from the variance of X to a 
measure of variability expressed in the original X-units. The units of 
Var (X) are squares of the units of X, so we recover the original units by 
taking the positive square root of the variance. The number so obtained is 
called the standard dev(ation of X. The standard deviation of X is denoted 
by rrx (read: "sigma sub-X"), or by the small Greek letter rr (read: "sigma") 
without a subscript, if it is clear from the context what the random 
variable is. 
5-9 Definition. ,Standard deviation. Let X be a random variable with 
mean it. The standard deviation of X is the positive square root of 
the variance, and is given by 
vx-- x/Vat (x) = x/E[(x- )]. (3) 
REMARK. Obviously, the variance of X is the square of the standard 
deviation: 
Var (X) ---- cr.-. (4:) 
186 RANDOM VARIABLES [CHAP. 5 
TABLE 5--12� MEASURES OF VARIABILITY FOR THE 
EXAMPLES OF FIG. 5--4. 
Mean 
Example Plobability function absolute Variance Standard 
deviation deviation 
�  6    0.500 
A f(x)  s s   
x :--1 0 1 
  . 2 0.816 
B f(x): a 
x :--1 0 1 
x  1 1.000 
C f(z): 
x :--1 1 
. I I I I  2 1.414 
x :--2 1 0 1 2 
 z 2 2.000 
E f(x):  2 
x :--2 2 
x  i i s 2.550 
F f(x):  i i   
x � --3 --2 2 3 
EXAMPLE 1. The fifth column of Table 5-12 shows the standard devi- 
ations of the examples A through F of Fig. 5-4. Note that both the mean 
absolute deviation in column 3 and the standard deviation in column 
5 assign measures of variability that increase as we read down the table. 
EXAMPLE 2. If X represents the number of heads that appear when 
one coin is tossed and Y the number of heads that appear when two coins 
are tossed, compare the variances of the random variables X and Y. The 
probability functions of X and Y are: 
Probability, /(x) . � z 
Values of X, x 0 1 
Probability, f(y) - � - 
Values of Y, y 1 2 
5-4] VXRLBILITY 187 
Solution. We first compute the means: 
I __ 1 
x = E(X) = O X �+1X  -- , 
r=5'(D=ox�+x�+2x�=. 
Then the variances are 
Var (X) = cr.. = E[(X -- x)l 
I 1 
---- (0 -- �)X �-[- (1 -- �)X  = -[- b---- k, 
Var (Y) = o-- = 5'[(Y - r) ] 
= (0 - )x  + ( - )x � + ( - )x � 
__ 1 1 = 
-+= 
The variance of the number of heads for two coins is double the variance 
of the number of heads for one coin. 
EXAMPLE 3 A single 6-sided die is tossed. Find the mean and variance 
of the number of dots on the top face. 
Solutzon. Let X represent the number of dots on the top face. The 
probability function of X is: 
I 1 1  1 1 1 
Probability, f(x)   g   
Values, x I 2 4 5 6 
The mean, as we have found before, is 
The variance is 
2 E[(X x) 2] 
= ( - ) x  + (2 - ) x  + (s - ) x  
1 35 
+ (4 - ) x  + (5 - {) x  + (6 - ) x  = lW. 
188 RANDOM VARIABLES [CHAP. 5 
We shall soon prove a general formula that is usually simpler for com- 
puting the variances. The formula is 
2 E(X 2) - [E(X)] 2. (5) 
O-X'  
We check that Eq. (5) gives the correct result for the variance of the score 
on the die: 
E(X 2) = 12x -{-22x -{-32x 3-{-42x-{-52x -{-62X 6-- 
[E(X)]2 (�)2 
so that 
E(X)2 _ [E(X)]2 _ 91 49 182 -- 147 35 
6 4 12 12 ' 
The result agrees with our previous calculation of the variance. 
Equation (5) says that the variance of X is the mean of the square of X 
minus the square of the mean of X. We state this important result as a 
theorem, give a proof for.any random variable that takes only three distinct 
values, and then indicate the proof in general. 
$-10 Theorem. Variance. Let X be a random variable with mean 
E(X) = / and variance Var (X) = o-2. Then 
o '2 = E(X 2) -- [E(X)] 2 = E(X 2) -- t 2. (6) 
Proof. Suppose the probability function of X is as follows: 
Probability, f(x) . f(x) f(x2) ... f(xt) 
Values of X, x Xl x2 �.. xt 
We temporarily assume that the number of values of X is t = 3 The 
proof for smaller or larger values of t is similar. 
By definition, 
Var (X) = o-2 = E[(X -- 
: (x -- /)2f(xt) + (x2 -- )2f(x2) + (xa -- )2f(xa). (7) 
5--4] VARIABILITY 189 
We expand the squares, and get 
(X! -- /)2f(Xl) ---- Xl2f(Xl) -- 21Xlf(Xl) - 2f(x), 
Summing both sides of these equations, and collecting terms on the right 
according to the powers of y, we get 
 T2 2 
E(Zi- .)f() [f(l) +- f() + f()] 
-- 2,[xlf(Xl) + xf(x) + 
+ ,2[f(xl) + f(x2) + f(x3)] 
2 2 
= xf(xl) -- 2.xf(x) +. f(xi). (8) 
By definition of mathematical expectation, we have 
2 
xf(x) = E(X), (9a) 
xf(x) = E(X), (9b) 
and, since the sum of the probabilities is l, 
Ef() = . (9c) 
If we introduce the right-hand terms from Eqs. (9a, b, c) into the right- 
hand side of the last line of Eq. (8), and recall that (X) = , we get 
E(- .)f() = (x ) - .(x) + . 
= E(x) - 2E(x). E(x) + [E(x)] 
= E(x) - [E(x)] = E(x) - .. (x0) 
If t = 3, the index i in the sums in Eqs. (8), (9), (10) goes from 1 to 3; 
more generally, it goes from 1 to t. Since, by definition, the left-hand side 
of Eq. (10) is E[(X -- )] = , the proof of the theorem is thus com- 
pleted.  
EMPLE 4. Find the mean and variance of the number of divisors X 
in an integer from 1 through 10 chosen at random. 
190 RANDOM VARIABLES [CHAP. 5 
Solution. The random variable X has probability function 
Probability, f(x) 0.1 0.4 0.2 0.3 
Values, x 1 2 3 4 
As we found earlier, the mean, or expected value, of X, is 
I - E(X) ----- xif(x,) ---- 2.7. 
It is not convenient to compute E[(X -- 2.7) 2] directly. But it is easy 
to apply Eq. (5)' 
2 E(X 2) 2 
----- 12X 0.1 q-22X 0.4q-32X 0.2q-42X 0.3 -- (2.7) 2 
----- 8.3 -- 7.29 = 1.01. 
Sometimes another way to cut down the labor involved in computing 
a variance is to shift the origin of the domain of values of the variable, 
or to change the scale. The addition of a constant to each value of a 
random variable shifts the mean by that same constant, but does not 
change the variance. But the multiplication of each value of the variable 
by a positive constant is equivalent to a change in units (for example, 
from tons to pounds, or from feet to miles). Such a change in units multi- 
plies both the mean and the standard deviation by the same factor; how- 
ever, it multiplies the variance by the square of that factor, since variance 
is measured in squares of the units of the variable. 
The following theorem states how the variance and standard deviation 
are affected by such transformations of the random variable. A proof 
of the theorem is called for in Exercise 9. 
5-11 Theorem. Let X be a random variable with variance a 2. Let c be 
a number. Then 
2 2 2. 
o'cx = c o'x , o'cx = Iclo'x (11) 
and 
2 2 
ax+ ----- ax; ax+: ax. (12) 
5-4] VXRIXSmT� 191 
EXAMPLE 5. Let the probability function of X be as follows: 
Pobability, fix) 0.3 0.2 0.5 
Values, x 2025 2050 2075 
Find � 
Solution. Subtract 2050 froin each value of X, and divide the results 
by 25. The new random variable is 
X -- 2050 
Y - 25 ' (3) 
and its probability function is 
Probability, g(y) 0.3 0.2 0.5 
Values, y --1  1 
We compute the mean of Y and of ya. 
Mr = E(Y) = --1 X 0.3 q- 0 X 0.2 q- 1 X 0.5 ---- 0.2, 
E(Y a) = (--1) a X 0.3 q- 0 a X 0.2 q- 1 a X 0.5 = 0.8. 
Therefore, the variance of Y is 
---- 0.8 -- 0.04 ----- 0.76. 
From Eq. (13), we see that 
X = 25Y q- 2050. 
Hence 
2  0.2 
0. (25Yq-2050) 
0.2 [by Eq. (12)] 
--- 25Y 
---- 6250.3 [by Eq. (11)l 
---- 625 X 0.76 
---- 475. 
The following corollary can be proved by the method used in the foregoing 
example. 
192 RAnOM �ARIABLES � [CHA?. 5 
$-12 Corollary. Let X be  random vrible with vrince :. Let 
a and b be numbers. Then the variance of aX q- b is a2: 
Vr (aX ,- b) = a  Vr (X), (14a) 
or 
a.�+b = a=c. (14b) 
EXERCISES FOR SECTION 5-4 
1. (a) Compute the variance and standard deviation for example B of Table 
5-12. (b) Compute the mean absolute deviation and compare your answer with 
that given in the table. 
2. (a) Compute the variance and standard deviation for example C of Table 
5-12. (b) Compute the mean absolute deviation and compare your answer with 
that given in the table. 
3. (a) Compute the variance and standard deviation for example E of Table 
5-12, and compare them with the corresponding results for example C. Com- 
ment. (b) Compute the mean absolute deviation for example E of Table 5-12. 
Compare your answer with that given in the table. Also compare with example C, 
and comment. 
4. The random variable X takes the values --1, 0, and I with probabihties 
0.3, 0.2, and 0.5, in that order. Find (a) the mean,/z, (b) the mean absolute 
deviation of X about/z, (c) the variance =, (d) the standard deviation . 
5. In the medical experiment, Example 2, Section 3-3, Table 3-5 (selections 
of 3 from 5 treatments), let X be the number of times that treatment a appears 
in the listing of the 3 chosen. That is, X = 0 ff a is not among those chosen, and 
X = 1 if a is chosen. Compute E(X) and Va (X). 
6. In Example 5, Section 5-1, for runs of 2 E's and 3 O's, let X be the number 
of runs in the sample point representing the outcome of the experiment. Com- 
pute E(X) and  
fiX' 
7. In Example 5, Section 5-1, Table 5-10, on turning points for 4 different 
measurements, compute E(X) and Var (X), if X represents the number of 
turning points in a sample point. Let  = W'-7 (X). What is the probability 
thatX > /zq-? That/z-- 2 < X < /zq-2? 
8. Using Eq. (5) of the text and the formulas 
n(n q- 1) 
lq- 2q- aq-...q- n - 
2  
2 n(n q- 1)(2n q- 1) 
12 q-2 q-32-,;-...q-n = , 
6 
5-4] VARASLTY 193 
show that the mean and variance of a random variable that takes the values 1, 
2, 3, . .., n, each with probability 1/, are 
2 
n-]-I 2 n --1 
2 12 
9. (a) Prove that Vat (cX ) = c 2Var (.�). (b) Prove that Var (X-I-c) = 
Var (\). 
10. Use Theorem 5-11 to prove Corollary 5-12. 
In each vf the following exercises, 11 through 14, the probability function of 
a random variable is given. Find the mean, the variance, and the standard 
deviation. 
11. Probability, f(x) . 0.1 0.2 0.3 0.4 
Values, x 9998 9999 10,000 10,001 
Probability, f(x) 0.6 0.3 0.1 
12. 
Values, x 0.0016 0.0032 0.0064 
13. Probability, f(x) . 0.25 0.35 0.15 0.25 
Values, x --300 --200 --100 0 
14. Probability, f(x) . 0.3 0.3 0.3 0.1 
Values, x 2.75 3.00 3.25 4.00 
For each of the following probabihty functions of X, calculate the mean, 
variance, and standard deviation: 
Probability, f(x) 0.4 0.2 0.4 
15. 
Values of X, x --1 0 1 
16. Probability, f(x) I. 0.1 0.3 0.4 0.2 
Values of X, x I 2 3 4 
Probability, f(x) i. 0.1 0.2 0.4 0.2 0.1 
17. Values of X", x --2 --4 6 4 2 
Probability, f(x) 0.1 0.4 0.5 
18. 
Values of X, x 650 700 750 
194 RANDOM VARIABLES [CHAP. 5 
19. An engineer's ruler with triangular cross section has the numbers 1, 2, and 
3 printed one on each of its three faces. Imagine rolling the ruler on the floor, 
and let X be the number of the face on the bottom when the ruler comes to rest. 
Use the result given in Exercise 8 to find the mean, the variance, and the standard 
deviation of X. 
20. Consider the experiment of Exercise 19 with two such rulers. Let Y be 
the sum of the number on the bottom faces of the rulers when they come to rest. 
Find the mean and the variance of Y. 
21. A regular tetrahedron is a symmetrical solid with four faces. The faces 
are numbered 1, 2, 3, 4, and the tetrahedron is rolled on the floor. Let the random 
variable X be the number on the bottom face after the tetrahedron is rolled. 
Use the result of Exercise 8 to find the mean, variance, and standard deviation 
of X. 
22. Consider rolling two tetrahedrons like the one described in Exercise 21. 
Let the random variable Y be the maximum face-down number when the two 
tetrahedrons come to rest. Find the mean, variance, and standard deviation of Y. 
23. (Continuation.) In the experiment of Exercise 22, let the random vari- 
able Z be the minimum number on a bottom face when the tetrahedrons come 
to rest. Find the mean, variance, and standard deviation of Z. 
24. If the variance of a random variable X is 0.76, what is the variance of 
the random variable 10X? Of 2X? Of X/27 
25. If the variance of the random variable Y is 15, what is the variance of 
Y-77 Y--37 
5-5. AVERAGE AND VARIANCE IN A SAMPLE 
In the first four sections of this chapter, ve have learned about randoin 
variables, probability functions, means, and variances. These ideas apply 
to theoretical outcomes of experiments. They help us to predict what is 
likely to happen as the result of an experiment, provided we know the 
probability function, but rarely can they tell us exactly xvhat will happen. 
In ths section, we study the results that actually did happen in some 
experiments. There are two main reasons for such a study: 
(1) A comparison of observed results xxSth predicted theoretical results 
gives us a better understanding of the theory and of its reliability when 
used for making predictions. 
(2) In many experiments, we don't know the probability distribution 
of the random variable under study. For instance, in the school bond- 
issue example, the proportion of people in the district xvho own property 
and favor the bond issue is unknown at the time the survey is planned. 
So we can't use that proportion to predict the outcome of the survey. In 
fact, we do just the opposite; we use the outcome of the survey to estimate 
the proportions of people in the four categories of interest. Or, we might 
5--5] AVERAGE AND VARIANCE IN A SAMPLE 195 
wish to estimate the probability distribution of heights of American men 
of age 20. It would be costly in time and money to make a complete 
analysis of heights of all American men of age 20, so a sample is studied; 
and inferences about the average height and the variability of heights in 
the population are based on the average height and variability of heights 
in the sample. 
EXAMrL 1. From an ordinary bridge deck of 52 cards, a hand of 
5 cards is dealt without replacement. The number of red cards is tallied. 
The cards are reshuffled and the experiment is repeated 29 more times, 
giving a total of 30 hands. The results are shown in Table 5-13. What is 
the average number of red cards per hand? What is the standard de- 
viation ? 
TABLE 5--13. RED CARDS IN HANDS OF FIVE. 
No. of red cards No. of hands 
0 1 
1 6 
2 10 
3 7 
4 5 
5 1 
Total 30 
Solution. The average number of red cards per hand is found as follows: 
total number of red cards in 30 hands 
average -= total number of hands in 30 hands 
0X l+lX6 .-k2X 10q-3X7-l-4X5q-SX 1 
30 
72 
30 
We denote this sample average by 2 (read: "x bar"). Thus 2 = 2.4. 
Next we compute the sample variance, i.e., the average squared devia- 
tion from 2, for this sample. In Table 5-13, the first column gives the 
possible values x, = 0, 1, 2, 3, 4, 5 for the number of red cards per hand; 
the second column shows the frequency ni with xvhi(.h the value x oc- 
curred. The squared deviations (x -- 7) 2 occur with these same fre- 
quencies, as shown in Table 5-14. 
196 RANDOM VARIABLES [CHAP. 5 
rABLE 5--14 
CALCULATION OF VARIANCE FOR DATA OF TABLE 5--13 (----- 2.4). 
No. of No. of Squared 
red cards, hands, Deviation, deviation, Product, 
X --  (X -- )2n 
x n (x -- )2 
0 1 --2.4 5.76 5.76 
I 6 --1.4 1.96 11.76 
2 10 --0.4 0.16 1.60 
3 7 +0.6 0.36 2.52 
4 5 +1.6 256 12s0 
5 i +26 6.76 6.76 
Totals 30 I I I 41.2 
Multiplying each squared deviation by the number of times it occurs, 
and adding, we get 41.20, the sum of the squared deviations for all 30 hands. 
The average squared deviation is called the sample variarce, and is denoted 
by s 2. Thus, for this example, 
s2_ 41.20  1.37. 
3O 
The sample standard deviation s is the positive square root of the variance: 
s    1.17. 
Thus, for the data of Table 5-13, we have found 
average number of red cards per hand: : = 2.4, 
standard deviation of numbers of red cards per hand = s  1.17. 
The sample average and standard deviation together provide a useful, 
quick summary of the frequency distribution in the sample. The average 
is a measure of location: it tells where the "centel'" of the sample is located. 
The standard deviation measures the dispersion, or spread, around the 
average. In the present example, 2 - 2.4 is almost exactly halfway be- 
tween the extreme values 0 and 5. And those extreme values, in turn, are 
at distances 2.4 and 2.6 from 2. If xve measure these distances in standard 
deviation units, we find 2.4/1.17 = 2.05 and 2.6/1.17 = 2.22. Thus all 
of the values of x, in this example, are within 2.22 standard deviations of 
5-5] AVERAGE AND VARIANCE IN A SAMPLE 197 
the sample average. It is usually true that all, or nearly all, of the ob- 
servations in a sample lie within 3 standard deviations of the sample 
average. 
We now make the following formal definitions: 
$-13 Definitions. Sample variance and standard deviation. Given a set 
of n observations or measurements in which the value x occurs 
n times, x2 occurs n2 times, and so on, .zt occurs nt times: 
Frequencies, n n n2 ... t Total: n 
Values, x Xl X2 ... Xt 
Let 7 be the average of the measurements' 
1 
: ---- - x(ni. (1) 
n 
The variance s is defined by 
2 (3:1 )2nl - (X2 )2n2 -- -- (xt )2n 
, 
Sx ---- nl -}- n2 -}- ''' -[- nt 
or 
Sx = -E(x, - (2) 
n 
The standard deviation Sx is the positive square root of the variance. 
Computational formula. The sample variance, Eq. (2), is the average 
of the squares of the deviations of the observations from their average' 
briefly, the average squared deviation. For computations it is often easier 
to use the following formula, which is analagous to formula (5) of Sec- 
tion 5-4: 
o I 2 __ 
n 
or 
2 :-2 
Sx = Ave(x 2)--. � (3b) 
198 RANDOM VARIABLES [CHAP. 5 
In Eq. (3a), we have omitted the limits of summation. [As in Eqs. (1) 
and (2), i goes from 1 through t. See Appendix II-1, following Eq. (4), 
for a discussion of omission of limits of summation.] In Eq. (3b), we have 
used the notation Ave (x 2) to denote the average value of x2: 
 -- -X i. 
Ave (x ) n 
A proof of Eq. (3a) is asked for in Exercise 1 at the end of this section. 
REMARK. If random samples of size n are drawn from a population 
vith variance , the sample variance s  varies from sample to sample. 
Its long-run average can be shown to be (n -- 1)/n. Some authors 
define the sample variance by dividing by n -- 1 in Eq. (2) rather than 
by n. Then their sample variance across many samples averages to 2. 
Hovever, (n -- 1)In is close to 1 vhen n is large, so the two definitions 
are practically identical for large samples. 
The numbers , Sx 2, and sx are also called sample average, sample variance, 
and sample slandard devalion, respectively, to distinguish them from the 
corresponding features of the population. The sample values are computed 
from the observed measurements. Any set of measurements can be thought 
of as a "sample" from the "population" of all possible sets of measurements 
obtainable or imaginable under comparable experimental conditions. In 
the example of 30 hands of 5 cards each (Section 5-5, Example 1), the 30 
hands are a "sample" drawn vith replacement from the "population" of 
(%) possible hands. For each hand the cards are dealt without replace- 
ment. A different sample would usually have a different average and a 
different variance. Also, the sample average and variance are usually 
different from the theoretical mean and variance of the population. 
EMrL 2. Compare the sample mean and variance with the popula- 
tion mean and variance for the problem in Example 1. 
Solution. For Example 1, the sample characteristic is the number of 
red cards in a hand of 5 cards. This is a random variable X whose possible 
values are 0, 1, 2, 3, 4, 5. There are 52 cards in the deck, composed of 
26 red cards and 26 black cards. A hand of 5 cards can be chosen in (%2) 
ways. A hand of 5 cards containing x red cards and 5 -- x black cards 
can be selected in (2)(sx) different ways. Hence 
The values of the right-hand side of Eq. (4), for x = 0, 1,..., 5, are 
shown in the following table (probabilities accurate to 3 decimal places): 
5-5] AVERAGE AND VARIANCE IN A SAMPLE 199 
Probability, f(x) 0.025 0.150 0.325 0.325 0.150 0.025 
Values, x 0 I 2 3 4 5 
The theoretical mean number of red cards in a hand of 5 is 
/ -. 0(.025) q- 1(.150) q- 2(.325) q- 3(.325) q- 4(.150) q- 5(.025) ---- 2.5. 
This result can also be obtained at once by noticing that the probability 
function s symmetric about x = 2.5. 
We compute the variance from the formula 
a2 = E(X 2) _ 
To compute E(X2), we square each possible value of X, multiply the 
result by the probability of that value, and add, to get 
E(X 2) = 7.400. 
Therefore, to three decimals, the theoretical (population) variance is 
a 2 = 7.400 -- 6.250 = 1.150. 
Recall that the sample average and variance for the sample of 30 hands 
were 
. = 2.4 and s 2  1.37, 
while 
/ = 2.5 and (r 2 = 1.15. 
We see that the sample average and variance serve as reasonable estimates 
of the theoretical mean and variance of the population. 
It is well at this point to summarize and compare some relevant char- 
acteristics of populations and samples. 
Population Sample 
Possible values' Xl, 12, � �., xt Observed values: x, 12, � .. , xt 
nl n2 nt 
Probability:f(xx),f(x2),. .,f(xt) Relative frequency:--,--,...,- 
' n n n 
Mean:/ = xf(x) Average: : 
n 
1 
Variance' o '2 ----  (x -- la) 2f(:ci) Variance: 
n 
200 RANDOM VARIABLES [CHAP. 5 
REMARK. Vhen we group the data, as we have in the frequency tallies, 
the z's in the sample go from zl through zt, just as in the population. 
But often we don't group the observed data, but list them as 
Then the z's in the sample are the values observed, in the order of their 
occurrence if there is an order. Ve then think of z as an observed value 
of a random variable 55, for each i from 1 through n. Thus one full 
sample, (zl, x2, � �., Xn), produces values for all of the random variables 
(, , ..., ). The sample average 
1 
n 
is an observed value of the random variable 
Usually, in a sample, all of the  random variables X have the same 
probabiliW function: that of the random variable in the population being 
sampled. 
EXERCISES FOR SECTION 5-5 
Compute ., s 2, and s for each of the following sets of measurements: 
1. 1, 1. 2. 1, 2, 3. 3. --1, 0, 1. 
4. 72, --2. 5. 4, --5, --6. 6. .1, .3, .6. 
7. Five measurements are l's, 3 measmements are 2's, and 1 measurement is 
3. Find , s 2, s. 
8. If half the measurements have value i and half have value 3, find the 
variance and standard deviation. 
In each of the following problems 9 through 13, values are given for n (the 
number of observations in a sample), -x, and -x  Using these data, find the 
sample average, variance, and standard deviatmn. If you think that the given 
data are inconsistent, state your reason for thinking so. 
9. 10 35 140 
10. 8 --56 408 
11. 25 100 400 
12. 12 30 65 
13. 100 3 .90 
5--5] AVERAGE AND VARIANCE IN A SAMPLE 201 
In Exercises 14 through 17, we use the following notation: x, x2, . .., x and 
Yl, Ys, � �., y, are sets of neasurements whose means are  and j and whose 
standard deviations are s and s, respectively; c and k ale constants. 
14. If y = x  , show that  =   k, sy = s and sy s. 
2 2 2 
15. If y = cx,, show that  = c, and su = cIs and sy = c s. 
= 2 = 2 2 
16. If y = cx, + k, show that  = c + k, and sy Ics and su c s. 
17. Ifz = xq-y,then = . 
18. Without calculation, explain why the numbers 100, 101, 200 have the 
same variance as the numbers 1000, 1001, 1100. 
19. Without calculation, explain why the standard deviation of the numbers 
1, 2, 3, and 4 is half the standard deviation of the numbers 2, 4, 6, and 8. 
20. In a certain neighborhood, 3 families have no car, 20 families have I car, 
15 families hax'e 2 cars, and 2 faroflies have 3 cars. Find the mean and standard 
deviation of the number of cars per family. 
21. The folloxxing frequency distribution was obtained in a breeding experi- 
ment with mice: 
number in litter: 1 2 3 4 5 6 7 8 9 
frequency: 7 11 16 17 26 31 11 I I 
Find the mean, variance, and standard deviation of the distribution. 
22. The following frequency distribution gives the lengths of 800 ears of corn 
in inches, to the nearest half inch. 
lengths of 
ears: 4.0 4.5 5.0 5.5 6.0 6.5 7.0 7.5 8.0 8.5 9.0 9.5 10.0 
frequencies: i i 8 33 70 110 176 172 124 61 32 10 2 
(a) Compute the mean and standard deviation of the distribution. (b) What 
percent of the measurements are within s of ? Within 2s? Within 3s? 
23. Ernest Thompson Seton gives, in The Arctic Prairies, the numbers of 
antelopes in 26 bands seen along the Canadian Pacific Railroad in Alberta, 
within a stretch of 70 miles, as follows: 
8, 4, 7, 18, 3, 9, 14, 1, 6, 12, 2, 8, 10, 
1, 3, 4, 6, 18, 4, 25, 4, 34, 6, 5, 16, 4. 
Find the average number in a band, the standard deviation, and the percent of 
bands within s of  and the percent within 2s of . 
24. Show that Eqs. (3a, b) in the text are valid. Compare with Eq. (5), 
Section 5-4. 
202 RAXuOM VAmXLES [cHxr. 5 
5-6. CHEBYSHEV'S THEOREM FOR A PROBABILITY DISTRIBUTION 
Up to this point, we have discussed the mean, the variance, and the 
standard deviation for probability dzstributions, and the sample average, 
variance, and standard deviation for observed sets of measurements. But 
we have not shown how the standard deviation can be used to provide 
information about the way probability accumulates in intervals centered 
on the mean as their widths grow. We have an intuitive feeling that 
when the standard deviation is small the probability piles up near the 
mean, and when the standard deviation is large the probability spreads 
out more. With the aid of a remarkable theorem due to Chebyshev, which 
we study in this section, we shall be able to answer questions like the 
following 
What percent of the total probability lies in a given interval centered 
at the mean? 
How wide an interval about the mean is needed to guarantee that, 
for example, three-quarters of the total probability of the random variable 
is included in that interval? 
Before stating the theorem, however, n e look at a simple example. 
Ex^MP 1. Consider the random variable X having the following 
probability function: 
Probabihtv, f(x) 2 2 9 __ 
 64 64 64 64 
Values of X, x 0 1 2 3 
Find the probabfiity that is associated with values of X: 
(a) at or within 1 standard deviation from the mean, 
(b) at or within 2 standard deviations from the mean, 
(c) at or within 3 standard deviations from the mean. 
Solttio,. For the mean and standard deviation, calculations give 
u=E(x) = 4. 
Figure 5-5 shows a graph of the probability fulmtion. The mean,  = , 
is marked with a snmll wedge, , to suggest a fulcrum. Intervals extend- 
ing 10., 20., and 30' to the left and right of the mean are also shown, along 
with the correaponding probabilities. 
5--6] CHEBYSHE�'S THEOREM 203 
2'; 27 
9 
I 
5-5. IntcrvMs of width 2a, 4a, 6a around thc mean. 
(a) The probability at or within q-let from/ is 
27 27 27 
6- q------  0.84. 
64- 32 
(b) The probability at or within q-2a from/u is 
27 27 9 63 
64 -- -- 
-- - 64 -- 64  0.984 
(c) The probability at or within q-3a from/ is 
27 27 
64:+ -- 4 -- 6- ---- 1. 
5-14 Theorem. Chebyshev's theorem. At least the fraction 1 -- (1/h 2) 
of the total probability of a random variable lies within h standard 
deviations of the mean. 
Discussion. The theorem says, for example, that at least 1 -- �, or , 
of the total probability is within q-2a from , for any random variable. 
In the example above, we found that the actual probability in the band 
63 which is much greater than . The 
from -- 2a to +2a was , 
theorem also says that at least � of the total probability is within 3a from 
the mean, and, in the example, we found that the band from  -- 3a to 
 q- 3a contained the total probability of 1. 
The theorem can be used to show that sample proportions of sufficiently 
large random samples from a population are likely to be close to the true 
proportion of the population. It forms a mathematical foundation for 
the use of samples to estimate characteristics of a population. 
Proof of Chebyshev's theorem. Suppose the random variable X has a 
mean  and standard deviation a. Figure 5-6 represents the domain of its 
20At RANDOM VARIABLES [CHAr. 5 
 -- 2'  + 2 
Left-hand Region  here Rght-hand 
region where region where 
't -- , > 2 .r - l  2 ,r --  > 2 
(at )2 > 42 (r,-- )2  42 (a_ )2 > 42 
Fro. 5-. Regions for Chebyshev heorem, h = 2. 
probability function. We shall first prove the theorem for the case h = 2, 
and to this end we have separated the possible values of X into two sets: 
(a) those in the interval  -- 2a to  h- 2a, including any lying at the 
boundaries, and 
(b) the remainder, those lying beyond the boundaries of the interval. 
We want to prove that the probability associated with values of X in 
the set (a) is at least . For convenience, we refer to the set (a) as the 
values of X within the interval and to set (b) as the values outside the 
interval. And the values outside the interval consist of those to the left 
of  -- 2a, xvhich we shall call the left-hand outer set,' and those to the right 
of/ h- 2a, which xve shall call the right-hand outer set. 
It is clear from Fig. 5-6 that any point on the x-axis and outside the 
indicated middle interval is more than 2a from the mean . Therefore the 
square of its distance from the mean is more than 4a 2. 
Recall the definition of the variance: 
! 
2: E[(X- )21:  (.rl- )2f(xi), (1) 
xvhere x, x2, . . . , xt are the possible values of X, and f(xO, f(.r2), . . . , f(xt) 
are their associated probabilities. Now the numbering of the x's (the 
subscripts) is completely arbitrary, so, for convenience, let Xl, x2, . .., x 
denote those that are outside the interval, if there are any outside. 
Case 1. If there are no values of xi outside the interval, all values are 
xvithin 2a of the mean, so the probability of set (a) is 1, and hence is at 
least a 
Case 2. If the number outside the interval is r >_ 1, we break up the 
sum in Eq. (1) into txvo parts' 
5-6] CHEBYSIIEV'S Tt{EOREM 205 
e 2 = [(. - )2f(.r) + (x2 - )-�f(.r) + ....  (.r 
+ [(x+ - )/(.r+) +... + (.r - )/(x)]. (2) 
Every squared deviation (xl -- )  is positive or zero, and f(.r) is also 
positive. If any squared deviation is replaced by a snmller number, then 
the right-hand side of Eq. (2) is reduced. We shall make such reductions 
and get an inequality that yields the proof. 
We reduce the first r squared deviations (arising from values of x out- 
side the interval) by replacing each of them by the smaller value 4 . 
Then we reduce the rest of the squared deviations (arising from values 
of x inside the interval) by replacing each of them by the smaller or pos- 
sibly equal value 0. When we have made these replacements, we get 
the inequality 
  4[f(x) + f(x)  .... + f(.r)]. (3) 
If e2= O, all the probability is concentrated at the mean, and therefore 
within 2e of the mean. Why? 
If e2> O, then we may divide both sides of the inequality (3) by 4e  
and get 
 > U(.r)  Y(.'. +... + f(.r)] = ([x - l > 2). (4) 
4 
The last equality in (4) follows from the definition of P([X -- z] > 2e)' 
it is the probability that X is more than 2e from the mean u, and this 
probability is the sum of the probabilities assigned to the points x, 
x2, ..., xr that are outside the interval. Therefore, from the inequal- 
ity (4), we see that at most � of the total probability is assigned to 
points lying outside the interval. Hence the probability assigned to points 
-  This 
lying within a distance 2e from the mean is at least I 4 -- . 
completes the proof of Chebyshev's theorem for h ---- 2. 
The demonstration just gven can be generalized to intervals  -- he 
to  d- ha for any h  0. We replace 2e by ha and 4e 2 by h2e throughout 
the argument. When these replacements are made, the inequality that 
replaces (4) is 
> 5,(Ix- ,,l > e). (5) 
This says that the probability assigned to values of X outside the interval 
/x -- ha to /x d- ha is at most 1/h 2. Hence the probability assigned to 
values of X within a distance ha of the mean is at least 1 -- (l/h2). [] 
206 IANDOM VARIABLES [CHAr. 5 
EXERCISE FOR SECTION 5-6 
Answer Exercises 1 through 4, assuming that/x = 0 and x' = 1. 
1. At least how much of the probability of � lies within 2 units of the mean? 
2. What is the minimum value of P(--3  X  3)? 
3. What is the maximum value of 
4. What value of  guarantees P('  )  0.96? 
Ansxer Exercises 5 through 8, assuming that x = 7 and .x' = 2. 
5. What is the least value of P(3 < � < 11)? Of P(1  X  13)? 
6. What is the greatest value of P(}X -- 7} > 2)? OfP(�-- 7 > 3)? 
7. What is the least value of P(X -- 7}  5)? 
8. What value of k guarantees that P(IX -- 71  k)  0.99? 
9. When h  1, Chebyshev's theorem is useless. Why? 
10. Make the required substitutions and generalize the proof of Theorem 5-14 
from intervals 
11. Using Chebyshev's theorem,  hat value of h guarantees that at least 90% 
of the probability is within h of the mean? What value of h guarantees 99%? 
12. What is the maximum probability lying at least 2 away from the mean? 
3? 5? 
13. Under what conditions on X is its variance zero? How much probability 
then lies more than 0.01 away from the mean? 
In Eereises 14 through 22, the random variable X takes the values --c, 0, 
and c, with probabilities p, 1 -- 2p, and p, in that order. 
14. Find  and 
15. Find a relation between  and c if P(X -- }  ) = 1. 
16. Show that c =  when p = . 
17. If c = 2, what does p equal? 
18. If c = 3, what does p equal? 
19. In Exercise 16, what is the probability of an absolute deviation 
at least as great as one standard deviation? As great as 2? 
20. In Exercise 17, what is the probability of an absolute deviation at least 
as great as 2? As great as 3? 
21. In Exercise 18, what is the pobability of an absolute deviation at least 
as great as 3 v As 4? 
22. By proper choice of p, can you make c = h, for any positive h? If so, 
what is the proper choice of p, in terms of h, and what is the probability of an 
absolute deviation at least as great as 
RmAK. Exercise 22 shows that if h is given, then we can find a random 
variable X such that the probability that X takes values at least h away 
from its mean is 1/h , the maximum allowed by the Chebyshev theorem. 
5--7] CHEBYSHEV'S TttEOREM 207 
In this sense, the conclusion in Chebyshev's theorem is the best possible. 
But a probability distribution that has the nmximum allowable probability 
at least ha away from  for one particular value of J may not do so for a 
different value of h. Part of the charm of the Chebyshev theorem is that 
it works for all probability distributions with finite means and variances. 
5-7. CHEBYSHEV'S THEOREM FOR A FREQUENCY DISTRIBUTION 
OF MEASUREMENTS 
]Ve have seen how the standard deviation rr provides a yardstick for 
measuring distances from the mean of a random variable X. Chebyshev's 
theorem tells us that the worst that can happen is that the fraction 1/h 2 
of the probability will be assigned to points more than h standard devia- 
tions away from the mean, for any positive h. We may well wonder if 
an analogous theorem holds for measurements or observations in a sample. 
The answer is "yes." We state the result formally as a theorem, but we 
do not give the proof, since it is almost identical with the proof given in 
Section 5-6. 
5-15 Theorem. Chebyshev's theorem for measurements. At least the 
fraction 1 -- (1/h 2) of the measurements in any sample lie within 
h standard deviations of the average of the measurements. 
ExMPL 1. Suppose the measurements are --8, --1, --1, 0, 0, 0, 0, 
1, 1, 8. Verify that at least  of the measurements are within 2 standard 
deviations of , and at least � are within 3 standard deviations of . 
Solution. The sum of the 10 measurements is 0, hence � = 0. The sum 
of the squares of the measurements is 132. Hence the average of the 
squared deviations is s 2 = Ave (x 2) -- 2 = 13.2 -- 0 = 13.2, and the 
standard deviation is s -- 3.6. The interval containing all measurements 
within two standard deviations from the mean extends from --7.2 to 
-t-7.2, and contains 80% of the measurements (hence at least  of them). 
The interval extending three standard deviations from the mean in both 
directions goes from --10.8 to -[-10.8 and contains 100% of the measure- 
ments (hence at least � of them). 
Rr^K. For work with large numbers of measurements, stronger 
results than those given by Chebyshev's theorem usually hold. Table 
5-15 gives a rough rule for the percentage of measurements usually found 
in intervals about the mean. The numbers given for the empirical rule 
agree exactly with those for the normal probability distribution, which we 
shall study in more detail in Section 6-[ and in Chapter 7. 
The results of Chebyshev's thcorem guarantee lower bounds on the 
percentage of measurements within h standard deviations of the average. 
208 RANDOM VARIABLES [CHAP. 5 
TABLE 5--15 
PERCENTAGES OF MEASUREMENTS CONTAINED IN 
INTERVALS ABOUT THE MEAN. 
Empirical rule Chebyshev's theorem 
Contains about Contains at least 
Interval this percentage of this percentage of 
the measurements the measurements 
--s tos 68% 0% 
 -- 2s to  q- 2s 95% 75% 
 -- 3s to 2 q- 3s 99.7% (nearly all) 89% 
They may help us (a) discover an error in calculation, or (b) interpret and 
use the standard deviation. The middle column of Table 5-15 is even more 
valuable in helping to interpret a set of measurements. The figures 68%, 
95%, and 99.7% are not to be taken literally. If you found 64% or 73% 
of the measurements within one standard deviation of the mean, you 
should not be startled..Indeed, it is possible to find 100% of the measure- 
ments within one standard deviation of the average. 
EXERCISES FOR SECTION 5-7 
Given that 2 = 0 and s = 1 for a set of n measurements, use Chcbyshev's 
theorem to answer the folloing: 
1. At least how ninny measurements lie ithin 3 units of the mean? 
2. At least how many measurements will lie between --2 and 2, inclusive ? 
3. At most hmv many measmements will be greatel' than 2 or less than --2? 
4. What value of k > 0 will guarantee that 96% of the measurements lie 
between --k and k, inclusive? 
If 2 = 5 and s = 2, use the empirical rule of Table 5-15 to answer the 
following: 
5. About what percent of the measurements lie between 3 and 7, inclusive? 
Between 1 and 9, inclusive? 
6. About xvhat percent of the measurements are greater than 9 or less than 17 
If 2 = 1 and s = 3, use Chebyshev's theorem to answer the following: 
7. What is the least number of mcasurelncnts that lie between --5 and 7, 
inclusive? Between --8 and 10, inclusive? 
8. What is the greatest number of measurements that are greater than 7 or 
less than --5? Greater than 10 or less than --8? 
CHAPTER 
JOINT DISTRIBUTIONS 
AND CONTINUOUS 
DISTRIBUTIONS 
6-1. JOINT PROBABILITY FUNCTION OF TWO RANDOM VARIABLES 
Several random variables may be associated with outcomes of the 
same experiment. In fact, if we wanted to study relations between weight, 
height, and age, we might let measures of weight, height, and age be 
three random variables whose values are determined by the experiment 
of measuring these characteristics of a person chosen at random from 
the population of a community. In the next example, we look at a simpler 
problem involving two random variables whose values are numbers 
determined by tossing a coin 3 times in succession. 
Ex.a.PLE 1. Let X be the number of heads and let Y be the number 
of runs when a coin is tossed 3 times in succession and each toss is recorded 
as H or T. Make tables showing possible pairs of values of X and Y and 
their corresponding probabilities. 
Solution. Table 6-1 shows a sample space for the experiment with 
the value of X and of Y for each point. 
Since X takes values 0, 1, 2, 3 and Y takes values 1, 2, 3, we set up 
a 4 by 3 array in Table 6-2 to display the probabilities of occurrences of 
the various combinations. We get each probability by counting the 
number of occurrences of that combination of values of X and Y and 
dividing the result by 8. 
The entries in Table 6-2 give the values of the joint probability function 
of X and Y. Missing entries have the value 0. The entry  that occurs 
in the column for X ---- 1 and the row for Y ---- 2 is the probability that 
X = 1 and, at the same time, Y ---- 2: 
P(X = 1, Y---- 2)= . 
209 
210 JOINT AND CONTINUOUS DISTRIBUTIONS [CHAP. 6 
TABLE 6--1. SAIPLE SPACE FOR SEQUENCE 
OF 3 COIN TOSSES. 
Sample point No. of heads, No. of runs, 
X Y 
HHH 3 1 
HHT 2 2 
HTH 2 3 
HTT 1 2 
THH 2 2 
THT i 3 
TTH i 2 
TTT 0 1 
TABLE 6--2. IROBABILITIES OF PAIRED VALUES OF 
X AND r FROM TABLE 6--1. 
Number of heads, X values 
0 i 2 3 Row totals 
i i 2 
1 
Number of runs, 
2 2 4 
Y values 2 
i i 2 
3 
Column ta]s 
The headings across the top of the table, together with the column 
totals at the bottom, provide the probability distribution for X by itself. 
Similarly, the probability distribution for values of Y can be obtained 
from the columns at the left and at the right. These are called marginal 
distributions, probably because they are found on the margins of tables. 
For this example, they are 
Probability distribution for X: 
1 3 3 1 
f(x)  � �  
x 0 1 2 3 
-l] JOINT PROBABILITY FUNCTION 211 
Probability distribution for Y: 
g(Y)    
y 1 2 3 
A graph of the joint probability function of Table 6-2 is shown in 
Fig. 6-1. 
P(X =x,I'  V) 
2 2 
1 Ii I 1 e 
 I 8 I 
I I I 1 2 I 
I I/ I--2- ..--- I x "X 
I .i z I/ / I 
6-1 Definition. Joint probability fmctiom Let X and Y be two random 
variables. Suppose that the possible values of X are x, x2, � � �, Xm, 
and that the possible values of Y are yx, Y2,..., Y. For each 
ordered pair (x, yj), let P(x, yj) be the probability that X takes 
the value x and Y takes the value yj, where i: 1, 2,..., ra 
and j = 1, 2,..., n. Then P(x, yj) is the value of the joint 
probability function of X and Y at (xl, y j). 
For the 3-coin example, the joint probability function of X and Y 
given by Table 6-2 tells more about the joint behavior of X and Y than 
we can get from their separate probability distributions. It shows that 
certain combinations have zero probability' for example, X = 3 and 
Y = 3. Suppose we let E be the event X ---- 3 and Ftheevent Y---- 3. 
Then, from Table 6-2, we see that 
P(E n F) = O, 
while 
P(E)  P(F) = . 
-- 8, 
212 JOINT AND CONTINUOUS DISTRIBUTIONS [CHAP. 6 
Thus P(Ei' P(F) = . 
Therefore these events E and F are not independent. We also say that 
the random variables X and Y are not independent, because they fail 
to satisfy the general criterion for independence that we now state. 
6-2 Definition. Independence. Let X and Y be two random variables 
over a sample space S. Let the values of X be x, x2,..., Xm 
and let the values of Y be y, Y2,.-., g- Then X and Y are 
s[aisfieally independen if and only if hey satisfy he produe rule: 
P(X = xl and Y 
= P(xl) � P(Yi) 
for all pairs of values (xl, Yi). 
Since Eq. (1) is not satisfied for x;: 3, yj = 3, in the 3-coin example, 
the random variables "number of heads" and "number of runs" are not 
independent. 
The notion of mathematical expectation can be extended to functions of 
two or more random variables. We give the definition for the expectation 
of a function h of two random variables X and Y. The function h is not 
ordinarily the probability function of X and Y. In particular, we are 
interested in such functions as 
h(X, Y) = X-q- Y, 
or 
t(x, Y) = xY, 
or even 
t(x, Y) = 
6-3 Definition. Mathematical expectation of a fimctioz of two variables. 
Let h(X, Y) be a function of the two variables X and Y. Then 
the mathematical expectation of h, written E[h(X, Y)], is defined 
to be the sum of the products of all possible values h(x, ya) each 
multiplied by its probability P(x;, 
I E[h(X, Y)] = Zh(xl, yj). P(x, y), (2) 
the sum being taken over all possible pairs (x, Yi). 
6-1] JOINT PROBABILITY FUNCTION 213 
EXAMPLE 2. Using the data of Table 6-2, find the mathematical 
expectations of (a) X Y, (b) X/  (c) Y, (d) �, (e) X q- �. 
Solutions. (a) Product X�. Table 6-3 shows xyP(x, y) in each cell 
where the probability is not zero. We omit the terms with zero proba- 
bilities, since they contribute nothing to the sum in Eq. (2). 
TABLE 6-3. VALUES OF X� AND THEIR PROBABILITIES, FOR 
DATA OF TABLE 6-2. EMPTY CELLS HAVE PROBABILITY ZERO. 
X values 
0 1 2 3 Row sums 
3 
1 ox- 3x-  
Y values 2 X  4 X   
8 
9 
sx 6x  
14 3 24  3 
Column sums 0  -   - 
We may sum the entries in Table 6-3 in any way we wish, provided 
s 2, and  and the sum of 
we include all entries. The row sums are s, 
these is  or 3. As a check, we note that the column sums also add up 
to 3. Hence, 
E(XY) = 0X q-3X +2 X+4 X -3 X +6X  
(b) Quotient X/Y. We omit the table for values of X/Y, which, like 
Table 6-3, would be constructed from the data of Table 6-2. When we 
multiply each probability by the value of X/Y for that cell, and add, 
the result is 
1  7 
E(X/D = { x +{ x +  x + x +  x + x - . 
(e) and (d). Means of  and Y. To eompu[e E(X), we may ei[her work 
dimefly horn Table 6-2 or from [he probabili[y dis[ribufion of X [ha[ 
we derived from iL The value of X remains cons[an[ for [he cells in [he 
same column of Table 6-2; we multiply the value of X in a cell by [he 
probabili[y in [ha[ cell, [hen sum. The resul[ is 
(X) = 0 X +  X +2 X +3 X  = 
214 JOINT AND CONTINUOUS DISTRIBUTIONS [CHAP. 6 
Similarly, 
E(�)=x+2x+sx=2. 
(e) Sum X 4' �. We compute E(X 4' �) directly from Table 6-2. 
Again the value of X 4. � in each cell is multiplied by the probability in 
that cell, and the results are added: 
E(X4. Y) = (04. 1) X 4. (34. 1) X  
4- (1 4.2) X 4' (24.2) X  
+ (1 + 3) x  + (2 + 3) x  
---- 3.5 ---- 1.5 4- 2 ---- E(X) 4. E(Y). 
We might wonder whether equations like 
E(X�) = E(X) . E(�) 
and 
E = E(Y) 
are true. We found that E(XY) = 3 andE(X).E(Y) = 1.5 X 2 = 3, 
so it is true (in this example, but not in general) that E(XY) =E(X) � E(Y). 
But 
() 7 1.5 E(X) 
 =   = (y 
The next theorem gives a general result about the mean of the sum of 
two random variables. The result, that the mean of the sum is the sum 
of the means, is not surprising; if every customer in a men's clothing store 
buys a hat, a suit, and a pair of shoes, then the average total price paid 
is the average paid for hats plus the average paid for suits plus the average 
paid for shoes. 
6-4 Theorem. )liean of sum. Let X and Y be random variables with 
means E(X) and E(Y). Then the mean of their sum is the sum 
of their means: 
E(x + Y) -- E(x) + E(Y). ] (3) 
6-1] JOINT PROBABILITY FUNCTION 215 
Proof. By definition of the mean, 
E(X -]- Y) ----  (x - V) � P(.r, y), (4) 
or 
E(.� -]- Y) = xP(', y) + yP(:ci, y), (5) 
the sums being taken oxer all pairs of values (x, y) that (X, ) can have. 
For example, f " has values x, z2, and Y has values y, y2, Y3, then 
the first sum on the right side of Eq. (5), written out in full, is 
: 'l[('rl, l)  P(;I, 2)  (;l, 3)] 
+ a-s[P(s, st) + P(.a, sa) + P(zs, w)]. (6) 
The bracketed coecient of 2l is the sum of the probabilities of all pairs 
(ah, y) that contain x. Hence this coecient is just P(X  x) or, 
more compactly, P(zt). Similarly, the bracketed coecient of x is P(x). 
Therefore Eq. (6) says that 
and this is E(X) in the special case under discussion. 
Similarly, if we expand the second term on the right of Eq. (5), and 
combine ternas that multiply y, y, and Ys, in order, we get 
E(2'z, ) = l(l)  2(2)  3(3) (7b) 
and this is E(Y) in the special case. 
More generally, if X has m possible values x, x, ..., x,, and Y has 
n possible values Yl, ya, � � � , Yn, the equations that replace Eqs. (7a, b) a'e 
i'('z, ) : 'l('l)  22(X2)  ' ' '  ;m(2m) (8a) 
and 
Ev,P(a', v,) = wP(sO + wP(va) + ." + VnP(S,O. (Sb) 
The right side of Eq. (Sa) is the sum of the products of the possible values 
of X, each multiplied by its probability, and is therefore E(X). Similarly, 
the right side of Eq. (Sb) is E(Y). When these results are introduced on 
the right side of Eq. (5), we get the desired result, 
216 JOINT AND CONTINUOUS DISTRIBUTIONS [CHAP. 6 
Equation (3) can also be exte{ded to the sum of three random variables 
X, Y, and Z: 
E[(X -1- Y) -1- Z] = E(X + Y) -1- E(Z) ---- E(X) -1- E(Y) -1- E(Z). (9) 
More generally, the mean of the sum of any finite number of random 
variables is the sum of their means. For future reference, we state this 
result formally as a corollary of Theorem 6-4: 
6-5 Corollary. Mean of sums of random variables. The mean of the 
sum of any finite collection of random variables is the sum of 
their means. 
Products. The mean of the product of two random variables is not 
always equal to the product of their means, as the following example 
shows. 
EXAMrLE 3. Two dependent rando,, variables. Suppose the joint proba- 
bility function of X and Y is as follows: 
Y 
0 1 sums 
�- 
Columnsums � ] � 
Compare E(X. Y) and E(X) � E(Y). 
Sohttion. 
E(XY) = 0.0.0q-0.1.�q- 1.0.�q- 1.1.0: 0, 
1 __ 1 
1 1. 
(xY) = o = 
We note in this example that we can predict X from Y and Y from X; 
for if X = 1 then Y = 0, and if X = 0 then Y = 1, and these are the 
only values possible. Dependence sometimes has the effect of improving 
the prediction of the value of one random variable from the known value 
of a dependent variable. 
6-1] JOXT PROBABILITY FUNCTION 217 
In Example 3 the random variables are dependent, and 1,J(XY)  
E(X) � E(�). In Example 2, the random variables were also dependent, 
but there we found that E(X�) was equal to the product E(X) � E(�). 
Thus, for dependent random variables, the mean of the product may or 
may not equal the product of the means. But for independent random 
variables, the following theorem is true. 
6-6 Theorem. ][ea of a product of independent raw,dom rariables. 
Let X and }' be independent random variables with means E(X) 
and E(Y). Then the mean of their product is equal to the product 
of their means: 
E(XY) = E(X). E(Y). (10) 
Proof. The condition for independence of X and Y is 
P(xl, y,) = P(x,) � P(y,), (11) 
for all pairs of values (x,, y). From the definition of E(XY), and Eq. (11), 
we have 
E(XY)---- x,y,P(x,)P(y,). (12) 
Again the sum is to extend over all pairs (x,, Yi). In particular, if xl takes 
two values x and x=, and if yy has three possible values yl, y2, and Y3, 
the sum in Eq. (12) is 
xyP(xl)P(y) : xyP(xl)P(y) q- xyP(x2)P(y) 
q- xxyP(x)P(y) q- xyP(x)P(y) 
q- xyP(x)P(y) q- x2yP(x2)P(y) 
= .%P(x)[yP(y) q- y2P(y2) q- yP(y)] 
q- x2P(x2)[yxP(y) q- y2P(Y2) q- yaP(ya)]. (13) 
The bracketed coefficient of XlP(Xl) is the sum of the products of values 
of Yi each multiplied by its probability; by definition, this is E(Y). Sub- 
stituting E(Y) for the two bracketed coefficients in Eq. (13), we get 
xyP(xOP(Yi) = xlP(x)E(Y) q- xP(x)E(Y) 
---- [xP(x) q- x2P(x2)]E(Y). (14) 
218 JOINT AND CONTINUOUS DISTRIBUTIONS [CHAP. 6 
The bracketed coefficient here is E(X), since we have assumed, temporarily, 
that X has just two values, Xl and x2. Thus, for the particular case where 
x, has 2 values and yj has 3 values, Eqs. (12) and (14) together give the 
desired result: 
E(X�) = E(X) � E(�). 
More generally, if x, has m values x, x2, . .., Xm, and Y1 has n values 
y, y2, . .., y, the expanded form of the sum in Eq. (13) contains m X n 
terms. But xiP(xi) is a common factor in n of these terms, the other 
factor being 
YxP(Y) + YP(Y) + ''' + Y,,P(Y,,), (15) 
which equals E(Y). Similarly, xP(x) is multiplied by the sum (15), and 
so are xaP(xa) and so on to xmP(xm). Finally, the sum in Eq. (14) becomes 
[XlP(Xl) - x2P(x2) - ''' - xmP(xm)]E(Y), (16) 
and the bracketed coefficient is E(X). Hence the theorem is true in 
general, for independent random variables. [] 
EXAMrLE 4. The joint probability distribution of two random variables 
is given in Table 6-4. Show that X and Y are independent. Compute 
E(XY) directly and compare with E(X) � E(Y). 
Solution. Each cell entry in the main body of the table is the product 
of the corresponding row and column sums, so the random variables are 
obviously independent. 
TABLE 6--4 
Values of X 
i 2 3 4 Row totals 
Values 
1 1 1 
Column totals     1 
Probability 
6-1] JOINT PROBABILITY FUNCTION 219 
We have: 
E(Y) = o x +  x +  x --3 x  
-4 X- +6 X-+$ X - 
24- 2 
E()=0{+}-4-2{= , 
and 
EXERCISES FOR SECTION 6-1 
1. Refer to Table 6-4. (a) Show thatP(XY = 2) = 4 andP(XY = 4) = 
- (b) Compute P(� < 21Y  1) (c) Find P(X - Y 3) (d) Compute 
(.�/x = 1). 
2. Compare E(XY) with E(X) .E(Y) and compare E(X--Y) with 
E(X) q- E(Y) if X and Y have the following joint probability distribution' 
Values of X 
OXV 
0 I totals 
Values 0 0   
Column totals a  
3. In Exercise 2 above, let Z = XY and construct the probability distribu- 
tion of Z. (That is, what are the values that Z may have and what are their 
probabilities?) Compute E(Z) from this distmbution. 
4. Construct the probability function of IV = X - Y from the table in 
Exercise 2. Compute E(W) from this function. 
5. Prediction and dependence. Show that X and ' are dependent if they 
have the following joint probability distribution: 
Values of X 
Poxv 
0 1 totals 
Values 0 0.1 0 3 0 4 
of Y 1 0.4 0 2 6 
Column totals 0.5 0 5 1.0 
990 JOINT AND CONTINUOUS DISTRIBUTIONS [CHAP. 6 
6. Look at the marginal probability distribution of Y in Exercise 5. Since 
the probability of 1 is greater than the probability of 0, in the absence of any 
other information our prediction of Y would be 1, and we would be right 60% 
of the timc. (a) Find the conditional probabilities 
?(r = olx = o), ?(r = = o), ?(r = o1� = 
(b) XVhat value of Y would you predict on the basis of these probabilities, 
knowing thatX = 07 IfX = 17 (e) Sinee the possible values X = 0, X = 1 
occur with equal probabilities 0.5 and 0.5, what is the expected percent of correct 
predictions of values of ' that you would make, given X? 
7. (Continuation.) In Exercise 6 above, reverse the roles of X and Y. Can 
you do better at predicting X, given Y, than you could without knowledge 
of l'? Compare the expected percents of correct predictions, knowing, and not 
knowing the value of Y. 
8. (Continuation.) Would there be an improvement n predicting X, given 
Y, if their joint distribution were as follows? 
X 
04 0.3 0.7 
0.2 0.1 0 3 
0.6 04 10 
9. (Continuation.) In Exercise 8 above, xxoul�l there be an improvement 
in predicting Y, given X? 
10. In the 3-coin example, Table 6-2 gives the frequency distribution for 
paired values of X and Y, number of heads and number of runs. (a) For which 
value (or values) of X is the corresponding value of Y completely determined? 
(b) I, there a value of }' for which the corresponding value of X is determined? 
(c) With no infornmtion about the value of Y, what is the probability that 
X is 2? (d) Given that Y = 1, what is the probability that X = 2? (e) Given 
that l' = 2, what is the probability that X = 2? 
11. A motorist tries to nnlock the door of his ear in the dark. He has 4 keys 
in his pocket, aud only one of them unlocks the car. Itc tries the keys, selecting 
one after another at random withont replacement. What is the expected number 
of keys that lie will try before lie gets the door uulocked? 
12. Answer Exercise 11 if lie samples with replacement. (Strictly speaking, 
the sample space is infinite, snee lie might never find the key.) 
6-2] PROBABILITY GRAPHS: PROBABILITIES REPRESENTED BY AREiS 221 
6-2. PROBABILITY GRAPHS FOR CONTINUOUS RANDOM VARIABLES: 
PROBABILITIES REPRESENTED BY AREAS 
The random variables discussed up to now have ordinarily had a finite 
number of possible values, each with positive or zero probability. 
An exceptional random variable occurred in the St. Petersburg paradox 
exercises, where a coin was tossed until a tail came up. Since there is no 
limit to the number of heads that can be tossed in succession, there are 
as many sample points as there are positive integers, i.e., an infinite 
number. These earlier distributions all had the features that each ad- 
missible point in the sample space had a probability either strictly positive 
or zero, and that the sum of the strictly positive probabilities was 1. 
Such distributions are called discrete distributions, to distinguish them 
from the contiuo,s distributions which we now introduce. 
In physical measurement problems, for example, it is convenient to 
think of the random variable as continuous. Length and time are typical 
of measurements that can, theoretically, take every possible value in an 
interval, however crudely we may choose to round off the values in prac- 
tice. Thus we associate possible values of these measures with all the points 
on an interval of the real line, the continuum. Here we want to assign 
probabilities not in chunks as in discrete distributions, but more smoothly. 
An example will show the need. 
Imagine the second hand on an electric clock that stops at your blind- 
folded command. What is the chance that it stops exactly at 10 seconds 
after a full minute? By "exactly," we mean 10.000... seconds with zeros 
carried on forever. Obviously, there is no chance at all. And the same 
thing is true for every possible number of seconds between 0 and 60. 
Yet the clock must stop somewhere; the probability that it stops is 1. Thus 
if we tried to add probabilities as usual, for one point at a time over the 
60 seconds, we would have nothing but zeros to add, and they cannot 
add to 1, though they must. 
But this difiSculty is not a new one. The area of a square one unit on a 
side is 1. Yet the area of every interior line segment perpendicular to 
its base is zero. Hence we cannot get the area of the square by adding 
areas of line segments. Nor can we get probability over the 60-second 
time interval by adding probabilities of individual points. 
The notion of areas signals a way around this problem. We assign 
probabilities to intervals, rather than to single points, and we do it by 
representing probabilities as areas over intervals. The following examples 
illustrate the method. 
EXAMPLE 1. Stopped cloc]c. If a clock stops at a random time, what 
is the probability that the hour hand stops between the numerals I and 57 
222 JOINT AND CONTINUOUS DISTRIBUTIONS [CHAP. 6 
Solution. The space between the numerals 1 and 5 is x52 of the circum- 
ference, so most people would say the probability is x 
EXAMPLE 2. Cut string. A child plays with a pair of scissors and a piece 
of string 8 inches long. He cuts the string in two. What is the probability 
that the longer piece is at least 6 inches long? 
Solution. We imagine the string laid out along the x-axis from 0 to 8. 
If the cut falls between 0 and 2 or between 6 and 8, the longer piece will 
be at least 6 inches long. The combined lengths of these two intervals is 
4 inches, so most people would say the probability is 84- or �. 
Each of these examples illustrates a random variable whose possible 
values form a continuous interval. In the clock example, the random 
variable T is a number from 0 through 12 indicatiHg the instant when 
the clock stopped. Any real value of T between 0 and 12 is possible. 
There are infinitely many possibilities; we wish to treat them all alike in 
assigning probabilities, but there is a difficulty that we haven't met before. 
We can't count equally likely cases, and probabilities have not been 
assigned. 
However, there is a way out of the difficulty. We assigH to each interval 
of values of T between 0 and 12 a probability proportional to the length 
of the interval. Since the entire interval from 0 to 12 has probability 1, 
aH interval of uHit length is assigned probability 1-%, and an interval 
of length 4 is assigned probability . Thus, as we said in the solution 
to Example 1, the probability that T will fall betweeH 1 arid 5 is 1' 
5--1 4 
P(1 _< T _< 5) ---- 12-- 0 = 1-' 
Likewise, in the string-cutting example, the random variable X repre- 
sents the length of the piece cut off between the end at 0 and the cut. 
Again, since X is to have the same chalices of falling in ally tWO intervals 
of equal length, wc assign to each interval a probability proportional to 
its length. The entire interval from 0 to S has probability 1; any shorter 
interval, say of leHgth d, is assigned probability d/8. The longer piece 
of string is at least 6 inches long if 0 _< X <_ 2 o1' 6 _< X _< 8 and 
since these are mutually exclusive intervals their probabilities add' 
P(o_<x< 2 or 6_<x_<$)=P(o_< x_< 2)+ 
2--0 8--6 
= 8--7-6 + 8--7-6 = 
6-2] PROBABILITY GRAPHS: PROBABILITIES REPRESENTED BY AREAS 223 
We wish to represent such results graphically. We do so by representing 
probabilities by areas. 
Clock e.:ample; area probability graph. We return now to the clock 
example, and construct what we shall call its area probability graph. The 
total area under this graph is to be 1. Its base should extend from 0 to 
12 along the T-axis. Because the clock can stop at random at any point 
between 0 and 12, we want to assign equal probabilities to any two time 
intervals that have equal widths. If their probabilities are to be measured 
by areas, the altitudes of the corresponding rectangles should have equal 
lengths. Thus, since the total area under it is to be 1, the area graph 
is bounded above by a line parallel to the T-axis and at distance � above 
the axis. Figure 6-2 shows the area graph. The shaded area represents 
the probability that the clock stops between I and 5. The area of the 
rectangle is (5 -- 1)/12  �. 
I2 
0 I 5 12 
Fc. 6-2. Area probability graph for stopped clock. 
Strig example. The area probability graph for the string-cutting 
example is shown in Fig. 6-3. The base extends along the X-axis from 
0 to 8 and the height above the axis is . Thus the total area is equal to 1. 
The shaded areas represent the probability that the longer piece of string 
is at least 6 inches long. The sum of these two areas is �, which is the 
desired probability. 
0 2 6 8 
F6. 6-3. Area probability graph for the string examplc. 
EXAMPLE 3. Commuter. A commuter who drives to work from his 
home in the suburbs tries to reach his office by 9 o'clock. Because of 
fluctuations in traffic and other factors, he actually arrives between 8:45 
and 9:05. The relative frequencies of his various arrival times suggest 
to him that the area probability graph for his time of arrival is approxi- 
mated very well by an isosceles triangle If this is true, what is the 
probability that he will arrive at work on time? 
Solution. Let his time of arrival be represented by T, a random variable. 
To simplify the writing, let T = 0 correspond to 8:55, the midpoint of 
224 JOINT AND CONTINUOUS DISTRIBUTIONS [CHAP. 6 
his arrival times. Then his are probability graph is an isosceles triangle 
with base from T = --10 to T = -10, and vertex above T = 0 at a 
distance h that will make the total area of the triangle equal to 1' 
2 -- 
The graph is shown in Fig. 6-4. The area of the small shaded triangle 
represents the probability of his arriving after 9 o'clock; the altitude is 
_1_ and the shaded area is (�)5(6) -. Therefore the probability that 
20, -- 
he will arrive on time is 1 --  -- . 
T 
--10 0 5 10 
FG. 6-4. Area probability graph for commuter's time of arrival at work. 
EXERCISES FOR SECTION 6-2 
1. In the stopped-clock example, what s the probability that the clock 
stops beteen S:13 and 9:45? 
2. In the cut-string example, what is the probability that the shorter piece 
is at least 2 inches long? 3 inches? 3.9 inches?  inches? 
3. In the cut-string example, what is the probability that the longer piece 
is (a) at least twice as long as the shorter? (b) At least a inches, where 
4a87 
4. In the commuter example, what is the probability that he arrives at least 
10 minutes early? Not more than 5 minutes ealy or late? 
6-3. CUMULATIVE PROBABILITY GRAPHS 
If 2 dice are thrown, xvhat is the probability that the sum is less than 
or equal to 5? If 3 coins are tossed, what is the probability that at most 
2 show heads? If a whole number between I and 10 is chosen at random, 
what is the probability that it has at most 3 divisors? 
Questions like these have the general form: What is the probability 
that a random variable takes on a value less than or equal to a prescribed 
number x? Answers to such questions are given by the cumdati,e distribu- 
tion function, which applies to continuous distributions as well as to those 
with a finite or countable number of points. 
6-3] CUMULATIVE PROBABILITY GRAPHS 225 
6-7 Definition. C,m,latwe dslribulion function (cdf). Let X be a 
random variable, x a real 12Ulnbel', and F(x) the probability that 
X takes on values less than or equal to x: 
�(.r) = P(x 
Then the function F defined by Eq. (1) is called the cumulative 
distribution function of X. 
EXAMPLE 1. TWO dice. Table 6-5 shows the possible total scores x 
when 2 dice are thrown, together with their probabilities f(x) and the 
cumulative probabilities F(.r) as given by Eq. (1). The cumulative dis- 
tribution function has a value, defined by Eq. (1), for every real number 
x, not just those listed in Table 6-5. For example, 
F(3.7) ---- P(X < 3.7) = f(2) m f(3) ---- F(3)  
-- ' -- 36' 
fact, if x is any number greater than or equal to 3 and less than 4, 
 The graph of F, showll ill Fig. 
__ X  4, then F(,r) : F(3) -- a6. 
TABLE 6--5. TWO DICE, PROBABILITY FUNCTION f 
AND CUMULATIVE DISTRIBUTION F. 
x f(x) F(x) 
<2 0 0 
2 --- -- 
36 36 
3 -- -- 
36 36 
4 -- --- 
36 36 
5 _4_ __o 
36 36 
36 36 
? -- __ 
36 36 
36 36 
9   
36 36 
10   
36 36 
36 36 
12   
36 36 
12  0 1 
226 JOINT AND CONTINUOUS DISTRIBUTIONS [CHAP. 6 
F(r) 
--2 0 1 2 4 6 8 10 12 14 16 
Fid. 6-5. Two dice. Graph of cumulative distribution function. 
6-5, consists of a sequence of horizontal line segments and two rays. One 
ray coincides with that part of the x-axis to the left of x ---- 2, because 
P(X _ x) is0ifx < 2. There is a jump in the graph at x ---- 2because 
F(x) ---- 0 to the left of 2 and F(2) -- . The graph has constant height 
F(x) i for 2 < x < 3. At x - 3, there is a jump amounting to 
-- 36 -- 
f(3) = 6 and bringing F(x) up to F(3) a As ve proceed along the 
-- 36' 
graph from left to right, we find a jump at each integer x = 2, 3, 4, 
5, 6, . . . , 12. For x  12, the graph has another ray extending indefinitely 
to the right because F(x) = P(X  x) is 1 if x is any real number greater 
than or equal to 12. 
The heavy dots on the horizontal line segments, above x = 2, 3, 4 
and so on, indicate that these line segments are closed at their left ends. 
The small open crescents at the right ends of the horizontal segments 
indicate that the segments are open at their right ends--they do not con- 
tain their right endpoints. The vertical dashed line through x = 8 
TABLE 6--6. THREE COINS: NUMBER OF HEADS, PROBABILITY 
FUNCTION, CUMULATIVE DISTRIBUTION FUNCTION. 
x f(x) F(x) 
<o o o 
1 1 
0 g 
8 8 
3 7 
2  
 1 
3 
>3 0 
6-3] CUMULATIVE PROBABILITY GRAPHS 227 
indicates that the value of F(8) is to be read from the left end of the higher 
horizontal segment where the heavy dot appears. 
EXAMPLE 2. Tlrec cotas. If three coins are tossed, the number of 
heads that show s a random variable X. Its probability function and 
cumulative distribution function are shown in Table 6-6. 
The graph of the cumulative distribution filnction is shown ill Fig. 6-6. 
Again we see that jumps occur at .r = 0, 1, 2, and 3. The jump ata' = 0 
is f(0) = , at 1 it is f(1): , at 2 it is f(2) = , and at 3 it is f(3) ---- . 
For negative x's, F(x) = 0; for x > 3, F(x) = 1. 
I 0 , 
0 1 2 3 
FI6. 6-6. Three coins. Cumulative distribution function for number of heads. 
EXAMPLE 3. Stopped clock. The area probability graph is shown in 
Fig. 6-2. The cumulative distribution function is defined by 
F(t) = P(T < t). (2) 
Discussion. The probability given by Eq. (2) is zero for t < 0 and 1 
for t > 12. For t between 0 and 12, the probab(lit!t is given by the area 
of a rectangle of base from 0 to t and of altiBude T; hence the area is t/12. 
Therefore F, the cumulative distribution function, may be described as 
follows: 
0, if t < 0, 
F(t) = t/12, ff 0 _< t _< 12, (3) 
[.1, if t > 12. 
The graph of F is shown in Fig. 6-7. To find the probability that the 
clock stopped between 0 and 3, we use F(3)  The probability that 
-- 12' 
5 1 1 
it stopped betweenland 5isF(5) -- F(1) -- x2 x2 -- a. 
EXAMPLE 4. oter. Figure 6-8 shows the area probability graph 
for the time before or after 8:55 that Mr. Commuter arrives at work. 
228 JOINT A\'D CONTINUOUS DISTRIBUTIONS [CHAP. 6 
1.0[ F = 1 
F=0 I 
0 t 12 
FG. 6-7. Stopped clock. Cumulative distribution function for T. 
t 
--10 ! 0 10 
FG. 6-8. Commuter example. Area probabi]ity graph. 
We now study the cumulative distribution function F defined by 
F(t) ---- P(T _< t). (4) 
We recall that T here represents the number of minutes from 8:45 until 
Mr. C. arrives at work. 
For t less thaH --10, the probability given by (4) is zero. For a value of 
t between --10 arid 0, the probability is given by the area of a triangle 
like the one shown shaded in Fig. 6-8. From similar triangles, we have 
h 1/10 1 
t - 10 10 100 
Hence 
h= t-f- 10 
100 (5) 
Check: ift = --10, Eq. (5) givesh = 0;ft = 0, h =  These results 
T' 
agree with what we know is correct, arid since (5) is linear, it is also correct 
for --10 < t < 0. Therefore the area of the shaded triangle is �(t - 10)h, 
or 
x(t q- 10)(t q- 10) (t q- 10) 2 
F(I) = 2 -- , --10 < t < 0. (6) 
100 200 -- -- 
For t between 0 and 10, compute the area of the small triangle to the 
right of t, and subtract the result from 1. We omit the calculation, xvhich 
6-3] CUMULATIVE PROBABILITY GRAPHS 229 
again makes use of similar triangles. The result is 
F(t) = 1 (10 - t) 2 
--20' 0 < t < 10. (7) 
For t > 10, F(T) = 1. Combining the several bits of informatio1 
about F(t), we have the following description of F: 
I 0, for t < --10, 
(t - 10) 2 
:200 ' for --10 _< t _< 0, 
F(t) = (8) 
1 (10 -- t) 2 for 0 < t < 10, 
200 ' -- -- 
1, for t > 10. 
The graph of F is shown in Fig. 6-9. 
10 F= 1 
0.5 
F=0 _ I 
-10 o 10 
Fro. 6-9. Commuter example. Cumulative distribution function for T. 
 -- ? and this is 
For example, F(5) = 1 -- (10 -- 5)2/200 = 1 8 -- 7, 
the probability that he arrives at a time T _< +5, i.e., by 9 o'clock. 
- -- - is the probability that he arrives 
Similarly, F(10) --F(0)= 1- 2- 2 
between 8:55 and 9:05. 
EXERCISES FOR SECTION 6-3 
1. In Example 2, for 3 coins, what is the probability that the number of 
heads is between x/ and r? Is it equal to F(r) -- F(X/)? 
2. In Example 1, for 2 dice, what is the probability that the sum is between 
2.3 and 9.4? Is it equal to F(9.4) -- F(2.3)? 
230 JOINT AND CONTINUOUS DISTRIBUTIONS [CHAP. 6 
3. Let X be a random variable whose values are all between 0 and 10 and let 
F be its cumulative distribution function. What is the numerical value of 
F(11) -- F(--1)? Why? 
4. Sketch the graph of the cdf for the number of divisors of an integer selected 
at random from 1 through 10. (See Table 5-5.) Where does it have jumps, 
and how big are they? 
5. For its area probability graph, a random variable X has a right triangle 
of base 5 running from X = 2 to X = 7. The hypotenuse slopes upward to 
the right. Sketch its graph and find (a) P(X _< 4), (b) P(X > 5), and 
(c) F(x) = P(X _< x) in these three cases: 
(i) x < 2, (ii) 2 _< x _< 7, (iii) x > 7. 
6-4. THE NORMAL CURVE AND THE NORMAL PROBABILITY DIS- 
TRIBUTION 
The most important probability distribution in the xvhole field of 
probability and statistics is the normal probability distribution. In the 
present section, we shall describe what this normal probability distribution 
is, how we use it, and why it is so important. 
What is the normal probability distribution? It is a special way of assign- 
ing probabilities to intervals of real numbers associated with continuous 
random variables. These probabilities are assigned by means of a special 
curve, called the normal curve, and are related to a special kind of random 
variable, called the standard normal random variable. The procedure is 
clarified by the following definition. 
6-8 Definition. Star, dard normal random variable. Let X be a random 
variable whose possible values are the real numbers between --co 
and +co. Then X is called a standard normal random variable if 
the probability assigned to the interval from a to b is the area from 
a to b between the x-axis and the normal ctrve, whose equation is 
(1) 
Graph of the standard normal curt,e. The graph of Eq. (1) can be ob- 
tained in the usual way by constructing a table of values. We recognize 
7r as an old friend: 
7r = 3.14159... 
The number e (like w, an important irrational number) is the base of the 
natural logarithms: 
e = 2.71828... 
6-4] THE NORMAL CURVE 231 
With these facts in mind, a table of logarithms is sufficient to enable 
us to compute the y that matches a given value of z. Table 6-7 gives 
coordinates of points on the normal curve for values of .r between --4.00 
and -+-4.00, at intervals of 0.25. 
TABLE 6-7 
COORDINATES OF POINTS ON THE STANDARD NORMAL CURVE 
1 _2/2 
x y x y x y 
0 00 3989 4-1 50 .1295 4-3 00 .0044 
4-0 25 .3867 4-1.75 .0863 4-3.25 .0020 
4-0 50 .3521 4-2 00 .0540 4-3 50 .0009 
4-0 75 .3011 4-2 25 .0317 4-3.75 .0004 
4-1 00 .2420 4-2 50 .0175 4-4.00 .0001 
4-1.25 1826 4-2 75 0091 
If we plot the points (x, y) whose coordinates are given in Table 6-7 
and then draw a smooth curve through them, we get the normal curve 
shown in Fig. 6-10. 
Properties of the stardard normal crve. From an inspection of the graph 
in Fig. 6-10, we note that the normal curve: 
(a) is symmetric about the y-axis; 
(b) has its highest point at (0, 1/v/), vhere 1/v/- = 0.40; 
(c) is concave downward between x ---- --1 and x ---- -+-1, and concave 
upward for values of x outside that interval; 
04' 
--3 --2 --1 0 1 2 3 
1 
FG. 6-10. Normal curve: y -  e -xU2. 
232 JOINT AND CONTINUOUS DISTRIBUTIONS [CHAP. 6 
(d) extends without limit to the left and to the right, and approaches 
the x-axis yew rapidly as we move away from x ---- 0 in either direction. 
We shall accept on faith a further property of the normal curve that 
can be proved by advanced calculus: 
(e) the total area under the curve and above the x-axis equals 1. 
Property (e) corresponds to the fact that 1 is the probability that the 
standard normal random variable X takes a value between -- and -. 
Properties (a) and (e) imply that the area below the curve and to the 
left of the y-axis is �, and so is the area to the right of the y-axis. 
How do we use the orrnal probability distribittio? The definition tells 
us what probabilities are to be assigned to the standard normal random 
variable. For example, the probability that X takes a value in the interval 
from x ---- a to x = b is equal to the measure of the area bounded by the 
normal curve, the x-axis, and the vertical lines x = a and x = b as indi- 
oated by the shaded region in Fig. 6-11. 
y 
a 0 b 
FIG. 6-11. P(a _< X _< b) = area of shaded region. 
How do we determine the measures of such areas? Recall that the 
total area under the normal curve is 1. Partial areas of the type shown 
in Fig. 6-11 can be approximated by rectangles. However, in practice 
we shall use Table 6-8 (which gives areas from 0 to x at intervals of 0.1, 
where 0 _< x _< 4.0) or the larger Table III at the back of the book. 
The following examples show how Table 6-8 is used to find probabilities. 
EX^Mp 1. What is the probability that a standard normal random 
variable takes a vahie between 0 and 1 ? 
Solutior. P(0 < X < 1) = A(1) = 0.3413. About 34% of the total 
probability is between 0 and 1 and, by symmetry, about 68% is between 
--1 and -bl. 
6-4] THE \'ORMiL CURVE 233 
T.LE 6--8 
\I{E_ LLXDEI, TIlE STA\'DARD NORMAL CUI,VE FROM 0 TO X, 
 SHO'N SHiDEI), IS 
0 
x Area, 1 () x Area, .1 (x) 
0 0 0000 .'2 1 4821 
0 1 0398 2 2 4861 
0 2 0793 2 3 4893 
0 3 1179 2 4 t918 
0 4 1554 2 5 4938 
0 5 1915 2 6 4953 
0 6 2257 2 7 .4965 
0 7 2580 2 S 4974 
0 S 2881 2 9 4981 
0 9 3159 3 0 4987 
1 0 3413 3 1 4990 
1 1 3643 3 2 4993 
1 2 3849 3 3 4995 
1 3 4032 3 4 4997 
1 4 4192 3 5 4998 
1 5 4332 3 6 4998 
1 6 4452 3 7 4999 
1 7 4554 3 8 4999 
1 S 4641 3 9 5000 
1 9 4713 4 0 5000 
2 0 4772 
EXAMPLE 2 What s the probabfilty that a standard normal random 
variable takes a value between --2 and -27 
,q'ol.tzoi. Since the normal curve s symmetric about the g-axis, the 
area from --2 to - 2 ia twice the area from 0 to 2' 
P(-2 < X < 2) -= (area fro1-n .r .... 2 to x = -2) 
= 2(area from x -- 0 to x --: +2) 
--- 2A(2) 
=- 2(0 4772) -- 0 9544 
Shghtly more than 9oe of the total area he over the interval betxxeen 
- 2 and - 2 
234 JOIXTT AND CONTINUOUS DISTRIBUTIONS [CHAP. 6 
EXAMPLE 3. Vhat is the pr;bability that a standard normal random 
variable takes a value between 0.3 and 3.2? 
Solution. The required probability is the area between 0.3 knd 3.2, 
and this is the area from 0 to 3.2 minus the area from 0 to 0.3. That is, 
P(0.3 _< X _< 3.2) ---- 2t(3.2) -- 2t(0.3) 
---- 0.4993 -- 0.1179 ---- 0.3814. 
EXAMPLE 4. Find P(--0.3 _ X _ 3.2). 
Solution. The desired probability is the area from --0.3 to 0 plus the 
area from 0 to 3.2. The area from --0.3 to 0 is the same as the area from 
0 to 4.0.3, A(0.3). Hence 
P(--0.3 _< X _< 3.2) = A(0.3)4. A(3.2) 
= 0.1179 4- 0.4993 ---- 0.6172. 
EXAMPLE 5. Find P(--3.2 _< X _< --0.3). 
Solution. Since the normal curve is symmetric about the y-axis, the 
area from --3.2 to --0.3 is the same as the area from 4.0.3 to 4.3.2, found 
in Example 3. Hence 
P(--3.2 _ X _ --0.3) = P(4.0.3 _ X _ 4.3.2) 
= A(3.2)- A(03) 
---- 0.3814. 
EXAMPLE 6. Find 
(a) e(x > 0.3), (b) (X > --0.3), (c) e(lxl > o.3). 
Solutions. (a) The area to the right of 0.3 is 0.5000 minus the area 
from 0 to 0.3' 
P(X > 0.3) = 0.5000 -- P(0 < X' < 0.3) 
= 0.5000 -- 0.1179 = 0.3821. 
6-4] THE NORMAT CURVE 235 
(b) The area to the right of --0.3 is equal to the area from --0.3 to 0 
plus 0.5000; hence 
P(X > --0.3) ---- P(--0.3 < X _ 0)  0.5 
-- ?(0 _ x < 0.3) -0.5000 
---- 0.1179  0.5000 ---- 0.6179. 
(c) P([X I > 0.3) = ?(X < --0.3) + ?(X > 0.3) 
= 2P(X > 0.3) 
---- 2(0 3821) = 0.7642. 
Why is the normal probability distribution important? In the first place, 
measurements of many things give rise to distributions that are ap- 
proximately normal. For example, the Pearson-Lee data on father-son 
statures provides the data used in plotting Fig. 6-12 showing the distri- 
bution of heights, in inches, of 1078 sons. 
_180 
]60 
740 
120 
100 
I � 
] 40 
, , I I Z A__I_ _ 
60 68 78 
Fo. 6-12. Statures of sons, in inches. 
The distribution of heights has a bell-shaped appearance that reminds 
us of the normal curve. Other examples of measurements that have 
approximately normal distributions include diameters of machined parts, 
lengths of tobacco leaves, IQ scores, and College Board aptitude and 
achievement test scores. 
236 JOINT AND CONTINUOUS DISTRIBUTIONS [CHAP. 6 
Since probabilities are readily available for the normal distribution, 
we have at hand a convenient method for obtaining approximations for 
probabilities in many other distributions� {oreover, some random 
variables, although not normally distributed, can be subjected to trans- 
formations that yield nexv random variables which are exactly or ap- 
proximately normally distributed� The result is a new ease of calculation 
of probabilities� 5Ve shall see an important example of this feature when 
we study the binomial probability distribution in Chapter 7. 
In the second place (and this is one of the most important reasons), 
under very general conditions, sums of random variables, normal or non- 
normal, are approximately normally distributed. For instance, if the 
random variables are independent and if we know only their means and 
standard deviations, it is possible to make accurate probability calcula- 
tions for the distribution of their sum, from the normal table. We return 
to this feature in Chapters 7 and 9. 
Mear and variance. The mean of the standard normal distribution is 
0 and its variance and standard deviation are both 1' 
 = O, 2= 1,  = 1. (2) 
It is evident from the symmetry of the normal curve that the mean is 
zero, but it is not obvious that the variance is 1. It requires calculus to 
prove that 2 _-- 1, but we can approximate 2 in the following xvay. 
We approximate the continuous normal random variable X by a random 
variable Z that takes on only the 17 values 
8 3 o 1 1 2 3 8 
2' , --, --, O, 
' ' ' --' , ' 2, � ' ' " 
These values arc the nmltiples of � between --4 and--4. Since practically all 
of the probabfiity of X is between --4 aud -4, we stop at these boundaries for 
g. With each value of g we associate the probability under the norma] eurve 
in a band of vidth one-half centered at the value. For example, with the value 
z = � we associate the probability of the standard normal distmbution from 
x = �rex = ,namely0.1747. , 
Now, Vat (Z) = E(Z- tz)  = E(Z=), since z = 0. If we square each 
positive value of z, multiply by its probability, acid these products, and then 
double that result to take account of the corresponding negative values of z, 
we get the variance of Z: 
Var (Z) = 1.021. (3) 
Naturally we don't expect the variances to be exactly the same for the discrete 
random variable Z and the continuous normal random variable X. Equation 
(3) does, howewq', suggest that the variance of X should be near 1; t is, in 
fact, exactly 1. 
6-4] THE XORMAL CURVE 237 
TABLE 6--9 
PROBABILITY DISTRIBUTION OF DISCRETE RANDOM VARIABLE 
z APPROXIMATING THE NORMAL. 
Values, Probabilities, 
z P(z) = A(z + 4) -- 4(z -- �) z 2 z2p(z) 
0 2(.0987 -- 0000) = 1974 0 0 
q- .50 2734-- .09S7 = .1747 25 .0437 
4-1.00 3944-- .2734 = .1210 1.00 .1210 
4-1.50 .4599 -- .3944 = .0655 2 25 .1474 
4-2 00 .4575 -- .4599 = .0279 4.00 .1116 
4-2 50 .4970 -- .4575 = .0092 6.25 .0575 
4-3 00 .4994-- 4970 = .0024 9.00 .0216 
4-3 50 .4999-- .4994 = .0005 12.25 .0061 
4-4.00 .5000 -- 4999 = 0001 16.00 .0016 
Total = .5105 
2 
z  2(Total) = 2(.5105) = 1.021 
Other normal random variables. We have thus far discussed the standard 
normal random variable with mean equal to 0 and standard deviation 
equal to 1. Questions about a normal random variable Y, with mean 
/r and standard deviation or can be answered by translating them into 
questions about the related random variable 
y -- 
O'y 
The mean and variance formulas of Chapter 5 hold for continuous random 
variables as well as for those with a finite number of values. Consequently 
this new random variable has mean 0, because 
x- = E( Y -- /r) 1 E(Y y) O. 
O'y fly 
The variance of X is 
 _-- Var[Y -- /r] 1 Var (Y /r) 1 
238 JOINT AND CONTINUOUS DISTRIBUTIONS [CHAP. 6 
Moreover, X is a normal random variable, too, and since its mean is 0 
and its variance is 1, it is a standard normal random variable. 
The following example shows how the standard normal table is used 
to answer questions about nonstandard normal random variables: 
EXAMPLE 7. Mr. Commuter has a statistician friend who believes 
that Mr. Commuter's time of arrival, measured in minutes after 8:55, is 
approximately a normal random variable T, with mean 0 and standard 
deviation 2.5. If this approximation is valid, vhat is the probability that 
Mr. Commuter arrives between 8:50 and 9:00 o'clock? 
Solution. The given clock times correspond respectively to T = --5 
and T ---- 4-5. The probability that T lies between --5 and 4-5 is 
P(--5 _< T _< 5). 
The statement 
--5T5 
is equivalent to 
--5--0 T--0 5--0 
2.5 -- 2.5 - 2.5 
or to 
-2 _< x <_ +2, 
where 
T--0 
X---- 
2.5 
Since T is normal with mean 0 and standard deviation 2.5, X is normal 
with mean 0 and standard deviation 1, i.e., X is a standard normal random 
variable. From Table 6-8, 
P(--2 __< X <_ 4-2) = 2(0.4772) ---- 0.9544. 
Hence, under the assumptions, Mr. Commuter arrives at work between 
8'50 and 9:00 o'clock about 95o of the time. 
In what follows in the text, we shall use the norlnal distribution (a 
continuous distribution) to approximate discrete distributions. On the 
other hand, approximations go both xvays. Discrete distributions on 
finite sample spaces can ordinarily approximate continuous distributions 
quite closely, if the discrete sample points are sufficiently numerous and 
appropriately placed. The xvord "finite" does not restrict the number of 
possible sample points to 10, 10 �, or even 10 ��. Therefore we do not 
hesitate to apply our discrete theory to continuous measurement problems. 
6-4] T NORMAL CURVE 239 
We are justified in three ways: (a) discrete distributions can be made 
to approximate continuous ones for most propertics that we study; (b) 
many of our results for discrete variables hold exactly for continuous 
variables as well, or they have suitable analogues; and (c) practical 
measurement is ordinarily discrete rather than continuous. 
EXERCISES FOR SECTION 6-4 
1. Chcck that 1/x/ is close to 0.4. Note that the ordinate of the normal 
at x = 0 is 1/x/-  0.4. 
2. From your normal tables find the arca above the interval froin 0 to h 
for h -- 0 01, 0.1, 0.2, 0.3, 0.4, 0.5. Compare these valucs xvith 0.4h. How big 
must h be before the difference is 10%? 
3. If h is ver 5 sinall, the area under the standard normal curve from a to 
a -]- h may be approximatcd by the area of a rectangle of base h and altitude 
equal to the normal ordinate at x = a. For example (from Table 6-7), the 
ordinate at x = 1.00 is 0.2420. Compute P(1.00 _ X _ 1.00--h) directly 
from Table 6-8 and compare the results with 0.2420h for (a)h--0.1, 
(b) h = 0.2, (c) h = 0.5, (d) h = 1.0. 
4. Although we can't find the area under the normal curve without advanced 
calculus methods, we can approximatc it closely by summing areas of rectangles. 
Consider a grid on the x-axis: 
5 4 3 2 1 O, 1 2 3 
' ' ' , 2' 2 2 2, 2' 2 2' 2 ' ' ' 
or, more gencrally, vith points at n/2, vhere n is any integer--positive, negative, 
or zero. On the interval betwcen two adjacent grid points ercct a rectangle 
whose height is the value of the normal ordinate at the midpoint. Then if f(x) 
is a normal ordinate, the sum of the arcas of the rectanglcs is 
nl 
(a) Explain why ve need sum only over positive valucs of n. (b) Look up the 
ordinates at n/4, n = 1, 3, 5, . . ., as far as your table goes, add thein, and 
pare the sum with 1, the true normal area. 
5. Do Exercise 4 vlth a grid on the x-axis at the intcgers 
..., --4, --3, --2, --1, 0, 1, 2, 3, 4, ... 
6. Use a table of logarithms to compute the ordinate y of the point on the 
normal curve 
1 --x2/2 
atx = 0, atx = 1, andatx = 2. 
240 JOINT AND CONTINUOUS DISTRIBUTIONS [CHAP. 6 
7. If X is a standard normal random variable, find 
(a) P(X < 0.3), (b) P(X < --0.3), 
(c) P(1.2 < X < 3.6), (d) P(--1 < X < 2). 
8. If X is a standard normal random variable, find k such that P(X < k) is 
(a) 0.4, (b) 0.05, (c) 0.95, (d) 0.75. 
9. If X is a standard normal random variable, find ]c such that P(]X]  It) is 
(a) 0.4, (b) 0.8, (c) 0.9, (d) 0.05. 
10. The College Entrance Examination Board test scores are scaled to ap- 
proximate a normal distribution with mean g = 500 and standard deviation 
 -- 100. (a) What is the probability that a randomly selected student will 
score 700 or more? 580 or less? (b) What is the probability that 3 randomly 
selected students will all score 700 or more? That at least 2 of the 3 will score 
less than 700? 
11. Make up three problems about CEEB scores, similar to those in Exercise 
10, and solve them. 
12. Use Table 6-8 to show that the "empirical rule" entries in Table 5-14 
apply to distributions that are approximately normal. 
CHAPTER 
REPEATED TRIALS WITH TWO 
TYPES OF OUTCOMES: THE 
BINOMIAL DISTRIBUTION 
7-1. EXAMPLES OF BINOMIAL EXPERIMENTS 
Some experiments are composed of repetitions of independent trials, 
cach with two possiblc outcomes. The binomial probability distribution 
may describe the variation that occurs from one set of trials of such a 
binomial experiment to another. Wc devote a chapter to the binomial 
distribution not only because it is a mathematical model for an enormous 
variety of real life phcnomena, but also because it has important properties 
that recur in many othcr probability models. We begin with a few 
examples of binomial experiments. 
Marksmanship exmnple. A trained marksman shooting five rounds at 
a target, all under practically the same conditions, may hit the bull's-eye 
from 0 to 5 times. In repeated sets of five shots his numbers of bull's-eyes 
vary. What can we say of the probabilitics of the different possible 
numbers of bull's-eyes? 
Inheritance in mice. In litters of eight mice from similar parents, the 
number of mice with straight instead of wavy hair is an integer from 
0 to 8. What probabilities should be attached to these possible outcomes? 
Aces (ones) with three dice. Whcn thrce dice are tossed repeatcdly, what 
is the probability that the numbcr of aces is 0 (or 1, or 2, or 3)? 
General binomial problem. 5lore generally, suppose that an experiment 
consists of a number of independent trials, that cach trial results in either 
a "success" or a "ron-success" ("failure"), and that the probability of 
success remains constant from trial to trial. In the examples above, the 
occurrence of a bull's-eye, a straight-haired mouse, or an ace could be 
called a "success." In general, any outcome we choose may be labeled 
"SUCCESS." 
242 TWO TYPES OF OUTCOMES; BINOMIAL DISTRIBUTION [CHAP. 7 
The major question in this chater is: What is the probability of exactly 
a ' successes in n trials? 
In Chapters 3 and 4 we answered questions like those in the examples, 
usually by counting points ill a sample space. Fortunately, a general 
formula of wide applicability solves all problems of this kind. Before 
deriving this formula, we explain what we mean by "problems of this kind." 
Experiments are often composed of several identical trials, and some- 
timcs experiments themselves are repeated. In the marksmanship 
example, a trial consists of "one round shot at a target" with outcome 
either one bull's-eye (success) or none (failure). Further, an experiment 
might consist of five rounds, and several sets of five rounds might be 
regarded as a super-experiment composed of several repetitions of the 
five-round experiment. If three dice are tossed, a trial is one toss of one 
die and the experiment is composed of three trials. Or, what amounts 
to the same thing, if one die is tossed three times, each toss is a trial, and 
the three tosses form the experiment. Mathematically, we shall not 
distinguish the experiment of three dice tossed once from that of one die 
tossed three times. These cxamples are illustrative of the use of the words 
"trial" and "experiment" as they are used in this chapter, but they are 
quite flexible words and it is well not to restrict them too narrowly. 
EXAMPLE 1. Stedcnt f(ot)all 7rlana7�'8. Ten students act as managers 
for a high-school football team, and of these managers a proportion p 
are licensed drivers. Each Friday one manager is chosen by lot to stay 
late and load the equipment on a truck. On three Fridays the coach 
has needed a driver. Considering only these Fridays, what is the proba- 
bility that the coach had drivers all 3 times? Exactly 2 times? 1 time? 
0 time? 
Disctssio. 1X'ote that there are 3 trials of interest. Each trial consists 
of choosing a student manager at random. The 2 possible outcomes on 
each trial are "driver" or "nondrivel'." Since the choice is by lot each week, 
the outcomes of different trials are independent. The managers stay 
the same, so that p - P (driver) is the same for all weeks. We now 
generalize these ideas for general binomial experiments. 
For an experiment to qualify as a bioil experiment, it must have 
foul' properties: 
(1) there must be a fixed number of trials, 
(2) each trial must result in a "success" Ol � a "failure" (a binomial trial), 
(3) all trials must have identical probabihties of success, 
(4) the trials must be independent of each othel'. 
Below we use our earlier examples to describe and illustrate these four 
7-1] EXAMPLES OF BINOMIAL EXPERIMENTS 243 
properties. We also give, for each property, an example xvhere the prop- 
erty is absent. The language and notation introduced are standard 
throughout the chapter. 
1. There mst be a fixed monbcr n of repeated lrzals. For the marksman, 
we study sets of five shots (n = 5); for the mice, we restrict attention to 
litters of eight (n = 8); and for the aces, we toss three dice (, = 3). 
Experimett witho,t a fixed rottuber of trials. Toss a die until an ace 
appears. tiere the number of trials is a random variable, not a fixed 
nmnber. 
2. Binomial trials. Each of the n trials is either a success or a failure. 
"Success" and "failure" are just convenient labels for the two categories 
of outcomes when we talk about binomial trials in general. These words 
are more expressive than labels like "A" and "not-A." It is natural 
from the nml'ksman's viewpoint to call a bull's-eye a success, but in the 
mice example it is arbitrary which category corresponds to straight hair 
in a mouse. The word "binomial" means "of two names" or "of two 
terms," and both usages apply in our work' the first to the names of 
the two outcomes of a binomial trial, and the second to the terms p and 
(1- p) that represent the probabilities of "success" and "failure." 
Sometimes when there are many outcomes for a single trial, we group 
these outcomes into two classes, as in the example of the die, where we 
have arbitrarily constructed the classes "ace" and "not-ace." 
Experiment without the two-class property. We classify mice as strm,ht- 
haired or xavy-halred," but a hairless mouse appears. We can escape 
from such a difficulty by ruling out the animal as not constituting a trial, 
but such a solution is not always satisfactory. 
3. All trials hare idetical probabilities of s,ecess. Each die has proba- 
bility p:  of producing an ace; the marksman has some probability 
p, perhaps 0.1, of making a bull's-eye. Note that we need not know the 
value of p, for the experiment to be binomial. 
Experimett where p is not co,statt. During a round of target practice 
the sun comes from behind a cloud and dazzles the marksman, lowering 
his chance of a bull's-eye. 
4. The trials are i,dependent. Strictly speaking, this means that the 
probability for each possible outcome of the experiment can be computed 
by multiplying together the probabilities of the possible outcomes of the 
single binomial trials. Thus in the three-dice example P (ace) = p = , 
 and he ildepeldence assumption implies 
P (not-ace) = 1 -- p = v, 
that the probability that the three dice fall ace, not-ace, ace in that order 
is ()(-)(). Experimentally, we expect independence when the trials have 
nothing to do with one another. 
Examples where independence fazls. A family of five plans to go together 
either to the beach or to the mountains, and a coin is tossed to decide. 
244 TVO TYPES OF OUTCOMES: BINOMIiL DISTRIBUTION- [CHAP. 7 
We want to know the number o people going to the mountains. When 
this experiment is viewed as composed of five binomial trials, one for each 
member of the family, the outcomes of the trials are obviously not inde- 
pendent. Indeed, the experiment is better viewed as consisting of one 
binomial trial for the entire family. The following is a less extreme example 
of dependence. Consider couples visiting an art museum. Each person 
votes for one of a pair of pictures to receive a popular prize. Voting for 
olle picture may be called "success," for the other "failure." An experi- 
ment consists of the voting of one couple, or two trials. In repetitions 
of the experiment from couple to couple, the votes of the two persons in a 
couple probably agree more often than independence would imply, be- 
cause couples who visit the museum together are more likely to have 
similar tastes than are a random pair of people drawn from the entire 
population of visitors. Table 7-1 illustrates the point. The table shovs 
that 0.6 of the boys and 0.6 of the girls vote for picture A. Therefore, 
under independent voting, 0.6 X 0.6 or 0.36 of the couples would cast two 
votes for picture A, and 0.4 X 0.4 or 0.16 would cast two votes for picture 
B. Thus in independent voting, 0.36 q- 0.16 or 0.52 of the couples would 
agree. But Table 7-1 shows that 0.45 q- 0.25 or 0.70 agree, too many for 
independent voting. 
TABLE 7--1. COUPLES VOTING FOR PICTURES A AND 
Girls' votes 
A B 
Boys' A 0 45 0 15 0.6 
votes B 0.15 0.25 0 4 
06 0.4 1 
Each performance of an n-trial binomial experinmnt results ill SOlale 
xxhole lmmber from 0 through n as the value of the random variable 
where 
X = total number of successes in , binomial trials. 
We want to study the probabilitj ftnction of this ralldona variable. For 
example, xxe are interested in the iratuber of bull's-eyes, not which shots 
were bull's-eyes. A binomial experilnent can produce random variables 
other than the lmmber of successes. For example, the marksman gets 
5 shots, but we take his score to be the iratuber of shots before his first 
bull's-eye, that is, 0, 1, 2, 3, 4 (or 5, if he gets no bull's-eye). Thus we do 
7--]] EXAMPLES OF BINOlIIAL EXPERIMENTS 245 
not score the numbel- of bull's-eyes, and the random variable is lOt the 
number of successes. 
The constancy of p ald the independence are the conditions most 
likely to give trouble in practice. Obviously, very slight changes ill p 
do not change the probabilities lnuch, and a slight lack of independence 
may not lnake an appreciable difference. (For instance, see Example 2 
of Section 5-5, on red cards in hands of 5.) On the other land, evel whell 
the binomial lnodel does not describe well the physical phenomenon being 
studied, the binomial model may still be used as a baseline for ('omparative 
purposes; that is, we lnay discuss the phenomenon in terms of its de- 
partures from the binomial model. 
To szmmarze: A binomzal e.cperiment consists of  (_1) 
independent bilomial trials, all with the same probability 
p (0 _ p _ 1) of yielding a success. The outcome of the ex- 
periment is X successes. The random variable X takes the 
values . ---- 0, 1, . .., n with probabilities P(X - z) or, more 
briefly P(z). 
Wc shall find a formula for the probability of exactly .- successes for 
given values of p and n. When each number of successes x is paired ith 
its probability of occurrence P(x), the set of pairs (.c, P(x)), :----0, 
1,..., n, is a probability function called a binomial distrib,tio. The 
choice of p and n determines the binomial distribution uliquely, and 
different choices always produce different distributions (except when 
p ---- 0; then the number of successes is always 0). The set of all binomial 
distributions is called the family of binomial distributions, but in general 
discussions this expression is often shortened to "the binomial distribu- 
tion," or even "the binomial" when the context is clear. Binomial 
distributions were treated by James Bernoulli about 1700, and for this 
reason binomial trials are sometimes called Bernoulli trials. 
Random variables. Each binomial trial of a binomial cxpcrimclt pro- 
duces either 0 or 1 success. Therefore each binomial trial can be thought 
of as producing a value of a random variable associated with that trial 
and taking the values 0 and 1, with probabilities q and p respectively. 
The several trials of a binomial experiment produce a lCW randolu variable 
X, the total number of successes, which is just the sum of the random 
variables associated with the single trials. 
EXMr.r 2. The marksman gets two bull%-cycs, one on his thid 
shot and one on his fifth. The numbers of successes on the five individual 
shots are, then, 0, 0, l, 0, 1. The number of successes o each shot is a 
value of a random variable that has values 0 or 1, and there arc 5 such 
random variables here. Their sum is X, the total nulnbcr of successes, 
which in this experimelt has the value . ---- 2. 
246 TWO TYPES OF OUTCOMES: BINOMIAL DISTRIBUTION [CHAP. 7 
]Ve turn now to another simle example that illustrates the features 
of a binomial experiment and its associated binomial distribution. 
EXAMPLE 3. Binomial experiment with three thumbtacks. W, hen a 
thumbtack is tossed it can land point up (U), or point down (D). Suppose 
three thumbtacks: 1 red, i white, 1 blue, but otherwise alike, are tossed. 
We want the probability function for the random variable X, the number 
of tacks landing U. The sample space of possible outcomes for the three 
thumbtacks contains 8 sample points, with associated values of X, and 
probabilities as shown in Table 7-2. The first, second, and third letters 
of any sample point indicate in order the outcomes for the red, white, 
and blue tacks. Thus UDD means that the red tack landed point up, 
and the other two landed point down. 
TABLE 7--2 
Sample point X, number 
of U's Probability 
DDD 0 q3 
D D U i pq2 
DUD ' i pq2 
UDD i pq2 
DUU 2 p2q 
UDU 2 p2q 
UUD 2 p2q 
UUU 3 pa 
Let the probabilities of U and D be p and q, respectively, where 
p-q= 1. Thenwehave 
P(U) -- p, P(D) = q. (1) 
We assume that outcomes on the three tacks are independent. Hence 
the probability assigned to any one of the sample points is obtained by 
multiplying three probabilities. For instance, 
P(UDU) = P(U). P(D). P(U) ----- pqp-. 22� 
We let b(x; 3, p) denote the probability of getting x U's when there 
are three tacks each with probability p of landing U. [For b(x; 3, p) read 
"b of x when n ---- 3 and probability of success is p."] We can summarize 
7--1] EXAMPLES OF BINOMIAL EXPERIMENTS 247 
the preceding results by writing the binomial distribution we have just 
obtained In the form of a probability table: 
Probability, b(x; 3, p) q3 3pq2 3p2q p3 
-- .-- (2) 
Value, x 1 2 3 
Since the coefficients 1, 3, 3, 1 in the top line of table (2) are binomial 
coefficients, we can write a formula for b(x; 3, p), as follows: 
b(x; 3, p) ---- () p'q 3-:, x ---- 0,1,2,3, (3) 
xvhere (3) = 3!/9:!(3 -- x)! is the number of permutations of x U's and 
(3 -- x) D's (Corollary 2-13). 
Although we derived Eq. (3) for the thumbtack problem, the formula 
is more general. The set of four probabilities arislug from fornmla (3), 
together with the associated values of .% form the binomial distribution 
for 3 independent trials, each having p as the probability of success. The 
probability table (2) also displays this binomial distribution. 
Note that the right-hand side of formula (3) is coinposed of two parts: 
(1) the binomial coefficient (), ,vhich counts the number of difere,t 
arrangements of exactly x successes and 3 -- , failures in 3 trials; 
(2) the factor pZqa-Z, which gives the probability for any one of the 
different ways of getting x successes and 3 - x failures. 
When we derive the generalization of formula (3) in the next section, 
we exploit the fact that binomial probabilities are always products of two 
such parts. 
Adding all of the probabilities given in formula (3) for x ---- 0, 1, 2, 3, 
we get 
q q- 3q2p q- 3qp 2 q- p. 
This sum is the binomial expansion of (q q- p)a, since 
(q q- p) 3 : q3 q- 3q2p q_ 3qp2 q- p3. (4) 
This is another reason for calling the set of probabilities obtained from 
formula (3) a "binomial" distribution. And because q q- p = 1 and 
13 = 1, Eq. (4) shows that the sum of the four probabilities is 1. 
248 TV70 TYPES OF OUTCOMES; BINOMIAL DISTRIBUTION [CHAP. 7 
We recall from Chapter 5 that another way to represent the proba- 
bilities in a binomial distribution is to make a graph. Thus, if we erect 
the ordinates qa, 3q2p, 3qp2, p3 at x: 0, 1, 2, 3, respectively, we obtain 
the probability graph. 
Because the four probabilities qa, 3q2p, 3qp2, pa add up to 1, the ordinates 
alo add up to l. In Fig. 7-1, p---- 0.3, q----- 0.7. 
Numerical values. Suppose P(U) = p = 0.3, as in Fig. 7-1. Compute 
the probabilities of getting 0, 1, 2, 3 U's when the three tacks are tossed. 
1.0 
h' O.5 
, 3q2p 
0 I  X 
0 2 3 
Fro. 7-1. Pobability graph for the binomial distribution n = 3, p = 0.3, 
q = 0.7. 
We use formula (3), with p = 0.3, q = 1 -- p = 0.7, and substitute 
iu succession x = 0, 1, 2, 3. From these calculations we get the following 
probability table for this binomial distribution' 
b(.r; 3, 0.3) 0.343 0.441 0 189 0.027 
(5) 
x 0 1 2 3 
Ill this same example, we cau calculate still other probabilities. For 
instance, the probability of getting at least .'2 (% is 
P(X >_ 2) = 3qp2 q - p3 
= b(2; 3, 0.3) q- b(3; 3, 0.3) = 0.189 q- 0.027 = 0.216. 
Tile probability of getting not nlol'e than oue U is 
P(X _< 1) ---- qa q- 3pq2 
= b(0; 3, 0.3) q- b(1;3, 0.3) ---- 0.343 q- 0.441 ---- 0.784. 
RMARl(. Ill examples like that of the marksman hitting the bull's-eye 
or that of the thumbtacks, the value of p camlot be readily guessed. 
7-1] EXAMPLES OF BINOMIAL EXPERIMENTS 249 
However, a good estimate of p in such problems can be made if we record 
the results of several hundred shots, or tosses, and take as the estimate 
the ratio of the number of bull's-eyes, or U's, to the total number of trials. 
EXERCISES FOR SECTION 7-1 
1. Verify that the probability table (5) for the thumbtack example is correct, 
and check by addition that the sum of the probabilities is 1. 
2. Verify that formula (3) is correct by substituting x = 0, 1, 2, 3 in suc- 
cession, and thus check the entries in probability table (2). 
3. A bag contains 1 red and 2 white balls, identical except for color. If 3 
balls are randomly drawn one at a time with replacement after each draw, 
find the exact binomial distribution of the number of white balls in the sample 
of 3. 
4. Three candidates run for different offices in different states. Each has 1 
chance in 3 of being elected in his state. What is the chance that at least one 
of them is elected? 
5. If n = 3 in a binomial experiment, what values can p have if P(0) is 
to equal P(1)? Determine p if P(0) = P(3). 
6. Three dice were thrown 648 times and the number of times a "5 or 6" 
appeared was tabulated as follows: 
Number of Observed 
"5 or 6"'s frequency 
0 179 
1 298 
2 141 
3 30 
Total 648 
Obtain the theoretical probability for each outcome for perfect dice, multiply 
by 648, and compare the resulting theoretical frequencies with the observed ones. 
7. Toss 3 coins 24 times and compare the observed numbers of heads with 
their theoretical frequencies. (Be sure each coin is flipped separately.) 
8. In a binomial experiment with n = 3 show that P(X = 1 or 2) = 3pq. 
9. (For students without calculus.) Make a graph of b(2; 3, p) as p varies 
from 0 to 1 and estimate the value of p that maximizes it. 
(For students with some calculus.) Find the value of p that makes b(2; 3, p) 
a maximum and evaluate the maximum. [Hint. Replace q by 1 -- p before 
differentiating.) 
10. A thumbtack that can fall point up, U, or point down, D, with P(U) = p 
and P(D) = q = 1 -- p, is independently tossed 4 times. List a sample space 
for the possible outcomes of this experiment. Assign probabilities to its points. 
Show that the sum of these probabilities is 1. Find the probability distribution 
of the number that fall point up. 
250 TWO TYPES OF OUTCOMES' BINOMIAL DISTRIBUTION [CHAP. 7 
11. Puzzle. In seeking the probability of either 3 heads or 3 tails in a single 
throw of 3 coins, it has been reasoned that of 3 coins at least 2 must show like 
faces, and the probability that the third coin is the same as the other 2 is �, 
the desired probability. What is the correct probability? Try to find the flaw 
in the reasoning. 
7-2. EXIEN$1ON OF IHE 81NOMI/k� [XP[RIM[NI 10 n IRI/kL$ 
In the thumbtack example, if re thumbtacks are tossed, there are two 
different outcomes for each toss, U or D. Therefore, by the multiplication 
rule, there arc 2 X 2 X ... X 2 (re factors), or 2 n, different outcomes for 
the experiment (Section 2-1). Hence the sample space S for this experi- 
ment has 2 n distinct points. Each point determines a value of the random 
variable X, where X is the number of U's in the sample point. What is 
the probability of x U's, where x is any one of the numbers 0, 1, 2, . . ., re? 
That is, what is b(x; n, p) ? 
In Section 7-1 we found that the binomial probabilities for 3 binomial 
trials had two parts: a coefficient () and a factor pq-. We proceed 
to find the two parts of b(x; n, p) in general, using the language of trials, 
successes, and failures instead of tosses, U's and D's. 
First, in how many ways can we get exactly x successes in n trials? 
From the theory of permutations of two kinds of objects (Section 2-4) 
the numbel' of ways is (}); that is, the sample space S has exactly () 
points representing outcomes with x successes and n -- x failures. 
Second, what is the probability of x successes and n -- x failures in 
a given order? If we assume that the outcomes for the n trials are inde- 
pendent, then the probability for x successes and n -- x failures ire any 
giver, order is the product of x p's and (re -- x) q's. We had the identical 
pattern in the set of probabilities in the thumbtack example of Section 
7-1 [calculations following Eq. (1) of that section]. The reason we get 
the same probability for each arrangemcnt of the x successes and n -- x 
failures is that independence implies multiplication, and multiplication 
is commutative. Thus the desired probability for any given order of x 
successes and n -- x failures is 
p q (1) 
This is true whcn x is any one of the numbers 0, 1, 2, . . . , n. 
Among the 2  points in S, there are (n) points with x successes, each 
point having the probability pXqn--x assigned to it. Therefore 
p q , x = 0, 1, 2,.. n. (2) 
Note that the probability b(x; 3, p) [Section 7-1, Eq. (3)] is a special 
case of formula (2) with n ---- 3. 
7-2] EXTENSION OF THE BINOMIAL EXPERIMENT TO n TRIALS 251 
We observe that ()pXq- is the (x q- 1)st term in the binomial 
expansion of (q q- p), because the binomial cxpansion (Section 2-5) 
can be displayed as follows: 
Since q q-p---- 1, (q q- p)= 1. This result is reassuring, because it 
shows that our derivation has accounted for all the probability in the 
sample space. The set of ordered pairs 
) 
, x---- O, 1,...,n, 
X, X pXqn -X 
is the general binomial distribution, or binomial probability function. We 
have proved the following general theorem about binomial experiments: 
7-1 Theorem. Bznomial distribution. If an experiment consists of n 
independent binomial trials, each with probability p of success 
and probability q (=1 -- p) of failure, then the probability that 
the experiment results ill exactly x successes and n -- x failures is 
---- p q , x = 0,1,2,...,n. (4) 
EXAMPLE 1. Five coin tosses. Ill tossing a coin, the probability of a head 
is assumed to be �. If the coin i tossed 5 times, what is the probability 
(a) of exactly two heads? (b) of more than one head? 
Solution. Let X be the number of heads on the 5 tosses. 
(a) By Eq. (2), 
(_) (5)(1) 2(_)3 1 1 10 5 
P(X---- 2) ---- b 2;5, = 2  ---- 10..- 3---- 1-' 
(b) P(X > 1) is found most easily by using complementary events. 
The various mutually exclusive events are 0, 1, 2, 3, 4, or 5 heads. There- 
fore 
P(X > 1) ---- 1 -- P(X _< 1) 
---- 1 -- O(0; S, �) -- O(1; S, �) 
-- 1 -- (�)s-- 5(�) s 2 13 
-- -- 32- 16' 
252 TV, r TYPES OF OUTCOMES: BINOMIAL DISTRIBUTION [CHAP. 7 
EXAMPLE 2. Batter's problem. Suppose the probability that a batter 
gets a hit is �. At first glance, some people interpret this figure to mean 
that the batter is sure to get a hit if he bats four times. Vhat is the 
probability ? 
Solution. In 4 times at bat, the probability of at least one hit is 
P(X _ 1) : I -- b(0;4,�) 
= i -- rlO/_4 175 
\4] \4] -- 256' 
The answer is about 0.68, which is far from a certainty. The confusion 
arises from the fact that the mean number of hits is one, a confusion be- 
tween a mean and a probability. We discuss the matter further in Sec- 
tion 7-3. 
EXAMPLE 3. Two- and four-engine planes. Suppose that, in flight, 
airplane engines fail with probability q, independently from engine to 
engine, and that a plane makes a successful flight if at least half of its 
engines run. For what values of q is a two-engine plane to be preferred 
to a four-engine one? The probability an engine does not fail is p: 1 -- q. 
Solution. We begin by computing the probabilities of successful flights 
for the two types of planes. Let X be the number of engines that do 
not fail. 
Two-engine plane Four-engine plane 
P (successful flight) = P (successful flight) = 
P(X _ 1) = 1 -- P(0) P(.� _ 2) = 1 -- P(0) -- P(1) 
= 1 -- b(0;2, p) = 1 -- b(0;4, p) -- b(1;4, p) 
= I -- q2 = I -- qa _ 4pq3 
= 1 -- q4 -- 4(1 -- q)q3 
= 1 -- 4q 3 q- 3q 4 
Graphical approach. In Fig. 7-2, a graph is given of the probabilities 
of successful flights for the two kinds of planes as a function of q, the 
probability that a single engine fails. The crossing point of the two 
curves cannot be read precisely (but it is near q = �), and so the following 
algebraic approach may be preferred. 
Algebraic approach. The inequality that implies that the probability 
of successful flight for the two-engine plane is greater than or equal to 
the corresponding probability for the four-engine plane is 
1 -- q2 _> I -- 4q q-3q 4. 
7-2] EXTENSION OF THE BINOMIAL EXPERIMENT TO ? TRIALS 253 
I 
1 0 4-chime 
0 9 2-engrave 
phme 
0q 
0 7 
w 2-engine 
 plane 
fen 
03  l:)hme 
O 
Ol 
/q  3 
() ()1 02 03 ()4 t) 5 06 ()7 OS ()9 10 
q = P(engme fals) 
Fo. 7-2 Probabilities of successful flights plotted against q, the proba- 
Nhty of failure for a single engine 
Subtrncting 1 -- q= from both sides of this lnequahty, we have the fol- 
lowing equivalent relation' 
0 >_ q= -- 4q a -f- 3q 4 
Factoring q= from the expression on the right yields 
0 _> q(1 -- 4q-{-3q ) 
Filmlly, we factor the right-hand side further and obtain 
o _> q-�O -- q)O 
If q ---- O, q = 1, o1' q = �, the right-hand member is zero and the two 
kinds of planes have equal chances of successful flights 
Figure 7-.3 graphs the right-hand side of lnequahty (5) against q The 
graph also shows that equal chances of successful flights for the two 
254 TWO TYPES OF OUTCOMES: BINOMIAL DISTRIBUTION [CHAP. 7 
y 
q = probability of failure for any single engine 
0.15 Y = P (successful 4-engine flight) -- P(successful 2-engine flight) 
O.lO I 
0.05 I 
1 
= 
"-.__1 --  , , , , , , /  q 
- 0.05 - 
--0.10 
--0.15 
--0.20 - 4-engine 
plane preferred --  2-engine plane preferred 
Fro. 7-3. Graph of y = q2(1 -- q)(1 -- 3q). 
- The graph further 
types of plane occur xvhn q = 0, q = 1, or q - a. 
shows that if � < q < 1, then the right side of inequality (5) is less than 
zero because the curve falls below the q-axis. Similar reasoning shows 
that for 0 < q < � the curve is above the q-axis and the four-engine 
plane is to be preferred. The foregoing facts can also be obtained by 
studying the signs of the factors of the right-hand member of inequality 
(5) for various values of q. Needless to say, the practical situation is 
that q, the probability that any one engine fails, is very nmch less than �. 
EXERCISES FOR SECTION 7-2 
1. A baseball player's batting average is .300. What is the probability that 
he gets exactly 2 hits in 4 times at bat? Are there considerations that make you 
doubt that the number of hits in 4 times at bat is binomially distributed? 
2. A thmnbtack falls point up 40% of the time. Compute the probability 
function for the number of times it falls poiut up in 5 tosses. Display the results 
in a probability table and in a graph. 
3. Why do you think the number of baseball games won by the home team 
in a double-header may fail to be binolnially distributed? 
4. If n coins are independently tossed, show that the probability that all or 
all but one will fall xxith the same face up is (n q- 1)/2 -, if n  2. What is 
the correct allsx,er whell n = 2? 
5. Suppose an amateur rifleman has a probability of 0.05 of hitting a bull's-eye 
on a single shot. What is the probability that in 20 shots he never hits the bull's- 
7-2] EXTENSION OF THE BINOMIAL EXPERIMENT TO n TRIALS 255 
eye? (Use logarithms.) Also sct lip, but do not evaluatc, the probability cal- 
culation for hitting it 4 or more times in 20 shots. (Thc setup of this latter 
problem is straightforward, but the numerical computations are excessively 
long. In the next section we show how such problems can be solved easily by 
the use of tables.) 
6. Compare the performance of a one-engine plane with that of a tAro-engine 
plane, using the same assumptions as those of Example 3. 
7. In a binomial experiment consisting of 3 trials, the probability of exactly 
2 successes is 12 times as great as that for 3 successes. Find p. 
$. 0ne-thild of the male freshmen cntering a college arc at least 6 feet tall. 
If roommates are assigned randomly for freshmen, 4 to a room, what is the 
probability that at least 3 of the 4 in one room are under 6 feet? (Ignore the 
fact that the actual sampling is without rcplacement.) 
9. Two-thirds of the secretaries in a large stenographic pool are licensed motor- 
vehicle operators. If 4 secretaries are drawn at random to go on a trip, what 
is the probability that at least 2 are licensed drivers? 
10. A quiz has 6 multiple-choice questions, each with 3 alternatives. Sheer 
guesswork yields xvhat probability of 5 or more right? 
11. A risky operation used for patients with no othcr hope of survival has 
a survival rate of 80%. Vhat is the probability that exactly 80% of the next 5 
patients operated upon survive? 
12. (For students with some calculus.) For given values of x and n, consider 
b(x; n, p) as a function of p and show that it is maximized when p -- m/n. 
[//int. First solve the problem assuming that x is neither 0 nor n, then handle 
those t'o cases separately.] learJc. This is onc reason for using the observed 
number of successes divided by the total number of trials as an estimate of p. 
That estimate, x/n,  -- 0, 1, . .., n, is called thc raaxrau liJcel]zood estizate 
of p because of the maximizing property you are to prove. 
13. Consider two binomial experiments each with p -- �, one of sizc n -- 2 
trials, the other of size n : 2z -- 1 trials, where  is a positive integer. Show 
that P(r) is the same for both experiments. 
14. For what valucs of q is a one-cnginc plane to bc preferred to a three- 
engine plane? Use the assumptions of Example 3. 
15. For what values of q is a two-engine plane to bc preferred to a three- 
engine plane? Use the assumptions of Example 3. 
16. Suppose three- and five-engine plancs fly if more than half their engines 
work. If q is the probability of failure for a single engine and cngines perform 
independently, find the values of q for which the thrce-engine plane is to bc 
preferred. 
17. Two independent binomial experiments, onc of n and thc other of ra trials, 
both have probability p of success on each trial. Show that the probability of a 
total of exactly x successes in the two experiments combined is 
and interpret this result. 
256 TWO TYPES OF OUTCOMES: BINOMIAL DISTRIBUTION [CHAP. 7 
7'-3 EXPECTED VALUE OF A'BINOMIAL RANDOM VARIABLE 
In this chapter we have dealt entirely wth probabilities for the binomial 
distribution, not at all with the mean or expected outcome E(X) = /. 
In Example 2 of Section 7-2 we computed the probability that a batter with 
P(hit) ----  gets at least 1 hit in 4 times at bat. To obtain the mean 
number of hits/& we multiply each possible number of hits by its proba- 
bility of occurrence and add as follows' 
 = OP(O) -[- 1P(1) - 2P(2) -[- 3P(3) - 4P(4) 
= O-1(-}) 4 -q- 1-4()(�)  2.6()(�)  
-[- 3.4()(�)  + 4. 1(�) 4 
-- 0 q- 
Thus, in the long run, the batter gets 1 hit in 4 tries. The misinterpreta- 
tion mentioned in Example 2, Section 7-2, stems from a confusion be- 
tween a mean and a probability. In his work, Cardan, one of the first 
writers on probability, had a similar confusion, which contributed to the 
neglect of Cardan's findings in the theory of probability until 1953,* 
when 0ystein Ore sorted out Cardan's mistakes from his discoveries. This 
is the same Cardan who worked on the solution of the cubic equation. 1 
In the batter example, the random variable X, whose value is deter- 
mined by the experiment of batting four times, is the number of hits, 
and its possible values are x ---- 0, 1, 2, 3, 4. The meat, number of hits/ 
is the mean of the distribution of the random variable X. If the batter 
repeats the batting experiment, then each repetition yields a value of X. 
In six repetitions the values might be 0, 2, 0, 3, 1, 1. If we compute the 
ordinary average X for these several values of X, we have  as an esti- 
mated value for/, instead of 1, the true value. If the outcomes of many 
experiments are used in computing the average of the observed values, 
the estimated value is likely to be very close to/, the mean of the distri- 
bution of X. 
More generally, we want to obtain the expected number of successes 
in n binomial trials. Recall that X is the sum of n random variables, each 
of which takes the value 1 with probability p and the value 0 with proba- 
bfilty (1 -- p). The mean of any one of these random variables is 
1 Xpq-OX (1 --p) = p. 
* Oystein Ore, Cardano, Princeton University Press, 1953. 
 D. E. Smith, History of Mathematics, Vol. 1, Ginn & Co., 1923, pp. 295-297. 
7-3] EXPECTED VALUE OF A BINOMIAL RANDOM VARIABLE 257 
In Chapter 6, Corollary 6-5 states' "The mean of the sum of any finite 
collection of random variables is the sum of their means." Siuee X is 
the sum of n random variables each having meau p, 
E(X) = p + p + . . . + p = np. 
� 
n terms 
7-:2 Theorem. Biomial mean. Let p be the probability of success 
at each trial of a binomial experiment. Then the mean numbel' 
of successes in n trials is 
= E(x) = np. (2) 
EXAMPLE 1. We apply formula (1) to the problem of the batter with 
p=P(hit) = �, n---- 4. What is the mean number of hits in 4 times 
at bat? 
Sohttion. / = 4(�) ---- 1, as before. 
EMPLE 2. Fifteen dice are thrown, what is the expected number 
of aces? 
Soltior*. t ---- 15() ---- 2.5. 
EXERCISES FOR SECTION 7-3 
1. The probability of a thumbtack landing point up is 0.3. If 15 thumbtacks 
are tossed, find the expected number that land with points up. 
2. How many dice must be tossed if the expected number of aces is to be 5? 
3. Two binomial experiments are performed' 13 cards are randomly drawn 
from a bridge deck, with replacement after each draw, and twelve dice are 
rolled. Find the expected total number of aces (ones) and deuces (twos) in the 
two experiments combined. 
4. In a binomial experiment, if/ must be at least a distance 3v/pq from both 
0 and n, show that n >_ 9 times the larger of p/q and q/p. 
 find the prol:ability of 1 
5. In a binomial experiment wth n = 2, p = 2-6W, 
or more successes, and compare the result with  =np. Make a similar compari- 
I . 
son forn = 3, p = -o,f�rn = 3, p = 0.1. Comment on these results. 
6. (Continuation.) For a binomial experiment, show that if / s near zero 
P(X >_ 1)  /. [Hint. Show thatP(X >_ 1) -- 1 -- (1 -- p)'* -- np 4- more 
binomial terms. Then show that any binomial term beyond the 4- sign, such 
as (})pq- _< (l/x!), and then note that if / is near zero, /2 and higher 
powers of/ are small compared with/.] 
7. (Continuation.) Use the results of Exercise 6 to find, approximately, 
1 
P(,Y >_ 1), in a binomial experiment xxth n = 50, p = 
258 TWO TYPES OF OUTCOMES: BINOMIAL DISTRIBUTION [CHAP. 7 
7-4. BINOMIAL PROBABILITY' TABLES 
It is a dreary task to compute the probability of every outcome for a 
large set of binomial trials. Extensive tables are available for the binomial 
distribution, and we present a small one (Table IV) for your use. 
Ths table, at the end of the book, is in two parts. Part A gives 
b(x; n, p), the probability of observing exactly x successes in a binomial 
experiment composed of n trials. Values are given for all x for n = 2 to 25 
and for p ---- .01, .05, .10, .20, .30, .40, .50, 60, .70, .80, .90, .95, .99. 
Part B of the table gives, for the same binomial distributions, the 
probability of observing r or more successes. Thus this part of the table 
gives the "cumulative" probability from r through n, rather than the 
probability of a single number of successes. l�Iany applications require 
sums rather than single probabilities. Symbolically, this part of the 
table gives 
P(X _ r) ----- b(r; n, p) q- b(r q- 1; n, p) q- � � � q- b(r; n, p) 
Each 3-digit entry in'the table should be read with a decimal preceding 
it. The symbol 1-- means a probability larger than 0.9995, but less than 
1. The symbol 0q- means a probability less than 0.0005, but greater than 0. 
Let us check a value in the table. To find the probability of 4 or more 
successes in 5 trials when p ---- 0.8, we compute 
b(4; 5, 0.8) q- b(5; 5, 0.8) 
and get 
(54)(.8)4(.2) q- (55)(.8)5(.2)�----.40960 q- .32768----.73728. 
We read in Table IV-A that b(4; 5, 0.8)  0.410 and that b(5; 5, 0.8)  
0.328, and in the cumulative Table IV-B for n---- 5, r ---- 4, p ---- 0.8, 
xve read 0.737. All three tabled probabilities agree to three decimal places 
xvith our calculated values. 
EXAMrLE 1. For n ---- 10, p ---- 0.4, find the probability of 3 or more 
successes. 
Solttlioz: Ilcading directly from Table IV-B, we find that the proba- 
bility to three decimal places is 0.833. 
EXAMr,E 2. Interpolatio i the tables. With n = 25, fid the value 
of p that makes P(X k 8) = 0.4. 
7-4] mNOMX^. rRor.rmxTY T^r.ES 259 
Solution: A tabular array assists with such a problem. 
P(X_> 8), n= 25 
p -- 0.20 0.109 
p = ? 0.400 
p = 0.30 0.488 
By ordinary interpolation, xve have 
p -- 0.2 0.400 -- 0.109 
0.3 -- 0.2 0.488 -- 0.109 
xvhence 
0.400 -- 0.109 (0.3 -- 0.2)  0.28. 
p  0.2 - 0.488 -- 0.109 
This result agrees, to two decimal places, with the value obtained from 
a bigger table than ours. 
Exxr.E 3. If n ---- 15, p =-- 0.05, find the probability of 2 or fewer 
successes. 
Solution 1. 
P(X _ 2) = 1 -- P(X 2 3) 
= 1 -- 0.036 = 0.964. 
Solution 2. We could, instead, focus on the number of failures. Two 
or fewer successes is equivalent to 13 or more failures. We would then 
enter the table with p appropriate to failures, 0.95. Then we read directly 
P(X > 13) = 0.964. 
Other tables. A brief list of more extensive tables of the binomial dis- 
tribution follows. The notation n = 111110151100 means that n goes 
from 1 to 10 in steps of 1 and from 10 to 100 in steps of 5. 
1. Harvard Computation Laboratory, Tables of the Cumulative Binomial 
Probability Dzstribution, Harvard University :Press (1955) Cumulatives 
only for n ---- 1111501211001101200120150015011000, p ---- 0.0010.0110.50 and 
1 1 1 3 1 5 1 3 5 7 
3, 6, 8, 8, 12, 12, 16, 16, 16 16' 
2. National Bureau of Standards, Tables of the Binomial Probability 
Distribution, Applied Mathematics Series 6 (1950). Gives both cumulative 
and single terms for n ---- 111149, p ---- 0.0010.0110.50. 
260 TVO TYPES OF OUTCOMES: BINOMIAL DISTRIBUTION [CHAP. 7 
3. Harry G. Romig, 50-100 ireornial Tables, New York' John Wiley 
& Sons, Inc. (1953). Gives both cumulative and single terms for 
n = 50151100, p = 0.0010.0110.50. 
4. Ordnance Corps, Tables of the Cumulative Binomial Probabilities, 
Ordnance Corps Pamphlet ORDP 20-1, U.S. Government Printing 
Office (September, 1952). Gives cumulative only for n----1111150, 
p ---- 0.0010.0110.50. 
EXERCISES FOR SECTION 7-4 
In the following exercises 1 through 9, let X denote the number of successes 
in n binomial trials, with probability p of success on each trial. 
1. For n = 15 and p = 0.6, find (a) P(X _ 7), (b) P(X = 7). 
2. Forn = 25 andp = 0.8, find (a) P(X > 19), (b) P(X = 19). 
3. For n = 20 and p = 0.3, find (a) P(X >_ 6), (b) P(X = 6). 
4. For n = 25andp = 0.65, find (a) P(X >_ 11), (b) P(ll or more failures), 
(e) P(X = 11). 
5. With n = 22, find the value of p that makes P(X _ 8) = 0.4. 
6. With n = 20, find the value of p that makes P(X _ 7) = 0.5. 
7. With n = 15, find the value of p that makes P(X _ 10) = 0.8. 
8. Given that n = 12 and p = 0.8, find (a) P(X = 8), (b) P(X_ 8), 
(c) P(X _ S). 
9. For n = 6 and p = 0.2, find the value of P(X = 2). 
10. In shooting a rifle the probability that John hits the target is 0.95, the 
probability that he gets a bull's-eye is 0.20. He shoots 25 times. What is the 
probability that he hits the target more than 20 times? That he gets exactly 
5 bull's-eyes? That he gets 5 or more bull's-eyes? 
11. (Continuation.) Suppose John shoots only 22 times. What is the proba- 
bility of exactly 10 bull's-eyes? Fewer than 107 More than 107 Check that the 
three results add to 1. 
12. A die is tossed 12 times. What is the probability of more than 4 aces? 
13. If the probability of seven or more successes in 25 trials in a binomial 
experiment is 0.5, what is the probability of success on each trial? (Give answer 
to two decimal places.) 
14. Twenty-five coins are poured from a sack onto a table. What is the proba- 
bility that the number of heads is between 8 and 17, inclusive? 
15. If 40% of the voters in a large town favor candidate .4, what is the proba- 
bility that in a random sample of 25 voters, the majority in the sample will 
favor him? 
16. A census of a United States town of 25,000 showed that 75/% of the 
families owned refrigerators. Txenty families were randomly selected for 
intensive sociological and economic investigation. Approximately what is the 
probability that 10 or fewer of these families have refrigerators? (A binomial 
calculation here is approximate because we ignore the fact that the sampling 
is done withoHt, instead of with, replacement. Since the sample size is small 
compared xxith the popntation size, the approximation is a good one.) 
7-4] BINOMIAL PROBkBILITY TABLES 261 
17. What IS the probability of exactly S successes in a binomial experiment 
of 11 trials if the probability of success on each trial is 0.87 
18. Use Table iV to xork Exercise 5 of Section 7-2. 
19. For n = 25, use binomial tables to find tile two values of p that satisfy 
P(X = S) = 0.075. 
20. Five balls were drawn, one at a time, with replacement, from a bag con- 
taining all equal number of black and white balls. The number of black bails 
was then tabulated for 819 sets of consecutive drawings to give the following ob- 
served frequency distribution: 
Vumber of Observed 
black balls freq,enc/ 
0 3O 
1 125 
2 277 
3 224 
4 136 
5 27 
Total 819 
Obtain the theoretical frequencies from the binomial and compare them with 
the observed vahtes. 
21. Assume that you serve on the school committee for your community, 
and that you know that tile population of 4th-grade school children has 95% 
right-handed and 5% left-handed children. You observe that the 4th-grade 
classroom has 20 tablet arlnehairs, all with the tablet on the right arm. Assume 
that tile 20 students assigned to this room are a random sample of the 4th- 
grade school population. 
(a) What is the probability of one or more left-handed students in a class of 20? 
(b) Suppose that you influence the school committee to exchange one of the 
chairs for a left-armed one. What is the probability that the chairs just come 
out even with the students: 1 left-armed ellair, 1 left-handed student; 19 right- 
armed chairs, 19 right-handed students? 
(e) How much have you improved the probability that everyone's handed- 
ness is provided for? 
(d) The chairs are permanently installed. You could arrange that tilere be 
21 chairs, one left-armed, for 20 students. Now what is the probability that 
everyone's handedness is provided for? 
22. (a) Compare b(1;4, .30) with b(3;4, .70). (b) Compare b(6; 18, .40) 
with b(12; 18, .60). 
23. (Continuation.) Prove b(r; n, p) = b(n -- r; n, q) if q = 1 -- p. This 
shows that tables like Table IV-A need not tabulate for values of p in excess 
of 0.5. 
262 TWO TYPES OF OUTCOMES: BINOMIAL DISTRIBUTION [CHAP. 7 
24. (a) Usc Table IV-B, n = 3, to compare P(X _ 11) for p = 0.6 with 
1- P(X _ 13) for p = 0.4. (b) Use Table IV-B, n = 5, to compare 
P(X >_ 2), p = 0.2 with 1 -- P(X _> 4) xvith p = 0.8. 
25. (Continuation.) Prove 
b(x;n,p) = 1 --  b(x;n,q). 
x=r x=n--r-[-1 
This shows that cumulative binomial tables necd not have values of p in excess 
of 0.5. However, such values are a convenience. 
26. Suppose 5 cards arc drawn from an ordinary bridge deck, with replace- 
ment and reshuffling after each card is drawn. Find the probability function of 
the number of red cards in the sample of 5 cards. Compare the results with those 
of the table following Eq. (4), Section 5-5, for sampling without replacement. 
7-5. PROPERTIES OF THE BINOMIAL DISTRIBUTION 
In this section we study the shapes of graphs of binomial distributions 
produced under two conditions: (1) for a fixed number of trials n, but 
different values of p; and (2) for a fixed value of p, but different values of n. 
We study especially how the graphs change shape as n grows large. Such 
a study helps us understand the family of binomial distributions, and it 
also helps us understand other sequences of probability distribution 
functions, because the changes within the binomial family resemble the 
changes within many other families of distributions. 
Some properties are merely stated and illustrated without proof. The 
binomial tables at the back of the book can provide further numerical 
illustrations. In the discussion of figures, we sometimes abbreviate the 
notation for the binomial ordinate at x, which is b(x; n, p), to b(x). 
(1) Fixed n, varying p. As p varies, the shape of the graph of the bi- 
nomial distribution changes. Figures 7-4(a) through (i) illustrate this 
for n ----- 5. For p near zero or near one (Fig. 7-4a, b), the probability 
spikes up at x ---- 0 and x = n, respectively. The abscissa corresponding 
to the largest ordinate is called the mode. For p more centrally located, 
b(x) increases with each successive .' until the largest b(x) is achieved 
(Fig. 7-4c, d, g, h) and then, except possibly for a tie at x h- 1 (Fig. 
7-4e, f, i), b(x) decreases as x continues to increase. Thus, unless two ad- 
jacent ordinates are tied in value, there is just one largest ordinate, and 
the ordinates decrease steadily as we move to the right or to the left from 
the mode. The proof would divert us, but t is an exercise in the manipula- 
tion of inequalities that is within the range of an enthusiastic student. 
When p = � (Fig. 7-4i), the distribution is symmetric about n/2; 
if n is odd two central values of x have equal ordinates (Fig. 7-4i); if n 
7--5] PROPERTIES OF THE BINOMIAL DISTRIBUTION 263 
is even the ordinate at the middle value of x is the largest (Fig. 7-5). If 
p  -} the distribution is asymmetric. 
In Figs. 7-4 and 7-5 the fulcrum i on the horizontal axis shows the 
mean, , for each distribution. You can see that the means of these bi- 
nomial distributions are within one unit of the abscissa with largest 
probability (the mode). In binomial distributions, the mean and mode 
are al,ays within one unit of each other. Furthermore, if np is an integer, 
the mode and mean m'e identical. We shall not prove these facts. 
(2) Fixed p, i,creasing n. As , increases, the successive binomial 
distributions (a) "walk" to the right, (b) flatten, and (c) "spread." We 
discuss these features in turn. 
(a) "Wall'ig." As n increases, the mean  moves to the right a 
distance p for each unit increase in n because  =np. The mode and the 
other large ordinates are near the mean, so the central mass of the distri- 
bution also "walks" to the right as n increases. 
Later we shall try to obtain a limiting shape for the binomial distribu- 
tion as n grows large, and to achieve this end we must prevent the distri- 
bution from walking off. We can do this by replacing the random variable 
Xbya new variable X -- p. For if X is replaced byX--np, then 
is replaced by  -- rip, or zero. Hence this adjustment keeps the successive 
distributions centered at the origin and prevents walk-off. 
(b) Flattenireg. Consider further the unadjusted random variable X. 
As the means walk, the distributions flatten (Fig. 7-5a through e). We 
wish to study the rate of flattening. It can be proved that, for large n, 
the sizes of the central ordinates are inversely proportional to x/-. We 
shall illustrate this fact graphically. To do this, let us first recall that 
y --= mx is an equation of a straight line through the origin with slope m. 
When y is a constant times x, y varies directly as x. If x: 1/x/-, then 
we usually say that y varies inversely as v/-. But xve can also say that 
y varies directly as 1/x/-; and when we plot y against 1/x/-, we get a 
straight line through the origin. The point is that we have a linear rela- 
tion if we regard 1/x/ as an independent variable. In other words, one 
way to show that y is inversely proportional to  is to show that ?! is 
directly poportional to 1/x/-. We use this idea in Fig. 7-6. 
3Iode proportional to 1/v/-. To return to the main discussion, Fig. 7-6 
shows how the middle ordinates of symmetric binomial distributions 
(p = �) decrease as n grows. The relation is smooth when n is taken as 
even. (A similar smooth relation holds for n odd.) The modal value of 
.c is n/2. When we choose the horizontal axis as the axis of 1/x/, we 
see that, as n grows, I)(mode) decreases, following a curve that is almost 
a straight line through the origin. (A scale of values of n is marked 
below the axis.) The points on the curve for p = � have coordinates 
26 TWO TYPES OF OUTCOMES; BINOMIAL DISTRIBUTION [CHAP. 7 
o s 
 (nuntirol of ,mccesse,,) b(x) (pxobabihty of exactly � u((eses: 
o b[ 
o  
oL  2  4      4 
 = 5, p  005, np = 025 n = 5. p = 095 t =475 
(a) 
0 61 
0 1 2 3 4 5 o 1 2 3 4 5 
t = 5, p - 020, np = 1 0 n = 5, p = oso, t,p = 40 
b() (c) (d) 
o 2 
I , . , I 
0 0 I  2 3 4 5 0 1 2 3 A 4 5 
t = 5, p  3, , = 3 
0 
0 1 2 3  5  } 1 2 3  
/ = 5, p  0 t0, tp - '2()  = 5 p = OO0. t t) = 3(1 
o 4 
() 1 '2  3  5 
m. 7-4. inomil distributions for n = 5, displ3in tle change in form 
As p varies. 
7--5] PROPERTIES OF THE BI.X'OMIAL i)iSTRiBUTiO\  265 
Fa. 7-5. Walking, flattening, and spreading as n increases. 
266 TIO TYPES OF OUTCOMES] BINOMIAL DISTRIBUTION [CHAP. 7 
(1/V/{, b(}n)), n even. Our bi[omial table IV-A can be used to check 
a point on the curve. For n ---- 24, b(12)  0.161 and 1/x/  0 204. 
Similarly, the relation between P(mode) and 1/ is approximated 
by a straight line through the origin for binomial distributions with p -- 5 
(for smoothness, we have chosen values of n that are multiples of 5, and 
then the mode is n/5). Our binomial table IV-A can be used to check a 
point for n ---- 25. Then 1/ ---- 0.20, and b(5)  0.196. This graph 
is adequate to illustrate the approximation' the modal ordinate and its 
neighbors decrease inversely as . 
When we study the limiting distribution of X as n grows, we shall need 
to prevent the binomial distributions with large n from collapsing onto 
the z-axis. As Fig. 7-6 illustrates, as n grows, 1/ tends to 0, and the 
largest ordinate tends to zeo. 
o 20 
018- / 
0.16 - x.///.  
7' 014 . J 
t 0.10 - 
0.06- 
O.O4 // 
0 02 ' 
 OJ)4 ():06 OOb 0.10 0 12 0 14 0 lb 0 lq 0.20 1/x . 
 1000 400 200 100 60 40 30 . 
Fro. 7-6. Plot of P(X = mode) agMnst 1/ to show the nearly straight- 
line relationship for p =  and p = . 
(c) Spreading. We recall that the sum of the ordinates is alxvays 1. 
Naturally, if the distributions fiatten as n increases, and the total proba- 
bility must remain constant, successive distributions must spread out. 
They spread at a rate proportional to v. For the standard deviation 
is a measure of spread, and its value for a binomial distribution, x/pq, 
is proportional to v when p is coustant. (We derive this standard 
7-5] PROPERTIES OF THE BINOMIAL DISTRIBUTION 267 
deviation in Chapter 9 but, for this discussion, you are asked to accept 
it on faith.) 
The total range of the binomial is from 0 to , and increases at a rate 
proportional to n. :But we know from Chcbyshev's theorem that there is 
very little probability near the ends of the dlstribuiion compared xvith 
the amount within a few standard deviations of the mean. The standard 
deviation is sensitive to the rate at whmh the central mas of the binomial 
distribulion spreads (the 75 or the 99% near the mean), and it is this 
central mass that we want to study. 
To summarize: as n grows, (a) successive binomial distributions xvalk 
to the right at a rate proportional to ; (b) the modal ordinates flatten 
at a rate proportional to 1/x/; and (c) the distributions spread out, i e., 
their standard deviations increase in proportion to x/. 
EXERCISES FOR SECTION 7-5 
All problems in this set refer to binomial distributions. 
1. Find the mean and standard deviation of the binomial distribution with 
(a) n = 4, p = �, (b) r = 10, p = �. 
2. If p, = 45, o- = v/pq = 6, findrandp. 
3. If p, = 10, o- = x/pq = 3, findrandp. 
4. Show thatlfp -- 3andn = 2,n-- 1, thenb(m-- 1) = b(m). 
5. Show that if p = � and n = 2v, then b(m) is larger than b(m -- 1) or 
6. (Continuation.) If p = 3, n = 2m, and m >_ 2, show thatb(r -- 1) _ b(r) 
forl < r < ,n. 
7. If p = �, how fast does the sequence of means of binomial distributions 
xxalk to the right, per unit increase in n? 
8. Make a graph like that of Fig. 7-6 for p = 0.4, using your binomial tables. 
Choose n's that are multiples of 5 to smooth the plotting. Do not forget to label 
the axes. Assume that the curve through the points passes through the origin. 
9. It is desired to show that points like those in Fig. 7-6 do not lie along a 
straight line. It is convenient to choose values of n that are perfect squares, 
say 1, 9, 25. If p = 3, the modal ordinates are b(0; 1, 3), b(4; 9, 3), b(12; 25, 3) 
at 1/x/T, 1/v/, 1/v/, respectively. Show that the slopes of the chords con- 
neeting the points are not equal. Use tables. 
10. For n = 2, find the values of p for which the three ordinates b(x; 2, p) 
lie on a straight linc when a binomial probability graph like Fig. 7-4 is made. 
11. For n = 4, p = 0.2, use your binomial tables to assist in plotting the 
graph of the probability function, as in Fig. 7-4. Be sure to indicate the mean. 
12. For n = 25, p = 3, use your tables to find the probabilities contained 
within r of the mean /, within 2r of the mean, and within 3r of the mean. 
Compare these results with those given by the Chebyshev inequality and xxith 
those gven by the empirical rule of Table 5-15, Section 5-7. 
'-)6S TWO TYPES OF OUTCOMES: BINOMIAL DISTRIBUTION [CHAP. 7 
13. (a) Use your tables to find the largest values of x such that P(X >_ x) >_ 
0.25 for p = �, ? = 4, 8, 12, 16, 24. (b) Compute y = x-- np for each x 
found in part (a). For example when n = 4, the largest x satisfying the require- 
roentis3. We reduce 3 by  and get y = 3 -- 2 = 1. (c) Now plot the points 
(x/, q), xherc y s the value computed for the given n, and observe that the 
points fall roughly along a straight line through the origin. If we regard the 
distance from the mean / to the 25% point of the distribution as a measure 
of spread, this hnearity suggests that the spread increases approximately in 
propration to x/. 
14. Assuming that Fig. 7-6 is correct and that the curve for any p passes 
through the oigm, show that for a fixed p (0 or 1), as n approaches infinity 
b(.z; n, p) approaches zero, and therefore that m the limit, the sequence of 
binomial distributions does collapse onto the x-axis. (Puzzle. The total proba- 
bility had to add to l; where did it go?) 
7-6. TOOLS FOR STUDYING THE LIMIT OF THE BINOMIAL DIS- 
TRIBUTION 
Vhen n is large, the binomial distribution can be adjusted so that it is 
closely approximated by the standard normal distribution. We shall 
study the appropriate adjustments that lead to this approximation. Al- 
though the formula for the standard normal distribution looks a bit un- 
friendly, bristling as it does with roots, exponents, and transcendental 
numbers like r and e, these are not important features. For our purposes, 
the important features are that tables of the normal are widely available, 
and that the transition from a binomial probability problem to a normal 
probability problem is easy to make once one knows how. 
With the exact binomial distribution, we are somewhat hampered by 
the extent of the available tablea. As we have seen, even the largest 
tables extend only to n ---- 1000 and run to hundreds of pages, with large 
gaps in the list of values for n. But a page or two of normal tables can 
usually provide re,ults with sufficient accuracy for most binomial problems. 
Xaturally, we appreciate having binomial tables, but we also appreciate 
a fine approximation. 
In addition, the uormal diatribntion formula sometimes offers a more 
manageable expression for a binomial probability than does a complicated 
summation. 
We recall (Seetiol 7-1, just before Example 2) that the random variable 
X, the total number of suecesse,s in n binomial trials, is the sum of n 
random variables, oue variable for each binomial trial, each having the 
possible values 0 and 1. Sum, of several random variables under general 
conditions are approximately uormally distributed, but we cannot prove 
this fact here. However, a graphical demonstration of the way the bi- 
nomial can be approximated by the normal distribution illustrates how 
7-6] TOOLS FOR STUDYING BINOMIAL DISTRIBUTION' LIMIT :269 
the distribution of sums can be approximated by the normal distribution 
as n grows. 
We need three pieces of equipment to help us study the limiting be- 
havior of the binomial family: 
(1) Because the binomial is a discrete distribution and the normal 
a continuous one, the probabilities represented by binomial ordinates 
need to be replaced by areas, since areas are used to represent probabilities 
in continuous distributions. 
(2) We need a change of scale for X that prevents both walk-off and 
flattening. 
(3) Ve need to know how to approximate binomial probabilities by 
areas under the normal curve. 
(1) Ordi'riales aid areas. In Section 6-2 ve found, for a continuous 
random variable, how to represent probabilities by areas contained be- 
tween the graph of the probability function f(x) and the .v-axis. Figure 
7-7 illustrates this point. The total probaNhty (area) contained betveen 
the curve aud the x-axis is 1. The probaNlity that the random variable 
X takes a value t0etween a and b is given by the area of the shaded part 
of Fig. 7-7. N'ote that P(X = a) = 0, because the area over a is just 
that of a line segment f(a) units long, but of zero width. On the other 
hand, the ordznates of discrete distributions represent probabilities, and 
P(X = 3), for example, need not be zero. If we wish to fit a binomial 
distribution by a eontiuuous probability function, the following is a simple 
way to use areas to represent the probabilities usually giveu by ordinates. 
f(,O 
FIG. 7-7. Continuous probability density function; shaded area gives 
p(a < x  _< ). 
We replace each ordinate of a binomial distribution by centering at x 
a rectangle xvhose width is one unit and whose height equals that of the 
original binomial ordinate. The area of the rectangle then has the same 
numerical measure as the height of the ordinate. To illustrate, the area 
over the interval from x -- � to x-t-� in Fig. 7-8(b) has the same 
numerical value as the height of the ordinate at x in Fig. 7-8(a). Thus 
the probabilities given as ordinates in Fig. 7-8(a) are represented as 
areas in Fig. 7-8(b). 
270 T'VO TYPES OF OUTCOMES: BINOMIAL DISTRIBUTION [CHAP. 7 
fO -- 1) 
f(a) 
f(x -}- 1) 1 
1 I 1 1 
Fro. 7-8. (a) Probabihfies as ordinates. (b) Probabilities as areas. 
(2) Changing scale. For one change in the random variable X, we 
prevent "walk-off" by subtracting the mean and creating the new variable 
Y = X -- rip. The distribution of the new variable Y has mean zero, 
and thus is centered at the origin, like the standard normal distribution. 
The standard deviation of Y is the same as that of X: a = pq. 
The standard normal distribution has mean zero and standard deviatiou 
1. Our new random variable Y has mean zero, and a further adjustment 
is needed to get a variable with a = 1. We divide Y by xpq, if npq > O, 
to get a new random variable Z whose standard deviation is 1, like that 
of the standard normal distribution. Let 
Y X -- np 
Pq pq 
The variance of Z is 1 because (Section 5q) 
VarZ: Vat Y : Var 1 y 1lq Var -- -- 1, 
npq 
1. 
Thus the variable Z has 9z = 0, ez = 1, just as does the standard normal 
distribution. Furthermore, since the standard deviation of Z is a constant, 
it does not depend on n. Therefore, with Z, we do not have the problem 
of spreading that afflicted X. 
Now that walk-off and spreading are taken care of, what about fiatten- 
ing? When we changed to the variable Z, we adjusted the scale of X. This 
adjnstment changed the width of the rectangles we used in the areal repre- 
sentation of probabilities. 
EXAMPLE 1. Let n = 8, p = ; then x =np: , and ex: pq = 
 2   The rectangles for the area graph (see Fig. 7-8b) of the 
7-6] TOOLS FOR STUDYING BINOMIAL DISTRIBUTION LIMIT 27l 
distribution of X have abscissa boundaries x 4- �, x = 0, 1, . . . , 8. Dis- 
cuss the area graph for the new random varinble Z. 
Dzscussion. Let us carry through our transformation for the boundaries 
of the rectangles centered at 0, 1, 2, and 3 
Boundaries for Boundaries for 
Boundaries for X Y = X s Z = 3I'/4 
3 
1 19 19 
2 6 
1 13 13 
 6 8 
3-- 7 7 
2 6 8 
5 1 1 
 6 8 
The successive boundaries for the X-rectangles are 1 unit apart, but the 
corresponding boundaries for the Z-rectangles are { unit apart. We want 
the probabilities represented by the areas in the Z-scale to be identical 
vith those in the X-scale. To preserve areas we have to increase the height 
of the rectangles on the Z-scale by multiplying by . The area in the 
X-scale was, for a rectangle centered at x, 
base X height = 1 X b(x; 8, ) = b(x; 8, �). 
The corresponding rectangle on the Z-scale, with height  that of the 
rectangle on the X-scale, has the same area: 
base X height = { X -b(x; 8, �) = b(x; 8, �). 
More generally, for any binomial distribution, the rectangle centered at 
x on the X-scale has area 
base X height = 1 X b(x; n, p) = b(x; n, p). 
The corresponding rectangle on the Z-scale, with height equal to that 
of the rectangle on the X-scale multiplied by v/-pq, has the same area: 
base X height = (Vpq) X [x/pq b(x; n, P)] = b(x; n, P), 
as we require. 
:Note that the height of the binomial ordinate has been multiplied by 
x/pq, a quantity proportional to x/. Recall that the flattening of the 
9,79, TVO TYPES OF OUTCOMES: BINOMIAL DISTRIBUTION [CHAP. 7 
central binomial ordinates was iversely proportional to x/. This means 
that the distribution of Z does not have the flattening feature because 
x = 1. 
To summarize: The random variable 
Z X -- np 
�Zpq 
has tz = 0, az = 1 and these values equal the mean and standard 
deviation of the standard normal distribution. Furthermore, as n grows 
large the central ordinates (heights of rectangles) of the area probability 
graph of Z do not fiatten. Since tz = 0, walk-off does not occur; and 
since az = 1, spreading does not occur for the central mass of the dis- 
tribution. 
In Section 7-7, we shall study the behavior of the probability function 
of the new random variable Z as n increases and show its relation to the 
normal distribution. But before we do that, we shall see how to use the 
table of areas under th6 normal curve to approximate binomial proba- 
bilities. 
(3) Using the normal tables for binomial problems. Given the binomial 
distribution with n = 8, p = �, we find 
#x = np ---- 8(�) = 4, x = vrpq ---- x/8 ;-;1- 
\2/\2/ 
= X/  1.41. 
The graph with areas as probabilities for this distribution is shown in 
Fig. 7-9. 
If we use the areas of Fig. 7-9 to evaluate the probability of 3 or more 
successes in this example, we want to include all the area in the rectangles 
above the x-axis to the right of x ---- 2�. If we used only the area to the 
right of x = 3, we would leave half of P(a) behind. We therefore take 
as the left-hand boundary for x the value 2�. 
We use a standard normal table to obtain an approximation of the 
area to the right of x: 2�. To do this, we need to change from P(X >_ 2�) 
to P(Z >_ z), and we have for z, the left-hand boundary value of Z, 
x -- np 2� -- 4 
z .... 1.06. 
X/pq V 
7--6] TOOLS FOR STUDY1N(] BINOMIAL DISTRIBUTION LIMIT 9,73 
03-- 
0.2 - 
FxG. 7-9. Area graph for [he binomial distribution n = 8, p = . 
Therefore P(X _> 2�) = P(Z _> --1.06). We approximate this by the 
area under the standard norlnal curve to the right of --1.06, obtained with 
the help of Table III. The area from z = 0 to z ---- -t-1.06 is the same as the 
area from z ---- --1.06 to z = 0 or, froIn the tables, 0.3554. The area to the 
right of 0 is �. So the total probability is 0.3554 '--0.5000 ---- 0.8554. 
This compares well with the true binomial 4-place answer 0.8555. 
NOTS. We included the tail area of the normal that goes to infinity; 
we did not stop with the z correspondilg to x = 8�, the right-hand bound 
of the rightmost rectangle. It is customary to regard the small area under 
this long right-hand tail of the normal as part of the area corresponding 
to the rightmost rectangle. A similar remark applies to leftmost rectangles. 
E'C_MPLE 2. Extreme ordinate. For n = 8, p = �, approximate P(8), 
using the normal table. The left boundary of the rectangle centered at 
x = 8is7  Therefore 
2' 
z 7� -- 4 
--  2.47. 
The normal area to the right of 2.47 is 0.0068. The exact value is P(8)  
(�)s _--   0.0039. The absolute magnitude of the error is not large, 
but the percentage error is nearly 75%. 
 approximate P(4), 
EXAMPLE 3. Central ordinate. For n = 8, p = , 
using the normal table. The rectangle boundaries are x = 3� and 4�, 
274 TWO TYPES OF OUTCOMES: BINOMIAL DISTRIBUTION [CHAP 7 
so the left and right boundaries For z are 
z -- 3� -- 4 4� -- 4 
 --0.35, z --  +0.35. 
The area from 0 to -]-0.35 is 0.1368, and we need to double this to get 
P(4)  0.2736. The true binomial value is 
P(4) =   0.2734. 
Note that both the absolute error and the percentage error are less in 
this example for a central ordinate than for the tail ordinate in the previous 
example. The method ordinarily approximates central ordinates better 
than tail ordinates. 
In the next section we show that the foregoing tools do the job; they 
enable us to show that for large n the adjusted binomial distribution is 
approximated by the standard normal distribution. 
EXERCISES FOR SECTION 7-6 
1. Why do we want to study the normal approximation to the binomial 
distribution? Give three reasons. 
2. (a) Plot side by side for n = 4, p = 0.3, the ordinary binomial proba- 
bility function (see Fig. 7-4, for eample), and the corresponding representation 
by areas (see Fig. 7-8b). (b) If you find the area to the left of x = 0 in the area 
diagram, what value do you get? (c) If you find the area to the left of x = � 
what value do you get? (d) Is b(0; 4, 0.3) the answer to (b) or to (c)? Explain 
what the other answer is. 
3. (Continuation.) (a) Over what interval would the area be taken to find 
P(2 _ X _ 3)? (b) Over what interval would the area be taken to find 
P(0 _ X _ 2)? Would other intervals do? How about the interval with 
left boundary at --87? How far to the left can you place the left-hand boundary? 
4. (a) For n = 4, p = , make an area chart for the binomial. Don't forget 
to label the axes and put on the scales. (b) Calculate  and  for this distri- 
bution. (c) Now make an area chart for Y which is X -- , again labeling axes 
and putting on the scales. (d) Now change the scale to Z which is (X -- )/, 
and make an area chart for Z, labeling axes and scaling them. Don't forget 
to adjust the heights of the rectan;les. 
5. (Continuation.) For n = 4, p = �, use the data from Eercise 4 and 
your normal tables to compute the normal approximation to 
(a) P(X = 2), (b) P(X = 0), (c) P(0 _ .� _ 2), 
(d) P(X _ 2), (e) P(O _ X _ 4). 
7--7] THE CENTRAL LIMIT THEOREM FOR THE BINOMIAL 275 
7-7. AREAS FOR BINOMIAL DISTRIBUTIONS TEND TO AREAS UNDER 
THE NORMAL AS n GROWS--THE CENTRAL LIMIT THEOREM FOR 
THE BINOMIAL 
In this section we illustrate how the area graphs for adjusted binomial 
distributions tend to the shape of the normal distribution. We shall also 
numerically compare probabilities obtained from the binomial tables vith 
those obtained from their normal approximations. We study a sequence 
of binomial distributions for a fixed p. To keep track of the value of n, 
it is convenient to make n a subscript on any variable arising from a 
binomial experiment with number of trials n. Thus X, is the number 
of successes in a binomial experiment composed of n trials. We review 
the adjustments of Section 7-6 briefly. 
We have seen that the distributions of the variables X, walk off and 
flatten as n increases. To prevent walk-off, a new set of variables Y, ---- 
X,, -- nv is introduced. The mean of the distribution of X, is np, and 
the standard deviation is x/gpq. Consequently, the mean of Y, is 0, 
and the standard deviation is x/pq. Thus each Y, has the same mean 
as the standard normal distribution but, in general, does not have the 
same standard deviation. 
The sequence of distributions of Y, still flattens as n increases, but this 
can be prevented by introducing a third set of variables Z, = Y,/x/pq. 
Note that we divided Y, by its standard deviation. This choice gives an 
adjustment inversely proportional to x/, which, from the work in Sec- 
tions 7-5 and 7-6, we know we need. Furthermore, the choice of the 
standard deviation as the divisor makes each Z, have standard deviation 
1. Thus the distribution of each Z, has mean 0 and standard deviation 1, 
just as the standard normal distribution does. These agreements make 
the use of the approximation seem promising. 
We wish to discuss the sequence of probability area graphs corresponding 
to Z, Z2,.. �, Z,,... for a given p. Recall that the probability ordinates 
for Z, are the ordinates for X, multiplied by x/npq to get the correct 
ordinates for the area probability graph. We are especially interested in 
large values of n, but we shall illustrate for sequences of distributions 
where n is of moderate size. 
In Fig. 7-10 we show, in parallel columns, area graphs of the distribu- 
tions for X,, Y,, and Z, for p ---- � and n ---- 2, 4, 8, and 16. On the area 
graph for Z, a standard normal curve is superimposed so that you can 
judge by eye the agreement between these area graphs and the normal 
curve. 
We want a numerical as xvell as a visual assessment of the agreement 
between the graphs of the normal and the adjusted binomials. Table 7-3 
shows a comparison between the binomial values from Tables IV-A 
276 T\VO TYPES OF OUTCOMES: BINOMIAL DISTRIBUTION [CHAP. 7 
04 
04 
O O O r 
02 ,  2 02 
02 
O1 I O1 
O1 - 
0 2c2 0 , , , I, ?J2 0 L". , I 2 
0 2 --2 0 2 --2 1 0 1 2 
04 04-- 04 
03 03t- 0 
02 02 0 
01 O1 0 
o -- 14 0 Y4 0 , z 4 
0 2 4 --2 0 2 --2 1 0 1 2 
04 04 
0 
03 03 
0 I '  ' 02 02 
O1 t O1 O1 
0 -- I I I I I I I J8 0 ' ' ..J 0 , , 
0 2 4 6 8 4-2 0 2 4 2 1 0 1 2 
0 04 
O4 
0 03f 03 
= 16 
Ol I Ol 
0 - 1 lb O[ t/l 6 ol -'lb 
0 2 4 6 8 10 1214 6 4 2 0 2 4 6 2 --1 0 1 2 3 
Fid. 7-10. Area graphs for binomiM approaching normal,/ = �. Horizontal 
scale for z differs from that for .n and 
7-7] THE CENTRAL LIMIT THEOREM FOR THE BINOMIAL 277 
TABLE 7-3 
TABULAR VALUES FOR SINGLE BINOMIAL TERMS AND FOR CUMULATIVE 
BINOMIALS FOR p  � TOGETHER %rITH THEIR NORMAL APPROXI- 
MATIONS CORRESPONDING TO THE GRAPHS OF FIG. 7--10. 
Normal approx. Normal approx. 
x b(z;n,�) rob(x) P(X>_ x) toP(X>_ x) 
n = 2: 0 250 .240 1.000 1.000 
1 500 .520 .750 760 
2 .250 .240 .250 .240 
n = 4: 0 .062 .067 1.000 1.000 
1 .250 .242 .938 .933 
2 .375 .383 688 .691 
3 .250 .242 .312 .309 
4 062 .067 062 .067 
n = 8: 0 .004 007 1.000 1.000 
1 .031 032 996 993 
2 .109 .106 .965 961 
3 .219 217 .855 .856 
4 .273 .277 .637 .638 
5 .219 217 .363 .362 
6 109 .106 .145 .144 
7 031 .032 .035 .039 
8 004 .007 .004 007 
n = 16:0 .000 .000 1 000 1 000 
1 .000 000 1 000 1 000 
2 .002 002 1 000 999 
3 .009 .009 998 997 
4 .028 .028 989 988 
5 .067 .066 962 .960 
6 122 .121 895 .894 
7 .175 .175 773 .773 
8 .196 .197 598 .599 
9 .175 175 402 .401 
10 122 .121 227 .227 
11 .067 .066 105 .106 
12 .028 028 .038 040 
13 .009 009 011 .012 
14 002 .002 002 .003 
15 .000 .000 .000 .001 
-978 TVO TYPES OF OUTCOMES: BINOMIAL DISTRIBUTION [CHAP. 7 
05� 05! 05-- 
04 04 04-- 
03[ 03[ 03-- 
02[ 02 t 02- 
01 01 01 
0 r5 0 [J 0 '   ', ' 
2 4 -2 0 2 4 -2 1 0 1 2 
04 0 - 04 i [ 
03-- 0 03 
 = 20 
O1 01-- 01- 
0  , x20 , Qo 0 L  I  i      
0 2 4 6 8 0 4 2 0 2 4 -2 - 0  2 2 
0 03m' 
0 I , = ll) 02 
0 ll , 0 l 
I 
o  'tto o 
0 2 4 6 8 10 12 14 16 2 1 0 1 2 't� 
F6. 7-11. Area graphs for binomial approaching normal, p = 0.2. Ilori- 
zontat scale for z,, differs from that for xn and yn. 
7-7] THE CENTRAL LIMIT TttEOREM FOR TIlE BINOMIAL 279 
TABLE 7--4 
ORMAL APPROXIMATIONS TO BINOMIAL PROBABILITIES 
FOrt p = , coRnnsPOXDING TO FIG. 7--11. 
ormal approx 
t b(a, , �) to b(x) t'(X > t) to P(X > 
n = 5 0 32S 28 1 000 1 000 
1 410 424 672 712 
2 205 241 263 2SS 
3 051 044 058 047 
4 006 003 007 003 
5 000 000 000 000 
n = 10 0 107 118 1 000 1 000 
1 268 229 S93 8S2 
2 302 307 624 654 
3 201 229 322 346 
4 OSS 094 121 
5 026 021 033 024 
6 006 003 006 003 
7 001 000 001 000 
$ 000 000 000 000 
n = 20 0 012 025 1 000 1 000 
1 058 056 98S 975 
2 137 120 931 919 
3 205 189 794 799 
4 212 221 5S9 610 
5 175 1S9 370 390 
6 109 120 196 201 
7 055 056 0S7 OS1 
S 022 019 032 025 
9 007 005 010 006 
10 002 .001 003 001 
1! 000 000 001 000 
12 000 000 000 000 
n = 40:0 .000 002 1 000 1 000 
1 001 004 1 000 998 
2 006 010 999 995 
3 021 023 992 9S5 
4 047 046 972 962 
5 085 07S 924 917 
6 .125 115 839 S38 
7 151 145 714 723 
S 156 157 563 57S 
9 139 145 407 422 
10 107 115 268 277 
11 073 078 161 162 
12 014 046 OSS 0S3 
13 .024 023 043 038 
14 Oli 010 019 015 
15 003 004 OOS 005 
16 002 001 003 002 
17 001 000 001 000 
IS 000 000 000 000 
280 TVO TYPES OF OUTCOMES' BINOMIAL DISTRIBUTION- [CHAP 
and IV-B and the probabilities approximated by normal areas by the 
methods described in Section 7-6. Comparisons are given both for single 
binomial terms and for cumulatives. You can readily see that the results 
are in close agreement, and that they improve as n increases. 
The asymmetric binomials tend to the normal more slowly than do the 
symmetric ones, so we also show in Fig. 7-11 a sequence for p = 0.2, 
n = 5, 10, 20, and 40. Table 7-4 shows the corresponding numerical 
comparisons between binomial probabilities and their normal approxima- 
tions. By n = 20, the maximum error in a single term is 0.017, and in 
the cumulative it s 0.021. 
In Figs. 7-12 and 7-13 we illustrate the comparison between the standard 
normal and the area graph of Z for n = 100, p = �, and p = -}. Tables 
7-5 and 7-6 give the corresponding numerical comparisons. By n = 100, 
the agreement is quite close--within 0.001 in the cumulative for p = 
and within 0.010 for p = �. 
These graphical and numerical results are intended to illustrate, though 
not to prove, that the limit of a sequence of adjusted area graphs of bi- 
nomial distributions with fixed p and increasing n is a normal distribution. 
The result is put more precisely in the following theorem that we state 
and use but do not prove. 
7-3 Theorem. De oivre-Laplace Theorem. Let X1, X2, �.., X,... 
be a sequence of random variables, where X is the number of 
successes in a binomial experiment with n trials, each with proba- 
bility of successp, 0 < p < 1. LetZ,n = 1,2,...,bethecor- 
responding sequence of adjusted random variables, where 
Z X -- np 
-- , 
x/pq 
and let z be a constant. Then, as n approaches infinity, P(Z, _> z) 
approaches the area to the right of z for the standard normal 
distribution. 
REMARK. In practice, the result of the theorem says that for large 
values of n 
=p Z> x 2 
where Z is a standard normal random variable, and 
1 
.c 2 np 
z  Pq (1) 
7-7] THE CENTRAL LIMIT THEOREM FOR TItE BINOMIAL 2Sl 
TABLE 7--5 
UMERICAL VALUES CORRESPONDIXG TO FIG. 7-12. 
n = 100, Normal appox Normal apptox 
p = �. r b(z, 100,�) rob(z) P( z) toP(X ) 
34 000 000 1 000 1 000 
35 001 001 999 999 
36 002 002 998 998 
37 003 003 997 997 
38 004 005 994 994 
39 007 007 990 989 
10 011 011 982 982 
41 016 016 972 971 
42 022 022 956 955 
43 030 030 933 933 
44 039 039 903 903 
45 048 048 S64 S64 
46 058 058 816 816 
47 067 067 758 758 
48 074 074 691 691 
49 078 078 618 618 
50 OSO 080 540 540 
51 078 078 460 460 
52 074 074 382 382 
53 067 067 309 309 
54 058 058 242 242 
55 048 048 184 184 
56 039 039 136 136 
57 030 030 097 097 
58 022 022 067 067 
59 016 016 044 045 
60 011 011 028 029 
61 007 007 018 018 
62 004 005 010 011 
63 003 003 006 006 
64 002 002 003 003 
65 001 001 002 002 
66 000 000 001 001 
67 000 000 000 000 
04 L 
i)2  
0 L  -'-'-- z O0 
-3 -2 1 0 1 3 
Fm. 7-12. Area graphsfor binomml approaching normal, p = �, n = 100. 
VO TYPES OF OUTCOMES: BINOMIAL DISTRIBUTION [CHAP. 7 
TABLE 7-6 
-UMERICAL VALUES CORRESPONDING TO FIG. 7-13. 
Normal approx Normal approx , 
z b(z, loo,�) to b(z) P(x > z) to P(x > x) 
6 000 000 1 000 1 000 
7 000 001 1 000 1 000 
S 001 001 1 000 999 
9 001 002 999 998 
10 003 004 998 996 
11 007 OOS 994 991 
12 013 014 987 983 
13 022 022 975 970 
14 034 032 953 948 
15 048 046 920 915 
16 06t 060 871 870 
17 079 075 808 809 
1S 091 OSS 729 734 
19 098 096 638 646 
20 099 099 540 550 
21 095 096 441 .450 
22 085 0SS 346 354 
23 072 075 261 266 
24 05S 060 189 191 
25 044 046 131 130 
26 032 032 087 085 
27 02 022 056 052 
28 014 014 034 030 
29 009 OOS 020 017 
30 005 004 011 009 
31 003 002 006 004 
32 002 001 003 002 
33 001 00! 002 001 
34 000 000 001 000 
35 000 000 000 000 
' �100 
'2 1 o 2 3 
Area graphs for binomml approaching normal, p = �, n = 100. 
7-7] THE CENTRAL LIMIT THEOREM FOR THE ]3INOMIAL 28:{ 
EXAMPLE. For n = 100, p = -, find P(�100 >_ 24). 
Solution. We find 
1 
x 2 P 24 i 20 
2 
z = = -- -0.875. 
x/pq x/100(�)() 
The area to the right of 0.875 in the normal table is 0.191, close to the 
true three-place value 0.189 obtained from large binomial tables. 
The � adjustment in the lmmerator of the expression for z in Eq. (1) 
is the sanhe adjustment that we used in our normal calculations at the 
close of Section 7-6, so as not to omit half the probability at x. (See 
Fig. 7-8b.) 
The De Moivre-Laplace Theorem is one form of a quite general =et of 
"central limit theorems." These theorems treat the limiting distributions 
of sums of random variables, and these limiting distributions are ordinarily 
normal. The value of the theorems is that they enable us to compute 
approximate probabilities for sums using the ln_ormal distribution without 
ever knowing the exact distribution of the sum. Exact distributions are 
often hard to get, so we are grateful for such approximations. 
In this section we have illustrated the approach to normality of a 
sequence of adjusted binomial distributions. Since the number of suc- 
cesses, X, in a binomial experiinent is an example of a random variable 
which is itself the sum of several independent random variables, we have 
also illustrated the more general idea that sums of independent random 
variables, suitably adjusted, tend to be normally distributed, under quite 
general conditions. 
No'r:. Accuracy of the approximation. Although the De Moivre-Laplaee 
Theorem is an exciting mathematical result, its practical value is the 
suggestion that the normal approximation may fit the binomial well even 
for moderate values of n. Matheinatieal investigations of the error in the 
normal approximation are extremely difficult, and their results are not 
easy to report here. However, extensive empirical investigation suggests 
that when the mean/x is "far" from 0 and ,, the extreme values of X, the 
approximation is quite good. If/x is at least 3o- from both 0 and n then it 
appears that the maximum error in evaluating a single term is at most 
0.011, and in the euinulative at most 0.025 
It will be observed in Table 7-3 that for n = 8, p ----- � the maximum 
errors in the individual terms and in the cumulative are both 0.004. Yet 
n ----- 8 does not quite put /x a distance of 3o- from the extremes. For 
n ---- 40, p = ,/x is more than 3o- from both extremes, and the maximum 
errors are 0.010 and 0.015 for individual terms and for eumulatives 
respectively. 
28 TVO TYPES OF OUTCOMES: BINOMIAL DIS,TRIBUTION [CHAP. 7 
EXERCISES FOR SECTION 7-7 
1. Froin Table 7-3, compute the maximum error in the approximation for a 
single term for n = 2, 4, 8, 16. Plot these maximum errors against a scale of 
l/n, and from the approximate linearity see that the maximum error is roughly 
proportional to 1In. 
2. Compute the maximum error in the approximation for the cumulatives 
for n = 2, 4, 8, 16 from Table 7-3. Plot these maximum errors against 1In. 
From the approximate linearity, what do you conclude? 
3. (a) Carry out the graphical work of Exercise 1 for Table 7-4. Note that 
the result for n = 5 does not line up with the rest of the points. (b) From 
Table 7-6, obtain the maximum error for n = 100 for individual terms. Drop 
n = 5, enlarge the scale for l/n, and plot the points for n = 10, 20, 40, 100, 
and note that the points fall close to a line through the origin. [Remark. We 
do not have tables extensive enough to study the corresponding result for the 
cumulative for Table 7-4.] 
4. For n = 8, p = �, plot the area chart of Z carefully on graph paper to 
large scale and get a larger version of the graph shoxxn in Fig. 7-10. Then from 
the table of normal ordinates, Table 6-7, plot points for the normal ordinates 
at intervals of � and sketch in the normal curve. 
5. For n = 9, p = �, do the graphical work corresponding to Exercise 4. 
6. A high-school basketball player sinks 60% of his foul shots, in the long 
run. During a season he'got 100 tries. Approximately, xhat is the probability 
that he sinks 70 or more of these? What is the probability that he makes 
exactly 60? 
7. If 1000 coins are tossed, the most likely outcome is 500 heads. l_'se the 
normal approximation to find how likely that outcome is. Compare your 
numerical ansxer with a reading from Fig. 7-6. 
8. (Continuation.) The area of a rectangle is base X height. We could 
approximate the result in Exercise 7 by multiplying the height of the normal 
ordinate at z = 0 by 1/, because the wdth of the rectangle is 1 in the x-units, 
and 1/ in the z-units. Make the calculation and compare it xxith the result 
obtained in Exercise 7. 
CHAPTER 
8 
SOME STATISTICAL 
APPLICATIONS OF 
PROBABILITY 
8-1. ESTIMATION AND THE TESTING OF HYPOTHESES 
Our work in the general theory of probability and our detailed study of 
the family of binomial distributions are applied in this chapter to a few 
problems in statistical inference. We focus attention on two related 
problems--estimation and hypothesis testzg. Each is studied first without 
the use of prior information, as it would ordinarily be treated by probabil- 
ists of the objective school, and then with the use of prior information, in 
the manner of the personalistic school. 
Starting with our experiments in Chapter 1, we have used observed 
averages to estimate population means and observed proportions to esti- 
mate p, the binomial probability of a success. These estimates are familiar 
and natural, but we have not established their variability or reliability. 
If a professional basketball player sinks 65 foul shots out of 100, we esti- 
mate the value of p, the probability that he sinks a foul, to be 6  _ 0.65 
100 -- 
Assuming that the outcomes of successive shots are independent (not too 
sfe an assumption in this example), how sure are we that 0.6 _ p _ 0.7? 
The method of confidence limits presented in this chapter gives one way 
of making probability statements about such an inte?'ual estimate. 
Suppose that the basketball player has a long history of foul-shooting, 
with an average success of 0.54. Is there good reason to suppose that his 
new performance of 0.65 represents a change in his probability of making 
a successful foul shot? Ths kind of question s treated n the statistical 
testing of hypotheses, discussed later in this chapter. Similar problems 
arise when a new medication for relief from headaches is proposed. Does 
the new medication relieve more headaches than the usual remedy? 
Does a new dust for the disease Botrytis reduce the number of affected 
plants, as compared with no treatment? 
285 
286 SOME STATISTICAL APPLICATIONS OF PROBABILITY [CHAP. S 
If prior information is available, both the theory of estimation and 
that of hypothesis testing can be extended by the use of Bayes' Theorem. 
In Sections 8-3 and 8-5 we discuss examples of such extensions within the 
limitation of discrete probability distributions. 
8-2. ESTIMATING p, THE BINOMIAL PROBABILITY OF SUCCESS 
An estimate of p. To establish our notation, recall the usual vay of ob- 
tailting a numerical estimate of p, the probability of success for a single 
trial of a binomial experiment. We execute n trials, count the number of 
successes x, and compute x/n =  to obtain a value for the estimate of p. 
If team A beats team B 4 times in 20, we estimate A's probability of 
beating team B as �60 = 0.2. Now we wish to discuss the properties of 
the estimate , giving special attention to its variability. What is its 
mean and what is its standard deviation? Answers to these questions 
follow easily from our work in Chapter 7. 
The usual estimate of p is 
X 
n 
where X is the number bf successes. Since X is a random variable, so is . 
Thus the value of  changes from one binomial experiment of size n to 
another. The possible values of X are x ---- O, 1,..., n, and so the pos- 
sible values of  are x/n = O, l/n, 2In, . . . , (n -- 1)In, 1. 
Mean of . What is the mean value of ? Since X is the random variable 
denoting the number of successes, E(X) = np. To get the mean of the 
random variable  = X/n, we have, from the definition of , 
E() = E (--Xn) � (1) 
Recall that E(cX) = cE(X), where c is a coilstaint. Because 1/n is a 
constant, we apply this theorem to Eq. (1) and get 
i E(X) i (rip) 1o. (2) 
= n n 
Thu "on the average" we get the correct value of p, which is the result 
most people expect. This feature has a name' we say that  is all unbiased 
estimate of p. Lack of bias gives some backing for the use of  as an 
estimate of p. 
Closeness. That an estimate has a long-run mean with value p is not an 
adequate basis for its use. We ought at least to know that  is often close 
to p. To illustrate this need, consider the outcome of a 19inomial experi- 
8-2] ESTIMATING p, TIlE BINOMIAL PROBiBILITY OF SUCCESS 287 
merit of size n = 100. Let us deliberately throw away the results of the 
last 99 trials and estimate the probability of success to be 1 if the first 
trial resulted in success, and 0 otherwise. This method of estimating is 
also unbiased because it corresponds to B/% with , = 1 and B the 
number of successes on the first trial, and here again 
Yet we do not like this estimate very xvell. Indeed we could eaqily be 
persuaded to use, instead, X/99 or X/101, with X the number of successes 
on the 100 trials, even though these estimates are not unbiased, since 
lO0 lOO 
k99/ = 99 p' E 1 -- 1 p' 
What is missing from our discussion is some notion of closeness. We 
want an estimate that is more likely than other estimates to be close, in 
some sense, to the true value of io. 
As just stated, the notion of closeness is rather vague, and xxe shall not 
pursue it. But we can find from binomial tables how often  is within a 
given distance of p. 
Distribution of  for small n. For given values of p, and n  25, we can 
obtain the probability function for  from our binomial tables. The only 
difference from our work in Chapter 7 is that we change the horizontal 
scale from x to . Figure 8-1 illustrates this for n = 10, p = 0.2. The 
ordinates for x = 0, 1,..., 10 are found in the binomial tables. But 
instead of plotting b(x; 10, 0.2) against x, we plot it against x/n, or x/10. 
The bulk of the probability in the binomial distribution with n: 10, 
03 
0 = m = io 
Values of 7 = 
Fro. 8-1. Probability function for ,  = 10, p = 0.2. 
288 SOME STATISTICAL APPLICATIONS OF PROBABILITY [CHAP. 8 
p = 0.2, is betveen 0 and � (= -6), inclusive. Thus we can be practically 
certain that  is within 0.3 of p if n = 10, p = 0.2. Furthermore, from 
Table IV, A or B, we can compute for n: 10, p = 0.2, the probabilities 
P(p-- 0.1 _  _ pq-0.1)  0.77, 
P(p --0.2 _<  _< pq-02)  0.97. 
The first is the probability that  falls within 0.1 of p; the second, the 
probability that it falls xvithin 0.2 of p. Alternatively, xve use the absolute 
value symbol, 1 -- p[, to denote the distance from p to . (See Appendix 
I, Section I-2.) Then 
P(i-- P[ - 0.1)  0.77, 
and 
P(] -- p[ _< 0.2) = 0.97. 
In this example, values of  are whole numbers of tenths. Consequently, 
the probability that  is xvithin 0.1 of p = 0.2 is also the probability that 
 is within 0.12, 0.15, or 0.1999 of p ---- 0.2. Thus when we speak of F 
as within 0.1 of p, we are choosing the shortest possible distance with 
probability 0.77, and if ve speak of  as within 0.199..., we are choosing 
the longest dstance with probability 0.77. Perhaps, for a fairer picture, 
we should regard the typical distance here as 0.15. 
By similar computations for many different values of p, we could make 
a graph of the probability that  is within, say, 0.15 of p. Figure 8-2 
shows such a graph for n = 10. (Extensive binomial tables were used in 
its construction.) Observe that for p = 0.2, P([ -- p[ _< 0.15) = 0.77, 
as computed above. 
Figure 8-2 is a good example of a discontinuous curve. The value at 
exactly p = 0.05 is indicated by the dot on the tipper branch. The value 
at p ---- 0.15 is indicated by the isolated dot at the top of the broken verti- 
cal line. An example will explain how the discontinuities at 0.05, 0.15, 0.25, 
and so on, come about. For  to be within 0.15 of p = 0.14, say,  must 
lie in the interval from p- 0.15------0.01 to p q- 0.15----0.29, and 
 = 0.0, 0.1, and 0.2 are the only values in this interval because values 
of  are whole numbers of tenths when n = 10. Similarly, for p = 0.16 
the interval for  runs from p -- 0.15 = 0.01 to p q- 0.15 = 0.31 and 
 = 0.1, 0.2, and 0.3 are in the interval. Even though p = 0.14 and 
p = 0.16 are dose, the fact that different sets of 's fall in their intervals 
makes the graph jump. Exactly at p ---- 0.15,  = 0.0, 0.1, 0.2, and 0.3 
are in the interval p 4- 0.15, and since this interval has 4 values of  
instead of 3, there is a high dot at p = 0.15. The figure illustrates that 
for some purposes we need a table xvith a finer grid for p than our Table 
IV has. 
8-2] ESTIMATING p, THE BINOMIAL PROBABILITY OF SUCCESS -989 
03 
0 '2 
Ol 
0 01 02 03 04 05 0.6 0.7 os I) 9 ll) 
p 
Flo. 8-2. Probability that  is within 0.15 of p, for0_ p _ 1;n = 10. 
In spite of its complexity, Fig. 8-2 has a fairly simple message. With 
n ---- 10, the probability is at least 0.64 that  is within 0.15 of p for all 
values of p. We also see that vhen p is near 0 or 1, ve are more likely 
to find  in the interval p  0.15 than vhen p is near . Similar curves 
could be drawn for other values of the distance and for other values of n. 
Fortunately, for large values of n we can forego the tiresome task of 
draving the graphs, for reasons that we now investigate. 
Large samples. If we know the variance of , or X/n, we can take 
further steps to answer questions about closeness. Although the proof 
that e = npq has been postponed to Chapter 9, where it can be carried 
out quite easily, we shall use this information here. 
In Section 5-4 we noted that Var (cX) = c%., for c a constant. We 
 where  = X/n. The number 1/n is a constant, therefore 
nov find  
a = Var = Var X 
1 . =  (npq) Pq (3) 
n 2  
290 SOME STATISTICAL APPLICATIONS OF PROBABILITY [CHAP. 8 
or 
 ---- . (4) 
This result is encouraging, and rather as we expected. Althoughx in- 
creases as n increases for fixed p,  decreases as n increases. 
Recall froin Chapter 7 that  is the a,,erage of the values of n random 
variables, each of xvhich has value 0 or 1, xvhile X is the sum of these 
same n random variables. The results for x and  are symptomatic of 
a more general fact: stms of independent random variables ordinarily vary 
more than their compo,tent random variables, whzle averages raw less. 
EXAMPLE 1. In a biological study, 1000 fruit flies are to be counted 
for the presence or absence of a certain characteristic. If  is used to 
estimate p, the probability that a fruit fly has the characteristic, how near 
p xvill the value of  be? 
Sohdion 1. Conservatwe Chebyshe,, approach. We apply Chebyshev's 
Theorem to the random variable , with mean / = p and standard 
deviation  = x//n. Thus the probability that p is within h of p 
is at least 1 -- 1/h 2. One difficulty is that we don't know  because we 
don't knmv the value'of p. However, we can find the value of p that 
maximizes - == pq/n. Since the graph of pq = p(1 -- p) is a parabola 
that is symmetrical about the line p = �, the maximum value of pq 
is attained when p = q = �. Therefore the maximum value of pq is 
-. - -- - and 
2 2 -- 4, 
o- = Pq -< n = x 4/n 
As before, we write the distance from  to p as - pl. Since p is the 
mean of , xve can say, from Chebyshev's theorem with h = 2, that the 
probability is at least 0.75 that the distance of  from its mean, p, is no 
more than 2: 2 
Similarly, xvith h = 3 the probability is at least 0.88 that the distance 
3 
pl < ; 
x/4n 
and, in general, the probability is at least 1 -- 1/h 2 that the distance 
h 
pl < ' (5) 
8-2] ESTIMATING p, THE BINOMIAL PROBABILITY OF SUCCESS 291 
For our example, if n = 1000 and if we choose h = 2, the probability is 
at least 0.75 that 
2 1 
{- p{ _< -- - 0.032. 
Or, in words, at least 75% of the probabihty distribution of  is within 
0.032 of its mean, p. 
Solution 2. Conservative normal approach If we are confident that np 
is at least 3x/pq from both 0 and n (see the Note on accuracy of the normal 
approximation to the binomial, Section 7-7), we are justified in using 
the stronger normal distribution theory instead of the Chebyshev Theorem. 
We know that Z---- (X- np)/x/pq is approximately normally dis- 
tributed. If we divide both nmnerator and denominator on the right 
by n, and recall that  ---- X/n, we have 
Z__--P 
v/n 
Now, since Z is approximately distributed according to the standard 
normal distribution, we can say that the probability is approximately 0.95 
that 
--2 < Z < 2 or --2 < -- p < 2, (6) 
_ _ _ v/pq/n 
where the 2's represent 2 standard deviations, to approximate the more 
precise 1.96 from the normal table. 
We now multiply all terms of the right-hand expression of inequality 
(6) by x/pq/n, and get 
--2x/pq/n _<  -- p <_ 2V/n, (6') 
or 
p[ < 2x//n. (6") 
Maximizing pq as before at pq = , we find froln the normal distribution 
that the probability is approximately 0.95, for p near � and larger other- 
wise, that 
2 1 
pl-< (7) 
If we choose h standard deviations instead of 2, the appropriate proba- 
bility is obtained from our normal table, Table III. (Throughout this 
derivation, we have ignored the � correction for continuity that we used 
in Chapter 7 to adjust our rectangular graphs for the binomial.) 
292 SOME STATISTICAL iPPLICATIONS OF PROBABILITY [CHAP. 8 
To return to the foregoing example, the co[servative normal ap- 
proximation gives the probability as at least 0.95 that 
1 1 
The approximation 0.95 from the normal distribution compares with the 
number 0.75 from Chebyshev's Theorem. 
These examples and demonstrations illustrate the following theorems 
about binomial experiments. All three theorems use the following nota- 
tion: X is the nmnber of successes, p is the probability of success on a single 
trial, n is the number of trials, and  ---- X/n is the estimate of p. 
8-1 Theorem. Mean and variance of . In a binomial experiment the 
mean of , , and the standard deviation of , , are given by 
' = = p, ($) 
and 
 = v. (9) 
8-:2 Theorem. Large sample distribution of Z. In a binomial experiment, 
for fixed p and large n, 
z_-p 
is approximately distributed according to the standard normal 
distribution. 
Discussiota Theorem 8-2 is equivalent to the final theorem of Chapter 
7. We gave illustrations for the distribution of X, but we cannot prove 
the theorem without much more advanced mathematics. Nevertheless, 
we shall use it to compute approximate probabilities. 
8-3 lheorem. Law of large numbers. For any positive number d, 
as n tends to infinity the probability tends to I that the inequality 
is satisfied. 
8--2] ESTIMATING p, THE BINOMIAL PROBABILITY OF SUCCESS 293 
Alternatively, xve may state the theorem thus: by making n sufficiently 
large, we can be as sure as we please that the estimate  is within a given 
nonzero distance d of p. 
Proof of Theorem $-$. For any positive number h, the folloxving in- 
equality is true: 
( 
The first inequality in (10) is true because every probability is less than 
or equal to one, and the lst inequality follows from the Chebyshev 
relation (5), Section 5-6. By proper choice of h, the distance d in Theorem 
S-3 can be equated to 
d -- --, (11) 
provided 
 = . 
Throughout the inequahty (i0), we substitute for  ts value 
from Eq. (i2), and get 
1 (13) 
Holding  fixed and letting  increase, we see from the first and last in- 
equalities in (i3) that 
tends to 1, as stated in Theorem 8-3. [] 
EXAMPLE 2. It is desired to use  to estimate p, with probability 0.97 
or higher, that  is within 0.05 of p. ttow large should n be? 
Solution 1. Using inequality (13), which is equivalent to usig the 
conservative Chebyshev approach, we take 
1 
d ---- 0.05, 1 -- ;lnd  -- 0.97. 
Solving for n, we find 
1 
n---- '(0.03)(4d) = 3333. 
Solution 2. Since n is evidently large, we apply Theorem 8-2' 
z_-p 
V-q/n 
294 SOME STATISTICAL APPLICATIONS OF PROBABILITY [CHAP. 
is approximately distributed according to the standard normal distribu- 
tion. The condition 
] -- p] _ 0.05 (14a) 
is equivalent to 
iZ I  0.05 . (14b) 
- 
The probability of the inequality (14b) is to be 0.97, meaning that the area 
under the standard normal curve from --O.05/x/pq/n to O.05/x/pq/n 
is to be 0.97. Hence the area from 0 to O.05/x/pq/n is �(0.97) - 0.485, 
so, from Table 
0.05 
__  2.17; 
v/n 
xvhence 
x//pq . 43.4, n  (43.4)2pq  1854pq. 
Although p is unknown, the maximum value of pq = p(1 -- p) is 3; hence 
ss4 __ 471 
n  4 
should be sufciently large. The contrast with the result obtained from 
applying Chebyshev's Theorem is remarkable. 
EXERCISES FOR SECTION 8-2 
Many of these problems require the use of Table IV-A or IV-B. 
1. What additional considerations, besides lack of bias, influence the choice 
of a method of estimation? 
2. Make a graph like that of Fig. 8-1 for the probability function of , 
n = 5, p -- 0.2. 
3. Use the three-place values from the binomial table IV-A for n = 5, 
p = 0.2, to obtain E() by direct calculation. 
4. For n = 5, p = 0.2, find P(I- P]-- 0.2). 
5. For n = 20, p = 0.01, make a table of the probability function for . 
6. By direct calculation from tabled values find E() for n = 20, p = 0.01. 
7. For n = 20, p = 0.01, find P(IP- P]-- 0.1). 
8. Check the value of the ordinate of Fig. 8-2 for p = 0.7. 
9. Compute the value of the ordinate of Fig. 8-2 for p = 0.75 (interpolation 
needed). 
10. For n = 3, make a graph like that of Fig. 8-2 for P(I -- P] - ). 
8-3] CONSERVATIVE CONFIDENCE LIMITS FOR p WITH LARGE n 295 
Fill in the missing cells in the folloxxing table' 
n p a 
1 
11. 4  
12. 9 0 2 
13. 100 0.05 
14. 0.1 0 1 
15. p p 
16. 1000 0 01 
17. Find the maximum value of a hen n = 4, 100, and 1000. 
18. For n = 5, p = 0.2, compute  directly from values in Table IV-A. 
Find the conservative Chebyshev and normal estimates for P([ -- p[ _< d) 
for 
19. d = 0.1, n = 1000 20. d = 0.05, n = 100 
21. d = 0.2, n = 10 22. d = 0.01, n = 400 
Use the conservative normal approximation to fill in the missing ('ells in 
the following: 
P([- Pl < d) d n 
23. 0.05 49 
24. 0.10 0.01 
25. 0.10 0.05 
26. 0.20 16 
27. 0.50 100 
28. 0.10 81 
29. 0.05 9 
30. The unknown size of a total population of animals is x. From tlns popu- 
lation, m are captured at random, marked, and released. On a second occasion, 
n arc capturcd, of which r arc found to be markcal. Suggest an estimate for x. 
8-3. CONSERVATIVE CONFIDENCE LIMITS FOR p WITH LARGE n 
In addition to reporting a value of  as an estimate of p, it may be 
helpful to make a statement to summarize our knoxvledge and our un- 
certainty about an inter;al in which p lies. The method of confidence 
limits offers a way to do this. In this method, we make a statement based 
on the result of the experiment. For example, xvith n = 100, x = 40, 
296 SOME STATISTICAL APPLICATIONS OF PROBABILITY [CHAP. 
and the value of  ---- 0.4, we can report vith about 95% confidence that 
the statement 
0.3 < p < 0.5 
is true. Each performance of an experiment gives rise to a satement 
that p is contained in a specific interval. Some statements will be true, 
some false, but 950/o confidence means that in the long run 950/o or more 
are to be true. 
The results of Section 8-2 enable us to construct such statements. We 
illustrate first xvith limits of 2 standard deviations that will give us 95% 
confidence. The approximate normality of the distribution of 
Z_--P 
x/pq/n 
for large n allowed us to write from inequality (7) of the preceding section 
that the probability is at least* 0.95 that 
2 1 
An alternative xvay t) vrite inequality (1) is 
< --p < --' (2) 
Since we want a statement about p, we wish it were alone as the middle 
member of an inequality. We achieve this in two steps. 
Step 1' Add -- to every member of the inequality (2) to get 
-< -P -< (3) 
Step 2: Change the sign of --p; multiply every member of inequality 
(3) by --1 and reverse the signs of inequality. The result is 
1 1 
 '- n -- p >-- --nn (4) 
* Since an approximation is involved, we cannot absolutely guarantee the 
"at least," but the result is usually conservative; the use of 2 standard devia- 
tions instead of 1.96 makes the "at least" even more forceful. 
8-3] CONSERVATIVE CONFIDENCE L1MITS FOR p V1TH LARGE n 297 
For example, with n = 100, and the value of  -- 0.4, inequality (4) 
gives 
0.5 > p> 0.3, 
and we are 0.95 confident that the statement is true. This statement, of 
course, is either true or false, but if in experiment after experiment xve 
construct such statements by the same inethod, at least 95% of the 
statements will be true in the long run. 
EXAWLE 1. Bou,ling. In a league season, a good bowler bmvled 400 
frames and got 120 strikes. Set conservative 95% confidence linfits on 
p, which measures his probability of a strike. (Assume independence 
between frames.) 
Sohdion. The value of  is 120/400 = 0.3. From inequality (4) the 
limits are 0.30 4- 1/---- 0.30 4- 1/20, or 0.25 to 0.35. Thus, we 
say 0.25 _< p _< 0.35, with 95, confidence. 
Generalization. If, in inequality (7) of Section 8-2, we use h rather than 
2 for our multiplier of 1/x/, the conservative estimate of (z, we have 
h 
Ip- p[ < (5) 
The probability that the statement is true, called the confidencc coe, l%cient, 
is at least the area under the standard normal distribution froin --h to 
+h. The same steps that led to inequality (4) lead to the coWidcnce state- 
merit 
h h 
or, "p is the interval  4- h/v/." The interval  4- h/vr is called a 
confidence interral. The numbers  + h/x/ and  -- h/v/ are called 
the upper and lower confidence limits, respectively. 
ExxrLE 2. In the bowling example, what confidence statement would 
be used for a 0.50 confidence coefficient? 
Solution. Froin the standard normal tables, xve find that the interval 
from --0.67 to +0.67 gives the desired area of 0.50. Therefore we choose 
h ---- 0.67. For the bowling example, xve get 
h 0.67 
 4- -- as 0.300 4- ____  0.300 4- 0.017, 
x/ v(400) 
and the lower and upper confidence limits are 0.283 and 0.317, respectively. 
298 SOME STATISTICAL APPLICATIONS OF PROBABILITY [CHAP. 8 
COMMENT. By rather heavy lgcbraic methods it is possible to compute 
limits that are not as conservative as these, and by using extensive tables 
ve can compute limits for small as well as large values of n. We shall not 
develop such methods here, but since there is a chart that is easy to use 
for 0.95 confidence limits, we present it as Chart I at the back of the book. 
We enter the chart with the observed value  on the horizontal axis, 
and erect a perpendicular from that point. The perpendicular crosses the 
two curves for the given value of n in two points. Using the vertical scale, 
we read the upper and lower confidence limits for p as the heights of the 
points. 
EX^MPLE 3. Use the curves in Chart I to get 95% confidence limits 
for the bowling example. 
Solution. Entering with  = 0.3 and interpolating roughly between the 
curves for n ---- 250 and n = 1000, we find 0.26 and 0.35 as the lower and 
upper confidence limits. Our own more conservative limits were 0.25 
and 0.35. 
Optimistic approach to confidence limits. If  is not close to � and n 
is large, the limits obtained by the conservative approach of substituting 
p ---- � into inequality (6 ) of Section 8-2 may be rather broad. A more 
optimistic value for the appropriate confidence interval could be ob- 
tained by setting p ----  under the square roots to give the general limits 
(7) 
where  = 1 -- . As usual, h is the number of standard deviations, and 
the confidence level is obtained from the normal table. These intervals 
are too small for small values of  or of 1 --  (consider  = 0 or  ---- 1), 
but they are fairly satisfactory over lnost of the range of . 
EXERCISES FOR SECT[ON 8-3 
All the following problems deal with binomial distributions. 
1. A random sample of 25 households from a large town shos that 10 buy 
nexspaper A. (a) Use inequality (4) to set 95% confidence limits on the pro- 
portion p in the town who buy newspaper A. (b) Use inequality (6) to set 50% 
confidence limits. (c) Set 95% confidence limits, using Chart I. 
2. In a sample of 20 students drawn from a large population, 16 recalled 
recently learned material better imlnediatcly after sleeping S hours than after 
S hours awake. Set S0% confidence limits on the population proportion p xxho 
xxouhl have performed better after sleeping S hours, had all been tested. 
3. If a andom sample of 50 families from Cambridge, Mass., showed 10 
$-4] BAYESIAN APPROACH, WITH PRIOR INFORMATION 299 
families ith incomes over $5000 during 1951, set a 90% confidence interval 
on p, the percent of families with incomes over $5000. 
4. On 100 different local telephone calls, a secretary fails to complete 25 at 
the first attempt. Use Chart I to set 95% confidence limits on p, the long-run 
proportion completed on the first attempt. 
5. Some parents of the 5th grade pupils in a large school system complained 
that their children could not read clear handwriting. A lengthy test on hand- 
written material showed that 225 out of a random sample of 250 pupils did read 
the handwriting ( -- 0.90). Use Chart I to set 95% confidence limits on the 
population proportion p reading handwriting. 
6. Use Chart I to set 95% confidence limits on p, the proportion of defective 
teacups produced, if a random sample of 25 had no defectives. 
7. Lse inequality (4) to decide on the sample size required to set a 95% 
confidence limit of total length less than or equal to 0.04. 
$. Use Chart I to answer Exercise 7 (note that the broadest limits occur when 
the value of  = �). 
8-4. BAYESIAN APPROACH WHEN PRIOR INFORMATION IS 
AVAILABLE 
In setting confidence limits, we give some notion of the unreliability 
of our estimate. Sometimes we may have prior information, and then 
we may try to combine it with experimental information to get an im- 
proved estimate of p. Bayes' Theorem, Theorem 4-9, offers some assistance. 
Since p is a continuous variable, it is natural to use calculus methods in 
this approach. To avoid the calculus, we shall act as if p had a discrete 
distribution. 
EXAMPLE 1. A manufacturing process has a machine that inspects 
every item for internal flaws. Over a long period, lots have had the follow- 
ing relative frequencies of percent of defective items. 
TABLE 8--1. DISTRIBUTION OF PERCENT DEFECTIVES. 
Relative frequencies of lots 0.6 0 3 0.1 
Percent defective 1 10 
Thus 60% of the lots are classified as in the 1% defective class, 30% in 
the 5% defective class, and 10% in the 10% defective class. 
The usual inspecting machine is broken, but the rest of the production 
process is working and a new lot is to be inspected by another more ex- 
pensive operation. A sample of 20 items is drawn from a large lot and none 
have internal flaws. What can we say of/, the proportion of defectives, 
for this lot? 
300 SOME STATISTICAL APPLICATIONS OF PROBABILITY [CHAP. 8 
Solution. We regard the discrete distribution of this example as ap- 
proximately the prior probability distribution for the random variable 
100p, the percent of defectives in a random lot chosen from this process. 
The new lot (the population from which the sample of 20 items is drawn) 
has some unknown percent of defectives, say 100po. On the basis of the 
past data, we want to use the sample information that 0 defectives were 
observed in 20 trials to find a posterior distribution for 100po. As usual 
in applying Bayes' Theorem, we set up in Table 8-2 the probabilities of 
getting this sample from each of the 3 possible compositions of the lot. 
We get the probabilities of 0 defectives in 20 trials from our binomial 
Table IV-A. 
For example, the prior probability that the new lot is in the 5% defective 
class is 0.3. Given the new lot is in this class, the probability of 0 defectives 
in 20 items is (0.95) 20  0.358. Consequently the probability that the 
new lot is in the 5% class and produces 0 defectives in the sample is 
0.3(0.358) = 0.1074. The posterior probability for the 5% class is the 
conditional probability that the lot is in the 5% class, given the sample 
outcome, or 0.1074/0.6104 = 0.176 (see Table 8-2). 
TABLE 8--2. CALCULATION OF POSTERIOR PROBABILITIES. 
Lot 
composition Probabilities of 0 defectives Posterior 
(% defective) in 20 trials probabilities 
1 .6(.99) 20  .6(.818) = .4908 .804 
5 .3(.95) 2o  .3(.358) = .1074 .176 
10 .1 (.90)2�  . 1 (. 122) = .0122 .020 
.6104 1.000 
The table of posterior probabilities suggests that the odds are 4 to 1 
that 100p0 is in the class symbolized by 1%, about I to 5 that it is in the 
class symbolized by 5%, and that the chance is very small that it is in 
the class symbolized by 10%. One estimate of 100po could be obtained 
by computing the mean of the posterior distribution (we round to two 
decimals): 
/: 1(0.80) -[- 5(0.18) -[- 10(0.02): 1.90. 
One advantage of the Bayes approach is that the probabilities derived 
apply to this lot, whereas the confidence coefficient of a confidence limit 
statement applies to a long sequence of confidence statements. A diffi- 
culty is to supply a prior distribution for p, with the view that the p for 
8--5] TESTING OF A BINOMIAL STATISTICAL HYPOTHESIS 301 
the new lot is drawn from that distribution. Practical problems in the 
use of prior distributions are currently being studied by experts in proba- 
bility and statistics. 
EXERCISES FOR SECTION 8-4 
1. In the textual example, replace the prior relative frequencies of lots 0.6, 
0.3, 0.1, by 0.7, 0.3, 0, respectively. Find the posterior probabilities and the 
mean,/, for the posterior distribution. 
2. In the textual example, replace the prior relative frequencies 0.6, 0.3, 0.1, 
by 0.4, 0.4, 0.2, respectively. Find the posterior probabilities and the mean, 
/, for the posterior distribution. 
3. In the textual exalnple, refer to Table 8-2. If the manufacturer uses 
the lot, he makes a profit of $100 if the lot s in the 1% class, loses $10 if the 
lot is in the 5% class, and loses $1000 if the lot is in the 10% class. Based on the 
posterior distribution, should he use the lot? That is, is his expected profit posi- 
tive? 
4. In the textual example, change the sample size to 25 and the number of 
defectives observed to 4. Find the posterior distribution and the mean,/, for 
this dstnbuton. (Strictly, p4q21 is the probability of the sample, given p, but 
since b(4; 25, p) is tabled and is proportional to p4q21, we use the binomial 
probability in the calculation of the posterior probabilities.) 
8-5. TESTING OF A BINOMIAL STATISTICAL HYPOTHESIS 
Sometimes we want to knov vhether the performance of a binomial 
process is consistent xvith the assumption that the probability of success 
has a given value, Po. 
EX.5;PLE 1. Acceptance sampling or quality control. A production 
process has been in control for some tme wth percent defectives Po  0.05. 
Samples of 25 are inspected, and if 4 or more defectives are observed the 
process is regarded as "out of control"; otherwise the process is accepted 
as "in control." For various values of the true percent defective p, how 
likely is this criterion to accept the process as "in control"? 
Solution. Let the random variable X be the number of defectives in the 
sample. From Table IV-B, the probability of accepting the process as 
"in control," P(X < 3), can be readily computed for any given propor- 
tion of defectives, p. Figure 8-3 shows P(X < 3), for different propor- 
tions of defectives p, 0 <_ p < 0.30. The ordinate gves the probabihty 
of accepting the process as "in control" for the value of p shown in the 
abscissa. The graph is called the operating characteristic of the test. When 
the process is operating at a level of 5% or fewer defectives the sample 
rarely gives the judgment "out of control." The graph shows the proba- 
bihty of accepting as greater than 0.95. On the other hand, if p is large 
302 SOME STATISTICAL APPLICATIONS OF PROBABILITY [CHAP. 8 
compared with 0.05, say p ---- (}.15, 4 or more defectives occur more than 
half the time, and the sampling process is likely to detect the change 
soon, if not immediately. If p is very large, say 0.25, the sample is almost 
certain to detect this at once, and appropriate action will be taken. 
1o 
o8 
' 06 
vI 
04 
02 
0 0.05 0.10 0 15 0.20 0.25 0 30 
Popoton defective:  
Fro. 8-3. Operating characteristic: n = 25, rejection number 4, acceptance 
number 3. 
For p = 0.15, Fig. 8-3 shows that about half the time the fact that the 
process is "out of control" will not bc detected by the sample. The manu- 
facturer may wish to stccpcn the curve so as to discriminate more im- 
mediately and sharply between a process producing 5% defectives and, 
say, 15%. By increasing the sample size n and changing the rejection 
number r (r = 4 in the example), the shape of the operating characteristic 
can be changed. 
Figure 8-4 shows a set of operating characteristics for several ac- 
ceptance sampling plans, all designed to have probability of about � of 
rejecting the process when p = 0.07. As n increases, r increases, and the 
operating characteristic becomes steeper at p = 0.07. The curves were 
made with the aid of large binomial tables, but the normal approximation 
could have been used for the larger values of . 
Such plans are used to help control a process or to help a buyer decide 
whether the lot of material he purchases has the quality the seller claims. 
8--5] TESTING OF A BINOMIiL STATISTICAL HYPOTHESIS 303 
P (accept) 
1.0 
0., 
--4 
Ob 
0.4 
I I  I I 
0 05 0 10 0 15 0 20 0.25 0.30 
Poportion defective: p 
8-4. Operating characteristics of several plans. 
Variations on the theme of acceptance sampling are quite COlnmon in 
scientific work. The sign test is typical of these. On the basis of a sample 
from a population, one wants to decide whether some percentfie of the 
population, for example the median, is equal to a known standard or not. 
The method proceeds by translating measurements in the sample into a 
q- or -- according as the measurement is above or below the standard. 
If the standard equals the population percentfie, then binomial theory 
applies to the number of q-'s and --'s, as we describe in the following 
examples. 
EXAMPLE 2. Xational results on a standardized achievement test are 
scored so that half of all ,students score 100 or over, and half score less 
(100 is the population median). A teacher wonders whether his class 
differs from this standard. Of his class of 20, 16 scored higher than 100, 
4 lower. He sees at once that the class has more than half above the 
standard. But he may al. ask, "Considering sampling fluctuations, is 
it reasonable that my class i, a sample from a large population of students 
304 SOME STATISTICAL APPLICATIONS OF PROBABILITY [CHAP. 
half of whom score 100 or more, and half less?" Specifically, he visualizes 
the population from which his students are drawn as the students attend- 
ing his school over a number of years, and he is willing to regard this 
class as a random sample from such a population. 
Discusswn. In this question, the teacher visualizes a population in 
1 We call Po his 
which the probability of scoring 100 or more is Po ---- .. 
ndl hypothesis, or standard. The word "null," in this setting, means that 
there is nothing special about the population. As an alternative hypothesis 
he considers that the true p for the population of students from vhich 
his school draws has p  3- He thinks his school's population may be 
worse or better than the national average. 
One way to test such questions is to use the sample to set confidence 
limits on the unknown p. If the confidence interval contains the value 
P ---- 3, we accept the possibility that the null hypothesis is true, other- 
vise we reject the null hypothesis in favor of the alternative. In a testing 
problem, we speak of the significance level of the test. It measures the 
probability of rejecting the null hypothesis when it is true. It is the 
complement of the confidence level, ff a confidence interval is used to make 
the test. Thus if the confidence level is chosen as 0.95, the significance 
level is 0.05. 
In Example 2, the 95% confidence limits for p, when 16 successes are 
observed out of 20, can be read from Chart I. We get 0.56 and 0.95 as 
lower and upper confidence limits for p. Since p ---- � is not in the interval, 
at the 5% level we reject the null hypothesis that p ---- � for the population 
of students from which the class is drawn. 
One-sided tests. In the test just described, the approach was two-sided, 
because the teacher thought both p  � and p 4 � were possibilities. 
He might have formulated the problem with the null hypothesis p _ � 
and the alternative p  3. Then he would reject the null hypothesis only 
for large values of X. 
The worst value of the null hypothesis, from the point of view of being 
- Assume, 
able to distinguish it from the alternative hypothesis, is p  .. 
temporarily, that p---- � and compute the probability of observing a 
result as extreme as, or more extreme than, the one observed in the direc- 
tion of the alternative (16 or more above 100 in our example). The re- 
sulting probability is called a descripti,c level of significance. If it is as 
small as or smaller than the level of significance the investigator would 
use in the problem, he rejects the null hypothesis in favor of the alterna~ 
tire. Our descriptive level of significance computed on the basis of the 
largest null hypothesis value of p (from Table IV-B) is P(� _ 16) ---- 
0.006, and we again would reject at the 0.05 level of significance. 
8--5] TESTING OF A BINOMIAL STATISTICAL HYPOTHESIS 30,5 
Other null hypotheses. The sorts of tests just described are not linlited 
to null hypotheses related to p = � 
EXAMrLE 3. A standard medication rednces reports of post-operative 
pain in 80, of patients treated. A new medication for the same purpose 
produces 90 patients relieved among the first 100 tested. What is an 
appropriate test of significance? 
Solution. If the new medication is better than the old--few are-we 
want to detect it. We take as the null hypothesis p _ 0.8 and as the al- 
ternative p > 0.8. We compute P(X _ 90) for n ---- 100, p ---- 0.8, 
because 0.8 is the standard the new medication ought to exceed if it is to 
replace the old medication. For the normal approximation wc compute 
0 -- � -- (0.8)00 
z-  2.37. 
V.8) (0./06 
The descriptive level of significance is approximately 
P(Z > 2.37) = 0.0089. 
If the investigator is using the 5% or the 1% level of significance he rejects 
the null hypothesis in favor of the alternative. 0perationally, he decides 
to use the new medication in preference to the old. 
Acceptzng or rejectzng the null hypothesis. Why do we reject the null 
hypothesis, or accept it? As you must have observed, we reject it when 
the probability of the occurrence of the observed event, or more extreme 
ones, is small. But that alone is not the reason. We reject it because the 
data do not support it, arid because we think the alternative hypothesis 
is tenable and that the data do support the alternative. The quality 
control man rejects the hypothesis that the process is "in control" in favor 
of the hypothesis that the process is "out of control" because he knows 
there is a good chance it may be, since machine settings, inattention, and 
new raw materials are all common sources of trouble. Therefore, when 
he sees a high number of defectives, he would rather assume that some- 
thing has gone wrong and look for it, than merely assume that a very un- 
usual sample has occurred in a process that is in control. Similarly, the 
teacher knows that there is variation in teaching ability and in school 
systems. His students were not randomly drawn from the national 
population, but from a special neighborhood. And the doctor looking for 
a new medication knows that medications better than the standard ones 
are found from time to time (after all, the standard xvas once unknown) 
but not often, so he will be very cautious about replacing a standard. In 
all these examples the decisions are not final; new data can overthrow them. 
306 SOME STATISTICAL APPLICATION'S OF PROBABILITY [CHAP. S 
]Vhat does it mean to accept the null hypothesis? 'Suppose the teachel' 
had observed 12 students with scores of 100 or over and 8 vith lover 
scores and had tested the null hypothesis p ---- � against the altel-native 
is M �. At usual levels of significance he accepts the null hypothesis. 
HoveVel', he does not believe thel-efore that/ ---- � exactly. A safnple of 
20 cal'l'ies practically lie information for diseriminatitg is ---- 0.500 from 
is ---- 0.501. All the teacher accepts is that is ib heal' �. I"ul'thermol'e, he 
l'eserves the right to change his mind if he gets more data that conflict 
with the hypothesis that is is near �. 
EXERCISES FOR SECTION 8-5 
1. From Fig. 8-3, what is the probability that tile proccss is judgel "in 'on- 
trol" if ;s -- 0.107 If ;s = 0.30? What is the probability it is judged "out of 
control ' if ;s = 0.20? 
2. From Fig. 8-3, what proportions of defectivs lead to judgments of "in 
control" 80% of the time? 10% of the time? 
3. Check the answers to Exercise 2, using binomial tables. 
4. !\(lake an operating characteristic like Fig. 8-3 for the plan: samples of 
size 2, reject if 1 or more defcctives are found. Compare with Fig. 8-3. 
5. (Continuation.) Suppose 2 items are randomly drawn without replacement 
from 10 items, and the lot of 10 is rejectcd if the sample has one or mol'e defec- 
tives. Show on the same graph as that of Exercise 4 the probabilities of accepting 
the lot of 10 for cach fraction defective in the lot. (Thus you are in a position 
to compare the cxact probabilitics of this excrcise with the binomial probabilities 
of Exercise 4 as an approximation. Binomial calculations are often made as if 
sampling wcrc done ith replacement, as an appl'oximation for calculations 
for sampling without replacemcnt.) 
6. From Fig. 8-4, find for the four plans the values of io for which lots are 
accepted 95% of the timc. 
7. Use your binomial tables to find a plan (sample size  and rejection number 
v) that accepts about 85% of lots or processes with  -- 0.05, and rejects about 
90% of lots with ;s -- 0.20. 
For Excr'ises S and 9, consider the plal of Fig. S-3 (t = 25, r = 4), and 
suppose that 100 large lots have 1% defectives, 100 have 5/0/o defectives, and 
100 have 20% defectiws. 
S. Find the expected number of lots accepted by the plan. 
9. Find the expected llumber of defective items accepted if each of the 300 
lots has 1000 items. Compute the percent of defective items accepted, and 
t'ompare it with tile percent in tile original 300 lots. 
10. Suppose a buyer uses one of the sampling plans described in this chapter. 
Suppose the sellcr always pcrstades the buyer to give an 5- l'ejectcd lot a "second 
chance," using the same plan. What is the relationship between the actual 
operating characteristic for the new procedure and the operating characteristic 
of the original plan? 
8--5] TESTING OF A BINOMIAL STATISTICAL HYPOTttESIS 307 
11. An accepted lot is orth about $500 to a manufacturer. A rejected one 
costs $200 for reworking, and so has a net worth of about $300. He can produce 
at p = 0.10 at no additional cost, and he can reduce p by an additional 0.01x 
for 5x dollars. If the plan of Fig. 8-3 is used, the probabilities of rejection for 
various values of p are as follows: 
p .10 .09 .08 .07 .00 .05 .04 .03 .02 .01 
P (rejectionlp)' .-- .--- .-5 .--- .--- .-- .-- .- .05 .000-- 
At what p should he operate to maximize expected profit (or to minimize ex- 
pected losses compared with the $500 value for an accepted lot)? 
12. Madame X says that she can tell by taste hether tea has been made 
with tea bags or with bulk tea. She sips from l0 pairs of cups, one with each 
kind of tea, and correctly identifies 9 of the pairs. What descriptive level of 
significance would you attach to this experiment? 
13. For the data of Exercise 12, use Chart I to set 95% confidence limits on 
p, the probability of correctly identifying a pair of cups. Then, at the 5% 
significance level, reject the null hypothesis, p = �, in favor of the alternative, 
p  �, if p = � is outside the confidence interval. 
14. Fred has a die he believes may be loaded in favor of the side marked 
"six." He tosses it 4 times and gets three "sixes." Using the 5% level of sig- 
nificance, do these results cause you to reject the null hypothesis p = 
15. Mr. Wfiliams played 5 hands of bridge one evening and got no aces 
4 times. He complains of poor shuffling. Assunnng good shuffling, the proba- 
bility p of getting at least one ace, on any one deal, is 0.7 (approximately). 
Are 4 no-ace hands out of 5 hands enough to reject the null hypothesis p = 0.7 
at the 5% level of significance? (Use the binomial formula.) 
16. A manufacturer of light bulbs says that only 10% of the frosted bulbs 
he manufactures have defective frosting, and that these defective bulbs occur 
at random during manufacture. A earton of 4 of his bulbs was purchased and 
2 of these had defective frosting. Would you reject his claim at the 1% level 
of significance? 
17. A patient suffering from chronic headaches has had 60% of a large number 
of headaches relieved by standard medication. A new component is added 
to his medication, and 17 of his next 20 headaches are relieved. Would you 
reject, at 5% level, the null hypothesis of p = 0.6? Criticize the application 
of the binomial distribution to this experiment. 
18. Find a 95% confidence interval for p in Exercise 17. 
19. Five items are drawn from a large lot. If two or fe er are defective, the 
lot is accepted. Compute roughly the operating characteristic of this test, 
graph it, and tell for what percent defective half the lots will be accepted and 
half rejected. 
20. To decide whether a coin is unbiased, one man flips the coin 4 tinms. If 
it comes up heads on all 4 flips, he rejects the hypothesis that it is unbiased. 
A second man thoroughly mixes an urn containing 15 white balls and 1 red one, 
308 SOME STATISTICAL APPLICATIONS OF PROBABILITY [CHAP. 8 
all alike except for color, and draw, s out 1 ball. If it is red, he rejects the hy- 
pothesis that the coin is unbiased, othersvise he assumes it is unbiased. (a) What 
is the null hypothesis for each man? (b) Vhat is the significance level for 
each t(st? (c) What are the circumstances under which the first man's test is 
preferable to the second man's? 
21. Fertilizers A and B are used on 5 pairs of adjacent (randomly selected) 
plots of cabbage. The differences in yeld in hundreds of pounds (A -- B) are 
6, 4, 2, 2, 1. Us the sign test to decide at the 10% level whether the fertilizers 
are equally likely to provide high yields. 
22. In the Weldon dice experiment, 12 dice were thrown 26,306 times and the 
appearance of a 5 or a 6 was considered a success. The mean number of successes 
observed was 4.052383. Is this result significantly different from the expected 
average number of successes, 4? (Use normal approximation.) 
23. In a psycho-physical experiment, a subject has a 30% detection rate for 
a signal, established by thousands of trials. After a vacation he returns to the 
laboratory and detects the signal on only one of the first 20 trials. The experi- 
menter wonders whether the equipment and/or the subject have changed, or 
whether this large a deviation from the 30% rate is a frequent occurrence under 
sampling valiation. Advise him and state your assumptions. 
24. In a coffee-tasting experiment a subject tastes each of 10 pairs of cups 
of coffee and decides for each pair which cup contains the instant rather than 
the percolated coffee. The experimenter decides to call a person a "taster" if 
he decides correctly in at least 8 out of l0 pairs, otherwise he is called a "non- 
tastcl'." Regarding this operation as a test of significance: (a) What is the null 
hypothesis? (b) What are the alternative hypotheses? (c) What is the level 
of significance? (d) If a subject has probability 0.8 of correctly calling a pair, 
what is the chance he will be called a taster . 
25. lright red clothing has regularly been used by hunters for visual protec- 
tion. (Rec('nt experiments have assessed goldenrod yellow for this purpose.) 
Suppose we know that under standard conditions red is sighted 80% of the tmle. 
Four hundred trials with yellow are tried. (a) What would you take as the 
null hypothesis? (b) Using a level of significance of 0.05, set a criterion number 
of successes at or above which you would regard yellow as better than red. 
(c) If p = 0.9 for sighting with yellow, what is the probability that yellow 
does not tea( h the criterion? 
26. Suppost, that method .1 is kno;;n to yield 50% successes. It is desired 
to detect the superiority of method B if t yields as few as 55% successes 
(p = 0.55). A significance level of 0.025 is to be used. How big a sample is 
required to be 95% sine that a p = 0.55 will be detected as better? [Note. You 
need the normal approximation. First set up the criterion, using the largest null 
hypothesis value of p, the find out how method B will perform against this 
crtterion for a general value of n.] 
27. The success rate in selecting applicants for a position is 60%. A new 
selection method may yield a �hfferent rate (not necessarily higher). Use the 
method of confidence limits to set up a 2-sided test using a 10% level of signifi- 
cance, n = 50. If p = 0.50 for the new method, what is the probability that 
you decide the success rate is different from 0.60?. 
8-5] TESTING OF A BINOMIAL STATISTICAL ItYPOTHESIS 309 
Data for Exercises 23, 29, 30. Of to bramts of "fireproof" glass ovenware, a 
wholesaler wants to choos the one that ithstands a greater sudden change of 
temperature. In testing the brands, be users an oven and a tub of icewater. He 
tests a Brand t piece and a Brand B piecc simultaneously as fottoxvs. He trans- 
fers the two pieces from the tub to the oven, which is set at 300 � F. If neither 
breaks, both are returned to the tub. If neither breaks now, both are eturned 
to the oven, in which the temperature has been advanced to 350 � F. This 
process continues ith 50 � increases in oven temperature untfi one piece breaks. 
(Forget about both breaking at the same time.) The piece that bcaks first is 
egardcd as poorer, and its brand is regarded as poorer on that trial. 
28. In l0 trials, lrand I broke first 9 times. Is this often enough to reject 
at the 5% level the hypothesis that the two brands are of equal quality? 
29..knother  holesater ran a series of trials. He reported that Brand 1 at ays 
broke first in his trials and that hc rejected the hypothesis of the equality of the 
brands at the 5% level. What is the smallest number of trials hc could have run? 
30. A third wholesaler said he had run a lot of trials, and Brand .1 had always 
broken first in his, too. He had concluded that Brand A would always break 
fist. In answer to a question, hc said he supposed the probability of A's breaking 
first  as 1.00. How many times in 1000 trials would Brand B have to break first 
to reject this supposition? 
Data for Exercises 31, 32, 33. A nanufaeturer produces capsules to be filled 
ith medicinal powder. These are inexpensive and are sold in large numbers, 
in boxes of 100 capsules. One segment of the drug ndustry demands capsules 
ith no discernible surface defects. Ten percent of our manufacturer's capsules 
have surface defects. In the handling and boxing operations the capsules get 
so thoroughly mixed around and stirred together that the 100 going nto any 
box are for all practical purposes selected at random. 
31. What fraction of the boes itt have just l0 defectives in each? (Hight. In 
the appropriate normal approximation, what fraction of the area lies between 
9.5 and 10.57) 
32. A box of 100 ith just l0 defectives in it is received by a drug concern 
which employs aCCel)rance sampling procedures. Their plan is to examine two 
(n = 2) of the capsules at random and if either of them is defective (r = 1), 
reject thc box. What is the probability that the box will be accepted? 
33. Another druggist does hs acceptance sampling by dumping l0 boxes 
together into one lot of 1000 with 100 defectives in it. IIe draws two capsules 
at random from the lot. Ite, too, rejects the lot if either of the two selected 
capsules is defective. How does his probability of rejecting the 1000 capsules 
compare ith the first druggist's probability of reje(ting a lot of size 1007 
34. Cartons of 8 60-watt lamps are called tots. Each ('arton is inspected by 
testing 2 of the lamps, selected at random. The acceptance rule is that if both the 
tested lamps light, the carton is accepted; if either fails, the carton is rejected. 
A customer who has adopted this acceptance plan picks up a earton which 
happens to contain 2 defective lamps and 6 nondefective ones. What is the 
probability that he will accept the carton? 
310 SOME STATISTICAL APPLICATIONS OF PROBABILITY [CHAP. 8 
8-6. BAYESIAN INFERENCE WITH PERSONAL PROBABILITIES 
Hypotheses are rejected because xve beheve their alternatives have a 
high chance of being true in the light of the total evidence. If we Calx 
quantify our prior beliefs, then Bayes' Theorem can aid us in problems like 
those treated in Section 8-5. Many people are unwilling to make such 
personalistic quantifications, but the following example illustrates how 
the personalistic approach through probability as degree of belief xvould 
work. The example is one about xvhich you may have viexvs. 
Extrasensory perception. Art claims that he has extrasensory perception 
(ESP). He says that if Bob conceals a red card in one hand and a black 
card in the other, he can tell which hand holds the red card. Bob doesn't 
believe it. Art admits he can't do this all the time, just "pretty often." 
First, we give an approach like that of Section 8-5. In this form the 
problem is appropriate for a significance test xvith null hypothesis p ---- � 
(Art has no ESP, and performs by guessing), alternative hypothesis 
p  � (Art has some ESP). Bob can give Art a test consisting of a number 
of trials and  criterion for passing such that Art has a good chance of 
passing the test if he has a noticeable amount of ESP, say p ---- 0.7, and 
not much chance of passing if he doesn't (p - �). Let X be the 
number of successes in n trials. Then xve read from our binomial tables 
that, with 25 trials, if p  �, P(X _ 16)  0.115, and if p---- 0.7, 
P(X _ 16)  0.811. One test is to try 25 times and be right 16 or more 
times. Thus if Art has no ESP he will fail in about 89% of such tests, and 
if he has ESP amounting to p ---- 0.7 he xvill pass in about 81%. Tests 
xvith more trials can reduce the risk of passing or failing Art erroneously, 
and improve his chance of passing if he has some ESP but less than 
p ---- 0.7. This approach is like that of the previous section. Let us turn 
now to the personalistic approach. 
To give a numerical value of Bob's disbelief in Art's ability, xve would 
have to ask Bob what he thinks the chances are that Art has no ability 
(p ---- 0.5), or that he has probability p ---- 0.6, p ---- 0.7, or so on, of 
passing. (Properly treated there would be a continuous probability dis- 
tribution for p, but xve shall treat the prior distribution xvith a discrete 
approximation, as we did in Section 8-4.) For simplicity, suppose Bob's 
degrees of belief in Art's ability are as follows: 
Hypothesis Art's ability Bob's prior degree of belief 
H: p -- 0.5 0.98 
H2: p -- 0.7 0.02 
1.00 
8--6] BAYESIAN' INFERENCE IVtTIt PERSONAL Pt{OBABILITIES 311 
If n trials are performed and Art has x successes, llen the folh,wing 
table helps iu calculating lhc posterior degrees of belief from Bayer' 
Theorem. 
Probabilit of otcome 
p = 0.5 0.9S(0.5)(0.5) 
= P (Bob has p = 0.5)P (sample[Bob has p = 0.5) 
p = 0.7 0.02(0.7)�0.3) 
= P(Bobhasp = 0.7)P(sampte[Ih)bhasp = 07) 
There are no binomial coecieuts because we compute the probability 
of the particular sample in the order of occurrence of the calls. Had xxe 
introduced binomial coefficients they would have dropped out of the later 
calculations. Therefore we may use them if there lS any advantage in 
calculatiou. 
In advance, Bob's odds (ratio of degrees of belief) were 0.98 to 0.02 or 
49 to 1 against Art hax lug ESP. After the experiment, Bob's odds against 
Art having abfiity are given by the ratio 
0.98(0.5)x(0.5) -x _-- 49(0.5)x(0.5) '-x ' 
0.02(0.7)x(0.3) -x (0.??(0.3) 
EXAMPLE 1. Suppose  = 25, x ---- 17. Find the posterior odds. 
,Solulwn From our Table IV-A, we get 
49b(17; 25, 0.5) 49(0.032) 
  9.5. 
b(17; 25, 0.7) 0.165 
Thus if Art correctly identifies exactly 17 cards out of 25, Bob's odds 
have gone down from 49 to 1 to 9 5 to 1 against Art having the ability. 
He still doesn't believe much in Art's ability, but he has xveakened by a 
factor of 5. 
Additional data can readily be added. 
Ex,xrLn 2. Starting with 49 to 1 odds, how big a sample do we need 
to be 95% sure that the odds are reversed (1 to 49), if Art has ESP to the 
extent p = 0.7? 
Solutiom XVe want to filld x and n that satisfy two properties: first 
49 (0'5)'(0'5)- = ' (1) 
(o.7)(o.39 49 
312 SOME STATISTICAL APPLICATIONS OF PROBABILITY [CHAP. 
Taking three-place logarithms tq the base 10 and simplifying gives 
0.368x -- 0.222n  3.380. (2) 
The second property is that if p = 0.7 we want to find n and x so that 
P(X >_ x)-0.95. The normal Table III shows that z---- --1.65 is 
exceeded 95% of the time. Applying the central limit theorem approxima- 
tion (Theorem 7-3) gives 
x } 0.7n _ --1.(55. (3) 
x/n(0.7)(0.5 
Let us pick a few convenient values of n, find the x's from Eq. (2), and 
see how well they satisfy Eq. (3). 
Normal tab}e 
n x Left side Eq. (3) probability 
100 70 --0 11 0.54 
200 130 --1 62 0 95 
300 190 --2 58 0. 995 
Consequently, a sample of n = 200 gives Art a probability of about 
0 95 of reversing the odds if he has an abihty p = 0.7. 
A difficulty in this approach is the initml assessment of degree of belief; 
Bob might have said 999 to 1 or even 999,999 to 1. But this uncertainty 
is inherent in the problem. In ordinary hypothesis testing and confidence 
limit work there is a parallel uncertainty about appropriate chmce of 
confidence level, significance level, and sample size. 
EXERCISES FOR SECTION 8-6 
The first three of the following exercises assume that Bob's odds against Art 
are 9.5 to l, and are concerned with the effects of further data on the odds. 
Start each problem at the 9.5 to 1 odds. 
1. If Art's next 3 trials are successes, what are the new odds? 
2. If Art's next 3 trials are failures, hat are the new odds? 
3. If Art has 3 successes and 3 failures, what are the new odds? 
4. Find the r(qation between n and x that leaves the odds approximately 
constant in the ESP problem. 
8--6] BAYESIAN INFERENCE VITH PERSONAL PROBABILITIES 313 
5. Startling with the initial 49 to 1 odds, what size n guarantees Art a 0.5 
chance of reversing the odds if his p = 0.7? 
6. Starting with 49 to i odds, f a test has n = 200 trials and Art's p = 0.5, 
what is the chance the odds against him go up to 100 to 1 ? 
7. If Art gets exactly half right and half wrong in a test of size n, what value 
of n yields odds of about 1000 to 1 against him, starting with 49 to 1 against? 
8. Bob holds prior odds of 1 to 1 that Madame X of Exercise 12 in Section 
8-5 has p = 0.5 or p = 0.7 of being able to distinguish the pairs of teacups. 
After she gets 9 out of 10 right, what are his posterior odds  
CHAPTER 
THEORY OF SAMPLING. 
VARIANCES OF SUMS 
AND OF AVERAGES 
9-1. CALCULATION OF THE DISTRIBUIION OF A SUM 
Precise wcasrement. When precise results are needed, an experimenter 
may repeat a measurement several times and compute the arithmetic 
average, :, to estimate the required value. Although there are many good 
reasons for taking repea. ted measurements, the one dealt with in this 
chapter is that arithmetic averages of several measurements ordinarily 
vary less from one determination to another than do single measurements 
from one to another. 
An average is obtained by summing the measurements and then divid- 
ing by the number of measurements. After we develop the theory of the 
variability of sums, that for averages follows easily. 
Sms of many uariabl�s. If 100 dice are rolled, what is the probability 
that the sum of the dots on their top faces exceeds 325? Here 100 inde- 
pendent random variables are being summed, and we are asked to compute 
a probability associated with the distribution of that sum. The methods 
of the next example can be extended to give an exact answer to the ques- 
tion. But the work required is long and tedious compared with the worth 
of the exact answer; consequently we later develop all approximate method 
based on the central limit theorem. 
Ex^r. 1. Sm of three indepedet 'andom variables. A coin is 
tossed once, and the random varitble X is the number of heads; a thumb- 
tack, xvith probability � of landig point up, is tossed once, and the 
random variable Y is the nullabet landing point up; all engineer's ruler 
with sides numbered l, 2, alxd 3 is rolled, and the random variable Z is the 
number on the face-down side. Find the exact distribution and the mean 
and variance of the l'andom variable U, where U: X q- l"q- Z. 
314 
9-1] THE DISTRIBUTION OF A SUM 315 
Solution. We find the distribution in two stages: first we get the proba- 
bility function of the sum of X and Y; then we combine X -]- Y and Z. 
This technique can be continued for sums of still more random variables, 
say X -]- Y -]- Z - IV, and so on. 
The entry in each cell in the top array in Table 9-1 shows, as the first 
elltry, the value of the sum x - y in the cell and, as the second entry, 
the probability of the cell. Probabilities are computed under the assump- 
tion of independence. The lower array of Table 9-1 shows the proba- 
bility function of X -]- Y. 
TABLE 9-1. SUM OF TVO INDEPENDENT RANDOM VARIABLES. 
Values of � 
i 0 P(X = x) 
Values  2,  1,  � 
of X 1,- 0, � 
P(Y = y) �  i 
3 1 
Probability    
Value of (X-]- Y) 0 1 2 
In Appendix III we show that when X, Y, and Z are independent 
random variables, then X + Y and Z are also independent. We use this 
result to fill in the probabilities in the cells of the upper array in Table 9-2. 
The first entry in each cell is the value of the sum (x -]- y) -]- z for that 
cell, and the second entry is the probability, i.e., the product of the row 
total and the column total. Collecting probabilities of the cells where 
2 
(x + y) -]- z ---- 3, for example, we get 1 4-  4- - ---- s for the cor- 
responding entry in the lower array of Table 9-2. Other entries are ob- 
tained in the same way. 
The mean of the sum is the sum of the means (Corollary 6-5, Section 
6-1). Consequently, since 
1 1 
then 
= 
The latter value can be checked by computing the mean for the lower 
array of Table 9-2. 
316 THEORY OF SAMPLING [CttAP. 9 
TABLE 9--2. JOINT DISTRIBUTION OF X-- Y AND Z, 
TOGETHER \VITH THEIR SUM (FIRST CELL ENTRY). 
Values of Z 
Probability � 
1 2 3 of X-( Y 
o 
Vtues 
i i i 1 
Probability of Z    
Probability s  6__ [ 4__ x 
18 18 18 
Values of u = x-l- y-l- z 1 2 3 4 5 
In the next sections, we show that the variance of a sum of independent 
random variables is the sum of their variances. Let us check the state- 
ment on the present example. Easy calculations give 
Their sum is 41 Direct calculation of  from the lower array of Table 9-2 
also gives 4x 
, so the statement checks in this example. 
Fortunately, under quite general conditions, sums of many independent 
random variables are almost normally distributed. This information 
makes it easy to get an answer to a question like "XVhat is the probability 
that the total score on 100 dice exceeds 325?" with enough accuracy for 
most purposes without going through the step-by-step construction il- 
lustrated in Example 1. But before we can use this approximation, we 
must know the mean and variance of the distribution of the sum in terms 
of the means and variances of the variables being suimned. 
EXERCISES FOR SECTION 9-1 
In Exercises 1 through 4 use the data of Example 1 and Tabtcs 9-1 and 9-2. 
1. Graph the probability functions of 
(a) .�, (b) ', (e) .\' + ', (d) Z, (e) .� + Y + Z. 
9-1] THE DISTRIBUTION OF A SUM 317 
2. l'se the probability functions of Exercise 1 to compute the means and 
variances of 
(a) X, (b) (c) Z, (d) X -l-- �, (e) X + + Z. 
3. If each value of X is increased by 1 and each value of Y is dec, eased by 1, 
xxhat is the effect on the probability function of 
4. If each value of X is decreased by } (=/x), each value of Y is decreased 
by � (=/r), and each valne of Z is decreased by 2 (=/z), (a) what is the proba- 
bility function of U (=Xq- Yq-Z)? (b) What is the new mean of U? (c) 
What is the new variance of U? Is it the same as before? 
In Exercises 5 through 7, X, Y, and Z are independent and have the following 
probability functions: 
Probability 
Values of x 
1 
Probability � � = 
Values of y --1 0 1 
Probability 0.1 __ 0.2 . 0 3 
Values of z I 3 4 
5. Construct the probability functions of X q- Y and X q- l' q- Z. 
6. Show thatP(Xq- Y = landZ = 2) = P(Xq- Y = 1).P(Z = 2). 
7. Graph the probability functions of X, X q- Y, and X q- I' q- Z. 
8. The random variables X and Y are independent, and both take on the 
values 0, 1, 2 with probabilities �, �, �. Set up the probability function of 
U = Xq- Yandcomputeafromit. Isa2g = a2xq-a2v? 
9. (Continuation.) Suppose that Z is another random variable with the same 
probability function as X and Y of Exercise 8. Find the probability function 
of W = (X q- I' q- Z). Compute E(W) and a2w and compare the results with 
E(X) and a2x . 
10. X and Y independently take on the values 0, 1, 2, with probabilities �, 
�, {, in that order. Find the probability function of U = X q- Y and sketch 
its graph. Compute E(U) and Var (U) from the probability fnnction of U. 
Compare the results with E(X) and Var (X). 
318 THEORY OF SAMPLING [CHAP. 9 
11. The joint probability function of X and Y is as follows: 
Values of Y 
--1 0 I P(X = x) 
Values I_11 0.1 0.1 0.2 0.4 
of X 0.1 0.2 0.3 0 6 
P(Y = y) 0.2 0.3 0.5 1.0 
Find E(X), E(Y), E(X q- Y), Var (X), Var (Y), Var (X q- Y). Is the variance 
of X q- Y equal to the sum of the variances of X and Y in this example? Are 
X and Y independent? 
12. Three engineer's rulers with numbers 1, 2, 3 on the faces are tossed. 
Find the probability function of the median of the numbers on their lower faces. 
(In this exercise, if there is no tie, 2 is the median; if there is a tie, the tied 
value is the median.) 
9-2. THE VARIANCE OF THE DISTRIBUTION OF THE SUM OF TWO 
INDEPENDENT RANDOM VARIABLES 
To derive the variance of the sum of two independent random variables, 
we let 
U = X q- Y, (1) 
where X and Y are the independent variables, and/x,/Y, a}, a, are their 
respective means and variances. Equation (1) defines a new random 
variable U whose variance, like that of any random variable, is the mean 
of its square minus the square of its mean: 
E(U 
= -- /v. (2) 
Hence, by substitution from Eq. (1), 
o's E(X q- y)2 2 
= - (3) 
We knmv from Theorem 6-4 that 
/.x'+' = /x q- /Y. (4) 
Furthermore, 
E(X q- y)2 = E(X 2 q- 2XY q- I'2). (5) 
9-2] VARIANCE OF THE DISTRIBUTION OF A SUM 319 
Xow �2, 2�Y, and }72 are also random variables and, by Corollary 6-5, 
tile mean of their sum is the sum of their means: 
= I(X") + 2E(XY) + E(Y"). (O) 
The middle term of the last line of Eq. (6) follows from the property that 
the mean of a constant tinms a random vnriable is that constant times 
the mean' 
S(Xr) = S(XY). 
So fnr, we have not used the assumption that X and Y are independent. 
We now make use of their independence, and Theorem 6-6, to write 
s(xY) = '(x). s(Y) = ,x,r. (7) 
We now substitute fi'om Eq. (7) into Eq. (6), then from (6) into (5), and 
the result, together wth Lq. (4), into Eq. (3), to get 
a  = E(X ) + 2xr x 2xr r 
-F E(Y)   
= (x ) _ } + (% - , 
We have therefore established the following theorem: 
9-1 Theorem. a.+r for independent variables. If U = X q- Y and 
if X, Y, U have variances aS, a-, and a, and if X and Y are 
independent, then 
c = . (8) 
EXAMPLE 1. A machine makes small round discs of thickness 0.5 inch, 
but with standard deviation of thickness 0.003 inch. Assemblies of two 
such discs tie one on top of the other (like a king in checkers). What is 
the standard deviation of the heights of finished assemblies? 
Solution. Let X stand for the thickness of tile bottom disc, Y for that 
of the top, and U (=X q- Y) for the thickness of the assembly. We 
assume that X and Y are independent. Then a} = (0.003)  '- (0.003), 
rc, - 0.0042. 
320 THEORY OF SAMPLING [CttAP. 9 
EX_AMrLE 2. Measurements. ,A boy saws board in lengths of about 
3 feet, and the standard deviation of the lengths is 0.2 inch. To check 
his precision the boy measures the lengths of the boards and obtains a 
distribution of measurements with standard deviation 0.25 inch. What 
is the standard deviation of his distribution of errors of measurement? 
Solution. Let the random variable X represent the actual length of a 
board, and let Y be the error of measurement; then the measurement 
U: X + Y. Assume that X and Y are independent. Then ax = 0.2, 
a = 0.25. To find at, we substitute in 
= + 
and get 
0.0625 = 0.04 + a. 
Solving for a gives 
a, = 0.0225, at, = 0.15. 
Thus the standard deviation of the distribution of measurement errors is 
0.15 inch, or roughly the size of the standard deviation of the distribution 
of actual lengths. The boy's sawing is nearly as accurate as his 
measuring. 
Er 3. Disparate standard deviations. Let X have standard 
deviation a and Y standard deviation ks, where k is large compared with 1. 
Find approximately the standard deviation of U = X + Y. 
Sobdion. By formula (8), 
= + = + = + 
Roughly, then, v = 1,' = t', the larger standard deviation. 
Rzn,RK. The moral of Example 3 is that if an experimental situation 
inw)lves a stun of random vm'iables, as when several measurements must 
be added, the reduction of a large variability counts heavily, but the 
reduction of a small variability is almost worthless for reducing the vari- 
ability of the sum. 
Use of aormal tables. When data are almost normally distributed and 
we know their mean and variance, we can use normal tables (Table III) 
to compute approximate probabilities. 
9-2] VARIANCE OF THE I)ISTRInUTION OF A SUM 321 
EXAMPLE 4. Performance on an algebra test and on a test of dramatic 
ability are approinmtely independent, and the distribution of the sum 
of their scores is approximately normal If the distribution of each set of 
test scores has mean 50 and standard deviation 10, what proportion of 
examinees scored a total of 125 points or more? 
Solution. Let X be the algebra score, Y the dramatic score, U their 
total. Then  = 50-50 = 100, and e = 10 a+ 10 a = 200, v = 
200 = 14.1. If we let 
U  lB0 
then Z is approximately no nml nnd has mean 0 and standard deviation 1. 
We can find P(Z  a), approximately, from the normal table, Table III. 
The event U  123 is equivalent to the event 
125- 100 
Z >  1.77. 
Therefore 
P(u 25) = P(z 1.77) 
and, from Table III, we find 
/>(Z _ 1.77)  0.0384. 
Thus about 4o of the examinees score 125 or higher. 
9-2 Theorem. Weighted stms of measurements. Let measurements X 
and Y be independently dravn from distributions with means 
/x,/r and variances o'x, at. Let their weighted sum, with weights 
a and b, be a new random variable Z. 
z -- aX + O Y. (9) 
Then 
Iz = al.v d- blr, (10) 
and 
2 2 o 2 
a} - a ax d- b'ar. (11) 
Proof. You are asked to prove this theorem in Exercise 1(5. 
322 THEORY OF SAMPLING [CHAP. 9 
EXAMPLE 5. Prices of alloys., Manufactured blocks of expensive alloys 
are sold by weight. Blocks of alloy A have standard deviation 3 pounds 
and cost $100 per pound. Blocks of alloy B have standard deviation 
4 pounds and cost $50 per pound. In repeated orders of two blocks (one 
A and one B), what is the standard deviation of the total price if the 
blocks are independently assembled to fill the order? 
Solution. Let X be the weight in pounds of a block of alloy A, Y that 
for alloy B. Then the total price in dollars is 
z -- 100x + 50Y. 
:By Theorem 9-2, 
a ---- 1002a + 502a = 130,000, az = 361, in dollars. 
9-3 �orolkr�. Differences. If X and Y are independent, with means 
/x, /' and variances a.-, a, then the distribution of their dif- 
ference D, 
D -- X- Y, (12) 
has mean 
z) ---- .� -- Mr (13) 
and variance 
= + (14) 
Proof. Theorem 9-2 applies to this corollary when we take a---- 1, 
b ---- --1, a 2 ---- 1, b 2 = 1. Substituting these values in Eqs. (10) and 
(11) yields Eqs. (13) and (14). [] 
EXAMPLE 6. Rods with washers. Rods (circular in cross section) have 
outside diameters that are normally distributed with mean 1.0 inch and 
standard deviation 0.003 inch. Washers (with holes circular in cross 
section) have inside diameters also normally distributed, with mean 1.005 
and standard deviation 0.004 inch. When rods and washers are randomly 
paired, in what percentage of pairs are washers too small to fit on their 
rods? (Assume that the difference of two independent normally distrib- 
uted random variables is normally distributed.) 
Solution. Let X measure the inside diameter of a washer in inches, 
Y the outside diameter of a rod. Let D = X -- Y measure the difference 
in these diameters. If D  0, the rod fits in the hole, otherwise not. 
By Corollary 9-3, /) = 1.005 -- 1.000 = 0.005, in inches, and (rD ---- 
x/0.0042-+ - 0.0032 = 0.005, in inches. Since / ----a, the percent of 
9-2] VARIANCE OF THE DISTRIBUTION OF A SUM 323 
Waher do not/// 
__.... 
D' 0 l)=(} I)>0 a D= 
Fro. 9-1. Distribution of difference of washer and rod diameters. 
washers too small to fit is equal to the probability to the left of --1 for 
a standard normal random variable Z, where 
z=D--o.. 
If D < 0, then Z < --1, and 
P (washer does not fit) = P(D < O) = P(Z < --1). 
From Table III, 
P(Z < --1) ---- P(Z > 1) = 0.5000 -- P(O _< Z _< 1) = 0.1587. 
Thus about 16% of the washers are too small to fit their rods (Fig. 9-1). 
EXERCISES FOR SECTION 9-2 
In Exercises 1 through 5, X and Y are independent, U = X q- Y. 
1. The variables X and Y both take values --1, 0, 1 with equal probabilities 
I I i Find ?v 
3 3 3' � 
2. Find the missing entrms in the following table: 
O' X O'y O' U 
(a) 
(b) 25 
(d) __28 10 
(g) I '-- 
(h) 
324 THEORY OF SAMPLING [CHAP. 9 
3. If o- -- 8, o-Sy = 8, what can, you tell about the distribution of X? 
4. What theorem from plane geometry does Eq. (8) look like? What do 
o-x, o-Y, and o-r correspond to in that theorem from geometry? 
5. If o-x = o'y, show that o-u = %/o-x. (Why not --%/o-x?) 
6. (Continuation.) Use the result of Exercise 5 to solve the disc assembly 
example, Example 1 of the text. 
7. When a die is rolled, a man receives $4 if an ace appears, but loses $1 
otherwise. On this basis, if a die is rolled twice, what are the mean and thc 
standard deviation of the distribution of his net receipts? 
8. (Continuation.) Suppose in the game of Exercise 7 the die is rolled a third 
time. What are the mcan and the standard deviation of the distribution of his 
net receipts on the three payments? [Hint. Let the outcome of the first two rolls 
be X.] 
9. The scores of College Board examinations have a mean of 500 and a 
standard deviation of 100. If two students are drawn at random from College 
Board examinees, approximately what are the mean and the standard deviation 
of the distribution of the sum of their mathematics scores? 
10. (Continuation.) Consider College Board examinees who have taken both 
an English and a Mathematics examination. Find the mean and the standard 
deviation of the distribution of the sum of the two scores, or explain why you 
cannot. 
11. In a large chemical operation, a man ladles out material with two scoops, 
one with capacity one pohnd, and the other, two pounds. For precise work, 
he uses the one-pound scoop twice to put two pounds of material into the mix. 
The standard deviation of the distribution of weight for the two-pound scoop 
is 0.5 ounce, for a single one-pound scoop 0.3 ounce. Should he have used the 
two-pound scoop for more precision? 
12. Two tests with independent scores X and Y are given. They have standard 
deviations of 7 and 24. For each student we take the sum of his scores on the 
tests, X h- Y. Show that the standard deviation of the distribution of these 
X h- Y scores is 25. 
la. The distributions of lengths of two kinds of xxoodcn parts A and B are 
approximately normal, with means /A = 2 inches and /B = 4 inches, and 
standard deviations o-A = 0.009 inch, o-B = 0.040 inch. An A part and a 
B part arc randomly assembled and laid end to end to form a length about 
6 iuches long. If an assembly is to fit, it must be between 5.92 and 6.08 inches 
long. What percentage of random assemblies fail to fit? (Assume that the 
distribution of the sum of two independent normally distributed random 
variables is normal.) 
14. Two measurements X and Y are drawn from the same distribution with 
mean / and variance o-=, and a weighted sum S = wX h- (! -- w)Y is com- 
puted. (a) Find/. (b) Find o-. (e) Find the value of w that minimizes o-. 
(d) Find the minimum value of o-. [Remark. For any w, S is called an unbiased 
estimate of/ because/s = /, and with w = �, S is called the miMmum variance 
,nbiased estimate of .] 
15. An assembly is made by putting two ;asherlike objects face-to-face on 
an axle. If the total thickness of the two objects is between 0.549 and 0.551 
9-3] VARIANCE OF t SUa, I. AVERAGE OF VARIABLES 325 
inch inclusive, the assembly is satisfactory--otherwisc not. The objects are 
randomly assemblcd from a population with mcan thinkhess 0.275 and standard 
deviation 0.0006 inch. What pcrccnt of the assemblies is unsatisfactory? (As- 
sume that the distribution of the sum of two indcpcndcnt normal random 
variables is normal.) 
16. Prove Theorem 9-2 (concerning weighted sums of measurcmcnts). 
In Exercises 17 through 19, X, Y, Z arc indcpcndent random variables taking 
the following values, cach with probability �' 
Values of X: --4, --1, 2, 3 
Values of t" --3, --1, 2, 3 
Values of Z: --2, --1, 0, 3 
17. Calculate a.+z by definition, and by formula. 
18. Calculate 2 by definition, and by formula. 
O-y+Zq- X 
19. Calculatc  by dcfinition, and by formula. 
�-3X+2Y_6Z 
9-3. VARIANCE OF THE SUM AND OF THE AVERAGE OF SEVERAL 
VARIABLES 
To solve the problem of the distribution of sample averages for the 
100-dice problem given at the beginning of this chapter, we need first 
the variance of the sum of many independent random variables, not just 
two as dealt with in Section 9-2. As soon as we have the variance of the 
sum we get the variance of sample averages by a trivial operation. 
Subscripts for random variables. Note that previously we have used 
subscripts maifiy on the values of the random variables, such as xi, not 
on the random variables themselves, because there were only one or two 
of them. Now we study many random variables, so we need subscripts 
for them. Thus in a theorem about the sum of n random variables, we 
denote the random variables by X1, X, ..., X. But if we have only a 
few random variables we shall continue to denote them by X, Y, Z. 
To extend the theorem on the variance of a sum of two random vari- 
ables, we need to show that we can add one more variable, and then 
another, and so on. For example, if X, Y, and Z are three independent ran- 
dom variables, we know that the variance of U = X 
If Z is independent of X and of Y, we naturally expect it to be inde- 
pendent of their sum U. If Z is independent of U, then we know that 
W ---- U  Z has variance  ----   az  ----     z . Provided 
Z is independent of U, this argument is enough to show that we have 
a general method for adding one more variable. Thus we can extend 
the theorem from 3 independent random variables to 4, then from 4 to 5, 
and so on to n variables. Of course a rigorous development requires a 
more formal induction argument. 
326 THEORY OF SAMPLING [CHAr. 9 
The entire argument above ,depends on the intuitively obvious but 
somewhat subtle fact that if X, Y, and Z are jointly independent, then 
the two variables Z and U (----X q- Y) are independent. The statement 
is true, but its proof, while easy, requires extra notation that we have not 
developed. A proof is given in Appendix III. This proof together with 
the previous arguments completes the demonstration of the following 
desired theorem: 
9-4 Theorem. Variance of sums of independent random variables. If 
X1, X2, � � �, Xn are independent random variables with variances 
a, ass,...,o' 2, and 
T = X q-X2q-...+X, 
then 
= a +... + (1) 
EXAMPLE 1. A three-unit assembly. In an electrical circuit, resistances 
in series form a resistance equal to the sum of their resistances. A 
10,000-ohm, a 20,000-ohm and a 50,000-ohm resistance are each drawn 
from a large stock to form an 80,000-ohm resistance. The standard devi- 
ation of these three kinds are 30, 60, and 150 ohms, in that order. Find 
the standard deviation of the distribution of 80,000-ohm resistances 
formed in this manner. 
Solution. If the resistances are randomly assembled, then 
a2 = (30)2 q- (60)2 q- (150)2 = 27,000, a = N/-,000  164. 
NOTE. Two or more random variables are said to be identically 
distributed if their probability functions are the same. 
9-5 Corollary. Variance of identically distribded variables: sampling 
theory. If X, X2,..., X, are independently and identically 
distributed random variables with means y and variances a 2, and 
if 
T = X q- X2 q- '" q- Xn, 
then 
= I (2) 
9-3] VARIANCE OF A SUM. AVERAGE OF VARIABLES 327 
and 
= (3) 
Proof. Equation (2) follows at once from Corollary 6-5, Section 6-1. 
Equation (3) is obtained by substituting a a for each a in Eq. (1) of 
Theorem 9-4 above. [] 
In Chapters 7 and 8 we frequently used the fact that the variance of 
the binomial distribution is 
 = npq: rip(1 -- p). 
Now we provide the long-heralded proof. 
9-6 C�)t�)[[lt�. Variance of a binomial distribution. Let p be the proba- 
bility of success on a single binomial trial, and let X be the total 
number of successes in n such trials. Then the variance of X is 
� a. = rip(1 -- p). (4) 
Proof. The distribution of the number of successes, B, on one binomial 
trial is 
Probability I__ 1--p 
Number of successes 0 
Consequently, the mean number of successes on a single trial is 
MB ---- p' lq- (1 -- p).0 = p. 
The variance of the number of successes on a single trial is 
o' ----- E(B. 2) -- /.t ---- 12p q- 02(1 __ p) __ p2 = P __ p2  p(1 -- p). 
The total number of successes on n trials is the sum of n independent 
random variables like B, one for each trial, and each having mean p 
and variance p(1- p). Therefore the mean, the variance, and the 
standard deviation for the number of successes in n trials are given by 
the formulas: 
(a) Ux = rip, (b) a: ---- np(1 -- p), (c) ax = N/np(1 -- p). (5) 
328 THEORY OF SAMPLIXG [CHAP. 9 
EXAMPLE 2. 1000 thumbtacks. If thumbtacls have a probability 
p ---- 0.3 of landing point up, what is the probability that at least 320 
out of 1000 tossed land point up? 
Soldion. Let X be the number landing point up. Then by, Eq. (5), 
the mean of X is (0.3)(1000)  300, and the standard deviation is 
,� ---- x/1000(0.3)(0.7)  14.5. Thus if 320 land point up, the number 
of "ups" in excess of the means is 320 -- 300 ---- 20, and the number of 
standard deviations from the mean is 20/1,i.5  1.38. From Table III 
the probability in excess of 1.38 standard deviations is approximately 
0.0838, or about 8%. 
Central limit theorem. In Chapter 7 we studied the behavior of the 
binomial distribution as n increases. We found that if X is the number 
of successes in n independent binomial trials, then the related random 
variable 
X -- np 
Z-- 
has a distribution that is closely approximated by the standard normal 
distribution if n is large. That result is a special case of a more general 
central limit theorem which we now state, and use, without proof. 
9-7 ?heorem. Central limit theorem. Let X1, X=,...,X,,,... be a 
sequence of identically distributed independent random variables, 
each with mean  and variance . Let 
T = X+X+...+X. 
Then, for each fixed value of z, as n tends to infinity, 
( T - n  z) 
approaches the probability that the standard normal random 
variable Z exceeds z. 
IEMARK. By subtracting E(T) ----  from T,, and then dividing by 
(rv---- x/- 5 ---- x/Var(7'n), we obtain a nev random variable whose 
mean is zero and whose standard deviation is 1, as are those of the standard 
normal. In more advanced work in probability, it is proved that the 
distribution of this new random variable approaches that of the standard 
normal as n tends to infinity. In practical terms, this means that if n is 
large we may use the standard norlnal tables to answer such questions 
as the one in the following example. 
, ( 
9-3] VARIANCE OF A SUM. AVERAGE OF V_R1ABLES 320 
EXMP.,: 3. 100 dice. Find the probability that when 100 dice are 
rolled the sum of the dots on their topmost faces exceeds 325. 
Sohdio. For the roll of one die we found the mean and variance of 
the number of dots on a face in Example 3, Section 5-4, to be 
 = 3.5, --  
-- 12' 
Applying Corollary 9-5 with n = 100, we find that thc mean, variance, 
and standard deviation of the sum of 100 rolls are 
= 00(3.5) = 350, = 
Then the vfiue 325 is --25 from the mean, or 25/17.1 = 1.46 standard 
deviation to the left of the mean. With the aid of Table III we find 
that the probability to the right of --1.46 is 0.9279 for the standard 
normal, or all but about 7. 
--3 2 --1.6 --1 0 i 2 3 
Ft. 9-2. Shaded area gives probability of total score on 100 dice ex- 
ceeding 325. 
Going from sums to averages. If xxe divide a sum by its number of 
measurements, xxe obtain an average. Therefore an average is a trivial 
adjustment of a sum. Theorem 9-7 completes the information we need 
about the distribution of a sum. X'ow we quickly derive the mean aud 
variance of the distribution of sample averages. These results will confirm 
the intuitive notion that sample averages are more stable than single 
measm'ements. 
As in Theorem 9-4, let 
Then the sample average i 
T 
330 THEORY OF SAMPLING [CHAP. 9 
We knoxv that xvhen xve multiply random variable by a constant xve multi- 
ply its mean, or expected value, by that same constant and xve multiply its 
variance by the square of the constant. Therefore 
Of course, since the mean of a sum is the sum of the means, 
/d, T = /d, 1 -- /d, 2 -- � . � -- 
and by Theorem 9-4, 
2 . 0.2 
0.3 = 0.12 + 0.2 +" + 
Thus we have proved the following theorem. 
9-8 Theorem. Means and variances of sample averages. Let the random 
variables X1, X2,.. �, X, be independent, with means 1, 2, � � � , 
 and variances 0.12, 0., . . . , 0.. Let the average of these variables 
be X, where 
Y = - (x + x2 +... + x). 
n 
Then X has a distribution with mean 
1 
.x =  (l + .2 +... + ,) (6) 
and variance 
(0.12 + 0. +... + 0.2) 
� (7) 
9-9 Corollary. Averages of dependent random variables having identical 
means and variances. Let the independent variables X1, X2, . . . , 
X have identical means, , and variances, 0.2, and let their aver- 
age be 
Y = -(;c + x2 +... + x). 
9-3] VARIANCE OF A SUM. AVERAGE OF V\RIhBLES 331 
Then 
av = (s) 
and 
o-.? n ' V (9) 
Proof. Substitute into Eqs. (6) and (7) of Theorem 9-8. [] 
Sampling with replacement. The most important application of Corollary 
9-9 is to samplitg with replacement from a finite population, or sampling 
from an infinite population. Let X1 represent some measured character- 
istic of the population element that is drawn first in the sample, X that 
of the element drawn secod, and so on. In sampling with replacement 
the probability functions of the random variables X, Z X are 
the same; the variables are identicall distribtded. Thus the observed 
measurements in such samples are values of random variables with equal 
means and variances, so Corollary 9-9 applies. 
In words, Eqs. (8) and (9) say that the expected value of the ai,erage 
of n measurements is the population mean , and the standard deviation 
of the averages from one set of measurements to another is inversely 
proportional to the square root of the number of lneasurements. Thus 
averages of 4 independent measurements drawn from the same popula- 
tion have a standard deviation equal to  the standard deviation of single 
measurements; and averages of 100 independent measurements drawn 
from the same population have a standard deviation equal to  the stand- 
ard deviation of single measurements. This shrinking of the standard 
deviation as n increases causes a tightening up of the probability distribu- 
tion of X around the population mean and practically guarantees that 
the sample average ties close to the population mean when n is suciently 
large. 
Bias, or systematic error. In an actual measuring procedure, there may 
be a systematic error. For example, one may consistently tend to read 
too high. Such a systematic error is not reduced by taking the average 
of repeated measurements. 
EXAPnE 4. Average for 100 dice. Find the probability that when 
100 dice are rolled their sample average exceeds 3.7. 
Sohdion. For a single die,  3.5,  s By Corollary 9-9, 
 --12' 
 = 2oo  0.0292, and eX-  0.171. If the 
332 TFOR� OF SAMPLING [CHAP. 9 
sample average exceeds 3.7, the, n it exceeds the population mean 3.5 by 
at least 0.2 (=3.7 -- 3.5), or by 0.2/0.171  1.17 standard deviations. 
From the normal tables, the probability in excess of 1.17 standard devia- 
tions is 0.121. There is less than i chance in 8 that the sample average 
for the 100 dice exceeds 3.7. 
9-10 Corollary. Mean proportion of successes for the binomial. Con- 
sider a binomial experiment composed of n binomial trials each 
with probability p of success and with total number of successes 
X. Let   X/n be the proportion of successes. Then the 
mean and variance of  are 
p(1 - p) (10) 
/2p = p and ap = 
n 
Proof. The proof for the mean was given in Section 8-2; the variance 
[] 
follows from Oorollary 9-6, since (f = 
EXAMPLm 5. Voting. If 60% of a large population favors a certain 
candidate, what is the probability that in a random sample of 100 voters 
the proportion in favor of the candidate is under 50%? 
 ---- (0.6)(0.4)/100 = 0.0024, ap = 0.049. 
Solution. /2p = p = 0.6,  
The difference 0.5 -- 0.6 = --0.1 is --0.1/0.049  --2 standard devia- 
tions, or 2 standard deviations below the mean. Using the normal approxi- 
mation, we find that the probability is about 0.025, or about 1 chance 
in 40. 
EXERCISES FOR SECTION 9-3 
In these exercises, samples are drawn with replacement. 
In Exercises 1 through 10, the population has mean /a = 6 and standard 
deviation a = 10. 
1. What is the variance? 
2. What is the standard deviation of the averages of samples of size 4 drawn 
from this population? 
3. What standard deviate, x' = (x- t)/(r, does the observation 16 cor- 
respond to? 
4. A new population is formed by adding 3 to every observation in the given 
population. Find the mean and standard deviation of the new population. 
5. A new population is formed by multiplying every observation in the given 
population by 3. Find the mean and standard deviation of this new population. 
6. Find the mean and standard deviation of .the distribution formed by 
taking the sum of the observations of every sample of size 4 drawn from the 
original population with replacement. 
9-3] VARIANCE OF A SUM. AVERAGE OF VARIABLES 333 
7. Chebyshcv's Theorem guarantees that at least 75% of the original popu- 
lation lies between what two numbers? 
8. If the original population is normally distributed, what proportion of it 
lies to the right of the mean? 
9. If the original population is normally distributed, what proportion of it 
has values larger than 11 ? 
10. If the original population is normally distributed, what is the valuc 
exceeded by 80% of the population? 
11. The standard deviation of a population of scores is 36. What is the 
standard deviation of the distribution of sample averages for samples of size 
16 drawn from this population? 
12. A large population of measurements has mean / = 20 and standard 
deviation  = 5. Consider samples of 4 measurements, each randomly drawn 
from the original population. Vhat is the expected value of the sample avcragc 
X for sueIx samples? What is the standard deviation of the distribution of X? 
13. The distribution of weights (in pounds) of a largc group of equipped army 
recruits is very closely approximated by a normal curve with mean 185.0 pounds 
and standard deviation 15.0 pounds. (a) If two recruits are picked at random 
from this group, what is the probability that both their weights are bet een 170.0 
and 200.0 pounds  (b) If 81 recruits from the large group are to enplane with 
an allowance of 190.0 pounds per man, what is the chance that the transport 
will be overloaded ? 
14. A certain mental test yields scores in months of mental age. The errors 
of measurement in ths test average zero in the long run, with a standard devia- 
tion  of 2 months. A fifth-grade class of 36 students took this test. What is 
the probability that the average score for the class is in error by 1 month or 
more (either too high or too lo)? 
15. Given a large population of test scores with mean 20 and variance 9. 
What is the standard deviation of the sampling distribution of averages of 
samples of sze 25 drawn from this population? 
16. In crossing two pink flowers of a certain variety thc resulting flowers arc 
either white, red, or pink, and the probabilities that attach to these various 
outcomes are �, i, and � respectively. If 300 flowers are obtained by crossing 
pink flowers of this variety, what is the probability that 90 or more of these 
flowers are white? 
17. In a certain large society thc standard deviation of the number of children 
in a family is 1.5 If an anthropologist wants the standard deviation of X, his 
estimate of thc mean number of children per family, to be 0.1, ho many families 
should be in his random sample? 
18. The standard deviation of the distribution of sample averages in samples 
of size 9 is 4. What is the standard deviation of the population from which the 
sample is drawn? 
19. A mass-produced object has 2 mirrored faces. The percentages of such 
objects with 2, 1, or 0 marred mirrored faces are 1, 1, 98, respectively. (a) If 
four objects are randomly drawn from a very large lot with this composition, 
compute the exact probability of 2 or fewer faces being unmarred all told (com- 
pute as if the sampling cre done with replacement). (b) If 900 objects are 
33-t THEORY OF SAMPLING [CHAP. 9 
raidoinly drawn, computc the approximate probability that there are 1775 or 
fex;er unmarred faces. [ttint. First compute the mean and standard deviation 
for the number of marred faces on one object.] 
20. The listening time per week to the musical programs of a radio station 
by a certain large group of people is approximately normally distributed, with 
mean z -- 4 hours and standard deviation a -- 1 hour. A_ sample of 16 people 
to be drawix from this group has average listening time �. What is the proba- 
bihty that  will differ from the group mean by more than half an hour? 
21. In a before-and-after cxperiment, the mean difference in the population 
is 1.00, and the standard deviation of the differences for individuals in the 
population is 2.00. Find the probability of a positive difference for a randomly 
selected individual. 
22. From a distribution with variance a2 = 1, two independent random 
samples arc drawn as in the following table: 
Sample size Sample average 
Sample 1 lO X 
Sample 2 5 X 
To estimate the population mcan 
(a) 0nc man weights the sample averages in proportion to their sample sizes 
and claims that the sample variance of such a weighted estimate,  -- 
is . Justify this result. 
(b) A second man merely averages the sample averages and uses (5:1 - X). 
Show that the sampling variance of such an estimate is  
40' 
(c) Show that both methods are unbiased (have mean equal to the true 
lnean z). 
(d) Explain (a) from the point of view of one sample of size 15. 
23. Oiven a population of 500 winch is coinposed of txo subpopulations 
or strata. Stratum 1 has 400 members and stratum 2 has 100 members. A 
random sample of size 20 is taken from each stratum. The average of the 
sample of stratum 1 is 4.0 and the standard deviation of this sample is 1.0. The 
average of the sample of stratum 2 is 8.0 and the standard deviation of the 
sample is 2.0. (a) Estimate the grand mcan of the total population. (b) Esti- 
mate tile standard deviation of the estimate of the grand mean. 
24. Which of the following choices is true? In sainplhg, as the size oT the 
sainple increases, the standard dcviation of the theoretical distribution of 
,sample averages (a) decreases in value, approaching 0; (b) increases in value, 
growing vlthout bound; (c) does not necessarily incrcase or decrease, but 
approaches the value of the true (uuiverse) standard deviation. 
25. A fair coin s tossed n times. Each time a head appears a is added to the 
scorc, each time a tail appears b is subtracted from the score. Determine the 
incan aml variance of the distribution of the score aftel' n throws. 
26. Suppose that the birth-weights of children are normally distributed with 
mean 7 pounds and standard deviation � pound. Suppose that the two sexes 
are equally hkely and that birth-weight is indcpendent of sex. (a) What is the 
probability that four children in a farofly are all males? (b) What is the proba- 
9-4] COVARIANCE AND CORRELATION 335 
bitity that the four children, xvhatever their sex, have an average birth-veight 
over 7.5 pounds? (c) What is the probability that the children are all males 
and have an average birth-weight over 7.5 pounds? (d) Are you using an 
assumption not stated? If so, what? 
27. In a motor-skill experiment, 100 subjects perform a task in 25 randomly 
selected groups of 4, but each subject has a separate cubicle, so that he does not 
influence other members of the group. From the frequency distribution of the 
scores of these 100 subjects, the experimenter finds the mcan to be 30 points and 
the standard deviation 10 points. Then the experimenter obtains for each of the 
25 groups the sum of the scores of the 4 subjects. He wants to know 
(a) the value of the mean of the 25 sums, 
(b) the value of the standard deviation of the 25 sums. 
Tell him the answer to (a) exactly. Using the data given and 5'our knowledge 
of the theory in this chapter make a good estimate of the result for (b). 
On hearing the result for the standard deviation of the sums for the groups, 
the experimenter is surprised. He says "The law of averages should have made 
the standard deviation for the groups smaller, not larger than the standard 
deviation of the individual scores." Comment crisply on his misapprehension. 
9-4. DEPENDENT RANDOM VARIABLES: COVARIANCE AND COR- 
RELATION 
In the earlier sections of this chapter, we found that the variance of a 
sum of independent random variables is the sum of their variances. What 
if the variables are dependent? The result, derived in this section, intro- 
duces two important new concepts: covariance and correlation. 
Having in mind some analogies between the variance of the sum of 
two random variables, on the one hand, and the theorem of Pythagoras 
and its generalization, the law of cosines, on the other hand, may help 
us to keep our bearings as we go through the algebra. Table 9-3 shows 
the two theorems for triangles and the corresponding results for random 
variables. The new symbols in the table are Coy (X, Y), read "cozariance 
of )f and Y," and p (Greek letter "rho"), which represents the correlation 
between )f and Y. 
Variance of X -[- Y. To compute the variance of )f -]- Y, we apply 
the definitional formula for variance, 
(r-+r = E(X - Y- /x+v) . (1) 
Whether X and Y are independent or not, the mean of X 4- Y is the 
mean of X plus the mean of Y: 
= + (2) 
336 THEORY OF SAMPLING [CHAP. 9 
TkBLE 9-3. ANALOGOUS THEOREMS FOR TRIANGLES AND 
FOR RANDOM VARIABLES. 
Theorems for trmngles Theorems for random variables 
In a right triangle, If X and Y are independent, 
c a s+ 2. 2 = . + .. 
(Theorem of Pythagoras) 
In any triangle, , For any random variables, with 
c  = a 2+b 2 - 2abcosC I finitexariances, 
I   +  +2Cov(X,Y) 
X+Y = X Y 
(gaw of eosines) I 
We How substitute from Eq. (2) into the right side of Eq. (1), group 
X terms and Y terms, square, and get 
 E[(x x)  + 2(x x)(Y r) + (Y r)]. 
UsiHg the fact that the expected value of a sum is the sum of the expected 
values, and that 
we expand the right side of Eq. (3), rearrange terms, and get 
X'+r = E(X -- x  E(Y -- r) 2 + 2E[(X -- x)(Y -- -)]. (4) 
We recognize the first two terms on the right side of Eq. (4) as } 
and , from their definitious: 
E(X x) 2 
-- = aX-, (Sa) 
Thus Eq. (4) agrees with one of the entries for a-+v in Table 9-3 pro- 
vided we deftHe Coy (X, Y) as follows: 
9-11 Definition. Covariaace. The covariance of two random variables 
is the mean value of the product of their paired deviatioHs from 
their own means. In symbols: 
Coy (x, Y) = s[(x - x)(  - r)]. �) 
9-4] COVALX.XCn AND CORRI�L %TI()N 337 
It {S ev{dcnt froin Eq (6) that the commutativc law for covariance 
holds: 
Corem utahve law for cora riancc: 
Coy (X, Y) = Coy (Y, X). 
It is also clear that the covariance of a random variable with itself is its 
variance, because 
�'ov (x, x) = E[(X - :c)(x - x)] = Var (X) = 
If we substitute from Eqs. (5a, b) and (6) into Eq. (4), we see that we 
have proved the following theorem: 
9-12 Theorem. Iariance of a sun: dependent or independent random 
variables. Let X and Y be random variables with finite variances 
(r.- and (r-, and with covariance as defined above. Then the 
variance of their sum is given by 
 =  ,  __ _o Cov (x, Y) 
x+r - - � (7) 
EXAMPLE 1. Means zero. Let X and Y have the joint probability 
function shown in the table below. Find the variances of X and Y, their 
covariance, and the variance of X-b Y. 
Values of ' 
--1 1 P(X = x) 
--2 0 4 0.1 0.5 
Values 
of X 2 0____ 0.4 0.5 
P(Y = y) 0.5 1.0 
Solution. Since the means are x = /zr ---- O, 
 '(X -- x)  E(X ) = 0.5(2)  + 0.5(--2)  4, 
, = E(Y- pt) 2 = E(Y ) = 0.5(1) 2 + 0.5(--1) 2 = 1, 
338 THEORY OF SAMPLING [CHAP. 9 
Coy (x, Y) = �[(x - ,x)(Y - )1: �(xY) 
= 0.4(2)(2) + 0.2(2)(-1) + 0.1(-2)(1) + 0.4(-2)(-1) 
---- 1.2, 
+- =  +  +  Co (X, ) 
= 4 + 1 + 2(1.2) = 7.4. 
Note that the covariance term increases the sum of the variances of 
X and Y by nearly 50% to give the variance of X  Y. 
Computational formula. When neither mean is zero, the covariance can 
be computed from a convenient formula derived from Eq. (6)' 
Coy (X, Y) = E[(X -- x)(Y -- )]  E[XY -- xY -- X  .]. 
The expectation of a sum is the sum of the expectations, so ve expand 
the right-hand term' 
Coy (X, Y)  E(XY) -- E(gxY) -- E(grX)  E(gxg.). 
Recall that E(cX)  cE(X), and E(c)  c. Since g.� and g- are 
constants, we can write 
Coy (X, Y): E(XY) -- xE(Y) -- rE(X)  �. 
We now substitute for E(Y) and E(X) their values gr and g.� and get 
Coy (X, Y)  E(XY) -- gxgr. (8) 
EXAMrnE 2. Let X, Y have the joint probability function shown in 
the table below. Find the variances of X and Y, their covariance, and 
the variance of X  Y. 
x y P(x, y) Sol, tion 
i i 0.1 .� = 0.2 
i --1 0 5  = --0 6 
--1 i 0 1 a = 0.96 
--1 --1 0 3 a = 0.64 
9-4] COV^R^NC AND CORRELATION 339 
Coy (X, Y) = 0.1(1)(1) q- 0.5(1)(--1) q- 0.1(--1)(1) 
+ 0.3(-1)(-) - (0.2)(-0.6) = -0.08, 
2 
}+r = x + - + 2 Coy (X, Y) 
= 0.96 q- 0.64 q- 2(--0.08) = 1.44. 
Here the covariance has the effect of decreasing the variance of the 
sum 10% below the value .- q-  ---- 1.60 that would result if X and 
Y were independent. 
Indepo, dence. Fro Theorem 6-6, Section 6-1, we know that if X 
and Y are independent, then 
E(XY) = E(X)E(Y) --- 
Therefore we find that if X and Y are independent their covariance is 
zero, because then 
Cox, (X, Y) = E(XY) -- xv = xv -- xv ---- 0. 
Thus the following theorem is true. 
9-13 ?heorem. If X and Y are independent, then 
I Coy (X, Y) = 0. (9) 
The population correlation. In some problems, it is convenient to have 
a measure of the degree of linear relationship between X and Y that does 
not depend upon their units of measurement. The covariance does de- 
pend upon the units of measurement, as the following theorem shows. 
9-14 ?heorem. Let X and Y be random variables with means 
tv and variances }, . Form the variables 
X X- 
o- X 
(10) 
Y-- /y 
yt  
O'y 
Then 
Coy (X, Y) = o'xoT Coy (X', Y'). (11) 
340 HEOR� OF SAMPLING [CHAP. 9 
Proof Both X' and Y' are constructed to have means 0 (and standard 
deviations 1). Their covariance is therefore 
Cov(X', Y')= E(X'Y')---- E[ X --x Y-- 
o' X O'y 
By factoring the constant 1/o'xo'y outside the expectation, we find 
Coy (x', Y') = �[(x - ,x)(Y - 
O'xO' Y 
Coy (X, Y). 
O'X7 Y 
Multiplying through by axaY completes the proof of Eq. (11). [] 
REMAIK. A shift of origin of X leaves the numerator and denominator 
of X' unchanged. A change in scale, or units, e.g., from feet to yards 
or from pounds to ounces, multiplies both the numerator and denominator 
by the same constant, and therefore leaves X' unchanged. Thus any 
combination of these transformations leaves X' and Y' unchanged. There- 
fore, Cov (X', Y') is independent of the origin and units of measurement 
of X and Y and provides a measure of linear relationship between them 
with the desired properties. 
9-15 Definition. Correlation. Let X and Y be random variables with 
positive standard deviations ax and ay. Then the correlation 
between X and Y, denoted by p, is defined by 
p =Cov (X', Y'), 
where 
(12a) 
X' X -- .v y, Y -- 
0' X O'y 
Alternatively, from Eqs. (11) and (12a)' 
Coy (X, Y) 
---- , Cov (X, Y) ---- pxr. (12b) 
P 
When X and Y are independent, Cov (X, Y)= O, and therefore 
p----O. But the converse is false: p = 0 does not necessarily imply 
9-4] COVARIANCE AND CORRELATION 341 
that X and Y are independent, as shown by the three-coin example, 
Table 6-2. When p ----- 0, we say X and Y are ttncorrelated. 
Conptttational fortreda for correlation. If we use Eq. (8) to evaluate 
Coy (X, Y) in the numerator of Eq. (12b), we get an alternative formula, 
often useful for calculations: 
E(XY) -- xr 
P ---- xY (13) 
EXAMPLE 3. For the population of Example 1, compute p. 
Solution. The substitution into Eq. (12b) of the values shown in the 
solution to Example 1 gives 
1.2 
__---- 0.6. 
= i 
EXAMPLE 4. For the population of Example 2, find the correlation 
between X and Y. 
Soltttion. The substitution into Eq. (12b) of the values found in the 
solution of Example 2 gives 
--0.08 --0.08 
= - --  --0.10. 
P x/0.96(0.64) 0.784 
Range ofp. Values of p can range from --1 to +1 inclusive. For discrete 
probability functions, p -- i or --1 if and only if all (X, Y) pairs with 
non-negative probability lie along a straight line. (If the line is horizontal 
or vertical, then x or y is zero, and p is undefined.) The slope of the line 
determines the sign of p. We shall not prove these facts. However, 
if we replace Coy (X, Y) by prrxrr in Eq. (7), Theorem 9-12, we get the 
following theorem, which bears a striking resemblance to the law of 
cosines, with p playing the role of cos (180 � -- C). 
9-16 ?heorem. Variance of a sum: correlated random variables. Let 
X and Y be random variables with finite variances }, , and 
correlation p. Then the variance of their sum is 
' 2 rr} d- o' d- 2pO'xO'�. (14) 
O'X+ Y  
342 HEO� OF SAMrLNG [CHXP. 9 
Note that zero correlatiqn gives the result 
2 2 
.�+y = x  . 
Sample correlation coecienl. For  set of pairs of measurements (x, y), 
i  1, 2,..., n, there is  smple correlation coecient, r, defined by 
nalogy with p s 
r = E( - )(y - ). 
nSxSy 
Two additional forms of r are 
r xiYi -- n: (16) 
---- , 
nSxSy 
r = xyi/n -- . (17) 
SxSy 
We shall study r in Chapter 10. 
Variance of the sum of two or more correlated random variables. Sometimes 
we deal with sums of several correlated random variables. For example, 
students may take several tests in the same course. Let the score on the 
first test be X1; on the second, X; on the third, Xs; and so on. What 
can we say of the mean and variance of the distribution of the sums of 
the scores? We answer this question by extending the results of Theorems 
9-12 and 9-16 to sums of more than two random variables. First we 
extend the results algebraically to three variables; the results for n vari- 
ables can be proved similarly, but we shall merely state the general result. 
Variance of the stun of three variables. Let X, X, and X have means 
1, , s and variances a[, a, a, respectively. Let 
Z = X 1 --{- X 2 --{- X' 3. 
Then, by definition, 
 = E(Z -- z)  
= E[(X1 -- /gl) - (X2 -- /g2) -- (X3 -- /,3)] 2 
Recall that 
(a q- b q- c)  = a 2 + b '� q- c  q- 2ab q- 2ac q- 2bc. 
9-4] COVARIANCE AND CORRELATION 343 
Therefore 
+ 2(X - - 
The expectation of a sum is the sum of the expectations. Therefore 
ff -- ff + if2 2 + ff + 2 COV (X1, X'2) + 2 Cov (X1, X3) 
(lS) 
+ 2 Coy (X, X). 
In words, the variance of a sum of sa'eral random variables is the sum of 
their variances plus twice the sum of the covariances of all possible pairs of 
rando7n variables. And this statement is true for many variables as well 
as for three. In Eq. (18), the 2's that are coefficients of the covariances 
can be viewed as coming from the squaring operation, or equivalently 
from 
Coy (.V, Xj) d- Coy (X, Xi) = 2 Coy (X, Xj), i  j. 
9-17 Theorem. Variance of a sum. Let the variables X1, X2, . . . , X n 
2 and covariances Coy (Xi, Xi), 
have variances al 2, a,...,a, 
i  j, and let 
Z = X' 1 - X' 2 - ' ' ' - Xn. 
Then 
:   -]- 2  Coy (X, Xj'), (19) 
where the sum of the covariance terms runs over all possible 
pairs with i < j, i---- 1, 2,...,n;j---- 1, 2,...,n. 
Note that n variances are added and that n(n -- 1)/2 covariances are 
added because there are () ---- n(n -- 1)/2 selcctions of 2 variablcs from 
n variables, disregarding order. If n ---- 3, n(n -- 1)/2 ---- 3, which chccks 
with our result for three variables. 
344 THEORY OF SAMPLING [CHAP. 9 
Sometimes Ave introduce P3-as the correlation between X, and X3, 
and then the covariance can be written as 
Coy (X, X,) = p,ma,. (20) 
We shall not explicitly substitute (20) into Eq. (19), but in the special 
case where all Pu equal p and all o- equal a, Eq. (19) can be rewritten as 
o' = n '2 + nn- 1) Coy (X,, X) 
= no ' - n(n -- 1)pa : na[1 - (n -- 1)p]. (21) 
EX^MrLE 5. Simphfied armq alpha test A test is coinposed of four 
parts. (l) arithmetic, (2) number completion, (3) word analogies, and 
(4) information. Let X stand for the score on arithmetic, X for that 
on number completion, and so on. Scores on each part have mean 50 
and standard dewarion 10, and the varmbles are correlated as follow 
(after J.P. Guilford, Psychometrzc Methods, McGraw-Hill, 1936, p 491) 
(1) (2) (3) () Sum 
(1) 49 .09 31 S9 
(2) 4o 6 6 
(3) 29 29 
 74 
Find the mean and standard deviation of Z, the sum of the score of the 
four parts. 
Solution. The mean of the sum is the sum of the means, or 4(50) ---- 200. 
Since all four variables have the same standard deviation, 10, Eq. (19) 
reduces to 
2 
z 10014 d- 2 (sum of correlations)] 
: 10014 d- 2(1.74)] = 748. 
9  , 
Therefore O'z = x/4 -,.o. The sums for such tests are usually 
normally distributed, so we know that about  of thc scorcs are within 
one standard deviation of the mean, i e., between 173 and 227. 
EXAMPLE 6 Matchi,g A deck of cards numbered 1, 2,...,  is 
,shuffled and the cards are dealt one at a time. If card iratuber i is the tth 
card dealt, we coimt it as a match, otherwise not. Find the mean and 
the variance of the total number of matches. 
9-4] COVARIANCE AND CORRELATION 
Solution. Ve introduce ** random variables X, z: 1, 2, . .., ?l; oe 
for each card dealt. If the ith card dealt has the number i on it, then 
X = 1' otherwise Xi = 0. The total number of matches is 
7' = X1 -X2 a ... + X,,. (2'2) 
We want to find the mean and variance of T. To this end, we need to 
find the mean and variance of each X, and the covariance. of all pairs 
X, and Xy. 
For any Xi, we have 
= 1/, 
because there is just one favorable card, i, among the , era-ds. Since all 
/, = 1/*, 
It is eaby to get Var (X,), because X, = 1 with probability 1/, and 0 
with probability 1 -- 1/m Therefore 
Var (X,) , .o 2 
To find the covariancc of X and Xi, axe apply the computational 
formula, Eq. (8)' 
Cov (X. X;) = 2 (XX ;) -- ,;. 
The possible values of XX are 0 and 1, and 
(, - 2) 1 
n  n(n - 1) 
since XiX i = 1 if and only if both cards number i and j are in their re- 
speetive matching places, and there are (n -- 2) permutations of the 
remaining n- 2 cards that correspond to this event. Therefore, if 
i j, 1 1 1 1 
Coy (X, X) ( -- 1) t n ns(n -- 1) (24) 
346 ThEOrY OF SAMPLING [CHAr. 9 
We are now ready to substitute from Eqs. (23) and (24) into the first 
line of Eq. (21). The result is 
n--1 1 
, = Var (T) = n. n----h---- -]- n(n -- 1) . n2(n _ 1) 
= _ i_ + = (25) 
n n 
The results are striking--the variance and the mean are both equal 
to one. Obviously, the result applies to the number of matches when a 
bridge deck is shuffled and then compared card by card with a previously 
laid out target deck. Although any number of matches between 0 and 52 
is possible, the mean number of matches is only 1, and the standard 
deviation is also 1, which is very small compared with the number of cards 
in the deck, 52. By Chebyshev's Theorem, we do not expect large fluctua- 
tions about the mean in repeated experiments of this type. 
� EXERCISES FOR SECTION 9-4 
1. Let X and t' have lhc following joint probability function: 
x y P(x, y) 
0 i 0.4 
0 --1 0 3 
i 0 0.2 
i i 01 
2 2 
Find (a) E(X), (b) E(Y), (c) (rx, (d) (e) E(XY) (f) Cov (X, Y), and 
fly,  
(g) p, the correlation between X and Y. 
2. If 2 x- = 0 or 2 = 0, what can you say about the covariance of X and Y? 
Explain. 
3. If t' = kX, for some constant k, how is Cov (X, Y) related to Var (X)? 
4. Prove that Coy (ag, bY) = ab Cov (X, t'). 
5. What is the correlation between aX arid X, if (a) a = 17 (b) a = --17 
(c) a = 57 (d) a = --5? 
6. What is the correlation betweeH X and Y if t' = aX + b? 
7. On the first quiz in a mathematics class, the class average was 85 and 
the standard deviation was 5. On the second quiz, the class average was 80 and 
the standard deviation was 7. The same 30 students took bofi tests. The 
correlation between grades on the two tests was 0.7. Let X represent tim 
grade on the first test, for a randomly chosen student, and let Y represent his 
9-4] COVARIANCE AND CORRELATION :547 
grade on the second test. Find (a) /v+- and (b) 2 
.�+y. (e) Asstiming that the 
probability that the variable  = �(X  d- Y) takes a value greater than 
can be approximated by a normal random variable with mean equal to 
and variance equal to ., find (approximately) the number of students whose 
average scores, on the two tests, xxere 75 or better. 
8. Prove the "distributive law": 
Coy (x + :f', z) = Coy (.v, z) + Coy (:f', z). 
9. By repeated applications of the commutative law, 
Coy (Y, X) = Coy (X, Y), 
and the distributive law, 
Cov (.v + t', z) = Co,. (.v, z) + Coy (�, z), 
show that 
Cov (X d- Y, \' d- Y) =Cov (.\', X) d- 2 Cov (\', 7f') d- Coy (Y, Y). 
To what previously known result does this correspond? 
10. (Continuation.) Show how the method of Exercise 9 can be generalized 
to produce formula (18) for the variance of a sum of three random variables. 
11. The 3 random variables X, Y, Z all have means equal to zero and variances 
equal to 1, and any two of them have correlation p = 0.5. Find (a) the mean 
of X d- Y d- Z, (b) the variance of X d- Y d- Z, (e) the probability that 
U = X d- Y d- Z exceeds 2, assuming that U is normally distributed. 
12. If the variance of X d- X2 d- � '' d- X in Eq. (21) is zero, but a 2  0, 
what can you say about the correlation p? Note that this is the smallest value 
p can have. 
13. Twenty married couples attend a masquerade ball. The men form one 
line and the women form another, each in random order. What is the proba- 
bility that the 4th man in line is the husband of the 4th woman in line? That 
the 5th man and woman are husband and wife, and that the 6th man and 
woman are also husband and wife? 
14. In Exercise 13, suppose the ith man in line dances with the ith woman 
in linc, i = 1, 2, . . ., 20. What is the expected number of couples with husband 
and wife dancing together? What is the standard deviation of that number? 
15. A secretary addresses 40 envelopes and types 40 individual letters to be 
mailed in them. While she is out of the occ for lunch, a practical joker puts 
the letters into the envelopes at random and malls them. What is the probability 
that both Mr. Jones and Mss Smith receive the letters intended for them? 
That Mr. Jones, Miss Smith, and Mr. Thornton all receive their proper letters? 
What is the expected number of persons who receive the letters meant for them? 
Using Chcbyshev's Theorem, state an upper bouml for the probability that 25% 
or more of the letters go to their intended recipients. 
16. Take the clubs and diamonds from an ordinary bridge deck and lay them 
out in order: Club ace, king, . . ., two; Diamond ace, . . . , two. Shuffle the hearts 
348 THEORY OF SAMPLING [CHAP. 9 
and spades together, then lay them out one after another beside the other cards: 
first card beside the Club ace, second beside the Club king, and so on. Whenever 
the card from the shuffled half-deck matches the target card in color and de- 
nomination, score a 1; otherwise don't count it. Count and record the total 
numbel' of matches. Perform this experiment 10 times and make a frequency 
distribution of the numbers of matches in the 10 performances. Find the mean 
and variance of this frequency distribution and compare them with the theoretical 
values. 
17. Each of n random variables has variance 2, and each pair has correlation 
p. Find the standard deviation of their average. 
18. Each of n questions on an examination is scored so that its mean is zero 
and its variance 2. The total score for the examination, T, is the sum of the 
scores of the individual test questions. The correlation between the scores of 
any pair of questions is p = 1/(n -- 1), n >_ 2. Find the variance of T. 
19. Let X and Y be independent random variables with zero means and 
variances 2x, 2 v. Find the correlation between X and Z, where Z = X q- Y. 
20. Let X, X2, ..., Xk be independent random variables with zero means 
and equal variances 2. Let a, a2, � � � , ak and b, b2, . � � , b be numbers such 
that 
k 
 a,b = O. 
*=1 
If U = a,Xtq-a2X2q-...q-osXs, and V = btXtq-b2Xeq-...q- 
prove that U and V are uneorrelated. 
21. Students applying for admission to a program are required to take an 
aptitude test and an achievement test. For this particular student population, 
the scores X and Y on the two tests have means 50, standard deviattons 10, and 
correlation p = 0.62. Only students who get a combined score X q- I' of at 
least 136 points are admitted to the program. Assume that the combined scores 
are normally distributed. (a) Find the mean and variance of the distribution 
of X q- Y. (b) What percent of the apphcants are admitted? (e) If the director 
wants to admit only the top 25% of the applicants, based on this score, what 
new cutoff score would you suggest? 
22. There are 2n items on a test. Each item is scored so that its mean is zero 
and its variance is 1. The correlation between an)' pair of items is p. Let the 
score on the first n items be �, where 
Y = X' q- X2 q- � � � q- X,,, 
and let the score on the second n items be Z, where 
Z = X,+l q- X,,+2 q- � � � q- 
and where X, is fie score on the ith item. Show that the correlation between 
Y and Z is 
p(I', Z) = np 
+ (n - 
Discuss tile result for the to special cases (a) n = 1; (b) np= 100. 
9-5] SAMPLING FROM A FINITE POPULATION :349 
23. An experimenter measured two characteristics on each of n = 25 animals, 
thus obtaining 25 pairs of measurements (x, y), i = 1, 2, ..., 25. From these 
data, he computed 
Find , , Sx, sy, and the correlation, r, between x and y. 
For Exercises 24 through 31, an ordinary six-sided die is thrown n timc in 
succession. Let X  1, and let X be 1 if the ith throw produces a result 
different from those obtained on the first i -- 1 throws. Otherwise X  0. 
Thus Y  Xt  X  ...  X is the number of different faces of the die 
that have appeared on top in the n throws. 
24. Find (a) P(X : 1), (b) E(X), and (c) Var (X). 
25. Find Coy (Xt, X). 
X 
26. Find p(X,, ). 
27. Find (a) E(Xt  X) and (b) Var (Xt  X). 
28. Find (a) P(X : 1), (b) E(X), and (c) Var (Xs). 
29. Find (a) Coy (Xt, Xs) and (b) Coy (X, Xs). 
30. Find (a) E(XtX-Xs) and (b) Var(Xt XXs). 
31. For fixedn  3, andk  1,2,...,n-- 1, find: 
(a) P(X  1), E(X), and Var (X). 
(b) Coy (X, X). 
(e) E(xt + .�2 + ... + 
In Exercises 32 through 34, two ordinary dice are thrown once. Let X be tle 
maximum of the scores on their topmost fces, and let Y be the absolute value 
of the difference between those scores. 
32. Write out a sample space for the experimenL together witl the paired 
values (X, Y) for the smple points. 
33. Obtain, from tle smple space of Exercise 32, the joint probability func- 
tion of X and Y. 
34. rom the result of Exercise 33, compute the eovarianco and the eorrelafiou 
of X and Y. 
9-5. SAMPLING WITHOUT REPLACEMENT FROM A FINITE POPULA- 
TION 
Our development of properties of distributions of sample averages has 
been restricted to infinite populations, or to sampling with replacement 
from a finite population. In many practical sampling problems the sam- 
pling is done without replacement, and the reader may wish to know the 
mean and variance of the distribution of sample averages from such popu- 
350 THEORY OF SAMPLING [CHAP. 9 
lations, and to compare the results with the corresponding formulas for 
sampling with replacement. For example, if you plan to ask opinions on 
a school issue from a random sample of 10 students drawn from a class 
of 50, you xvish to know what accuracy to expect. 
EXAMPLE ]. For a population of size N----4 with measurements 
x ---- 0, x2 = 3, xa---- 2, x4---- 3, find the mean and variance of the 
distribution of sample averages for random samples of size n ---- 2, drawn 
without replacement. 
Solution. Using the ampling notation introduced in Section 9-4, we 
let X1 represent the measurement of the population element drawn first, 
Y2 that of the element drawn second. There are 4.3---- 12 possible 
samples of individuals, as shown in Table 9-4 together with the sample 
averages ;i, J ---- 1,..., 12. The table is symmetrical about the main 
diagonal because each pair can be reversed, but the diagonal cells are empty 
because the sampling is without replacement. 
TABLE 9--4. SAMPLES (71, 72) AND SAMPLE AVERAGES 
�2, second sample element 
�1, Xl ---- 0 1.5 1.0 1.5 
first 
x2 = 3 1.5 2.5 3.0 
sample 
element x3 = 2 1.0 2.5 , 2.5 
x4 = 3 1.5 3.0 ' 2.5 
E i = 24.0, E.; 2---- 54.00. 
The probability function of the random variable 7, whose values are 
the :'s, is given by the probability table 
_4_ 4__ 
12 12 12 12 
 1.0 1.5 2.5 3.0 
We can compute the mean and variance of  either from Table 9-4 
or from the probability table. Using Table 9-4, we get 
12 
12 -- 12- 2, 
9-5] SAIIPLING Fn01! A FiN1TE POPULATION 351 
12 --2 
2 x3 54 22 1 
This example illustrates the distribution of sample verages and the 
-- 2 
calculation of E(X) and a X- for a small sample from a small population. 
We now generalize the results. 
Let the ith individual in a population of size N have a measured char- 
acteristic x,, i = 1, 2,..., N, and let the population have Incan 
N 
and variance 
In the derivation of the mean and varmnce of averages for random 
samples drawn from this population, the algebra is simplified if we trans- 
late the origin of the x measurements to their mean. Assume that this 
has been done; then Eq. (1) implies that 
N N 2 
(a) / ---- 0, (b)  .' ---- 0, (c) rr 2  '% 
= y. (3) 
=1 z=l 
Suppose a random sample of n is drawn, without replacement, from 
this population. Wc introduce n random variables, X1, X2,..., X, one 
for each element in the sample. The value of X1 is the measurement of 
the characteristic of the particular population member drawn first, the value 
of X2 is the measurement of the population member drawn secozd, and 
so on, X i being the measurement of the population member drawn as the 
jth element of the sample. Observe that order counts in this setup; we 
distinguish between the sample of 2 elements in which X1 ---- x, X2 - x, 
say, and the sample in which X1 ---- x, X2 ---- x2. 
We want the mean and variance of the sum 
Z ---- X 1 A,- X2 -[- ...-[- Xn ' (4) 
and of the sample average 
 __-- Z = Xl ,A- �2 -[- ' ' ' + -,�n (5) 
n n 
:352 THEORY OF SAMPLING [CttAP. 9 
2 and covari- 
We therefore need to compute,the means, 3, variances, a3, 
ances, Cov (X, X). 
Mean aid variance of X. The population measurements x, x=, . . . , xv 
have equal probabilities of being the jth in the sequence X, X=,..., Xn. 
Hence the probability function of X is given by 
1 
P(v, = x) = '(x, = z,) ..... P(x, = .v) = . 
Therefore 
 1  xi_ 
= o, 
xxhere the result   0 follows from Eq. (3a). 
Shxcc   0, the variance of X is given by 
 2 
a; = E(X = . x = . 
To smmnarizc, the implications for a population with general mean  
and variance if2 am 
lld 
 2  ...  &n  
By applying the result of Corollary 6-5 of Section 6-1, xxe find the 
following: 
s(z) = s(Zx2 = Zs(x2 = Z = ,, (6) 
xhere the summations extend from j = 1 to , and 
E(x) = '(z/,t) = s(z)/, = ,/, = . (7) 
So the mean of sample gvemgea is the population mean 
Covariance of X; and X. We lean agaiu on the fact that the origin of 
measurements has been chosen to make the population mean equal to 
zero. Since X i and X also hztve zero means, 
Coy (xi, x) = L(X;X). (8) 
To evaluate this expeetatlon, we need the joint p'obability function 
of X; and X. When j  k, the m'dered pair (X;, X) is a sample of 
9--5] SiMPLING FROM A FIN1TE POPULATION 353 
2 different individuals from the population, say (xT, x). There are 
N(N -- 1) such ordered pairs of individuals from the population, as in 
Table 9-4 where N = 4. Thee pairs are equally likely, so each has 
probability 1/N(N- 1). Therefore 
1 
P(X3 = xr,-� = .r) -- N(N -- 1)' (9) 
where Eq. (9) is true for all pairs of different random variable and all 
pairs of different individuals in the population. 
To compute E(X3X, for fixed j  k, observe that this product can 
take every value xrx, with r and s different. Therefore 
1 
E(XiXi) :  N(N -- 1) 
1 
-- N(N -- 1)  .rrx. (10) 
r=$ 
The final sum in Eq. (10) is most easily evaluated as a part of the square 
of Zl +z+"'+z,v, because 
(x, +a'=+... +a'.v)  = .r(-x+.., +a'-+.rra', (11) 
where the final sum in Eq. (11) is the same as that in Eq. (10). But, 
from Eq. (3b), 
and therefore the left side of Eq. (11) is 0 , or 0. Hence 
N 
2 
The final result follows from Eq. (3c). 
Substituting from Eq. (12) into Eqs. (10) and (8), we find 
Cov(X,X) = -- N _ 1 
354 THEORY OF SAMPLING [CHAP. 9 
Variance of X d-X2 d-'.: d-X,. It is now an easy matter to find 
the variance of the sum Z: 
Z = X d- X2 d- '" d- X. 
The variances of all the X3's are equal, and the covariances of all pairs 
X3, Xk are also equal. Therefore Eq. (21) of Section 9-4 applies, with 
the following result: 
- 1) 
2 1 
or 
 = n2 N -- n (14) 
N--1 
To go from the variance of the sum Z to that of the sample average 
 = Z/n, we recall that 
(15) 
cz---- C 
Taking c = 1In in Eq. (15) and substituting for $ from Eq. (14), we get 
o'x- ---- T -- ' (16) 
Note that the variance of X is zero when the sample size n is equal to 
the population size N, because then each sample is the whole population 
and there is no variation of the sample average from one sample to another; 
all have sample averages X equal to . When n = 1, we are drawing 
samples of size 1, and X = x, where the ith individual is the one chosen 
in the sample. Naturally, for n = 1, a}- = a2, because a2 is the variance 
of the population of individuals. 
EXAMPLE 2. Calculate X- and a}- for the data of Example 1, using 
formulas (7) and (16). 
Solution. We need /a and cr 2 for the population: 
4 2 22 3 
x82, o. 2x 2_ 4=-. 
: 4--4-- =   4 2 
9--5] SAMPLING FROM A FINITE POPULATION 
We substitute these results into formulas (7) and (16), with N----4, 
n = 2, to get 
X' =  = 2, }__ -- _ 3/2 -- 2 1 
n - 2 -Y 
These values agree with the calculations of Example 1. 
EXAMPLE 3. Among 50 students, a fraction p hold a favorable opinion 
on an issue, and a fraction q = 1 -- p are against it. In random samples 
of size 10, find the mean and variance of the proportion  who hold a 
favorable opinion. 
Solution. Let a student with a favorable opinion have the score 1, a 
student xvith an unfavorable opinion, the score 0. For the population, 
there are 50p members with scores 1, and 50q with scores 0. Therefore, 
for the popn]ation of 50 students, 
(50p  ) + (50q  O) 
 = 50 = p' 
= (50pX 1 )+(50qx0 ) 
50 -- p = p-- p = p(1 -- p) = pq. 
We apply formulas (7) and (16), with N = 50, n = 10, to get 
-- d0 - = W = 
Discussion. Had the samples been drawn xvith replacement, ve knoxv 
from Corollary 9-10, Section 9-3, that we would then have/ = p and 
(2 
2 Pq (18) 
n n 
Compare the denominators 12.25 and n of the rightmost terms of Eqs. (17) 
and (18). In this problem, if we measure accuracy by the size of as, 
a sample of 10 drawn without replacement is vorth a little more than a 
sample of 12 drawn with replacement. 
When n is small compared with N, however, the factor (N -- n)/(N -- 1) 
in Eq. (16) is near 1, and there is little difference between the values of 
2 computed according to Eq. (16) and according to the formula  
o'=/n corresponding to sampling with replacement. 
356 HEOR� OF SAMPLING [CHAP. 9 
EXERCISES FOR SECTION -5 
I. Three similar tags, numbered 1, 2, and 3, are put into a bowl. Two tags 
are drawn without replacement. Set up a sample space of possible samples of 
size n = 2. Compute the probability function for sample averages of the 
numbers on the tags and find its incan and variance. Compare these with the 
mcan and variance of the original measurements. 
2. Measurements associated with four objects are --3, --1, 1, and 3. (a) 
Compute the mean and variance of this set of measurements. (b) Form the 
probability function of averages of samples of size n = 1 drawn without replace4 
ment from this population and give the mean and variance of this distribution. 
3. Same as Exercise 2(b) but for n = 2. 
4. Same as Exercise 2(b) but for n = 3. 
5. Same as Exercise 2(b) but for n = 4. 
6. Compare the means obtained in Exercises 2(a), 2(b), 3, 4, 5. 
7. Coinpare the variances obtained in Exercises 2(a), 2(b), 3, 4, 5. 
In Exercises 8 through 14, the objects in the population are a nickel, dime, 
quarter, half-dollar, and dollar. 
8. Set up a sample space for samples of size n = 2 drawn without replace4 
merit from this population. 
9. Find the probability function of the amount of money contained in the 
samples of Exercise 8. 
10. Find the mean and variance of the probability function of Exercise 9. 
11. Compute the mean and variance of the original set of five amounts. 
12. Use the results of your work in Exercises 8 and 9 to compute the proba- 
bility function of the average amount of money in a sample. Find the mean 
and variance of this distribution. 
13. Compare the means obtained in Exercises 10, 11, and 12. 
14. Compare the variances found in Exercises 10, 11, and 12. 
In Exercises 15 and 16 the population consists of 4 students who are carrying 
the following amounts of money: 40, 80, $1.00, and $1.60. A sample of 2 
students is randomly drawn without replacement. 
15. Find the mean and variance of the amounts of money carried by the 
4 students. 
16. Set up a sample space for sums of money carried by the 2 students in 
the samples. Find the mean and variance of these sums and compare with the 
population mean and variance. Check your answers by comparing them with 
Eqs. (6) and (14). 
17. In a population of 8 seedlings, at the end of three weeks 4 had no true 
leaves, 2 had 2 true leaves, and 2 had 4 true leaves. Compute the mean and 
variance of the random variable: number of true leaves on a seedling drawn 
at random from this population. 
18. (Continuation.) A sample of size n = 3 is drawn without replacement 
from the population of Exercise 17. The average number of true leaves per 
seedling is the new random variable to be considered. Find its mean and variance. 
9-5] SAMPLING FROM A FINITE POPULATION 357 
19. In a lottery, 1000 tickets are sold. There is 1 first prize ticket worth $50, 
there are 4 second prize tickets worth $1 each, and the rest arc worthless. If 
two tickets are drawn from this population, what is their total worth, on the 
average? What is the standard deviation of their total worth? 
20. A school with 1500 students plans to draw a random sample of 100 
students to estimate the average expenditure for school supplies. If the standard 
deviation of the expenditures measured in dolhrs is actually a = 2, what is 
the standard deviation of sample averages? 
21. (Continuation.) Suppose the smplc size in Exorcise 20 is n (not neces- 
sarily 100). If it is desired to make the standard deviation of sample averages 
0.1 or less, how large a sample (approximately) should bc drwn? 
22. Let f = n/N represent the fraction that the sample size n is of the popu- 
lation size N. Then if N is large compared with 1, show that, approximately, 
 = (2In)(1 -- f). 
Use Eq. (16) in the following Exercises 23 through 28: 
2 increases when 2 increases. 
23. If n and N are fixed, show that a2 
24. For fixed N and a =, show that a decreases as n increases. 
25. For fixed a = and n, show that a  increases xxith N (if n > 1), but is 
always less than or equal to 
26. Show that the sample size n required to give a standard deviation for 
sample averages half as big as that for samples of size n is nl = 4nN/(N  3n) 
when this relation is satisfied by integers. (Otherwise the relation is an 
proximation.) 
27. (Continuation.) Find the value of nx for the formula of Exercise 26 
when n = N. Interpret the result. 
28. (Continuation.) In the formula of Exercise 26, show that if n is quite 
small compared with N, then n is approximately 4n. Thus halving the standard 
deviation requires a quadrupling of the sample size, roughly. Comment on the 
relation to the comparable result for sampling with replacement. 
From Exercise 8 of Section 5-4 the mean and standard deviation of the set of 
N numbers 1, 2, ..., N are 
-- '   12 
Use this information and Eq. (16) in Exercises 29 through 31. 
29. If n numbers are drawn without replacement from this set, show that 
+ - n) 
12n 
30. (Continuation.) If, in Exercise 29, N = 1000, how large must n be for 
a2 to be approximately 50 ? 
31. (Continuation.) Regard 3-digit numbers in the table of random numbers 
as the 1000 integers 001,002, ..., 999, and 1000 (let 000 stand for 1000). Draw 
358 THEORY OF SAMPLING [CHAP. 9 
a sample of 32 3-digit numbers ,withott repetitions, obtain their average, aud 
see how many units of � that average is from B = 500.5, the mean of the 
population. 
32. Iu a lot of 1000 steel bars, the standard deviation of diameters is  = 0.002 
inch. A sample of 25 of these bars is measured, and the sample average is com- 
puted as an estimate of the mean diameter B for the lot. Compute the standard 
deviation of the distribution of possible sample averages. Note that the factor 
/(N -- ,/( -- 1) is very close to 1 in this calculation. 
33. For the purpose of estimating mean height, consider drawing a sample of 
100 adnlt males from (a) a fraternal group of 200 adult males, (b) a college of 
1000 adult males, (e) a town with 10,000 adult males, (d) a state with 1,000,000 
adult males. Suppose the standard deviation for each population, measured in 
inches, is a = 3. Compare the standard deviations of the distributions of 
sample averages for the several populations. Explain why sampling experts 
often say with approxinmte truth, "It's not the size of the population that 
matters, it's the size of the sample." 
34. From a population of neasurenents of size N = 100, a sample of size 
n = 10 is drawn, with replacement, and the average of the measurements in 
the sample is computed. How large a sample, from the same population, drawn 
without replacement, would provide the same accuracy, as measured by a}.? 
35. From a population of size N = 500, a sample of size n is to be drawn, 
without replacement. With each element of the population there is associated 
a value of a random variable X, and X is the average of X's in a sample. It is 
desired to use the observed value of X to estimate the population mean/, with 
95% confidence that the error is no larger than 0.1. From past experience with 
similar populations, the expellmeuter is willing to assume that X is normally 
distributed, and that the population standard deviation a is equal to 3. What 
percent of the population should he take into his sample? What would your 
answer have been for a sample drawn wit], replacement? 
36. From a populatiou of size N = 40, a sample is to be drawn, and the 
sample average  is to be used to estimate the population mean /. Which 
provides a more accurate estimate, a sample of 8 drawn without replacement 
or a sample of 10 drawn with replacement? Why? 
37. A population consists of /1I l's and (N -- .1I) O's. A random sample of 
size n is drawn, without replacement, and X is the number of l's in the sample, 
-- 2 
while .� = X/n. Find/.? and . in terms of n, 3I, and V. 
38. In Exercise 37, if 3! = iN, while N is large and n is of moderate size, 
how large a sample , is needed to be 75,% sure that the number of l's in the 
sample is between 0.4n and 0.6n (a) using Chebyshev's Theorem, and (b) 
assuming that X is normally distributed? 
For Exercises 39 through 42 use the following information. A bridge deck has 
4 suits of 13 eards each, including jack, queen, king, ace, which are sometimes 
scored 1, 2, 3, and -t respectively for bidding purposes. The rest of the eards 
have zero point count. 
39. Compute/ and  for the probability distribution of point counts per 
card in a bridge deck. 
9--5] SAMPLING FROM A FINITE POPULATION 359 
40. A bridge hand consists of 13 cards dealt without replacement from the 
deck of 52 described above. What is the men and standard deviation of the 
probability distribution of the total point count arising from the sample space 
of all bridge hands? 
41. Each member of a large bridge club plays 300 hands per season in a certain 
competition. What is the mean and standard deviation of the sum of the point 
counts for the probability distribution of the stuns yielded by samples of 300 
hands (note that single hands are dealt without replacement of eards, but that 
successive hands are dealt with replacement of hands). 
42. Use the results of Eereise 41 and the normal approximation to suggest 
that at the end of the season about � of the club members are not within 72 
points of the population average. Assuming that half these people score above 
and half below average, this means that in points about  of the members have 
in point count the equivalent of at least an extra 18 aces or 72 lacks above 
average. Another  is similarly below average. If the club is very large, use the 
normal approximation to show that about 5% of the members are not within 
143 points of the mean. Thus 2.5% of the members are lucky enough to have 
the equivalent in point count of an etra ]aek or more above average on about 
half the hands. 
43. Let m be the size of a sample drawn with replacement and n the size of 
a sample drawn without replacement from the same population, of size N. 
Find a formula for m in terms of n and N so that the variance of averages for 
the two kinds of samples are equal. 
CHAPTER ' 
111 
LEAST SQUARES, 
CURVE-FITTING, 
AND REGRESSION 
10-1. CURVE-FITTING 
Vherever data are processed, the occasion arises for fitting lines, curves, 
or surfaces to represent relations betveen two or more variables. The 
astronomer predicts the path of a satellite; the businessman projects a 
trend; the biologist relates the size of a growing population to time; the 
educator compares students' college grades vith their high-school grades; 
the meteorologist forecasts the path of a storm; and the doctor compares 
the rate of relief from pain with the size of the analgesic dose. All such 
tasks involve the fitting of curves. For some problems the vork is easy 
and can be done by hand; for others the calculations are horrendous and 
would not even be considered without modern high-speed computers. 
X'evertheless, the central ideas behind work m curve-fitting are relatively 
simple, and we present them in this chapter. 
EXAMPLE. Forgetting. To discover how fast people forget, the psy- 
chologist Strong slowly read lists of 20 vords to 5 people and asked them 
later to recognize the vords vhen mixed vith 20 other vords. As soon 
as the llst vas read, the subjects vere kept busy at other intellectual 
tasks, so that they could not rehearse. Strong, the investigator, imposed 
penalties for false recognition, and computed the score for retention, R, 
on the following basis: 
number correct -- number falsely recognized 
R ---- 100 X 20 
Thus a subject with 14 words correct and 2 falsely recognized had a score 
of 60%. 
360 
10-1] CUIVIg-FITTING 361 
The time interval before the subject was asked to recognize a list of 
words was varied from one minute to one week. The data are given in 
Table 10-1, where T is the number of ninutes between the reading of the 
list and the testing, and t ----- log T. 
TABLE 10--1 
DATA FOR STRONG'S EXPEI>I3,IENT O.N' RETEN'TION'. 
T, t, R, 
number of logarithm average 
minutes till of tilnc in retention score 
testing minutes in percent 
1 0 84 
5 0.7 71 
15 1.2 61 
30 1.5 56 
60 (1 hr) 1.$ 54 
120 (2 hr) 2.1 47 
240 (4 hr) 2.4 45 
480 (8 hr) 2.7 38 
720 (12 hr) 2.9 36 
1,440 (1 day) 3.2 26 
2,880 (2 days) 3.5 20 
5,760 (4 days) 3.8 16 
10,080 (7 days) 4.0 8 
In Fig. 10-1, the average R for the five subjects is plotted against T. 
The data are quite curvilinear when plotted this way. The dashed lines 
neerely lead the eye from one point to the next. 
An alternative plot of these data is given in Fig. 10-2, where the vertical 
axis is the average retention score for the five subjects and the horizontal 
scale is t, the logarithm of 7'. 
Strong, who plotted his data on ordizary graph paper, as in Fig. 10-1, 
noted that, initially, retention scores went down quickly as time increased 
between reading and testing, but that at about the 8-hour interval scores 
began to decay more slowly. This suggested that the data be plotted on 
logarithmic paper (R against t), as ill Fig. 10-2. You will observe that 
while these data fluctuate slightly, they seem to lie quite near a straight 
362 LJ.AST SQUARES, CURVE-FITTING AND REGRESSION [CHAP. 10 
oo 
90 
1 
80 
1 
1 
I 
70 
� 
 4o 
- , 
10 
0 i 2 3 4 5 6 7 8 9 10 
FIG. 10-1. Strong's data for retention R plotted against time in thousands 
of minutes. 
line, which meaus that retention is approximately linearly related to t, or 
R =mt + b, 
where b and n are constants. How do we find b and m to determine the 
relation between R and t, and thus reduce the data to a simple formula? 
Black-thread method. An old and simple method of fitting a straight 
line is to stretch a black thread across a graph, and rearrange its position 
until one's eye is satisfied that the thread fits the points well. Then the 
coordinates are read at convelfient points at the left and right edges of 
the figure. In onr figure, it is convenient to read the intercepts at t = 0 
and t: 4, and then to work out an equation for the line as follows. 
The slope-intercept formula for the equation of a straight line is 
y =mx + b, 
10-1] CURVE-FITTIX 363 
100 
$0 
[ 
o 
1 2 3 4 
Log (nuinber of mnutes) 
Fro. 10-2. Strong's data on retention. 
where b is the y-intercept and m is the slope of the line: 
rise Y2 -- Y 
m -- -- , 
run x2 -- Xl 
where (Xl, Yl) and (x, Y2) are the coordinates of two points on the line. 
In our problem, R corresponds to y, andttox. We sett---- 0andget 
the R-intercept, R0. Let the R-reading at t ---- 4 be R4. Then the slope 
R4- R0 R4- R0 
m -- -- , 
4--0 4 
and b = Ro. Thus, since our variables are t and R, the fitted equation is 
R= (R4-- R�.)t + Ro . (1) 
4 
The specific details are left as an exercise. 
364: LEAST SQUARES, CURVE-FITTING, AND REGRESSION [CHAP. 10 
For many purposes, the black-thread method is adequate, and its 
simplicity recommends it. Why search further? There are a number of 
Yeasons  
(1) When data are as clean and beautiful as Strong's, there is little 
problem about the fitting, but vhen we have a svarm of points with 
considerable variation (see Fig. 10-8), our eye is not a sure guide in 
placing the line. 
(2) Scientists prize repeatable objective methods, because they fear 
that unconscious bias on the part of an observer may distort the results. 
(3) It is desirable to have a general method of curve-fitting that can 
be extended to new and more complicated situations. For example, we 
may vish to fit a quadratic y ---- ax 2 -]- bx -[- c, and the thread method 
is not satisfactory for this vork. Or ve may vish to introduce more 
variables, e.g., how does height of seedlings depend on amount of water 
and amount of fertilizer? 
Once we see the need for more general methods, our task is to find 
some and to make a choice among them 
Why fit curves? Before proceeding vith the search for general methods, 
let us consider further why vc want to fit lines and curves. Here are some 
reasons; 
(1) We want a concise way of expressing the relation between variables. 
(2) Ve may wish to use the relation for forecasting results. 
(3) We may wish to assess the reliability of predictions by looking at 
the scatter of observed points about the fitted curve. 
(4) We may have a theory that ve vish to test. For example, Strong 
may have felt from earlier work that retention was a linear function of 
log T, and his results support that hypothesis. 
(5) An established theory may suggest the form of a curve to be fitted; 
then, fitting the curve to data assesses the constants. For example, if s 
is distance in feet, t is time in seconds, and g is the gravitational constant, 
then a body dropped from rest in a vacuum near the surface of the earth 
falls according to the law s = �gt 2. We can drop a body and measure 
the time t required to traverse a distance s. These data can be used to 
estimate g. 
In this chapter, we especially treat the fitting of straight lines, and this 
needs some justification; after all, many relations are curvilinear. Some 
reasons for studying the fitting of straight lines are: 
(a) Generality of method. The technique we develop for fitting straight 
lines extends to other 1nethods, so the foundation for generalization is laid. 
(b) Actual linearity. Many relations are linear or nearly so. 
(c) Transformations. Even when a relation is not linear, often an easy 
transformation of one or both of the variables produces a nearly linear 
10-1 ] CURVE-FITTING 365 
relation. For example, Strong used the logarithm of time rather than 
time itself, a common transformation. 
(d) Limited range. Often researchers study a limited range of a variable. 
The functional relation over the limited range may be nearly linear even 
though the total functional relation is far from linear. Examples: (a) The 
average height of males in the United States is obviously nonlinear xvhen 
plotted against age. Yet average heights from age 12 to age 14 are nearly 
linear xvhen plotted against age. (b) On the circumference of a circle 
2 inches in radius, look at a segment about � inch long; it is nearly a 
straight line. Yet what curve have you sudied that is less linear than 
a circle? 
(e) Convenience i reporting. Even when a function is not a straight-line 
relation, a researcher may find it convenient to report the linear approxi- 
mation, for ease in communication. 
In the next sections ve provide a general method for fitting straight 
lines. 
EXERCISES FOR SECTION 10-1 
1. Apply the black-thread method to obtain an equation for the retention 
data of Fig. 10-2. 
2. (a) Use the equation fitted in Exercise 1 to find the value of t, and then T, 
when R = 0. (b) The work in part (a) is an extrapolation from the data; why 
must it be regarded with caution, or even distrust? 
3. Use the black-thread method to find an equation for a line fitting the 
points (0, 0), (1, 0), (2, 1), (3, 1), (4, 2), (5, 2). 
4. Use the black-thread method to find an equation for a line fitting the 
points (--2, 3), (--1, 2), (0, 1), (0, 0), (1, --1), (3, --3). 
5. Plot the 10 points with coordinates (--1,--1), (0,--2), (1, 0), (1, 2), 
(2, 2), (3,--3), (3, 1), (4, 3), (5, 0), (5, 1) on ordinary graph paper and use 
the black-thread method to find an equation of the fitted straight line. 
6. Plot the points (0, 0), (0, 1), (1, 0), (1, 1) on ordinary graph paper and 
use the black-thread method to find an equation of a fitted straight line. 
7. (Continuation.) Is there a line perpendicular to the one you fitted in 
Exercise 6 that fits the data equally well? If so, find its equation. 
8. Study the four points of Exercise 6 and give a list of four equations that 
a reasonable person might suggest for close-fitting lines. 
9. (a) Suggest an objective criterion for choosing unambiguously among 
straight lines that can be fitted to a set of data. (b) Apply your criterion to the 
fitting of the four points of Exercise 6. (c) Apply your eftteflon to the fitting 
of the three points (0, 0), (1, 1), (2, 0). 
10. An apparatus measures falling time, t, to the hundredth part of a second 
when an object is dropped a distance s, in feet. The following data are gathered: 
366 LEAST SQUARES, CURVE-FITTING, AND REGRESSION [CHAP. 10 
s 4 3 2 1 
t 0.50 0.43 0.35 0.25 
Find g by fitting a straight line through the origin with the black-threal method. 
(To obtain linearity, plot T -- t 2 as the horizontal axis.) 
10-2. THE METHOD OF LEAST SQUARES 
Vhen we use a black thread to fit a straight line, we try to make sure 
the thread is close to the points. Mentally, we compromise, making some 
points farther from and some closer to the line. To measure the closeness 
of a point to a fitted line, we use the ,,erlical deviation, since we are trying 
to predict y from x. Strong didn't want to predict t from retention, he 
xvanted to predict retention from t. 
If xve have fitted the line y ---- x - 1 to a set of points, then the y- 
coordinate of the point _P on the line with x ---- 3 is y ---- 3-]- 1 ---- 4. 
Therefore, if the point Q with coordinates (3, 5) belongs to the fitted set, 
its vertical deviation from the line is PQ: 5 - 4:  1. More generally, 
the vertical deviation d of a point with coordinates (x, y) f'om the line 
y ---- mx+ b is 
EXAMPLE 1. Figure 10-3 shows three points (its adjoining table gives 
coordinates) that have been fitted by eye with the equation 
y = 1. (2) 
Since we plan to use the line to predict y from x, find the vertical devi- 
ations from the line to the three given points. 
ol, tion. Upon substitution in Eq. (2), the fitted values of y for x ---- l, 
2, 3 are found to be y ---- �, 2, 3�, respectively. We summarize the work 
in Table 10-2. 
The deviations seem rather small, and the visual fit satisfactory. :But 
xvithout a criterion, xve can scarcely discuss the possibility of finding a 
better-fitting line. 
What sorts of criteria might we use? We could ask that the sum of the 
deviations be zero. :But there are many lines that have this property, 
including ill-fitting ones such as the horizontal line with equation y ---- x-. 
We could ask that the sum of the absolute vertical deviations be minimized. 
But the intractibility that ruled out absolute deviations as a measure of 
variability in Chapter 5 is even more of a roadblock in fitting lines. The 
program is extremely laborious. 
10-2] THE METHOD OF LEftST SQUARES 367 
// 
I /// I/ , 
q = ]a -1 
/ 
2 / 
/ / 
/ / 
/ 
/ / 1 1 
/ / 1 - 
/ / 
/ / ' 2 
/ / 3 4 
/ ! 3 ,c 
i 2 4 
/ 
/ 
/ 
/ 
/ 
/ 
-1 
FIO. 10-3. Line y = x -- 1 fitted by eye to 3 points. 
TABLE 10--2. DEVIATIONS FOR FIG. 10--3. 
Deviations for y 
x Observed y's Fitted y's (observed -- fitted) 
I i I 1 
1 1   -  
1 
2 1� 2 1�- 2 = 2 
1 
3 4 3� 4 - 3� = 
The next suggestion usually is to mznzmzze the sum of the squared vertzcal 
deviations. It turns out that this method is tractable, generalizes easily, 
and is not difficult to carry out. It is part of the general method of least 
squares. We shall examine this method in connection with Example 1. 
In looking at Fig. 10-3, we intuitively feel that the slope of the line is 
right, but possibly the y-intercept could be improved. 
368 LEAST SQUARES, CURVE-FITTING, AND REGRESSION [CHAP. 10 
EXAMrt,E i (continued). Let ,us retain the slope of the line of Fig. 10-3 
and, by numerical trial and error, test various values of the y-intercept, 
to find the y-intercept that minimizes the sum of squares of vertical 
deviations (ve abbreviate "vertical deviations" to "deviations"). 
Solution. We surely do not want all points on one side of the line, so 
1 
ve restrict attention to values of the y-intercept between --1� and 2 
(see the dashed lines of Fig. 10-3). It seems reasonable, too, that if an 
adjustment is made the new line should move toxvard the two points 
above the present line. Therefore let us try as y-intercepts the values 
--.5, --.6, --.7, --.$, --9, --1.0 (our present value). In this range, 
the amount we add to our present y-intercept, --1, can be subtracted 
from the deviations for the two endpoints, and added to the deviation 
for the middle point. Results are given in Table 10-3. 
TABLE 10--3. SUMS OF SQUARES OF VERTICAL DEVIATIONS. 
--1.0 --.9 --.6 --.5 
b dev dev 2 dev dev 2 --.8 --.7 dev dev = dev dev = 
.... dev dev 2 dev dv I .... 
.5 .25 .4 .16 .3 .09 .2 .04 I .1 .01 00 00 
--.5 .25 --.6 .36 --.7 .49 --.8 .64 --.9 .81 --1.0 1.0 
.5 .25 .4 .16 .3 I I .09 .2 I .04  .1 ' .01 
Sum .75 i, .68 .67 .72 .83 1.0 
To see xvhat value of b approximately minimizes the sum of squared 
deviations, in Fig. 10-4 xve plot values of these sums from Table 10-3. 
The graph of Fig. 10-4 suggests a parabola. Both Fig. 10-4 and Table 
10-3 suggest that the minimizing value of b is about --0.8. 
This outcome satisfactorily resolves the present problem. But we want 
to find a general formula, to speed future work. To this end, we reviexv 
some facts about the quadratic function. 
Minimizing a quadratic function. The general quadratic function of x, 
y---- Ax -]-Bx-]-C, A > O, 
has a minimum value. To find the ninimum value of y, we complete 
the square as follows: 
B + C 
y = A x-{-.x  4A 
10-2] THE METHOD OF LEAST SQUARES {(;9 
I 0 - -- -- 
0.7  [  -- 
06 -- -- - 
--1.0 --0.9 --0S --07 --06 --05 
FG. 10-4. Plot of sums of squared derrations for various intercept value% 
from data of Table 10-3. 
or 
y = A z+,3-  4A 
The only term on the right that can be adjusted by a choice of x is 
A[x 4- B/(2A)] 2. To minimize y we want that term to be small, and 
zero is the least we can make it, because both A and Ix 4- B/(2A)]  ale 
non-negative. Therefore we have 
B 
mirfimizing value of x: z = -- 2A (3) 
Thus, if x = --B/(2A), the quadratic is minimized, and xve have 
8 2 
minimum value of y: y = C -- --. 
4A (4) 
For future use let us note that 
B = coefficient of first-degree term in the quadratic, 
A = coefficient of second-degree term in the quadratic. 
370 LEAST SQUARES, CURVE-FITTING, AND REGRESSION [CHAP. 10 
We mention this now to avoid confusion later when other letters are used 
as coefficients. 
We have proved what we shall call the fundamental lemma for the 
method of least squares. 
10-1 Lemma. Fundamental lemma for least squares. If the coefficient 
of the second-degree term of a quadratic function is positive, 
then the function has a minimum when the variable is set equal to 
B coefficient of lst-degree term 
_ 
2A 2 (coefficient of 2nd-degree term) 
and the minimum value of the quadratic function is 
B 2 (coefficient of lst-degree term) 2 
C -- constant term -- 
4A 4(coefficient of 2nd-degree term) 
(6) 
EXAMPLE 2. Find the value of x that minimizes 
y - 3x 2 -- 2x+ 1, 
and find the minimum value of the function. 
Solutions. Formulas (5) and (6) give 
B 2 1 
2A 6 3 
for the minimizing value of x, and 
y---- C B" _ 1 (--2)2 2 
4A 4(3) 3 
for the minimum value of y. 
Notation. In fitting least-squares lines, it helps to have standard symbols 
for the values of the slope rn and the intercept b that yield the minimum 
sum of squared deviations, and for the minimum value of the sum of 
squared deviations, S. We call these the least-squares values and use 
h, , and  (read m-hat, b-hat, and S-hat) to distinguish these least- 
squares values from the general values m, b, and S. 
10-2] THE METHOD OF LEAST SQUARES 371 
EXAMPLE 3. Three-point example. In the example of Fig. 10-3, choose 
the intercept b in y = b -]- 3x/2 so as to minimize the sum of squares 
of the deviations. In other words, find 
Solution. Let us set up the table of observed and predicted values, using 
b as an undetermined constant. 
TABLE 10-4. DEVIATIONS FOR THE THREE-POINT EXAMPLE. 
Observed Fitted 
Deviation (Deviation) 2 
x y y 
1 1 b+ 2 
2 � b+s   (--)  = +s+  
2 
Sum = t5bab  
To find , we need to minimize the quadratic function of b obtained 
as the sum in the last column of Table 10-4' - + 5b + 3b s. Applyiug 
formula (5) to this quadratic, xve find 
5 _  _ 5  -0.sg. 
s(g) 
We call  "the least-squares value of b";  is close to --0.8, as xve deduced 
from Table 10-3. The value of the sum of squares of deviations at this 
minimum is, from (6), 
 _ 11 25 2 0.667, 
4 4() 
where we carry three decimal places to show that the true nfinimum is 
only slightly less than the value 0.67 obtained in Table 10-3 for b = --0.S. 
Our method also works for finding a number b that minimizes 
n 
= - 
xvhere the y's are numbers. 
It turns out that the least-squares value of b is  = , the average of 
the measurements. Consequently, xvhen xve compute the sum of squares 
in the numerator of a sample variance 
8y  
n 
372 LEAST SQUARES CURVE-FITTING AND REGRESSION [CHAP. 10 
we are computing a minimum sum of squares. We develop this fact as 
a theorem through the following examples. 
EXAMrLE 4. Find the number b that minimizes the sum of the squares 
of the deviations of the numbers 4, --1, 0 from b. 
Solution. Let S be the sum of squares (sum of squared deviations): 
S = (4 -- b)2q- (--1 -- b)2q- (0-- b) ---- 17-- 6bq-3b . 
Applying (5) to this quadratic function of b, we get for  
2(3) 
Observe that 1 is the average of the three numbers 4, --1, 0. 
EXAMPLE 5. Find  for the three values y, Y2, Y3 of y. 
Solution. For an arbitrary value of b, let the sum of squares be S as 
before: 
3 
 =  (y- ) _- (y - ) + (y2 - ) + (y - ). 
When we expand the squares and collect like powers of b, we get 
S ---- y q- ys s q- y] -- 2(y q- Ys q- y3)b q- 3b a. 
From formula (5), we find 
2(3) 3 
From forlnula (6), the minimum x'alue of S is 
S = y + y + y - 4(v + y + y) 
4(3) 
= Z y - 3 . 
Note that if ' were divided by 3 we would have the variance of the 
y-measnreme nt s: 
3 3 ' 
10-2] THE METHOD OF LEAST SQUARES 373 
Thus  is just 3sv . Actually, the moment we find out that 5 ---- , substi- 
tution in the original equation gives 
the numerator of the definitional form for the variance. 
The methods of Example 5 can be generalized at once to give a theoreln: 
10-2 Theorem. Minimum sm of squares. Let y, Y2,..., Y,, be 
measurements, let b be a number, and let 
= (y -- b) 2 q_ (Y2 -- b) 2 q_ '" q_ (Y -- b)2. 
Then 5, the value of b that minimizes S, i 
(7) 
and the minimum value of S is 
 = .,, (8) 
where s is the variance of the y's. 
10-3 Corollary. Sum of deviations. Given the numbers y, y2, � �., 
and , the least-squares value of b, then 
n 
 (, - ) = o. 
Proof. From the theorem, 5 = Y. Furthermore, 
 (y- ) y;-   y- ny = y- n(-)----- O. [] 
=l 
374 LEAST SQUARES, CURVE-FITTING, AND REGRESSION [CHAP. 10 
EXm'LE 6. Find 5, the value of b that minimizes the squared devia- 
tions from 10, --5, 6, 4, 0. 
Sohdion. 5  --   -- 3. 
 5 -- 
We now have all the equipment that we need for fitting straight lines. 
In the next sections we complete the task in two steps. First we learn 
to fit lines through the origin, and then to fit a general line. 
EXERCISES FOR SECTION 10-2 
1. For the data of Fig. 10-3, find the deviations from the line y =  men- 
rioned in the text. Show that their sum is zero. Compare them ith the dewa- 
rions of Table 10-3. 
2. Find the value of b that minimizes the sum of the squared deviations from 
the numbers 8, 6, 9, 3, 5, and evaluate the minimum sum of squares, . 
3. Check by direct calculation that the sum of the deviations from the mean 
in Exercise 2 is zero. 
4. Find the value of b that minimizes the sum of squares for the numbers 
14, --10, --20, 0, and evaluate the minimum sum of squares, . 
5. Check by direct ealculation that the sum of the deviations from the mean 
in Exercise 4 is zero. 
6. If S is the sum of sc[uares, find the least-squares value of b for the numbers 
--9, --1, 6, 14, 13, --5, and find . 
7. If S is the sum of squares, find the least-squares value of b for the numbers 
9, 2, 0, and find . 
8. For the numbers 4, 3, 2, compute the sum of squares of tleir deviations 
from b = 4, 3.5, 3, 2.5, 2, and graph the results, rodmating  and  on the graph. 
9. Find the value of x that minimizes the quadratic y = 3x 2 q- 2x q- 1, 
and find the mmimunI value of y. 
10. Find the value of x that minimizes the quadratic y = x 2 -- 2x q- 1, 
and find the minimum value of y. 
1l. Why can't we find the minimum value of the quadratic 
y = --2x 2q-x-- 17 
12. Find the value of x that minimizes the quadratic in x: a q- ax q- a2x 2, 
xhere a s a constant, and find the value of the quadratic for that value of x. 
13. For the quadratic in b: x a q- x2b q- xb 2, x > O, find the minimizing 
value of b, and the value of the function for that b. Cheek the latter value by 
substituting the minimizing b into the quadratic. 
14. (a) Write out the steps of the proof of Theorem 10-2 on minimum sum 
of squares for two nmnbers, y and y2. (b) Write out the steps for n = 4. 
15. Write out the steps of the proof of Theorem 10-2 on minimum sum of 
squares for a general wdue of n. 
16. Show that (y -- c) 2 = (y -- 7j) 2 q- n( -- c) 2, where c is a eon- 
stant and n is the number of y-nIcasurcments. From this result, deduce that 
=F. 
10-3] FITTING A LINE TItROUGH THE ORIGIN 375 
10--3. FITTING A LINE THROUGH THE ORIGIN 
Before begilming the more difficult task of fitting a general line, let us 
organize the method by doing the easier problem of fitting a line through 
the origin. 
EXAMPLE 1. Temperat,res below the earth's surface. Melior reports 
temperatures taken at various depths in an artesian well at Grenoble, 
France. The following table, derived from his data, uses a point about 
28 meters below the earth's surface as an origin for both temperature and 
depth. The number of meters below the origin is x, and y is the number 
of centigrade degrees above the temperature at the origin. 
Number of meters beloxx origin, x 40 150 220 270 
Number of degrees above 
the telnperature at the origin, y 1.2 4.7 9.3 10.5 
These data are plotted in Fig. 10-5. Use the method of least squares (a) to 
fit a straight line through the origin, (b) to find from its slope the rate 
of increase in temperature per additional 100 meters of depth. 
12 
 y= 03 
/, 
 I 
0 50 100 150 200 250 300 
Increase h dearth, . 
FzG. 10-5. Temperature changes with increased depth in an artesian well 
at Grenoble, Francc. 
,S'ol,tio,. An equation of a line through the origin is 
 = tax, (1) 
7(J LEAS'I SQUARES CURVE-FITTING AND nEGRESSION [CHAP. 10 
where m is the slope of the liuc Let the pairs of measurements be desig- 
nated (.r, yi), i = 1, 2, 3, 4. When we find the appropriate value of m, 
the estimate of y from (1) is mx. The sum of squares of deviations then is 
4 
= ( - ..)* + ( - m-)  + (y - m)* + (y - m)*. () 
Do not lose sight of the fct that the x,'s nd the y's re known numbers 
in the tblc bovc. It is m that is to be determined so as to minimize S. 
When we expnd the four squres on the right-bmod side of Eq. (2) 
md collect terms in like powers of m, we get 
-- 2(.ry  xy  xsys  x4y4)m 
+ ( +  + :- + )m . 
Note that S is a quad:atic in m. We now apply the fundamental lemma 
of least squares, Lemma 10-1, to get the minimizing value of m: 
B --2(xy + x2y2 + x3Y3 + XqY4) 
?h -- 
XlYl + x2Y2 + x3Y3  x4Y4 
 + . +  + . 
0r, written more compactly, 
. (4) 
2 
Now we are ready to clcultc r nd .vy, md so complete the 
numerical problem. Tble 10-5 shows the details. 
We substitute the numerical results of the bottom lie of Tblc 10-5 
ito Eq. (4) to get h' 
h-- 5,634 - 0.03875. 
145,400 
10-3] FITTING A LINE THROLTGH THE ORIGIN 377 
Therefole OUF fitted line is 
y - 0.039x. 
Since the slope ?h of our fitted line gives the change ill y per unit change 
in x, we conclude that the temperature increases about 0.039�C per meter, 
or by about 3.9�C per hundred meters. 
TA 10-5. CCONS o  S-w XMr. 
2 
X y X xy 
40 1 2 1,600 48 
150 4.7 22,500 705 
220 9.3 48,400 2046 
270 10 5 72,900 2885 
Xx = 145,400 
The Mgebmic method of the rtesin-well example generalizes t once 
to  points, nd we hve the theorem: 
14 heorem. Fitting y = m. Given the measurement pMrs 
i = 1,..., , then the lest-squres line through he origin, 
whose equation is 
y  tax, 
hs 
_ xy 
Xd ' i = n. 
EXAMPLE 2. Given the (x, y) pairs in the first two lines of the following 
table, fit the least-squares line through the origin. 
x 1 1 2 2 3 4 4 
y I 2 I 3 5 3 4 
x 2 1 1 4 4 9 16 16 51 = x 
xy I I 2 2 I 6 [ 15 12 16 54 = x,y 
378 LEAST SQUARES CURVE-FITTING AND REGRESSION [CHAP. lO 
Solution. The required squares and cross products are shown in the 
bottom two lines of the table of data. Dropping subscripts, we have 
h -- xy _ 54  1.06. 
-.z 2 51 ' 
Therefore the fitted line is y ---- 1.06x. 
EXERCISES FOR SECTION 10-3 
1. Five points have x,y, = 4, -x 2 = 10. Find the equation of the least- 
squares line through the origin. 
2. For the four points (0, 0), (0, 1), (1, 0), (1, 1), find the equation of the 
least-squares line through the origin. 
3. Find an equation of the least-squares line through the origin for the fol- 
lowing measurement pairs: 
x: 1 1 2 2 3 4 4 
y: 1 2 2 3 1 3 4 
4. Find an equation of the least-squares line through the ongn for the fol- 
lowing measurement pairs: 
x: 0 1 2 3 4 
y: 0 1 4 9 16 
5. (Continuation.) Note in Exercise 4 that y = x 2, for the given pairs. 
Substitute x = 0, 1, 2, 3, 4 into your fitted equation and compare the results 
with the truc values of y. 
6. (y = sinx.) For xin degrees, y = sinxis given to two decimals in the 
table 
x: 0 10 20 30 
y: 0 .17 .34 .50 
Fit the least-squares line through the origin, and use the resulting equation to 
estimate sm 5 � and sin 25 �, checking the values against trigonometric tables. 
7. (y = tanx.) For x in degrees, y = tanxis given to two decimals in the 
table 
x: 0 10 20 30 
y: 0 .18 .36 .58 
Fit the least-squares line through the origin, and use the resulting equation to 
estimate tan 5 � and tan 25 � , checking the values against trigonometric tables. 
$. (y = log (1 - x).) Three decimal values of y = log (1 -- x) are given 
for small values of x in the table (logarithms are to the base e) 
x: --.2 --.1 0 .1 .2 
y: --.223 --.105 0 .095 .182 
Fit the least-squares line through the origin, and then compare the fitted values 
of y with the true values for x = 0.2 and --0.2. 
10-4] FITTING A GENERAL LINE 379 
te?nark. Exercises 5, 6, 7, and 8 illustrate the textual point (d) of Section 10-1 
--that over limited ranges many functions are nearly linear. 
9. Pylhagorean appro:cmalon. Consider right triangles with sides , 1, c, 
where a _ 1 _ c. We want to develop the approximation c  m(a -, 1). 
The Pythagorean theorem yields the one-decimal place table for selected values 
ofq- 1' 
a- 1: 1 1.2 1.4 1.6 1.S 2.0 
c: I 1.1 1.2 1.3 1.3 1.4 
Find the least-squares value of m. It can be used as the multiplier of the sum of 
the two short sides of a right triangle to estimate roughly the hypotenuse. 
10. For the points with coordinates (5, 2), (5, 4), (5, 6), find the least-squares 
line through the origin. 
11. (Continuation.) Invent and prove a theorem about the least-squares line 
through the origin for the points with coordinates (x, y), (x, y), (x, ya). 
12. (Continuatiou.) Extend the theorem of Exercise 11 from 3 to n points. 
13. Consider the points with coordinates (2, 4), (3, 6), (5, 10). Find the 
least-squares line through the origin. 
14. Prove that the least-squares line through the origin for the points with 
coordinates (x, kx), i = 1, 2, ..., n, has slope k. 
10-4. FITTING A GENERAL LINE 
Instead of fitting a general line in the form 
y ---- b +mx, 
we use the form 
y = a - m(x -- .-g), (1) 
where  is the average of the x scores. The use of the deviations from 
the average, x -- , instead of x leads to a simplification in algebra. 
When we have fitted Eq. (1), we have also fitted y---- b -mx with 
b---- a-- m. As usual, m is the slope of the line. And a is the y- 
coordinate of a point on the line whose x-coordinate is x ---- . 
EXMrLS 1. Given the points (1, 1), (2, 1�), (3, 4) (see Example 1, 
Section 10-2, and Fig. 10-3), fit the general least-squares line. 
Solution. It is convenient to denote the points by (Xl, y), (x, y), 
(xs, y) until the fitting is completed. For form (1) and given values of 
a and m, the vertical deviation d of the tth point from the line is 
d ---- y -- [a - m(x -- )], i ---- 1, 2, 3, 
or 
d---- (y-- a) -- m(x-- ), i= 1,2,3. 
380 LEAST SQUARES, CURVE-FITTING AND REGRESSION [CHAP. 10 
Then � 
s= 
By choosing a and m, we wish to minimize S. We have 
d[----[(y- a) -- m(x- )]2. 
We expand the square on the right-hand side to get 
d ---- (y -- a) 2 -- 2(y -- a)(x -- )m q- (x -- )2m 2. (2) 
Next, we break the middle term on the right side of Eq. (2) into two 
terms: 
--2y(x -- )m + 2a(x, -- )m. 
When these parts are summed, the second vanishes because  (x -- )  0. 
Thus, summing Eq. (2), we get 
S:  d[ : (y -- a)  -- 2y(x -- )m + [(x -- )2]m 2. (3) 
Note that a affects only the first sum on the right: 
(y- a) 2. (3') 
This sum can be minimized by applying the minimum-sum theorem 10-2 
to make 
a = y.. (4) 
The last two terms on the right-hand side of Eq. (3) form a quadratic 
in m: 
-2Ey,(.r, - )m + [E( - )]m . (") 
To minimize this quadratic we apply the fundamental lemma, and get 
B y(- - ) 
2A (x -- )s 
The numerator in the rightmost member of the foregoing equation can 
be rewritten for purposes of calculation as 
]0--4] FITTING A GENERAL LINE 381 
Therefore h = x,yl -- 3'. 
W.(., - ) (5) 
Finally, the minimum value of S is the minimum of (3') plus the mini- 
mum of (3"). Therefore 
 = E(Y-- F)+C-- 4' 
and C = 0. Therefore 
Replacing the numerator of the rightmost term yields 
[-.xiy- 3y]  
=W.(yi-)- W.(x-) ' (6) 
We now replace the x's and y's with their given numerical values, and 
get 
+2+3 
:= 3 --2, 
1++4 is 
F-- 3 6' 
E(x-)=(-s)+(s-s)+(s-s)=s, 
xy= 1X 1+2X +3X 4 = 16, 
3y = 13, 
E(y-)=Ey-S= ++-s(')= 
Finally, we have 
h = xiyi- 32 16 -- 13 _  
Therefore our fitted line is 
_  + ( - s) 
 6 
6' 
382 LEAST SQUARES, CURVE-FITTING, AND REGRESSION [CHAP. 10 
Let us compare these constants with the corresponding values from our 
visuat fit in Section 10-2. The slope is , as we guessed in Section 10-2. 
To check the intercept, we compute d -- ?h. 13 3 ---- --- which 
-- 6 6, 
also agrees with the minimizing value we obtained by least squares in 
Section 10-2, when we assumed m ---- -. 
We substitute numbers into Eq. (6), to get the minimum value of S: 
 = 53- (26- 3)  _ 2 
2 3 
as in Section 10-2. 
The average squared deviation is 
n--3-- 
This result is like a sample variance; it is the variance of the deviation 
scores d. Recall that /n ---- d/n. If �di ---- O, /n is the variance 
of the d's. Later, we prove that when d and ?h are used in fitting the 
line, d  0. 
The derivation for Example 1 is quite general, except that the number 
of measurements was 3 instead of n. If we extend the derivation to n, 
we have the following. theorem. 
10-5 Theorem. The general least-squares line. Given the set of paired 
measurements (xl, y), i---- 1, 2,...,n, then the least-squares 
line 
y---- d-]-?h(x-- ) 
is obtained when 
E(zl- ) ' (7) 
where  (xi -- .)2  O; and the minimum sum of squared devi- 
ations is 
The line of best fit can be written as 
Y - Y = '(. - :). (9) 
10-4] FITTING A GENERAL LINE 383 
In form (9), it is easy to see that the line passes through the point with 
coordinates (, j), which is called the centroid of the set of points. Among 
criteria for a close-fitting line, the requirement that the line pass through 
the centroid might have been chosen. We are pleased to have satisfied 
this requirement by least squares, so to speak, at no additional charge. 
In plotting the least-squares line on a graph, it is oftell convenient to plot 
the centroid and draw through it a line with slope ?. 
EXAMPLE 2. For Strong's data on retention in Table 10-1, derive the 
least-squares line relating retention, /, and the logarithm of the time, t. 
Solution. The calculations are straightforward but tedious. We get 
approximately' 
---- 2.29,  = 43.2, Rt = 963.8, 
Et = s6.22,  = 30,40,  = 459.91,  = 1.39, 
Y.(tl -- ) ---- -.t -- n   86.22 -- 68.17  18.05, 
Rt -- n  963.8 -- 1286.1 = --322.3, 
d  R ---- 43.2, 
?h = Rt -- nRt  --322.3  --17.9, 
-.l _ n 18.05 
_ = E:(- ) (E:t- ) 459.91 (-3'3) 1. 
n n n(tl- ) 13(18.05) 
From Eq. (9), the least-squares line is 
R- =(t-) 
or 
/� -- 43.2 = --17.9(t -- 2.29) 
or 
/ = 84.2 -- 17.9t. 
Using this line, we estimate retention as 84.2% at time 1 minute 
(log 1 = 0); at 100 minutes, log T = t = 2, so we estimate percent re- 
tention as 84.2 -- 35.8 = 48.4. 
We want to show that /n is the variance of the vertical deviations 
from the line. Then x/,/n is the standard deviation of the d, and we 
384 LEAST SQUARES, CURVE-FITTING, AND REGRESSION [CHAr. 10 
have a measure of error in estimation. We call 4/n the standard error 
of estimate, and we use the symbol 
sd = 
We show that Ed = 0 for the least-squares line, and thus prove that 
s 2  
n 
10-6 Theorem. Ed = 0. The general least-squares line leads to 
deviations d whose sum is zero. 
Proof. For a point (x, y), the fitted line predicts the value of y as 
y ----  + ?h(x -- :). 
Then 
di= y-- y = (y -- ) -- ?h(x-- :), 
Ed = E(y - Y) -- hE( - ) = 0.  
10-7 Theorem. Variance of deviations. The minimum average squared 
error, ,/n, is the variance of the deviations from the general 
least-squares line. 
Proof. Because d = 0, we have 3 = dl/n = 0, and therefore 
n n 
2 
REMARK. Some authors use the symbol s for our 
10-8 Definition. Standard error of estimate. The standard error of 
estimate is 
Thus the standard error of estimate is just the standard deviation of 
the vertical deviations from the least-squares line. For Strong's data, 
sd  v/-  4.1, so by Chebyshev's Theorem we are sure that at least 
75% of the lneasurenmnts al'C within 2(4.1) = 8.2 of the line. 
10-4] FITTING A GENERAL LINE 385 
EXERCISES FOR SECTION 10-4 
1. Find the general least-squares line for the set of pairs (1, 1), (2, 3), (3, 3) 
and find the standard error of estimate. 
2. (Continuation.) Compute the three vertical deviations in Exercise 1 
and compare their absolute magnitudes. 
3. Compute the deviation scores for Example 1, compare their magnitudes, 
and find their sum. 
4. Find an equation of the general least-squares line and the standard error 
of estimate for the points with coordinates 
x: --2 --1 0 0 1 2 2 
y: --2 --2 --2 --1 --1 0 1 
5. (Continuation.) Use the equation of Exercise 4 to estimate y when 
z = --1, 0, 1. 
6. Find the slope and y-intercept of the general least-squares line fitting the 
points with coordinates 
x: --1 --1 0 0 0 1 1 
y: 0 1 --1 0 1 0 --1 
7. (Continuation.) Plot the points and the least-squares line for the data 
of Exercise 6. Note that the line passes through the mean of the y-values cor- 
responding to each 
8. In an eight-week course, the final grades A, B, C, D, E were given scores 
y = 2, 1, 0, --1, --2, respectively. Students with more than 3 absences were 
dropped from the course. The fitted line for predicting grade scores from ab- 
sences was y = 1.5 -- 0.5z. What average letter grade is predicted for students 
with 1 or 3 absences? For one with 4 absences if the same equation held? 
9. For the points with coordinates (0, 0), (0, 1), (1, 0), (1, 1), find the general 
least-squares line for predicting y from x. 
10. The general least-squares line for predicting y from x is y = 3x -4. 
If = 1, findS. 
11. Show that if  = 0, the slope of the general least-squares line and of the 
least-squares line through the origin are identical. 
12. Two points always determine a straight line. But for what kind of pairs 
of points is it impossible to apply formulas like (7) or ($)? 
13. Show that 
i' = 
2 
8x 
14. Show that 
8d  8y -- 
2 
8x 
15. Show that s = s2 
3sa LEAST SQUARES, CURVE-FITTING, AND REGRESSION [CHAP. 10 
16. Write out the steps of the.proof of Theorem 1(-5 on the general least- 
squares line for two points, (Xl, y) and (x2, y2). 
17. One set of points has x-coordinate x, another set has x-coordinate 
x2  Xl. Use a verbal argument to prove that the general least-squares line 
passes through the points (Xl, ), (x2, 2), where  is the average y for points 
with x-coordinate x, i = 1, 2. 
18. For three points with x-coordinates equally spaced, (x -- 1, yl), (x, y2), 
(x - 1, y3), fit the general least-squares line and compare the absolute magni- 
tudes of the deviations. Comment. 
19. If several sets of points have x-coordinates Xl, x2,..., xk and y-means 
y, y2,..., k, and if the ordered pairs (x, ) are collinear, show that the 
least-squres line passes through these points. 
20. Write out the steps of Theorem 10-5 on the general least-squares line 
lorn points (x, y0, i = 1, 2,...,n. 
10-5. PRECISION OF ESTIMATION AND THE CORRELATION CO- 
EFFICIENT 
The prediction equation 
y =  + (z - ) 
usually gives us a way to use the relation between x and y to improve 
the prediction of y. When the slope ?h is zero (or nearly zero), our estimate 
of y would be 
y ---- , 
and the variance of the deviation scores is then just the sample variance 
of the y's: 
 E(y - 7)  
n 
On the other hand, the deviation scores d, obtained when we use 
Eq. (1) to predict y, have variance 
d = -- 
E(y - ) (Ey - .) 
= n - nZ(x.- ) (3) 
We recall from Section 9-4, Eq. (16), that the correlation coecient r is 
 = � (4) 
nSxSy 
10--5] PRECISION OF ESTIM_TIOX 387 
We Call rewrite the last term of Eq. (3) in terms of the correlation co- 
efficient and s u as follows' 
222 
 SxSy 
22 
When e substitute the results of Eqs. (2) and (5) into Eq. (3), we get 
82 2 2 2 
- = r )s. (6) 
a ---- Sy r Sy (1 -- s s 
2 2 
The extent to which s is smaller than Sy measures the improvement in 
8d/Sy. 
prediction, at least in large samples. Let us consider the ratio  s 
Using Eq. (6), we have 
2 
s = 1 -- r " (7) 
2 
8y 
s that has been removed 
Consequently r s measures the fraction of Sy 
from the variance of deviations by the fitting of the linear equation, 
Eq. (1). It is customary to say that r s is the fraction of variance, s, 
"explained" by the regression line. 
s [from Eq. (6)], we have, 
What values Call r take? Since 0 _ s _ su 
upon dividing through by % 
0 s 1, 
8y 
or 
0_< 1-r s_< 1. (8) 
The values of r that satisfy (8) are 
-1 < r _< l, (9) 
and this inequality shoxva that the value of the correlation coeftScient can 
run only from --1 to +1. 
388 LEAST SQUARES CURVE-FITTING AND REGRESSION [CttAP. 10 
� x y 
x y 
y Y 6 --1 
64 i! r=l r=-I f 4 -- 
2 2 5 5 2 0 
1 
0   4- O, 
--2 0 --2 1 
--4 1 .  --4 1-1, 
--6 2 L / " _ --6 2 
1_; 12 345 
.... 6-5-4-- -(1 I 2 3-5 6 
Y y 
4    z y 4 
I i ' 014 
3  2 3 I 3 
2 4 3-   3 3 
3 4 4 2 
r = %087 r = -089 
o I 2 3 4 o 
(c) 
x y 
3 S 
5 7 
Y 5 9 Y 
r=0 6 6 r=0 
15 -  8 z Y 
z 10 
  7 18 
7 9 20 , 6 11 
x 8 7  5 6 
10 - x x 10 5  4 3 
' 3 2 
x x 10 S 
x x x x x 10 11  2 3 
 x 11 b 10 I 6 
5- x 13 6  / 011 
13 l0 1 
15 
I 
5 lO 15 5 5 lO 
FIGU 10-6 
10-5] PRECISION OF ESTIMATION 389 
By rewriting Eq. (1) we can see at once the meaning of various values 
of r. First we rewrite h in terms of r, sx, and Sy' 
2 8y 
xy -- n Sy 
SxSy 8x 
and finally 
Sy (10) 
=r. 
When we substitute this value into Eq. (1), the forecasting equation 
becomes 
Sy (11) 
- = (z - 
The standard deviations 8y and sx are both positive, so if r > 0, then 
the slope rSy/S > 0, and the line rises to the right. Thus as x increases, 
y increases. If r < 0, then the line rises to the left. If r ---- 0, the line 
is horizontal. 
Note that r: 0 does not imply that no relation exists between y and x, 
merely that the degree of linear relationship is zero. 
Figure 10-6 shows a variety of "scatter diagrams," together with the 
coordinates of the points and the correlation coefficients between x and y. 
In practice, there are usually many more points than these, but easy 
examples are worth study. 
Parts (a) and (b) of Fig. 10-6 show that when the points are collinear, 
then r ---- 1 or --1, as we expect (unless the line is vertical or horizontal). 
Parts (c) and (d) show large positive and negative values of r; a line 
2 
fitted to them would have a value of Sd  that is much less than 
Parts (e) and (f) show two different examples of r: 0. In (e), the 
points are scattered about rather haphazardly, and we can well imagine 
that there is little or no relation between y and x. In (f), the points were 
constructed to fall on the parabola with equation 
y = (x-- 3) 2+2; 
thus there is a mathematical relation between y and x, but it is not meas- 
ured by r. The reason is that r measures degree of linear relationship, 
and more advanced methods are required to assess curvilinear relations 
of this sort. 
390 LEAST SQUARES, CURVE-FITTING, AND REGRESSION [CttAP. 10 
Calculations. For calculations of r made with paper and pencil without 
the aid of machines, the formula 
r ---- , (12) 
SxSy 
is a bit more convenient than Eq. (4), because the numbers in the numer- 
ator on the right of Eq. (12) are smaller than those in the corresponding 
position of Eq. (4). 
In these calculations, numbers should not be rounded off too soon, or 
the information in the data will be lost. 
EXAMPLE. Rounding off too early. Given the three (x, y) pairs (1, 1.1), 
(2, 1.2), (3, 1.4) in the table below, compute r. 
Solution (with excessive rounding). 
x y xy 
6 3.7 
I 1.1 1.1 -- 3 -- 2, -- 3  1.2, 
2 1.2 2.4 
3 1.4 4.2 xy 7.7 
--  2.6, 
� n 3 
6 3.7 7.7 
2 (--1) 2 q- 0 2 q- (q-l) 2 2 
S ---- 3 3 ' 
2 (y __ y)2 (__0.1)2 q- 0 2 q- (0.2)2 
n 3 
O.O5 
: --  0.02 
3 
r = (xy/n) -- Z 2.6 -- 2(1.2) 0.2 0.2 
x o.o 4o.o 
The trouble here is not the careless rounding in the calculation of 2 2 
8xSy, 
though it might have been. Careful calculation of 2 2 
SSy gives 0.28/27  
0.01037, and v/0.01037  0.102. The rounding in the numerator pro- 
duced the error. We should have had Exy/n -- 7  2.567 -- 2(1.233) 
= 0.101 instead of 0.2. 
Today, hand calculating machines are quite widely available, and unless 
n is small and the numbers of significant figures in the x and yl values are 
small, hand calculators are used. That is why in our exercises the numbers 
have been kept modest and easy, even though in practice n may be very 
large and the numbers Culnbersome. 
10-6] MODELS FOR REGRESSION 391 
EXERCISES FOR SECTION 10-5 
1. From the data of Fig. 10-6(a), calculate directly that r = 1. 
2. From the data of Fig. 10-6(c), calculate that r  0.87. 
3. From the data of Fig. 10-6(d), calculate that r  --0.89. 
4. From the data of Fig. 10-6(f), calculate directly that r = 0. 
5. Show that when s = su, if, = r. 
6. When r = 0, s > 0, find the prediction equation of y from x, and explain 
the use of the expression "from x" in this situation. 
7. Given two points not both on either a horizontal or a vertical line, what 
values can r have? Explain. 
8. If r = 0.3, hat percentage of the variance of y is explained by the re- 
gression line? What if r = --0.3? What if r is 0.8? 
9. The percentage of the variance of y explained by the regression line is 16. 
What values can r have? 
10. If  =  = 0, s = su > 0, find the linear cquation for predicting 
y from x. 
11. If  = 2, g = 3, s = 5, su = 10, r = --0.4, find the linear equation 
for predicting y from x. 
12. Show that ifx, andyhave = 0, -- 0, s = s = 1, thenr = Yx,y/n. 
Note that n this special case x and y are standardized variables, having 0 mean 
and variance 1. 
13. If s] = 12, s = 20, find the possible values of r. 
14. If r = 0.6, sa = 8, find sv. 
15. If the variable y is correlated 0.1 with x, what would you say of the 
effectivencss of x in helping to predict y? 
16. If the correlation between x and y is r, show that if x = x, - c, where 
c is a constant, the correlation between x  and y is also r. 
17. Show that if x = cx, where c is a positive constant, the correlation 
between x  and y is thc same as the correlation between x and y. 
18. Show that in Exercise 17 if c is negative the correlation between x  and 
y is the negative of the correlation between x and y. 
19. Show that if x and y are correlated r, the new variables x  and y, where 
x( = c - dx, and y = e - fy,, are correlated r if df :> O, and --r if df  O. 
This theorem is usually referred to as "the invariance of r under linear trans- 
formations in either or both variables." 
10-6. MODELS FOR REGRESSION 
In addition to the name "curve-fitting" that we have used in this 
chapter, the theory ve treat is also called "regression theory." The origin 
of this puzzling name will emerge later. Many different mathematical 
descriptions or models are required to describe the physical situations 
where regression theory applies. We discuss here two models that arise 
frequently. 
392 LEAST SQUARES, CURVE-FITTING, AND REGRESSION [CHAP. 10 
Control-lcnob model. In the c.ontrol-knob model, one of the variables is 
more or less under the control of the investigator. When Strong did his 
retention study (see Section 10-1), he thought retention depended on 
time, and so he chose to control, as with a control knob on a radio or 
television set, the set of times in the experiment. These times Were not 
randomly drawn. On the other hand, the retention of the subjects in the 
experiments can be regarded as a random variable. If we consider a fixed 
time until testing, the corresponding variability of retention has at least 
two important sources. 
First, the same person recalling lists repeatedly remembers varying 
numbers of words. Anyone studying a foreign language has had this 
experience. Second, as every student knows, individuals differ in their 
ability to remember. This second source of variability is present in 
Strong's study, because a population of people was sampled. More 
generally, in the control-knob model, let us think of picking a single 
value of one variable, x, and evaluating the other variable Y, repeatedly. 
The value of Y depends on the outcome of an experiment: Y is a random 
variable. There is a distribution of the random variable Y for each value 
of x, but we do not think of x as a random variable. 
For any given x, let us suppose that the distribution of Y has a mean 
E(Y[x) (read "expected. value of Y given x"). Suppose that as x moves 
continuously the points with coordinates (x, E(Y[x)) trace out a smooth 
curve, or a straight line. Suppose, for example, that 
E(YIx ) -- a q- m(x - ). 
We have developed methods for fitting a straight-line relation. A 
control-knob experiment produces paired data (x, y), i = 1, 2, . .., n. 
We fit these data with the line 
Then d is an estimate of a and h is an estimate of m. If we carry out 
many repetitions of the experiment, then there are many values of d and 
many values of rh. It turns out that E(d)  a and E({,)  m; we shall 
not prove these equalities, but their demonstration is not difficult. 
In the artesian-well example (beginning of Section 10-3), the depth 
beloxv the origin is the independent variable. The experimenter chooses 
these depths; they do not have a probability distribution. On the other 
hand, repeated careful measurements of temperatures at a given depth 
probably would vary from week to week. Thus, for each depth, it is 
reasonable to think of the temperature difference as a random variable. 
It may occur to you that when a temperature-measuring gadget 300 
10-6] MODELS FOR tCEGRESSO. 393 
meters long goes into the water, there may be some uncertainty about 
the depth x. A more complicated model might be needed to handle this 
source of variation. 
REMARK. In the statistical literature, the control-knob model is known 
by the unhappy name "the fixed variate model." The word "fixed" refers 
to the fact that the x's are not values of a random variable. 
Bi'ariate model. Suppose that we have a population of individuals, 
each with two measurable characteristics, such as height and weight. If 
a person is drawn at random from the population, then the random 
variable � ----- height takes a value, and so does Y ---- weight. In such a 
sampling procedure, neither variable is controlled by the experimenter. 
Furthermore, we might wish to predict height from weight or weight from 
height. 
Galton, a biometrician, in the late 1800's probably had heard discussions 
by the older generation concerning the younger generation: Why was it 
going to the dogs? In particular, why was it that great fathers didn't 
have great sons? The sons of great statesmen were often not as impressive 
as their fathers. Gaiton was nterested m heredity, so he set about to see 
if he could study such a question in an objective biological situation. 
He studied tallness rather than greatness. 
He obtained data on heights of fathers and heights of their sons, and 
thus he had ordered pairs (x, y), where x s the height of the father and 
y the height of the son. Galton's data are not in a form suitable for our 
purposes, but similar data gathered by Karl Pearson and Alice Lee are 
shown in Table 10-6. 
Galton found that tall fathers, on the average, had sons who were 
shorter than they, although not as short as the population mean. On 
the other hand, short fathers had sons on the average taller than they, 
though not as tall as the population mean. Thus if tallness were regarded 
as desirable, tall fathers had disappointing sons, but tall sons had dis- 
appointing fathers, too. On the other hand, short fathers would have 
taller sons, and short sons would have taller fathers. (In school a similar 
effect is familiar. The student at the top of the class on one test can only 
maintain his position or drop in rank on the next test, while the student 
at the bottom can only maintain his position or move up ) Galton re- 
garded this result as a "regression" toward the mean of the population, 
and he called the line fitted to predict a son's height y from a father's 
height x, the regression of sons' heighls on fathers', or the regression of y on x. 
Then he turned the data around and predicted a father's height x 
from a son's height y, and obtained the regression of x on y. Thus Galton 
found two regression lines, one for predicting y from x, the other for 
predicting x from y. And these regression lines are not identical. 
394 LEAST SQUARES CURVE-FITTING AND REGRESSION [CHAP. 10 
OtlOU! u[ 'uo jo oanll 
10-6] MODELS FOR REGRESSION 
The regression of x on y. We know how to get the regreasion of y on 
by fitting y = d 4- h(x -- 2). The procedure for obtaining the regression 
of x on  is similar; we fit 
x= g4-h(--). 
A deviation score for predicting .ri from y is 
& = (.r- g) -- h( -- ), i = l, 2,..., . 
We minimize 
= = [(x. - - - 
The methods we used in Section 10-4 apply, and we find the minimizing 
values of g and h to be 
Z(, - ) () 
Recall that the parallel results for the regression of y on .r were 
 = , h = x- 
2(.r - ) 
EXAMPLE 1. Find the two regression lines for the points plotted in 
Fig. 10-7. 
Solution. The graphs in Fig. 10-7 have 9 points; the heavy dots are the 
data. The simplicity of the example makes possible the calculation of the 
regression lines without using heavy calculations Exercise 19 of Sectiou 
10 tells us that xvhen the points plotted at the means of the vertical 
arrays for each x-value are collinear, the least-squares line passes through 
the means. In the figure a cross indicates the column average for one value 
of x. For example, at x = 1, there are two values of y, 1 and 0. These 
values average 0.5, so the X in that column is plotted at y = 0.5. In 
this example, all crosses lie on a straight line, consequently the regression 
of y on x is the line through the crosses. Because it pases through the 
origin and has slope , its equation is 
y = x (regressran of y on x). (2) 
396 LE&$T SQUARES, CURVE-FITTING, AND REGRESSION [CHAP. 10 
y Y' 
I ' Regression of x on y x = y 
. Regression 
1, � � 1, y n 1 
of y o x: y = x 
I I - x ' � ..:'" .  
(a) (b) 
Fla. 10-7. Scatter diagram illustrating two regression lines. 
Similarly, for a given datum value of y the average of the x's is the same 
as the x-coordinate of the middle dot in the row for that y. These averages 
are shown as open circles. They too fall on a straight line, which is the 
regression line of x on y. It also passes through the origin, but has slope 
1, so an equation for it is 
x = y (regression of x on y). (3) 
Thus for predicting y from x we use Eq. (2), but for predicting x from 
y we use Eq. (3). In this example, these equations are distinct. 
The simplicity of the array of points in this example made possible the 
construction of the equations without the usual calculations. Naturally, 
such an example would be rare in practice. However, a quick way to get 
an approximate regression line is to compute column (or row) means, plot 
them, and pass a visually fitted straight line through them. 
The bivariate model differs from the control-knob model in that it has 
two legitimate regression lines instead of one. In the control-knob model, 
we would not compute the regression of x on y. 
EXMrLE 2. Pearson-Lee data on heights. Find the two regression lines 
for heights of fathers and sons. 
Solution. Heavy computation gives 
y = 0.51x q- 33.9 (regression of sons' heights on fathers'), 
x = 0.51y q- 32.8 (regression of fathers' heights on sons'). 
10-6] MODELS FOR RECRESSO  ;]97 
EXAMPLE 3. Ftrsl-ace problem. In the first-ace problem of Section 1-5, 
find the regression of the (r  1)st count on the rth. That is, find the 
regression line for predicting from one count that on the next. We know 
that with perfect shuffling there should be independence between the 
successive counts, so this calculation provides a check on the model. 
For example, if high counts are followed by lows, and vice versa, we 
should get a negative slope for our regression line. 
Solulion. The original data are given in Table 1-1, and from them 
are derived a set of ordered pairs of successive counts. The original 
sequence started 5, 4, 29, 3, 24, . . . The pairs start (5, 4), (4, 29), (29, 3), 
(3, 24), .. The data are plotted in Fig. 10-8 (we have added the count 
for shuffle number 101 -- it was 5 -- to get an even 100 pairs.) 
y 
35- 
 � 
 25 � 
 20- 
 ' ' ' Fitted regression 
1[ 15 ..... line of y on x: y = 0.02 - 10.7 
� . . Th retcal regression 
0 5 10 15 20 25 30 35 
x = count on rth trial 
F. 10-8. Graph of the data from the first-ace problem, Table 1-1, with 
theoretical and fitted regression lines for predicting the count on trial r- 1 
from the count on trial r. 
398 LEAST SQUARES, CURVE-FITTING, AND REGRESSION [CItAP. 10 
If the first and second members of a pair of counts are independent, as 
ve would assume, then the mean of the second count, given the first, is 
the same as the mean of the second, unconditionally. That is, the expected 
value of the second count is uy: 10.6 (as we found in Section 1-5) 
for every possible first count. Therefore the theoretical regression line 
of y on x is the horizontal line with equation 
y---- 10.6. 
The use of formulas. (7), Theorem 10-5 of Section 10-4, and heavy 
calculation gives the observed regression for the 100 points of Fig. 10-8 as 
y ---- 0.021x q- 10.73. 
Clearly, h is near zero, as it should be if we have independence betveen 
successive counts, and the constant term 10.73 is close to the theoretical 
value of 10.6. We continue to be satisfied with the independence assump- 
tion for successive counts in the first-ace experiment. 
EXERCISES FOR SECTION 10-6 
1. Toss 2 coins indepehdently. If the first coin falls heads, let x = 1; if first 
coin falls tails, let x = 0. The random variable Y is the total number of heads 
in the two tosses. 
Discussion. We have the following table of joint probabilities: 
Totals 
1 
0 0 
1 
i  
2 0 
 1 
Totals �  
(a) Compute E(I'lx) for each value of x. (b) Fit the theoretical regression of y 
OllX. 
2. Use the data in Exercise i to obtain the regression of x on y. 
3. Repeat the experiment in Exercise 1, but toss 3 coins independently. As 
before, let x = i if the first coin fails heads, x = 0 if the first coin falls tails, 
and Y = total number of heads. (a) Construct the table of joint probabilities 
for x and y. (b) Compute E(}'lx ). (e) Fit the regression of y on x. 
4. Use the data in Exercise 3 to obtain the regression of x on y. 
10-6] MODELS FOR REGRESSION' 399 
5. An engineer's ruler of triangular cross section has faces numbered re- 
spectively, 1, 2, 3. Imagine rolling two such rulers, one after another, on the 
floor. Let � = number on the bottom face of the first ruler and I = sum of 
the numbers on the two bottom faces. (a) Compute E(Ilx) for each x. (b) Fit 
the regression of y on x. 
6. Use the data in Exercise 5 to fit the regression of x on y. 
7. Metal rods of length x units are inspected for flaws. For a given length x, 
the fraction with each number of flas, y, is shown in the following table: 
Proportion of defective rods 
x, length of rods 1 2 3 
0 .6 3 .2 
Y, i .3 .5 3 
number of 2 .1 I .3 
flaws 3 0 .1 .2 
(a) Compute (�1x) for each x. (b) Fit the regression of y on x. 
8. Four coins are tossed and the value of the number of heads x is recorded. 
Then the coins that showed tails are retossed and the value of the numbcr of 
total heads Y is recorded. For each x compute E(YIx), and then find a pre- 
diction equation for estimating Y from x. 
9. In Exercise 8, the initial number of heads Y is a random variable. For 
each value of Y, compute (2ly) , and find the prediction equation for X from y. 
10. Let x be the number of the trial, and let R be a random digit. Let 
Y = x -- R. This is an example of a control-knob model for regression. Show 
that (YIx) = x  4.5. 
11. (Continuation.) Refer to Exercise 10 and use the last 5 random digits 
on the top line of your random-number table to get values of R for trials 0, 1, 
2, 3, 4, and find the value of I"for each trial. Now fit the resulting (x, y) data 
with a general least-squares line, and compare the slope and y-intercept with 
those of the theoretical hne in Exercise 10. 
12. (Continuation.) In Exercise 10 let Y = x-- R, where R is the average 
of 5 random digits. Use the averages of the last 5 digits in the first 5 rows of 
your random-number tables to get five (x, y) pairs, and fit the general least- 
squares line. Compare the slope and y-intercept of the fitted line with those 
ofy = x-I-, 4.5. 
13. (Continuation.) For Exercise 12 compute s and compare it with the 
theoretical value  = X/�'   1.3. 
Note. You may wonder why we do exercises like 11, 12, and 13, when we 
already know the theoretical regression from Exercise 10. The answer is that we 
need to gain experience in relating data to a model, and that experience only 
comes when we know both data results and the model. In practice, it is rare 
that we know the model and its constants exactly. If we only fit data and 
400 LEAST SQUARES CURVE-FITTING .AND REGRESSION [CHAP. 10 
compare data results with fitted !ines, we may get an exaggerated view of how 
wcll the modcl is being reproduced by the data. 
14. The following data are derived by grouping the staturcs in Table 10-6 
by 5-inch intcrvals. 
x 
60 65 70 75 Total 
62 10 49 7 0 66 
67 22 332 262 6 622 
72 0 88 257 25 370 
77 0 0 13 7 20 
Total 32 469 539 38 1078 
Use this table to compute ., , s, sy, -xy, and from these obtain the regression 
of y on x and the regression of x on y. In calculating, you will find it convenient 
to replace x by x  = (x -- 70)/5 and y by y = (y -- 67)/5, and then make 
appropriate adjustments at the end. Compare your equations with those 
shown in the text. 
15. Three cards are numbered 1, 2, and 3. One card is drawn at random from 
the three and its value is recorded as x, then a second card is drawn at random 
from the two remaining cards and its value y is recorded. For each x (= 1, 2, 3) 
find E(YIx), and fit the regression of y on x to the resulting means. 
APPENDICES 
APPENDIX 
I 
COLLECTIONS OF 
OBJECTS: SETS 
I-1. THE NOTION OF A SET 
The idea of a set is common in everyday life, where it implies a recog- 
nition of some coinmon property possessed by a group of objects. We 
speak of a set of dishes, a set of stamps, a set of books, and so on. The 
implication is that one can tell whether or not a given dish or stamp or 
book belongs to the group under discussion. 
The idea of a set is basic in mathematics. Indeed, it has been said that 
the whole of contemporary mathematics can be derived from the concept 
of a set and the rules of logic. In mathematics, we use the word set to 
denote any well-defined collection of objects, things, or symbols. By 
"well-defined" we mean that it must be possible to tell beyond doubt 
whether or not a given object belongs to the collection that we are con- 
sidering. Thus the connotation of the word set is the same as when it is 
used in its nontechnical, everyday sense 
Anything that is a member of a set is called an element of the set. If 
some positive number or zero is the number of elements in a set, we say 
that the set is finite; otherwise, the set is ifinite. Our present purpose is 
to introduce some of the vocabulary and ideas of the theory of sets, 
cause these notions will contribute to the ease and clarity with which 
probability and statistics can be treated. tiere are some examples. 
EXAMPL 1. The following collections satisfy the requirements of a set: 
(a) the people in your immediate family (father, mother, you, your 
sisters and brothers); 
(b) your class in school; 
(c) the students who take mathematics in your school; 
(d) the eighty-second Congress of the United States; 
(e) the fifty states in the U.S.A.; 
(f) the positive integers with tvo digits; 
(g) the prime numbers less than 50 
403 
404 APPENDIX I 
EXAMPLE 2. What is the sea of points (x, y) whose coordinates satisfy 
the inequality y >_ 4 -- 2x? 
Solution. All points whose coordinates satisfy y ---- 4 -- 2x lie on the 
straight line passing through the points (2, 0) and (0, 4). The points 
whose coordinates satisfy y  4 -- 2x lie above this line. Therefore the 
required set of points consists of the half-plane above and including the 
llne whose equation is y ----- 4 -- 2x. (See Fig. I-1.) 
y 
I-1. Graph ofy >_ 4-- 2x. 
EXERCISES FOR SECTION I-1 
1. Give two examples of sets whose elements are: (a) people, (b) books, 
(e) letters of the alphabet, (d) numbers, (e) geometrical objects. 
2. Each of the numbers 0, 1, 2, 3, 4, 5 is written on a ping-pong ball, and the 
balls are put in a hat and thoroughly mixed. A blindfolded person reaches 
into the hat and draws out two balls in succession. The number x on the first 
ball and the number y on the second ball are recorded, in order. 
(a) Plot the points (x, y) that correspond to the set of all possible outcomes 
of this experiment. [Note. Order counts in these pairs; for example, (1, 2) is 
different from (2, 1).] 
(b) How many points are there in the set of part (a)? 
(c) Complete the following chart to show the set of all possible sums x q- y 
that can be formed by adding the numbers on the two ping-pong balls, and the 
set of all pairs (x, y) that give each sum. 
(d) Indicate the sums of part (c) by drawing the set of lines x + y = k 
for k = 1, 2, 3, . .., 9 on your diagram of part (a). 
APPENDIX I 405 
x-- y (x, y) having givcn sum x q- y 
1 (0, 1), (l, 0) 
2 (0, 2), (2, o) 
3 (0, 3), % 2), (2, ), (3,0) 
3. In the experiment of Exercisc 2, suppose that the first ball is put back into 
the hat before the second ball is drawn. Repeat the four parts of Exercise 2 
for this new experiment. 
4. Describe and sketch the sets of points whosc x and y coordinatcs satisfy 
cach of the following conditions: 
(a) y = z, (b) y > z, (c) V <z, 
(d) y > x+ 1, (e) x+V < 4. 
I-2. TWO WAYS OF SPECIFYING SETS 
In common practice, there are two ways of specifying a set: 
(1) List the names of all members of the set (the "roster" method). This 
method is clear-cut and convenient when the number of elements of the 
set is not too great. It is customary to use a capital letter (for example, 
A, B, S, ...) for the name of the whole set, and to enclose the names of 
the members of the set in braces. Thus, the set S consisting of the numbers 
of dots on the faces of a die is 
S= {1, 2, 3, 4, 5, 6}. 
(2) State the requirements that any object must meet in order to be a member 
of the set (the "rule" method). In stating a rule for membership, we can 
express it in words: 
"S is the set of all elements x such that x is an integer and x is between 
I and 6, inclusive." 
However, it is much more compact to express the rule thus' 
S = {x 'x is an integer and 1 _< x _< 6}. 
The notation 
406 APPENDIX I 
is called the "set-builder." The colon ":" is read "seh that." On the left 
of the colon is a symbol for an arbitrary element of the set; on the 
right is the rule that defines membership in the set. 
EXAMPL 1. Specify the set of vowels S in two ways. 
Sohtion. We can write 
S---- l a, e, i, o, u} (roster method), 
or 
S ---- l*: * is a vowel} (rule method). 
oT. The asterisk and the "x" are used in the foregoing as symbols 
that hold places for arbitrary elements of a set. Thus "x" is not used in 
this connection as a letter of the alphabet. 
EXAMPLe2. IfS -- /1,2,3,4,5,6} and Tisthe set of numbers that 
are squares of the elements of S, specify the set T in two ways. 
Solutions. We have 
T---- /1, 4, 9, 16, 25, 36} (roster method), 
T ---- /z 2 :x is an element of S} (rule method). 
ExAP 3. Specify the set of all points inside the circle x 2 - y ---- 4. 
Sohtion. S ---- I (x, y) : x and y are real numbers and x y ( 4}. 
EXAmPlE 4. A cent and a dime are tossed into the air. Let x represent 
the face of the cent that lands on top, and y that of the dime. Specify in 
two ways the set S of possible pairs (% y) that the coins my show. 
Sohttion. We can write 
S----{(H,H), (H,T), (T,H), (T,T)}, 
or 
S---- {(x,y) :.isHorTandyisHor T}, 
where xve have used H for "head" and T for "tail." 
Rrnc. Note that in Example 4 we use z to represent something 
other than a number: .z is used as  symbol to represent an element of the 
set {m, T}. 
APPENDIX I 407 
EXERCISES FOR SECTION 1-2 
1. Use both the roster method and the rule method to specify the following 
sets: (a) the consonants in the first half of the alphabet; (b) the prime numbers 
less than 25. 
2. Specify the following sets by the rule inethod and discuss why the roster 
method is difficult or impossible: (a) the set of people who live in your com- 
munity; (b) the set of all even numbers; (c) the set consisting of squares 
of integers. 
Inequalities and absolute values: useful symbols in specifying sets. Many 
of the sets that we shall study are sets of numbers whose elements can be 
concisely specified by the use of equations, inequalities, and other mathe- 
matical symbols. The rules for dealing with inequalities are summarized 
as follows: 
(1) If the same number is added to, or subtracted from, both sides of 
an inequality, the new inequality holds with the same inequality sign. 
(2) If both sides of an inequality are multiplied or divided by the same 
positive number, the new inequality holds with the same inequality sign. 
(3) If both sides of an inequality are multiplied or divided by the same 
negative number, the new inequality holds with the reversed inequality sign. 
Note that the operations with inequalities are similar to those with 
equations, except for rule (3). Thus, 
if x > y, 
then kx > ky, if lc is positive, 
and kx < ky, if k is negative. 
What becomes of the inequality if k = 07 
A useful mathematical symbol is that of absolute value, which we shall 
now discuss. Suppose the number a is represented by the point A on a 
number scale (Fig. I-2). Then the absolute value of a, denoted by ]al, is 
B o 
, t 
---lb'- - - 
b 0 
FIG. 1-2. Absotutc value. 
408 AVVEnX  
the number of units in the distance OA. Thus, if a ---- -3-3, la[ ---- 3 
and if b = --2, [b[ ---- 2. Similarly, we have 
The foregoing geometrical interpretation of ]a] leds to the following 
lgebric definition. 
I-1. Definition. Absolute value. 
a 0, Uhn lal = a;if  ( 0, then lal = --a. 
Evg 1. Represent graphically x � ]x[  2}, where x is rel. 
Solution. The inequality ]x]  2 states that the measure of the distance 
from point x to point 0 on the number scale is less thn or equl to 2. 
Hence the domain of x extends from --2 to 2, inclusive, s shown in 
Fig. I-3. 
--2 0 2 
F. I-3. Graph of I:11  2/. 
NOTE. We use the filled-in dot, "o," at 2 and --2 to indicate that these 
numbers are included in the set. An open dot, "o," may be used to indi- 
cate that a number is excluded. 
What geometrical interpretation can be given to Ix -- 517 The defini- 
tion of absolute value suggests two cases: 
(1) If x-- 5 >_ 0, then Ix-- 5] = x-- 5. For this case, Fig. I-4 
shows that Ix -- 51,or x -- 5, represents the measure of the distance 
from.z to5 on the number scale. (If x -- 5 = 0, x = 5 and ]x -- 51 = 0.) 
5 - .,-x -- 5 =  
0 5 x 
Flo. I-4. Ix-- 5] for x-- 5 k 0. 
(2) If x-- 5 < 0, then I x-- 51 ---- 5-- x. Referring to Fig. 1-5, we 
see that once again I x -- 5], or 5 -- x, measures the distance from x to 5. 
Thus, in all possible cases, Ix -- 51 measures the distance between x and 5 
on the number scale. In general, I x -- a I is the distance between x and a 
on the number scale. 
APPENDIX I 409 
0 x 5 
FIG. I-5. [x -- 5[ for x -- 5 < 0. 
EXAMPLE 2. Represent graphically {x :Ix -- 41 < 2}, where . is a 
real number. 
Sohaion. The inequality I.; -- 41 _< 2 states that the distance from 
x to 4 is less than or equal to 2. This means that x may have values 
from 2 to 6, inclusive: 
2<.v< 6. 
Figure I-6 is a graph of the domain of x. 
0 2 4 6 
FiG. I-6. Graph of {x'lx- 4] _< 2}. 
The following table exhibits some sets of real numbers, defined with 
and xvithout the absolute value symbol. Graphs are included. 
TABLE I--1 
SPECIFICATION OF SETS OF REAL NUMBERS. 
Using absolute 
Without absolute values Graph 
values 
{x .Ixl >_ 3} {x: x2>_ 9} or , , , I 
{x'x >_ 3 or x_ --3} -3 0 
{x :Ix -- 11 < 2} {x:--I < x < 3} ' -1 0 1 3 
{x :lxq- 51 > 1} {x :x < --6 or x > --4} ' , ? {  , ,  
-6-5 -4 
{x 'Ix - al < 2} {x � a - 2 < x < a q- 2} ' ? ' } '  '  
a--2 a a+2 
410 APPENDIX I 
FURTHER EXERCISES FOR SECTION I-2 
1. Evaluate each of the following: I-5I, I-x/l, Iq-41 q-[-4I, 15- 
18 - 5[, Ic - 71, 17 - el, Iml. 
2. Verify by trial, using both positive and ncgative values of c, that x/ = 
3. Vcrify by trial, using both positive and negative values of z, that z < Ix] 
and --x < 
4. Given that x is a real number, describe each of the following sets by using 
the set-buildcr notation and thc absolute value symbol. Represent each of 
the sets on a graph. 
(a) x is between --4 and 4, inclusivc; 
(b) x s greater than 6 or less than 
(c) x is numerically greater than 2; 
(d) x is numerically equal to, or less than, 1; 
(c) x differs from 5 by 2 or lcss; 
(f) the distance from x to 7 is less than 3 units on the number scale. 
5. Describe each of the following intervals with a statement involving the 
word "distance." Represent each interval on a graph. (a) Ixl < 3, (b) Ixl _> 2, 
(c) [xl _< 4, (d) Ixl >_ o, (e) Ix - 51 _< , (f) Ix q- 31 _< x, <g> Ix - 31 >_ 
(h) lx--[_<s. 
6. Below are shown the graphs of a number of sets of real numbers. For each 
graph, specify the set, using the set-builder and any other mathematical symbols 
that you wish. 
+ . + + . 
7 --3 --1 
(c) (d 
--3 --1 --S 5 
((,) 
1 3 -5 5 
g) (h) 
--12 12 k - 3 ], Z +3 
(i) (j) 
^rrmxmx  411 
I-3. UNIVERSAL SET AND SUBSETS 
Ill some contexts, we may wish to restrict our attention to objects that 
belong to some fixed, large set. In plane geometry, for example, this large 
set might be the set of all points in a plane. We could call this totality 
of all points under consideration our "universal set," U. From U, we 
might then select special subsets: for example, the points on a given line 
L, or the points inside a given circle, or the points of intersection of a line 
and a circle, and so on. 
As another example, consider 
U = the set of all automobiles registered in the U.S.A. in 1960, 
A = the set of all automobiles registered in New Jersey in 1960, 
B  the set of all registered automobiles in the U.S.A., and not 
involved in an accident in 1960. 
Sets A and B are subsets of the universal set U. 
Venn diagrams. It is often helpful to have a schematic reprcsentation 
of the universal set and its subsets. One such scheme is known as a Ven 
diagram (Fig. I-7). The rectangle U in the diagram represents the 
versal set U, and the elements of U are represented by the points in the 
rectangle. Sets of elements of U (uch as A and B) are represented by 
the points in circles within the rectangle. See Fig. I-7. 
F. I-7, Uenn Diagram. 
I-2 Definition. ,S',bset. If every element of set A is also an element 
of set B, then we say that A is a subset of B. (See Fig. 1-8.) 
Thus if U is the set of all students, B the set of students in your school, 
and A the set of all students of mathematics in your school, then A is 
a subset of B. 
Fo. I-8. A is a subset of B. 
412 APPENDIX I 
Subsets of a given set. Consider a finite, universal set U. If we know 
the number of elements in U, can we tell how many different subsets U 
has? The answer is "yes," and we shall show a method for finding the 
number of subsets. Let us first illustrate the method for the qase where 
U has four elements, and then extend it to the case where U has any 
finite number n of elements. 
EXAMPLE 1. Let U ---- {a, b, c, d}. How many different subsets has U? 
Solution. The multiplication principle of Section 2-1 provides the 
method we seek. For the number of possible subsets of U is simply the 
number of ways of making a selection from the four elements a, b, c, and 
d. The procedure is as follows. 
We can deal with a in 2 ways (take it into the set or leave it out). Then, 
after dealing with a in either of these ways, we can deal with b in 2 ways 
(take it or leave it). Similarly, we can deal with c in 2 ways, and then 
with d in 2 ways. Therefore, by the multiplication principle, there are 
exactly 
2X2X2X2---- 2 4-- 16 
ways of making a selection from the four elements of U. 
All of these 16 selections except two give rise to readily acceptable sub- 
sets of U. These two deserve special mention: 
(l) The selection in which we take a and b and c and d gives rise to the 
set 
{a, b, c, d}, 
which is identical with U. Shall we call U a subset of itself? Since every 
element of U is an element of U, the definition of subset is satisfied, so 
we agree that U is a subset of itself. 
(2) The selection in which we leave a and b and c and d gives rise to a 
set with no elements. We call such a set the empty set or the null set, 
and denote it by . (This symbol for the empty set is the Greek letter 
phi, pronounced "fie" or "fee. ") By special agreement, we accept the empty 
set as a subset of any set whatever. Why do we adopt such a convention? 
Here are two reasons: 
(1) The empty set plays a role similar to that of zero in the number 
system: if the empty set is adjoined to any set A, the result is A. We 
are saved from making exceptions in stating theorems. 
(2) The convention does not violate the definition, which requires that 
every element of  must belong to any given set B. There are no members 
of  to violate this condition. 
APPEND1X I 413 
--3 Theorem. If U is a finite set having n elements then there are 2  
different subsets of U including U and . 
The proof is left as an exercise for the reader. 
EXERCISES FOR SECTION 1-3 
1. Make up some examples of universal sets and subsets, using sets of people, 
or objects, or ideas. 
2. Let U be the set of fingers (including thumb) on )'our right hand. How 
many different "sets of fingers" can you make from U (a) if at least one finger 
must be included, (b) if the empty set (fingers closed in a fist) is permitted? 
3. Suppose that a code is devised so that one symbol of the form 
(x,xa,...,x20), xxherex, -- 0 or 1, � = 1,2,...20, 
is assigned to a complete message. Can a million different messages be so en- 
coded? What is the exact number of messages possible? 
4. How many nonempty subsets can be formed from a set of n elements? 
5. A proper subset of U is defined to be a subset that does not include the 
entire set U. How many proper subsets can be formed from a set of n elements? 
6. Prove Theorem I-3. 
I-4. OPERATIONS WITH SETS 
Let U denote the universal set, and let A, B, C,... denote subsets of U. 
We can perform on these subsets certain operations that produce other 
(or perhaps the same) subsets. Three particularly important operations 
are intersection, nion, and cornplementation. We shall define these terms, 
and illustrate their meanings with Venn diagrams. 
I-4 Defiifiom Intersection. The intersection of A and B is the set of 
all elements of U that belong to both A and B. 
We denote the intersection of A and B by "A  B" (read: "A intersect 
B," or "A cap B"). In symbols, 
A 91 B ---- I z: x belongs to A and x belongs to B/. 
The intersection of A and B is indicated by the shaded area in Fig. I-9. 
F. I-9. Intersection of A and B. 
414 APPENDIX I 
I-5 Definition. Disjoint; m,utually exclusive. wo sets A and B are 
said to be disjoint, or mutually exclusive, if they have no elements 
in common. 
In other words, A and B are disjoint if their intersection is the empty 
set. (See Fig. 1-10.) 
Fm. 1-10. Disjoint sets. 
I-6 Definition. Union. The union of A and B is the set of all elements 
of U that belong either to A or to B or to both. 
We denote the union of A and B by A U B (read: "A union B," or 
"A cup B"). In symbols, 
A U B = {x :x belongs to A or to B or to both}. 
The union of A and B is indicated by the shaded area in Fig. 1-11. 
Fro. 1-11. Union of A and B. F1G. 1-12. Complement of A. 
I-7 Definition. Complcrnent. The complement of A is the set of all 
elements in U that are not in A. 
We denote the complement of A by  (read. "A bar"). In symbols, 
] = {x :x belongs to U but not to A}. 
The complement of A is indicated by the shaded region in Fig. 1-12. 
PPxmx  415 
ExPL 1. Let U consist of the numbers 1, , 3, ..., 9 and the 6 
letters of the alphabet, a, b, �, ..., z. If 
A = I1, 3, 5, a, e, hl 
and 
B---- /1, 2, 3, 4, 5, a, b, c, d, el, 
find (a) , (b) A AB, (c) A uB, (d) A A. 
Solution. From the foregoing definitions, we have 
B---- {6, 7, 8, 9, f, g, h,..., z}, 
A fB = {1, 3, 5, a, e}, 
A uB = {1, 2, 3, 4, 5, a, b, c, d, e, h}, 
.4 n  = {}. 
ExAm, n 2. Given that U = {0, 1, 2, 3, 4, 5,...}, 
A = {3x 'x is in U}, 
B = {5x'xisin U}. 
Find A  B. 
Solution. In words, "A is the set of all integral multiples of 3," and 
"B is the set of all integral multiples of 5." In order that an element belong 
to both A and B, it must be a multiple of 3 and also a multiple of 5, and 
hence a multiple of 15. Therefore, 
A B = {15x 'xisin U}. 
EXERCISES FOR SECTION 1-4 
1. Let U bc the set of all points in the (x, y)-planc: 
U = { (x, y) � x and y arc rcal numbcrs}. 
Givcn that 
 = {(x, y) � y = Ix[I, B = {(x, y) � y > Id}, 
L = {(x,y):x+y = 2}, .ll = {(x,y).x+y< 2}. 
Indicate, by graphs, the following sets: (a) .t, (b) B, (�) A U B, (d) , 
(c) L, (f) A FIL, (g) BFI M, (h) 27, (i) E. 
2. If A is any subset of the universal set U, prove that (a) A U A = A, 
(b) .t F A = A, (c) .4_ U- = U, (d) A F X =,;t,. 
416 APPENDIX 
3. Given that A and B are subsets of a finite universal set U, and that the 
numbers of elements in various sets are as recorded in the first four rows of 
the following table. Make a Venn diagram to illustrate the given data, indicating 
members of sets by dots in your diagram. Then complete the table by filling 
in the number of elements for each of the last five sets. 
Set U A B AFqB Af'qB AFqB AUB AUB 
No. of 
elements 20 3 
For those who may wish to undertake further study of sets, the fol- 
lowing reference list is appended: 
Aiken and Beseman, Modern Mathematics: Topics and Problems, pp. 1-127. 
New York: McGraw-Hill Book Co. 
Breuer, J. (translated by H. F. Fehr), Introduction to the Theory of Sets. 
Englewood Cliffs, N.J.: Prentice-Hall, Inc. 
Christian, R., Introduction to Logic and Sets. Boston: Ginn & Co. 
Committee on the Undergraduate Program, Mathematical Association of 
America, Elementary Mathemattcs of Sets with Applications. New Orleans: 
Tulane University Book Store. 
Halmos, P. R., Naive.Set Theory. Princeton, N.J.: D. Van Nostrand Co. 
Kemeny, J. G., J. L. Snell, and G. L. Thompson, Introduction to Finite Mathe- 
matics, Ch. 2. Englewood Cliffs, N.J.: Prentice-Hall, Inc. 
May, K. 0., Elements of Modern Mathematics, Ch. 3. Rcading, Mass.: Addison- 
Wesley Publishing Co., Inc. 
McShane, E. J., Insights into Modern Mathematics, Ch. 3 ("Operating with 
Sets"). National Council of Teachers of Mathematics, 1201 Sixteenth St., N. W., 
Washington 6, D.C. 
Woodward, E. J., and R. C. McLcnnan, Elementary Concepts of Sets. New 
York: Henry Holt and Co. 
APPENDIX 
II 
SUMMATIONS 
AND SUBSCRIPTS 
I1-1. SUBSCRIPTS AND THE SUMMATION SYMBOL,  
We often wish to indicate the sum of several measurements or ob- 
servations. For example, if 30 students take a test, we may wish to know 
their average score, which is o the sum of their scores. Or we may wish 
to talk about the sum of the points on the top face of a die thrown many 
times. It is convenient to be able to express such sums in compact form. 
The Greek letter  (capital sigma) is used for this purpose, to denote 
"summation of." 
Suppose, for example, we arrange the names of the 30 students in 
alphabetical order, and then let Xl represent the test score of the first 
student, x2 the score of the second student, and so on, with za0 repre- 
senting the score of the 30th student. The subscripts 1, 2,..., 30 cor- 
respond to the positions of the students' names on the alphabetical list. 
If the first 3 students received scores of 85, 79, and 94, in that order, then 
xl: 85, x2 ---- 79, xa ---- 94. 
The sum of the 30 scores could be represented by 
+ x2 + '" + za0, (1) 
where the three dots are used to indicate "and so on." Another way of 
representing the same sum, using the summation symbol , is 
30 
(2) 
i=1 
We read expression (2): "summation of x-sub-/from i ---- 1 through 30." 
It has exactly the same meaning as expression (1); both indicate the 
417 
418 APPENDIX II 
sum of the thirty scores Xl, x2, and so on through Xao. In other words, the 
symbol 
3O 
means that we are to replace i by integers in ascending order, beginning 
at 1 and ending at 30, and add the results. 
The subscript may be any convenient letter, although i, j, It, and n 
are most frequently used. 
EXAMPLE 1. If Xl ---- --3, X2 ---- 5, X3 ---- 7, and x4 = 6, find 
4 4 3 
*=1 i=2 21 
4 3 4 
Solutions. 
4 
(a) Z x, = .T 1 -[-'X 2 -[- X 3 -[- .T 4 = --3 + 5 + 7 + 6 ---- 15. 
4 
(b) x, = x2d-xad-x4 = 5d-7d-6---- 18. 
3 
4 
(d)  5x = 5271 - 5,1.? 2 - 503 3 - 5.l' 4 
= 5(5) = 75. 
= 9 + 18 = 27. 
APPENDIX II 419 
4 
(f) ]  = . + .; + . + . = (-) + s  + 7  + 6  = o. 
REMARK 1. Part (b) of this example illustrates a sum froin i = 2 
through i = 4. The equation "i = 2" written beneath the summation 
sign tells us where the sum starts. The subscript i on .r, is first to be 
replaced by 2. We then proceed through the integers from the starting 
place (in this case, 2) until we reach the integer corresponding to the 
symbol written above the summation sign (here, 4). Thus, in x, we 
replace i by 2, 3, and 4, and add the results. 
x + x3 + x4. 
REMARK 2. In part (c) we used the letter j, instead of i, for the sub- 
script on x, and for the corresponding index of summation. The notation 
''  1" beneath the sigma tells us the first value to substitute for j, and 
this substitution converts x i into Xl. We then proceed one-by-one through 
the integers until we reach the upper limit of summation, in this case 3. 
Then we add the results, and get 
Xl + x + x. 
lEMARK 3. In Example l(d), xve replace the subscript h by 1, 2, 3, and 
4, in that order, but we have the common factor 5 in each term. In fact, 
we see that 
4 4 
kl kl 
and this result can easily be generalized. We shall do so in the next section. 
REMARK 4. In part (e), the index of summation n takes the values 
1, 2, and 3. The subscript on x takes these same values, but the subscript 
on x+ 1 takes the values n  1 = 2, 3, and 4, in that order. By rearrang- 
ing terms, we also see that 
3 3 3 
n:l n:l 
3 4 
= + 
420 APPENDIX II 
REM.RI 5. Equation (3),and Examples l(c,'d, e) illustrate that the 
letter used for the index of summation is immaterial. This also explains 
why that index is often called a "dummy index." The only requirement is 
that when the index is everywhere replaced by the consecutive integers, 
beginning with the lower hmit of summation (written beneath'the sigma) 
and extending through the upper limit of summation (written above the 
sigma), we get the desired result by adding these expressions. Thus 
4 1 1 
j2 k=--I 
I1-1 Definitions. Summation. With each integer i from m through n, 
let there be associated a number denoted by 92,. The sum of the 
numbers 
,Tin, 92m-1-1, �.., 92n 
n 
is represented in summation notation by Z xl. 
I 
Limits of summation. In Eq. (4) the lower limit of summation is 
ra, the upper limit is n. 
The omission of limits of summation We sometimes omit the linfits 
and write simply 92. This notation means that the summation is to extend 
oer all values of x under discussion, unless something is said to the con- 
trary. For instance, if the only values in a particular discussion are 
Xl, .C2, 923, X4, then x means x q- x2 q- xa q- 2'4. 
5 
ExPrE 2. If x = i(i -- 1), evaluate  x. 
5 
---- 1(1 -- 1) q- 2(2-- 1) q- 3(3-- 1) q- 4(4-- 1) 
+ 5(5 - 
= 0+2-[-0-[ 2+2o= 40. 
421 
2 jq-1 
EXAMPLE 3. Evaluate  j 4- 3 ' 
2 
Solution. 
3=oj-1-$-- 0q_ 3 -lq_3 -] 2_1_3-- 3-1--1-------- 
Ex_tPLE 4. Express the following sum as a simple function of : 
n 
 [� + 1) -o - 71. 
Solution. Replacing k by O. 1, 9 n and adding, we get 
[(+1)o_o]=[1 _0o]+[2 _ 1 ]+[3_2 ] 
k=0 
+... + [( + ) - n2]. (6) 
The positive terms on the right side of Eq. (6) have a sum expressed by 
12 + 2 2 + 3 2 + ... + (n + 1) 2 , (7) 
and from this we must subtract 
0 2 + 12 + 2 2 + -.. + n 2. (8) 
Therefore 
n 
E [(]C- 1) 2 -- ]C 2] : [12 -[- 2 2 -''' - n 2 - (z-q- 1) 2 ] 
/c=o __ [0 2 -[- 12 -}- ' ' ' _}_ n2 ] 
= ( + 1) . 
EXERCISES FOR SECTION II-1 
Evaluate the following sums: 
3 3 3 
2 3. Ex 
.En 
nl n=l n=l 
2 3 4 
k 0 k=O k=O 
422 APPENDIX II 
3 2 102 
7. (i2q-i) 8: Z (n2 -- 4) 9. Z n 
1 n--2 n=100 
2 
10. ]] (n q- 100) 
n0 
11. Use the result of Eq. (9) and the fact that 
(kq- 1) 2 -- k 2 = 2kq- 1 
is an odd integer to prove that the sum of the first n q- I positive odd integers 
is a perfect square. What square? 
12. By expanding the left side of the following equation and rearranging 
terms, show that 
3 3 3 
k:0 k=0 k:0 
Can you generalize this result in two ways? 
13. By expanding the left side of the following equation and using the dis- 
tributive law, show that 
3 3 
Z7x, = 7x. 
Generalize the result in as many ways as you can. 
14. If all the x's are equal to the same constant c, what is the value of 
11-2. THEOREMS ABOUT SUMMATIONS 
In Exercise 12 above you probably discovered the result stated in the 
folloxving theorem. 
11-2. Theorem The summation of the sum of two or more variables is 
the sum of their summations. Thus, 
n n n 
Z (a q- b) ---- Z aiq- Z bl. (1) 
Proof. To prove Eq. (1), we need only expand the left side and rearrange 
terms, as follows: 
APPENDIX II 423 
 (as q- b,) = (a, q- b,) q- (a,+ q- b+) q- ... q- (a,, q- b) 
= 
The result can be extended to the summation of three or more variables 
by repeated applications of Eq. (1). For example, 
where i goes from m to n in all summations. [] 
11-3 Theorem. A constant factor can be moved across the summation 
sign. Thus, if c is a constant, 
Proof. We expand the left side of Eq. (2) and get 
cx,,, q- cx,,+ q- ... q- 
which can also be written in the form of the right side of Eq. (2). [] 
3 3 
EXAMrLE 1.  2i 2 ---- 2  i 2: 2(1 q- 4 q- 9) = 28, 
i=1 i=1 
3 3 
 (2i) 2 = 4i 2 = 4(1 q-4q-9) = 56. 
11-4 Theorem. The summation of a constant is equal to the product 
of that constant and the number of integers from the lower 
limit of summation through the upper limit. Thus 
424 APPENDIX II 
Proof. If eachx, is equal to c, fori= 1,2,...,n, then 
n 
 xi= ., + x2 +... + x = c + c + ... + c = cn. [] 
EXAPI.E 2. Use Eq. (9) of Section II-1 and Theorems II-2, II-3, 
n 
II-4, to evaluate ]] k as a function of n. 
Solution. From Eq. (9), we have 
n 
 [q + l)  - ] = (n + l) . (4) 
:0 
But also, 
(k+ 1) -- 1  = k+2k+ 1 -- k  = 2k+ 1. 
Reversing the order in Eq. (4) and substituting 2k + 1 for the difference 
of squares, we get 
(n +  =  (s +  =  s +   = s   + (n + . 
k0 k0 k0 k0 
To find the value of , we subtract (n + 1) and get, after reversing the 
order of the first and last terms, 
n 
2 k = (n+ 1)  -- (n+ 1) = (n+ 1)[(n+ 1) -- 1] = (n+ 1)n. 
k=0 
Therefore 
n 
  = (n + )n 
 � (6) 
EXERCISES FOR SECTION II-2 
Write the following summations in expanded form and simplify the results 
as much as possible. 
3 3 2 
1.  ka 2.  k2(�)/ 3.  10  
k=O k:l :--1 
xpPxr>x n 425 
5 3 2 
4.  (2i -- 5) 5.  (a, q- b,_) 6.  (2x -- 3y,) 
2 nl nO 
7. x 8. x y 
k0 k=0 
 � x 
9. P q- 10. 
x0 
11. Show that 
[( + 1)  _ ] 
is equal to (n + 1)  
12. (Continuation.) Use the result of Exercise 11 above and the relation 
(k+ 1)  -- k  = 3k 2+3k+ l to show that 
k =0 k =0 k=0 
13. (Continuation.) Use the results of Exercise 12 and Eqs. (3) and (6) in 
the text to show that 
 k2 n(n q- 1)(2n q- 1) 
= 6 (7) 
14. How would the results in Eq. (6) and in Exercise 13 be affected if the 
lower limits of sumnmtion were changed to k = 17 Explain. 
15. Write each of the following sums in summation form' 
(a) z + z2 + ... + z2a. 
(b) xiyi q- x2y2 q- ''' q- xsYs. 
(C) (Xl -- Yl) + (X2 -- Y2) --''' + (Xm -- Ym). 
3 
(d) xf -- x23 f2 + ... - x,f9. 
16. Prove that (x, -- m) 2 = x, -- 2m x, q- nm. 
17. In Exercise 16, suppose that ra = , the arithmetic mean of x, x2,..., 
Prove that 
1 1 
18. Express 
 (ax, q- b!/,) 2 
as a sum of three summations. 
ReMAn}r. We sometimes have to deal with sums of the form Yx,!lj, where 
the sum s to be extended over certain pairs of values of i and j. Suppose, for 
example, that i goes from 1 through 3, and j takes the values 1 and 2. Then there 
are 3 X 2, or 6, pairs of i, j values, and the corresponding summation is 
X2yj = Xl/1 q- XlY2 q- X2Yl q- X2/2 q- X3]l q- x3Y2. 
APPENDIX 
III 
A THEOREM 
ON INDEPENDENCE 
The purpose of this appendix is to prove the following theorem. The 
result is used in proving that the variance of the sum of three or more 
independent random variables is the sum of their variances. 
Ill-] Theorem. Let X, Y, and Z be independent random variables 
whose possible values are 
.T1, .T2, . . . , Xm, /1, /2, � � � , /n, Zl, 2, � � � , t. 
If U ---- X - Y, then U and Z are independent. 
Proof. We must show that if u is any of the possible values of 
then 
P(U = u, Z = zk) = P(U-- u). P(Z = zk), (1) 
where z is any one of the possible values of Z. Table III-1 shows the 
possible ordered pairs (xl, Yi). By entering one more symbol, say Zl, in 
each cell, we can represent all the ordered triples (x, yj, z), with the value 
of Z fixed as zx. Another table like Table III-1 with z2 added in each 
cell can be used to represent the ordered triples (x, Yi, z2), corresponding 
to the value z for z. And so on for the remaining values z3,..., zt, 
using a separate table for each value of z. If these t different tables 
were printed on different sheets of paper, one per sheet, and the sheets 
of paper were then stacked on top of one another, the three-dimensional 
stack of tables would represent all possible ordered triples (x, Y3, z). 
The cells of Table III-1 resemble the squares of a chessboard, and the 
totality of cells in the stack of tables resembles a three-dimensional 
chessboard. 
Now suppose, temporarily, that Table III-1 has a zx written in every 
cell, so that the entries are converted from (x, Yi) into (x, y3, zx). Since 
427 
428 APPENDIX III 
TABLE III-1. , PAIRS OF VALUES 
Values of yy 
Yl Y2 � � � Y 
Xl (Xi, Yl) (Xl, Y2) � .. (Xl, Yn) 
Values 
x2 (x2, yl) (x, y2) ... (x2, y) 
of x 
Xm (Xm, Yl) (Xm, Y2) ... ] (Xm, Yn) 
X, Y, and Z are independent, the corresponding probability of the cell is 
P(xz) � P(y.) � P(Zl). (2) 
To compute the probability P(U: u,Z = Zl), the left member of 
Eq. (1) for k = 1, we must add those probabilities given by the product 
(2) over all cells having xi q- yj = u: 
P(U : .u, Z = Zl) = EP(i) ' P(Y.) ' P(Zl), (3) 
where the summation runs over those pairs (xi, y) with xi q- y = u. 
Since P(Zx) remains constant for all terms in this summation, we get 
P(U ---- Re, Z---- Zl) ---- P(Zl)YP(xi) . P(yj), (4) 
where the summation in Eq. (4) also runs over those pairs with sum 
x q- y = u. This sum is exactly equal to the probability that X q- Y, 
or U, is equal to the particular value u. Therefore Eq. (4) gives the result 
P(V  it, Z---- Zl)  P(Zl)'P(l/). (5) 
There is nothing special about Zl in Eqs. (2) through (5) so far as the 
proof is concerncal. We could equally well repeat the argument for z2, 
or z3, or any othcr value of zk. Thus wc have 
P(U =., Z = zk) = P(z) . P(.), 
which establishcs the result that U and Z arc indepcndcnt. 
By a slightly more gcncral argulnent, we can extend the theorem to 
more than three indcpendcnt randoln variables: any one of them is inde- 
pcndcnt of thc sum of thc others. 
TABLES 
430 
TABLE I 
2500 RANDOM DIGITS 
00 49487 52802 28667 62058 87822 14704 18519 17889 45869 14454 
01 29480 91539 46317 84803 86056 62812 33584 70391 77749 64906 
02 25252 97738 23901 11106 86864 55808 22557 23214 15021 54268 
03 02431 42193 96960 19620 29188 05863 92900 06836 13433 21709 
04 69414 89353 70724 67893 23218 72452 03095 68333 13751 37260 
05 77285 35179 92042 67581 67673 68374 71115 98166 43352 06414 
06 52852 11444 71868 34534 69124 02760 06406 95234 87995 78560 
07 98740 98054 30195 09891 18453 79464 01156 95522 06884 55073 
08 85022 58736 12138 35146 62085 36170 25433 80787 96496 40579 
09 17778 03840 21636 56269 08149 19001 67367 13138 02400 89515 
10 81833 93449 57781 94621 90998 37561 59688 93299 27726 82167 
11 63789 54958 33167 10909 40343 81023 61590 44474 39810 10305 
12 61840 81740 60986 12498 71546 42249 13812 59902 27864 21809 
13 42243 10153 20891 90883 15782 98167 86837 99166 92143 82441 
14 45236 09129 53031 12260 01278 14404 40969 33419 14188 69557 
15 40338 42477 78804 36272 72053 07958 67158 60979 79891 92409 
16 54040 71253 88789 98203 54999 96564 00789 68879 47134 83941 
17 49158 20908 44859 29089 76130 51442 34453 98590 37353 61137 
18 80958 03808 83655 18415 96563 43582 82207 53322 30419 64435 
19 07636 04876 61063 57571 69434 14965 20911 73162 33576 52839 
20 37227 80750 08261 97048 60438 75053 05939 34414 16685 32103 
21 99460 45915 45637 41353 35335 69087 57536 68418 10247 93253 
22 60248 75845 37296 33783 42393 28185 31880 00241 31642 37526 
23 95076 79089 87380 28982 97750 82221 35584 27444 85793 69755 
24 20944 97852 26586 32796 51513 47475 48621 20067 88975 39506 
25 30458 49207 62358 41532 30057 53017 10375 97204 98675 77634 
26 38905 91282 79309 49022 17405 18830 09186 07629 01785 78317 
27 96545 15638 90114 93730 13741 70177 49175 42113 21600 69625 
28 21944 28328 00692 89164 96025 01383 50252 67044 70596 58266 
29 36910 71928 63327 00980 32154 46006 62289 28079 03076 15619 
30 48745 47626 28856 28382 60639 51370 70091 58261 70135 88259 
31 32519 91993 59374 83994 59873 51217 62806 20028 26545 16820 
32 75757 12965 29285 11481 31744 41754 24428 81819 02354 37895 
33 07911 97756 89561 27464 25133 50026 16436 75846 83718 08533 
34 89887 03328 76911 93168 56236 39056 67905 94933 05456 52347 
35 30543 99488 75363 94187 32885 23887 10872 22793 26232 87356 
36 68442 55201 33946 42495 28384 89889 50278 91985 58185 19124 
37 22403 56698 88524 13692 55012 25343 76391 48029 72278 58586 
38 70701 36907 51242 52083 43126 90379 60380 98513 85596 16528 
39 69804 96122 42342 28467 79037 13218 63510 09071 52438 25840 
40 65806 22398 19470 63653 27055 02606 43347 65384 02613 81668 
41 43902 53070 54319 19347 59506 75440 90826 53652 92382 67623 
42 49145 71587 14273 62440 15770 03281 58124 09533 43722 03856 
43 47363 36295 62126 42358 20322 82000 52830 93540 13284 96496 
44 26244 87033 90247 79131 38773 67687 45541 54976 17508 18367 
45 72875 39496 06385 48458 30545 74383 22814 36752 10707 48774 
46 09065 16283 61398 08288 00708 21516 39615 03102 02834 04116 
47 68256 51225 92645 77747 33104 81206 00112 53445 04212 58476 
48 38744 81018 41909 70458 72459 66136 97266 26490 10877 45022 
49 44375 19619 35750 59924 82429 90288 61064 26489 87001 84273 
Rcl)rintcd by p('rnfission of tile I)ublishcr, Tile Free Press of Glencoo, 
illinois, from A Million, Random Dfgits with I00,000 Normal Dcvfates, copyright 
1955, 1)3,The Rand Corporation. 
431 
TABLE II 
VALUES OF ?t! AND log n! 
The values of n! are given to five significant figures, and for n >_ 9 these 
values must be multiplied by a power of ten. This power is the raised number to 
the right of the five significant figures. For example, 15!  13,077 X 108. 
n nl log n! n n! log n! n nl log n! 
I I 00000 26 40,329  26 60562 51 15,511  66.19065 
2 2 .30103 27 10,889  28 03698 52 80,658 s 67 90665 
3 6 .77815 28 30,489  29 48414 53 42,749  69 63092 
4 24 1 38021 29 88,418  30 94654 54 23,084 7 71 36332 
I 
5 120 2.07918 30 26,525 s 32 42366 I 55 12,696 9 73.10368 
6 720 2 85733 31 82,228 9 33 91502 56 71,10070 74 85187 
7 5,040 3 70243 32 26,313 1 35 42017 57 40,5277 76 60774 
8 40,320 4 60552 33 86,833  36 93869 58 23,506 TM 78 37117 
9 36,2881 5.55976 34 29,523 a4 38.47016 59 13,8687 80 14202 
10 36,288  6 55976 35 10,333 a 40.01423 60 83,21077 81 92017 
11 39,917 a 7 60116 36 37,199 a7 41.57054 61 50,758 TM 83.70550 
12 47,900 i 8 68034 37 13,764 ao 43 13874 62 31,470 sl 85 49790 
13 62,270  9.79428 38 52,30240 44 71852 63 19,826 s 87 29724 
14 87,178 e 10 94041 39 20,3984 46. 30959 64 12,689 s 89. 10342 
15 13,077 s 12 11650 40 81,5924 47.91165 65 82,477 s 90 91633 
16 20,923  13.32062 41 33,4534 49 52443 66 54,434 ss 92.73587 
17 35,569 l� 14.55107 42 14,05047 51 14768 67 36,471 � 94 56195 
18 64,024 TM 15.80634 43 60,4154s 52 78115 68 24,800  96.39446 
19 12,1651 17 08509 44 26,583 � 54.42460 69 17,112 4 98.23331 
20 24,329 TM 18.38612 45 11,962  56 07781 70 11,979  100.07841 
21 51,0911 19 70834 46 55,026  57.74057 71 85,048 7 101 92966 
22 11,24017 21 05077 47 25,862  59.41267 72 61,234  103.78700 
23 25,852 is 22 41249 48 12,414 7 61.09391 73 44,7011�1 105 65032 
24 62,045 lo 23 79271 49 60,828 s 62.78410 74 33,079 TM 107.51955 
25 15,511 1 25.19065 50 30,414 � 64.48307 75 24,8091� 109 39461 
432 
� TABLE III 
NOM^L CURVE 
Area under the standard normal curve from 0 to z, shmvn shaded, is A(z). 
Examples. If Z is the standard 
normal random variable and z = 1.54, 
then 
A(z) = P(O < Z < z) = 4382, 
P(Z > z) = .0618 
P(Z < z) = .9382, 
P(IZI < z) = .8764 ,, z 
z 00 01 02 03 04 05 06 07 08 ] 09 
0 0 0000 0040 0080 0120 0160 0199 0239 0279 0319 0359 
0 1 0398 0438 0478 0517 0557 0596 0636 0675 0714 0753 
0 2 0793 0832 0871 0910 0948 0987 1026 1064 1103 1141 
0.3 1179 1217 1255 1293 1331 1368 1406 1443 .1480 1517 
0 4 1554 .1591 1628 1664 1700 1736 1772 1808 1844 1879 
0 5 1915 .1950 1985 2019 2054 2088 2123 2157 2190 2224 
0 6 2257 2291 2324 2357 2389 2422 2454 2486 2517 2549 
0 7 2580 2611 2642 2673 2704 2734 2764 2794 2823 2852 
0 8 2881 2910 2939 2967 2995 3023 3051 3078 3106 3133 
0 9 3159 3186 3212 3238 3264 3289 3315 3340 3365 3389 
1 0 3413 3438 3461 3485 3508 3531 3554' 3577 3599 3621 
1 1 3643 3665 3686 3708 3729 3749 3770 3790 3810 3830 
1 2 3849 3869 3888 3907 3925 3944 3962 3980 3997 4015 
1 3 4032 4049 4066 4082 4099 4115 4131 4147 4162 4177 
1 4 4192 .4207 4222 4236 4251 4265 4279 4292 4306 4319 
1 5 4332 4345 4357 4370 4382 4394 4406 i 4418 4429 4441 
1 6 4452 4463 4474 4484 4495 4505 4515 4525 4535 4545 
1 7 4554 .4564 .4573 4582 4591 4599 4608 4616 4625 4633 
1 8 4641 4649 4656 4664 4671 4678 4686 4693 4699 4706 
1 9 4713 4719 4726 4732, 4738 4744 4750 4756 4761 4767 
2 0 4772 4778 4783 .4788 4793 4798 4803 4808 .4812 4817 
2 1 4821 4826 4830 4834 4838 4842 4846 4850 4854 4857 
2 2 .4861 4864 4868 4871 4875 4878 4881 4884 4887 4890 
2.3 4893 4896 4898 4901 4904 4906 4909 4911 4913 4916 
2 4 4918 4920 4922 .4925 4927 4929 4931 4932 4934 4936 
2 5 4938 4940 4941 4943 4945 4946 4948 4949 4951 4952 
2 6 4953 4955 4956 ,1957 4959 4960 4961 4962 4963 4964 
2 7 4965 4966 4967 4968 .4969 4970 4971 4972 4973 4974 
2 8 4974 4975 .4976 4977 4977 4978 4979 4979 4980 4981 
2 9 4981 .4982 4982 4983 4984 4984 4985 4985 4986 4986 
3 0 [ 4987[ 4987[ 4987 4988, 4988 4989 4989 4989 4990[ 4990 
TABLE IV 
THREE-PLACE TABLES OF THE BINOMIAL DISTRIBUTION 
Part A of this table givcs the vtlucs of the function 
b(.r;n, p)= (;)p(1- p)- 
This is the probability of cxactly x successes iu n idepcndcnt binomial t'ials 
with probabihty of success on a single t'iat equal to p. 
Part I{ gives the values of the cumulative binomial 
P(A'  r) is the probability of r or more successes in n independent biomial 
trials  ith probability p of success on a shgle trial. 
In both parts of the table values of the functions arc given for x (or r) = 0, 
.90, .95, and .99. 
In th(sc tables, each three-digit entry should be read with a decimal preceding 
it. For entries 1--, the probability is larger tha 0.9995 but less than 1. For 
ctrh's 0+, the probabihty is less tha 0.0005 but greater than 0. 
TABLE IV 435 
]:)ART A: INDIVIDUAL TERMS, b(X; n, p) 
p 
n x .01 05 .10 20 .30 .40 .50 60 .70 .80 .90 95 .99 x 
2 0 980 902 810 640 490 360 250 160 090 040 010 002 o+ 0 
! 020 095 180 320 420 480 500 480 420 320 18o 095 020 1 
2 o+ 002 010 040 090 160 250 360 490 640 810 902 980 2 
3 0 970 857 729 512 343 216 125 064 027 008 001 o+ o+ 0 
I 029 135 243 384 441 432 375 288 189 096 027 007 o+ 1 
2 o+ 007 027 096 189 288 375 432 441 384 243 135 029 2 
3 o+ o+ 001 008 027 064 125 216 343 512 729 857 970 3 
4 0 961 815 656 410 240 130 062 026 008 002 o+ o+ o+ 0 
I 039 171 292 410 412 346 250 154 076 026 004 o+ o+ 1 
2 0Ol 014 049 154 265 346 375 346 265 154 049 014 001 2 
3 o+ o+ 004 026 076 154 250 346 412 410 292 171 039 3 
4 o+ o+ o+ 002 008 026 062 130 240 410 656 815 961 4 
5 0 951 774 590 328 168 078 031 010 002 o+ o+ o+ o+ 0 
i 048 204 328 410 360 259 156 077 028 006 o+ o+ o+ 1 
2, 0Ol 021 073 205 309 346 312 230 132 051 008 001 o+ 2 
3 o+ 001 008 051 132 230 312 346 309 205 073 021 001 3 
4 o+ o+ o+ 006 028 077 156 259 360 410 328 204 048 4 
5 o+ o+ o+ o+ 002 010 031 078 168 328 590 774 951 5 
6 0 941 735 531 262 118 047 016 004 001 o+ o+ o+ o+ 0 
I 057 232 354 393 303 187 094 037 010 002 o+ o+ o+ 1 
2 0Ol 031 098 246 324 311 234 138 060 015 001 o+ o+ 2 
3 o+ 002 015 082 185 276 312 276 185 082 015 002 o+ 3 
4, o+ o+ 001 015 060 138 234 311 324 246 098 031 001 4 
5 0-]- o+ o+ 002 010 037 094 187 303 393 354 232 057 5 
6 o+ o+ o+ o+ 001 004 016 047 118 262 531 735 941 6 
7 0 932 698 478 210 082 028 008 002 o+ o+ o+ o+ o+ 0 
I 066 257 372 367 247 131 055 017 004 o+ o+ o+ o+ 1 
2 002 041 124 275 318 261 164 077 025 004 o+ o+ o+ 2 
3 o+ 004 023 115 227 290 273 194 097 029 003 o+ o+ 3 
4 o+ o+ 003 029 097 194 273 290 227 115 023 004 o+ 4 
5 o+ o+ o+ 004 025 077 164 261 318 275 124 041 002 5 
6 o+ o+ o+ o+ 004 017 055 131 247 367 372 257 066 6 
7 o+ o+ o+ o+ o+ 002 008 028 082 210 478 698 932 7 
8 0 923 663 430 168 058 017 004 0Ol o+ o+ o+ o+ o+ 0 
I 075 279 383 336 198 090 031 008 001 o+ o+ o+ o+ 1 
2 003 051 149 294 296 209 109 041 010 001 o+ o+ o+ 2 
3 o+ 005 033 147 254 279 219 124 947 009 o+ o+ o+ 3 
4 o+ o+ 005 046 136 232 273 232 136 046 005 o+ o+ 4 
5 o+ o+ o+ 009 047 124 219 279 254 147 033 005 o+ 5 
6 0-]- o+ o+ ool 010 041 109 209 296 294 149 051 003 6 
7 o+ o+ o+ o+ 001 008 031 090 198 336 383 279 075 7 
8 o+ o+ o+ o+ o+ 001 004 017 058 168 430 663 923 , 8 
436 mAE v 
PART A: INDIVIDUAL TERMS, b(:c; n, p) 
p 
n x 01 05 10 20 30 40 50 60 70 80 90 95 .99 x 
9 0 914 630 387 134 040 010 002 0-]- 0-]- 0-]- 0-]- 0-]- 0+ 0 
I 083 299 387 302 156 060 018 004 0-]- 0-]- 0-]- 0-]- 0-]- 1 
2 003 063 172 302 267 161 070 021 004 0-]- 0-]- 0-]- 0+ 2 
3 0+ 008 045 176 267 251 164 074 021 003 0+ 0+ 0+ 3 
4 0-]- 001 007 066 172 251 246 167 074 017 001 0-]- 0+ 4 
5 0-]- 0-]- 001 017 074 167 246 251 172 066 007 001 0+ 5 
6 0-]- 0-]- 0+ 003 021 074 164 251 267 176 045 008 0+ 6 
7' 0+ 0+ 0+ 0+ 004 021 070 161 267 302 172 063 003 7 
8 0-]- 0-]- 0+ 0+ 0+ 004 018 060 156 302 387 299 083 8 
9 0+ 0-]- 0-]- 0-]- 0-]- 0-]- 002 010 040 134 387 630 914 9 
10 0 904 599 349 107 028 006 001 0-]- 0+ 0+ 0-]- 0-]- 0-]- 0 
1 091 315 387 268 121 040 010 002 0-]- 0+ 0-]- 0+ 0-]- 1 
2 004 075 194 302 233 121 044 011 001 0+ 0-]- 0-]- 04- 2 
3 0+ 010 057 201 267 215 117 042 009 001 0-]- 0-]- 0-]- 3 
4 0+ 001 011 088 200 251 205 111 037 006 0+ 0-]- 0-]- 4 
5 0+ 0-]- 001 026 103 201 246 201 103 026 001 0+ 0-]- 5 
6 0+ 0+ 0-]- 006 037 111 205 251 200 088 011 001 0-]- 6 
7 0-]- 0+ 0-]- 001 009 042 117 215 267 201 057 010 0+ 7 
8 0-]- 0-]- 0-]- 0+ 001 011 044 121 233 302 194 075 004 8 
9: 0+ 0+ 0-]- 0-]- 0-]- 002 010 040 121 268 387 315 091 9 
10' 0-]- 0+ 0+ 0+ 0+ 0+ 001 006 028 107 349 599 904 10 
11 0 895 569 314 086 020 004 0-]- 0-]- 0+ 0-]- 0-]- 0-]- 0-]- 0 
I 099 329 384 236 093 027 005 001 0+ 0-]- 0-]- 0-]- 0-]- 1 
2 005 087 213 295 200 089 027 005 001 0-]- 0-]- 0-]- 04- 2 
3 0+ 014 071 221 257 177 081 023 004 0+ 0+ 0+ 0+ 3 
4 ' 0-]- 001 016 111 220 236 161 070 017 002 0+ 0+ 0+ 4 
5 0+ 0+ 002 039 132 221 226 147 057 010 0+ 0+ 0+ 5 
6 0+ 0+ 0+ 010 057 147 226 221 132 039 002 0+ 0+ 6 
7 0+ 0+ 0+ 002 017 070 161 236 220 111 016 001 0+ 7 
8 0+ 0+ 0-]- 0+ 004 023 081 177 257 221 071 014 0-]- 8 
9 0+ 0-]- 0-]- 0-]- 001 005 027 089 200 295 213 087 005 9 
10 0-]- 0-]- 0-]- 0-]- 0+ 001 005 027 093 236 384 329 099 10 
11 0+ 0+ 0- 0+ 0+ 0+ 0+ 004 020 086 314 569 895 11 
12 0 886 540 282 069 014 002 0-]- 0-]- 0-]- 0-]- 0-]- 0-]- 0-]- 0 
1 107 341 377 206 071 017 003 0+ 0+ 0+ 0+ 0+ 0+ 1 
2 006 099 230 283 168 064 016 002 0-]- 0-]- 0-]- 0-]- 0+ 2 
3 0-]- 017 085 236 240 142 054 012 001 0-]- 0-]- 0+ 0-]- 3 
4 0-]- 002 021 133 231 213 121 042 008 001 0- 0+ 0-]- 4 
5 0-]- 0-]- 004 053 158 227 193 101 029 003 0-]- 0-]- 0-]- 5 
6 0+ 0+ 0+ 016 079 177 226 177 079 016 0+ 0+ 0+ 6 
7 0+ 0+ 0+ 003 029 101 193 227 158 053 004 0+ 0+ 7 
8 0-]- 0-]- 0+ 001 008 042 121 213 231 133 021 002 0-]- 8 
9 0+ 0+ 0-]- 0-]- 001 012 054 142 240 236 085 017 0-]- 9 
T.nLE 437 
P.',RT A: I'DWm:X *EmS, p) 
p 
n x Ol 05 10 20 30 40 50 60 70 80 90 95 99 
12 10 O+ O+ O+ O+ O+ 002 016 064 168 283 230 099 006 
11 O+ O+ O+ O+ O+ O+ 003 017 071 206 377 341 107 
12 O+ O+ O+ O+ O+ O+ O+ 002 014 069 282 540 886 
13 0 878 513 254 055 01o 001 O+ O+ O+ O+ O+ O+ O+ 
i 115 351 367 179 054 011 002 O+ O+ O+ O+ o+ O+ 
2 007 111 245 268 139 045 010 001 O+ O+ O+ O+ O+ 
3 O+ 021 100 246 218 111 035 006 001 O+ O+ O+ O+ 
4 0+ 003 028 154 234 184 087 024 003 0+ 0+ 0+ 0+ 
5 0+ 0+ 006 069 180 221 157 066 014 001 0+ 0+ 0+ 
6 0+ 0+ 001 023 103 197 209 131 044 006 0+ 0+ 0+ 
7 0+ 0+ 0+ 006 044 131 209 197 103 023 001 0+ 0+ 
8 0+ 0+ 0+ 001 014 066 157 221 180 069 006 0+ 0+ 
9 0+ 0+ 0+ 0+ 003 024 087 184 234 154 028 003 0+ 
10 0+ 0+ 0+ 0+ 001 006 035 111 218 246 100 021 0+ 
11 0+ 0+ 0+ 0+ 0+ 001 010 045 139 268 245 111 007 
12 0+ 0+ 0+ 0+ 0+ 0+ 002 011 054 179 367 351 115 
13 0+ 0+ 0+ 0+ 0+ 0+ 0+ 001 010 055 254 513 878 
14 0 869 488 229 044 007 001 0+ 0+ 0+ 0+ 0+ 0+ 0+ 0 
1 123 359 356 154 041 007 001 0+ 0+ 0+ 0+ 0+ 0+ 
2 008 123 257 250 113 032 006 001 0+ 0+ 0+ 0+ 0+ 
3 0+ 026 114 250 194 085 022 003 0+ 0+ 0+ 0+ 0+ 
4 0+ 004 035 172 229 155 061 014 001 0+ 0+ 0+ 0+ 
5 0+ 0+ 008 086 196 207 122 041 007 0+ 0+ 0+ 0+ 
6 0+ 0+ 001 032 126 207 183 092 023 002 0+ 0+ 0+ 
7 0+ 0+ 0+ 009 062 157 209 157 062 009 0+ 0+ 0+ 
8 0+ 0+ 0+ 002 023 092 183 207 126 032 001 0+ 0+ 8 
9 0+ 0+ 0+ 0+ 007 041 122 207 196 086 008 0+ 0+ 9 
10 0+ 0+ 0+ 0+ 001 014 061 155 229 172 035 004 0+ 10 
11 0+ 0+ 0+ 0+ 0+ 003 022 085 194 250 114 026 0+ 1 
12 0+ 0+ 0+ 0+ 0+ 001 006 032 113 250 257 123 008 1 
13 0+ 0+ 0+ 0+ 0+ 0+ 001 007 041 154 356 359 123 1 
14 0+ 0+ 0+ 0+ 0+ 0+ 0+ 001 007 044 229 488 869 1 
15 0 860 463 206 035 005 0+ 0+ 0+ 0+ 0q- 0+ 0+ 0+ 
1 130 366 343 132 031 005 0+ 0+ 0+ 0+ 0+ 0+ 0+ 
2 009 135 267 231 092 022 003 0+ 0+ 0+ 0+ 0+ 0+ 
3 0+ 031 129 250 170 063 014 002 0+ 0+ 0+ 0+ 0+ 
4 0+ 005 043 188 219 127 042 007 001 05- 0+ 0q- 0+ 
5 0q- 001 010 103 206 186 092 024 003 0+ 0+ 0+ 0+ 
6 0+ 0+ 002 043 147 207 153 061 012 001 0+ 0+ 0+ 
7 O+ O+ Oq- 014 081 177 196 118 035 003 O+ O+ O+ 
8 0+ 0+ 0+ 003 035 118 196 177 081 014 0+ 0+ 0+ 
9 0+ 0+ 0+ 001 012 061 153 207 147 043 002 0+ 0+ 
438 TABLE IV 
PART A: INDIVIDUAL TERMS, )(:; n, p) 
p 
n z 01 05 l0 20 30 40 50 60 70 80 90 95, 99 z 
I 
15 10 0+ 0+ 0+ 0+ 003 024 092 186 206 103 010 001 0+ 10 
11 0+ 0+ 0+ 0+ 001 007 042 127 219 188 043 005 0+ 11 
12 0+ 0+ 0+ 0+ 0+ 002 014 063 170 250 129 031 0+ 12 
13 0-4- 0-4- 0-4- 0-4- 0-% 0-% 003 022 092 231 267 135 009 13 
14 0-% 0-% 0-% 0-% 0-% 0-% 0-% 005 031 132 343 366 130 14 
15 0-% 0-% 0-% 0-% 0-% 0-% 0-% 0-% 005 035 206 463 860 15 
16 0 851 440 185 028 003 0-% 0-% 0-% 0-% 0-% 0-% 0-% 0-% 0 
1 138 371 329 113 023 003 0-% 0-% 0-% 0-% 0-% 0-% 0-% 1 
2 010 146 275 211 073 015 002 0-% 0-% 0-% 0-% 0-% 0-% 2 
3 0-% 036 142 246 146 047 009 001 0-% 0-% 0-% 0-% 0-% 3 
4 0-% 006 051 200 204 101 028 004 0-% 0-% 0-% 0-% 0-% 4 
5 0-% 001 014 120 210 162 067 014 001 0-% 0-% 0-% 0-% 5 
6 0-% 0-% 003 055 165 198 122 039 006 0-% 0-% 0-% 0-% 6 
7 0-% 0-% 0-% 020 101 189 175 084 019 001 0-% 0-% 0-% 7 
8 0-% 0-% 0-% 006 049 142 196 142 049 006 0-% 0-% 0-% 8 
9 0-% 0-% 0-% 001 019 084 175 189 101 020 0-% 0-% 0-% 9 
10 0-% 0-% 0-%. 0-% 006 039 122 198 165 055 003 0-% 0-% 10 
11 0-% 0-% 0-% 0-% 001 014 067 162 210 120 014 001 0-% 11 
12 0-% 0-% 0-% 0-% 0-% 004 028 101 204 200 051 006 0-% 12 
13 0-% 0-% 0-% 0-% 0-% 001 009 047 146 246 142 036 0-% 13 
14 0-% 0-% 0-% 0-% 0-% 0-% 002 015 073 211 275 146 010 14 
15 0-% 0-% 0-% 0-% 0-% 0-% 0-% 003 023 113 329 371 138 15 
16 O+ O+ O+ O+ O+ O+ O+ O+ 003 028 185 440 851 16 
17 0 843 418 167 023 002 O+ O+ O+ O+ O+ O+ O+ O+ 0 
1 145 374 315 096 017 002 O+ O+ O+ O+ O+ O+ O+ 1 
2 012 158 280 191 058 010 001 0-% 0-% 0-% 0-% 0-% 0-% 2 
3 001 041 156 239 125 034 005 O+ O+ O+ O+ O+ O+ 3 
4 0-% 008 060 209 187 080 018 002 0-% 0-% 0-% 0-% 0-% 4 
5 0-% OO1 017 136 208 138 047 008 001 0-% 0-% 0-% 0-% 5 
6 O+ O+ 004 068 178 184 094 024 003 O+ O+ O+ O+ 6 
7 O+ O+ 001 027 120 193 148 057 009 O+ O+ O+ O+ 7 
8 O+ O+ O+ 008 064 161 185 107 028 002 O+ O+ O+ 8 
9 O+ O+ O+ 002 028 107 185 161 064 008 O+ O+ O+ 9 
10 O+ O+ O+ O+ 009 057 148 193 120 027 001 O+ O+ 10 
11 O+ O+ O+ O+ 003 024 094 184 178 068 004 O+ O+ 11 
12 O+ O+ O+ O+ 001 008 047 138 208 136 017 001 O+ 12 
13 O+ O+ O+ O+ O+ 002 018 080 187 209 060 008 O+ 13 
14 O+ O+ 0-% O+ O+ O+ 005 034 125 239 156 041 001 14 
15 O+ O+ O+ O+ 0-% O+ 001 010 058 191 280 158 012 15 
16 0-% 0-% 0-% 0-% 0-% 0-% 0-% 002 017 096 315 374 145 16 
17 10+ O+ O+ O+ O+ O+ O+ O+ 002 023 167 418 843 I 17 
TABLE IV 439 
I)ART A. INDiVIDU^L TERMS, b(X; t, p) 
P 
x .01 05 .10 .20 30 40 .50 60 70 80 90 95 99 
0 835 397 150 018 002 0+ 0+ 0+ 0+ 0+ 0+ 0+ 0+ 
1 152 376 300 081 013 001 0+ 0+ 0+ 0+ 0+ 0+ 0+ 
2 013 168 284 172 046 007 001 0+ 0+ 0+ 0+ 0+ 0+ 
3 001 047 168 230 105 025 003 0+ 0+ 0+ 0+ 0+ 0+ 
4 0+ 009 070 215 168 061 012 001 0+ 0+ 0+ 0+ 0+ 
5 0+ 001 022 151 202 115 033 004 0+ 0+ 0+ 0+ 0+ 
6 O+ O+ 005 082 187 166 071 015 001 O+ O+ O+ O+ 
7 O+ O+ 001 035 138 189 121 037 005 O+ O+ O+ O+ 
8 O+ O+ O+ 012 081 173 167 077 015 001 O+ O+ O+ 
9 O+ O+ O+ 003 039 128 185 128 039 003 O+ O+ O+ 9 
10 O+ O+ O+ 001 015 077 167 173 081 012 O+ O+ O+ 10 
11 O+ O+ O+ O+ 005 037 121 189 138 035 001 O+ O+ 11 
12 O+ O+ O+ O+ 001 015 071 166 187 082 005 O+ O+ 12 
13 O+ O+ O+ O+ O+ 004 033 115 202 151 022 001 O+ 13 
14 O+ O+ O+ O+ O+ 001 012 061 168 215 070 009 O+ 14 
15 O+ O+ O+ O+ O+ O+ 003 025 105 230 168 047 001 15 
16 O+ O+ O+ O+ O+ O+ 001 007 046 172 284 168 013 16 
17 O+ O+ O+ O+ O+ O+ O+ 001 013 081 300 376 152 17 
18 O+ O+ O+ O+ O+ O+ O+ O+ 002 018 150 397 835 18 
0 826 377 135 011 001 O+ O+ O+ O+ O+ O+ O+ O+ 
1 159 377 285 068 009 001 O+ O+ O+ O+ O+ O+ O+ 
2 014 179 285 154 036 005 O+ O+ O+ O+ O+ O+ O+ 
3 001 053 180 218 087 017 002 O+ O+ O+ O-k O+ O+ 
4 O+ 011 080 218 149 047 007 001 O+ O+ O+ O+ O+ 
5 O+ 002 027 164 192 093 022 002 O+ O+ O+ O+ O+ 
6 O+ O+ 007 095 192 145 052 008 001 O+ O+ O+ O+ 
7 O-k O+ 001 044 153 180 096 024 002 O+ O-k O+ O+ 
8 O+ O+ O+ 017 098 180 144 053 008 O+ O+ O+ O+ 
9 O+ O+ O+ 005 051 146 176 098 022 001 O+ O+ O+ 9 
10 O+ O+ O+ 001 022 098 176 146 051 005 O+ O+ O+ 10 
11 O+ O+ O+ O+ 008 053 144 180 098 017 O+ O+ O+ 11 
12 O+ O+ O+ O+ 002 024 096 180 153 044 001 O+ O+ 12 
13 O+ O+ O+ O+ 001 008 052 145 192 095 007 O+ O+ 13 
14 O+ 0-t- O+ O+ O+ 002 022 093 192 164 027 002 O+ 14 
15 O+ O+ O+ O+ O+ 001 007 047 149 218 080 011 O+ 15 
16 O+ O+ O+ O+ O+ O+ 002 017 087 218 180 053 001 16 
17 O+ O+ O+ O+ O+ O+ O+ 005 036 154 285 179 014 17 
18 O+ O+ O+ O+ O+ O+ O+ 001 009 068 285 377 159 18 
19 O+ O+ O+ O+ O+ O+ O+ O+ 001 014 135 377 826 19 
0 818 358 122 012 001 O+ O+ O+ O+ O+ O+ O+ O+ 0 
1 165 377 270 058 007 O+ O+ O+ O+ O+ O+ O+ O+ 1 
2 016 189 285 137 028 003 O+ O+ O+ O+ O+ O+ O+ 2 
3 001 060 190 205 072 012 001 O+ O+ 0-]- O+ O+ O+ ,34 
4 O+ 013 090 218 130 035 005 O+ O+ O+ O+ O+ O+ 
440 TABLE IV 
PART A: INmVlDUAL TERMS, b(X; n, p) 
p 
n x 01 05 10 .20 30 40 50 .60 70 80 90 .95 99 
20 5 O+ 002 032 175 179 075 015 001 O+ O+ O+ O+ O+ 
6 O+ O+ 009 109 192 124 037 005 O+ O+ O+ O+ O+ 
7 O+ O+ 002 055 164 166 074 015 001 O+ O+ O+ O+ 
8 O+ O+ O+ 022 114 180 120 035 004 O+ O+ O+ O+ 
9 O+ O+ O+ 007 065 160 160 071 012 O+ 0-% O+ O+ 
10 O+ O+ 0-% 002 031 117 176 117 031 002 O+ O+ O+ 
11 O+ O+ O+ O+ 012 071 160 160 065 007 O+ O+ O+ 
12 O+ O+ O+ O+ 004 035 120 180 114 022 O+ O+ O+ 
13 O+ O+ O+ O+ 001 015 074 166 164 055 002 O+ O+ 
14 O+ O+ O+ O+ O+ 005 037 124 192 109 009 O+ O+ 
15 O+ O+ O+ O+ O+ 001 015 075 179 175 032 002 O+ 15 
16 O+ O+ O+ O+ O+ O+ 005 035 130 218 090 013 O+ 16 
17 O+ O+ O+ O+ O+ O+ 001 012 072 205 190 060 001 17 
18 O+ O+ O+ O+ O+ O+ O+ 003 028 137 285 189 016 18 
19 O+ O+ O+ O+ O+ O+ O+ O+ 007 058 270 377 165 19 
20 O+ O+ O+ O+ O+ O+ O+ O+ 001 012 122 358 818 20 
21 0 810 341 109 009 001 O+ O+ O+ O+ O+ O+ O+ O+ 
1 172 376 255. 048 005 O+ O+ O+ O+ O+ O+ O+ O+ 
2 017 198 284 121 022 002 O+ O+ O+ O+ O+ O+ O+ 
3 001 066 200 192 058 009 001 O+ 0-% O+ O+ O+ O+ 
4 O+ 016 100 216 113 026 003 O+ 0-% O+ O+ O+ O+ 
5 O+ 003 038 183 164 059 010 001 O+ O+ O+ O+ O+ 
6 O+ O+ 011 122 188 105 026 003 O+ O+ O+ O+ O+ 
7 O+ O+ 003 065 172 149 055 009 O+ O+ 0-% O+ O+ 
8 O+ O+ 00! 029 129 174 097 023 002 O+ O+ O+ O+ 
9 O+ O+ O+ 010 080 168 140 050 006 O+ O+ O+ O+ 
10 O+ O+ O+ 003 041 134 168 089 018 001 O+ O+ O+ 
11 O+ O+ O+ 001 018 089 168 134 041 003 O+ O+ 0-% 
12 O+ O+ O+ O+ 006 050 140 168 080 010 O+ O+ O+ 
13 O+ O+ 0-% O+ 002 023 097 174 129 029 001 O+ O+ 
14 O+ O+ 0-% O+ O+ 009 055 149 172 065 003 O+ O+ 
15 O+ O+ 0-% O+ O+ 003 026 105 188 122 011 O+ 0-% 
16 O+ O+ O+ O+ O+ 001 010 059 164 183 038 003 O+ 
17 O+ O+ O+ O+ O+ O+ 003 026 113 216 100 016 O+ 
18 O+ O+ O+ O+ O+ O+ OO1 009 058 192 200 066 001 
19 O+ O+ O+ 0-% O+ 0-% 0-% 002 022 121 284 198 017 
20 O+ O+ O+ 0-% O+ O+ 0-% O+ 005 048 255 376 172 
21 O+ O+ O+ 0-% O+ O+ O+ O+ 001 009 109 341 810 
22 0 802 324 098 007 O+ O+ O+ O+ O+ O+ O+ O+ O+ 
1 178 375 241 041 004 O+ O+ O+ O+ 0-% 0-% O+ 0-% 
2 019 207 281 107 017 001 O+ O+ O+ 0-% 0-% O+ O+ 
3 00! 073 208 178 047 006 O+ O+ O+ O+ O+ O+ O+ 
4 O+ 018 110 211 096 019 002 0-% O+ O+ O+ O+ O+ 
TABLE IV 441 
PART A. I.mVDVA TEn, MS, b(x;n, p) 
p 
z O1 05 10 20 30 40 50 60 70 80 90 95 99 z 
5 0-+- 003 044 190 149 046 006 0-+- 0-+- 0-+- 0-+- 0-+- 0-+- 5 
6 0-+- 001 014 134 181 086 018 001 0-+- 0-+- 0-+- 0-+- 0-+- 6 
7 0-+- 0-+- 004 077 177 131 041 005 0-+- 0-+- 0-+- 0-+- 0-+- 7 
8 0-+- 0-+- 001 036 142 164 076 014 001 0-+- 0-+- 0-+- 0-+- 8 
9 0-+- 0-+- 0-+- 014 095 170 119 034 003 0-+- 0-+- 0-+- 0-+- 9 
10 0-+- 0-+- 0-+- 005 053 148 154 066 010 0-+- 0-+- 0-+- 0-+- 10 
11 0-+- 0-+- 0-+- 001 025 107 168 107 025 001 0-+- 0-+- 0-+- 11 
12 0-+- 0-+- 0-+- 0-+- 010 066 154 148 053 005 0-+- 0-+- 0-+- 12 
13, 0-+- 0-+- 0-+- 0-+- 003 034 119 170 095 014 0-+- 0-+- 0-+- 13 
14 : 0-+- 0-+- 0-+- 0-+- 001 014 076 164 142 036 001 0-+- 0-+- 14 
15 0-+- 0-+- 0-+- 0-+- 0-+- 005 041 131 177 077 004 0-+- 0-+- 15 
16 0-+- 0-+- 0-+- 0-+- 0-+- 001 018 086 181 134 014 001 0-+- 16 
17 0-+- 0-+- 0-+- 0-+- 0-+- 0-+- 006 046 149 190 044 003 0-+- 17 
18 0-+- 0-+- 0-+- 0-+- 0-+- 0-+- 002 019 096 211 110 018 0-+- 18 
19 0-+- 0-+- 0-+- 0-+- 0-+- 0-+- 0-+- 006 047 178 208 073 001 19 
20 0-+- 0-+- 0-+- 0-+- 0-+- 0-+- 0-+- 001 017 107 281 207 019 20 
21 0-+- 0-+- 0-+- 0-+- 0-+- 0-+- 0-+- 0-+- OOi 041 241 375 178 21 
22 O+ O+ O+ O+ O+ O+ O+ O+ O+ 007 098 324 802 22 
0 794 307 089 006 0-+- 0-+- 0-+- 0-+- 0-+- 0-+- 0-+- 0-+- 0-+- 0 
1 184 372 226 034 003 0-+- 0-+- 0-+- 0-+- 0-+- 0-+- 0-+- 0-+- 1 
2 020 215 277 093 013 001 0-+- 0-+- 0-+- 0-+- 0-+- 0-+- 0-+- 2 
3 001 079 215 163 038 004 0-+- 0-+- 0-+- 0-+- 0-+- 0-+- 0-+- 3 
4 0-+- 021 120 204 082 014 001 0-+- 0-+- 0-+- 0-+- 0-+- 0-+- 4 
5 0-+- 004 051 194 133 035 OOi 0-+- 0-+- 0-+- 0-+- 0-+- 0-+- 5 
6 0-+- 001 017 145 171 070 012 001 0-+- 0-+- 0-+- 0-+- 0-+- 6 
7 0-+- 0-+- 005 088 178 113 029 003 0-+- 0-+- 0-+- 0-+- 0-+- 7 
8 0-+- 0-+- 001 044 153 151 058 009 0-+- 0-+- 0-+- 0-+- 0-+- 8 
9 0-+- 0-+- 0-+- 018 109 168 097 022 002 0-+- 0-+- 0-+- 0-+- 9 
10 0-+- 0-+- 0-+- 006 065 157 136 046 005 0-+- 0-+- 0-+- 0-+- 10 
11 0-+- 0-+- 0-+- 002 033 123 161 082 014 0-+- 0-+- 0-+- 0-+- 11 
12 0-+- 0-+- 0-+- 0-+- 014 082 161 123 033 002 0-+- 0-+- 0-+- 12 
13 0-+- 0-+- 0-+- 0-+- 005 046 136 157 065 006 0-+- 0-+- 0-+- 13 
14 0-+- 0-+- 0-+- 0-+- 002 022 097 168 109 018 0-+- 0-+- 0-+- 14 
15 0-+- 0-+- 0-+- 0-+- 0-+- 009 058 151 153 041 001 0-+- 0-+- 15 
16 0-+- 0-+- 0-+- 0-+- 0-+- 003 029 113 178 088 005 0-+- 0-+- 16 
17 0-+- 0-+- 0-+- 0-+- 0-+- 001 012 070 171 145 017 001 0-+- 17 
18 0-+- 0-+- 0-+- 0-+- 0-+- 0-+- 004 035 133 194 051 O0-i 0-+- 18 
19 0-+- 0-+- 0-+- 0-+- 0-+- 0-+- 001 014 082 204 120 021 0-+- 19 
20 0-+- 0-+- 0-+- 0-+- 0-+- 0-+- 0-+- 001 038 163 215 079 001 20 
21 0-+- 0-+- 0-+- 0-+- 0-+- 0-+- 0-+- 001 013 093 277 215 020 21 
22 0-+- 0-+- 0-+- 0-+- 0-+- 0-+- 0-+- O+ 003 034 226 372 184 22 
23 [ O+ 0-+- 0-+- 0-+- 0-+- 0-+- 0-+- 0-+- 0-+- 006 089 307 794 23 
I 
442 TABLE IV 
PART A: INmVDUAL TEn,MS, b(X; n, p) 
p 
n x .01 05 10 20 .30 40 50 .60 70 80 90 95 , 99 x 
24 0 786 292 080 005 O+ O+ O+ O+ O+ O+ O+ O+ O+ 0 
1 190 369 213 028 002 0-+- 0-+- 0-+- 0-+- 0-+- 0-+- 0-+- 0-+- 1 
2 022 223 272 081 010 001 O+ O+ O+ O+ O+ O+ O+ 2 
3 002 086 221 149 031 003 O+ O+ O+ O+ O+ O+ O+ 3 
4 O+ 024 129 196 069 010 001 O+ O+ O+ O+ O+ O+ 4 
5 0-+- 005 057 196 118 027 003 0-+- 0-+- 0-+- 0-+- 0-+- 0-+- 5 
6 0-+- 001 020 155 160 056 008 0-+- 0-+- 0-+- 0-+- 0-+- 0-+- 6 
7 O+ O+ 006 100 176 096 021 002 O+ O+ O+ O+ O+ 7 
8 O+ O+ 001 053 160 136 044 005 O+ O+ O+ O+ O+ 8 
9 O+ O+ O+ 024 122 161 078 014 001 O+ O+ O+ O+ 9 
10 O+ O+ O+ 009 079 161 117 032 003 O+ O+ O+ O+ 10 
11 0-+- 0-+- 0-+- 003 043 137 149 061 008 0-+- 0-+- 0-+- 0-+- 11 
12 O+ O+ O+ 001 020 099 161 099 020 001 O+ O+ O+ 12 
13 O+ O+ O+ O+ 008 061 149 137 043 003 O+ O+ O+ 13 
14 0-+- 0-+- 0-+- 0-+- 003 032 117 161 079 009 0-+- 0-+- 0-+- 14 
15 0-+- 0-+- 0-+- 0-+- 001 014 078 161 122 024 0-+- 0-+- 0-+- 15 
16 0-+- 0-+- 0-+- 0-+- 0-+- 005 044 136 160 053 001 0-+- 0-+- 16 
17 0-+- 0-+- 0-+- 0-+- 0-+- 002 021 096 176 100 006 0-+- 0-+- 17 
18 0-+- 0-+- 0-+- 0-+- 0-+- 0-+- 008 056 160 155 020 001 0-+- 18 
19 0-+- 0-+- 0-+-'0-+- 0-+- O+ 003 027 118 196 057 005 0-+- 19 
20 O+ O+ O+ O+ O+ O+ 001 010 069 196 129 024 O+ 20 
21 0-+- 0-+- 0-+- 0-+- 0-+- 0-+- 0-+- 003 031 149 221 086 002 21 
22 O+ O+ O+ O+ O+ O+ O+ 001 010 081 272 223 022 22 
23 O+ '0+ O+ O+ O+ O+ O+ O+ 002 028 213 369 190 23 
24 O+ O+ O+ O+ O+ O+ O+ O+ O+ 005 080 292 786 24 
25 0 778 277 072 004 0-+- 0-+- 0-+- 0-+- 0-+- 0-+- 0-+- 0-+- 0-+- 0 
1 196 365 199 024 001 0-+- 0-+- 0-+- 0-+- 0-+- 0-+- 0-+- 0-+- 1 
2 024 231 266 071 007 O+ O+ O+ O+ O+ O+ O+ O+ 2 
3 002 093 226 136 024 002 O+ O+ O+ O+ O+ O+ O+ 3 
4 O+ 027 138 187 057 007 O+ O+ O+ O+ O+ O+ O+ 4 
5 O+ 006 065 196 103 020 002 O+ O+ O+ O+ O+ O+ 5 
6 O+ 001 024 163 147 044 005 O+ O+ O+ O+ O+ O+ 6 
7 0-+- 0-+- 007 111 171 080 014 001 0-+- 0-+- 0-+- 0-+- 0-+- 7 
8 O+ O+ 002 062 165 120 032 003 O+ O+ O+ O+ O+ 8 
9 O+ O+ O+ 029 134 151 061 009 O+ O+ O+ O+ O+ 9 
10 O+ O+ O+ 012 092 161 097 021 001 O+ O+ O+ O+ 10 
11 0-+- 0-+- 0-+- 004 054 147 133 043 004 0-+- 0-+- 0-+- 0-+- 11 
12 0-+- 0-+- 0-+- 001 027 114 155 076 011 0-+- 0-+- 0-+- 0-+- 12 
13 0-+- 0-+- O-k 0-+- 01l 076 155 114 027 001 0-+- 0-+- 0-+- 13 
14 O+ O+ O+ O+ 004 043 133 147 054 004 O+ O+ O+ 14 
15 O+ O+ O+ O+ 001 021 097 161 092 012 O+ O+ O+ 15 
16 0-+- 0-+- 0-+- 0-+- 0-+- 009 061 151 134 029 0-+- 0-+- 0-+- 16 
17 O+ O+ O+ O+ O+ 003 032 120 165 062 002 O+ O+ 17 
18 Od- Od- Od- Od- Od- 001 014 080 171 111 007 Od- Od- 18 
19 04- 04- 0-4- 0-4- 0_4_ O aa. at.l ,17 a ao am a b  
TABLE IV 443 
PART A: IXmVDU^L TERMS, b(X; n, p) 
p 
n z O1 .05 10 .20 30 .40 50 .60 .70 80 90 95 .99 z 
25 20 O+ O+ O+ O+ O-k O+ 002 020 103 196 065 006 O+ 20 
21 0-+- 0-+- O-k 0-+- 0-+- 0-+- O-k 007 057 187 138 027 0-+- 21 
22 O+ O+ O+ O+ O-k O+ O-k 002 024 136 226 093 002 22 
23 i O+ O+ O+ O+ O-k O+ O+ O+ 007 071 266 231 024  23 
24 [ O+ O+ O+ O+ O+ O+ O+ O+ 001 024 199 365 196 24 
25. O+ O-k O+ O-k O+ O+ O+ O+ O+ 004 072 277 778 25 
444 TABLE IV 
n 
PART B: CVMVLATVE TERMS, b(x; n, p) 
p 
n r 01 .o5 lO 20 30 .40 50 60 70 80 90 .95 99 r 
2 o I I I I I I I I I I I I I 0 
1 020 098 19o 360 510 640 750 8t0 91o 960 990 998 1-- 1 
2 0+ 002 OlO 040 090 16o 250 360 490 640 810 902 980 2 
3 o I I I I I I I I I I I I I 0 
I 030 143 271 488 657 784 875 936 973 992 999 1-- 1- 1 
2 0+ 007 028 104 216 352 500 648 784 896 972 993 1- 2 
3 o+ 0+ OOl 008 027 064 125 216 343 512 729 857 970 3 
4 o I I I I I I I I 1 I I I 1 0 
I 039 185 344 590 760 870 938 97,t 992 998 1-- 1-- 1- 1 
2 OOl o14 052 181 348 525 688 821 916 973 996 1-- 1- 2 
3 0+ 0+ 004 027 084 179 312 475 652 819 948 986 999 3 
4 o+ o+ o+ 002 008 026 062 13o 240 41o 656 815 961 4 
5 0 I I I I I I I I I I I I I o 
I 049 226 410 672 832 922 969 990 998 1-- 1- 1- 1- 1 
2 o01 023 o81 263 472 663 812 913 969 993 1- 1- 1- 2 
3 o+ OOl 009 058 163 317 500 683 837 942 991 999 1- 3 
4 0+ 0+ 0+ 007 o31 087 188 337 528 737 919 977 999 4 
5 o+ o+ o+,0+ 002 OlO o31 078 168 328 590 774 951 5 
6 o I I I I I I 1 I I I I I I o 
I 059 265 469 738 882 953 984 996 999 1- 1- 1- 1- 1 
2 OOl 033 114 345 580 767 891 959 989 998 1- 1-- 1- 2 
3 o+ 002 o16 099 256 456 656 821 930 983 999 1-- 1-- 3 
4 0+ 0+ OOl o17 070 179 344 544 744 901 984 998 1-- 4 
5 ! o+ o+ 0+ 002 Oll 041 lO9 233 420 655 886 967 999 5 
6 0+ 0-k 0+ 0+ 001 004 016 047 118 262 531 735 941 6 
7 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 
1 068 302 522 790 918 972 992 998 1-- 1-- 1-- 1-- 1-- 1 
2 002 044 150 423 671 841 938 981 996 1-- 1-- 1-- 1-- 2 
3 0+ 004 026 148 353 580 773 904 971 995 1-- 1-- 1-- 3 
4 0- 0- 003 033 126 290 500 710 874 967 997 1-- l-- 4 
5 0- 0- 0- 005 029 096 227 420 647 852 974 996 1-- 5 
6 0+ 0+ 0+ 0+ 004 019 062 159 329 577 850 956 998 6 
7 Od- Od- Od- Od- Od- 002 OOS 028 082 210 478 698 932 7 
$ 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 
1 077 337 570 832 942 983 996 999 1-- 1-- 1-- 1-- 1-- 1 
2 003 057 187 497 745 894 965 99 999 1-- 1-- 1-- 1-- 2 
3 Od- 006 038 203 448 685 855 950 989 999 1-- 1-- 1-- 3 
4 Od- Od- 005 056 194 406 637 826 942 990 1-- 1-- 1-- 4 
5 Od- Od- Od- 010 058 174 363 594 806 944 995 1-- 1-- 5 
6 Od- Od- Od- 001 011 050 145 315 552 797 962 994 1-- 6 
7 Od- Od- Od- Od- 001 009 035 106 255 503 813 943 997 7 
8 Od- Od- 0-- Od- Od- 001 004 017 058 168 430 663 923 8 
TABLE tV 445 
n 
B: n, p) 
xr 
p 
n r Ol 05 lO 20 30 40 50 60 70 80 90 95 99 
9 o 1 I 1 1 1 1 1 1 1 1 1 1 1 
1 086 370 613 866 960 990 998 1- 1- 1- 1- 1- 1- 
2 003 o71 225 564 804 929 980 996 1- 1- 1- 1- 1- 
3 o+ 008 053 262 537 768 91o 975 996 1- 1- 1- 1- 
4 o+ OOl 008 086 270 517 746 9Ol 975 997 1- 1- 1- 
5 o+ o+ ool 020 099 267 500 733 9Ol 980 999 1- 1- 
6 o+ 0+ o+ 003 025 099 254 483 730 914 992 999 1- 
7 o+ o+ o+ o+ 004 025 090 232 463 738 947 992 1- 
8 o+ o+ o+ o+ o+ 004 020 o71 196 436 775 929 997 
9 o+ o+ o+ o+ o+ o+ 002 OlO 040 134 387 630 914 
10 0 1 1 1 1 1 1 1 1 1 1 1 1 1 
1 096 401 651 893 972 994 999 1-- 1-- 1-- 1-- 1-- 1-- 
2 004 086 264 624 851 954 989 998 1-- 1-- 1-- 1-- 1-- 
3 0+ 012 070 322 617 833 945 988 998 1-- 1-- 1-- 1-- 
4 0+ 001 013 121 350 618 828 945 989 999 1-- 1-- 1-- 
5 0+ 0+ 002 033 150 367 623 834 953 994 1-- 1-- 1-- 
6 O+ O+ O+ 006 047 166 377 633 850 967 998 1-- 1-- 
7 0+ 0+ 0+ 001 011 055 172 382 650 879 987 999 1-- 
8 0+ 0+ 0+ 0+ 002 012 055 167 383 678 930 988 1-- 
9 0+ 0+ 0+ 0+ 0+ 002 011 046 149 376 736 914 996 
10 0+ 0+ 0+ 0+ 0+ 0+ 001 006 028 107 349 599 904 
11 0 1 I I I I I I I I I I I 1 
1 105 431 686 914 980 996 1-- 1-- 1-- 1-- 1-- 1-- 
2 005 102 303 678 887 970 994 999 1-- 1-- 1-- 1-- 1-- 
3 0+ 015 090 383 687 881 967 994 999 1-- 1-- 1-- 1-- 
4 0+ 002 019 161 430 704 887 971 996 1-- 1-- 1-- 1-- 
5 0+ 0+ 003 050 210 467 726 901 978 998 1-- 1-- 1-- 
6 0+ 0+ 0+ 012 078 247 500 753 922 988 1-- 1-- 1-- 
7 0+ 0+ 0+ 002 022 099 274 533 790 950 997 1-- 1-- 
8 0+ 0+ 0+ 0+ 004 029 113 296 570 839 981 998 1-- 
9 0+ 0+ 0+ 0+ 001 006 033 119 313 617 910 985 1-- 
10 0+ 0+ 0+ 0+ 0+ 001 006 030 113 322 697 898 995 
11 0+ 0+ 0+ 0+ 0+ 0+ 0+ 004 020 086 314 569 895 
12 0 1 I I I I I I 1 I I I I 1 
I 114 460 718 931 986 998 1-- 1-- 1-- 1-- 1-- 1-- 1-- 
2 006 118 341 725 915 980 997 1-- 1-- 1-- 1-- 1-- 1-- 
3 0+ 020 111 442 747 917 981 997 1-- 1-- 1-- 1-- 1-- 
4 O+ 002 026 205 507 775 927 985 998 1-- 1-- 1 
5 O+ O+ 004 073 276 562 806 943 991 999 1-- 1-- 1-- 
6 O+ O+ 001 019 118 335 613 842 961 996 1-- 1-- 1-- 
7 O+ O+ O+ 004 039 158 387 665 882 981 999 1-- 1-- 
8 O+ O+ O+ 001 009 057 194 438 724 927 996 1-- 1-- 
9 O+ O+ O+ O+ 002 015 073 225 493 795 974 998 1-- 
446 TABLE IV 
B: b(x; n, p) 
p 
n r Ol 05 .lO .2o 30 40 50 60 .7o 80 .9o 95 .99 r 
12 lO o+ o+ o+ o+ o+ 003 o19 083 253 558 889 980 1- lO 
11 o+ o+ o+ o+ o+ o+ 003 020 085 275 659 882 994 11 
12 o+ o+ o+ o+ o+ o+ o+ 002 o14 069 282 540 886 12 
13 o I I I I I I I I I I I I I o 
I 122 487 746 945 990 999 1- 1- 1- 1- 1- 1- 1- 1 
2 007 135 379 766 936 987 998 1- 1- 1- 1- 1- 1-- 2 
3 o+ 025 134 498 798 942 989 999 1- 1- 1- 1- 1- 3 
4 o+ 003 034 253 579 831 954 992 999 1-- 1-- 1-- 1- 4 
5 o+ o+ 006 099 346 647 867 968 996 1- 1- 1- 1-- 5 
6 O+ O+ 001 030 165 426 709 902 982 999 1-- 1-- 1-- 6 
7 O+ O+ O+ 007 062 229 500 771 938 993 1-- 1-- 1--  7 
8 O+ O+ O+ 001 018 098 291 574 835 970 999 1-- 1-- 8 
9 O+ O+ O+ O+ 004 032 133 353 654 901 994 1-- 1-- 9 
10 O+ O+ O+ O+ 001 008 046 169 421 747 966 997 1-- 10 
11 O+ 0-]- O+ 0-]- O+ 001 011 058 202 502 866 975 1-- 11 
12 O-k O-k O-k O-k O-k O-k 002 013 064 234 621 865 993 12 
13', O+ O+ O+ O+ O+ O+ O+ 001 010 055 254 513 878 13 
14 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 
1' 131 512 771 956 993 999 1-- 1-- 1-- 1-- 1-- 1-- 1-- 1 
2 008 153 415 802 953 992 999 1-- 1-- 1-- 1-- 1-- 1-- 2 
3 O-k 030 158 552 839 960 994 999 1-- 1-- 1-- 1-- 1-- 3 
4 O+ 004 044 302 645 876 971 996 1-- 1-- 1-- 1-- 1-- 4 
5 O+ O-k 009 130 416 721 910 982 998 1-- 1-- 1-- 1-- 5 
6 O-k O-k 001 044 219 514 788 942 992 1-- 1-- 1-- 1-- 6 
7 O+ O+ O+ 012 093 308 605 850 969 998 1-- 1-- 1-- 7 
8 O+ O+ O+ 002 031 150 395 692 907 988 1-- 1-- 1-- 8 
9 O+ O+ O+ O+ 008 058 212 486 781 956 999 1-- 1-- 9 
10 O+ O+ O+ O+ 002 018 090 279 584 870 991 1-- 1-- 10 
11 O+ O+ O+ O+ O+ 004 029 124 355 698 956 996 1-- 11 
12 O+ O+ O+ O+ O+ 001 006 040 161 448 842 970 1-- 12 
13 O-k O-k O-k O-k O-k O-k 001 008 047 198 585 847 992 13 
14 O-k O-k O-k O-k O-k O-k O-k 001 007 044 229 488 869 14 
15 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 
I 140 537 794 965 995 1-- 1-- 1-- 1-- 1-- 1-- 1-- 1-- 1 
2 010 171 451 833 965 995 1-- 1-- 1-- 1-- 1-- 1-- 1-- 2 
3 O+ 036 184 602 873 973 996 1-- 1-- 1-- 1-- 1-- 1-- 3 
4 O 005 056 352 703 909 982 998 1-- 1-- 1-- 1-- 1-- 4 
5 O+ 001 013 164 485 783 941 991 999 1-- 1-- 1-- 1-- 5 
6 O+ O+ 002 061 278 597 849 966 996 1-- 1-- 1-- 1-- 6 
7 O+ O+ O+ 018 131 390 696 905 985 999 1-- 1-- 1-- 7 
8 O+ O+ O+ 004 050 213 500 787 950 996 1-- 1-- 1-- 8 
9 O+ O+ O+ 001 015 095 304 610 869 982 1-- 1-- 1-- 9 
TABLE IV 447 
n 
B' p) 
p 
r .01 05 10 20 30 iO 50 60 70 80 90 95 99 r 
10 O+ O+ O+ O+ 004 034 151 403 722 939 998 1-- 1-- 10 
11 O+ O+ O+ O+ 001 009 059 217 515 836 987 999 1-- 11 
12 O+ O+ O+ O+ Od- 002 018 091 297 648 944 995 1-- 12 
13 O+ O+ O+ O+ O+ O+ 004 027 127 398 816 964 1-- 13 
14 O+ Od- O+ Od- O+ O+ O+ 005 035 167 549 829 990 14 
15 O+ O+ O+ O+ O+ O+ O+ O+ 005 035 206 463 860 15 
0 I I I I I I I I I I I I I 0 
1 149 560 815 972 997 1-- 1-- 1-- 1-- 1-- 1-- 1-- 1-- 1 
2 011 189 485 859 974 997 1-- 1-- 1-- 1-- 1-- 1-- 1-- 2 
3 001 043 211 648 901 982 998 1-- 1-- 1-- 1-- 1-- 1-- 3 
4 0-]- 007 068 402 754 935 989 999 1-- 1-- 1-- 1-- 1-- 4 
5 0-]- 001 017 202 550 833 962 995 1-- 1-- 1-- 1-- 1-- 5 
6 0-]- 0-]- 003 082 340 671 895 981 998 1-- 1-- 1-- 1-- 6 
7 0-]- 0-]- 001 027 175 473 773 942 993 1-- 1-- 1-- 1-- 7 
8 0+ 0+ 0+ 007 074 284 598 858 974 999 1-- 1-- 1-- 8 
9 0+ 0+ 0+ 001 026 142 402 716 926 993 1-- 1-- 1-- 9 
10 0+ 0+ 0+ 0+ 007 058 227 527 825 973 999 1-- 1-- 10 
11 0- 0- 0- 0- 002 019 105 329 660 918 997 1-- 1-- 11 
12 0- 0+ 0+ 0+ 0+ 005 038 167 450 798 983 999 1-- 12 
13 0+ 0+ 0+ 0+ 0+ 001 011 065 246 598 932 993 1-- 13 
14 0+ 0+ 0+ 0+ 0+ 0+ 002 018 099 352 789 957 999 14 
15 0- 0- 0- 0+ 0- 0- 0- 003 026 141 515 811 989 15 
16 0+ 0+ 0+ 0+ 0+ 0+ 0+ 0+ 003 028 185 440 851 16 
011 1 1 1 1 1 1 1 1 1 1 1 1 0 
1 157 582 833 977 998 1-- 1-- 1-- 1-- 1-- 1-- 1-- 1-- 1 
2 012 208 518 882 981 998 1-- 1-- 1-- 1-- 1-- 1-- 1-- 2 
3 001 050 238 690 923 988 999 1-- 1-- 1-- 1-- 1-- 1-- 3 
4 0-]- 009 083 451 798 954 994 1-- 1-- 1-- 1-- 1-- 1-- 4 
5 0-]- 001 022 242 611 874 975 997 1-- 1-- 1-- 1-- 1-- 5 
6 i 0-]- 0-]- 005 106 403 736 928 989 999 1-- 1-- 1-- 1-- 6 
7 ' 0-]- 0-]- 001 038 225 552 834 965 997 1-- 1-- 1-- 1-- 7 
80-]- 0-]- 0-]- 011 105 359 685 908 987 1-- 1-- 1-- 1-- 8 
9 0-]- 0-]- 0-]- 003 040 199 500 801 960 997 1-- 1-- 1-- 9 
10 0-]- 0-]- Oq- O 013 092 315 641 895 989 1-- 1-- 1-- 10 
11 0-]- 0-]- 0-]- 0-]- 003 035 166 448 775 962 999 1-- 1-- 11 
12 O 0-]- 0-]- O 001 011 072 264 597 894 995 1-- 1-- 12 
13 0-]- 0-]- 0-]- 0-]- 0-]- 003 025 126 389 758 978 999 1-- 13 
14 O O O O O 0-]- 006 046 202 549 917 991 1-- 14 
15 0-]- 0-]- 0-]- 0-]- 0-]- O 001 012 077 310 762 950 999 15 
16 0-]- 0-]- 0-]- 0-]- 0-]- 0-]- 0-]- 002 019 118 482 792 988 16 
17 Oq- 0-]- 0-]- O 0-]- 0-]- 0-]- 0-]- 002 023 167 418 843 17, 
448 TABLE IV 
n 
PART B: CUMULATIVE TERMS, Z b(Jf; n, p) 
p 
n r O1 .05 .10 20 30 .40 50 60 70 80 90 95 99 r 
18 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 
1 165 603 850 982 998 1-- 1-- l-- 1-- 1-- 1-- 1-- 1-- 1 
2 014 226 550 901 986 999 1-- l-- 1-- 1-- 1-- 1-- 1-- 2 
3 001 058 266 729 940 992 999 1-- 1-- 1-- 1-- 1-- 1-- 3 
4 0.% 011 098 499 835 967 996 1-- 1-- 1-- 1-- 1-- 1-- 4 
5 0.% 002 028 284 667 906 985 999 1-- 1-- 1-- 1-- 1-- 5 
6 0.% O+ 006 133 466 791 952 994 1-- 1-- 1-- 1-- 1-- 6 
7 O+ O+ 001 051 278 626 881 980 999 1-- 1-- 1-- 1-- 7 
8 O+ O+ O+ 016 141 437 760 942 994 1-- 1-- 1-- 1-- 8 
9 O+ O+ O+ 004 060 263 593 865 979 999 1-- 1-- 1-- 9 
10 O+ O+ O+ 001 021 135 407 737 940 996 1-- 1-- 1-- 10 
11 O+ O+ O+ O+ 006 058 240 563 859 984 1-- 1-- 1-- 11 
12 O+ O+ O+ O+ OO1 020 119 374 722 949 999 1-- 1-- 12 
13 O+ O+ O+ O+ O+ 006 048 209 534 867 994 1-- 1-- 13 
14 O+ O+ O+ O+ O+ 001 015 094 333 716 972 998 1-- 14 
15 O+ O+ O+ O+ O+ O+ 004 033 165 501 902 989 1-- 15 
16 O+ O+ O+ O+ O+ O+ 001 008 060 271 734 942 999 16 
17 O+ O+ O+ O+ O+ O+ O+ 001 014 099 450 774 986 
18 0-% 0-% 0. 0+ 0+ 0+ 0.% 0-% 002 018 150 397 835 18 
19 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 
1 174 623 865 986 999 1-- 1-- 1-- 1-- 1-- 1-- 1-- 1-- 1 
2 015 245 580 917 990 999 1-- 1-- l-- 1-- 1-- 1-- 1-- 2 
3 OO1 067 295 763 954 995 1-- 1-- 1-- 1-- 1-- 1-- 1-- 3 
4 0-% 013 115 545 867 977 998 1-- 1-- 1-- 1-- 1-- 1-- 4 
5 0-% 002 035 327 718 930 990 999 1-- 1-- 1-- 1-- 1-- 5 
6 0-% 0-% 009 163 526 837 968 997 1-- 1-- 1-- 1-- 1-- 6 
7 0-% 0-% 002 068 334 692 916 988 999 1-- 1-- 1-- 1-- 7 
8 0-% 0-% 0-% 023 182 512 820 965 997 1-- 1-- 1-- 1-- 8 
9 0-% 0-% 0-% 007 084 333 676 912 989 1-- 1-- 1-- 1-- 9 
10 0-% 0-% 0-% 002 033 186 500 814 967 998 1-- 1-- 1-- 10 
11 0-% 0-% 0-% 0-% 011 088 324 667 916 993 1-- 1-- 1-- 11 
12 0-% 0-% 0-% 0-% 003 035 180 488 818 977 1-- 1-- 1-- 12 
13 0-% 0-% 0-% 0-% 001 012 084 308 666 932 998 1-- 1-- 13 
14 0-% 0-% 0-% 0-% 0-% 003 032 163 474 837 991 1-- 1-- 14 
15 0-% 0-% 0-% 0-% 0-% 001 010 070 282 673 965 998 1-- 15 
16 0-% 0-% 0-% 0-% 0-% 0-% 002 023 133 455 885 987 1-- 16 
17 0-% 0-% 0-% 0-% 0-% 0-% 0-% 005 046 237 705 933 999 17 
18 0-% 0-% 0-% 0-% 0-% 0-% 0-% 00 010 083 420 755 985 18 
19 O+ O+ 0-% O+ 0.% 0.% 0.% 0-% 001 014 135 377 826 19 
20 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 
1 182 642 878 988 999 1-- l-- 1-- 1-- 1-- l-- 1-- 1-- 1 
2 017 264 608 931 992 999 1-- 1-- l-- 1-- 1-- 1-- 1-- 2 
3 001 075 323 794 965 996 l-- 1-- 1-- 1-- 1-- 1-- 1-- 3 
4 0-% 016 133 589 893 984 999 1-- 1-- 1-- 1-- 1-- 1--[ 4 
TABLE IV 449 
n 
PAT B: p) 
p 
r 01 05 10 20 30 40 50 60 70 80 90 95 99 r 
5 0+ 003 043 370 762 949 994 1-- 1-- 1- 1-- 1-- 1-- 5 
6 0+ 0+ 011 196 584 874 979 998 1-- 1-- 1-- l-- 1-- 6 
7 0+ 0+ 002 087 392 750 942 994 l-- 1-- 1-- 1-- 1-- 7 
8 0+ 0+ 0+ 032 228 584 868 979 999 1-- 1-- 1-- 1-- 8 
9 0+ 0+ 0+ 010 ll3 404 748 943 995 1-- 1-- 1-- l-- 9 
l0 0+ 0+ 0+ 003 048 245 588 872 983 999 1-- 1-- 1-- 10 
11 0-]- 0-]- 0+ 001 017 128 412 755 952 997 1-- 1-- 1-- 11 
12 0+ 0+ 0+ 0+ 005 057 252 596 887 990 1-- 1-- 1-- 12 
13 0+ 0+ 0+ 0+ 001 021 132 416 772 968 1-- 1-- 1-- 13 
14 0+ 0+ 0+ 0+ 0+ 006 058 250 608 913 998 1-- 1-- 14 
15 0+ 0+ 0+ 0+ 0+ 002 021 126 416 804 989 1-- 1-- 15 
16 0+ 0+ 0+ 0+ 0+ 0+ 006 051 238 630 957 997 1-- 16 
17 0+ 0+ 0+ 0+ 0+ 0+ 001 016 107 411 867 984 1-- 17 
18 0+ 0+ 0+ 0+ 0+ 0+ 0+ 004 035 206 677 925 999 18 
19 0+ 0+ 0+ 0+ 0+ 0+ 0+ 001 008 069 392 736 983 19 
20 0+ 0+ 0+ 0+ 0+ 0+ 0+ 0+ 001 012 122 358 818 20 
0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 
1 190 659 891 991 999 1-- 1-- 1-- 1-- 1-- 1-- 1-- 1-- 1 
2 019 283 635 912 994 1-- 1-- 1-- 1-- 1-- 1-- 1-- 1-- 2 
3 001 085 352 821 973 998 1-- 1-- 1-- 1-- 1-- 1-- 1-- 3 
4 0+ 019 152 630 914 989 999 1-- 1-- 1-- 1-- 1-- 1-- 4 
5 0+ 003 052 414 802 963 996 1-- 1-- 1-- 1-- 1-- 1-- 5 
6 0+ 0+ 014 231 637 904 987 999 1-- 1-- 1-- 1-- 1--, 6 
7 0 0 003 109 449 800 961 996 1-- 1-- 1-- 1-- 1-- 7 
8 0+ 0+ 001 043 277 650 905 988 999 1-- 1-- 1-- 1-- 8 
9 0+ 0+ 0+ 014 148 476 808 965 998 1-- 1-- 1-- 1-- 9 
10 0+ 0+ 0+ 004 068 309 668 915 991 1-- 1-- 1-- 1-- 10 
11 0+ 0 0 001 026 174 500 826 974 999 1-- 1-- 1-- 11 
12 0+ 0+ 0+ 0+ 009 085 332 691 932 996 1-- 1-- 1-- 12 
13 0+ 0+ 0+ 0+ 002 035 192 524 852 986 1-- 1-- 1-- 13 
14 0+ 0+ 0+ 0+ 001 012 095 350 723 957 999 1-- 1-- 14 
15 0+ 0+ 0+ 0+ 0+ 004 039 200 551 891 997 1-- 1-- 15 
16 0+ 0+ 0+ 0+ 0+ 001 013 096 363 769 986 l-- 1-- 16 
17 0+ 0+ 0+ 0+ 0+ 0+ 004 037 198 586 948 997 1-- 17 
18 O+ O+ O+ O+ O+ O+ 001 011 086 370 848 981 1-- 18 
19 O+ O+ O+ O+ O+ O+ O+ 002 027 179 648 915 99919 
20 O+ O+ O+ O+ O+ O+ O+ O+ 006 058 365 717 981 20 
21 O+ O+ O+ O+ O+ O+ O+ O+ 001 009 109 341 810 , 21 
0 1 1 1 1 1 1 1 1 1 1 1 1 1 i 0 
1 198 676 902 993 1-- 1-- 1-- 1-- 1-- 1-- 1-- 1-- 1-- 1 
2 020 302 661 952 996 1-- 1-- 1-- 1-- 1-- 1-- 1-- 1-- 2 
3 001 095 380 846 979 998 1-- 1-- 1-- 1-- 1-- 1-- 1-- 3 
4 O+ 022 172 668 932 992 1-- 1-- 1-- 1-- 1-- 1-- 1-- 4 
450 TABLE IV 
PART B: CUMULATIVE TERMS,  b(x; n, p) 
p 
n r 01 .05 10 20 30 40 50 60 .70 80 .90 95 .99 r 
22 5 O+ 004 062 457 835 973 998 1-- 1-- 1-- 1-- 1-- 1-- 5 
6 O+ 001 018 267 687 928 992 1-- 1-- 1-- 1-- l-- 1-- 6 
7 O+ O+ 004 133 506 842 974 998 1-- 1-- 1-- 1-- 1-- 7 
8 O+ O+ OO1 056 329 710 933 993 1-- 1-- 1-- 1-- 1-- 8 
9 O+ O+ O+ 020 186 546 857 979 999 1-- 1-- 1-- 1-- 9 
10 O+ O+ O+ 006 092 376 735 945 996 1-- 1-- 1-- 1-- 10 
11 O+ O+ O+ 002 039 228 584 879 986 1-- 1-- 1-- 1-- 11 
12 O+ O+ O+ O+ 014 121 416 772 961 998 1-- 1-- 1-- 12 
13 O+ O+ O+ O+ 004 055 262 624 908 994 1-- 1-- 1-- 13 
14 O+ O+ O+ O+ 001 021 143 454 814 980 1-- 1-- 1-- 14 
15 O+ O+ O+ O+ O+ 007 067 290 671 944 999 l-- 1-- 15 
16 O+ O+ O+ O+ O+ 002 026 158 494 867 996 1-- 1-- 16 
17 O+ O+ O+ O+ O+ O+ 008 072 313 733 982 999 1-- 17 
18 O+ O+ O+ O+ O+ O+ 002 027 165 543 938 996 1-- 18 
19 O+ O+ O+ O+ O+ O+ O+ 008 068 332 828 9Y8 1-- 19 
20 O+ O+ O+ O+ O+ O+ O+ 002 021 154 620 905 999 20 
21 O+ O+ O+ O+ O+ O+ O+ O+ 004 048 339 698 980 21 
22 O+ O+ O+ O+ O+ O+ O+ O+ O+ 007 098 324 802 22 
23 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 
1 206 693 911 994 1-- 1-- 1-- 1-- 1-- 1-- 1-- 1-- 1-- 1 
2 022 321 685 960 997 1-- 1-- 1-- 1-- 1-- 1-- 1-- 1-- 2 
3 002 105 408 867 984 999 1-- 1-- 1-- 1-- 1-- 1-- 1-- 3 
4 O+ 026 193 703 946 995 1-- 
5 O+ 005 073 499 864 981 999 1-- 1-- 1-- 1-- 1-- 1-- 5 
6 O+ 001 023 305 731 946 995 1-- 1-- 1-- 1-- 1-- 1-- 6 
7 O+ O+ 006 160 560 876 983 999 1-- 1-- 1-- 1-- 1-- 7 
8 O+ O+ 001 072 382 763 953 996 1-- 1-- 1-- 1-- 1-- 8 
9 O+ O+ O+ 027 229 612 895 987 999 1-- 1-- 1-- 1-- 9 
10 O+ O+ O+ 009 120 444 798 965 998 1-- 1-- 1-- 1-- 10 
11 O+ O+ O+ 003 055 287 661 919 993 1-- 1-- 1-- 1-- 11 
12 O+ O+ O+ 001 021 164 500 836 979 999 1-- 1-- 1-- 12 
13 O+ O+ O+ O+ 007 081 339 713 945 997 1-- 1-- 1-- 13 
14 O+ O+ O+ O+ 002 035 202 556 880 991 1-- 1-- 1-- 14 
15 O+ O+ O+ O+ 001 013 105 388 771 973 1-- 1-- 1-- 15 
16 O+ O+ O+ O+ O+ 004 047 237 618 928 999 1-- 1-- 16 
17 O+ O+ O+ O+ O+ 001 017 124 440 840 994 1-- 1-- 17 
18 O+ O+ O+ O+ O+ O+ 005 054 269 695 977 999 1-- 18 
19 O+ O+ O+ O+ O+ O+ 001 019 136 501 927 995 1-- 19 
20 O+ O+ O+ O+ O+ O+ O+ 005 054 297 807 974 1-- 20 
21 O+ O+ O+ O+ O+ O+ O+ 00[ 016 133 592 895 998 21 
22 O+ O+ O+ O+ O+ O+ O+ O+ 003 040 315 679 978 22 
TABLE IV 451 
n 
PART B: CUMULiTIrE TERMS, Z b(x; n, p) 
r .01 .05 10 .20 30 .40 50 60 70 80 90 95 99  
0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 
1 214 708 920 995 l-- 1-- 1-- 1-- 1-- 1-- 1-- 1-- 1-- 1 
2 024 339 708 967 998 1-- 1-- 1-- 1-- 1-- 1-- 1-- 1-- 2 
3 002 116 436 885 988 999 1-- 1-- 1-- 1-- 1-- 
4 Oq- 030 214 736 958 996 1-- 1-- 1-- 1-- 1-- 1-- 1-- 
5 Oq- 006 085 540 889 987 999 1-- 1-- 1-- 1-- 1-- 1-- 5 
6 Oq- 001 028 344 771 960 997 1-- 1-- 1-- 1-- 1-- 1-- 6 
7 Od- Oq- 007 189 611 904 989 999 1-- 1-- 1-- 1-- 1-- 7 
8 Od- Oq- 002 089 435 808 968 998 1-- 1-- 1-- 1-- 1-- 8 
9 Oq- Oq- Oq- 036 275 672 924 992 1-- 1-- 1-- 1-- 1-- 
10 Oq- Od- Oq- 013 153 511 846 978 999 1-- 1-- 1-- 1-- 
11 Oq- Od- Oq- 004 074 350 729 947 996 1-- 1-- 1-- 1-- 
12 Oq- Oq- Oq- 001 031 213 581 886 988 1-- 1-- 1-- 1-- 
13 O+ O-i- O+ O+ 012 114 419 787 969 999 1-- 1-- 1-- 
14 O+ O+ O+ O+ 004 053 271 650 926 996 1-- 1-- 1-- 
15 O+ O+ O+ O+ 001 022 154 489 847 987 1-- 1-- 1-- 
16 O+ O+ O+ O+ O+ 008 076 328 725 964 1-- 1-- 1-- 
17 O+ O+ O+ O+ O+ 002 032 192 565 911 998 1-- 1-- 
18 0-]  0-]  0-]  0-]  O+ 001 011 096 389 811 993 1-- 1-- 
19 O+ O+ O+ O+ O+ O+ 003 040 229 656 972 999 1-- 
20 O+ O+ O+ O+ O+ O+ 001 013 111 460 915 994 1-- 
21 O+ O+ O+ O+ O+ O+ O+ 004 042 26t 786 970 1-- 
22 O+ O+ O+ O+ O+ O+ 04- 001 012 115 564 884 998 
23 O+ O+ O+ O+ O+ O+ O+ O+ 002 033 292 661 976 
24 O+ O+ O+ O+ O+ O+ O+ O+ O+ 005 080 292 786 
0 1 1 1 1 1 1 1 1 1 1 1 1 1 
1 222 723 928 996 1-- 1-- 1-- 1-- 1-- 1-- 1-- 1-- 
2 026 358 729 973 998 1-- 1-- 1-- 1-- 1-- 1-- 1-- 1-- 
3 002 127 463 902 991 1-- 1-- 1-- 1-- 1-- 1-- 1-- 1-- 
4 O+ 034 236 766 967 998 1-- 1-- 1-- 1-- 1-- 1-- 1-- 
5 O+ 007 098 579 910 991 1-- 1-- 1-- 1-- 1-- 1-- 1-- 
6 O+ 001 033 383 807 971 998 1-- 1-- 1-- 1-- 1-- 1-- 
7 O+ O+ 009 220 659 926 993 l-- 1-- 1-- 1-- 1-- 1-- 
8 O+ O+ 002 109 488 846 978 999 1-- 1-- 1-- 1-- 1-- 
9 O+ O+ O+ 047 323 726 946 996 l-- 1-- 1-- 1-- 1-- 
10 Oq- Oq- Oq- 017 189 575 885 987 1-- 1-- 1-- 1-- 1-- 
11 O+ O+ O+ 006 098 414 788 966 998 1-- 1-- 1-- 1-- 11 
12 O+ O+ O+ 002 044 268 655 922 994 1-- 1-- 1-- 1-- 12 
13 O+ O+ O+ O+ 017 154 500 846 983 1-- 1-- 1-- 1-- 13 
14 O+ O+ O+ O+ 006 078 345 732 956 998 1-- 1-- 1-- 14 
15 O+ O+ O+ O+ 002 034 212 586 902 994 1-- 1-- 1-- 15 
16 O+ O+ O+ O+ 04- 013 115 425 811 983 1-- 1-- 1-- 16 
17 O+ O+ O+ O+ O+ 004 054 274 677 953 1-- 1-- 1-- 17 
18 O+ O+ O+ O+ O+ 001 022 154 512 891 998 1-- 1-- 18 
19 O+ O+ O+ O+ O+ O+ 007 074 311 780 991 1-- 1-- 19 
452 TABLE IV 
n 
PART ]: CUMULATIVE TERMS  b(3; T, p) 
p 
n r .01 .05 .10 .20 .30 .40 .50 .60 .70 .80 .90 .95 .99 r 
25 20 0+ 0+ 0+ 0+ 0+ 0+ 002 029 193 617 967 999 1-- 20 
21 0+ 0+ 0+ 0+ 0+ 0+ 0+ 009 090 421 902 993 1- 21 
22 0+ 0+ 0+ 0+ 0+ 0+ 0+ 002 033 234 764 966 1- 22 
23 0+ 0+ 0+ 0+ 0+ 0+ 0+ 0+ 009 098 537 873 998 23 
24 0+ 0+ 0+ 0+ 0+ 0+ 0+ 0+ 002 027 271 642 974 24 
25 O+ O+ O+ O+ O+ O+ O+ O+ O+ 004:072 277 778 25 
453 
CHART I 
Chart for 95% confidence limits on p, the probability of success on a singlc 
binomial trial. 
To obtain confidence limits for p enter the horizontal axis at the observed 
value of . Read the vertical axis at the two points where thc two curves for ? 
cross the vertical line erccted from . /x�:  = 0.3,  = 50, lower con- 
fidence limit 0.18, upper confidence limit 0.45. 
By permission of the Biometrika Trustees this chart has bccn reproduced 
from C. J. Clopper and E. S. Pearson, "The use of confidcnce or fiducial limits 
illustrated in the casc of the binomial," o�lr{, Vol. 26 (1934), p. 410. 
ANSWERS TO EVEN-NUMBERED EXERCISES 
Section 2-1 
2. 720; 480; 240 4. 120; 216 
6. 3,024 8. 720 
10. 14,400 12. 255 
14. 9! -- 1 16. 502 
18. 1024; 2 ' 20. n r 
Section 2-2 
2. 1 4. 5040 
6. 10(9)8 = 720 = 6! 8. 288 
10. 1440; 3600 12. 100,000; 30,240; 69,760 
Section 2-3 
2.1 4.56 
6. 399 or 400 8. 45,120; 210 
10. 20!/(3!5!12!) 12. (19�s�) > (4 �) 
14. 301(201)101 --2 16. 21 
18. 330; 330 X 12! X 8! 
Permutation ad Combinations: Sets . and B 
2. 2450, 1225 4. 190 
6. 8!/3! 8. 360; 120; 240; 12 
10. 56 12. 3,168 
14. 3,360 16. 90, 18; 72 
5 4 
18. (4s)(15�)() 20. (a)(a)6! 
22. 8(9!) 24. 8! -- 1 
26. 171 
Section 2-4 
2. 10!/[(2!)2(3!) 2 ] 4. 1716 
6. 210 $. 11!/[(3!)(2!)2];9!/[3!2!2!];3(8!) 
10. 126 12. 11(11!)/[3!4!5!] 
455 
456 ANSWERS TO EVEN-NUMBERED EXERCISES 
Review Exercises 
2. 72 
4. 9204 (including no cutting at some positions, but excluding complete 
blanks) 
6. 16 
8. No. He has missed 2 
Section 2-5 
2. 101; m 1��, m99n, m98n 2 
4. (14%0)a51x49; (11%0)aSlx19; (16%0)a40x60 
6. I - 5b - lob 2 - lob 3 - 5b 4 - bS; 
I - 5(.01) - 10(.01) 2 - 10(.01) 3 - 5(.01) 4 - (.01) 5 
8. I - 7p - 21p 2 - 35p 3 - 35p 4 - 21p 5 - 7p 6 - p7 
10. I -- 5x2 - 10x 4 -- 10x6 - 5x s -- x l� 
12. x 6 - 6xSy - 15x4y 2 - 20x3y 3 - 15x4y 2 - 6xSy - y6 
14. I - 2a - a 2 - �a 3 - a 4 
16. a 6 - 6a6x - 15a6x 2 - 20a6x 3 - 15a6x 4 - 6a6x 5 - a6x 6 
18. x TM -- 6x 3  15x TM -- 20x 5  15x 6 -- 6x TM  x s 
20. p5O _ 50p49q _ 1225p4Sq2 
22. I -- 40a2 - 780a 4 
24. 1.02 26. 1.02 
28. 3.015 30. 5.1 
32. 121.25 34. I- x 
36. 2 - 2x 40. 3; 6; 10; �(n - 1)(n -]- 2) 
Section 3-1 
2. 1. . 
� 4. 
2 
6. 4.5 12. 
99 
14. ; ; 4 (assuming that 1 is not a prime); 
16. 
Section 3-2 
2. IHHH, HHT, HTH, THH, HTT, THT, TTH, TTT} ; 
{0, 1, 2, 3}, where no. denotes no. of heads 
4. l pr, pd, pq, np, nd, nq, . . .} 
6. A = l bbbb, bbbg, bbgb, bgbb, gbbb, bbgg, bgbg, bggb, ggbb, gbgb, gbbg, bggg, 
gbgg, ggbg, gggb, gggg} 
For A: 16; 4; 8 
8. See first 3 rows and columns of Table 3-3. 
10. {ABC, ABD, ABE, ACD, ACE, ADE, BCD, BCE, BDE, CDE} 
6; 4; 3; 9 
12. Sets (a), (b), and (d) are acceptable 
14. 3; 6; 18; 6; 2; 21 
ANSWERS TO EVEN-NUMBERED EXERCISES 457 
Section 3-3 
2.- 
4.3 
6. r  c;r = c- 2;c _ r-2;r ---- 2c 
1. 1 
8. ; �;�;�; 6, 6 
10. ; ; ; ;  
12. {; ; ; {; 1. 3 
2 8 
14. }; {;  
Review Exercises for Sections 3-1, 3-2, and 3-3 
2. 
35 7 
14 
6. {rational, irrational} 
8. Two are: {1, 2, 3, ..., 49}, where no. denotes position of first ace, num- 
bered from top; and {Ace on top, Ace not on top} 
10. {2, N2, NN2, NNN2, ...}. Note.' This sample space is not finite 
12. aa, aA, AA 
14. 2652; x . 4 . s 
2652 63 663 
Section 3-4 
2. 4.?= c.o 
s.  lO. i 12.  
Sectioa 3-5 
2. Yes; yes 
4. Less than or equal to 0.4; yes; perhaps 
c. ' s. I lO.  12 1 
 � 
14.  lC. { is.  20.  
Section 3- 
i 4. 
2. (1)(1) -  
(})(1)  0 
6. Not independent 8. s 
144 
Section 3-7 
= 4.} 
2.=.= 
s. lO.I 
12. xs 14.  
1. } 18. 1 . 44. 8 
20. 0.00075; 0.02425; 0.94575 22. 7 
24. xs xx  26.  
 5, 35 
458 ANS]VERS TO EVEN-NUMBERED EXERCISES 
Section 3-8 � 
12.11 ... (12 -- k q- 1) 
2. 1 -- (447)](542); 1 4. I -- 
12 k 
6. 1128. 6; 
748 
729 
Section 4-1 
2. 0.51; 0.70; 0.70; 0.58 
6. (a) to (d): 0.0064; (c) to (j)' 0.0256; 
(k) 0.0256; (1) 0.1536 
Section 4-2 
2. �,�; 3. 3. 
' 4, 4, 2, 2, 4,  4. 0.025; 0.005; 0.03 
6. 1/(n q- r); (r q- 1)/(n q- r) 8. {-; 3; � 
10. 11 12. 60% against it. 
,Section 4-3 
6. 0.000001; about 0.97 
8. $; 17. 2. 
36 9 6 
Section 4-4 
2. P(AIB ) = 1 4. � 
6.� 10. 2'3 
12. About 0.205 14. 4. 
5 10 
16. P(k) = log k/log [(2n)!] 
Section 4-5 
2. 4. 1. 4 
g, 1; 0; 4. r/(b  r)' r/(b q- r); r/(b q- r) 
5 ' ' 
6. 1 . 1. 1. a. cO 8. \52 \51/\ \�- \�'!  0.082 
''   4, 0; 10 
10. 0.0002228 12. l-Z6; ia6o 
14. 1. 6 
Section 4-6 
$ . 9 . 6 
2. 23 23 23 
I. 12 
4. 13 13 
8 
6.; 
9 
8. (a) Urn I: 1--, � Urn II: 10' 4 3- 
10' 28 
10. About 1/10 4 
12. 12 
14. Odds proportional to 3 X 99 =, 5 X 97 , 2 X 90" 
16. q 
18. P(HllE)  1;P(Hi[)  � 
ANS]VERS TO EVEN-NUMBERED EXERCISES 459 
Miscellaneous Exercises--Chapter 4 
125 
8.- 10.  12.  
396 
14. 66 16. About 0.1 18. About 0.011 
22. About 0.15 24. About 0.33 26. About 0.025 
Section 5-1 
z 3 
6. f(z) = ()(6-- [z-- 7[) fo z = 2,3,..., 2 
14. 4 
x 
6 
16. -- 
x 2 
Section 5-2 
2.1 6.1.2 
8. 83 10. 1 
12. 7 14.  
16. No; 86 18. 12 
20.  (dollar) 22. () 3.52 cents (b) 352 
24. E(X) = 1 for 11 n. The limit of E(X) s 1 
Section 5-3 
Probability [ 0.2 _ 0.1 0.3 0.3 0.1 
Vlues of 3X -- 1 --7 --4 --1 2 5 
Probbillty  0.3 [ 0.4 0.3 
Values of X 2 [ 1 0 
E(X ) = 1.6 
6. 2.7 
8. 8.9 
460 ANSWERS TO EVEN-NUMBERED EXERCISES 
St. Petersburg paradox exercises  
f(x)  � 
x I 2 
4  16 
12.-- 
x I 2 4 
14. 2 16. 3 
18. E(Y) tends to infinity 20. Infinitc amount 
22. $20.27 
Section 5-4 
2. () 1; I (b) I 4. () 0.2 (b) O.S 
(c) o.7c () o.s 
6. 3.4; 0.84 12. 0.00256; 2.15  10-; 1.47  10 -s 
14. 3.1; 0.1275; 0.357 16. 2.7; 0.81; 0.9 
18. 720; 1100; 33.2 20. 4;  
22. ' ' 0.927 24. 76; 3.04; 0.19 
8 
Section 5 
2. 2; ; 0.816 4. O; 4; 2 
6. ; 0.042; 0.205 8. 2; 1; 1 
10. --7; 2; 1.41 
12. Inconsistent bccusc thc smple wrince is never negative 
18. Y = X  900 20. 1.4; 0.49; 0.7 
22. () 7.25; 0.922 (b) 72.75; 97.25; 99.875 
Section 5-6 
c. 1;] 
8. 20 12. x. .  14. O; 2cp 
 9  25 
18.  20. ; 0 22. Yes; 1/(2h); 1/h  
Section 5-7 
2. 3n/4 4. 5 
6. 5% 8. n/4; 
Section 6-1 
. (xr):  (zz). E(') =  
(x + r)   - (-) + (�) 
o  
f(w) 
4. 
w 2 
E(W) 
.XSWERS TO EV-XUBRD XERCSS 461 
6. (a).2, .8, .6, .4 
(b) Predict Y = 1 if X = O; predict 1' = 0 if X = 1 
(c) 70% 
8. No 
lo. (a) 0,3 (b) (c) -] (d) 0 (e) � 
12. 4 
Section 6-2 
2. �; �' oo; 1 -- Or/4) = 0.215 4. ' 
Section 6-3 
2. 29. 
�, yes 
4. Jump at x is fix) as given by Table 5-5 
Section 6-4 
2. h A(h) 0.4h 0.4h -- A(h) 
0.01 0.0040 0.004 0'- 
0.1 0.0398 0.04 0.0002 
0.2 0.0793 0.08 0.0007 
0.3 0.1179 0.12 0.0021 
0.4 0.1554 0.16 0.0046 
0.5 0.1915 0.20 0.0085 
0.76 0.2764 0.304 0.0276 
0.77 0.2794 0.308 0.0286 
Difference exceeds 10% of A(h) when h = 0.77 
4. (b) From Table 6-7, sum is 0.9999 
S. (a) --0.253 (b) --1.645 
(c) 1.645 (d) 0.675 
10. (a) 0.0228; 0.7881 
(b) 11.85 X 10-6; 0.9985 
Section 7-1 
4. 19 
6. Theoretical: 192, 288, 144, 24 
10. Sum of probabilities = p4 _ 4p3q ._ 6p2qZ._ 4pq3 ._ q4 
= (p_q)4 = 1 
Probability __ 4pq3 6p2q2 4p3q p4 
Number of U's ----- - 4 
Section 7-2 
b(x; 5, .4)..078 .259 --------- .010 
2. .346 .230 .077 __ 
x 0 I 5 
462 ANSWERS TO EVEN-NUMBERED EXERCISES 
4. Vhen n = 2, the probability is 1 
6. Two-engine plane preferrcd unless p = 0 or 1 
13 
8. 16 10. 729 
14. � < q < 1 16. � < q < 1 
Section 7-3 
2. 3O 
Section 7-4 
2. (a) 0.617 (b) 0.163 
4. (a) 0.982 (b) 0.256 (c) 0.024 
6. 0.33 
8. (a) 0.133 (b) 0.205 (c) 0.927 
10. 0.993; 0.196; 0.579 
12. 0.050 
14. 0.956 
16. 0.026 
18. 0.358; 0.016 
20. Theoretical: 25.6, 128, 256, 256, 128, 25.6 
22. (a) Both equal 0.412 
(b) Both equal 0.166 
24. (a) Both cqual 0.919 
(b) Both equal 0.263 
f(x) .031 .156 .312 .312 .156 .031 
x 0 1 5 
Section 7-5 
2. 225,  
10. 0.211 or 0.789 
12. Interval Actual Chcbyshev Empirical 
probability (at least) rule 
10_ x_ 15 .770 0 .68 
7.5 < x < 17.5 .956 .75 .95 
5 _ x _ 20 1-- .89 [ .997 
Section 7-6 
2. (b) 0.120 (c) 0.240 
(d) 0.240 = b(O; 4, 0.3), and 
0.120 = �b(O; 4, 0.3) 
4. (b) p, =np = 0.8, o- = %/npq = 0.$ 
ANS]VERS TO EVEN-NUMBERED EXERCISES 463 
Section 7-7 
2. n I.10 4 8 16 
Max. error in 
cumulative .005 .004 .002 
6. 0.0262, 0.0812 
8. (0.3989)(0.0632) . 0.0252 
Section 8-2 
4. 0.943 6. 0.0100 8. 0.700 
12. 0.133 14. 9 16. 0.887 or 0.113 
18. 0.178 20. 0; 0.6826 22. Negative; 0.3108 
24. 40 26. 0.032 28. 0.9282 
30. nm/r 
Section 8-3 
2. (0.66, 0.94) 4. (0.65, 0.83) 
6. (0, 0.15) 8. n _ 1000 
Section 8-4 
2. 0.661, 0.289, 0.049; 2.61 4. 0, 0.370, 0.630; 8.15 
Section 8-5 
2. 0.09, 0 25 4. Graph 
6. n = 10, r = 1, p = 0.0050; 8. 220 
n = 40, r = 3, p = 0.0225; 
n = 100, r = 7, p = 0.0375; 
n = 1000, r = 70, p = 0.0550 
10. Let J(p) bc the probability of accepting lot under original plan; under 
new plan, the probability is 2 -- J(p) times as great 
12. 1.1% 14. Yes; descriptive level is 1.6% 
16. No; descriptive level is 5.23% 18. (0.62, 0.97) 
20. (a) Coin is unbiased 22. Yes 
(b) 
(c) When the coin is biased in favor of heads 
24. (a) Clmncc of getting any pair correct is p = � 
(b) p  � 
(c) 5.5% 
(d) 0.678 
26. 1300 28. Yes 
30. Once, if he means p is exactly 1 
s 34.  
32. 11o 
464: ANS]VERS TO EVEN-NUMBERED EXERCISES 
Sectio 8-6 
2. 44 to I 4. n = 1.66x 
6. 0.9977 8. 1 to 12 
Section 9-1 
2 __ 17 
= ; (d) x+' = ,x+ -  
(e) z 2, =  -- 
(e) x+'+z ,  _ 4 
=  ax+Y+z -- ' 
Probability - -   & 
18 18 1 
4. (a) Values of U I I __  { 1 
6 6 6 
(b) 0 
(c) 1. 
, yes 
9 
8. -- 
Values of U 0 1 2 4 
2 2 2 
 = ] = + 
Probability       
10. 
of v 
E(U) = 2 = 2E(X), Vr(U) = 1 = 2Vr(X) 
7 
Probability   
12. ' 
Vlucs of median 1 2 3 
Section 9-2 
2. (a) 5 (b) 24 (c) 41 
(d) 6 (e) 15 (f) 5 
(g) v' (h) / 
4. Pythagorean theoreln wxth .x' and  being the lengths of the sides ad- 
jacent to the right angie and v the length of the hypotenuse 
6. 0.003x/ = 0.0042 
8. --�; 3.23 (dollars) 
10. Mean = 1000; standard deviation would be 100x/ if Mathematics 
and English scores were independent, but they probably are not 
14. (a) / (b) (1 -- 2w-]- 2w2)r  
(c) � (d) r2/2 
18. %7 
ANSVERS TO EVEN-NUMBERED EXERCISES 465 
Section 9-3 
2. 5 4. 9; l0 6. 24; 20 
8. � 10. --2.42 12. 20; 2.5 
14. 0.0026 16. 0.0228 18. 12 
20. 0.0456 
24. Choice (a) 
26. (a) (b) 0.0228 (�) 0.0014 
(d) Yes; that births in a family form a series of binomial trials with 
regard to sex 
Section 9-4 
2. Coy(X, Y) = 0 6. alia I 
12. p = --1/(n-- 1) 14. (a) 1 (b) 1 
16. Theoretical mean and variancc are both 1. 
18. 2na  24. (a)  (b)  (c) 6 
26. 0 28. (a) 
(b) (o) 
 1296 
30. (a)  
(b) 1296 
32. Sccond die 
1 122- 3 4 (5-- 6 
1 (1, 0) (3, 2) (4, 3) (6, 5) 
2 (2, 1) :2, 0) (3, 1) (4, 2) (5, 3) [ (6, 4) 
3 (3, 2) :3, 1) [ (3,0) (4, 1) (5, 2) [ (6, 3) 
Fxrst die 4 (4, 3) :4, 2) 4, 1) (4, 0) (5, 1) (6, 2) 
5 (5, 4) 5, 3) 5, 2) (5, 1) (5, O) (6, 1) 
6 (6, 5) (6, 4) (6, 3) (6, 2) (6, 1) (6, 0) 
X = maximum, Y = }diel --die2 
Cell entries: (X, Y) 
34. 66. 0.51 
6--  
Section 9-5 
2. (a) o; 5 
(b) Probability   
Sample ave. --3 -- 1 3 
 = 0, a 2 = 5 
4 
Sample ave. -- 
_ 5 
/ O; er -- 9 
6. Means are equal 
8. S = {(N, D), (N,Q), (N, II), (N, $), (D,Q), (D, It), (D, $), (Q, H), 
(Q, $), (H, $), and permutations of these} 
406 ANSWERS TO EVEN-NUMBERED EXERCISES 
10. 0.76; 0.1809 
12. 
Proba- ? i!6o io 26  
1 1 1 1 
bility 19 Y 1-- Y . 0 1 
lO 
Sample [-75 --5 -00 [-7- 
ave.: 
n = 2 .075 .150 I  .525 .550 .750 
g2 = 0.38; (r = 0.045225 
14. Ex. Variance 
10 0.1809 (sum of 2) 
11 0.1206 (single obs) 
12 0.0452 (average of 2) 
16. S = {1.20, 1.40, 1.80, 2.00, 2.40, 2.60} 
 = 1.90 = 2(0.95) 
(r 2 = 0.25 = 2(0.1875)(]) 
18. 1.5; 0.655 20. 0.193 
30. 32 32. 0.000395 
34. 9 (ahnost as accurate) 
36. 10 with eplacement because 2/10 is slightly less than 42/39 
38. (a) 100 (b) 33 ' 
Section 10-1 
2. (a) 26,400 (minutes) 4. y = --x -]- � 
8. Lines of form y -- - = m(x -- �) and x = � 
10. !7 = 32 (it/see/see) 
Section 10-2 
2. 6.2; 22.8 4. --4; 632 
6. 3; 454 
8. Values of S: 5; 2.75; 2; 2.75; 5 
� 1 
10. x = 1, y = 0 12. x = --1/(2a); y = a 4 
Section 10-3 
2. y = x/2 4. y = 10x/3 
6. y = 0.0168x; sin 5�: estilnated 0.084, true 0.087; sin 25�: estimated 0.420, 
true 0.423 
$. y  1.01x 10. y = 0.Sx 
12. / = /x 
Section 10-4 
2. In absolute magnitudes, middle deviation tvice an end deviation 
4. y  --1 q- 0.67(x -- ); sa  0.53 
6. /h = --; d = 0 8. B; C; C-- or D-I- 
ANS'WERS TO EVEN-NUMBERED EXERCISES 467 
10. 7 
12. When the x-coordinates are identical, so that (x -- )2 = 0 
18. See answer for Exercises 2 and 3 
Section 10-5 
6. y = , for every x 8. 9%; 9%; 64% 
10. y = rx 14. 10 
Section 10-6 
2. x = y/2 4. x = 
6. x = y/2 8. y -- 2q- 
12. y = 4.56 q-0.88x 
14.  -- 67.70 
Yy = 68.60 
sx = 3.075 
Sy = 3.069 
y = 39.04 q- 0.436x 
x = 37.65  0.438y 
APPENDIX I ANSWERS 
Section I-1 
2. (b) 30 
(c) 4 (0, 4), (1, 3), (3, 1), (4, O) 
5 (0, 5), (1, 4), (2, 3), (3, 2), (4, 1), (5, O) 
6 (1, 5), (2, 4), (4, 2), (5, 1) 
7 (2, 5), (3, 4), (4, 3), (5, 2) 
8 (3, 5), (5, 3) 
9 (4, 5), (5, 4) 
4. (a) Straight line, of slope 1, through the oilgin 
(b) Points above the line y = x 
(c) Points below the line y = x 
(d) Points above the liney = xq- 1 
(e) Points below and on the line x q- y = 4 
Section I-2 
2. (a) {x:x is a person in my commuiity} 
(b) { x: x is an even number } 
(c) {y:y = x 2 and x is an integer} 
In (a), the roster method is difficult because we probably do not know the 
names of all people in the community, and there are also more of them than 
we care to list. In (b) and (c), the sets are infinite and it is impossible to 
list the elements explicitly. 
468 ANSWERS TO EVEN-NUMBERED EXERCISES 
Further answers for Section I-2 � 
4. (a) {x:lxl < 4/ (b) {x:lxl > 6} 
(c) {x:lx I > 2} (d) {x:Ix I _ 1} 
(e) {z:Iz-5]_< 2} (f) {z:Iz--71 < 3} 
6. (a) {z:z < O} (b) {z:z < 7} 
(e) {x:x _ 7} (d) {x:--3 _ x_ 
(e) {x:]xd-21 < 11 (f) {x:Jxd- 1.51 > 6.5} 
(g) {x'lx-- 2] _ 1} (h) {x:x 2_ 25} 
(i) {x:lx I _ 12} (j) {x:lx-- k I _ 3} 
Section I-3 
2. If you have five fingers on ),our right hand; 
(a) 31 (b) 32 
4. 2n--1 
6. Proof. Suppose the elements of U are ex, e2, . . . , e. In forming a subset 
of U we deal with e in two ways (either take it into the subset or leave 
it out). Similarly, having dealt with e, there are 2 ways of dealing with 
e; thereafter, 2 ways of dealing with ea; and so on. By the multiplication 
principle there are 2 X 2 X .'. X 2 (n factors) or 2 n ways of forming 
different subsets of U, including the empty set and U itself. 
APPENDIX II ANSWERS 
Section II-1 
2.14 4.9 
6. 25 8. --10 
10. 303 
3 
12.  (ak-- bk) = ao d- hod- a d- bx d- a2 d- b2 d- aa d- b 
k0 
= (ao d- ax d- a d- a) d- (bo d- bx d- b d- b) 
3 3 
k=0 k =0 
One generalization is obtained by replacing the upper limit of summation, 
3, by n: 
k=0 k=0 
A second generalization is obtained by incrcasing the number of sum- 
mands: 
= 
=o =o =o =o 
14. nc 
ANSVERS TO EVEN-NUMBERED EXERCISES 469 
Section 1I-2 
2. 
4. (2.2 -- 5) q- (2.3 -- 5) q- (2.4 -- 5) q- (2.5 -- 5) = 8 
6. (2xo -- 3yo) - (2x -- 3y)  (2x -- 3y) 
= 2(xo  x  x) -- 3(yo  y - y) 
(2 2 o (2   (2 o 2 2 y2 
8. oAx y  Ax y  Ax y = x  2xy  = (x  y) 
0 2 12 2 2 
lO. 
12. (n+ 1) 3 =  [(k+ 1) 3 -- k 3] =  (3k2+ 3k+ 1) 
k=0 k =0 
= 3k23k1 
k 0 k=0 k=0 
Hence: 3k2 (nl) --3k--1 
k 0 k =0 k0 
14. k = k - (n 1)n, 
2 
k=O kl 
because the first term in the summation of Eq. (6) is 0 when k = 0. 
Similarly, the first term of the summation in Eq. (7), Ex. 13, is 0 when 
k = 0, so that 
 k 2 =  k 2 = n(n  1)(2n 1) 
6 
k0 kl 
16. 
=1 
=1 1 1 
18. a 
BIBLIOGRAPHY 
CRAMm, H., The Elemets of Probability Theor!! and Some of Its Applications. 
New York John Wiley & Sons, Inc., 1955. This book uses integral calculus in 
some prts. 
DXON, V. J., and F. J. MAssY, JR., Itrodctton to Statistical Analysis. 
New York: McGraw-Hill Book Company, Inc., 1957. 
GOLDEa, S.. Probability--A, Introdaction. Englewood Cliffs, N.J.: Prentice- 
Hall, Inc., 1960. 
Gmx Boor: Introductory Probability and Statistical Inference. 425 West 117th 
Street, New York: College Entrance Examination Board, 1959. 
HODES, J. L., Ja., and E. L. LEnMXh', Basic Concepts of Probability and 
Statistics. San Francisco 11, Calif. (728 Montgomery St.): Holden-Day, Inc., 
1960. 
M.cr, S. F., Elementary Statistics. New York: Henry Holt and Company, 
1960. 
WXLLS, W. A., and H. V. ROEaTS, Statistics--A New Approach. Glencoe, 
II1.: The Free Press, 1956. 
WLrS, S.S., Elementary Statistical Analysis. Princeton: Princeton University 
Press, 1949. 
Except as noted, the above books require approximately the same level of 
mathematical preparation as this course. Any one of these books is suited for 
supplementary reading. 
The folloxxing books on probability are of a more advanced nature: 
FELLER, W., An Introduction to Probability Theory and Its Applications. 
New York: John Wiley & Sons, Inc., 1957. 
NEYMAN, J., First Course in Probabilit and Statistics. New York: Henry 
Holt and Company, 1950. 
PxazE, E., Modern Probability Theory and Its Applications. New York: 
John Wiley & Sons, Inc., 1960. This book requires calculus. 
The following books, all requiring calculus, can serve as introductions to 
mathematical statistics: 
Baux, H. D., An Introduction to Mathematical Statistics. Boston: Ginn and 
Company, 1960. 
FaXSEa, D. A. S., Statistics--An Introduction. New York: John Wiley & 
Sons, Inc., 1958. 
471 
472 BIBLIOGRAPHY 
HtoEL, P. G., Introdction to Mathematical Statistics. New York: John Wilcy 
& Sons, Inc., 1954. 
The following book offcrs an introduction to combinatorial mathematics: 
RIORDAN, J., An Introduction to Combinatorial Analysis. New Work: John 
Wiley & Sons, Inc., 1958. This is not an easy book. 
Mathcmatics refresher: 
WALKER, H. M., Mathematics Essential for Elementary Statistics. New York: 
Hcnry Holt and Company, 1951. 
Also, Chapter i of Mack, see above. 
The following books offer a large variety of illustrations of uscs of probability 
and statistics: 
BROSS, I.D. J., Design for Decision. New York: Macmillan Company, Inc., 
1953. 
Cox, D. R., Planning of Experiments. New York: John Wiley & Sons, Inc., 
1958. 
EDWARDS, A. L., Experimental Design in Psychological Research. New York: 
Rinehart & Company, Inc., 1950. 
FISHER, R. A., Statistical Methods for Research Workers. New York: Hafner 
Publishing Company Inc., 1958. 
GRANT, E. L. Statistical Quality Control. New York: McGraw-Hill Book 
Company, Inc., 1952. 
LEVINSON, H. C., The Science of Chance. New York: Rinehart & Company, 
1950. 
SCHLAIFER, R., Probability and Statistics for Business Decisions. New York: 
McGraw-Hill Book Company, Inc., 1959. 
SNEDECOR, G. W., Statistical Methods. Ames, Iowa: The Iowa State Collcgc 
Press, 1956 (applications to experiments in agriculture and biology). 
WILLIAS, J. D., The Cornpleat Strategyst. New York: McGraw-Hill Book 
Company, Inc., 1954 (elemcntary game thoery). 
WILSON, E. B., JR., An Introduction to Scientific Research. New York: 
McGraw-Hill Book Company, Inc., 1952. 
YOUDEN, W. J., Statistical Methods for Chemists. Ncw York: John Wilcy& 
Sons, Inc., 1951. 
INDEX 
Numbers in parentheses refer to exercises on the indicated pages. 
Absolute deviation, mean, 184 variance,  = tpq, 327 
Absolute value, 408 walking, 263 
Acceptance sampling, 301 Binomial experiment, 241, 245 
Addition principle, 26 Binomial experiment% combined, 255 
Agricultural appheations of staffsties, (17) 
6 Binomial probability tables, 258, 
AIKEN and BESEMAN, 416 433 
Airplane safety example, 252 Binomial random variable, 244, 245 
Alternative hypotheses, 304 adjusted, 270 
AMERICAN MaTHE.raTICaL MONTHLY, expected value of, 257 
3 probabilities approximated by 
Approximation of (1 d- x)", 52 normal, 272 
Average, 168 Binomial tables, 433 
and variance in samples, 194, 197, list of, 259, 260 
199 Binomial theorem, 50 
expected value of, 330 Binomial trials, 243 
Axioms for probaMhty, 118 Birthday problem, 95 
BREUnR, J., 416 
Bayes' theorem, 146 
Bayesian approach to confidence CA,DAN, 256 
limits, 299 Census, U.S., 6 
Bayesian inference, 310 Central limit theorem 
BERNOULLI, JAMES, 245 for binomial, 275, 280 
Bernoulli trials, 245 for identically distlibuted valiables, 
Bias, 331 328 
Binomial, mean proportion of Chebyshev's theorem 
successes, 332 and empirical rule, 208 
Binomial coefficients, 50 for frequency distribution, 207 
Binomial distribution, 251,433 for probability distribution, 203 
central limit theorem for, 275, 280 CHRISTIAN, R., 416 
fixed n, varying p, 262 Chuck-a-luck, 173 (19) 
fixed p, increasing n, 263 Combinations, 33 
flattening, 263 definition of, 34 
mean of,/ = np, 257 of n things, r at a tilne, 35 
mean and variance, 327 Complement of a set, 414 
mode proportional to 1/V, 263 Complementary events, 79, 80, 121 
probabilities as incas, 269 Complete independence, 127 
properties of, 262 Con&tlonal probability, 88, 133 
spreading, 266 and reduced sample space, 88, 134 
standard deviation,  = V'pq, 266 of independent events, 91 
tables 433 Confidence interval 
individual terlns, 435 conservative, Chebyshev, 290 
cumulative terms, 444 conservative, normal, 291 
473 
474 D:X 
Confidence limits, 285 Elementary event, 116 
Bayesian approach, xxith prior � Empty set, 4:12 
inforination, 299 Epidemles, 
chart for, 95%, 453 Equally likely outeoines, 55 
conservative Chebyshev approaeh, Estimation, 285 
290 and hypothesis testing, 285 
conservative normal, 291 of binoinial p, 286 
conservative, with large n, 295, 297 pi'eeision of, 386 
optimistic approach, 298 Events, 60 
Correlation and sets, 73 
and precision of estiination, 386 eoinplementary, 79, 80, 121 
coefficient, sample, 342 elementary, 116 
eomputationat formula, 341 independent, 81, 82, 124, 125 
population, 339, 340 independent and dependent, 82 
Covarianee, 336 mutually exclusive, 76, 121 
Craps, 153 (8 through 15) probability of, 60, 69, 117 
Cumulative distribution funetlon, 225 Expeetation, 169, 176, 212 
for binoinial, 444 (also see mean) 
for noimal, 432 Expected value, 170, 176, 212 
Cumulative probability graphs, 224 (also see mean) 
Curve fitting, 360 Extrasensory perception (ESP), 310 
D'ALLMERT, 56 Faetoriat, 28 
Degree of belief, 3 table, 431 
D1; .IRI], 3 FEHR, H. F., 416 
Dc MOlW'e-Laptacc theorem, 280 FELLER, W., 17 
Dependent random variables, FERMAT, 4 
covarlance and correlation, 335 Field tests, 6 
Descriptive level of significance, 304 Finite population, sampling without 
Deviations, sum of, 373 replacement, 349 
Dice, sample space, 68 variance of average, 354 
Differences, mean and variance of, of sum, 354 
322 Finite sample space, 116 
Digits First ace, 7, 93 
first, distribution of, 13 average count, 10 
last, of telephone nulnbcrs, 12 count frequencies, 9 
random (see random digits) and regression, 397 
Disjoint events, 76 Frequency distribution, 9 
Disjoint sets, 414 
Distribution Genetics and radiation, 6 
frequency, 9 Gcotogy, 7 
of a sum, 314 
of  = .Y/n, 292 HXLMO, P. R., 
probability, 160 Heredity, 5 
Divisors, 158 Hypcrgeometrie distribution, 99 
Drugs, screening of, 6 Hypotheses, null and alternative, 
Duration of play, 4 304 
XVEX 475 
Hypothesis testing, 285 McLENNAN, R. C., 416 
null, other than p -- �, 305 MCSHAN L E. J., 416 
of p = Po in binomial, 301 Mean, 167, 169 
one-sided tests, 304 and variance 
of binomial distribution, 327 
Identically distributed random of normal distribution, 236 
variables, 326 of , 292 
Independence, S1, 82, 124, 125, 127, of sample averages, 330 
212 of a function, 176 
of several events, 84 of tx o variables, 212 
of two random variables, 212 of binomial distribution, 257 
Independent events, 81, 82, 124, 125 of  = X/n, 286 
three or more, 127 of product of independent random 
Independent random variables, 427 variables, 217 
Inequalities, 407 of sum of several random variables, 
Inheritance in biology, 5 216 
Intersection of sets, 413 of sum of two random variables, 214 
Interval estimate, 285 Mean absolute deviation, 184 
Mean squared deviation, 185 
Mean value of a random variable, 167 
Joint probability function, 211 
ENDEL, GREGOR, 5, 58 
Mendelian theory, 5 
KEMEN�, J. G., 416 
Milon Random Dgts, A, 15 
KERRICH, J. E., 56 
Minimizing a quadlatic function, 369 
Key problems, 220 (l l, 12) Minimum variance unbiased estiinate, 
KOLMOGOOV, A., 2 324 (14) 
Models, mathematical, 2 
Law of cosines, 336 probabilistic, 2 
Law of large numbers, 292 Moment about c, 178 
Least squares Monte Carlo method, 103 
curve fitting, 360 mu, , 169 
line Multiplication principle, 19, 22 
and correlation, 389 Mutually exclusive events, 76, 121 
general, 382 Mutually exclusive sets, 414 
through origin, 377 Mutation, 6 
method, 366 
Line fitting =C and (), 35 
general line, 379, 382 =P=, 29 
line through origin, 375, 377 P, 30 
n factoffal, 28 
Marginal distributions, 210 table, 431 
/ATHEMATICAL ASSOCIATION OF Normal approximation to binomial, 
AMEmCA, 416 280 
Mathematical expectation, 169 accuracy in, 283 
of H(X), 176 gaphs and tables, 276-279, 281,282 
of h(�, Y), 212 Normal curve, 231 
MAY, K. 0., 416 areas under, 233, 432 
476 xvEx 
Normal probability distribution, 230, graphs for continuous random 
233, 432 variables, 221 
mcan and variance of, 236 in finite sample spaces, 67, 118 
Normal random variable, standard, interpretations of, 2 
230 measure of chance, 5S 
othm, 237 objective, 2 
Null hypothesis, 304 of an event, 60, 118 
accepting or lejecting, 305 personalistic, 3 
posterior, 144, 146 
Odds, 61 prior, 143, 146 
One-sided tests, 304 that I -- Pl =< 0.15, n = 10, 
289 
Operating characteristic, 301, 302, 303 
Problem of points, 173 (8) 
ORE, 0YSTEIN, 3, 256 
Product rule, used to assign 
probabilities, 137 
Partition of sample space, 77 
PASCAL, BLAISE, 3 Quality control, 301 
Pascal's rule, 38 Qucueing theory, 4 
Pascal's triangle, 39 
Pearson-Lee data on heights, 394 Radiation and genetics, 6 
Permutation(s), 21 RAND CORPORATION, THE, 15, 430 
and combinations, 19 Random digits, 14, 105, 430 
formulas for, 29 frequencies of, 15 
of n things, all together, 29 tables, 105, 430 
of n things, r at a time, 30 construction of, 105 
of things not all different, 43 use of, 106 
of two kinds of things, 44 Random drawings, 100 
Points, problem of, 4 Random numbers, 104 
Population, 2 brief table of, 105 
and sample (summary), 199 table of 2500, 430 
correlation, 340 Random variable(s), 155, 159 
mean, 169 and probability functions, 155 
standard deviation, 1S5 identically distributed, 3-96 
variance, 185 independence of, 427 
Prediction and dependence, 219, 220 Random xxatk, 4, 13, 14 
(5-9) Range, 1S1 
Prime number, 62 (14) Reduced sample space, SS, 134 
Probability, 1 Regression 
and statistics, 1 bivariate model, 393 
axioms in finite Salnpte space, 11S control-knob model, 392 
by area, 223 in first-ace problem, 397 
conditional, 8S models for, 391 
distribution, 160 of heights, 396 
function, 155, 159 of x on y, 395 
for , n = 10, p = 0.2, 2S7 Relative frequency, 3 
of a sum, 31 t Roster inethod for sets, 405 
of txxo random variables, 211 Roulette, 173 (13) 
DZX 477 
Rule method for sets, 405 SMITH, D. E., 256 
Runs, 162 SNELL, J. L., 416 
Spread, 181 
Saint Petersburg paradox, 150 (1-13) Standard deviation, 185 
Sample, 2 (also see variance) 
average, 168 of binomial, 266 
average and variance, 197, 199 of , 290 
correlation coefficient, 342 sample, 197 
standard deviation, 197 Standard error of estimate, 384 
variance and standard deviation, Statistical applications, 285 
197, 199 Statistical inference, 2 
Sample average(s), 168 Statistics, 2 
mean and variance of, 330 applications of, 6, 285 
Sample point, 64 Subscripts, 417 
Sample space, 64, 116 Subset(s), 411 
for two dice, 68 number of, 413 
reduced, 88, 134 Successes, number of, 251 
xith many elements, 93 Sum(s) 
Sample surveys, 6 of integers, 424 
Sampling of random variables, 314 
from finite population, variance of of squares, mini,hum, 373 
sum and average, 354 of squares of integers, 425 (13) 
with replacement, 331 Summation, 420 
xithout replacement from finite index, 419 
population, 349 limits, 420 
Sampling fluctuations, 8 omission of, 420 
Sampling problem, 98 of a constant, 423 
Sampling theory, 314 of cx, 423 
Sampling variation, 8 of a sum, 422 
Selective Service numbers, 101 symbol, , 417 
Sequences of A's and B's, 45 Systematic error, 331 
Set(s), 403 
complement, 414 Table of random digits, 105, 430 
disjoint, 414 use of, 106 
element of, 403 Telephone numbers, last digits, 12 
finite or infinite, 403 Testing of binomial hypothesis, 301 
intersection, 74, 413 THOMPSON, G. L., 416 
mutually exclusive, 414 Thumbtack, 113 
operations, 413 Tree graph, 19 
references, 416 Turning points, 163 
union, 73, 414 
ways of specifying, 404 
Set-builder notation, 406 Union of sets, 73, 414 
SETON, E. T., 201 (23) Universal set, 411 
Sigma, , 417; , 185 
Sign test, 303 Variability, 181 
Significance level, 304 empirical study of, 7 
478 INDEX 
Variance, 185, 188, 199 of independent variables, 319, 
of average, from finite population, 326 
354 sample, 197, 199 
of binomial, 327 Venn diagrams, 347 
of deviations from least-squares 
line, 384 WALLIS and ROBERT% 173 (19) 
of identically distributed vanrobles, Weighted sums, 321 
326 WOODWARD, E. J., 416 
of normal, 236 Word-length, 11 
of sum, World Almanac, The, l 11 
flora finite population, 354 World of Mathematics, 58 
of correlated random variables, World Series exercises, 130, 173 (17) 
337, 341, 343 
ABCDE698765 
DS OF COUNTING 
mong k operations, the ith can be done 
be done in nl X n2 X ns X '-- X nk ways. 
erations are mutually exclusive, and the 
d the second in n ways, then one or the 
n be done in m q- n ways. 
at a time: 
ns (order counts) is P -- n!/(n -- r). 
] t-he number o combinations (order does not count) is 
Cr -- ()  n!/[r!(n -- r)!]. 
4. For n objects of h kinds (ni of kind i, nt -- n), taken all together, the 
number of permutations is n!/[nl!n!... n!]. 
SrECXL CSE. If k ----- 2 (r of one kind, n -- r of another), the number 
of permutations is (2). 
5. Pascal's Rule. () q- (_) -- (fi'.'f']), I . r . /'/,. 
B. BINOMIAL FORMULAS 
I. Binomial expansion. (9 q- )" --  ()?9 '-- 
2. Binomial approximation. (1 q- )" -. I q- , [ Inxl  sinai!. 
c. SETS AND PROBABILITIES 
[Notation: S is the universal set; A, B, C, At, Hi, E are subsets;  is the 
empty set; P(A) is the probability of A; P() -- 0.] 
1. Axioms. I. P(A) _ O. II. P(S)= 1. III. If AB-- , 
P(A U B) ----- P(A) q- P(B). 
2. Addition rules. P(A U B) -- P(A) q- P(B) -- P(A  B). 
P(A U B U C) '- P(A) q- P(B) q- P(C) -- P(A  B) -- r(A  C) 
-- P(B , C) q- P(A  B  C). 
3. Complements. If A U  -- S, and A / [  , then 
P()----- 1- P(A). 
4. Partitions. If subsets At form a partition of S, then P(A) -- 1. 
5. Conditional probability. P(AIB ) -- P(A V1B)/P(B), P(B)  O. 
CHAIN RULES. P(A V1 B) ---- P(A)P(B[A) = P(B)P(A[B), 
P(A vIB vI C) ----- P(A)P(BIA)P(C[A VI B), and so on. 
INdEPEnDEnCE. If A and B are independent, P(A V1 B) = P(A)P(B). 
6. 8ayes' ?heorem. If subsets Hi form a partition of S, 
P(HI -- P(Hi n E)/P(E) ---- P(Hi n E)/ZP(H n ) 
-- P(H)P(E[H)/ZP(H)P(E[H). 
7. Odds. If P(A) -- m/n, 'the odds in favor of A are m/(n -- m). If 
the odds in favor of A are a/b, then P(A) -- a/(a -[- b). 
D. POPULATIONS: MEANS, VARIANCES, COVARIANCES, 
AND CORRELATION 
[Notation: X and Y are random variables with values x;, yl; X, X,..., 
are random variables; a and b are constants; P(X ---- xi) "- f(xi); 
E(X) is the expected value of X; a is the variance of 
I. Mean. /x = E(X) = zJ(z);/x+ -- a!.x + b. 
2. Variance. = E(X -- px) 2 = E(X 2) -- p"- Var (X);x+ 
STANDARD DEVIATION. (X = x/Var (X); ,x+ = ialrx. 
3. $andardized random varlable Z. If Z ---- (X -- !.x)/O'x, then/z = 0, 
�z=l. 
4. Cheb�she�'s ?heorem. P([X -- [ > hr) _< 1/h; 
P([X- ,[ _< ) _>  - 1/h . 
$. �oarla.:e. Coy (X, X) --- 
Coy (X, Y) = E[(X -- yx)(Y -- yr)] = E(XI0 -- yxyr = =�r. 
6. Coelafio,. p =Cov (X, Y)/o'xo'r. 
7. Sums. E(X '4- Y) = E(X) + E(Y); E(ZX) -- ZE(Xi); �+r = 
 + , + 2 Cov (X, = = za + z Cov (X, X), i < j. 
Srcr CASES. If all correlations p�i -- p and all variances i = 
then O�x ---- n=[1 + (n -- 1)p]. 
If X and Y are independent and/or uncorrelatcd (p -- 0): 
--- O' X +b � '-- 
,_, =  + , E(Xr) = E(X)E(r) = 
Continued inside ack cover. 
GLOSSARY 
 approximately equals : such that 
>, < greater than, less than Hi hypothesis 
_, _ greater than or equal to, P(Hi) a priori probability of H 
less than or equal to 
P(H[E) a posteriori probability of 
 not equal to H;, given E 
[xl absolute value of x E(X) expected value of X 
:Z sum of  population mean, often 
with subscript 
() binomial coefficient; also 
Cr � sample average 
Capital letter: usually denotes a r population standard de- 
set or random variable viation, often with sub- 
S sample space (or sum of script 
squares), according to 8 sample standard devia- 
context tion, often with subscript 
�i sample point a  population variance 
El set consisting of sample  sample variance 
point e� p population correlation co- 
 complement of the set A efficient 
P(A) probability of A r sample correlation coef- 
ficient 
A U B union of sets A and B 
d, , , 0,/,  least squares values 
A r B intersection of sets A and 
B A(x) area from 0 to x of the 
standard normal distribu- 
[ given tion 
{ } set consisting of N finite population size 
 empty set [] end of proof 
TL V 
Cell entry is the probability p that a standard normal random variable Z 
takes values less than z, where the units digit of z is given in the left-hand 
column, and the tenths digit is given in the top row. 
EXAMPLES:(1) p-- P(Z <--2.7) -0.0035  .,,, 
(2) p = P(Z < 1.4)= 0.9192  
z .0 .1 .2 .3 .4 .5 .6 .7 .8 .9 
--3 .0013 .0010 .0007 .0005 .0003 .0002 .0002 .0001 .0001 .0000+ 
--2 .0228 .0179 .0139 .0107 .0082 .0062 .0047 .0035 .0026 .0019 
--1 .1587 .1357 .1151 .0968 .0808 .0668 .0548 .0446 .0359 .0287 
--0 .5000 .4602 .4207 .3821 .3446 .3085 .2743 .2420 .2119 .1841 
0 .5000 .5398 .5793 .6179 .6554 .6915 .7257 .7580 .7881 .8159 
I .8413 .8643 .8849 .9032 .9192 .9332 .9452 .9554 .9641 .9713 
2 .9772 .9821 .9861 .9893 .9918 .9938 .9953 .9965 .9974 .9981 
3 .9987 .9990 .9993 .9995 .9997 .9998 .9998 .9999 .9999 !.0000-- 
TABLE Vl 
Cell entry is the value of z such that the area to the left of z, under the standard normal curve is p, 
where the tenths digit of p is given in the left-hand column and the hundredths digit is given along the top. 
(2) p = .63, z = -t-0.332 
.00 .01 02 .03 04 05 06 07 08 09 
--o --2.326 --2.054 --1.881 --1.751 --1 645 --1 555 --1.476 --1.405 --1.341 
--1.282 --1.227 --1.175 --1.126 --1.080 --1.036 --0 994 --0.954 --0.915 --0.878 
--0.842 --0.806 --0.772 --0.739 --0.706 --0.674 --0.643 --0.613 --0.583 --0.553 
--0.524 --0.496 --0.468 --0.440 --0.412 --0.385 --0.368 --0.332 --0.305 --0.279 
--0.253 --0.228 --0.202 --0.176 --0.151 --0.126 --0.100 --0 075 --0.050 --0.025 
0.000 0.025 0.050 0.075 O. 100 O. 126 O. 151 O. 176 0.202 0.228 
O. 253 O. 279 O. 305 O. 332 O. 358 O. 385 O. 412 O. 440 O. 468 O. 496 
0.524 0.553 0.583 0....613 0.643 0.674 0.706 0 739 0.772 0.806 
0.842 0.878 0.915 0.954 0.994 1.036 1.080 1.126 1.175 1.227 
1.282 1.341 1.405 1.476 1.555 1.645 1.75I 1.881 2.054 2.326 
E. SAMPLING THEORY 
[Notatzon: Xa, X., ..., X,, are random variables and the mean and 
variance of Xi are/zl and r, respectively.] 
1. Mean and variance of a sum. If X, X, ..., X,, are independent 
and/or uncorrelated, then 
E(ZX) = = 
2. For n identicallit distributed random variables Xi with means 
and variances or/ ---- cr ', if  = llX/n, then E() = p, and cr -- 
(sampling with replacement--independent variables); or 
-- n) 
a-- n(N- 1) 
(sampling without replacement, population size N). 
3. Standard normal curve (random variable X). 
 ,.'__L e-/  P(0 < X < z) 
--3 -2 -1 
See Table II1. 
4. Central Limit Theorem (in large samples drawn with replacement). 
P[(' --/z)/(r/x/) 1> z]  area to right of z for the standard normal 
distribution ---- I -- area to left of z. See Tables V and VI. 
. Binomlal distribution. b(x; n, p) -- ()p(1 -- p)'-, z -- 0, 1,..., n. 
If X is number of successes, ] the proportion of successes (]p = X/n) 
then 
E(X) = np, o' = np(1 -- p); E(]) = p, r = p(1 -- p)/n. 
6. Central Limit Theorem for the binomial distribution. If Z is a standard 
normally distributed random variable, and np is far from 0 and n, then 
P(X > x) -. P(Z > z), where z = (z --  -- np)/ n. 
F. SAMPLES 
[Notation: For grouped data, the value xi occurs nl times, ni -- n; for 
ungrouped data, ni ----- 1, but xi may have the same value for 
different i's, i = 1, 2,..., n.] 
I. Grouped da;a.  -- xini/n, s  = (xi -- )ni/n -- xni/n -- .. 
2. Ungrouped data. , = xi/n, s 
3. Correlation coefficient. r ---- 
G. LEAST SQUARES 
[Notatwn: Formulas are for ungrouped data.] 
1. Fundamental lemma. If y -- Ax  q- Bx q- C, A > O, then the min- 
imizing x = --B/(2A) and the minimum y = C -- B/(4A). 
2. Least squares line through origin (y on x): 
3. General least squares line (yonx): [ y-- ---- h(x--), ] where 
 ---- (Zxiyi -- ny)/Z(xi -- )' ---- r%/sx, and minimum average sum 
of squares is s = /n -- s(1 -- r ') -- s -- ". 
Srgcrt. cts. If �  ]7 -- 0 and s -- sy, then y ---- rx is the general 
least squares line (y on x). 
