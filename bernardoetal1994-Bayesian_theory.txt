Bayesian 
THEORY 
Jose M. Bernardo 
Professor of Statistics 
Universidad de Valencia, Spain 
Adrian E M. Smith 
Professor of Statistics 
Imperial College of Science, Technology and Medicine 
London, UK 
JOHN WILEY & SONS 
Chichester ' New York • Brisbane * Toronto * Singapore 
vii 
Preface 
This is the first volume of a related series of three—Bayesian Theory, Bayesian 
Computation, and Bayesian Methods— which set out to give an up-to-date overview 
of our version of the why? how? and what? of Bayesian Statistics. The second 
volume will be written by Alan E. Gelfand and Adrian F. M. Smith; the third volume 
will be written by the current authors. 
The original motivation for this enterprise stemmed from the impact and  
influence of de Finetti's two-volume Theory of Probability, which one of us helped 
translate into English from the Italian in the early 1970's. This was widely  
acknowledged as the definitive exposition of the operationalist, subjectivist approach 
to uncertainty, and provided further impetus at that time to a growth in activity and 
interest in Bayesian ideas. 
From a philosophical, foundational perspective, the de Finetti volumes provide 
—in the words of the author's dedication to his friend Segre— 
... a necessary document for clarifying one point of view in its entirety. 
From a statistical, methodological perspective, however, the de Finetti  
volumes end abruptly, with just the barest introduction to the mechanics of Bayesian 
inference. 
A decade or so ago, we decided to try to write a series of books which would 
take up the story where de Finetti left off, with the grandiose objective of "clarifying 
in its entirety" the world of Bayesian statistical theory and practice. 
viii 
Preface 
It is now clear that this was a hopeless undertaking. Over the past decade, the 
world of Bayesian Statistics has been changing shape and growing in size rapidly 
and unpredictably—most notably in relation to developments in computational 
methods and the subsequent opening up of new application horizons. We are 
greatly relieved that we were too incompetent to finish our books a few years ago! 
And, of course, these changes and developments continue. There is no static 
world of Bayesian Statistics to describe in a once-and-for-all way. Moreover, we 
are dealing with a field of activity where, even among those whose intellectual 
perspectives fall within the broad paradigm, there are considerable differences of 
view at the level of detail and nuance of interpretation. 
This volume on Bayesian Theory attempts to provide a fairly complete and 
up-to-date overview of what we regard as the key concepts, results and issues. 
However, it necessarily reflects the prejudices and interests of its authors—as well 
as the temporal constraints imposed by a publisher whose patience has been sorely 
tested for far too long. We can but hope that our sins of commission and omission 
are not too grievous. 
Too many colleagues have taught us too many things for it to be practical to 
list everyone to whom we are beholden. However, Dennis Lindley has played a 
special role, not least in supervising us as Ph.D. students, and we should like to 
record our deep gratitude to him. We also shared many enterprises with Morrie 
DeGroot and continue to miss his warmth and intellectual stimulation. For detailed 
comments on earlier versions of material in this volume, we are indebted to our 
colleagues M. J. Bayarri, J. O. Berger, J. de la Horra, P. Diaconis, F. J. Giron, M. A. 
Gomez-Villegas, D. V. Lindley, M. Mendoza, J. Muiioz, E. Moreno, L. R. Pericchi, 
A. van der Linde, C. Villegas and M. West. 
We are also grateful, in more ways than one, to the State of Valencia. It has 
provided a beautiful and congenial setting for much of the writing of this book. 
And, in the person of the Governor, Joan Lerma, it has been wonderfully supportive 
of the celebrated series of Valencia International Meetings on Bayesian Statistics. 
During the secondment of one of us as scientific advisor to the Governor, it also 
provided resources to enable the writing of this book to continue. 
This volume has been produced directly in TgX and we are grateful to Maria 
Dolores Tortajada for all her efforts. 
Finally, we thank past and present editors at John Wiley & Sons for their 
support of this project: Jamie Cameron for saying "Go!" and Helen Ramsey for 
saying "Stop!" 
Valencia, Spain 
January 17, 1994 
J. M. Bernardo 
A. F. M. Smith 
ix 
Contents 
1. INTRODUCTION 1 
1.1. Thomas Bayes 1 
1.2. The subjectivist view of probability 2 
1.3. Bayesian Statistics in perspective 3 
1.4. An overview of Bayesian Theory 5 
1.4.1. Scope 5 
1.4.2. Foundations 5 
1.4.3. Generalisations 6 
1.4.4. Modelling 7 
1.4.5. Inference 7 
1.4.6. Remodelling 8 
1.4.7. Basic formulae 8 
1.4.8. Non-Bayesian theories 9 
1.5. A Bayesian reading list 9 
FOUNDATIONS 13 
2.1. Beliefs and actions 13 
2.2. Decision problems 16 
2.2.1. Basic elements 16 
2.2.2. Formal representation 18 
2.3. Coherence and quantification 23 
2.3.1. Events, options and preferences 23 
2.3.2. Coherent preferences 23 
2.3.3. Quantification 28 
2.4. Beliefs and probabilities 33 
2.4.1. Representation of beliefs 33 
2.4.2. Revision of beliefs and Bayes' theorem 38 
2.4.3. Conditional independence 45 
2.4.4. Sequential revision of beliefs 47 
2.5. Actions and utilities 49 
2.5.1. Bounded sets of consequences 49 
2.5.2. Bounded decision problems 50 
2.5.3. General decision problems 54 
2.6. Sequential decision problems 56 
2.6.1. Complex decision problems 56 
2.6.2. Backward induction 59 
2.6.3. Design of experiments 63 
2.7. Inference and information 67 
2.7.1. Reporting beliefs as a decision problem 67 
2.7.2. The utility of a probability distribution 69 
2.7.3. Approximation and discrepancy 75 
2.7.4. Information 77 
2.8. Discussion and further references 81 
2.8.1. Operational definitions 81 
2.8.2. Quantitative coherence theories 83 
2.8.3. Related theories 85 
2.8.4. Critical issues 92 
GENERALISATIONS 105 
3.1. Generalised representation of beliefs 105 
3.1.1. Motivation 105 
3.1.2. Countable additivity 106 
3.2. Review of probability theory 109 
3.2.1. Random quantities and distributions 109 
3.2.2. Some particular univariate distributions 114 
3.2.3. Convergence and limit theorems 125 
3.2.4. Random vectors, Bayes' theorem 127 
3.2.5. Some particular multivariate distributions 133 
3.3. Generalised options and utilities 141 
3.3.1. Motivation and preliminaries 141 
3.3.2. Generalised preferences 145 
3.3.3. The value of information 147 
3.4. Generalised information measures 150 
3.4.1. The general problem of reporting beliefs 150 
3.4.2. The utility of a general probability distribution 151 
3.4.3. Generalised approximation and discrepancy 154 
3.4.4. Generalised information 157 
3.5. Discussion and further references 160 
3.5.1. The role of mathematics 160 
3.5.2. Critical issues 161 
MODELLING 165 
4.1 Statistical models 165 
4.1.1. Beliefs and models 165 
4.2. Exchangeability and related concepts 167 
4.2.1. Dependence and independence 167 
4.2.2. Exchangeability and partial exchangeability 168 
4.3. Models via exchangeability 172 
4.3.1. The Bernoulli and binomial models 172 
4.3.2. The multinomial model 176 
4.3.3. The general model 177 
Contents 
4.4. Models via invariance 181 
4.4.1. The normal model 181 
4.4.2. The multivariate normal model 185 
4.4.3. The exponential model 187 
4.4.4. The geometric model 189 
4.5. Models via sufficient statistics 190 
4.5.1. Summary statistics 190 
4.5.2. Predictive sufficiency and parametric sufficiency 191 
4.5.3. Sufficiency and the exponential family 197 
4.5.4. Information measures and the exponential family 207 
4.6. Models via partial exchangeability 209 
4.6.1. Models for extended data structures 209 
4.6.2. Several samples 211 
4.6.3. Structured layouts 217 
4.6.4. Covariates 219 
4.6.5. Hierarchical models 222 
4.7. Pragmatic aspects 226 
4.7.1. Finite and infinite exchangeability 226 
4.7.2. Parametric and nonparametric models 228 
4.7.3. Model elaboration 229 
4.7.4. Model simplification 233 
4.7.5. Prior distributions 234 
4.8. Discussion and further references 235 
4.8.1. Representation theorems 235 
4.8.2. Subjectivity and objectivity 236 
4.8.3. Critical issues 237 
INFERENCE 241 
5.1. The Bayesian paradigm 241 
5.1.1. Observables, beliefs and models 241 
5.1.2. The role of Bayes' theorem 242 
5.1.3. Predictive and parametric inference 243 
5.1.4. Sufficiency, ancillarity and stopping rules 247 
5.1.5. Decisions and inference summaries 255 
5.1.6. Implementation issues 263 
5.2. Conjugate analysis 265 
5.2.1. Conjugate families 265 
5.2.2. Canonical conjugate analysis 269 
5.2.3. Approximations with conjugate families 279 
5.3. Asymptotic analysis 285 
5.3.1. Discrete asymptotics 286 
5.3.2. Continuous asymptotics 287 
5.3.3. Asymptotics under transformations 295 
5.4. Reference analysis 298 
5.4.1. Reference decisions 299 
5.4.2. One-dimensional reference distributions 302 
5.4.3. Restricted reference distributions 316 
5.4.4. Nuisance parameters 320 
5.4.5. Multiparameter problems 333 
5.5. Numerical approximations 339 
5.5.1. Laplace approximation 340 
5.5.2. Iterative quadrature 346 
5.5.3. Importance sampling 348 
5.5.4. Sampling-importance-resampling 350 
5.5.5. Markov chain Monte Carlo 353 
5.6. Discussion and further references 356 
5.6.1. An historical footnote 356 
5.6.2. Prior ignorance 357 
5.6.3. Robustness 367 
5.6.4. Hierarchical and empirical Bayes 371 
5.6.5. Further methodological developments 373 
5.6.6. Critical issues 374 
REMODELLING 377 
6.1. Model comparison 377 
6.1.1. Ranges of models 377 
6.1 
6.1 
6.1 
6.1 
6.1 
6.1 
2. Perspectives on model comparison 383 
3. Model comparison as a decision problem 386 
4. Zero-one utilities and Bayes factors 389 
5. General utilities 395 
6. Approximation by cross-validation 403 
7. Covariate selection 407 
xiv 
Contents 
6.2. Model rejection 409 
6.2.1. Model rejection through model comparison 409 
6.2.2. Discrepancy measures for model rejection 412 
6.2.3. Zero-one discrepancies 413 
6.2.4. General discrepancies 415 
6.3. Discussion and further references 417 
6.3.1. Overview 417 
6.3.2. Modelling and remodelling 418 
6.3.3. Critical issues 418 
A. SUMMARY OF BASIC FORMULAE 427 
A.l. Probability distributions 427 
A.2. Inferential processes 436 
B. NON-BAYESIAN THEORIES 443 
B.l. Overview 443 
B.2. Alternative approaches 445 
B.2.1. Classical decision theory 445 
B.2.2. Frequentist procedures 449 
B.2.3. Likelihood inference 454 
B.2.4. Fiducial and related theories 456 
B.3. Stylised inference problems 460 
B.3.1. Point estimation 460 
B.3.2. Interval estimation 465 
B.3.3. Hypothesis testing 469 
B.3.4. Significance testing 475 
B.4. Comparative issues 478 
B.4.1. Conditional and unconditional inference 478 
B.4.2. Nuisance parameters and marginalisation 479 
B.4.3. Approaches to prediction 482 
B.4.4. Aspects of asymptotics 485 
B.4.5. Model choice criteria 486 
REFERENCES 489 
SUBJECT INDEX 555 
AUTHOR INDEX 573 
1 
Chapter 1 
Introduction 
Summary 
A brief historical introduction to Bayes' theorem and its author is given, as 
a prelude to a statement of the perspective adopted in this volume regarding 
Bayesian Statistics. An overview is provided of the material to be covered in 
successive chapters and appendices, and a Bayesian reading list is provided. 
1.1 THOMAS BAYES 
According to contemporary journal death notices and the inscription on his tomb 
in Bunhill Fields cemetery in London, Thomas Bayes died on 7th April, 1761, at 
the age of 59. The inscription on top of the tomb reads: 
Rev. Thomas Bayes. Son of the said Joshua and Ann Bayes (59). 7 April 1761. 
In recognition of Thomas Bayes's important work in probability. The vault was 
restored in 1969 with contributions received from statisticians throughout the 
world. 
Definitive records of Bayes' birth do not seem to exist, but, allowing for the 
calendar reform of 1752 and accepting that he died at the age of 59, it seems likely 
that he was born in 1701 (an argument attributed to Bellhouse in the Inst. Math. 
2 
1 Introduction 
Statist. Bull. 26, 1992). Some background on the life and the work of Bayes may 
be found in Barnard (1958), Holland (1962), Pearson (1978), Gillies (1987), Dale 
(1990, 1991) and Earman (1990). See, also, Stigler (1986a). 
That his name lives on in the characterisation of a modern statistical  
methodology is a consequence of the publication of An essay towards solving a problem in 
the doctrine of chances, attributed to Bayes and communicated to the Royal Society 
after Bayes' death by Richard Price in 1763 (Phil. Trans. Roy. Soc. 53, 370-418). 
The technical result at the heart of the essay is what we now know as Bayes' 
theorem. However, from a purely formal perspective there is no obvious reason 
why this essentially trivial probability result should continue to excite interest. 
In its simplest form, if H denotes an hypothesis and D denotes data, the 
theorem states that 
P(H | D) = P(D | H) x P{H)/P(D). 
With P(H) regarded as a probabilistic statement of belief about H before obtaining 
data D, the left-hand side P(H | D) becomes a probabilistic statement of belief 
about H after obtaining D. Having specified P(D | H) and P{D), the mechanism 
of the theorem provides a solution to the problem of how to learn from data. 
Actually, Bayes only stated his result for a uniform prior. According to Stigler 
(1986b), it was Laplace (1774/1986)—apparently unaware of Bayes' work—who 
stated the theorem in its general (discrete) form. 
Like any theorem in probability, at the technical level Bayes' theorem merely 
provides a form of "uncertainty accounting", which asserts that the left-hand side of 
the equation must equal the right-hand side. The interest and controversy, of course, 
lie in the interpretation and assumed scope of the formal inputs to the two sides of 
the equation—and it is here that past and present commentators part company in 
their responses to the idea that Bayes' theorem can or should be regarded as a central 
feature of the statistical learning process. At the heart of the controversy is the issue 
of the philosophical interpretation of probability—objective or subjective?—and 
the appropriateness and legitimacy of basing a scientific theory on the latter. 
What Thomas Bayes—from the tranquil surroundings of Bunhill Fields, where 
he lies in peace with Richard Price for company—has made of all the fuss over the 
last 233 years we shall never know. We would like to think that he is a subjectivist 
fellow-traveller but, in any case, he is in no position to complain at the liberties we 
are about to take in his name. 
1.2 THE SUBJECTIVIST VIEW OF PROBABILITY 
Throughout this work, we shall adopt a wholehearted subjectivist position regarding 
the interpretation of probability. The definitive account and defence of this position 
are given in de Finetti's two-volume Theory of Probability (1970/1974,1970/1975) 
1.3 Bayesian Statistics in Perspective 
3 
and the following brief extract from the Preface to that work perfectly encapsulates 
the essence of the case. 
The only relevant thing is uncertainty—the extent of our own knowledge and 
ignorance. The actual fact of whether or not the events considered are in some 
sense determined, or known by other people, and so on, is of no consequence. 
The numerous, different, opposed attempts to put forward particular points 
of view which, in the opinion of their supporters, would endow Probability Theory 
with a 'nobler* status, or a 'more scientific' character, or 'firmer' philosophical 
or logical foundations, have only served to generate confusion and obscurity, and 
to provoke well-known polemics and disagreements—even between supporters 
of essentially the same framework. 
The main points of view that have been put forward are as follows. 
The classical view, based on physical considerations of symmetry, in which 
one should be obliged to give the same probability to such 'symmetric' cases. But 
which symmetry? And, in any case, why? The original sentence becomes  
meaningful if reversed: the symmetry is probabilistically significant, in someone's 
opinion, if it leads him to assign the same probabilities to such events. 
The logical view is similar, but much more superficial and irresponsible 
inasmuch as it is based on similarities or symmetries which no longer derive 
from the facts and their actual properties, but merely from the sentences which 
describe them, and from their formal structure or language. 
The frequentist (or statistical) view presupposes that one accepts the  
classical view, in that it considers an event as a class of individual events, the latter 
being 'trials' of the former. The individual events not only have to be 'equally 
probable', but also 'stochastically independent" ... (these notions when applied 
to individual events are virtually impossible to define or explain in terms of the 
frequentist interpretation). In this case, also, it is straightforward, by means of 
the subjective approach, to obtain, under the appropriate conditions, in a perfectly 
valid manner, the result aimed at (but unattainable) in the statistical formulation. 
It suffices to make use of the notion of exchangeability. The result, which acts as 
a bridge connecting the new approach with the old, has often been referred to by 
the objectivists as "de Finetti's representation theorem". 
It follows that all the three proposed definitions of 'objective' probability, 
although useless per se, turn out to be useful and good as valid auxiliary devices 
when included as such in the subjectivist theory. 
(de Finetti, 1970/1974, Preface, xi-xii) 
1.3 BAYESIAN STATISTICS IN PERSPECTIVE 
The theory and practice of Statistics span a range of diverse activities, which are 
motivated and characterised by varying degrees of formal intent. Activity in the 
4 
1 Introduction 
context of initial data exploration is typically rather informal; activity relating to 
concepts and theories of evidence and uncertainty is somewhat more formally  
structured; and activity directed at the mathematical abstraction and rigorous analysis 
of these structures is intentionally highly formal. 
What is the nature and scope of Bayesian Statistics within this spectrum of 
activity? 
Bayesian Statistics offers arationalist theory of personalistic beliefs in contexts 
of uncertainty, with the central aim of characterising how an individual should act 
in order to avoid certain kinds of undesirable behavioural inconsistencies. The 
theory establishes that expected utility maximisation provides the basis for rational 
decision making and that Bayes' theorem provides the key to the ways in which 
beliefs should fit together in the light of changing evidence. The goal, in effect, 
is to establish rules and procedures for individuals concerned with disciplined 
uncertainty accounting. The theory is not descriptive, in the sense of claiming to 
model actual behaviour. Rather, it is prescriptive, in the sense of saying "if you 
wish to avoid the possibility of these undesirable consequences you must act in the 
following way". 
From the very beginning, the development of the theory necessarily presumes 
a rather formal frame of discourse, within which uncertain events and available 
actions can be described and axioms of rational behaviour can be stated. But this 
formalism is preceded and succeeded in the scientific learning cycle by activities 
which, in our view, cannot readily be seen as part of the formalism. 
In any field of application, a prerequisite for arriving at a structured frame of 
discourse will typically be an informal phase of exploratory data analysis. Also, it 
can happen that evidence arises which discredits a previously assumed and accepted 
formal framework and necessitates a rethink. Part of the process of realising that 
a change is needed can take place within the currently accepted framework using 
Bayesian ideas, but the process of rethinking is again outside the formalism. Both 
these phases of initial structuring and subsequent restructuring might well be guided 
by "Bayesian thinking"—by which we mean keeping in mind the objective of 
creating or re-creating a formal framework for uncertainty analysis and decision 
making—but are not themselves part of the Bayesian formalism. That said, there 
is, of course, often a pragmatic ambiguity about the boundaries of the formal and 
the informal. 
The emphasis in this book is on ideas and we have sought throughout to keep 
the level of the mathematical treatment as simple as is compatible with giving what 
we regard as an honest account. However, there are sections where the full story 
would require a greater level of abstraction than we have adopted, and we have 
drawn attention to this whenever appropriate. 
1.4 An Overview of Bayesian Theory 
5 
1.4 AN OVERVIEW OF BAYESIAN THEORY 
1.4.1 Scope 
This volume on Bayesian Theory focuses on the basic concepts and theory of 
Bayesian Statistics, with chapters covering elementary Foundations, mathematical 
Generalisations of the Foundations, Modelling, Inference and Remodelling. In 
addition, there are two appendices providing a Summary of Basic Formulae and a 
review of Non-Bayesian Theories. The emphasis throughout is on general ideas— 
the Why?—of Bayesian Statistics. A detailed treatment of analytical and numerical 
techniques for implementing Bayesian procedures—the How?—will be provided in 
the volume Bayesian Computation. A systematic study of the methods of analysis 
for a wide range of commonly encountered model and problem types—the What?— 
will be provided in the volume Bayesian Methods. 
The selection of topics and the details of approach adopted in this volume 
necessarily reflect our own preferences and prejudices. Where we hold strong 
views, these are, for the most part, rather clearly and forcefully stated, while, 
hopefully, avoiding too dogmatic a tone. We acknowledge, however, that even 
colleagues who are committed to the Bayesian paradigm will disagree with at least 
some points of detail and emphasis in our account. For this reason, and to avoid 
complicating the main text with too many digressionary asides and references, each 
of Chapters 2 to 6 concludes with a Discussion and Further References section, in 
which some of the key issues in the chapter are critically re-examined. 
In most cases, the omission of a topic, or its abbreviated treatment in this 
volume, reflects the fact that a detailed treatment will be given in one or other of 
the volumes Bayesian Computation and Bayesian Methods. Topics falling into this 
category include Design of Experiments, Image Analysis, Linear Models,  
Multivariate Analysis, Nonparametric Inference, Prior Elicitation, Robustness, Sequential 
Analysis, Survival Analysis and Time Series. However, there are important topics, 
such as Game Theory and Group Decision Making, which are omitted simply  
because a proper treatment seemed to us to involve too much of a digression from our 
central theme. For a convenient source of discussion and references at the interface 
of Decision Theory and Game Theory, see French (1986). 
1.4.2 Foundations 
In Chapter 2, the concept of rationality is explored in the context of representing 
beliefs or choosing actions in situations of uncertainty. We introduce a formal 
framework for decision problems and an axiom system for the foundations of 
decision theory, which we believe to have considerable intuitive appeal and to be 
an improvement on the many such systems that have been previously proposed. 
Here, and throughout this volume, we stress the importance of a decision-oriented 
6 
1 Introduction 
framework in providing a disciplined setting for the discussion of issues relating to 
uncertainty and rationality. 
The dual concepts of probability and utility are formally defined and analysed 
within this decision making context and the criterion of maximising expected utility 
is shown to be the only decision criterion which is compatible with the axiom  
system. The analysis of sequential decision problems is shown to reduce to successive 
applications of the methodology introduced. 
A key feature of our approach is that statistical inference is viewed simply as 
a particular form of decision problem; specifically, a decision problem where an 
action corresponds to reporting a probability belief distribution for some unknown 
quantity of interest. Thus defined, the inference problem can be analysed within 
the general decision theory framework, rather than requiring a separate "theory of 
inference". 
An important special feature of what we shall call a pure inference problem is 
the form of utility function to be adopted. We establish that the logarithmic utility 
function—more often referred to as a score function in this context—plays a special 
role as the natural utility function for describing the preferences of an individual 
faced with a pure inference problem. 
Within this framework, measures of the discrepancy between probability  
distributions and the amount of information contained in a distribution are naturally 
defined in terms of expected loss and expected increase, respectively, in  
logarithmic utility. These measures are mathematically closely related to well-known 
information-theoretic measures pioneered by Shannon (1948) and employed in 
statistical contexts by Kullback (1959/1968). A resulting characteristic feature of 
our approach is therefore the systematic appearance of these information-theoretic 
quantities as key elements in the Bayesian analysis of inference and general decision 
problems. 
1.4.3 Generalisations 
In Chapter 3, the ideas and results of Chapter 2 are extended to a much more 
general mathematical setting. An additional postulate concerning the comparison 
of a countable collection of events is appended to the axiom system of Chapter 2, 
and is shown to provide a justification for restricting attention to countably additive 
probability as the basis for representing beliefs. The elements of mathematical 
probability theory required in our subsequent development are then reviewed. 
The notions of actions and utilities, introduced in a simple discrete setting in 
Chapter 2, are extended in a natural way to provide a very general mathematical 
framework for our development of decision theory. A further additional  
mathematical postulate regarding preferences is introduced and, within this more general 
framework, the criterion of maximising expected utility is shown to be the only 
decision making criterion compatible with the extended axiom system. 
1.4 An Overview of Bayesian Theory 
7 
In this generalised setting, inference problems are again considered simply as 
special cases of decision problems and generalised definitions of score functions 
and measures of information and discrepancy are given. 
1.4.4 Modelling 
In Chapter 4, we examine in detail the role of familiar mathematical forms of  
statistical models and the possible justifications—from a subjectivist perspective— 
for their use as representations of actual beliefs about observable random  
quantities. A feature of our approach is an emphasis on the primacy of observables and 
the notion of a model as a (probabilistic) prediction device for such observables. 
From this perspective, the role of conventional parametric statistical modelling is 
problematic, and requires fundamental re-examination. 
The problem is approached by considering simple structural characteristics 
—such as symmetry with respect to the labelling of individual counts or  
measurements, afeature common to many individual beliefs about sequences of observables. 
The key concept here is that of exchangeability, which we motivate, formalise and 
then use to establish a version of de Finetti's celebrated representation theorem. 
This demonstrates that judgements of exchangeability lead to general mathematical 
representations of beliefs that justify and clarify the use and interpretations of such 
familiar statistical concepts as parameters, random samples, likelihoods and prior 
distributions. 
Going beyond simple exchangeability, we show that beliefs which have  
certain additional invariance properties—for example, to rotation of the axes of  
measurements, or translation of the origin—can lead to mathematical representations 
involving other familiar specific forms of parametric distributions, such as normals 
and exponentials. 
A further approach to characterising belief distributions is considered, based 
on data reduction. The concept of a sufficient statistic is introduced and related to 
representations involving the exponential family of distributions. 
Various forms of partial exchangeability judgements about data structures are 
then discussed in a number of familiar contexts and links are established with a 
number of other commonly used statistical models. Structures considered include 
those of several samples, multiway layouts, problems involving covariates, and 
hierarchies. 
1.4.5 Inference 
In Chapter 5, the key role of Bayes' theorem in the updating of beliefs about  
observables in the light of new information is identified and related to conventional 
mechanisms of predictive and parametric inference. The roles of sufficiency, an- 
cillarity and stopping rules in such inference processes are also examined. 
8 
1 Introduction 
Various standard forms of statistical problems, such as point and interval 
estimation and hypothesis testing, are re-examined within the general Bayesian 
decision framework and related to formal and informal inference summaries. 
The problems of implementing Bayesian procedures are discussed at length. 
The mathematical convenience and elegance of conjugate analysis are illustrated in 
detail, as are the mathematical approximations available under the assumption of 
the validity of large-sample asymptotic analysis. A particular feature of this volume 
is the extended account of so-called reference analysis, which can be viewed as a 
Bayesian formalisation of the idea of "letting the data speak for themselves". An 
alternative, closely related idea is that of how to represent "vague beliefs" or  
"ignorance". We provide a detailed historical review of attempts that have been made to 
solve this problem and compare and contrast some of these with the reference  
analysis approach. A brief account is given of recent analytic approximation strategies 
derived from Laplace-type methods, together with outline accounts of numerical 
quadrature, importance sampling, sampling-importance-resampling, and Markov 
chain Monte Carlo methods. 
1.4.6 Remodelling 
In Chapter 6, it is argued that, whether viewed from the perspective of a sensitive 
individual modeller or from that of a group of modellers, there are good reasons for 
systematically entertaining a range of possible belief models, rather than predicating 
all analysis on a single assumed model. 
A variety of decision problems are examined within this framework, some 
involving model choice only, some involving model choice followed by a terminal 
action, such as prediction, others involving only a terminal action. 
A feature of our treatment of this topic is that, throughout, a clear  
distinction is drawn among three rather different perspectives on the comparison of and 
choice from among a range of competing models. The first perspective arises when 
the range of models under consideration is assumed to include the "true" model. 
The second perspective arises when the range of models is assumed to be under 
consideration in order to provide a more conveniently implemented proxy for an 
actual, but intractable, belief model. The third perspective arises when the range 
of models is under consideration because the models are "all there is available", in 
the absence of any specification of an actual belief model. Our discussion relates 
and links these ideas with aspects of hypothesis testing, significance testing and 
cross-validation. 
1.4.7 Basic Formulae 
In Appendix A, we collect together for convenience, in tabular format, summaries 
of the main univariate and multivariate probability distributions that appear in the 
7.5 A Bayesian Reading List 
9 
text, together with summaries of the prior/posterior/predictive forms corresponding 
to these distributions in the context of conjugate and reference analyses. 
1.4.8 Non-Bayesian Theories 
In Appendix B, we review what we perceive to be the main alternatives to the  
Bayesian approach; namely, classical decision theory, frequentist procedures, likelihood 
theory, and fiducial and related theories. 
We compare and contrast these alternatives in the context of "stylised"  
inference problems such as point and interval estimation, hypothesis and significance 
testing. Through counter-examples and general discussion, we indicate why we 
find all these alternatives seriously deficient as formal inference theories. 
1.5 A BAYESIAN READING LIST 
As we have already remarked, this work is—necessarily—a selective account of 
Bayesian theory, reflecting our own interests and perspectives. The following is 
a list of other Bayesian books—by no means exhaustive—whose contents would 
provide a significant complement to the material in this volume. 
In those cases where there are several editions, or when the original is not in 
English, we quote both the original date and the date of the most recent English 
edition. Thus, Jeffreys (1939/1961) refers to Jeffreys' Theory of Probability, first 
published in 1939, and to its most recent (3rd) edition, published in 1961; similarly, 
de Finetti (1970/1974) refers to the original (1970) Italian version of de Finetti's 
Teoria delle Probabilita vol. 1 and to its English translation (published in 1974). 
Pioneering Bayesian books include Laplace (1812), Keynes (1921/1929),  
Jeffreys (1939/1961), Good (1950,1965), Savage (1954/1972,1962), Schlaifer (1959, 
1961), RaiffaandSchlaifer (1961), Mostellerand Wallace (1964/1984), Dubins and 
Savage (1965/1976), Lindley (1965, 1972), Pratt et al. (1965), Tribus (1969), De- 
Groot (1970), de Finetti (1970/1974, 1970/1975, 1972) and Box and Tiao (1973). 
Elementary and intermediate Bayesian textbooks include those of Savage 
(1968), Schmitt (1969), Lavalle (1970), Lindley (1971/1985), Winkler (1972), 
Kleiter (1980), Bernardo (1981b), Daboni and Wedlin (1982), Iversen (1984), 
O'Hagan (1988a), Cifarelli and Muliere (1989), Lee (1989), Press (1989), Scoz- 
zafava (1989), Wichmann (1990), Borovcnik (1992), and Berry (1994). 
More advanced Bayesian monographs include Hartigan (1983), Regazzini 
(1983), Berger (1985a), Savchuk (1989), Florens et al. (1990), Robert (1992) and 
O'Hagan (1994a). Poison and Tiao (1994) is a collection of classic papers in 
Bayesian inference. 
Special topics have also been examined from a Bayesian point of view; these 
include Actuarial Science (Klugman, 1992), Biostatistics (Girelli-Bruni, 1981; 
10 
/ Introduction 
Lecoutre, 1984; Barrai et al., 1992; Berry and Stangl, 1994), Control Theory 
(Aoki, 1967; Sawagari et ai, 1967), Decision Analysis (Duncan and Raiffa, 1957; 
Chernoff and Moses, 1959; Grayson, 1960; Fellner, 1965; Roberts, 1966;  
Edwards andTversky, 1967; Hadley, 1967; Martin, 1967; Morris, 1968; Raiffa, 1968; 
Lusted, 1968; Schlaifer, 1969; Aitchison, 1970; Fishburn, 1970, 1982; Halter 
and Dean, 1971; Lindgren, 1971; Keeney and Raiffa, 1976; Rfos, 1977; Lavalle, 
1978; Roberts, 1979; French etal. 1983; French, 1986, 1989; Marinell and Seeber, 
1988; Smith, 1988a), Dynamic Forecasting (Spall, 1988; West and Harrison, 1989; 
Pole et ai, 1994), Economics and Econometrics (Morales, 1971; Zellner, 1971; 
Richard, 1973; Bauwens, 1984; Boyer and Kihlstrom, 1984; Cyert and DeGroot, 
1987), Educational and Psychological Research (Novick and Jackson, 1974;  
Pollard, 1986), Foundations (Fishburn, 1964, 1970, 1982, 1987, 1988a; Berger and 
Wolpert, 1984/1988; Brown, 1985), History (Dale, 1991), Information Theory (Ya- 
glom and Yaglom, 1960/1983; Osteyee and Good, 1974), Law and Forensic Science 
(DeGroot et ai, 1986; Aitken and Stoney, 1991), Linear Models (Lempers, 1971; 
Learner, 1978; Pilz, 1983/1991; Broemeling, 1985), Logic and Philosophy of  
Science (Jeffrey, 1965/1983, 1981; Rosenkranz, 1977; Seidenfeld, 1979; Howson and 
Urbach 1989; Verbraak, 1990; Rivadulla, 1991), Maximum Entropy (Levine and 
Tribus, 1978; Smith and Grandy, 1985; Justice, 1987; Smith and Erickson, 1987; Er- 
ickson and Smith, 1988; Skilling, 1989; Fougere, 1990; Grandy and Schick, 1991; 
Kapur and Kesavan, 1992; Mohammad-Djafari and Demoment, 1993), Multivariate 
Analysis (Press, 1972/1982; Berger and DasGupta, 1991), Optimisation (Mockus, 
1989), Pattern Recognition (Simon, 1984), Prediction (Aitchison and Dunsmore, 
1975; Geisser, 1993), Probability Assessment (Stael von Holstein, 1970; Stael von 
Holstein and Matheson, 1979; Cooke, 1991), Reliability (Martz and Waller, 1982; 
Claroti and Lindley, 1988), Sample Surveys (Rubin, 1987), Social Science (Phillips, 
1973) and Spectral Analysis (Bretthorst, 1988). 
A number of collected works also include a wealth of Bayesian material. 
Among these, we note particularly; Kyburg and Smokier (1964/1980), Meyer and 
Collier (1970), Godambe and Sprott (1971), Fienberg and Zellner (1974), White and 
Bowen (1975), Aykac and Brumat (1977), Parenti (1978), Zellner (1980), Savage 
(1981), Box et al. (1983), Dawid and Smith (1983), Florens et al. (1983, 1985), 
Good (1983), Jaynes (1983), Kadane (1984), Box (1985), Goel and Zellner (1986), 
Smith and Dawid (1987), Viertl (1987), Gardenfors and Sahlin (1988), Gupta and 
Berger (1988, 1994), Geisser et al. (1990), Hinkelmann (1990), Oliver and Smith 
(1990), Ghosh and Pathak (1992), Goel and Iyengar (1992), de Finetti (1993), Fearn 
and O'Hagan (1993), Gatsonis et al. (1993), Freeman and Smith (1994), and last, 
but not least, the Proceedings of the Valencia International Meetings on Bayesian 
Statistics (Bernardo etal. 1980, 1985, 1988 and 1992). 
General discussions of Bayesian Statistics may be found in review papers and 
encyclopedia articles. Among these, we note de Finetti (1951), Lindley (1953, 
1976, 1978, 1982b, 1982c, 1984, 1990, 1992), Anscombe (1961), Savage (1961, 
1.5 A Bayesian Reading List 
11 
1970), Edwards et al. (1963), Bartholomew (1965), Cornfield (1969), Good (1976, 
1992), Roberts (1978), DeGroot (1982), Dawid (1983a), Smith (1984), Zellner 
(1985, 1987, 1988a), Pack (1986a, 1986b), Cifarelli (1987), Bernardo (1989), 
Ghosh (1991) and Berger (1993). 
For discussion of important specific topics, see Luce and Suppes (1965), Birn- 
baum(1968,1978), deFinetti (1968), Press (1980a, 1985a), Fishburn (1981, 1986, 
1988b), Dickey (1982), Geisser (1982, 1986), Good (1982, 1985, 1987, 1988a, 
1988b), Dawid (1983b, 1986a, 1992), Joshi (1983), LaMotte (1985), Genest and 
Zidek (1986), Goldstein (1986c), Racine-Poon et al. (1986), Hodges (1987), Eric- 
son (1988), Zellner (1988c), Trader (1989), Breslow (1990), Lindley (1991), Smith 
(1991), Barlow and Irony (1992), Ferguson et al. (1992), Arnold (1993), Kadane 
(1993), Bartholomew (1994), Berger (1994) and Hill (1994). 
13 
Chapter 2 
Foundations 
Summary 
The concept of rationality is explored in the context of representing beliefs or 
choosing actions in situations of uncertainty. An axiomatic basis, with intuitive 
operational appeal, is introduced f"rfhj* fniinrlntif ns of HpHdnn thpnry The dual 
concepts of probability and utility_are formally denned and analysed within this 
context. The criterion of maximising expected utility is shown to be the only 
decision criterion which is compatible with the axiom system. The analysis of 
sequential decision problems is shown to reduce to successive applications of the 
methodology introduced. Statistical inference is viewed as a particular decision 
problem which may be analysed within the framework of decision theory. The 
logarithmic score is established as the natural utility function to describe the 
preferences of an individual faced with a pure inference problem. Within this 
framwyfir1', ttlf fPICfpt of d'scrpp""^ hptwpen prohnbility rliitrihnrioni nnrl thf 
quantification of the amount of information in new data are naturally defined in 
terms of expected loss and expected inrrpasp in utility, respectively. 
2.1 BELIEFS AND ACTIONS 
We spend a considerable proportion of our lives, both private and professional, in 
a state of uncertainty. This uncertainty may relate to past situations, where direct 
14 
2 Foundations 
knowledge or evidence is not available, or has been lost or forgotten; or to present 
and future developments which are not yet completed. Whatever the circumstances, 
there is a sense in which all states of uncertainty may be described in the same way: 
namely, an individual feeling of incomplete knowledge in relation to a specified 
situation (a feeling which may, of course, be shared by other individuals). And yet 
it is obvious that we do not attempt to treat all our individual uncertainties with the 
same degree of interest or seriousness. 
Many feelings of uncertainty are rather insubstantial and we neither seek to 
analyse them, nor to order our thoughts and opinions in any kind of responsible 
way. This typically happens when we feel no actual or practical involvement with 
the situation in question. In other words, when we feel that we have no (or only 
negligible) capacity to influence matters, or that the possible outcomes have no (or 
only negligible) consequences so far as we are concerned. In such cases, we are not 
motivated to think carefully about our uncertainty either because nothing depends 
on it, or the potential effects are trivial in comparison with the effort involved in 
carrying out a conscious analysis. 
On the other hand, we all regularly encounter uncertain situations in which 
we at least aspire to behave "rationally" in some sense. This might be because we 
face the direct practical problem of choosing from among a set of possible actions, 
where each involves a range of uncertain consequences and we are concerned to 
avoid making an "illogical" choice. Alternatively, we might be called upon to 
summarise our beliefs about the uncertain aspects of the situation, bearing in mind 
that others may subsequently use this summary as the basis for choosing an action. 
In this case, we are concerned that our summary be in a form which will enable 
a "rational" choice to be made at some future time. More specifically, we might 
regard the summary itself, i.e., the choice of a particular mode of representing and 
communicating our beliefs, as being a form of action to which certain criteria of 
"rationality" might be directly applied. 
Our basic concern in this chapter is with exploring the concept of "rationality" 
in the context of representing beliefs or choosing actions in situations of uncertainty. 
To choose the best among a set of actions would, in principle, be immediate if we 
had perfect information about the consequences to which they would lead. So far 
as this work is concerned, interesting decision problems are those for which such 
perfect information is not available, and we must take uncertainty into account as 
a major feature of the problem. 
It might be argued that there are complex situations where we do have complete 
information and yet still find it difficult to take the best decision. Here, however, 
the difficulty is technical, not conceptual. For example, even though we have, 
in principle, complete information, it is typically not easy to decide what is the 
optimal strategy to rebuild a Rubik cube or which is the cheapest diet fulfilling 
specified nutritional requirements. We take the view that such problems are purely 
technical. In the first case, they result from the large number of possible strategies; 
2.1 Beliefs and Actions 
15 
in the second, they reduce to the mathematical problem of finding a minimum 
under certain constraints. But in neither case is there any doubt about the decision 
criterion to be used. In this work we shall not consider these kinds of combinatorial 
or mathematical programming problems, and we shall assume that in the presence 
of complete information we can, in principle, always choose the best alternative. 
Our concern, instead, is with the logical process of decision making in  
situations of uncertainty. In other words, with the decision criterion to be adopted 
when we do not have complete information and are thus faced with, at least some, 
elements of uncertainty. 
To avoid any possible confusion, we should emphasise that we do not interpret 
"actions in situations of uncertainty" in a narrow, directly "economic" sense. For 
example, within our purview we include the situation of an individual scientist 
summarising his or her own current beliefs following the results of an experiment; 
or trying to facilitate the task of others seeking to decide upon their beliefs in the 
light of the experimental results. 
It is assumed in our approach to such problems that the notion of "rational 
belief" cannot be considered separately from the notion of "rational action". Either 
a statement of beliefs in the light of available information is, actually or potentially, 
an input into the process of choosing some practical course of action, 
... it is not asserted that a belief... does actually lead to action, but would lead 
to action in suitable circumstances; just as a lump of arsenic is called poisonous 
not because it actually has killed or will kill anyone, but because it would kill 
anyone if he ate it (Ramsey, 1926). 
or, alternatively, a statement of beliefs might be regarded as an end in itself, in 
which case the choice of the form of statement to be made constitutes an action, 
Frequently, it is a question of providing a convenient summary of the data ... 
In such cases, the emphasis is on the inference rather than the decision aspect of 
problem, although formally it can still be considered a decision problem if the 
inferential statement itself is interpreted as the decision to be taken (Lehmann, 
1959/1986). 
We can therefore explore the notion of "rationality" for both beliefs and actions 
by concentrating on the latter and asking ourselves what kinds of rules should govern 
preference patterns among sets of alternative actions in order that choices made in 
accordance with such rules commend themselves to us as "rational", in that they 
cannot lead us into forms of behavioural inconsistency which we specifically wish 
to avoid. 
In Section 2.2, we describe the general structure of problems involving choices 
under uncertainty and introduce the idea of preferences between options. In  
Section 2.3, we make precise the notion of "rational" preferences in the form of axioms. 
16 
2 Foundations 
We describe these as principles of quantitative coherence because they specify the 
ways in which preferences need to be made quantitatively precise and fit together, 
or cohere, if "illogical" forms of behaviour are to be avoided. In Sections 2.4 and 
2.5, we prove that, in order to conform with the principles of quantitative coherence, 
degrees of belief about uncertain events should be described in terms of a (finitely 
additive) probability measure, relative values of individual possible consequences 
should be described in terms of a utility function, and the rational choice of an 
action is to select one which has the maximum expected utility. 
In Section 2.6, we discuss sequential decision problems and show that their 
analysis reduces to successive applications of the maximum expected utility  
methodology; in particular, we identify the design of experiments as a particular case 
of a sequential decision problem. In Section 2.7, we make precise the sense in 
which choosing a form of a statement of beliefs can be viewed as a special case 
of a decision problem. This identification of inference as decision provides the 
fundamental justification for beginning our development of Bayesian Statistics with 
the discussion of decision theory. Finally, a general review of ideas and references 
is given in Section 2.8. 
2.2 DECISION PROBLEMS 
2.2.1 Basic Elements 
We shall describe any situation in which choices are to be made among  
alternative courses of action with uncertain consequences as a decision problem, whose 
structure is determined by three basic elements: 
(i) a set {at, i G /} of available actions, one of which is to be selected; 
(ii) for each action ait a set {Ej, j G J} of uncertain events, describing the 
uncertain outcomes of taking action a8; 
(iii) corresponding to each set {Ej, j G J}, a set of consequences {cj, j G J}. 
The idea is as follows. Suppose we choose action a*; then one and only one of 
the uncertain events Ej, j £ J, occurs and leads to the corresponding consequence 
Cj, j G J. Each set of events {Ej, j G J} forms a partition (an exclusive and 
exhaustive decomposition) of the total set of possibilities. Naturally, both the set 
of consequences and the partition which labels them may depend on the particular 
action considered, so that a more precise notation would be {Eij, j G JJ and 
{cij, j G Jj} for each action ai. However, to simplify notation, we shall omit this 
dependence, while remarking that it should always be borne in mind. We shall 
come back to this point in Section 2.6. 
In practical problems, the labelling sets, / and J (for each i), are typically 
finite. In such cases, the decision problem can be represented schematically by 
means of a decision tree as shown in Figure 2.1. 
2.2 Decision Problems 
17 
Figure 2.1 Decision tree 
The square represents a decision node, where the choice of an action is  
required. The circle represents an uncertainty node where the outcome is beyond our 
control. Following the choice of an action and the occurrence of a particular event, 
the branch leads us to the corresponding consequence. 
Of course, most practical problems involve sequential considerations but, as 
shown in Section 2.6, these reduce, essentially, to repeated analyses based on the 
above structure. 
It is clear, either from our general discussion, or from the decision tree  
representation, that we can formally identify any a^, i £ I, with the combination of 
{Ej, j G J} and {cj, j G J} to which it leads. In other words, to choose a, is 
to opt for the uncertain scenario labelled by the pairs (Ej,Cj), j G J. We shall 
write at = {cj \Ej, j G J} to denote this identification, where the notation Cj \ Ej 
signifies that event Ej leads to consequence Cj, i.e., that ai{Ej) = Cj. 
An individual's perception of the state of uncertainty resulting from the choice 
of any particular ai is very much dependent on the information currently available. 
In particular, {Ej, j G J} forms a partition of the total set of relevant possibilities 
as the individual decision-maker now perceives them to be. Further information, 
of a kind which leads to a restriction on what can be regarded as the total set of 
possibilities, will change the perception of the uncertainties, in that some of the 
£j's may become very implausible (or even logically impossible) in the light of 
the new information, whereas others may become more plausible. It is therefore 
of considerable importance to bear in mind that a representation such as Figure 2.1 
only captures the structure of a decision problem as perceived at a particular point 
in time. Preferences about the uncertain scenarios resulting from the choices of 
actions depend on attitudes to the consequences involved and assessments of the 
uncertainties attached to the corresponding events. The latter are clearly subject to 
change as new information is acquired and this may well change overall preferences 
among the various courses of action. 
The notion of preference is, of course, very familiar in the everyday context 
of actual or potential choice. Indeed, an individual decision-maker often prefaces 
18 
2 Foundations 
an actual choice (from a menu, an investment portfolio, a range of possible forms 
of medical treatment, a textbook of statistical methods, etc.) with the phrase "I 
prefer... " (caviar, equities, surgery, Bayesian procedures, etc.). To prefer action 
ai to action a2 means that if these were the only two options available ax would 
be chosen (conditional, of course, on the information available at the time). In 
everyday terms, the idea of indifference between two courses of action also has 
a clear operational meaning. It signifies a willingness to accept an externally 
determined choice (for example, letting a disinterested third party choose, or tossing 
a coin). 
In addition to representing the structure of a decision problem using the three 
elements discussed above, we must also be able to represent the idea of preference 
as applied to the comparison of some or all of the pairs of available options. We 
shall therefore need to consider a fourth basic element of a decision problem: 
(iv) the relation < , which expresses the individual decision-maker's preferences 
between pairs of available actions, so that a\ < a-i signifies that a\ is not 
preferred to a^. 
These four basic elements have been introduced in a rather informal manner. 
In order to study decision problems in a precise way, we shall need to reformulate 
these concepts in a more formal framework. The development which follows, here 
and in Section 3.3, is largely based on Bernardo, Ferrandiz and Smith (1985). 
2.2.2 Formal Representation 
When considering a particular, concrete decision problem, we do not usually  
confine our thoughts to only those outcomes and options explicitly required for the 
specification of that problem. Typically, we expand our horizons to encompass 
analogous problems, which we hope will aid us in ordering our thoughts by  
providing suggestive points of reference or comparison. The collection of uncertain 
scenarios defined by the original concrete problem is therefore implicitly embedded 
in a somewhat wider framework of actual and hypothetical scenarios. We begin by 
describing this wider frame of discourse within which the comparisons of scenarios 
are to be carried out. It is to be understood that the initial specification of any such 
particular frame of discourse, together with the preferences among options within 
it, are dependent on the decision-maker's overall state of information at that time. 
Throughout, we shall denote this initial state of mind by Mo. 
We now give a formal definition of a decision problem. This will be presented 
in a rather compact form; detailed elaboration is provided in the remarks following 
the definition. 
Definition 2.1. (Decision problem). A decision problem is defined by the 
elements (£, C, A, <), where: 
(i) S is an algebra of relevant events, Ey, 
2.2 Decision Problems 
19 
(ii) C is a set of possible consequences, cy, 
(iii) A is a set of options, or potential acts, consisting of functions which map 
finite partitions of CI, the certain event in £, to compatibly-dimensioned, 
ordered sets of elements ofC; 
(iv) < is a preference order, taking the form of a binary relation between 
some of the elements of A. 
We now discuss each of these elements in detail. Within this wider frame of 
discourse, an individual decision-maker will wish to consider the uncertain events 
judged to be relevant in the light of the initial state of information M0. However, it 
is natural to assume that if Ex £ £ and E2 £ £ are judged to be relevant events then 
it may also be of interest to know about their joint occurrence, or whether at least 
one of them occurs. This means that E\ n £2 and Ex U £2 should also be assumed 
to belong to £. Repetition of this argument suggests that £ should be closed under 
the operations of arbitrary finite intersections and unions. Similarly, it is natural 
to require £ to be closed under complementation, so that Ec £ £. In particular, 
these requirements ensure that the certain event Cl and the impossible event 0, both 
belong to £. Technically, we are assuming that the class of relevant events has the 
structure of an algebra. (However, it can certainly be argued that this is too rigid 
an assumption. We shall provide further discussion of this and related issues in 
Section 2.8.4.) 
As we mentioned when introducing the idea of a wider frame of discourse, 
the algebra £ will consist of what we might call the real-world events (that is, 
those occurring in the structure of any concrete, actual decision problem that we 
may wish to consider), together with any other hypothetical events, which it may 
be convenient to bring to mind as an aid to thought. The class £ will simply be 
referred to as the algebra of (relevant) events. 
We denote by C the set of all consequences that the decision-maker wishes to 
take into account; preferences among such consequences will later be assumed to 
be independent of the state of information concerning relevant events. The class C 
will simply be referred to as the set of (possible) consequences. 
In our introductory discussion we used the term action to refer to each  
potential act available as a choice at a decision node. Within the wider frame of 
discourse, we prefer the term option, since the general, formal framework may  
include hypothetical scenarios (possibly rather far removed from potential concrete 
actions). 
So far as the definition of an option as a function is concerned, we note that 
this is a rather natural way to view options from a mathematical point of view: an 
option consists precisely of the linking of a partition of Cl, {Ej, j e J}, with a 
corresponding set of consequences, {cj, j £ J}. To represent such a mapping 
we shall adopt the notation {cj | Ej, j £ J}, with the interpretation that event Ej 
leads to consequence cy, j £ J. 
20 
2 Foundations 
It follows immediately from the definition of an option that the ordering of the 
labels within J is irrelevant, so that, for example, the options {c\ \E, c2 \ Ec), and 
{c2\Ec, c\ | E) are identical, and forms such as {c | E\, c\E2, Cj\Ej,j € J} and 
{c | Ei U E2, Cj\Ej, j € J} are completely equivalent. Which form is used in any 
particular context is purely a matter of convenience. Sometimes, the interpretation 
of an option with a rather cumbersome description is clarified by an appropriate 
reformulation. For example, a = {cx | E n G, c2 \ Ec n G, c3\ Gc} may be more 
compactly written as a = {ax | G, c3 | Gc}, with ax = {q | E1, c2 \ Ec}. Thus, if 
a = (c*0) I £*(,-) n fj- fc(j) € ^j.3 € J}. ai = ick(j) I ^*(j). *0') € Xj}. 
we shall use the composite function notation a = {a,j \Fj,j € J}. In all cases, 
the ordering of the labels is irrelevant. The class A of options, or potential actions, 
will simply be referred to as the action space. 
In defining options, the assumption of& finite partition into events oi£ seems 
to us to correspond most closely to the structure of practical problems. However, 
an extension to admit the possibility of infinite partitions has certain mathematical 
advantages and will be fully discussed, together with other mathematical extensions, 
in Chapter 3. 
In introducing the preference binary relation <, we are not assuming that all 
pairs of options (a\, a2) € A x A can necessarily be related by <. If the relation 
can be applied, in the sense that either a\ < a2 or a2 < a\ (or both), we say that 
a\ is not preferred to a2, or a2 is not preferred to a\ (or both). From <, we can 
derive a number of other useful binary relations. 
Definition 2.2. {Induced binary relations). 
(i) a,\ ~ a2 <=> ax < a2 and a2 < a\. 
(ii) a\ < a2 <=> a\ < a2 and it is not true that a2 < a\. 
(iii) a\ > a2 <=> a2 < ax. 
(iv) a\ > a2 <=> a2 < ax. 
Definition 2.2 is to be understood as referring to any options a\, a2 in A. To 
simplify the presentation we shall omit such universal quantifiers when there is no 
danger of confusion. The induced binary relations are to be interpreted to mean 
that a\ is equivalent to a2 if and only if ax ~ a2, and a\ is strictly preferred to a2 if 
and only if a\ > a2. Together with the interpretation of <, these suffice to describe 
all cases where pairs of options can be compared. 
We can identify individual consequences as special cases of options by writing 
c = {c | Cl}, for any c € C. Without introducing further notation, we shall simply 
regard c as denoting either an element of C, or the element {c | Q} of A. There 
will be no danger of any confusion arising from this identification. Thus, we shall 
2.2 Decision Problems 
21 
write c\ < c2 if and only if {c\ | Q,} < {c2 | 0} and say that consequence ci is 
not preferred to consequence c2. Strictly speaking, we should introduce a new 
symbol to replace < when referring to a preference relation over C x C, since < 
is defined over A x A. In fact, this parsimonious abuse of notation creates no 
danger of confusion and we shall routinely adopt such usage in order to avoid a 
proliferation of symbols. We shall proceed similarly with the binary relations ~ 
and < introduced in Definition 2.2. To avoid triviality, we shall later formally 
assume that there exist at least two consequences c\ and c2 such that c\ < c2. 
The basic preference relation between options, <, conditional on the initial 
state of information M0, can also be used to define a binary relation on £ x £, 
the collection of all pairs of relevant events. This binary relation will capture the 
intuitive notion of one event being "more likely" than another. Since, once again, 
there is no danger of confusion, we shall further economise on notation and also 
use the symbol < to denote this new uncertainty binary relation between events. 
Definition 2.3. (Uncertainty relation). 
E<F ^=> for all a < c2, {c2 | E,cx \ Ec} < {c2 | F,a | Fc}; 
we then say that E is not more likely than F. 
The intuitive content of the definition is clear. If we compare two dichotomised 
options, involving the same pair of consequences and differing only in terms of their 
uncertain events, we will prefer the option under which we feel it is "more likely" 
that the preferred consequence will obtain. Clearly, the force of this argument  
applies independently of the choice of the particular consequences c\ and c2, provided 
that our preferences between the latter are assumed independent of any  
considerations regarding the events E and F. 
Continuing the (convenient and harmless) abuse of notation, we shall also use 
the derived binary relations given in Definition 2.2 to describe uncertainty relations 
between events. Thus, E ~ F if and only if E and F are equally likely, and E > F 
if and only if E is strictly more likely than F. Since, for all c\ < c2, 
ci = {c210, cx | n} < {c21 n, ci 10} = c2, 
it is always true, as one would expect, that 0 < fl. 
It is worth stressing once again at this point that all the order relations over 
A x A, and hence over C x C and £ x £, are to be understood as personal, in 
the sense that, given an agreed structure for a decision problem, each individual 
is free to express his or her own personal preferences, in the light of his or her 
initial state of information M0. Thus, for a given individual, a statement such 
as E > F is to be interpreted as "this individual, given the state of information 
described by M0, considers event E to be more likely than event F ". Moreover, 
22 
2 Foundations 
Definition 2.3 provides such a statement with an operational meaning since for all 
c\ < c2, E > F is equivalent to an agreement to choose option {c2 | E, c\ \ Ec} in 
preference to option {c2 | F, cx \ Fc}. 
To complete our discussion of basic ideas and definitions, we need to  
consider one further important topic. Throughout this section, we have stressed that 
preferences, initially defined among options but inducing binary relations among 
consequences and events, are conditional on the current state of information. The 
initial state of information, taking as an arbitrary "origin" the first occasion on 
which an individual thinks systematically about the problem, has been denoted by 
M0. Subsequently, however, we shall need to take into accountfurther information, 
obtained by considering the occurrence of real-world events. Given the assumed 
occurrence of a possible event G, preferences between options will be described 
by a new binary relation <g, taking into account both the initial information Mo 
and the additional information provided by G. The obvious relation between < 
and <g is given by the following: 
Definition 2.4. {Conditional preference). For any G > 0, 
(i) a-i <G a,2 <=> for all a, {ai \G,a\ Gc} < {a2 | G, a \ Gc}; 
(ii) E <G F «=» for cx <G c2, {c2 \E,d\ Ec} <G {c2 \F,Cl\ Fc}. 
The intuitive content of the definition is clear. If we do not prefer a\ to a2, 
given G, then this preference obviously carries over to any pair of options leading, 
respectively, to a\ or a2 if G occurs, and defined identically if Gc occurs.  
Conversely, comparison of options which are identical if Gc occurs depends entirely on 
consideration of what happens if G occurs. Naturally, the induced binary relations 
set out in Definition 2.2 have their obvious counterparts, denoted by ~g and <g- 
The induced binary relation between consequences is obviously defined by 
C\ <G C2 <=> {Ci | Q} <G {C2 | ft}. 
However, when we come, in Section 2.3, to discuss the desirable properties of < 
and <G we shall make formal assumptions which imply that, as one would expect, 
ci <g c2 if and only if Cj < c2, so that preferences between pure consequences are 
not affected by additional information regarding the uncertain events in E. 
The definition of the conditional uncertainty relation <G is a simple translation 
of Definition 2.3 to a conditional preference setting. The conditional uncertainty 
relation <g induced between events is of fundamental importance. This relation, 
with its derived forms ~g and <g, provides the key to investigating the way in 
which uncertainties about events should be modified in the light of new  
information. Obviously, if G = ft , all conditional relations reduce to their unconditional 
counterparts. Thus, it is only when 0 < G < ft that conditioning on G may yield 
new preference patterns. 
2.3 Coherence and Quantification 
23 
2.3 COHERENCE AND QUANTIFICATION 
2.3.1 Events, Options and Preferences 
The formal representation of the decision-maker's "wider frame of discourse"  
includes an algebra of events £, a set of consequences C, and a set of options A, 
whose generic element has the form {c, | Ej, j € J}, where {Ej, j € J} is a finite 
partition of the certain event ft, Ej € £,Cj € C, j € J. The set .4 x A is equipped 
with a collection of binary relations <g, G > 0, representing the notion that one 
option is not preferred to another, given the assumed occurrence of a possible event 
G. In addition, all preferences are assumed conditional on an initial state of  
information, Mo, with the binary relation < (i.e., <q) representing the preference 
relation on Ax A conditional on Mo alone. 
We now wish to make precise our assumptions about these elements of the 
formal representation of a decision problem. Bearing in mind the overall objective 
of developing a rational approach to choosing among options, our assumptions, 
presented in the form of a series of axioms, can be viewed as responses to the 
questions: "what rules should preference relations obey?" and "what events should 
be included in £?" 
Each formal axiom will be accompanied by a detailed discussion of the  
intuitive motivation underlying it. 
It is important to recognise that the axioms we shall present areprescriptive, not 
descriptive. Thus, they do not purport to describe the ways in which individuals 
actually do behave in formulating problems or making choices, neither do they 
assert, on some presumed "ethical" basis, the ways in which individuals should 
behave. The axioms simply prescribe constraints which it seems to us imperative 
to acknowledge in those situations where an individual aspires to choose among 
alternatives in such a way as to avoid certain forms of behavioural inconsistency. 
2.3.2 Coherent Preferences 
We shall begin by assuming that problems represented within the formal framework 
are non-trivial and that we are able to compare any pair of simple dichotomised 
options. 
Axiom 1. (Comparability of consequences and dichotomised options). 
(i) There exist consequences C\, c2 such that c\ < C2. 
(ii) For all consequences C\, c2, c3, c4 and events E, F, 
either {c2 | E, cx \ Ec} < {c2 \ F, cx \ Fc} 
0/-{c2|£,Cl|£c}>{c2|F,Cl|Fc}. 
24 
2 Foundations 
Discussion of Axiom 1. Condition (i) is very natural. If all consequences were 
equivalent, there would not be a decision problem in any real sense, since all choices 
would certainly lead to precisely equivalent outcomes. We have already noted that, 
in any given decision problem, C can be defined as simply the set of consequences 
required for that problem. Condition (ii) does not therefore assert that we should 
be able to compare any pair of conceivable options, however bizarre or fantastic. 
In most practical problems, there will typically be a high degree of similarity in the 
form of the consequences (e.g. all monetary), although it is easy to think of examples 
where this form is complex (e.g. combinations of monetary, health and industrial 
relations elements). We are trying to capture the essence of what is required for 
an orderly and systematic approach to comparing alternatives of genuine interest. 
We are not, at this stage, making the direct assumption that all options, however 
complex, can be compared. But there could be no possibility of an orderly and 
systematic approach if we were unwilling to express preferences among simple 
dichotomised options and hence (with E = F = fl) among the consequences 
themselves. Condition (ii) is therefore to be interpreted in the following sense: "If 
we aspire to make a rational choice between alternative options, then we must at 
least be willing to express preferences between simple dichotomised options." 
There are certainly many situations where we find the task of comparing  
simple options, and even consequences, very difficult. Resource allocation among 
competing health care programmes involving different target populations and 
morbidity and mortality rates is one obvious such example. However, the  
difficulty of comparing options in such cases does not, of course, obviate the need 
for such comparisons if we are to aspire to responsible decision making. 
We shall now state our assumptions about the ways in which preferences 
should fit together or cohere in terms of the order relation over Ax A. 
Axiom 2. (Transitivity of preferences). 
(i) a <a. 
(ii) Ifai <a2 and a2 < a3, then a\ < a3. 
Discussion of Axiom 2. Condition (i) has obvious intuitive support. It would 
make little sense to assert that an option was strictly preferred to itself. It would 
also seem strangely perverse to claim to be unable to compare an option with 
itself! We note that, from Definition 2.2 (i), if a < a, then a ~ a. Condition (ii) 
requires preferences to be transitive. The intuitive basis for such a requirement is 
perhaps best illustrated by considering the consequences of intransitive preferences. 
Suppose, therefore, that we found ourselves expressing the preferences a\ < a2, 
a2 < a3 and a3 < ax among three options a\, a2 and a3. The assertion of 
strict preference rules out equivalence between any pair of the options, so that our 
2.3 Coherence and Quantification 
25 
expressed preferences reveal that we perceive some actual difference in value (no 
matter how small) between the two options in each case. Let us now examine 
the behavioural implications of these expressed preferences. If we consider, for 
example, the preference ax < a2, we are implicitly stating that there exists a "price", 
say x, that we would be willing to pay in order to move from a position of having 
to accept option a\ to one where we have, instead, to accept option a2. Let y and 
z denote the corresponding "prices" for switching from a2 to a3 and from a3 to 
a\, respectively. Suppose now that we are confronted with the prospect of having 
to accept option a\. By virtue of the expressed preference a\ < a2 and the above 
discussion, we are willing to pay x in order to exchange option ax for option a2. 
But now, by virtue of the preference a2 < a3, we are willing to pay y in order 
to exchange a2 for a3. Repeating the argument once again, since a3 < a\ we 
are willing to pay z in order to avoid a3 and have, instead, the prospect of option 
a\. We would thus have paid x + y + z in order to find ourselves in precisely 
the same position as we started from! What is more, we could find ourselves 
arguing through this cycle over and over again. Willingness to act on the basis 
of intransitive preferences is thus seen to be equivalent to a willingness to suffer 
unnecessarily the certain loss of something to which one attaches positive value. 
We regard this as inherently inconsistent behaviour and recall that the purpose of 
the axioms is to impose rules of coherence on preference orderings that will exclude 
the possibility of such inconsistencies. Thus, Axiom 2(ii) is to be understood in the 
following sense: "If we aspire to avoid expressing preferences whose behavioural 
implications are such as to lead us to the certain loss of something we value, then 
we must ensure that our preferences fit together in a transitive manner." 
Our discussion of this axiom is, of course, informal and appeals to directly 
intuitive considerations. At this stage, it would therefore be inappropriate to 
become involved in a formal discussion of terms such as "value" and "price". It 
is intuitively clear that if we assert strict preference there must be some amount 
of money (or grains of wheat, or beads, or whatever), however small, having a 
"value" less than the perceived difference in "value" between the two options. We 
should therefore be willing to pay this amount to switch from the less preferred 
to the more preferred option. 
The following consequences of Axiom 2 are easily established and will prove 
useful in our subsequent development. 
Proposition 2.1. (Transitivity of uncertainties). 
(i) E ~ E. 
(ii) Ex < E2 and E2 < E3 imply Ex < E3. 
Proof. This is immediate from Definition 2.3 and Axiom 2. <, 
26 
2 Foundations 
Proposition 2.2. (Derived transitive properties). 
(i) Ifa\ ~ a2 an*/ a2 ~ a3 f/ten aj ~ 03. 
If Ei ~ E2 and E2 ~ E3 then Ex ~ E3. 
(ii) //"ai < a2 and a2 ~ 03 then ax < a3. 
If Ex < E2 and E2 ~ E3 then Ex < E3. 
Proof. To prove (i), let ax ~ a2anda2 ~ a3 so that, by Definition 2.2, ax < a2, 
a2 < a\ and a2 < a3, a3 < a2. Then, by Axiom 2(ii), ax < a3 and a3 < ax, and 
thus ax ~ a3. A similar argument applies to events using Proposition 2.1. Again, 
part (ii) follows rather similarly. < 
Axiom 3. (Consistency of preferences). 
(i) lfc\ < c2 then, for all G > 0, cx <G c2. 
(ii) // for some cx < c2, {c2 \E,cx\ Ec] < {c2 \F,cx\ Fc}, then E < F. 
(iii) If, for some candG> 0, {ax \G,c\Gc} < {a2 \G,c\ Gc}, 
then ax <g a2- 
Discussion of Axiom 3. Condition (i) formalises the idea that preferences 
between pure consequences should not be affected by the acquisition of further 
information regarding the uncertain events in £. Conditions (ii) and (iii) ensure 
that Definitions 2.3 and 2.4 have operational content. Indeed, (ii) asserts that if 
we have {c2 \ E, cx \ Ec} < {c2 \F,cx\ Fc} for some cx < c2 then we should 
have this preference for any cx < c2. This formalises the intuitive idea that the 
stated preference should only depend on the "relative likelihood" of E and F 
and should not depend on the particular consequences used in constructing the 
options. Similarly, (iii) asserts that if we have the preference {ax \G,c\ Gc} < 
{a2 \G,c\ Gc} for some c then, given G, ax should not be preferred to a2, so that, 
for any a, {ax \G,a\ Gc} < {a2 \ G, a | Gc}. This latter argument is a version 
of what might be called the sure-thing principle: if two situations are such that 
whatever the outcome of the first there is a preferable corresponding outcome of 
the second, then the second situation is preferable overall. 
An important implication of Axiom 3 is that preferences between  
consequences are invariant under changes in the information "origin" regarding events 
in £. 
Proposition 2.3. (Invariance of preferences between consequences). 
Ci < c2 if and only if there exist G > 0 such that cx <q c2. 
Proof. If cx < c2 then, by Axiom 3(i), cx <g c2 for any event G. Conversely, 
by Definition 2.4(i), for any G > 0, cx <G c2 implies that for any option a, one 
has {cx I G, a | Gc} < {c2 \G,a\ Gc}. Taking a = {cx | G, c2 | Gc}, this implies 
that {ci I G, c2 I Gc} < {cx | 0, c2 \ O}. If cx > c2 this implies, by Axiom 3(ii), that 
G < 0, thus contradicting G > 0. Hence, by Axiom l(ii), cx < c2. < 
2.3 Coherence and Quantification 
27 
Another important consequence of Axiom 3 is that uncertainty orderings of 
events respect logical implications, in the sense that if E logically implies F, i.e., 
if E C F, then F cannot be considered less likely than E. 
Proposition 2.4. (Monotonicity). IfECF then E < F. 
Proof. For any c\ < c2, define 
a1 = {c2\E,c1\Ec} = {c1\F-E,{c2\E,cl\Ec}\(F-E)c}, 
a2 = {c2\F,Cl\Fc} ={c2\F-E,{c2\E,c1\Ee}\(F-E)e}. 
By Axiom 3(i) with G = F - E = FC\ Ec, a\ < a2. It now follows immediately 
from Definition 2.2 that E < F. <, 
This last result is an example of how coherent qualitative comparisons of 
uncertain events in terms of the "not more likely" relation conform to intuitive 
requirements. 
If follows from Proposition 2.4 that, as one would expect, for any event E, 
0 < E < £1. We shall mostly work, however, with "significant" events, for which 
this ordering is strict. 
Definition 2.5. (Significant events). An event E is significant given G > 0 
ifc\ <g c2 implies that C\ <g {c2 \ E,cx \ Ec} <G c2. IfG = £1, we shall 
simply say that E is significant. 
Intuitively, significant events given G are those operationally perceived by 
the decision-maker as "practically possible but not certain" given the information 
provided by G. Thus, given G > 0 and assuming cx <g c2, if E is judged to be 
significant given G, one would strictly prefer the option {c2 | E, cx \ Ec} to cx for 
sure, since it provides an additional perceived possibility of obtaining the more 
desirable consequence c2. Similarly, one would strictly prefer c2 for sure to the 
stated option. 
Proposition 2.5. (Characterisation of significant events). An event E is  
significant given G > 0, if and only if 0 < E n G < G. In particular, E is 
significant if and only if$<E<£l. 
Proof. Using Definitions 2.4 and 2.5, if E is significant given G then, for all 
ci <g C2 and for any option a, 
{ci | G, a | Gc} < {c2 | E n G, cx \ Ec n G, a | Gc} <{c2\G,a\ Gc} . 
Taking a = c\, we have 
Cl = {c2 | 0, Cl | n} < {c2 IE n G, d|(£n G)c} < {c2 | G, Cl I Gc} 
28 
2 Foundations 
and hence, by Definition 2.3, 0 < E n G < G. Conversely, if 0 < £ n G < G, 
{ci | G, d | Gc} <{c2|£nG,c1|£cn G, c: | Gc} < {c2 | G, a \ Gc} 
and hence, by Axiom 3(iii), c\ <g {c2 \E,c\\ Ec} <g c2. If, in particular, G - Ct 
then E is significant if and only if 0 < E < £1. < 
The operational essence of "learning from experience" is that a  
decisionmaker's preferences may change in passing from one state of information to a 
new state brought about by the acquisition of further information regarding the 
occurrence of events in £, which leads to changes in assessments of uncertainty. 
There are, however, too many complex ways in which such changes in assessments 
can take place for us to be able to capture the idea in a simple form. On the other 
hand, the very special case in which preferences do not change is easy to describe 
in terms of the concepts thus far available to us. 
Definition 2.6. (Pairwise independence of events). 
We say that E and F are (pairwise) independent, denoted by E J_ F, if, and 
only if, for all c, C\, c2 
(i) c • {c2 | E,ci | Ec} => c «F {c2 \E,ci\ Ec}, 
(ii) c • {c2 | F, cx \Fc}^c »E {c2 I F, cx | F% 
where • is any one of the relations < , ~ or >. 
The definition is given for the simple situation of preferences between pure 
consequences and dichotomised options. Since by Proposition 2.3 preferences  
regarding pure consequences are unaffected by additional information, the condition 
stated captures, in an operational form, the notion that uncertainty judgements about 
E, say, are unaffected by the additional information F. We interpret E J_ F as 
"E is independent of F". An alternative characterisation will be given in  
Proposition 2.13. 
2.3.3 Quantification 
The notion of preference between options, formalised by the binary relation <, 
provides a qualitative basis for comparing options and, by extension, for comparing 
consequences and events. The coherence axioms (Axioms 1 to 3) then provide a 
minimal set of rules to ensure that qualitative comparisons based on < cannot have 
intuitively undesirable implications. 
We shall now argue that this purely qualitative framework is inadequate for 
serious, systematic comparisons of options. An illuminating analogy can be drawn 
between < and a number of qualitative relations in common use both in an everyday 
setting and in the physical sciences. 
2.3 Coherence and Quantification 
29 
Consider, for example, the relations not heavier than, not longer than, not 
hotter than. It is abundantly clear that these cannot suffice, as they stand, as an 
adequate basis for the physical sciences. Instead, we need to introduce in each case 
some form of quantification by setting up a standard unit of measurement, such 
as the kilogram, the metre, or the centigrade interval, together with an (implicitly) 
continuous scale such as arbitrary decimal fractions of a kilogram, a metre, a 
centigrade interval. This enables us to assign a numerical value, representing 
weight, length, or temperature, to any given physical or chemical entity. 
This can be achieved by carrying out, implicitly or explicitly, a series of 
qualitative pairwise comparisons of the feature of interest with appropriately chosen 
points on the standard scale. For example, in quantifying the length of a stick, we 
place one end against the origin of a metre scale and then use a series of qualitative 
comparisons, based on "not longer than" (and derived relations, such as "strictly 
longer than"). If the stick is "not longer than" the scale mark of 2.5 metres, but is 
"strictly longer than" the scale mark of 2.4 metres, we might lazily report that the 
stick is "2.45 metres long". If we needed to, we could continue to make qualitative 
comparisons of this kind with finer subdivisions of the scale, thus extending the 
number of decimal places in our answer. The example is, of course, a trivial one, 
but the general point is extremely important. Precision, through quantification, is 
achieved by introducing some form of numerical standard into a context already 
equipped with a coherent qualitative ordering relation. 
We shall regard it as essential to be able to aspire to some kind of quantitative 
precision in the context of comparing options. It is therefore necessary that we 
have available some form of standard options, whose definitions have close links 
with an easily understood numerical scale, and which will play a role analogous to 
the standard metre or standard kilogram. As a first step towards this, we make the 
following assumption about the algebra of events, E. 
Axiom 4. {Existence of standard events). There exists a subalgebra S of ' E 
and a function // : S —► [0,1] such that: 
(i) Sx < S2 if, and only if, n{Sx) < fi(S2); 
(ii) Si n S2 = 0 implies that n(Sx U S2) = //(Si) + n{S2); 
(iii) for any number a in [0,1], and events E, F, there is a standard event S 
such that fi(S) = a, E 1 S and F 1 S; 
(iv) Si 1 S2 implies that fi(Si n S2) = n(Si)fi(S2). 
(v) ifE LS,F LS and ELF, then E ~ S => E ~F S. 
Discussion of Axiom 4. A family of events satisfying conditions (i) and (ii) 
is easily identified by imagining an idealised roulette wheel of unit circumference. 
We suppose that no point on the circumference is "favoured" as a resting place for 
the ball (considered as a point) in the sense that given any ci, c2 and events Si, S2 
corresponding to the ball landing within specified connected arcs, or finite unions 
30 
2 Foundations 
and intersections of such arcs, {c\ \ Si, ci \ Sf} and {c\ \ S2,6215*2} are considered 
equivalent if and only if n(Si) = /i(<?2), where p, is the function mapping the "arc- 
event" to its total length. Conditions (i) and (ii) are then intuitively obvious, as is the 
fact, in (iii), that for any a G [0,1] we can construct an S with p,(S) = a. Note that 
S is required to be an algebra and thus both 0 and fi are standard events. It follows 
from Proposition 2.4 and Axiom 4(i) that ^(0) = 0 and p,(Q) = 1. The remainder 
of (iii) is intuitively obvious; we note first that the basic idea of an idealised roulette 
wheel does assume that each "play" on such a wheel is "independent", in the sense 
of Definition 2.6, of any other events, including previous "plays" on the same wheel. 
Thus, for any events E, F in €, we can always think of an "independent" play which 
generates independent events 5* in S with p,(S) = a for any specified a in [0,1]. In 
this extended setting, if we think of the circumferences for two independent plays 
as unravelled to form the sides of a unit square, with p, mapping events to the areas 
they define, condition (iv) is clearly satisfied. Finally, (v) encapsulates an obviously 
desirable consequence of independence; namely, that if E is independent of F and 
5*, and F is independent of S, a judgement of equivalence between E and 5* should 
not be affected by the occurrence of F. 
We will refer to S as a standard family of events in £ and will think of £ as 
the algebra generated by the relevant events in the decision problem together with 
the elements of S. Other forms of standard family satisfying (i) to (v) are easily 
imagined. For example, it is obvious that a roulette wheel of unit circumference 
could be imagined cut at some point and "unravelled" to form a unit interval. The 
underlying image would then be that of a point landing in the unit interval and an 
event S such that p(S) = p would denote a subinterval of length p; alternatively, 
we could imagine a point landing in the unit square, with 5* denoting a region of 
area p. The obvious intuitive content of conditions (i) to (v) can clearly be similarly 
motivated in these cases, the discussion for the unit interval being virtually identical 
to that given for the roulette wheel. It is important to emphasise that we do not 
require the assumption that standard families of events actually, physically exist, 
or could be precisely constructed in accordance with conditions (i) to (v). We only 
require that we can invoke such a set up as a mental image. 
There is, of course, an element of mathematical idealisation involved in  
thinking about all p G [0,1], rather than, for example, some subset of the rationals, 
corresponding to binary expansions consisting of zeros from some specified  
location onwards, reflecting the inherent limits of accuracy in any actual procedure 
for determining arc lengths or areas. The same is true, however, of all scientific 
discourse in which measurements are taken, in principle, to be real numbers, rather 
than a subset of the rationals chosen to reflect the limits of accuracy in the physical 
measurement procedure being employed. Our argument for accepting this degree 
of mathematical idealisation in setting up our formal system is the same as would 
apply in the physical sciences. Namely, that no serious conceptual distortion is 
introduced, while many irrelevant technical difficulties are avoided; in particular, 
2.3 Coherence and Quantification 
31 
those concerning the non-closure of a set of numbers with respect to operations of 
interest. This argument is not universally accepted, however, and further, related 
discussion of the issue is provided in Section 2.8. 
Our view is that, from the perspective of the foundations of decision-making, 
the step from the finite to the infinite implicit in making use of real numbers is 
simply a pragmatic convenience, whereas the step from comparing a finite set of 
possibilities to comparing an infinite set has more substantive implications. We 
have emphasised this latter point by postponing infinite extensions of the decision 
framework until Chapter 3. 
Proposition 2.6. (Collections of disjoint standard events). 
For any finite collection {a\,..., an} of real numbers such that a, > 0 and 
a\ + ••• + an < 1 there exists a corresponding collection {Si,..., S„} of 
disjoint standard events such that /i(S,) = a,, i = 1,..., n. 
Proof. By Axiom 4(iii) there exists Si such that /x(Si) = a\. For 1 < j < n, 
suppose inductively that Si,..., Sj_i are disjoint, Bj = Si U • • • U Sj_x and define 
ft = a\-\ (- otj-i = n{Bj). By Axiom4 (iii, iv), there exists Tj in S such that 
n{Bj n Tj) = n{Bj){aj/{l - ft)}. Define Sj = Tj n B), so that Sj D St = 0, 
i = 1,... ,j - 1. Then, Tj = Sj U (Tj n Bj) and hence, using Axiom 4(ii), 
n(Tj) = n(Sj) + n(Tj n Bj). Thus, n(Sj) = aj/(l - ft) - a^j/i1 ~ ft) = ai 
and the result follows. < 
Axiom 5. (Precise measurement of preferences and uncertainties). 
(i) Ifci<c< c2, there exists a standard event S such that 
c~{c2|S,Ci|Sc}. 
(ii) For each event E, there exists a standard event S such that E ~ S. 
Discussion of Axiom 5. In the introduction to this section, we discussed the 
idea of precision through quantification and pointed out, using analogies with other 
measurement systems such as weight, length and temperature, that the process is 
based on successive comparisons with a standard. Let Sq denote a standard event 
such that n{Sq) = q. We start with the obvious preferences, {c2 | S0,ci | Sg} < c < 
{c2 | Si, c\ | S[}, for any Ci < c < c2, and then begin to explore comparisons with 
standard options based on Sx, Sy with 0 < x < y < 1. In this way, by gradually 
increasing x away from 0 and decreasing y away from 1, we arrive at comparisons 
suchas{c2 | Sx,c\ \ S£} < c < {c2 | Sy,C\ | Sy}, with the difference?/—x becoming 
increasingly small. Intuitively, as we increase x, {c2 | Sx, C\ \ Sx} becomes more 
and more "attractive" as an option, and as we decrease y, {c2 | Sy, C\ \ Sy} becomes 
less "attractive". Any given consequence c, such that c\ < c < c2, can flierefore be 
"sandwiched" arbitrarily tightly and, in the limit, be judged equivalent to one of the 
standard options defined in terms of c\, c2. The essence of Axiom 5(i) is that we 
can proceed to a common limit, a, say, approached from below by the successive 
32 
2 Foundations 
values of x and above by the successive values of y. The standard family of options 
is thus assumed to provide a continuous scale against which any consequence can 
be precisely compared. 
Condition (ii) extends the idea of precise comparison to include the assumption 
that, for any event E and for all consequences c1; c2 such that C\ < c2, the option 
{c2 | E, c\ | Ec} can be compared precisely with the family of standard options 
{c2 | Sx, c\ | S£}, x G [0,1], defined by cx and c2. The underlying idea is similar 
to that motivating condition (i). Indeed, given the intuitive content of the relation 
"not more likely than", we can begin with the obvious ordering {c2 | S0, c\ \ Sq) < 
{c2 | E, c\ | Ec} < {c21 Si, ci | Si} for any event E, and then consider refinements 
of this of the form {c2 | Sx, cx \ Scx} < {c2 | E, cl | Ec} < {c2 | Sy, cx \ Scy}, with x 
increasing gradually from 0, y decreasing gradually from 1, and y — x becoming 
increasingly small, so that, in terms of the ordering of the events, Sx < E < Sr 
Again, the essence of the axiom is that this "sandwiching" can be refined arbitrarily 
closely by an increasing sequence of a;'s and a decreasing sequence of y's tending 
to a common limit. 
The preceding argument certainly again involves an element of mathematical 
idealisation. In practice, there might, in fact, be some interval of indifference, in 
the sense that we judge {c2 | Sx, C\ \ Sx} <c< {c2 | Sy, c\ \ Sy} for some (possibly 
rational) x and y but feel unable to express a more precise form of preference. This 
is analogous to the situation where a physical measuring instrument has inherent 
limits, enabling one to conclude that a reading is in the range 3.126 to 3.135, say, 
but not permitting a more precise statement. In this case, we would typically report 
the measurement to be 3.13 and proceed as if 'this were a precise measurement. We 
formulate the theory on the prescriptive assumption that we aspire to exact  
measurement (exact comparisons in our case), whilst acknowledging that, in practice, 
we have to make do with the best level of precision currently available (or devote 
some resources to improving our measuring instruments!). 
In the context of measuring beliefs, several authors have suggested that this 
imprecision be formally incorporated into the axiom system. For many  
applications, this would seem to be an unnecessary confusion of the prescriptive and 
the descriptive. Every physicist or chemist knows that there are inherent limits 
of accuracy in any given laboratory context but, so far as we know, no one has 
suggested developing the structures of theoretical physics or chemistry on the 
assumption that quantities appearing in fundamental equations should be  
constrained to take values in some subset of the rationals. However, it may well be 
that there are situations where imprecision in the context of comparing  
consequences is too basic and problematic a feature to be adequately dealt with by an 
approach based on theoretical precision, tempered with pragmatically  
acknowledged approximation. We shall return to this issue in Section 2.8. 
2.4 Beliefs and Probabilities 
33 
The particular standard option to which c is judged equivalent will, of course, 
depend on c, but we have implicitly assumed that it does not depend on any  
information we might have concerning the occurrence of real-world events. Indeed, 
Proposition 2.3 implies that our "attitudes" or "values" regarding consequences are 
fixed throughout the analysis of any particular decision problem. It is intuitively  
obvious that, if the time-scale on which values change were not rather long compared 
with the time-scale within which individual problems are analysed, there would be 
little hope for rational analysis of any kind. 
2.4 BELIEFS AND PROBABILITIES 
2.4.1 Representation of Beliefs 
It is clear that an individual's preferences among options in any decision problem 
should depend, at least in part, on the "degrees of belief which that individual 
attaches to the uncertain events forming part of the definitions of the options. 
The principles of coherence and quantification by comparison with a standard, 
expressed in axiomatic form in the previous section, will enable us to give a formal 
definition of degree of belief, thus providing a numerical measure of the uncertainty 
attached to each event. 
The conceptual basis for this numerical measure will be seen to derive from 
the formal rules governing quantitative, coherent preferences, irrespective of the 
nature of the uncertain events under consideration. This is in vivid contrast to what 
are sometimes called the classical w&frequency approaches to defining numerical 
measures of uncertainty (see Section 2.8), where the existence of symmetries and 
the possibility of indefinite replication, respectively, play fundamental roles in 
defining the concepts for restricted classes of events. 
We cannot emphasise strongly enough the important distinction between  
defining a general concept and evaluating a particular case. Our definition will depend 
only on the logical notions of quantitative, coherent preferences; our practical  
evaluations will often make use of perceived symmetries and observed frequencies. 
We begin by establishing some basic results concerning the uncertainty relation 
between events. 
Proposition 2.7. (Complete comparability of events). 
Either Ex > E2, or Ex ~ E2, or E2 > Ei. 
Proof. By Axiom 5(ii), there exist Si and S2 such that E\ ~ Si and E2 ~ S2; 
the complete ordering now follows from Axiom 4(i) and Proposition 2.1. < 
34 
2 Foundations 
We see from Proposition 2.7 that, although the order relation < between 
options was not assumed to be complete (i.e., not all pairs of options were assumed 
to be comparable), it turns out, as a consequence of Axiom 5 (the axiom of precise 
measurement), that the uncertainty relation induced between events is complete. 
A similar result concerning the comparability of all options will be established in 
Section 2.5. 
Proposition 2.8. (Additivity of uncertainty relations). If A < B,C < D 
and AC\C = B D D = 0, then A U C < B U D. Moreover, if A < B or 
C < D, then Au C < B U D. 
Proof. We first show that, for any G, if An G = B n G = 0 then A < B <=> 
A U G < B U G. For any c2 > c1; A n G = B n G = 0, define: 
a, = {c2 | A,Cl | Ac} = {c, | G, {c2 | Aci | Ac} | Gc} 
a2 = {c2\B,cl\Bc} = {cl\G,{c2\B,cl\Bc}\Gc} 
a3 = {c2|iUG,Cl|(AU Gf} = {c2 | G,{c2 | A,c, \AC} | Gc} 
a4 = {c2 | 5 U G, Cl|(BU G)c} = {c2 | G, {c2 | B, c, | 5C} | Gc}. 
Then, by Definition 2.3, A < B <=> a\ < a2; by Axiom 3, a\ < a2 <^=> 
a3 < a4; and using again Definition 2.3, 03 < a4 <=$■ A U G < B U G. Thus, 
AU(C-5)<BU(C-J?) = BUC = CU(B-C)<DU(B-C), 
yluC = Au(C-B)u(CnB)<Du(j?-C)u(CnB) = BuD. 
The final statement follows from essentially the same argument. < 
We now make the key definition which enables us to move to a quantitative 
notion of degree of belief. 
Definition 2.7. (Measure of degree of belief). Given an uncertainty relation 
<, the probability P(E) of an event E is the real number fj,(S) associated 
with any standard event S such that E ~ S. 
This definition provides a natural, operational extension of the qualitative 
uncertainty relation encapsulated in Definition 2.3, by linking the equivalence of 
any E 6 S to some S G S and exploiting the fact that the nature of the construction 
of S provides a direct obvious quantification of the uncertainty regarding S. 
With our operational definition, the meaning of a probability statement is clear. 
For instance, the statement P(E) = 0.5 precisely means that E is judged to be 
equally likely as a standard event of 'measure' 0.5, maybe a conceptual perfect coin 
falling heads, or a computer generated 'random' integer being an odd number. 
2.4 Beliefs and Probabilities 
35 
It should be emphasised that, according to Definition 2.7, probabilities are 
always personal degrees of belief, in that they are a numerical representation of 
the decision-maker's personal uncertainty relation < between events. Moreover, 
probabilities are always conditional on the information currently available. It makes 
no sense, within the framework we are discussing, to qualify the word probability 
with adjectives such as "objective", "correct" or "unconditional". 
Since probabilities are obviously conditional on the initial state of information 
Mo, a more precise and revealing notation in Definition 2.7 would have been 
P(E | M0). In order to avoid cumbersome notation, we shall stick to the shorter 
version, but the implicit conditioning on M0 should always be borne in mind. 
Proposition 2.9. (Existence and uniqueness). Given an uncertainty relation 
<, there exists a unique probability P(E) associated with each event E. 
Proof. Existence follows from Axiom 5(ii). For uniqueness, if E ~ Si and 
E ~ S2 then by Proposition 2.2(ii), Si ~ S2. The result now follows from 
Axiom 4(i). <j 
Definition 2.8. (Compatibility). A function f : £ —> 9ft is said to be  
compatible with an order relation <on£x£if for all events, 
E<F <=^ f(E) < f(F). 
Proposition 2.10. (Compatibility of probability and degrees of belief). 
The probability function P(.) is compatible with the uncertainty relation <. 
Proof. By Axiom 5(ii) there exist standard events Si and S2 such that E ~ Si 
and F ~ S2. Then, by Proposition 2.2(ii) , E < F iff Si < S2 and hence, by 
Axiom 4(i), iff /x(Si) < /x(S2). The result follows from Definition 2.7. < 
The following proposition is of fundamental importance. It establishes that 
coherent, quantitative degrees of belief have the structure of a finitely additive  
probability measure over £. Moreover, it establishes that significant events, i.e., events 
which are "practically possible but not certain", should be assigned probability 
values in the open interval (0,1). 
Proposition 2.11. (Probability structure of degrees of belief). 
(i) P(0) = 0 and P{ft) = 1. 
(ii) lfEC\F = 0, then P(E U F) = P(E) + P(F). 
(iii) E is significant if, and only if,0< P(E) < 1. 
36 
2 Foundations 
Proof, (i) By Definition 2.7,0 < P{E) < 1. Moreover, by Axiom 4(iii) there 
exist S* and S* such that [i(St) = 0 and /x(S*) = 1. By Proposition 2.4, 0 < S, 
and, by Proposition 2.10 P(0) < 0; hence, P(0) = 0; similarly, S* < Q implies 
that P(ft) = 1. 
(ii) If E = 0 or F = 0, or both, the result is trivially true. If E > 0 and F > 0, 
then, by Proposition 2.8, F U F > F; thus, if a = P(E) and /? = P(F, U F), we 
have a < 0 and, by Proposition 2.6, there exist events Si, S2 such that Si n S2 = 0, 
P^) = a and P{S2) = /?-a. By Proposition 2.7, F > S2orF ~ S2orF < S2. 
If F > S2, then, by Proposition 2.8, E U F > Si U S2 and hence P(F U F) > /?, 
which is impossible; similarly, if F < S2 then F U F < Si U S2 and P(F U F) < /? 
which, again, is impossible. Hence, F ~ S2 and therefore P{F) = 0 — a, so that 
P(E U F) = P(F) + P(F), as stated. 
(iii) By Proposition 2.5, E is significant iff 0 < F < ft. The result then 
follows immediately from Proposition 2.10. < 
Corollary. (Finitely additive structure of degrees of belief). 
(i) If{Ej,j G J} is a finite collection of disjoint events, then 
(ii) For any event E, P{EC) = 1 - P(F). 
Proo/. The first part follows by induction from Proposition 2.11 (iii); the  
second part is a special case of (i) since if UjEj = Q, then, by Proposition 2.1 l(i), 
XjPiEj) = 1. < 
Proposition 2.11 is crucial. It establishes formally that coherent, quantitative 
measures of uncertainty about events must take the form of probabilities, therefore 
justifying the nomenclature adopted in Definition 2.6 for this measure of degree of 
belief. In short, coherent degrees of belief are probabilities. 
It will often be convenient for us to use probability terminology, without 
explicit reference to the fact that the mathematical structure is merely serving as a 
representation of (personal) degrees of belief. The latter fact should, however, be 
constantly borne in mind. 
Definition 2.9. (Probability distribution). If{Ej,j 6 J} form a finite  
partition ofCl, with P(Ej) = pj,j e J, then {pj,j e J} is said to be a probability 
distribution over the partition. 
2.4 Beliefs and Probabilities 
37 
This terminology will prove useful in later discussions. The idea is that total 
belief (in Q, having measure 1) is distributed among the events of the partition, 
{Ej, j £ J}, according to the relative degrees of belief {pj,j 6 J}, with Ejp, = 
^pm = i. 
Starting from the qualitative ordering among events, we have derived a  
quantitative measure, P(.) = P(.\M0), over £ and shown that, expressed in conventional 
mathematical terminology, it has the form of^finitely additive probability measure, 
compatible with the qualitative ordering <. We now establish that this is the only 
probability measure over £ compatible with <. 
Proposition 2.12. {Uniqueness of the probability measure). P is the only 
probability measure compatible with the uncertainty relation <. 
Proof If P' were another compatible measure, then by Proposition 2.8 we 
would always have P'{E) < P'{F) «=► P(E) < P{F); hence, there exists 
a monotonic function / of [0,1] into itself such that P'(E) = f{P(E)}. By 
Proposition 2.6, for all non-negative a, 3 such that a + 3 < 1, there exist disjoint 
standard events Si and S2, such that P(S\) = a and P{S2) = 3. Hence, by 
Axiom 4(ii), f(a + 8) = P'{SX U S2) = P'(Si) + P'(S2) = f(a) + f(3) 
and so (Eichhorn, 1978, Theorem 2.63), /(a) = ka for all a in [0,1]. But, by 
Proposition 2.9, P'{Q) = 1 and hence, k = 1, so that we have P'(E) = P(E) for 
all E. < 
We shall now establish that our operational definition of (pairwise)  
independence of events is compatible with its more standard, ad hoc, product definition. 
Proposition 2.13. (Characterisation of independence). 
ELF «=> P{EC\F) = P{E)P{F). 
Proof. Suppose ELF. By Axiom 4(iii), there exists Si such that P(Si) = 
P{E), E L Sy and F L Si. Hence, by Axiom 4(v), £ ~f Si, so that, for any 
consequences C\ < c2, and any option a, 
{c2 | E n F, cx | Ec n F, a \ Fc} ~ {c2 | Sx n F, a | SI n F, a | Fc}. 
Taking a = C\, we have 
{c2|£nF,ci|(£nF)c}~{c2|SinF,ci|(SinF)c}, 
so that E n F ~ Si n F. Again by Axiom 4(iii), given F, Si, there exists S2 such 
that P(S2) = P{F), F J- S2 and Si ± S2. Hence, by an identical argument to the 
above, and noting from Definition 2.6 the symmetry of J., we have 
Si n F ~ Si n s2. 
38 
2 Foundations 
By Propositions 2.1, 2.10, and Axiom 4(iv), 
P(E n F) = Pfr n s2) = p(sl)P(s2), 
and hence P(F n F) = P{E)P{F). 
Suppose P(F n F) = P(E)P(F). By Axiom 4(iii), there exists S such that 
P(S) = P{F) and F ± S, F J_ S. Hence, by the first part of the proof, 
P{E n S) = P(E)P{S) = P(E)P{F) = P{E n F), 
so that E n F ~ E D S. Now suppose, without loss of generality, that c < 
{c2 | £, ci | Fc}. Then, by Definition 2.6, 
{c|S,Ci|Sc} < {c2 | Fn 5,Ci | {EDSf}. 
But {c | S, d | Sc} ~ {c | F, ci | Fc} and 
{C2|FnS,Cl|(FnS)c}~{C2|FnF,Cl|(£nF)c}; 
hence by Proposition 2.2, 
{c|F,Cl|Fc}<{c2|FnF)Cl|(FnF)c}, 
so that c <p {c2 | F, ci | Ec}. A similar argument can obviously be given reversing 
the roles of F and F, hence establishing that F 1 F. < 
2.4.2 Revision of Beliefs and Bayes' Theorem 
The assumed occurrence of a real-world event will typically modify preferences 
between options by modifying the degrees of belief attached, by an individual, to the 
events defining the options. In this section, we use the assumptions of Section 2.3 
in order to identify the precise way in which coherent modification of initial beliefs 
should proceed. 
The starting point for analysing order relations between events, given the  
assumed occurrence of a possible event G, is the uncertainty relation <c defined 
between events. Given the assumed occurrence of G > 0, the ordering < between 
acts is replaced by <g. Analogues of Propositions 2.1 and 2.2 are trivially  
established and we recall (Proposition 2.3) that, for any G > 0, c2 < C\ iff c2 <c c\. 
Proposition 2.14. (Properties of conditional beliefs). 
(i) E <G F «=> E n G < F n G. 
(ii) If there exist c\ < c2 such that {c21 E, ci | Ec} <g {c2 | F, C\ \ Fc}, 
then E <G F. 
2.4 Beliefs and Probabilities 
39 
Proof. By Definition 2.4 and Proposition 2.3, E <g F iff, for all c2> cx, 
{c2|F,Cl|Fc}<G{c2|F)Cl|Fc}, 
i.e., if, and only if, for all a, 
{c2 | E n G, c, | Fc n G, a | Gc} < {c2 | F n G, d | Fc n G, a | Gc}. 
Taking a = cx, 
E<GF <=^ {c2|JenG,c1|(FnG)c}<{c2|FnG,c1|(FnG)c}, 
and this is true iff E n G < F n G. 
Moreover, if there exist c2 > Cx such that {c2 | F, ci | Fc} <G {c2 | F, Cx | Fc} 
then, by Definition 2.4, with a = clt 
{czlFnG.dlF^G^^G^^jcalFnG^xlF^^dlG0}, 
so that 
{c2|FnG,c1|(FnG)c}<{c2|FnG,c1|(FnG)c} 
and the result follows from Axiom 3(ii) and part (i) of this proposition. < 
Definition 2.10. (Conditional measure of degree of belief). Given a  
conditional uncertainty relation <g,G > 0, the conditional probability P(E\G) 
of an event E given the assumed occurrence of G is the real number n(S) 
such that E ~g S. 
Generalising the idea encapsulated in Definition 2.7, P(E \ G) provides a 
quantitative operational measure of the uncertainty attached to E given the assumed 
occurrence of the event G. The following fundamental result provides the key to 
the process of revising beliefs in a coherent manner in the light of new information. 
It relates the conditional measure of degree of belief P (. | G) to the initial measure 
of degree of belief P (.). 
We have, of course, already stressed that all degrees of belief are conditional. 
The intention of the terminology used above is to emphasise the additional  
conditioning resulting from the occurrence of G; the initial state of information, 
M0, is always present as a conditioning factor, although omitted throughout for 
notational convenience. 
Proposition 2.15. (Conditionalprobability). For any G > 0, 
p(EnG) 
P(£|G)- P(G) ' 
Proof. By Axiom 4(iii) and Proposition 2.13, there exists S±G such that 
H(S) = P(EC\G)/P(G). By Proposition 2.13, 
P(S n G) = P(S)P(G) = n{S)P(G) = P{E n G). 
Thus, by Proposition 2.10, SnG~ Ef)G and, by Proposition 2.14, 5 ~G E. 
Thus, by Definition 2.10, P(E \ G) = n(S) = P(E n G)/P(G). < 
40 
2 Foundations 
Note that, in our formulation, P(E \ G) = P(E n G)/P{G) is a logical 
derivation from the axioms, not an ad hoc definition. In fact, this is the simplest 
version of Bayes' theorem. An extended form is given later in Proposition 2.19. 
Proposition 2.16. (Compatibility of conditional probability and conditional 
degrees of belief). 
E<GF <=> P(E\G)<P(F\G). 
Proof. By Proposition 2.14(i), E <G F iff E n G < F n G, which, by 
Proposition 2.10, holds if and only if P(E n G) < P(F n G); the result now 
follows from Proposition 2.15. < 
We now extend Proposition 2.11 to degrees of belief conditional on the  
occurrence of significant events. 
Proposition 2.17. (Probability structure of conditional degrees of belief). 
For any event G > 0, 
(i) P(0 | G) = 0 < P(E | G) < P(Q | G) = 1; 
(ii) ifE(lF(lG = <D, then P{E U F \ G) = P{E | G) + P{F \ G); 
(iii) E is significant given G <=> 0 < P(E \ G) < 1. 
Prao/. By Proposition 2.15, P(E\G) > 0 and P(0 | G) = 0; moreover, 
since E n G < G, Proposition 2.10 implies that P(£ n G) < P(G), so that, 
by Proposition 2.15, P(E \ G) < 1. Finally, Q n G = G, so that, using again 
Proposition 2.15 , P(ft | G) = 1. 
By Proposition 2.15, 
°(G) 
P(EnG) P(PnG) 
" p(g) + p(g) -n^l^ + ^^IG). 
Finally, by Proposition 2.5, E is significant given G iff 0 < £ D G < G. 
Thus, by Proposition 2.10, E is significant given G iff 0 < P(E C\G) < P{G). 
The result follows from Proposition 2.15. <| 
Corollary. (Finitely additive structure of conditional degrees of belief). 
ForallG><D, 
(i) if{Ej nG,j£ J} is a finite collection of disjoint events, then 
p(\JEj g) ^PC^IG); 
(ii) /or any event E, P(EC | G) = 1 - P(E | G). 
Proof. This parallels the proof of the Corollary to Proposition 2.11. <| 
2.4 Beliefs and Probabilities 
41 
Proposition 2.18. (Uniqueness of the conditional probability measure). 
P(. | G) is the only probability measure compatible with the conditional  
uncertainty relation <c- 
Proof. This parallels the proof of Proposition 2.12. < 
Example 2.1. (Simpson'sparadox). The following example provides an instructive 
illustration of the way in which the formalism of conditional probabilities provides a coherent 
resolution of an otherwise seemingly paradoxical situation. 
Suppose that the results of a clinical trial involving 800 sick patients are as shown in 
Table 2.1, where T,TC denote, respectively, that patients did or did not receive a certain 
treatment, and R, Rc denote, respectively, that the patients did or did not recover. 
Table 2.1 Trial results for all patients 
T 
rpc 
R 
200 
160 
Rc 
200 
240 
Total 
400 
400 
Recovery rate 
50% 
40% 
Intuitively, it seems clear that the treatment is beneficial, and were one to base  
probability judgements on these reported figures, it would seem reasonable to specify 
P(R | T) = 0.5, P(R | Tc) = 0.4, 
where recovery and the receipt of treatment by individuals are now represented, in an obvious 
notation, as events. Suppose now, however, that one became aware of the trial outcomes for 
male and female patients separately, and that these have the summary forms described in 
Tables 2.2 and 2.3. 
Table 2.2 Trial results for male patients 
T 
rpc 
R 
180 
70 
Rc 
120 
30 
Total 
300 
100 
Recovery rate 
60% 
70% 
The results surely seem paradoxical. Tables 2.2 and 2.3 tell us that the treatment is 
neither beneficial for males nor for females; but Table 2.1 tells us that overall it is beneficial! 
How are we to come to a coherent view in the light of this apparently conflicting evidence? 
42 
2 Foundations 
Table 2.3 Trial results for female patients 
R Rc Total Recovery rate 
T 20 80 100 20% 
Tc 90 210 300 30% 
The seeming paradox is easily resolved by an appeal to the logic of probability which, 
after all, we have just demonstrated to be the prerequisite for the coherent treatment of 
uncertainty. With M, Mc denoting, respectively, the events that a patient is either male or 
female, were one to base probability judgements on the figures reported in Tables 2.2 and 
2.3, it would seem reasonable to specify 
P(R | M n T) = 0.6, P{R | M 0 7*) = 0.7, 
P{R\MCC\T) =0.2, P(R| W nT) = 0.3. 
To see that these judgements do indeed cohere with those based on Table 2.1, we note, from 
the Corollary to Proposition 2.11, Proposition 2.15 and the Corollary to Proposition 2.17, 
that 
P{R | T) = P{R | M n T)P{M | T) + P{R | Mc 0 T)P{MC \ T) 
P{R | T) = P{R | M n T)P{M | T) + P{R | Mc n T)P{MC \ T), 
where 
P{M | T) = 0.75, P(M | Tc) = 0.25. 
The probability formalism reveals that the seeming paradox has arisen from the confounding 
of sex with treatment as a consequence of the unbalanced trial design. See Simpson (1951), 
Blyth (1972, 1973) and Lindley and Novick (1981) for further discussion. 
Proposition 2.19. (Bayes'theorem). 
For any finite partition {Ej,j 6 J} o/fi and G > 0, 
ZieJP(G\Ej)P(Ej) 
Proof. By Proposition 2.15, 
P{EiCiG) P{G\Ei)P{Ei) 
P(Ei | G) = 
P(G) P(G) 
The result now follows from the Corollary to Proposition 2.11 when applied to 
G=Uj(GnEj). <j 
2.4 Beliefs and Probabilities 
43 
Bayes' theorem is a simple mathematical consequence of the fact that  
quantitative coherence implies that degrees of belief should obey the rules of probability. 
From another point of view, it may also be established (Zellner, 1988b) that, under 
some reasonable desiderata, Bayes' theorem is an optimal information processing 
system. 
Since the {Ej, j G J} form a partition and hence, by the Corollary to  
Proposition 2.17, J2j P(Ej | G) = 1, Bayes' theorem may be written in the form 
P{Ej | G) oc P{G | Ej)P(Ej), j G J, 
since the missing proportionality constant is [F(G)]_1 = [EjP(G \ Ej)P(Ej)]~1, 
and thus it is always possible to normalise the products by dividing by their sum. 
This form of the theorem is often very useful in applications. 
Bayes' theorem acquires a particular significance in the case where the  
uncertain events {Ej, j G J} correspond to an exclusive and exhaustive set of hypotheses 
about some aspect of the world (for example, in a medical context, the set of possible 
diseases from which a patient may be suffering) and the event G corresponds to a 
relevant piece of evidence, or data (for example, the outcome of a clinical test). If we 
adopt the more suggestive notation, E; = Hj,j G J,G = D, and, as usual, we omit 
explicit notational reference to the initial state of information Mo, Proposition 2.17 
leads to Bayes' theorem in the form P(Hj | D) = P(D \ Hj)P{Hj)/P(D),j G J, 
where P(D) = T,jP(D | Hj)P(Hj), characterizing the way in which initial  
beliefs about the hypotheses, P(Hj), j G J, are modified by the data, D, into a 
revised set of beliefs, P(Hj \ D), j G J. This process is seen to depend crucially 
on the specification of the quantities P(D | Hj), j G J, which reflect how beliefs 
about obtaining the given data, D, vary over the different underlying hypotheses, 
thus defining the "relative likelihoods" of the latter. The four elements, P{Hj), 
P(D | Hj), P(Hj | D) and P(D), occur, in various guises, throughout Bayesian 
statistics and it is convenient to have a standard terminology available. 
Definition 2.11. (Prior, posterior, and predictive probabilities). 
If {Hj, j G J} are exclusive and exhaustive events (hypotheses), then for any 
event (data) D, 
(i) P(Hj), j G J, are called the prior probabilities of the Hj, j G J; 
(ii) P(D | Hj), j G J, are called the likelihoods of the Hj, j G J, given D; 
(iii) P(Hj | D), j G J, are called the posterior probabilities of the Hj, j G J; 
(iv) P(D) is called the predictive probability ofD implied by the likelihoods 
and the prior probabilities. 
It is important to realise that the terms "prior" and "posterior" only have 
significance given an initial state of information and relative to an additional piece of 
information. Thus, P(i?;), which could be moreproperly be written asP(Hj \ Mo), 
44 
2 Foundations 
represents beliefs prior to conditioning on data D, but posterior to conditioning 
on whatever history led to the state of information described by Mq. Similarly, 
P(Hj | D), or, more properly, P(Hj | Mo n D), represents beliefs posterior to 
conditioning on M0 and D, but prior to conditioning on any further data which 
may be obtained subsequent to D. 
The predictive probability P{D), logically implied by the likelihoods and the 
prior probabilities, provides a basis for assessing the compatibility of the data D 
with our beliefs (see Box, 1980). We shall consider this in more detail in Chapter 6. 
Example 2.2. (Medical diagnosis). In simple problems of medical diagnosis, Bayes' 
theorem often provides a particularly illuminating form of analysis of the various  
uncertainties involved. For simplicity, let us consider the situation where a patient may be characterised 
as belonging either to state Hu or to state H?, representing the presence or absence,  
respectively, of a specified disease. Let us further suppose that P{H\) represents the prevalence 
rate of the disease in the population to which the patient is assumed to belong, and that further 
information is available in the form of the result of a single clinical test, whose outcome is 
either positive (suggesting the presence of the disease and denoted by D = T), or negative 
(suggesting the absence of the disease and denoted by D = Tc). 
Figure 2.2 P{HX \ T) and P{HX \ Tc) as functions ofP{H{) 
The quantities P{T\H{) and P(TC \ H2) represent the true positive and true negative 
rates of the clinical test (often referred to as the test sensitivity and test specificity,  
respectively) and the systematic use of Bayes' theorem then enables us to understand the manner in 
which these characteristics of the test combine with the prevalence rate to produce varying 
degrees of diagnostic discriminatory power. In particular, for a given clinical test of known 
sensitivity and specificity, we can investigate the range of underlying prevalence rates for 
which the test has worthwhile diagnostic value. 
2.4 Beliefs and Probabilities 
45 
As an illustration of this process, let us consider the assessment of the diagnostic value 
of stress thallium-201 scintigraphy, a technique involving analysis of Gamma camera image 
data as an indicator of coronary heart disease. On the basis of a controlled experimental 
study, Murray et al. (1981) concluded that P(T | Hx) = 0.900, P(TC | H2) = 0.875 were 
reasonable orders of magnitude for the sensitivity and specificity of the test. 
Insight into the diagnostic value of the test can be obtained by plotting values of 
P{HX | T), P{HX | Tc) against P{HX), where 
p(h1\d)= pvwpm 
P(D | Hi)P(Ht) + P(D | H2)P(H2) 
for D = T or D = Tc, as shown in Figure 2.2. 
As a single, overall measure of the discriminatory power of the test, one may consider 
the difference P(HX | T) — P(HX \ Tc). In cases where P(HX) has very low or very high 
values (e.g. for large population screening or following individual patient referral on the basis 
of suspected coronary disease, respectively), there is limited diagnostic value in the test. 
However, in clinical situations where there is considerable uncertainty about the presence 
of coronary heart disease, for example, 0.25 < P{HX) < 0.75, the test may be expected to 
provide valuable diagnostic information. 
One further point about the terms prior and posterior is worth emphasising. 
They are not necessarily to be interpreted in a chronological sense, with the  
assumption that "prior" beliefs are specified first and then later modified into "posterior" 
beliefs. Propositions 2.15 and 2.17 do not involve any such chronological notions. 
They merely indicate that, for coherence, specifications of degrees of belief must 
satisfy the given relationships. Thus, for example, in Proposition 2.15 one might 
first specify P(G) and P(E \ G) and then use the relationship stated in the  
theorem to arrive at coherent specification of P(E n G). In any given situation, the 
particular order in which we specify degrees of belief and check their coherence is 
a pragmatic one; thus, some assessments seem straightforward and we feel  
comfortable in making them directly, while we are less sure about other assessments 
and need to approach them indirectly via the relationships implied by coherence. It 
is true that the natural order of assessment does coincide with the "chronological" 
order in a number of practical applications, but it is important to realise that this is 
a pragmatic issue and not a requirement of the theory. 
2.4.3 Conditional Independence 
An important special case of Proposition 2.15 arises when E and G are such 
that P(E | G) = P{E), so that beliefs about E are unchanged by the assumed 
occurrence of G. Not surprisingly, this is directly related to our earlier operational 
definition of (pairwise) independence. 
Proposition 2.20. For all F > 0, E±F <==> P{E | F) = P{E). 
46 
2 Foundations 
Proof. ELF <=> P(E n F) = P{E)P(F) and, by Proposition 2.15, we 
have P(£ n F) = P(£ | F)P(F). < 
In the case of three events, E, F and G, the situation is somewhat more 
complicated in that, from an intuitive point of view, we would regard our degree 
of belief for E as being "independent" of knowledge of F and G if and only if 
P(E | H) = P(E), for any of the four possible forms of H, 
{FnG, FnG, FnGc, Fnff}, 
describing the combined occurrences, or otherwise, of F and G (and, of course, 
similar conditions must hold for the "independence" of F from E and G, and of 
G from E and F). These considerations motivate the following formal definition, 
which generalises Definition 2.6 and can be shown (see e.g. Feller, 1950/1968, 
pp. 125-128) to be necessary and sufficient for encapsulating, in the genera] case, 
the intuitive conditions discussed above. 
Definition 2.12. (Mutual independence). 
Events {Ej ,j 6 J} are said to be mutually independent if, for any I C J, 
p(f]Ei]=Y[P(Ei). 
\iel ) iel 
An important consequence of the fact that coherent degrees of belief combine 
in conformity with the rules of (finitely additive) mathematical probability theory 
is that the task of specifying degrees of belief for complex combinations of events 
is often greatly simplified. Instead of being forced into a direct specification, we 
can attempt to represent the complex event in terms of simpler events, for which 
we feel more comfortable in specifying degrees of belief. The latter are then 
recombined, using the probability rules, to obtain the desired specification for the 
complex event. Definition 2.12 makes clear that the judgement of independence for 
a collection of events leads to considerable additional simplification when complex 
intersections of events are to be considered. Note that Proposition 2.20 derives from 
the uncertainty relation <p and therefore reflects an inherently personal judgement 
(although coherence may rule out some events from being judged independent: for 
example, any E, F such that 0 c E C F c Q). 
There is a sense, however, in which the judgement of independence (given 
Mo) for large classes of events of interest reflects a rather extreme form of belief, 
in that scope for learning from experience is very much reduced. This motivates 
consideration of the following weaker form of independence judgement. 
2.4 Beliefs and Probabilities 
47 
Definition 2.13. {Conditional independence). The events {Ej, j 6 J} are 
said to be conditionally independent given G > 0 if for any I C J, 
p(f]El\G]=l{P{Et\G). 
For any subalgebra T of£, the events {Ej,j G J} are said to be conditionally 
independent given J7 if and only if they are conditionally independent given 
anyG>% in T. 
Definitions 2.12 and 2.13 could, of course, have been stated in primitive terms 
of choices among options, as in Definition 2.6. However, having seen in detail 
the way in which the latter leads to the standard "product definition", it will be 
clear that a similar equivalence holds in these more general cases, but that the 
algebraic manipulations involved are somewhat more tedious. 
The form of degree of belief judgement encapsulated in Definition 2.13 is one 
which is utilised in some way or another in a wide variety of practical contexts 
and statements of scientific theories. Indeed, a detailed discussion of the kinds of 
circumstances in which it may be reasonable to structure beliefs on the basis of 
such judgements will be a main topic of Chapter 4. Thus, for example, in the  
practical context of sampling, with or without replacement, from large dichotomised 
populations (of voters, manufactured items, or whatever), successive outcomes 
(voting intention, marketable quality, ...) may very often be judged independent, 
given exact knowledge of the proportional split in the dichotomised population. 
Similarly, in simple Mendelian theory, the genotypes of successive offspring are 
typically judged to be independent events, given the knowledge of the two genotypes 
forming the mating. In the absence of such knowledge, however, in neither case 
would the judgement of independence for successive outcomes be intuitively  
plausible, since earlier outcomes provide information about the unknown population 
or mating composition and this, in turn, influences judgements about subsequent 
outcomes. For a detailed analysis of the concept of conditional independence, see 
Dawid (1979a, 1979b, 1980b). 
2.4.4 Sequential Revision of Beliefs 
Bayes' theorem characterises the way in which current beliefs about a set of  
mutually exclusive and exhaustive hypotheses, Hj, j 6 J, are revised in the light of 
new data, D. In practice, of course, we typically receive data in successive stages, 
so that the process of revising beliefs is sequential. 
As a simple illustration of this process, let us suppose that data are obtained 
in two stages, which can be described by real-world events Dt and D2. Omitting, 
48 
2 Foundations 
for convenience, explicit conditioning on Mo, revision of beliefs on the basis of the 
first piece of data A is described by P(Hj | A) = ^(A | Hj)P(Hj)/P{Di), 
j G J. When it comes to the further, subsequent revision of beliefs in the light of 
A> the likelihoods and prior probabilities to be used in Bayes' theorem are now 
P(A | Hj n A) and P(Hj \ A), j G J, respectively, since all judgements are 
now conditional on A. We thus have, for all j e J, 
P{Hj\D^D2)- pJn^T) ' 
where P(A | A) = E; P{Ph I Hj n A)P(flj | A). 
From an intuitive standpoint, we would obviously anticipate that coherent 
revision of initial belief in the light of the combined data, A n A» should not 
depend on whether A, A were analysed successively or in combination. This is 
easily verified by substituting the expression for P(Hj \ A) mto the expression for 
P{Hj | A H A)> whereupon we obtain 
P(A I Hj n A)P(A | Hj)P{Hj) P(A n A1 HfiPjHj) 
F(A|A)P(A) P(AnA) 
the latter being the direct expression for P(Hj \ A n A) from Bayes' theorem 
when A H A is treated as a single piece of data. 
The generalisation of this sequential revision process to any number of stages, 
corresponding to data, A, A, • • •, A, • ■ •, proceeds straightforwardly. If we 
write DW = A H A n • ■ • n A to denote all the data received up to and including 
stage k, then, for all j G J, 
P(H I n(*+ih _ P{Dk+x\Hj?\DW)P{Hj\DW) 
F[Hjl >~ P{DM\DW) ' 
which provides a recursive algorithm for the revision of beliefs. 
There is, however, a potential practical difficulty in implementing this process, 
since there is an implicit need to specify the successively conditioned likelihoods, 
P(A+i I Hj n D^), j G J, a task which, in the absence of simplifying  
assumptions, may appear to be impossibly complex if k is at all large. One possible 
form of simplifying assumption is the judgement of conditional independence for 
A, A, • • •, Dn, given any Hj, j G J, since, by Definition 2.13, we then only need 
the evaluations F(A+i I Hj n A*>) = F(A+i I Hj), j G J. Another possibility 
might be to assume a rather weak form of dependence by making the judgement 
that a (Markov) property such as P(A+i I Hj n A*>) = P{A+i I Hj n Dk), 
j € J, holds for all k. As we shall see later, these kinds of simplifying structural 
assumptions play a fundamental role in statistical modelling and analysis. 
2.5 Actions and Utilities 
49 
In the case of two hypotheses, H\,H2, the judgement of conditional  
independence for Di, D2, ■ ■ ■, Dn,..., given Ht or H2, enables us to provide an alternative 
description of the process of revising beliefs by noting that, in this case, 
P(m | D^) = PjH, I £)(*>) P(Dk+l | Hj) 
P(H2 | D^+D) P{H2 | £>(*>) P(Dk+1 | H2) ' 
With due regard to the relative nature of the terms prior and posterior, we can 
thus summarise the learning process (in "favour" of H\) as follows: 
posterior odds = prior odds x likelihood ratio. 
In Section 2.6, we shall examine in more detail the key role played by the 
sequential revision of beliefs in the context of complex, sequential decision  
problems. 
2.5 ACTIONS AND UTILITIES 
2.5.1 Bounded Sets of Consequences 
At the beginning of Section 2.4, we argued that choices among options are governed, 
in part, by the relative degrees of belief that an individual attaches to the uncertain 
events involved in the options. It is equally clear that choices among options should 
depend on the relative values that an individual attaches to the consequences flowing 
from the events. The measurement framework of Axiom 5(i) provides us with a 
direct, intuitive way of introducing a numerical measure of value for consequences, 
in such a way that the latter has a coherent, operational basis. Before we do this, 
we need to consider a little more closely the nature of the set of consequences C. 
The following special case provides a useful starting point for our development of 
a measure of value for consequences. 
Definition 2.14. (Extreme consequences). The pair of consequences c* and 
c* are called, respectively, the worst and the best consequences in a decision 
problem if, for any other consequence c G C, ct < c < c*. 
It could be argued that all real decision problems actually have extreme  
consequences. Indeed, we recall that all consequences are to be thought of as relevant 
consequences in the context of the decision problem. This eliminates pathological, 
mathematically motivated choices of C, which could be constructed in such a way 
as to rule out the existence of extreme consequences. For example, in mathematical 
modelling of decision problems involving monetary consequences, C is often taken 
to be the real line 3? or, in a no-loss situation with current assets k, to be the interval 
[k, oo). Such Cs would not contain both a best and a worst consequence but, on the 
50 
2 Foundations 
other hand, they clearly do not correspond to concrete, practical problems. In the 
next section, we shall consider the solution to decision problems for which extreme 
consequences are assumed to exist. 
Nevertheless, despite the force of the pragmatic argument that extreme  
consequences always exist, it must be admitted that insisting upon problem formulations 
which satisfy the assumption of the existence of extreme consequences can  
sometimes lead to rather tedious complications of a conceptual or mathematical nature. 
Consider, for example, a medical decision problem for which the consequences 
take the form of different numbers of years of remaining life for a patient. Assuming 
that more value is attached to longer survival, it would appear rather difficult to 
justify any particular choice of realistic upper bound, even though we believe there 
to be one. To choose a particular c* would be tantamount to putting forward c* 
years as a realistic possible survival time, but regarding c* + 1 years as impossible! 
In such cases, it is attractive to have available the possibility, for conceptual and 
mathematical convenience, of dealing with sets of consequences not possessing 
extreme elements (and the same is true of many problems involving monetary 
consequences). For this reason, we shall also deal (in Section 2.5.3) with the 
situation in which extreme consequences are not assumed to exist. 
2.5.2 Bounded Decision Problems 
Let us consider a decision problem (£, C, A, <) for which extreme consequences 
c* < c* are assumed to exist. We shall refer to such decision problems as bounded. 
Definition 2.15. (Canonical utility function for consequences). Given a 
preference relation <, the utility u(c) = u(c\ c*,c*) of a consequence c, 
relative to the extreme consequences c« < c*, is the real number n{S)  
associated with any standard event S such that c ~ {c* \ S, c* \ Sc}. The mapping 
u : C —»3? is called the utility function. 
It is important to note that the definition of utility only involves comparison 
among consequences and options constructed with standard events. Since the 
preference patterns among consequences is unaffected by additional information, 
we would expect the utility of a consequence to be uniquely defined and to remain 
unchanged as new information is obtained. This is indeed the case. 
Proposition 2.21. (Existence and uniqueness of bounded utilities). For any 
bounded decision problem (£,C,A,<) with extreme consequences c* < c*, 
(i) for all c, u(c | c„, c*) exists and is unique; 
(ii) the value ofu(c \ct,c*) is unaffected by the assumed occurrence of an 
event G > 0; 
(iii) 0 = u(ct \ct,c*) < u{c\ct,c*) < u(c* |c*,c*) = 1. 
2.5 Actions and Utilities 
51 
Proof, (i) Existence follows immediately from Axiom 5(i). For uniqueness, 
note that if c ~ {c* \ S\,c, | Sf} and c ~ {c* \ S2,c* | S2} then, by transitivity 
and Axiom 3(ii), {c* | Si, c* | SJ} ~ {c* \ S2, c, | S2} and Si ~ S2; the result now 
follows from Axiom 4(i). 
(ii) To establish this, let c ~ {c* | Si, c* | SJ}, so that u(c | c*, c*) = /x(Si); 
using Axiom4(iii), for any G > 0 choose S2 such that G _L S2 and /x(S2) = n(S\). 
Then, by Definition 2.6, c ~g {c* | S2, c* | S2} and so the utility of c given G is 
just the original value /x(S2). 
(iii) Finally, since c* = {c* | 0, c* | ft}, c* = {c* | fl, c* | 0}, and both 0 and fi 
belong to the algebra of standard events, we have u(c„ | c*,c*) = /x(0) = 0 and 
u(c* | c*,c*) = /x(fi) = 1. It then follows, from Definition 2.15 and Axiom 4(i), 
that 0 < u(c I ct,c*) < 1. <, 
It is interesting to note that u(c \ c*,c*), which we shall often simply denote 
by u(c), can be given an operational interpretation in terms of degrees of belief. 
Indeed, if we consider a choice between the fixed consequence c and the option 
{c* I E, c* I Ec}, for some event E, then the utility of c can be thought of as defining 
a threshold value for the degree of belief in E, in the sense that values greater than 
u would lead an individual to prefer the uncertain option, whereas values less than 
u would lead the individual to prefer c for certain. The value u itself corresponds to 
indifference between the two options and is the degree of belief in the occurrence 
of the best, rather than worst, consequence. 
This suggests one possible technique for the experimental elicitation of utilities, a 
subject which has generated a large literature (with contributions from economists 
and psychologists, as well as from statisticians). We shall illustrate the ideas in 
Example 2.3. 
Using the coherence and quantification principles set out in Section 2.3, we 
have seen how numerical measures can be assigned to two of the elements of 
a decision problem in the form of degrees of belief for events and utilities for 
consequences. It remains now to investigate how an overall numerical measure of 
value can be attached to an option, whose form depends both on the events of a 
finite partition of the certain event Q, and on the particular consequences to which 
these events lead. 
Definition 2.16. (Conditional expected utility). 
For any c, < c*,G > 0, and a = {cj \ Ej,j 6 J}, 
u{a I c*, c*,G) = ^u{cj I c*,c*)P(Ej \ G) 
is the expected utility of the option a, given G, with respect to the extreme 
consequences c», c*. If G = Q, we shall simply write u(a \ c„, c*) in place of 
u(a\c*,c*,Q). 
52 
2 Foundations 
In the language of mathematical probability theory (see Chapter 3), if the utility 
value of a is considered as a "random quantity", contingent on the occurrence of 
a particular event Ej, then u is simply the expected value of that utility when the 
probabilities of the events are considered conditional on G. 
Proposition 2.22. (Decision criterion for a bounded decision problem). 
For any bounded decision with extreme consequences c* < c*, and G > 0, 
«i <G «2 <*=^ u(a\ |c*,c*,G) < u(a2 |c»,c*,G). 
Proof. Let a, = {ctj \ E(j,j = 1,...,n*}, i — 1,2. By Axioms 5(ii), 4(iii), 
and Proposition 2.13, for all (i,j) there exist Sy and Sy such that 
*, ~ {c* | S£,c | 5%}, Sy±(Ey n G), P(Sy) = P(Sy). 
Hence, by Proposition 2.10, cy ~ {c* |Sy,c*|Sy} with Sy_L(£y D G) and 
P(Sjj) = u(cij | c*, c*). By Definition 2.6, for i = 1, 2 and any option a, 
{[cy I Eij n G], j = 1,..., m, a\Gc} 
~ {[(c* | Sa, c* | 5y) I Eij n G], j = 1,..., m, a \ Gc}, 
which may be written as {c* \ Ai,c* \Bi,a\ Gc}, where At = Uj(£"y n G n 5y) 
and Bj = Uj(£y D G D 5y). By Propositions 2.14(H) and 2.16, and using 
Definition 2.5, ax <G a2 => A\ <G A2 => P{Ai \ G) < P(A2 \ G). But, by 
Proposition2.15, P(JS^nGnSy) = P(^ynG)P(5y) = P(5y)P(^y | G)P{G). 
Hence, 
ni 
P{Ai | G) = $3u(Cy I c*,c*)P{Eij \ G) = u{ai | c„c*. G) 
and so oi <g «2 <=> «(«i I c„, c*, G) < «(a2 | c, c*, G). < 
The result just established is sometimes referred to as the principle of  
maximising expected utility. In our development, this is clearly not an independent 
"principle", but rather an implication of our assumptions and definitions. In  
summary form, the resulting prescription for quantitative, coherent decision-making is: 
choose the option with the greatest expected utility. 
Technically, of course, Proposition 2.22 merely establishes, for each <c, a 
complete ordering of the options considered and does not guarantee the existence 
of an optimal option for which the expected utility is a maximum. However, in 
most (if not all) concrete, practical problems the set of options considered will be 
finite and so a best option (not necessarily unique) will exist. In more abstract 
mathematical formulations, the existence of a maximum will depend on analytic 
features of the set of options and on the utility function u : C —» di. 
2.5 Actions and Utilities 
53 
Example 2.3. (Utilities of oil wildcatters). One of the earliest reported systematic 
attempts at the quantification of utilities in a practical decision-making context was that of 
Grayson (1960), whose decision-makers were oil wildcatters engaged in exploratory searches 
for oil and gas. The consequences of drilling decisions and their outcomes are ultimately 
changes in the wildcatters' monetary assets, and Grayson's work focuses on the assessment 
of utility functions for this latter quantity. 
Forthe purposes of illustration, suppose that we restrict attention to changes in monetary 
assets ranging, in units of one thousand dollars, from —150 (the worst consequence) to +825 
(the best consequence). Assuming u(—150) = 0, u(825) = 1, the above development 
suggests ways in which we might try to elicit an individual wildcatter's values of u(c) for 
various c in the range —150 < c < 825. For example, one could ask the wildcatter, using a 
series of values of c, which option he or she would prefer out of the following: 
(i) c for sure, 
(ii) entry into a venture having outcome 825 with probability p and an outcome —150 with 
probability 1 — p, for some specified p. 
If Cy, emerges from such interrogation as an approximate "indifference" value, the theory 
developed above suggests that, for a coherent individual, 
u{cv) =pu(825) + (l-p)u(-150) =p. 
Repeating this exercise for a range of values of p, provides a series of {cp,p) pairs, from 
which a "picture" of u(c) overthe range of interest can be obtained. An alternative procedure, 
of course, would be to fix c, perform an interrogation for various p until an "indifference" 
value, pc is found, and then repeat this procedure for a range of values of c to obtain a series 
of (c,pc) pairs. 
Utility 
-200 
200 
400 
Thousands of dollars 
600 800 
Figure 2.3 William Beard's utility function for changes in monetary assets 
54 
2 Foundations 
Figure 2.3 shows the results obtained by Grayson using procedures of this kind to 
interrogate oil company executive, W. Beard, on October 23, 1957. A "picture" of Beard's 
utility function clearly emerges from the empirical data. In particular, over the range  
concerned, the utility function reflects considerable risk aversion, in the sense that even quite 
small asset losses lead to large (negative) changes in utility compared with the (positive) 
changes associated with asset gains. 
Since the expected utility u is a linear combination of values of the utility 
function, Proposition 2.22 guarantees that preferences among options are invariant 
under changes in the origin and scale of the utility measure used; i.e., invariant 
with respect to transformations of the form Au(.) + B, provided we take A > 0 , 
so that the orientation of "best" and "worst" is not changed. In general, therefore, 
such an origin and scale can be chosen for convenience in any given problem, 
and we can simply refer to the expected utility of an option without needing to 
specify the (positive linear) transformation of the utility function which has been 
used. However, there may be bounded decision problems where the probabilistic 
interpretation discussed above makes it desirable to work in terms of canonical 
utilities, derived by referring to the best and worst consequences. 
In the next section, we shall provide an extension of these ideas to more general 
decision problems where extreme consequences are not assumed to exist. 
2.5.3 General Decision Problems 
We begin with a more general definition of the utility of a consequence which 
preserves the linear combination structure and the invariance discussed above. 
Definition 2.17. (General utility function). Given a preference relation <, 
the utility u(c | C\, c2) of a consequence c, relative to the consequences c\ < C2, 
is defined to be the real number u such that 
ifc < c\ and c\ ~ {c2 | Sx, c \ Sx}, then u = — x/(l — x); 
ifc\ <c<c2 and c ~ {c2 | Sx, c\ | Sx}, then u = x; 
ifc > c2 and C2 ~ {c | Sx, C\ \ Sx}, then u = 1/x 
where x = n(Sx) is the measure associated with the standard event Sx. 
Our restricted definition of utility (Definition 2.15) relied on the existence 
of extreme consequences c„, c*, such that ct < c < c* for all c € C. In the 
absence of this assumption, we have to select some reference consequences, c\, c2 
to play the role of c*,c*. However, we cannot then assume that C\ < c < c2 for 
all c, and this means that if c\, c2 are to define a utility scale by being assigned 
values 0,1, respectively, we shall require negative assignments for c < cx and 
assignments greater than one for c > c2. The definition is motivated by a desire 
to maintain the linear features of the utility function obtained in the case where 
2.5 Actions and Utilities 
55 
extreme consequences exist. It can be checked straightforwardly that if c^), q2), 
C(3) denote any permutation of c, C\,c2, where c\ < c2 and C(i) < C(2) < C(3), the 
definition given ensures that for any G > 0, C(2) ~g {c(3) | Si. c^) | S£} implies 
that 
«(C(2) I Cl, C2) = I«(C(3) | Ci,C2) + (1 - X) U(C(1) | Ci, C2). 
The following result extends Proposition 2.21 to the general utility function 
defined above. 
Proposition 2.23. (Existence and uniqueness of utilities). For any decision 
problem, and for any pair of consequences C\ < c% 
(i) for all c, u(c \ C\, c2) exists and is unique; 
(ii) the value ofu(c\ci,c2) is unaffected by the occurrence of an event G > 0; 
(iii) u(c\ |ci,c2) = 0andu(c2 |ci,c2) = 1. 
Proof. This is virtually identical to the proof of Proposition 2.21. <, 
The following results guarantee that the utilities of consequences are linearly 
transformed if the pair of consequences chosen as a reference is changed. 
Proposition 2.24. (Linearity). For all c\ < c2 and c$ < c^ there exist A > 0 
and B such that, for all c, u(c \ c\, c2) = Au(c | c3, C4) + B. 
Proof. Suppose first that C3 > c\, C4 < c2, and c\ < c < c2. By  
Axiom 5(ii), C3 < c < Ci implies that there exists a standard event Sx such that 
c ~ {c41 Sx, c3 I 5^}. Hence, by Proposition 2.22, 
u(c|ci,c2) = xu(c4 |ci,c2) + (1 -x)w(c3|ci,c2), 
where x = P(SX) and, by Definition 2.17, u(c \ C3,Ci) = x. Hence, u(c\ ci,c2) = 
Au(c IC3, C4) + B, where A = u{ci \ c\, c2) - u(c3 | ci, c2) and B — u{c^ | ci, c2). 
By Axiom 5(ii), if C3 > c there exists 5y such that c3 ~ {C41 Sy, c \ Sy}. 
Hence, by Proposition 2.22, 
u{c3\ci,c2) = yu(ci\ci,c2) + (l -y)u(c\cuc2), 
where y = P{Sy) and, by Definition 2.17, u(c| c3,c4) = -y/{l - y). Hence, 
y(c I ci, c2) = Au(c I c3, c4) + B, with A and B as above. Similarly, if c > c4 there 
exists Sz such that c4 ~ {c | 52, c3 | Scz} and 
«(c4|ci,c2) = 2/u(c|cl5c2) + (1 -2/)u(c3|ci,c2), 
where y = P(Sy) and, by Definition 2.17, u(c\ 03,04) = 1/y. Hence, we have 
u(c I Ci,c2) = Au(c I c3, C4) + B, with A and B as above. 
Now suppose that the c's have arbitrary order, subject to c2 > c\, C4 > c3. 
Let c*, c* be the minimum and maximum, respectively, of {ci, c2, C3, C4, c}. Then, 
by the above, there exist A\, B\, A2, B2 such that, for c^ € {c\, c2, c3,C4, c}, 
u(c(j) |c*,c*) = Aiu(c(i) |ci,c2) + Bi andu(c(i) | c*,c*) = A2u(c(i) | c3,c4) + S2; 
hence, u{c{i) \ cu c2) = (A2/Ai)u(c(i) | c3, c4) + (B2 - Bi)/Ai. < 
56 
2 Foundations 
Finally, we generalise Proposition 2.22 to unbounded decision problems; 
Proposition 2.25. (General decision criterion). 
For any decision problem, pair of consequences ci < c2, and event G > 0, 
«i <G «2 <*=> u{a\\c\,c2,G) <u{a2\c\,c2,G). 
Proof. Suppose at — {cy|2?jj,j = l,...,rii},i = 1,2, andletc,,c* be 
such that for all Cy, c* < Cy- < c*. Then, by Proposition 2.22, a2 <g «i iff 
u(a2 \c*,c*,G) < u(a\ | c*, c*, G). But, by Proposition 2.24, there exists A > 0 
and B such that u(c | c„, c*) = Au(c | c\,c2) + B, and so the result follows. < 
An immediate implication of Proposition 2.25 is that all options can be  
compared among themselves. We recall that we did not directly assume that  
comparisons could be made between all pair of options (an assumption which is often 
criticised as unjustified; see, for example, Fine 1973, p. 221). Instead, we merely 
assumed that all consequences could be compared among themselves and with the 
(very simply structured) standard dichotomised options, and that the latter could 
be compared among themselves. 
This completes our elaboration of the axiom system set out in Section 2.3. 
Starting from the primitive notion of preference, <, we have shown that quantitative, 
coherent comparisons of options must proceed as if a utility function has been 
assigned to consequences, probabilities to events and the choice of an option made 
on the basis of maximising expected utility. 
If we begin by defining a utility function over u : C —»9ft, this induces in turn 
a preference ordering which is necessarily coherent. Any function can serve as a 
utility function (subject only to the existence of the expected utility for each option, 
a problem which does not arise in the case of finite partitions) and the choice is a 
personal one. In some contexts, however, there are further formal considerations 
which may delimit the form of function chosen. An important special case is 
discussed in detail in Section 2.7. 
2.6 SEQUENTIAL DECISION PROBLEMS 
2.6.1 Complex Decision Problems 
Many real decision problems would appear to have a more complex structure than 
that encapsulated in Definition 2.1. For instance, in the fields of market research 
and production engineering investigators often consider first whether or not to 
run a pilot study and only then, in the light of information obtained (or on the 
basis of initial information if the study is not undertaken), are the major options 
considered. Such a two-stage process provides a simple example of a sequential 
2.6 Sequential Decision Problems 
57 
decision problem, involving successive, interdependent decisions. In this section, 
we shall demonstrate that complex problems of this kind can be solved with the 
tools already at our disposal, thus substantiating our claim that the principles of 
quantitative coherence suffice to provide a prescriptive solution to any decision 
problem. 
Before explicitly considering sequential problems, we shall review, using a 
more detailed notation, some of our earlier developments. 
Let A = {at, i G /} be the set of alternative actions we are willing to consider. 
For each at, there is a class {Eij,j G Jj} of exhaustive and mutually exclusive 
events, which label the possible consequences {ci3■., j G J,} which may result from 
action a,. Note that, with this notation, we are merely emphasising the obvious 
dependence of both the consequences and the events on the action from which they 
result. If Mq is our initial state of information and G > 0 is additional information 
obtained subsequently, the main result of the previous section (Proposition 2.25) 
may be restated as follows. 
For behaviour consistent with the principles of quantitative coherence, action 
a\ is to be preferred to action a2, given M0 and G, if and only if 
u{a\ | G) > u(a2 | G), 
where 
u{(H | G) = Y^ u{cij)P{Eij | (H, M0, G), 
jeJt 
u(Cij) is the value attached to the consequence foreseen if action a, is taken and the 
event E,j occurs, and P(E(j | a*, Mo, G) is the degree of belief in the occurrence of 
event E(j, conditional on action a, having been taken, and the state of information 
being (M0, G). 
We recall that the probability measure used to compute the expected utility is taken 
to be a representation of the decision-maker's degree of belief conditional on the 
total information available. By using the extended notation P(Eij \ ai, G, M0), 
rather than the more economical P(Ej \ G) used previously, we are emphasising 
that (i) the actual events considered may depend on the particular action envisaged, 
(ii) the information available certainly includes the initial information together 
with G > 0, and (iii) degrees of belief in the occurrence of events such as Etj are 
understood to be conditional on action at having been assumed to be taken, so 
that the possible influence of the decision-maker on the real world is taken into 
account. 
For any action a», it is sometimes convenient to describe the relevant events 
E(j, j G J, in a sequential form. For example, in considering the relevant events 
which label the consequences of a surgical intervention for cancer, one may first 
58 
2 Foundations 
think of whether the patient will survive the operation and then, conditional on 
survival, whether or not the tumour will eventually reappear were this particular 
form of surgery to be performed. 
These situations are most easily described diagrammatically using decision 
trees, such as that shown in Figure 2.4, with as many successive random nodes 
as necessary. Obviously, this does not represent any formal departure from our 
previous structure, since the problem can be restated with a single random node 
where relevant events are defined in terms of appropriate intersections, such as 
Eij D Fijk in die example shown. It is also usually the case, in practice, that it 
is easier to elicit the relevant degrees of belief conditionally, so that, for example, 
P(Eij D Fijk I a>i, G, M0) would often be best assessed by combining the separately 
assessed terms P(Fyfc | Etj,Oi, G, M0) and P(E(j | ait G, M0). 
Figure 2.4 Conditional description of relevant events 
Conditional analysis of this kind is usually necessary in order to understand the 
structure of complicated situations. Consider, for instance, the problem of placing 
a bet on the result of a race after which the total amount bet is to be divided up 
among those correctly guessing the winner. Clearly, if we bet on the favourite we 
have a higher probability of winning; but, if the favourite wins, many people will 
have guessed correctly and the prize will be small. It may appear at first sight that 
this is a decision problem where the utilities involved in an action (the possible 
prizes to be obtained from a bet) depend on the probabilities of the corresponding 
uncertain events (the possible winning horses), a possibility not contemplated 
in our structure. A closer analysis reveals, however, that the structure of the 
problem is similar to that of Figure 2.4. The prize received depends on the bet 
you place (a;) the related betting behaviour of other people (E^) and the outcome 
of the race (Fijk). It is only natural to assume that our degree of belief in the 
possible outcomes of the race may be influenced by the betting behaviour of other 
people. This conditional analysis straightforwardly resolves the initial, apparent 
complication. 
We now turn to considering sequences of decision problems. We shall consider 
situations where, after an action has been taken and its consequences observed, a 
2.6 Sequential Decision Problems 
59 
new decision problem arises, conditional on the new circumstances. For  
example, when the consequences of a given medical treatment have been observed, a 
physician has to decide whether to continue the same treatment, or to change to an 
alternative treatment, or to declare the patient cured. 
If a decision problem involves a succession of decision nodes, it is intuitively 
obvious that the optimal choice at the first decision node depends on the optimal 
choices at the subsequent decision nodes. In colloquial terms, we typically cannot 
decide what to do today without thinking first of what we might do tomorrow, 
and that, of course, will typically depend on the possible consequences of today's 
actions. In the next section, we consider a technique, backward induction, which 
makes it possible to solve these problems within the framework we have already 
established. 
2.6.2 Backward Induction 
In any actual decision problem, the number of scenarios which may be contemplated 
at any given time is necessarily finite. Consequently, and bearing in mind that the 
analysis is only strictly valid under certain fixed general assumptions and we cannot 
seriously expect these to remain valid for an indefinitely long period, the number of 
decision nodes to be considered in any given sequential problem will be assumed to 
be finite. Thus, we should be able to define a finite horizon, after which no further 
decisions are envisaged in the particular problem formulation. If, at each node, the 
possibilities are finite in number, the situation may be diagrammatically described 
by means of a decision tree like that of Figure 2.5. 
Ejk, j G 4' 
Figure 2.5 Decision tree with several decision nodes 
Let n be the number of decision stages considered and let a(m' denote an 
action being considered at the mth stage. Using the notation for composite options 
60 
2 Foundations 
introduced in Section 2.2, all first-stage actions may be compactly described in the 
form 
where {Eij,j £ J\ } is the partition of relevant events which corresponds to 
a\ ' and the notation "max ak refers to the most preferred of the set of options 
{af\k eKij} which we would be confronted with were the event E^ to occur. 
The "maximisation" is naturally to be understood in the sense of our conditional 
preference ordering among the available second-stage options, given the occurrence 
of E^i. Indeed, the "consequence" of choosing a\ and having Etj occur is that we 
are confronted with a set of options {ak ,k £ Kij} from which we can choose that 
option which is preferred on the basis of our pattern of preferences at that stage. 
Similarly, second-stage options may be written in terms of third-stage options, and 
the process continued until we reach the nth stage, consisting of "ordinary" options 
defined in terms of the events and consequences to which they may lead. Formally, 
we have 
r(m) 
af} = { max a[m+1) | Ey, j G J\m> }, m = 1, 2,... ,n - 1, 
af) = {Cl3|%iej|"»}. 
It is now apparent that sequential decision problems are a special case of the general 
framework which we have developed. 
It follows from Proposition 2.25 that, at each stage m, if Gm is the relevant 
information available, and u(.) is the (generalised) utility function, we may write 
(m) , (m) 
ai <Gm a) 
u{a(r)\Gm}<u{afl)\Gm} 
where 
u{atn)\Gm}= £ maxu{4m+1)|Gm+1} 
u {a<n) | Gn] = Y, <*i)P{Ei} I Gn). 
P{Ejj | Gm), 
This means that one has to first solve the final (nth) stage, by maximising the 
appropriate expected utility; then one has to solve the (n—1 )th stage by maximizing 
2.6 Sequential Decision Problems 
61 
the expected utility conditional on making the optimal choice at the nth stage; and 
so on, working backwards progressively, until the optimal first stage option has 
been obtained, a procedure often referred to as dynamic programming. 
This process of backward induction satisfies the requirement that, at any stage 
of the procedure, the mth, say, the continuation of the procedure must be identical 
to the optimal procedure starting at the mth stage with information Gm. This  
requirement is usually known as Bellman's optimalityprinciple (Bellman, 1957). As 
with the "principle" of maximising expected utility, we see that this is not required 
as a further assumed "principle" in our formulation, but is simply a consequence 
of the principles of quantitative coherence. 
Example 2.4. (An optimal stopping problem). We now consider a famous problem, 
which is usually referred to in the literature as the "marriage problem" or the "secretary 
problem". Suppose that a specified number of objects n > 2 are to be inspected sequentially, 
one at a time, in order to select one of them. Suppose further that, at any stage r, 1 < r < n, 
the inspector has the option of either stopping the inspection process, receiving, as a result, 
the object currently under inspection, or of continuing the inspection process with the next 
object. No backtracking is permitted and if the inspection process has not terminated before 
the nth stage the outcome is that the nth object is received. At each stage, r, the only 
information available to the inspector is the relative rank (l=best, r=worst) of the current 
object among those inspected so far, and the knowledge that the n objects are being presented 
in a completely random order. 
When should the inspection process be terminated? Intuitively, if the inspector stops 
too soon there is a good chance that objects more preferred to those seen so far will remain 
uninspected. However, if the inspection process goes on too long there is a good chance that 
the overall preferred object will already have been encountered and passed over. 
This kind of dilemma is inherent in a variety of practical problems, such as property 
purchase in a limited seller's market when a bid is required immediately after inspection, 
or staff appointment in a skill shortage area when a job offer is required immediately after 
interview. More exotically—and assuming a rather egocentric inspection process, again 
with no backtracking possibilities—this stopping problem has been suggested as a model 
for choosing a mate. Potential partners are encountered sequentially; the proverb "marry in 
haste, repent at leisure" warns against settling down too soon; but such hesitations have to 
be balanced against painful future realisations of missed golden opportunities. 
Less romantically, let c*, i = 1,..., n, denote the possible consequences of the  
inspection process, with c* = i if the eventual object chosen has rank i out of all n objects. We 
shall denote by m(a) = u(i), i = 1,..., n, the inspector's utility for these consequences. 
Now suppose that r < n objects have been inspected and that the relative rank among 
these of the object under current inspection is x, where 1 < x < r. There are two actions 
available at the rth stage: ai = stop, a2 = continue (where, to simplify notation, we have 
dropped the superscript, r). The information available at the rth stage is Gr = (x,r); 
the information available at the (r + l)th stage would be Gr+1 = (y,r + 1), where y, 
1 < y < r + 1, is the rank of the next object relative to the r + 1 then inspected, all values 
of y being, of course, equally likely since the n objects are inspected in a random order. If 
we denote the expected utility of stopping, given Gr, by us(x, r) and the expected utility 
62 
2 Foundations 
of acting optimally, given Gr, by uo(x, r), the general development given above establishes 
that 
f 1 r+1 1 
Tio(x,r) = max. I u„(x,r), 7^«o(y.»,+ 1) > - 
y=l 
where 
us(x,r) = ]T] u(z) 
z — 1\ In — z 
x - 1/ \r - x 
IT 
u0(x, n) = ua(x, n) = u(x), x = 1, ...,n. 
Values of Uo(x, r) can be found from the final condition and the technique of backwards 
induction. The optimal procedure is then seen to be: 
(i) continue if uo(x, r) > us(x, r), 
(ii) stop if Tio(x, r) = ua(x, r). 
For illustration, suppose that the inspector's preference ordering corresponds to a  
"nothing but the best" utility function, defined by w(l) = 1, u(x) = 0, x = 2 n. It is then 
easy to show that 
us(l,r) = - , 
n 
thus, if x > 1, 
u„(x, r) = 0, x = 2,...,n; 
Uo(x, r) > us(x,r), r = 1,..., n — 1. 
This implies that inspection should never be terminated if the current object is not the best 
seen so far. The decision as to whether to stop if x = 1 is determined from the equation 
Uo(x, r) = max-l -, -( -\ H-H< 
[n n\n-l r) ) 
which is easily verified by induction. If r* is the smallest positive integer for which 
11 1 , 
+ - + ■■■ + — < 1, 
ra—1 n — 2 r* 
the optimal procedure is defined as follows: 
(i) continue until at least r* objects have been inspected: 
(ii) if the r*th object is the best so far, stop; 
(iii) otherwise, continue until the object under inspection is the best so far, then stop  
(stopping in any case if the nth stage is reached). 
If n is large, approximation of the sum in the above inequality by an integral readily 
yields the approximation r* « n/e. For further details, see DeGroot (1970, Chapter 13), 
whose account is based closely on Lindley (1961 a). For reviews of further, related work on 
this fascinating problem, see Freeman (1983) and Ferguson (1989). 
2.6 Sequential Decision Problems 
63 
Applied to the problem of "choosing a mate", and assuming that potential partners are 
encountered uniformly over time between the ages of 16 and 60, the above analysis suggests 
delaying a choice until one is at least 32 years old, thereafter ending the search as soon as one 
encounters someone better than anyone encountered thus far. Readers who are suspicious 
of putting this into practice have the option, of course, of staying at home and continuing 
their study of this volume. 
Sequential decision problems are now further illustrated by considering the 
important special case of situations involving an initial choice of experimental 
design. 
2.6.3 Design of Experiments 
A simple, very important example of a sequential problem is provided by the 
situation where we have available a class of experiments, one of which is to be 
performed in order to provide information for use in a subsequent decision problem. 
We want to choose the "best" experiment. The structure of this problem, which 
embraces the topic usually referred to as the problem of experimental design, may 
be diagrammatically described by means of a sequential decision tree such as that 
shown in Figure 2.6. 
Figure 2.6 Decision tree for experimental design 
We must first choose an experiment e and, in light of the data D obtained, 
take an action a, which, were event E to occur, would produce a consequence 
having utility which, modifying earlier notation in order to be explicit about the 
elements involved, we denote by u(a, e, D, E). Usually, we also have available 
64 
2 Foundations 
the possibility, denoted by e0 and referred to as the null experiment, of directly 
choosing an action without performing any experiment. 
Within the general structure for sequential decision problems developed in the 
previous section, we note that the possible sets of data obtainable may depend on 
the particular experiment performed, the set of available actions may depend on 
the results of the experiment performed, and the sets of consequences and labelling 
events may depend on the particular combination of experiment and action chosen. 
However, in our subsequent development we will use a simplified notation which 
suppresses these possible dependencies in order to centre attention on other, more 
important, aspects of the problem. 
We have seen, in Section 2.6.2, that to solve a sequential decision problem 
we start at the last stage and work backwards. In this case, the expected utility of 
option a, given the information available at the stage when the action is to be taken, 
is 
u(a,e,Dt) = ^2u(a,e,Di,Ej)P(Ej \e,Dt,a). 
For each pair (e, Dt) we can therefore choose the best possible continuation; 
namely, that action a* which maximises the expression given above. Thus, the 
expected utility of the pair (e, Dt) is given by 
u(e, Dt) = u(a*,e, DA = maxS(a, e, £>,■). 
a 
We are now in a position to determine the best possible experiment. This is 
that e which maximises, in the class of available experiments, the unconditional 
expected utility 
u(e) = ^2u(ale,Di)P(Di\e), 
iei 
where P(A | e) denotes the degree of belief attached to the occurrence of data 
Di if e were the experiment chosen. On the other hand, the expected utility of 
performing no experiment and choosing that action a^ which maximises the (prior) 
expected utility is 
u(e0) =u(a,Q,e0) = max^\(a,eo,-Ej,)P(Ej \ e0,a), 
jeJ 
so that an experiment e is worth performing if and only if u(e) > u(eo). 
Naturally, u(a, e, Di), u(e, Dt) and u(e) are different functions defined on 
different spaces. However, to simplify the notation and without danger of confusion 
we shall always use u to denote an expected utility. 
Proposition 2.26. {Optimal experimental design). The optimal action is to 
perform the experiment e* ifu(e*) > u(eo) andu(e*) = m&xeu(e);  
otherwise, the optimal action is to perform no experiment. 
Proof. This is immediate from Proposition 2.25. 
< 
2.6 Sequential Decision Problems 
65 
It is often interesting to determine the value which additional information 
might have in the context of a given decision problem. 
The expected value of the information provided by new data may be computed 
as the (posterior) expected difference between the utilities which correspond to 
optimal actions after and before the data have been obtained. 
Definition 2.18. (The value of additional information). 
(i) The expected value of the data Di provided by an experiment e is 
v(e, Dt) = ^2 {u(a*, e, Du Ej) - u(a*0, e0, Ej)} P(Ej | e, Dit a*); 
where a*, Oq are, respectively, the optimal actions given Dt, and with no 
data. 
(ii) the expected value of an experiment e is given by 
«(e) = ^«(e,A)p(A|e). 
16/ 
It is sometimes convenient to have an upper bound for the expected value v(e) 
of an experiment e. Let us therefore consider the optimal actions which would be 
available with perfect information, i.e., were we to know the particular event Ej 
which will eventually occur, and let a*(j) be the optimal action given Ej, i.e., such 
that, for all £j, 
u(a^i),eo,Ej) = maxu(a, eo,Ej). 
Then, given Ej, the loss suffered by choosing any other action a will be 
u(a*j}, e0, Ej) - u(a, e0, Ej). 
For a = a.Q, the optimal action under prior information, this difference will  
measure, conditional on Ej, the value of perfect information and, under appropriate 
conditions, its expected value will provide an upper bound for the increase in utility 
which additional data about the Ej's could be expected to provide. 
Definition 2.19. (Expected value of perfect information). The opportunity 
loss which would be suffered if action a were taken and event Ej occurred is 
l(a,Ej) = maxu(ai,eo,Ej) — u(a,eo,Ej); 
ai 
the expected value of perfect information is then given by 
v*(e0) = J2l«Ej)P(Ej\a*0). 
66 
2 Foundations 
It is important to bear in mind that the functions v(Di),v(e) and the number 
v*(e0), all crucially depend on the (prior) probability distributions {{P(Ej | a), 
a £ A} although, for notational convenience, we have not made this dependency 
explicit. 
In many situations, the utility function u(a, e, A, A) may be thought of as 
made up of two separate components. One is the (experimental) cost of performing 
e and obtaining Dt; the other is the (terminal) utility of directly choosing a and then 
finding that Ej occurs. Often, the latter component does not actually depend on the 
preceding e and A> so that, assuming additivity of the two components, we may 
write u(a,e,Di,Ej) = u(a,eo, Ej) — c(e,Di) where c(e, A) > 0. Moreover, 
the probability distributions over the events are often independent of the action 
taken. When these conditions apply, we can establish a useful upper bound for the 
expected value of an experiment in terms of the difference between the expected 
value of complete data and the expected cost of the experiment itself. 
Proposition 2.27. (Additive decomposition). If the utility function has the 
form 
u(a,e,DitEj) = u(a,e0, A) -c(e, A), 
with c(e, A) > 0, and the probability distributions are such that 
p(Ej | e, Dt, a) = p(Ej | e, A), p(Ej \ e0, a) = p(Ej \ e0), 
then, for any available experiment e, 
v(e) <v*(e0)-c(e), 
where 
c(e) = ^c(e,A)P(A|e) 
iel 
is the expected cost ofe. 
Proof. Using Definitions 2.18 and 2.19, v(e) may be written as 
J2 [ Y, {«> eo, Ej) - c(e, A) - u(a*0, e0, A)} P(Ej | e, A)] P(A | e) 
iel jeJ 
£ [max^ Ha'eo' Ei) - u« e°' Ei)} P(Ei I e' D')} P(D> I e) ~ 5(e) 
iel jeJ 
< Y1Y1 [mfx«(a,e0,A) -u(a*0,e0,Ej)]p(EjnDi\e) -c(e) 
iel jeJ 
2.7 Inference and Information 
67 
and, hence, 
v(e) < {£/(a,^)P(^ |e0)}{ $>(A | Ej,e)} - c(e) 
= {Y,l(a,Ej)P(Ej\e0)}-c(e) 
= v*(e0) -c(e), 
as stated. < 
In Section 2.7, we shall study in more detail the special case of experimental 
design in situations where data are being collected for the purpose of pure inference, 
rather than as an input into a directly practical decision problem. 
We have shown that the simple decision problem structure introduced in  
Section 2.2, and the tools developed in Sections 2.3 to 2.5, suffice for the analysis of 
complex, sequential problems which, at first sight, appear to go beyond that simple 
structure. In particular, we have seen that the important problem of experimental 
design can be analysed within the sequential decision problem framework. We 
shall now use this framework to analyse the very special form of decision problem 
posed by statistical inference, thus establishing the fundamental relevance of these 
foundational arguments for statistical theory and practice. 
2.7 INFERENCE AND INFORMATION 
2.7.1 Reporting Beliefs as a Decision Problem 
The results on quantitative coherence (Sections 2.2 to 2.5) establish that if we aspire 
to analyse a given decision problem, {£, C, A, < }, in accordance with the axioms of 
quantitative coherence, we must represent degrees of belief about uncertain events 
in the form of a finite probability measure over £ and values for consequences in 
the form of a utility function over C. Options are then to be compared on the basis 
of expected utility. 
The probability measure represents an individual's beliefs conditional on his or 
her current state of information. Given the initial state of information described by 
M0 and further information in the form of the assumed occurrence of a significant 
event G, we previously denoted such a measure by P(.\G). We now wish to 
specialise our discussion somewhat to the case where G can be thought of as a 
description of the outcome of an investigation (typically a survey, or an experiment) 
involving the deliberate collection of data (usually, in numerical form). The event 
G will then be defined directly in terms of the counts or measurements obtained, 
either as a precise statement, or involving a description of intervals within which 
68 
2 Foundations 
readings lie. To emphasise the fact that G characterises the actual data collected, 
we shall denote the event which describes the new information obtained by D. 
An individual's degree of belief measure over £ will then be denoted P(. \ D) 
representing the individual's current beliefs in the light of the data obtained (where, 
again, we have suppressed, for notational convenience, the explicit dependence on 
Mo). So far as uncertainty about the events of £ is concerned, P(.\D) constitutes 
a complete encapsulation of the information provided by D, given the initial state 
of information Mo. Moreover, in conjunction with the specification of a utility 
function, P(. \ D) provides all that is necessary for the calculation of the expected 
utility of any option and, hence, for the solution of any decision problem defined 
in terms of the frame of reference adopted. 
Starting from the decision problem framework, we thus have a formal  
justification for the main topic of this book; namely, the study of models and techniques 
for analysing the ways in which beliefs are modified by data. However, many  
eminent writers have argued that basic problems of reporting scientific inferences do 
not fall within the framework of decision problems as defined in earlier sections: 
Statistical inferences involve the data, a specification of the set of possible  
populations sampled and a question concerning the true population... Decisions 
are based on not only the considerations listed for inferences, but also on an 
assessment of the losses resulting from wrong decisions... (Cox, 1958); 
... a considerable body of doctrine has attempted to explain, or rather to  
reinterpret these (significance) tests on the basis of quite a different model, namely as 
means to making decisions in an acceptance procedure. The differences between 
these two situations seem to the author many and wide,... (Fisher, 1956/1973). 
If views such as these were accepted, they would, of course, undermine our 
conclusion that problems concerning uncertainty are to be solved by revising  
degrees of belief in the light of new data in accordance with Bayes' theorem. Our 
main purpose in this section is therefore to demonstrate that the problem of reporting 
inferences is essentially a special case of a decision problem. 
By way of preliminary clarification, let us recall from Section 2.1 that we 
distinguished two, possibly distinct, reasons for trying to think rationally about 
uncertainty. On the one hand, quoting Ramsey (1926), we noted that, even if an 
immediate decision problem does not appear to exist, we know that our statements 
of uncertainty may be used by others in contexts representable within the decision 
framework. In such situations, our conclusion holds. On the other hand, quoting 
Lehmann (1959/1986), we noted that the inference, or inference statement, may 
sometimes be regarded as an end in itself, to be judged independently of any 
"practical" decision problem. It is this case that we wish to consider in more detail 
in this section, establishing that, indeed, it can be regarded as falling within the 
general framework of Sections 2.2 to 2.5. 
2.7 Inference and Information 
69 
Formalising the first sentence of the remark of Cox, given above, a pure 
inference problem may be described as one in which we seek to learn which of a 
set of mutually exclusive "hypotheses" ("theories", "states of nature", or "model 
parameters") is true. From a strictly realistic viewpoint, there is always, implicitly, 
a finite set of such hypotheses, say {Hj, j € J}, although it may be mathematically 
convenient to work as if this were not the case. We shall regard this set of hypotheses 
as equivalent to a finite partition of the certain event into events {Ej, j € J}, 
having the interpretation Ej = "the hypothesis Hj is true". The actions available 
to an individual are the various inference statements that might be made about 
the events {Ej, j e J}, the latter constituting the uncertain events corresponding 
to each action. To complete the basic decision problem framework, we need to 
acknowledge that, corresponding to each inference statement and each Ej, there 
will be a consequence; namely, the record of what the individual put forward as an 
appropriate inference statement, together with what actually turned out to be the 
case. 
If we aspire to quantitative coherence in such a framework, we know that 
our uncertainty about the {Ej, j e J} should be represented by {P(Ej | D), 
j £ J}, where P(. | D) denotes our current degree of belief measure, given data 
D in addition to the initial information Mo. It is natural, therefore, to regard the 
set of possible inference statements as the class of probability distributions over 
{Ej, j e J} compatible with the information D. The inference reporting problem 
can thus be viewed as one of choosing a probability distribution to serve as an 
inference statement. But there is nothing (so far) in this formulation which leads to 
the conclusion that the best action is to state one's actual beliefs. Indeed, we know 
from our earlier development that options cannot be ordered without an (implicit 
or explicit) specification of utilities for the consequences. We shall consider this 
specification and its implications in the following sections. A particular form 
of utility function for inference statements will be introduced and it will then be 
seen that the idea of inference as decision leads to rather natural interpretations of 
commonly used information measures in terms of expected utility. In the discussion 
which follows, we shall only consider the case of finite partitions {Ej, j € J}. 
Mathematical extensions will be discussed in Chapter 3. 
2.7.2 The Utility of a Probability Distribution 
We have argued above that the provision of a statistical inference statement about 
a class of exclusive and exhaustive "hypotheses" {Ej, j £ J}, conditional on 
some relevant data D, may be precisely stated as a decision problem, where the 
set of "hypotheses" {Ej, j e J} is a partition consisting of elements of £, and the 
action space A relates to the class Q of conditional probability distributions over 
{Ej, jeJ}; thus, 
Q = {q = (<&, j e J); qj > o, 53 Qj = i}, 
70 
2 Foundations 
where qj is assumed to be the probability which, conditional on the available data 
D, an individual reports as the probability of Ej = Hj being true. The set of 
consequences C, consists of all pairs (q,Ej), representing the conjunctions of 
reported beliefs and true hypotheses. The action corresponding to the choice of q 
is defined as {(q, Ej) \ Ej, j € J}. 
To avoid triviality, we assume that none of the hypotheses is certain and that, 
without loss of generality, all are compatible with the available data; i.e., that all 
the Ej's are significant given D, so that (Proposition 2.5) 0 < Ej n D < D for all 
j e J. If this were not so, we could simply discard any incompatible hypotheses. 
It then follows from Proposition 2.17(iii) that each of the personal degrees of belief 
attached by the individual to the conflicting hypotheses given the data must be 
strictly positive. Throughout this section, we shall denote by 
p = (Pj = P(Ej\D),j£j), Pj>0, ^2j€jPj = l, 
the probability distribution which describes, conditional again on the available data 
D, the individual's actual beliefs about the alternative "hypotheses". 
We emphasise again that, in the structure described so far, there is no logical 
requirement which forces an individual to report the probability distribution p 
which describes his or her personal beliefs, in preference to any other probability 
distribution q in Q. 
We complete the specification of this decision problem by inducing the  
preference ordering through direct specification of a utility function u (.), which describes 
the "value" u(q, Ej) of reporting the probability distribution q as the final  
inferential summary of the investigation, were Ej to turn out to be the true "state of nature". 
Our next task is to investigate the properties which such a function should possess 
in order to describe a preference pattern which accords with what a scientific  
community ought to demand of an inference statement. This special class of utility 
functions is often referred to as the class of score functions (see also Section 2.8) 
since the functions describe the possible "scores" to be awarded to the individual 
as a "prize" for his or her "prediction". 
Definition 2.20. (Score function). A score function u for probability  
distributions q = {qj, j e J} defined over a partition {Ej, j £ J} is a mapping 
which assigns a real number u{q, Ej} to each pair (q, Ej). This function is 
said to be smooth if it is continuously differentiable as a function of each qj. 
It seems natural to assume that score functions should be smooth (in the intuitive 
sense), since one would wish small changes in the reported distribution to produce 
only small changes in the obtained score. The mathematical condition imposed 
is a simple and convenient representation of such smoothness. 
2.7 Inference and Information 
71 
We have characterised the problem faced by an individual reporting his or her 
beliefs about conflicting "hypotheses" as a problem of choice among probability 
distributions over {Ej, j € J}, with preferences described by a score function. 
This is a well specified problem, whose solution, in accordance with our  
development based on quantitative coherence, is to report that distribution q which 
maximises the expected utility 
J£u(q,Ej)P(Ej\D). 
In order to ensure that a coherent individual is also honest, we need a form of 
u(.) which guarantees that the expected utility is maximised if, and only if, q3- = 
Pj = P{Ej | D), for each j; otherwise, the individual's best policy could be to 
report something other than his or her true beliefs. This motivates the following 
definition: 
Definition 2.21. {Proper score function). A score function u is proper if for 
each strictly positive probability distribution p = {pj, j € J} defined over a 
partition {Ej, j € J}, 
sup { V u(q, Ej)pj \ = V u{p, Ej)pj , 
qeQ [jeJ J jeJ 
where the supremum, taken over the class Q of all probability distributions 
over {Ej, j € J}, is attained if, and only if, q = p. 
It would seem reasonable that, in a scientific inference context, one should require 
a score function to be proper. Whether a scientific report presents the inference 
of a single scientist or a range of inferences, purporting to represent those that 
might be made by some community of scientists, we should wish to be reassured 
that any reported inference could be justified as a genuine current belief. 
Smooth, proper score functions have been successfully used in practice in the 
following contexts: (i) to determine an appropriate fee to be paid to meteorologists 
in order to encourage them to report reliable predictions (Murphy and Epstein, 
1967); (ii) to score multiple choice examinations so that students are encouraged 
to assign, over the possible answers, probability distributions which truly describe 
their beliefs (de Finetti, 1965; Bernardo, 1981b, Section 3.6); (iii) to devise general 
procedures to elicit personal probabilities and expectations (Savage, 1971); (iv) 
to select best subsets of variables for prediction purposes in political or medical 
contexts (Bernardo and Bermudez, 1985). 
The simplest proper score function is the quadratic function (Brier, 1950; de 
Finetti, 1962) defined as follows. 
72 
2 Foundations 
Definition 2.22. (Quadratic score function). A quadratic score Junction for 
probability distributions q = {qj,j € J}definedoverapartition{Ej, j € J} 
is any function of the form 
u{q, Ej} = Al2qj-J2^\+Bh A > 0, 
where q = {qj, j € J} is any probability distribution over {Ej, j £ J}. 
Using the indicator function for Ej, Iej, an alternative expression for the 
quadratic score function is given by 
u{q, Ej} = A 11 - Y, (ft - M2 I + Bj, A > 0, 
which makes explicit the role of a 'penalty' equal to the squared euclidean distance 
from q to a perfect prediction. 
Proposition 2.28. A quadratic score function is proper. 
Proof. We have to maximise, over q, the expected score 
53 u{g, £,-} p, = £ j A ( 2<n - 53 q? J + B, 
Pi 
Taking derivatives with respect to the g/s and equating them to zero, we have the 
system of equations 2pj — 2qj{^2k pk} = 0, j e J, and since ^ p« = 1, we have 
Qj = pj for all j. It is easily checked that this gives a maximum. ^ 
Note that in the proof of Proposition 2.28 we did not need to use the condition 
53 ■ <7j = 1; this is a rather special feature of the quadratic score function. 
A further condition is required for score functions in contexts, which we shall 
refer to as "pure inference problems", where the value of a distribution, q, is only 
to be assessed in terms of the probability it assigned to the actual outcome. 
Definition 2.23. (Local score function). A score function u is local if, for 
each element q = {qj, j € J} of the class Q of probability distributions 
defined over a partition {Ej, j e J}, there exist functions {uj(.), j £ J} 
such that u{q, Ej} = Uj(qj). 
It is intuitively clear that the preferences of an individual scientist faced with a 
pure inference problem should correspond to the ordering induced by a local score 
function. The reason for this is that, by definition, in a "pure" inference problem 
we are solely concerned with "the truth". It is therefore natural that if Ej, say, turns 
out to be true, the individual scientist should be assessed (i.e., scored) only on the 
basis of his or her reported judgement about the plausibility of Ej. 
2.7 Inference and Information 
73 
This can be contrasted with the forms of "score" function that would typically 
be appropriate in more directly practical contexts. In stock control, for example, 
probability judgements about demand would usually be assessed in the light of the 
relative seriousness of under- or over-stocking, rather than by just concentrating 
on the belief previously attached to what turned out to be the actual level of 
demand. 
Note that, in Definition 2.23, the functional form Uj{pj) of the dependence 
of the score on the probability attached to the true Ej is allowed to vary with the 
particular Ej considered. By permitting different Uj(.)'s for each Ej, we allow for 
the possibility that "bad predictions" regarding some "truths" may be judged more 
harshly than others. 
The situation described by a local score function is, of course, an idealised, 
limit situation, but one which seems, at least approximately, appropriate in reporting 
pure scientific research. In addition, later in this section we shall see that certain 
well-known criteria for choosing among experimental designs are optimal if, and 
only if, preferences are described by a smooth, proper, local score function. 
Proposition 2.29. (Characterisation of proper local score functions). Ifu 
is a smooth, proper, local score function for probability distributions q = 
{ijd € «/} defined over a partition {Ej, j € J} which contains more than 
two elements, then it must be of the form u{q, Ej} = Alogqj + Bj, where 
A > 0 and the Bj's are arbitrary constants. 
have 
Proof. Since u(.) is local and proper, then for some {uj(.),j € J}, we must 
supj^ u(q,Ej) Pj = sup V uj (qj) Pj = V Uj (Pj) Pj, 
* jeJ "* jeJ jeJ 
where pj > 0, ^ Pj = 1 and the supremum is taken over the class of probability 
distributions q = (qj,j G J), qj > 0, Ylj Qj: = 1- 
Writingp = {pi,P2, ■ ■ ■} and q = {qi,q2, ■ ■ •}, with 
pi = i - Ylv^ qi = * ~ 5Z *• 
we seek {uj(.) , j £ J}, giving an extremal of 
F{q2, &,...}= ( 1 - Y^TPi I "i I1 _ 51 * 1 + ^PiuMi) > 
V j>i / V j>i / j>\ 
For {q2, Q3,...} to make F stationary it is necessary (see e.g. Jeffreys and Jeffreys, 
1946, p. 315) that 
-Q-F{q2 + a£2,93 + <*£3.---} 
= 0 
a=0 
74 
2 Foundations 
for any s ~ {£2, £3,. • •} such that all the ej are sufficiently small. Calculating this 
derivative, the condition is seen to reduce to 
EI (l-1>J < \l-I>) -pju'Mj)I £j- = ° 
for all £j's sufficiently small, where it' stands for the derivative of it. Moreover, 
since it is proper, {p2,P3, ■ • ■} must be an extremal of F and thus we have the 
system of equations 
pju'j(pj) = [l~Ep*) ui (l-Epi)- j =2'3,• • • 
so that all the functions Uj,j = 1,2,... satisfy the same functional equation, 
namely 
Pj u'j(Pj) = Pi u'i(Pi)> j = 2,3,..., 
for all {j?2,P3, • ■} and, hence, 
pu'j(p) = A, 0<p<l, for all j = l,2,... 
so that Uj(p) = A \ogp + Bj. The condition A > 0 suffices to guarantee that the 
extremal found is indeed a maximum. ^ 
Definition 2.24. {Logarithmic score function). A logarithmic score function 
for strictly positive probability distributions q = {qj,j € J} defined over a 
partition {Ej, j € J} is any function of the form 
u{q, Ej} = A log qj + Bj, A>0. 
If the partition {Ej, j e J} only contains two elements, so that the partition 
is simply {H, Hc}, the locality condition is, of course, vacuous. In this case, 
uiQ,Ej} = u{{li,l - 9i), l/f} = /(<Zi,lff), say, where 1H is the indicator 
function for H, and the score function only depends on the probability qi attached 
to H, whether or not H occurs. 
For u{(qi, I - q\), Iff } to be proper we must have 
sup {pi f(qu 1) + (1 -pi) /(l - ft, 0)} = px f(Pl, 1) + (1 - pi) /(l -pi.O) 
<zie[o,i] 
so that, if the score function is smooth, then / must satisfy the functional equation 
a:/'(a:,l) = (l-a:)/'(l-a:,0). 
2.7 Inference and Information 
75 
The logarithmic function f(x, 1) = A log x + Bx, f(x, 0) = A log(l - x) + B2 is 
then just one of the many possible solutions (see Good, 1952). 
We have assumed that the probability distributions to be considered as options 
assign strictly positive qj to each Ej. This means that, given any particular q € Q, 
we have no problem in calculating the expected utility arising from the logarithmic 
score function. It is worth noting, however, that since we place no (strictly positive) 
lower bound on the possible qj, we have an example of an unbounded decision 
problem; i.e., a decision problem without extreme consequences. 
2.7.3 Approximation and Discrepancy 
We have argued that the optimal solution to an inference reporting problem (either 
for an individual, or for each of several individuals) is to state the appropriate actual 
beliefs, p, say. From a technical point of view, however, particularly within the 
mathematical extensions to be considered in Chapter 3, the precise computation of 
p may be difficult and we may choose instead to report an approximation to our 
beliefs, q, say, on the grounds that q is "close" to p, but much easier to calculate. 
The justification of such a procedure requires a study of the notion of "closeness" 
between two distributions. 
Proposition 2.30. {Expected loss in probability reporting). If preferences 
are described by a logarithmic score function, the expected loss of utility in 
reporting a probability distribution q = {qj, j e J} defined over a partition 
{Ej, j G J}, rather than the distribution p = {pj, j € J} representing 
actual beliefs, is given by 
6{q \p}=A^2Pj log (pj/qj), A > 0. 
jeJ 
Moreover, 6{q | p} > 0 with equality if and only if q = p. 
Proof. Using Definition 2.24, the expected utility of reporting q when p is the 
actual distribution of beliefs is u(q) = ^2j{Alogqj + Bj}pj, and thus 
6{q | p} = u(p) - u(q) 
= $3 i(Al°SPi + Bi) ~ (Al°S<lJ + BJ)}PJ = AY,TPiXo^. ' 
jeJ jeJ qj 
The final statement in the theorem is a consequence of Proposition 2.29 since, 
because the logarithmic score function is proper, the expected utility of reporting q 
is maximised if, and only if, q = p, so that u(p) > u(q), with equality if, and only 
76 
2 Foundations 
if, p = q. An immediate direct proof is obtained using the fact that for all x > 0, 
log x < x — 1 with equality if, and only if, x = 1. Indeed, we then have 
-6{q\p} = ^Pj log 
with equality if, and only if, qj = pj for all j. < 
The quantity 8{q \p}, which arises here as a difference between two expected 
utilities, was introduced by Kullback and Leibler (1951) as an ad hoc measure of 
(directed) divergence between two probability distributions. 
Combining Propositions 2.29 and 2.30, it is clear that an individual with 
preferences approximately described by a proper local score function should beware 
of approximating by zero. This reflects the fact that the "tails" of the distribution 
are, generally speaking, extremely important in pure inference problems. This 
is in contrast to many practical decision problems where the form of the utility 
function often makes the solution robust with respect to changes in the "tails" of 
the distribution assumed. 
Proposition 2.30 suggests a natural, general measure of "lack of fit", or  
discrepancy, between a distribution and an approximation, when preferences are described 
by a logarithmic score function. 
Definition 2.25. {Discrepancy of an approximation). The discrepancy  
between a strictly positive probability distribution p = {pj, j € J} over a 
partition {Ej, j € J} and an approximation p = {pj, j € J} is defined by 
Pj 
HP\p} = YspJlog~:l. 
* 
Example 2.5. (Poisson approximation to a binomial distribution). The behaviour 
of 6{p\p} is well illustrated by a familiar, elementary example. Consider the binomial 
distribution 
P; = ("V(l - 0)n-J, j=0,l,...,n, 
= 0, otherwise 
and let 
Pj = exp{-n0}—^-, j =0,1,... 
2.7 Inference and Information 
77 
O.OSt <HPoisson I Binomial} 
n = l 
0.00 
0.25 
Figure 2.7 Discrepancy between a binomial distribution and its Poisson  
approximation (logarithms to base 2). 
be its Poisson approximation. It is apparent from Figure 2.7 that 6{p | p} decreases as either 
n increases or 9 decreases, or both, and that the second factor is far more important than the 
first. However, it follows from our previous discussion that it would not be a good idea to 
reverse the roles and try to approximate a Poisson distribution by a binomial distribution. 
When, as in Figure 2.7, logarithms to base 2 are used, the utility and discrepancy 
are measured on the well-known scale of bits of information (or entropy), which 
can be interpreted in terms of the expected number of yes-no questions required 
to identify the true event in the partition (see, for example, de Finetti, 1970/1974, 
p. 103, or Renyi, 1962/1970, p. 564). 
Clearly, Definition 2.25 provides a systematic approach to approximation in 
pure inference contexts. The best approximation within a given family will be that 
which minimises the discrepancy. 
2.7.4 Information 
In Section 2.4.2, we showed that, for quantitative coherence, any new information D 
should be incorporated into the analysis by updating beliefs via Bayes' theorem, so 
that the initial representation of beliefs P(.) is updated to the conditional probability 
measure P(. | D). In Section 2.7.2, we showed that, within the context of the pure 
inference reporting problem, utility is defined in terms of the logarithmic score 
function. 
78 
2 Foundations 
Proposition 2 31. (Expected utility of data). If preferences are described by 
a logarithmic score function for the class of probability distributions defined 
over a partition {Ej, j € J}, then the expected increase in utility provided 
by data D, when the initial probability distribution {P(Ej),j € J} is strictly 
positive, is given by 
^gP(Ej|D).og™. 
where A > 0 is arbitrary, and {P{Ej \ D),j € J} is the conditional  
probability distribution, given D. Moreover, this expected increase in utility is 
non-negative and is zero if, and only if, P{Ej | D) = P(Ej)for all j. 
Proof. By Definition 2.24, the utilities of reporting P(.) or P(. | D), were 
Ej known to be true, would be AlogP(Ej) + Bj and A\ogP(Ej \ D) + Bj, 
respectively. Thus, conditional on D, the expected increase in utility provided by 
D is given by 
J^logP^ | D) + Bj) - {A\ogP{E3) + Bj)}P(Ej \ D) 
jeJ 
P{Ej\D) 
= A^P{EjlD)log-^l . 
jeJ v J' 
which, by Proposition 2.30, is non-negative and is zero if and only if, for all j, 
P(Ej\D) = P(Ej). < 
In the context of pure inference problems, we shall find it convenient to  
underline the fact that, because of the use of the logarithmic score function, utility 
assumes a special form and establishes a link between utility theory and classical 
information theory. This motivates Definitions 2.26 and 2.27. 
Definition 2.26. (Information from data). The amount of information about 
a partition {Ej, j £ J} provided by the data D, when the initial distribution 
over {Ej, j G J} is p0 - {P(Ej),j e J}, is defined to be 
I(D\p0) = Y,P(Ej\D)\ogP{^°) , 
jeJ * •'' 
where {P(Ej \D), j £ J} is the conditional probability distribution given 
the data D. 
2.7 Inference and Information 
79 
It follows from Definition 2.26 that the amount of information provided by 
data D is equal to 6(p0 | pD), the discrepancy measure if p0 = {P(Ej), j € J} is 
considered as an approximation to pD = {P(Ej \D),j £ J}. Another interesting 
interpretation of I(D | p0) arises from the following analysis. Conditional on Ej, 
log P(Ej) and log P(Ej \ D) measure, respectively, how good the initial and the 
conditional distributions are in "predicting" the "true hypothesis" Ej = Hj, so 
that log P{Ej | D) — log P(Ej) is a measure of the value of D, were Ej known to 
be true; I(D \p0) is simply the expected value of that difference calculated with 
respect to pD. 
It should be clear from the preceding discussion that I(D | p0) measures  
indirectly the information provided by the data in terms of the changes produced in the 
probability distribution of interest. The amount of information is thus seen to be 
a relative measure, which obviously depends on the initial distribution. Attempts 
to define absolute measures of information have systematically failed to produce 
concepts of lasting value. 
In the finite case, the entropy of the distribution p = {pi,..., pn }, defined by 
n 
has been proposed and widely accepted as an absolute measure of uncertainty. 
The recognised fact that its apparently natural extension to the continuous case 
does not make sense (if only because it is heavily dependent on the particular 
parametrisation used) should, however, have raised doubts about the universality 
of this concept. The fact that, in the finite case, H{p} as a measure of uncertainty 
(and —H{p} as a measure of "absolute" information) seems to work correctly is 
explained (from our perspective) by the fact that 
n 
V]Pi log -777 = logn - H{p}, 
so that, in terms of the above discussion, —H{p} may be interpreted, apart from an 
unimportant additive constant, as the amount of information which is necessary 
to obtain p = {p\,... ,pn} from an initial discrete uniform distribution (see 
Section 3.2.2), which acts as an "origin" or "reference" measure of uncertainty. 
As we shall see in detail later, the problem of extending the entropy concept 
to continuous distributions is closely related to that of defining an "origin" or 
"reference" measure of uncertainty in the continuous case, a role unambiguously 
played by the uniform distribution in the finite case. For detailed discussion of 
H{p} and other proposed entropy measures, see Renyi (1961). 
We shall on occasion wish to consider the idea of the amount of information 
which may be expected from an experiment e, the expectation being calculated 
before the results of the experiment are actually available. 
80 2 Foundations 
Definition 2.27. (Expected information from an experiment). The expected 
information to be provided by an experiment e about a partition {Ej, j € J}, 
when the initial distribution over {Ej, j € J} is p0 = {P(Ej), j € J}, is 
given by 
I(e\p0) = J2I(Di\Po)P(Di), 
i€l 
where the possible results of the experiment e, {A, i € /}, occur with  
probabilities {P(Di),i e I}. 
Proposition 2.32. An alternative expression for the expected information is 
where P{Ej n A) = -P(A) P(Ej | A), and {P(Ej \ Dl),j € J} is the 
conditional distribution, given the occurrence of Dit corresponding to the 
initial distribution p0 = {P(Ej),j € J}. Moreover, 7(e|p0) > 0, with 
equality if and only if for all E{ and Djt P(Ej n D{) = P(Ej)P{Di). 
Proof Let % = P(A), Pj = P{Ej) and pj( = P(Ej | A). Then, by 
Definition 2.27, 
I(e\Po) = Y^{ Y^P*log?T f * = S Y^PiMlog: 
and the result now follows from the fact that, by Bayes' theorem, 
P(Ej n A) = P(Ej | A)P(A) = Pm- 
Since, by Proposition 2.31,1(D{ \ p0) > 0 with equality iff, P(A I A) = -P(A), 
it follows from Definition 2.27 that I(e | p0) > 0 with equality if, and only if, for 
all Ej and A, P{Ej n A) = P(A)P(A)- < 
The expression for 7(e | p0) given by Proposition 2.32 is Shannon's (1948) 
measure of expected information. We have thus found, in a decision theoretical 
framework, a natural interpretation of this famous measure of expected information: 
Shannon's expected information is the expected utility provided by an experiment 
in a pure inference context, when an individual's preferences are described by a 
smooth, proper, local score function. 
In conclusion, we have suggested that the problem of reporting inferences 
can be viewed as a particular decision problem and thus should be analysed within 
2.8 Discussion and Further References 
81 
the framework of decision theory. We have established that, with a natural  
characterisation of an individual's utility function when faced with a pure inference 
problem, preferences should be described by a logarithmic score function. We 
have also seen that, within this framework, discrepancy and amount of information 
are naturally defined in terms of expected loss of utility and expected increase in 
utility, respectively, and that maximising expected Shannon information is a  
particular instance of maximising expected utility. We shall see in Section 3.4 how 
these results, established here for finite partitions, extend straightforwardly to the 
continuous case. 
2.8 DISCUSSION AND FURTHER REFERENCES 
2.8.1 Operational Definitions 
In everyday conversation, the way in which we use language is typically rather  
informal and unselfconscious, and we tolerate each other's ambiguities and vacuities 
for the most part, occasionally seeking an ad hoc clarification of a particular  
statement or idea if the context seems to justify the effort required in trying to be a little 
more precise. (For a detailed account of the ambiguities which plague qualitative 
probability expressions in English, see Mosteller and Youtz, 1990.) 
In the context of scientific and philosophical discourse, however, there is 
a paramount need for statements which are meaningful and unambiguous. The 
everyday, tolerant, ad hoc response will therefore no longer suffice. More rigorous 
habits of thought are required, and we need to be selfconsciously aware of the 
precautions and procedures to be adopted if we are to arrive at statements which 
make sense. 
A prerequisite for "making sense" is that the fundamental concepts which 
provide the substantive content of our statements should themselves be defined in 
an essentially unambiguous manner. We are thus driven to seek for definitions of 
fundamental notions which can be reduced ultimately to the touchstone of actual 
or potential personal experience, rather than remaining at the level of mere words 
or phrases. 
This kind of approach to definitions is closely related to the philosophy of 
pragmatism, as formulated in the second half of the nineteenth century by Peirce, 
who insisted that clarity in thinking about concepts could only be achieved by  
concentrating attention on the conceivable practical effects associated with a concept, 
or the practical consequence of adopting one form of definition rather than another. 
In Peirce (1878), this point of view was summarised as follows: 
Consider what effects, that might conceivably have practical bearings, we  
conceive the object of our conception to have. Then, our conception of these effects 
is the whole of our conception of the object. 
82 
2 Foundations 
In some respects, however, this position is not entirely satisfactory in that it fails 
to go far enough in elaborating what is to be understood by the term "practical". This 
crucial elaboration was provided by Bridgman (1927) in a book entitled The Logic 
of Modern Physics, where the key idea of an operational definition is introduced 
and illustrated by considering the concept of "length": 
... what do we mean by the length of an object? We evidently know what we 
mean by length if we can tell what the length of any and every object is and for 
the physicist nothing more is required. To find the length of an object, we have 
to perform certain physical operations. The concept of length is therefore fixed 
when the operations by which length is measured are fixed: that is, the concept 
of length involves as much as, and nothing more, than the set of operations by 
which length is determined. In general, we mean by any concept nothing more 
than a set of operations; the concept is synonymous with the corresponding set 
of operations. If the concept is physical, ... the operations are actual physical 
measurements ... ; or if the concept is mental, ... the operations are mental 
operations... 
Throughout this work, we shall seek to adhere to the operational approach to 
defining concepts in order to arrive at meaningful and unambiguous statements in 
the context of representing beliefs and taking actions in situations of uncertainty. 
Indeed, we have stressed this aspect of our thinking in Sections 2.1 to 2.7, where we 
made the practical, operational idea of preference between options the fundamental 
starting point and touchstone for all other definitions. 
We also noted the inevitable element of idealisation, or approximation, implicit 
in the operational approach to our concepts, and we remarked on this at several 
points in Section 2.3. Since many critics of the personalistic Bayesian viewpoint 
claim to find great difficulty with this feature of the approach, often suggesting 
that it undermines the entire theory, it is worth noting Bridgman's very explicit 
recognition that all experience is subject to error and that all we can do is to take 
sufficient precautions when specifying sets of operations to ensure that remaining 
unspecified variations in procedure have negligible effects on the results of interest. 
This is well illustrated by Bridgman's account of the operational concept of length 
and its attendant idealisations and approximations: 
.. .we take a measuring rod, lay it on the object so that one of its ends coincides 
with one end of the object, mark on the object the position of the rod, then move 
the rod along in a straight line extension of its previous position until the first 
end coincides with the previous position of the second end, repeat this process as 
often as we can, and call the length the total number of times the rod was applied. 
This procedure, apparently so simple, is in practice exceedingly complicated, and 
doubtless a full description of all the precautions that must be taken would fill a 
large treatise. We must, for example, be sure that the temperature of the rod is 
the standard temperature at which its length is defined, or else we must make a 
2.8 Discussion and Further References 
83 
correction for it; or we must correct for the gravitational distortion of the rod if 
we measure a vertical length; or we must be sure that the rod is not a magnet or is 
not subject to electrical forces... we must go further and specify all the details 
by which the rod is moved from one position to the next on the object, its precise 
path through space and its velocity and acceleration in getting from one position 
to another. Practically, of course, precautions such as these are not taken, but 
the justification is in our experience that variations of procedure of this kind are 
without effect on the final result... 
This pragmatic recognition that there are inevitable limitations in any concrete 
application of a set of operational procedures is precisely the spirit of our discussion 
of Axioms 4 and 5 in Section 2.3. In practical terms, we have to stop somewhere, 
even though, in principle, we could indefinitely refine our measurement operations. 
What matters is to be able to achieve sufficient accuracy to avoid unacceptable 
distortion in any analysis of interest. 
2.8.2 Quantitative Coherence Theories 
In a comprehensive review of normative decision theories leading to the expected 
utility criterion, Fishburn (1981) lists over thirty different axiomatic formulations 
of the principles of coherence, reflecting a variety of responses to the underlying 
conflict between axiomatic simplicity and structural flexibility in the representation 
of decision problems. Fishburn sums up the dilemma as follows: 
On the one hand, we would like our axioms to be simple, interpretable,  
intuitively clear, and capable of convincing others that they are appealing criteria 
of coherency and consistency in decision making under uncertainty, but to do 
this it seems essential to invoke strong structural conditions. On the other hand, 
we would like our theory to adhere to the loose structures that often arise in 
realistic decision situations, but if this is done then we will be faced with fairly 
complicated axioms that accommodate these loose structures. 
In addition, we should like the definitions of the basic concepts of probability 
and utility to have strong and direct links with practical assessment procedures, in 
conformity with the operational philosophy outlined above. 
With these considerations in mind, our purpose here is to provide a brief 
historical review of the foundational writings which seem to us the most significant. 
This will serve in part to acknowledge our general intellectual indebtedness and 
orientation, and in part to explain and further motivate our own particular choice 
of axiom system. 
The earliest axiomatic approach to the problem of decision making under 
uncertainty i s that of Ramsey (1926), who presented the outline of a formal system. 
The key postulate in Ramsey's theory is the existence of a so-called ethically neutral 
84 
2 Foundations 
event E, say, which, expressed in terms of our notation for options, has the property 
that {ci | E, C2 | Ec} ~ {ci | Ec, c2 | E}, for any consequences c\, c2. It is then 
rather natural to define the degree of belief in such an event to be 1/2 and, from 
this quantitative basis, it is straightforward to construct an operational measure of 
utility for consequences. This, in turn, is used to extend the definition of degree of 
belief to general events by means of an expected utility model. 
From a conceptual point of view, Ramsey's theory seems to us, as indeed it 
has to many other writers, a revolutionary landmark in the history of ideas. From 
a mathematical point of view, however, the treatment is rather incomplete and it 
was not until 1954, with the publication of Savage's (1954) book The Foundations 
of Statistics that the first complete formal theory appeared. No mathematical  
completion of Ramsey's theory seems to have been published, but a closely related 
development can be found in Pfanzagl (1967, 1968). 
Savage's major innovation in structuring decision problems is to define what 
he calls acts (options, in our terminology) as functions from the set of uncertain 
possible outcomes into the set of consequences. His key coherence assumption 
is then that of a complete, transitive order relation among acts and this is used 
to define qualitative probabilities. These are extended into quantitative  
probabilities by means of a "continuously divisible" assumption about events. Utilities are 
subsequently introduced using ideas similar to those of von Neumann and Mor- 
genstern (1944/1953), who had, ten years earlier, presented an axiom system for 
utility alone, assuming the prior existence of probabilities. 
The Savage axiom system is a great historical achievement and provides the 
first formal justification of the personalistic approach to probability and decision 
making; for a modern appraisal see Shafer (1986) and lively ensuing discussion. 
See, also, Hens (1992). Of course, many variations on an axiomatic theme are  
possible and other Savage-type axiom systems have been developed since by Stigum 
(1972), Roberts (1974), Fishburn (1975) and Narens (1976). Suppes (1956)  
presented a system which combined elements of Savage's and Ramsey's approaches. 
See, also, Suppes (1960, 1974) and Savage (1970). There are, however, two major 
difficulties with Savage's approach, which impose severe limitations on the range 
of applicability of the theory. 
The first of these difficulties stems from the "continuously divisible"  
assumption about events, which Savage uses as the basis for proceeding from qualitative 
to quantitative concepts. Such an assumption imposes severe constraints on the 
allowable forms of structure for the set of uncertain outcomes: in fact, it even 
prevents the theory from being directly applicable to situations involving a finite or 
countably infinite set of possible outcomes. 
One way of avoiding this embarrassing structural limitation is to introduce 
a quantitative element into the system by a device like that of Ramsey's ethically 
neutral event. This is directly defined to have probability 1/2 and thus enables 
Ramsey to get the quantitative ball rolling without imposing undue constraints on 
2.8 Discussion and Further References 
85 
the structure. All he requires is that (at least) one such event be included in the 
representation of the uncertain outcomes. In fact, a generalisation of Ramsey's 
idea re-emerges in the form of canonical lotteries, introduced by Anscombe and 
Aumann (1963) for defining degrees of belief, and by Pratt, Raiffa and Schlaifer 
(1964, 1965) as a basis for simultaneously quantifying personal degrees of belief 
and utilities in a direct and intuitive manner. 
The basic idea is essentially that of a standard measuring device, in some sense 
external to the real-world events and options of interest. It seems to us that this 
idea ties in perfectly with the kind of operational considerations described above, 
and the standard events and options that we introduced in Section 2.3 play this 
fundamental operational role in our own system. Other systems using standard 
measuring devices (sometimes referred to as external scaling devices) are those of 
Fishburn (1967b, 1969) and Balch and Fishburn (1974). A theory which, like ours, 
combines a standard measuring device with a fundamental notion of conditional 
preference is that of Luce and Krantz (1971). 
The second major difficulty with Savage's theory, and one that also exists in 
many other theories (see Table I in Fishburn, 1981), is that the Savage axioms 
imply the boundedness of utility functions (an implication of which Savage was 
apparently unaware when he wrote The Foundations of Statistics, but which was 
subsequently proved by Fishburn, 1970). The theory does not therefore justify 
the use of many mathematically convenient and widely used utility functions; for 
example, those implicit in forms such as "quadratic loss" and "logarithmic score". 
We take the view, already hinted at in our brief discussion of medical and 
monetary consequences in Section 2.5, that it is often conceptually and  
mathematically convenient to be able to use structural representations going beyond what we 
perceive to be the essentially finitistic and bounded characteristics of real-world 
problems. And yet, in presenting the basic quantitative coherence axioms it is 
important not to confuse the primary definitions and coherence principles with the 
secondary issues of the precise forms of the various sets involved. For this reason, 
we have so far always taken options to be defined by finite partitions; indeed, within 
this simple structure, we hope that the essence of the quantitative coherence theory 
has already been clearly communicated, uncomplicated by structural complexities. 
Motivated by considerations of mathematical convenience, however, we shall, in 
Chapter 3, relax the constraint imposed on the form of the action space. We shall 
then arrive at a sufficiently general setting for all our subsequent developments and 
applications. 
2.8.3 Related Theories 
Our previous discussion centred on complete axiomatic approaches to decision 
problems, involving a unified development of both probability and utility concepts. 
In our view, a unified treatment of the two concepts is inescapable if operational 
86 
2 Foundations 
considerations are to be taken seriously. However, there have been a number of  
attempted developments of probability ideas separate from utility considerations, as 
well as separate developments of utility ideas presupposing the existence of  
probabilities. In addition, there is a considerable literature on information-theoretic 
ideas closely related to those of Section 2.7. In this section, we shall provide a 
summary overview of a number of these related theories, grouped under the  
following subheadings: (i) Monetary Bets and Degrees of Belief, (ii) Scoring Rules and 
Degrees of Belief, (iii) Axiomatic Approaches to Degrees of Belief, (iv) Axiomatic 
Approaches to Utilities and (v) Information Theories. 
For the most part, we shall simply give what seem to us the most important 
historical references, together with some brief comments. The first two topics will, 
however, be treated at greater length; partly because of their close relation with 
the main concerns of this book, and partly because of their connections with the 
important practical topic of the assessment of beliefs. 
Monetary Bets and Degrees of Belief 
An elegant demonstration that coherent degrees of belief satisfy the rules of (finitely 
additive) probability was given by de Finetti (1937/1964), without explicit use of 
the utility concept. Using the notation for options introduced in Section 2.3, de 
Finetti's approach can be summarised as follows. 
If consequences are assumed to be monetary, and if, given an arbitrary  
monetary sum m and uncertain event E, an individual's preferences among options are 
such that {pm | $7} ~ {m | E, 0 | Ec}, then the individual's degree of belief in E is 
defined to be p. 
This definition is virtually identical to Bayes' own definition of probability 
(see our later discussion under the heading of Axiomatic Approaches to Degrees of 
Belief). In modern economic terminology, probability can be considered to be a 
marginal rate of substitution or, more simply, a kind of "price". 
Given that an individual has specified his or her degrees of belief for some 
collection of events by repeated use of the above definition, either it is possible 
to arrange a form of monetary bet in terms of these events which is such that the 
individual will certainly lose, a so-called "Dutch book", or such an arrangement is 
impossible. In the latter case, the individual is said to have specified a coherent set 
of degrees of belief. It is now straightforward to verify that coherent degrees of 
belief have the properties of finitely additive probabilities. 
To demonstrate that 0 < p < 1, for any E and m, we can argue as follows. An 
individual who assigns p > 1 is implicitly agreeing to pay a stake larger than m to 
enter a gamble in which the maximum prize he or she can win is m; an individual 
who assigns p < 0 is implicitly agreeing to offer a gamble in which he or she will 
pay out either m or nothing in return for a negative stake, which is equivalent to 
paying an opponent to enter such a gamble. In either case, a bet can be arranged 
2.8 Discussion and Further References 
87 
which will result in a certain loss to the individual and avoidance of this possibility 
requires that 0 < p < 1. 
To demonstrate the additive property of degrees of belief for exclusive and 
exhaustive events, Ei,E2,...,En,'we proceed as follows. If an individual specifies 
Pi > P2, ■ ■ ■, pn, to be his or her degrees of belief in those events, this is an implicit 
agreement to pay a total stake of p\m\ + p2rri2 + • ■ ■ 4- pnmn in order to enter a 
gamble resulting in a prize of rrn if Ei occurs and thus a "gain", or "net return", 
of gi = mi — J2j Pjmji which could, of course, be negative. In order to avoid the 
possibility of the rrij's being chosen in such a way as to guarantee the negativity 
of the firm's for fixed p/s in this system of linear equations, it is necessary that the 
determinant of the matrix relating the m/s to the gt's be zero so that the linear 
system cannot be solved; this turns out to require that p\ + p2 + • • • + pn = 1- 
Moreover, it is easy to check that this is also a sufficient condition for coherence: 
it implies J2j Pj9j = 0> f°r any choice of the rrij 's, and hence the impossibility of 
all the returns being negative. 
The extension of these ideas to cover the revision of degrees of belief  
conditional on new information proceeds in a similar manner, except that an  
individual's degree of belief in an event E conditional on an event F is defined to 
be the number q such that, given any monetary sum m, we have the equivalence 
{qm 117} ~ {m\E n F,0\Ec n F,qm\ FC}, according to the individual's  
preference ordering among options. The interpretation of this definition is  
straightforward: having paid a stake of qm, if F occurs we are confronted with a gamble with 
prizes m if E occurs, and nothing otherwise; if F does not occur the bet is "called 
off' and the stake returned. 
However, despite the intuitive appeal of this simple and neat approach, it has 
two major shortcomings from an operational viewpoint. 
In the first place, it is clear that the definitions cannot be taken seriously in 
terms of arbitrary monetary sums: the "perceived value" of a stake or a return is 
not equivalent to its monetary value and the missing "utility" concept is required in 
order to overcome the difficulty. This point was later recognised by de Finetti (see 
Kyburg and Smokier, 1964/1980, p. 62, footnote (a)), but has its earlier origins 
in the celebrated St. Petersburg paradox (first discussed in terms of utility by 
Daniel Bernoulli, 1730/1954). For further discussion of possible forms of "utility 
for money", see, for example, Pratt (1964), Lavalle (1968), Lindley (1971/1985, 
Chapter 5) and Hull et al. (1973). Additionally, one may explicitly recognise that 
some people have a positive utility for gambling (see, for instance, Conlisk, 1993). 
An ad hoc modification of de Finetti's approach would be to confine attention 
to "small" stakes (thus, in effect, restricting attention to a range of outcomes over 
which the "utility" can be taken as approximately linear) and the argument, thus 
modified, has considerable pedagogical and, perhaps, practical use, despite its rather 
informal nature. A more formal argument based on the avoidance of certain losses 
in betting formulations has been given by Freedman and Purves (1969). Related 
88 
2 Foundations 
arguments have also been used by Cornfield (1969), Heath and Sudderth (1972) 
and Buehler (1976) to expand on de Finetti's concept of coherent systems of bets. 
In addition to the problem of "non-linearity in the face of risk", alluded to 
above, there is also the difficulty that unwanted game-theoretic elements may enter 
the picture if we base a theory on ideas such as "opponents" choosing the levels of 
prizes in gambles. For this reason, de Finetti himself later preferred to use an  
approach based on scoring rules, a concept we have already introduced in Section 2.7. 
Scoring Rules and Degrees of Belief 
The scoring rule approach to the definition of degrees of belief and the derivation of 
their properties when constrained to be coherent is due to de Finetti (1963, 1964), 
with important subsequent generalisations by Savage (1971) and Lindley (1982a). 
In terms of the quadratic scoring rule, the development proceeds as follows. 
Given an uncertain event E, an individual is asked to select a number, p, with 
the understanding that if E occurs he or she is to suffer a penalty (or loss) of 
L = (1 — p)2, whereas if E does not occur he or she is to suffer a penalty of 
L = p2. Using the indicator function for E, the penalty can be written in the 
general form, L = (lg — p)2. The number, p, which the individual chooses is 
defined to be his or her degree of belief in E. 
Suppose now that E\, E2, ..., En are an exclusive and exhaustive collection of 
uncertain events for which the individual, using the quadratic scoring rule scheme, 
has to specify degrees of belief p\,p2, ■ ■ ■ ,pn, respectively, subject now to the 
penalty 
L = (l£l -Pl)2 + (l£2 -p2)2 + --- + (lEn -Pnf- 
Given a specification, Pi,p2,..-,pn, either it is possible to find an alternative 
specification, qx, q2,..., qn, say, such that 
E(i£i-*)2<B^-^2' 
for any assignment of the value 1 to one of the Ei's and 0 to the others, or it is 
not possible to find such q\, q2,..., qn. In the latter case, the individual is said 
to have specified a coherent set of degrees of belief. The underlying idea in this 
development is clearly very similar to that of de Finetti's (1937/1964) approach 
where the avoidance of a "Dutch book" is the basic criterion of coherence. 
A simple geometric argument now establishes that, for coherence we must 
have 0 < pi <1, fori = 1,2,... ,n, and p\ + pi H V pn = 1. To see this, note 
that the n logically compatible assignments of values 1 and 0 to the Ei's define 
n points in 5ft". Thinking of Pi,p2, ■ ■ ■ ,p„ as defining a further point in 5ft", the 
coherence condition can be reinterpreted as requiring that this latter point cannot 
be moved in such a way as to reduce the distance from all the other n points. This 
2.8 Discussion and Further References 
89 
means thatpi,P2, • • • ,Pn must define a point in the convex hull ofthe other n points, 
thus establishing the required result. 
The extension of this approach to cover the revision of degrees of belief  
conditional on new information proceeds as follows. An individual's degree of belief 
in an event E conditional on the occurrence of an event F is defined to be the 
number q, which he or she chooses when confronted with a penalty defined by 
L = If ( 1e — q)2- The interpretation of this penalty is straightforward. Indeed, 
if F occurs, the specification of q proceeds according to the penalty (lg — q)2; 
if F does not occur, there is no penalty, a formulation which is clearly related to 
the idea of "called-off" bets used in de Finetti's 1937 approach. Suppose now 
that, in addition to the conditional degree of belief q, the numbers p and r are the 
individual's degrees of belief, respectively, for the events E(~\ F and F, specified 
subject to the penalty 
L= 1F{1E -q)2 + {lElF ~P? + ( If - r)2. 
To derive the constraints on p, q and r imposed by coherence, which  
demands that no other choices will lead to a strictly smaller L, whatever the logically 
compatible outcomes of the events are, we argue as follows. 
If u, v, w, respectively, are the values which L takes in the cases where EnF, 
Ec n F and Fc occur, then p, q, r satisfy the equations 
u = (l-q)2 + (l-p)2 + (l-r)2 
v= q2+ p2 + (1 - r)2 
1 2 
w = p + r . 
If p, q, r defined a point in 5R3 where the Jacobian of the transformation defined 
by the above equations did not vanish, it would be possible to move from that point 
in a direction which simultaneously reduced the values u, v and w. Coherence 
therefore requires that the Jacobian be zero. A simple calculation shows that this 
reduces to the condition q = p/r, which is, again, Bayes' theorem. 
De Finetti's 'penalty criterion' and related ideas have been critically  
re-examined by a number of authors. Relevant additional references are Myerson (1979), 
Regazzini (1983), Gatsonis (1984), Eaton (1992) and Gilio (1992a). See, also, 
Piccinato(1986). 
Axiomatic Approaches to Degrees of Belief 
Historically, the idea of probability as "degree of belief' has received a great deal of 
distinguished support, including contributions from James Bernoulli (1713/1899), 
Laplace (1774/1986,1814/1952), De Morgan (1847) and Borel (1924/1964).  
However, so far as we know, none of these writers attempted an axiomatic development 
of the idea. 
The first recognisably "axiomatic" approach to a theory of degrees of belief 
was that of Bayes (1763) and the magnitude of his achievement has been clearly 
90 
2 Foundations 
recognised in the two centuries following his death by the adoption of the adjective 
Bayesian as a description of the philosophical and methodological developments 
which have been inspired, directly or indirectly, by his essay. 
By present day standards, Bayes' formulation is, of course, extremely  
informal, and a more formal, modern approach only began to emerge a century and a 
half later, in a series of papers by Wrinch and Jeffreys (1919,1921). Formal axiom 
systems which whole-heartedly embrace the principle of revising beliefs through 
systematic use of Bayes' theorem, are discussed in detail by Jeffreys (1931/1973, 
1939/1961), whose profound philosophical and methodological contributions to 
Bayesian statistics are now widely recognised; see for example, the evaluations 
of his work by Geisser (1980a), by Good (1980a) and by Lindley (1980a), in the 
volume edited by Zellner (1980). 
From a foundational perspective, however, the flavour of Jeffreys' approach 
seems to us to place insufficient emphasis on the inescapably personal nature of  
degrees of belief, resulting in an over-concentration on "conventional" representations 
of degrees of belief derived from "logical" rather than operational considerations 
(despite the fact that Jeffreys was highly motivated by real world applications!). 
Similar criticisms seem to us to apply to the original and elegant formal  
development given by Cox (1946,1961) and Jaynes (1958), who showed that the probability 
axioms constitute the only consistent extension of ordinary (Aristotelian) logic in 
which degrees of belief are represented by real numbers. 
We should point out, however, that our emphasis on operational considerations 
and the subjective character of degrees of belief would, in turn, be criticised by 
many colleagues who, in other respects, share a basic commitment to the Bayesian 
approach to statistical problems. See Good (1965, Chapter 2) for a discussion of the 
variety of attitudes to probability compatible with a systematic use of the Bayesian 
paradigm. 
There are, of course, many other examples of axiomatic approaches to  
quantifying uncertainty in some form or another. In the finite case, this includes work by 
Kraft et al. (1959), Scott (1964), Fishburn (1970, Chapter 4), Krantz etal. (1971), 
Domotor and Stelzer (1971), Suppes and Zanotti (1976,1982), Heath and Sudderth 
(1978) and Luce and Narens (1978). The work of Keynes (1921/1929) and Car- 
nap (1950/1962) deserves particular mention and will be further discussed later in 
Section 2.8.4. Fishburn (1986) provided an authoritative review of the axiomatic 
foundations of subjective probability, which is followed by a long, stimulating 
discussion. See, also, French (1982) and Chuaqui and Malitz (1983). 
Axiomatic Approaches to Utilities 
Assuming the prior existence of probabilities, von Neumann and Morgenstern 
(1944/1953) presented axioms for coherent preferences which led to a justification 
of utilities as numerical measures of value for consequences and to the optimality 
criterion of maximising expected utility. Much of Savage's (1954/1972) system 
2.8 Discussion and Further References 
91 
was directly inspired by this seminal work of von Neumann and Morgenstern and 
the influence of their ideas extends into a great many of the systems we have  
mentioned. Other early developments which concentrate on the utility aspects of the 
decision problem include those of Friedman and Savage (1948, 1952), Marschak 
(1950), Arrow (1951a), Herstein and Milnor (1953), Edwards (1954) and Debreu 
(1960). Seminal references are reprinted in Page (1968). General accounts of 
utility are given in the books by Blackwell and Girshick (1954), Luce and Raiffa 
(1957), Chernoff and Moses (1959) and Fishburn (1970). Extensive bibliographies 
are given in Savage (1954/1972) and Fishburn (1968, 1981). 
Discussions of the experimental measurement of utility are provided by  
Edwards (1954), Davison et al. (1957), Suppes and Walsh (1959), Becker et al. (1963), 
DeGroot (1963), Becker and McClintock (1967), Savage (1971) and Hull et al. 
(1973). DeGroot (1970, Chapter 7) presents a general axiom system for utilities 
which imposes rather few mathematical constraints on the underlying decision 
problem structure. Multiattribute utility theory is discussed, among others, by 
Fishburn (1964) and Keeney and Raiffa (1976). Other discussions of utility theory 
include Fishburn (1967a, 1988b) and Machina (1982, 1987). See, also, Schervish 
etal. (1990). 
Information Theories 
Measures of information are closely related to ideas of uncertainty and probability 
and there is a considerable literature exploring the connections between these topics. 
The logarithmic information measure was proposed independently by Shannon 
(1948) and Wiener (1948) in the context of communication engineering; Lindley 
(1956) later suggested its use as a statistical criterion in the design of experiments. 
The logarithmic divergency measure was first proposed by Kullback and Leibler 
(1951) and was subsequently used as the basis for an information-theoretic approach 
to statistics by Kullback (1959/1968). A formal axiomatic approach to measures 
of information in the context of uncertainty was provided by Good (1966), who 
has made numerous contributions to the literature of the foundations of decision 
making and the evaluation of evidence. Other relevant references on information 
concepts are Renyi (1964, 1966, 1967) and Sarndal (1970). 
The mathematical results which lead to the characterisation of the logarithmic 
scoring rule for reporting probability distributions have been available for some 
considerable time. Logarithmic scores seem to have been first suggested by Good 
(1952), but he only dealt with dichotomies, for which the uniqueness result is not 
applicable. The first characterisation of the logarithmic score for a finite distribution 
was attributed to Gleason by McCarthy (1956); Aczel and Pfanzagl (1966), Arimoto 
(1970) and Savage (1971) have also given derivations of this form of scoring rule 
under various regularity conditions. 
By considering the inference reporting problem as a particular case of a  
decision problem, we have provided (in Section 2.7) a natural, unifying account of 
92 
2 Foundations 
the fundamental and close relationship between information-theoretic ideas and 
the Bayesian treatment of "pure inference" problems. Based on work of Bernardo 
(1979a), this analysis will be extended, in Chapter 3, to cover continuous  
distributions. 
2.8.4 Critical Issues 
We shall conclude this chapter by providing a summary overview of our position 
in relation to some of the objections commonly raised against the foundations of 
Bayesian statistics. These will be dealt with under the following subheadings: (i) 
Dynamic Frame of Discourse, (ii) Updating Subjective Probability, (iii) Relevance 
of an Axiomatic Approach, (iv) Structure of the Set of Relevant Events, (v)  
Prescriptive Nature of the Axioms, (vi) Precise, Complete, Quantitative Preference, 
(vii) Subjectivity of Probability, (viii) Statistical Inference as a Decision Problem 
and (ix) Communication and Group Decision Making. 
Dynamic Frame of Discourse 
As we indicated in Chapter 1, our concern in this volume is with coherent beliefs 
and actions in relation to a limited set of specified possibilities, currently assumed 
necessary and sufficient to reflect key features of interest in the problem under 
study. In the language of Section 2.2, we are operating in terms of a fixed frame 
of discourse, defined in the light of our current knowledge and assumptions, M0. 
However, as many critics have pointed out, this activity constitutes only one static 
phase of the wider, evolving, scientific learning and decision process. In the more 
general, dynamic, context, this activity has to be viewed, either potentially or 
actually, as sandwiched between two other vital processes. On the one hand, the 
creative generation of the set of possibilities to be considered; on the other hand, the 
critical questioning of the adequacy of the currently entertained set of possibilities 
(see, for example, Box, 1980). We accept that the mode of reasoning encapsulated 
within the quantitative coherence theory as presented here is ultimately conditional, 
and thus not directly applicable to every phase of the scientific process. But we 
do not accept, as Box (1980) appears to, that alternative formal statistical theories 
have a convincing, complementary role to play. 
The problem of generating the frame of discourse, i.e., inventing new  
models or theories, seems to us to be one which currently lies outside the purview of 
any "statistical" formalism, although some limited formal clarification is actually 
possible within the Bayesian framework, as we shall see in Chapter 4. Substantive 
subject-matter inputs would seem to be of primary importance, although  
informal, exploratory data analysis is no doubt a necessary adjunct and, particularly 
in the context of the possibilities opened up by modern computer graphics, offers 
considerable intellectual excitement and satisfaction in its own right. 
2.8 Discussion and Further References 
93 
The problem of criticising the frame of discourse also seems to us to remain 
essentially unsolved by any "statistical" theory. In the case of a "revolution", or 
even "rebellion", in scientific paradigm (Kuhn, 1962), the issue is resolved for 
us as statisticians by the consensus of the subject-matter experts, and we simply 
begin again on the basis of the frame of discourse implicit in the new paradigm. 
However, in the absence of such "externally" directed revision or extension of the 
current frame of discourse, it is not clear what questions one should pose in order 
to arrive at an "internal" assessment of adequacy in the light of the information thus 
far available. 
On the one hand, exploratory diagnostic probing would seem to have a role to 
play in confirming that specific forms of local elaboration of the frame of discourse 
should be made. The logical catch here, however, is that such specific diagnostic 
probing can only stem from the prior realisation that the corresponding specific 
elaborations might be required. The latter could therefore be incorporated ab initio 
into the frame of discourse and a fully coherent analysis carried out. The issue 
here is one of pragmatic convenience, rather than of circumscribing the scope of 
the coherent theory. 
On the other hand, the issue of assessing adequacy in relation to a total absence 
of any specific suggested elaborations seems to us to remain an open problem. 
Indeed, it is not clear that the "problem" as usually posed is well-formulated. For 
example, is the key issue that of "surprise"; or is some kind of extension of the 
notion of a decision problem required in order to give an operational meaning to 
the concept of "adequacy"? 
Readers interested in this topic will find in Box (1980), and the ensuing  
discussion, a range of reactions. We shall return to these issues in Chapter 6. Related 
issues arise in discussions of the general problem of assessing, or "calibrating", 
the external, empirical performance of an internally coherent individual; see, for 
example, Dawid (1982a). 
Overall, our responses to critics who question the relevance of the coherent 
approach based on a fixed frame of reference can be summarised as follows. So 
far as the scope and limits of Bayesian theory are concerned: (i) we acknowledge 
that the mode of reasoning encapsulated within the quantitative coherence theory 
is ultimately conditional, and thus not directly applicable to every phase of the 
scientific process; (ii) informal, exploratory techniques are an essential part of the 
process of generating ideas; there can be no purely "statistical" theory of model 
formulation; this aspect of the scientific process is not part of the foundational 
debate, although the process of passing from such ideas to their mathematical 
representation can often be subjected to formal analysis; (iii) we all lack a decent 
theoretical formulation of and solution to the problem of global model criticism in 
the absence of concrete suggested alternatives. 
However, critics of the Bayesian approach should recognise that: (i) an  
enormous amount of current theoretical and applied statistical activity is concerned 
94 
2 Foundations 
with the analysis of uncertainty in the context of models which are accepted, for 
the purposes of the analysis, as working frames of discourse, subject only to local 
probing of specific potential elaborations, and (ii) our arguments thus far, and those 
to follow, are an attempt to convince the reader that within this latter context there 
are compelling reasons for adopting the Bayesian approach to statistical theory and 
practice. 
Updating Subjective Probability 
An issue related to the topic just discussed is that of the mechanism for updating 
subjective probabilities. 
In Section 2.4.2, we defined, in terms of a conditional uncertainty relation, 
<G, the notion of the conditional probability, P(E \ G), of an event E given the 
assumed occurrence of an event G. From this, we derived Bayes' theorem, which 
establishes thatp(£ | G) = P(G \ E)P(E)/P(G). If we actually know for certain 
that G has occurred, P(E\G) becomes our actual degree of belief in E. The prior 
probability P(E), has been updated to the posterior probability P(E \ G). 
However, a number of authors have questioned whether it is justified to identify 
assessments made conditional on the assumed occurrence of G with actual beliefs 
once G is known. We shall not pursue this issue further, although we acknowledge 
its interest and potential importance. Detailed discussion and relevant references 
can be found in Diaconis and Zabell (1982), who discuss, in particular, Jeffrey's 
rule (Jeffrey, 1965/1983), and Goldstein (1985), who examines the role of temporal 
coherence. See, also, Good (1977). 
Relevance of the Axiomatic Approach 
Arguments against over-concern with foundational issues come in many forms. 
At one extreme, we have heard Bayesian colleagues argue that the mechanics and 
flavour of the Bayesian inference process have their own sufficient, direct, intuitive 
appeal and do not need axiomatic reinforcement. Another form of this argument 
asserts that developments from axiom systems are "pointless" because the  
conclusions are, tautologically, contained in the premises. Although this is literally 
true, we simply do not accept that the methodological imperatives which flow from 
the assumptions of quantitative coherence are in any way "obvious" to someone 
contemplating the axioms. At the other extreme, we have heard proponents of 
supposedly "model-free" exploratory methodology proclaim that we can evolve 
towards "good practice" by simply giving full encouragement to the creative  
imagination and then "seeing what works". 
Our objection to both these attitudes is that they each implicitly assume, albeit 
from different perspectives, the existence of a commonly agreed notion of what 
constitutes "desirable statistical practice". This does not seem to us a reasonable 
assumption at all, and to avoid potential confusion, an operational definition of the 
notion is required. The quantitative coherence approach is based on the assumption 
2.8 Discussion and Further References 
95 
that, within the structured framework set out in Section 2.2, desirable practice 
requires, at least, to avoid Dutch-book inconsistencies, an assumption which leads 
to the Bayesian paradigm for the revision of belief. 
Structure of the Set of Relevant Events 
But is the structure assumed for the set of relevant events too rigid? In particular, 
is it reasonable to assume that, in each and every context involving uncertainty, 
the logical description of the possibilities should be forced into the structure of 
an algebra (or cr-algebra), in which each event has the same logical status? It 
seems to us that this may not always be reasonable and that there is a potential 
need for further research into the implications of applying appropriate concepts of 
quantitative coherence to event structures other than simple algebras. For example, 
this problem has already been considered in relation to the foundations of quantum 
mechanics, where the notion of "sample space" has been generalised to allow for 
the simultaneous representation of the outcomes of a set of "related" experiments 
(see, for example, Randall and Foulis, 1975). In that context, it has been established 
that there exists a natural extension of the Bayesian paradigm to the more general 
setting. 
Another area where the applicability of the standard paradigm has been  
questioned is that of so-called "knowledge-based expert systems", which often operate 
on knowledge representations which involve complex and loosely structured spaces 
of possibilities, including hierarchies and networks. Proponents of such systems 
have argued that (Bayesian) probabilistic reasoning is incapable of analysing these 
structures and that novel forms of quantitative representations of uncertainty are 
required (see Spiegelhalter and Knill-Jones, 1984, and ensuing discussion, for  
references to these ideas). However, alternative proposals, which include "fuzzy logic", 
"belief functions" and "confirmation theory", are, for the most part, ad hoc and 
the challenge to the probabilistic paradigm seems to us to be elegantly answered 
by Lauritzen and Spiegelhalter (1988). We shall return to this topic later in this 
section. 
Finally, another form of query relating to the logical status of events is  
sometimes raised (see, for example, Barnard, 1980a). This draws attention to the in- 
terpretational asymmetry between a statement like "the underlying distribution is 
normal" and its negation. This raises questions about their implicitly symmetric 
treatment within the framework given in Section 2.2. Choices of the elements to 
be included in £ are, of course, bound up with general questions of "modelling" 
and the issue here seems to us to be one concerning sensible modelling strategies. 
We shall return to this topic in Chapters 4 and 6. 
Prescriptive Nature of the Axioms 
When introducing our formal development, we emphasised that the Bayesian  
foundational approach is prescriptive and not descriptive. We are concerned with un- 
96 
2 Foundations 
derstanding how we ought to proceed, if we wish to avoid a specified form of 
behavioural inconsistency. We are not concerned with sociological or  
psychological description of actual behaviour. For the latter, see, for example, Wallsten (1974), 
Kahneman and Tversky (1979), Kahneman etal. (1982), Machina (1987), Bordley 
(1992), Luce (1992) and Yilmaz (1992). See, also, Savage (1980). 
Despite this, many critics of the Bayesian approach have somehow taken 
comfort from the fact that there is empirical evidence, from experiments involving 
hypothetical gambles, which suggests that people often do not act in conformity 
with the coherence axioms; see, for example, Allais (1953) and Ellsberg (1961). 
Allais' criticism is based on a study of the actual preferences of individuals 
in contexts where they are faced with pairs of hypothetical situations, like those 
described in Figure 2.8, in each of which a choice has to be made between the two 
options where C stands for current assets and the numbers describe thousands of 
units of a familiar currency. 
1.00 
Situation 1 
500 + C 
010 2500 + C 
0.89 
0.01 
03, 
Situation 2 ^*r ^ Q.g9 
0.11 
500 + C 
c 
500 + C 
C 
&IP_ 2500 + C 
0.90 
C 
Figure 2.8 An illustration of Allais' paradox 
It has been found (see, for example, Allais and Hagen, 1979) that there are a 
great many individuals who prefer option 1 to option 2 in the first situation, and at 
the same time prefer option 4 to option 3 in the second situation. 
To examine the coherence of these two revealed preferences, we note that, 
if they are to correspond to a consistent utility ordering, there must exist a utility 
2.8 Discussion and Further References 
97 
function u(.), defined over consequences (in this case, total assets in thousands of 
monetary units), satisfying the inequalities 
u(500 + C) > 0.10 u(2,500 + C) + 0.89 u(500 + C) + 0.01 u(C) 
0.10u(2,500 + C) + 0.90u(C) > 0.11 u(500 + C) + 0.89u(C). 
But simple rearrangement reveals that these inequalities are logically  
incompatible for any function u(.), and, therefore, the stated preferences are incoherent. 
How should one react to this conflict between the compelling intuitive  
attraction (for many individuals) of the originally stated preferences, and the realisation 
that they are not in accord with the prescriptive requirements of the formal theory? 
Allais and his followers would argue that the force of examples of this kind is so 
powerful that it undermines the whole basis of the axiomatic approach set out in 
Section 2.3. This seems to us a very peculiar argument. It is as if one were to argue 
for the abandonment of ordinary logical or arithmetic rules, on the grounds that 
individuals can often be shown to perform badly at deduction or long division. 
The conclusion to be drawn is surely the opposite: namely, the more liable 
people are to make mistakes, the more need there is to have the formal prescription 
available, both as a reference point, to enable us to discover the kinds of mistakes 
and distortions to which we are prone in ad hoc reasoning, and also as a suggestive 
source of improved strategies for thinking about and structuring problems. 
Table 2.4 Savage's reformulation of Allais' example 
situation 1 
situation 2 
Ticket number 
option 1 
option 2 
option 3 
option 4 
1 
500 + C 
C 
500 +C 
C 
2-11 
500+ C 
2500 + C 
500+ C 
2500 + C 
12-100 
500+ C 
500 + C 
C 
C 
In the case of Allais' example, Savage (1954/1972, Chapter 5) pointed out 
that a concrete realisation of the options described in the two situations could be 
achieved by viewing the outcomes as prizes from a lottery involving one hundred 
numbered tickets, as shown in Table 2.4. Indeed, when the problem is set out in 
this form, it is clear that if any of the tickets numbered from 12 to 100 is chosen it 
will not matter, in either situation, which of the options is selected. Preferences in 
both situations should therefore only depend on considerations relating to tickets in 
the range from 1 to 11. But, for this range of tickets, situations 1 and 2 are identical 
in structure, so that preferring option 1 to option 2 and at the same time preferring 
option 4 to option 3 is now seen to be indefensible. 
98 
2 Foundations 
Viewed in this way, Allais' problem takes on the appearance of a decision- 
theoretic version of an "optical illusion" achieved through the distorting effects of 
"extreme" consequences, which go far beyond the ranges of our normal experience. 
The lesson of Savage's analysis is that, when confronted with complex or tricky 
problems, we must be prepared to shift our angle of vision in order to view the 
structure in terms of more concrete and familiar images with which we feel more 
comfortable. 
Ellsberg's (1961) criticism is of a similar kind to Allais', but the "distorting" 
elements which are present in his hypothetical gambles stem from the rather vague 
nature of the uncertainty mechanisms involved, rather than from the extreme nature 
of the consequences. In such cases, where confusion is engendered by the  
probabilities rather than the utilities, the perceived incoherence may, in fact, disappear 
if one takes into account the possibility that the experimental subjects' utility may 
be a function of more than one attribute. In particular, we may need to consider 
the attribute "avoidance of looking foolish", often as a result of thinking that there 
is a "right answer" if the problem seems predominantly to do with sorting out 
"experimentally assigned" probabilities, in addition to the monetary consequences 
specified in the hypothetical gambles. Even without such refinements, however, 
and arguing solely in terms of the gambles themselves, Raiffa (1961) and Roberts 
(1963) have provided clear and convincing rejoinders to the Ellsberg criticism.  
Indeed, Roberts presents a particularly lucid and powerful defence of the axioms, 
also making use of the analogy with "optical" and "magical" illusions. The form 
of argument used is similar to that in Savage's rejoinder to Allais, and we shall 
not repeat the details here. For a recent discussion of both the Allais and Ellsberg 
phenomena, see Kadane (1992). 
Precise, Complete, Quantitative Preferences 
In our axiomatic development we have not made the a priori assumption that all 
options can be compared directly using the preference relation. We have,  
however, assumed, in Axiom 5, that all consequences and certain general forms of 
dichotomised options can be compared with dichotomised options involving  
standard events. This latter assumption then turns out to imply a quantitative basis for 
all preferences, and hence for beliefs and values. 
The view has been put forward by some writers (e.g. Keynes, 1921/1929, and 
Koopman, 1940) that not all degrees of belief are quantifiable, or even comparable. 
However, beginning with Jeffreys' review of Keynes' Treatise (see also Jeffreys, 
1931/1973) the general response to this view has been that some form of  
quantification is essential if we are to have an operational, scientifically useful theory. Other 
references, together with a thorough review of the mathematical consequences of 
these kind of assumptions, are given by Fine (1973, Chapter 2). 
Nevertheless, there has been a widespread feeling that the demand for precise 
quantification, implicit in "standard" axiom systems, is rather severe and certainly 
2.8 Discussion and Further References 
99 
ought to be questioned. We should consider, therefore, some of the kinds of  
suggestions that have been put forward from this latter perspective. 
Among the attempts to present formal alternatives to the assumption of  
precise quantification are those of Good (1950, 1962), Kyburg (1961), Smith (1961), 
Dempster (1967,1985), Walley and Fine (1979), Gir6n and Rios (1980), DeRober- 
tis and Hartigan (1981), Walley (1987, 1991) and Nakamura (1993). In essence, 
the suggestion in relation to probabilities is to replace the usual representation of 
a degree of belief in terms of a single number, by an interval defined by two  
numbers, to be interpreted as "upper" and "lower" probabilities. So far as decisions are 
concerned, such theories lead to the identification of a class of "would-be" actions, 
but provide no operational guidance as to how to choose from among these.  
Particular ideas, such as Dempster's (1968) generalization of the Bayesian inference 
mechanism, have been shown to be suspect (see, for example, Aitchison, 1968), but 
have led on themselves to further generalizations, such as Shafer's (1976, 1982a) 
theory of "belief functions". This has attracted some interest (see e.g., Wasserman 
(1990a, 1990b), but its operational content has thus far eluded us. 
In general, we accept that the assumption of precise quantification, i.e., that 
comparisons with standard options can be successively refined without limit, is 
clearly absurd if taken literally and interpreted in a descriptive sense. We therefore 
echo our earlier detailed commentary on Axiom 5 in Section 2.3, to the effect 
that these kinds of proposed extension of the axioms seem to us to be based on 
a confusion of the descriptive and the prescriptive and to be largely unnecessary. 
It is rather as though physicists and surveyors were to feel the need to rethink 
their practices on the basis of a physical theory incorporating explicit concepts of 
upper and lower lengths. We would not wish, however, to be dogmatic about this. 
Our basic commitment is to quantitative coherence. The question of whether this 
should be precise, or allowed to be imprecise, is certainly an open, debatable one, 
and it might well be argued that "measurement" of beliefs and values is not totally 
analogous to that of physical "length". An obvious, if often technically involved 
solution, is to consider simultaneously all probabilities which are compatible with 
elicited comparisons. This and other forms of "robust Bayesian" approaches will 
be reviewed in Section 5.6.3. In this work, we shall proceed on the basis of a 
prescriptive theory which assumes precise quantification, but then pragmatically 
acknowledges that, in practice, all this should be taken with a large pinch of salt and 
a great deal of systematic sensitivity analysis. For a related practical discussion, 
see Hacking (1965). See, also, Chateaneuf and Jaffray (1984). 
Subjectivity of Probability 
As we stressed in Section 2.2, the notion of preference between options, the  
primitive operational concept which underlies all our other definitions, is to be understood 
as personal, in the sense that it derives from the response of a particular individual 
to a decision making situation under uncertainty. A particular consequence of this 
100 
2 Foundations 
is that the concept which emerges is personal degree of belief, defined in Section 2.4 
and subsequently shown to combine for compound events in conformity with the 
properties of a finitely additive probability measure. 
The "individual" referred to above could, of course, be some kind of group, 
such as a committee, provided the latter had agreed to "speak with a single voice", 
in which case, to the extent that we ignore the processes by which the group arrives 
at preferences, it can conveniently be regarded as a "person". Further comments 
on the problem of individuals versus groups will be given later under the heading 
Communication and Group Decision Making. 
This idea that personal (or subjective) probability should be the key to the 
"scientific" or "rational" treatment of uncertainty has proved decidedly unpalatable 
to many statisticians and philosophers (although in some application areas, such as 
actuarial science, it has met with a more favourable reception; see Clarke, 1954). At 
the very least, it appears to offend directly against the general notion that the  
methods of science should, above all else, have an "objective" character. Nevertheless, 
bitter though the subjectivist pill may be, and admittedly difficult to swallow, the 
alternatives are either inert, or have unpleasant and unexpected side-effects or, to 
the extent that they appear successful, are found to contain subjectivist ingredients. 
From the objectivistic standpoint, there have emerged two alternative kinds 
of approach to the definition of "probability" both seeking to avoid the subjective 
degree of belief interpretation. The first of these retains the idea of probability as 
measurement of partial belief, but rejects the subjectivist interpretation of the latter, 
regarding it, instead, as a unique degree of partial logical implication between one 
statement and another. The second approach, by far the most widely accepted 
in some form or another, asserts that the notion of probability should be related 
in a fundamental way to certain "objective" aspects of physical reality, such as 
symmetries or frequencies. 
The logical view was given its first explicit formulation by Keynes (1921/1929) 
and was later championed by Carnap (1950/1962) and others; it is interesting to note, 
however, that Keynes seems subsequently to have changed his view and  
acknowledged the primacy of the subjectivist interpretation (see Good, 1965, Chapter 2). 
Brown (1993) proposes the related concept of "impersonal" probability. 
From an historical point of view, the first systematic foundation of the frequen- 
tist approach is usually attributed to Venn (1886), with later influential contributions 
from von Mises (1928) and Reichenbach (1935). The case for the subjectivist  
approach and against the objectivist alternatives can be summarised as follows. 
The logical view is entirely lacking in operational content. Unique probability 
values are simply assumed to exist as a measure of the degree of implication between 
one statement and another, to be intuited, in some undefined way, from the formal 
structure of the language in which these statements are presented. 
The symmetry (or classical) view asserts that physical considerations of  
symmetry lead directly to a primitive notion of "equally likely cases". But any uncertain 
2.8 Discussion and Further References 
101 
situation typically possesses many plausible "symmetries": a truly "objective"  
theory would therefore require a procedure for choosing a particular symmetry and for 
justifying that choice. The subjectivist view explicitly recognises that regarding 
a specific symmetry as probabilistically significant is itself, inescapably, an act of 
personal j udgement. 
The frequency view can only attempt to assign a measure of uncertainty to 
an individual event by embedding it in an infinite class of "similar" events having 
certain "randomness" properties, a "collective" in von Mises' (1928) terminology, 
and then identifying "probability" with some notion of limiting relative frequency. 
But an individual event can be embedded in many different "collectives" with no 
guarantee of the same resulting limiting relative frequencies: a truly "objective" 
theory would therefore require a procedure for justifying the choice of a particular 
embedding sequence. Moreover, there are obvious difficulties in defining the  
underlying notions of "similar" and "randomness" without lapsing into some kind of 
circularity. The subjectivist view explicitly recognises that any assertion of  
"similarity" among different, individual events is itself, inescapably, an act of personal 
judgement, requiring, in addition, an operational definition of which is meant by 
"similar". 
In fact, this latter requirement finds natural expression in the concept of an 
exchangeable sequence of events, which we shall discuss at length in Chapter 4. 
This concept, via the celebrated de Finetti representation theorem, provides an 
elegant and illuminating explanation, from an entirely subjectivistic perspective, of 
the fundamental role of symmetries and frequencies in the structuring and evaluation 
of personal beliefs. It also provides a meaningful operational interpretation of the 
word "objective" in terms of "intersubjective consensus". 
The identification of probability with frequency or symmetry seems to us to 
be profoundly misguided. It is of paramount importance to maintain the distinction 
between the definition of a general concept and the evaluation of a particular 
case. In the subjectivist approach, the definition derives from logical notions of 
quantitative coherent preferences: practical evaluations in particular instances often 
derive from perceived symmetries and observed frequencies, and it is only in this 
evaluatory process that the latter have a role to play. 
The subjectivist point of view outlined above is, course, not new and has been 
expounded at considerable length and over many years by a number of authors. The 
idea of probability as individual "degree of confidence" in an event whose outcome 
is uncertain seems to have been first put forward by James Bernoulli (1713/1899). 
However, it was not until Thomas Bayes' (1763) famous essay that it was explicitly 
used as a definition: 
The probability of any event is the ratio between the value at which an expectation 
depending on the happening of the event ought to be computed, and the value of 
the thing expected upon its happening. 
102 
2 Foundations 
Not only is this directly expressed in terms of operational comparisons of 
certain kinds of simple options on the basis of expected values, but the style of 
Bayes' presentation strongly suggests that these expectations were to be interpreted 
as personal evaluations. 
A number of later contributions to the field of subjective probability are 
collected together and discussed in the volume edited by Kyburg and Smokier 
(1964/1980), which includes important seminal papers by Ramsey (1926) and 
de Finetti (1937/1964). An exhaustive and profound discussion of all aspects 
of subjective probability is given in de Finetti's magisterial Theory of  
Probability (1970/1974, 1970/1975). Other interpretations of probability are discussed in 
Renyi (1955), Good (1959), Kyburg (1961, 1974), Fishburn (1964), Fine (1973), 
Hacking (1975), de Finetti (1978), Walley and Fine (1979) and Shafer (1990). 
Statistical Inference as a Decision Problem 
Stylised statistical problems have often been approached from a decision-theoretical 
viewpoint; see, for instance, the books by Ferguson (1967), DeGroot (1970), Bar- 
nett(1973/1982), Berger( 1985a) and references therein. However, we have already 
made clear that, in our view, the supposed dichotomy between inference and  
decision is illusory, since any report or communication of beliefs following the receipt 
of information inevitably itself constitutes a form of action. In Section 2.7, we 
formalised this argument and characterised the utility structure that is typically  
appropriate for consequences in the special case of a "pure inference" problem. The 
expected utility of an "experiment" in this context was then seen to be identified 
with expected information (in the Shannon sense), and a number of information- 
theoretic ideas and their applications were given a unified interpretation within a 
purely subjectivist Bayesian framework. 
Many approaches to statistical inference do not, of course, assign a primary role 
to reporting probability distributions, and concentrate instead on stylised estimation 
and hypothesis testing formulations of the problem (see Appendix B, Section 3). 
We shall deal with these topics in more detail in Chapters 5 and 6. 
Communication and Group Decision Making 
The Bayesian approach which has been presented in this chapter is predicated on the 
primitive notion of individual preference. A seemingly powerful argument against 
the use of the Bayesian paradigm is therefore that it provides an inappropriate 
basis for the kinds of interpersonal communication and reporting processes which 
characterise both public debate about beliefs regarding scientific and social issues, 
and also "cohesive-small-group" decision making processes. We believe that the 
two contexts, "public" and "cohesive-small-group", pose rather different problems, 
requiring separate discussion. 
2.8 Discussion and Further References 
103 
In the case of the revision and communication of beliefs in the context of 
general scientific and social debate, we feel that criticism of the Bayesian paradigm 
is largely based on a misunderstanding of the issues involved, and on an  
oversimplified view of the paradigm itself, and the uses to which it can be put. So far as 
the issues are concerned, we need to distinguish two rather different activities: on 
the one hand, the prescriptive processes by which we ought individually to revise 
our beliefs in the light of new information if we aspire to coherence; on the other 
hand, the pragmatic processes by which we seek to report to and share perceptions 
with others. The first of these processes leads us inescapably to the conclusion that 
beliefs should be handled using the Bayesian paradigm; the second reminds us that 
a "one-off' application of the paradigm to summarise a single individual's revision 
of beliefs is inappropriate in this context. 
But, so far as we are aware, no Bayesian statistician has ever argued that the 
latter would be appropriate. Indeed, the whole basis of the subjectivist philosophy 
predisposes Bayesians to seek to report a rich range of the possible belief mappings 
induced by a data set, the range being chosen both to reflect (and even to challenge) 
the initial beliefs of a range of interested parties. Some discussion of the Bayesian 
reporting process may be found in Dickey (1973), Dickey and Freeman (1975) 
and Smith (1978). Further discussion is given in Smith (1984), together with a 
review of the connections between this issue and the role of models in facilitating 
communication and consensus. This latter topic will be further considered in 
Chapter 4. 
We concede that much remains to be done in developing Bayesian reporting 
technology, and we conjecture that modern interactive computing and graphics 
will have a major role to play. Some of the literature on expert systems is relevant 
here; see, for instance, Lindley (1987), Spiegelhalter (1987) and Gaul and Schader 
(1988). On the broader issue, however, one of the most attractive features of the 
Bayesian approach is its recognition of the legitimacy of the plurality of (coherently 
constrained) responses to data. Any approach to scientific inference which seeks to 
legitimise an answer in response to complex uncertainty seems to us a totalitarian 
parody of a would-be rational human learning process. 
On the other hand, in the "cohesive-small-group" context there may be an 
imposed need for group belief and decision. A variety of problems can be isolated 
within this framework, depending on whether the emphasis is on combining  
probabilities, or utilities, or both; and on how the group is structured in relation to such 
issues as "democracy", "information-sharing", "negotiation" or "competition". It 
is not yet clear to us whether the analyses of these issues will impinge directly on 
the broader controversies regarding scientific inference methodology, and so we 
shall not attempt a detailed review of the considerable literature that is emerging. 
Useful introductions to the extensive literature on amalgamation of beliefs or 
utilities, together with most of the key references, are provided by Arrow (1951b), 
Edwards (1954), Luce and Raiffa (1957), Luce (1959), Stone (1961), Blackwell 
104 
2 Foundations 
and Dubins (1962), Fishburn (1964,1968,1970,1987), Kogan and Wallace (1964), 
Wilson (1968), Winkler (1968, 1981), Sen (1970), Kranz et al. (1971), Marschak 
and Radner (1972), Cochrane and Zeleny (1973), DeGroot (1974, 1980),  
Morris (1974), White and Bowen (1975), White (1976a, 1976b), Press (1978, 1980b, 
1985b), Lindley et al. (1979), Roberts (1979), Hogarth (1980), Saaty (1980), Berger 
(1981), French (1981, 1985, 1986, 1989), Hylland and Zeckhauser (1981), Weer- 
ahandi and Zidek (1981, 1983), Brown and Lindley (1982, 1986), Chankong and 
Haimes (1982), Edwards and Newman (1982), DeGroot and Feinberg (1982,1983, 
1986), Raiffa (1982), French et al. (1983), Lindley (1983, 1985, 1986), Bunn 
(1984),Caroetal. (1984),Genest(1984a, 1984b), Yu( 1985), De Waaletal. (1986), 
Genest and Zidek (1986), Arrow and Raynaud (1987), Clemen and Winkler (1987, 
1993), Kim and Roush (1987), Barlow etal. (1988), Bayarri and DeGroot (1988, 
1989,1991), Huseby (1988), West (1988,1992a), Clemen (1989,1990), Rios etal. 
(1989), Seidenfeld etal. (1989), Rios (1990), DeGroot and Mortera (1991), Kelly 
(1991), Lindley and Singpurwalla (1991, 1993), Goel et al. (1992), Goicoechea 
et al. (1992), Normand and Tritchler (1992) and Gilardoni and Clayton (1993). 
Important, seminal papers are reproduced in Gardenfors and Sahlin (1968). For 
related discussion in the context of policy analysis, see Hodges (1987). 
References relating to the Bayesian approach to game theory include Harsany 
(1967), DeGroot and Kadane (1980), Eliashberg and Winkler (1981), Kadane 
and Larkey (1982, 1983), Raiffa (1982), Wilson (1986), Aumann (1987), Smith 
(1988b), Nau and McCardle (1990), Young and Smith (1991), Kadane and  
Seidenfeld (1992) and Keeney (1992). 
A recent review of related topics, followed by an informative discussion, is 
provided by Kadane (1993). 
105 
Chapter 3 
Generalisations 
Summary 
The ideas and results of Chapter 2 are extended to a much more general  
mathematical setting. An additional postulate concerning the comparison of a countable 
collection of events is introduced and is shown to provide a justification for  
restricting attention to countably additive probability as the basis for representing 
beliefs. The elements of mathematical probability theory are reviewed. The  
notions of options and utilities are extended to provide a very general mathematical 
framework for decision theory. A further additional postulate regarding  
preferences is introduced, and is shown to justify the criterion of maximising expected 
utility within this more general framework. In the context of inference problems, 
generalised definitions of score functions and of measures of information and 
discrepancy are given. 
3.1 GENERALISED REPRESENTATION OF BELIEFS 
3.1.1 Motivation 
The developments of Chapter 2, based on Axioms 1 to 5, led to the fundamental 
result that quantitatively coherent degrees of belief for events belonging to the 
algebra £ should fit together in conformity with the mathematical rules of finitely 
106 
3 Generalisations 
additive probability. From a directly practical and intuitive point of view, there 
seems no compelling reason to require anything beyond this finitistic framework, a 
view argued forcefully, and in great detail, by de Finetti (1970/1974, 1970/1975). 
However, there are many situations where the implied necessity of choosing 
a particular finitistic representation of a problem can lead to annoying conceptual 
and mathematical complications, as we remarked in Section 2.5.1 when discussing 
bounded sets of consequences. The example given in that context involved the 
problem of representing the length of remaining life of a medical patient. Most 
people would accept that there is an implicit upper bound, but find it difficult 
to justify any particular choice of its value. Similar problems obviously arise in 
representing other forms of survival time (of equipment, transplanted organs, or 
whatever) and further difficulties occur in representing the possible outcomes of 
many other measurement processes, since these are generally regarded as being 
on a continuous scale. For these reasons, and provided we do not feel that in so 
doing we are distorting essential features of our beliefs, it is certainly attractive, 
from the point of view of descriptive and mathematical convenience, to consider 
the possibility of extending our ideas beyond the finite, discrete framework. 
In Section 3.1.2, we shall provide a formal extension of the quantitative  
coherence theory to the infinite domain. Our fundamental conclusion about beliefs in this 
setting will be that quantitatively coherent degrees of belief for events belonging 
to a a-algebra £ should fit together in conformity with the mathematical rules of 
countably additive probability. 
The major mathematical advantage of this generalised framework is that all 
the standard manipulative tools and results of mathematical probability theory then 
become available to us; convenient references are, for example, Kingman and Taylor 
(1966) or Ash (1972). A selection of these tools and results will be reviewed in 
Section 3.2, and then used in Section 3.3 to develop natural extensions of our 
finitistic definitions of actions and utilities, thus establishing an extremely general 
mathematical setting for the representation and analysis of decision problems. The 
important special case of inference as a decision problem will be considered in 
Section 3.4, which extends to the general mathematical framework the discussion 
of the finite, discrete case, given in Section 2.7. Finally, a discussion of some 
particular issues is given in Section 3.5. 
3.1.2 Countable Additivity 
In Definition 2.1 and the subsequent discussion, we assumed that the collection £ 
of events included in the underlying frame of discourse should be closed under the 
operations of arbitrary finite intersections and unions. As the first step in providing 
a mathematical extension to the infinite domain, we shall now assume that we 
allow arbitrary countable intersections and unions in £, so that the latter is taken 
to be a a-algebra. Within this extended structure, Axioms 1 to 5 will continue to 
3.1 Generalised Representation of Beliefs 
107 
encapsulate the requirements of quantitative coherence for preferences, and hence 
for degrees of belief, provided that only finite combinations of events of £ are 
involved. However, if we wish to deal with countable combinations of events, we 
shall need an extension of the existing requirements for quantitative coherence. 
One possible such extension is encapsulated in the following postulate. 
Postulate 1. (Monotone continuity). 
00 
// for all j, Ej D Ej+1 and Ej > F, then Q Ej > F. 
i=i 
Discussion of Postulate 1. If the relation £} > F holds for every member of a 
decreasing sequence of events E\ D E?..., and if we accept the limit event f\. Ej 
into our frame of discourse, then it would seem very natural in terms of "continuity" 
that the relation should "carry over". The operational justification for considering 
such a countable sequence of comparisons is certainly open to doubt. However, if, 
for descriptive and mathematical convenience, we admit the possibility, then this 
form of continuity would seem to be a minimal requirement for coherence. 
Proposition 3.1. (Continuity at 0). If, for all j, Ej ~D Ej+X 
00 
and C\Ej = <b, then, for any G > 0, lim PIE, I G) = 0. 
Proof. We note first that the condition encapsulated in Postulate 1 carries 
over to conditional preferences. By Proposition 2.14(i), if Ej >g F then we have 
Ej n G > F n G; moreover, EjDGD Ej+X n G, for all j = 1,2,..., and thus, 
by Postulate 1, (f|, Ej) n G > F n G. It now follows from Proposition 2.14(i) 
that (flj Ej) >G F. By Proposition 2.16, P{E} \ G) > P(Ej+l | G) > 0, and 
so there exists a number p > 0 such that lim^oo P(Ej \G) = p, and, for all j, 
P(Ej | G) > p. By Axiom 4(iii) and Proposition 2.16, there exists a standard event 
S such that fi(S) = p, G _L S and, for all j, Ej >G S. Hence, by the above, we 
have (n ■ Ej) = 0 >g S, which implies that S ~ 0 and thus, by Propositions 2.10 
and 2.11, that p = 0. < 
Since we have already established in Proposition 2.17 that P(. \ G) is a finitely 
additive probability measure, the above result, based on the postulate of  
monotone continuity, enables us to establish immediately that, in this extended setting, 
P{. | G) is a countably additive probability measure. 
Proposition 3.2. (Countably additive structure of degrees of belief). 
If{Ej, j = 1,2,...} are disjoint events in £, and G > 0, then 
(00 \ 00 
\jEj\G = £>(£,. I G). 
3=1 I 3=1 
108 
3 Generalisations 
Proof. Since P(. \ G) is finitely additive we have, for any n > 1, 
(oo \ n 
\jEj\G\ ='£P(Ej\G) + P(Fn\G) 
3=1 / J=l 
where 
00 
F„= (J Ej. 
j=n+l 
It follows that Fn D Fn+i with f|„ Fn = 0; hence, by Proposition 3.1, 
lim P(Fn | G) = 0. 
n—>oo 
The result follows by taking limits in the last expression for P(\Jj Ej \G). ^ 
We shall consider the finite versus countable additivity debate in a little more 
detail in Section 3.5.2. For the present, we simply note that philosophical allegiance 
to the finitistic framework is in no way incompatible with the systematic adoption 
and use of countably additive probability measures for the overwhelming majority 
of applications. The debate centres on whether this particular restriction to a 
subclass of the finitely additive measures should be considered as a necessary 
feature of quantitative coherence, or whether it is a pragmatic option, outside 
the quantitative coherence framework encapsulated in Axioms 1 to 5. From a 
philosophical point of view, we identify strongly with this latter viewpoint; but 
in almost all the developments which follow we shall rarely feel discomfited by 
implicitly working within a countably additive framework. 
We have established (in Propositions 2.4 and 2.10) that if E and F are events 
in £ with E C F, then P(E) < P(F), so that if P(F) = 0 then P(E) = 0. 
However, in general, not all subsets of an event of probability zero (a so-called null 
event) will belong to £ and so we cannot logically even refer to their probabilities, 
let alone infer that they are zero. In some circumstances it may be desirable, as 
well as mathematically convenient, to be able to do this. If so, this can be done 
"automatically" by simply agreeing that £ be replaced by the smallest a-algebra, 
F, which contains £ and all the subsets of the null events of £ (the so-called 
completion of £). The induced probability measure over T is unique and has the 
property that all subsets of null events are themselves null events. It is called a 
complete probability measure. 
Definition 3.1. (Probability space). A probability space is defined by the 
elements {Q, F, V} where T is a a-algebra of Q and P is a complete, a- 
dditive probability measure on T. 
3.2 Review of Probability Theory 
109 
From now on, our mathematical development will take place within the  
assumed structure of a probability space. We do not anticipate encountering situations 
where these mathematical assumptions lead to conceptual distortions beyond the 
usual, inevitable element of mathematical idealisation which enters any formal  
analysis. However, as de Finetti (1970/1974, 1970/1975) has so eloquently warned, 
one must always be on guard and aware that distortions might occur. 
The material which follows (in Section 3.2) will differ in flavour somewhat 
from our preceding discussion of the general foundations of coherent beliefs and 
actions (Chapter 2 and Section 3.1) and our subsequent discussions of generalised 
decision problems (Section 3.3) and of the link between beliefs about observables 
and the structure of specific models for representing such beliefs (Chapter 4). These 
developments systematically invoke the subjectivist, operationalist philosophy as 
a basic motivation and guiding principle. In the next section, we shall  
concentrate instead on reviewing, from a purely mathematical standpoint, the concepts 
and results from mathematical probability theory which will provide the technical 
underpinning of our theory. 
3.2 REVIEW OF PROBABILITY THEORY 
3.2.1 Random Quantities and Distributions 
In the framework we have been discussing, the constituent possibilities and  
probabilities of any decision problem are encapsulated in the structure of the probability 
space {ft, T, P}. Now, in a certain abstract sense, we might think of ft as the 
"primitive" collection of all possible outcomes in a situation of interest; for  
example, that surrounding the birth of an infant, or the state of international commodity 
markets at a particular time point. However, we are not really interested in a 
"complete description", even if such were possible, but rather in some numerical 
summary of the outcomes, in the forms of counts or measurements. 
Recalling the discussion of Section 2.8, it might be argued that "measurements" 
are always, in fact, "counts". However, when convenient, we shall distinguish 
the two in the usual pragmatic (fuzzy) way: "counts" will typically mean integer- 
valued data; "measurements" will typically mean data which we pretend are 
real-valued. 
We move, therefore, from {ft, F, P} to a more explicitly numerical setting by 
invoking a mapping, 
x : ft -> X C 3?, 
which associates a real number x(w) with each elementary outcome w of ft (our 
initial exposition will be in terms of a single-valued x; the vector extension will be 
110 
3 Generalisations 
made in Section 3.2.4). Subsets of Q are thus mapped into subsets of 5ft and the 
probability measure P defined on T will induce a probability measure, Px, say, 
over appropriate subsets of 5ft. However, we shall wish to ensure that Px is defined 
on certain special subsets of 5ft, for example, intervals such as (—oo, a], a G 5ft, in 
which case we shall want sets of the form {w; — oo < x(oj) < a), a G 5ft, to 
belong to T, and this will constrain the class of functions x which we would wish 
to use to define the numerical mapping. The standard requirement is that Px be 
definable on the cr-algebra of Borel sets, B, of 5ft, the smallest cr-algebra containing 
intervals of the form (-oo, a], a G 5ft, and hence all forms of interval, since the 
latter can be generated by appropriate countable unions and intersections of the 
intervals (—oo, a], a G 5ft. 
Definition 3.2. (Random quantity). A random quantity on a probability 
space {O, F, P} is a function x : O —► X C 5ft such that x~l(B) G T, for 
all B eB. 
Following de Finetti (1970/1974, 1970/1975), we use the term random  
quantity to signify a numerical entity whose value is uncertain, rather than use the 
traditional, but potentially confusing, term random variable, which might suggest 
a restriction to contexts involving repeated "trials" over which the quantity may 
vary. Notationally, we shall use the same symbol for both a random quantity and 
its value. Thus, for example, x may denote a function, or a particular value of the 
function x(u) = x, say. The interpretation will always be clear from the context. 
For a random quantity x, the induced measure Px is defined in the natural way 
by 
Px(B) = P(x-1(B)), BeB. 
The function Px is easily seen to be a probability measure, and describes the way in 
which probability is "distributed" over the possible values £ G X. This information 
can also be encapsulated in a single real-valued function. 
Definition 3.3. (Distribution function). The distribution function of a  
random quantity x : Q —► X C 5ft on {O, T, P} is the function Fx : 5ft —► [0,1] 
defined by 
Fx(x) = P{w; x(u) <x} = Px{(-oo,x]}, x G 5ft. 
If the probability distribution concentrates on a countable set of values, so 
that X = {x\, x2, ■ ■ ■}, x is called a discrete random quantity and the function 
px : 5ft -» [0,1] such that 
px(x) = P{w; x(uj) = x} 
3.2 Review of Probability Theory 
111 
is called its probability (mass) function. The distribution function is then a step 
function with jumps px(xi) at each a;,. 
If the probability distribution is such that there exists a real, non-negative 
(measurable) function px such that 
Px(x G B) = [ px{t)dt = [ dFx(t), BeB, 
JB JB 
then x is called an (absolutely) continuous random quantity and px is called its 
densityfunction. In addition, of course, we might have a mixture of both discrete and 
continuous elements. No use of singular distributions will be made in this volume 
(for discussion of such distributions, see, for example, Ash, 1972, Section 2.2). 
We shall use the same notation, px, for both the mass function of a discrete 
random quantity and the density function of a continuous random quantity. In 
measure-theoretic terms, both are, of course, special cases of the Radon-Nikodym 
derivative. In general, we shall use the notation and results of Lebesgue and 
Lebesgue-Stieltjes integration theory as and when it suits us. Readers unfamiliar 
with these concepts need not worry: virtually none of the machinery will be 
visible, and the meanings of integrals will rarely depend on the niceties of the 
interpretation adopted. Moreover, when there is no danger of confusion, we 
shall often omit the suffix x in px(x), using p{x) both to represent the density 
or mass function px{-) and its value px(x) at a particular x £ X. Also, to 
avoid tedious repetition of phrases like "almost everywhere", we shall, when 
appropriate, simply state that densities are equal, leaving it to be understood that, 
with respect to the relevant measure, this means "equal, except possibly on a set 
of measure zero". 
If £ is a random quantity defined on {Ct, F, P} such that x : Q, —► X C 5ft, 
and if 5 : 5ft -> F C 5ft is a function such that (g o xy^B) G T for all BeB, 
then g o x is also a random quantity. We shall typically denote g o x by g(x), and, 
whenever we refer to such functions of a random quantity x, it is to be understood 
that the composite function is indeed a random quantity. Writing y = g(x), the 
random quantity y induces a probability space {5ft, B, Py}, where 
Py(B) = Px({g-\B)}) = P({(g o x)-\B)}), BeB. 
Functions such as Fy and pv are defined in the obvious way. These forms are easily 
related to those of Fx and px. In particular, if g~l exists and is strictly monotonic 
increasing we have 
Fy(y) = Px(g(x) <y)=Px(x< g-l(y)) = Fx(g-l(y)) 
and, in the continuous case, if g is monotonic and differentiable, the density pv is 
given by 
dg-\y) 
dy 
Some examples of this relationship are given at the end of Section 3.2.2. 
py(y) = F'(y) = F'ig-'iy)) =px{g-x{y)) 
112 
3 Generalisations 
Definition 3.4. {Expectation). Ifx,y are random quantities with y = g(x), 
the expectation ofy, E[y] = E[g(x)], is defined by either 
Ylypv(y) = Y^9{x)Px{x), or 
yeY xeX 
/ ypy(y)dy= / g(x)px{x)dx, 
Jy Jx 
for the discrete and continuous cases, respectively, where the equality is to be 
interpreted in the sense that if either side exists so does the other and they are 
equal. 
As in Definition 3.4, most sums or integrals over possible values of random 
quantities will involve the complete set of possible values (for example, X and Y). 
To simplify notation we shall usually omit the range of summation or integration, 
assuming it to be understood from the context. To avoid tiresome duplication, 
we shall also typically use the integral form to represent both the continuous and 
the discrete cases. 
It is useful to be able to summarise the main features of a probability  
distribution by quantities defined to encapsulate its location, spread, or shape, often 
in terms of special cases of Definition 3.4. Assuming, in each case, the right-hand 
side to exist, such summary quantities include: 
(i) E[x), the mean of the distribution of the random quantity x; 
(ii) E[xk], the fcth (absolute) moment; 
(iii) V[x] = E[(x - E[x\f] = E[x2] - E2[x], the variance; 
(iv) D[x) = Vfz]1/2, the standard deviation; 
(v) M[x], a mode of the distribution of x, such that 
px(M[x}) = suppx(x); 
xeX 
(vi) <2a[a:], an a-quantile of the distribution of x, such that 
Fx(Qa[x}) = Px(x<Qa[x})=a; 
(vi) Me[x) = Qo.s[x},&median; 
(vii) (<3(1_p)/2[a;], <3(1+p)/2[a;]), ap-interquantile range. 
The expectation operator of Definition 3.4 is linear, so that if x\, x2 are two 
random quantities and c\, c2 are finite real numbers then 
E[cxxi + c2x2] = ciE[xi\ + c2E[x2]. 
3.2 Review of Probability Theory 
113 
In the special case of the transformation g(x) = ex, for some real constant c, we 
clearly have 
E[g(x)} = cE[x}=g(E[x}), 
D[g{x)] = (V[g{x)\)1'2 = (c2V[x})1/2 = g(D[x)). 
For general transformations g(x), E[g{x)) ^ g{E[x\) and the moments of a  
transformed random quantity g(x) do not exactly relate in any straightforward manner 
to those of x. However, for suitably well-behaved g(x) the following result, which 
we shall illustrate at the end of Section 3.2.2, often provides useful approximations. 
Proposition 3 J. (Approximate mean and variance). Ifx is a random  
quantity with E[x] = fi, V[x] = o2 and y = g(x) then, subject to conditions on 
the distribution ofx and the smoothness ofg, 
E[y] « g(n) + \e2g"{n\ 
V[y]~*2[g'(n)]2. 
Outline proof. Expanding g(x) in a Taylor series about fj,, we obtain 
g(x) « g(n) + {x- n)g'(n) + \{x - n)2g"{n), 
where we are assuming regularity conditions sufficient to ensure the adequacy of 
this approximation in what follows. Taking expectations immediately yields the 
approximate form for E[y); subtracting the latter approximation from both sides, 
squaring, taking expectations and ignoring higher order terms, yields the result for 
V[y\. Clearly, more refined approximations are easily obtained by including higher 
order terms. < 
In Definition 2.6, we introduced the notion of the independence of two events 
with subsequent generalisations to mutual independence (Definition 2.12) and  
conditional independence (Definition 2.13). These notions can be extended to random 
quantities in the following way. 
Definition 3.5. (Mutual independence). The random quantities xi,...,xn 
are mutually independent if, for any tj G 5ft, the events {w; Xi{oj) < £,}, for 
i = 1,..., n, are mutually independent. 
We note that for independent random quantities X\,..., xn 
IT 
Yl E[Xi] and V 
= £n« 
«=i 
Definition 3.6. (Conditional independence). For any random quantity y, the 
random quantities X\,..., xn are conditionally independent given y if, for 
any tt G 5i, i = 1,..., n, the events {a;; £,(u;) < U) are conditionally 
independent given the event {w; y(w) < y},for all y. 
114 
3 Generalisations 
Conditional independence will play a major role in our later discussion of 
modelling in Chapter 4. 
Many forms of technical manipulation of probability distributions are greatly 
facilitated by working with some suitable transformation of the original density or 
distribution function. One of the most useful such transforms is the following. 
Definition 3.7. (Characteristic function). The characteristic function of a 
random quantity x is the function <\>x, mapping 5ft to the complex plane, given 
by 
4>x{t) = E[eitx], t£$l. 
Among the most important properties of the characteristic function, we note 
the following. 
(i) \Mt)\ < land 0^(0) = 1. 
(ii) 4>x is a uniformly continuous function of t. 
(iii) If x\,..., xn are independent random quantities, and s = Y^l=i xi'tnen 
&(*) = n?=i m*)- 
(iv) Two random quantities have the same distribution if and only if they have the 
same characteristic function. 
(v) lfE[xk] < oo, then <f>x(t) = ^^-E[x>] + o(tk). 
Many similar properties hold for the closely related alternative transforms E[etx], 
the moment generating function, and E[tx], the probability generating function. 
3.2.2 Some Particular Univariate Distributions 
In this section, we shall review a number of particular univariate distributions which 
are frequently used in applications, and list some of their properties and  
characteristics. We shall assume that the reader is familiar with most of this material, and 
detailed discussion and derivations are therefore not given. The books by Johnson 
and Kotz (1969,1970) provide a mass of detail on these and other distributions. 
One important initial warning is required! These distributions provide the 
building blocks for statistical models and are typically denned in terms of  
"parameters". The role and interpretation of "models" and "parameters" within the general 
subjectivist, operationalist framework are extremely important issues, which will 
be discussed at length in Chapter 4. For the present," parameters" should simply be 
regarded as "labels" of the various mathematical functions we shall be considering, 
although, as we shall see, these "labelling parameters" often relate closely to one 
or other of the characteristics of the distribution. 
3.2 Review of Probability Theory 
115 
The Binomial Distribution 
A discrete random quantity x has a binomial distribution with parameters 6 and n 
(0 < 6 < 1, n = 1,2,...) if its probability function Bi(x | n, 6) is 
Bi(z | 6, n) = (j 9X{1 - 6)n~x, x = 0,1,..., n. 
The mean and variance are i^x] = n#, and V[x] = nd{\ — 6). A mode is attained 
at the greatest integer M[x] which does not exceed xm = (n + 1)0; if xm is an 
integer, then both xm and £m — 1 are modes. 
If n — 1, x is said to have a Bernoulli distribution, with probability function 
denoted by Br(a; | 6). The sum of k independent binomial random quantities with 
parameters (6, n,), i = 1,... fc, is a binomial random quantity with parameters 6 
and rii + • • • + rifc. 
The Hypergeometric Distribution 
A discrete random quantity x has an hypergeometric distribution with integer  
parameters N, M and n (n < N + M) if its probability function Hy(a; | N, M, n) 
is 
Uy(x\N,M,n) = c( )( I, max{0, n - M} < x < min{n, TV}, 
where 
'N + MY1 
c ■ 
n 
The mean and variance are given by 
_r . nN nMN (N + M - n) 
e[x] = WTm and v[x] = (N + My(N + M-i) 
A mode is attained at the greatest integer M[x] which does not exceed 
(n+l)(AT+l) 
m" M + 7V + 2 ' 
if xm is an integer, then both xm and xm — 1 are modes. 
116 
3 Generalisations 
The Negative-Binomial Distribution 
A discrete random quantity x has a negative-binomial distribution with parameters 
6 and r (0 < 6 < 1, r = 1,2,...) if its probability function Nb(a; | r, 6) is 
Nb(z10,r) = c(r +rx_~lVi-ey, x = o,i,2,... 
where c = 0r. The mean and variance are E[x] = r(l — 6)/6 and V[x] = 
r(l - 0)/02. If r(l - 0) > 1 the mode M[z] is the least integer not less than 
[r(l - 0)]/0; if r(l - 0) = 1, there are two modes at 0 and 1; if r(l - 0) < 1, 
M[a;] = 0. 
If r = 1, £ is said to have a geometric or Pasca/ distribution. Moreover, 
the sum of k independent negative binomial random quantities with parameters 
(6, ri), i = 1,..., k, is a negative binomial random quantity with parameters 6 and 
n + --- + rk. 
The Poisson Distribution 
A discrete random quantity x has a Poisson distribution with parameter A (A > 0) 
if its probability function Pn(a; | A) is 
Xx 
Pn(z|A) = c— - £ = 0,1,2,... 
x\ 
where c = e~A. The mean and variance are given by E[x] = V[x] = A. A mode 
M[x] is attained at the greatest integer which does not exceed A. If A is an integer, 
both A and A — 1 are modes. 
The sum of k independent Poisson random quantities with parameters A,, 
i = 1,..., k, is a Poisson random quantity with parameter Ai + • • • + A*. 
The Beta Distribution 
A continuous random quantity x has a beta distribution with parameters a and j3 
(a > 0, (3 > 0) if its density function Be(x\a,f3) is 
Be(x\a,f3) = cxa-1(l-x)0-\ 0 < x < 1 , 
where 
r(a + /?) 
r(a)r(/?) 
and r(a;) = /0°° *I-1e_td*; integer and half-integer values of the gamma function 
are easily found from the recursive relation r(a; + 1) = xV(x), and the values 
r(l) = 1 and T(l/2) = y/ir « 1.7725. 
3.2 Review of Probability Theory 
117 
Systematic application of the beta integral, 
/' 
Jo 
x^il-xf-Hx 
T{a)T{(3) 
Y{a + (3) 
gives 
E\x) = -^—z and V[x] = a.„, a—— • 
L J a + (3 L J (a + (3)2(a + (3 + 1) 
If a > 1 and (3 > 1, there is a unique mode at (a — l)/(a + j3 — 2). If £ has a 
Be(a: | a, /?) density, then y = 1 — x has a Be(y | f3, a) density. If a = (3 = 1, x is 
said to have a uniform distribution Un(a; | 0,1) on (0,1). 
By considering the transformed random quantity y = a + x(b — a), where 
x has a Be(a; | a, (3) density, the beta distribution can be generalised to any finite 
interval (a, b). In particular, the uniform distribution Un(y | a, b) on (a, b), 
Un(y \a,b) = (b- a)-1, a < y <b , 
has mean i?[y] = (a + 6)/2 and variance V[y] = (b — a)2/12. 
77ie Binomial-Beta Distribution 
A discrete random quantity x has a binomial-beta distribution with parameters a, 
(3 and n (a > 0, (3 > 0, n = 1,2,...) if its probability function Bb(a; | a, /?, n) is 
Bb(z | a, /?, n) = c( jF(a + x)F((3 + n - x), x = 0,...,n, 
where 
T(a + f3) 
T(a)T(p)T{a + p + n) 
The distribution is generated by the mixture 
Bb(x\a,/3,n)= [ Bi(x \ 6, n)Be{6\ a, f3) d6. 
Jo 
The mean and variance are given by 
P, , a na(3 (a + /3 + n) 
E\x\ = n and V\x] = -, ^r-, ^ r ■ 
L J a + (3 L J (a + 0)2 (a + 0+1) 
A mode is attained at the greatest integer M[x] which does not exceed 
_(n+l)(q-l) 
Xm~ a + (3-2 ' 
if xm is an integer, both xm and xm — 1 are modes. If a = (3 = 1 we obtain the 
discrete uniform distribution, assigning mass (n + 1)_1 to each possible x. 
118 
3 Generalisations 
The Negative-Binomial-Beta Distribution 
A discrete random quantity x has a negative-binomial-beta distribution with  
parameters a, 0 and r (a > 0,0 > 0,r = 1,2,...) if its probability function 
Nbb(a: \a,0,r) is 
mHx\a,0,r) = c(r + X-1)r( V{P + X) ., , = 0,1,... 
v ' ' \ r-l Jr{a + 0 + x + r) 
where 
_ T(a + 0)Y{a + r) 
C~ T(a)T(0) 
The distribution is generated by the mixture 
dd. 
The mean is E[x] = r0(a - 1)_1, a > 1, and the variance is given by 
a> 2. 
Nbb(z|a,p» = / Nb(x 16>, r) Be(6> | «,/?) 
Jo 
a — 1 
a + /5+r-l r/? 
a-2 (a-l)(a-2) 
77ie Gamma Distribution 
A continuous random quantity x has a gamma distribution with parameters a and 0 
(a > 0,0 > 0) if its density function Ga(a; | a, /?) is 
Ga(x\a,0) = cxa-1e-0x, x > 0, 
where c = 0a/T(a). Systematic application of the gamma integral 
Jo 0a 
a~l^xdx - V{a)- 
gwe$,E[x) = a/0andV[x] — a/02. If a > 1, there isaunique mode at(a-l)//?; 
if a < 1 there are no modes (the density is unbounded). 
If a = 1, £ is said to have an exponential Ex(x | 0) distribution with parameter 
0 and density 
Ex(x\0) = 0e~0x, x>0. 
The mode of an exponential distribution is located at zero. If 0 = 1, £ is said to 
have an Erlang distribution with parameter a. If a = v/2, 0 = 1/2, £ is said to 
have a (central) chi-squared (x2) distribution with parameter u (often referred to 
as degrees of freedom) and density denoted by x2(% \ v) or xlix)- 
By considering the transformed random quantities y = a + x or z = b — x, 
where x has a Ga(a; | a, 0) density, the gamma distribution can be generalised to the 
ranges (a, oo) or (—oo, b). Moreover, the sum of k independent gamma random 
quantities with parameters (a*, 0), i = 1,..., k, is a gamma random quantity with 
parameters a\ + ■ ■ ■ + a^ and 0. 
3.2 Review of Probability Theory 
119 
The Inverted-Gamma Distribution 
A continuous random quantity x has an inverted-gamma distribution with  
parameters a and 0 (a > 0, 0 > 0) if its density function Ig(a; | a, (3) is 
Ig(z|a,/?) = cz-^'e-^, z>0, 
where c = 0a/T(a). Systematic application of the gamma integral gives 
E[X] = J^T)' a>1' 
VM=(a-l£(«-2)' a>2- 
There is a unique mode at 0/(a + 1). The term inverted-gamma derives from the 
easily established fact that if y has a Ga(y | a, (3) density then x = y~x has an 
lg(x\a,f3) density. 
If x has an inverted-gamma distribution with a = v/2, j3 = 1/2, then x is 
said to have an inverted-^ distribution. 
A continuous random quantity y has a square-root inverted-gamma density, 
GdT^I2\y | a,/?), if £ = y-2 has aGa(a;|a,/?) density. 
77*e Poisson-Gamma Distribution 
A discrete random quantity a; has a Poisson-gamma distribution with parameters 
a, p and n (a > 0, /? > 0, n = 1,2,...) if its probability function Pg(a; | a, (3, n) 
is 
Pg(x\a,f3,n) = c , ln \ ^ > z = 0,1,2,... 
61 ' ^' ' z! (f3 + n)a+x 
where c = f3a/T(a). The distribution is generated by the mixture 
/•oo 
Pg(z | a, /?, n) = / Pn(a; | nX) Ga(A | a, 0) dX. 
Jo 
This compound Poisson distribution is, in fact, a generalisation of the negative 
binomial distribution Nb(z | a,j3/{(3 + n)), previously defined only for integer 
a. The mean is E[x] = na/(3y and the variance is V[x] = na((3 + n)//32. If 
an> f3 + n, there is a mode at the least integer not less than (n(a - I)/13) — 1; if 
an = 0 + n, there are two modes at 0 and 1; if an < 0 + n, M[x] = 0. 
120 
3 Generalisations 
The Gamma-Gamma Distribution 
A continuous random quantity x has a gamma-gamma distribution with parameters 
a, 0 and n (a > 0, (3 > 0, n = 1,2,...) if its probability function Gg(x | a, j3, n) 
is 
ev ' 'M y r(a) r(n) (/? + a:)«+n 
The distribution is generated by the mixture 
/•OO 
Gg{x\a,(3,n) = / Ga(x | n, A) Ga(A | a, 0) dX. 
Jo 
The mean and variance are given by 
E[x] = n —-— ' a > 1, 
a — 1 
11 (a-l)2(a-2) ^ 
7/ie Pareto Distribution 
A continuous random quantity x has a Pareto distribution with parameters a and 
/? (a > 0, /? > 0) if its density function Pa(x | a, (3) is 
Pa(x|a,/3) = cx~{a+1), x>0, 
where c = a(3a. The mean and variance are given by 
E[x] = -2£7> ifa>l 
a — 1 
^N = 7 7t£ ST' ifa>2. 
1 J (a-l)2(a-2) 
The mode is M[x] = 0. The distribution is generated by the mixture 
/■OO 
Pa(x | a, /?) = / Ex(x - 0 \ 6) Ga(6> | a, /?) d». 
A continuous random quantity y has an inverted-Pareto density Ip(y | a, 0) if 
a; = y_1 has a Fa(x | a, 0) density. 
3.2 Review of Probability Theory 
121 
The Normal Distribution 
A continuous random quantity x has a normal distribution with parameters fx and A 
(/i £ S, A > 0) if its density function N(x | /i, A) is 
N(x |/i, A) = c exp < ——(x - /i)2 > , iSS, 
where 
c=l^) 
1/2 
The distribution is symmetrical about x = /i. The mean and mode are E[x] = 
M[x] = \i and the variance is V[x] = A-1, so that A here represents the precision 
of the distribution. Alternatively, N(x | /i, A) is denoted by N(x | /i, o~2), where 
a2 — V[x] is the variance. If \i = 0, A = 1, x is said to have a standard normal 
distribution, with distribution function $ given by 
*(*) = ^= J_jM~\t2}dt. 
If y = A1/2(x - /i) = (x - aO/ct, where x has a normal density N(x | /i, A), then 
y has a N(y | 0,1) (standard) density. In general, if y = a + ^2i=1 hxi, where 
the Xi are independent with N(x^ | fa, \) densities, then y has a normal density, 
N(y Ia + Yli=i bHi, A) where A = (Ya=i ^/^«)_1» a weighted harmonic mean 
of the individual precisions. 
If xi,..., xjt are mutually independent standard normal random quantities, 
then z = Yli=i x1 nas a (central) \\ distribution. 
The Non-central x2 Distribution 
A continuous random quantity x has a non-central x2 distribution with parameters 
v (degrees of freedom) and A (non-centrality) {v > 0, A > 0) if its density function 
X2{x\v,\)is 
00 
X2(x |«/, A) = Y,Pn (* I A/2) x2(x 11/ + 2t), 
i=0 
i.e., a mixture of central x2 distributions with Poisson weights. It reduces to 
a central x2('/) when A = 0. The mean and variance are E[x] = v + A and 
^[x] = 2{y + 2A). The distribution is unimodal; the mode occurs at the value 
M[x] such that x2(M[x] | v, A) = x2(M[x] | v - 2, A). 
If xi,..., Xjt are mutually independent normal random quantities A^(x^ | /i«, 1), 
then z = Yli=i x2 nas a non-central x2 distribution, 
X2(2|fc,^=1/i2). 
The sum of k independent non-central x2 distributions with parameters (i/,, A^) 
is a non-central x2 with parameters v\ + ■ ■ • + vk and Ai + \- Xk- 
122 
3 Generalisation: 
The Logistic Distribution 
A continuous random quantity x has a logistic distribution with parameters a and 0 
(a e 3?, 0 > 0) if its density function Lo(x | a, 0) is 
exp 
Lo(x |a,/3) = c 
{-(=r)} 
> XgS, 
r-M=r)}]' 
where c = 0~x. An alternative expression for the density function is 
so that the logistic is sometimes called the sech-squared distribution. 
The logistic distribution is most simply expressed in terms of its distribution 
function, 
Fx{x) 
1 + exp 
\-m. 
The distribution is symmetrical about x = a. The mean and mode are given by 
E[x] = M[x] = a, and the variance is V[x] — 02tt2/3. 
The Student (t) Distribution 
A continuous random quantity x has a Student distribution with parameters /i, A 
and a (/i e K, A > 0, a > 0) if its density St(x | /i, A, a) is 
St(x |/i, A, a) = c 
l + -(x-/i)2 
a 
-(a+l)/2 
» x e 3J, 
where 
, r((a + l/2)) /A\1/2 
r(a/2)r(l/2) \aj 
The distribution is symmetrical about x = /i, and has a unique mode M[x] = 
\i. The mean and variance are 
E[x] = /i, if a > 1, 
V[x] = \t^2)' ifa>2- 
The parameter a is usually referred to as the degrees of freedom of the distribution. 
3.2 Review of Probability Theory 
123 
The distribution is generated (Dickey, 1968) by the mixture 
St(x | /i, A, a) = / N(x | /i, Ay) Ga [y | -, -J dy, 
and includes the normal distribution as a limiting case, since 
Nix I /i, A) = lim Stfa; I /i, A, a). 
a-»oo 
If y = A1//2(a;-/i), wherexhas aSt(a: | /i, A, a) density, theny has ^{standard) 
student density St(y | 0,1, a). If a — 1, x is said to have a Cauchy distribution, 
with density Ca(a: | /i, A). 
If a; has a standard normal distribution, y has a x2. distribution, and x and y 
are mutually independent, then 
_ x 
has a standard Student density St(z | 0,1, f). 
The Snedecor (F) Distribution 
A continuous random quantity x has a Snedecor, or Fisher, distribution with  
parameters a and 0 {degrees of freedom) {a > 0, 0 > 0) if its density Fs (a; | a, 0) 
is 
x(a/2)-l 
Fs(*|a,/?) = c(/? + QxO(aW2. *>0' 
where 
r((q + fl/2) /2 ,/2 _ 
V{a/2)Y{0/2) P 
If /? > 2, E[x] = /?/(/? - 2) and there is a unique mode at [/?/(/? + 2)][(a - 2)/a]; 
moreover, if /3 > 4, 
/32 (a + 0 - 2) 
F[x] = 2 
a(/J-4) (/3-2)2 
If a; and y are independent random quantities with central x2 distributions, 
with, respectively, v\ and i/2 degrees of freedom, then 
has a Snedecor distribution with v\ and v2 degrees of freedom. 
Relationships between some of the distributions described above can be  
established using the techniques described in Section 3.2.1. For a geometrical  
interpretation of some of these relations, see Bailey (1992). 
124 
3 Generalisations 
Example 3.1. (Gamma and \2 distributions). Suppose that x has a Ga(x | a, 0)  
density and let y = 20x. Then, for y > 0, we have 
pv(y)=px(^) 
d(y/20) 
dy 
¥~r)ya-leM-l} = Ga(y\a,l), 
so that y has a x1{y\a) density. Since \2 distributions are extensively tabulated, this 
relationship provides a useful basis for numerical work involving any gamma distribution. 
Example 3.2. (Beta, Binomial and Snedecor (F) distributions). Suppose that x has 
a density Be(x | a, 0) and let y = 0x[a{\ — x)]"1. Then, for y > 0, and noting that 
x = ay[0 + ay]*1, we have 
Py{y) =pAay[0 + ay] l) 
d{ay[0 + ay}^} 
dy 
0 
0-1 
a0 
= T(a + 0)f ay 
T{a)T{0)\0 + ay J \0 + ay) (0 + ayf 
r(a + 0)aa0V ya~' 
= r(a)r(0) (0 + ay)^=FS^2a'20)> 
so that y has the stated Snedecor (F) density. Binomial probabilities may also be obtained 
from the F distribution using the exact relation between their distribution functions given 
by Peizer and Pratt (1968) 
FBi(x\0,n)=FF 
(s + l)(l-fl) 
0(n - x) 
2(n-x), 2(x + l) , i = 0, l,...,n. 
Since F distributions are extensively tabulated, these relationships provide a useful basis for 
numerical work involving any beta or binomial distribution. 
Example 3.3. (Approximate moments for transformed random quantities). Suppose 
that x has a Be(x | a, 0) density, 0 < x < 1, but that we are interested in the means and 
variances of the transformed random quantities 
2/i =9\(x) = loZ\JZr^)> y2 =92{x) =2 sin'1 ^/x. 
Recalling that 
and noting that 
E[x] = fi ■■ 
Sfdx) = 
a + 0 
x(l — x) 
V[x\ = a 
, sTAx) = 
2 = M(l - m) 
a + 0 + 1 
x — (1 — x) 
x2(l-x) 
3.2 Review of Probability Theory 125 
I t \ „"/~\ X — (l — X) 
xV2(l-X)l/2' »1V~> X3/2(1-X)3/2 
application of Proposition 3.3 immediately yields the following approximations: 
~H-(l-fi)~ 
m2(i-m)2 
£[2y2]«2sin-1^7i+-(T2 
L m2(i - m)2 
H - (1 - /x) 
2 Lm3/2(1 - M)3/2 
V[Wi 
(i(l-(i) a+ 0+1 
We note, in particular, that if /x ~ \ the second (correction) terms in the mean approximations 
will be small, and that, for all /x, the variance is "stabilised" (i.e., does not depend on /x) 
under the second transformation. 
3.2.3 Convergence and Limit Theorems 
Within the countably additive framework for probability which we are currently  
reviewing, much of the powerful resulting mathematical machinery rests on various 
notions of limit process. We shall summarise a few of the main ideas and  
results, beginning with the four most widely used notions of convergence for random 
quantities. 
Definition 3.8. (Convergence). A sequence X\,X2,. ■ ■, of random quantities: 
(i) converges in mean square to a random quantity x if and only if 
lim E[(Xi - x)2} = 0; 
i—>oc 
(ii) converges almost surely to a random quantity x if and only if 
p({«;i!imxi(u) = xH}) = l; 
in other words, ifxi(w) tends to x(u) for all u except those lying in a 
set of P-measure zero; 
(iii) converges in probability to a random quantity x if and only if 
for all e > 0, lim P({u;; \xi(u) - x(u)\ > s}) —> 0; 
(iv) converges in distribution to a random quantity x if and only if the  
corresponding distribution functions are such that 
lim Fi(t) = F(t) 
at all continuity points tofF in K; we denote this by F{ —► F. 
126 
3 Generalisations 
Convergence in mean square implies convergence in probability; for finite 
random quantities, almost sure convergence also implies convergence in probability. 
Convergence in probability implies convergence in distribution; the converse is 
false. Convergence in distribution is completely determined by the distribution 
functions: the corresponding random quantities need not be defined on the same 
probability space. Moreover, 
(i) Fi —» F if and only if, for every bounded continuous function g, the sequence 
of the expected values E[g(xi)] with respect to Ft converges to the expected 
value E[g(x)] with respect to F; 
(ii) if Fi —> F and <f>i(t) and <f>(t) are the corresponding characteristic functions, 
then <pi(t) —> <p(t) for all t £ K; the converse also holds, provided that <p(t) is 
continuous at t = 0; 
(iii) (Helly's theorem) Given a sequence {Fi, F2,...} of distributions functions 
such that for each e > 0 there exists an a such that for all i sufficiently 
large Fi(a) — F^—a) > 1 — e, there exists a distribution function F and a 
subsequence F^, Fi2,..., such that Ft. —> F. 
An important class of limit results, the so-called laws of large numbers, link 
the limiting behaviour of averages of (independent) random quantities with their 
expectations. Some of the most basic of these are the following: 
(i) If xi, X2, ■ ■., are independent, identically distributed random quantities with 
E[xf} < oo and E[xi\ = \x, then the sequence of random quantities xn = 
n~l 53™=1 xu n = 1, 2,..., converges in mean square (and hence in  
probability) to n; that is to say, to a degenerate, discrete random quantity which 
assigns probability one to /i. 
(ii) The weak law of large numbers. If x^,X2, ■ ■ ■ are independent, identically 
distributed random quantities with E[x,] = \i < oo, then the sequence of 
random quantities xn, n — 1,2,..., converges in probability to /i. 
(iii) The strong law of large numbers. Under the same conditions as in (ii), xn, 
n = 1,2,..., converges almost surely to \i. 
In addition, there is a further class of limit results which characterises in more 
detail the properties of the distance between the sequence and the limit values. Two 
important examples are the following: 
(i) The central limit theorem. If£i,a;2, • • .are independent identically distributed 
random quantities with E[x{] = /i and V[xj] = a2 < oo, for all i, then the 
sequence of standardised random quantities 
Xn A* n n 
zn = , r- ' n=l,2,... 
converges in distribution to the standard normal distribution. 
3.2 Review of Probability Theory 
127 
(ii) The law of the iterated logarithm. Under the conditions assumed for the central 
limit theorem, _ 
lim sup ", ,- (2 log log n) ~ ' = 1. 
There are enormously wide-ranging variations and generalisations of these 
results, but we shall rarely need to go beyond the above in our subsequent discussion. 
3.2.4 Random Vectors, Bayes' Theorem 
A random quantity represents a numerical summary of the potential outcomes 
in an uncertain situation. However, in general each outcome has many different 
numerical summaries which may be associated with it. For example, a description 
of the state of the international commodity market would typically involve a whole 
complex of price information; the birth of an infant might be recorded in terms of 
weight and heart-rate measurements, as well as an encoding (for example, using a 0- 
1 convention) of its sex. It is necessary therefore to have available the mathematical 
apparatus for handling a vector of numerical information. 
Formally, we wish to define a mapping 
x : n — X C 5ft* 
which associates a vector x(u) of k real numbers with each elementary outcome 
u of Q.. As in the case of (univariate) random quantities, we move the focus of 
attention from the underlying probability space {ft, J7, P} to the context of 5ft* and 
an induced probability measure Px. However, we shall again wish to ensure that Px 
is well-defined for particular subsets of 5ft* and this puts mathematical constraints 
on the form of the function x. Generalising our earlier discussion given in Section 
3.2.1, we shall take this class of subsets to be the smallest tr-algebra, B, containing 
all forms of fc-dimensional interval (the so-called Borel sets of 5ft*). This then 
prompts the following definition. 
Definition 3.9. (Random vector). A random vector xona probability space 
{ft, T, P} is a function x : ft —► X C 5ft* such that 
x~\B)£T, for all B e B. 
For a random vector x, the induced probability measure Px is defined in the 
natural way by 
Px(B)=P(x-\B)), BeB. 
The possible forms of distribution for x, Px, are potentially much more  
complicated for a random vector than in the case of a single random quantity, in that they 
128 
3 Generalisations 
not only describe the uncertainty about each of the individual component random 
quantities in the vector, but also the dependencies among them. 
As in the one-dimensional case, we can distinguish discrete distributions, 
where x takes only a countable number of possible values and the distribution can 
be described by the probability (mass) function 
px{x) = P{{u; x{u) = x}), 
and (absolutely) continuous distributions, where the distribution may be described 
by a density function px(x) such that 
PX(B)= px{x)dx= px{xl,...,xk)dxi---dxk, B£B. 
Jb Jb 
The distribution function of a random vector x is the real-valued function 
Fx : H* -► [0,1] denned by 
Fx(x) =Fx(xi,...,xk) =Pa;{(-oo,Xi]n...n(-oo,X|t]}- 
In addition, we could have cases where some of the components are discrete 
and others are continuous. Some components, of course, might themselves be a 
mixture of the two types. In what follows, we shall usually present our discussion 
using the notation for the continuous case. It will always be clear from the context 
how to reinterpret things in the discrete (or mixed) cases. 
The density px{x) = px(xi,..., Xk) of the random vector x is often referred 
to as the joint density of the random quantities xi,...,Xk- If the random vector x 
is partitioned into x = (y, z), say, where y = (x\,. ■., xt), z = (xt+i,. ■ ■, Xk), 
the marginal density for the vector y is given by 
Py{y) = / Px{y,z)dz, 
or alternatively, dropping the subscripts without danger of confusion, 
p(xi,...,xt)= / p(xi,...,Xk)dxt+i---dxk. 
This operation of passing from a joint to a marginal density occurs so  
commonly in Bayesian inference contexts (see Chapter 5, in particular) that it is useful to 
have available a simple alternative notation, emphasising the operation itself, rather 
than the technical integration required. To denote the marginalisation operation 
we shall therefore write 
Px(y,z)-^py{y). 
The conditional density for the random vector z, given that y(u) = y, is 
defined by . 
n /„ |„A Px{v,z) 
3.2 Review of Probability Theory 
129 
or, alternatively, again dropping the subscripts for convenience, 
p(xi,...,xk) 
p{Xt+l,...,Xk\Xl,...,Xt) 
p(xi,...,xt) 
We shall almost always use the generic subscript-free notation for densities. It is 
therefore important to remember that the functional forms of the various marginal 
and conditional densities will typically differ. 
Proposition 3.4. (Generalised Bayes' theorem). 
, , x Pz\y{z\v)Py{y) 
^(v|z) = _~^w— 
Proof. Exchanging the roles of y and z in the above, it is obvious that 
Px{x) = pz]y{z | y)py(y) = py\z{y | z)ft(z), 
which immediately yields the result. <j 
It is often convenient to re-express Bayes' theorem in the simple  
proportionality form 
Py\z{y\ z) ocpzly(z\y)py{y), 
since the right-hand side contains all the information required to reconstruct the 
normalising constant, 
[ft(*)l l = \ Pz\y{z I V)Py{y) dy 
should the latter be needed explicitly. In many cases, however, it is not explicitly 
required since the "shape" of py\z(y | z) is all that one needs to know. 
In fact, this latter observation is often extremely useful for avoiding unnecessary 
detail when carrying out manipulations involving Bayes' theorem. More  
generally, we note that if a density function p(x) can be expressed in the form cq(x), 
where q is a function and c is a constant, not depending on x, then 
/ q(x) dx = c"\ since / p(x) dx = P(Kfc) = 1. 
Any such q(x) will be referred to as a kernel of the density p(x). The  
proportionality form of Bayes' theorem then makes it clear that, up to the final stage 
of calculating the normalising constant, we can always just work with kernels of 
densities. 
130 
3 Generalisations 
For further technical discussion of the generalised Bayes' theorem, see Mou- 
chart (1976), Hartigan (1983, Chapter 3) and Wasserman and Kadane (1990). 
As with marginalising, the Bayes' theorem operation of passing from a  
conditional and a marginal density to the "other" conditional density is also fundamental 
to Bayesian inference and, again, it is useful to have available an alternative  
notation. To denote the Bayes' theorem operation, we shall therefore write 
Pz\y(z\y) ® Py(v) = Py\z(y\z)- 
In more explicit terms, and dropping the subscripts on densities, Bayes'  
theorem can be written in the form 
p(Xi,...,Xt\xt+U...,Xk) 
= p(xt+i,. ..,Xk\Xi,...,Xt) p{X\,.. .,Xt) 
Jst p(xt+i, ...,Xk\xi,...,xt) p(xi, ...,xt)dxi--- dxt 
Manipulations based on this form will underlie the greater part of the ideas 
and results to be developed in subsequent chapters. 
In particular, extending the use of the terms given in Chapter 2, we shall  
typically interpret densities such as p(xi,..., xt) as describing beliefs for the  
random quantities X\,...,xt before (i.e., prior to) observing the random quantities 
xt+i,... ,Xk,andp(x!,...,xt \xt+i,... ,Xk) as describing beliefs after (i.e.,  
posterior to) observing xt+i,..., x^. 
Often, manipulation is simplified if independence or conditional independence 
assumptions can be made. For example, ifx\,...,xt were independent we would 
have 
t 
p{xu...,xt) = Y[p{xi) ; 
1=1 
if xt+i,... ,Xk were conditionally independent, given x\,..., xt, we would have 
k 
p(xt+1,...,xk\xi,...,xt) = J\ p{xi\xu...,xt) . 
i=t+l 
If a; is a random vector defined on {Q, T, P} such that x : Q —> X C 3?* 
and if g : K* -+ Y C K/l (h < k) is a function such that (g o x)'1 (B) G T for 
all B G B, then g o x is also a random vector. We shall typically denote g o x by 
g{x), and, whenever we refer to such vector functions of a random vector x, it is 
to be understood that the composite function is indeed a random vector. Writing 
y=g(x), the random vector y induces a probability space {3?ft, B, Py} where 
Py{B) = Px{g-l(B)) = P((gox)-l(B)), BeB, 
3.2 Review of Probability Theory 
131 
and distribution and density functions Fy, py are defined in the obvious way. These 
forms are easily related to Fx, px. In particular, if g is a one-to-one differentiable 
function with inverse g~l, we have, for each y € Y, 
py{y) = Px{g~l{y))\Jg-i{y)\, 
where 
J9-^ = —dy— 
is the Jacobian of the transformation gr1, defined by 
dK(y) 
Jg-i (l/) 
i, j — 1,..., k , 
dyj 
where 
Hv) = [g-l(v)]i ■ 
If h < k, we are usually able to define an appropriate z, with dimension k — h, 
such that w = f(x) = (y, z) — (g{x), z) is a one-to-one function with inverse 
f~l, and then proceed in two steps to obtain py{y) by first obtaining 
P»(«)=ft*(l/.*)=P*(/"1(tD))|J/-i(w)|» 
and then marginalising to 
Mv)= , uPy*(y>z)dz- 
The expectation concept generalises to the case of random vectors in an  
obvious way. 
Definition 3.10. (Expectation of a random vector). Ifx, y are random  
vectors such that y = g(x), x : Q -* X C »*, g : K* -> Y C Kft (h < k), the 
expectation ofy, E[y] = E[g(x)], is a vector whose ith component, 
E[yi] = E[g(x)]i = E [9i{x)], i = l /i, 
is defined by either 
^ViPy{y) = ^ViPyiith)= J29i(x)px{x), 
yeY Vi xeX 
or 
ViPy{y)dy= ViPyAyi) dyi = \ gi(x)px(x)dx, 
for the discrete and absolutely continuous cases, respectively, where all the 
equalities are to be interpreted in the sense that if either side exists, so does 
the other and they are equal. 
132 
3 Generalisations 
In particular, the forms defined by E [iTfLjx"*] are called the moments of x of 
order n = n\ H h n^. Important special cases include the first-order moments, 
.Efo], i = 1,..., k, and the second-order moments, E[xf], (i = l,...,k), E[xiXj], 
(1 < i ¥" 3 ^ k). If x\,..., Xk are independent then E [Hx"*] = IIj E[x™1}. 
The covariance between x, and Xj is defined by 
C[xt, Xj] = E[(xi - E[xi\) (xj - E[xj])] = E[xiXj] - E[xi] E[xj] , 
and the correlation by 
xt[Xj, Xj 
Kj [Xj, Xj 
V[xiY/2V[xj]1/2 
The Cauchy-Schwarz inequality establishes that | R[xt, Xj] | < 1. The  
expectation vector with components i?[xi],..., E[xk] is also called the mean vector, 
E[x], of x; the k x k matrix with (i, j)th element C[xt, Xj] is called the covariance 
matrix, V[x], of x. If the components of x are independent random quantities, 
V[x] reduces to a diagonal matrix with (i, i)th entry given by V[xi\. 
As in the case of a single random quantity, exact forms for moments of an 
arbitrary transformation, y = g(x), are not available. We shall not need very 
general results in this area, but the following will occasionally prove useful. 
Proposition 3.5. (Approximate mean and covariance). If x is a random 
vector in $lk, with E[x] = /x, V[x] = S and y = g(x) is a one-to-one 
transformation of x such that g~l exists, then, subject to conditions on the 
distribution ofx and on the smoothness ofg, 
E[(g(x))t] = E[9i(x)] «fli(/i) + £tr [SV^/x)] 
V[fl(s)]« Jfl(/i)E4(/i), 
where, for i = 1,... ,k, 
(V2ft(M)) 
_ d2gj(x) 
il dxjdxi 
x=\i 
(Jgi^n 
dxt 
x=[i, 
where tr[.] denotes the trace of a matrix argument. 
Proof. This follows straightforwardly from a multivariate Taylor expansion; 
the details are tedious and we will omit them here. < 
3.2 Review of Probability Theory 
133 
A note on measure theory. Readers familiar with measure theory will be aware 
that there are many subtle steps in passing to a density representation of the 
probability measure Px. In particular, a detailed rigorous treatment of densities 
(Radon-Nikodym derivatives) requires statements about dominating measures 
and comments on the versions assumed for such densities. Readers unfamiliar 
with measure theory will already have assumed—correctly!—that we shall almost 
always be dealing with the "standard" versions of probability mass functions and 
densities (corresponding to counting and Lebesgue dominating measures and 
"smoothly" defined). Only occasionally, in Chapter 4, do we refer to general, 
i.e., non-density, forms. 
3.2.5 Some Particular Multivariate Distributions 
We conclude our review of probability theory with a selection of the more frequently 
used multivariate probability distributions; that is to say, distributions for random 
vectors. As in Section 3.2.3, no very detailed discussion will be given: see, for 
example Wilks (1962), Johnson and Kotz (1969, 1972) and DeGroot (1970) for 
further information. 
The Multinomial Distribution 
A discrete random vector x = {x\,..., x/t) has a multinomial distribution of  
dimension k, with parameters 6 = (9\,... ,0k) and n (0 < 9, < 1, £V#, < 1, 
n = 1,2,...) if its probability function M\ik(x | 6, n), for x, = 0,1,2,..., with 
X^=i Xi < n, is 
k 
The mean vector and covariance matrix are given by 
E[Xi] = n6i, V[xi] = n0j(l - 9t), C[xu xj\ = -nQfij. 
The mode(s) of the distribution is (are) located near E[x], satisfying 
n6i < M[xi] <(n+k- 1)0,, i = l,...,k; 
these inequalities, with the condition ^i=1 x, < n, restrict the possible modes to a 
relatively few points. 
134 
3 Generalisations 
The marginal distribution of x^ = (xi,..., xm), m < k, is the multinomial 
Mum(a;(m) \0i,...,6m,n). The conditional distribution of x^ given the  
remaining x,'s is also multinomial, and it depends on the remaining x, 's only through their 
sum s = X)*=m+i xi' specifically, 
p(x^\xm+l,...,xk) = Muk[x^ 
0} 6„ 
If a; = (xi,..., Xk) has density Mu* (x \ 6, n) then y = [y\..., yt) where 
2/i = X! + • • • + xiv ..., yt = xit_l+i + ... + xiv l<t<k, 
has density M\it{y \ <f>, n), where 
0i = 0i + • • • + 0^ ..., 0t = Vi+i + ' • • + ft*- 
If z is the sum of m independent random vectors having multinomial densities 
with parameters (6, n,), i = 1,..., m, then z also has a multinomial density with 
parameters 0 and (ni H \- nm). If fc = 1, Mu/t (a; | 6, n) reduces to the binomial 
density Bi(x| 0, n). 
If Xi,..., Xk are k independent Poisson random quantities with densities 
Pn(xj | Xi), then the joint distribution of a; = (xi,...,Xk) given Ylj=\ xi = n 
is multinomial Mu(x | 6, n), with #, = \/ Ylj=i Ar 
The Dirichlet Distribution 
A continuous random vector x = (xi,... , x*) has a Dirichlet distribution of 
dimension k, with parameters a = (a\,..., ak+i) (a, > 0, i = 1,..., k 4- 1) if 
its probability density Di^(a; | a), 0 < x, < 1 and xi + • • • -I- xk < 1, is 
Bik(x\a) = cx?-1---x?-\l-Y!LiXi)ak+l~1> 
where 
IE-Mo,) 
If k = 1, Di^(a; | a) reduces to the beta density Be(x | a\, 02). In the general case, 
the mean vector and covariance matrix are given by 
EM - ai ■ V\r] - ^N(l-^N). rw. _ 1 _ -E[Xj]E[xj] 
^FiJ - ^fc+i . M^iJ - t+1 , o[x!;Xjj - ,+1 
3.2 Review of Probability Theory 135 
If a* > 1, i = 1,..., fc, there is a mode given by 
M[Xi]- <Xi~1 ■--- 
The marginal distribution of a;(m) = (xi,...,xm), m < fc, is the Dirichlet 
p(XW) = Dim(a;(m> | au ..., am, Ej^+i";)- 
The conditional distribution, given xm+1,..., x*, of 
^ = 7—^ ' * = l,...,m 
is also Dirichlet, Dim(a;'1,..., x'm \ a\,..., am, a^+1). In particular, 
p(x-|xm+1,,..,x*) = Be(x-1 oti, YJf=\ai + ak+i -Q0> i = l,...,m. 
Moreover, if a; = (xy,..., x*) has density Di* (a: | a), then y = (j/i,..., yt) where 
yi = xi H \-xiv ..., yt = xi{_l+i H h x*, 1 < £ < fc, 
has density Dit(y | /3), where 
/3i =ax H hajp ..., Pt = Qtt_i+i H l-ajfc.A+i = ak+i- 
The Multinomial-Dirichlet Distribution 
A discrete random vector a; = (xi,...,Xk) has a multinomial-Dirichlet distribution 
of dimension fc, with parameters a = (a\,..., a/t+i) and n where a, > 0, and 
n = 1,2,..., if its probability function Mdjt(a: | a, n), for x, = 0,1,2,..., with 
Ei=i ^ < n, is 
Mdk(x\a,n) = cJJ 
*+i / J-XP 
~i 
where a'*' = I"TC=i (a + 3 ~ 1) defines the ascending factorial function, with 
xk+1 = n- Y.)=i xi ^d 
n! 
136 
3 Generalisations 
The mean vector and covariance matrix are given by 
OLi 
E[Xi] = npi, Pi = ^k+l 
EK+L 
i + E'ii1^ 
n + 2^~i otj 
C[xi,Xj] = - Jk+1 nplPj 
1 + E,=i <*i 
The marginal distribution of the subset {x1;...,xs}isa multinomial-Dirichlet 
with parameters {ai, ...,as, Y^tl aj ~ Xw=i aj} m& n- ^n particular, the  
marginal distribution of x; is the binomial-beta Bb(x, | a,, ]C,=i ai ~~ a«)-  
Moreover, the conditional distribution of {xs+i,..., Xk} given {x\,..., xs} is also 
multinomial-Dirichlet, with parameters {as+i,..., a*, X^i Q.y'~X^=s+i aj) ^^ 
n — X)1=i xj- P°r ^ interesting characterization of this distribution, see Basu and 
Pereira(1983). 
The Normal-Gamma Distribution 
A continuous bivariate random vector (x, y) has a normal-gamma distribution, 
with parameters n, A, a and 0, (n e K, A > 0, a > 0, ,9 > 0) if its density 
Ng(x, y\n,X, a, /3) is 
Ng(x,y | m, A,a,/3) = N(x| /i,\y)Ga{y \a,/3), x e K,y > 0, 
where the normal and gamma densities are defined in Section 3.2.2. It is clear 
from the definition, that the conditional density of x given y is N(x | fi, Ay) and 
that the marginal density of y is Ga(y | a, /3). Moreover, the marginal density of x 
isSt(x| n,\a//3,2a). 
The shape of a normal-gamma distribution is illustrated in Figure 3.1, where 
the probability density of Ng(x, y | 0,1,5,5) is displayed both as a surface and in 
terms of equal density contours. 
The Multivariate Normal Distribution 
A continuous random vector x = (x\,..., Xk) has a multivariate normal  
distribution of dimension k, with parameters £i = (n\,. ■ ■ ,^)andA, whereat € 3?*andA 
is a k x k symmetric positive-definite matrix, if its probability density N& (x \ /i, A) 
is 
Njt(a: | /i, A) = c exp{-i(a; - ^i)fA(a; - ^i)}, x e $lk, 
3.2 Review of Probability Theory 137 
Figure 3.1 The Normal-gamma density Ng(x, y 10,1,5,5) 
wherec = (27r)-fc/2|A|1/2. 
If fc = 1, so that A is a scalar, A, Nk{x | /i, A) reduces to the univariate normal 
density N(x \n,X). 
In the general case, E[xi] = fa, and, with £ = A-1 of general element cry, 
V[xi] = an and C[xt, Xj] = cry, so that V[x] = A-1. The parameter fj. therefore 
labels the mean vector and the parameter A the precision matrix (the inverse of the 
138 
3 Generalisations 
covariance matrix, £). If y = Ax, where A is an m x fc matrix of real numbers 
such that AHA1 is non-singular, then y has density Nm(y | An, (ASA')-1). 
In particular, the marginal density for any subvector of x is (multivariate) 
normal, of appropriate dimension, with mean vector and covariance matrix given 
by the corresponding subvector of /x and submatrix of A-1. 
Moreover, if x = (£1,2:2) is a partition of a;, with Xi having dimension fc,, 
and fci + fc2 = fc, and if the corresponding partitions of \i and A are 
\^2j ' V^21 ^22 
then the conditional density of X\ given x2 is also (multivariate) normal, of  
dimension fc! with mean vector and precision matrix given, respectively, by 
Hx ~ Af11A12(a;2 - /x2) and Au. 
The random quantity y = (x — fi)1 \(x — fi) has a x2{y I &) density. 
We also note that, from the form of the multivariate normal density, we can 
deduce the integral formula 
/, 
, exp{-l1{x-n)t\{x-n)}dx= * 
The Wishart Distribution 
A symmetric, positive-definite matrix x of random quantities x,j = Xju for i = 
1,..., fc, j = 1,..., fc, has a Wishart distribution of dimension fc, with parameters 
a and /3 (with 2a > fc - 1 and /3 a fc x fc symmetric, nonsingular matrix), if the 
density Wi/t (x \ a, f3) of the fc(fc + l)/2 dimensional random vector of the distinct 
entries of x is 
Wik{x \a,0) = c |sc|a-(*+1)/2 exp{- tr (fix)}, 
where c=|/9|a/rt(Q), 
rk(a) = ^k-^l[r( 
is the generalised gamma function and tr(.), as before, denotes the trace of a matrix 
argument. If fc = 1, so that (3 is a scalar (3, then W* (a; | a, 0) reduces to the gamma 
density G&(x\a,0). 
3.2 Review of Probability Theory 
139 
If {xi,..., xn} is a random sample of size n > 1 from a multivariate normal 
Nk(Xi |/x, A), and a: = n_1 J^a;,, then xis N^(x|/x, nA), and 
n 
5 = ^ (a:, -xXaJi-x)* 
8=1 
is independent of x, and has a Wishart distribution Wi^S | i(n — 1), ^A). 
The following properties of the Wishart distribution are easily established: 
E[x] = a(3~l and E[x~l] = (a - (fc + l)/2)"1/3; if y = AajAf where A is 
an m x fc matrix (m < fc) of real numbers, then y has a Wishart distribution of 
dimension m with parameters a and (A/3-1 Af)_1, if the latter exists; in particular, 
if x and /3_1 conformably partition into 
x= { xu x12 
, ^21 ^22 
/ \ CT21 CT22 / ' 
where x\\, a\\ are square h x h matrices (1 < h < fc), then x\\ has a Wishart 
distribution of dimension h with parameters a and (<rn)-1. Moreover, if x\,... ,xs 
are independent fc x fc random matrices, each with a Wishart distribution, with 
parameters a,, /3, i = 1,..., s, then x\-\ h xs also has a Wishart distribution, 
with parameters ax + ha, and (3. 
We note that, from the form of the Wishart density, we can deduce the integral 
formula 
' |a;|a-(':+1)/2exp{-tr(/3a;)} dx = c"1, 
/' 
the integration being understood to be with respect to the fc(fc + l)/2 distinct 
elements of the matrix x. 
The Multivariate Student Distribution 
A continuous random vector x = (x\,... , x*) has a multivariate Student  
distribution of dimension fc, with parameters /x = (/xi,..., Hk), A and a (/x € SR*, 
A a symmetric, positive-definite fc x fc matrix, a > 0) if its probability density 
Stfc(a;|fi,A,a)is 
Stk(x\(j,, A, a) = c 
where 
1 
-i -(a+*)/2 
H—(x — /x)(A(a: - /x) 
a 
xer, 
c = 
r((a + fe)/2) 
lAI1/2 • 
r(a/2)(a7r)*/2' 
If fc = 1, so that A is a scalar, A, then St^(a; | /x, A, a) reduces to the  
univariate Student density St(a; | (j., A, a). In the general case, E[x] = \i and V[x] = 
140 
3 Generalisations 
A-1 (a/(a — 2)). Although not exactly equal to the inverse of the covariance  
matrix, the parameter A is often referred to as the precision matrix of the distribution. If 
y = Ax, where A is an m x k matrix (m < k) of real numbers such that AX'1 A1 
is non-singular, then y has density Stm(y | Afi, (AA_1 A')-1, a). In particular, 
the marginal density for any subvector of x is (multivariate) Student, of  
appropriate dimension, with mean vector and precision matrix given by the corresponding 
subvector of /z and submatrix of A. Moreover, if x = (x\, x%) is a partition of x 
and the corresponding partitions of n and A are given by 
then the conditional density of X\, given X2 is also (multivariate) Student, of 
dimension k\, with a + fa degrees of freedom, and mean vector and precision 
matrix, respectively, given by 
Mi - A711Ai2(x2 -M2), 
. r a 4- fa " 
La + (x2 - /i2)'(A22 - A2iAr11Ai2)-1(:E2 - /i2). 
The random quantity y = (x — fJ,)lX(x — m) has an Fs(y | A;, a) density. 
The Multivariate Normal-Gamma Distribution 
A continuous random vector x = (x\,... ,xn) and a random quantity y have 
a joint multivariate normal-gamma distribution of dimension k, with parameters 
li,X,a,/3 (/j £ tf, A a fc x S; symmetric, positive-definite matrix, a > 0 and 
/3 > 0) if the joint probability density of x and y, NgA (x, y | /z, A, a, (3) is 
Ng*(a;, 3/1 /*, A, a, /?) = Nfc(x | n, ><y)Ga(y \ a, (3), 
where the multivariate normal and gamma densities have already been defined. 
From the definition, the conditional density of x given y is N^(x | /z, Ay) and 
the marginal density of y is Ga(y | a, /?). Moreover, the marginal density of x is 
Stt(as I/i, or^A, 2a). 
TTie Multivariate Normal-Wishart Distribution 
A continuous random vector x and a symmetric, positive-definite matrix of random 
quantities y have a joint Normal- Wishart distribution of dimension k, with  
parameters fi,X,a,/3 (/i € Sk, A > 0, integer 2a > k - 1, and (3 ak x k symmetric, 
non-singular matrix), if the probability density of x and the fc(fc 4- l)/2 distinct 
elements of y, Nwjt(x, y | /i, A, a, /3) is 
Nwfc(x, 2/1 m, A, a, /3) = Nfc(x| /z, Ay) Wi*(y | a, /3), 
where the multivariate normal and Wishart densities are as defined above. 
From the definition, the conditional density of x given y is N& (x \ fx, Ay) and 
the marginal density of y is Wi/t(y | a, /3). Moreover, the marginal density of x is 
Stt(ae|/i,Aa/r1,2a). 
3.3 Generalised Options and Utilities 
141 
The Bilateral Pareto Distribution 
A continuous bivariate random vector (x, y) has a bilateral Pareto distribution with 
parameters 0o, 0\, and a ({0o, 0\] £ 3£2, 0o < A, a > 0) if its density function 
Pa2(x,2/|a,^0,A)is 
Pa2(x, y | a, fa, ft) = c (j/ - x)-(a+2), s < fa, y > 0U 
where c = a(a + l)(/?i — /9o)a- The mean and variance are given by 
E[x] = 2^zA, E[y] = ^i^A, ifa>i, 
a — 1 a — 1 
and the correlation between x and y is — a-1. The marginal distributions of 
ti = 0i - x and £2 = y — 0o are both Pa(£ | /3i - 0o, a). 
3.3 GENERALISED OPTIONS AND UTILITIES 
3.3.1 Motivation and Preliminaries 
For reasons of mathematical or descriptive convenience, it is common in statistical 
decision problems to consider sets of options which consist of part or all of the real 
line (as in problems of point estimation) or are part of some more general space. It 
is therefore desirable to extend the concepts and results of Chapter 2 to a much more 
general mathematical setting, going beyond finite, or even countable, frameworks, 
first by taking £ to be a a-algebra and then suitably extending the fundamental 
notion of an option. 
In the finite case, an option was denoted by a = {cj | Ej, j £ J}, with the 
straightforward interpretation that, if option a is chosen, c, is the consequence 
of the occurrence of the event Ej. The extension of this function definition to 
infinite settings clearly requires some form of constructive limit process, analogous 
to that used in Lebesgue measure and integration theory in passing from simple 
(i.e., "step") functions to more general functions. Since the development given in 
Chapter 2 led to the assessment of options in terms of their expected utilities, the 
"natural" definition of limit that suggests itself is one based fundamentally on the 
expected utility idea (Bernardo, Ferrindiz and Smith, 1985). 
Let us therefore consider a decision problem {.4, £, C, <}, which is described 
by a probability space {fi, T, P} and utility function u : C —> 3t, and let 
V = {d : Q -► C; u(d) = [ «(d(w)) dP(u) < 00}. 
Jn 
142 
3 Generalisations 
In other words, 25 consists of those functions (soon to be called decisions) d :Q->C 
for which u o d = u(d(.)) is a random quantity whose expectation exists. In the 
case of the particular subset A of 25, u(a) = u(a | $1) is precisely the expected 
utility of the simple option a (see Definition 2.16 with G = Q; we shall return 
later to the case of a general conditioning event G and corresponding probability 
measure P(-\G)). In all the definitions and propositions in this section, A and 
{fl, JF, P} are to be understood as fixed background specifications. 
Definition 3.11. (Convergence in expected utility). For a given utility  
function, u : C —* 3t, a sequence of functions d\,d2,...inT> is said to u-converge 
to a function d in T>, written di —>u d, if and only if 
(i) u o di converges touod almost surely (with respect to P), 
(ii) u(di) -> u(d). 
Definition 3.12. (Decisions). For a given utility function, u : C —► % a  
function d 6 25 is a decision (generalised option) if and only if there exists a 
sequence ai,a,2,... of simple options such that at —>u d; the value of 
u(d) = limu(aj) 
i 
is then called the expected utility of the decision d. 
Discussion of Definitions 3.11 and 3.12. In abstract mathematical terms, 
the extension from simple functions, mapping C to 3t, to more general functions 
requires some form of limit process. However, the fundamental coherence result 
of Proposition 2.25 was that simple options should be compared in terms of their 
expected utilities. In order for this to carry over smoothly to decisions (generalised 
options), it is natural to require a constructive definition of the latter in terms of a 
limit concept directly expressed in terms of expected utilities. 
As it stands, however, this constructive definition does not provide a  
straightforward means of checking whether or not, given a specified utility function, u, a 
function d e 25 is or is not a generalised option. However, we can prove that any 
d e 25 such that u o d is essentially bounded (i.e., u o d is bounded except on a 
subset of Q. of P-measure zero) is a decision. More specifically, we can prove the 
following. 
Proposition 3.6. Given a utility function u : C —> 3t, for any function d e 25 
such that uo d is essentially bounded, there exist sequences ai,a,2,..- and 
a'i,a'2, ■ ■ ■ of simple options such that at —>u d, a^ —>u d and, for all i, 
u(cii) < u(d) < u(a't). 
3.3 Generalised Options and Utilities 
143 
Proof. We prove first that if u o d is essentially bounded above then there 
exists a sequence of simple acts a\,a^,..., such that at —*u d and w(aj) > u(d), 
for all i. (An exactly parallel proof exists if "above" is replaced by "below" and > 
by <.) We begin by defining the partitions {Etj, j £ Jt}, i = 1,2,..., where 
Eij ={wefi; «o d(u>) < -i} if j = -i2! - 1 
= {a; £ fl; u o d(u;) £ [j2"\ (j + 1)2~']} if j = -i2\ ..., i? - 1 
= {a; £ ft; u o d(u) > i} if j = i2\ 
For each i, this establishes a partition of Q. into 2(i2! -f 1) events, in such a 
way that two extreme events contain outcomes with values of u o d(ui) < —i or 
> i, whereas the other events contain outcomes whose values of u o d(ui) do not 
differ by more than 2~!. 
We now define a sequence {a^}, of simple options a; — {cy | E^, j £ J;} 
such that: 
(i) if P{Eij) = 0 then cy is an arbitrary element of C; 
(ii) if P(Ey) > 0 then ctj £ d{Eij) and 
uia^PiEij) > I u{d{u))dP{ui). 
JeH 
To see that the cy exist and are well defined, note that, since u{d) < oo, there 
exists Uij(d) < oo, defined by 
tZy(d) = —^ / u(d(«))dP(«); 
but, if u(d(u;)) < «y (d) for all u; £ i?y, then we would have 
/ u(d(u;))dP(u;)<uij(d)P(Eij), 
J Ei:j 
thus contradicting the definition of «y(d). 
By construction, at —* d almost surely. Hence, for all e > 0, there exists i 
such that u o d(u;) e [—i, i), with 2"J < e and, for this i, |a,-(u;) — d(u;)| < e. In 
addition, for all i, 
u(oi) = J2u(cij)p(Ea) ^ 5Z / «(d(w))dP(w) = «(d). 
144 
3 Generalisations 
To show that a, —♦„ d, it remains to prove that 
\u(d(u)) - u(a,i(u})) | dP(ui) —* 0, as i —> oo. 
/ 
Writing /n = fA. + fAc, where As = {a; £ Q | u(d(u>)) < -i}, we note that for 
< i 
sufficiently large i (larger than the essential supremum of u o d), 
Ja* 
i 
| u(d(«)) - u(a<(«)) | dP(«) < 2-'P(AJ), 
which converges to zero as i —> oo; moreover, since «(oj) > u(d), for all i, and 
u(d) < oo, 
/ |u(d(w)) - u(ai(u))\dP(u) < -2 / «(d(w))dP(w) 
and, since At —> 0 as i —♦ oo, this also converges to zero. <] 
In fact, we can show that any decision, whether or not u o d is essentially 
bounded, can be obtained as the limit of "bounding sequences of simple options" 
in the sense made precise in the following. 
Proposition 3.7. Given a utility function u : C —> 3t, for any decision d £ D, 
there exist sequences ai, a,2,..., and a'j, a'2,.. ■, of simple options such that 
o-i —*u d, a'} —*u d and, for all i, w(aj) < u(d) < w(a9- 
Proof. We shall show that there exists a sequence of simple options ai,a^,... 
such that at —*u d and, for all i, u(ai) < u(d). An obviously parallel proof exists 
for the other inequality. 
We first note that either u o d is essentially bounded above or it is not. In 
the former case, let K denote the essential supremum and define A0 = {a; £ fi; 
u o d{ui) = K}; in the latter case, define K = oo and Aq = 0. 
If P( Aq) > 0, choose a decreasing sequence of real numbers a, £ [0,1] such 
that ctj —♦ 0. Then by Axiom 4 and Proposition 2.6 there exists a sequence of 
standard events Si,S2,... such that Sj+i D Sj, n(Sj) = ctj and Pr(A0 n Sj) = 
F(Ao)aj, for all j; then, define Aj = Ao n Sj and choose a consequence c £ C 
such that u(c) < K. 
If P(A0) = 0, choose a consequence c £ C and an increasing sequence of real 
numbers J3j, such that fy —► oo and u(c) < A; let Aj = {a; £ fl; u o d(uj) > j3j}. 
In either case, define 
*,<«>-{*■"• u 
ueA) 
u £ Aj. 
3.3 Generalised Options and Utilities 
145 
Since d is a decision, there exists a sequence a\, a2,... of simple options such that 
a,i —*u d. If, for each at, we now define a new sequence of simple options a*-, by 
*r \ Ja»(a,)> if 
««(«) = |c, if 
a; eAJ 
w e A,-, 
we clearly have a*^ —>u dt, so, that, for all i, dt is a decision. Moreover, by 
construction dt —>u d, u(di) < u(d) and u o dj is bounded above, for all i Hence, 
by Proposition 3.6, there exist sequences of simple options 
af, <$,..., such that af ->u dt and u(af) > u(di). 
If we now choose a subsequence 
afx, ajj,..., such that u (afk ) - «(dj) < «(d) - «(dj), 
the required result follows, since for all k, 
u{di) <«(ajj|) <«(d) 
and dj —>u d implies that a,*' —> d. <j 
3.3.2 Generalised Preferences 
Given the adoption, for mathematical or descriptive convenience, of the extended 
framework developed in the previous section, it is natural to require that  
preferences among simple options should "carry over", under the limit process we have 
introduced, to the corresponding decisions. This is made precise in the following. 
Postulate 2. (Extension of the preference relation). Given a utility function 
u:C—>% for any decisions d\,d2, and sequences of simple options {at} and 
{a^} such that {a;} —*u d\, and {a^} —*u d2, we have: 
(i) if, for all i > Iq, for some Iq, a^ > a\, then d\ > c^,- 
(ii) if for all i > i0, for some i0, at > a'it then d\ > d2; 
The first part of the postulate simply captures the notion of the carry-over of 
preferences in the limit; the second part of the postulate is an obvious necessary 
condition for strict preference. 
Together with our previous axioms, this postulate enables us to establish a very 
general statement of the identification of quantitative coherence with the principle 
of maximising expected utility. 
146 
3 Generalisations 
Proposition 3.8. (Maximisation of expected utility for decisions). 
Given any two decisions d\,d2, 
d\ > di <=>• u{d\) > u(d2). 
Proof. We first establish that u(d2) > u(d\) implies that d2 > d\. By  
Proposition 3.7, there exist sequences of simple options a\, a2,..., and a\,a'2,..., such 
that a,i —♦„ di, a't —♦„ d2 and, for all i, u(o,i) > w(di) and w(aj) < u(d2). With 
e = [u(d2) — «(di)]/3, we can choose ii, i2 such that, for all j > max{ii, i2}, 
u(a.j) - u(d\) < e, u(dk) - u(a'j) < e, 
and we can choose i[, i'2 such that, for all j > i[, 
u{aj) — u(di) < w(ajj) — u(di) 
and, for all j > i'2, 
u(d2) -u{a'j) < u{d2) -u{a'i2). 
It follows from Proposition 2.25 that, for all j > max{i',, i'2}, 
a'j > a'i2 > aix > a,j 
and so, by Postulate 2, d2> d\. 
To complete the proof (d\ ~ d2 => u{d\) = u(d2) being obvious), we must 
show that u(d\) = u{d2) implies that d\ ~ d2. By Proposition 3.7, there exist 
sequences of simple options (a\',k = l,2,3,4) such that a] ' —»u d\ for k = 1,2, 
a,- ' —»u d2 for A; = 3,4, and, for all i, 
uiaf]) < ufa) < u(af]), u(af]) < u{d2) < u{af]). 
Since we have u{d\) = u(d2), this implies, by Proposition 2.25, that af' > aj1', 
and af] \ 
d\ ~ d2. 
and a) ' > a\ \ for all i, and hence, by Postulate 2, d2 > d\ and d\ > d2, so that 
Proposition 3.9. For any G > 0, 
d! >G d2 <^=> w(dj | G) > u(d2 | G). 
Proo/. Throughout the above, the probability measure P(-) can be replaced 
by P(-1 G) without any basic modifications to the proofs and results Writing 
uG(d)= [ uod(u)dP(u\G), 
Jn 
it is easily verified that P(G)uc(d) = u( lg od), where 1g is the indicator function 
of G. It follows that if u o d is integrable with respect to P(-) then it is integrable 
with respect to P(- \ G). < 
3.3 Generalised Options and Utilities 
147 
This establishes in full generality that the decision criterion of maximising 
the expected utility is the only criterion which is compatible with an intuitive set 
of quantitative coherence axioms and the natural mathematical extensions  
encapsulated in Postulates 1 and 2. Specifically, we have shown that, given a general 
decision problem, where u; £ SI labels the uncertain outcomes associated with 
the problem, u(d(u;)) describes the current preferences among consequences and 
p{w), the probability density of P with respect to the appropriate dominating 
measure, describes current beliefs about a;, the optimal action is that d^ which 
maximises the expected utility, 
u(d) = / u(d(u})) p{uj) dw. 
As we saw in Section 2.6.3, it is natural before making a decision to consider 
trying to reduce the current uncertainty by obtaining further information by  
experimentation. Whether or not this is sensible obviously depends on the relative 
costs and benefits of such additional information, and we shall now extend the 
notions related to the value of information, introduced in Section 2.6.3, into the 
more general mathematical framework established in this chapter. 
3.3.3 The Value of Information 
For the general decision problem, the decision tree for experimental design, given 
originally in Figure 2.6, now takes the form given in Figure 3.2, where, as in Section 
2.6.3, the utility notation is extended to make explicit the possible dependence on 
the experiment performed e (or eo if no data are collected) and the data obtained, 
x. If dg is the optimal decision corresponding to eo, the expected utility from an 
optimal decision with no additional information is defined by 
u(e0) = u{dl,e0) = sup / u{d,e0,u})p(v \ e0,d)dw. 
d J 
Let d* be the optimal decision after experiment e has been performed and data 
x have been obtained, so that u{d*x, e, a;), the expected utility from the optimal 
decision given e and x, is 
u{d*x, e, x) = sup / u(d, e, x, u;)p(u; | e, x, d)du, 
d Jtt 
and, hence, the expected utility from the optimal decision following e is 
u(e) = I u(dx,e,x)p(x\e)dx, 
Jx 
148 
3 Generalisations 
eo 
u(d, e, x,u) 
u(d, eo, u>) 
Figure 3.2 Generalised decision tree for experimental design 
where p(a; | e) describes beliefs about the occurrence of x, were e to be performed. 
Proposition 3.10. (Optimal experimental design). The optimal decision is 
to perform experiment e* if u{e*) = maxe u(e) and u(e*) > u(eo), and to 
perform no experiment otherwise. 
Proof. This follows immediately. <j 
The expected value of the information provided by additional data x may 
be computed as the (posterior) expected difference between the utilities which 
correspond to optimal decisions after and before the data. Thus, 
Definition 3.13. (The value of additional information). 
(i) The expected value of the information provided by x, is 
v(e,x)= / {u(d*,e,x,u) - u(d%,eo,u)}p(u\e,x,d*x)dw \ 
Jn 
(ii) the expected value of the experiment e is 
v(e) = / v(e,x)p(x\e)dx. 
Jx 
Let us now consider the optimal decisions which would be available to us if 
we knew the value of a;. Thus, let c£, be the optimal decision given a;; i.e., such 
that, for all d, 
u(dl, e0,u})>u(d,eo,u), a; € Q. 
3.3 Generalised Options and Utilities 
149 
Then, given a;, the loss suffered by choosing another decision d^d*^ would be 
u(d*u,eo,u) -u(d,eo,u). 
For d = dg, the optimal decision with no additional data, this utility difference 
measures (conditional on a;) the value of perfect information. Its expected value 
with respect to p(u;) will define, under certain conditions, an upper bound on 
the increase in utility which additional information about w could be expected to 
provide. 
Definition 3.14. (Expected value of perfect information). The opportunity 
loss of choosing d is defined to be 
l(d,u) = u(dt,,eo,u) -u{d,e0,u), 
and the expected value of perfect information about w is defined by 
v*{e0} = / l(d^,u)p(u\e0,do)du, 
Jo. 
where dj$ is the optimal decision with no additional information. 
As we remarked in Section 2.6.3, in many situations the utility function may 
often be thought of as made up of two separate components: the experimental cost 
of performing e and obtaining x, and the utility of directly taking decision d and 
finding u; to be the state of the world. Given such an (additive) decomposition, we 
can establish a useful upper bound for the expected value of an experiment. 
Proposition 3.11. (Additive decomposition). If the utility function has the 
form 
u(d, e, x, u) = u(d, e0, ui) — c(e, x), 
with c(e, x) > 0, and the probability distributions are such that 
p{u | e, x, d) = p(u | e, x), p{u \ e0, d) = p(u | e0), 
then, for any available experiment e, 
v(e) <v*{e0)-c(e), 
where c(e) = / c(e, a;) p(x \ e) dx is the expected cost ofe. 
Proof. This closely parallels the proof, given in Proposition 2.27, for the finite 
case. <] 
This concludes the mathematical extension of the basic framework and  
associated axioms. In the next section, we reconsider the important special problem 
of statistical inference, previously discussed in detail in its finitistic setting in  
Section 2.7. 
150 
3 Generalisations 
3.4 GENERALISED INFORMATION MEASURES 
3.4.1 The General Problem of Reporting Beliefs 
In Section 2.7, we argued that the problem of reporting a degree of belief  
distribution for a (finite) class of exclusive and exhaustive "hypotheses" {Hj, j € </}, 
conditional on some relevant data D and initial state of information Mo, could be 
formulated as a decision problem {£,C, A, <}. Here, {Ej, j e J} is a partition 
of fi, consisting of elements of £, with the interpretation Ej = "hypothesis Hj is 
true", A relates to 
Q = {q = (qj, je J); qj>0 J2jeJqj = 1}, 
where g, is the probability which, conditional on D, an individual reports as the 
probability of Ej being true, and the set of consequences C consists of all pairs 
(q, Ej), representing the possible conjunction of reported beliefs and true  
hypotheses. In the previous finitistic setting, we denoted by 
p={Pj = P(Ej\D),jeJ}, Pj>0, £je,Pj = l, 
the probability measure describing an individual's actual beliefs, conditional on D. 
We then proceeded to consider a special class of utility functions (score functions) 
appropriate to this reporting problem and to examine the resulting forms of implied 
decisions and the links with information theory. In this section, we shall generalise 
these concepts and results to the extended framework developed in the previous 
sections. 
The first generalisation consists in noting that the set of alternative  
"hypotheses" now corresponds to the set of possible values of a (possibly continuous) random 
vector, u;, say, labelling the "unknown states of the world", so that the relevant  
uncertain events are Eu = {u>}, wf(], with the interpretation Ew = "the hypothesis 
u> is true". Quantitative coherence requires that any particular individual's  
uncertainty about u>, given data D and initial state of information M0, should be 
represented by a probability distribution P over a cr-algebra of subsets of fi, which 
we shall assume can be described by a density (to be understood as a mass function 
in the discrete case) 
pw(. | D) = \p(w | D), we(], p(u | D) > 0, j p{u | D) du = 1 
We shall take the set of possible inference statements to be the set of probability 
distributions for u>, compatible with D. We denote by V the set of functions dp, 
one for each pw(-1 D), which map u> to the pair (pw(-1 D), u>). 
3.4 Generalised Information Measures 
151 
3.4.2 The Utility of a General Probability Distribution 
In this general setting, the problem of providing an inference statement about a class 
of exclusive and exhaustive "hypotheses" {u>, u> G ft}, conditional on data D, is 
a decision problem, which we can conveniently denote by {T>, ft, u, P}, where ft 
is the set of possible values of the random quantity u>, V relates to the class of 
probability densities for u> G ft compatible with D, 
Q=\ Qui-1 D)\ g(u> | £>) > 0, u> G ft and / q{u \ D) dw = 11 . 
where qu (• | D) is the density which an individual reports as the basis for describing 
beliefs about u> conditional on D. The set of consequences C consists of all pairs 
(dq, u>), representing the conjunction of reported beliefs and true "states of nature". 
Throughout this section, we shall denote an individual's actual belief density by 
pw(-1 D). The decision space "D consists of d9's corresponding to choosing to report 
Qu(-\D)'s and defined by dq(u>) = (<?w(-1 D), u). We shall assume the individual 
to be coherent, so that dp e V. Without loss of generality, we shall assume that 
Pu(-1 D) and the qu(-1 D) G Q are strictly positive probability densities, so that, 
for all w e ft, p(u> | D) > 0 and q(u | Z?) > 0 for all dq e V. 
We complete the specification of this decision problem, by inducing the  
preference ordering through direct specification of a utility function u, which describes 
the "value" u(qu]{- \ D),iv) of reporting the probability density qu(-1 D) were a; 
to turn out to be the true "state of nature". For this purpose and with the same  
motivation, we generalise the notion of score function introduced in Definition 2.20. 
Definition 3.15. (Score function). A score function for probability densities 
qu(-\D) defined on ft, is a mapping u : Q x ft —> 3?. A score function is said 
to be smooth if it is continuously differentiate as a function ofq{u} \ D) for 
each u) G ft. 
The solution to the decision problem is then to report the density qu(-1 D) 
which maximises the expected utility 
u(dq) = / u(Qu(- \D),u>) p(u> I D) duj. 
Jn 
As in our earlier development in Chapter 2, we shall wish to restrict utility 
functions for the reporting problem in such a way as to encourage a coherent 
individual to be honest, given data D, in the sense that his or her expected utility 
is maximised if and only if dq is chosen such that, for each a; G ft, q{u) | jD) = 
p(u> | D). The appropriate generalisation of Definition 2.21 is the following. 
152 
3 Generalisations 
Definition 3.16. (Proper score function). A score function u is proper if for 
each strictly positive probability density p<j{- \ D), 
sup / u(qu,(-\D),u;) p(u)\D)dw= \ u(pu(- \ D),u) p(u>\D)dw, 
where the supremum, taken over the class Q of all distribution for uj compatible 
with D, is attained if and only if'qu(-\D) = qu(-\ D), up to sets of zero 
measure. 
As in the finite case (see Definition 2.22), the simplest proper score function 
in the general case is the quadratic. 
Definition 3.17. (Quadratic score function). A quadratic score function for 
probability densities qu (• | D) £ Q defined on fi is a mapping u : Q x fi —> 3? 
of the form 
u(<k,(-|£>),a;) = Al2q(u}\D)- f q2(u> | D) dw \ + B(u>), A > 0, 
such that the otherwise arbitrary function, B(-), ensures the existence ofu(dq) 
for all dq e V. 
Proposition 3.12. A quadratic score function is proper. 
Proof. Given data D, we must choose qu(- \ D) e Q to maximise 
I(dq) = / u(qu(-1 D), u) p(u> | D) du 
Jn 
= [ Al2q(u>\D)- f q2(u>\D)du>\ + B(u>) 
p{u) | D) dw , 
subject to / q{u> \ D)dw = 1. Rearranging, it is easily seen that this is equivalent 
to maximising 
- [(p(u>\D)-q(u>\D))2daJ, 
from which it follows that we require q{u)\D) = p(u> \ D) for almost all u> e fi. 
We note again (cf. Proposition 2.28) that the constraint / q(u) \ D)dw = 1 has not 
been needed in establishing this result for the quadratic scoring rule. < 
3.4 Generalised Information Measures 
153 
In fact, as we argued in Section 2.7, for the problem of reporting pure inference 
statements it is natural to restrict further the class of appropriate utility functions. 
The following generalises Definition 2.23. 
Definition 3.18. (Local score function). A score function is local if for each 
Qu(-1 D) e Q there exist functions u^, u> e fi, defined on 3?+ such that 
u(qu(-\D),w) =uu(q(u;\D)). 
Note that, as in Definition 2.23, the functional form, Uu(-), of the dependence 
of the score function on the density value q(u> \ D) which dq assigns to u> is allowed 
to vary with the particular u> in question. Intuitively, this enables us to incorporate 
the possibility that "bad predictions", i.e., values of q(u> \ D), for some "true states 
of nature", u>, may be judged more harshly than others. 
The next result generalises Proposition 2.29 and characterises the form of a 
smooth, proper, local score function. 
Proposition 3.13. (Characterisation of proper local score functions). 
Ifu : Q x fi —> 3? is a smooth, proper, local score function, then it must be of 
theform 
«(<*,(•1 D),u) = Alogq(u> | D) + B{u>) 
where A> Ois an arbitrary constant and B(-) is an arbitrary function o/u>, 
subject to the existence ofu{dq)for all dq e V. 
Proof. Given data D, we need to maximise, with respect to q(-1 D), the  
expected utility 
u(dq) = / u(qu(-1 D),u) p{u | D) dw 
subject to /n q{u | D)dw = 1. Since u is local, this reduces to finding an extremal 
of 
F{qu{-\D))= IUu(q{u\D))p{u\D)du>-A fq(u\D)du-l . 
However, for qu(-1 D) to give a stationary value of F(qu(-1 D)) it is necessary 
that 
^-F(q(of\D) + QT(u)) =0, 
da v ' a=o 
for any function r : O —> 3? of sufficiently small norm (see, for example, Jeffreys 
and Jeffreys, 1946, Chapter 10). This condition reduces to the differential equation 
Diuu (q{u | D)) p{u\D)-A = 0, 
154 
3 Generalisations 
where D\Uu denotes the first derivative of u^. But, since u^ is proper, the  
maximum of F(qu(-1 D)) must be attained at qu(-1 D) — pu(- \ D), so that a smooth, 
proper, local utility function must satisfy the differential equation 
£>i«w(p(« I D)) p(u>\D)-A = 0, 
whose solution is given by 
Uu(p{of\D)) = A\ogp(u\D) + B(u>), 
as stated. < 
This result prompts us to make the following formal definition. 
Definition 3.19. (Logarithmic score function). A logarithmic score  
function for probability densities qu(-\D) G Q defined on fi is a mapping 
u:Qxfi-tS of the form 
«(<*,(•1 D),of) = A\ogq(u> | D) + B(u), 
A > 0, B(-) arbitrary, subject to the existence ofu(dq)for all dq € V. 
For additional discussion of generalised score functions see Good(1969) and 
Buehler(1971). 
3.4.3 Generalised Approximation and Discrepancy 
As we remarked in Section 2.7.3, although the optimal solution to an inference 
problem under the above conditions is to state one's actual beliefs, there may 
be technical reasons why the computation of this "optimal" density pu(- \ D) is 
difficult. In such cases, we may need to seek a tractable approximation, qu(-1 D), 
say, which is in some sense "close" to pw {-\D), but much easier to specify. As in 
the previous discussion of this idea, we shall need to examine carefully this notion 
of "closeness". The next result generalises Proposition 2.30. 
Proposition 3.14. (Expected loss in probability reporting). If preferences 
are described by a logarithmic score function, the expected loss of utility 
in reporting a probability density q(u> \ D), rather than the density p(u | D) 
representing actual beliefs, is given by 
*(M- \D) | PU- \D)) = AJp{Lj\D)log^^du,. 
Moreover, 6(gu)(- \D) | pw(-1 D)) is non-negative and is zero if, and only if, 
qu,(-\D)=Pu{-\D). 
3.4 Generalised Information Measures 155 
Proof. From Definition 3.19 the expected utility of reporting q<j (• | D) when 
Pu{-1 D) is the actual belief distribution is given by 
u{dq) = f [A\ogq{u | D) + B(u)] p(u \ D) du>, 
so that 
*(&>(■1 D) | pw(-1 £>)) = u(^) - u{dq) = A [p(u> | D) log ^j^T dw- 
The final condition follows either from the fact that u is proper, so that u{dp) > 
u(d9) with equality if and only if gw(-1 D) = pw(-1 Z>); or directly from the fact 
that for all x > 0, log x < x - 1 with equality if and only if x = 1 (cf. Proposition 
2.30). < 
As in the finitistic discussion of Section 2.7, the above result suggests a natural, 
general measure of "lack of fit", or discrepancy, between a distribution and an 
approximation, when preferences are described by a logarithmic score function. 
Definition 3.20. (Discrepancy of an approximation). 
The discrepancy between a strictly positive probability density Pu,(-) ond an 
approximation pui), u> e fi, is defined by 
S{pu(-)\pu(-))=Jp(uf)\og^du. 
Example 3.4. (General normal approximations). Suppose that p(u) > 0, u E 5ft, is 
an arbitrary density on the real line, with finite first two moments given by 
/oo /*oo 
u) p((j) dw = m. / (u> — m)2p(ui) dui = t'1, 
oo J—oo 
and that we wish to approximatep(-) by ap(-) corresponding to a normal density, N(u \p,X), 
with labelling parameters p, A chosen to minimise the discrepancy measure given in  
Definition 3.20. It is easy to see that, subject to the given constraints, minimising 6(p \ p) with 
respect to p and A is equivalent to minimising 
F 
p(cj) log N(w I p, A) dw. 
J oo 
and hence to minimising 
A f°° 
-£logA+-/ p(w)(u - pfdio. 
156 
3 Generalisations 
m + m — /i)2, this 
Invoking the two moment constraints, and writing (u — n)2 = (u 
reduces to minimising, with respect to fi and A, the expression 
-logA + - + A(m-/i)2. 
It follows that the optimal choice is fi = m, A = t. In other words, for the reporting 
problem with a logarithmic score function, the best normal approximation to a distribution 
on the real line (whose mean and variance exist) is the normal distribution having the same 
mean and variance. 
Example 3.5. (Normal approximations to Student distributions). Suppose that we 
wish to approximate the density St(x | /x, A, a), a > 2, by a normal density. We have just 
shown that the best normal approximation to any distribution is that with the same first two 
moments (assuming the latter to exist, corresponding here to the restriction a > 2). Thus, 
recalling from Section 3.2.2 that the mean and precision of St(x | //, A, a) are given by (J. and 
\(a — 2)/a, respectively, it follows that the best normal approximation to St(x | //, A, a) is 
provided by N(x | ^, A(a - 2)/a). From Definition 3.20, the corresponding discrepancy 
will be 
<5(N|St) = |st(x|/,,A,a)log. St^'A'a) 
' N(x|/x,A(a-2)/a) 
dx. 
0.08. 
<5(N | St) 
0.02.. 
0 10 20 
Figure 3.3 Discrepancy between Student and normal densities 
This is easily evaluated (see, for example, Bernardo, 1978a) using the fact that the entropy 
of a Student distribution is given by 
H{St(x | n, A, a)} = - / St(x | n, A, a) log St(x \n,\,a)dx 
log 
r((a+l)/2) 
r(a/2)r(l/2) ' 2 
^M^ki)-^) 
3.4 Generalised Information Measures 
157 
where tp(z) = T'(z)/T(z) denotes the digamma function (see, for example, Abramowitz 
and Stegun, 1964), from which it follows that <5(N | St) may be written as 
which only depends on the degrees of freedom, a, of the Student distribution. Figure 3.3 
shows a plot of <5(N | St) against a. 
Using Stirling's approximation, 
logr(2) ~ (z - - J log 2 - z + - log(27r), 
we obtain, for moderate to large values of a, 
<5(N | St) « [a{a - 2)}~l = 0(l/a2), 
so that [a(a — 2)]_1 provides a simple, approximate measure of the departure from normality 
of a Student distribution. 
3.4.4 Generalised Information 
In Section 2.7.4, we examined, in the finitistic context, the increase in expected 
utility provided by given data D. We now extend this analysis to the general 
setting, writing x to denote observed data D. 
Proposition 3.15. (Expected utility of data). If preferences are described by 
a logarithmic score function for the class of probability densities p(u> | x) 
defined on fi, then the expected increase in utility provided by data x, when 
the prior probability density is p(u>), is given by 
Aj^lx)]^±M^ 
where p{u> \ x) is the density of the posterior distribution for u>, given x. This 
expected increase in utility is non-negative, and zero if, and only if, p(u> | x) 
is identical to p(u>). 
Proof. Using Definition 3.19, the expected increase in utility provided by x 
is given by 
/ j[Alogp(u>|x) + £(u>)] - [Alogp(u>) + £(u>)]| p{u\x) du 
f p(u; | x) 
= Ap(u>\x) log \\J du, 
J P(w) 
which, by Proposition 3.14, is non-negative with equality if and only if p(u> | x) 
and p(u>) are identical. < 
158 
3 Generalisations 
The following natural definition of the amount of information provided by the 
data extends that given in Definition 2.26. 
Definition 3.21. {Information from data). The amount of information about 
w£(1 provided by data x when the prior density is p(u>) is given by 
I{x | M")} = Jp(o>\*) log ^j^ da;, 
where p(u> | x) is the corresponding posterior density. 
As in the finite case, it is interesting to note that the amount of information 
provided by x is equivalent to the discrepancy measure if the prior is considered as 
an approximation to the posterior. Alternatively, we see that log p(u>), log p(u> \ x), 
respectively, measure how "good", on a logarithmic scale, the prior and posterior 
are at "predicting" the "true state of nature" u>, so that logp(u> | a;) — logp(u>) 
is a measure of the usefulness of x were u> known to be the true value. Thus 
I { x | Pu (■)} is simply the expected value of that utility difference with respect to 
the posterior density, given x. 
The functional J p(u>) \ogp(u>) dw has been used (see e.g., Lindley, 1956, and 
references therein) as a measure of the 'absolute' information about u contained 
in the probability density p(u>). The increase in utility from observing x is then 
/ p(o> | a;) logp(u; \x)dw — I p(co) logp(o>) dw, 
instead of our Definition 3.21. However, this expression is not invariant under 
one-to-one transformations of u, a property which seems to us to be essential. 
Note, however, that both expressions have the same expectation with respect 
to the distribution of x. Draper and Guttman (1969) put forward yet another 
non-invariant definition of information. 
Additional references on statistical information concepts are Renyi (1964, 
1966, 1967), Goel and DeGroot (1979) and De Waal and Groenewald (1989). 
More generally, we may wish to step back to the situation before data become 
available, and consider the idea of the amount of information to be expected from 
an experiment e. We therefore generalise Definition 2.27. 
Definition 3.22. (Expected information from an experiment). The expected 
information to be provided by an experiment e about u> e fi, when the prior 
density is p(u>) is given by 
l{e\pu{-)}= \ I{x \Pu{-)}p(x\e)dx, 
Jx 
where the distribution of the possible data outcomes x e X resulting from the 
experiment e is described by p(x | e). 
3.4 Generalised Information Measures 
159 
The following result, which is a generalisation of Proposition 2.32, provides 
an alternative expression for /{e | pu)()}. 
Proposition 3.16. An alternative expression for the expected information is 
l{e\pu(-)}= [ fp(o>,x\e) log ^"'^f.dwdx, 
1 ' WJ J J V 6p(u;)p(x|e) 
where p(u>, x | e) = p(u> | x, e)p(x \ e) andp{u} \ x, e) is the posterior density 
for u> given data x and priordensity p(u>). Moreover, l{e | Pu>(-)} > 0, with 
equality if and only if x and u> are independent random quantities, so that 
p(u>, x | e) = p(u>)p(x | e)for all u> and x. 
Proof. 
I{e |pw(-)} = / | P(v\x,e) log JJx'6 ^ | P(a:|e)da; 
"// 
plo; x, e) p(x e) log ' . , . ouj ax 
p(u)p(x\e) 
and the result now follows from the fact that p(u> \x,e) = p(o; | x, e)p(x \ e). 
Moreover, since, by Proposition 3.14, /{e |pu,()} > 0 witn equality if and only 
if p(u> | x, e) = p(u>), it follows from Definition 3.19 that l{e | Pu(-)} > 0 with 
equality if and only if, for all u> and x, p(u>, x\e) = p(u>)p(x | e). < 
Maximisation of the expected Shannon information was proposed by Lind- 
ley (1956) as a "reasonable" ad hoc criterion for choosing among alternative  
experiments. Fedorov (1972) proved later that certain classical design criteria (in 
particular, Z?-optimality) are special cases of this when normal distributions are 
assumed. We have shown that maximising expected information is just a particular 
(albeit important) case of the general criterion, implied by quantitative coherence, 
of maximising the expected utility in the case of pure inference problems. See 
Poison (1992) for a closely related argument. 
It follows from Proposition 2.31 and the last remark, that someone who adopts 
the classical Z?-optimality criterion of optimal design under standard normality 
assumptions should, for consistency, have preferences which are described by a 
logarithmic scoring rule; otherwise, such designs are not optimal with respect to 
his or her underlying preferences. 
There is a considerable literature on the Bayesian design of experiments, which 
we will not attempt to review here. A detailed discussion will be given in the volume 
Bayesian Methods. We note that important references include Blackwell (1951, 
160 
3 Generalisations 
1953), Lindley (1956), Chernoff (1959), Stone (1959), DeGroot (1962, 1970), 
Duncan and DeGroot (1976), Bandemer (1977), Smith and Verdinelli (1980), 
Pilz (1983/1991), Chaloner (1984), Sugden (1985), Mazloum and Meeden (1987), 
Felsenstein (1988, 1992), DasGupta and Studden (1991), El-Krunz and Studden 
(1991), Pardo et al. (1991), Mitchell and Morris (1992), Pham-Gia and Turkkan 
(1992), Verdinelli (1992), Verdinelli and Kadane (1992), Lindley and Deely (1993), 
Lad and Deely (1994) and Parmigiani and Berry (1994). 
3.5 DISCUSSION AND FURTHER REFERENCES 
3.5.1 The Role of Mathematics 
The translation of any substantive theory into a precise mathematical formalism 
necessarily involves an element of idealisation. 
We have already had occasion to remark on aspects of this problem in  
Chapter 2, in the context of using real numbers rather than subsets of the rationals to 
represent actual measurements (necessarily "finitised" by inherent accuracy limits 
of the uncertainty apparatus). Similar remarks are obviously called for in the  
context of using, for example, probability densities to represent belief distributions for 
real-valued observables. 
In some situations, as we shall see in Chapter 4, the adoption of specific 
forms of density may follow from simple, structural assumptions about the form 
of the belief distribution. In other situations, however, if we really try to think of 
such a density as being practically identified by expressions of preference among, 
say, standard options, we would encounter the obvious operational problem that, 
implicitly, an infinite number of revealed preferences would be required. 
Clearly, in such situations the precise mathematical form of a density is likely 
to have arisen as an approximation to a "rough shape" obtained from some finite 
elicitation or observation process, and has been chosen, arbitrarily, for reasons 
of mathematical convenience, from an available mathematical tool-kit. Similar 
remarks apply to the choice, for descriptive or mathematical convenience, of infinite 
sets to represent consequences or decisions, with the attendant problems of defining 
appropriate concepts of expected utility. 
There are obvious dangers, therefore, in accepting too uncritically any  
orientation, or would-be insightful mathematical analysis, that flows from arbitrary, 
idealized mathematical inputs into the general quantitative coherence theory.  
However, given an awareness of the dangers involved, we can still systematically make 
use of the power and elegance of the (idealised) mathematics by simultaneously 
asserting, as a central tenet of our approach, a concern with the robustness and  
sensitivity of the output of an analysis to the form of input assumed (see Section 5.6.3). 
Of course, we shall later have to make precise the sense in which these terms are 
to be interpreted and the actual forms of procedures to be adopted. That being 
3.5 Discussion and Further References 
161 
understood, our approach, as with the earlier formalism of Chapter 2, will be to 
work with the mathematical idealization, in order to exploit its potential power and 
insight, while constantly bearing in mind the need for a large pinch of salt and a 
repertoire of sensitivity diagnostics. 
3.5.2 Critical Issues 
We shall comment further on three aspects of the general mathematical structure 
we have developed and will be using throughout the remainder of this volume. 
These will be dealt with under the following subheadings: (i) Finite versus  
Countable Additivity; (ii) Measure versus Linear Theory; (iii) Proper versus Improper 
Probabilities; (iv) Abstract versus Concrete Mathematics. 
Finite versus Countable Additivity 
In Chapter 2, we developed, from a directly intuitive and operational perspective, 
a minimal mathematical framework for a theory of quantitative coherence. The 
role of the mathematics employed in this development was simply that of a tool to 
capture the essentials of the substantive concepts and theory; within the resulting 
finitistic framework we then established that uncertainties should be represented in 
terms of finitely additive probabilities. 
The generalisations and extensions of the theory given in the present chapter 
lead, instead, to the mathematical framework of countable additivity, within which 
we have available the full panoply of analytic tools from mathematical probability 
theory. The latter is clearly highly desirable from the point of view of mathematical 
convenience, but it is important to pause and consider whether the development of 
a more convenient mathematical framework has been achieved at the expense of a 
distortion of the basic concepts and ideas. 
First, let us emphasise that, from a philosophical perspective, the monotone 
continuity postulate introduced in Section 3.1.2 does not have the fundamental 
status of the axioms presented in Chapter 2. We regard the latter as encapsulating 
the essence of what is required for a theory of quantitative coherence. The former 
is an "optional extra" assumption that one might be comfortable with in specific 
contexts, but should in no way be obliged to accept as a prerequisite for quantitative 
coherence. 
Secondly, we note that the effect of accepting that preferences should conform 
to the monotone continuity postulate is to restrict one's available (in the sense of 
coherent) belief specifications to a subset of the finitely additive uncertainty  
measures; namely, those that are also countably additive. This is, of course, potentially 
disturbing from a subjectivist perspective, since a key feature of the theory is that 
the only constraints on belief specifications should be that they are coherent. For 
some such representations to be ruled out a priori, as a consequence of a postulate 
adopted purely for mathematical convenience, would indeed be a distortion of the 
162 
3 Generalisations 
theory. This is why we regard such a postulate as different from the basic axioms. 
However, provided one is aware of, and not concerned about, the implicit restriction 
of the available belief representations, its adoption may be very natural in contexts 
where one is, in any case, prepared to work in an extended mathematical setting. 
Throughout this work, we shall, in fact, make systematic use of concepts 
and tools from mathematical probability theory, without further concern or debate 
about this issue. However, to underline what we already said in Section 3.1.2, it is 
important to be on guard and to be aware that distortions might occur. To this end, 
we draw attention to some key references to which the reader may wish to refer in 
order to heighten such awareness and to study in detail the issues involved. 
De Finetti (1970/1974, pp. 116-133, 173-177 and 228-241; 1970/1975, 
pp. 267-276 and 340-361), provides a wealth of detailed analysis, illustration and 
comment on the issues surrounding finite versus countable (and other) additivity 
assumptions, his own analysis being motivated throughout by the guiding principle 
that 
... mathematics is an instrument which should conform itself strictly to the  
exigencies of the field in which it is to be applied. (1970/1974, p. 3) 
Further technical and philosophical discussion is given in de Finetti (1972, Chapters 
5 and 6); see, also, Stone (1986). Systematic use of finite additivity in decision- 
related contexts is exemplified in Dubins and Savage (1965/1976), Heath and  
Sudderth (1978, 1989), Stone (1979b), Hill (1980), Sudderth (1980), Seidenfeld and 
Schervish (1983), Hill and Lane (1984), Regazzini (1987) and Regazzini and Petris 
(1993). A discussion of the statistical implications of finitely additive probability 
is given by Kadane et al. (1986). 
In Section 2.8.3 we discussed, within a finitistic framework, several "betting" 
approaches to establishing probability as the only coherent measure of degree of 
belief. These ideas may be extended to the general case. Dawid and Stone (1972, 
1973) introduce the concept of "expectation consistency", and show the necessity 
of using Bayes' theorem to construct probability distributions corresponding to fair 
bets made with additional information. Other generalised discussions on coherence 
of inference in terms of gambling systems include Lane and Sudderth (1983) and 
Brunk(1991). 
Measure versus Linear Theory 
Mathematical probability theory can be developed, equivalently, starting either from 
the usual Kolmogorov axioms for a set function defined over a er-field of events 
(see, for example, Ash, 1972), or from axioms for a linear operator defined over 
a linear space of random quantities (see, for example. Whittle, 1976). The former 
deals directly with probability measure; the latter with an expectation operator (or 
a prevision, in de Finetti's terminology). 
3.5 Discussion and Further References 
163 
In our development of a quantitative coherence theory, the axiomatic approach 
to preferences among options has led us more naturally towards probability  
measures as the primary probabilistic element, with expectation (prevision) defined  
subsequently. In the approach to coherence put forward in de Finetti (1972,1970/1974, 
1970/1975), prevision is the primary element, with probability subsequently  
emerging as a special case for 0-1 random quantities. The case for adopting the linear 
rather than the measure theory approach is argued at length by de Finetti, there 
being many points of contact with the argument regarding finite versus countable 
additivity, particularly the need to avoid, in the mathematical formulation, going 
beyond those aspects required for the problem in hand. In the specific context of 
statistical modelling and inference, Goldstein (1981, 1986a, 1986b, 1987a, 1987b, 
1988, 1991, 1994) has systematically developed the linear approach advocated by 
de Finetti, showing that a version of a subjectivist programme for revising beliefs 
in the light of data can be implemented without recourse to the full probabilistic 
machinery developed in this chapter. Lad et al. (1990) provide further discussion 
on the concept of prevision. 
We view these and related developments with great interest and with no  
dogmatic opinion concerning the ultimate relative usefulness and acceptance of  
"linear" versus "probabilistic" Bayesian statistical concepts and methods. That said, 
the present volume is motivated by our conviction that, currently, there remains a 
need for a detailed exposition of the Bayesian approach within the, more or less, 
conventional framework of full probabilistic descriptions. 
Proper versus Improper Probabilities 
Whether viewed in terms of finite or countable additivity, we have taken probability 
to be a measure with values in the interval [0,1]. However, it is possible to adopt 
axiomatic approaches which allow for infinite (or improper) probabilities: see, for 
example, Renyi (1955, 1962/1970, Chapter 2, and references therein), who uses 
conditional arguments to derive proper probabilities form improper distributions, 
and Hartigan (1983, Chapter 3), who directly provides an axiomatic foundation for 
improper or, as he terms them, non-unitary, probabilities. We shall not review such 
axiomatic theories in detail, but note that we shall encounter improper distributions 
systematically in Section 5.4. 
Abstract versus Concrete Mathematics 
When probabilistic mathematics is being used as a tool for the representation and 
analysis of substantive non-mathematical problems, rather than as a direct  
mathematical concern in its own right, there is always a dilemma regarding the appropriate 
level of mathematics to be used. Specifically, there are basic decisions to be made 
about how much measure-theoretical machinery should be invoked. The  
introduction of too much abstract mathematics can easily make the substantive content seem 
totally opaque to the very reader at whom it is most aimed. On the other hand, too 
164 
3 Generalisations 
little machinery may prove inadequate to provide a complete mathematical  
treatment, requiring the omission of certain topics, or the provision of just a partial, 
non-rigorous treatment, with insight and illustration attempted only by concrete 
examples. 
Thus far, we have tried to provide a complete, rigorous treatment of the  
Foundations and Generalisations of the theory of quantitative coherence, within the 
mathematical framework of Chapters 2 and 3. This chapter essentially defines the 
upper limit of mathematical machinery we shall be using and, in fact, most of our 
subsequent development will be much more straightforward. However, it will be 
the case, for example in Chapter 4, that some results of interest to us require rather 
more sophisticated mathematical tools than we have made available. Our response 
to this problem will be to try to make it clear to the reader when this is the case, 
and to provide references to a complete treatment of such results, together with 
(hopefully) sufficient concrete discussion and illustration to illuminate the topic. 
For more sophisticated mathematical treatments of Bayesian theory, the reader 
is referred to Hartigan (1983) and Florens et al. (1990). 
165 
Chapter 4 
Modelling 
Summary 
The relationship between beliefs about observable random quantities and their 
representation using conventional forms of statistical models is investigated. It 
is shown that judgements of exchangeability lead to representations that justify 
and clarify the use and interpretation of such familiar concepts as parameters, 
random samples, likelihoods and prior distributions. Beliefs which have certain 
additional invariance properties are shown to lead to representations involving 
familiar specific forms of parametric distributions, such as normals and  
exponentials. The concept of a sufficient statistic is introduced and related to  
representations involving the exponential family of distributions. Various forms of 
partial exchangeability judgements about data structures involving several  
samples, structured layouts, covariates and designed experiments are investigated, 
and links established with a number of other commonly used statistical models. 
4.1 STATISTICAL MODELS 
4.1.1 Beliefs and Models 
The subjectivist, operationalist viewpoint has led us to the conclusion that, if we 
aspire to quantitative coherence, individual degrees of belief, expressed as  
probabilities, are inescapably the starting point for descriptions of uncertainty. There can 
166 
4 Modelling 
be no theories without theoreticians; no learning without learners; in general, no 
science without scientists. It follows that learning processes, whatever their  
particular concerns and fashions at any given point in time, are necessarily reasoning 
processes which take place in the minds of individuals. To be sure, the object of 
attention and interest may well be an assumed external, objective reality: but the 
actuality of the learning process consists in the evolution of individual, subjective 
beliefs about that reality. However, it is important to emphasise, as in our earlier 
discussion in Section 2.8, that the primitive and fundamental notions of  
individual preference and belief will typically provide the starting point for interpersonal 
communication and reporting processes. In what follows, both here, and more 
particularly in Chapter 5, we shall therefore often be concerned to identify and 
examine features of the individual learning process which relate to interpersonal 
issues, such as the conditions under which an approximate consensus of beliefs 
might occur in a population of individuals. 
In Chapters 2 and 3, we established a very general foundational framework for 
the study of degrees of belief and their evolution in the light of new information. We 
now turn to the detailed development of these ideas for the broad class of problems 
of primary interest to statisticians; namely, those where the events of interest are 
defined explicitly in terms of random quantities, x\,...,xn (discrete or continuous, 
and possibly vector-valued) representing observed or experimental data. 
In such cases, we shall assume that an individual's degrees of belief for 
events of interest are derived from the specification of a joint distribution  
function P(x\,..., xn), which we shall typically assume, without systematic reference 
to measure-theoretic niceties, to be representable in terms of a joint density function 
p(xi,..., xn) (to be understood as a mass function in the discrete case). 
Of course, any such specification implicitly defines a number of other degrees 
of belief specifications of possible interest: for example, for 1 < m < n, 
p(xu...,xm) = / p(xi,...,xn)dxm+y ...dxn 
provides the marginal joint density for X\,..., xm, and 
P\%m+1> ■ ■ ■ iXn\ X\, . . . , Xm) = p(X\, . . . , Xn)IP\X\, . . . , Xmj 
gives the joint density for the as yet unobserved xm+\,... ,xn, conditional on 
having observed x-y = x\,... ,xm = xm. Within the Bayesian framework, this 
latter conditional form is the key to "learning from experience". 
We recall that, throughout, we shall use notation such as P and p in a generic sense, 
rather than as specifying particular functions. In particular, P may sometimes  
refer to an underlying probability measure, and sometimes refer to implied  
distribution functions, such as .P(zi), P{xx,.. .,xn) or P(xm+i,. ..,xn\xlt..., xm). 
Similarly, we may write p(xl),p(xl,..., xn), etc, so that, for example, 
P(xm+U ...,xn\xu..., xm) = p(xu ..., xn)/p{xu ■■■,xm) 
4.2 Exchangeability and Related Concepts 
167 
simply indicates that the conditional density for xm+x, ...,xn given x\,...,xm 
is given by the ratio of the specified joint densities. Such usage avoids notational 
proliferation, and the context will always ensure that there is no confusion of 
meaning. 
Thus far, however, our discussion is rather "abstract". In actual applications 
we shall need to choose specific, concrete forms for joint distributions. This is 
clearly a somewhat daunting task, since direct contemplation and synthesis of the 
many complex marginal and conditional judgements implicit in such a specification 
are almost certainly beyond our capacity in all but very simple situations. We shall 
therefore need to examine rather closely this process of choosing a specific form 
of probability measure to represent degrees of belief. 
Definition 4.1. (Predictive probability model). A predictive model for a  
sequence of random quantities Xi,X2,-.- is a probability measure P, which 
mathematically specifies the form of the joint belief distribution for any subset 
ofxux2,.... 
In some cases, we shall find that we are able to identify general types of 
belief structure which "pin down", in some sense, the mathematical representation 
strategy to be adopted. In other cases, this "formal" approach will not take us very 
far towards solving the representation problem and we shall have to fall back on 
rather more pragmatic modelling strategies. 
At this stage, a word of warning is required. In much statistical writing, the 
starting point for formal analysis is the assumption of a mathematical model form, 
typically involving "unknown parameters", the main object of the study being to 
infer something about the values of these parameters. From our perspective, this 
is all somewhat premature and mysterious! We are seeking to represent degrees 
of belief about observables: nothing in our previous development justifies or gives 
any insight into the choice of particular "models", and thus far we have no way of 
attaching any operational meaning to the "parameters" which appear in conventional 
models. However, as we shall soon see, the subjectivist, operationalist approach 
will provide considerable insight into the nature and status of these conventional 
assumptions. 
4.2 EXCHANGEABILITY AND RELATED CONCEPTS 
4.2.1 Dependence and Independence 
Consider a sequence of random quantities x\, x2, ■ ■ ■, and suppose that a predictive 
model is assumed which specifies that, for all n, the joint density can be written in 
168 
4 Modelling 
the form 
n 
p(xi,...,xn) = Y\p(xi), 
2=1 
so that the #j are independent random quantities. It then follows straightforwardly 
that, for any 1 < m < n, 
P\%m+\ i ■ ■ ■ i %n | X\, . . . , Xm j = p(Xm+\,. . . , Xn), 
so that no learning from experience can take place within this sequence of  
observations. In other works, past data provide us with no additional information about 
the possible outcomes of future observations in the sequence. 
A predictive model specifying such an independence structure is clearly  
inappropriate in contexts where we believe that the successive accumulation of data will 
provide increasing information about future events. In such cases, the structure of 
the joint density p{x\,..., xn) must encapsulate some form of dependence among 
the individual random quantities. In general, however, there are a vast number of 
possible subjective assumptions about the form such dependencies might take and 
there can be no all-embracing theoretical discussion. Instead, what we can do is 
to concentrate on some particular simple forms of judgement about dependence 
structures which might correspond to actual judgements of individuals in certain 
situations. 
There is no suggestion that the structures we are going to discuss in  
subsequent subsections have any special status, or ought to be adopted in most cases, or 
whatever. They simply represent forms of judgement which may often be felt to 
be appropriate and whose detailed analysis provides illuminating insight into the 
specification and interpretation of certain classes of predictive models. 
4.2.2 Exchangeability and Partial Exchangeability 
Suppose that, in thinking about P(x\, ..., xn), his or her joint degree of belief 
distribution for a sequence of random quantities x\,...,xn,an individual makes 
the judgement that the subscripts, the "labels" identifying the individual random 
quantities, are "uninformative", in the sense that he or she would specify all the 
marginal distributions for the individual random quantities identically, and similarly 
for all the marginal joint distributions for all possible pairs, triples, etc., of the 
random quantities. It is easy to see that this implies that the form of the joint 
distribution must be such that 
P(Xi ,...,Xn) = P(XKW, ..., £*(„)), 
for any possible permutation tt of the subscripts {1,..., n}. We formalise this 
notion of "symmetry" of beliefs for the individual random quantities as follows. 
4.2 Exchangeability and Related Concepts 169 
Definition 4.2. {Finite exchangeability). The random quantities x\,...,xn 
are said to be judged (finitely) exchangeable under a probability measure P 
if the implied joint degree of belief distribution satisfies 
P{xu...,xn) = P(xn{i),...,xn{n)) 
for all permutations n defined on the set {1,..., n}. In terms of the  
corresponding density or mass function, the condition reduces to 
p(xu ...,£„) = P{xn{l), ..., X„(n)). 
Example 4.1. (Tossing a thumb tack). Consider a sequence of tosses of a standard 
metal drawing pin (or thumb tack), and let x, = 1 if the pin lands point uppermost on the 
ith toss, xt = 0 otherwise, i = 1,..., n. If the tosses are performed in such a way that 
time order appears to be irrelevant and the conditions of the toss appear to be essentially 
held constant throughout, it would seem to be the case that, whatever precise quantitative 
form their beliefs take, most observers would judge the outcomes of the sequence of tosses 
x\, x2,... to be exchangeable in the above sense. 
In general, the exchangeability assumption captures, for a subjectivist interested 
in belief distributions for observables, the essence of the idea of a so-called 
"random sample". This latter notion is, of course, of no direct use to us at this 
stage, since it (implicitly) involves the idea of "conditional independence, given 
the value of the underlying parameter", a meaningless phrase thus far within our 
framework. 
The notion of exchangeability involves a judgement of complete symmetry 
among all the observables xi,...,xn under consideration. Clearly, in many  
situations this might be too restrictive an assumption, even though a partial judgement 
of symmetry is present. 
Example 4.1. (cont.). Suppose that the sequence of tosses of a drawing pin are not 
all made with the same pin, but that the even and odd numbered tosses are made with 
different pins: an all metal one for the odd tosses; a plastic-coated one for the even tosses. 
Alternatively, suppose that the same pin were used throughout, but that the odd tosses are 
made by a different person, using a completely different tossing mechanism from that used 
for the even tosses. In such cases, many individuals would retain an exchangeable form 
of belief distribution within the sequences of odd and even tosses separately, but might be 
reluctant to make a judgement of symmetry for the combined sequence of tosses. 
170 
4 Modelling 
Example 4.2. (Laboratory measurements). Suppose that x\,X2,... are real-valued 
measurements of a physical or chemical property of a given substance, all made on the same 
sample with the same measurement procedure. Under such conditions, many individuals 
might judge the complete sequence of measurements to be exchangeable. 
Suppose, however, that sequences of such measurements are combined from k different 
laboratories, the substance being identical but the measurement procedures varying from 
laboratory to laboratory. In this case, judgements of exchangeability for each laboratory 
sequence separately might be appropriate, whereas such a judgement for the combined 
sequence might not be. 
Example 4.3. (Physiological responses). Suppose that {x\, x<i, ■. ■,} are real-valued 
measurements of a specific physiological response in human subjects when a particular 
drug is administered. If the drug is administered at more than one dose level and if there 
are both male and female subjects, spanning a wide age range, most individuals would be 
very reluctant to make a judgement of exchangeability for the entire sequence of results. 
However, within each combination of dose-level, sex and appropriately defined age-group, 
a judgement of exchangeability might be regarded as reasonable. 
Judgements of the kind suggested in the above examples correspond to forms 
of partial exchangeability. Clearly, there are many possible forms of departure 
from overall judgements of exchangeability to those of partial exchangeability and 
so a formal definition of the term does not seem appropriate. In general, it simply 
signifies that there may be additional "labels" on the random quantities (for example, 
odd and even, or the identification of the tossing mechanism in Example 4.1) with 
exchangeable judgements made separately for each group of random quantities 
having the same additional labels. A detailed discussion of various possible forms 
of partial exchangeability will be given in Section 4.6. 
We shall now return to the simple case of exchangeability and examine in detail 
the form of representation of p(x\,..., xn) which emerges in various special cases. 
As a preliminary, we shall generalise our previous definition of exchangeability 
to allow for "potentially infinite" sequences of random quantities. In practice, it 
should, at least in principle, always be possible to give an upper bound to the number 
of observables to be considered. However, specifying an actual upper bound may be 
somewhat difficult or arbitrary and so, for mathematical and descriptive purposes, it 
is convenient to be able to proceed as if we were contemplating an infinite sequence 
of potential observables. Of course, it will be important to establish that working 
within the infinite framework does not cause any fundamental conceptual distortion. 
These and related issues of finite versus infinite exchangeability will be considered 
in more detail in Section 4.7.1. For the time being, we shall concentrate on the 
"potentially infinite" case. 
4.2 Exchangeability and Related Concepts 
171 
Definition 4.3. {Infinite exchangeability). The infinite sequence of  
random quantities x\, X2, ■ ■ ■ is said to be judged (infinitely) exchangeable if 
every finite subsequence is judged exchangeable in the sense of Definition 4.2. 
One might be tempted to wonder whether every finite sequence of  
exchangeable random quantities could be embedded in or extended to an infinitely  
exchangeable sequence of similarly defined random quantities. However, this is certainly 
not the case as the following example shows. 
Example 4.4. (Non-extendible exchangeability). Suppose that we define the three 
random quantities Xi,X2,X3 such that either x,- = 1 or x,- = 0, i = 1,2,3, with joint 
probability function given by 
p(x\ = 0, X2 = 1, x3 = 1) = p{x\ = 1, X2 = 0, X3 = 1) 
= p{x\ = 1, X2 = 1, X3 = 0) 
= 1/3, 
with all other combinations of X\ ,x2,X3 having probability zero, so that X\, x2, x3 are clearly 
exchangeable. We shall now try to identify an x4, taking only values 0 and 1, such that 
xi,..., X4 are exchangeable. For this to be possible, we require, for example, 
p(xi = 0, x2 = 1, x3 = 1, x4 = 0) = p(xi = 0, x2 = 0, x3 - 1, x4 = 1). 
But 
p(x\ = 0, x2 = 1, xz = 1, x4 = 0) 
= p(xi = 0, x2 = 1, x3 = 1) - p(x\ = 0. x2 = 1, x3 = 1, x4 = 1) 
= 1/3 - p(xi = 0,x2 = 1, x3 = 1, x4 = 1) 
= 1/3 - p{xi = 1, x2 = 1, x3 = 1, x4 = 0), 
where 
p{xx = 1, x2 = 1, x3 = 1, x4 = 0) < p{x\ = 1, x2 = 1, x3 = 1) = 0, 
so that 
p(x\ = 0, x2 = 1, x3 = 1, x4 = 0) = 1/3. 
However, we also have 
p{x\ = 0, x2 = 0, x3 = 1, Xi = 1) < p(xi = 0, X2 = 0, X3 = 1) = 0 
and so 
p{x\ = 0, x2 = 1, x3 = 1, x4 = 0) ^ p(ii = 0, x2 = 0, x3 = 1, Xi = 1). 
It follows that a finitely exchangeable sequence cannot even necessarily be embedded 
in a larger finitely exchangeable sequence, let alone an infinitely exchangeable sequence. 
172 4 Modelling 
4.3 MODELS VIA EXCHANGEABILITY 
4.3.1 The Bernoulli and Binomial Models 
We consider first the case of an infinitely exchangeable sequence of 0-1 random 
quantities, x\,xi,..., with X{ = 0 or xi = 1, for all i = 1,2,.... Without loss 
of generality, we shall derive a representation result for the joint mass function, 
p(x\,..., xn), of the first n random quantities 
•Eli • • • i Xn- 
Proposition 4.1. (Representation theorem for 0-1 random quantities). 
lfx\, X2, ■ ■ ■ is an infinitely exchangeable sequence of0-1 random quantities 
with probability measure P, there exists a distribution function Q such that 
the joint mass function p{x\,..., xn)for x\..., x„ has the form 
p(xu...,xn) = f 11^(1-ey-'idQiO), 
Jv i=l 
where, 
Q{0)= Urn P[y„/n<0], 
n—»oo 
with yn = xi-i \-xn, and 9 = lim^^ yn/n. 
Proof. (De Finetti, 1930, 1937/1964; here we follow closely the proof given 
by Heath and Sudderth, 1976; see also Barlow, 1991). Suppose x\ H \-xn = yn, 
then, by exchangeability, for any 0 < yn < n, 
p(x\ + ---+xn = yn)= I Ip^i),..., xff(n)) 
\Vn/ 
for any permutation 7r of {1,..., n} such that x^ii) -\ (- x^^ = yn. Moreover, 
for arbitrary N > n > yn > 0, and with the summations below taken over the 
range yN = yntoyN = N - (n - yn), we see that 
p{xx + ■ ■ ■ + xn = yn) 
= ~52p(xi + ■ ■ ■ + xn = yn I Xi + ■ ■ ■ + xN = Vn)p(x\ + ■■■ + xN = yN), 
= E(") "(*)(JI:^)^. + - • + - = »), 0<9„<n<W, 
where (^)j/„ = 2/jv(2/jv - 1) • ■ • [vn - {yn - 1)], etc. (Intuitively, we can imagine 
sampling n items without replacement from an urn of TV items containing y^ l's 
and N — yN O's, corresponding to the hypergeometric distribution of Section 3.2.2.) 
4.3 Models via Exchangeability 
173 
If we now define Qn (0) on 3? to be the step function which is 0 for 0 < 0 and 
has jumps of p{x\ H + xn = 2/jv) at 0 = j/n/N, ys = 0,..., N, we see that 
P(x, + ... + * = *> = (»)/' (^[(■-»>M^ «,„(„. 
As TV —» co, 
uniformly in 0. Moreover, by Helly's theorem (see, for example, Section 3.2.3 and 
Ash, 1972, Section 8.2), there exists a subsequence Qnx , Qn2 ,... such that 
lim QN, = Q, 
where Q is a distribution function. The result follows. < 
The interpretation of this representation theorem is of profound significance 
from the point of view of subjectivist modelling philosophy. It is as if: 
(i) the Xi are judged to be independent, Bernoulli random quantities (see  
Section 3.2.2) conditional on a random quantity 9; 
(ii) 0 is itself assigned a probability distribution Q; 
(iii) by the strong law of large numbers, 0 = limn^oc(yn/n), so that Q may be 
interpreted as "beliefs about the limiting relative frequency of l's". 
In more conventional notation and language, it is as if, conditional on 6, 
xi,..., xn are a random sample from a Bernoulli distribution with parameter 0, 
generating a parametrised joint sampling distribution 
n n 
P{xu... ,Xn i e) = Y[P(xi 10) = n^(i - oy-xi, 
i=\ i=l 
where the parameter is assigned a prior distribution Q{0). The operational content 
of this prior distribution derives from the fact that it is as if we are assessing beliefs 
about what we would anticipate observing as the limiting relative frequency from a 
"very large number" of observations. Thought of as a function of 6, we shall refer 
to the joint sampling distribution as the likelihood function. 
In terms of Definition 4.1, the assumption of exchangeability for the infinite 
sequence of 0-1 random quantities x\,X2,... places a strict limitation on the 
family of probability measures P which can serve as predictive probability models 
for the sequence. Any such P must correspond to the mixture form given in 
Proposition 4.1, for some choice of prior distribution Q{9). As we range over 
all possible choices of this latter distribution, we generate all possible predictive 
174 
4 Modelling 
probability models compatible with the assumption of infinite exchangeability for 
the 0-1 random quantities. 
Thus, "at a stroke", we establish a justification for the conventional model 
building procedure of combining a likelihood and a prior. The likelihood is defined 
in terms of an assumption of conditional independence of the observations given 
a parameter; the latter, and its associated prior distribution, acquire an operational 
interpretation in terms of a limiting average of observables (in this case a limiting 
frequency). 
In many applications involving 0-1 random quantities, we may be more  
interested in a summary random quantity, such as y„ = X\ + • • • + xn, than in the 
individual sequences of xfs. The representation of p(x\ + ■ ■ ■ + xn = yn) is 
straightforwardly obtained from Proposition 4.1. 
Corollary 1. Given the conditions of Proposition 4.1, 
p(Xl + ... + Xn = yn)= f (n)ff^(i-e)n-^dQ(9). 
Jo VW 
Proof. This follows immediately from Proposition 4.1 and the fact that 
p{xi + --- + xn = y„) = I )p(xi,...,xn) 
for all a:i,..., x„ such that x\ + ■ ■ ■ + x„ = yn. < 
This provides a justification, when expressing beliefs about yn, for acting as 
if we have a binomial likelihood, defined by Bi(yn | 9, n), with a prior distribution 
Q(9) for the binomial parameter 9. 
The formal learning process for models such as this will be developed  
systematically and generally in Chapter 5. However, this simple example provides 
considerable insight into the learning process, showing how, in a sense, the key 
step is a straightforward consequence of the representation theorem. 
Corollary 2. Ifxi, £2, -is an infinitely exchangeable sequence of 0-1  
random quantities with probability measure P, the conditional probability  
function p(xm+u...,xn\xi,...,xm), for xm+i,...,xn given xi,...,xm, has 
the form 
ri n 
/ H 9xi(l-9)1-xidQ(9\xu..-,xm), l<m<n, 
*'° «=m+l 
4.3 Models via Exchangeability 175 
i 
where 
awi,,,...,*.)- TOa^-*f-*im 
/o'nr.1 »*<a-«)'-'«« 
and 
Q(0) = lim P(y„/n < 6). 
n—>oo 
Proof. Clearly, 
p(xi,...,o:n) 
Pv-^m+l> • • • i -^n | •£!> • • • > 3'mJ 
p(xi,...,xm) 
and the result follows by applying Proposition 4.1 to both p{x\, ..., xn) and 
p(x\,... ,xm) and rearranging the resulting expression. 4 
We thus see that the basic form of representation of beliefs does not change. 
All that has happened, expressed in conventional terminology, is that the prior 
distribution Q(9) for 9 has been revised, via Bayes' theorem, into the posterior 
distribution Q{6 \ x\,..., xm). 
The conditional probability function p(xm+\, ...,xn\x\,..., xm) is called 
the {conditional, or posterior) predictive probability function for xm+\, ..., xn 
given x\,...,xm, and this, of course, also provides the basis for deriving the  
conditional predictive distribution of any other random quantity defined in terms of the 
future observations. For example, given X\,... ,xm, the predictive probability  
function p(yn-m I xi,..., xm) for yn-m, i.e., the total number of l's in xm+i, ■ ■ ■, xn, 
has the form 
I 
(j1 m>Wm(l _ 0)(n-m)-yn-mdQ(0 | Xu . . . >Xm). 
\ Vn—m / 
0 \ Vn—m 
A particularly important random quantity defined in terms of future  
observations is the frequency of l's in a large sample. But, by Proposition 4.1 and its 
Corollary 2, 
lim P( n~m xu...,xm) = Q(0\xi,...,xm). 
(n-m)-»oo \(n — m) J 
Thus, a posterior distribution for a parameter is seen to be a limiting case of 
a posterior (conditional) predictive distribution for an observable. 
176 
4 Modelling 
4.3.2 The Multinomial Model 
An alternative way of viewing the 0-1 random quantities discussed in Section 3.1 is 
as defining category membership (given two exclusive and exhaustive categories), 
in the sense that x, = 1 signifies that the ith observation belongs to category 1 and 
Xi = 0 signifies membership of category 2. We can extend this idea in an obvious 
way by considering A;-dimensional random vectors Xi whose jth component, Xy, 
takes the value 1 to indicate membership of the jth of k +1 categories. At most one 
of the k components can take the value 1; if they all take the value 0 this signifies 
membership of the (k + l)th category. In what follows, we shall refer to such X{ 
as "0-1 random vectors". If x\, X2, ■ ■ ■ is an infinitely exchangeable sequence of 
0-1 random vectors, we can extend Proposition 4.1 in an obvious way. 
Proposition 4.2. (Representation theorem for 0-1 random vectors). 
Ifx\, X2, ■ ■ ■ is an infinitely exchangeable sequence of 0-1 random vectors 
with probability measure P, there exists a distribution function Q such that 
the joint mass function p{x\,..., xn)for x\,..., xn has the form 
P(xu...,xn)= f iK'1^2-"*?* !-!>. 
k \ 1_Exu 
_ j 
dQ{6), 
Q{6) = lim P [(xln <9U)U...U (xkn < 9k)] , 
n—>oo L x ' v 'J 
with xin = n~l(xu H (- xni), and 6t = lim^^ xin. 
Proof This is a straightforward, albeit algebraically cumbersome,  
generalisation of the proof of Proposition 4.1. < 
As in the previous case, we are often most interested in the summary random 
vector yn = x\ + ■ ■ ■ + xn whose jth component yni is the random quantity 
corresponding to the total number of occurrences of category j in the n observations. 
We shall give the representation of p(xy + ■ ■ ■ + xn = yn) = p(ynU ..., ynk), 
generalising Corollary 1 to Proposition 4.1, and then comment on the interpretation 
of these results. 
4.3 Models via Exchangeability 
177 
Corollary. Given the conditions of Proposition 4.2, the joint mass function 
p{Vn\, ■ ■ ■, Vnk) may be represented as 
[ ( n \e\nlev2n2---elnk\{i-^ei)n-T'v^dQ{6) 
Je* \ym ■ ■ -Vnk) 
where 
n \ n\ 
KVnl ■ ■ ■ Vnk) Vn\^Vn^}- • • ' VnkK71 ~ S2/m)! 
Proof This follows immediately from the generalisation of the argument used 
in proving Corollary 1 to Proposition 4.1. < 
Thus, we see in Proposition 4.2 that it is as if we have a likelihood  
corresponding to the joint sampling distribution of a random sample x\,..., xn, where 
each X{ has a multinomial distribution with probability function Muk(xi | G, 1), 
together with a prior distribution Q over the multinomial parameter 6, where the 
components Oj of the latter can be thought of as the limiting relative frequency of 
membership of the jth category. In the corollary, it is as i/we assume a multinomial 
likelihood, Muk(yn, \ 0, n), with a prior Q(0) for 6. 
4.3.3 The General Model 
We now consider the case of an infinitely exchangeable sequence of real-valued 
random quantities X\, X2,.... As one might expect, the mathematical technicalities 
of establishing a representation theorem in the real-valued case are somewhat more 
complicated than in the 0-1 cases, and a rigorous treatment involves the use of 
measure-theoretic tools beyond the general mathematical level at which this volume 
is aimed. For this reason, we shall content ourselves with providing an outline proof 
of a form of the representation theorem, having no pretence at mathematical rigour 
but, hopefully, providing some intuitive insight into the result, as well as the key 
ideas underlying a form of proper proof. 
Proposition 4 J. (General representation theorem). 
If X\,X2, ■ ■ ■, is an infinitely exchangeable sequence of real-valued random 
quantities with probability measure P, there exists a probability measure Q 
over 3, the space of all distribution functions on 3?, such that the joint  
distribution function ofx\,... ,xn has the form 
f n 
P(Xl,...,xn)= / T\F(xi)dQ(F), 
where 
Q(F) = Urn P(Fn) 
n—>oo 
and Fn is the empirical distribution function defined by xi,..., xn. 
+ 
178 4 Modelling 
Outline proof. (See Chow and Teicher, 1978/1988). Since 
1 " 
F*(x) = ~Hh*i<*) 
i=\ 
we have, by exchangeability, 
E(F„(x) - FN(x))2 = ~™ [P{xi < x) - P[(xi < x) n (x2 <*)]}• 
To see this, writing It in place of I[x.<x] and noting that If = It, we have 
\ «=1 «=1 i=\ / 
Note also that E(/,) = P(xx < x) and E(JjJj) = P[(xi < x) n (x2 < x)], 
for all i, j, by exchangeability. A straightforward count of the numbers of terms 
involved in the summations then gives the required result. 
The right-hand side tends to zero as N, n —> oo, and hence the random quantity 
Fn(x) tends in probability to some random quantity, F(x), say, which implies that 
n n 
in probability as N —> oo, for fixed n. 
Suppose we now let qi, ..., a„ denote positive integers and set 
A = {a = (ai,... ,a„); 1 < at < N for 1 < i < n} 
and 
A* = 1(a) = I[(xai < xi) n • • • n (xan < xn)]. 
For N > n, it then follows that 
n fN(Xj)=Nnf[j21^)=N~n e j(*) 
j=\ j=\ j=l aeA 
= N 
\a6i4-i4* aeA*/ 
4.3 Models via Exchangeability 
179 
However, as N —► oo, 
AT" ^2 1(a) <N~n ^2 l = [Nn-N{N-l)---(N-n + l)]/Nn ^0 
so that, 
But, by exchangeability, 
/ /(a) dP = /|(11<11)n...n(i„<1„)] dP 
= P[(xi < xi)n• • • n (xn < xn)} = P(xu...,xn) 
and so 
r n 
/ l[FN{xj)dP *[N---(N-n + l)/Nn]P[(Xl < *i) n ■ • • n (sn < xn)}. 
J i=\ 
Recalling (*), we see that, as N —» oo, 
/ 
l\F(xj)dQ(F)KP(xi,...,xn) 
where Q(F) = liniiv^oo P(FN). < 
The general form of representation for real-valued exchangeable random  
quantities is therefore as i/we have independent observations x\,..., xn conditional on 
F, an unknown (i.e., random) distribution function (which plays the role of an 
infinite-dimensional "parameter" in this case), with a belief distribution Q for F, 
having the operational interpretation of "what we believe the empirical distribution 
function would look like for a large sample". 
The structure of the learning process for a general exchangeable sequence of 
real-valued random quantities, with the distribution function representation given 
in Proposition 4.3, cannot easily be described explicitly. In what follows, we shall 
therefore find it convenient to restrict attention to those cases where a corresponding 
representation holds in terms of density functions, labelled by a finite-dimensional 
parameter, 8, say, rather than the infinite-dimensional label, F. For ease of  
reference, we present this representation as a corollary to Proposition 4.3. 
180 
4 Modelling 
Corollary 1. Assuming the required densities to exist, under the conditions 
of Proposition 4.3 the joint density ofxi,..., xn has the form 
f n 
p(xu...,xn) = / Y[p(xi\0) dQ(0), 
-'e «=i 
with p{-1 6) denoting the density function corresponding to the "unknown 
parameter" 0 € 6. 
The role of Bayes' theorem in the learning process is now easily identified. 
Corollary 2. If x\, X2, ■ ■. is an infinitely exchangeable sequence of real- 
valued random quantities admitting a density representation as in Corollary 1, 
then 
f " 
p{xm+u... , Xn I X\, . . . , Xm) — 
■'8 i—™j.i 
i=m+l 
where 
dQ(e\xx x) nr-iPfoig)dQ(g) 
Proof This follows immediately on writing 
, . s _ p(xi,...,xn) 
p(xi,...,xm) 
applying the density representation form to both p(x\,..., xn) and p{x\,... ,xm), 
and rearranging the resulting expression. < 
The technical discussion in this section has centred on exchangeable  
sequences, X\, X2, ■ ■ ■, of real-valued random quantities. In fact, everything carries 
over in an obviously analogous manner to the case of exchangeable sequences 
X\, x2,..., with Xi £ SR*. All that happens, in effect, is that the distribution  
functions and densities referred to in Proposition 4.3 and its corollaries become the 
joint distribution functions and densities for the k components of the as*. To avoid 
tedious distinctions between x e 3? and x € 5Rfc, in subsequent developments we 
shall often just write x G X. In cases where the distinction between k = 1 and 
k > 1 matters, it will be clear from the context what is intended. 
In Section 4.8.1, we shall give detailed references to the literature on  
representation theorems for exchangeable sequences, including far-reaching  
generalisations of the 0-1 and real-valued cases. However, even the simple cases we 
have presented already provide, from the subjectivist perspective, a deeply  
satisfying clarification of such fundamental notions as models, parameters, conditional 
independence and the relationship between beliefs and limiting frequencies. 
4.4 Models via Invariance 
181 
In terms of Definition 4.1, the assumption of exchangeability for the real- 
valued random quantities X\,X2,--- again places (as in the 0-1 case) a limitation 
on the family of probability measures P which can serve as predictive probability 
models. In this case, however, in the context of the general from of representation 
given in Proposition 4.3, the "parameter", F, underlying the conditional  
independence structure within the mixture is a random distribution function, so that the 
"parameter" is, in effect, infinite dimensional, and the family of coherent predictive 
probability models is generated by ranging through all possible prior distributions 
Q(F). The mathematical form of the required representation is well-defined, but 
the practical task of translating actual beliefs about real-valued random quantities 
into the required mathematical form of a measure over a function space seems, 
to say the least, a somewhat daunting prospect. It is interesting therefore to see 
whether there exist more complex formal structures of belief, imposing further 
symmetries or structure beyond simple exchangeability, which lead to more  
specific and "familiar" model representations. In particular, it is of interest to identify 
situations in which exchangeability leads to a mixture of conditional independence 
structures which are defined in terms of a finite dimensional parameter so that the 
more explicit forms given in the corollaries to Proposition 4.3 can be invoked. 
Given the interpretation of the components of such a parameter as strong law limits 
of simple sequences of functions of the observations, the specification of Q, and 
hence of the complete predictive probability model P, then becomes a much less 
daunting task. 
4.4 MODELS VIA INVARIANCE 
4.4.1 The Normal Model 
Suppose that in addition to judging an infinite sequence of real-valued random 
quantities x1,x2, • • • to be exchangeable, we consider the possibility of further 
judgements of invariance, perhaps relating to the "geometry" of the space in which 
a finite subset of observations, x\,...,xn, say, lie. The following definitions 
describe two such possible judgements of invariance. As with exchangeability, 
there is no claim that such judgements have any a priori special status. They are 
intended, simply, as possible forms of judgement that might be made, and whose 
consequences might be interesting to explore. 
Definition 4.4. {Spherical symmetry). A sequence of random quantities 
x\, ..., xn is said to have spherical symmetry under a predictive probability 
model P if the latter defines the distributions ofx = {x\,..., xn) and Ax to 
be identical, for any orthogonal n x n matrix A. 
182 
4 Modelling 
This definition encapsulates a judgement of rotational symmetry, in the sense 
that, although measurements happened to have been expressed in terms of a  
particular coordinate system (yielding xx,..., xn), our quantitative beliefs would not 
change if they had been expressed in a rotated coordinate system. Since  
rotational invariance fixes "distances" from the origin, this is equivalent to a judgement 
of identical beliefs for all outcomes of x\,... ,x„ leading to the same value of 
xl + ■ ■ • + xn. 
The next result states that if we make the judgement of spherical symmetry 
(which in turn implies a judgement of exchangeability, since permutation is a special 
case of orthogonal transformation), the general mixture representation given in 
Proposition 4.3 assumes a much more concrete and familiar form. 
Proposition 4.4. (Representation theorem under spherical symmetry). 
If X\,X2,. ■. is an infinite sequence of real-valued random quantities with 
probability measure P, and if, for any n, x\,..., x„ have spherical symmetry, 
there exists a distribution function Q on 5R+ such that the joint distribution 
function ofx\,..., xn has the form 
~ n 
P(xu...,xn) = / JI*(A,/2x,-)dQ(A), 
J%+ i=l 
where $ is the standard normal distribution function and 
Q(A) = lim P(s~2 < A), 
n—>oo 
with s„ = n"1 (x\-\ h x\), and A-1 = lim^oo s\■ 
Proof. See, for example, Freedman (1963a) and Kingman (1972); details are 
omitted here, since the proof of a generalisation of this result will be given in full 
in Proposition 4.5. < 
The form of representation obtained in Proposition 4.4 tells us that the  
judgement of spherical symmetry restricts the set of coherent predictive probability 
models to those which are generated by acting as if: 
(i) observations are conditionally independent normal random quantities, given 
the random quantity A (which, as a "labelling parameter", corresponds to the 
precision; i.e., the reciprocal of the variance); 
(ii) A is itself assigned a distribution Q; 
(iii) by the strong law of large numbers, A-1 = lim^oo s\, so that Q may be 
interpreted as "beliefs about the reciprocal of the limiting mean sum of squares 
of the observations". 
4.4 Models via Invariance 
183 
For related work see Dawid (1977,1978). To obtain a justification for the usual 
normal specification, with "unknown mean and precision", we need to generalise 
the above discussion slightly. 
We note first that the judgement of spherical symmetry implicitly attaches a 
special significance to the origin of the coordinate system, since it is equivalent 
to a judgement of invariance in terms of distance from the origin. In general, 
however, if we were to feel able to make a judgement of spherical symmetry, it 
would typically only be relative to an "origin" defined in terms of the "centre" of 
the random quantities under consideration. This motivates the following definition. 
Definition 4.5. {Centred spherical symmetry). A sequence of random  
quantities X\,..., xn is said to have centred spherical symmetry if the random quanti- 
tiesx\ —x„,..., xn—xn have spherical symmetry, wherexn — n~x Yl xi- This 
is equivalent to a judgement of identical beliefs for all outcomes ofx\,... ,xn 
leading to the same value of{xx — xn)2 + \- (xn — xn)2. 
Proposition 4.5. (Representation under centred spherical symmetry). 
If X\,X2,- ■ ■ is an infinitely exchangeable sequence of real-valued random 
quantities with probability measure P, and if for any n, {x\,... xn} have 
centred spherical symmetry, then there exists a distribution function Q on 
SR x SR+ such that the joint distribution ofx\,...,xn has the form 
- n 
P(Xl,...,Xn)= / Yl^lX^iXi-^dQ^X), 
where $ is the standard normal distribution function and 
Q(fi, A) = lim P[(xn < ft) n (s;2 < A)], 
with xn = n~l(xx + h xn), s2n = n_1 [(xx - xn)2 + h (xn - xn)2], 
H = lim^co xn, and A-1 = lira 
•n->oo •'rr 
Proof (Smith, 1981). Since the sequence X\,X2,--. is exchangeable, by 
Proposition 4.3 there exists a random distribution function F such that,  
conditional on F, the random quantities x\,..., xn, for any n, are independent. There 
is therefore a random characteristic function, <j>, corresponding to F, such that 
E 
«p mE* 
jxj 
3=1 
and hence 
E 
exp ii ^2 
tjXj 
= E 
3=1 
184 
4 Modelling 
If we now define yj = Xj — xn, j = 1,..., n, it follows that 
E 
exp I i^Sjyj 
= E 
U=i 
(*) 
for all real s\,..., sn such that s\ + 1- s„ — 0. Since y\,..., y„ are spherically 
symmetric, both sides of this latter equality depend only on s2 + • • • + s2. 
Recalling that <j){—t) = <j>{t), the complex conjugate, and that 0(0) = 1, it 
follows that, for any real u and v, 
E{\<f>(u + v)<}>{u -v)- <f{u)<}>{v)<}>{-v)\2} 
= E{<j)(u + v)(j>(u — v)(j)(—u — v)(j>(v — u)} 
- E{<j>{u + v)<j>{u - v)4>2{-u)(j)(-v)(t){v)} 
- E{<j)(-u - v)(j){v - u)(j)2{u)<)){v)(j)(-v)} 
+ E{/(U)02(r;)02(-i;)02(-u)}, 
where all four terms in this expression are of the form of the right-hand side of (*) 
with n = 8, s\ + h sg = 0 and s2 + • • • + s| — 4(u2 + v2). All the four terms 
are therefore equal, so that the overall expression is zero. This implies that, almost 
surely with respect to the probability measure P, <j> satisfies the functional equation 
(j){u + V)(j){u — v) = (j)2(u)(j)(v)(j)( — v) 
for all real u and v. This can be rewritten in the form 
*!(u + v) + V2{u -v)= A{u) + B(v), 
where *i(i) = *2(<) — log <£(*)> md where Mu) = 21og0(u) and B{v) = 
\og[ij){v)<j){—v)\, it follows that log <j>{t) is a quadratic in t (see, for example, Kagan, 
Linnik and Rao, 1973, Lemma 1.5.1). Again using <j){—t) = 0(f), 0(0) = 1, 
we see that, for this quadratic, the constant coefficient must be zero, the linear 
coefficient purely imaginary and the quadratic coefficient real and non-positive. 
This establishes that the random characteristic function <j> takes the form 
<j>{t) = exp < i\it 
2AJ 
for some random quantities n € 5?, A € 5?+. 
If we now define a random quantity z by 
z = exp 
4.4 Models via Invariance 
185 
then, by iterated expectation, we have 
E(z \n,\)= E[E(z \F)\n,\] = E 
so that 
.3=1 
n,x 
E 
H,X 
It 
: JJexplt/itj-^ ). 
This establishes that, conditional on \i and A, x\,..., xn are independent  
normally distributed random quantities, each with mean [i and precision A. The mixing 
distribution in the general representation theorem reduces therefore to a joint  
distribution over [i and A. But, by the strong law of large numbers, 
lim 
n—>oo 
Xi-\ Vxn 
n 
= /*, 
lim 
n—>oo 
and the result follows. 
(xi - xnf + h (x„ - xnf = 1_ 
n A 
We see, therefore, that the combined judgements of exchangeability and  
centred spherical symmetry restrict the set of coherent predictive probability models 
to those which, expressed in conventional terminology, correspond to acting as if: 
(i) we have a random sample from a normal distribution with unknown mean and 
precision parameters, \i and A, generating a likelihood 
n 
p(x1,...,x„|/x,A) = JJjV(x,-|/i,A); 
j=i 
(ii) we have a joint prior distribution Q{n, A) for the unknown parameters, \i 
and A, which can be given an operational interpretation as "beliefs about the 
sample mean and reciprocal sample variance which would result from a large 
number of observations". 
4.4.2 The Multivariate Normal Model 
Suppose now that we have an infinitely exchangeable sequence of random vectors 
Xi,x2,... taking values in 5ft*, k > 2, and that, in addition, we judge, for all 
n and for all c € 5ft*, that the random quantities cfxi,..., clxn have centred 
spherical symmetry. The next result then provides a multivariate generalisation of 
Proposition 4.5. 
186 
4 Modelling 
Proposition 4.6. (Multivariate representation theorem under centred  
spherical symmetry). Ifxl,X2,..- is an infinitely exchangeable sequence of  
random vectors taking values in 5R*, with probability measure P, such that, for 
any n and c € 5R*. the random quantities dx\,..., clxn have centred  
spherical symmetry, the structure of evaluations under P of probabilities of events 
defined by xi,..., xn is as if the latter were independent, multivariate  
normally distributed random vectors, conditional on a random mean vector fi 
and a random precision matrix A, with a distribution over /i and A induced 
by P, where 
1 " 1 " 
a = lim — > Xi, A-1 = lim — y^fx, — xn)ixj — xn)1. 
n-»oo n. -^—' n-too n *-^ J J 
j=l 
n-too n ■ 
j=l 
Proof. Defining y3 = clXj,j — 1,..., n, we see that the random quantities 
yi,... ,yn have centred spherical symmetry and so, by Proposition 4.5, there exist 
H = n(c) and A = A(c) such that, for all t^ € 5ft, j = 1,. - -, n. 
E 
expU^t/% 
/i, A 
U 
= IlexP^^-2 A") ' 
where 
But 
and 
n n 
H = /x(c) = lim -J^yj, A-1 = A_1(c) = lim V(yj - ynf 
lim I - y^ Xj I = c(/x 
n_>o0 V71^ J 
H = M(c) = c( 
A"1 = A-1(c) = c( lim 
1 ™ 
J = l 
c = c'A^c 
so that 
E 
exp j icl}^tjXj 
^, A 
JJexp[ic(/^ - i(c(A_1c^)], 
for all c € 5ft*, ij € 5ft, j = 1,..., n. It follows that, for all t, e 5ft*, j = 1,..., n, 
£ 
exp I i^t-Xj 
^,A 
=nexp[*M%-3(*jA_i*>)] 
.7=1 
so that, conditional on /x and A, cci,..., xn are independent multivariate normal 
random quantities each with mean /x and precision matrix A. <| 
4.4 Models via Invariance 
187 
4.4.3 The Exponential Model 
Suppose x\, X2, ■ ■ ■ is judged to be an infinitely exchangeable sequence of positive 
real-valued random quantities. In particular, we note that this implies, for any 
pair Xi, Xj, an identity of beliefs for any events in the positive quadrant which are 
symmetrically placed with respect to the 45° line through the origin. 
/ 0 
/- 
Ai 
A2 
£- p~ 
Figure 4.1 A\, A2, Bx, B2 reflections in 45° line. C\, C2 reflections in (dashed) 45° line 
Thus, for example, in Figure 4.1, the probabilities assigned to A\ and A2, Bx 
and B2, respectively, must be equal, for any i ^ j. In general, however, the 
assumption of exchangeability would not imply that events such as C\ and C2 have 
equal probabilities, even though they are symmetrically placed with respect to a 
45° line (but not the one through the origin). 
It is interesting to ask under what circumstances an individual might judge 
events such as C\, C2 to have equal probabilities. The answer is suggested by the 
additional (dashed) lines in the figure, //we added to the assumption of  
exchangeability the judgement that the "origins" of the x,- and Xj axes are "irrelevant", so 
far as probability judgements are concerned, then the probabilities of events such 
as Cx and C2 would be judged equal. In perhaps more familiar terms, this would 
be as though, when making judgements about events in the positive quadrant, an 
individual's judgement exhibited a form of "lack of memory" property with respect 
to the origin. If such a judgement is assumed to hold for all subsets of n (rather 
than just two) random quantities, the resulting representation is as follows. 
188 
4 Modelling 
Proposition 4.7. (Continuous representation under origin invariance). 
If Xi,X2, ■ ■ ■ is an infinitely exchangeable sequence of positive real-valued 
random quantities with probability measure P, such that, for all n, and any 
event Ain9l+ x ... x SR+, 
P[(xu..., xn) e A] = P[{xx ,...,xn)eA + a] 
for all a € SRx. ..x^.suchthatall = OandA+aisaneventin$l+x.. . x3?+, 
then the joint density for x\,...,xn has the form 
roo n 
p(xi xn)=l JJ#exp(-0a;j)d<3(0), 
where ° i=i 
Q{6) = lim P[{x-1) < 9], xn = n-\xl + --- + xn). 
n—>oo 
Outline proof. (Diaconis and Ylvisaker, 1985). By the general representation 
theorem, there exists a random distribution function F, such that, conditional on 
F, x\,..., xn are independent, for any n. It can be shown that the additional 
invariance property continues to hold conditional on F, so that, for any i ^ j, 
P[(xt,xj) £A\F} = P[(Xi,Xj)eA + a\ F] 
for A and a as described above. If we now take a1 = {a\, a2) and 
A = {(xi,Xj); Xi> ai + a2,Xj > 0} 
we have 
P[{Xi >ai + a2) n (Xj > 0) | F] = P[(xi > ai) n (x, > a2) \ F] 
= P[(xi>al)\F}P[(xj>a2)\F]. 
By exchangeability, and recalling that Xj is certainly positive for all j, this implies 
that 
P(x{ > ax + a2 | F) = P(x{ > d | F)P(xf > a2\F). 
But this functional relationship implies, for positive real-valued x^ that 
p(Xi > x | F) = e~9x 
for some 6, so that the density, p(xt | F) — p(xt \6),i& the derivative of 
1 - exp(-0Xi), 
and hence given by 0exp(—dxi). The rest of the result follows on noting that, by 
the strong law of large numbers, 0_1 = lim^ojn-1 (x\ + ■ ■ ■ + xn)}. <| 
4.4 Models via Invariance 
189 
Thus, we see that judgements of exchangeability and "lack of memory" for 
sequences of positive real-valued random quantities constrain the possible  
predictive probability models for the sequence to be those which are generated by acting 
as if we have a random sample from an exponential distribution with unknown 
parameter 6, with a prior distribution Q for the latter. In fact, if Q* denotes the 
corresponding distribution for <j> = 6~l = lim^co xn, it may be easier to use the 
"reparametrised" representation 
/■oo " 
p{xu...,xn)= / J|0~1exp(-0_1a;l)dQ*(0), 
"'0 i=i 
since Q* is then more directly accessible as "beliefs about the sample mean from 
a large number of observations". 
Recalling the possible motivation given above for the additional invariance 
assumption on the sequence x\,X2,--., it is interesting to note the very specific 
and well-known "lack of memory" property of the exponential distribution; namely, 
P(xi > ax + a.2 10, X( > aj) = P(xt > a2 10), 
which appears implicitly in the above proof. 
4.4.4 The Geometric Model 
Suppose x\, x2, ■ ■ ■ is judged to be an infinitely exchangeable sequence of strictly 
positive integer-valued random quantities. It is easy to see that we could repeat 
the entire introductory discussion of Section 4.4.3, except that events would now 
be defined in terms of sets of points on the lattice Z+ x ... x Z+, rather than as 
regions in SR+ x ... x SR+. This enables us to state the following representation 
result. 
Proposition 4.8. (Discrete representation under origin invariance). 
Ifx\ ,X2, ■ ■ ■ is an infinitely exchangeable sequence of positive integer-valued 
random quantities with probability measure P, such that, for all n and any 
event A in Z+ x ... x Z+, 
P[{xu ..., xn) £ A] = P[(xu ...,xn)eA + a] 
for all a G Z x ... x Z such that all = 0 and A + a is an event in 
Z+ x ... x Z+, then the joint density for x\,..., xn has the form 
P(»,,...,»„)= / 6(i-ey*-ldQ(e), 
Jo 
where 
Q(e)=iimP[(x-l)<e] 
n—>oo 
andxn = n~l(x\ + • • • +x„). 
190 
4 Modelling 
Outline proof. This follows precisely the steps in the proof of Proposition 4.7, 
except that, for positive integer-valued x,, the functional equation 
P(xi >al + a2\F) = P(xi > ax \F)P(Xi >a2\F) 
implies that 
P(xi>x\F) = Ox, 
so that the probability function, p(x, | F) = p(x, | 9)is easily seen to be#(l—0)1*-1. 
Again, by the strong law of large numbers, 9~x = lim^oo xn, where, since x, > 1 
foralH.O <9< 1. < 
In this case, the coherent predictive probability models must be those which 
are generated by acting as if we have a random sample from a geometric distribution 
with unknown parameter 9, with a prior distribution Q for the latter, where 9~l = 
iimu—,00 xn. 
Again, recalling the possible motivation for the additional invariance property, 
it is interesting to note the familiar "lack of memory" property of the geometric 
distribution; 
P(xi > ai + a2 | 9, Xi > ai) = P(xt > a2 19). 
4.5 MODELS VIA SUFFICIENT STATISTICS 
4.5.1 Summary Statistics 
We begin with a formal definition, which enables us to discuss the process of  
summarising a sequence, or sample, of random quantities, Xi,..., xm. (In general, our 
discussion carries over to the case of random vectors, but for notational simplicity 
we shall usually talk in terms of random quantities.) 
Definition 4.6. (Statistic). Given random quantities (vectors) x\,...,xm, 
with specified sets of possible values X\,..., Xm, respectively, a random vec- 
tor tm : Xi x • • • x Xm —> $tk(m\k{m) < m) is called a k(m)-dimensional 
statistic. 
A trivial case of such a statistic would be tm(x\,..., xm) = (xi,..., xm), 
but this clearly does not achieve much by way of summarisation, since k(m) — m. 
Familiar examples of summary statistics are: 
tm = m_1(xi + ... + xm), the sample mean (k(m) = 1); 
tm = [m, (xi + ... + xm), (xf + h !„)], the sample size, total and sum 
of squares (k(m) = 3); 
tm = [m, med{x!,..., xm}], the sample size and median (k(m) = 2); 
tm = max{x!,..., xm} — min{xi,..., xm}, the sample range (k(m) = 1). 
4.5 Models via Sufficient Statistics 
191 
To achieve data reduction, we clearly need fc(m) < m: moreover, as with the 
above examples, further clarity of interpretation is achieved if k(m) = fe, a fixed 
dimension independent of m. 
In the next section, we shall examine the formal acceptability and implications 
of seeking to act as if particular summary statistics have a special status in the 
context of representing beliefs about a sequence of random vectors. We shall not 
concern ourselves at this stage with the origin of or motivation for any such choice 
of particular summary statistics. Instead, we shall focus attention on the general 
questions of whether, and under what circumstances, it is coherent to invoke such 
a form of data reduction and, if so, what forms of representation for predictive 
probability models might result. Throughout, we shall assume that beliefs can be 
represented in terms of density functions. 
4.5.2 Predictive Sufficiency and Parametric Sufficiency 
As an example of the way in which a summary statistic might be assumed to play 
a special role in the evolution of beliefs, let us consider the following general 
situation. Past observations xi,..., xm are available and an individual is  
contemplating, conditional on this given information, beliefs about future observations 
xm+i,..., xn, to be described by p(xm+i,..., xn | xx,...,xm). The following 
definition describes one possible way in which assumptions of systematic data 
reduction might be incorporated into the structure of such conditional beliefs. 
Definition 4.7. {Predictive sufficiency). 
Given a sequence of random quantities x\,x2, ■ ■ ■, with probability measure P, 
where x, takes values in Xi,i = 1,2,... the sequence of statistics t\, t2,..., 
with tj defined on X\ x • • • x Xj, is said to be predictive sufficient for the 
sequence X\,x2, ■ ■ .ifforallm > l,r > \and{i\,..., v}n{l,... , m} = 0, 
PyXiy,..., Xjr | xi,...,xm) = p[Xiy,...,Xjr | tm), 
where p(. \.) is the conditional density induced by P. 
The above definition captures the idea that, given tm = tm(x\,... ,xm), 
the individual values of xi,..., xm contribute nothing further to one's evaluation 
of probabilities of future events defined in terms of as yet unobserved random 
quantities. Another way of expressing this, as is easily verified from Definition 4.7, 
is that future observations (x^,..., x,r) and past observations (xi,..., xm) are 
conditionally independent given tm. Clearly, from a pragmatic point of view the 
assumption of a specified sequence of predictive sufficient statistics will, in general, 
greatly simplify the process of assessing probabilities of future events conditional on 
past observations. From a formal point of view, however, we shall need additional 
192 
4 Modelling 
structure if we are to succeed in using this idea to identify specific forms of the 
general representation of the joint distribution of xi,..., xn. 
As a particular illustration of what might be achieved, we shall assume in 
what follows that the probability measure P describing our beliefs implies both 
predictive sufficiency and exchangeability for the infinite sequence X\, x2, — As 
with our earlier discussion in Section 4.4, a mathematically rigorous treatment is 
beyond the intended level of this book and so we shall confine ourselves to an 
informal presentation of the main ideas. 
In particular, throughout this section we shall assume that the exchangeability 
assumption leads to a finitely parametrised mixture representation, as in  
Corollary 1 to Proposition 4.3, so that, as shown in Corollary 2 to that proposition, the 
conditional density function of xm+\,... ,xn, given Xi,..., xm, has the form 
f n 
p(xm+u... , Xn I Xi, . . . , Xmj — X\, . . . , Xmj 
where 
dQ{0\xi,...,xm) 
fTl7=Mxi\0)dQ((>) 
and all integrals, here and in what follows, are assumed to be over the set of possible 
values of 9. 
This latter form makes clear that, for such exchangeable beliefs, the learning 
process is "transmitted" within the mixture representation by the updating of beliefs 
about the "unknown parameter" 0. This suggests another possible way of defining 
a statistic tm = tm(x\,..., xm) to be a "sufficient summary" of xi,..., xm. 
Definition 4.8. (Parametric sufficiency). If x\, x2, ■ ■ ■ is an infinitely  
exchangeable sequence of random quantities, where xt takes values in Xi — 
X, i = 1,2,..., the sequence of statistics t\,t2,..., with tj defined on X\ x 
• • • x Xj, is said to be parametric sufficient for xi,X2,... if, for any n > 1, 
dQ(0\x1,...,xn) = dQ(0\tn), 
for any dQ (6) defining an exchangeable predictive probability model via the 
representation 
/n 
Y[p(xi\e) dQ(6). 
Definitions 4.7 and 4.8 both seem intuitively compelling as encapsulations 
of the notion of a statistic being a "sufficient summary". It is perhaps reassuring 
therefore that, within our assumed framework, we can establish the following. 
4.5 Models via Sufficient Statistics 
193 
Proposition 4.9. (Equivalence of predictive and parametric sufficiencies). 
Given an infinitely exchangeable sequence of random quantities X\,X2,--., 
where x, takes values in X, = X, i = 1,2,..., the sequence of statistics 
t\,t2,.-. with tj defined on X\ x • ■ • x Xj is predictive sufficient if, and only 
if, it is parametric sufficient. 
Outline proof. For any xi,...,xm, xm+i, ...,xn and any sequence of  
statistics tm, where tm = tm (x\, ..., xm), m = 1,..., n — 1, the representation 
theorem implies that 
P\Xm+\i ■ • • ,Xn\ tm) = —j- -p(Xm_|_i, . . ., Xn, tm), 
P{tm) 
l r 
— ~7~L T / P\-^li • • • i 2-m) ^m+1, • • • , Xn) uXi, .. . , dXm 
Pytm) J A 
where A = {(xi..., xm); tm(xi,...,xm) = tm}, which, in turn, can be easily 
shown to be expressible as 
-rj-r f \ [f[p(xi\0)dx1,...,dxn dQ(0) 
h: f fl p{xi\e)p(tm\e)dQ{d)= f f[ p(xt\e)dQ(e\tm), 
p(t 
where 
dQ(V | tm) = -j-r-rPitm | 0) dQ(d) - 
p(tmfKmll ' ^V ' fp(tm\8)dQ(0) 
It follows that 
P\Xm+\-> ■ ■ ■ > Xn I tm) = P{Xm+li ■ ■ • iXn\X\, . . . , Xm) 
n 
Y[ p(xi\d)dQ(6\xi,...,xm), 
i=m+l 
if, and only if, dQ(0 \ xu ..., xm) = dQ{9 \ tm) for all dQ{9). < 
f. 
To make further progress, we now establish that parametric sufficiency is itself 
equivalent to certain further conditions on the probability structure. 
Proposition 4.10. (Neymanfactorisation criterion). The sequence t\, t2,... 
is parametric sufficient for infinitely exchangeable xi,x2,..- admitting a 
finitely parametrised mixture representation if and only if, for any m > 1, 
the joint density for x\,..., xm given 8 has the form 
p(xi,...,xm\6) = hm(tm,6)g(xl,...,xm), 
for some functions hm > 0, g > 0. 
194 4 Modelling 
Outline proof. Given such a factorisation, for any dQ(0) we have 
p(xi,...,xm|0)dQ(0) _ hm(tm,O)dQ(0) 
dQ{0\xi,...,xm) 
Jep(xu...,xm\e)dQ{e) fehm(tm,8)dQ(0) 
for some hm > 0. The right-hand side depends on x\,..., xm only through tm 
and, hence, dQ(6 \ xi,..., xm) — dQ(6 \ tm). Conversely, given parametric  
sufficiency, we have, for any dQ(0) with support 6, 
P(ai, xw|fl)dq(g) = 
p(xi,...,xm) 
-wnrfll/ l-pMM 
-dg(«|tm)- p(tm) 
so that 
p(xi,...,xm|0) = hm(tm,0)g(xi,...,xm) 
for some /im > 0, g > 0 as required. < 
Proposition 4.11. (Sufficiency and conditional independence). 
The sequence t\,t2, ■ ■ ■ is parametric sufficient for infinitely exchangeable 
xi, X2, ■ • • if, and only if, for any m > 1, the density p(x\,..., xm \ 6, tm) is 
independent of 6. 
Outline proof. For any tm = tm(x\,..., xm) we have 
p(xi,... ,xm | 0) = p(xi, ...,xm\ 0,tm)p(tm | 9). 
Ifp(xi,... ,xm \6,tm) is independent of 0, the parametric sufficiency of tls t2,... 
follows immediately from Proposition 4.10. 
Conversely, suppose that t\, t2, ■ ■ ■ is parametric sufficient, so that, by  
Proposition 4.10, 
p(x1,...,xm|0) = hm(tm,6)g(xi,...,xm) 
for some hm > 0,g > 0. Integrating over all values {xi,... ,xm} such that 
tm(xi,..., xm) = tm, we obtain 
P(tm\6) = hm(ttn,e)G(tm) 
forsomeG > 0. Substituting for hm(tm,6) in the expression for p(xi, ...,xm\9), 
we obtain 
g(xi,...,xm) 
p(xi,...,xm|0) =p(tm\0)- 
P\X\ , . . . , Xm I 17, tm ) 
G(tm) 
so that 
g(X!,...,Xm) 
G(tm) 
which is independent of 6. <] 
4.5 Models via Sufficient Statistics 
195 
In the approach we have adopted, the definitions and consequences of  
predictive and parametric sufficiency have been motivated and examined within the 
general framework of seeking to find coherent representations of subjective  
beliefs about sequences of observables. Thus, forexample, the notion of parametric 
sufficiency has so far only been put forward within the context of exchangeable 
beliefs, where the operational significance of "parameter" typically becomes clear 
from the relevant representation theorem. 
In fact, however, as the reader familiar with more "conventional" approaches 
will have already realised, related concepts of "sufficiency" are also central to non- 
subjectivist theories. In particular, we note that the non-dependence of the density 
p(xi,..., xm 19, tm) on 0, established here in Proposition 4.11 as a consequence 
of our definitions, was itself put forward as the definition of a "sufficient statistic" 
by Fisher (1922), and the factorisation given in Proposition 4.10 was established 
by Neyman (1935) as equivalent to the Fisher definition. 
From an operational, subjectivist point of view, it seems to us rather  
mysterious to launch into fundamental definitions about learning processes expressed 
in terms of conditioning on "parameters" having no status other than as "labels". 
However, from a technical point of view, since our representation for  
exchangeable sequences provides, for us, a justification for regarding the usual (Fisher) 
definition as equivalent to predictive and parametric sufficiency, we can exploit 
many of the important mathematical results which have been established using 
that definition as a starting point. 
In the context of our subjectivist discussion of beliefs and models, we shall 
mainly be interested in asking the following questions. 
When is it coherent to act as if there is a sequence of predictive sufficient 
statistics associated with an exchangeable sequence of random quantities? 
What forms of predictive probability model are implied in cases where we can 
assume a sequence of predictive sufficient statistics? 
Aside from these foundational and modelling questions, however, the results 
given above also enable us to check the form of the predictive sufficient statistics 
for any given exchangeable representation. We shall illustrate this possibility with 
some simple examples before continuing with the general development. 
Example 4.5. (Bernoulli model). We recall from Proposition 4.1 that if x\, x2, ■ ■ ■ 
is an infinitely exchangeable sequence of 0-1 random quantities, then we have the general 
representation 
p(xi,...,x„)= / p(xi,...,xn\6)dQ(6) 
Jo 
= f f[BT(x, \0)dQ(0) 
Jo j=1 
= / esn(i-e)n-sndQ(e), 
Jo 
196 
4 Modelling 
where sn = x\-\ \- xn. Defining tn = [n, sn] and noting that we can write 
p(x1,..., xn | 6) = hn(tn, 6)g(x1, ...,xn), 
with 
M*n, o) = es»(i - e)n-s'\g(Xl,..., Xn) = i, 
it follows from Propositions 4.9 and 4.10 that the sequence ti, t2,... is predictive and 
parametric sufficient for x\, x2, This corresponds precisely to the intuitive idea that the 
sequence length and total number of 1 's summarises all the interesting information in any 
sequence of observed exchangeable 0-1 random quantities. 
Example 4.6. (Normalmodel). We recall from Proposition 4.5 that if xi, x2,... is 
an exchangeable sequence of real-valued random quantities with the additional property of 
centred spherical symmetry then we have the general representation 
/oo />oo 
/ p(xi,...,zJ/i,A)dQ(/i,A) 
■oo JO 
/OO yOO n 
/ Y[^(xi\^X)dQ(fM,X) 
oo JO j=1 
/oo /.oo " / » \ 1/2 ( \ 1 
J n(s) «p {-i<*<-rf}««M> 
y t) exp|--[n(x„-/i)2 + ns2]WQ(/i,A) 
where 
1 " 1 " 
n z—' n z—' 
i=l i=l 
In the light of Propositions 4.10 and Proposition 4.11, inspection of p(xx,... ,xn\fi,\) 
reveals that 
^n — i^i ^n* SnJ 
defines a sequence of predictive and parametric sufficient statistics for xi, x2, In view 
of the centring and spherical symmetry conditions, it is perhaps not surprising that the 
sample size, mean and sample mean sum of squares about the mean turn out to be sufficient 
summaries. Of course, tn is not unique; for example, since 
ns2n = x2 + • • • + x2n - n(xn)2 
we could equally well define tn = [n, xn, n~l (x\-\ h x2)] as the sequence of sufficient 
statistics. 
4.5 Models via Sufficient Statistics 197 
Example 4.7. (Exponentialmodel). We recall from Proposition 4.7 that if X\,X2, ••. 
is an exchangeable sequence of positive real-valued random quantities with an additional 
"origin invariance" property, then we have the general representation 
p(xu...,xn) = / p(xi,...,xn\0)dQ(0) 
Jo 
Y[Ex(xi\e)dQ(0) 
= / enexP(-esn)dQ(e) 
Jo 
where sn = xx + ■ ■ ■ + xn. Again, it is immediate from Propositions 4.10 and 4.11 that 
t„ = [n, sn] defines a sequence of predictive and parametric sufficient statistics, although, 
in this example, there is not such an obvious link between the form of invariance assumed 
and the form of the sufficient statistic. 
It is clear from the general definition of a sufficient statistic (parametric or 
predictive) that tn(x\,..., xn) — [n, (x\,... ,xn)] is always a sufficient statistic. 
However, given our interest in achieving simplification through data reduction, it is 
equally clear that we should like to focus on sufficient statistics which are, in some 
sense, minimal. This motivates the following definition. 
Definition 4.9. (Minimal sufficient statistic). Ifxi,x2, ■ ■ ■, is an infinitely  
exchangeable sequence of random quantities, where x, takes values in X, = X, 
the sequence of statistics t\, t2,..., with tj defined on X\ x ... x X,, is 
minimal sufficient for x\, x2,. ■ ■ if given any other sequence of sufficient  
statistics, 81,82, ■ ■-, there exist functions gl(-),g2(), ■ ■ ■ such thatU = g^Si), 
t = l,2,... 
It is easily seen that the forms of t(x) identified in Examples 4.5 to 4.7 are 
minimal sufficient statistics. From now on, references to sufficient statistics should 
be interpreted as intending minimal sufficient statistics. 
Finally, since n very often appears as part of the sufficient statistic, we shall 
sometimes, to avoid tedious repetition, omit explicit mention of n and refer to the 
"interesting function(s) of the sufficient statistic. 
4.5.3 Sufficiency and the Exponential Family 
In the previous section, we identified some further potential structure in the general 
representation of joint densities for exchangeable random quantities when  
predictive sufficiency is assumed. We shall now take this process a stage further by 
examining in detail representations relating to sufficient statistics of fixed  
dimension. 
198 
4 Modelling 
Since we have established, in the finite parameter framework, the equivalence 
of predictive and parametric sufficiency for the case of exchangeable random  
quantities, and their equivalence with the factorisation criterion of Proposition 4.11, we 
shall from now on simply use the term sufficient statistic, without risk of confusion. 
We begin by considering exchangeable beliefs constructed by mixing, with 
respect to some dQ(9), over a specified parametric form 
k 
P(xi,...,xn\e) = Y[p(xi\9), (xi,...,i»)a"cr 
i=\ 
where 9 is a one-dimensional parameter. By Proposition 4.10, if the form of p(x\0) 
is such that p(x\,...,xn\9) factors into hn(tn, 9)g(x\,...,xn), for some hn, g, 
the statistic tn = tn(x\,..., xn) would be sufficient. An important class of such 
p(x\0) is identified in the following definition. 
Definition 4.10. (One-parameter exponential family). A probability density 
(or mass function) p(x \ 9), labelled by 9 e 0 C 5J, is said to belong to the 
one-parameter exponential family if it is of the form 
p(x ] 9) = Ef(x | /, g, h, <f>, 9, c) = f(x)g(9) exp{c0(fl)Ji(s)}, x e X, 
where, given f,h,<j), and c,[g(9)]~l = Jx f(x)exp{c<j)(9)h(x)}dx < oo. 
The family is called regular ifX does not depend on 9; otherwise it is called 
non-regular. 
Proposition 4.12. (Sufficient statistics for the one-parameter exponential 
family). Ifxi,x2,... ,xn e X, is an exchangeable sequence such that, given 
regular Ef(-|-), 
p(xi,...,x„)= / YlEf(xi\f,g,h,<}>,9,c)dQ{9), 
for some dQ(9), then tn = tn(x\,... ,xn) = [n, h(x\) + h h(xn)], for 
n = 1,2,..., is a sequence of sufficient statistics. 
Proof. This follows immediately from Proposition 4.10 on noting that 
n n f n 
J] Ef(Xi\f,g, h, &9,c) = H f(Xi) ■ [g(9)f exp I aj>{9) £ h(Xi) 
i=l i=l I i=l 
< 
4.5 Models via Sufficient Statistics 
199 
The following standard univariate probability distributions are particular cases 
of the (regular) one-parameter exponential family with the appropriate choices of 
/, g, etc. as indicated. 
Bernoulli 
p{x\9)=Br{x\9) = 9x{l-9)1-x, x e {0,1}, fle[0,l]. 
/(x) = l, g{9) = 1-9, h(x) = x, <j>(9) = log^ , c = 1. 
Poisson 
9xe~e 
p{x | 9) = Po(x | 9) = —r, x€ {0,1,2,...}, «6S+. 
/(x) = (x!)"1, g(6) = e-$, fc(x) = x, <A(0)=log0, c = 1. 
Ecponenria/ 
p(x | 0) = Ex(x | 9) = 9e'9x, x e 3*+, «eS+. 
/(x) = l, g(9) = 9, h(x) = x, <t>{9) = 9, c = -l. 
Normal (variance unknown) 
p(x|fl) = N(x|0,fl) = (0/(27r))1/2exp[-i0x2], x e K, 0 G »+. 
/(x) = (27T)"1/2, g(e) = 61'2, h(x) = x2, <j>{9) = 9, c=-l/2. 
We note that the term c0(0) appearing in the general Ef (• | •) form could always 
be simply written as <f>*(9) with 4>* suitably defined (see, also, Definition 4.11). 
However, it is often convenient to be able to separate the "interesting" function of 
9, <j)(9), from the constant which happens to multiply it. 
In Definition 4.10, we allowed for the possibility (the non-regular case) that 
the range, X, of possible values of x might itself depend on the labelling parameter 
9. Although we have not yet made a connection between this case and forms of 
representation arising in the modelling of exchangeable sequences, it will be useful 
at this stage to note examples of the well-known forms of distribution which are 
covered by this definition. We shall indicate later how the use of such forms in the 
modelling process might be given a subjectivist justification. 
Uniform 
p(x\6) = U(x\0,9) = 9-\ xe(0,fl), 0S3J+. 
/(x) = l, g(6) = 6-\ h(x) = 0, 4(0) = 6, c = l. 
200 
4 Modelling 
Truncated exponential 
p(x\e) = Jex(x\e)=exp[-(x-6)}, x-9£^+, 9 e 3?+. 
f(x) = e~', g(0) = eB, h(x) = 0, <f>(0)=0, c = 1. 
In order to identify sequences of sufficient statistics in these and similar cases, 
we make use of the factorisation criterion given in Proposition 4.10. 
For the uniform, we rewrite the density in the form 
p(x\6) = e-lIm(x), xe3J, 
so that, for any sequence x\,...,xn which is conditionally independent given 9, 
n 
p(xi,...,xn\6) = ]\p(xi\6) 
= e-nI(m( max {£;}), (xi,...,xn) e3?n. 
v ' \ !=l,...,n / 
It then follows immediately from Proposition 4.10 that 
tn tn\X\ , ... , Xnj 
n, max {xi} 
i=l,...,n 
71= 1,2,... 
is a sequence of sufficient statistics in this case. 
For the truncated exponential, if we rewrite the density in the form 
p(x | 9) = exp[-0 - 9)} /(o,cx>)(aO, xt'Sl, 
a similar argument shows that, for (x\,..., xn) e 3?+, 
p{xi, ...,xn\9)= exp[nxn] exp[-n^] Lgoo) ( min {Xi}), 
\ i=l,...,n / 
so that, for n = 1,2,..., 
tn — tn \X\ , . . . , Xn j 
provides a sequence of sufficient statistics. 
n, min {xj 
«=l,...,n 
The above discussion readily generalises to the case of exchangeable  
sequences generated by mixing over specified parametric forms involving a fc-di- 
mensional parameter 6. 
4.5 Models via Sufficient Statistics 201 
Definition 4.11. (k-parameter exponential family). A probability density (or 
mass function) p(x \ 6), x G X, which is labelled by 6 G 6 C 3?*, is said to 
belong to the k-parameter exponential family if it is of the form 
p(x 16) = Eik(x | /, g, h, <p, 6, c) = f{x)g{8) exp j J] c^M*) | ' 
vv/iere h = (hi,...,hk), <t>{0) = (0i,...,0*) and, given the functions 
/, h, 0, and f/ie constants c,, 
-tttt = / /0*0 exP { Y" Ci4>i{9)hi{x) }dx<oo. 
The family is called regular ifX does not depend on 6; otherwise it is called 
non-regular. 
Proposition 4.13. (Sufficient statistics for the k-parameter exponential  
family). If X\,X2, ■ ■ ■ ,Xi G X, is an exchangeable sequence such that, given 
regular k-parameter Ef*(-1 •), 
P n 
p(xi,...,x„) = / T\Efk(xt\f,g,h,<t>,0,c)dQ(0), 
for some dQ(6), then 
t*n — t,n\X\, . . . , Xn) 
n,Y^h(xi),... ,^ hk(xi) 
i=l t=l 
,71=1,2,... 
is a sequence of sufficient statistics. 
Proof. This is analogous to Proposition 4.12 and is a straightforward  
consequence of Proposition 4.10. < 
The following standard probability distributions are particular cases (the first 
regular, the second non-regular) of the fc-parameter exponential family with the 
appropriate choices of /, g etc. as indicated. 
202 
4 Modelling 
Normal (unknown mean and variance) 
p(x | 9) = p(x | fi, t) = N(x | fi, t) 
( t \i/2 
fe) 6XP 
-Ux-v? , i£», A*€», re9?+. 
In this case, A; = 2 and 
/(*) = (27T)"1/2, 5(0) = r1/2 exp[-I r/i2], /i(x) = (x, x2), 
0(0) = (r/i,r), Cl=l, C2 = -l/2, 
so that tn = [n, £)™=1 a^, £)"=1 ^f] . n = 1,2,... is a sequence of sufficient 
statistics. 
Uniform (over the interval [61,62]) 
p(x\8) = p(x\6u62) = U(x\6u62) = (62 - 6^, 
In this case, 
/(x) = l, 5(0) = (02 -ft)"1, /i(x) = 0, <P(8) = (6U62), Cl = c2 = 0, 
and 
£„ = [n,min{xi,...,a;n}, maxfs!,...,^}],!! = 1,2,... 
is easily seen to give a sequence of sufficient statistics. 
The description of the exponential family forms given in Definitions 4.10 and 
4.11, is convenient for some purposes (relating straightforwardly to familiar  
versions of parametric families), but somewhat cumbersome for others. This motivates 
the following definition, which we give for the general fc-parameter case. 
Definition 4.12. (Canonical exponential family). 
The probability density (or mass function) 
p(y I ip) = Ccf{y I a, b, */>) = a(y) exp{yV - b(ip)}, y £Y, 
derived from Ef^(-1 •) in Definition 4.11, via the transformations 
y = (yi,---,yk), */> = W'l,---,^*), 
Vi = hi(x), fa = Ci<j>i(0), i = l,...,k, 
is called the canonical form of representation of the exponential family. 
4.5 Models via Sufficient Statistics 
203 
Systematic use of this canonical form to clarify the nature of the Bayesian 
learning process will be presented in Section 5.2.2. Here, we shall use it to  
examine briefly the nature and interpretation of the function b(ip), and to identify the 
distribution of sums of independent Cef random quantities. 
Proposition 4.14. (First two moments of the canonical exponential family). 
For y in Definition 4.12, 
E{v\i/>) = VbW>), V{y\i>) = V2bW). 
Proof. It is easy to verify that the characteristic function of y conditional on 
if) is given by 
E(exp{iuly} \ t/>) = exp{b(iu + t/>) - b(i/))}, 
from which the result follows straightforwardly. < 
Proposition 4.15. (Sufficiency in the canonical exponential family). 
VVir "" > Vn are independent Cef\y \ a, b, t/>) random quantities, then 
n 
i=\ 
is a sufficient statistic and has a distribution Cef (s | a^, nb, t/>), where a^ 
is the n-fold convolution of a. 
Proof. Sufficiency is immediate from Proposition 4.12. We see immediately 
that the characteristic function of s is exp{nb(iu + t/>) - nb(tj))}, so that the 
distribution of s is as claimed, where a^ satisfies 
nb(%l)) = log / a^n\s)exp{ilyts}ds. 
Examination of the density convolution form for n = 1, plus induction, establishes 
the form of a(n). < 
Our discussion thus far has considered the situation where exchangeable belief 
distributions are constructed by assuming a mixing over finite-parameter  
exponential family forms. A consequence is that sufficient statistics of fixed dimension 
exist. Moreover, classical results of Darmois (1936), Koopman (1936), Pitman 
(1936), Hipp (1974) and Huzurbazar (1976) establish, under various regularity 
conditions, that the exponential family is the only family of distributions for which 
such sufficient statistics exist. 
204 
4 Modelling 
In the second part of this subsection, we shall consider the question of whether 
there are structural assumptions about an exchangeable sequence Xi, x?,..., which 
imply that the mixing must be over exponential family forms. 
Previously, in Section 4.4, we considered particular invariance assumptions, 
which, together with exchangeability, identified the parametric forms that had to 
appear in the mixture representation. Here, we shall consider, instead, whether 
characterisations can be established via assumptions about conditional  
distributions, motivated by sufficiency ideas. 
As a preliminary, suppose for a moment that an exchangeable sequence, {y,}, 
is modelled by 
/n 
Y[Cd(yt\a,b,ip)dQ(ip). 
Now consider the form of p(ylt. ■., yk \ yx H h yn = s), k < n. Because 
of exchangeability, this has a representation as a mixture over 
P(l/i. •--.!/* I l/i + •■■+!/« = *> V0- 
But the latter does not involve t/> because of the sufficiency of yx + • • • + yn 
(Propositions 4.11 and 4.15), so that 
p(yn---,yk\Y™=iVi = s) = p(i/i»-■•>!/* I ElUi/i = *.V0 
nr n r ,t , / , m a(n~kHa - Sfc)exp{'0tSfc - (n - k)b(ib)} 
a{Vl) expWyi - b(il> } (n)( I sit 1//U 
where, in the numerator, s^ = yx -\ \-yk < s. The exponential family mixture 
representation thus implies that, 
Now suppose we consider the converse. If we assume yr, y2, ■ ■ ■ to be  
exchangeable and also assume that, for all n and k < n, the conditional distributions 
have the above form (for some a defining a Cef(y \ a, b, i/>) form), does this  
imply that p(yi, ■ ■ ■, yn) has the corresponding exponential family mixture form? 
A rigorous mathematical discussion of this question is beyond the scope of this 
volume (see Diaconis and Freedman, 1990). However, with considerable licence 
in ignoring regularity conditions, the result and the "flavour" of a proof are given 
by the following. 
4.5 Models via Sufficient Statistics 
205 
Proposition 4.16. (Representation theorem under sufficiency). 
VV\ i Vi! • • • " any exchangeable sequence such that, for alln> 2 and k < n, 
k 
P(Vu ■ ■ ■, Vk I Vi + ■ • • + Vn = «) = Ha(yi)a{n"k)(s ~ «*)A»(n)(«)» 
where s^ = yx + ■ • • + yk and a(-) defines Cef(y \ a, b, ip), then 
r n 
P(Vi, • • ■ > 1/n) = J II Cef^ I a' 6' V0<W), 
i=l 
for some dQ(ip). 
Outline proof. We first note that exchangeability implies a mixture  
representation, mixing over distributions which make the yi independent. But each of the 
latter distributions, with densities denoted generically by /, themselves imply an 
exchangeable sequence, so that, forn > 2, k < n, f(yr,... ,yk | y±-\ \-yn = s) 
also has the specified form in terms of a(). 
Now consider n = 2, k = 1. Independence implies that 
ti i n f{Vi)fi8-Vi) 
f{yl\yx + V2 = s) = —^-j—. 
where /(•) denotes the marginal density and /(2> (•) its twofold convolution, so that 
/(■) must satisfy 
/(l/i)/(* - Vi) _ a(yiHs ~ Vi) 
If we now define 
and 
it follows that 
/(2)(s) a(2>(a) 
tt{v,)=Hm losm 
a(l/i) a(°) 
aW(s) a(0) 
u{yj) + u{s - yj) = v(s). 
Setting yx = s, and noting that u(0) = 0, we obtain u(s) = v(s), and hence 
u(yi) + u{y2) =u(y1+y2). 
This implies that u(y) = iply, for some ip, so that 
f(y) = a(y) exp{t/>'y - b(tp)}. 
The following example provides a concrete illustration of the general result. 
206 
4 Modelling 
Example 4.8. (Characterisation of the Poisson model). Suppose that the sequence 
of non-negative integer valued random quantities yi,V2,--- is judged exchangeable, with 
the conditional distribution of y = (yi,.- ■ ,yk) given yi + ■ ■■ + yn = s, n > 2, k < n, 
specified to be the multinomial Mu^iy \ s, 0), where 0 = (1/n, ■ • ■, 1/n), so that 
s\ JL f\\Vi ( k\"~Sk 
^•-"*i^+-+^=')=nf.lw!(a_at)1nuJ v-n) ' 
where sk = yi -\ 1- yk- Noting that the Poisson distribution, Pn(y \ ip), can be written in 
Cef(y | a, b, ip) form as 
Pn(y\ip) = — exp{yip-e*} = a(y) exp{yip - b(ip)}, 
from which it easily follows that a(n) (s) = ns/s\, it is straightforward to check that, in terms 
ofa(-)anda<n>(-). 
Mk(y 18,0) = Y[a(yi)dn-k\s - sk)/a^(s). 
By Proposition 4.16, it follows that the belief specification for yu y2,... is coherent and 
implies that 
Y[Pa(Vi\il>)dQ(il>), 
i=l 
for some dQ(V'), ip € 5R+. < 
As we remarked earlier, the above heuristic analysis and discussion for the k- 
parameter regular exponential family has been given without any attempt at rigour. 
For the full story the reader is referred to Diaconis and Freedman (1990). Other 
relevant references for the mathematics of exponential families include Barndorff- 
Nielsen (1978), Morris (1982) and Brown (1985). 
We conclude this subsection by considering, briefly and informally, what can 
be said about characterisations of exchangeable sequences as mixtures of non- 
regular exponential families. For concreteness, we shall focus on the uniform, 
U(x | 0,6), distribution, which has density 6~lI(p^ (x), x e 3?, and sufficient  
statistic max{xi,... ,xn}, given a sample Xi,...,xn. This sufficient statistic is clearly 
not a summation, as is the case for regular families (and plays a key role in  
Proposition 4.16). However, conditional on mn = max{xi,... ,xn},X\,... ,x^,k «n, 
are approximately independent U(xi \ 0, mn) and this will therefore be true for 
all exchangeable X\,X2,... constructed by mixing over independent Ufa \0,6). 
Conversely, we might wonder whether positive exchangeable sequences having 
4.5 Models via Sufficient Statistics 
207 
conditional property are necessarily mixtures of independent U(xi\Q,0).  
Intuitively, if mn tends to a finite 9 from below, as n —> oo, one might expect the result 
to be true. This is indeed the case, but a general account of the required  
mathematical results is beyond our intended scope in this volume. The interested reader is 
referred to Diaconis and Freedman (1984), and the further references discussed in 
Section 4.8.1. 
4.5.4 Information Measures and the Exponential Family 
Our approach to the exponential family has been through the concept of predictive 
or, equivalently, parametric sufficient statistics. It is interesting to note, however, 
that exponential family distributions can also be motivated through the concept of 
the utility of a distribution (c.f. Section 3.4), using the derived notions of  
approximation and discrepancy. 
Consider the following problem. We seek to obtain a mathematical  
representation of a probability density p(x), which satisfies the k (independent)  
constraints 
hi(x)p(x)dx = to, < oo, i = 1,..., k, 
L 
X 
where m\,..., mk are specified constants, together with the normalizing constraint 
Jxp(x)dx = 1, and, in addition, is to be approximated as closely as possible by a 
specified density f(x). 
We recall from Definition 3.20 (with a convenient change of notation) that the 
discrepancy from a probability density p(x) assumed to be true of an approximation 
f(x) is given by 
6(f\p) = JxP(x)\og^dx, 
where / and p are both assumed to be strictly positive densities over the same range, 
X, of possible values. Note that we are interested in deriving a mathematical  
representation of the true probability density p(x), not of the (specified) approximation 
f(x). Thus, we minimise 6(f | p) over p subject to the required constraints on p, 
rather than 6(f | p) over / subject to constraints on /. Hence, we seek p to minimise 
F(p) = f p{ 
Jx 
x) log -jr-{dx 
+ y* 6i l hi(x)p(x)dx — rrii +c / p(x)d: 
^ Ux J Ux 
where 6\,...,0k and c are arbitrary constant multipliers. 
208 
4 Modelling 
Proposition 4.17. (The exponential family as an approximation). 
The functional F(p) defined above is minimised by 
p(x) = Ef*(x | /,g,h,<j>,0,c),x£ X 
where f and h are given in F(p), Ci = 1, (ft = 6 = {0\,..., Ok) and 
— = / f{x) exp I ^Oihiix) \ dx. 
9(d) 
Proof. By a standard variational argument (see, for example, Jeffreys and 
Jeffreys, 1946, Chapter 10), a necessary condition for p to give a stationary value 
of F(p) is that 
{d/da)F{p(x) + aT(x))\a=0 = 0 
for any function r : x —» 3? of sufficiently small norm. This condition reduces to 
the equation 
/ 
log(p(x)/f(x)) + Y, Oihi(x) + (c + 1) 
from which it follows that 
i=\ 
r(x)dx = 0, 
Y/Olhi(x)\, 
.i=i J 
p(x) oc f(x) exp ■ 
as required. (For an alternative proof, see Kullback, 1959/1968, Chapter 3.) < 
The resulting exponential family form for p(x) was derived on the basis of a 
given approximation f(x) and a collection of "constant" functions h(x) = [hi(x), 
..., hk(x)]. If we wish to emphasise this derivation of the family, we shall refer to 
Ef(x | /, g, h, 4>, 0, c) as the exponential family generated by f and h. 
In general, specification of the sufficient statistic 
m, Y^ hi(xi),..., ^ hk(xi) 
«=i 
i=l 
does not uniquely identify the form of f(x) within the exponential family  
framework. Consider, for example, the Ga(x | a, 0) family with a known. Each distinct 
a defines a distinct exponential family with density 
(xa-l/T(a))0aexp{-0x}, 
4.6 Models via Partial Exchangeability 
209 
so that, in addition to h{x) — x, we need to specify f(x) = xa^1/T(a) in order to 
identify the family. 
Returning to the general problem of choosing p to be "as close as possible" to 
an "approximation" /, subject to the k constraints defined by h(x), it is interesting 
to ask what happens if the approximation / is very "vague", in the sense that / is 
extremely diffusely spread over X. A limiting form of this would be to consider 
f(x) = constant, which leads us to seek the p minimising Jxp(x) logp(x)dx 
subject to the given constraints. The solution is then 
p(x) = \ ^ 
Sx exP \T,i=i #AO)} dx 
which, since minimising Jxp(x) logp(x) dx is equivalent to maximising H(p) = 
— Jx p(x) logp(x) dx, is the so-called maximum entropy choice of p. 
Thus, for example, if X = 3?+and/i(a;) = x, the maximum entropy choice for 
p(x) is Ex(x | (f>), the exponential distribution with <A_1 — E(x \ <f>). If X = 3? and 
h(x) = (x, x2), the maximum entropy choice for p{x) turns out to be N(x | fi, A), 
the normal distribution with /i = E(x | /i, A), A-1 = V(x | /i, A) (c.f. Example 3.4, 
following Definition 3.20). 
Our discussion of modelling has so far concentrated on the case of beliefs 
about a single sequence of observations X\, x2,. ■., judged to have various kinds 
of invariance or sufficiency properties. In the next section, we shall extend our 
discussion in order to relate these ideas to the more complex situations, which arise 
when several such sequences of observations are involved, or when there are several 
possible ways of making exchangeable or related judgements about sequences. 
4.6 MODELS VIA PARTIAL EXCHANGEABILITY 
4.6.1 Models for Extended Data Structures 
In Section 4.5, we discussed various kinds of justification for modelling a sequence 
of random quantities X\,X2,--- as a random sample from a parametric family 
with density p(x | 6), together with a prior distribution dQ{6) for 6. We also 
briefly mentioned further possible kinds of judgements, involving assumptions 
about conditional moments or information considerations, which further help to 
pinpoint the appropriate specification of a parametric family. 
However, in order to concentrate on the basic conceptual issues, we have 
thus far restricted attention to the case of a single sequence of random quantities, 
X\, X2,..., labelled by a single index, i = 1,2,..., and unrelated to other random 
quantities. C learly, in many (if not most) areas of application of statistical modelling 
the situation will be more complicated than this, and we shall need to extend and 
210 
4 Modelling 
adapt the basic form of representation to deal with the perceived complexities of 
the situation. Among the typical (but by no means exhaustive) kinds of situation 
we shall wish to consider are the following, 
(i) Sequences xn, x^,... of random quantities are to be observed in each of i G / 
contexts. For example: we may have sequences of clinical responses to each of 
/ different drugs; or responses to the same drug used on / different subgroups 
of a population. A modelling framework is required which enables us to learn, 
in some sense, about differences between some aspect of the responses in the 
different sequences. 
(ii) In each of i G / contexts, j G J different treatments are each replicated 
k G K times, and the random quantities x^ denote observable responses for 
each context/treatment/replicate combination. For example: we may have / 
different irrigation systems for fruit trees, J different tree pruning regimes and 
K trees exposed to each irrigation/pruning combination, with x^ denoting 
the total yield of fruit in a given year; or we may have J different geographical 
areas, J different age-groups and K individuals in each of the IJ  
combinations, with Xijk denoting the presence or absence of a specific type of disease, 
or a coding of voting intention, or whatever. A modelling framework is  
required which enables us to investigate differences between either contexts, or 
treatments, or context/treatment combinations. 
(iii) Sequences of random quantities xn, xi2,..., i G /, are to be observed, where 
some form of qualitative assumption has been made about a form of  
relationship between the Xy and other specified (controlled or observed) quantities 
*i = (ziii ■ ■ ■ ,Zik),k > 1. For example: xtj might denote the status (dead or 
alive) of the j'th rat exposed to a toxic substance administered at dose level zt, 
with an assumed form of relationship between zt and the corresponding "death 
rate"; or x\3- might denote the height or weight at time zt from the jth replicate 
measurement of a plant or animal following some assumed form of "growth 
curve"; or x,j might denote the output yield on the jth run of a chemical 
process when k inputs are set at the levels z, = (zu,... ,Zki) and the  
general form of relationship between process output and inputs is either assumed 
known or well-approximated by a specified mathematical form. In each case 
a modelling framework is required which enables us to learn about the  
quantitative form of the relationship, and to quantify beliefs (predictions) about the 
observable x* corresponding to a specified input or control quantity z*. 
(iv) Exchangeable sequences, xu,Xi2,..., of random quantities are to be observed 
in each of i G I contexts, where / is itself a selection from a potentially larger 
index set I*. Suppose that for each sequence, 
+(0 - 
£s(i)^) 
3=1 
i G /, 
4.6 Models via Partial Exchangeability 
211 
is judged to be a sufficient statistic, that the strong law limits 
t-»oo m 
exist and that the sequence d\, 62, ■ ■ ■ is itself judged exchangeable. For  
example: sequence i may consist of 0 - 1 (success-failure) outcomes on repeated 
trials with the zth of / similar electronic components; or sequence i may  
consist of quality measurements of known precision on replicate samples of the 
zth of I chemically similar dyestuffs. In the first case, the sequence of long-run 
frequencies of failures for each of the components might, a priori, be j udged to 
be exchangeable; in the second case, the sequence of large-sample averages of 
quality for each of the dyestuffs might, a priori, be judged to be exchangeable. 
A modelling framework is required which enables us to exploit such further 
judgements of exchangeability in order to be able to use information from all 
the sequences to strengthen, in some sense, the learning process within an 
individual sequence. 
4.6.2 Several Samples 
We shall begin our discussion of possible forms of partial exchangeability  
judgements for several sequences of observables, xn,Xi2,..-,i = 1,..., m, by  
considering the simple case of 0 - 1 random quantities. 
In many situations, including that of a comparative clinical trial, joint beliefs 
about several sequences of 0 - 1 observables would typically have the property 
encapsulated in the following definition, where, here and throughout this section, 
Xi(n,i) denotes the vector of random quantities (xu,..., xin.). 
Definition 4.13. {Unrestricted exchangeability for 0-1 sequences). 
Sequences ofO—1 random quantities, Xn, x&,..., i = 1,..., m, are said 
to be unrestrictedly exchangeable if each sequence is infinitely exchangeable 
and, in addition, for alln, < Ni,i — 1,..., m, 
m 
p{xi{ni),..., xm(nm) I yi(Ni),..., ym(Nm)) = Y[p(xj(ni) \ yi(Ni)), 
i=l 
where yi(N,) = xtl-\ + xiNi,i = 1,..., m. 
In addition to the exchangeability of the individual sequences, this definition 
encapsulates the judgement that, given the total number of successes in the first 
Nt observations from the zth sequence, i = 1,... m, only the total for the ith 
sequence is relevant when it comes to beliefs about the outcomes of any subset 
of n, of the Nj observations from that sequence. Thus, for example, given 15 
212 
4 Modelling 
deaths in the first 100 patients receiving Drug 1 (Nt = 100, y\(N\) = 15) and 
20 deaths in the first 80 patients receiving Drug 2 (N2 = 80, y2(N2) = 20), we 
would typically judge the latter information to be irrelevant to any assessment of the 
probability that the first three patients receiving Drug 1 survived and the fourth one 
died (#n = 0, #i2 = 0, X13 = 0,xu = 1). Of course, the information might well 
be judged relevant if we were not informed of the value ofyi(Ni). The definition 
thus encapsulates a kind of "conditional irrelevance" judgement. 
As an example of a situation where this condition does not apply, suppose that 
xn,xl2,... is an infinitely exchangeable 0-1 sequence and that another sequence 
£21, x22, ■ ■ ■ is defined by x2j = x\j (or by x2j = 1 — xlj). Then x2i, #22,... is 
certainly an exchangeable sequence (since xn,xi2,... is), but, taking x2j = X\j 
and ni = n2 = Nt = N2 = 2, 
p(xu = 0, xl2 = 1, x2i = 1, £22 = 012/12 = 1,2/22 = 1) = 0, 
whereas 
p{xn = 0, xl2 = 112/12 = 1) p{x2i = 1, x22 = 01 y22 = 1) = 1/2 x 1/2 = 1/4. 
Further insight is obtained by noting (from Definition 4.13) that unrestricted  
exchangeability implies that 
P\X\\, • • ■ )2-lnj j • • • j Xmi, . . . , Xmnm) 
= P(a;l7r1(l), • • • i^lirjfnj)) • • • j^rmrmfl), • ■ • > a;rrnrm(nm)) 
foranywnrertrictedchoiceofpermutations7TjOf{l, ...,rii},i — 1,... m, whereas, 
in the case of the above counter-example, we only have invariance of the joint 
distribution when K\ = n2. For a development starting from this latter condition 
see deFinetti (1938). 
We can now establish the following generalisation of Proposition 4.1. 
Proposition 4.18. (Representation theorem for several sequences of 0-1 
random quantities). Ifxu,xi2, ...,i = l, mare unrestrictedly infinitely 
exchangeable sequences of 0-1 random quantities with joint probability  
measure P, there exists a distribution function Q such that 
- m ni 
p(x1(n1),...,xm(nm))= / TT n^l-*.-)1"*"'^) 
i[o,irf=t ,=1 
where, with y,(n,) = xn -\ + Xi„t, i = 1,..., m, 
q(9)= Hm p\(yiM<ei)n...n(y-^A<em] . 
4.6 Models via Partial Exchangeability 
213 
Corollary. Under the conditions of Proposition 4.18, 
p{yi(ni),...,ym{nm)) 
= f fl( 7 .W^a-ei^MdQWu...^) 
Proof. We first note that 
P(K.(«I), • ■ ■ , Vm{nm)) = (J^J . . • (jJ;m))p(*i(»i). • ■ • . *»(»»)) 
so that, to prove the proposition, it suffices to establish the corollary. Moreover, 
for any N{ > nu i = 1,..., m, we may express p(yi (m),..., j/m(nm)) as 
^P(j/i0*i), • • •, ym{nm) | yi(iVi),.. •, ym(Nm)) p(yi(Ni),..., ym(Nm)), 
where the zth of the m summations ranges from yt(Ni) = yi(rii) to yt(Ni) — Ni, 
and where, by Definition 4.4 and a straightforward generalisation of the argument 
given in Proposition 4.1, 
p(j/i(«i), • • •, ym(nm) I yi(iVi),..., ym(Nm)) 
-n^)i.<«))=nU,)(,w-"»(X,)/©- 
Writing (yjv)m = yjv(yjv - 1) ■ • • (Vn - On - 1)). etc., and defining the  
function QjVj,...,jvm (#i, • • •, 0m) on 3tm to be the m-dimensional "step" function with 
"jumps" of p(yi(Ni),.. .,ym(Nm)) at 
m a x fvi(Ni) ym(Nm) 
(,#!> • . . ,Vm) — 
JVi JV, 
m 
where ^(./V,) = 0,..., Nt, i = 1,..., m. We see that p(yi(ni),..., ym(nm)) is 
equal to 
IM 
ni \ (eiNi)y.{n.)[(i - e^Nj},,.^.^ 
AsNu...,Nm->oo 
m 
TT [ ^'^^("ilK1 ~ ^)-/V']"i-J/i("i) 1 _^ TT 0Wi("i) n _ Q.\m-Vi(ni) 
i=i l (iVi)ni J i=i 
uniformly in #!,..., #m, and, by the multidimensional version of Helly's theorem 
(see Section 3.2.3), there exists a subsequence QN1(j),...,Nm(j),3 = 1,2,... having 
a limit Q, which is a distribution function on 3tm. The result follows. < 
214 
4 Modelling 
Considering, for simplicity, m = 2, Proposition 4.18 (or its corollary) asserts 
that if we judge two sequences of 0 - 1 random quantities to be unrestrictedly 
exchangeable, we can proceed as if: 
(i) the xtj are judged to be independent Bernoulli random quantities (or the y, (n,) 
to be independent binomial random quantities) conditional on random  
quantities^ = 1,2; 
(ii) (#i, #2) are assigned a joint probability distribution Q; 
(iii) by the strong law of large numbers, #8 = limnj_i00(yi(ni)/m), so that Q may 
be interpreted as "joint beliefs about the limiting relative frequencies of l's in 
the two sequences". 
The model is completed by the specification of dQ(#i, #2), whose detailed 
form will, of course, depend on the particular beliefs appropriate to the actual 
practical application of the model. At a qualitative level, we note the following 
possibilities: 
(a) knowledge of the limiting relative frequency for one of the sequences would 
not change beliefs about outcomes in the other sequence, so that we have the 
independent form of prior specification, dQ{6\, #2) = dQ{d\)dQ{d2); 
(b) the limiting relative frequency for the second sequence will necessarily be 
greater than that for the first sequence (due, for example, to a known  
improvement in a drug or an electronic component under test), so that dQ(#i, #2) is 
zero outside the range 0 < d\ < 92 < 1; 
(c) there is a real possibility, to which an individual assigns probability ir, say, 
that, in fact, the limiting frequencies could turn out to be equal, so that, writing 
8 = B\ = 82, in this case dQ(di,d2) has the form 
TrdQ*(6) + (l-Tr)dQ+(6u62) 
and the representation, for (yi„i, y2n2), say, has the form 
P(l/i(ni),lte("2)) = 7r / Bi(y1(n1)|n1,e)Bi(y2(n2)|n2,e)dQ*(e) 
Jo 
+ (1 - tt) /" f Bi(y1(n1)|n1,e1)Bi(y2(n2)|n2,e2)dg+(e1,e2), 
Jo Jo 
where dQ+ (d\, 62) assigns probability over the range of values of (#1,62) such 
that 9i^62. 
As we shall see later, in Chapter 5, the general form of representation of 
beliefs for observables defined in terms of the two sequences, together with detailed 
specifications of dQ{6\ ,92), enables us to explore coherently any desired aspect of 
the learning process. For example, we may have observed that out of the first ni, n2 
4.6 Models via Partial Exchangeability 
215 
patients receiving drug treatments 1, 2, respectively, y\(n\) and y2(n2) survived, 
and, on the basis of this information, wish to make judgements about the relative 
performance of the drugs were they to be used on a large future sequence of patients. 
This might be done by calculating, for example, 
p( lim (yi(N)/N) - lim (y2(N)/N) | y1(nl),y2(n2)), 
JV->oo N->oo 
which, in the language of the conventional paradigm, is the "posterior density for 
#i - 6»2, given yi(ni), y2(n2)". 
Clearly, the discussion and resulting forms of representation which we have 
given for the case of unrestrictedly exchangeable sequences of 0-1 random  
quantities can be extended to more general cases. One possible generalisation of  
Definition 4.13 is the following. 
Definition 4.14. (Unrestricted exchangeability for sequences with predictive 
sufficient statistics). Sequences of random quantities Xn,xi2,... taking  
values in Xi, i = 1,..., m, are said to be unrestrictedly infinitely exchangeable 
if each sequence is infinitely exchangeable and, in addition, for all rii < Ni, 
i = 1 ,m, 
m 
p(xi(ni),..., Xm{nm) \tNv..., tNm) = Wp{Xi{ni) \ tNj) 
i=\ 
where t^v = tN-(Xi{Ni)), i = 1,..., m, are separately predictive sufficient 
statistics for the individual sequences. 
In general, given m unrestrictedly exchangeable sequences of random  
quantities, Xji, x8'2,.. •, with Xij taking values in Xi, we typically arrive at a representation 
of the form 
. 171 "j 
p(xl(ni),...,Xm(rim))= / II nPi(XiJ I6*') dQ(6u...,6m), 
Je* j=i j=i 
where ©* = n^i ©i and the parametric families 
Pi(x I 0t), x e Xi, Oi G ©,;, i = 1,..., m, 
have been identified through consideration of sufficient statistics of fixed  
dimension, or whatever, as discussed in previous sections. Most often, the fact that the 
k sequences are being considered together will mean that the random quantities 
%ii, X(2, ■ ■ ■ relate to the same form of measurement or counting procedure for all 
i = 1,...,m, so that typically we will have Pi(x \ &,) = p(x | #j), i = 1 m, 
where the parameters correspond to strong law limits of functions of the sufficient 
statistics. The following forms are frequently assumed in applications. 
216 
4 Modelling 
Example 4.9. (Binomial). If yi(rn) denotes the number of 1 's in the first n; outcomes 
of the ith of m unrestrictedly exchangeable sequences of 0 - 1 random quantities, then 
» m 
p{yi(ni),...,ym(nm))= / TTBi(2/;(n;) |0;,n,) dQ{6u-.. ,9m). 
J\o,ir it 
Example 4.10. (Multinomial). Ify^n,) denotes the category membership count (into 
the first fc of k+1 exclusive categories) from the first n,; outcomes of the ith of m unrestrictedly 
exchangeable sequences of "0 - 1 random vectors" (see Section 4.3), then 
» m 
P(yi(n\),.--,ym(nm))= / TTMu*(yi(rij)|0i,ni)dQ(0i,...,0m), 
ie- it 
where 0, = lim^^ {y,(n)/n) and 9 = {0 = (0i,.. .,0*) such that 0 < 6{ < 1,1 < i < fc, 
and 0i H h 0/t < 1}. This model describes beliefs about an m x (fc +1) contingency table 
of count data, with row totals n\,..., nm. It generalises the case of the m x 2 contingency 
table described in Example 4.9. < 
Example 4.11. (Normal). If x^, j = 1,..., rn, i = 1,..., m, denote real-valued  
observations from m unrestrictedly exchangeable sequences of real-valued random quantities, 
the assumed sufficiency of the sample sum and sum of squares within each sequence might 
lead to the representation 
p(xi(nl),...,xm(nm))= / TT Y]N(xij\m,Xi)dQ(0), 
where, with xn(i) =n~l(Xi\-\ \-xin) andsj;(i) = n"1 ^J=1(x,J,-xn(i))2, wehave^i = 
limn^Xnti), A,"1 =limn^00s2n(i),6 = (^1;... ,^m, A1;..., Am) and 9 = 3?m x (3?+)m. 
In many applications, the further judgement is made that Ai = • • • = Am = A, say, so 
that the representation then takes the form 
i. m ni 
p(xl(n1),...,xm(nm))= / TT T\N(xij\iJ,i,\)dQ(iiU...,nm,\). 
This is the model most often used to describe beliefs about a one-way layout of measurement 
data. <] 
As in the case of 0 - 1 random quantities with m = 2, discussed earlier in 
this section, we could make analogous remarks concerning the various qualitative 
forms of specification of the prior distribution Q that might be made in these cases. 
We shall not pursue this further here, but will comment further in Section 4.7.5. 
4.6 Models via Partial Exchangeability 
217 
4.6.3 Structured Layouts 
Let us now consider the situation described in (ii) of Section 4.6.1, where the random 
quantity Xijk is triple-subscripted to indicate that it is the fcth of K "replicates" of 
an observable in "context" i E I, subject to "treatment" j G J. In general terms, 
we have a two-way layout, having / rows and J columns, with K replicates in each 
of the IJ cells. 
In such contexts, most individuals would find it unacceptable to make a  
judgement of complete exchangeability for the random quantities x^k. For example, if 
rows represent age-groups, columns correspond to different drug treatments,  
replicates refer to sequences of patients within each age-group/treatment combination 
and the xi3k measure death-rates, say, it is typically not the case that beliefs about 
the Xijk would be invariant under permutations of the subscript i. On the other 
hand, for the kinds of mechanisms routinely used to allocate patients to treatment 
groups in clinical trials, many individuals would have exchangeable beliefs about 
the sequence x,ji, 2^2, -for any fixed i, j. 
Technically, such a situation corresponds to the invariance of joint beliefs for 
the collection of random quantities, xtjk, under some restricted set of permutations 
of the subscripts, rather than under the unrestricted set of all possible  
permutations (which would correspond to complete exchangeability). The precise nature 
of the appropriate set of invariances encapsulating beliefs in a particular  
application will, of course, depend on the actual perceived partial exchangeabilities in 
that application. In what follows, we shall simply motivate, using very minimal 
exchangeability assumptions, a model which is widely used in the context of the 
two-way layout. There is no suggestion that the particular form discussed has any 
special status, or ought to be routinely adopted, or whatever. 
Suppose that, for any fixed i, j, we think of Xyi, Xij2,... as a (potentially)  
infinite sequence of real-valued random quantities (x G 3t), such that the IJ sequences 
of this kind, with / and J fixed, are judged to be unrestrictedly exchangeable. If 
further assumptions of centred spherical symmetry or sufficiency for each sequence 
then lead to the normal form of representation, we have 
, / J nij 
P{xn(nn),---,xIj(nIJ))= JJ J| JjN^y* | My, A0) dQ(6), 
JeIJ i=\ j=\ k=\ 
wherefl = (/in,. ■., m/j, An,..., A/j) and© = 3t/Jx(3t+)/J, so that conditional, 
for each (i, j), on the strong law limits 
Hij = lim K-\xin + ■■■ + XijK) = lim K~lxi3(K), 
K—>oo K—>oo 
K 
(Ay)"1 = lim K-1 ^(xm ~ Xij{K)f = lim s^K), 
K—>oo *—' K—»oo J 
k=\ 
218 
4 Modelling 
the x^k are assumed independently and normally distributed with means /iy and 
variances (Ay)-1. 
In many cases, the nature of the observational process leads to the judgement 
that lim/f _,oo sfj(K) may be assumed to be the same for all (i, j), so that Ay = A, 
say, for all i, j. Letting 
j j 
Hi. = lim K~lJ'1 Y, Xij(K) = J~lJ~]nij 
»=1 j=l 
~^°° i=l i=l 
I J I J 
H„ = lim K-lrlJ-1 Y Y Xij(K) = r1 Y M,;. = J'1 Y »•* 
i=l j'=l i=l j'=l 
denote the strong law limits of the row averages, column averages and overall 
average, respectively, from the two-way layout with / and J fixed, we can always 
write 
Hij = M + a>; + Pj + lij, 
where 
an = {in. - fi), Pj = (M.j - M). Hi = (My - Mi. - M.j), 
so that the random quantities Xjjk are conditionally independently distributed with 
p(xijk | (i, Qj, /3j, 7y, A) = N(Xyi | \i + a, + /3j + 7y, A). 
The full model representation is then completed by the specification of a prior 
distribution Q for A and any IJ linearly independent combinations of the /zy. 
In conventional terminology, \i is referred to as the overall mean, aj as the zth 
row effect, (3j as the jth column effect and 7y as the (ij)th interaction effect. 
Collectively, the {aj and {/?,} are referred to as the main effects and {-y^} as the 
interactions. Interest in applications often centres on whether or not interactions or 
main effects are close to zero and, if not, on making inferences about the magnitudes 
of differences between different row or column effects. 
In the above discussion, our exchangeability assumptions were restricted to 
the sequence Xiji,x<j2, ■ ■ ■ for fixed i,j. It is possible, of course, that further 
forms of symmetric beliefs might be judged reasonable for certain permutations of 
the i,j subscripts. We shall return to this possibility in Section 4.6.5, where we 
shall see that certain further assumptions of invariance lead naturally to the idea of 
hierarchical representations of beliefs. 
4.6 Models via Partial Exchangeability 
219 
4.6.4 Covariates 
In (iii) of Section 4.6, we gave examples of situations where beliefs about sequences 
of observables Xn,Xi2,...,i = 1,..., m are functionally dependent, in some 
sense, on the observed values, z,, i = 1,..., m, of a related sequence of (random) 
quantities. We shall refer to the latter as covariates and, in recognition of this 
dependency, we shall denote the joint density of Xij,j = 1,..., n*, i = 1,..., m, 
by 
p(cci(ni),.. .,xm(nm) | Zi,..., zm). 
The examples which follow illustrate some of the typical forms assumed in  
applications. Again, there is no suggestion that these particular forms have any special 
status; they simply illustrate some of the kinds of models which are commonly 
used. 
Example 4.12. (Bioassay). Suppose that at each of m specified dose levels, z\, ..., 
Zm, of a toxic substance, typically measured on a logarithmic scale, sequences of 0 - 1 
random quantities, xn, x&,..., i = 1,...,m, are to be observed, where xi} = 1 if the 
jth animal receiving dose zt survives, x^ = 0 otherwise. If. for each i = 1 , m, the 
sequences xn, Xa, ■ ■ ■ are judged exchangeable, and if we denote the number of survivors 
out of 7i; animals observed in the ith sequence by yi (rn) = x,-H \-Xini, a straightforward 
generalisation of the corollary to Proposition 4.18 implies a representation of the form 
» m 
p(yi( T^l) i * * • i Urn (n-)|z)= / \\myi{nl)\ei{z),ni) dQ{0{z)), 
./[o.ip f=f v ' 
where z = (zu...,zm), 0(z) = (0i(z),..., 6m(z)) and 0, (z) = lim rr'y^n). 
n—*oo 
In many situations, investigators often find it reasonable to assume that 
9l(z) = 9(zi) = G(^;z,), 
where the functional form G (usually monotone increasing from 0 to 1) is specified, but <j> is 
a random quantity. Functions having the form G(<j>; z{) = G(<f>\ + fozi), with <j>\ G 3?, <j>2 G 
3?+, are widely used (see, for example, Hewlett and Plackett, 1979), with 
N(/i | 0, l)dn (the prvbit model) 
■oc 
and 
G(0i + 022.) = exp(0! + 4>2Zi)/{\ + exp(0i + <fez,-)} (the logitmodel) 
being the most common. For any specified G(.; z,), the required representation has the form 
- m 
)\z)= J[Bi(yi(ni)\G(4,;zi),nl)d(r(4,), 
220 
4 Modelling 
with dQ*(<t>) specifying a prior distribution for </> e $. In practice, the specification of 
<2 might be facilitated by reparametrising from </> to a more suitable (1-1) transformation 
* = *(^). In the probit and logit cases, for example, ipx = -</>i/</>2 corresponds to the 
(log) dose, Zi, at which G(</>i + <fozi) = 1/2. Beliefs about %>x then correspond to beliefs 
about the (log-) dose level for which the survival frequency in a large series of animals would 
equal 1/2, the so-called LD50 dose. Experimenters might typically be more accustomed to 
thinking in terms of (-<j>\l<fa, </>2)> say, than in terms of (<j>i,<fa). <j 
Example 4.13. (Growth-curves ). Suppose that at each of m specified time points, 
say z\,. ■ .,zm, sequences of real-valued random quantities, xn,xi2,... ,i = 1,... , m, are 
to be observed, where xi} is the jth replicate measurement (perhaps on a logarithmic scale) 
of the size or weight of the subject or object of interest at time z,. Suppose further that the 
kinds of judgements outlined in Example 4.11 are made about the sequences xn,xa, 
with i = 1,..., m, so that we have the representation 
p(xl(n1),...,xm(nm)) = n Y[N(xtJ\nz(z),Xi(z))dQ(0(z)), 
where 0(z) = (Mi(z),..., ^(z), A, (z),..., Am(z)) and 6, = »m x (»+)m. 
In many such situations, the judgement is made that Ai(z) = • • • = Am(z) = A 
(particularly if measurements are made on a logarithmic scale) and that 
Hi(z) = Hi(zi) =g(<t>;zi), 
where the functional form g (usually monotone increasing) is specified, but 4> is a random 
quantity. Commonly assumed forms include 
g(4>; Zi) = (4>i + (fc^-P)1! (tne logistic model) 
and 
g(4>; Zi) = 4>\ + (p2Zi (the straight-line model). 
For any specified g{.;zi), the joint predictive density representation has the form 
p{xl{n1),...,xm(nm))= Y[ Y[fi[xi:j\g((l),zt),\)dQ((l),\), 
where dQ(<j>, A) specifying a prior distribution for 4> e $ and A G R+. 
As with Example 4.12, specification of Q might be facilitated if we reparametrise from 
4> to a more suitable (1-1) transformation, i/> = ip(4>). In the logistic case, for example, we 
might take ip-i = fa1, corresponding to the "saturation" growth level reached as zt ~* oo, 
and^2 = {4>\ -l-^)-1, corresponding to the growth level at the "time origin", z,: = 0. Beliefs 
about ip-i, ip2 then acquire an operational meaning as beliefs about the average growth-levels, 
at times "oo" and "0", respectively, that would be observed from a large number of replicate 
measurements. A third possible parameter to which investigators could easily relate in some 
applications might be ip3 = log[0i<j!)2/(20i + <fe)]/k>g((!!>3), the time at which growth is 
half-way from the initial to the final level. <, 
4.6 Models via Partial Exchangeability 
221 
Example 4.14. (Multiple regression). Suppose that, for each i = 1, ...,m,  
sequences of real-valued random quantities xn,xa,... are to be observed, where each x{j 
is related to certain specified observed quantities z, = (z,\,..., z^) and judgements are 
made which lead to the belief representation 
f m "J 
p{x1{n1),...,xm(nm)) = / Y[ \[N{xij\iJLi{zi),\i{zi))dQ{e{z)), 
Jez i=i j=i 
where 
Hi(zi) = lim xn(i), \7l(zi) = lim s2n(i), 
n—>oo n—*oo 
0(z) = (/xi(zi),...,/xm(zm), A1(z1),...,Am(zm) 
and6 = »mx (»+)m,withz = (z,,...,zm). 
In many situations, the furtherjudgements are made that A; (zO = A; = Aand/x;(z8) = 
li(zi),i = 1 m, where A and /x(.) are unknown, but the latter is assumed to be a 
"smooth" function, adequately approximated by a first-order Taylor expansion, so that, for 
some (unspecified) z*, 
li(zi) = n(z*) + (z{ - z*) v M(2*) = a-iO, 
where we define 
a, = (1, Zii,..., zik) (row vector) 
and 
6 = (60,0i,..., 0ky (column vector) 
with 
6»0 = n(z*) - z* V»(z*),Oi = [V»(z*)]i,i = 1 k. 
Conditional on 4> = {0, A), the joint distribution of 
x = (•ci(wi)j ■ ■ • i xm\nmj) 
is thus seen to be multivariate normal, Nn(x\ A6, A), where A is an n x fc matrix (n = 
ri] h (- nm), whose rows consist of a} replicated nx times, followed by a2 replicated n2 
times, and so on, and A = \I„, with J„ denoting the n x n identity matrix. The unconditional 
representation can therefore be written as 
p(x)= f Nn{x\AO,\)dQ{0,\). 
It is conventional to refer to z\},zi},... as values of the regres sor variables z(i\ j = 1,..., fc, 
to 0 as the vector of regression coefficients and A as the design matrix. The form /x (z) = AO 
is called a regression equation and the structure 
E(a; | A, 0, A) = AO 
222 
4 Modelling 
is said to define a linear model. If k = 1, we have the simple regression (straight-line) 
model, E(Xij) = #o + Oizi}; for k > 2, we have ^multiple regression model. 
From an operational point of view, beliefs about 6 in the general case relate to  
beliefs about the intercept (60) of the regression equation and the marginal rates of change 
(Q],...,0k) oftheiy with respect to the regressor variables (z^,... ,zk). However, within 
this general structure we can represent various special cases such as z^ = zj (polynomial 
regression) or z^' = sin(jH/N), for some N (a version of trigonometric regression); in 
these cases, beliefs about 6 will stem from rather different considerations. < 
Specification of the kinds of structures which we have illustrated in Examples 
4.12 to 4.14 essentially reduces to the same process as we have seen in earlier 
representations of joint predictive densities as integral mixtures. We proceed as if: 
(i) the random quantities are conditionally independent, given the values of the 
relevant covariates, z, and given the unknown parameters, d>; 
(ii) the latter are assigned a prior distribution, dQ{<f>). 
In many cases, the likelihood, defined through conditional independence,  
involves familiar probability models, often of exponential family form (as with the 
binomial, normal and multivariate examples seen above), but with at least some 
of the usual "labelling" parameters replaced by more complex functional forms 
involving the covariates. From a conceptual point of view, this is all that really 
needs to be said for the time being. However, when we consider the applications 
of such models, together with the problems of computation, approximation, etc., 
which arise in implementing the Bayesian learning process, it is often useful to 
have a more structured taxonomy in mind: for example, linear versus non-linear 
functional forms; normal versus non-normal distributions, and so on. 
4.6.5 Hierarchical Models 
In Section 4.6.2, we considered the general situation where several sequences of 
random quantities, xn,Xi2,...,i = 1,..., m are judged unrestrictedly infinitely 
exchangeable, leading typically to a joint density representation of the form 
p(xi(ni),...,xm(nm))= / TT T\p(xij\6i)dQ(6i,...,6m). 
We remarked at that time that nothing can be said, in general, about the prior  
specification Q{9\,... ,6m), since this must reflect whatever beliefs are appropriate for 
the specific application being modelled. However, it is often the case that  
additional judgements about relationships among the m sequences lead to interestingly 
structured forms of Q(9\,..., 9m). 
4.6 Models via Partial Exchangeability 
223 
In Section 4.6.1, we noted some of the possible contexts in which judgements 
of exchangeability might be appropriate not only for the random quantities within 
each of m separate sequence of observables, but also between the m strong law 
limits of appropriately defined statistics for each of the sequences. The following 
examples illustrate this kind of structured judgement and the forms of hierarchical 
model which result. 
Example 4.15. (Exchangeable binomial parameters). Suppose that we have  
unrestrictedly infinitely exchangeable sequences of 0-1 random quantities, ii\,xa,..., with 
i = l,...,m. Then, for i = 1,2,..., [ni; yi(ri{) = xix -\ 1- xinj\, is a sufficient statistic 
for the ith sequence and 
p{yi{nA),..., ym{nm)) = I p(V\(ni),..., 2/m(nm) | 9U ..., 9m)dQ(Qu. ..,6n) 
m 
\[B\{yi{ni)\6l,ni)dQ(6u...,ein), 
J 
-Mo,: 
/|0,1] 
where 
6i = lim(y,(ni)/n). 
n—»oo 
As we remarked in Section 4.6.1, if the sequences consists of success-failure outcomes 
on repeated trials with m different (but, to all intents and purposes, "similar") types of 
component, it might be reasonable to judge the m "long-run success frequencies" to be 
themselves exchangeable. This corresponds to specifying an exchangeable form of prior 
distribution for the parameters 6\,..., 6m. If the m types of component can be thought of 
as a selection from a potentially infinite sequence of similar components, we then have (see 
Section 4.3.3) the general representation 
Q(0it-.-,em)= I Q(eu...,em\G)<m{G) 
, m 
Y[G(6i) <m(G). 
The complete model structure is then seen to have the hierarchical form 
m 
p{Vi(ni),.--,y )|6»,,...,6»m) = Y[B\(yi(ni)\6i,ni) 
Q(eu...,6m\G) = f[G(6,) 
t=i 
n(G) 
In conventional terminology, the first stage of the hierarchy relates data to parameters via 
binomial distributions; the second stage models the binomial parameters as i/they were a 
random sample from a distribution G; the third, and final, stage specifies beliefs about G. < 
L 
224 
4 Modelling 
The above example is readily generalised to the case of exchangeable  
parameters for any one-parameter exponential family. In practice, beliefs about G might 
concentrate on a particular parametric family, so that, assuming the existence of 
the appropriate densities, the prior specification takes the form 
g(0 
- - m 
,--.,6m) = J g(6l,...,6m\<t>)<m(<t>) = J \\g{di\(S>) dR{(S>) 
and, for appropriate sufficient statistics j/i(n8), i = 1,..., m, defines the  
hierarchical structure 
m 
P(yi(ni),..., ym{nm) | 9U ..., dm) = \Yp{yi{ni) | 0*) 
g(0u---,Om\<fi)=l[g(0i\<fi) 
!=1 
11(0). 
As before, the first stage of the hierarchy relates data to parameters in a form  
assumed to be independent of G; the second stage now models the parameters as if 
they were a random sample from a parametric family labelled by the hyperparam- 
eter <f> € $; the third, and final, stage specifies beliefs about the hyperparameter. 
Such beliefs acquire operational significance by identifying the hyperparameter 
with appropriate strong law limits of observables, as we shall indicate in the  
following example. 
Example 4.16. (Exchangeable normal mean parameters). Suppose that we have m 
unrestrictedly infinitely exchangeable sequences xn, xt2, ...,i = 1,..., m, of real valued 
random quantities, for which (see Example 4.11) the joint density has the representation 
r. rn "i 
p(x}{n}),...,xm(nm))= / TT TTN(iy |/i,-,A) dQ(/x1(...,/im,A), 
where we recall that A-1 = lim„^oo s^(i) and /x, = lim,,^^ xn(i), where 
n 
nxn(i) = (xn H h xin), ns2n{i) = Y^(xij ~ xn{i))2, i = 1,..., m. 
So far as the specification of Q(ni,..., Mm, A) is concerned, we first note that in many 
applications it is helpful to think in terms of 
<3(/il, • . . , Mm, A) = <3^(/il, • • • ,Mm I A)<?A(A), 
4.6 Models via Partial Exchangeability 
225 
for some QM, Q\. In some cases, knowledge of the strong law limits of sums of squares 
about the mean may be judged irrelevant to the assessment of beliefs for strong law limits 
of the sample averages: in such cases, Q^(/ii,..., /xm | A) will not depend on A. In other 
cases, we might believe, for example, that variation among the limiting sample averages is 
certainly bigger (or certainly smaller) than within-sequence variation of observations about 
the sample mean: in such cases, QM (/ii,..., /xm | A) will involve A. In either case, it is useful 
to think in terms of the product form of Q. 
Now suppose that, conditional on A, the limiting sample means are judged  
exchangeable. If the m sequences can be thought of as a selection from a potentially infinite collection 
of similar sequences, we have (see Section 4.3) a further representation of QM in the form 
Q^(Mi,---.Mm|A) = / Q/1(/x1,...,/im|A,G)dn(G|A) 
r, m 
= Y[G(jM\\)dn(G\\). 
J* t=i 
The complete model then has the hierarchical structure 
m 
p(xi(ni),...,xm(n 
m) I Ml > • • • i Mmj 
m 
<2„(Mi,---,MmiA,G) = JjG(/ii|A) 
t=i 
n(G|A)QA(A). 
In practice, beliefs about G, given A, might concentrate on a particular parametric family, 
so that, assuming the existence of the appropriate densities, the hierarchical structure would 
take the form 
m 
pix^rii),..., xm(nm) \/j,u...,iJ,m,\) = Y\_Pix<(n') Im.'j^) 
m 
5/. (Mi, • • •, Mm I A, <j>) = Y[ g^ \ A, 4>) 
•=i 
ILr(0|A)QA(A). 
For an explicit example of this, suppose that, given a potentially infinite sequence jjlx , /x2, ■ ■ • 
(or, more concretely, x„\ (1), xn2(2),..., for very large n}, n2,...) the quantities m, Jx(m) = 
m'1 (//] h h /xm) and s2(m) = m"1 Y1J=\ (Mj — M(m))2 (or the large sample analogues 
of p.{m) and s2(m)) were judged sufficient for the sequence. It would then be natural (see 
Section 4.5) to take g,,(/i; I A, <j>) = N(Hi \ <t>\,4>2), where 
4>x = lim p.{m), 4>2 = lim s2{m). 
m—.oo m—*oo 
From an operational standpoint, the final stage specification of the joint prior distribution for 
0!, 4>2 and A then reduces to a specification of beliefs about the following limits of observable 
quantities (for large m and rii, i = 1 m): 
(i) the mean of all the observations from all the sequences (^i); 
226 
4 Modelling 
(ii) the mean sum of squares of the individual sequence means about the overall mean (</>2); 
(iii) the mean (over sequences) of the mean sum of squares of observations within a sequence 
about the sequence mean (A). 
The precise form of specification at this stage will, of course, depend on the particular 
situation in which the model is being applied. <, 
Hierarchical modelling provides a powerful and flexible approach to the  
representation of beliefs about observables in extended data structures, and is being 
increasingly used in statistical modelling and analysis. This section has merely 
provided a brief introduction to the basic ideas and the way such structures arise 
naturally within a subjectivist, modelling framework. In the context of the  
Bayesian learning process, further brief discussion will be given in Section 5.6.4, where 
links will be made with empirical Bayes ideas. 
An extensive discussion of hierarchical modelling will be given in the volumes 
Bayesian Computation and Bayesian Methods. A selection of references to the 
literature on inference for hierarchical models will be given in Section 5.6.4. 
4.7 PRAGMATIC ASPECTS 
4.7.1 Finite and Infinite Exchangeability 
The de Finetti representation theorem for 0-1 random quantities, and the  
various extensions we have been considering in this chapter, characterise forms of 
p{x\,..., xn) for observables xi,...,xn assumed to be part of an infinite  
exchangeable sequence. However, in general, mathematical representations which 
correspond to probabilistic mixing over conditionally independent parametric forms 
do not hold for finite exchangeable sequences. 
To see this, consider n = 2 and finitely exchangeable 0-1 xi,x2, such that 
p(xi =0,x2 = 0) =p(xi = l,x2 = 1) = 0 
p(xi = 1, x2 = 0) = p(xr = 0, x2 = 1) = \ ■ 
If the de Finetti representation held, we would have 
I 02dQ(0) = f (1 - 6)2dQ(6) = 0, 
Jo Jo 
for some Q(9), an impossibility since the latter would have to assign probability 
one to both 6 = 0 and 9=1 (Diaconis and Freedman, 1980a). 
4.7 Pragmatic Aspects 
227 
It appears, therefore, that there is a potential conflict between realistic  
modelling (acknowledging the necessarily finite nature of actual exchangeability  
judgements) and the use of conventional mathematical representations (derived on the 
basis of assumed infinite exchangeability). 
To discuss this problem, let us call an exchangeable sequence, xr,..., xn, with 
Xi € X, N-extendible if it is part of the longer exchangeable sequence xx,..., xn- 
Practical judgements of exchangeability for specific observables x\,..,,xn are 
typically of this kind: the xit..., xn can be considered as part of a larger, but 
finite, potential sequence of exchangeable observables. Infinite exchangeability 
corresponds to the possibly unrealistic assumption of N-extendibility for all N > n. 
In general, the assumption of infinite exchangeability implies that the  
probability assigned to an event (x\,..., xn) EEC Xn is of the form 
PQ(E) = J Fn(E)dQ(F), 
for some Q. If we denote by P(E) the corresponding probability assigned under 
./V-extendibility for a specific TV, a possible measure of the "distortion" introduced 
by assuming infinite exchangeability is given by 
sap\P(E)-PQ(E)\, 
E 
where the supremum is taken over all events in the appropriate u-field on Xn. 
Intuitively, one might feel that \fxi,...,xn'\s JV-extendible for some N > > n, the 
"distortion" should be somewhat negligible. This is made precise by the following. 
Proposition 4.19. (Finite approximation of infinite exchangeability). 
With the preceding notation, there exists Q such that 
sup\P(E)-PQ(E)\<^T> 
where f(n) is the number of elements in X, if the latter is finite, and f(n) = 
(n — 1) otherwise. 
Proof See Diaconis and Freedman (1980a) for a rigorous statement and  
technical details. < 
The message is clear and somewhat comforting. If a realistic judgement of 
./V-extendibility for large, but finite, N is replaced by the mathematically  
convenient assumption of infinite exchangeability, no important distortion will occur in 
quantifying uncertainties. 
For further discussion, see Diaconis (1977), Jaynes (1986) and Hill (1992). 
For extensions of Proposition 4.19 to multivariate and linear model structures, see 
Diaconis et al. (1992). 
228 
4 Modelling 
4.7.2 Parametric and Nonparametric Models 
In Section 4.3, we saw that the assumption of exchangeability for a sequence 
x\, x2, ■ ■ ■ of real-valued random quantities implied a general representation of the 
joint distribution function of x\,..., xn of the form 
r n 
P(xu...,xn)= / T\F(xi)dQ(F), 
where 
Q(F) = lim P{Fn) 
n—>oo 
and Fn is the empirical distribution function defined by x\,..., xn. This implies 
that we should proceed as ifv/e have a random sample from an unknown distribution 
function F, with Q representing our beliefs about "what the empirical distribution 
would look like for large n". 
As we remarked at the end of Section 4.3.3, the task of assessing and  
representing such a belief distribution Q over the set 3 of all possible distribution functions 
is by no means straightforward, since F is, effectively, an infinite-dimensional  
parameter. Most of this chapter has therefore been devoted to exploring additional 
features of beliefs which justify the restriction of 3 to families of distributions 
having explicit mathematical forms involving only a finite-dimensional labelling 
parameter. 
Conventionally, albeit somewhat paradoxically, representations in the finite- 
dimensional case are referred to as parametric models, whereas those involving the 
infinite-dimensional parameter are referred to as nonparametric modelsl The  
technical key to Bayesian nonparametric modelling is thus seen to be the specification 
of appropriate probability measures over function spaces, rather than over finite- 
dimensional real spaces, as in the parametric case. For this reason, the Bayesian 
analysis of nonparametric models requires considerably more mathematical  
machinery than the corresponding analysis of parametric models. In the rest of this 
volume we will deal exclusively with the parametric case, postponing a treatment 
of nonparametric problems to the volumes Bayesian Computation and Bayesian 
Methods. 
Among important references on this topic, we note Whittle (1958), Hill (1968, 
1988, 1992), Dickey (1969), Kimeldorf and Wahba (1970), Good and Gaskins 
(1971, 1980), Ferguson (1973, 1974), Leonard (1973), Antoniak (1974), Doksum 
(1974), Susarla and van Ryzin (1976), Ferguson and Phadia (1979), Dalai and Hall 
(1980), Dykstra and Laud (1981), Padgett and Wei (1981), Rolin (1983), Lo (1984), 
Thorburn (1986), Kestemont (1987), Berliner and Hill (1988), Wahba (1988), Hjort 
(1990), Lenk (1991) and Lavine (1992a). 
As we have seen, the use of specific parametric forms can often be given a 
formal motivation or justification as the coherent representation of certain forms of 
4.7 Pragmatic Aspects 
229 
belief characterised by invariance or sufficiency properties. In practice, of course, 
there are often less formal, more pragmatic, reasons for choosing to work with a 
particular parametric model (as there often are for acting, formally, as //"particular 
forms of summary statistic were sufficient!). In particular, specific parametric  
models are often suggested by exploratory data analysis (typically involving graphical 
techniques to identify plausible distributional shapes and forms of relationship with 
covariates), or by experience (i.e., historical reference to "similar" situations, where 
a given model seemed "to work") or by scientific theory (which determines that 
a specific mathematical relationship "must" hold, in accordance with an assumed 
"law"). In each case, of course, the choice involves subjective judgements; for 
example, regarding such things as the "straightness" of a graphical normal plot, 
the "similarity" between a current and a previous trial, and the "applicability" of 
a theory to the situation under study. From the standpoint of the general  
representation theorem, such judgements correspond to acting as //"one has a Q which 
concentrates on a subset of 3 defined in terms of a finite-dimensional labelling 
parameter. 
4.7.3 Model Elaboration 
However, in arriving at a particular parametric model specification, by means of 
whatever combination of formal and pragmatic judgements have been deemed 
appropriate, a number of simplifying assumptions will necessarily have been made 
(either consciously or unconsciously). It would always be prudent, therefore, to 
"expand one's consciousness" a little in relation to an intended model in order 
to review the judgements that have been made. Depending on the context, the 
following kinds of critical questions might be appropriate: 
(i) is it reasonable to assume that all the observables form a "homogeneous  
sample", or might a few of them be "aberrant" in some sense? 
(ii) is it reasonable to apply the modelling assumptions to the observables on 
their original scale of measurement, or should the scale be transformed to 
logarithms, reciprocals, or whatever? 
(iii) when considering temporally or spatially related observables, is it reasonable 
to have made a particular conditional independence assumption, or should 
some form of correlation be taken into account? 
(iv) if some, but not all, potential covariates have been included in the model, is it 
reasonable to have excluded the others, or might some of them be important, 
either individually or in conjunction with covariates already included? 
We shall consider each of these possibilities in turn, indicating briefly the 
kinds of elaboration of the "first thought of model that might be considered. 
230 
4 Modelling 
Outlier elaboration. Suppose that judgements about a sequence X\,x2,... of 
real-valued random quantities have led to serious consideration of the model 
p(xu...,xn) = / T\N(xi\fj,,\)dQ(fj,,\), 
but, on reflection, it is thought wise to allow for the fact that (an unknown) one of 
Xi,...,xn might be aberrant. If aberrant observations are assumed to be such that 
a sequence of them would have a limiting mean equal to /z, but a limiting mean 
square about the mean equal to (tA)-1, 0 < 7 < 1, where /i and A-1 denote the 
corresponding limits for non-aberrant observations, a suitable form of elaborated 
model might be 
p(xi,...,xn) = tt / TTn(:e;|/z,A) dQ(fj,,\) 
+(1 - tt) f J2 -n(*> 1 /*. ^ \ n n(x> \»>xu dQ^ a) d^y 
Jxx*+xMUn (7*j J 
This model corresponds to an initial assumption that, with specified probability n, 
there are no aberrant observations, but, with probability 1 — 7r, there is precisely one 
aberrant observation, which is equally likely to be any one of x\,..., x„.  
Generalisations to cover more than one possible aberrant observation can be constructed in 
an obviously analogous manner. Such models are usually referred to as "outlier" 
models, since 7 < 1 implies an increased probability that, in the observed sample 
x\,..., xn, the aberrant observation will "outlie". Since for an aberrant  
observation x, E[(x - /1)2 I fi, A, 7] = (7A)-1, prior belief in the relative inaccuracy of an 
aberrant observation as a "predictor" of fi is reflected in the weight attached by the 
prior distribution Qij) to values of 7 much smaller than 1. 
De Finetti (1961) and Box and Tiao (1968) are pioneering Bayesian papers on 
this topic. More recent literature includes; Dawid (1973), O'Hagan (1979, 1988b, 
1990), Freeman (1980), Smith (1983), West (1984,1985), Pettit and Smith (1985), 
Amaiz and Ruiz-Rivas (1986), Muirhead (1986), Pettit (1986, 1992), Guttman and 
Pena (1988) and Pefia and Guttman (1993). 
Transformation elaboration. Suppose now that judgements about a sequence 
X\, x2, -.. of real-valued random quantities are such that it seems reasonable to  
suppose that, if a suitable 7 were identified, beliefs about the sequence x\ , x2 , ■ ■ ■, 
defined by 
x?} = (x] -1)/7 (7^o,7er) 
= log(x,-) (7 = 0), 
4.7 Pragmatic Aspects 
231 
would plausibly have the representation 
p(47),...,4»)= / nN(x{7)|/i,A)dQ*(/i,A|7). 
It then follows that 
7 +f[N(*r 
p(x1,...,x„)= / FT Nfx^ | /., A) ^(iE, T) dQ*(/z, A | T) dQ+(T) 
./sixSJ+xr i=1 
where 
i=l 
The case 7 = 1 corresponds to assuming a normal parametric model for the  
observations on their original scale of measurement. If T includes values such as 
7 = -1,7 = 1/2,7 = 0, the elaborated model admits the possibility that  
transformations such as reciprocal, square root, or logarithm, might provide a better 
scale on which to assume a normal parametric model. Judgements about the  
relative plausibilities of these and other possible transformations are then incorporated 
in Q+. For detailed developments see Box and Cox (1964), Pericchi (1981) and 
Sweeting (1984, 1985). 
Correlation elaboration. Suppose that judgements about X\, x2,... again lead 
to a "first thought of" model in which 
p(xi, ...,xn\(i,X) = Yl N(x« I A*' A)> 
8=1 
but that it is then recognised that there may be a serial correlation structure among 
xi,...,xn (since, for example, the observations correspond to successive time- 
points, t = 1, t = 2, etc.) A possible extension of the representation to incorporate 
such correlation might be to assume that, for a given 7 e [— 1,1), and conditional on 
/i and A, the correlation between x, and xi+h is given by R(xi,xi+h | fi, A, 7) = *yh, 
so that 
p(x\n,\,-y) =p(xi,...,xn\n,\,-y) = N„(x \ fil, XT 1), 
where 
r = 
1 
7 
7 
1 
7 
7 
-,n-l rJti-1 -,n-3 
yfi-1 -, 
,,n-2 
1 J 
232 
4 Modelling 
The elaborated model then becomes, for some Q*,Q+, 
p(xu...,xn)= [ Nn(a;|/il,Ar-1)dQ*(/i)A|7)dQ+(7) 
J»xS+x(-l,l) 
The "first thought of model corresponds to 7 = 0 and beliefs about the relative 
plausibility of this value compared with other possible values of positive or negative 
correlation are reflected in the specification of Q+. 
Covariate elaboration. Suppose that the "first thought of model for the ob- 
servables x = {x\ (ni), ..., x„(nm)), where xl(nl) = (xn,... ,xini) denotes 
replicate observations corresponding to the observed value z, = (zn,..., ztk) of 
the covariates z\,..., Zk, is the multiple regression model with representation 
p(x)= f Nn(x\AO,\)dQ(0,\) 
as described in Example 4.16 of Section 4.6. If it is subsequently thought that 
covariates zk+1,..., zl should also have been taken into account, a suitable  
elaboration might take the form of an extended regression model 
p(x)= [ N„(a!|A0 + B7,A)dQ*(0,7,A), 
where B consists of rows containing 6, = (zik+i, ■ ■ •, zu) replicated n, times, 
i = 1,..., m and 7 = (9k+i, ■■ ■ ,9i) denotes the regression coefficients of the 
additional regressor variables Zk+i, ■ ■ ■ ,zi. The value 7 = 0 corresponds to the 
"first thought of model. 
In all these cases, an initially considered representation of the form 
p{x) = Jp{x\<t>)dQ{<t>) 
is replaced by an elaborated representation 
p(x)= p(x\<j>,~f)dQ*(<j),7), 
the latter reducing to the original representation on setting the elaboration parameter 
7 equal to 0. Inference about such a 7, imaginatively chosen to reflect interesting 
possible forms of departure from the original model, often provides a natural basis 
for checking on the adequacy of an initially proposed model, as well as learning 
about the directions in which the model needs extending. 
Other Bayesian approaches to the problem of covariate selection include 
Bernardo and Bermudez (1985), Mitchell and Beauchamp (1988) and George and 
McCulloch (1993a). 
4.7 Pragmatic Aspects 
233 
4.7.4 Model Simplification 
The process of model elaboration, outlined in the previous section, consists in 
expanding a "first thought of model to include additional parameters (and possibly 
covanates), reflecting features of the situation whose omission from the original 
model formulation is, on reflection, thought to be possibly injudicious. 
The process of model simplification is, in a sense, the converse. In  
reviewing a currently proposed model, we might wonder whether some parameters (or 
covanates) have been unnecessary included, in the sense that a simpler form of 
model might be perfectly adequate. As it stands, of course, this latter  
consideration is somewhat ill-defined: the "adequacy", or otherwise, of a particular form 
of belief representation can only be judged in relation to the consequence arising 
from actions taken on the basis of such beliefs. These and other questions relating 
to the fundamentally important area of model comparison and model choice will 
be considered at length in Chapter 6. For the present, it will suffice just to give 
an indication of some particular forms of model simplification that are routinely 
considered. 
Equality of parameters. In Section 4.6, we analysed the situation where  
several sequences of observables are judged unrestrictedly infinitely exchangeable, 
leading to a general representation of the form 
p(xl(ni),...,xn(nm))= / YiY[p(xlj\9l) dQ{9u...,em), 
Je* i=i j=i 
where 9t e 0,-, 0* = \\^=l 0* and the parameter 9i relating to the ith sequence 
can typically be interpreted as the limit of a suitable summary statistic for the ith 
sequence. If, on the other hand, the simplifying judgement were made that, in fact, 
the labelling of the sequences is irrelevant and that any combined collection of 
observables from any or all of the sequences would be completely exchangeable, 
we would have the representation 
, m n 
p(xl(nl),...,xn(nm))= Y[ Y[p(xrj\9)dQ(e) 
JS* i=l 3 = 1 
where the same parameter 9 e 0 now suffices to label the parametric model for 
each of the sequences. In conventional terminology, the simplified representation 
is sometimes referred to as the null-hypothesis (9\ = ■ ■ ■ = 9m) and the original 
representation as the alternative hypothesis (9\ ^ • ■ • ^ 9m). As we saw in  
Section 4.6 (for the case of two 0-1 sequences), rather than opt for sure for one or other 
of these representations, we could take a mixture of the two (with weight it, say, on 
the null representation and 1 — ir on the alternative, general, representation). This 
234 
4 Modelling 
form of representation will be considered in more detail in Chapter 6, where it will 
be shown to provide a possible basis for evaluating the relative plausibility of the 
"null and alternative hypotheses" in the light of data. 
Absence of effects. In Section 4.6, we considered the situation of a structured 
layout with replicate sequences of observations in each of IJ cells, and a possible 
parametric model representation involving row effects (ai,..., a{), column effects 
(Pi,..., Pj) and interaction effects (711,..., ju). A commonly considered  
simplifying assumption is that there are no interaction effects (711 = • • • = ju = 0), 
so that large sample means in individual cells are just the additive combination of 
the corresponding large sample row and column means. 
Further possible simplifying judgements would be that the row (or column) 
labelling is irrelevant, so that a.\ = ■ ■ ■ = ai = 0 (or P\ = ■ ■ ■ = Pj = 0) and 
large sample cell means coincide with column (or row) means. Again, conventional 
terminology would refer to these simplifying judgements as "null hypotheses". 
Omission of covariates. Considering, for example, the multiple regression 
case, described in Example 4.14 of Section 4.6 and reconsidered in the previous 
section on model elaboration, we see that here the simplification process is very 
clearly j ust the converse of the elaboration process. If 7 denotes the regression  
coefficients of the covariates we are considering omitting, then the model corresponding 
to 7 = 0 provides the required simplification. 
In fact, in all the cases of elaboration which we considered in the previous 
section, setting the "elaboration parameter" 7 to 0 provides a natural form of  
simplification of potential interest. Whether the process of model comparison and choice 
is seen as one of elaboration or of simplification is then very much a pragmatic issue 
of whether we begin with a "smaller" model and consider making it "bigger", or 
vice versa. In any case, issues of model comparison and choice require a separate 
detailed and extensive treatment, which we defer until Chapter 6. 
4.7.5 Prior Distributions 
The operational subjectivist approach to modelling views predictive models as  
representations of beliefs about observables (including limiting, large-sample  
functions of observables, conventionally referred to as parameters). Invariance and 
sufficiency considerations have then been shown to justify a structured approach to 
predictive models in terms of integral mixtures of parametric models with respect 
to distributions for the labelling parameters. In familiar terminology, we specify 
a distribution for the observables conditional on unknown parameters (a sampling 
distribution, defining a likelihood), together with a distribution for the unknown 
parameters (a prior distribution). It is the combination of prior and likelihood 
which defines the overall model. In terms of the mixture representation, the  
specification of a prior distribution for unknown parameters is therefore an essential 
4.8 Discussion and Further References 
235 
and unavoidable part of the process of representing beliefs about observables and 
hence of learning from experience. 
From the operational, subjectivist perspective, it is meaningless to approach 
modelling solely in terms of the parametric component and ignoring the prior 
distribution. We are, therefore, in fundamental disagreement with approaches to 
statistical modelling and analysis which proceed only on the basis of the sampling 
distribution or likelihood and treat the prior distribution as something optional, 
irrelevant, or even subversive (see Appendix B). 
That said, it should be readily acknowledged that the process of representing 
prior beliefs itself involves a number of both conceptual and practical difficulties, 
and certainly cannot be summarily dealt with in a superficial or glib manner. 
From a conceptual point of view, as we have repeatedly stressed throughout this 
chapter, prior beliefs about parameters typically acquire an operational significance 
and interpretation as beliefs about limiting (large-sample) functions of observables. 
Care must therefore obviously be taken to ensure that prior specifications respect 
logical or other constraints pertaining to such limits. Often, the specification process 
will be facilitated by suitable "reparametrisation". 
From a practical point of view, detailed treatment of specific cases is very 
much a matter of "methods" rather than "theory" and will be dealt with in the third 
volume of this series. However, a general overview of representation strategies, 
together with a number of illustrative examples, will be given in the inference 
context in Chapter 5. In particular, we shall see that the range of creative possibilities 
opened up by the consideration of mixtures, asymptotics, robustness and sensitivity 
analysis, as well as novel and flexible forms of inference reporting, provides a rich 
and illuminating perspective and framework for inference, within which many of the 
apparent difficulties associated with the precise specification of prior distributions 
are seen to be of far less significance than is commonly asserted by critics of the 
Bayesian approach. 
4.8 DISCUSSION AND FURTHER REFERENCES 
4.8.1 Representation Theorems 
The original representation theorem for exchangeable 0-1 random quantities  
appears in de Finetti (1930), the concept of exchangeability having been considered 
earlier by Haag (1924) and also in the early 1930's by Khintchine (1932).  
Extensions to the case of general exchangeable random quantities appear in de Finetti 
(1937/1964) and Dynkin (1953), with an abstract analytical version appearing in 
Hewitt and Savage (1955). Seminal extensions to more complex forms of  
symmetry (partial exchangeability) can be found in de Finetti (1938) and Freedman 
236 
4 Modelling 
(1962). See Diaconis and Freedman (1980b) and Wechsler (1993) for overviews 
and generalisations of the concept of exchangeability. 
Recent and current developments have generated an extensive catalogue of 
characterisations of distributions via both invariance and sufficiency conditions. 
Important progress is made in Diaconis and Freedman (1984, 1987, 1990) and 
Kiichler and Lauritzen (1989). See, also, Ressel (1985). Useful reviews are given by 
Aldous (1985), Diaconis (1988a) and, from a rather different perspective, Lauritzen 
(1982, 1988). The conference proceedings edited by Koch and Spizzichino (1982) 
also provides a wealth of related material and references. For related developments 
from a reliability perspective, see Barlow and Mendel (1992, 1994) and Mendel 
(1992). 
4.8.2 Subjectivity and Objectivity 
Our approach to modelling has been dictated by a subjectivist, operational  
concern with individual beliefs about (potential) observables. Through judgements of 
symmetry, partial symmetry, more complex invariance or sufficiency, we have seen 
how mixtures over conditionally independent "parameter-labelled" forms arise as 
typical representations of such beliefs. We have noted how this illuminates, and 
puts into perspective, linguistic separation into "likelihood" (or "sampling model") 
and "prior" components. But we have also stressed that, from our standpoint, the 
two are actually inseparable in defining a belief model. 
In contrast, traditional discussion of a statistical model typically refers to the 
parametric form as "the model". The latter then defines "objective" probabilities 
for outcomes defined in terms of observables, these probabilities being determined 
by the values of the "unknown parameters". It is often implicit in such discussion 
that if the "true" parameter were known, the corresponding parametric form would 
be the "true" model for the observables. Clearly, such an approach seeks to make 
a very clear distinction between the nature of observables and parameters. It is as 
if, given the "true" parameter, the corresponding parametric distribution is seen as 
part of "objective reality", providing the mechanism whereby the observables are 
generated. The "prior", on the other hand, is seen as a "subjective" optional extra, a 
potential contaminant of the objective statements provided by the parametric model. 
Clearly, this view has little in common with the approach we have  
systematically followed in this volume. However, there is an interesting sense, even from 
our standpoint, in which the parametric model and the prior can be seen as having 
different roles. 
Instead of viewing these roles as corresponding to an objective/subjective 
dichotomy, we view them in terms of an intersubjective/subjective dichotomy  
(following Dawid, 1982b, 1986b). To this end, consider a group of Bayesians, all 
concerned with their belief distributions for the same sequence of observables. In 
the absence of any general agreement over assumptions of symmetry, invariance or 
4.8 Discussion and Further References 
237 
sufficiency, the individuals are each simply left with their own subjective  
assessments. However, given some set of common assumptions, the results of this chapter 
imply that the entire group will structure their beliefs using some common form of 
mixture representation. Within the mixture, the parametric forms adopted will be 
the same (the intersubjective component), while the priors for the parameter will 
differ from individual to individual (the subjective component). Such  
intersubjective agreement clearly facilitates communication within the group and reduces areas 
of potential disagreement to just that of different prior judgements for the  
parameter. As we shall see in Chapter 5, judgements about the parameter will tend more 
towards a consensus as more data are acquired, so that such a group of Bayesians 
may eventually come to share very similar beliefs, even if their initial judgements 
about the parameter were markedly different. We emphasise again, however, that 
the key element here is intersubjective agreement or consensus. We can find no 
real role for the idea of objectivity except, perhaps, as a possibly convenient, but 
potentially dangerously misleading, "shorthand" for intersubjective communality 
of beliefs. 
4.8.3 Critical Issues 
We conclude this chapter on modelling with some further comments concerning 
(i) The Role and Nature of Models, (ii) Structural and Stochastic Assumptions, (iii) 
Identifiability and (iv) Robustness Considerations. 
The Role and Nature of Models 
In the approach we have adopted, the fundamental notion of a model is that of a 
predictive probability specification for observables. However, the forms of  
representation theorems we have been discussing provide, in typical cases, a basis for 
separating out, if required, two components; the parametric model, and the belief 
model for the parameters. Indeed, we have drawn attention in Section 4.8.2 to 
the fact that shared structural belief assumptions among a group of individuals can 
imply the adoption of a common form of parametric model, while allowing the 
belief models for the parameters to vary from individual to individual. One might 
go further and argue that without some element of agreement of this kind there 
would be great difficulty in obtaining any meaningful form of scientific discussion 
or possible consensus. 
Non-subjectivist discussions of the role and nature of models in statistical 
analysis tend to have a rather different emphasis (see, for example, Cox, 1990, 
and Lehmann, 1990). However, such discussions often end up with a similar 
message, implicit or explicit, about the importance of models in providing a focused 
framework to serve as a basis for subsequent identification of areas of agreement and 
disagreement. In order to think about complex phenomena, one must necessarily 
work with simplified representations. In any given context, there are typically 
238 
4 Modelling 
a number of different choices of degrees of simplification and idealisation that 
might be adopted and these different choices correspond to what Lehmann calls "a 
reservoir of models", where 
... particular emphasis is placed on transparent characterisations or descriptions 
of the models that would facilitate the understanding of when a given model is 
appropriate. (Lehmann, 1990) 
But appropriate for what? Many authors—including Cox and Lehmann— 
highlight a distinction between what one might call scientific and technological 
approaches to models. The essence of the dichotomy is that scientists are assumed to 
seek explanatory models, which aim at providing insight into and understanding of 
the "true" mechanisms of the phenomenon under study; whereas technologists are 
content with empirical models, which are not concerned with the "truth", but simply 
with providing a reliable basis for practical action in predicting and controlling 
phenomena of interest. 
Put very crudely, in terms of our generic notation, explanatory modellers 
take the form of p(x | 6) very seriously, whereas empirical modellers are simply 
concerned that p(x) "works". For an elaboration of the latter view, see Leonard 
(1980). 
The approach we have adopted is compatible with either emphasis. As we 
have stressed many times, it is observables which provide the touchstone of  
experience. When comparing rival belief specifications, all other things being equal 
we are intuitively more impressed with the one which consistently assigns higher 
probabilities to the things that actually happen. If, in fact, a phenomenon is  
governed by the specific mechanism p(x | 0) with 0 = 6Q, a scientist who discovers 
this and sets p(x) = p(x | 6q) will certainly have ap(x) that "works". 
However, we are personally rather sceptical about taking the science versus 
technology distinction too seriously. Whilst we would not dispute that there are 
typically real differences in motivation and rhetoric between scientists and  
technologists, it seems to us that theories are always ultimately judged by the predictive 
power they provide. Is there really a meaningful concept of "truth" in this context 
other than a pragmatic one predicated on p(x)l We shall return to this issue in 
Chapter 6, but our prejudices are well-captured in the adage: "all models are false, 
but some are useful". 
Structural and Stochastic Assumptions 
In Section 4.6, we considered several illustrative examples where, separate from 
considerations about the complete form of probability specification to be adopted, 
the key role of the parametric model component p(x \ 8) was to specify structured 
forms of expectations for the observables conditional on the parameters. We recall 
two examples. 
4.8 Discussion and Further References 
239 
In the case of observables x{jk in a two-way layout with replications  
(Section 4.6.3), with parameters corresponding to overall mean, main effects and  
interactions, we encountered the form 
E(xijk) = n + ai + (3j + 70-; 
in the case of a vector of observables x in a multiple regression context with design 
matrix A (Section 4.6.4, Example 4.14), we encountered the form 
E(x) = AO. 
In both of these cases, fundamental explanatory or predictive structure is  
captured by the specification of the conditional expectation, and this aspect can in many 
cases be thought through separately from the choice of a particular specification of 
full probability distribution. 
Identifiability 
A parametric model for which an element of the parametrisation is redundant is said 
to be non-identified. Such models are often introduced at an early stage of model 
building (particularly in econometrics) in order to include all parameters which may 
originally be thought to be relevant. Identifiability is a property of the parametric 
model, but a Bayesian analysis of a non-identified model is always possible if a 
proper prior on all the parameters is specified. For detailed discussion of this issue, 
see Morales (1971), Dreze (1974), Kadane (1974), Florens and Mouchart (1986), 
Hills (1987) and Florens et al. (1990, Section 4.5). 
Robustness Considerations 
For concreteness, in our earlier discussion of these examples we assumed that the 
p(x 10) terms were specified in terms of normal distributions. As we demonstrated 
earlier in this chapter, under the a priori assumption of appropriate invariances, or 
on the basis of experience with particular applications, such a specification may 
well be natural and acceptable. However, in many situations the choice of a specific 
probability distribution may feel a much less "secure" component of the overall 
modelling process than the choice of conditional expectation structure. 
For example, past experience might suggest that departures of observables 
from assumed expectations resemble a symmetric bell-shaped distribution  
centred around zero. But a number of families of distributions match these general 
characteristics, including the normal, Student and logistic families. Faced with a 
seemingly arbitrary choice, what can be done in a situation like this to obtain further 
insight and guidance? Does the choice matter? Or are subsequent inferences or 
predictions robust against such choices? 
240 
4 Modelling 
An exactly analogous problem arises with the choice of mathematical  
specifications for the prior model component. 
In robustness considerations, theoretical analysis—sometimes referred to as 
"what if?" analysis—has an interesting role to play. Using the inference machinery 
which we shall develop in Chapter 5, the desired insight and guidance can often 
be obtained by studying mathematically the ways in which the various "arbitrary" 
choices affect subsequent forms of inferences and predictions. For example, a"what 
if?" analysis might consider the effect of a single, aberrant, outlying observation on 
inferences for main effects in a multiway layout under the alternative assumptions of 
anormal or Student parametric model distribution. Itcan be shown that the influence 
of the aberrant observation is large under the normal assumption, but negligible 
under the Student assumption, thus providing a potential basis for preferring one 
or other of the otherwise seemingly arbitrary choices. 
More detailed analysis of such robustness issues will be given in Section 5.6.3. 
241 
Chapter 5 
Inference 
Summary 
The role of Bayes' theorem in the updating of beliefs about observables in the 
light of new information is identified and related to conventional mechanisms 
of predictive and parametric inference. The roles of sufficiency, ancillarity and 
stopping rules in such inference processes are also examined. Forms of common 
statistical decisions and inference summaries are introduced and the problems of 
implementing Bayesian procedures are discussed at length. In particular,  
conjugate, asymptotic and reference forms of analysis and numerical approximation 
approaches are detailed. 
5.1 THE BAYESIAN PARADIGM 
5.1.1 Observables, Beliefs and Models 
Our development has focused on the foundational issues which arise when we aspire 
to formal quantitative coherence in the context of decision making in situations 
of uncertainty. This development, in combination with an operational approach 
to the basic concepts, has led us to view the problem of statistical modelling as 
that of identifying or selecting particular forms of representation of beliefs about 
observables. 
242 
5 Inference 
For example, in the case of a sequence Xi, X2, ■ ■ ■, of 0 - 1 random quantities 
for which beliefs correspond to a judgement of infinite exchangeability,  
Proposition 4.1, (de Finetti's theorem) identifies the representation of the joint mass 
function foTX\,...,xn as having the form 
p(Xl,...,xn)= I X\6xi{\-6)l-x>dQ{0), 
for some choice of distribution Q over the interval [0,1]. 
More generally, for sequences of real-valued or integer-valued random  
quantities, x\, X2, ■ ■ ■, we have seen, in Sections 4.3 - 4.5, that beliefs which combine 
judgements of exchangeability with some form of further structure (either in terms 
of invariance or sufficient statistics), often lead us to work with representations of 
the form 
f " 
p(xu...,xn) = T]p(xi\0)dQ(6), 
where p(x \ 6) denotes a specified form of labelled family of probability  
distributions and Q is some choice of distribution over %tk. 
Such representations, and the more complicated forms considered in  
Section 4.6, exhibit the various ways in which the element of primary significance 
from the subjectivist, operationalist standpoint, namely the predictive model of 
beliefs about observables, can be thought of as if constructed from a parametric 
model together with a prior distribution for the labelling parameter. 
Our primary concern in this chapter will be with the way in which the updating 
of beliefs in the light of new information takes place within the framework of such 
representations. 
5.1.2 The Role of Bayes' Theorem 
In its simplest form, within the formal framework of predictive model belief  
distributions derived from quantitative coherence considerations, the problem  
corresponds to identifying the joint conditional density of 
P\p^n+\ i • • ■ i 3*71+171 | «El i ■ • ■ i ^n) 
foranym > 1, given, for any n > 1, the form of representation of the joint density 
p(xu...,xn). 
In general, of course, this simply reduces to calculating 
n(r ,, r I r, t \ — P(Zl,- ■ -,Xn+m) 
P[X], . . . , Xn) 
5.7 The Bayesian Paradigm 
243 
and, in the absence of further structure, there is little more that can be said.  
However, when the predictive model admits a representation in terms of parametric 
models and prior distributions, the learning process can be essentially identified, in 
conventional terminology, with the standard parametric form of Bayes' theorem. 
Thus, for example, if we consider the general parametric form of representation 
for an exchangeable sequence, with dQ(9) having density representation, p(6)d9, 
we have 
from which it follows that 
Pv^n+l i ■ ■ • i Xn+m \ X\, . . . , Xn) 
p(xU...,Xn)= j[[ P(Xi I 0)P(0) d0> 
fWTp^\o)p(Q)do 
/n+m 
Y[ p(xi I 6)p(0 \xu...,xn)dO, 
i=n+\ 
where 
*. I....... ,j- IE-, **!•)*•> 
S\[n„iP(':i\e)v(e)dB 
This latter relationship is just Bayes' theorem, expressing the posterior density 
for 6, given x\,..., xn, in terms of the parametric model for x\,..., xn given 6, 
and the prior density for 9. The (conditional, or posterior) predictive model for 
xn+i,..., xn+m, given x\,...,xnis seen to have precisely the same general form 
of representation as the initial predictive model, except that the corresponding  
parametric model component is now integrated with respect to the posterior distribution 
of the parameter, rather than with respect to the prior distribution. 
We recall from Chapter 4 that, considered as a function of 8, 
lik(0 | £!,...,£„) =p(x\,...,Xn\0) 
is usually referred to as the likelihood function. A formal definition of such a 
concept is, however, problematic; for details, see Bayarri et al. (1988) and Bayarri 
and DeGroot (1992b). 
5.1.3 Predictive and Parametric Inference 
Given our operationalist concern with modelling and reporting uncertainty in terms 
of observables, it is not surprising that Bayes' theorem, in its role as the key to 
a coherent learning process for parameters, simply appears as a step within the 
predictive process of passing from 
| 0)p(0) dO 
244 
5 Inference 
to 
p{xn+u...,x 
n+m \X\, ■ ■ ■ , Xn) / p(xn+i,..., xn+m | 9)p(0 \xi,...,xn) d6, 
by means of 
p{6\xi,...,xn) 
p(xu...,xn\0)p(0) 
fp(xi,...,xn\0)p(6)d6 
Writing y = {yu ... ,ym} = {xn+u... ,xn+m} to denote future (or, as 
yet unobserved) quantities and x = {xi, ...,xn} to denote the already observed 
quantities, these relations may be re-expressed more simply as 
and 
p{x) = J p(x\0)p(0)dO, 
p(y \x)= p(y\9)p{9 \x)di 
p(6\x) = p(x\0)p(0)/p(x). 
However, as we noted on many occasions in Chapter 4, if we proceed purely 
formally, from an operationalist standpoint it is not at all clear, at first sight, how we 
should interpret "beliefs about parameters", as represented by p(9) and p(9 \ x), 
or even whether such "beliefs" have any intrinsic interest. We also answered these 
questions on many occasions in Chapter 4, by noting that, in all the forms of 
predictive model representations we considered, the parameters had interpretations 
as strong law limits of (appropriate functions of) observables. Thus, for example, 
in the case of the infinitely exchangeable 0 - 1 sequence (Section 4.3.1) beliefs 
about 6 correspond to beliefs about what the long-run frequency of l's would be 
in a future sample; in the context of a real-valued exchangeable sequence with 
centred spherical symmetry (Section 4.4.1), beliefs about \x and a2, respectively, 
correspond to beliefs about what the large sample mean, and the large sample mean 
sum of squares about the sample mean would be, in a future sample. 
Inference about parameters is thus seen to be a limiting form of predictive 
inference about observables. This means that, although the predictive form is 
primary, and the role of parametric inference is typically that of an intermediate 
structural step, parametric inference will often itself be the legitimate end-product 
of a statistical analysis in situations where interest focuses on quantities which 
could be viewed as large-sample functions of observables. Either way, parametric 
inference is of considerable importance for statistical analysis in the context of the 
models we are mainly concerned with in this volume. 
5.7 The Bayesian Paradigm 
245 
When a parametric form is involved simply as an intermediate step in the 
predictive process, we have seen that p(9 | x\,..., xn), the full joint posterior 
density for the parameter vector 6 is all that is required. However, if we are 
concerned with parametric inference per se, we may be interested in only some 
subset, 4>, of the components of 6, or in some transformed subvector of parameters, 
g{6). For example, in the case of a real-valued sequence we may only be interested 
in the large-sample mean and not in the variance; or in the case of two 0-1 
sequences we may only be interested in the difference in the long-run frequencies. 
In the case of interest in a subvector of 6, let us suppose that the full parameter 
vector can be partitioned into 9 = {d), A}, where d> is the subvector of interest, 
and A is the complementary subvector of 0, often referred to, in this context, as the 
vector of nuisance parameters. Since 
p(x) 
the (marginal) posterior density for d> is given by 
p(d>\x) = I p(0\x)d\ = J p((f>,X\x)d\, 
p(x)= J p(x\e)p(e)d0= J p(x\(j>,\)p(<l>,\)d(t>d\, 
with all integrals taken over the full range of possible values of the relevant  
quantities. 
Expressed in terms of the notation introduced in Section 3.2.4, we have 
p(x | <M) ® p(d>, A) = p((f>, A | x), 
p{d>,\\x)—*p{d>\x). 
<t> 
In some situations, the prior specification p(d>, A) may be most easily arrived 
at through the specification of p(A | d>)p(d)). In such cases, we note that we could 
first calculate the integrated likelihood for <f>, 
where 
p(x \(f>) = p(x | (j>, A)p(A | 4>) dX, 
and subsequently proceed without any further need to consider the nuisance  
parameters, since 
p(x) 
246 
5 Inference 
In the case where interest is focused on a transformed parameter vector, g(6), 
we proceed using standard change-of-variable probability techniques as described 
in Section 3.2.4. Suppose first that ij) = g{9) is a one-to-one differentiable  
transformation of 6. It then follows that 
P0(V» I x) = pe(g~l(il>) I sb) I J9-i (V) I, 
where 
is the Jacobian of the inverse transformation 6 = g_1 (■0). Alternatively, by  
substituting 9 — g~x(ip), we could write p(x | 6) as p(a; | ifr), and replace p(0) by 
Pe{g~l WO) I «Jfl-i WO I. to obtain pW | x) = p(x \ i/))p(ij})/p(x) directly. 
If ip = g(9) has dimension less than 6, we can typically define 7 = (ip, lj) = 
h(6), for some u> such that 7 = /i(0) is a one-to-one differentiable transformation, 
and then proceed in two steps. We first obtain 
p(V,w I x) = pe(hr\~i) I a) | Jh-i (7) |, 
where 
C?7 
and then marginalise to 
p(^|a;)= p(i(j,u\x)dw. 
These techniques will be used extensively in later sections of this chapter. 
In order to keep the presentation of these basic manipulative techniques as 
simple as possible, we have avoided introducing additional notation for the ranges 
of possible values of the various parameters. In particular, all integrals have been 
assumed to be over the full ranges of the possible parameter values. 
In general, this notational economy will cause no confusion and the parameter 
ranges will be clear from the context. However, there are situations where specific 
constraints on parameters are introduced and need to be made explicit in the analysis. 
In such cases, notation for ranges of parameter values will typically also need to be 
made explicit. 
Consider, for example, a parametric model, p(x | 6), together with a prior 
specification p(9), 6 € G, for which the posterior density, suppressing explicit use 
of G, is given by 
p(x I 0)p(0) 
p{91 a) = 
fp(x\e)p(G)dO 
5.7 The Bayesian Paradigm 
247 
Now suppose that it is required to specify the posterior subject to the constraint 
6 e e0 C 6, where /^ p{8)d8 > 0. 
Defining the constrained prior density by 
we obtain, using Bayes' theorem, 
From this, substituting for po(6) in terms of p(6) and dividing both numerator and 
denominator by 
p{x) = Ip{x\ 8)p{8)d8, 
Je 
we obtain 
P(o\x,eee„) = , p{°\x\,a> »ee„, 
feop(0\x)dO 
expressing the constraint in terms of the unconstrained posterior (a result which 
could, of course, have been obtained by direct, straightforward conditioning). 
Numerical methods are often necessary to analyze models with constrained 
parameters; see Gelfand et al. (1992) for the use of Gibbs sampling in this context. 
5.1.4 Sufficiency, Ancillarity and Stopping Rules 
The concepts of predictive and parametric sufficient statistics were introduced in 
Section 4.5.2, and shown to be equivalent, within the framework of the kinds of 
models we are considering in this volume. In particular, it was established that 
a (minimal) sufficient statistic, t(x), for 9, in the context of a parametric model 
p(x | 6), can be characterised by either of the conditions 
or 
p(0\x)=p(0\t{x)), forallp(6>), 
p{x\ t(x),9) =p{x\t{x)). 
The important implication of the concept is that t(x) serves as a sufficient summary 
of the complete data x in forming any required revision of beliefs. The resulting data 
reduction often implies considerable simplification in modelling and analysis. In 
248 
5 Inference 
many cases, the sufficient statistic t(x) can itself be partitioned into two component 
statistics, t(x) = [a(x), s(x)\ such that, for all 9, 
p(t{x) | 6>) = p(s(x) | a(x), 0)p(a(x) \ 6>) 
= p(s{x)\a(x),6)p(a(x)). 
It then follows that, for any choice of p(9), 
p{9 | x) = p(6 | t{x)) oc p(t(x) | 0)p(6) 
oc p(s(x)\a(x),9)p(9), 
so that, in the prior to posterior inference process defined by B ayes' theorem, it 
suffices to use p(s(x) \ a(x), 6), rather than p(t(x) \ 6) as the likelihood function. 
This further simplification motivates the following definition. 
Definition 5.1. (Ancillary statistic). A statistic, a(x), is said to be ancillary, 
with respect to 6 in a parametric modelp(x \ 6), ifp(a(x) \ 6) = p(a{x))for 
all values of 6. 
Example 5.1. (Bernoulli model). In Example 4.5, we saw that for the Bernoulli 
parametric model 
n 
p(xu ...,xn\0) = Y[p(xi I 6) = er"(l - 0)""r" 
= p{n, rn | 0), 
where rn = xx-\ h xn, so that tn = [n, r„] provides a minimal sufficient statistic. 
If we now write 
P(n, rn\8) = p(rn | n, 6)p{n \ 6), 
and make the assumption that, for all n > 1, the mechanism by which the sample size, n, is 
arrived at does not depend on 6, so that p(n | 6) = p(n), n > 1, we see that n is ancillary 
for 6, in the sense of Definition 5.1. It follows that prior to posterior inference for 6 can 
therefore proceed on the basis of 
p(6 | x) = p{6 | n, r„) ex p(rn | n, 0)p(0), 
for any choice of p(^), 0 < 0 < 1. From Corollary 4.1, we see that 
p(rn\n,0) = fnjern(l-e)"-rn, 0<rn<n, 
= Bi(rn\6,n), 
5.7 The Bayesian Paradigm 
249 
so that inferences in this case can be made as if we had adopted a binomial parametric model. 
However, if we write 
p(n,r„\6)= p(n\rn,8)p{rn \ 0) 
and make the assumption that, for all rn > 1, termination of sampling is governed by a 
mechanism for selecting r„, which does not depend on 6, so that p(rn 10) — p{rn),rn > 1, 
we see that r„ is ancillary for 8, in the sense of Definition 5.1. It follows that prior to posterior 
inference for 6 can therefore proceed on the basis of 
p{6 | x) = p{6 | n, rn) oc p{n | rn, 6)p(6), 
for any choice of p(0), 0 < 0 < 1. It is easily verified that 
p{n\rn,6) = ("~_l\T»{\-6)n-rn, n>rn, 
= Nb{n\e,rn) 
(see Section 3.2.2), so that inferences in this case can be made as if we had adopted a 
negative-binomial parametric model. 
We note, incidentally, that whereas in the binomial case it makes sense to consider 
p(0) as specified over 0 < 0 < 1, in the negative-binomial case it may only make sense to 
think of p(0) as specified over 0 < 0 < 1, since p(rn \ 8 = 0) = 0, for all rn > 1. 
So far as prior to posterior inference for 0 is concerned, we note that, for any specified 
p(6), and assuming that either p(n \ 0) = p(n) or p(rn \ 8) = p{rn), we obtain 
p(6 \xu ... ,xn) = p(6\n,rn) <x 8r»(l - e)n-T"p(e) 
since, considered as functions of 6, 
p(rn\n,9)<xp(n\rn,e)cxer»(l-6)n-r». 
The last part of the above example illustrates a general fact about the  
mechanism of parametric Bayesian inference which is trivially obvious; namely,/or any 
specified p(6), if the likelihood functions p\(xi \ 0),p2(x2 \ 9) are proportional as 
functions of 6, the resulting posterior densities for 6 are identical. It turns out, 
as we shall see in Appendix B, that many non-Bayesian inference procedures do 
not lead to identical inferences when applied to such proportional likelihoods. The 
assertion that they should, the so-called Likelihood Principle, is therefore a  
controversial issue among statisticians. In contrast, in the Bayesian inference context 
described above, this is a straightforward consequence of Bayes' theorem, rather 
than an imposed "principle". Note, however, that the above remarks are predicated 
on a specified p{6). It may be, of course, that knowledge of the particular sampling 
mechanism employed has implications for the specification of p{9), as illustrated, 
for example, by the comment above concerning negative-binomial sampling and 
the restriction to 0 < 9 < 1. 
250 
5 Inference 
Although the likelihood principle is implicit in Bayesian statistics, it was  
developed as a separate principle by Barnard (1949), and became a focus of interest 
when Birnbaum (1962) showed that it followed from the widely accepted  
sufficiency and conditionality principles. Berger and Wolpert (1984/1988) provide an 
extensive discussion of the likelihood principle and related issues. Other relevant 
references are Barnard et al. (1962), Fraser (1963), Pratt (1965), Barnard (1967), 
Hartigan (1967), Birnbaum (1968, 1978), Durbin (1970), Basu (1975), Dawid 
(1983a), Joshi (1983), Berger (1985b), Hill (1987) and Bayarri et al. (1988). 
Example 5.1 illustrates the way in which ancillary statistics often arise  
naturally as a consequence of the way in which data are collected. In general, it is 
very often the case that the sample size, n, is fixed in advance and that inferences 
are automatically made conditional on n, without further reflection. It is, however, 
perhaps not obvious that inferences can be made conditional on n if the latter has 
arisen as a result of such familiar imperatives as "stop collecting data when you feel 
tired", or "when the research budget runs out". The kind of analysis given above 
makes it intuitively clear that such conditioning is, in fact, valid, provided that the 
mechanism which has led to n "does not depend on 6". This latter condition may, 
however, not always be immediately obviously transparent, and the following  
definition provides one version of a more formal framework for considering sampling 
mechanisms and their dependence on model parameters. 
Definition 5.2. (Stopping rule). A stopping rule, r, for (sequential) sampling 
from a sequence of observables xi G -X"i,£2 G ^2, ■ ■ ■, is a sequence of 
functions t„ : X\ x • • • x Xn —> [0,1], such that, ifx^ = (x\,... ,xn) is 
observed, then sampling is terminated with probability t„(x(„)); otherwise, 
the (n + l)th observation is made. A stopping rule is proper if the induced 
probability distribution pT(n), n = 1,2,..., for final sample size guarantees 
that the latter is finite. The rule is deterministic j/r„(X(n)) G {0,1} for all 
(n, X(„)); otherwise, it is a randomised stopping rule. 
In general, we must regard the data resulting from a sampling mechanism 
defined by a stopping rule r as consisting of (n, X(„)), the sample size, together 
with the observed quantities x\,... ,xn. A parametric model for these data thus 
involves a probability density of the form p(n, X(n) | t, 6), conditioning both on 
the stopping rule (i.e., sampling mechanism) and on an underlying labelling  
parameter 6. But, either through unawareness or misapprehension, this is typically 
ignored and, instead, we act as if the actual observed sample size n had been fixed 
in advance, in effect assuming that 
p(n, x(n) \t,0)= p(x(n) | n, 6) = p(x(n) 10), 
using the standard notation we have hitherto adopted for fixed n. The important 
question that now arises is the following: under what circumstances, if any, can 
5.7 The Bayesian Paradigm 
251 
we proceed to make inferences about 0 on the basis of this (generally erroneous!) 
assumption, without considering explicit conditioning on the actual form of t? Let 
us first consider a simple example. 
Example 5.2. ("Biased" stopping rule for a Bernoulli sequence). Suppose, given 8, 
that xi,X2,-.- may be regarded as a sequence of independent Bernoulli random quantities 
with p(x, | 8) = Bi(xi | 8,1), Xi = 0,1, and that a sequential sample is to be obtained using 
the deterministic stopping rule t, defined by: rx (1) = 1, rx (0) = 0, t2(xx , x2) = 1 for all 
X\, x2. In other words, if there is a success on the first trial, sampling is terminated (resulting 
in n = 1, X\ = 1); otherwise, two observations are obtained (resulting in either n = 2, 
X\ = 0, x2 — 0 or n = 2, xx = 0, x2 = 1). 
At first sight, it might appear essential to take explicit account of r in making inferences 
about 8, since the sampling procedure seems designed to bias us towards believing in large 
values of 8. Consider, however, the following detailed analysis: 
p(n = 1,xi = 11 t,8) = p(xx = 11n = 1,t,8)p(n = 1\t,0) 
= l-p{xl=l\8)=p{xl = l\8) 
and, fori = 0,1, 
p(n = %x\ = 0,x2 = x | t,8) = p(xi = 0,x2 = x |n = 2,t,8)p(n = 2\t,8) 
= p(xi = 0|n = 2, t, 8)p(x2 = x | xx = 0, n = 2, t, 0)p(n = 2 | t, 0) 
= 1 • p(i2 = x\xl=0,8)p(xi =0\8) 
— p(x2 = x,xx = 0| 8). 
Thus, for all (n, X(„)) having non-zero probability, we obtain in this case 
P(n, x{n) | t, 8) =p(x{n)\8), 
the latter considered pointwise as functions of 8 (i.e., likelihoods). It then follows trivially 
from Bayes' theorem that, for any specified p(8), inferences for 8 based on assuming n to 
have been fixed at its observed value will be identical to those based on a likelihood derived 
from explicit consideration of t. 
Consider now a randomised version of this stopping rule which is defined by tx (1) = 7r, 
Ti(0) = 0, t2(xi, x2) = 1 for all xx,x2. In this case, we have 
p{n = 1, xx = 11 t, 8) = p(xx = 11 n = 1, t, 8)p(n = 11 t, 0) 
= 1 -7r -p(a;i = l\8), 
with, for a; = 0.1, 
p(n =2, ii = 0, £2 = x I t, 0) 
= p(n = 2\xx = O,t,0) 
x ^(1! = 01 t, 8)p{x2 = x\x\ = 0, n = 2, t, 0) 
= 1 • p(ii = 018)p(x2 = x\8) 
252 
5 Inference 
and 
p(n = 2, Xi = 1, x2 = x | t, 9) = p(n = 2 | X\ = 1, t, 0)p(£i = 11 t, 0) 
x p(i2 = a: | £i = 1, n = 2, t, 0) 
= (1 - jr)p(i, = 11 fl)p(i2 = 11 fl). 
Thus, for all (n, X(„)) having non-zero probability, we again find that 
p(n, x(n) | t, 6) <x p(x{n) 16) 
as functions of 9, so that the proportionality of the likelihoods once more implies identical 
inferences from Bayes' theorem, for any given p(8). 
The analysis of the preceding example showed, perhaps contrary to intuition, 
that, although seemingly biasing the analysis towards beliefs in larger values of 
9, the stopping rule does not in fact lead to a different likelihood from that of the 
a priori fixed sample size. The following, rather trivial, proposition makes clear 
that this is true for all stopping rules as defined in Definition 5.2, which we might 
therefore describe as "likelihood non-informative stopping rules". 
Proposition 5.1. (Stopping rules are likelihood non-informative ). 
For any stopping rule t, for (sequential) sampling from a sequence ofobserv- 
ables xi,X2,..., having fixed sample size parametric model p(X(n) | n, 6) = 
P(X(n) I 0), 
p(n,x{n)\T,0) <xp(x{n)\6), 6eQ, 
for all (n, X(„)) such that p(n, x^ \t,6) ^ 0. 
Proof. This follows straightforwardly on noting that 
n-l 
p(n, x(n) \t,0)= T(xn) Yl (1 - r(xl)) p(x{n) | 6), 
i=l 
and that the term in square brackets does not depend on 6. < 
Again, it is a trivial consequence of Bayes' theorem that, for any specified 
prior density, prior to posterior inference for 0 given data (n, x(n)) obtained using 
a likelihood non-informative stopping rule t can proceed by acting as if x^ were 
obtained using a fixed sample size n. However, a notationally precise rendering of 
Bayes' theorem, 
p{9 | n, x{n), t) oc p{n, x{n) | t, 0)p(0 | t) 
CCp(x{n)\O)p(0\T), 
5.7 The Bayesian Paradigm 
253 
reveals that knowledge ofr might well affect the specification of the prior density! It 
is for this reason that we use the term "likelihood non-informative" rather than just 
"non-informative" stopping rules. It cannot be emphasised too often that, although 
it is often convenient for expository reasons to focus at a given juncture on one or 
other of the "likelihood" and "prior" components of the model, our discussion in 
Chapter 4 makes clear their basic inseparability in coherent modelling and analysis 
of beliefs. This issue is highlighted in the following example. 
Example 5.3. ("Biased" stopping rule for a normal mean). Suppose, given 8, that 
Xi, x2,..., may be regarded as a sequence of independent normal random quantities with 
p(xi | 9) = N(ii | 9,1), Xi e M. Suppose further that an investigator has a particular concern 
with the parameter value 9 = 0 and wants to stop sampling if xn = ^ Xi/n ever takes on 
a value that is "unlikely", assuming 9 = 0 to be true. 
For any fixed sample size n, if "unlikely" is interpreted as "an event having probability 
less than or equal to a", for small a, a possible stopping rule, using the fact that p(xn | n, 9) = 
N(x„ | 9, n), might be 
, _ (I, if \xn\ > k{a)/s/n 
r„{xin)) ^ .f ^^ <fc(Q)/v^ 
for suitable k(a) (for example, k = 1.96 for a = 0.05, k = 2.57 for a = 0.01, or k = 3.31 
for a = 0.001). It can be shown, using the law of the iterated logarithm (see, for example, 
Section 3.2.3), that this is a proper stopping rule, so that termination will certainly occur for 
some finite n, yielding data (n, aj(n)). Moreover, denning 
{k(a) 
x{n); \xi\ <k(a), \x2\ < —^ ■••• ■ 
k(a) , , k(a) "1 
M<-^,|*,.|>^}. 
we have 
p(n, x(n) | t, 9) = p{x(n) | n, t, 9)p(n \ t, 9) 
= p(x(n)\Sn,9)p(Sn\9) 
= P(X(n)\6), 
as a function of 9, for all (n, x^) for which the left-hand side is non-zero. It follows that t 
is a likelihood non-informative stopping rule. 
Now consider prior to posterior inference for 9, where, for illustration, we assume the 
prior specification p(9) = N(0 | ^, A), with precision A ~ 0, to be interpreted as indicating 
extremely vague prior beliefs about 9, which take no explicit account of the stopping rule t. 
Since the latter is likelihood non-informative, we have 
p(9 | x(n), n) oc p(x(n) | n, 9)p{9) 
oc p(xn | n, 6)p(6) 
oc N(x„ 19, n)N{9 \ n, A) 
254 
5 Inference 
by virtue of the sufficiency of (n, xn) for the normal parametric model. The right-hand side 
is easily seen to be proportional to exp{—\Q(0)}, where 
Q{0) = (n + t) 
0 
nxn + Xfj, 
n + \ 
which implies that 
p(0\xln),n) = N\0 
nxn + XfJ, 
n + X 
^N(0|i„,n) 
(n + A) 
for A ~ 0. 
One consequence of this vague prior specification is that, having observed (n, aj(„)), 
we are led to the posterior probability statement 
0e 
fc(o)\ 
1 vW 
71, djfi 
= 1-a. 
But the stopping rule t ensures that | xn | > k(a)/^/n. This means that the value 9 = 0 
certainly does not lie in the posterior interval to which someone with initially very vague 
beliefs would attach a high probability. An investigator knowing 0 = 0 to be the true value 
can therefore, by using this stopping rule, mislead someone who, unaware of the stopping 
rule, acts as if initially very vague. 
However, let us now consider an analysis which takes into account the stopping rule. 
The nature of t might suggest a prior specification p{0 \ t) that recognises 0 = 0 as a 
possibly "special" parameter value, which should be assigned non-zero prior probability 
(rather than the zero probability resulting from any continuous prior density specification). 
As an illustration, suppose that we specify 
p(61 r) = 7r 1(S=O)(0) + (1 - 7r)l((¥o)(0)N(0 I 0, A0), 
which assigns a "spike" of probability, 7r, to the special value, 0 = 0, and assigns 1 — 7r 
times a N(0 10, A0) density to the range 0^0. 
Since t is a likelihood non-informative stopping rule and (n, xn) are sufficient statistics 
for the normal parametric model, we have 
p(0 | n, xln), t) <x N(x„ I 0, n)p{0 \ t). 
The complete posteriorp{0 \ n, X(n), t) is thus given by 
7T l(g=0)(e)N(z„ | 0, n) + (1 - n)l(l¥0)(B)H{xn | 0, n)N(0 | 0, A0) 
■k N(i„ | 0, n) + (1 - jt) /_°^ N(x„ | 0, n)N(fl | 0, Xo)d0 
n*l{e=o)(0) + (l-Tr')l{e/o)K[6 
nx„ 
n + A0 
, n + A0 
5.7 The Bayesian Paradigm 
255 
where, since 
f Nfo,|0,n)N(0|O,Ao)<0 = Nri„|O) 
TO 
71 + A0 
it is easily verified that 
»r* = a + 
1-7T N^IO.nAoCn + Ao)-1) 
-m -1 
1 + 
^K) 
N(x„ | 0, n) 
1/2 
exp 
}' 
\(\/nx, 
,-,,(I+s)1} 
The posterior distribution thus assigns a "spike" n* to 8 = 0 and assigns 1 — tt* times a 
N(0 | (n + A0)_1nx„, n + A0) density to the range 8 ^ 0. 
The behaviour of this posterior density, derived from a prior taking account of t, is 
clearly very different from that of the posterior density based on a vague prior taking no 
account of the stopping rule. For qualitative insight, consider the case where actually 8 = 0 
and a has been chosen to be very small, so that k(a) is quite large. In such a case, n is likely 
to be very large and at the stopping point we shall have xn ~ k(a)/y/n. This means that 
1 + 
^(- 
-1/2 
exp(|fc2(a)) 
for large n, so that knowing the stopping rule and then observing that it results in a large 
sample size leads to an increasing conviction that 8 = 0. On the other hand, if 8 is appreciably 
different from 0, the resulting n, and hence 7r*, will tend to be small and the posterior will 
be dominated by the N(0 | (n + Ao)_1nxn, n + Ao) component. 
5.1.5 Decisions and Inference Summaries 
In Chapter 2, we made clear that our central concern is the representation and 
revision of beliefs as the basis for decisions. Either beliefs are to be used directly in 
the choice of an action, or are to be recorded or reported in some selected form, with 
the possibility or intention of subsequently guiding the choice of a future action. 
With slightly revised notation and terminology, we recall from Chapters 2 and 3 
the elements and procedures required for coherent, quantitative decision-making. 
The elements of a decision problem in the inference context are: 
(i) a £ A, available "answers" to the inference problem; 
(ii) wefi, unknown states of the world; 
(iii) u : A x fl —> 5R, a function attaching utilities to each consequence (a,w) 
of a decision to summarise inference in the form of an "answer", a, and an 
ensuing state of the world, w; 
256 
5 Inference 
(iv) p(w), a specification, in the form of a probability distribution, of current beliefs 
about the possible states of the world. 
The optimal choice of answer to an inference problem is an a G A which maximises 
the expected utility, 
u(a,u)p(u)du!. 
Alternatively, if instead of working with u(a, u) we work with a so-called loss 
function, 
l{a,u) = f(u)-u(a,u), 
where / is an arbitrary, fixed function, the optimal choice of answer is an a G A 
which minimises the expected loss, 
l(a,u)p(u)du). 
It is clear from the forms of the expected utilities or losses which have to be 
calculated in order to choose an optimal answer, that, if beliefs about unknown 
states of the world are to provide an appropriate basis for future decision making, 
where, as yet, A and u (or 0 may be unspecified, we need to report the complete 
belief distribution p(w). 
However, if an immediate application to a particular decision problem, with 
specified A and u (or /), is all that is required, the optimal answer—maximising 
the expected utility or minimising the expected loss—may turn out to involve only 
limited, specific features of the belief distribution, so that these "summaries" of the 
full distribution suffice for decision-making purposes. 
In the following headed subsections, we shall illustrate and discuss some of 
these commonly used forms of summary. Throughout, we shall have in mind the 
context of parametric and predictive inference, where the unknown states of the 
world are parameters or future data values (observables), and current beliefs, p(u), 
typically reduce to one or other of the familiar forms: 
p{6) initial beliefs about a parameter vector, 9; 
p{6 | x) beliefs about 0, given data x; 
p{tjj | x) beliefs about tp = g{0), given data x; 
p(y | x) beliefs about future data y, given data x. 
/ 
Jn 
/ 
5.7 The Bayesian Paradigm 
257 
Point Estimates 
In cases where u e ft corresponds to an unknown quantity, so that ft is 5R, or 3tk, 
or 5R+, or 0? x 5R+, etc., and the required answer, a £ A is an estimate of the 
true value of w (so that A = fl), the corresponding decision problem is typically 
referred to as one of point estimation. 
If w = 6 or u = i/>, we refer to parametric point estimation; if w = y, we 
refer to predictive point estimation. Moreover, since one is almost certain not to 
get the answer exactly right in an estimation problem, statisticians typically work 
directly with the loss function concept, rather than with the utility function. A 
point estimation problem is thus completely defined once A = ft and l(a, w) are 
specified. Direct intuition suggests that in the one-dimensional case, distributional 
summaries such as the mean, median or mode of p(u>) could be reasonable point  
estimates of a random quantity w. Clearly, however, these could differ considerably, 
and more formal guidance may be required as to when and why particular func- 
tionals of the belief distribution are justified as point estimates. This is provided 
by the following definition and result. 
Definition 5.3. (Bayes estimate). A Bayes estimate ofu with respect to the 
loss function l(a,u) and the belief distribution p(u) is ana G A= ft which 
minimises fQ l(a, w)p(w) du. 
Proposition 5.2. (Forms of Bayes estimates). 
(i) If A = ft = 5R* andl(a,u) = (a - u)lH(a - u), the Bayes estimate 
satisfies 
Ha = HE{u). 
If H~l exists, a = E(u), and so the Bayes estimate with respect to 
quadratic form loss is the mean ofp(u), assuming the mean to exist. 
(ii) If A = fl = $landl(a,u}) = ci(a-cj)l{(J<a)(a) + c2(u-a)l{u>a)(a), 
the Bayes estimate with respect to linear loss is the quantile such that 
P{cJ<a)=c2/(ci+c2). 
lfc\ — C2, the right-hand side equals 1 /2 and so the Bayes estimate with 
respect to absolute value loss is a median ofp(u). 
(iii) If A = flCS* andl{a,u) = 1 — l(B£(a))(u)< where BE{a) is a ball 
of radius e in ft centred at a, the Bayes estimate maximises 
/ p(w)dw. 
J Be (a) 
As e —> 0, the function to be maximised tends to p(a) and so the Bayes 
estimate with respect to zero-one loss is a mode ofp(u), assuming a 
mode to exist. 
258 
5 Inference 
Proof. Differentiating J(a — w)tH{a — w)p(w) dw with respect to a and 
equating to zero yields 
2H (a- w)p(w) dw = 0. 
This establishes (i). Since 
l(a,w)p(w)dw = ci (a - w)p(w) dw + c2 (w-a)p(w)dw, 
J J{(jj<a) J{w>a) 
differentiating with respect to a and equating to zero yields 
c\ I p(w) dw — c2 p(w) dw, 
J{w<a) J{u»a} 
whence, adding c2 Ju<ap{w) dw to each side, we obtain (ii). Finally, since 
/ l(a,w)p(w)dw = 1 - / lBe(a)(w)p(w)dw, 
and this is minimised when JB ., p(u) dw is maximised, we have (iii). < 
Further insight into the nature of case (iii) can be obtained by thinking of a 
unimodal, continuous p(u) in one dimension. It is then immediate by a continuity 
argument that a should be chosen such that 
p(a - e) =p(a + e). 
In the case of a unimodal, symmetric belief distribution, p{u), for a single 
random quantity u, the mean, median and mode coincide. In general, for unimodal, 
positively skewed, densities we have the relation 
mean > median > mode 
and the difference can be substantial if p(to) is markedly skew. Unless, therefore, 
there is a very clear need for a point estimate, and a strong rationale for a specific 
one of the loss functions considered in Proposition 5.3, the provision of a single 
number to summarise p{u) may be extremely misleading as a summary of the 
information available about u. Of course, such a comment acquires even greater 
force if p(u) is multimodal or otherwise "irregular". 
For further discussion of Bayes estimators, see, for example, DeGroot and Rao 
(1963, 1966), Sacks (1963), Farrell (1964), Brown (1973), Tiao and Box (1974), 
Berger and Srinivasan (1978), Berger (1979, 1986), Hwang (1985, 1988), de la 
Horra (1987, 1988, 1992), Ghosh (1992a, 1992b), Irony (1992) and Spall and 
Mary ak (1992). 
5.7 The Bayesian Paradigm 
259 
Credible regions 
We have emphasised that, from a theoretical perspective, uncertainty about an 
unknown quantity of interest, w, needs to be communicated in the form of the full 
(prior, posterior or predictive) density, p(w), if formal calculation of expected loss 
or utility is to be possible for any arbitrary future decision problem. In practice, 
however, p(w) may be a somewhat complicated entity and it may be both more 
convenient, and also sufficient for general orientation regarding the uncertainty 
about w, simply to describe regions C C fl of given probability under p(w). Thus, 
for example, in the case where Si C 5R, the identification of intervals containing 
50%, 90%, 95% or 99% of the probability under p(u) might suffice to give a good 
idea of the general quantitative messages implicit in p(u). This is the intuitive basis 
of popular graphical representations of univariate distributions such as box plots. 
Definition 5.4. (Credible Region). A region C C SI such that 
p(w) duj = 1 — a 
is said to be a 100(1 — a)% credible region for u, with respect to p(w). 
If SI C 3ft, connected credible regions will be referred to as credible 
intervals. 
Ifp(ui) is a (prior-posterior-predictive) density, we refer to  
(prior-posterior-predictive) credible regions. 
Clearly, for any given a there is not a unique credible region—even if we 
restrict attention to connected regions, as we should normally wish to do for obvious 
ease of interpretation (at least in cases where p(w) is unimodal). For given SI, 
p(w) and fixed a, the problem of choosing among the subsets C C SI such that 
Jc p(w) djuj — 1 — a could be viewed as a decision problem, provided that we are 
willing to specify a loss function, l(C, u), reflecting the possible consequences of 
quoting the 100(1 — a)% credible region C. We now describe the resulting form of 
credible region when a loss function is used which encapsulates the intuitive idea 
that, for given a, we would prefer to report a credible region C whose size ||C|| 
(volume, area, length) is minimised. 
Proposition 5.3. (Minimal size credible regions). Letp(u) be a probability 
density for w e Q almost everywhere continuous; given a, 0 < a < 1, if 
A = {C; P(weC) = l-a}/i and 
i(C,u) = k\\c\\-ic(u), CeA, wen, fc>o, 
then C is optimal if and only if it has the property that p(u\) > p(u2) for all 
Wi e C, u>2 £ C (except possibly for a subset ofQ of zero probability). 
260 
5 Inference 
Proof. It follows straightforwardly that, for any C € A, 
l(C,u)p(u)aw = k\\C\\ + l-a, 
j. 
so that an optimal C must have minimal size. 
If C has the stated property and D is any other region belonging to A, then since 
c = (cnD)u(cnDc),D = (cnD)u(ccnD)<mdP(u) e c) = P{u e D), 
we have 
inf pH||Cn/3c||< / p(u)dw 
ueCnDc 
with 
/ p(u;)dw< sup p{u))\\Ccr\D\ 
JccnD u>eCcnD 
sup p{u) < inf p(w) 
u>eCcru> u>€Cn£)': 
sothat||Cn£»c|| <||Ccn£>||, and hence ||C|| < ||D||. 
If C does not have the stated property, there exists A C C such that for all 
lji S .4, there exists m? g C such that p(u>2) > p{<^\)- Let B C Cc be such that 
P(u; e .4) = P(lj e B) and p(u>2) > p(wi) for all U2 € B and Wi € A Define 
D = (Cf1 A) U B. Then D € A and by a similar argument to that given above 
the resu It follows by showing that 11D11 < 11C | |. < 
The property of Proposition 5.3 is worth emphasising in the form of a definition 
(Box and Tiao, 1965). 
Definition 5.5. (Highestprobability density (HPD) regions). 
A region C CQis said to be a 100(1 — a) % highest probability density region 
for u) with respect to p(u;) if 
(i) P(u e C) = 1 - a 
(ii) p{ijJ\) >p(u)2) for all u) i € C andu)2 & C, except possibly for a  
subset offl having probability zero. 
If p(<jj) is a (prior-posterior-predictive) density, we refer to highest (prior- 
posterior-predictive) density regions. 
Clearly, the credible region approach to summarising p(u;) is not particularly 
useful in the case of discrete Q, since such regions will only exist for limited choices 
of a. The above development should therefore be understood as intended for the 
case of continuous Q. 
For a number of commonly occurring univariate forms of p(u>), there exist 
tables which facilitate the identification of HPD intervals for a range of values of a 
5.1 The Bayesian Paradigm 
261 
wo c 
Figure 5.1a u)0 almost as "plausible" as allu € C 
.» 
w0 c 
Figure 5.1b u>0 much less "plausible " than most lj € C 
(see, for example, Isaacs et ai, 1974, Ferrandiz and Sendra,1982, and Lindley and 
Scott, 1985). 
In general, however, the derivation of an HPD region requires numerical  
calculation and, particularly if p(<jj) does not exhibit markedly skewed behaviour, it 
may be satisfactory in practice to quote some more simply calculated credible re- 
262 
5 Inference 
gion. For example, in the univariate case, conventional statistical tables facilitate 
the identification of intervals which exclude equi-probable tails of p(u;) for many 
standard distributions. 
Although an appropriately chosen selection of credible regions can serve to 
give a useful summary of p(u)) when we focus just on the quantity u), there is 
a fundamental difficulty which prevents such regions serving, in general, as a 
proxy for the actual density p(<jj). The problem is that of lack of invariance under 
parameter transformation. Even if v = g(u)) is a one-to-one transformation, it 
is easy to see that there is no general relation between HPD regions for u) and v. 
In addition, there is no way of identifying a marginal HPD region for a (possibly 
transformed) subset of components of u) from knowledge of the joint HPD region. 
In cases where an HPD credible region C is pragmatically acceptable as a 
crude summary of the density p{u), then, particularly for small values of a (for 
example, 0.05,0.01), a specific value ljo € ft will tend to be regarded as somewhat 
"implausible" if u;0 £ C. This, of course, provides no justification for actions 
such as "rejecting the hypothesis that u> = u>o"• If we wish to consider such 
actions, we must formulate a proper decision problem, specifying alternative actions 
and the losses consequent on correct and incorrect actions. Inferences about a 
specific hypothesised value u>o of a random quantity u; in the absence of alternative 
hypothesised values are often considered in the general statistical literature under 
the heading of "significance testing". We shall discuss this further in Chapter 6. 
For the present, it will suffice to note—as illustrated in Figure 5.1—that even 
the intuitive notion of "implausibility if u0 £ C" depends much more on the 
complete characterisation of p{u) than on an either-or assessment based on an 
HPD region. 
For further discussion of credible regions see, for example, Pratt (1961), 
Aitchison (1964, 1966), Wright (1986) and DasGupta (1991). 
Hypothesis Testing 
The basic hypothesis testing problem usually considered by statisticians may be 
described as a decision problem with elements 
Q = {lj0 = [H0 : 0 e e0], wi = [Hx : 6 £ 6,]}, 
together with p(lj), where 0 68 = 6oU6i,istheparameterlabellingaparametric 
model, p(x \6), A — {a0, ai}, with ai(a0) corresponding to rejecting hypothesis 
Hq(Hi), and loss function l{a,i,Uj) = Zy-, i, j € {0,1}, with the Zy reflecting the 
relative seriousness of the four possible consequences and, typically, Z0o = 'n =0. 
Clearly, the main motivation and the principal use of the hypothesis testing 
framework is in model choice and comparison, an activity which has a somewhat 
different flavour from decision-making and inference within the context of an  
accepted model. For this reason, we shall postpone a detailed consideration of the 
5.1 The Bayesian Paradigm 
263 
topic until Chapter 6, where we shall provide a much more general perspective on 
model choice and criticism. 
General discussions of Bayesian hypothesis testing are included in Jeffreys 
(1939/1961), Good (1950, 1965, 1983), Lindley (1957, 1961b, 1965, 1977),  
Edwards etal (1963), Pratt (1965), Smith (1965), Farrell (1968), Dickey (1971,1974, 
1977), Lempers (1971), Rubin (1971), Zellner (1971), DeGroot (1973), Learner 
(1978), Box (1980), Shafer (1982b), Gilio and Scozzafava (1985), Smith, (1986), 
Berger and Delampady (1987), Berger and Sellke (1987) and Hodges (1990,1992). 
5.1.6 Implementation Issues 
Given a likelihood p(x \ 0) and prior density p(9), the starting point for any form 
of parametric inference summary or decision about 0 is the joint posterior density 
mx)- "WW"' 
Jp(x\O)p(0)d0 
and the starting point for any predictive inference summary or decision about future 
observables y is the predictive density 
p(y\x) = j p{y\0)p{0\x)dO. 
It is clear that to form these posterior and predictive densities there is a technical 
requirement to perform integrations over the range of 6. Moreover, further  
summarisation, in order to obtain marginal densities, or marginal moments, or expected 
utilities or losses in explicitly defined decision problems, will necessitate further 
integrations with respect to components of 6 or y, or transformations thereof. 
The key problem in implementing the formal Bayes solution to inference  
reporting or decision problems is therefore seen to be that of evaluating the required 
integrals. In cases where the likelihood just involves a single parameter,  
implementation just involves integration in one dimension and is essentially trivial. However, 
in problems involving a multiparameter likelihood the task of implementation is 
anything but trivial, since, if 0 has k components, two fc-dimensional integrals are 
required just to form p(61 x) and p(y \ x). Moreover, in the case of p(6 \ x), for 
example, k (k — l)-dimensional integrals are required to obtain univariate marginal 
density values or moments, (*) (k - 2)-dimensional integrals are required to obtain 
bivariate marginal densities, and so on. Clearly, if k is at all large, the problem of 
implementation will, in general, lead to challenging technical problems, requiring 
simultaneous analytic or numerical approximation of a number of multidimensional 
integrals. 
The above discussion has assumed a given specification of a likelihood and 
prior density function. However, as we have seen in Chapter 4, although a specific 
264 
5 Inference 
mathematical form for the likelihood in a given context is very often implied or 
suggested by consideration of symmetry, sufficiency or experience, the  
mathematical specification of prior densities is typically more problematic. Some of the 
problems involved—such as the pragmatic strategies to be adopted in translating 
actual beliefs into mathematical form—relate more to practical methodology than 
to conceptual and theoretical issues and will be not be discussed in detail in this  
volume. However, many of the other problems of specifying prior densities are closely 
related to the general problems of implementation described above, as exemplified 
by the following questions: 
(i) given that, for any specific beliefs, there is some arbitrariness in the precise 
choice of the mathematical representation of a prior density, are there choices 
which enable the integrations required to be carried out straightforwardly 
and hence permit the tractable implementation of a range of analyses, thus 
facilitating the kind of interpersonal analysis and scientific reporting referred 
to in Section 4.8.2 and again later in 6.3.3? 
(ii) if the information to be provided by the data is known to be far greater than 
that implicit in an individual's prior beliefs, is there any necessity for a precise 
mathematical representation of the latter, or can a Bayesian implementation 
proceed purely on the basis of this qualitative understanding? 
(iii) either in the context of interpersonal analysis, or as a special form of actual 
individual analysis, is there a formal way of representing the beliefs of an 
individual whose prior information is to be regarded as minimal, relative to 
the information provided by the data? 
(iv) for general forms of likelihood and prior density, are there analytic/numerical 
techniques available for approximating the integrals required for implementing 
Bayesian methods? 
Question (i) will be answered in Section 5.2, where the concept of a conjugate 
prior density will be introduced. 
Question (ii) will be answered in part at the end of Section 5.2 and in more detail 
in Section 5.3, where an approximate "large sample" Bayesian theory involving 
asymptotic posterior normality will be presented. 
Question (iii) will be answered in Section 5.4, where the information-based 
concept of a reference prior density will be introduced. An extended historical 
discussion of this celebrated philosophical problem of how to represent "ignorance" 
will be given in Section 5.6.2. 
Question (iv) will be answered in Section 5.5, where classical applied  
analysis techniques such as Laplace's approximation for integrals will be briefly  
reviewed in the context of implementing Bayesian inference and decision summaries, 
together with classical numerical analytical techniques such as Gauss-Hermite 
quadrature and stochastic simulation techniques such as importance sampling, 
sampling-importance-resampling and Markov chain Monte Carlo. 
5.2 Conjugate Analysis 
265 
5.2 CONJUGATE ANALYSIS 
5.2.1 Conjugate Families 
The first issue raised at the end of Section 5.1.6 is that of tractability. Given a 
likelihood function p(x 10), for what choices of p(0) are integrals such as 
p(x) = J p(x | 0)p(0)dO and p(y | x) = f p(y\ 0)p(0 \ x)dO 
easily evaluated analytically? However, since any particular mathematical form of 
p(6) is acting as a representation of beliefs—either of an actual individual, or as 
part of a stylised sensitivity study involving a range of prior to posterior analyses— 
we require, in addition to tractability, that the class of mathematical functions from 
which p{6) is to be chosen be both rich in the forms of beliefs it can represent and 
also facilitate the matching of beliefs to particular members of the class. Tractability 
can be achieved by noting that, since Bayes' theorem may be expressed in the form 
p(O\x)<xp(x\0)p(0), 
both p(01 x) and p(0) can be guaranteed to belong to the same general family of 
mathematical functions by choosing p(0) to have the same "structure" as p(x | 0), 
when the latter is viewed as a function of 0. However, as stated, this is a rather 
vacuous idea, since p(0 \ x) and p{9) would always belong to the same "general 
family" of functions if the latter were suitably defined. To achieve a more  
meaningful version of the underlying idea, let us first recall (from Section 4.5) that if 
t = t(x) is a sufficient statistic we have 
p(0\x)=p(0\t)(xp(t\e)p(0), 
so that we can restate our requirement for tractability in terms of p(0) having the 
same structure as p(t \ 0), when the latter is viewed as a function of 0. Again, 
however, without further constraint on the nature of the sequence of sufficient 
statistics the class of possible functions p(0) is too large to permit easily interpreted 
matching of beliefs to particular members of the class. This suggests that it is only 
in the case of likelihoods admitting sufficient statistics of fixed dimension that we 
shall be able to identify a family of prior densities which ensures both tractability 
and ease of interpretation. This motivates the following definition. 
Definition 5.6. (Conjugate prior family). The conjugate family of prior  
densities for 0 € 6, with respect to a likelihood p(x \ 9) with sufficient statistic 
t = t(x) = {n, s(x)} of a fixed dimension k independent of that ofx, is 
{p(0\T),T=(TO,TU...,Tk)eT}, 
266 5 Inference 
where 
T = \ t; / p(s = (n,...,Tk)\e,n = TQ)dO < oo \ 
and 
p(0\T)- P(s = (Tu---^k)\O,n = T0) 
Jep(s = (TU...,Tk)\0,n = TO)dO 
From Section 4.5 and Definition 5.6, it follows that the likelihoods for which 
conjugate prior families exist are those corresponding to general exponential family 
parametric models (Definitions 4.10 and 4.11), for which, given /, h, (j> and c, 
* 
p(x | 9) = f(x)g(6) exp I ^^(fl^s) \, x G X, 
(ffW)"1 = J f(x)exp | f^afcWhiix) 1 dx. 
The exponential family model is referred to as regular or non-regular, respectively, 
according as X does not or does depend on 9. 
Proposition 5.4. {Conjugate families for regular exponential families). If 
x = (xi,...,xn) is a random sample from a regular exponential family 
distribution such that 
n ( k / n \ ^ 
p(x\0) = Hf(xj){g(0)}nexplJ£ci<pi(0) (X>fo)J f > 
then the conjugate family for 9 has the form 
p(9 | r) = [K(r)]-l[g(9)Yo exp j £^(0)7* 1, 9 £ G, 
where t is such that K(t) = f6\g(0)]To exp j X)f=i ci<t>i{0)Ti \d9<oo. 
Proof By Proposition 4.10 (the Neyman factorisation criterion), the sufficient 
statistics for (f> have the form 
tn\%\i ■ • • > xn) 
1=1 j=l 
,s{x)}, 
5.2 Conjugate Analysis 267 
so that, for any r = (r0, n,..., t„) such that JqP(0 \ r)dO < 00, a conjugate 
prior density has the form 
p(9\r) <xp(si(x) = n sk(x) = Tk\9,n = T0) 
oc[<7(0)pexp j^c^^Til 
by Proposition 4.2. <, 
Example 5.4. (Bernoulli likelihood; beta prior). The Bernoulli likelihood has the 
form 
n 
P(xu... xn | e) = [] e*(i - ef-x' (o < e < i) 
= (l-0rexp^log 
so that, by Proposition 5.4, the conjugate prior density for 0 is given by 
&m 
P(e | T0, t,) oc (i - eyo exp |iog (^) T^} 
.... , ^(T0,Ti 
assuming the existence of 
1 0Ti (1 - 0)T°-Ti, 
^(ro,r,)= / 0Ti(l-0)To-Tid0. 
./0 
Writing a = n + 1, and/3 = r0 -Ti + 1, wehavep(0 | a, (3) oc 0Q_1(l- 0)'3"1, and hence, 
comparing with the definition of a beta density, 
p(8\To,T1)=p(8\a,0) = Be(8\a,0), a > 0, 0 > 0. 
Example 5.5. (Poisson likelihood; gamma prior). The Poisson likelihood has the 
form 
P(xu...,Xn\e) = Yle^t^ (0>o) 
I J\ Xil\ exp(-ra0) exp I log 6 ^ x, J 
so that, by Proposition 5.4, the conjugate prior density for 0 is given by 
?)exp(nlog0) 
-0Ti exp(-ro0), 
p(6 | To, r,) oc exp(-ro0) exp(ri log 0) 
1 
K(t0,t}) 
268 
5 Inference 
assuming the existence of 
/•oc 
K(t0,t1)= / 0Tiexp(-TO0)d0. 
Jo 
Writing a = tx +1 and 0 = r0 we havep(0 | a, (3) oc #Q_1 exp(-/30) and hence, comparing 
with the definition of a gamma density, 
p(0|7b,Ti)=p(0|a)/3)=Ga(0|a)/?), a > 0, 0 > 0. 
Example 5.6. (Normal likelihood; normal-gamma prior). The normal likelihood, 
with unknown mean and precision, has the form 
p{xi,..., xn | n, A) = Jl ( — J exp ^ --[Xi: - ^)2 | 
= (27r)-/2 [a1/2 exp (-^2)]"exp <La|> - £ |>2} , 
so that, by Proposition 5.4, the conjugate prior density for 6 = (/i, A) is given by 
P(/i,A|to,Ti,t2) oc 
A1/2 
expf--A^2j expj^Ar, - -Xr2\ 
* -ACo-^expt-l^A^Aiexpj-^L-Iiyl, 
,Tl,T2J [ 2 V To/ J 
assuming the existence of K(r0, rlt r2), given by 
f^-H{/>4^K)H 
cfyi > dX. 
Writing a = j(r0 + 1), 0 = |r2, 7 = ti/t0, and comparing with the definition of a 
normal-gamma density, we have 
p(h,X\t0,t1,t2) =p(/i,A|a,/?,7) 
= Ng(^,A|a,/S,7) 
= N(^|7,A(2a-l))Ga(A|a,/S), 
with a > i /? > 0, 7 € 3?. 
5.2 Conjugate Analysis 269 
5.2.2 Canonical Conjugate Analysis 
Conjugate prior density families were motivated by considerations of tractability 
in implementing the Bayesian paradigm. The following proposition demonstrates 
that, in the case of regular exponential family likelihoods and conjugate prior  
densities, the analytic forms of the joint posterior and predictive densities which underlie 
any form of inference summary or decision making are easily identified. 
Proposition 5.5. (Conjugate analysis for regular exponential families). 
For the exponential family likelihood and conjugate prior density of  
Proposition 5.4: 
(i) the posterior density for 6 is 
where / p(0\x,t)^ p(6\r+ tn(x)) n 
T + tn(x) = I r0 + n, Ti + ^ hl(xj)' ■ ■ ■ > Tk + X] hk(Xj) ) > 
(ii) the predictive density for future observables y = (j/i,... ,ym) is 
p(y \x,T)=p(y\r + tn(x)) 
ft K(T + tn(x)+tm(y)) 
where tm{y) = [m, YT=i hi(yi), • ■ ■. IXi hk{vi)l 
Proof. By Bayes' theorem, 
p{61 x, t) oc p(x I 0)p{61 t) 
ex [ff(0)]To+"exp J $>&(*) U + |>te) J \ 
<Xp(0\T + tn(x)), 
which proves (i). Moreover, 
p(y\x,T)= I p{y\0)p{0\x)dO 
Je 
m , 
= II fin) ■ iR(T + tnWT1 / [g(0)Y0+n+m 
{k / n m \ 1 
J2Ci<t>i{0) (n + J2hi(xj) + Y, Mm) ) \de 
ft ^K(r + tn(x)+tm(y)) 
which proves (ii). < 
270 
5 Inference 
Proposition 5.5(i) establishes that the conjugate family is closed under  
sampling, with respect to the corresponding exponential family likelihood, a concept 
which seems to be due to G. A. Barnard. This means that both the joint prior 
and posterior densities belong to the same, simply defined, family of distributions, 
the inference process being totally defined by the mapping r —> (r + tn(x)), 
under which the labelling parameters of the prior density are simply modified by 
the addition of the values of the sufficient statistic to form the labelling parameter 
of the posterior distribution. The inference process defined by Bayes' theorem is 
therefore reduced from the essentially infinite-dimensional problem of the  
transformation of density functions, to a simple, additive finite-dimensional transformation. 
Proposition 5.5(H) establishes that a similar, simplifying closure property holds for 
predictive densities. 
The forms arising in the conjugate analysis of anumber of standard exponential 
family forms are summarised in Appendix A. However, to provide some preliminary 
insights into the prior —> posterior —» predictive process described by Proposition 
5.5, we shall illustrate the general results by reconsidering Example 5.4. 
Example 5.4. (continued). With the Bernoulli likelihood written in its explicit  
exponential family form, and writing r„ = x\ H \-xn, the posterior density corresponding to 
the conjugate prior density, p(61 r0, n), is given by 
p(6 | X, To, Tl) OC P(X | Q)p(B | To, Ti) 
tx (1 - 0)n exp {log (j^g) rn\ (1 - 0Y° 
XeXP{l0g(l^)Tl} 
-(l-fl)^" 
r(r0(n)_+2) „ mTn(n) 
rXnCrO + iMroW-nH + i) 
xexpjlogf ^-—^ J r^n) I, 
wherer0(n) = T0+n, ri(n) = T\+rn, showing explicitly how the inference process reduces 
to the updating of the prior to posterior hyperparameters by the addition of the sufficient 
statistics, n and rn. 
Alternatively, we could proceed on the basis of the original representation of the  
Bernoulli likelihood, combining it directly with the familiar beta prior density, Be(# | a, /3), so 
that 
p{0\x,a,p)<xp(x\e)p(e\a,p) 
K6rn(l-6)n-rnOa-\l~O)0-1 
t|Qn + Pn) flan-l/i _ Q\Bn~l 
r(an)r(&r [ } ' 
where a„ = a + r„, 0„ = 0 + n — rn and, again, the process reduces to the updating of the 
prior to posterior hyperparameters. 
5.2 Conjugate Analysis 
271 
Clearly, the two notational forms and procedures used in the example are 
equivalent. Using the standard exponential family form has the advantage of  
displaying the simple hyperparameter updating by the addition of the sufficient  
statistics. However, the second form seems much less cumbersome notationally and is 
more transparently interpretable and memorable in terms of the beta density. 
In general, when analysing particular models we shall work in terms of  
whatever functional representation seems best suited to the task in hand. 
Example 5.4. (continued). Instead of working with the original Bernoulli likelihood, 
p(xi,... ,xn\9), we could, of course, work with a likelihood defined in terms of the sufficient 
statistic (n, rn). In particular, if either n or rn were ancillary, we would use one or other of 
p(rn | n, 6) or p{n \ r„, 9) and, in either case, 
p(6\n,rn,a,l3) oc 0r"(l - 0)n-r"0°-1(l - 0)""1. 
Taking the binomial form, p(rn | n, 6), the prior to posterior operation defined by Bayes' 
theorem can be simply expressed, in terms of the notation introduced in Section 3.2.4, as 
Bi(r„ | 0, n) ® Be(0 \a,/3)= Be(0 | a + r„, 0 + n - rn). 
The predictive density for future Bernoulli observables, which we denote by 
y = (i/ii • • ■. Vm) = (xn+i,..., xn+m), 
is also easily derived. Writing r'm = y\ -\ 1- ym, we see that 
p(y | x, a, p) = p(y | a„, /3n) 
= I p(y\e)P(e\an,0n)de 
Jo 
- r(°<n+0n) f gan+r'm-l(1_g]3n+m-r'm~ldg 
r(an)T(0n) 1 (1 "] 
_ T{an + l3n)T(an+m)r(l3n+m) 
r(an)r(/3„)r(an+m + /3n+m) ' 
where 
an+m = a„ + r'm=a + rn + r'm, 
Pn+m = /3n + m-r'm= /3 + (n + m)-(r„ + r'J, 
a result which also could be obtained directly from Proposition 5.5(ii). 
If, instead, we were interested in the predictive density for r'm, it easily follows that 
p(r'm | an, pn,m) =- f p(r'm | m, 9)f{9 \ a„, A) dB 
Jo 
= fQ(™:^p(y\o)p{6\an,0n)de 
= (™JP(y\<*n,0n). 
272 
5 Inference 
Comparison with Section 3.2.2 reveals this predictive density to have the binomial-beta form, 
Bb(r'm\an,pn,m). 
The particular case m = 1 is of some interest, since p(r'm = l\an,/3„,m= 1) is then 
the predictive probability assigned to a success on the (n + l)th trial, given rn observed 
successes in the first n trials and an initial Be(# | a, 0) belief about the limiting relative 
frequency of successes, 6. 
We see immediately, on substituting into the above, that 
p{r'm = 11 a„, 0„, m = 1) = j-r- = E{01 an, j3n), 
using the fact that T(t + 1) = tT(t) and recalling, from Section 3.2.2, the form of the mean 
of a beta distribution. 
With respect to quadratic loss, E{6 \ an, 0n) = (a + rn)/(a + 0 + n) is the optimal 
estimate of 8 given current information, and the above result demonstrates that this should 
serve as the evaluation of the probability of a success on the next trial. In the case a = Q = 1 
this evaluation becomes (rn + l)/(n + 2), which is the celebrated Laplace's rule of  
succession (Laplace, 1812), which has served historically to stimulate considerable philosophical 
debate about the nature of inductive inference. We shall consider this problem further in 
Example 5.16 of Section 5.4.4. For an elementary, but insightful, account of Bayesian 
inference for the Bernoulli case, see Lindley and Phillips (1976). 
In presenting the basic ideas of conjugate analysis, we used the following 
notation for the ^-parameter exponential family and corresponding prior form: 
p(x | 6) = f(x)g(0) exp { > c^(6%(z) } , x € X, 
P{6\t) = \K{T)]-l\g{0)Y*w\ $>(0h > ° € e> 
and 
the latter being defined for r such that K(t) < oo. 
From a notational perspective (cf. Definition 4.12), we can obtain considerable 
simplification by defining ip = (ipi,...,ipk), V = (Vi, ■ ■ ■ ,Vk), where rpt = 
Ci<f>i(0) and r/j = h(xi), i = 1,..., k, together with prior hyperparameters no, y0, 
so that these forms become 
P(y I ^) = a(y) exp {y1^ - b(ip)}, y GY, 
PWI "o, y0) = c(n0, y0) exp {noyfy - n0b(ip)}, ip € *, 
for appropriately defined Y, ^ and real-valued functions a, b and c. We shall refer 
to these (Definition 4.12) as the canonical (or natural) forms of the exponential 
family and its conjugate prior family. If * = Rk, we require n0 > 0, y0 € Y 
5.2 Conjugate Analysis 273 
in order for p(i/» | n0, y0) to be a proper density; for * ^ 5ft*\ the situation is 
somewhat more complicated (see Diaconis and Ylvisaker, 1979, for details). We 
shall typically assume that * consists of all tp such that fY p(y \ tp)dy = 1 and 
that 6(i/j) is continuously differentiable and strictly convex throughout the interior 
of*. 
The motivation for choosing n0, y0 as notation for the prior hyperparameter 
is partly clarified by the following proposition and becomes even clearer in the 
context of Proposition 5.7. 
Proposition 5.6. (Canonical conjugate analysis). Ifyly...,yn are the  
values ofy resulting from a random sample of size nfrom the canonical  
exponential family parametric model, p(y | 1/7), then the posterior density  
corresponding to the canonical conjugate form, p(ip | no, y0), is given by 
noy0 + nyn 
p{^\m,y0,yl,...,yn) = p[4> 
where yn = £?=1 yjn. 
Proof. 
n0 + n, 
(n + n0) 
P(ip I "0, y0. Vi, ■ ■ ■, Vn) « Ylp(Vi I 4>)p(4> I "0, Vo) 
i=l 
oc exp {ny^xp — nb(ip)} 
xexp{n02/0i/>-no&(i/>)} 
oc exp {(n0y0 + ny„Yip - (n0 + n)b(ip)} 
and the result follows. 
Example 5.4. (continued). In the case of the Bernoulli parametric model, we have 
seen earlier that the pairing of the parametric model and conjugate prior can be expressed as 
H* I 0) ^1 - 0) exp jzlog (^0) } 
p(0 I ro, 71) = [K(t)}-\1 ~ 0)T° exp |n log (j^j } , 
The canonical forms in this case are obtained by setting 
y = x, ^ = log (j-Z-^) • a(2/) = 1< &(V0 = Ml + e*), 
, . r(n0 + 2) 
C{TUhyo> r(not/o + l)r(no - no2/o + 1) ' 
and, hence, the posterior distribution of the canonical parameter i> is given by 
P(il>\n0,y0,yu---,yn) oc exp 
/ , \ 1 "olfo + ny„ , , , n 
(no + n)\ ■ V - Kw) 
' n + no 
274 
5 Inference 
Example 5.5. (continued). In the case of the Poisson parametric model, we have 
seen earlier that the pairings of the parametric model and conjugate form can be expressed 
as 
p(x | 6) = — exp(-0) exp(xlog#) 
P(0 I ro, n) = \K(t)}-1 exp(-ro0) expfa log*), 
The canonical forms in this case are obtained by setting 
1 
„M>+1 
y = x, i\j = log0, a(y) = — > b(ip) = e*, c(no, yQ) = 
y- • ■- ■ r(jto + i) 
The posterior distribution of the canonical parameter ip is now immediately given by  
Proposition 5.6. 
Example 5.6. (continued). In the case of the normal parametric model, we have seen 
earlier that the pairings of the parametric model and conjugate form can be expressed as 
A1/2 exp I --A^2 J exp < x(Xfi) — -x2\ i 
Xi/2exp(--\fj2) exp|n(A/i)--r2Al 
p(/i,A|r0,ri,r2) = [K(t)}"1 
The canonical forms in this case are obtained by setting 
y = (2/1,2/2) = (x,x2), v = W»i.V»2)= (A^>~2A) 
a(y) = (27T)-1/2 
6(V)=iog(-2^)-1/2-rf 
c("o,9o) = ( — r/W , .xx 
V"o/ r (i(no + l)) 
Again, the posterior distribution of the canonical parameters t/> = (V>i, ^2) is now  
immediately given by Proposition 5.6. 
For specific applications, the choice of the representation of the parametric 
model and conjugate prior forms is typically guided by the ease of interpretation 
of the parametrisations adopted. Example 5.6 above suffices to demonstrate that 
the canonical forms may be very unappealing. From a theoretical perspective, 
however, the canonical representation often provides valuable unifying insight, as 
in Proposition 5.6, where the economy of notation makes it straightforward to 
demonstrate that the learning process just involves a simple weighted average, 
n0y0+nyn ^ 
n0 + n 
of prior and sample information. Again using the canonical forms, we can give a 
more precise characterisation of this weighted average. 
5.2 Conjugate Analysis 
275 
Proposition 5.7. (Weighted average form of posterior expectation). 
Ifyl,..., yn are the values of y resulting from a random sample of size n 
from the canonical exponential family parametric model, 
p(y | ip) = a(y) exp {ylip - b(ip)} , 
with canonical conjugate prior p(ip | n0, y0), then 
E [Vb(ip) | n0, y0, y] = nyn + (1 - ir)y0, 
where 
Proof By Proposition 5.6, it suffices to prove that E(Vb(xp) | n0, y0) = y0. 
But 
"otyo-^CVfcWIno.yo)] = / no{Vo ~ V6(^))p(^l no,y0)d*l> 
= / Vp(ip\n0,y0)dip. 
This establishes the result. < 
Proposition 5.7 reveals, in this natural conjugate setting, that the posterior 
expectation of V6(i/>), that is its Bayes estimate with respect to quadratic loss 
(see Proposition 5.2), is a weighted average of y0 and yn. The former is the prior 
estimate of Vb(tp); the latter can be viewed as an intuitively "natural" sample-based 
estimate of V6(^), since 
E(y | ^) - Vb(4>) = J(y - Vb(4>))p(y | i>)dy 
= j Vp(y \tp)dy = Vp(y\ tp)dy = 0 
and hence E(y | ip) = E(yn | ip) = Vb(xp). 
For any given prior hyperparameters, (n0, y0), as the sample size n becomes 
large, the weight, it, tends to one and the sample-based information dominates the 
posterior. In this context, we make an important point alluded to in our discussion of 
"objectivity and subjectivity", in Section 4.8.2. Namely, that in the stylised setting 
of a group of individuals agreeing on an exponential family parametric form, but 
assigning different conjugate priors, a sufficiently large sample will lead to more 
or less identical posterior beliefs. Statements based on the latter might well, in 
common parlance, be claimed to be "objective". One should always be aware, 
however, that this is no more than a conventional way of indicating a subjective 
consensus, resulting from a large amount of data processed in the light of a central 
core of shared assumptions. 
276 
5 Inference 
Proposition 5.7 shows that conjugate priors for exponential family parameters 
imply that posterior expectations are linear functions of the sufficient statistics. 
It is interesting to ask whether other forms of prior specification can also lead to 
linear posterior expectations. Or, more generally, whether knowing or  
constraining posterior moments to be of some simple algebraic form suffices to characterise 
possible families of prior distributions. These kinds of questions are considered 
in detail in, for example, Diaconis and Ylvisaker (1979) and Goel and DeGroot 
(1980). In particular, it can be shown, under some regularity conditions, that, 
for continuous exponential families, linearity of the posterior expectation does 
imply that the prior must be conjugate. 
The weighted average form of posterior mean, 
E[V6W|n0,yo,y] = »±^, 
n0 + n 
obtained in Proposition 5.7, and also appearing explicitly in the prior to posterior 
updating process given in Proposition 5.6 makes clear that the prior parameter, n0, 
attached to the prior mean, y0 for V6(i/>), plays an analogous role to the sample 
size, n, attached to the data mean yn. The choice of an n0 which is large relative to 
n thus implies that the prior will dominate the data in determining the posterior (see, 
however, Section 5.6.3 for illustration of why a weighted-average form might not 
be desirable). Conversely, the choice of an no which is small relative to n ensures 
that the form of the posterior is essentially determined by the data. In particular, 
this suggests that a tractable analysis which "lets the data speak for themselves" can 
be obtained by letting n0 —> 0. Clearly, however, this has to be regarded as simply a 
convenient approximation to the posterior that would have been obtained from the 
choice of a prior with small, but positive n0. The choice n0 = 0 typically implies 
a form of p(ip \ n0,y0) which does not integrate to unity (a so-called improper 
density) and thus cannot be interpreted as representing an actual prior belief. The 
following example illustrates this use of limiting, improper conjugate priors in the 
context of the Bernoulli parametric model with beta conjugate prior, using standard 
rather than canonical forms for the parametric models and prior densities. 
Example 5.4. (continued). We have seen that if rn = X\ + • ■ ■ + xn denotes the 
number of successes in n Bernoulli trials, the conjugate beta prior density, Be(# | a, 0), for 
the limiting relative frequency of successes, 9, leads to a Be(# | a + rn, 0 + n — r„) posterior 
for 0, which has expectation 
a + rn /T„\ ( a \ 
___=x(_j+(l_x)|__j, 
where 7r = (a + /3 + n)_1n, providing a weighted average between the prior mean for 6 and 
the frequency estimate provided by the data. In this notation, no —» 0 corresponds to a —► 0, 
5.2 Conjugate Analysis 
277 
0 —> 0, which implies a Be(# | rn,n — rn) approximation to the posterior distribution, having 
expectation rn/n. The limiting prior form, however, would be 
p(9\a = 0,0 = 0) x 9~1(1 - 9)~\ 
which is not a proper density. As a technique for arriving at the approximate posterior 
distribution, it is certainly convenient to make formal use of Bayes' theorem with this 
improper form playing the role of a prior, since 
p{9 | a = 0, 0 = 0, n, r„) oc p(rn | n9)p(9 \a = 0,0 = 0) 
K6T"{l-0)n-rn0-l{\-6)-1 
ocBe(0|r„,n-r„). 
It is important to recognise, however, that this is merely an approximation device and 
in no way justifies regarding p(9 \ a = 0,0 = 0) as having any special significance as a 
representation of "prior ignorance". Clearly, any choice of a, 0 small compared with rn, 
n — rn (for example, a = 0=\ora = 8 = l for typical values of rn,n — rn) will lead to 
an almost identical posterior distribution for 9. 
A further problem of interpretation arises if we consider inferences for functions of 9. 
Consider, for example, the choice a = 0 = 1, which implies a uniform prior density for 
9. At an intuitive level, it might be argued that this represents "complete ignorance" about 
9, which should, presumably, entail "complete ignorance" about any function, g(9), of 9. 
However, p(9) uniform implies that p{g{9)) is not uniform. This makes it clear that ad hoc 
intuitive notions of "ignorance", or of what constitutes a "non-informative" prior distribution 
(in some sense), cannot be relied upon. There is a need for a more formal analysis of the 
concept and this will be given in Section 5.4, with further discussion in Section 5.6.2. 
Proposition 5.2 established the general forms of Bayes estimates for some 
commonly used loss functions. Proposition 5.7 provided further insight into the 
(posterior mean) form arising from quadratic loss in the case of an exponential 
family parametric model with conjugate prior. Within this latter framework, the 
following development, based closely on Gutierrez-Pena (1992), provides further 
insight into how the posterior mode can be justified as a Bayes estimate. 
We recall, from the discussion preceding Proposition 5.6, the canonical forms 
of the ^-parameter exponential family and its corresponding conjugate prior: 
p(y\ip) = a(y)exp{yttp-b(^)} , y£Y 
and 
PW\no,Vo) = c(n0,y0)exp {noyfy - n0b(^)} , ipGV, 
for appropriately defined Y, ^ and real-valued functions a, b and c. 
278 5 Inference 
Considerp(ip\no,y0) and define d(s,t) = — logc(s,s~1t), with s > 0 and 
t € y. Further define 
'dd(s.t) ad(s,t)l( 
Vd(s,t) 
d*i ' eft* 
= [di(s,t),...,4(s,t)]t 
and do(s,t) = dd(s,t)/ds. As a final preliminary, recall the logarithmic  
divergence measure 
S(B\0O)= fp(x\0)\og^jprdX 
J p(x\0o) 
between two distributions p(x\6) andp(x\6o). We can now establish the following 
technical results. 
Proposition 5.8. (Logarithmic divergence between conjugate distributions). 
With respect to the canonical form of the k-parameter exponential family and 
its corresponding conjugate prior: 
(i) 6(i/,\il,0) = bty0) - b(i/,) + {il>- 4>0yVb(4>); 
(ii) E[6(ip\ip0)] = d0(n0, n0y0) + b(ip0) 
+n0-1{k + [Vd(n0,noy0) - Vo]'"ol/o}- 
Proof. From the definition of logarithmic divergence we see that 
6W\1>0) = b(xj>0) - b^) + (^ - ^oYEy^ly], 
and (i) follows. Moreover, 
E[6W\il,0)] = b(1>0) ~ E[b(4,)] + E[lfVb{i>)\ - ^E[VbW)\. 
Differentiation of the identity 
log / exp{tlip - sb(ip)}dil> = d(s,t), 
with respect to s, establishes straightforwardly that 
E[b(tp)] = -d0(n0,n0y0). 
Recalling that E[Vb(tp)] = y0, we can write, for i = 1,..., k, 
log / bi(ip)exp{tlip - sb(xp)}dil> = logU - logc(s,s~lt) — logs. 
Differentiating this identity with respect to U, and interchanging the order of  
differentiation and integration, we see that 
A'0j6i(i/;)c(s,s-1t)exp{tV - sb(^)}dtp = s_1[l + di(s,t)U], 
for i = 1,..., k, so that 
EtyVbty)] = riol[k + Vdino^oyoYinoyo)] - n^Vofao.l/o) 
and (ii) follows. < 
5.2 Conjugate Analysis 
279 
This result now enables us to establish easily the main result of interest. 
Proposition 5.9. (Conjugateposterior modes as Bayes estimates). 
With respect to the loss function l(a,tp) = 8(ip\a), the Bayes estimate for 
ip, derived from independent observations yl,...,yn from the canonical 
k-parameter exponential family p(y\ip) and corresponding conjugate prior 
p(ip\n0, y0), is the posterior mode, ip*, which satisfies 
Vb(ip*) = (no + n)~l(n0y0 + nyn), 
withyn = n-l(yl + --- + yn). 
Proof. We note first (see the proof of Proposition 5.6) that the logarithm of 
the posterior density is given by 
constant + (n0y0 + ny^tp - (n0 + n)b(ip), 
from which the claimed estimating equation for the posterior mode, ip*, is  
immediately obtained. The result now follows by noting that the same equation arises in the 
minimisation of (ii) of Proposition 5.8, with no + n replacing n0, and noy0 + nyn 
replacing n0y0- < 
For a recent discussion of conjugate priors for exponential families, see Con- 
sonni and Veronese (1992b). In complex problems, conjugate priors may have 
strong, unsuspected implications; for an example, see Dawid (1988a). 
5.2.3 Approximations with Conjugate Families 
Our main motivation in considering conjugate priors for exponential families has 
been to provide tractable prior to posterior (or predictive) analysis. At the same 
time, we might hope that the conjugate family for a particular parametric model 
would contain a sufficiently rich range of prior density "shapes" to enable one 
to approximate reasonably closely any particular actual prior belief function of 
interest. The next example shows that might well not be the case. However, it 
also indicates how, with a suitable extension of the conjugate family idea, we can 
achieve both tractability and the ability to approximate closely any actual beliefs. 
Example 5.7. (The spun coin ). Diaconis and Yl visaker (1979) highlight the fact that, 
whereas a tossed coin typically generates equal long-run frequencies of heads and tails, this 
is not at all the case if a coin is spun on its edge. Experience suggests that these long-run 
frequencies often turn out for some coins to be in the ratio 2:1 or 1:2, and for other coins 
even as extreme as 1:4. In addition, some coins do appear to behave symmetrically. 
Let us consider the repeated spinning under perceived "identical conditions" of a given 
coin, about which we have no specific information beyond the general background set out 
280 
5 Inference 
above. Under the circumstances specified, suppose we judge the sequence of outcomes to 
be exchangeable, so that a Bernoulli parametric model, together with a prior density for the 
long-run frequency of heads, completely specifies our belief model. How might we represent 
this prior density mathematically? 
We are immediately struck by two things: first, in the light of the information given, any 
realistic prior shape will be at least bimodal, and possibly trimodal; secondly, the conjugate 
family for the Bernoulli parametric model is the beta family (see Example 5.4), which 
does not contain bimodal densities. It appears, therefore, that an insistence on tractability, 
in the sense of restricting ourselves to conjugate priors, would preclude an honest prior 
specification. 
However, we can easily generate multimodal shapes by considering mixtures of beta 
densities, 
771 
p(9|7r,a,/3) = ^7riBe(f9|ai,/3,), 
t=l 
with mixing weights 7T; > 0,7Ti+- • - + 7rm = 1, attached to a selection of conjugate densities, 
Be(01 ai, ft), i = 1,..., m. Figure 5.2 displays the prior density resulting from the mixture 
0.5 Be(0110,20) + 0.2 Be(0 115,15) + 0.3 Be(01 20,10), 
which, among other things, reflects a judgement that about 20% of coins seem to behave 
symmetrically and most of the rest tend to lead to 2:1 or 1:2 ratios, with somewhat more of 
the latter than the former. 
Suppose now that we observe n outcomes x = (x\,... ,xn) and that these result in 
rn = x\ + ■ ■ ■ + xn heads, so that 
n 
p(xu-..,xn i e) = \\exi{\ - oy-*i = ern{i - e)n~rn. 
Considering the general mixture prior form 
771 
p(9|7r,a,/3)=^7r,Be(f9|a„A), 
we easily see from Bayes' theorem that 
p(0|7r,a,/3,a:)=p(0|7r*,a*,/r), 
where 
a*=ai + rn, /?* = # +ra - rn 
and 
7T* OC 7T; / 6»r"(l - 6>)"-r"Be(0 | Q,-, ft) dd 
Jo 
T(ai + ft) r(q-)r(ff) 
0C7rT(ai)r(/3i)'r(a*+/3;) ' 
5.2 Conjugate Analysis 
281 
so that the resulting posterior density, 
m 
p(017T,a,/3, a) = ]>>* Be(01 a*,/?*), 
i = \ 
is itself a mixture of m beta components. This establishes that the general mixture class of 
beta densities is closed under sampling with respect to the Bernoulli model. 
In the case considered above, suppose that the spun coin results in 3 heads after 10 
spins and 14 heads after 50 spins. The suggested prior density corresponds to m = 3, 
tt = (0.5,0.2,0.3), a = (10,15,20), /3 = (20,15,10). 
1 p(e\rn = 14, n = 50) 
Figure 5.2 Prior and posteriors from a three-component beta mixture prior density 
Detailed calculation yields: 
forra = 10, rn = 3; tt* = (0.77.0.16,0.07), 
a* = (13,18,23), /3* = (27,22,17) 
for n = 50, r„ = 14; tt* = (0.90, 0.09, 0.006), 
a* = (24,29,34), /3* = (56,51.46), 
and the resulting posterior densities are shown in Figure 5.2. 
282 
5 Inference 
This example demonstrates that, at least in the case of the Bernoulli parametric 
model and the beta conjugate family, the use of mixtures of conjugate densities 
both maintains the tractability of the analysis and provides a great deal of flexibility 
in approximating actual forms of prior belief. In fact, the same is true for any 
exponential family model and corresponding conjugate family, as we show in the 
following. 
Proposition 5.10. (Mixtures of conjugate priors). Let x = (x\,..., x„) be 
a random sample from a regular exponential family distribution such that 
n f m / n \ ~\ 
P(x\d) = n/te)fo(0)]"exp EC^W EM*;) \ 
and let 
m 
p(<9|7T,Tl,...,Tm) = E7r'P(0lT')' 
1=1 
where, for I = 1,..., m, 
p(0\Tt) = [K(Tl)]-1[g(d)]Tioe^> I Ec^(0)r<* f 
are elements of the conjugate family. Then 
m 
p(0\ir,TU..., rm, x) = p(0 | 7T*, t\, ..., t*J = E «*P(0 I r*t), 
1=1 
where, with tn(x) = |n, £"=1 hi(xj),..., ^=1 hk(j)} 
T\ = Tt + tn(x), 
and 
"f«^n/^)|gf- 
Proof The results follows straightforwardly from B ayes' theorem and  
Proposition 5.5. <. 
5.2 Conjugate Analysis 
283 
It is interesting to ask just how flexible mixtures of conjugate prior are. The 
answer is that any prior density for an exponential family parameter can be  
approximated arbitrarily closely by such a mixture, as shown by Dalai and Hall (1983), 
and Diaconis and Ylvisaker (1985). However, their analyses do not provide a  
constructive mechanism for building up such a mixture. In practice, we are left with 
having to judge when a particular tractable choice, typically a conjugate form, a 
limiting conjugate form, or a mixture of conjugate forms, is "good enough," in the 
sense that probability statements based on the resulting posterior will not differ 
radically from the statements that would have resulted from using a more honest, 
but difficult to specify or intractable, prior. 
The following result provides some guidance, in a much more general setting 
than that of conjugate mixtures, as to when an "approximate" (possibly improper) 
prior may be safely used in place of an "honest" prior. 
Proposition 5.11. (Prior approximation). Suppose that a belief model is  
defined by p(x 19) and p{9), 9 6 9 and that q{9) is a non-negative function 
such that q(x) — jep{x \ 6)q{6)d0 < oo, where, for some Q0 C 9 and 
(a) 1 < p(9)/q(9) < 1 + a, for all 9 e 90, 
(b) p(9)/q(9) < (3, for all 6 e 9. 
Letp = fe p(91 x)d9, q = Je q(9 \ x)d9, and 
q{91 x) = p(x | 9)q(9)/fp(x 19)q{9)d9. Then, 
0) (l-p)/p<0(l-q)/q 
(ii) q < p(x)/q(x) < (1 + a)/p 
(iii) for all 9eQ, p{91 x)/q(9 \ x) < \p(9)/q(9)]/q < (3/q 
(iv) for all 9 e 90, p/(l + a) < p{9 \ x)/q{9 \ x) < (1 + a)/q 
(v) fore = max{(l —p), (1 - q)} and f : 9 -> $t such that \f(9) \ < m, 
m 
-11 / f{9)p{91 x)d9 - f f(9)q(91 x)d9 
<a + 3e 
Proof. (Dickey, 1976). Part (i) clearly follows from 
l_p _fecP(x\9)p(0)dO 1_q 
~T~ ~ JeQp(x\9)P(9)d9 ~ P~q~ ' 
Clearly, 
p(x)> [ p(x\9)p(9)d9> f p(x\9)q(9)d9 = q-q(x), 
Je0 Je0 
284 
5 Inference 
q{x) > [ q{x\0)q{6)d6 > -J— [ p(x\9)p(9)de = -^—p(x), 
Je0 i- + a JQ0 l + a 
which establishes (ii). Part (iii) follows from (b) and (ii), and part (iv) follows from 
(a) and (ii). Finally, 
m 
-l 
[ f(0)p(0\x)dO- f f{0)q{0\x)dO < f 
Je Je J 
p{6 | a;) - q{6 \ x) 
d9 
J 
Je{ 
J 
Je{ 
j 
Je, 
p(01 x) - q{8 | x) 
^w£rl 
I 
Jb[ 
p(6\x)-q(6\x)\dB 
p{6\x) 
d0 + 
J&A 
x 
'% 
dO 
q(6\x)l—-l 
dB+(l-p) + (l-q) (byiv) 
= (1 + a - q) + (1 - p) + (1 - q) < a + 3e, 
which proves (v). <| 
If, in the above, 60 is a subset of 6 with high probability under q(6 \ x) and a 
is chosen to be small and /3 not too large, so that q(6) provides a good approximation 
to p{6) within 0O and p{6) is nowhere much greater than q{6), then (i) implies that 
6o has high probability under p(6 \ x) and (ii), (iv) and (v) establish that both the 
respective predictive and posterior distributions, within 0O, and also the posterior 
expectations of bounded functions are very close. More specifically, if / is taken 
to be the indicator function of any subset 0* C 0, (v) implies that 
/ p{0\x)dO 
Je* 
I q{0\x)dO 
Je* 
< a + 3e, 
providing a bound on the inaccuracy of the posterior probability statement made 
using q{6 \ x) rather than p{6 \ x). 
Proposition 5.11 therefore asserts that if a mathematically convenient  
alternative, q(0), to the would-be honest prior, p(6), can be found, giving high posterior 
probability to a set 0O C9 within which it provides a good approximation to p{6) 
and such that it is nowhere orders of magnitude smaller than p{6) outside 0o, then 
q(6) may reasonably be used in place of p(6). 
In the case of 9 = 5ft, Figure 5.3 illustrates, in stylised form, a frequently 
occurring situation, where the choice q{6) = c, for some constant c, provides 
5.3 Asymptotic Analysis 
285 
Figure 5.3 Typical conditions for precise measurement 
a convenient approximation. In qualitative terms, the likelihood is highly peaked 
relative to p(9), which has little curvature in the region of non-negligible likelihood. 
In this situation of "precise measurement" (Savage, 1962), the choice of the 
function q(0) = c, for an appropriate constant c, clearly satisfies the conditions of 
Proposition 5.10 and we obtain 
p{6 | x) ~ q(6 | x) = 
p(x | 0)c _ p(x | 0) 
f^p(x\e)cde~ fxP(x\9)d9' 
the normalised likelihood function. 
The second of the implementation questions posed at the end of Section 5.1.6 
concerned the possibility of avoiding the need for precise mathematical  
representation of the prior density in situations where the information provided by the 
data is far greater than that implicit in the prior. The above analysis goes some way 
to answering that question; the following section provides a more detailed analysis. 
5.3 ASYMPTOTIC ANALYSIS 
In Chapter 4, we saw that in representations of belief models for observables  
involving a parametric model p(x \ 0) and a prior specification p{6), the parameter 6 
acquired an operational meaning as some form of strong law limit of observables. 
Given observations x = (x\,..., xn), the posterior distribution, p(9 \ x), then  
describes beliefs about that strong law limit in the light of the information provided 
by x\,... ,xn. To answer the second question posed at the end of Section 5.1.6, we 
286 
5 Inference 
now wish to examine various properties of p(0 \ x) as the number of observations 
increases; i.e., as n —► oo. Intuitively, we would hope that beliefs about 6 would 
become more and more concentrated around the "true" parameter value; i.e., the 
corresponding strong law limit. Under appropriate conditions, we shall see that 
this is, indeed, the case. 
5.3.1 Discrete Asymptotics 
We begin by considering the situation where 6 = {61,62, ■ ■ ■, } consists of a  
countable (possibly finite) set of values, such that the parametric model corresponding to 
the true parameter, 6t, is "distinguishable" from the others, in the sense that the  
logarithmic divergences, Jp(x \ 6t) \og[p(x \ 6t)/p(x | 0j)] dx are strictly larger than 
zero, for all i ^ t. 
Proposition 5.12. (Discrete asymptotics). Let x = (x\,..., xn) be  
observations for which a belief model is defined by the parametric model p(x \ 6), 
where 6 G 6 = {61,62,...}, and the prior p(6) = {p\,P2, ■ ■ ■}, Pi > 0, 
YliPi ~ 1- Suppose that 6t e 0 is the true value of 6 and that, for all i ^ t, 
~P(x I 0t) 
I 
p(x I 6t) log 
p(x 16i) 
dx > 0; 
then 
lim p(6t I x) = 1, lim pld, | x) = 0, i ^ t. 
n—KX n—KX 
Proof By Bayes' theorem, and assuming thatp(a;|0) = Yl"=l p(xi\6), 
p(x I 6i) 
p{6l \x)=pi 
p(x) 
Pi{p{x\6i)/p{x\6t)} 
ZiPi{p(x\0i)/p(x\et)} 
exp {logp^ + Si} 
Hi exp {logpi + Si}' 
where 
s* = £iog 
p(xj 1 et 
3=1 J 
Conditional on 0t, the latter is the sum of n independent identically distributed 
random quantities and hence, by the strong law of large numbers (see Section 
3.2.3), 
p(x I 8j 
.Pi?\0t)\ 
The right-hand side is negative for all i ^ t, and equals zero for i = t, so that, 
as n —» 00, St —> 0 and Si -> — 00 for i ^ t, which establishes the result. 4 
lim -Si= p(x\6t) log 
n—>oo 71 J 
dx. 
5.3 Asymptotic Analysis 
287 
An alternative way of expressing the result of Proposition 5.12, established 
for countable 0, is to say that the posterior distribution function for 0 ultimately 
degenerates to a step function with a single (unit) step at 0 = 6t. In fact, this 
result can be shown to hold, under suitable regularity conditions, for much more 
general forms of 6. However, the proofs require considerable measure-theoretic 
machinery and the reader is referred to Berk (1966, 1970) for details. 
A particularly interesting result is that if the true 0 is not in 6, the  
posterior degenerates onto the value in 0 which gives the parametric model closest in 
logarithmic divergence to the true model. 
5.3.2 Continuous Asymptotics 
Let us now consider what can be said in the case of general 6 about the forms of 
probability statements implied by p{6 \ x) for large n. Proceeding heuristically for 
the moment, without concern for precise regularity conditions, we note that, in the 
case of a parametric representation for an exchangeable sequence of observables, 
n 
P(8\x)<xp(0)Y[p(xl\8) 
oc exp {logp(6) + logp(a; | 0)} . 
If we now expand the two logarithmic terms about their respective maxima, m0 
and 0„, assumed to be determined by setting Vlogp(0) = 0, Vlogp(a; | 0) = 0, 
respectively, we obtain 
logp(0) = logp(mo) - -(0 - moyHo(0 - m0) + Ro 
iogP(x | e) = iogP(x 10„) -1(0 - enyH(en)(o - en) + K, 
where R0, Rn denote remainder terms and 
_ / d2logP(0)\ r ( d*\ogp{x\0)\ 
0=0n 
Assuming regularity conditions which ensure that Rq, R„ are small for large n, and 
ignoring constants of proportionality, we see that 
p(6 | x) « exp |-i(0 - mo)'Ho(0 - m0) - \(0 - 0„)(#(0„)(0 - 0n)\ 
oc exp I --(0 - mn)lHn{e - m„) \ > 
288 
5 Inference 
with 
Hn = Hq + H(9n) 
mn = H~l (H„m„ + H(On)bn) , 
where m0 (the prior mode) maximises p(9) and 6n (the maximum likelihood  
estimate) maximises p(x \ 6). The Hessian matrix, H(6n), measures the local  
curvature of the log-likelihood function at its maximum, 0n, and is often called the 
observed information matrix. 
This heuristic development thus suggests that p(6 \ x) will, for large n, tend 
to resemble a multivariate normal distribution, N*(0 | mn, Hn) (see Section 3.2.5) 
whose mean is a matrix weighted average of a prior (modal) estimate and an 
observation-based (maximum likelihood) estimate, and whose precision matrix 
is the sum of the prior precision matrix and the observed information matrix. 
Other approximations suggest themselves: for example, for large n the prior 
precision will tend to be small compared with the precision provided by the data 
and could be ignored. Also, since, by the strong law of large numbers, for all i, f, 
&\ogp{x\0)Y\ _ \l^(_&}ogp{xW\\ 
we see that H(0n) —» nl(0n), where 1(0), defined by 
is the so-called Fisher (or expected) information matrix. We might approximate 
p(0 | x), therefore, by either Nk(0 | 0n, H(Qn)) or N*(0 | 6n, nl(6n)), where k is 
the dimension of 0. 
In the case of 0 e 9 C 3?, 
H(e) = -^\ogP(x\e), 
so that the approximate posterior variance is the negative reciprocal of the rate of 
change of the first derivative of logp(a; | 6) in the neighbourhood of its maximum. 
Sharply peaked log-likelihoods imply small posterior uncertainty and vice-versa. 
There is a large literature on the regularity conditions required to justify  
mathematically the heuristics presented above. Those who have contributed to the field  
include: Laplace (1812), Jeffreys (1939/1961, Chapter 4), LeCam (1953,1956,1958, 
5.3 Asymptotic Analysis 
289 
1966, 1970, 1986), Lindley (1961b), Freedman (1963b, 1965), Walker (1969), 
Chao (1970), Dawid (1970), DeGroot (1970, Chapter 10), Ibragimov and Hasmin- 
ski (1973), Heyde and Johnstone (1979), Hartigan (1983, Chapter 4), Bermudez 
(1985), Chen (1985), Sweeting and Adekola (1987), Fu and Kass (1988), Fraser 
and McDunnough (1989), Sweeting (1992) and Ghosh etal. (1994). Related work 
on higher-order expansion approximations in which the normal appears as a leading 
term includes that of Hartigan (1965), Johnson (1967, 1970), Johnson and Ladalla 
(1979) and Crowder (1988). The account given below is based on Chen (1985). 
In what follows, we assume that 0 e 0 C 3tk and that {pn(9),n = 1, 
2,...} is a sequence of posterior densities for 6, typically of the form pn(6) = 
p{6 | X\,..., xn), derived from an exchangeable sequence with parametric model 
p(x | 0) and prior p(9), although the mathematical development to be given does 
not require this. We define Ln(0) = logpn(0), and assume throughout that, for 
every n, there is a strict local maximum, m„, of p„ (or, equivalently, Ln) satisfying: 
L'n(mn) = VLn{6) | e=mn = 0 
and implying the existence and positive-definiteness of 
E„ = {-Ll(mn)Tl, 
where [IK)]^ = (92L„(0)/^^) \e=rrin. 
Defining \0\ = (fl'fl)1/2 and B6{6*) = {6 e 9; | 6 - 6* \ < 6}, we shall 
show that the following three basic conditions are sufficient to ensure a valid normal 
approximation for pn(0) in a small neighbourhood of mn as n becomes large. 
(cl) "Steepness". ~a1n —» 0 as n —» oo, where &„ is the largest eigenvalue of £„. 
(c2) "Smoothness". For any e > 0, there exists N and 6 > 0 such that, for any 
n> N and 6 6 Bs(mn), L'^(6) exists and satisfies 
J - A(e) < Ll{6){L"{mn)}-1 <I + A(e), 
where / is the k x k identity matrix and A(e) is a k x k symmetric positive- 
semidefinite matrix whose largest eigenvalue tends to zero as e —► 0. 
(c3) "Concentration". For any 6 > 0, fB ,jn ■. pn(Q)d6 —> 1 as n —» oo. 
Essentially, we shall see that (cl), (c2) together ensure that, for large n, inside 
a small neighbourhood of mn the function pn becomes highly peaked and behaves 
like the multivariate normal density kernel exp{—^ (O—rrinYT,^1 (9—mn)}. The 
final condition (c3) ensures that the probability outside any neighbourhood of mn 
becomes negligible. We do not require any assumption that the mn themselves 
converge, nor do we need to insist that mn be a global maximum of pn. We 
implicitly assume, however, that the limit of pn(mn) \ T,n \1//2 exists as n —» oo, 
and we shall now establish a bound for that limit. 
290 
5 Inference 
Proposition 5.13. (Bounded concentration). 
The conditions (cl), (c2) imply that 
lim Pn(mn) |E„|1/2 < (27r)-fc/2, 
n—>oo 
with equality if and only if(c3) holds. 
Proof. Given s > 0, consider n > N and 6 > 0 as given in (c2). Then, for 
any 0 £ Bf,(mn), a simple Taylor expansion establishes that 
pn(0) = pn(mn) exp {Ln(0) - Ln(mn)} 
= Pn(mn) exp | --(0 - m„)'(J + i^E"1^ - mn) \, 
where 
for some 0+ lying between 9 and mn. It follows that 
Pn(6) = [ Pn(0)d9 
JBg(mn) 
is bounded above by 
P:(6) = pn(mn) | E„ |l'2 | / - A(s) | -1/2 / exp {-\zlz} dz 
J \Z\<sn 
and below by 
P~(6) = p„(m„) | En | ^ | / + A(s) | -1/2 /" exp {-\zlz} dz, 
J\Z\<tn 
where sn = 8(1 - a(e)y/2/an and tn = 6(1 + a(e))ll2/an, with a2n(^) and 
a(£)(c*(£)) the largest (smallest) eigenvalues of £n and A(s), respectively, since, 
for any k x k matrix V, 
Bs/y(0) C [z; (z'Vz)1'2 <6}c Bs/y(0), 
where V (V?) are the largest (smallest) eigenvalues of V. 
Since (cl) implies that both sn and tn tend to infinity as n —> oo, we have 
\I-A(e)\1'2 lim Pn(6) < lim pn(mn)\En\1f2(2ir)k/2 
n—>oo n—>oo 
<|J + A(£)|]/2limP„(6), 
n—>oo 
and the required inequality follows from the fact that \I ± A(s) | —» 1 as £ —> 0 and 
^n(^) < 1 f°r a^ n- Clearly, we have equality if and only if lim^oo Pn(8) = 1, 
which is condition (c3). <j 
5.3 Asymptotic Analysis 
291 
We can now establish the main result, which may colloquially be stated as "9 
has an asymptotic posterior Nk(0\mn, E"1) distribution, where L'n(mn) = 0 and 
e;1 = -K(mn)r 
Proposition 5.14. (Asymptotic posterior normality). For each n, consider 
pn(-) as the density function of a random quantity 9n, and define, using the 
notation above, <j>n = Y,~l!2(9n — mn). Then, given (cl) and (c2), (c3) is 
a necessary and sufficient condition for <f>n to converge in distribution to (f>, 
where p(4>) = (27r)~*:/2exp{-^(</)}. 
Proof. Given (cl) and (c2), and writing b > a, for a, b £ 3?fc, to denote that 
all components of b — a are non-negative, it suffices to show that, as n —» oo, 
Pn(a <<j>n< b) -► P(a < <j> < b) if and only if (c3) holds. 
We first note that 
Pn(a <4>n< b) - f pn(0)d6, 
where, by (cl), for any 8 > 0 and sufficiently large n, 
G„ = {6>; T}J2a <(0- mn) < T}J2b) c Bs(mn). 
It then follows, by a similar argument to that used in Proposition 5.13, that, 
for any s > 0, Pn(a < 4>n < b) is bounded above by 
Pn(mn) \I - A(e)\~1/2 |E„|1/2 f exp {-\zlz} dz, 
JZ(e) 
where 
Z(e) = [z; [I - A{e)]1'2 a < z < [I - A(e)}1'2 b) , 
and is bounded below by a similar quantity with +A(s) in place of —A{e). 
Given (cl), (c2), as s —► 0 we have 
lim Pn(a <4>n<b)= lim pn(mn) \ E„ |1/2 / exp {-Wz} dz, 
n^oo n^co Jz(0) 
where Z(0) = {z; a < z < b}. The result follows from Proposition 5.13. < 
292 
5 Inference 
Conditions (cl) and (c2) are often relatively easy to check in specific  
applications, but (c3) may not be so directly accessible. It is useful therefore to have 
available alternative conditions which, given (cl), (c2), imply (c3). Two such are 
provided by the following: 
(c4) For any 6 > 0, there exists an integer N and c, d e 3?+ such that, for any 
n> N and 9 0 Bs(mn), 
Ln(0) - Ln(mn) <-c{(0- mnY-E-'iO - mn)}d . 
(c5) As (c4), but, with G(9) = log g{0) for some density (or normalisable positive 
function) g(0) over ©, 
Ln(0) - Ln(mn) < -c |E„rd + G(0). 
Proposition 5.15. (Alternative conditions). Given (cl), (c2), either (c4) or 
(c5) implies (c3). 
Proof. It is straightforward to verify that 
f pn(e)de<pn(mn)\i:n\l/2 f eW{-c{ztz)d}dz, 
Je-B6(mn) J\z\ >s/wn 
given (c4), and similarly, that 
/ Pn(0)d6 < Pn(mn) |E„|1/2 |E„|-1/2 exp i-c \Znld\ , 
Je-B6(m„) L > 
given (c4). 
Sincepn(mn) \ E„ | ll2 isbounded (Proposition 5.11) and the remaining terms 
or the right-hand side clearly tend to zero, it follows that the left-hand side tends to 
zero as n —* oo. < 
To understand better the relative ease of checking (c4) or (c5) in applications, 
we note that, if pn(0) is based on data x, 
Ln{6) = \ogp{6) + logp(x 19) - logp(x), 
so that Ln (0)—Ln (mn) does not involve the, often intractable, normalising constant 
p(x). Moreover, (c4) does not even require the use of a proper prior for the vector 8. 
We shall illustrate the use of (c4) for the general case of canonical conjugate 
analysis for exponential families. 
5.3 Asymptotic Analysis 
293 
Proposition 5.16. (Asymptotic normality under conjugate analysis). 
Suppose that ylt..., yn are data resulting from a random sample of size n 
from the canonical exponential family form 
p(y | ij)) = a(y) exp {y V - b(ip)} 
with canonical conjugate prior density 
P(i/> \n0,y0)= c(n0, y0) exp {noy^ - no6(i/>)} . 
For each n, consider the posterior density 
Pn{iJ>) = P(ij> I no + n, n0y0 + nyn), 
with yn = J3"=1 Vi/n, to be the density function for a random quantity ipn, 
and define 4>n = T.~l^{xj}n - b'{mn)), where 
b'{mn) = V6(</>) 
(W(m w (fM.\ 
= rcol/o + nyn 
%l>=mn n0 + n 
(n0 + n) (E„)i.. 
%(>=mn 
Then <pn converges in distribution to (j>, where 
p(^) = (27r)-';/2exp{-^^}. 
Proof. Colloquially, we have to prove that ip has an asymptotic posterior 
Nfc(^> | b(mn), S"1) distribution, where b'(m„) = (n0 + n)-1(n02/0 + nVn) m^ 
E"1 = (n0 + n)~lb"{mn). From a mathematical perspective, 
pn(xl>) oc exp {(n0 + n)h(xp)} , 
where h(tp) = [6'(m„)]'^ - b{xji), with b(xj)) a continuously differentiable and 
strictly convex function (see Section 5.2.2). It follows that, for each n, p„(tp) is 
unimodal with a maximum at ij> = mn satisfying Vh(mn) = 0. By the strict 
concavity of h(-), for any 6 > 0 and 6 & Bs(mn), we have, for some ip+ between 
tp and m„, with angle 0 between ip — mn and V/i(i/>+), 
h(t/>) - /!(m„) = (</> - mn)(V/i(</>+) 
= |V —m„| | Vh(ip+) | cos# 
< -c | ip - rrin |, 
294 5 Inference 
for c = inf { | Vh(tp+) \; t/> <£ B6(mn)} > 0. It follows that 
Ln(x/)) - Ln(mn) < -(n0 + n)\t/)-mn\ 
< -c1{(tp-mn)tT,-1(t(}-mn)}l/2, 
where c\ = c\~l, with A2 the largest eigenvalue of b"(m„), and hence that (c4) is 
satisfied. Conditions (cl), (c2) follows straightforwardly from the fact that 
(no + n)^1 = b"(mn), 
KMiKimn)}-1 = b"^){b\mn)}-\ 
the latter not depending on n0 + n, and so the result follows by Propositions 5.12 
and 5.13. <, 
Example 5.4. (continued). Suppose that Be(01 an, fin), where an = a + r„, and 
fin = fi + n — r„, is the posterior derived from n Bernoulli trials with rn successes and a 
Be(0 | a, fi) prior. Proceeding directly, 
Ln(9) = \ogpn(9) = \ogp(x | 9) + \ogp(0) - logp(x) 
= (an - 1) log 9 +(fin- 1) log(l -9)- \ogp{x) 
so that 
(a„ - 1) (fin - 1) 
L'n(9) = 
K(0) = -1 
9 1-9 
and 
K - i) (A -1) 
02 (1 - 9)2 
It follows that 
Qn- 1 , ,„, „-i (a„ - l)(/?„ - 1) 
m"=(an + A-2)' ^^ = («, + &-2)' ' 
Condition (cl) is clearly satisfied since (-LJ(m„))"' —+ 0 as n —> oo; condition (c2) 
follows from the fact that L'n(9) is a continuous function of 0. Finally, (c4) may be verified 
with an argument similar to the one used in the proof of Proposition 5.16. 
Taking a = fi = 1 for illustration, we see that 
n n n V n / 
and hence that the asymptotic posterior for 9 is 
N 0 
n ^n n\ n/J 
(As an aside, we note the interesting "duality" between this asymptotic form for 9 given 
n, rn, and the asymptotic distribution for rn/n given 9, which, by the central limit theorem, 
has the form 
' r„ 
N 
n 
0, 
Further reference to this kind of "duality" will be given in Appendix B.) 
o-*>r 
5.3 Asymptotic Analysis 
295 
5.3.3 Asymptotics under Transformations 
The result of Proposition 5.16 is given in terms of the canonical parametrisation of 
the exponential family underlying the conjugate analysis. This prompts the obvious 
question as to whether the asymptotic posterior normality "carries over", with 
appropriate transformations of the mean and covariance, to an arbitrary (one-to-one) 
reparametrisation of the model. More generally, we could ask the same question 
in relation to Proposition 5.14. A partial answer is provided by the following. 
Proposition 5.17. (Asymptotic normality under transformation). 
With the notation and background of Proposition 5.14, suppose that 9 has 
an asymptotic N/t(0|mn, E"1) distribution, with the additional assumptions 
that, with respect to a parametric model p(x\9o), u\ —► 0 and mn —> 9$ 
in probability, and a\ = Op(a^), where u2n (a^) is the largest (smallest) 
eigenvalue ofY?n. Then, ifu = g(9) is a transformation such that, at 9 = 90, 
T (9) - d9m 
is non-singular with continuous entries, v has an asymptotic distribution 
Nfc (y g(mn), [Jg(mn)T,nJg(mn)}~1) . 
Proof. This is a generalization and Bayesian reformulation of classical results 
presented in Serfling (1980, Section 3.3). For details, see Mendoza (1994). < 
For any finite n, the adequancy of the normal approximation provided by 
Proposition 5.17 may be highly dependent on the particular transformation used. 
Anscombe (1964a, 1964b) analyses the choice of transformations which improve 
asymptotic normality. A related issue is that of selecting appropriate parametrisa- 
tions for various numerical approximation methods (Hills and Smith, 1992, 1993). 
The expression for the asymptotic posterior precision matrix (inverse  
covariance matrix) given in Proposition 5.17 is often rather cumbersome to work with. 
A simpler, alternative form is given by the following. 
Corollary 1. (Asymptoticprecision after transformation). 
In Proposition 5.10, ifHn = E"1 denotes the asymptotic precision matrix for 
9, then the asymptotic precision matrix for v = g(9) has the form 
Jg-i {g{mn))HnJg-x (g{mn)), 
where 
J9-l{v)-—dZ- 
is the Jacobian of the inverse transformation. 
Proof. This follows immediately by reversing of the roles of 9 and v. < 
296 
5 Inference 
In many applications, we simply wish to consider one-to-one transformations 
of a single parameter. The next result provides a convenient summary of the required 
transformation result. 
Corollary 2. (Asymptotic normality after scalar transformation). 
Suppose that given the conditions of Propositions 5.14, 5.17 with scalar 0, the 
sequence mn tends in probability to 6$ under p[x\6q), and that L'^{mn) —> 0 
in probability as n —> oo. Then, ifv = g{9) is such that g1 (6) = dg(0)/ d6 is 
continuous and non-zero at 6 = 6$, the asymptotic posterior distribution for 
v is 
N(v\g(mn),L:(mn)[g'(mn)]-2). 
Proof. The conditions ensure, by Proposition 5.14, that 0 has an asymptotic 
posterior distribution of the form N(0|m„, [L^(m„)]_1), so that the result follows 
from Proposition 5.17. < 
Example 5.4. (continued). Suppose, again, that Be(0 | an, 0n), where an = a + r„, 
and (5n = 0 + n — r„, is the posterior distribution of the parameter of a Bernoulli distribution 
afte n trials, and suppose now that we are interested in the asymptotic posterior distribution 
of the variance stabilising transformation (recall Example 3.3) 
v = g(6) = 2 sin-1 Ve. 
Straightforward application of Corollary 2 to Proposition 5.17, leads to the asymptotic 
distribution 
N(i/|2siir1(v/»v!A0.n). 
whose mean and variance can be compared with the forms given in Example 3.3. 
It is clear from the presence of the term [</ (m„)] ~2 in the form of the asymptotic 
precision given in Corollary 2 to Proposition 5.17 that things will go wrong if 
g'(mn) —> Oasn —► oo. This is dealt with in the result presented by the requirement 
that g'(9o) ^ 0, where mn —► 6$ in probability. A concrete illustration of the 
problems that arise when such a condition is not met is given by the following. 
Example 5.8. (Non-normal asymptotic posterior). Suppose that the asymptotic  
posterior for a parameter 9 e 5ft is given by N(0\xn, n), nxn = xx H \- xn, perhaps derived 
from N(xi\6,1), i = 1,..., n, with N(0|O, h), having h « 0. Now consider the  
transformation v = g(6) = 62, and suppose that the actual value of 6 generating the x, through 
N(x;|0,1) is 0 = 0. 
Intuitively, it is clear that v cannot have an asymptotic normal distribution since the 
sequence x\ is converging in probability to 0 through strictly positive values. Technically, 
<?'(0) = 0 and the condition of the corollary is not satisfied. In fact, it can be shown that the 
asymptotic posterior distribution of nv is x2 in this case. 
5.3 Asymptotic Analysis 
297 
One attraction of the availability of the results given in Proposition 5.17 and 
Corollary 1 is that verification of the conditions for asymptotic posterior normality 
(as in, for example, Proposition 5.14) may be much more straightforward under one 
choice of parametrisation of the likelihood than under another. The result given 
enables us to identify the posterior normal form for any convenient choice of  
parameters, subsequently deriving the form for the parameters of interest by  
straightforward transformation. An indication of the usefulness of this result is given in 
the following example (and further applications can be found in Section 5.4). 
Example 5.9. (Asymptotic posterior normality for a ratio). Suppose that we have a 
random sample x\,...,x„ from the model {n"=i ^{xi\6\, l),N(#i|0, Ai)} and,  
independently, another random sample j/i, • • •, y„ from the model {Y["=i N(2/i|#2,1), N(02|O, A2)}, 
where X^ « 0, A2 « 0 and 02 ^ 0. We are interested in the posterior distribution of 
fa = Oi/O? as n —> 00. 
First, we note that, for large n, it is very easily verified that the joint posterior distribution 
for 0 = (#i, 62) is given by 
xn\ fn 0\ I 
W'VO njj' 
where nxn = xt + ■ ■ • + xn, nyn = j/i -H ■ • ■ -H y«. Secondly, we note that the marginal 
asymptotic posterior for fa can be obtained by defining an appropriate c/>2 such that (9\, 62) —> 
(0i, (fe) is a one-to-one transformation, obtaining the distribution of <j> = (0i, (fe) using 
Proposition 5.17, and subsequently marginalising to fa. 
An obvious choice for fa is fa = #2. so that, in the notation of Proposition 5.17, 
9(61,62) = (0i, 02) and 
9 \dfafd9i dfa/d92) \ 0 1 
The determinant of this, 0?1, is non-zero for 02 7^ 0, and the conditions of Proposition 5.17 
are clearly satisfied. It follows that the asymptotic posterior of <f> is 
N, 
XnlVn\ -2 / * + (Xn/Vn)2 
yn J' nH -*„ 
so that the required asymptotic posterior for fa = 9i/02 is 
Any reader remaining unappreciative of the simplicity of the above analysis may care to 
examine the form of the likelihood function, etc., corresponding to an initial  
parametrisation directly in terms of fa, fa, and to contemplate verifying directly the conditions of 
Proposition 5.14 using the fa, fa parametrisation. 
298 
5 Inference 
5.4 REFERENCE ANALYSIS 
In the previous section, we have examined situations where data corresponding 
to large sample sizes come to dominate prior information, leading to inferences 
which are negligibly dependent on the initial state of information. The third of the 
questions posed at the end of Section 5.1.6 relates to specifying prior distributions 
in situations where it is felt that, even for moderate sample sizes, the data should be 
expected to dominate prior information because of the "vague" nature of the latter. 
However, the problem of characterising a "non-informative" or "objective" 
prior distribution, representing "prior ignorance", "vague prior knowledge" and 
"letting the data speak for themselves" is far more complex than the apparent 
intuitive immediacy of these words and phrases would suggest. 
In Section 5.6.2, we shall provide a brief review of the fascinating history of 
the quest for this "baseline", limiting prior form. However, it is as well to make 
clear straightaway our own view—very much in the operationalist spirit with which 
we began our discussion of uncertainty in Chapter 2—that "mere words" are an 
inadequate basis for clarifying such a slippery concept. Put bluntly: data cannot 
ever speak entirely for themselves; every prior specification has some informative 
posterior or predictive implications; and "vague" is itself much too vague an idea 
to be useful. There is no "objective" prior that represents ignorance. 
On the other hand, we recognise that there is often a pragmatically important 
need for a form of prior to posterior analysis capturing, in some well-defined sense, 
the notion of the prior having a minimal effect, relative to the data, on the final 
inference. Such a reference analysis might be required as an approximation to 
actual individual beliefs; more typically, it might be required as a limiting "what 
if?" baseline in considering a range of prior to posterior analyses, or as a default 
option when there are insufficient resources for detailed elicitation of actual prior 
knowledge. 
In line with the unified perspective we have tried to adopt throughout this  
volume, the setting for our development of such a reference analysis will be the  
general decision-theoretic framework, together with the specific information-theoretic 
tools that have emerged in earlier chapters as key measures of the discrepancies (or 
"distances") between belief distributions. From the approach we adopt, it will be 
clear that the reference prior component of the analysis is simply a mathematical 
tool. It has considerable pragmatic importance in implementing a reference  
analysis, whose role and character will be precisely defined, but it is not a privileged, 
"uniquely non-informative" or "objective" prior. Its main use will be to provide 
a "conventional" prior, to be used when a default specification having a claim to 
being non-influential in the sense described above is required. We seek to move 
away, therefore, from the rather philosophically muddled debates about "prior  
ignorance" that have all too often confused these issues, and towards well-defined 
decision-theoretic and information-theoretic procedures. 
5.4 Reference Analysis 
299 
5.4.1 Reference Decisions 
Consider a specific form of decision problem with possible decisions d £ V 
providing possible answers, a € A, to an inference problem, with unknown 
state of the world w = (u>i,u>2). utilities for consequences (a, w) given by 
u(d(u>i)) = u(a,u>i) and the availability of an experiment e which consists of 
obtaining an observation x having parametric model p(x | W2) and a prior  
probability density p(u>) — p(u)\ | W2MW2) for the unknown state of the world, w. This 
general structure describes a situation where practical consequences depend  
directly on the u>i component of w, whereas inference from data x £ X provided by 
experiment e takes place indirectly, through the a>2 component of w as described 
by p{u)\ 10*2)- If u>i is a function of u>2> the prior density is, of course, simply 
p(u)2). 
To avoid subscript proliferation, let us now, without any risk of confusion, 
indulge in a harmless abuse of notation by writing u>i = w, u>2 = 9. This both 
simplifies the exposition and has the mnemonic value of suggesting that w is the 
state of the world of ultimate interest (since it occurs in the utility function), whereas 
9 is a parameter in the usual sense (since it occurs in the probability model). Often 
w is just some function w = <j>{9) of 9; if w is not a function of 9, the relationship 
between w and 0 is that described in their joint distribution p(w, 9) = p(w 10)p{9). 
Now, for given conditional prior p(uj \ 9) and utility function u(a, w), let us 
examine, in utility terms, the influence of the prior p(9), relative to the observational 
information provided by e. We note that if a*a denotes the optimal answer under p(w) 
and a* denotes the optimal answer under p{uj \ x), then the expected (utility) value 
of the experiment e, given the prior p(9), is (see Definition 3.13, with appropriate 
notational changes) 
vu{e,p(9)} = p(x) / p(u>\x)u(a*,u))ckjdx - / p(w)u(ao,w)dw, 
where, assuming w is independent of x, given 9, 
p(w) = Jp(e> 19)p{8) dO, p(w \x) = f Pixl9p)(P^l6)p(9)d9 
and 
p(x) = j p(x\9)p(9)d9. 
If e(k) denotes the experiment consisting of k independent replications of e, that 
is yielding observations {xi,..., x^} with joint parametric model f\i=l p(xi | 8), 
then vu{e(k),p(8)}, the expected utility value of the experiment e(k), has the 
same mathematical form as vu{e,p(9)}, but with x = (x\,- ■. ,xic)andp(x\9) = 
Yli=i Pixi 19). Intuitively, at least in suitably regular cases, as k —» 00 we obtain. 
300 
5 Inference 
from e(oo), perfect (i.e., complete) information about 9, so that, assuming the limit 
to exist, 
vu{e(oo),p(0)} = lim vu{e(k),p(0)} 
K—>00 
is the expected (utility) value of perfect information, about 9, given p{9). 
Clearly, the more valuable the information contained in p{9), the less will be 
the expected value of perfect information about 9; conversely, the less valuable 
the information contained in the prior, the more we would expect to gain from 
exhaustive experimentation. This, then, suggests a well-defined "thought  
experiment" procedure for characterising a "minimally valuable prior": choose, from 
the class of priors which has been identified as compatible with other assumptions 
about (u;, 9), that prior, 7r(0), say, which maximises the expected value of perfect 
information about 9. Such a prior will be called a u-reference prior, the posterior 
distributions, 
7r(u; I x) 
?r(0 I a) a p(x | 0)ir(6) 
derived from combining it (9) with actual data x, will be called u-reference  
posteriors; and the optimal decision derived from 7r(u> | x) and u(a, u) will be called a 
u-reference decision. 
It is important to note that the limit above is not taken in order to obtain 
some form of asymptotic "approximation" to reference distributions; the "exact" 
reference prior is defined as that which maximises the value of perfect information 
about 6, not as that which maximises the expected value of the experiment. 
Example 5.10. (Prediction with quadratic loss). Suppose that beliefs about a  
sequence of observables, x — {x\,.. . , x„), correspond to assuming the latter to be a random 
sample from an N(x | /x, A) parametric model, with known precision A, together with a prior 
for /x to be selected from the class {7V(/x | /xo, A0), /x0 G 3?, A0 > 0}. Assuming a quadratic 
loss function, the decision problem is to provide a point estimate for xn+\, given x-\,...,xn. 
We shall derive a reference analysis of this problem, for which A = 3?, u> = xn+\, and 
6 = fx. Moreover, 
n 
u{a,u>) = -(a- xn+i)2, p(x\9) = J^A^a;, |/x,A) 
and, for given /io. Ao, we have 
p{u, 8) = p(xn+i, /x) = p{xn+i | /x)p(/x) = AT(a;n+i | /x, A)7V(/x | /x0, A0). 
For the purposes of the "thought experiment", let zk ~ {x\,..., xk) denote the (imagined) 
outcomes of k replications of the experiment yielding the observables {x\,..., a:*„), say, 
5.4 Reference Analysis 
301 
and let us denote the future observation to be predicted (x^ \) simply by x. Then 
vu{e(k), N(n | /xo, A0)} = - / p(zk) inf / p(x | zk)(a - x)2dxdzk 
+ inf / p(x)(a — x)2dx. 
However, we know from Proposition 5.3 that optimal estimates with respect to quadratic 
loss functions are given by the appropriate means, so that 
vu{e(k), N(p | no, A„)} = - Jp(zk)V[x | zk]dzk + V[x\ 
= -V[x\zk] + V[x], 
since, by virtue of the normal distributional assumptions, the predictive variance of a; given 
zk does not depend explicitly on zk. In fact, straightforward manipulations reveal that 
vu{e(oo), N{n | Ho, A0)} = lim vu{e(k), N(n | n0, A0)} 
= lim {- [A"1 + (A0 + kn)-1] + (A"1 + Aq1)} = V. 
so that the u-reference prior corresponds to the choice Ao = 0, with /xo arbitrary. 
Example 5.11. (Variance estimation). Suppose that beliefs about x = {x\,... ,xn] 
correspond to assuming x to be a random sample from N(x \ 0, A) together with a gamma 
prior for A centred on Ao, so that p(A) = Ga(A | a, aX^1), a > 0. The decision problem is 
to provide a point estimate for a2 = A-1, assuming a standardised quadratic loss function, 
so that 
~(a-a2y2 
i(a,a ) = 
Thus, we have A — SR+, 9 = A, w = a2, and 
= -{aX - iy. 
p(x, A) = Yl N{Xi | 0, A) Ga(A | a, aX^1). 
t=i 
Let zk = {xi,..., Xa,} denote the outcome of A; replications of the experiment. Then 
vu{e(k),p(X)} =- p{zk)inf p(X\zk)(aX- l)2dXdzk 
+ inf J p(X) (aX - l)2 dA, 
where 
p(A) = Ga(A | a, qAo1), p(X \zk)=Ga(\\a+ ^, aX^1 + ^j , 
302 
5 Inference 
and kns2 = £^ £V a;^. Since 
inf /" Ga(A | a, /?) (aA - l)2 dA = —!— 
and this is attained when d = j3/(a + 1), one has 
Vu{e(oo),p(A)} = lim vu{e(k),p(A)} 
lim 
r 1 1 ] _ 1 
\ 1 + a + (fcn)/2 1 + a J ~ 1+ a 
This is maximised when a = 0 and, hence, the u-reference prior corresponds to the choice 
a = 0, with Ao arbitrary. Given actual data, x = (xi,..., xn), the u-reference posterior for 
A is Ga(A | n/2, ns2/2), where ns2 = J^ x\ and, thus, the u-reference decision is to give 
the estimate 
-2 _ ns2/2 _ Sj2 
_ (n/2) + 1 ~ n + 2 ' 
Hence, the reference estimator of a2 with respect to standardised quadratic loss is not the 
usual s2, but a slightly smaller multiple of s2. 
It is of interest to note that, from a frequentist perspective, a2 is the best invariant 
estimator of a2 and is admissible. Indeed, a2 dominates s2 or any smaller multiple of s2 in 
terms of frequentist risk (cf. Example 45 in Berger, 1985a, Chapter 4). Thus, the M-reference 
approach has led to the "correct" multiple of s2 as seen from a frequentist perspective. 
Explicit reference decision analysis is possible when the parameter space 
6 = {#i,..., 9m} is finite. In this case, the expected value of perfect information 
(cf. Definition 2.19) may be written as 
M M 
3U] 
V 
vu{e(oo),p(0)} = ]Pp(0i) supu(d(6i)) - sup^p(^) u(d(0j))> 
and the u-reference prior, which is that 7r (0) which maximises 7Ji({e(oo), p(0)}, may 
be explicitly obtained by standard algebraic manipulations. For further information, 
see Bernardo (1981a) and Rabena (1994). 
5.4.2 One-dimensional Reference Distributions 
In Sections 2.7 and 3.4, we noted that reporting beliefs is itself a decision problem, 
where the "inference answer" space consists of the class of possible belief  
distributions that could be reported about the quantity of interest, and the utility function is 
a proper scoring rule which—in pure inference problems—may be identified with 
the logarithmic scoring rule. 
5.4 Reference Analysis 303 
Our development of reference analysis from now on will concentrate on this 
case, for which we simply denote vu{-} by v{-}, and replace the term "u-reference" 
by "reference". 
In discussing reference decisions, we have considered a rather general utility 
structure where practical interest centred on a quantity u related to the 6 of an 
experiment by a conditional probability specification, p(u | 6). Here, we shall 
consider the case where the quantity of interest is 6 itself, with 6 6 6 c 5R. More 
general cases will be considered later. 
If an experiment e consists of an observation x 6 X having parametric model 
p(x\0), with u = 6,A = {q(-);q(0) > Q,jBq{6)d{6) = 1} and the utility 
function is the logarithmic scoring rule 
u{q{-),e} = A\ogq(fi) + B{0), 
the expected utility value of the experiment e, given the prior density p{6), is 
v{e,p(6)} = Jp(x) J «{&(•), 6}p{61 a) d6dx - J «(«,(•), 6)p{6) M, 
where qo{-),qx(-) denote the optimal choices of q(-) with respect to p(6) and 
p(61 x), respectively. Noting that u is a proper scoring rule, so that, for any 
sup J u{q(-), 6}p(6) d6 = J u{p(-),e}p(6) M, 
it is easily seen that 
v{e,p(0)} oc Jp(x) jp{61 x) log ^^d6dx = l{e,p(0)} 
the amount of information about 6 which e may be expected to provide. 
The corresponding expected information from the (hypothetical) experiment 
e(k) yielding the (imagined) observation z* = (x\,..., Xk) with parametric model 
k 
p{zk\e) = \\p{Xl\e) 
i=\ 
is given by 
I{e(k),p(e)} = Jp{zk) Jp{91 zk) log ^^- d9dzk, 
and so the expected (utility) value of perfect information about 6 is 
J{e(oo),p(0)}= lim I{e(k),p(0)}, 
k—>oc 
304 
5 Inference 
provided that this limit exists. This quantity measures the missing information 
about 6 as a function of the prior p(6). 
The reference prior for 6, denoted by w(0), is thus defined to be that prior 
which maximises the missing information functional. Given actual data x, the 
reference posterior -k(0 \ x) to be reported is simply derived from Bayes' theorem, 
as 7r(01 x) oc p(x | 0)ir(0). 
Unfortunately, lim^_,oo/{e(A;),p(0)} is typically infinite (unless 6 can only 
take a finite range of values) and a direct approach to deriving 7r(#) along these 
lines cannot be implemented. However, a natural way of overcoming this technical 
difficulty is available: we derive the sequence of priors -nk (#) which maximise 
I{e(k), p(0)}, k = 1,2,..., and subsequently take n(6) to be a suitable limit. This 
approach will now be developed in detail. 
Let e be the experiment which consists of one observation x from p(x \ 6), 
6 6 6 C 5R. Suppose that we are interested in reporting inferences about 6 and that 
no restrictions are imposed on the form of the prior distribution p{6). It is easily 
verified that the amount of information about 6 which k independent replications 
of e may be expected to provide may be rewritten as 
f{e(k),p(e)} = JP(e)iog^de, 
where 
fk(0) = exp l / p(zk | 6) \ogp(01 zk)dzk 
and zk = {x\,..., xk} is a possible outcome from e(k), so that 
k 
P(e\zk)(xY[p{xi\e)p(e) 
is the posterior distribution for 6 after zk has been observed. Moreover, for any 
prior p(0) one must have the constraint J p(6) d6 = 1 and, therefore, the prior 
■Kk{6) which maximises Ie{e(k),p(6)} must be an extremal of the functional 
F{P(-)} = Jp(0) log ^d6 + A yp(0) d6 
Since this is of the form F{p(-)} = J g{p(-)} d6, where, as a functional of p(), 
g is twice continuously differentiable, any function p() which maximises F must 
satisfy the condition 
^-F{p(-)+sr(-)} 
= 0, for all r. 
£=0 
5.4 Reference Analysis 305 
It follows that, for any function r, 
J \r{6) log fk(6) f J^f'M - r(0) (1 4- logp(0)) + t(0)a} d0 = 0, 
where, after some algebra, 
(vAffWna 
Jp(zk\e){p(e) + er(e)}de 
/*(*) = j^e*P 
/ p[zk 10) log f /_ i/.f,,,^ , __,„" Jrt<fefc 
£=0 
-fk{e)W)' 
Thus, the required condition becomes 
/' 
t(0) {log fk(0) - \ogp(0) + \}de = 0, for all r(0), 
which implies that the desired extremal should satisfy, for all 0 G 6, 
log/*(#)-logp(0) + A = O 
and hence thatp(#) a fk(6). 
Note that, for each k, this only provides an implicit solution for the prior 
which maximises Ie{e(k),p(6)}, since fk(0) depends on the prior through the 
posterior distribution p(61 zk) = p(6 \ xit..., xk). However, for large values of 
k, an approximation, p*(6 | zk), say, may be found to the posterior distribution of 
6, which is independent of the prior p(6). It follows that, under suitable regularity 
conditions, the sequence of positive functions 
Plifi) = exp Up(zk | 6) \ogp*(0 | zk)dzk\ 
will induce, by formal use of Bayes' theorem, a sequence of posterior distributions 
Kk(e\x)<xp(x\e)Pi(e) 
with the same limiting distributions that would have been obtained from the  
sequence of posteriors derived from the sequence of priors 7r^(^) which maximise 
Ie{e(k),p(6)}. This completes our motivation for Definition 5.7. For further 
information see Bernardo (1979b) and ensuing discussion. 
306 
5 Inference 
Definition 5.7. (One-dimensional reference distributions). 
Let x be the result of an experiment e which consist of one observation from 
p(x | 6), x e X, 6 6 6 C 5R, let zk = {xx..., xk) be the result of k 
independent replications ofe, and define 
fk(9) = ™vyp(zk\e)iogP*(e\zk)dzk\, 
where 
The reference posterior distribution of 6 after x has been observed is defined 
to be-n(6\ x), such that 
E[8{irk(e\x),n(6\x)}} -»0, as k -^ oo, 
assuming the limit to exist, where 6{g, h} = Je g{6) log[g(6)/h(6)} d6, 
irk(e\x) = ck(x)p(x\e)fk(e) 
and the ck(x) 's are the required normalising constants. Any positive function 
■k(6) such that, for some c(x) > 0 and all 0 6 6, 
ir(0\x) = c(x)p(x\0)ir(0) 
will be called a reference prior for 0 relative to the experiment e. 
It should be clear from the argument which motivates the definition that any 
asymptotic approximation to the posterior distribution may be used in place of 
the asymptotic approximation p*{6 \ zk) defined above. The use of convergence 
in the information sense, the natural convergence in this context, rather than just 
pointwise convergence, is necessary to avoid possibly pathological behaviour; for 
details, see Berger and Bernardo (1992c). 
Although most of the following discussion refers to reference priors, it must be 
stressed that only reference posterior distributions are directly interpre table in  
probabilistic terms. The positive functions -k(6) are merely pragmatically convenient 
tools for the derivation of reference posterior distributions via Bayes' theorem. An 
explicit form for the reference prior is immediately available from Definition 5.7, 
and it will be clear from later illustrative examples that the forms which arise may 
have no direct probabilistic interpretation. 
We should stress that the definitions and "propositions " in this section are by 
and large heuristic in the sense that they are lacking statements of the technical 
conditions which would make the theory rigorous. Making the statements and 
5.4 Reference Analysis 
307 
proofs precise, however, would require a different level of mathematics from that 
used in this book and, at the time of writing, is still an active area of research. The 
reader interested in the technicalities involved is referred to Berger and Bernardo 
(1989,1992a, 1992b, 1992c) and Berger etal. (1989). Sofaras the contents of this 
section are concerned, the reader would be best advised to view the procedure as an 
"algorithm", which compared with other proposals—discussed in Section 5.6.2— 
appears to produce appealing solutions in all situations thus far examined. 
Proposition 5.18. {Explicit form of the reference prior). 
A reference prior for 6 relative to the experiment which consists of one  
observation from p(x | 6), x 6 X, 6 6 6 C 5R, is given, provided the limit exists, 
and convergence in the information sense is verified, by 
7r(0) = climi^-, Bee 
where c > 0, 6$ 6 6, 
f;(0) = exp Up(zk | 0) \ogp*(e | zk)dzk\ , 
with zk = {x\,..., xk} a random sample from p(x | 6), andp*(61 z/.) is an 
asymptotic approximation to the posterior distribution of 6. 
Proof. Using n(0) as a formal prior, 
„(» I „)« „(* I mo)« «. I e> lim Jffl K & jffiffjKMp. 
and hence 
it{61 a) - lim Kk(01 a), Kk{0 \ x) a p(x \ 0)fk{0) 
k—xyo 
as required. Note that, under suitable regularity conditions, the limits above will 
not depend on the particular asymptotic approximation to the posterior distribution 
used to derive fk(0). <] 
If the parameter space is finite, it turns out that the reference prior is uniform, 
independently of the experiment performed. 
Proposition 5.19. (Reference prior in the finite case). Let x be the result 
of one observation from p(x | 6), where 6 £ 6 = {0\,... ,0m}. Then, any 
function of the form it(0i) = a, a > 0, i = 1,..., M, is a reference prior and 
the reference posterior is 
n(9j | x) = c(x)p(x | 0j), i = l,...,M 
where c(x) is the required normalising constant. 
308 
5 Inference 
Proof. We have already established (Proposition 5.12) that if 6 is finite then, 
for any strictly positive prior, p(6i | x\,..., xk) will converge to 1 if 6i is the true 
value of 6. It follows that the integral in the exponent of 
fk(0i) = expl / p(zk 10i) \ogp(0i | zk)dzk L i = 1,..., M, 
will converge to zero as k —> oo. Hence, a reference prior is given by 
*(*.") = lim ^4 = 1. 
The general form of reference prior follows immediately. <] 
The preceding result for the case of a finite parameter space is easily derived 
from first principles. Indeed, in this case the expected missing information is finite 
and equals the entropy 
M 
ff{p(0)} = -5>(^)i°gp(0i) 
of the prior. This is maximised if and only if the prior is uniform. 
The technique encapsulated in Definition 5.7 for identifying the reference prior 
depends on the asymptotic behaviour of the posterior for the parameter of interest 
under (imagined) replications of the experiment to be actually performed. Thus far, 
our derivations have proceeded on the basis of an assumed single observation from 
a parametric model, p(x \ 6). The next proposition establishes that for experiments 
involving a sequence of n > 1 observations, which are to be modelled as if they 
are a random sample, conditional on a parametric model, the reference prior does 
not depend on the size of the experiment and can thus be derived on the basis of 
a single observation experiment. Note, however, that for experiments involving 
more structured designs (for example, in linear models) the situation is much more 
complicated. 
Proposition 5.20. (Independence of sample size). 
Let en,n > 1, be the experiment which consists of the observation of a random 
sample x\,...,xn from p(x | 6),x 6 X, 6 6 6, and let Vn denote the 
class of reference priors for 9 with respect to en, derived in accordance with 
Definition 5.7, by considering the sample to be a single observation from 
n"=i P(xi | 9). Then Vx = V„, for all n. 
5.4 Reference Analysis 
309 
Proof. If zk — {x\,..., xk} is the result of a A;-fold independent replicate of 
e\, then, by Proposition 5.18, V\ consists of -k(0) of the form 
with c> 0, 9, 90 G 6 and 
#(0) = exp {y"p(** I 0) \ogp*{6 | **)}<***, 
where p*(0\ Zk) is an asymptotic approximation (as k —► oo) to the posterior 
distribution of 0 given z*. 
Now consider znk = {xi,..., xn, xn+i,..., x2n, ■ ■ ■, xkn) which can be 
considered as the result of a fc-fold independent replicate of e„, so that Vn consists 
of 7r(0) of the form 
■k(6) = c lim 
/n%(g) 
'*-<»/„%(«o) 
But zn/t can equally be considered as a nfc-fold independent replicate of e\ and so 
the limiting ratios are clearly identical. <j 
In considering experiments involving random samples from distributions  
admitting a sufficient statistic of fixed dimension, it is natural to wonder whether the 
reference priors derived from the distribution of the sufficient statistic are identical 
to those derived from the joint distribution for the sample. The next proposition 
guarantees us that this is indeed the case. 
Proposition 5.21. (Compatibility with sufficient statistics). 
Let en,n > 1, be the experiment which consists of the observation of a random 
sample x\,..., xn from p(x \6),x e X, 6 G O, where, for all n, the latter 
admits a sufficient statistic t„ = t{x\ xn). Then, for any n, the classes 
of reference priors derived by considering replications of (x\,... ,xn) andtn 
respectively, coincide, and are identical to the class obtained by considering 
replications ofe\. 
Proof. If Zk denotes a fc-fold replicate of (x\,..., xn) and yk denotes the 
corresponding fc-fold replicate of in, then, by the definition of a sufficient  
statistic, p(61 Zk) = p{6 | Vk), for any prior p(6). It follows that the corresponding 
asymptotic distributions are identical, so thatp*(0 | Zk) — p*{0 \ Vk)- We thus have 
fm = exp I J p{zk | 6) \ogp*{61 zk)dzk 
= exp I / p{zk | 6) \ogp*(6 | yk)dzk 
= exp | / p(yk | 0) \ogp*(0 \ yk)dyk 
310 
5 Inference 
so that, by Definition 5.7, the reference priors are identical. Identity with those 
derived from e\ follows from Proposition 5.20. < 
Given a parametric model, p(x \ 6), x 6 X, 6 6 0, we could, of course, 
reparametrise and work instead with p(x \ (j>), x 6 X, <j> = (f)(9), for any monotone 
one-to-one mapping g : 0 —> $. The question now arises as to whether  
reference priors for 0 and </>, derived from the parametric models p(x \ 6) and p(x \ (j>), 
respectively, are consistent, in the sense that their ratio is the required Jacobian 
element. The next proposition establishes this form of consistency and can clearly 
be extended to mappings which are piecewise monotone. 
Proposition 5.22. (Invariance under one-to-one transformations). 
Suppose that ne(6), 7ty($) are reference priors derived by considering  
replications of experiments consisting of a single observation from p(x | 6), with 
x 6 X, 6 6 0 and from p(x \ (f>), with x 6 X, <fr 6 $, respectively, where 
0 = g(9) andg : 0 —+ $ is a one-to-one monotone mapping. Then, for some 
c > 0 and for all <£ 6 $: 
(i) 7T0(0) = cue (g_1 (<fr)), ifQ is discrete; 
(ii) tt0(0) = cne (g'1^)) | Jj, |, if J<t> = 9'^ exists. 
Proof. If 0 is discrete, so is $ and the result follows from Proposition 5.19. 
Otherwise, if z* denotes a fc-fold replicate of a single observation from p(x \ 6), 
then, for any proper prior p{6), the corresponding prior for <fr is given by p#($) = 
Pe (fl,_1'($)) \J<t>\ ^d hence, for all <£ 6 $, 
Ptf(0|Zfc) =P0(5"1(<A)|2/fc)l^|- 
It follows that, as A; —> oo, the asymptotic posterior approximations are related by 
the same Jacobian element and hence 
rk{e) = ^ijp{zk\e)\ogp*{e\zk)dzk^ 
= |J</,|exp^ / p(zk\<f>)\ogp*(<t>\zk)dzk > 
The second result now follows from Proposition 5.18. < 
The assumed existence of the asymptotic posterior distributions that would 
result from an imagined fc-fold replicate of the experiment under consideration 
clearly plays a key role in the derivation of the reference prior. However, it is 
important to note that no assumption has thus far been required concerning the 
form of this asymptotic posterior distribution. As we shall see later, we shall 
typically consider the case of asymptotic posterior normality, but the following 
example shows that the technique is by no means restricted to this case. 
5.4 Reference Analysis 
311 
Example 5.12. (Uniform model). Let e be the experiment which consists of observing 
the sequence xit. ..,xn,n> 1, whose belief distribution is represented as that of a random 
sample from a uniform distribution on [6 -1,8 + \ ], 6 G 3ft, together with a prior distribution 
p{6) for 6. If 
» J") 
*" — I xmin i xmax I i xmin — min{xi, . . . , Xn), Xmax — max{Xi, . . . , X„}, 
then tn is a sufficient statistic for 8, and 
p(0 | a) = p{6 1t„) ex p{6), x^x - I < 0 < xJ2 + | • 
It follows that, as k —> oo, a fc-fold replicate of e with a uniform prior will result in the 
posterior uniform distribution 
P*{9\tkn) OCC, X, 
It is easily verified that 
(kn) _ 1 < n < (kn) 1 
max 2 — — min ' 2 
/ 
p{tkn\6)\0gpt(6\tkn)dtkn = E 
log{l-(x^-xS)} 
the expectation being with respect to the distribution of tkn- For large k, the right-hand side 
is well-approximated by 
-logjl-^ 
Jkn) 
E 
Jkn) 
)} 
and, noting that the distributions of 
(kn) _ f. _ i 
''max " 2 
v = x(k") _ n , 1 
y xm n p i 9 
are Be(« | kn, 1) and Be(v 11, kn), respectively, we see that the above reduces to 
'kn+V 
log 
1 
kn 1 
+ ■ 
kn + 1 kn + 1 
It follows that /£n(0) = (fcra + l)/2, and hence that 
log 
7r(0) = c lim -h ttit: = c . 
v ' fc^oo (fcn+l)/2 
Any reference prior for this problem is therefore a constant and, therefore, given a set of 
actual data x = (xi,..., xn), the reference posterior distribution is 
7r(0|a:) oc c, 
(kn) _ 1 < q < (kn) 1 
''max 2 — — min ~ 2 
a uniform distribution over the set of 6 values which remain possible after x has been 
observed. 
312 
5 Inference 
Typically, under suitable regularity conditions, the asymptotic posterior  
distribution p*{61 z^), corresponding to an imagined fc-fold replication of an  
experiment en involving a random sample of n from p(x | 6), will only depend on zkn 
through an asymptotically sufficient, consistent estimate of 6, a concept which is 
made precise in the next proposition. In such cases, the reference prior can easily 
be identified from the form of the asymptotic posterior distribution. 
Proposition 5.23. (Explicit form of the reference prior when there is a  
consistent estimator). Let en be the experiment which consists of the observation 
of a random sample x = {xi,..., xn}fromp(x | 0), x 6 X, 6 6 0 C 3?, and 
let z^ be the result of a k-fold replicate ofen. If there exists 6kn = Qkn(zkn) 
such that, with probability one 
and, as k —► oo, 
/ 
lim 0kn 
k—+oo 
p{zkn | 0) log _ | . ; dzkn 
o, 
P*(0\0kn) 
then, for any c > 0, 6q 6 0, reference priors are defined by 
fU8) 
n(0) 
c lim 
where 
fkn(0)=pr(0\0kn) 
>kn= 
Proof As k —► oo, it follows from the assumptions that 
fkn(0) = exP i / P(Zkn I 0) \ogp*{6 | Zkn)dzkn \ 
> | / P(zkn I 8) \ogp*(61 0kn)dzkn \ 
• ij P0kn I B) log p*(0 | 6kn)d6kn } 
= exp' 
= exp 
{ 
expaogp*(0|0fc„ 
'fcn= 
.}"< 
(0|flfen 
'fcn = 
The result now follows from Proposition 5.18. 
< 
5.4 Reference Analysis 
313 
Example 5.13. (Deviation from uniformity model). Let en be the experiment which 
consists of obtaining a random sample from p(x |0),O<x<l,0>O, where 
( B{2x}e-1 for 0 < x < | 
p(x \6)=\ 
[ 0{2(1 - x)}e-1 for i < x < 1 
defines a one-parameter probability model on [0,1], which finds application (see Bernardo 
and Bayarri, 1985) in exploring deviations from the standard uniform model on [0,1] (given 
by 6 = 1). 
It is easily verified that if zkn = {xi,..., xkn} results from a fc-fold replicate of en, 
the sufficient statistic tkn is given by 
1 hn 
tkn = t ^2 {log{2xi}l[0,i/2|(a:£) + log{2(l - Xi)}l\1/2.i](xi)} 
i=l 
and, for any prior p{0), 
p(6\zkn)=p{6\tkn) 
ocp{6)6knexp{-kn(6-l)tkn}. 
It is also easily shown that p{tk„ 10) = Ga(4n I fen, kn6), so that 
E[tkn\e] = ), v[tkn]e] = IL,, 
from which we can establish that 9kn = tj£ is a sufficient, consistent estimate of 6. It follows 
that 
p*(0|Moc^exp(-M^H 
I 0kn ) 
provides, for large fc, an asymptotic posterior approximation which satisfies the conditions 
required in Proposition 5.23. From the form of the right-hand side, we see that 
p*{6 | hn) = Ga(0 | fen + 1, kn/6kn) 
r(fcn+i) p\ ekn J 
so that 
°kn=<> 
and, from Proposition 5.18, for some c> 0, 90 > 0, 
r(fcn+l)0 
(a, v fL(0) c60 1 
The reference posterior for 8 having observed actual data x = (x\,..., xn), producing the 
sufficient statistic t = t(x), is therefore 
7r(8\x) = 7r(6\t)cxp(x\8)^ 
oc 0"_1 exp{-n(0 - l)i}, 
which is a Ga(0 | n, nt) distribution. 
314 
5 Inference 
Under regularity conditions similar to those described in Section 5.2.3, the 
asymptotic posterior distribution of 6 tends to normality. In such cases, we can 
obtain a characterisation of the reference prior directly in terms of the parametric 
model in which 6 appears. 
Proposition 5.24. (Reference priors under asymptotic normality). 
Let en be the experiment which consists of the observation of a random sample 
Xi,.. .,xnfrom p(x\0), x 6 X, 0 6 6 C 5R. Then, if the asymptotic 
posterior distribution of 6, given a k-fold replicate of e„, is normal with 
precision knh(6kn)> where 6kn is a consistent estimate of 6, reference priors 
have the form 
tt(0) oc {/i(0)}1/2. 
Proof. Under regularity conditions such as those detailed in Section 5.2.3, it 
follows that an asymptotic approximation to the posterior distribution of 6, given a 
fc-fold replicate of en, is 
p*(e\6kn) = N(6\§kn,knh(§kn)), 
where Qkn is some consistent estimator of 6. Thus, by Proposition 5.23, 
rkn{e)=P*{e\ekn) 
kn= 
{2K)-^{h{e)yi\ 
and therefore, for some c > 0, 60 6 0, 
• lim MM. : 
*(9) = c iim &&L = M^ a {h{e)yi\ 
as required. ^ 
The result of Proposition 5.24 is closely related to the "rules" proposed by 
Jeffreys (1946,1939/1961) and by Perks (1947) to derive "non-informative" priors. 
Typically, under the conditions where asymptotic posterior normality obtains we 
find that 
h(6) = Jp(x | 9) (~ \ogp(x | 6)\ dx. 
i.e., Fisher's information (Fisher, 1925), and hence the reference prior, 
7r(0) a M0)1/2> 
5.4 Reference Analysis 
315 
becomes Jeffreys' (or Perks') prior. See Poison (1992) for a related derivation. 
It should be noted however that, even under conditions which guarantee  
asymptotic normality, Jeffreys' formula is not necessarily the easiest way of deriving a 
reference prior. As illustrated in Examples 5.12 and 5.13 above, it is often  
simpler to apply Proposition 5.18 using an asymptotic approximation to the posterior 
distribution. 
It is important to stress that reference distributions are, by definition, a function 
of the entire probability model p(x \0),x £ X,6 e 0, not only of the observed 
likelihood. Technically, this is a consequence of the fact that the amount of  
information which an experiment may be expected to provide is the value of an integral 
oven the entire sample space X, which, therefore, has to be specified. We have, 
of course, already encountered in Section 5.1.4 the idea that knowledge of the data 
generating mechanism may influence the prior specification. 
Example 5.14. (Binomial and negative binomial models). Consider an experiment 
which consists of the observation of n Bernoulli trials, with n fixed in advance, so that 
x — \^i,..., xnj, 
p(x\8) = 6x(l-6y-x, xe{0,l}, 0 < (9 < 1, 
h{6) = -Yjp(x\6)^f2\ogp(x\6) = 6-\l-6)-\ 
and hence, by Proposition 5.24, the reference prior is 
n{9) ex e-l'\l - 8)-1'2. 
If r = Ya=i xi' the reference posterior, 
7r((9 I x) oc p(x 10)n(0) ex 0r-l/2(l - e)""-1'2, 
is the beta distribution Be(0 \r + \,n - r + \). Note that ir(6 \ x) is proper, whatever the 
number of successes r. In particular, if r = 0, tt(6 | a;) = Be(011, n + |), from which 
sensible inference summaries can be made, even though there are no observed successes. 
(Compare this with the Haldane (1948) prior, n(9) ex 0_1(1 — 0)_1, which produces an 
improper posterior until at least one success is observed.) 
Consider now, however, an experiment which consists of counting the number x of 
Bernoulli trials which it is necessary to perform in order to observe a prespecified number 
of successes, r > 1. The probability model for this situation is the negative binomial 
p(x\6)= (*IiV(l-0r~r, x = r.r+l,... 
from which we obtain 
W) = - £p(* 10)-h iogp(* 19) = r-r2(i - 0)-1 
316 
5 Inference 
and hence, by Proposition 5.24, the reference prior is n(9) oc 6'1 (1 — 9)~1/2. The reference 
posterior is given by 
n{6 I x) oc p(x | 6)n{6) oc r-^l - (9):c-r-1/2, x = r, r + 1 
which is the beta distribution Be(0 | r, x — r + |). Again, we note that this distribution is 
proper, whatever the number of observations x required to obtain r successes. Note that 
r = 0 is not possible under this model: the use of an inverse binomial sampling design 
implicitly assumes that r successes will eventually occur/or sure, which is not true in direct 
binomial sampling. This difference in the underlying assumption about 9 is duly reflected 
in the slight difference which occurs between the respective reference prior distributions. 
See Geisser (1984) and ensuing discussion for further analysis and discussion of this 
canonical example. 
In reporting results, scientists are typically required to specify not only the 
data but also the conditions under which the data were obtained (the design of 
the experiment), so that the data analyst has available \hefull specification of the 
probability model p(x \6), x e X, 6 e 6. In order to carry out the reference 
analysis described in this section, such a full specification is clearly required. 
We want to stress, however, that the preceding argument is totally compatible 
with a full personalistic view of probability. A reference prior is nothing but 
a (limiting) form of rather specific beliefs; namely, those which maximise the 
missing information which a particular experiment could possibly be expected to 
provide. Consequently, different experiments generally define different types of 
limiting beliefs. To report the corresponding reference posteriors (possibly for a 
range of possible alternative models) is only part of the general prior-to-posterior 
mapping which interpersonal or sensitivity considerations would suggest should 
always be carried out. Reference analysis provides an answer to an important 
"what if?" question: namely, what can be said about the parameter of interest 
if prior information were minimal relative to the maximum information which a 
well-defined, specific experiment could be expected to provide? 
5.4.3 Restricted Reference Distributions 
When analysing the inferential implications of the result of an experiment for a 
quantity of interest, 0, where, for simplicity, we continue to assume that 9 6 6 C 3?, 
it is often interesting, either per se, or on a "what if?" basis, to condition on 
some assumed features of the prior distribution p(6), thus defining a restricted 
class, Q, say, of priors which consists of those distributions compatible with such 
conditioning. The concept of a reference posterior may easily be extended to 
this situation by maximising the missing information which the experiment may 
possibly be expected to provide within this restricted class of priors. 
5.4 Reference Analysis 
317 
Repeating the argument which motivated the definition of (unrestricted)  
reference distributions, we are led to seek the limit of the sequence of posterior  
distributions, 7T/fc(0 | x), which correspond to the sequence of priors, -irk(9), which are 
obtained by maximising, within Q, the amount of information 
i{e(k),p(e)} = JP(e)iog^de, 
where 
fk{0) = exp | / p(zk | 0) logp(0 | zk)dzk \ , 
which could be expected from k independent replications z = {xi,..., xk} of the 
single observation experiment. 
Definition 5.8. (Restricted reference distributions). 
Let x be the result of an experiment e which consists of one observation from 
p(x | 0), x e X, with 0 6 G C 3?, let Q be a subclass of the class of all 
prior distributions for 0, let zk = {xi,..., xk } be the result ofk independent 
replications ofe and define 
fl;(e) = exp Up(zk | 0) logp*(0 | zk)dzk\ , 
where 
n,(f)l. UliP(xj\e) 
P \,U\Zk) 
SUlMxi\e)de 
Provided it exists, the Q-reference posterior distribution of0, after x has been 
observed, is defined to be n®(01 x), such that 
E[6{nk3{0\x),TTQ(6\x)}]-^O, as k -► oo, 
n^(e\x)<xp(x\6)^(0), 
where 6 is the logarithmic divergence specified in Definition 5.7, and nk(0) 
is a prior which maximises, within Q 
I 
P(e)iog^d0. 
A positive function n®(0) in Q such that 
irQ(0\x)otp(x\6)irQ(0), for all 9 6 6, 
is then called a Q-reference prior for 0 relative to the experiment e. 
318 
5 Inference 
The intuitive content of Definition 5.8 is illuminated by the following result, 
which essentially establishes that the Q-reference prior is the closest prior in Q 
to the unrestricted reference prior n(0), in the sense of minimising its logarithmic 
divergence from n(6). 
Proposition 5.25. (The restricted reference prior as an approximation). 
Suppose that an unrestricted reference prior n(0) relative to a given  
experiment is proper; then, if it exists, a Q-reference prior ttq (9) satisfies 
Proof It follows from Proposition 5.18 that ir(0) is proper if and only if 
J fi(0) <W = <* < oo, 
in which case, 
Tr(fl) = lim nk(6) = lim Cj^/» 
k—»oo k—>oo 
Moreover, 
= \ogck- jP{9)\ogAde, 
which is maximised if the integral is minimised. Let 71^(0) be tne P^or which 
minimises the integral within Q. Then, by Definition 5.8, 
ttq(6 | x) oc p(x 16) lim n?(0) = p(x \ 6)nQ{6), 
k—too 
where, by the continuity of the divergence functional, ttq (6) is the prior which 
minimises, within Q, 
^ k—>oo ) 
5.4 Reference Analysis 319 
If 7r(0) is not proper, it is necessary to apply Definition 5.8 directly in order to 
characterise nQ(0). The following result provides an explicit solution for the rather 
large class of problems where the conditions which define Q may be expressed as 
a collection of expected value restrictions. 
Proposition 5.26. (Explicit form of restricted reference priors). 
Let e be an experiment which provides information about 9, and, for given 
{(gi(-), A). i= !>••-, m}, let Q be the class of prior distributions p(9) of 6 
which satisfy 
I 
gi(e)P(6)de = ft, i = i,...,m. 
Let n(6) be an unrestricted reference prior for 6 relative to e; then, a  
Conference prior of 9 relative to e, if it exists, is of the form 
7r«(0)a7r(0)expj]TA^(0)L 
where the A, 's are constants determined by the conditions which define Q. 
Proof. The calculus of variations argument which underlay the derivation of 
reference priors may be extended to include the additional restrictions imposed by 
the definition of Q, thus leading us to seek an extremal of the functional 
fp{e)\bg^de + \^jP{e)de-i^ + Y/xi^jgi{e)p{e)de-i3^, 
corresponding to the assumption of a fc-fold replicate of e. A standard argument 
now shows that the solution must satisfy 
m 
log f*k{e) - logp(0) + A + Y,xm(0) = 0 
! = 1 
and hence that 
p(0)oc/t*(0)exp. 
m ) 
. i=l J 
Taking k —> oo, the result follows from Proposition 5.18. < 
320 
5 Inference 
Example 5.15. (Location models). Let x = {xx,..., xn} be a random sample from 
a location model p(x 18) = h(x — 8), x € 5ft, 8 € 5ft, and suppose that the prior mean and 
variance of 6 are restricted to be E[8] = Ho,V[8] = a$. Under suitable regularity conditions, 
the asymptotic posterior distribution of 8 will be of the form p*(# | X\,... ,x„) oc f(8„ — 8), 
where 8„ is an asymptotically sufficient, consistent estimator of 8. Thus, by Proposition 5.23, 
ir(0)ocp*(0|0„) 
ex /(0), 
which is constant, so that the unrestricted reference prior will be uniform. It now follows 
from Proposition 5.26 that the restricted reference prior will be 
ttq(8) oc exp {Ai0 + A2(0 - Ho?} , 
with / 8-kq(8) d8 = /z0 and J(8 — ho)2-kq(8) d8 = (Tq- Thus, the restricted reference prior 
is the normal distribution with the specified mean and variance. 
5.4.4 Nuisance Parameters 
The development given thus far has assumed that 6 was one-dimensional and that 
interest was centred on 9 or on a one-to-one transformation of 9. We shall next 
consider the case where 6 is two-dimensional and interest centres on reporting 
inferences for a one-dimensional function, <j> = <p(6). Without loss of generality, 
we may rewrite the vector parameter in the form 6 — (<j>, A), <j> € <fr, A G A, 
where cp is the parameter of interest and A is a nuisance parameter. The problem is 
to identify a reference prior for 6, when the decision problem is that of reporting 
marginal inferences for <j>, assuming a logarithmic score (utility) function. 
To motivate our approach to this problem, consider Zk to be the result of a 
fe-fold replicate of the experiment which consists in obtaining a single observation, 
x, from p(x | 0) = p(x \ <f>, A). Recalling that p(6) can be thought of in terms of 
the decomposition 
p(0)=p(<p,\)=p(<p)p(\\<p), 
suppose, for the moment, that a suitable reference form, 7r(A | </>), for p(X | </>) has 
been specified and that only 7r(</>) remains to be identified. Proposition 5.18 then 
implies that the "marginal reference prior" for cp is given by 
k—>oo 
where 
tt(0) a lim [fiM/f'kifo)}, ^6 *. 
k—>oo 
/*(</») = exP \ / P(zk I 4>) logp*(4> I zk)dzk > , 
5.4 Reference Analysis 321 
p*(4> | zk) is an asymptotic approximation to the marginal posterior for 0, and 
p{zk | 0) = / p{zk | 0, A)?r(A | 0) dX 
= ff[p(xi\<P,X)AX\<P)dX. 
By conditioning throughout on 0, we see from Proposition 5.18 that the "conditional 
reference prior" for A given 0 has the form 
/t*(A|0) 
7r(A|0) oc lim 
k—>oo 
where 
L/t*(Ao 10) 
A, A0 G A, 0 G $, 
/fc* (A | 0) = exp ^ / p(zk | </., A) logp*(A | <p, zk)dzk \ , 
p* (A | <j>, zk) is an asymptotic approximation to the conditional posterior for A given 
0, and 
k 
p{zk\<j),X) = J\p{Xi\<p,X). 
j=l 
Given actual data x, the marginal reference posterior for cj>, corresponding to 
the reference prior 
7r(0) = 7r(<M) = 7r(0)7r(A|</>) 
derived from the above procedure, would then be 
n((p | x) oc / Tv((p, X | x) dX 
a 7r(0) / p(x | <fi, A)7r(A I <fi)dX. 
This would appear, then, to provide a straightforward approach to deriving reference 
analysis procedures in the presence of nuisance parameters. However, there is a 
major difficulty. 
In general, as we have already seen, reference priors are typically not proper 
probability densities. This means that the integrated form derived from 7r(A | <p), 
p(zk |0)=/ p{zk | 0, A)7r(A I 0) dX, 
which plays akey role in the above derivation of 7r(0), will typically notbe aproper 
probability model. The above approach will fail in such cases. 
Clearly, a more subtle approach is required to overcome this technical problem. 
However, before turning to the details of such an approach, we present an example, 
involving/im're parameter ranges, where the approach outlined above does produce 
an interesting solution. 
322 
5 Inference 
Example 5.16. (Induction). Consider a large, finite dichotomised population, all of 
whose elements individually may or may not have a specified property. A random sample is 
taken without replacement from the population, the sample being large in absolute size, but 
still relatively small compared with the population size. All the elements sampled turn out 
to have the specified property. Many commentators have argued that, in view of the large 
absolute size of the sample, one should be led to believe quite strongly that all elements of 
the population have the property, irrespective of the fact that the population size is greater 
still, an argument related to Laplace's rule of succession. (See, for example, Wrinch and 
Jeffreys, 1921, Jeffreys, 1939/1961, pp. 128-132 and Geisser, 1980a.) 
Let us denote the population size by N, the sample size by n, the observed number 
of elements having the property by x, and the actual number of elements in the population 
having the property by 8. The probability model for the sampling mechanism is then the 
hypergeometric, which, for possible values of x, has the form 
8 
( \a\ \xj_\n-x 
**!«) = py 
If p(8 = r), r = 0,..., N defines a prior distribution for 8, the posterior probability that 
8 = N, having observed x = n, is given by 
p{0 = N\x = n) = ^=n\e = NMe = N^ - 
J2r=„P(x = n\6 = r)p(6 = r) 
Suppose we considered 8 to be the parameter of interest, and wished to provide a reference 
analysis. Then, since the set of possible values for 8 is finite, Proposition 5.19 implies that 
^ = r) = ArTT' r = 0'1>---<N> 
is a reference prior. Straightforward calculation then establishes that 
71+1 
p(8 = N\x = n) 
N+l 
which is not close to unity when n is large but n/N is small. 
However, careful consideration of the problem suggests that it is not 8 which is the 
parameter of interest: rather it is the parameter 
(1 if 8 = N 
To obtain a representation of 8 in the form (0, A), let us define 
(1 if 0 = N 
{8 ifd^N. 
5.4 Reference Analysis 
323 
By Proposition 5.19, the reference priors 7r(0) and 7r(A | </>) are both uniform over the  
appropriate ranges, and are given by 
7T(0 = 0) = 7T(0 = 1) = | , 
tt(A = 0 I 0 = 1) = 1, 7r(A = r|0 = O) = — . r = 0,1,...,JV-1. 
These imply a reference prior for 8 of the form 
P(0) = { 
if 0 = N 
(2N i{6*N 
and straightforward calculation establishes that 
p(6 = N | x = n) = 
^orhyC1-^) 
n+ 1 
n + 2 
which clearly displays the irrelevance of the sampling fraction and the approach to unity for 
large n (see Bernardo, 1985b, for further discussion). 
We return now to the general problem of defining a reference prior for 6 = 
(<p, A), <j> € <fr, A € A, where <j> is the parameter vector of interest and A is a nuisance 
parameter. We shall refer to the pair (</>, A) as an ordered parametrisation of the 
model. We recall that the problem arises because in order to obtain the marginal 
reference prior 7r(0) for the first parameter we need to work with the integrated 
model 
p(zk |0)=/ p{zk | cp, A)?r(A | cp) dX. 
However, this will only be a proper model if the conditional prior 7r(A | <j>) for the 
second parameter is a proper probability density and, typically, this will not be the 
case. 
This suggests the following strategy: identify an increasing sequence {Aj} 
of subsets of A, \J{ Aj = A, which may depend on <f>, such that, on each Aj, the 
conditional reference prior, 7r(A| <fi) restricted to Aj can be normalised to give a 
reference prior, 7Tj(A | <j>), which is proper. For each i, a proper integrated model 
can then be obtained and a marginal reference prior 7Tj (<j>) identified. The required 
reference prior tt((J) \ A) is then obtained by taking the limit as i —> oo. The strategy 
clearly requires a choice of the Aj's to be made, but in any specific problem a 
"natural" sequence usually suggests itself. We formalise this procedure in the next 
definition. 
324 
5 Inference 
Definition 5.9. (Reference distributions given a nuisance parameter). 
Let x be the result of an experiment e which consists of one observation from 
the probability model p(x \ <f>, A), x G X, (0, A) G $ x A C 5R x JR. The 
reference posterior, n((j) \ x), for the parameter of interest cj), relative to the 
experiment e and to the increasing sequences of subsets of'A, {At (</>)}, cj) G $, 
U^ Aj (<fi) = A, is defined to be the result of the following procedure: 
(i) applying Definition 5.7 to the model p(x | <fi, A), for fixed cj), obtain the 
conditional reference prior, 7r(A | <fi), for A; 
(ii) for each (j>, normalise 7r(A | (j>) within each Aj(</>) to obtain a sequence of 
proper priors, 7Tj(A | <fi); 
(iii) use these to obtain a sequence of integrated models 
Pi {zk \(p)= P{zk\(p, A)7Ti (A I <p) dX 
J\i(<i>) 
for the Zk resulting from a k-fold replication ofe; 
(iv) use those to derive the sequence of reference priors 
nM) = c lim 1^- 
/£(</>) = exp I / pi{zk | 4>) logp*{<p | zk)dzk \ , 
and, for data x, obtain the corresponding reference posteriors 
TVi{4>\x) oc7Ti(0) / p(x|0,A)7rj(A|(/>)dA; 
J\i(4>) 
(v) define n(<fi | x) such that 6{n(<fi \ x), ni(<fi | x)} —> 0, as i —> oo. 
77ie reference prior, relative to the ordered parametrisation (<j>, A), w any 
positive function n((f>, A), jmc/i f/iar 
7r(01 x) a / p(aj | </>, \)n((j), A) dA. 
77iw vv/W typically be simply obtained as 
7ri(0)7ri(A| 0) 
7r(0, A) = lim 
fc->oo 7Tj((/)o)7ri(Ao I </>o) 
Ghosh and Mukerjee (1992) showed that, in effect, the reference prior thus 
defined maximises the missing information about the parameter of interest, <f>, 
5.4 Reference Analysis 
325 
subject to the condition that, given <p, the missing information about the nuisance 
parameter, A, is maximised. 
In a model involving a parameter of interest and a nuisance parameter, the 
form chosen for the latter is, of course, arbitrary. Thus, p(x \ <fi, A) can be  
written alternatively as p(x \ <p, ip), for any ip = tp((p, A) for which the transformation 
(<p, A) —> ((/>, ip) is one-to-one. Intuitively, we would hope that the reference  
posterior for (j> derived according to Definition 5.9 would not depend on the particular 
form chosen for the nuisance parameters. The following proposition establishes 
that this is the case. 
Proposition 5.27. (Invariance with respect to the choice of the nuisance 
parameter). Let e be an experiment which consists in obtaining one  
observation from p(x \<p,X), (</>, A) G $ x A C 5R x 3£, and let e' be an 
experiment which consists in obtaining one observation from p(x \ <p, ip), 
(^) e$x* C j}x| where (<f>, A) —> (</>, ip) is one-to-one  
transformation, with ip = 5^(A). Then, the reference posteriors for <p, relative to 
[e, {Aj(</»)}] and [e1, {#j(</>)}], where #*(</>) = g^{A*(</>)}, are identical. 
Proof. By Proposition 5.22, for given <f>, 
**{1>\<l>) = *\(9?M\<l>)\J-i{rl>)\, 
where 
Hence, if we define 
dg-\ip) 
ip = g<t>{\), J^{4>) = v 
and normalise n^(tp | <p) over ^i{<p) and nxigZ1 {ip) | 4>) over A*(<j>), we see that the 
normalised forms are consistently related by the appropriate Jacobian element. If 
we denote these normalised forms, for simplicity, by 7r;(A | <f>), iti(ip \ <p), we see 
that, for the integrated models used in steps (iii) and (iv) of Definition 5.9, 
Pi(x\<p)= p{x\<p,X)-Ki(\\<p)d\ 
= / p{x\<p,ip)-Ki{ip\<p)dip, 
J9M) 
and hence that the procedure will lead to identical forms of 7r(0 | x). < 
326 
5 Inference 
Alternatively, we may wish to consider retaining the same form of nuisance 
parameter, A, but redefining the parameter of interest to be a one-to-one function 
of <f>. Thus, p(x | </>, A) might be written as p(x 17, A), where 7 = g{4>) is now the 
parameter vector of interest. Intuitively, we would hope that the reference posterior 
for 7 would be consistently related to that of <j> by means of the appropriate Jacobian 
element. The next proposition establishes that this is the case. 
Proposition 5.28. (Invariance under one-to-one transformations). 
Let e be an experiment which consists in obtaining one observation from 
p(x I <j>, A), (p G <fr, A € A, and let e' be an experiment which consists in 
obtaining one observation from p(x | 7, A), 7 € I\ A € A, where 7 = g(<j)). 
Then, given data x, the reference posteriors for <f> and 7, relative to [e, {A$ (</>)}] 
and [e\ {$^(7)}]- *:(7) = ^iidW} are related by: 
(i) tt7(7 I x) = 7r^(5_1 (7) I x), if$ is discrete; 
da~l(i) 
(ii) tt7(7 I x) = Tt^g-1 (7) I x) I Jg-i (7) I, ifJg-i (7) = ^ exists- 
Proof. In all cases, step (i) of Definition 5.9 clearly results in a conditional  
reference prior ir(\\<f>) = 7r(A|5_1(7)). For discrete*, A, 7Tj(0) and 7^(7) defined by 
steps (ii)-(iv) of Definition 5.9 are both uniform distributions, by Proposition 5.18, 
and the result follows straightforwardly. If J -1 (7) exists, ttj (</>) and 7Tj (7) defined 
by steps (ii)—(iv) of Definition 5.9 are related by the claimed Jacobian element, 
I J 1 (7) I, by Proposition 5.22, and the result follows immediately. <] 
In Proposition 5.23, we saw that the identification of explicit forms of reference 
prior can be greatly simplified if the approximate asymptotic posterior distribution 
is of the form 
P*(9\zk)=p*{9\9k), 
where 9k is an asymptotically sufficient, consistent estimate of 9. Proposition 5.24 
establishes that even greater simplification results when the asymptotic distribution 
is normal. We shall now extend this to the nuisance parameter case. 
Proposition 5.29. (Bivariate reference priors under asymptotic normality). 
Let en be the experiment which consists of the observation of a random sample 
Xi,...,xn from p(x\<p,\),((fi,\) £ $ x A C 5R x 3£, and let {Aj(</>)} 
be suitably defined sequences of subsets of A, as required by Definition 5.9. 
Suppose that the joint asymptotic posterior distribution of(<fi, A), given a k-fold 
replicate ofen, is multivariate normal with precision matrix knH(<pkn, Afcn). 
where (<^„, Afcn) is a consistent estimate of ((/>, A) and suppose that hij = 
5.4 Reference Analysis 
327 
hij{((>kni Xkn), i = 1,2, j = 1,2, « f/iepartition ofH corresponding to <fi, A. 
Then 
7r(A|<A)oc{/i22(<M)}1/2; 
^,A)=r(A|^)Iim{-=^g'}, <Ao € *, 
define a reference prior relative to the ordered parametrisation (</>, A), where 
TTifo) a exp J J ^(A | 0) log ({M<A, A)}1/2) dA 1, 
with 
and 
7Tj(A| 0) =a{(j>)ir{X\(j>) 
ir(\\4>) 
hi(4,)n(X\®dX 
h(/) = (/in - hi2h22h2i). 
Proof Given <j>, the asymptotic conditional distribution of A is normal with 
precision knh,22{<f>kn, Xkn). The first part of Proposition 5.29 then follows from 
Proposition 5.24. 
Marginally, the asymptotic distribution of <f> is multivariate normal with  
precision knh^, where h^ = (hn — hnh^h2\)■ To derive the form of 7Tj (<j>), we note 
that if zk € Z denotes the result of a fe-fold replication of en, 
fkn(<P) = exP 
U." 
{zk | <j>) logp*((t> | zk)dzk 
}■ 
where, with 7Tj(A | <j>) denoting the normalised version of 7r(A | <j>) over At(<j>), the 
integrand has the form 
/ / p(zk\(p,X)TVi(X\(p)dX \ogN(((>\4>kn,knh(l))dzk 
JZ LJAj(0) J 
= / -Ki (A I <f>) / p(zk I <f>, A) log N(cp | 4>kn, knh^dzk dX 
Jam) lJz J 
/ 7Ti(A| 
<A)log 
(M<M)} 
2tt 
1/2 
dX, 
for large fe, so that 
■Ki{4>) = lim 
/LW 
fc^°° /fcn(<^o) 
328 
5 Inference 
oc 
oc 
has the stated form. Since, for data x, the reference prior 7r(</>, A) is defined by 
7r(01 x) = lim 7Tj(01 x) oc lim pi(x | 0)7Tj(</>) 
i—>oo i—>oo 
lim 7Tj(</>) / p(x| <f>, A)q((/))7r(A|(/))dA 
/ p{x\4>,\)n(<t>,X)dX. 
the result follows. < 
In many cases, the forms of {/i22 (</>, A)} and {/i^ (</>, A)} factorise into products 
of separate functions of <j> and A, and the subsets {Aj} do not depend on (p. In such 
cases, the reference prior takes on a very simple form. 
Corollary. Suppose that, under the conditions of Proposition 5.29, we choose 
a suitable increasing sequence of subsets {Aj} of A, which do not depend on 
cp, and suppose also that 
{M<M)}1/2 = /i W5i (A), {hv(4>,\)}1/2 = /2(<A)S2(A). 
Then a reference prior relative to the ordered parametrisation (<p, A) is 
7r(0,A)oc/i(0)sa(A) 
Proof. By Proposition 5.29,7r(A | <j>) oc /2(</>)<72(A), and hence 
7Tj(A|</>) = (2j52(A), 
where a^1 = JA 52(A) dX. It then follows that 
ttj(0) a exp l / <2j52(A)log[/i(</>)5i(A)] dA l 
OC&j/i (</>), 
where 6, = /A aig2(X) log 51 (A) dA, and the result easily follows. < 
Example 5.17. (Normal mean and standard deviation). Let e„ be the experiment 
which consists in the observation of a random sample x = {xi,..., xn} from a normal 
distribution, with both mean, /j, and standard deviation, a, unknown. We shall first obtain a 
reference analysis for /j, taking a to be the nuisance parameter. 
5.4 Reference Analysis 
329 
Since the distribution belongs to the exponential family, asymptotic normality obtains 
and the results of Proposition 5.29 can be applied. We therefore first obtain the Fisher 
(expected) information matrix, whose elements we recall are given by 
from which it is easily verified that the asymptotic precision matrix as a function of 0 = (/x, a) 
is given by 
Ho(»°)=(a0 2a-0' 
K(n,a)}l/2=a-\ 
{h22(»,a)Y/2 = V2a-\ 
This implies that 
7T(<7 I fl) OC {/l22(/i, C)}1/2 « V~^ 
so that, for example, A; = {a; e~' < a < e'}, i = 1,2,..., provides a suitable sequence 
of subsets of A = 5R+ not depending on /j, over which 7r(cr | /j) can be normalised and the 
corollary to Proposition 5.29 can be applied. It follows that 
7r(/j, a) = 7r(/i)7r(<7 | /j) oc 1 x <j_1 
provides a reference prior relative to the ordered parametrisation (/j, a). The corresponding 
reference posterior for /j, given x, is 
7r(/j I a;) oc / p(a; | /x, a)n(n, a) da 
K "■(/•*) / II N(zi I M, cr)7r(<7 I /x) da 
oc / (j""exp|--^ [(s -/i)2 + s2] I a"1 da 
oc y A"/2-1 exp j-^ [(5 - M)2 + s2} | dA 
oc[S2 + (M-x)2]"n/2 
= St(/i | x, (n — l)s~2, n — 1), 
where ns2 = E(xi — x)2. 
If we now reverse the roles of/x and a, so that the latter is now the parameter of interest 
and fi is the nuisance parameter, we obtain, writing <fi = (a, (/,) 
H4>^^=(\ a°2)' 
330 
5 Inference 
so that {ha(a, fj)}1/2 = \/2<t"\ h22((T, m)}1/2 = a~l and, by a similar analysis to the above, 
7r(/i | cr) oc a~l 
so that, for example, Aj = {/i; —e' < \i < e'},i = 1,2,... provides a suitable sequence 
of subsets of A = U not depending on a, over which 7r(/i | a) can be normalised and the 
corollary to Proposition 5.29 can be applied. It follows that 
7r(/i, a) = n(a)n(fj, \ a) oc 1 x a~l 
provides a reference prior relative to the ordered parametrisation (a, fi). The corresponding 
reference posterior for a, given x, is 
7r(cr I x) oc / p(x | /i, a) 7r(/i, a) d/i 
oc 7r(a) / J^[ N(ij | /i, a) 7r(/i | a) dfi, 
J «=i 
the right-hand side of which can be written in the form 
a-"expi-^l /a-^xpj-^^-sc)2! dfi. 
Noting, by comparison with a N(fi | x, nX) density, that the integral is a constant, and 
changing the variable to X = a"1, implies that 
7r(A|x)ocA("-1)/2-1exp{ins2A} 
= Ga(A | f(n-l), |ns2) , 
or, alternatively, 
7r(Arcs2 | x) = Ga (Arcs2 I |(rc - 1), |) 
= X2(Arcs2|rc-l). 
One feature of the above example is that the reference prior did not, in fact, 
depend on which of the parameters was taken to be the parameter of interest. In the 
following example the form does change when the parameter of interest changes. 
Example 5.18. (Standardised normal mean). We consider the same situation as that 
of Example 5.17, but we now take </> = \ija to be the parameter of interest. If a is taken as 
the nuisance parameter (by Proposition 5.27 the choice is irrelevant), ip = (</>, a) = g(/i, a) 
is clearly a one-to-one transformation, with 
1 |)-(J J) 
\ d<f> do 
5.4 Reference Analysis 
331 
and using Corollary 1 to Proposition 5.17. 
Again, the sequence A, = {a; e~' < a < e'}, i = 1,2,..., provides a reasonable basis foi 
applying the corollary to Proposition 5.27. It is easily seen that 
iMAoirvp+^v'v-', 
so that the reference prior relative to the ordered parametrisation (4>, a) is given by 
7r(^a)oc(2 + ^2)-1/V-1. 
In the (/i, a) parametrisation this corresponds to 
/ /i2VV2 
7r(/i,<r)oc (2+^jJ a 2, 
which is clearly different from the form obtained in Example 5.16. Further discussion of 
this example will be provided in Example 5.26 of Section 5.6.2. 
We conclude this subsection by considering a rather more involved example, 
where a natural choice of the required A.i(<j>) subsequence does depend on 4>. In 
this case, we use Proposition 5.29, since its corollary does not apply. 
Example 5.19. (Product of normal means). Consider the case where independent 
random samples x = {x,,..., xn} and y = {j/i,..., yn} are to be taken, respectively, from 
N(x | a, 1) and N(y \f3,l),a>Q,0> 0, so that the complete parametric model is 
n m 
p(x, y\a,0)=Y[ N(Xi | Q, 1) J] N(V> I A !). 
for which, writing 0 = (a, 0) the Fisher information matrix is easily seen to be 
HB{B) = HM=§ °). 
Suppose now that we make the one-to-one transformation ip = (</>, A) = (a0, a/0) = 
g(a, 0) = g(6), so that <j> = a0 is taken to be the parameter of interest and A = a/0 is 
taken to be the nuisance parameter. Such a parameter of interest arises, for example, when 
332 
5 Inference 
inference about the area of a rectangle is required from data consisting of measurements of 
its sides. 
The Jacobian of the inverse transformation is given by 
JgM1>)- 
da da \ 
dlj> ~d\ 
d£ d0_ 
d<f> d\) 
f(-:)"2 © 
vUv aUJ 
1/2 . 
1/2 
nm 
and hence, using Corollary 1 to Proposition 5.17 
JfyW) =Jtg-^)He{g-\xl>))Jg-x{i,) = ^ 
ThTYl 
with |ffy(V>)| =^j.sothat 
4> \m n) \m n) 
\m nj \m nX) 
7r(A|<£)oc|/i22(^A)rzoc 
A \m nA/ 
1/2 
The question now arises as to what constitutes a "natural" sequence {A;(<£)}, over which to 
define the normalised ^(A | (j>) required by Definition 5.9. A natural increasing sequence of 
subsets of the original parameter space, SJ+ x SJ+, for (a, 0) would be the sets 
Si = {(a,0); O<a<i,O<0<i}, i = 1,2,..., 
which transform, in the space of A 6 A, into the sequence 
H4>) 
b ** 
<A< 
We note that unlike in the previous cases we have considered, this does depend on (j>. 
To complete the analysis, it can be shown, after some manipulation, that, for large i, 
n(\14>) = .,f™ r^\-x (- + \) 
1/2 
and 
nifa) = * / — + — A log — + — dX, 
i(y/m + y/n) JA.W \m nXJ \m nX) 
i (y/m + sjn) 
which leads to a reference prior relative to the ordered parametrisation (<£, A) given by 
^W'^(A + _L) 
1/2 
5.4 Reference Analysis 
333 
In the original parametrisation, this corresponds to 
tt(q, A) oc (no2 + m/?2)1/2, 
which depends on the sample sizes through the ratio m/n and reduces, in the case n = m, 
to 7t(q, 0) oc (a2 + 02)l/2, a form originally proposed for this problem in an unpublished 
1982 Stanford University technical report by Stein, who showed that it provides approximate 
agreement between Bayesian credible regions and classical confidence intervals for (j>. For a 
detailed discussion of this example, and of the consequences of choosing a different sequence 
Aj(0), see Berger and Bernardo (1989). 
We note that the preceding example serves to illustrate the fact that reference 
priors may depend explicitly on the sample sizes defined by the experiment. There 
is, of course, nothing paradoxical in this, since the underlying notion of a reference 
analysis is a "minimally informative" prior relative to the actual experiment to be 
performed. 
5.4.5 Multiparameter Problems 
The approach to the nuisance parameter case considered above was based on the 
use of an ordered parametrisation whose first and second components were (<f>, A), 
referred to, respectively, as the parameter of interest and the nuisance parameter. 
The reference prior for the ordered parametrisation (<f>, A) was then constructed by 
conditioning to give the form 7r(A | 4>)it{4>). 
When the model parameter vector 9 has more than two components, this  
successive conditioning idea can obviously be extended by considering 9 as an ordered 
parametrisation, (9i,. ..,0m), say, and generating, by successive conditioning, a 
reference prior, relative to this ordered parametrisation, of the form 
7T(0) = 7T(0m | 0U . . . , 6m-l) ■ ■ ■ 7T(02 I 0lM0l). 
In order to describe the algorithm for producing this successively conditioned 
form, in the standard, regular case we shall first need to introduce some notation. 
Assuming the parametric model p(x \9), 9 6 6, to be such that the Fisher 
information matrix 
H^ = -E-l0{^ogP(X\9)} 
has full rank, we define S(9) = H~l (9), define the component vectors 
9W = (9u...,9j), O]j] = (0j+l,...,9m), 
and denote by Sj (9) the corresponding upper left j x j submatrix of S(9), and by 
hj(9) the lower right element of SJ1^). 
Finally, we assume that 9 = 9ix •• x9m,with^j 6 9j, and, fori = 1,2,..., 
we denote by {9'}, I — 1,2,..., an increasing sequence of compact subsets of 9,, 
and define 9f,., = 9',, x •• • x 9' . 
334 5 Inference 
Proposition 5.30. {Ordered reference priors under asymptotic normality). 
With the above notation, and under regularity conditions extending those of 
Proposition 5.29 in an obvious way, the reference prior ir{9), relative to the 
orderedparametrisation {6\,..., 9m), is given by 
tt(0) = lim -^k for some 9* G 6, 
(->oo 7r(0 ) 
where nl(6) is defined by the following recursion: 
(i) For j = m, and 0m G Qlm, 
nlm (eMieh-*) =nlm(em\eu...,em^) = {£mftd0m • 
(ii) For j = m - 1, m — 2,..., 2, and 9j G 8j, 
*' l^-11'e )- ^ y°m ' ° J ye,P{E[log{hj(e)Yft]}d0j ' 
3 
where 
E\ hog^O)}1/2] = j logih^fW^ (OU]\0W) d9m. 
(Hi) For j = 1, 0[O] = 9, with 0'°' vacuous, and 
7r'(0)=7ri(0[o]|0l°]) 
Proof. This follows closely the development given in Proposition 5.29. For 
details see Berger and Bernardo (1992a, 1992b, 1992c). < 
The derivation of the ordered reference prior is greatly simplified if the {hj (0)} 
terms in the above depend only on #b1: even greater simplification obtains if H(0) 
is block diagonal, particularly, if, for j = 1,..., m, the jth term can be factored 
into a product of a function of 0j and a function not depending on 9}. 
Corollary. Ifhj(6) depends only on 9^\ j = 1,..., m, then 
0 6 6'. 
u \\iei{hmyi2^ 
1 i 
5.4 Reference Analysis 
335 
lfH{9) is block diagonal (i.e., 9\,..., Om are mutually orthogonal), with 
/hu(0) 0 ... 0 
V 0 0 ••• hmm(9). 
then hj(9) = hjj(9),j = 1,..., m. Furthermore, if, in this latter case, 
where gj{9) does not depend on 9j, and if the 8j 's do not depend on 9, then 
m 
Proof The results follow from the recursion of Proposition 5.29. < 
The question obviously arises as to the appropriate ordering to be adopted in 
any specific problem. At present, no formal theory exists to guide such a choice, 
but experience with a wide range of examples suggests that—at least for non- 
hierarchical models (see Section 4.6.5), where the parameters may have special 
forms of interrelationship—the best procedure is to order the components of 9 on 
the basis of their inferential interest. 
Example 5.20. (Reference analysis for m normal means). Let e„ be an experiment 
which consists in obtaining {xl5..., x„}, n > 2, a random sample from the multivariate 
normal model Nm(x | /x, rlm), m > 1, for which the Fisher information matrix is easily 
seen to be 
if(/X'T)=(TJm mn/(2r*)) 
It follows from Proposition 5.30 that the reference prior relative to the natural parametrisation 
(/ii,...,/im,-r), is given by 
7r(/ii,...,/im,-r) oct"1. 
Clearly, in this example the result does not, in fact, depend on the order in which the 
parametrisation is taken, since the parameters are all mutually orthogonal. 
The reference prior 7r(//i,..., fim, t) oc t-1 or 7r(/i!,..., /im, a) oc a'1 if we  
parametrise in terms of a = t~1/2, is thus the appropriate reference form if we are interested in 
any of the individual parameters. The reference posterior for any fij is easily shown to be 
the Student density 
■n(n}\xu...,xn) =St(/ij \Xj,(n- l)s"2,m(n - 1)) 
T T* . -=- \ ' 
:5ZH(»« 
nXj = y xij, nms = y j < 
which agrees with the standard argument according to which one degree of freedom should 
be lost by each of the unknown means. 
336 
5 Inference 
Example 5.21. (Multinomial model). Let x = {ru ..., rm} be an observation from 
a multinomial distribution (see Section 3.2), so that 
p(ri,...,rm|0i,...,< 
ri!---rm!(n-Er<)! 
from which the Fisher information matrix 
1 + 0i - X0i 
0?-"C(l-E0O""&i. 
H(0\,.. .,6n 
1 
1 
1 + 02 - £0; 
1 
1 
1 + 8m — E0j 
is easily derived, with 
|H| = 
m \ m 
<=i / <=i j 
In this case, the conditional reference priors derived using Proposition 5.28 turn out to 
be proper, and there is no need to consider subset sequences {0(}. In fact, noting that 
H~l (0i,..., 0m) is given by 
0i(l-0i) -0i02 
— 0102 02(1 — 02) 
v\Vm 
02 8 m 
#m(l — dm) 
we see that the conditional asymptotic precisions used in Proposition 5.29 are easily  
identified, and hence that 
7T(0,- | »!,..., fl^OoC 
i + ECift 
i + ZUOi 
V2 ,-X 
The required reference prior relative to the ordered parametrisation (9i,..., 0m), say, is then 
given by 
7T(0l, . . . , 0m) OC 7T(0l)7r(02 | 0l) • • • 7T(0m | 01, . . . , 0m_i) 
OC 0rV2(l - 0l)-1/202-1/2(l - 0, ~ 02)-l/2 ■ ■ ■ 0"1/2(l - 01 0m)-l/\ 
and corresponding reference posterior for 0i is 
7r(0i |n,...,rm) oc / p(n,..., )7r(0i,...,0m)d02. -.dOm, 
5.4 Reference Analysis 
337 
which is proportional to 
/^-^...^-^(l-Eft)""5*-172 
x(l-»1)-1/J(l-«1-JI)-V2."(l-«1 
After some algebra, this implies that 
em)-^d92---de„ 
7r(6»i I n,..., rm) = Be (6>i I n + |, n - n + i) , 
which, as one could expect, coincides with the reference posterior which would have been 
obtained had we initially collapsed the multinomial analysis to a binomial model and then 
carried out a reference analysis for the latter. Clearly, by symmetry considerations, the 
above analysis applies to any 6iti = 1,..., m, after appropriate changes in labelling and 
it is independent of the particular order in which the parameters are taken. For a detailed 
discussion of this example see Berger and Bernardo (1992a). Further comments on ordering 
of parameters are given in Section 5.6.2. 
Example5.22. (Normalcorrelation coefficient). Let{xi,. 
pie from a bivariate normal distribution, A^x | /x, t), where 
, xn } be a random sam- 
M 
\pa\i 
2 
I 
02 
p<J\02 
Suppose that the correlation coefficient p is the parameter of interest, and consider the ordered 
parametrisation {p, p,\, \i2, 0i, a2}. It is easily seen that 
H(p,p,x,p,2,ax,a2) 
so that 
H1 
1,02) = (1 ~ 
(1-P2)2 
0 
0 
?*-•> 
?,(i-V) 
p2)-1 
0 
*? 
pa\(Ji 
0 
0 
r J- + /3 
i-p2 
0 
0 
ZR 
01 
ZR 
- a2 
0 
pcj\(j2 
a2 
0 
0 
0 
1 
~P 
0\02 
0 
0 
TP(1- 
P2 
0 
0 
0? 
2 
0 
~P 
7l<T2 
1 
0 
0 
-P2) 
0\<J2 
2 
z£ 
01 
0 
0 
2-P2 
0? 
Z± 
0\02 
°2 /I 
TP(1- 
0 
0 
Z 
p 
02 
0 
0 
z± 
0102 
2-P2 
022 
P2)" 
2 0102 
2 
338 
5 Inference 
After some algebra it can be shown that this leads to the reference prior 
t(p.Mi,M2,^i,^2) oc (l-p2)"1^-1^""1, 
whatever ordering of the nuisance parameters p.\,^2,cr\,a2 is taken. This agrees with 
Lindley's (1965, p. 219) analysis. Furthermore, as one could expect from Fisher's (1915) 
original analysis, the corresponding reference posterior distribution for p 
(1 _ p2)(n-3)/2 (1 ! 1 1 + 
tt^Ixl-.^oc (i_^)n^/aF^-,-,n--,-r- 
(where F is the hypergeometric function), only depends on the data through the sample  
correlation coefficient r, whose sampling distribution only depends on p. For a detailed analysis 
of this example, see Bayarri (1981); further discussion will be provided in Section 5.6.2. 
See, also, Hills (1987), YeandBerger(1991)andBergerandBernardo(1992b) 
for derivations of the reference distributions for avariety of other interesting models. 
Infinite discrete parameter spaces 
The infinite discrete case presents special problems, due to the non-existence of an 
asymptotic theory comparable to that of the continuous case. It is, however, often 
possible to obtain an approximate reference posterior by embedding the discrete 
parameter space within a continuous one. 
Example 5.23. (Infinite discrete case). In the context of capture-recapture problems, 
suppose it is of interest to make inferences about an integer 6 € {1,2,...} on the basis of a 
random sample z = {x\,..., xn} from 
For several plausible "diffuse looking" prior distributions for 9 one finds that the  
corresponding posterior virtually ignores the data. Intuitively, this has to be interpreted as suggesting 
that such priors actually contain a large amount of information about 6 compared with that 
provided by the data. A more careful approach to providing a "non-informative" prior is 
clearly required. One possibility would be to embed the discrete space {1, 2,...} in the 
continuous space ]0, oo[ since, for each 9 > 0, p{x\6) is still a probability density for x. 
Then, using Proposition 5.24, the appropriate refrence prior is 
tt(0) oc h(9)xl2 oc (6> + I)"1*"1 
and it is easily verified that this prior leads to a posterior in which the data are no longer 
overwhelmed. If the physical conditions of the problem require the use of discrete 6 values, 
one could always use, for example, 
p(0 = l|z)=/ n(9\z)de, P(6=j\z)= n(6\z)de, j>l 
JO Jj-l/2 
as an approximate discrete reference posterior. 
5.5 Numerical Approximations 
339 
Prediction and Hierarchical Models 
Two classes of problems that are not covered by the methods so far discussed are 
hierarchical models and prediction problems. The difficulty with these problems 
is that there are unknowns (typically the unknowns of interest) that have specified 
distributions. For instance, if one wants to predict y based on z when (y, z) has 
density p(y, z | 9), the unknown of interest is y, but its distribution is conditionally 
specified. One needs a reference prior for 9, not y. Likewise, in a hierarchical 
model with, say, p.\, p,2, ■ ■ ■, Mj> being N(/Xj | /xo, A), the /x/s may be the parameters 
of interest but a prior is only needed for the hyperparameters hq and A. 
The obvious way to approach such problems is to integrate out the variables 
with conditionally known distributions (y in the predictive problem and the {ii,} in 
the hierarchical model), and find the reference prior for the remaining parameters 
based on this marginal model. The difficulty that arises is how to then identify 
parameters of interest and nuisance parameters to construct the ordering necessary 
for applying the reference prior method, the real parameters of interest having been 
integrated out. 
In future work, we propose to deal with this difficulty by defining the parameter 
of interest in the reduced model to be the conditional mean of the original parameter 
of interest. Thus, in the prediction problem, £[y|0] (which will be either 9 or some 
transformation thereof) will be the parameter of interest, and in the hierarchical 
model E\pii | p,0, M = Mo will be defined to be the parameter of interest. This 
technique has so far worked well in the examples to which it has been applied, but 
further study is clearly needed. 
5.5 NUMERICAL APPROXIMATIONS 
Section 5.3 considered forms of approximation appropriate as the sample size  
becomes large relative to the amount of information contained in the prior distribution. 
Section 5.4 considered the problem of approximating a prior specification maximi s- 
ing the expected information to be obtained from the data. In this section, we shall 
consider numerical techniques for implementing Bayesian methods for arbitrary 
forms of likelihood and prior specification, and arbitrary sample size. 
We note that the technical problem of evaluating quantities required for  
Bayesian inference summaries typically reduces to the calculation of a ratio of two  
integrals. Specifically, given a likelihood p(x 19) and a prior density p(9), the starting 
point for all subsequent inference summaries is the joint posterior density for 9 
given by 
p(x 19)p(9) 
p(9 | a;) 
Jp(x\9)p(9)d9 
From this, we may be interested in obtaining univariate marginal posterior densities 
for the components of 9, bivariate joint marginal posterior densities for pairs of 
340 
5 Inference 
components of 0, and so on. Alternatively, we may be interested in marginal 
posterior densities for functions of components of 6 such as ratios or products. 
In all these cases, the technical key to the implementation of the formal solution 
given by Bayes' theorem, for specified likelihood and prior, is the ability to perform 
a number of integrations. First, we need to evaluate the denominator in Bayes' 
theorem in order to obtain the normalising constant of the posterior density; then 
we need to integrate over complementary components of 0, or transformations 
of 6, in order to obtain marginal (univariate or bivariate) densities, together with 
summary moments, highest posterior density intervals and regions, or whatever. 
Except in certain rather stylised problems (e.g., exponential families together with 
conjugate priors), the required integrations will not be feasible analytically and, 
thus, efficient approximation strategies will be required. 
In this section, we shall outline five possible numerical approximation  
strategies, which will be discussed under the subheadings: Laplace Approximation; 
Iterative Quadrature; Importance Sampling; Sampling-importance-resampling; 
Markov Chain Monte Carlo. An exhaustive account of these and other methods 
will be given in the second volume in this series, Bayesian Computation. 
5.5.1 Laplace Approximation 
We motivate the approximation by noting that the technical problem of evaluating 
quantities required for Bayesian inference summaries, is typically that of evaluating 
an integral of the form 
E[g(0)\x} = J g{d)p{d\x)d0, 
where p(6 | x) is derived from a predictive model with an appropriate representation 
as a mixture of parametric models, and g(6) is some real-valued function of interest. 
Often, g{6) is a first or second moment, and since p(6 \ x) is given by 
Jp(x\O)p(O)d0 
we see that E[g(6) \ x)] has the form of a ratio of two integrals. 
Focusing initially on this situation of a required inference summary for g(6), 
and assuming g(6) almost everywhere positive, we note that the posterior  
expectation of interest can be written in the form 
E[g(0)\x}= jeM_nh(e)}de 
5.5 Numerical Approximations 341 
where, with the vector x = {x\,..., xn) of observations fixed, the functions h(0) 
and h*(0) are defined by 
-nh(0) = \ogp(6) + \ogp(x | 6), 
-nh*{6) = log<7(0) + \ogp{0) + \ogp{x | 6). 
Let us consider first the case of a single unknown parameter, 6 = 6 6 3?, and define 
9,6* and a, a* such that 
-h{6) = sup {-h(6)} , a = [h"(d)}~1/2 
e 
-h*(d*) = SUP {-/!*(«)} , <7* = [/l*"(0)]"1/2 
Assuming h(-), h*(-) to be suitably smooth functions, the Laplace approximations 
for the two integrals defining the numerator and denominator of E[g(9) | x] are 
given (see, for example, Jeffreys and Jeffreys, 1946) by 
VW-n"1/2 exp { -nh* (9*)} 
and 
27r<7n-1/2 exp j-n/i(0)j . 
Essentially, the approximations consist of retaining quadratic terms in Taylor  
expansions of h(-) and h*(-), and are thus equivalent to normal-like approximations 
to the integrands. In the context we are considering, it then follows immediately 
that the resulting approximation for E[g(6) \ x] has the form 
E[g(6) | x] = (y) exp [-n [h*(6*) - h(0)] } , 
and Tierney and Kadane (1986) have shown that 
Elg(0)\x}=E[g(6)\x}{l+O(n-2)). 
The Laplace approximation approach, exploiting the fact that Bayesian inference 
summaries typically involve ratios of integrals, is thus seen to provide a potentially 
very powerful general approximation technique. See, also, Tierney, Kass and 
Kadane (1987, 1989a, 1989b), Kass, Tierney and Kadane (1988, 1989a, 1989b, 
1991) and Wong and Li (1992) for further underpinning of, and extensions to, this 
methodology. 
Considering now the general case of 6 6 3?fc, the Laplace approximation to 
the denominator of E[g(6) \ x) is given by 
I exp{-nh(6)}<10 = (27r)fc/2 \nV2h(0) '^exp |-n/i(0)} , 
342 
5 Inference 
where 0 is defined by 
and 
-h(0) = sup h(0) 
e 
V2h(0) 
d2h(0) 
M] 
ddiddj 
0=0, 
the Hessian matrix of h evaluated at 0, with an exactly analogous expression for 
the numerator, defined in terms of h*(-) and 0*. Writing 
a — 
nV2h(0) 
-1/2 
a* = \nV2h*(0*)\~1/2, 
the Laplace approximation to E[g(0) \ x] is given by 
E{g{0) | x] = (?p\ exp {-nh*(0*) - h(0)} , 
completely analogous to the univariate case. 
If 0 = (<p, A) and the required inference summary is the marginal posterior 
density for <p, application of the Laplace approximation approach corresponds to 
obtaining p(<p | x) pointwise by fixing (j> in the numerator and defining g(X) = 1. 
It is easily seen that this leads to 
p{4> | x) (x / exp < —n/i(x(A) > dX 
(x \v2h(t>(X<j>)\ exp|-n/i0(A^,)| • 
where 
-nh^X) = logp(0, A) + logp(x | 0, A), 
considered as a function of A for fixed <p, and 
-M-M = -sup/ty(A). 
The form p{(j> \ x) thus provides (up to proportionality) a pointwise approximation 
to the ordinates of the marginal posterior density for (j>. Considering this form in 
more detail, we see that, if p((f), A) is constant, 
p{4> | x) (x \-V2 logp(x | 0, A^,) p(x \4>,X<p)- 
5.5 Numerical Approximations 
343 
The form V2 \ogp(x | </>, A^,) is the Hessian of the log-likelihood function,  
considered as a function of A for fixed 0, and evaluated at the value A^ which maximises 
the log-likelihood over A for fixed </>; the form p(x | </>, A^) is usually called the 
profile likelihood for (j>, corresponding to the parametric model p(x \ cj>, A). The 
approximation to the marginal density for (j> given by p{6 \ x) has a form often 
referred to as the modified profile likelihood (see, for example, Cox and Reid, 
1987, for a convenient discussion of this terminology). Approximation to Bayes- 
ian inference summaries through Laplace approximation is therefore seen to have 
links with forms of inference summary proposed and derived from a non-Bayesian 
perspective. For further references, see Appendix B, Section 4.2. 
In relation to the above analysis, we note that the Laplace approximation is 
essentially derived by considering normal approximations to the integrands  
appearing in the numerator and denominator of the general form E[g{8) \x\. If the 
forms concerned are not well approximated by second-order Taylor expansions of 
the exponent terms of the integrands, which may be the case with small or  
moderate samples, particularly when components of 6 are constrained to ranges other 
than the real line, we may be able to improve substantially on this direct Laplace 
approximation approach. 
One possible alternative, at least if 6 = 6 is a scalar parameter, is to attempt 
to approximate the integrands by forms other than normal, perhaps resembling 
more the actual posterior shapes, such as gammas or betas. Such an approach has 
been followed in the one-parameter case by Morris (1988), who develops a general 
approximation technique based around the Pearson family of densities. These are 
characterised by parameters m, /xo and a quadratic function Q, which specify a 
density for 6 of the form 
where 
qQ(0\m,fi0)=KQ{m,(io)^r > 
Q(0)=qo + qiO + q202 
and the range of 6 is such that 0 < Q(6) < oo. 
It is shown by Morris (1988) that, for a given choice of quadratic function 
Q, an analogue to the Laplace-type approximation of an integral of a unimodal 
function f(9) is given by 
344 
5 Inference 
where m = r"(6)Q{6) and 6 maximises r(6) = log[f(6)Q(6)}. Details of the 
forms of K~l, Q and p for familiar forms of Pearson densities are given in Morris 
(1988), where it is also shown that the approximation can often be further simplified 
to the expression 
/ 
f(6)d6 
[_r"(£)]i/2 
A second alternative is to note that the version of the Laplace approximation 
proposed by Tierney and Kadane (1986) is not invariant to changes in the  
(arbitrary) parametrisation chosen when specifying the likelihood and prior density 
functions. It may be, therefore, that by judicious reparametrisation (of the  
likelihood, together with the appropriate, Jacobian adjusted, prior density) the Laplace 
approximation can itself be made more accurate, even in contexts where the original 
parametrisation does not suggest the plausibility of a normal-type approximation 
to the integrands. We, note, incidentally, that such a strategy is also available in 
multiparameter contexts, whereas the Pearson family approach does not seem so 
readily generalisable. 
To provide a concrete illustration of these alternative analytic approximation 
approaches consider the following. 
Example 5.24. (Approximating the mean of a beta distribution). 
Suppose that a posterior beta distribution, Be(01 r„ - \, n — rn + |), has arisen from 
a Bi(r-n | n, 6) likelihood, together with, Be(0 \\,\) prior (the reference prior, derived in 
Example 5.14). Writing r„ = x, we can, in fact, identify the analytic form of the posterior 
mean in this case, 
E[0 | x] = 
71+ 1 
but we shall ignore this for the moment and examine approximations implied by the  
techniques discussed above. 
First, defining g(0) = 9, we see, after some algebra, that the Tierney-Kadane form of 
the Laplace approximation gives the estimated posterior mean 
E[8\x] = ^—^ ( T 2 ' 
If, instead, we reparametrise to <t> = sin"1 \/0, the required integrals are defined in terms of 
g(<P) = sin2 <t>, p(x\<P)oc{sin2<P)x(l-sm2<t>)n-\ tt(0) oc 1, 
and the Laplace approximation can be shown to be 
nn+1'2(x+l)x+1 
E[0 | x] = 
(n + l)"+3/2^ 
5.5 Numerical Approximations 
345 
Alternatively, if we work via the Pearson family, with Q(9) = 9(1 - 9) as the "natural" 
choice for a beta-like posterior, we obtain 
JHflH=(n+1)"+1/23(* + 3/2rl- 
By considering the percentage errors of estimation, defined by 
true - estimated 
100 x 
true 
we can study the performance of the three estimates for various values of n and x. Details 
are given in Achcar and Smith (1989); here, we simply summarise, in Table 5.1, the results 
for n = 5, x — 3, which typify the performance of the estimates for small n. 
Table 5.1 Approximation ofE{91 a;] from Be((? \x +\,n- x+ \) 
(percentage errors in parentheses) 
True value Laplace 
E[9\x] 
0.583 0.563 
(3.6%) 
approximations 
E{9 | x] 
0.580 
(0.6%) 
Pearson approximation 
E*[9\x] 
0.585 
(0.3%) 
We see from Table 5.1 that the Pearson approximation, which is, in some sense, preselected to 
be best, does, in fact, outperform the others. However, it is striking that the performance of the 
Laplace approximation under reparametrisation leads to such a considerable improvement 
over that based on the original parametrisation, and is a very satisfactory alternative to the 
"optimal" Pearson form. Further examples are given in Achcar and Smith (1989). 
In general, it would appear that, in cases involving a relatively small number of 
parameters, the Laplace approach, in combination with judicious reparametrisation, 
can provide excellent approximations to general Bayesian inference summaries, 
whether in the form of posterior moments or marginal posterior densities. However, 
in multiparameter contexts there may be numerical problems with the evaluation 
of local derivatives in cases where analytic forms are unobtainable or too tedious to 
identify explicitly. In addition, there are awkward complications if the integrands 
are multimodal. At the time of writing, this area of approximation theory is very 
much still an active research field and the full potential of this and related methods 
(see, also, Lindley, 1980b, Leonard et ai, 1989) has yet to be clarified. 
346 
5 Inference 
5.5.2 Iterative Quadrature 
It is well known that univariate integrals of the type 
/ e-' f(t)dt 
are often well approximated by Gauss-Hermite quadrature rules of the form 
n 
^WifiU), 
2 = 1 
where U is the ith zero of the Hermite polynomial Hn(t). In particular, if f(t) is 
a polynomial of degree at most 2n — 1, then the quadrature rule approximates the 
integral without error. This implies, for example, that, if h(t) is a suitably well 
behaved function and 
g(t) = h(t) (27ra2)-1/2exp { - \ (^j } . 
then 
/oo n 
g(t)dt^^2mig(zi), 
-°° i=\ 
where 
rrii — Wi exp(t*)V2a, zt = /x + \f2aU 
(see, for example, Naylor and Smith, 1982). 
It follows that Gauss-Hermite rules are likely to prove very efficient for  
functions which, expressed in informal terms, closely resemble "polynomial x normal" 
forms. In fact, this is a rather rich class which, even for moderate n (less than 12, 
say), covers many of the likelihood x prior shapes we typically encounter for 
parameters defined on (—oo, oo). Moreover, the applicability of this  
approximation is vastly extended by working with suitable transformations of parameters 
defined on other ranges such as (0, oo) or (a, 6), using, for example, log(t) or 
log(t — a) — log(6 - t), respectively. Of course, to use the above form we must 
specify /x and a in the normal component. It turns out that, given reasonable  
starting values (from any convenient source, prior information, maximum likelihood 
estimates, etc.), we typically can successfully iterate the quadrature rule,  
substituting estimates of the posterior mean and variance obtained using previous values of 
rrii and Zi. Moreover, we note that if the posterior density is well-approximated 
by the product of a normal and a polynomial of degree at most 2n — 3, then an 
n-point Gauss-Hermite rule will prove effective for simultaneously evaluating the 
normalising constant and the first and second moments, using the same (iterated) 
5.5 Numerical Approximations 
347 
set of mi and zt. In practice, it is efficient to begin with a small grid size (n = 4 or 
n = 5) and then to gradually increase the grid size until stable answers are obtained 
both within and between the last two grid sizes used. 
Our discussion so far has been for the one-dimensional case. Clearly,  
however, the need for an efficient strategy is most acute in higher dimensions. The 
"obvious" extension of the above ideas is to use a cartesian product rule giving the 
approximation 
/(*„ ..., tk)dtx ...dtk*J2 m^s(zn\ • • ■. *($l 
«* 
where the grid points and the weights are found by substituting the appropriate 
iterated estimates of /x and a2 corresponding to the marginal component tj. 
The problem with this "obvious" strategy is that the product form is only  
efficient if we are able to make an (at least approximate) assumption of posterior 
independence among the individual components. In this case, the lattice of  
integration points formed from the product of the two one-dimensional grids will 
efficiently cover the bulk of the posterior density. However, if high posterior  
correlations exist, these will lead to many of the lattice points falling in areas of negligible 
posterior density, thus causing the cartesian product rule to provide poor estimates 
of the normalising constant and moments. 
To overcome this problem, we could first apply individual parameter  
transformations of the type discussed above and then attempt to transform the resulting 
parameters, via an appropriate linear transformation, to a new, approximately  
orthogonal, set of parameters. At the first step, this linear transformation derives from 
an initial guess or estimate of the posterior covariance matrix (for example, based 
on the observed information matrix from a maximum likelihood analysis).  
Successive transformations are then based on the estimated covariance matrix from the 
previous iteration. 
The following general strategy has proved highly effective for problems  
involving up to six parameters (see, for example, Naylor and Smith, 1982, Smith et 
al, 1985, 1987, Naylor and Smith, 1988). 
(1) Reparametrise individual parameters so that the resulting working parameters 
all take values on the real line. 
(2) Using initial estimates of the joint posterior mean vector and covariance  
matrix for the working parameters, transform further to a centred, scaled, more 
"orthogonal" set of parameters. 
(3) Using the derived initial location and scale estimates for these "orthogonal" 
parameters, carry out, on suitably dimensioned grids, cartesian product  
integration of functions of interest. 
/-/ 
348 
5 Inference 
(4) Iterate, successively updating the mean and covariance estimates, until stable 
results are obtained both within and between grids of specified dimension. 
For problems involving larger numbers of parameters, say between six and 
twenty, cartesian product approaches become computationally prohibitive and  
alternative approaches to numerical integration are required. 
One possibility is the use of spherical quadrature rules (Stroud, 1971,  
Sections 2.6, and 2.7), derived by transforming from cartesian to spherical polar  
coordinates and constructing optimal integration formulae based on symmetric  
configurations over concentric spheres. Full details of this approach will be given in the 
volume Bayesian Computation. For a brief introduction, see Smith (1991). Other 
relevant references on numerical quadrature include Shaw (1988b), Flournoy and 
Tsutakawa (1991), O'Hagan (1991) and Dellaportas and Wright (1992). 
The efficiency of numerical quadrature methods is often very dependent on the 
particular parametrisation used. For further information on this topic, see Marriott 
(1988), Hills and Smith (1992, 1993) and Marriott and Smith (1992). For related 
discussion, see Kass and Slate (1992). 
The ideas outlined above relate to the use of numerical quadrature formulae 
to implement Bayesian statistical methods. It is amusing to note that the roles 
can be reversed and Bayesian statistical methods used to derive optimal numerical 
quadrature formulae! See, for example, Diaconis (1988b) and O'Hagan (1992). 
5.5.3 Importance Sampling 
The importance sampling approach to numerical integration is based on the  
observation that, if / is a function and g is a probability density function 
Jf(x)dx = J\ 
-I 
= EG 
M 
9{x)_ 
g(x)dx 
dG{x) 
Mx). 
which suggest the "statistical" approach of generating a sample from the distribution 
function G—referred to in this context as the importance sampling distribution— 
and using the average of the values of the ratio f/g as an unbiased estimator of 
J f(x)dx. However, the variance of such an estimator clearly depends critically 
on the choice of G, it being desirable to choose g to be "similar" to the shape of /. 
In multiparameter Bayesian contexts, exploitation of this idea requires  
designing importance sampling distributions which are efficient for the kinds of integrands 
arising in typical Bayesian applications. A considerable amount of work has  
focused on the use of multivariate normal or Student forms, or modifications thereof, 
5.5 Numerical Approximations 
349 
much of this work motivated by econometric applications. We note, in particular, 
the contributions of Kloek and van Dijk (1978), van Dijk and Kloek (1983, 1985), 
van Dijk et al. (1987) and Geweke (1988, 1989). 
An alternative line of development (Shaw, 1988a) proceeds as follows. In the 
univariate case, if we choose g to be heavier-tailed than /, and if we work with 
y = G(x), the required integral is the expected value of f[G~1(x)]/g[G~l(x)] 
with respect to a uniform distribution on the interval (0,1). Owing to the periodic 
nature of the ratio function over this interval, we are likely to get a reasonable 
approximation to the integral by simply taking some equally spaced set of points 
on (0,1), rather than actually generating "uniformly distributed" random numbers. 
If / is a function of more than one argument (k, say), an exactly parallel argument 
suggess that the choice of a suitable g followed by the use of a suitably selected 
"uniform" configuration of points in the fc-dimensional unit hypercube will provide 
an efficient multidimensional integration procedure. 
However, the effectiveness of all this depends on choosing a suitable G, bearing 
in mind that we need to have available a flexible set of possible distributional shapes, 
for which G~l is available explicitly. In the univariate case, one such family defined 
on 3? is provided by considering the random variable 
xa — a h{u) — (I — a) h(l — u), 
where u is uniformly distributed on (0,1), h : (0,1) —> 3? is a monotone increasing 
function such that 
lim h(u) = — oo 
u—>0 
and 0 < a < 1 is a constant. The choice a = 0.5 leads to symmetric distributions; 
as a —> 0 or a —> lwe obtain increasingly skew distributions (to the left or right). 
The tail-behaviour of the distributions is governed by the choice of the function h. 
Thus, for example, h(u) = log(u) leads to a family whose symmetric member is the 
logistic distribution; h(u) = — tan [7r(l — u)/2] leads to afamily whose symmetric 
member is the Cauchy distribution. Moreover, the moments of the distributions of 
the xa are polynomials in a (of corresponding order), the median is linear in a, etc., 
so that sample information about such quantities provides (for any given choice of 
h) operational guidance on the appropriate choice of a. To use this family in the 
multiparameter case, we again employ individual parameter transformations, so 
that all parameters belong to 3?, together with "orthogonalising" transformations, 
so that parameters can be treated "independently". In the transformed setting, it 
is natural to consider an iterative importance sampling strategy which attempts to 
learn about an appropriate choice of G for each parameter. 
As we remarked earlier, part of this strategy requires the specification of  
"uniform" configurations of points in the fc-dimensional unit hypercube. This problem 
has been extensively studied by number theorists and systematic experimentation 
350 
5 Inference 
with various suggested forms of "quasi-random" sequences has identified effective 
forms of configuration for importance sampling purposes: for details, see Shaw 
(1988a). The general strategy is then the following. 
(1) Reparametrise individual parameters so that resulting working parameters all 
take values on the real line. 
(2) Using initial estimates of the posterior mean vector and covariance matrix for 
the working parameters, transform to a centred, scaled, more "orthogonal" set 
of parameters. 
(3) In terms of these transformed parameters, set 
k 
J=l 
for "suitable" choices of gj,j = 1,..., k. 
(4) Use the inverse distribution function transformation to reduce the problem to 
that of calculating an average over a "suitable" uniform configuration in the 
fc-dimensional hypercube. 
(5) Use information from this "sample" to learn about skewness, tailweight, etc. 
for each gj, and hence choose "better" gj, j = 1,..., k, and revise estimates 
of the mean vector and covariance matrix. 
(6) Iterate until the sample variance of replicate estimates of the integral value is 
sufficiently small. 
Teichroew (1965) provides a historical perspective on simulation techniques. 
For further advocacy and illustration of the use of (non-Markov-chain) Monte Carlo 
methods in Bayesian Statistics, see Stewart (1979,1983, 1985, 1987), Stewart and 
Davis (1986), Shao (1989, 1990) and Wolpert (1991). 
5.5.4 Sampling-importance-resampling 
Instead of just using importance sampling to estimate integrals—and hence  
calculate posterior normalising constants and moments—we can also exploit the idea 
in order to produce simulated samples from posterior or predictive distributions. 
This technique is referred to by Rubin (1988) as sampling-importance-resampling 
(SIR). 
We begin by taking a fresh look at Bayes' theorem from this sampling- 
importance-resampling perspective, shifting the focus in Bayes' theorem from 
densities to samples. Our account is based on Smith and Gelfand (1992). 
As a first step, we note the essential duality between a sample and the  
distribution from which it is generated: clearly, the distribution can generate the sample; 
conversely, given a sample we can re-create, at least approximately, the distribution 
5.5 Numerical Approximations 
351 
(as a histogram, an empirical distribution function, a kernel density estimate, or 
whatever). In terms of densities, Bayes' theorem defines the inference process as 
the modification of the prior density p(8) to form the posterior density p(6 | x), 
through the medium of the likelihood function p(x 18). Shifting to a sampling 
perspective, this corresponds to the modification of a sample from p{0) to form a 
sample from p{6 \ x) through the medium of the likelihood function p(x \ 8). 
To gain insight into the general problem of how a sample from one density 
may be modified to form a sample from a different density, consider the following. 
Suppose that a sample of random quantities has been generated from a density g (8), 
but that what it is required is a sample from the density 
,,<<»= m 
}f(e)de 
where only the functional form of f(8) is specified. Given f(8) and the sample 
from g(6), how can we derive a sample from h{8)l 
In cases where there exists an identifiable constant M > 0 such that 
f(8)/g(8)<M, for all 8, 
an exact sampling procedure follows immediately from the well known rejection 
method for generating random quantities (see, for example, Ripley, 1987, p. 60): 
(i) consider a 8 generated fromg{0); 
(ii) generate u from Un(u | 0,1); 
(iii) if u < f(G)/Mg(6) accept 8; otherwise repeat (i)-(iii). 
Any accepted 8 is then a random quantity from h(6). Given a sample of size 
N for g{6), it is immediately verified that the expected sample size from h{8) is 
M-1Njf(x)dx. 
In cases where the bound M in the above is not readily available, we can 
approximate samples from h(8) as follows. Given 8i,...,8N fromg(8), calculate 
" — where w, = 
If we now draw 8* from the discrete distribution {8U...,8N} having mass qt 
on 8t, then 8* is approximately distributed as a random quantity from h(8). To 
see this, consider, for mathematical convenience, the univariate case. Then, under 
appropriate regularity conditions, if P describes the actual distribution of 9*, 
N 
P(0*<a)=£gjl(-oo,a](0i) 
i=l 
n 1Tli=lwi1(-oo.a](ei) 
n~ 
1 ET=i «* 
352 
5 Inference 
so that 
lim P(0* <a) = 
E lm\ 
a E9\W)i 
f f(0)d8 
^ = / H0) 
/ f(0) M J-oc 
J — oo 
Since sampling with replacement is not ruled out, the sample size generated 
in this case can be as large as desired. Clearly, however, the less h(0) resembles 
g{9) the larger N will need to be if the distribution of 6* is to be a reasonable 
approximation to h(9). 
With this sampling-importance-resampling procedure in mind, let us return 
to the prior to posterior sample process defined by Bayes' theorem. For fixed x, 
define fx(6) = p(x \ 6)p{0). Then, if 6 maximising p(x | 6) is available, the 
rejection procedure given above can be applied to a sample for p(6) to obtain a 
sample from p(6 \ x) by taking g(6) — p{6), f(6) = fx(0) and M = p(x \ 6). 
Bayes' theorem then takes the simple form: 
For each 6 in the prior sample, accept 6 into the posterior sample with  
probability 
fx(0) =P(x\0) 
Mp(0) p(x | 9) 
The likelihood therefore acts in an intuitive way to define the resampling 
probability: those 0 with high likelihoods are more likely to be represented in 
the posterior sample. Alternatively, if M is not readily available, we can use the 
approximate resampling method, which selects 0t into the posterior sample with 
probability 
p{x\6i) 
Qi = 
ZLp(x\0j) 
Again we note that this is proportional to the likelihood, so that the inference process 
via sampling proceeds in an intuitive way. 
The sampling-resampling perspective outlined above opens up the possibility 
of novel applications of exploratory data analytic and computer graphical  
techniques in Bayesian statistics. We shall not pursue these ideas further here, since the 
topic is more properly dealt with in the subsequent volume Bayesian Computation. 
For an illustration of the method in the context of sensitivity analysis and intractable 
reference analysis, see Stephens and Smith (1992); for pedagogical illustration, see 
Albert (1993). 
5.5 Numerical Approximations 
353 
5.5.5 Markov Chain Monte Carlo 
The key idea is very simple. Suppose that we wish to generate a sample from a 
posterior distribution p(0\x) for 6 G 6 c K* but cannot do this directly. However, 
suppose that we can construct a Markov chain with state space 6, which is  
straightforward to simulate from, and whose equilibrium distribution is p(0\x). If we then 
run the chain for a long time, simulated values of the chain can be used as a basis 
for summarising features of the posterior p{6\x) of interest. To implement this 
strategy, we simply need algorithms for constructing chains with specified  
equilibrium distributions. For recent accounts and discussion, see, for example, Gelfand 
and Smith (1990), Casella and George (1992), German and Rubin (1992a, 1992b), 
Geyer(1992), Raftery and Lewis (1992), Ritter and Tanner (1992), Roberts (1992), 
Tierney (1992), Besag and Green (1993), Chan (1993), Gilks et al. (1993) and 
Smith and Roberts (1993); see, also, Tanner and Wong (1987) and Tanner (1991). 
Under suitable regularity conditions, asymptotic results exist which clarify the 
sense in which the sample output from a chain with equilibrium distribution p{6 \ x) 
can be used to mimic a random sample from p(0\x) or to estimate the expected 
value, with respect to p(6\x), of a function g(0) of interest. 
If 6l,62,... ,0l,... is a realisation from an appropriate chain, typically  
available asymptotic results as t —> oo include 
0l —> 0 ~ p(6\x), in distribution 
and 
1 l 
-J29(0l) -* Eo\x{9(0)} almost surely. 
«=i 
Clearly, successive 0l will be correlated, so that, if the first of these asymptotic 
results is to be exploited to mimic a random sample from p(6\x), suitable spacings 
will be required between realisations used to form the sample, or parallel  
independent runs of the chain might be considered. The second of the asymptotic results 
implies that ergodic averaging of a function of interest over realisations from a 
single run of the chain provides a consistent estimator of its expectation. 
In what follows, we outline two particular forms of Markov chain scheme, 
which have proved particularly convenient for a range of applications in Bayesian 
statistics. 
The Gibbs Sampling Algorithm 
Suppose that 6, the vector of unknown quantities appearing in Bayes' theorem, has 
components 0\,..., #*, and that our objective is to obtain summary inferences from 
the joint posterior p{6 \ x) = p{0\,..., 9k\x). As we have already observed in this 
section, except in simple, stylised cases, this will typically lead, unavoidably, to 
challenging problems of numerical integration. 
354 
5 Inference 
In fact, this apparent need for sophisticated numerical integration technology 
can often be avoided by recasting the problem as one of iterative sampling of random 
quantities from appropriate distributions to produce an appropriate Markov chain. 
To this end, we note that 
p(0i\x,9j,j ^i), i = l,...,k, 
the so-called full conditional densities for the individual components, given the data 
and specified values of all the other components of 8, are typically easily identified, 
as functions of #,, by inspection of the form of p(61 x) oc p(x\ 6)p{6) in any given 
application. Suppose then, that given an arbitrary set of starting values, 
for the unknown quantities, we implement the following iterative procedure: 
draw 0{x) fromp^! | x, flf,..., 0<o)), 
draw OP from p(92 \ x, 9{'\9f\ ..., 0<O)), 
draw 0^ from p(93 \ x, 9^, 9^, ef\ ■ ■ ■, 9f]), 
draw 0™ fromp(9k \ x, 0?\ ..., 9^\), 
draw 9f] fromp{9x1 x, 0§\ ..., 9{kx)), 
and so on. 
Now suppose that the above procedure is continued through t iterations and 
is independently replicated m times so that from the current iteration we have m 
replicates of the sampled vector 0l = (of*,..., 9^), where 8l is a realisation of a 
Markov chain with transition probabilities given by 
n(6t,et+1) = Y[p(9j+1 {9], j > I, 9]+\j < I, x). 
Then (see, for example, Geman and Geman, 1984, Roberts and Smith, 1993), 
as t —> oo, (9\',..., #[ ) tends in distribution to a random vector whose joint 
density is p{6 \ x). In particular, 9^ tends in distribution to a random quantity 
whose density is p(9j \ x). Thus, for large t, the replicates (9){,... ,0^) are 
approximately a random sample from p(#, | a;). It follows, by making m suitably 
5.5 Numerical Approximations 
355 
large, that an estimate p(0i \ x) for p(0« | x) is easily obtained, either as a kernel 
density estimate derived from (©,■{',..., 0^), or from 
1 m 
m i=i 
So far as sampling from the p(6i \ x, Of[,j ¥" i) is concerned, i = 1 k,  
either the full conditionals assume familiar forms, in which case computer routines 
are typically already available, or they are simple arbitrary mathematical forms, in 
which case general stochastic simulation techniques are available—such as  
envelope rejection and ratio of uniforms—which can be adapted to the specific forms 
(see, for example, Devroye, 1986, Ripley, 1987, Wakefield et al., 1991, Gilks, 
1992, Gilks and Wild, 1992, and Dellaportas and Smith, 1993). See, also, Carlin 
and Gelfand (1991). 
The potential of this iterative scheme for routine implementation of Bayesian 
analysis has been demonstrated in detail for a wide variety of problems: see, for 
example, Gelfand and Smith (1990), Gelfand et al. (1990) and Gilks et al. (1993). 
We shall not provide a more extensive discussion here, since illustration of the 
technique in complex situations more properly belongs to the second volume of 
this work. We note, however, that simulation approaches are ideally suited to 
providing summary inferences (we simply report an appropriate summary of the 
sample), inferences for arbitrary functions of Q\,..., Ok (we simply form a sample 
of the appropriate function from the samples of the 0« 's) orpredictions (for example, 
in an obvious notation, p(y \ x) = m"1 YaL\ p(v I &i )> tne average being over 
the 8\', which have an approximate p(6 \ x) distribution for large t). 
The Metropolis-Hastings algorithm 
This algorithm constructs a Markov chain 01,02,..., 6l,... with state space 6 and 
equilibrium distribution p(6\x) by defining the transition probability from 6l = 6 
to the next realised state 6t+1 as follows. 
Let q(8,0') denote a (for the moment arbitrary) transition probability function, 
such that, if 0l = 0, the vector 0' drawn from q(6,6') is considered as a proposed 
possible value for 6t+1. However, a further randomisation now takes place. With 
some probability a(0,6'), we actually accept Gt+l = 0'\ otherwise, we reject the 
value generated from q{6,6') and set0(+1 = 6. This construction defines a Markov 
chain with transition probabilities given by 
' q(0,0')a(O,O') \f0'^0, 
p{W)={ 
l-^q(0,0")a(0,6") if0' = 0. 
356 
5 Inference 
If now we set 
1 if p(6\x)q(6,6') = 0, 
it is easy to check thatp(0|a;)p(0,6') = p(0\x)p(6', d), which, provided that the 
thus far arbitrary q(6,6') is chosen to be irreducible and aperiodic on a suitable 
state space, is a sufficient condition for p(6\x) to be the equilibrium distribution of 
the constructed chain. 
This general algorithm is due to Hastings (1970); see, also, Metropolis et 
al. (1953), Peskun (1973), Tierney (1992), Besag and Green (1993), Roberts and 
Smith (1993) and Smith and Roberts (1993). It is important to note that the  
(equilibrium) distribution of interest, p(G\x), only enters p{6,6') through the ratio 
p(6'\x)/p(6\x). This is quite crucial since it means that knowledge of the  
distribution up to proportionality (given by the likelihood multiplied by the prior) is 
sufficient for implementation. 
5.6 DISCUSSION AND FURTHER REFERENCES 
5.6.1 An Historical Footnote 
Blackwell (1988) gave a very elegant demonstration of the way in which a simple 
finite additivity argument can be used to give powerful insight into the relation 
between frequency and belief probability. The calculation involved has added 
interest in that—according to Stigler (1982)—it might very well have been made 
by Bayes himself. 
The argument goes as follows. Suppose that 0-1 observables x\,..., xn+i are 
finitely exchangeable. We observe x = (x\,..., xn) and wish to evaluate 
P(xn+i = \\x) 
P(xn+1 = 0\x) ' 
Writing s ~ x\ + • • • + xn, p(t) = P(x\ + ■ ■ ■ + xn+\ = t), this ratio, by 
virtue of exchangeability, is easily seen to be equal to 
p(s+1V(s+i) p(S+i) s+i 
M/i n + n P(s) n-s + 1 
P(s)/ [ s 
if p(s) ~ p(s + 1) and s and n — s are not too small. 
5.6 Discussion and Further References 
357 
This can be interpreted as follows. If, before observing x, we considered s 
and s + 1 to be about equally plausible as values for xH h xn+\, the resulting 
posterior odds for xn+\ = 1 will be essentially the frequency odds based on the 
first n trials. 
Inverting the argument, we see that if one wants to have this "convergence" 
of beliefs and frequencies it is necessary thatp(s) « p(s + 1). But what does this 
entail? 
Reverting to an infinite exchangeability assumption, and hence the familiar 
binomial framework, suppose we require thatp(#) be chosen such that 
p(*) = jf (")^(i-*)""W)<» 
does not depend on s. An easy calculation shows that this is satisfied if p{6) is 
taken to be uniform on (0,1)—the so-called Bayes (or Bayes-Laplace) Postulate. 
Stigler (1982) has argued that an argument like the above could have been 
Bayes' motivation for the adoption of this uniform prior. 
5.6.2 Prior Ignorance 
To many attracted to the formalism of the Bayesian inferential paradigm, the idea 
of a non-informative prior distribution, representing "ignorance" and "letting the 
data speak for themselves" has proved extremely seductive, often being regarded 
as synonymous with providing objective inferences. It will be clear from the  
general subjective perspective we have maintained throughout this volume, that we 
regard this search for "objectivity" to be misguided. However, it will also be clear 
from our detailed development in Section 5.4 that we recognise the rather special 
nature and role of the concept of a "minimally informative" prior specification 
—appropriately defined! In any case, the considerable body of conceptual and  
theoretical literature devoted to identifying "appropriate" procedures for formulating 
prior representations of "ignorance" constitutes a fascinating chapter in the history 
of Bayesian Statistics. In this section we shall provide an overview of some of the 
main directions followed in this search for a Bayesian "Holy Grail". 
In the early works by Bayes (1763) and Laplace (1814/1952), the definition of 
a non-informative prior is based on what has now become known as the principle of 
insufficient reason, or the Bayes-Laplace postulate (see Section 5.6.1). According 
to this principle, in the absence of evidence to the contrary, all possibilities should 
have the same initial probability. This is closely related to the so-called Laplace- 
Bertrand paradox; see Jaynes (1971) for an interesting Bayesian resolution. 
In particular, if an unknown quantity, tj>, say, can only take a finite number of 
values, M, say, the non-informative prior suggested by the principle is the discrete 
uniform distribution p((j>) = {1/M,..., 1/M}. This may, at first sight, seem 
358 
5 Inference 
intuitively reasonable, but Example 5.16 showed that even in simple, finite, discrete 
cases care can be required in appropriately defining the unknown quantity of interest. 
Moreover, in countably infinite, discrete cases the uniform (now improper) prior is 
known to produce unappealing results. Jeffreys (1939/1961, p. 238) suggested, for 
the case of the integers, the prior 
7r(n) oc n_1, n=l,2, 
More recently, Rissanen (1983)useda coding theory argument to motivate the prior 
/ x 1 1 1 
7r(n) oc — x x x ..., n = 1,2 
n log n log log n 
However, as indicated in Example 5.23, embedding the discrete problem within a 
continuous framework and subsequently discretising the resulting reference prior 
for the continuous case may produce better results. 
If the space, 4>, of (j> values is a continuum (say, the real line) the principle of 
insufficient reason has been interpreted as requiring a uniform distribution over 4>. 
However, a uniform distribution for 4> implies a non-uniform distribution for any 
non-linear monotone transformation of (j> and thus the Bayes-Laplace postulate is 
inconsistent in the sense that, intuitively, "ignorance about (j>" should surely imply 
"equal ignorance" about a one-to-one transformation of (j>. Specifically, if some 
procedure yields p{tj>) as a non-informative prior for (t> and the same procedure 
yields p{Q as a non-informative prior for a one-to-one transformation C, = £(<£) of 
0, consistency would seem to demand \hdXp{C,)dC, = p(4>)d(j>; thus, a procedure for 
obtaining the "ignorance" prior should presumably be invariant under one-to-one 
reparametrisation. 
Based on these invariance considerations, Jeffreys (1946) proposed as a non- 
informative prior, with respect to an experiment e = {X, <j>,p(x | <f>)}, involving 
a parametric model which depends on a single parameter 0, the (often improper) 
density 
7T(0) OC h{^'\ 
where 
K4>) = ~ P(x\ <t>)-QjploSP(x \<t>)dx . 
In effect, Jeffreys noted that the logarithmic divergence locally behaves like 
the square of a distance, determined by a Riemannian metric, whose natural length 
element is /i(0)'/2, and that natural length elements of Riemannian metrics are 
invariant to reparametrisation. In an illuminating paper, Kass (1989) elaborated 
on this geometrical interpretation by arguing that, more generally, natural volume 
elements generate "uniform" measures on manifolds, in the sense that equal mass 
5.6 Discussion and Further References 
359 
is assigned to regions of equal volume, the essential property that makes Lebesgue 
measure intuitively appealing. 
In his work, Jeffreys explored the implications of such a non-informative prior 
for a large number of inference problems. He found that his rule (by definition 
restricted to a continuous parameter) works well in the one-dimensional case, but 
can lead to unappealing results (Jeffreys, 1939/1961, p. 182) when one tries to 
extend it to multiparameter situations. 
The procedure proposed by Jeffreys' preferred rule was rather ad hoc, in that 
there are many other procedures (some of which he described) which exhibit the 
required type of invariance. His intuition as to what is required, however, was 
rather good. Jeffreys' solution for the one-dimensional continuous case has been 
widely adopted, and a number of alternative justifications of the procedure have 
been provided. 
Perks (1947) used an argument based on the asymptotic size of confidence 
regions to propose a non-informative prior of the form 
-k{4>) oc s{4>)~1 
where s(0) is the asymptotic standard deviation of the maximum likelihood estimate 
of <p. Under regularity conditions which imply asymptotic normality, this turns out 
to be equivalent to Jeffreys' rule. 
Lindley (1961b) argued that, in practice, one can always replace a continuous 
range of 0 by discrete values over a grid whose mesh size, 6(0), say, describes the 
precision of the measuring process, and that a possible operational interpretation of 
"ignorance" is a probability distribution which assigns equal probability to all points 
of this grid. In the continuous case, this implies a prior proportional to 6(0)-1. 
To determine 6(0) in the context of an experiment e = {X, 0,p(x | 0)}, Lindley 
showed that if the quantity can only take the values 0 or 0 + 6(0), the amount of 
information that e may be expected to provide about 0, if p(0) = p(<f> + 6(0)) = 
5, is 262(0)/i(0). This expected information will be independent of 0 if 6(0) 
oc /i(0)~1'/2) thus defining an appropriate mesh; arguing as before, this suggests 
Jeffreys' prior ir((j>) oc h(6)xl2. Akaike (1978a) used a related argument to justify 
Jeffreys' prior as "locally impartial". 
Welch and Peers (1963) and Welch (1965) discussed conditions under which 
there is formal mathematical equivalence between one-dimensional Bayesian  
credible regions and corresponding frequentist confidence intervals. They showed that, 
under suitable regularity assumptions, one-sided intervals asymptotically coincide 
if the prior used for the Bayesian analysis is Jeffreys' prior. Peers (1965) later 
showed that the argument does not extend to several dimensions. Hartigan (1966b) 
and Peers (1968) discuss two-sided intervals. Tibshirani (1989), Mukerjee and Dey 
(1993) and Nicolau (1993) extend the analysis to the case where there are nuisance 
parameters. 
360 
5 Inference 
Hartigan (1965) reported that the prior density which minimises the bias of 
the estimator d of 4> associated with the loss function l(d, (j>) is 
\d2 i"1/2 
*(<!>) = Hi) Qj2l(d><t>) 
If, in particular, one uses the discrepancy measure 
l{d,(j>) = Jp(x\(j>)\og^^dx 
as a natural loss function (see Definition 3.15), this implies that n((j>) = h{4>)112, 
which is, again, Jeffreys' prior. 
Good (1969) derived Jeffreys' prior as the "least favourable" initial distribution 
with respect to a logarithmic scoring rule, in the sense that it minimises the expected 
score from reporting the true distribution. Since the logarithmic score is proper, 
and hence is maximised by reporting the true distribution, Jeffreys' prior may 
technically be described, under suitable regularity conditions, as a minimax solution 
to the problem of scientific reporting when the utility function is the logarithmic 
score function. Kashyap (1971) provided a similar, more detailed argument; an 
axiom system is used to justify the use of an information measure as a payoff 
function and Jeffreys' prior is shown to be a minimax solution in a —two person— 
zero sum game, where the statistician chooses the "non-informative" prior and 
nature chooses the "true" prior. 
Hartigan (1971,1983, Chapter 5) defines a similarity measure for events E, F 
to be P{EC\F)/P{E)P{F) andshows that Jeffreys' prior ensures, asymptotically, 
constant similarily for current and future observations. 
Following Jeffreys (1955), Box and Tiao (1973, Section 1.3) argued for  
selecting a prior by convention to be used as a standard of reference. They suggested 
that the principle of insufficient reason may be sensible in location problems, and 
proposed as a conventional prior /k{4>) for a model parameter </> that 7r(<£) which 
implies a uniform prior 
7T(C) = 7T(<£) 
dC-1 
oc c 
dtj> 
for a function C, = C,{4>) such mat P(x I 0 is, at least approximately, a location 
parameter family; that is, such that, for some functions g and /, 
P(* 10) ~ 0[CW-/(*)]■ 
Using standard asymptotic theory, they showed that, under suitable regularity  
conditions and for large samples, this will happen if 
C(4>) = [* h(4>)1/2d*t>, 
5.6 Discussion and Further References 
361 
i.e., if the non-informative prior is Jeffreys' prior. For a recent reconsideration and 
elaboration of these ideas, see Kass (1990), who extends the analysis by  
conditioning on an ancillary statistic. 
Unfortunately, although many of the arguments summarised above generalise 
to the multiparameter continuous case, leading to the so-called multivariate Jeffreys' 
rule 
tt(0)oc |.H"(0)|1/2, 
where 
/f\2 
p(x\e)-^-logp(x\6)dx 
is Fisher's information matrix, the results thus obtained typically have intuitively 
unappealing implications. An example of this, pointed out by Jeffreys himself 
(Jeffreys, 1939/1961 p. 182) is provided by the simple location-scale problem, 
where the multivariate rule leads to n(0, a) oc a~2, where 0 is the location and a 
the scale parameter. See, also, Stein (1962). 
Example 5.25. (Univariate normal model). Let {x\,... ,xn} be a random sample 
from N{x | n, A), and consider a = A-1/2, the (unknown) standard deviation. In the case of 
known mean, \x = 0, say, the appropriate (univariate) Jeffreys' prior is 7r(cr) oc <x_1 and the 
posterior distribution of a would be such that [E"=1x2]/cr2 is \2n- In the case of unknown 
mean, if we used the multivariate Jeffreys' prior n(n, <r) oc a~2 the posterior distribution 
of a would be such that [E"=1(x, — x)2]/a2 is, again, x2,- This is widely recognised as 
unacceptable, in that one does not lose any degrees of freedom even though one has lost 
the knowledge that \i = 0, and conflicts with the use of the widely adopted reference prior 
7r(//, <t) = a~x (see Example 5.17 in Section 5.4), which implies that [E"=1(x; — x)2]/a2 is 
V2 
A.n-1- 
The kind of problem exemplified above led Jeffreys to the ad hoc  
recommendation, widely adopted in the literature, of independent a priori treatment of location 
and scale parameters, applying his rule separately to each of the two subgroups of 
parameters, and then multiplying the resulting forms together to arrive at the overall 
prior specification. For an illustration of this, see Geisser and Cornfield (1963): for 
an elaboration of the idea, see Zellner (1986a). 
At this point, one may wonder just what has become of the intuition  
motivating the arguments outlined above. Unfortunately, although the implied information 
limits are mathematically well-defined in one dimension, in higher dimensions the 
forms obtained may depend on the path followed to obtain the limit. Similar  
problems arise with other intuitively appealing desiderata. For example, the Box and 
Tiao suggestion of a uniform prior following transformation to a parametrisation 
ensuring data translation generalises, in the multiparameter setting, to the  
requirement of uniformity following a transformation which ensures that credible regions 
362 
5 Inference 
are of the same size. The problem, of course, is that, in several dimensions, such 
regions can be of the same size but very different in form. 
Jeffreys' original requirement of invariance under reparametrisation remains 
perhaps the most intuitively convincing. If this is conceded, it follows that, whatever 
their apparent motivating intuition, approaches which do not have this property 
should be regarded as unsatisfactory. Such approaches include the use of limiting 
forms of conjugate priors, as in Haldane (1948), Novick and Hall (1965), Novick 
(1969), DeGroot (1970, Chapter 10) and Piccinato (1973, 1977), a predictivistic 
version of the principle of insufficient reason, Geisser (1984), and different forms 
of information-theoretical arguments, such as those put forward by Zellner (1977, 
1991), Geisser (1979) and Torgesen (1981). 
Maximising the expected information (as opposed to maximising the expected 
missing information) gives invariant, but unappealing results, producing priors that 
can have finite support (Berger et al, 1989). Other information-based suggestions 
are those of Eaton (1982), Spall and Hill (1990) and Rodriguez (1991). 
Partially satisfactory results have nevertheless been obtained in multiparameter 
problems where the parameter space can be considered as a group of transformations 
of the sample space. Invariance considerations within such a group suggest the use 
of relatively invariant (Hartigan, 1964) priors like the Haar measures. This idea was 
pioneered by Barnard (1952). Stone (1965) recognised that, in an appropriate sense, 
it should be possible to approximate the results obtained using a non-informative 
prior by those obtained using a convenient sequence of proper priors. He went on 
to show that, if a group structure is present, the corresponding right Haar measure 
is the only prior for which such a desirable convergence is obtained. It is reassuring 
that, in those one-dimensional problems for which a group of transformations does 
exist, the right Haar measures coincides with the relevant Jeffreys' prior. For some 
undesirable consequences of the left Haar measure see Bernardo (1978b). Further 
developments involving Haar measures are provided by Zidek (1969), Villegas 
(1969,1971, 1977a, 1977b, 1981), Stone (1970), Florens (1978, 1982), Chang and 
Villegas (1986) and Chang and Eaves (1990). Dawid (1983b) provides an excellent 
review of work up to the early 1980's. However, a large group of interesting models 
do not have any group structure, so that these arguments cannot produce general 
solutions. 
Even when the parameter space may be considered as a group of  
transformations there is no definitive answer. In such situations, the right Haar measures are 
the obvious choices and yet even these are open to criticism. 
Example 5.26. (Standardised mean). Let x = {xi,..., xn} be a random sample 
from a normal distribution N{x \ fi, A). The standard prior recommended by group invariance 
arguments is n(/i, a) = a-1 where A = a"2. Although this gives adequate results if one 
wants to make inferences about either fi or a, it is quite unsatisfactory if inferences about the 
standardised mean 4> = \xja are required. Stone and Dawid (1972) show that the posterior 
5.6 Discussion and Further References 
363 
distribution of 0 obtained from such a prior depends on the data through the statistic 
(E^2)1'2' 
whose sampling distribution, 
p(t \n,a)= p(t | <t>) 
„ ( fi -I ("-3)/2 ,.00 ( 2 -\ 
only depends on <j>. One would, therefore, expect to be able to "match" the original inferences 
about <j> by the use of p(t \ <j>) together with some appropriate prior for 0. However, no such 
prior exists. 
On the other hand, the reference prior relative to the ordered partition (0, a) is (see 
Example 5.18) 
7r(0,a) = (2 + 02)-1/2^1 
and the corresponding posterior distribution for <j> is 
7r(0|x)oc(2 + 02)-1/2 e""*2/2 /""w-'expj-Y + A^w}dw| • 
We observe that the factor in square brackets is proportional to p(t \ <j>) and thus the  
inconsistency disappears. 
This type of marginalisation paradox, further explored by Dawid, Stone and 
Zidek (1973), appears in a large number of multivariate problems and makes it 
difficult to believe that, for any given model, a single prior may be usefully regarded 
as "universally" non-informative. Jaynes (1980) disagrees. 
An acceptable general theory for non-informative priors should be able to 
provide consistent answers to the same inference problem whenever this is posed 
in different, but equivalent forms. Although this idea has failed to produce a 
constructive procedure for deriving priors, it may be used to discard those methods 
which fail to satisfy this rather intuitive requirement. 
Example 5.27. (Correlation coefficient). Let (x, y) = {{x\, j/i),..., (xn, yn)} be a 
random sample from a bivariate normal distribution, and suppose that inferences about the 
correlation coefficient p are required. It may be shown that if the prior is of the form 
*(l*l, 1*2,01,02, p) = 7T(p)(T1"a(T2"a, 
which includes all proposed "non-informative" priors for this model that we are aware of, 
then the posterior distribution of p is given by 
■w(p\x,y) = n{p\r) 
W{P)(1 - p2)(n+2q-3)/2 
~ (1-pr)n+°-<5/2) 
F^h^ + a-l1-^) 
364 
5 Inference 
where 
= ^Xjyj-rixy 
r tEi(^-^)2]1/2[E,(2/i-¥)2]1/2 
is the sample correlation coefficient, and F is the hypergeometric function. This posterior 
distribution only depends on the data through the sample correlation coefficient r; thus, with 
this form of prior, r is sufficient. On the other hand, the sampling distribution of r is 
p(r\nuH2,cri,cr2,P) = p(r\p) 
_(l-p2)(n-1)/2(1_r2)(.-4)/2 / i l + pr\ 
(1-pr)""3/2 V'2' *' 2 j' 
Moreover, using the transformations <5 = tanh_1p and t = tanhrV, Jeffreys' prior for this 
univariate model is found to be n(p) oc (1 — p2)'1 (see Lindley, 1965, pp. 215-219). 
Hence one would expect to be able to match, using this reduced model, the posterior 
distribution n(p | r) given previously, so that 
7r(p|r)ocp(r|p)(l-p2)-1. 
Comparison between n(p \ r) and p{r \ p) shows that this is possible if and only if a = 1, 
and w(p) = (1 - p2)-1. Hence, to avoid inconsistency the joint reference prior must be of 
the form 
^{lM,fJ-2,crucr2,p) = (l-p2)-1^1^1, 
which is (see Example 5.22) the reference priorrelative to the natural order {p, nx,H2,01,0^2}- 
However, it is easily checked that Jeffreys' multivariate prior is 
ir(nu H2,(T\, Cr2, p) = (1 - p2)~3/2<7l2CT22 
and that the "two-step" Jeffreys' multivariate prior which separates the location and scale 
parameters is 
n(n,H2)ir(aua2,p) = (1 - p2)'3'2^^^■ 
For further detailed discussion of this example, see Bayarri (1981). 
Once again, this example suggests that different non-informative priors may 
be appropriate depending on the particular function of interest or, more generally, 
on the ordering of the parameters. 
Although marginalisation paradoxes disappear when one uses proper priors, to 
use proper approximations to non-informative priors as an approximate description 
of "ignorance" does not solve the problem either. 
5.6 Discussion and Further References 
365 
Example 5.28. (Stein'sparadox). Let x = {xi,..., xn} be a random sample from 
a multivariate normal distribution iV^x | /i, 1^}. Let ij be the mean of the n observations 
from coordinate i and let t = £\ 5?. The universally recommended "non-informative" prior 
for this model is 7r(/ii,..., /^) = 1, which may be approximated by the proper density 
m 
n(fiu...,fik) = J|jV(/ii|0,A), 
where A is very small. However, if inferences about <j> = JZ, A4? are desired, the use of this 
prior overwhelms, for large k, what the data have to say about 4>. Indeed, with such a prior 
the posterior distribution of n<j> is a non-central x2 distribution with k degrees of freedom 
and non-centrality parameter nt, so that 
E[4>\x]=t+±, V[4>\x] = l 
while the sampling distribution of nt is a non-central x2 distribution with k degrees of 
freedom and parameter n6 so that E[t | <f>] = <j> + kjn. Thus, with, say, k = 100, n = 1 and 
t = 200, we have E[<f> \ x] « 300, V[<j> \ x] ss 322, whereas the unbiased estimator based on 
the sampling distribution gives <j> = t — k w 100. 
However, the asymptotic posterior distribution of 4> is N{<j> \ <j>, (40)"1) and hence, by 
Proposition 5.2, the reference posterior for <j> relative to p{t \ (j>) is 
n((j> | x) oc n(4>)p(t | <j>) oc 0_1/2x2(nt | k, n<f>) 
whose mode is close to <t> . It may be shown that this is also the posterior distribution of <j> 
derived from the reference prior relative to the ordered partition {4>, ux,..., Uk-i}, obtained 
by reparametrising to polar coordinates in the full model. For further details, see Stein 
(1959), Efron (1973), Bernardo (1979b) and Femindiz (1982). 
Naive use of apparently "non-informative" prior distributions can lead to  
posterior distributions whose corresponding credible regions have untenable coverage 
probabilities, in the sense that, for some region C, the corresponding posterior 
probabilities P(C \ z) may be completely different from the conditional values 
P(C 16) for almost all 6 values. 
Such a phenomenon is often referred to as strong inconsistency (see, for  
example, Stone, 1976). However, by carefully distinguishing between parameters of 
interest and nuisance parameters, reference analysis avoids this type of  
inconsistency. An illuminating example is provided by the reanalysis by Bernardo (1979b, 
reply to the discussion) of Stone's (1976) Flatland example. For further discussion 
of strong inconsistency and related topics, see Appendix B, Section 3.2. 
Jaynes (1968) introduced a more general formulation of the problem. He 
allowed for the existence of a certain amount of initial "objective" information and 
then tried to determine a prior which reflected this initial information, but nothing 
2t+- 
n 
366 
5 Inference 
else (see, also, Csiszar, 1985). Jaynes considered the entropy of a distribution to be 
the appropriate measure of uncertainty subject to any "objective" information one 
might have. If no such information exists and <j> can only take a finite number of 
values, Jaynes' maximum entropy solution reduces to the Bayes-Laplace postulate. 
His arguments are quite convincing in the finite case; however, if 4> is continuous, 
the non-invariant entropy functional, H{p{cj))} = — Jp(4>) \ogp(cj))d4>, no longer 
has a sensible interpretation in terms of uncertainty. Jaynes' solution is to introduce 
a "reference" density ir((f>) in order to define an "invariantised" entropy, 
-J «»***$« 
and to use the prior which maximises this expression, subject, again, to any initial 
"objective" information one might have. Unfortunately, 7r(<£) must itself be a  
representation of ignorance about <j> so that no progress has been made. If a convenient 
group of transformations is present, Jaynes suggests invariance arguments to select 
the reference density. However, no general procedure is proposed. 
Context-specific "non-informative" Bayesian analyses have been produced for 
specific classes of problems, with no attempt to provide a general theory. These  
include dynamic models (Pole and West, 1989) and finite population survey sampling 
(Meeden and Vardeman, 1991). 
The quest for non-informative priors could be summarised as follows. 
(i) In the finite case, Jaynes' principle of maximising the entropy is convincing, 
but cannot be extended to the continuous case. 
(ii) In one-dimensional continuous regular problems, Jeffreys' prior is appropriate. 
(iii) The infinite discrete case can often be handled by suitably embedding the 
problem within a continuous framework. 
(iv) In continuous multiparameter situations there is no hope for a single, unique, 
"non-informative prior", appropriate for all the inference problems within a 
given model. To avoid having the prior dominating the posterior for some 
function (j> of interest, the prior has to depend not only on the model but also 
on the parameter of interest or, more generally, on some notion of the order 
of importance of the parameters. 
The reference prior theory introduced in Bernardo (1979b) and developed in 
detail in Section 5.4 avoids most of the problems encountered with other proposals. 
It reduces to Jaynes' form in the finite case and to Jeffreys' form in one-dimensional 
regular continuous problems, avoiding marginalisation paradoxes by insisting that 
the reference prior be tailored to the parameter of interest. However, subsequent 
work by Berger and Bernardo (1989) has shown that the heuristic arguments in 
Bernardo (1979b) can be misleading in complicated situations, thus necessitating 
more precise definitions. Moreover, Berger and Bernardo (1992a, 1992b, 1992c) 
5.6 Discussion and Further References 
367 
showed that the partition into parameters of interest and nuisance parameter may 
not go far enough and that reference priors should be viewed relative to a given 
ordering—or, more generally, a given ordered grouping—of the parameters. This 
approach was described in detail in Section 5.4. Ye (1993) derives reference priors 
for sequential experiments. 
A completely different objection to such approaches to non-informative priors 
lies in the fact that, for continuous parameters, they depend on the likelihood  
function. This is recognised to be potentially inconsistent with a personal interpretation 
of probability. For many subjectivists, the initial density p{<j>) is a description of 
the opinions held about (j>, independent of the experiment performed; 
why should one's knowledge, or ignorance, of a quantity depend on the  
experiment being used to determine it? Lindley (1972, p. 71). 
In many situations, we would accept this argument. However, as we argued 
earlier, priors which reflect knowledge of the experiment can sometimes be  
genuinely appropriate in Bayesian inference, and may also have a useful role to play 
(see, for example, the discussion of stopping rules in Section 5.1.4) as technical 
devices to produce reference posteriors. Posteriors obtained from actual prior  
opinions could then be compared with those derived from a reference analysis in order 
to assess the relative importance of the initial opinions on the final inference. 
In general we feel that it is sensible to choose a non-informative prior which 
expresses ignorance relative to information which can be supplied by a particular 
experiment. If the experiment is changed, then the expression of relative  
ignorance can be expected to change correspondingly. (Box and Tiao, 1973, p. 46). 
Finally, "non-informative" distributions have sometimes been criticised on the 
grounds that they are typically improper and may lead, for instance, to  
inadmissible estimates (see, e.g. Stein, 1956). However, sensible "non-informative" priors 
may be seen to be, in an appropriate sense, limits of proper priors (Stone, 1963, 
1965,1970; Stein, 1965; Akaike, 1980a). Regarded as a "baseline" for admissible 
inferences, posterior distributions derived from "non-informative" priors need not 
be themselves admissible, but only arbitrarily close to admissible posteriors. 
However, there can be no final word on this topic! For example, recent work 
by Eaton (1992), Clarke and Wasserman (1993), George and McCulloch (1993b) 
and Ye (1993) seems to open up new perspectives and directions. 
5.6.3 Robustness 
In Section 4.8.3, we noted that some aspects of model specification, either for the 
parametric model or the prior distribution components, can seem arbitrary, and cited 
368 
5 Inference 
as an example the case of the choice between normal and Student-t distributions 
as a parametric model component to represent departures of observables from their 
conditional expected values. In this section, we shall provide some discussion of 
how insight and guidance into appropriate choices might be obtained. 
We begin our discussion with a simple, direct approach to examining the ways 
in which a posterior density for a parameter depends on the choices of parametric 
model or prior distribution components. Consider, for simplicity, a single  
observable x € 5ft having a parametric density p(x\0), with 9 £ 5ft having prior density 
p(9). The mechanism of Bayes' theorem, 
pm = p(y, 
p(x) 
involves multiplication of the two model components, p(x\6), p(9), followed by 
normalisation, a somewhat "opaque" operation from the point of view of comparing 
specifications of p(x\9) orp(9) on a "what if?" basis. 
However, suppose we take logarithms in Bayes' theorem and subsequently 
differentiate with respect to 9. This now results in a linear form 
r\ r\ r\ 
— \ogp(e\x) = —\ogP(x\e) + — \og P(9). 
The first term on the right-hand side is (apart from a sign change) a quantity known 
in classical statistics as the efficient score function (see, for example, Cox and 
Hinkley, 1974). On the linear scale, this is the quantity which transforms the prior 
into the posterior and hence opens the way, perhaps, to insight into the effect of a 
particular choice of p(x\9) given the form of p{9). See, for example, Ramsey and 
Novick (1980) and Smith (1983). Conversely, examination of the second term on 
the right-hand side for given p(x\9) may provide insight into the implications of 
the mathematical specification of the prior. 
For convenience of exposition—and perhaps because the prior component is 
often felt to be the less secure element in the model specification—we shall focus 
the following discussion on the sensitivity of characteristics of p(9 \ x) to the choice 
of p{9). Similar ideas apply to the choice of p(x \ 9). 
With x denoting the mean of n independent observables from a normal  
distribution with mean 9 and precision A, we shall illustrate these ideas by considering 
the form of the posterior mean for 9 when p(x\9) = N(x\9, n\) and p(9) is of 
"arbitrary" form. 
Defining 
p{x) = Jp(x\9)p(9)d9, 
= aiogpOr) f 
ox 
5.6 Discussion and Further References 
369 
it can be shown (see, for example, Pericchi and Smith, 1992) that 
E(6\x) = x-n~1X~ys(x). 
Suppose we carry out a "what if?" analysis by asking how the behaviour of 
the posterior mean depends on the mathematical form adopted for p(0). 
What if we take p(9) to be normaP. With p(9) = N(#|/z, Ao), the reader can 
easily verify that in this case p(x) will be normal, and hence s(x) will be a linear 
combination of x and the prior mean. The formula given for E(9\x) therefore 
reproduces the weighted average of sample and prior means that we obtained in 
Section 5.2, so that 
E{9\x) = (nX + X0)~l(nXx + A0/i). 
What if we take p(9) to be Student-tf With p{9) = St(0|/x, A0,q) the  
exact treatment of p(x) and s(x) becomes intractable. However, detailed analysis 
(Pericchi and Smith, 1992) provides the approximation 
E{e\x) = x- (Q + 1^-^ 
nA[aA01 + (x — /x)2] 
What if we take p(9) to be double-exponentiaP. In this case, 
for some u > 0, /x € 5ft and the evaluation of p(x) and s(x) is possible, but 
tedious. After some algebra—see Pericchi and Smith (1992)—it can be shown 
that, if6 = n"1i/-1A-1v/2, 
E(9\x) = w(x)(x + b) + [1 - w(x)}(x - b), 
where w(x) is a weight function, 0 < w(x) < 1, so that 
x - b < E(9\x) < x + b. 
Examination of the three forms for E(9\x) reveals striking qualitative  
differences. In the case of the normal, the posterior mean is unbounded in x — /x, the 
departure of the observed mean from the prior mean. In the case of the Student-t, 
we see that for very small x — \x the posterior mean is approximately linear in x — /x, 
like the normal, whereas for x — /x very large the posterior mean approaches x. 
In the case of the double-exponential, the posterior mean is bounded, with limits 
equal to x plus or minus a constant. 
370 
5 Inference 
Consideration of these qualitative differences might provide guidance  
regarding an otherwise arbitrary choice if, for example, one knew how one would like the 
Bayesian learning mechanism to react to an "outlying" x, which was far from ji. 
See Smith (1983) and Pericchi et al. (1993) for further discussion and elaboration. 
See Jeffreys (1939/1961) for seminal ideas relating to the effect of the tail-weight 
of the distribution of the parametric model on posterior inferences. Other relevant 
references include Masreliez (1975), O'Hagan (1979, 1981, 1988b), West (1981), 
Main (1988), Poison (1991), Gordon and Smith (1993) and O'Hagan and Le (1994). 
The approach illustrated above is well-suited to probing qualitative  
differences in the posterior by considering, individually, the effects of a small number 
of potential alternative choices of model component (parametric model or prior 
distribution). 
Suppose, instead, that someone has in mind a specific candidate component 
specification, po, say, but is all too aware that aspects of the specification have 
involved somewhat arbitrary choices. It is then natural to be concerned about 
whether posterior conclusions might be highly sensitive to the particular  
specification po, viewed in the context of alternative choices in an appropriately defined 
neighbourhood of po. 
In the case of specifying a parametric component p0—for example an  
"error" model for differences between observables and their (conditional) expected 
values—such concern might be motivated by definite knowledge of symmetry and 
unimodality, but an awareness of the arbitrariness of choosing a conventional  
distributional form such as normality. Here, a suitable neighbourhood might be formed 
by taking p0 to be normal and forming a class of distributions whose tail-weights 
deviate (lighter and heavier) from normal: see, for example, the seminal papers of 
Box and Tiao (1962,1964). 
In the case of specifying a prior component p0, such concern might be  
motivated by the fact that elicitation of prior opinion has only partly determined the 
specification (for example, by identifying a few quantiles), with considerable  
remaining arbitrariness in "filling out" the rest of the distribution. Here, a suitable 
neighbourhood of p0 might consist of a class of priors all having the specified  
quantiles but with other characteristics varying: see, for example, O'Hagan and Berger 
(1988). 
From a mathematical perspective, this formulation of the robustness problem 
presents some intriguing challenges. How to formulate interesting neighbourhood 
classes of distributions? How to calculate, with respect to such prior classes, bounds 
on posterior quantities of interest such as expectations or probabilities? 
At the time of writing, this is an area of intensive research. For example, 
should neighbourhoods be defined parametrically or non-parametrically? And, if 
nonparametrically, what measures of distance should be used to define a  
neighbourhood "close" to po? Should the elements, p, of the neighbourhood be those such 
that the density ratio p/p0 is bounded in some sense? Or such that the maximum 
5.6 Discussion and Further References 
371 
difference in the probability assigned to any event under p and p0 is bounded? Or 
such that p can be written as a "contamination" of p0,p = (1 — e)p0 + eq, for small 
e and q belonging to a suitable class? 
As yet, few issues seem to be resolved and we shall not, therefore, attept a  
detailed overview. Relevant references include; Edwards et al. (1963), Dawid (1973), 
Dempster (1975), Hill (1975), Meeden and Isaacson (1977), Rubin (1977, 1988a, 
1988b), Kadane and Chuang (1978), Berger (1980, 1982, 1985a), DeRobertis and 
Hartigan (1981), Hartigan (1983), Kadane (1984), Berger and Berliner (1986), 
Kempthorne (1986), Berger and O'Hagan (1988), Cuevas and Sanz (1988), Peric- 
chi and Nazaret (1988), Polasek and Potzelberger (1988, 1994), Carlin and  
Dempster (1989), Delampady (1989), Sivaganesan and Berger (1989,1993), Wasserman 
(1989, 1992a, 1992b), Berliner and Goel (1990), Delampady and Berger (1990), 
DoksumandLo(1990), Wasserman andKadane(1990,1992a, 1992b), Rfos (1990, 
1992), Angers and Berger (1991), Berger and Fan (1991), Berger and Mortera 
(1991b, 1994), Lavine (1991a, 1991b, 1992a, 1992b, 1994), Lavine et al. (1991, 
1993), Moreno and Cano (1991), Pericchi and Walley (1991), Potzelberger and Po- 
lasek(1991), Sivaganesan (1991), Walley (1991), Berger and Jefferys (1992), Gilio 
(1992b), Gomez-Villegas and Main (1992), Moreno and Pericchi (1992, 1993), 
Nau (1992), Sanso and Pericchi (1992), Liseo et al. (1993), Osiewalski and Steel 
(1993), Bayarri and Berger (1994), de la Horra and Fernandez (1994), Delampady 
and Dey (1994), O'Hagan (1994b), Pericchi and Perez (1994), Rfos and Martin 
(1994), Salinetti (1994). There are excellent reviews by Berger (1984a, 1985a, 
1990, 1994) and Wasserman (1992a), which together provide a wealth of further 
references. 
Finally, in the case of a large data sample, one might wonder whether the data 
themselves could be used to suggest a suitable form of parametric model component, 
thus removing the need for detailed specification and hence the arbitrariness of the 
choice. The so-called Bayesian bootstrap provides such a possible approach; see, 
for instance, Rubin (1981) and Lo (1987, 1993). However, since it is a heavily 
computationally based method we shall defer discussion to the volume Bayesian 
Computation. 
The term Bootstrap is more familiar to most statisticians as a computationally 
intensive frequentist data-based simulation method for statistical inference; in 
particular, as a computer-based method for assigning frequentist measures of 
accuracy to point estimates. For an introduction to the method-and to the related 
technique of jackknifing—see Efron (1982). For a recent textbook treatment, see 
Efron and Tibshirani (1993). See, also, Hartigan (1969, 1975). 
5.6.4 Hierarchical and Empirical Bayes 
In Section 4.6.5, we motivated and discussed model structures which take the form 
of an hierarchy. Expressed in terms of generic densities, a simple version of such 
372 
5 Inference 
an hierarchical model has the form 
p(x\0) =p(xi,...,xk\e1,...,ek) = Y[p(xi\Oi), 
8=1 
p{o\<f>)=p(o1,...,ok\<f>)=np(ol\<f>), 
p{4>). 
S=l 
The basic interpretation is as follows. Observables X\,...,xk are available 
from k different, but related, sources: for examples, k individuals in a homogeneous 
population, or k clinical trial centres involved in the same study. The first stage of 
the hierarchy specifies parametric model components for each of the k observables. 
But because of the "relatedness" of the k observables, the parameters 6\,..., 0k 
are themselves judged to be exchangeable. The second and third stages of the 
hierarchy thus provide a prior for 0 of the familiar mixture representation form 
p{6) = p(eu ...,ek)= f flpiOiWpWdt. 
J t=i 
Here, the "hyperparameter" <j> typically has an interpretation in terms of 
characteristics—for example, mean and covariance—of the population (of  
individuals, trial centres) from which the k units are drawn. 
In many applications, it may be of interest to make inferences both about the 
unit characteristics, the fy's, and the population characteristics, <£. In either case, 
straightforward probability manipulations involving Bayes' theorem provide the 
required posterior inferences as follows: 
p(Oi\x) = / p{6i\4>,x)p{(j)\x)d4>, 
where 
p(Oi\<i),x) <xp{x\ei)p{ei\4>) 
p{4>\x) <xp(x\4>)p{4>), 
and 
p{x\4>) = jp{x\6)p{6\4>)d6. 
Of course, actual implementation requires the evaluation of the appropriate 
integrals and this may be non-trivial in many cases. However, as we shall see in 
the volumes Bayesian Computation and Bayesian Methods, such models can be 
implemented in a fully Bayesian way using appropriate computational techniques. 
5.6 Discussion and Further References 
373 
A detailed analysis of hierarchical models will be provided in those volumes; some 
key references are Good (1965,1980b), Ericson (1969a, 1969b), Hill (1969,1974), 
Lindley (1971), Lindley and Smith (1972), Smith (1973a, 1973b), Goldstein and 
Smith (1974), Leonard (1975), Mouchart and Simar (1980), Goel and DeGroot 
(1981), Goel (1983), Dawid (1988b), Berger and Robert (1990), P6rez and Pericchi 
(1992), Schervish et al. (1992), van der Merwe and van der Merwe (1992), Wolpert 
and Warren-Hicks (1992) and George et al. (1993, 1994). 
A tempting approximation is suggested by the first line of the analysis above. 
We note that if p{4>\x) were fairly sharply peaked around its mode, <f>*, say, we 
would have 
p(^|x)«p(^|^,x). 
The form that results can be thought of as if we first use the data to estimate 4> and 
then, with 4>* as a "plug-in" value, use Bayes' theorem for the first two stages of 
the hierarchy. The analysis thus has the flavour of a Bayesian analysis, but with an 
"empirical" prior based on the data. 
Such short-cut approximations to a fully Bayesian analysis of hierarchical 
models have become known as Empirical Bayes methods. This is actually slightly 
confusing, since the term was originally used to describe frequentist estimation of 
the second-stage distribution: see Robbins (1955, 1964, 1983). However, more 
recently, following the line of development of Efron and Morris (1972, 1975) and 
Morris (1983), the term has come to refer mainly to work aimed at approximating 
(aspects of) posterior distributions arising from hierarchical models. 
The naive approximation outlined above is clearly deficient in that it ignores 
uncertainty in <£. Much of the development following Morris (1983) has been  
directed to finding more defensible approximations. For more whole-hearted  
Bayesian approaches, see Deely and Lindley (1981), Gilliland et al. (1982), Kass and 
Steffey (1989) and Ghosh (1992a). An eclectic account of empirical Bayes methods 
is given by Maritz and Lwin (1989). 
5.6.5 Further Methodological Developments 
The distinction between theory and methods is not always clear-cut and the extensive 
Bayesian literature on specific methodological topics obviously includes a wealth 
of material relating to Bayesian concepts and theory. We shall review this material 
in the volume Bayesian Methods and confine ourselves here to simply providing a 
few references. 
Among the areas which have stimulated the development of Bayesian theory, 
we note the following: Actuarial Science and Insurance (Jewell, 1974, 1988; 
Singpurwalla and Wilson, 1992), Calibration (Dunsmore, 1968; Hoadley, 1970; 
Brown and Makelainen, 1992), Classification and Discrimination (Geisser, 1964, 
1966; Binder, 1978; Bernardo, 1988, 1994; Bernardo and Gir6n, 1989; Dawid 
374 
5 Inference 
and Fang, 1992), Contingency Tables (Lindley, 1964; Good, 1965,1967; Leonard, 
1975; Leonard and Hsu, 1994), Control Theory (Aoki, 1967; Sawagaricf al., 1967), 
Econometrics (Mills, 1992; Steel, 1992), Finite Population Sampling (Basu, 1969, 
1971; Ericson, 1969b, 1988; Godambe, 1969, 1970; Smouse, 1984; Lo, 1986), 
Image Analysis (Geman and Geman, 1984; Besag, 1986, 1989; Geman, 1988; 
Mardia et al., 1992; Grenander and Miller, 1994), Law (Dawid, 1994), Meta- 
Analysis (DuMouchel and Harris, 1992; Wolpert and Warren-Hicks, 1992), Missing 
Data (Little and Rubin, 1987; Rubin, 1987; Meng and Rubin, 1992), Mixtures 
(Titterington et al., 1985; Berliner, 1987; Bernardo and Giron, 1988; Florens et 
al., 1992; West, 1992b; Diebolt and Robert, 1993; Robert and Soubiran, 1993; 
West et al., 1994), Multivariate Analysis (Brown et al., 1994), Quality Assurance 
(Wetherill and Campling, 1966; Hald, 1968; Booth and Smith, 1976; Irony et al., 
1992; Singpurwalla and Soyer, 1992), Splines (Wahba, 1978, 1983, 1988; Gu, 
1992; Ansley et al., 1993; Cox, 1993), Stochastic Approximation (Makov, 1988) 
and Time Series and Forecasting (Meinhold and Singpurwalla, 1983; West and 
Migon, 1985; Mortera, 1986; Smith and Gathercole, 1986; West and Harrison, 
1986, 1989; Harrison and West, 1987; Ameen, 1992; Carlin and Poison, 1992; 
Gamerman, 1992; Smith, 1992; Gamerman and Migon, 1993; McCulloch and 
Tsay, 1993; Pole et al., 1994). 
5.6.6 Critical Issues 
We conclude this chapter on inference by briefly discussing some further issues 
under the headings: (i) Model Conditioned Inference, (ii) Prior Elicitation, (iii) 
Sequential Methods and (iv) Comparative Inference. 
Model Conditioned Inference 
We have remarked on several occasions that the Bayesian learning process is  
predicated on a more or less formal framework. In this chapter, this has translated into 
model conditioned inference, in the sense that all prior to posterior or predictive 
inferences have taken place within the closed world of an assumed model structure. 
It has therefore to be frankly acknowledged and recognised that all such  
inference is conditional. If we accept the model, then the mechanics of Bayesian 
learning—derived ultimately from the requirements of quantitative coherence— 
provide the appropriate uncertainty accounting and dynamics. 
But what if, as individuals, we acknowledge some insecurity about the model? 
Or need to communicate with other individuals whose own models differ? 
Clearly, issues of model criticism, model comparison, and, ultimately, model 
choice, are as much a part of the general world of confronting uncertainty as 
model conditioned thinking. We shall therefore devote Chapter 6 to a systematic 
exploration of these issues. 
5.6 Discussion and Further References 
375 
Prior Elicitation 
We have emphasised, over and over, that our interpretation of a model requires—in 
conventional parametric representations—both a likelihood and a prior. 
In accounts of Bayesian Statistics from a theoretical perspective—like that of 
this volume—discussions of the prior component inevitably focus on stylised forms, 
such as conjugate or reference specifications, which are amenable to a mathematical 
treatment, thus enabling general results and insights to be developed. 
However, there is a danger of losing sight of the fact that, in real applications, 
prior specifications should be encapsulations of actual beliefs rather than stylised 
forms. This, of course, leads to the problem of how to elicit and encode such beliefs, 
i.e., how to structure questions to an individual, and how to process the answers, in 
order to arrive at a formal representation. 
Much has been written on this topic, which clearly goes beyond the boundaries 
of statistical formalism and has proved of interest and importance to researchers 
from a number of other disciplines, including psychology and economics. However, 
despite its importance, the topic has a focus and flavour substantially different from 
the main technical concerns of this volume, and will be better discussed in the 
volume Bayesian Methods. 
We shall therefore not attempt here any kind of systematic review of the very 
extensive literature. Very briefly, from the perspective of applications the best 
known protocol seems to be that described by Stael von Holstein and Matheson 
(1979), the use of which in a large number of case studies has been reviewed by 
Merkhofer (1987). General discussion in a text-book setting is provided, for  
example, by Morgan and Henrion (1990), and Goodwin and Wright (1991). Warnings 
about the problems and difficulties are given in Kahneman et al. (1982). Some key 
references are de Finetti (1967), Winkler (1967a, 1967b), Edwards et al. (1968), 
Hogarth (1975, 1980) Dickey (1980), French (1980), Kadane (1980), Lindley 
(1982d), Jaynes (1985), Garthwaite and Dickey (1992), Leonard and Hsu (1992) 
and West and Crosse (1992). 
Sequential Methods 
In Section 2.6 we gave a brief overview of sequential decision problems but for most 
of our developments, we assumed that data were treated globally. It is obvious, 
however, that data are often available in sequential form and, moreover, there are 
often computational advantages in processing data sequentially, even if they are all 
immediately available. 
There is a large Bayesian literature on sequential analysis and on  
sequential computation, which we will review in the volumes Bayesian Computation 
and Bayesian Methods. Key references include the seminal monograph of Wald 
(1947), Jackson (1960), who provides a bibliography of early work, Wetherill 
(1961), and the classic texts of Wetherill (1966) and DeGroot (1970). Berger and 
376 
5 Inference 
Berry (1988) discuss the relevance of stopping rules in statistical inference. Some 
other references, primarily dealing with the analysis of stopping rules, are Amster 
(1963), Barnard (1967), Bartholomew (1967), Roberts (1967), Basu (1975) and 
Irony (1993). Witmer (1986) reviews multistage decision problems. 
Comparative Inference 
In this and in other chapters, our main concern has been to provide a self-contained 
systematic development of Bayesian ideas. However, both for completeness, and 
for the very obvious reason that there are still some statisticians who do not currently 
subscribe to the position adopted here, it seems necessary to make some attempt to 
compare and contrast Bayesian and non-Bayesian approaches. 
We shall therefore provide, in Appendix B, a condensed critical overview of 
mainstream non-Bayesian ideas and developments. Any reader for whom our  
treatment is too condensed, should consult Thatcher (1964), Pratt (1965), Bartholomew 
(1971), Press (1972/1982), Barnett (1973/1982), Cox and Hinkley (1974), Box 
(1983), Anderson (1984), Casella and Berger (1987, 1990), DeGroot (1987), Pic- 
cinato (1992) and Poirier (1993). 
377 
Chapter 6 
Remodelling 
Summary 
It is argued that whether viewed from the perspective of a sensitive individual 
modeller, or from that of a group of modellers, there are good reasons for  
systematically entertaining a range of possible belief models. A variety of decision 
problems are examined within this framework: some involving model choice 
only; some involving model choice followed by a terminal action, such as  
prediction; other involving only a terminal action. Throughout, a clear distinction is 
drawn between three radier different perspectives: first, the case where die range 
of models under consideration is assumed to include the '"true" belief model; 
secondly, die case where the range of models is being considered in order to  
provide a proxy for a specified, but intractable, actual belief model; finally, the case 
where the range of models is being considered in the absence of specification of 
an actual belief model. Links with hypothesis testing, significance testing and 
cross-validation are established. 
6.1 MODEL COMPARISON 
6.1.1 Ranges of Models 
We recall from Chapter 4 that our ultimate modelling concern is with predictive  
beliefs for sequences of observables. More specifically, most of our detailed develop- 
378 
6 Remodelling 
menthas centred on belief models corresponding to judgements of exchangeability 
or, more generally, various forms of partial exchangeability. 
In such cases, the predictive model typically has a mixture representation in 
terms of a random sample from a labelled model, together with a prior  
distribution for the label, the latter being interpretable in terms of a strong law limit of 
observables. For example, we saw that for an exchangeable real-valued sequence, 
a predictive belief distribution, P, has the general representation 
P{xi,...,xn)= / Y[F(Xi)dQ(F). 
This corresponds to an (as if) assumption of a random sample from the unknown 
distribution function, F, together with a prior distribution, Q, for F, defined over 
the space, T, of all distribution functions on 3?. 
However, the very general nature of this representation precludes it—at least in 
terms of current limitations on intuition and technique— from providing a practical 
basis for routine concrete applications. This is why, in Chapter 4, much of our 
subsequent development was based on formal assumptions of further invariance 
or sufficiency structure, or pragmatic appeal to historical experience or scientific 
authority, in order to replace the general representation by mixtures involving finite- 
parameter families of densities. 
Inescapably, however, this passage from the general, but intractable, form to 
a specific, but tractable, model involves judgements and assumptions going far  
beyond the simple initial judgement of exchangeability. These further judgements, 
and hence the models that result from them, are therefore typically much less  
securely based in terms of individual beliefs, and certainly much less likely to be 
mutually acceptable in an interpersonal context, than the straightforward  
symmetry judgement. Both from the perspective of a sensitive individual modeller and 
also from that of a group of modellers, there are therefore strong reasons for  
systematically entertaining a range of possible models (see, for example, Dickey, 1973, 
and Smith, 1986). 
Given the assumption of exchangeability, a range of different belief models, 
Pi, P2,... can each be represented in the general form 
Pj{xi, • ■ ■, s„) = / II F{Xi)dQj{F), 
•'■Ft=i 
for some Qi,Q2,..., the latter encapsulating the particular alternative judgements 
that characterise the different models. The following stylised examples serve to 
illustrate some of the kinds of ranges of models that might be entertained in  
applications involving simple exchangeability judgements. In each case, the range of 
6.1 Model Comparison 
379 
models can either be thought of as generated by a single, non-dogmatic  
individual (seeking to avoid commitment to one specific form); or generated as concrete 
suggestions by a group of individuals (each committed to one of the forms); or 
generated purely formally, as an imaginative proxy for models thought likely to 
correspond to the ranges of judgements which might be made by the eventual 
readership of inference reports based on the models. In general, our subsequent 
development will be expressed in terms of a possibly infinite sequence of models 
Pi, P2, ■ ■.; in practice, we typically only work with a finite range, Pi,..., P* for 
some k > 2. 
Inference for a Location Parameter 
Suppose that observations xi,. ■., xn,..., can be thought of, conditional on 
1 " 
u, — lim — > Xi, 
n->oo n *-^ 
j=l 
as measurements of \i with errors ei,..., e„,... „ so that 
Xi = n + ei, i = l,...,n, 
with ei,e2,..., exchangeable. Various beliefs are then possible about the "error 
distribution". For example, appeal to the central limit theorem (Section 3.2.3) 
might suggest the assumption of normality; however, past experience might suggest 
a substantial proportion of "aberrant" or "outlying" measurements, thus requiring 
a distribution with heavier tails than normality; different past experience might 
suggest that the experimenter automatically suppresses any observations suspected 
of being "aberrant", thus requiring the assumption of a distribution with lighter 
tails than normality. With k = 3, and using density representations throughout, a 
choice of a range of models to cover these possibilities might be: 
Pj(xi,...,xn)- / / TT pj {xi I /x, a)pj (n, a)diJ,da, 
where 
Pi(x\ti,a) = -j=^expl—^{x-n)2[, x G 3? 
P2(x\n,a) =-^-expl—— |a;-/x|V, xeK 
and 
(x\(i,a) = —|-, x £ (ji - V3a, /j, + s/Zo^j , 
380 
6 Remodelling 
with Pj(n, cr), j = 1,2,3, specifying prior beliefs for the location and scale  
parameters appearing in these normal, double-exponential and uniform parametric 
models. Thus Pj(/x, a) = dQj(F) corresponds to a belief over T which assigns 
probability one to the family with parametric formpj (• | /x, a), with density pj (/x, a) 
for the two parameters of this family. If these modelling possibilities emanate from a 
single individual, pj (/x, a) might not depend on j; in general, however, the pj (/x, a) 
could differ, even though, in this case, the interpretations of the parameter as strong 
law limits of observable measures of the location and spread of the measurements 
are the same. 
Normality versus non-Normality 
Suppose that J\f C T is the set of all normal distributions on the real line, and hence 
that Afc = Jr — Afis the set of all distributions other than normal. Then, given 
the assumption of exchangeability for a real-valued sequence, an individual  
dogmatically asserting normality is specifying, in the general representation, a Q\ (F) 
which concentrates with probability one on M. Conversely, an individual  
dogmatically asserting non-normality is specifying a Q2(F) which concentrates with 
probability one on Mc. Our purpose here is mainly to point out how choices within 
the general exchangeable framework correspond to specification of Q. However, 
given the "size" of T, one cannot but be struck by the monumental dogmatism 
implicit in Qi! 
Parametric Hypotheses 
Suppose that Qj, j = 1,..., k, are even more dogmatic, in that they not only all 
focus on a single parametric family, p(x \ 6), but, within the family, they specify 
61,.. .,6 k, respectively, as the values of the parameter, so that 
n 
Pj(xi, ...,Xn) = Y[p(xi I 0j)- 
8=1 
If k = 2, this is often referred to as a situation of two simple hypotheses. 
A somewhat different situation arises if k = 2, Qi again focuses on a specific 
parameter value, 6 , but Q2 simply assigns a prior density p(6) over 0 in p(x \ 6). 
The rival models then have the representations 
n 
pi(xu...,xn) = Y[p(xi\6i) 
pi(xu...,xn)= Ylp(xi\0)p(0)d6, 
J ! = 1 
corresponding to what are usually referred to, within the context of the parametric 
family p(x \ 6), as a simple hypothesis and a general alternative. 
6.1 Model Comparison 
381 
In the contexts of judgements of partial rather than full exchangeability, the 
many versions of the former discussed in Chapter 4 clearly provide considerable 
scope for positing interesting ranges of models in any given application. The  
examples which follow illustrate just a few of these possibilities, expanding somewhat on 
the earlier discussion of model elaboration and simplification given in Sections 4.7.3 
and 4.7.4. 
Several Samples 
Consider the situation of m unrestrictedly exchangeable sequences of zero-one 
random quantities, discussed in detail in Section 4.6.2. We recall that, if x(n,i) = 
(xa,..., xin.), i = 1,..., m, the general representation of the joint predictive 
density for x(ni),..., x(nm) is given by 
so that, given a basic assumption of unrestricted exchangeability, alternative models 
are defined by different forms of Q. 
As a stylised illustration of the possibilities, we might consider: 
Qi'. assigning probability one to 9\ = ■ ■ ■ = 6m = 6, say, corresponding to 
the assumed equality of the limiting frequencies of ones in each of the m 
sequences, so that dQ{6\,..., 9m) reduces to dQ\{6); 
Q2: assigning probability one to #i = </>i,#2 = • ■ ■ = #m = <fo, say, so that 
dQ(eu ...,0m) reduces to dQ2{4>i, <fo); 
Q& retaining a general, non-degenerate, form dQ{6\,..., 9m) over the limiting 
frequencies. 
For example, in the context of 0-1 responses in m clinical trial treatment 
groups, Qi corresponds, loosely speaking, to the hypothesis that all treatments 
have the same effect; Q2 corresponds to the hypothesis that one of the treatments 
(possibly a "control") is different from all the other treatments, which themselves 
have the same effects; Q3 corresponds to a general hypothesis that all treatments 
have different effects, any further (non-degenerate) relationships among them being 
defined by the specific form of Q3. 
Structured Layouts 
In Section 4.6.3, we considered triply subscripted random quantities, Xijk,  
representing the fcth of a number of replicates of an observable in "context" i E I, 
subject to "treatment" j E J. In particular, we considered the situation where the 
predictive model might be thought of as generated via conditionally independent 
normal 
N(xijk \n + ati+Pi+ 7y-, t) 
382 
6 Remodelling 
distributions, together with a prior distribution Q for t and any IJ linearly  
independent combinations of {a*}, {Pi}, {lij}, i E I,j E J. 
As a stylised illustration of alternative modelling possibilities, we might  
consider: 
Q i: specifying 7^ = 0 for alH, j, together with a non-degenerate specification for 
{at}, {pi} and n; 
Q2: specifying 7^ = 0 for all i, j and /3,- = 0 for all j, together with a non- 
degenerate specification for {aj and /z; 
Q3: specifying 7^ = 0, for all i, j and a* = 0 for all i, together with a non- 
degenerate specification for {/3j} and /x; 
Q4: specifying 7^ = 0, a* = 0, fy = 0, for all i, j, together with a non-degenerate 
specification for /x. 
The reader familiar with analysis of variance methods will readily identify 
these prior specifications with conventional forms of hypotheses regarding absences 
of interaction and main effects. 
Covariates 
In Section 4.6.4, we discussed a variety of models involving covariates, where 
beliefs about the sequence of observables (x's) were structurally dependent on 
another set of observables (z's). Given the enormous potential variety of such 
covariate dependent models, it does not seem appropriate to attempt a notationally 
precise illustration of all possibilities. Instead, we shall simply indicate in general 
terms, for each of the cases considered in Section 4.6.4, the kinds of alternative 
models that might be considered. 
Example 6.1. (Bioassay). Alternative models for a single experiment might  
correspond to different assumptions about the functional dependence of the survival probabilities 
on the dose (for example, logit versus probit). In the case of several separate experiments, 
alternative models might assume the same functional form, but differ in whether or not they 
constrain model parameters—for example, the LD50's—to be equal. 
Example 6.2. (Growth curves). Alternative models for an individual growth curve 
might correspond to different assumptions about the functional dependence of the response 
on time (for example, linear versus logistic). In the case of several growth curves for 
subjects from a relatively homogeneous population, alternative models might be concerned 
with whether some or all of the parameters denning the growth curves are identical or differ 
across subjects. 
6.1 Model Comparison 
383 
Example 63. (Multiple regression). Alternative models in the multiple regression 
context typically correspond to whether or not various regressor variables can be omitted 
from the linear regression form; equivalently, to whether or not various regression coefficients 
can be set equal to zero. 
In the third volume of this work, Bayesian Methods, we shall discuss in detail 
a number of practical applications of this kind. 
Hierarchical Models 
Given the enormous variety of potential hierarchical models and alternative forms, 
we shall just content ourselves with some general comments for one of the specific 
cases considered in Section 4.6.5. 
Example 6.4. (Exchangeable normal mean parameters). 
InExample4.16of Section4.6.5, we considered a case where all the means, pi,..., fim 
of the m groups of observables with normal parametric models were judged exchangeable, 
and where this latter relationship was modelled as a mixture over a further parametric form, 
reflecting a symmetric judgement of "similarity" for ^,..., /jm. However, other symmetry 
judgements are possible: for example, that m — 1 of the /x; 's are exchangeable, the other one 
is not, but all are equally likely, a priori, to be the odd one out. This would create a model 
allowing potential "outliers" among the m groups themselves. (See Section 4.7.3 for further 
development of this idea in a non-hierarchical setting.) 
Confronted with a range of possible models, how should an individual or a 
group proceed? 
From the perspective adopted throughout this book, clearly the answer depends 
on the perceived decision problem to which the modelling is a response. In the  
remainder of this chapter, we shall therefore illustrate various of the kinds of decision 
problems that might be considered. The emphasis will be on somewhat stylised, 
typically simple, versions of such problems, in order to highlight the conceptual 
issues. Detailed case-studies, involving the substantive complexities of context 
and the computational complexities of implementation will be more appropriately 
presented in the volumes Bayesian Computation and Bayesian Methods. 
6.1.2 Perspectives on Model Comparison 
To be concrete, let us assume that all the belief models Piyi 6 /, say, under 
consideration for observations x can be described in terms of finite parameter 
mixture representations. Given the specifications of the various densities forming 
the mixtures, the predictive distributions for the alternative models are described 
by 
Pi(x) = p(x | Mt) = / pi(x | O^ptie^dOi, i e I. 
384 
6 Remodelling 
For mnemonic convenience, from now on we shall denote the alternative models 
by {Mi, i e 1} (rather than Pt,i e J, as in our previous discussions) and the set of 
these models by M = {Mi, i e I}. 
Before we turn to a detailed discussion of decisions concerning model choice or 
comparison among {M,, i 6 /}, we need to draw attention to important distinctions 
among three alternative ways in which these possible models might be viewed. 
The first alternative, which we shall call the M-closed view, corresponds 
to believing that one of the models {Mi,i 6 1} is "true", without the explicit 
knowledge of which of them is the true model. From this perspective, which may 
reflect either the range of uncertainties within an undecided individual, or the range 
of different beliefs of a group of individuals, the overall model specifies beliefs for 
x of the form 
p(x) = YtP(Mi)p(x\Mi), 
iel 
with P(Mi) denoting prior weights on the component models {Mi, i 6 /}. There 
is, of course, some ambiguity as to what should be regarded as a component model 
(for example, the renormalised mixture of Mi and M2 could itself be regarded as 
a model), but this can be resolved pragmatically by taking {Mi, i e 1} to be those 
individual models we are interested in comparing or choosing among. 
But, continuing the discussion of Section 4.8.3 on the role and nature of 
models, when does it actually make sense to speak of a "true" model and hence to 
adopt the M -closed perspective? 
Clearly, this would be appropriate whenever one knew for sure that the real 
world mechanism involved was one of a specified finite set. One rather artificial 
situation where this would apply would be that of a computer simulation "inference 
game", where data are known to have been generated using one of a set of  
possible simulation programs, each a coded version of a different specified probability 
model, but it is not known which program was used. 
Beyond such "controlled" situations, it seems to us to be difficult to accept 
the M -closed perspective in a literal sense. However, there may be situations 
where one might not feel too uncomfortable in proceeding "as if one meant it. 
For example, suppose that a parametric model with a specified parameter has been 
extensively adopted and found to be a successful predictive device in a range of 
applications. Now suppose that a new application context arises and that it is felt 
necessary to reconsider whether to continue with the previous specified parameter 
value or, in this new context, to incorporate uncertainty about the appropriate value. 
Provided we feel comfortable, in principle, with assigning prior weights to these 
two alternative formulations, we can exploit the M-closed framework. 
However, reality is typically not as relatively straightforward as this. Nature 
does not provide us with an exhaustive list of possible mechanisms and a guarantee 
that one of them is true. Instead, we ourselves choose the lists as part of the process 
6.1 Model Comparison 
385 
of settling on a predictive specification that we hope will prove "fit for purpose" 
—in the jargon of modern quality assurance. 
But if we abandon the M -closed perspective, how else might we approach 
the very real and important problem of comparing or choosing among alternative 
models? It seems to us that the approach depends critically on whether one has 
oneself separately formulated a clear belief model or not. 
In the former case, the alternative models are presumably being contemplated 
as a proxy because the actual belief model is too cumbersome to implement;  
however, they will still have to be evaluated and compared in the light of these actual 
beliefs. In the latter case, in the absence of an actual specified belief model, it would 
seem intuitively—and we shall see this more formally later—that the alternative 
models have to battle it out among themselves on some cross-validatory basis. 
We now proceed to give these alternative perspectives a somewhat more formal 
description. 
The second alternative, which we shall call the M-completed view,  
corresponds to an individual acting as if {Mt ,i £ 1} simply constitute a range of specified 
models currently available for comparison, to be evaluated in the light of the  
individual's separate actual belief model, which we shall denote by Mt. From this  
perspective, assigning the probabilities {P(Mi), i e 1} does not make sense and the 
actual overall model specifies beliefs for x of the form p(x) — pt(x) = p(x \Mt). 
.M-completed models, relative to a given proposed range of models Mt,i e J 
might be adopted for a variety of reasons. Typically, {Mt, i e 1} will have been 
proposed largely because they are attractive from the point of view of tractability 
of analysis or communication of results compared with the actual belief model Mt. 
The third alternative, which we shall call the M -open view, also acknowledges 
that {Mi, i 6 1} are simply a range of specified models available for comparison, 
so that assigning probabilities {P(Mi), i e 1} does not make sense. However, 
in this case, there is no separate overall actual belief specification, p(x)—perhaps 
because we lack the time or competence to provide it. 
Examples of lists of "proxy" models that are widely used include familiar 
ones based on parametric components, corresponding to: regression models with 
different choices of regressors; generalised linear models with different choices of 
covariates, link functions, etc.; contingency table structures with different patterns 
of independence and dependence assumptions. 
The M -open perspective requires comparison of such models in the absence of 
a separate belief model. The M -completed perspective will typically have selected 
the particular proxy models in the light of an actual belief model. For example, 
if the actual belief model is based on non-linear functions of many covariates, 
together with Student probability distribution specifications, the proxy models to 
be evaluated might be various linear regression models with limited numbers of 
covariates and normal probability distribution specifications. 
386 
6 Remodelling 
6.1.3 Model Comparison as a Decision Problem 
We shall now discuss various possible decision problems where the answer to an 
inference problem involves model choice or comparison among the alternatives in 
M = {Mi,i 6 /}. Some of these only make sense from an M-closed perspective; 
others can be approached from either an Al-closed, an M-completed or an M- 
open perspective. Throughout the following development, observed data on which 
decisions are to be based will be denoted by x, and the choice of model Mit either 
as an end in itself, or as the basis for a subsequent answer to an inference problem, 
will be denoted by m^ i E I. 
The first decision problem we shall consider involves only the choice of an Mi, 
without any subsequent action, so that the utility function has the form u(mj,u;), 
where u; is some unknown of interest. This decision structure is shown  
schematically in Figure 6.1. 
Provided we feel comfortable, in principle, with assigning prior weights to 
these two alternative formulations, we can exploit the M-closed framework. 
Figure 6.1 A decision problem involving model choice only 
It is perhaps not obvious why such a problem would be of interest from an 
Al-open perspective. However, from an M -closed perspective, an example of 
an obvious u; of interest might be the Mi for which, imagining a large future 
sample of observations, y = (j/i,..., ys), P(M,-1 y) —► 1 as s —► oo. Recalling 
Proposition 5.9 of Section 5.2.3, u in this case labels the "true model", and the 
utility of choosing a particular model then depends on whether a correct choice has 
been made. 
Whatever the forms of u; andu(mj, u), in the general decision problem defined 
by Figure 6.1, maximising expected utility implies that the optimal model choice 
m* is given by 
u{m* | x) = supiZ(mj | x) , 
iei 
where 
u{mi | x) = u(m,i,u})p(u}\x)du), iei, 
with p(w | x) representing actual beliefs about u having observed x. 
6.1 Model Comparison 
387 
In the M -closed case, 
p(u | x) = ^2pi(u | x)P(Mi | x), 
i€l 
where 
P(Ml\x) = P^M^\M, 
Z^PiMjMxlMj) 
and Pi(u | x) = p(u \ Mi, x) is given by standard (posterior or predictive)  
manipulations conditional on model Mi,i e I. We note, in particular, the key role played 
by the quantities {P(Mt \x),i 6 /}, which, within the purview of the M-closed 
framework, are the posterior probabilities, given x, of model Mi,i 6 J, being true. 
From the M -completed perspective, we can, at least in principle, obtain 
p(u |x) and evaluate u{rrii \x), i 6 J, even though this may require extensive 
(Monte Carlo) numerical calculations in specific applications. In this way, one can 
compare the models in M, even though none of them corresponds to one's own 
assumption regarding the true model. 
From the M -open perspective, nothing canbesaidin general about the explicit 
form of p(u \x). It turns out, however, perhaps surprisingly, that, at least  
approximately, the same analysis can be carried out in the M-open as in the .M-completed 
framework; in other words, one can compare the models in M on the basis of their 
expected utilities without actually having specified an alternative "true" model. We 
shall defer a detailed discussion of this until Section 6.1.6. 
Let us now consider a rather different form of decision problem which first 
requires the choice of model Mi from M, which we denote by mi, and then,  
assuming Mi to be the model, requires an answer a,j ,jEJi relating to an unknown 
"state of the world" u of interest. For example, we may wish to predict a future  
observation, or estimate a parameter common to all the models in M. If u(mj, a3■,, u) 
denotes the utility resulting from the successive choices m* (i.e., model Mi) and 
ai-, J 6 Ji (answer to inference question, given Mi), when u is the actual "state of 
the world", the resulting decision problem is shown schematically in Figure 6.2. 
u(m,i,aj, w) 
Figure 6.2 A decision problem involving model choice and subsequent inference 
388 
6 Remodelling 
Systematic application of the criterion of maximising expected utility  
establishes that the optimal model choice is that m* for which 
u(m* | x) = supu(mj | x), 
i€l 
where 
u(m.i | x) = / u(m,i, a*, u)p(u \ x)dw 
is the expected utility, given x, of optimal behaviour given model Mt, so that a* is 
obtained from maximising 
/ u(mi,aj,u})pi(u}\x)du} 
The formpj (w | x) in the above is again given by standard (posterior or  
predictive) manipulation conditional on model Mt,i e I, while the formp(u | x) again 
represents actual beliefs about u given x. 
The explicit form of p(u | x) as a mixture of the pi(u \ x), has been given 
above in the M -closed case. 
In the M -completed case, we have also noted above that evaluation of p(u \ x) 
and {u(mi \x),i 6 /} can in principle be carried out, numerically if necessary. 
Detailed analysis for the M-open case will be given in Section 6.1.6. 
From a conceptual perspective, it is important to recognise that different 
choices of u; and different forms of utility structure will naturally imply  
different forms of solution to the problem of model choice. In the next two subsections, 
we shall explore a number of specific cases, in order to underline the general  
message that coherent comparison of a finite or countable set of alternative models 
depends on the specification (at least implicitly) of a decision structure, including 
a utility function. 
Before proceeding to further aspects of model choice and comparison,  
however, it is worth remarking that, in the above context, it is not necessary to choose 
among the elements of M in order to provide an answer a, to an inference problem. 
If we omit the explicit model choice step, the resulting, different form of decision 
problem is that shown schematically in Figure 6.3. 
>^ ^^ u(a,«) 
Figure 6.3 A decision problem involving terminal decision only 
6.1 Model Comparison 
389 
In this case, maximising expected utility leads immediately to the optimal 
answer a*, given by 
u(a* | x) = sup u(a \ x) 
where 
u(a* | x) = / u(a,u})p(u \ x) a\o, 
withp(a;| x) as discussed above. In the particular case of an M-closed perspective, 
it follows from the posterior weighted mixture form of p(w | x) that, although we 
have omitted the model choice step, model comparison in the light of the data x 
is still being effected through the presence of {P(Mi \x),i 6 /}. In general, if 
we entertain a range of possible models for data x, solutions to decision problems 
conditional on x will always implicitly depend on a comparison of the models in 
the light of the data, even if explicit choice among the models is not part of the 
decision problem. 
6.1.4 Zero-one Utilities and Bayes Factors 
In this section, we confine attention to the M -closed perspective and consider first 
the problem of choosing a model from M, without any subsequent decision, when 
the "state of the world" of interest is defined to be the "true" model, Mt, so that 
assuming a future sample y = (j/i,..., ys), P(Mt \ y) —► 1 as s —> oo. From the 
M -closed perspective, the problem, stated colloquially, is that of choosing the true 
model. 
In this case, a natural form of utility function may be 
i \ J1 if 
u = Mi 
It is then easily seen from the analysis relating to Figure 6.1. that 
u = Mi 
Uy^Mi, 
Pi("\x) = {0 if 
and 
P[U ' X) \ 0, if u; yt Mi. 
The expected utility of the decision m* (choosing Mi), given x, is hence 
u(m,i | x) = I u(rrii,u})p(u} \ x)a\o 
= P(Mi\x), iel. 
The optimal decision is therefore to "choose the model which has the highest 
posterior probability". 
390 
6 Remodelling 
Bayes Factors 
Less formally, suppose that some form of intuitive measure of pairwise comparison 
of plausibility is required between any two of the models {Mi, i £ I}. The above 
analysis suggests that M,, Mj may be usefully compared using the posterior odds 
ratio, 
p(Mj | x) _ p(x | Mj) P(Mj) 
p(Mj\x) p(x\Mj)X P(Mj)' 
where, for example, 
P(x\Mi) = J Piixie^iie^dOi. 
In words, the above comparison can be described as 
"posterior odds ratio = integrated likelihood ratio x prior odds ratio", 
making explicit the key role of the ratio of integrated likelihoods in providing the 
mechanism by which the data transform relative prior beliefs into relative posterior 
beliefs, in the context of parametric models. 
The fundamental importance of this transformation warrants the following 
definition, apparently due to Turing (see, for example, Good, 1988b). 
Definition 6.1. (Bayes factor). Given two hypotheses Hit Hj corresponding 
to assumptions of alternative models, M,, Mj, for data x, the Bayes factor in 
favour of Hi (and against Hj) is given by the posterior to prior odds ratio. 
p(x\Mj) _fp(Mj\x)\ /(p(Mj)\ 
Bij[X) - P(x | Mj) ~ \p(Mj \x))/ \p(Mj) } 
Intuitively, the Bayes factor provides a measure of whether the data x have 
increased or decreased the odds on Hi relative to Hj. Thus, B{j(x) > 1 signifies 
that Hi is now more relatively plausible in the light of x; Btj(x) < 1 signifies that 
the relative plausibility of Hj has increased. 
Good (1950) has suggested that the logarithms of the various ratios in the above 
be called weights of evidence (a term apparently first used in a related context by 
Peirce, 1878), so that logSjj(a;) corresponds to the integrated likelihood weight 
of evidence in favour of Mt (and against Mj). On this logarithmic scale, the prior 
weight of evidence and log By (a;) combine additively to give the posterior weight 
of evidence. 
In Section 6.1.1, we noted the extremely simple forms of predictive models 
which result when beliefs not only concentrate on a specific parametric family of 
distributions, but also identify the value of the parameter. An alternative set of such 
models, Mt, i £ I, then just corresponds to the specifications {pt(x \ 0j), i £ I}, 
and the integrated likelihood ratios reduce to simple ratios of likelihoods. 
6.1 Model Comparison 
391 
Hypothesis Testing 
The problem of hypothesis testing has its own conventional terminology, which, 
within the framework we are adopting, can be described as follows. Two alternative 
models, Mx, M2 are under consideration and both are special cases of the predictive 
model 
p(x) = Jp(x\9)dQ(9), 
with the same assumed parametric form p(x \ 9), 9 £ 0, but with different choices 
of Q. If, for model Mu Qt assigns probability one to a specific value, 0,, say, 
the model is said to reduce to a simple hypothesis for 9 (recalling that the form 
p(x 19) is assumed throughout). If, for model Mj, Qj defines a non-degenerate 
density Pj (9) over Qj C 0, the model is said to reduce to a composite hypothesis 
for 9. If a simple hypothesis is being compared with a composite hypothesis, so 
that Qj = 0 — {9t}, the latter is called a general alternative hypothesis. 
In the situation where the "state of the world" of interest, w, is defined to be 
the true model Mt, we can generalise slightly the zero-one utility structure used 
earlier by assuming that 
u(m,i,u) = -Uj, u> = Mj, i = 1,2, ,7 = 1,2, 
with ln = I22 = 0 and lx2, l2x > 0. Intuitively, there is a (possibly asymmetric) loss 
in choosing the wrong model, and there is no loss in choosing the correct model. 
Given data x, and using, again pi(w\x) = 1 if w = Mt and 0 otherwise, the 
expected utility of m, is then easily seen to be 
u{rrn \x) = - [lnP(Mi \ x) + li2P(M2 \ x)], 
so that 
u(mi I x) < u(m2 \ x) iff li2p(M2 \ x) > hiP(Mx \ x). 
We thus prefer M2 to M\, if and only if 
P{MX I x) h2 
P(M2 I x) 1^ ' 
revealing a balancing of the posterior odds against the relative seriousness of the 
two possible ways of selecting the wrong model. In the symmetric case, lx2 = l2X, 
the choice reduces to choosing the a posteriori most likely model, as shown earlier 
for the zero-one case. 
The following describes the forms of so-called Bayes tests which arise in 
comparing models when the latter are defined by parametric hypotheses. 
392 
6 Remodelling 
Proposition 6.1. (Forms ofBayes tests). In comparing two models, M1( M% 
defined by parametric hypotheses for p(x 16), with utility structure 
u(mi,u)) = -kj, u) = Mj, z = 1,2, .7 = 1,2, 
with In = I22 = 0 and I12, hi > 0, M2 is preferred to Mi, if and only if 
In P{M2) 
Bn{x) < 
hi P{Mi) 
where: 
D ^_P(*|0i) 
p(x\02) 
*><*)-, P<X'8'> 
(simple versus simple test), 
- (simple versus composite test), 
f p(x\6)pi(9)d6 
Bi2(x) = -=-*—;—i-T7—... ,. (composite versus composite test). 
Ie2P(x\B)p2(e)d0 
Proof. The results follow directly from the preceding discussion. < 
The following examples illustrate both general model comparison and a  
specific instance of hypothesis testing. 
Example 6.5. (Geometric versus Poisson). Suppose we wish to compare the two 
completely specified parametric models, Negative-Binomial and Poisson, defined for  
conditionally independent xt,..., x„, by 
Mi :Nb(xi\l,0i), M2:Pn(xi\e2), i = l,...,n. 
The Bayes factor in this case is given by the simple likelihood ratio 
nr=1 NbteiMo w - so"5 
Bu{x) = 
rT/=iPn(z;|02) efe-^{Y[^lXi\yl 
Suppose for illustration that ft = |, 62 = 2 (implying equal mean values for the models); 
then, for example, with n = 2, xi = x2 = 0, we have Bu(x) = e4/9 « 6.1, indicating 
an increase in plausibility for Mi; whereas with n = 2, xi = x2 = 2, we have B\2(x) = 
4e4/729 « 0.3, indicating a slight increase in plausibility for M2. 
Suppose now that ft, ft are not known and are assigned the prior distributions 
p1(ft)=Be(ft|ai,/3i), «,(&)= Ga(02 | a2) ft), 
6.1 Model Comparison 
393 
whose forms are given in Section 3.2.2 (where details of Nb(x; 11, #i) and Pn(x; | 02) can 
also be found). It follows straightforwardly that 
= r(q1+/31) T(n + a1)T(nx + /3l) 
~ r(a1)r(/31) r^ + n^ + aj+A) ' 
and that 
p(xh ...,xn\M2) = —-^ ^ 6f+a^e-^^d02 
r(a2)[[i=lXil Jo 
r(nx + a2)l%2 1 
T(a2)(n + 02)™+°2YY>=1xi\ ' 
We further note that 
E(xi\Ml)= f E(xi\Mu61)Be(O1\au01)dO1 
Jo 
0, 
Jo 01 
BeiOtlauP^de 
r(ai + fl) r(ai - i)r(/?! +1) # 
r(a1)r(/31) ' T(a1 + /31) a, 
and 
(ii | M2) = / 
./o 
S(ij | Af2) = / E(xi | M2, 02) Ga(02 | a2, #,)<% 
/•oo 
: / 02Ga(02|a2,&) 
Jo 
so that prior specifications with aia2 = /3i/32 imply the same means for the two predictive 
models. 
Table 6.1 Dependence ofBn(x) on prior-data combinations 
ai = l,/9i 
a2 = 2, fo 
x\ = x2 = 0 1.5 
Xj = x2 = 2 0.29 
= 2 a1=30,/3i=60 a1 = l,/31 
= 1 a2 = 60, & = 30 a2 = 4, /3b 
5.5 0.27 
0.30 0.38 
= 2 
= 2 
394 
6 Remodelling 
As an illustration of the way in which the prior specification can affect the inferences, 
we present in Table 6.1 a selection of values of Bi2(x) resulting from particular prior-data 
combinations. 
In the first two columns, the priors specify the same predictive means for the two models, 
but the priors in the second column are much more informative. In the final column, different 
predictive means are specified. Column 2 gives Bayes factors close to those obtained above 
assuming 6\, $2 known, as might be expected from prior distributions concentrating sharply 
around the values 6\ = \, 02 = 2. However, comparison of the first and third columns for 
x\ = X2 = 0 makes clear that, with small data sets, seemingly minor changes in the priors 
for model parameters can lead to changes in direction in the Bayes factor. 
The point made at the end of the above example is, of course, a general one. 
In any model comparison, the Bayes factor will depend on the prior distributions 
specified for the parameters of each model. That such dependence can be rather 
striking is well illustrated in the following example. 
Example 6.6. (Lindley's paradox). Suppose that for a: = (x\,..., xn) two alternative 
models M\, M2, with P(Mi) > 0,i= 1,2, correspond to simple and composite hypotheses 
about fi in N(xt | /z, A) defined by 
n 
Mi: pi(x) =Y\N{xi\na,\), /io, A known, 
!=1 
M2: P2(x) = / Y[N(xi\n,X)N(n\tJ,i,Xi)dn, ^.Aj, A known. 
J 1=1 
In more conventional terminology, x\,..., xn are a random sample from N(x \ \i, A), with 
precision A known; the null hypothesis is that \i = fio, and the alternative hypothesis is that 
\i / \io, with uncertainty about \i described by N([i \ ^, Ai). 
Since x = n"1 YL1=\ x> *s a sufficient statistic under both models, we easily see that 
d (r\ = N(x\fM,,nX) 
lH ; jN(x\^,nX)N(^\^,X1)d^ 
= fXl+nX\1/2 expJHAr' + M)-')-1^-^} 
V ^i / exp{^nX(x - fj,0)2} 
It is easily checked that,/or any fixed x, Bn(x) —> oo as Ai —> 0, so that evidence in 
favour of Mt becomes overwhelming as the prior precision in M2 gets vanishingly small, 
and hence P(M\ \x) —> 1. In particular, this is true for x such that X1/2 \x — (io\ is 
large enough to cause the "null hypothesis" to be "rejected" at any arbitrary, prespecified 
level using a conventional significance test! This "paradox" was first discussed in detail by 
Lindley (1957) and has since occasioned considerable debate: see Smith (1965), Bernardo 
(1980), Shafer (1982b), Berger and Delampady (1987), Moreno and Cano (1989), Berger 
and Mortera (1991a) and Robert (1993) for further contributions and references. 
6.1 Model Comparison 
395 
A model comparison procedure which seems to be widely used implicitly in 
statistical practice, but rarely formalised, is the following. Given the assumption of 
a particular predictive model, {p(x \ 8),p(8)}, 9 £ 6, a posterior density, p(8 \ x), 
is derived and, as we have seen in Section 5.1.5, may be at least partially summarised 
by identifying, for some 0 < p < 1, a highest posterior density credible region 
Rp(x), which is typically the smallest region such that 
/. 
p(91 x)d8 = p. 
Rp(x) 
Intuitively, for large p, Rp(x) contains those values of 9 which are most plausible 
given the model and the data. Conversely, Rcp{x) consists of those values of 9 
which are rather implausible. 
Now suppose that, given a specifiedp and derived Rp{x), one is going to assert 
that the "true value" of 9 (i.e., the value onto which p{9 \ y) would concentrate as 
the size of a future sample tended to oo) lies in Rp(x). Defining the decision 
problem to be the choice of p, so that the possible answers to the inference problem 
are in A = [0,1], with the state of the world u) defined to be the true 9, a value 
ap = p has to be chosen. An appropriate utility function may be 
„(„ 9\-if{p) for9eRp(x) 
"^'"'-{gil-p) for9eRcp(x) 
where / and g are decreasing functions defined on [0,1]. Essentially, such a utility 
function extends the idea of a zero-one function by reflecting the desire for a 
"correct" decision, but modified to allow for the fact that choosing p close to one 
leads to a rather vacuous assertion, whereas a correct assertion with p small is rather 
impressive. 
The expected utility of choosing ap = p is easily seen to be 
«(flp) = PfiP) + (1 - P)fl(l - P), 
from which the optimal p may be derived for any specific choices of / and g. 
We note that if / = g, the unique maximum is at p = 0.50, so that it becomes 
optimal to quote a 50% highest posterior density credible region. If, for example, 
f(p) = 1 — p, g(l — p) = [1 — (1 — p)]2 = p2, the resulting optimal value of p 
is l/\/3 « 0.58, so that a 58% credible region is appropriate. More exotically, if 
f(p) — 1 - (2.7)p2, g(l — p) = (1 - p)"\ the reader might like to verify that a 
95% credible region is optimal. 
6.1.5 General Utilities 
Continuing for the present with the (M-closed) hypothesis testing framework, the 
consequences of incorrectly choosing a model may be less serious if the alternative 
models are "close" in some sense, in which case utilities of the zero-one type, which 
take no account of such "closeness", may be inappropriate. 
396 
6 Remodelling 
One-sided Tests 
We shall illustrate this idea, and forms of possibly more reasonable utility functions, 
by considering the special case of 9 e 0 C 5ft, with parametric form p(x \ 9) and 
models Mx, M2 defined by 
Pl(x\9)=p(x\9), 9eG1 = {9;9<90} 
p2(x\9) = p(x\9), 9eG2 = {9; 9 > 90} 
for some 9q G G. The models thus correspond to the hypotheses that the parameter 
is smaller or larger than some specified value 9q. 
It seems reasonable in such a situation to suppose that if one were to incorrectly 
choose M2 (9 > 90) rather than Mi (9 < 90), in many cases this would be much 
less serious if the true value of 9 were actually 9$ — e than if it were 9$ — 100e, 
say, for e > 0. Such arguments suggest that, with the state of the world u now 
representing the true parameter value 9, we might specify a utility function of the 
form 
i \ ( a\ J° for9eGi, 
u(mi,u;)=u(mi,9) = {_m fot9eQl 
for i = 1,2 where l\, l2 are increasing positive functions of (9 — 9q) and (#o — 9), 
respectively. The expected utility of the decision corresponding to m* (i.e., the 
choice of M,) is therefore given by 
u(mi \x) = - h(9)p(91 x)d9, 
i 
where 
p{9\x) = ^e» 
JBp(x\9)p(9)d9 
The optimal answer to the inference problem is to prefer Mx to M2 if and only if 
u(mx \x) > u(m2|a;), 
with explicit solutions depending, of course, on the choices of llt l2, and the form 
of p(91 x), as illustrated in the following example. 
Example 6.7. (Normalposterior; linear losses). 
lfl\(0) — 0-0o,l2(0) = k(00-9), with k reflecting the relative seriousness of  
"overestimating" by choosing model M1; and p(0 \ x), given x = x\,..., xn, is N(01 \in, Xn), 
say, then we have 
u(mi | x) = - [ (e- e0)N(e \ nn, xn) do 
Jo>o0 
= -x-1^1[x-l/2(e0-^)], 
6.1 Model Comparison 
397 
where 
/oo 
N(s\0,l)ds. 
and 
u(m2\x) = -k[ (0o-0)N(0\iin,\n)d6 
= -k\-nl^W(e0-»n)], 
where 
*2(*) = N(t \0,1)+t f N(s | 0,1) cfe. 
Jo 
It is therefore optimal to prefer Mi to M2 if and only if 
k*2 [X-l'2(90 - tln)} > ¥, [A^/^^o - fin)] . 
In the symmetric case, k = 1, it is easily seen that this reduces to preferring Mi if and only 
if ^ < Oo, as one might intuitively have expected. For references and further discussion of 
related topics, see DeGroot (1970, Chapter 11), and Winkler (1972, Chapter 6). 
Prediction 
Moving away now from model comparisons which reduce to hypothesis tests in 
parametric models, let us consider the problem of model comparison or choice, 
given data x, in order to make a point prediction for a future observation y. 
The general decision structure is that given schematically in Figure 6.2, where, 
assuming real-valued observables, m* corresponds to acting in accordance with 
model Mi, aj, j £ J, denotes the choice, based on Mt, of a prediction, yt, for a 
future observation y, and we shall assume a "quadratic loss" utility, 
u(mi,yi,y) = -(yi-y)2, iel. 
We recall from the analysis given in Section 6.1.3 that the optimal model choice is 
m*, given by 
u(m* | x) = sup / u(mi, y*, y)p(y \ x)dy, 
iel J 
where y\ is the optimal prediction of a future observation y, given data x and 
assuming model Mi, that is, the value y which minimises 
j (y-y)2Pi(y\x)dy, 
398 6 Remodelling 
where pt(y \ x) is the predictive density for y given model Mt. It then follows 
immediately that 
Vi = j yPiiv I x)dy = E[y \ x, Mi] , 
the predictive mean, given model Miy so that 
/ u(rrn,y*,y)p(y | x)dy = - (y* - yfp(y \ x)dy, i e /. 
Completion of the analysis now depends on the specification of the overall actual 
belief distribution p(y \ x) and the computation of the expectation of (y* — y)2, 
i £ I, with respect to p(y \ x). 
Again, in the M-completed case there is nothing further to be said explicitly; 
one simply carries out the necessary evaluations, using the appropriate form of 
p(y | x), by numerical integration if necessary. 
In the .M-open case, the detailed analysis of the problem of point prediction 
with quadratic loss will be given in Section 6.1.6. 
In the M -closed case, we have 
P{v\x) = ^2p(Mi\x)pi(y\x), 
iel 
and, after some rearrangement, it is easily seen that 
/ (V*i - y)2p(y I x)dy = ^p{Mj \ x) J (y* - y* + y* - y)2Pj(y \ x)dy, 
which reduces to 
5>(Mj | x)V[y | Mj, x) + "£($ ~ yj)MMj \ x). 
jel jel 
The first term does not depend on i, and the second term can be rearranged in the 
form 
(ff-^ + EtfJ-w*)2^!*), 
jel 
where y* is the weighted prediction 
y* = ^y*p{Mj\x). 
jel 
6.1 Model Comparison 
399 
The preferred model M{ is therefore seen to be that for which the resulting  
prediction, y*, is closest to y*, the posterior weighted-average, over models, of the 
individual model predictions. If k = 2, it is easily checked that the preferred 
model is simply that with the highest posterior probability. 
If we wish to make a prediction, but without first choosing a specific model, 
it is easily seen that the analysis of the problem in terms of the schematic decision 
problem given in Figure 6.3 of Section 6.1.2 leads directly to y* as the optimal 
prediction. 
Clearly, the above analyses go through in an obvious way, with very few 
modifications, if, instead of prediction, we were to consider point estimation, with 
quadratic loss, of a parameter common to all the models. More generally, the 
analysis can be carried out for loss functions other than the quadratic. 
Reporting Inferences 
Generalising beyond the specific problems of point prediction and estimation, let us 
consider the problem of model comparison or choice in order to report inferences 
about some unknown state of the world u). For example, the latter might be a 
common model parameter, a function of future observables, an indicator function 
of the future realisation of a specified event, or whatever. 
A major theme of our development in Chapters 2 and 3 has been that the 
problem of reporting beliefs about u) is itself a decision problem, where the  
possible answers to the inference problem are the consists of the class of probability 
distributions for w which are compatible with given data. The appropriate utility 
functions in such problems were seen to be the score functions discussed in Sections 
2.7 and 3.4. This general decision problem is thus a special case of that represented 
by Figure 6.2, where, given data x, rrii represents the choice of model Mt, the 
subsequent answer a,, j £ Jt, to the inference problem is some report of beliefs 
about u), assuming Mit and the utility function is defined by 
u(mi,aj,u>) = Ui{qi{-\x),u)), 
for some score function m, and form of belief report, g, (• | x), about u),  
corresponding to dj,j e Ji. 
If Pi(-1 x) is the form of belief report for u) actually implied by m, and if m 
is a.proper scoring rule (see, for example, Definition 3.16) then it follows that the 
optimal dj, j e Ji must be a* = p,(-1 x) and that 
u{mi,a*,u))=Ui{pi{-\x),u), i £ I. 
If, moreover, the score function is local (see, for example, Definition 3.18), we have 
the logarithmic form 
u(mi,a*,(t>) = A\ogPi(v\x) +B(v), iel, A>0, 
400 6 Remodelling 
for a > 0 and B(uj) arbitrary, in accordance with Proposition 3.13. The expected 
utility of m, is therefore given by 
u(rrii \x) = I {a\ogpi(tjJ \ x) + B(cj)}p(cj \ x)du, 
and the preferred model is the M, for which this is maximised over i £ I. 
Comments about the detailed implementation of the analysis in the .M-open 
case are similar to those made in the previous problem. 
For M -closed models, we have the more explicit form 
S(m, | x) = a y] p(Mj \ x) j Pj{ij3 \ x) \ogpi(ijj \ x)du} + / B{ui)p((jj | x)da), 
jei •* •* 
which, after straightforward rearrangement, shows that the preferred m, is given 
by minimising, over i € /, 
~Pj{u\x)' 
Pi(u I x) 
du, 
Y^P{Mj\x) Pj{u\x) log 
jei J 
the posterior weighted-average, over models, of the logarithmic divergence (or 
discrepancy) between p,(o; | x) and each of pj(u} | x), j ^i & I. 
If, instead, we were to adopt the (proper) quadratic scoring rule (see, for 
example, Definition 3.17), we obtain, ignoring irrelevant constants, 
u(rrii | x) oc / < 2p,(o; \x) — J p*(u | x)dui > p(u \ x) du, 
so that, after some algebraic rearrangement, in the case of M. -closed models the 
preferred M:- is seen to be that which minimises, over i e I, 
jei 
where 
$>(M,-1 x) fPj(u | x) [f {Pj(u | *)} - /{jh(u I x)}] du,, 
f {pj{u | x)} = 2pj{u I x) - I pj(u I x)dw. 
Comparison of the solutions for the logarithmic and quadratic cases reveals that if, 
for arbitrary /, 
%(u>) \p(u)} = Jp(u)[f{p(u)} - f{q(u)}}<k>, 
defines a discrepancy measure between p and q, both may be characterised as 
identifying the M, for which 
Y2p(MJ I x)HPj(u \x)\pi(u\x)} 
jei 
is minimised over i e /, the differences in the two cases corresponding to the form 
of / (logarithmic or quadratic, respectively). 
6.1 Model Comparison 
401 
Example 6.8. (Point prediction versus predictive beliefs). 
To illustrate the different potential implications of model comparison on the basis of 
quadratic loss for point prediction versus model comparison on the basis of logarithmic score 
for predictive belief distributions, consider the following simple (M -closed) example. 
Suppose that alternative models M\, M2 for x = (x\,...,x„) are defined by: 
. n 
Mj : p(x) = / I] N(Xi I **' Xi)N(lI I W>> A°)^> J = X> 2> 
J i=l 
with Ai, A2, /io. Ao known: we are thus assuming normal data models with precisions Ai, A2, 
respectively, and uncertainty about /x described by AT(/x | ^0, A0) in both cases. 
Now, given x, consider two decision problems: the first problem consists in selecting 
a model and then providing a point prediction, with respect to quadratic loss, for the next 
observable, xn+\; the second problem consists in selecting a model and then providing a 
predictive distribution for arn+i, with respect to a logarithmic score function. 
For the first problem, straightforward manipulation shows that the predictive  
distribution for arn+i assuming model Mj is given by 
p(xn+11 Mj,x) = p3(xn+i \x) = N (xn+i n„(j),X3(n" 1 + A yA.) J ' 
where _ 
/ x \)IM> + nXjX 
Unb) = 1 ,„, ' 
Ao + nAj 
so that, corresponding to the analysis given earlier in this section, model Mj leads to the 
prediction /j,n(j), j = 1,2, and the preferred model is M\ if and only if 
P{Mi\x) 
P(M2\x) * ■ 
To identify these posterior probabilities, we note that, if s2 = n"1 E(arj - x)2, 
= p(x I Mi) = p{x,s2\Ml) 
12W p(a;|M2) p(x,s2\M2) 
= Xixl-i(nXis2)N(x I fig, A0 + nAi) 
A2Xn-i (nX2s2)N(x I hq, A0 + nX2) ' 
which, for small A0, is well approximated by 
, ("-l)/2 
m -{-"—»} 
The posterior model probabilities are then given by 
1 + B12(x)pi2 1 + Si2(a;)pi2 
402 
6 Remodelling 
where pi2 = P(Mi)/P(M2). Model Mi is therefore preferred if and only if 
log[J3i2(a;)pi2] > 0. 
In the case of equal prior weights, pu = 1 and, assuming small Ao, if we write the condition 
in terms of the model variances a2 = Aj1, j = 1,2, we prefer Mi when 
T,(xi-xf ofafllogaf-logaf] 
n - 1 cr^-af 
Noting that the left-hand side is an intuitively reasonable data-based estimate of the model 
variance, we see that model choice reduces to a simple cut-off rule in terms of this estimate. 
For the second decision problem, the logarithmic divergence of p(a;n+i | Mi, a;) from 
p{xn+\ | M2, a;) is given, for small A0, by 
: N(xn+1 
n x AT(a;n+1|x,A2(n/(n+l)J J 
x, A2——- log —) ) (-(- dxn+i 
1, A2 1 
2l0gAT+2 
n+ 1 
N (xn+1\x,\1(n/(n+ 1))} 
H-H3-0) 
with a corresponding expression for 621. The general analysis given above thus implies that 
model Mi is preferred if and only if 
P(Mi|a;)<52i>P(M2|a;)<512, 
i.e., if and only if 
P(Mi I x) > S12 
P(M2\x) 6- 
21 
(rather than > 1, as in the point prediction case). Note, incidentally, that should it happen 
that P(Mi I a;) = P(M21 x), model Mi, would be preferred if and only if *5i2 < <52i, which 
happens if and only if A! > A2. Intuitively, all other things being equal, we prefer in this 
case the model with the smallest predictive variance. 
To obtain some limited insight into the numerical implications of these results, consider 
the case where a\ = 1, of = 25, n = 4, P(Mi) = P(M2) = \ and s2 = 3, which gives 
J5i2 = 0.394, so that P(Mi | a;) = 0.28, P(M2 | a;) = 0.72. Using the point prediction with 
quadratic loss criterion, we therefore prefer M2. However, 6i2 = 1.129 and 62i = 10.31, so 
that if we want to choose a predictive distribution in accordance with the logarithmic score 
criterion we prefer Mu since (0.28)/(0.72) > (1.129)/(10.31). However, if s2 = 4, the 
reader might like to verify that M2 is preferred under both criteria (i312 = 0.058, implying 
that Pr(Mi | a;) = 0.055). 
6.1 Model Comparison 
403 
6.1.6 Approximation by Cross-validation 
For the general problem of model choice followed by a subsequent answer to an 
inference problem, the analysis based on Figure 6.2 implies that the optimal choice 
of model from M is the M, for which 
/ u{mi,a*,u)p{u\x)duj 
is maximised over i £ I, where a* denotes the optimal subsequent decision given 
Mi. In the M-closed case, we have seen that the mixture form of p(u) \ x) enables 
an explicit form of general solution to be exhibited; in the At-completed case, we 
have noted that the solution is in principle available, given appropriate computation. 
We turn now to the case of model comparison within the .M-open framework. 
What can be done to compare the values of M,, i e I, as proxies for an actual 
belief model which itself has not been specified, so that p{u \ x) is not available? 
We shall illustrate a possible approach to this problem by detailed  
consideration of the special case where u> = y, a future observation, for which a point 
prediction with respect to quadratic loss, or a predictive distribution with respect 
to logarithmic or quadratic score, is required. 
First, we note that, in all these cases, the expected utility of the choice M,, 
i e I, has the mathematical form 
/ u{mu a*, y)p{y \ x)dy = I fi(y, x)p(y \ x)dy, 
for some function /, of y and x, depending on i, whose form can be explicitly 
identified. For example, for point prediction with quadratic loss, we have 
fi{y,x) = -{E[y\Mi,x]-y}2; 
for a predictive distribution with logarithmic score function we have, ignoring 
irrelevant terms, 
fi(y,x) =logp(y\Mi,x); 
and with a quadratic score function we have 
fi(y, x) = 2p(y | M(, x) - / p2(y \ Mi,x)dy. 
Secondly, we note that there are n possible partitions of x = xn = (xi,... ,xn) 
into xn = [xn-i(j),Xj], j = l,...,n, where x„_i(j) = xn - {xj} denotes x„ 
with xj deleted, and that, if n is reasonably large, and the x's are exchangeable, 
each such partition effectively provides xn-\ (j) as a "proxy" for x and Xj as a 
404 
6 Remodelling 
I' 
"proxy" for y. If we now randomly select k from these n partitions, a standard law 
of large numbers argument suggests that, as n, k —> oo, 
1 * 
u(rrii,a*,y)p(y\x)dy- -Y^fi{xj,xn^(j)) -> 0, 
so that the expected utilities of M,, i e I, can be compared on the basis of the 
quantities k 
i=\ 
In the case of point prediction, if y is a future observation and y*(j) denotes the 
value of E[y | M,, x] when x is replaced by x„-i(j), this approximation implies 
that we minimise, over i £ I, 
i=\ 
which is an average measure, using squared distance, of how well A/j performs 
when, on a leave-one-out-at-a-time basis, it attempts to predict a missing part of 
the data from an available subset of the data. 
In the case of a predictive distribution with a logarithmic score, we maximise, 
over i £ I, t 
- Y^ logp(zj I Mu xn-i(J)), 
i=\ 
which can be regarded as an average measure based on the logarithm of the  
integrated likelihood under model Mi, and can be conveniently rewritten, for  
computational purposes, in the form 
1 k 
logp(x \Mi)--^2 logp(xn_i (j) | Mi). 
j=l 
In the case of comparing two models, M\, Mi, this criterion can be given 
an interesting reformulation. Under the logarithmic prediction distribution utility, 
and writing pt(y | x) = p(y | Mt, x), we can rearrange the criterion to see that we 
prefer model M\ if 
/ 
P2{y\x) 
where, however, in this .M-open perspective, p(y \ x) is not specified. But, as we 
saw above, we can form n partitions x„ = [x„-\(j),Xj] such that xn-\{j) = 
x„ — Xj is, for large n, a "proxy" for x and Xj is a "proxy" for y. It follows that, 
if we randomly select k of these partitions, the quantity 
■1 y^lo Pi(xj\xn-i(j)) 
kjhi P2(xj\xn-i(j)) 
6.1 Model Comparison 
405 
provides a (consistent, as n —> oo) Monte Carlo estimate of the left-hand side of 
the model criterion above. But this, in turn, can be rewritten so that the criterion 
implies preferring model Mx if 
V* k 
= Y[[Bi2(Xj,xn-i(j))]1/k>l, 
3 = 1 
for j = 1,..., k, where Bn{xj, xn-\(j)) denotes the Bayes factor for Mi against 
Mi, based on the versions 
{Pi {xj | 0i), Pi (0i | xn_i (j))} 
of Mi, Mi- We recall from Section 6.1.4 the role of Bayes factors based on the 
versions {Pi(x \ 0i),pj(6i)} of M\, M2, in the contextof zero-one lossfunctions and 
the M -closed perspective. Although there are clear differences here in formulation 
(jM-open versus M-closed, log-predictive utility versus 0-1 loss), it is interesting 
to note the role played again by the Bayes factor. One interesting difference is the 
following (Pericchi, 1993). In Section 6.1.4, the Bayes factor is evaluating Mi, M2 
on the basis of the models' ability to "predict" x given no data (beyond what has 
been used to specify pi(0,)). In contrast, in the above we are taking a geometric 
average of Bayes factors which are evaluating Mi, M2 on the basis of the models' 
ability to predict one further observable, given n — 1 observations. The former 
situation puts the emphasis on "fidelity to the observed data"; the latter puts the 
emphasis on "future predictive power". 
These kinds of approximate performance measurements for comparing models 
could obviously be generalised by considering random partitions of x involving 
leave-several-out-at-a-time techniques. We shall not develop such ideas further 
here—apart from giving one further interesting illustration in Section 6.3.3—but 
merely note that the above approximation to the optimal Bayesian procedure leads 
naturally to a cross-validation process, which results in a preference for models 
under which the data achieve the highest levels of "internal consistency". Thus, 
for example, in both the quadratic loss and logarithmic score cases, if under model 
Mj there are Xj which are "surprising" in the light of xn-\ (j), thus leading to large 
squared distance terms or small log-integrated-likelihood values, respectively, the 
performance measure will penalise M,. 
Model choice and estimation procedures involving cross-validation  
(sometimes called predictive sample reuse) have been proposed by several authors, from 
a mainly non-Bayesian perspective, as a pragmatic device for generating statistical 
methods without seeming to invoke a "true" sampling model: see, for example, 
Stone (1974) and Geisser (1975) for early accounts and Shao (1993) for a recent 
perspective. The above development clearly establishes that such cross-validatory 
techniques do indeed have an interesting role in a Bayesian decision-theoretic 
k 
n 
j=i 
Pi(xj\xn-i(j)) 
M 
%j | &n—1 
(j)) 
406 
6 Remodelling 
setting for approximating expected utilities in decision problems where a set of  
alternative models are to be compared without wishing to act as if one of the models 
were "true", and in the absence of a specified actual belief model. 
Example 6.9. (Lindley's paradox revisited). 
In Example 6.6, we considered the case of two alternative models, M1; M2, for x = 
(x\,..., xn), corresponding to simple and composite hypotheses about p. in N(x{ | p, A) and 
defined by: 
n 
Mi : pi (x) = Yl N(xi | no, A), no, A known; 
t=l 
M2 : P2(x)= Y[N(xi\n,X)N(n\nl,Xi)dn, pu Xu X known. 
-' i=l 
The analysis given in Example 6.6 was within the M-closed context with P(Mt) > 0, 
i = 1,2, and it was shown that, as Ai —> 0, P{M\ \ x) —* 1 for any fixed x. It follows from 
results given in Sections 6.1.3 and 6.1.4 that, as Ai —» 0, M\ would be the preferred model 
under either zero-one utility, or quadratic loss utility for point prediction (since in this latter 
case, the criterion reduces to the comparison of posterior probabilities when just two models 
are being compared). 
We shall now reconsider the case of quadratic loss for point prediction in the .M-open 
context. 
First, we note that, given x, the optimal prediction of a future observation, y, under 
Mi is just y\ = no, whereas (making appropriate notational changes to the results given in 
Example 5.10) under M2 the optimal prediction is 
(Ai/xi + nXx) 
2/2 = Mn = —^—;—r^ = (1 - wn)m + wnx 
Al + TIA 
where wn = nA(Ai + nA)_1. Secondly, from the cross-validation approximation analysis 
given above we see that Mi is preferred to M2 if and only if, based on k random partitions 
of a; into Xj and xn^(j), 
k k 
5^(A«0 - Xjf < ^ [(1 - U)„_i)/Lti + U)„_i (Xn-i(j) - Xj)}2 
3=1 j=l 
where x„_i(j) = x + (n - 1)_1(^ — xj) is the mean of the sample x with Xj omitted. 
Intuitively, M2 will be preferred if the posterior mean on average does better as a predictor 
than ^o. In particular, if Ai —> 0, and k = n, an approximate analysis shows that Mx is 
preferred to M2 if and only if 
X]0*O - Xj)2 < ^2(xn-i(j) - Xj)2 
3=1 3=1 
<{£z)'±»—r. 
x ' 3=1 
6.1 Model Comparison 
407 
This is easily seen (Pericchi, 1993) to be equivalent to preferring M\ if, and only if, 
n(/io - x)2 2n - 1 
which, under M\, is equivalent to rejecting Mi if a Snedecor Fi„_i random quantity exceeds 
the value 2. See Leonard and Ord (1976) for a related argument. 
This result provides a marked contrast to that obtained in Example 6.6 and makes 
clear that, even given the same data and utility criterion, preferences among models in M 
may differ radically, depending on whether one is approaching their comparison from an 
.M-closed or .M-open perspective. 
6.1.7 Covariate Selection 
We have already had occasion to remark several times that our emphasis in this 
volume is on concepts and theory, and that complex case-studies and associated 
computation will be more appropriately discussed in subsequent volumes. That 
said, it might be illuminating at this juncture to indicate briefly how the theory we 
have developed above can be applied in contexts which are much more complicated 
than those of the simple, stylised examples on which most of our discussion has been 
based. To this end, we shall consider the important problem of model comparison 
which arises when we try to identify appropriate covariates for use in practical 
prediction and classification problems. 
To fix ideas, consider the following problem. Some kind of decision is to 
be made regarding an unknown state of the world a; relating to an individual unit 
of a population: for example, classifying the, as yet unknown, disease state of a 
specific patient, or predicting the, as yet unknown, quality level of the output from 
a particular run of an industrial production process. Possible predictive models 
are to be based, for various choices of m, on covariates j/i(z),..., ym(z), which 
are themselves selected functions of z = (z\,.. .,zs), representing all possible 
observed relevant attributes (discrete or continuous) for the individual population 
unit: for example, the patient's complete recorded clinical history, or a record of 
all the input and control parameters of the production run. To aid the modelling 
process, a data bank (of "training data") is available consisting of 
D = {(cS,z{,...zi),j = l,...,n}, 
recording all the attributes and (eventually known) states of the world for n  
previously observed units of the same population: for example, n previous patients 
presenting at the same clinic, or n previous runs of the same production process. 
We shall suppose that the ultimate objective is to provide, for the state of 
the world u of the new individual unit, a predictive distribution p(w \ y(z), D), 
408 
6 Remodelling 
where y denotes a generic element of the set of all possible {yi(-), i = 1,..., m} 
under consideration for defining covariates. If a; is discrete, we typically refer to 
the problem as one of classification; if u is continuous, we refer to it as one of 
prediction. 
To simplify the exposition, we shall suppose that identification of the density 
p(' I !/(z)> D) is equivalent to the identification of y € y, where y denotes the 
class of all y under consideration. The particular forms in y will depend, of course, 
on the practical problem under consideration: typically, however, it will include 
functions mapping z to Z{, i = 1,..., s, so that individual attributes themselves 
are also eligible to be chosen as covariates. Then, if u{p(- \y(z), D), a;} denotes a 
utility function for using the predictive form p(- \ y(z), D) when a; turns out to be 
the true state of the world, the resulting decision problem is shown schematically 
in Figure 6.4. 
(z,w) 
u(p{.\y{z),D),u) 
Figure 6.4 Selection of covariates as a decision problem 
lfp(z, u | D) represents the predictive distribution for (z, u), given the  
"training data" D, the different possible models corresponding to the different possible 
choices of covariates, y € Y, are then compared on the basis of their expected 
utilities 
u(y \D)= u{p(-1 y(z), D), w}p(z, u> | D)dzdu, yeY. 
The resulting optimal choice will, of course, depend on the form of the utility 
function. Typically, the latter will not only incorporate a score function component 
for assessing p(- \y(z),D), but possibly also a cost component, reflecting the 
different costs associated with the use of different covariates y. For example, in the 
case of disease classification the use of fewer covariates could well mean cheaper 
and quicker diagnoses; in the case of predicting production quality the use of fewer 
covariates could cut costs by requiring less on-line measurement and monitoring. If 
we suppose, for simplicity, that the utility function can be decomposed into additive 
score and cost components, 
*{?(• I V(z), D),u) ~ c{y(x)}, 
6.2 Model Rejection 
409 
the expected utility of the choice y is given by 
u(y \D)= J s{p(-1 y(z),D),u}p(z, u> | D)dzdw - J c{y(z)}dz. 
In many cases, it will be natural to use proper score functions, for example, quadratic 
or logarithmic. If costs are omitted, the optimal model will typically involve a large 
number of covariates; if cost functions are used which increase with the number of 
covariates in the model, a small subset of the latter will typically be optimal. More 
pragmatically, one could ignore costs, identify the optimal y*,~, i = 1,2,... over 
all possible choices of one covariate, two covariates, etc., observe that 
u(y*{1) | D) < u{y\2) | £>)<••• < u(y*(i) \D)<-- 
is typically concave, reflecting marginal expected utility for the incorporation of 
further covariates, and hence select that y*,^ for which u(y1i+l\ \ D) — u(yL | D) 
is less than some appropriately predefined small constant. 
Given the complexity of problems of this type, the set y = M of possible 
models is typically a rather pragmatically defined collection of stylised forms, 
and, recalling the discussion of Section 6.1.2, an M-closed perspective would 
not usually be appropriate. In fact, in most applications p(z, u> | D) is likely to 
prove far too complicated for any honest representation, so that, in the terminology 
of Section 6.1.2, we need to perform a comparison of the models in y from the 
.M-open perspective. There are interesting open problems in the development of 
the cross-validation techniques, that might be employed in particular cases, but 
discussion of these would take us far into the realm of methods and case-studies, 
and so will be deferred to the second and third volumes of this work. 
6.2 MODEL REJECTION 
6.2.1 Model Rejection through Model Comparison 
In the previous section, we considered model comparison problems arising from the 
existence of a proposed range of well-defined possible models, .M = {M,, i e /}, 
for observations x, where the primary decision consisted in choosing m^i € /, 
with the implication of subsequently acting as if the corresponding Mt,i e /, were 
the predictive model. 
In this section, we shall be concerned with the situation which arises when 
just one specific well-defined model for x, M0 say, has been proposed initially, 
and the primary decision corresponds either to the choice m0, which corresponds 
to subsequently acting as if M0 were the predictive model, or to the choice mg 
(thus, in a sense, rejecting M0), with the implication of "doing something else". 
410 
6 Remodelling 
u{rriQ | x) 
u(mc0\x)"l 
Figure 6.5 Model rejection as a decision problem 
If, given x, u(-1 x) denotes the ultimate expected utility of a primary action, this 
model rejection problem might be represented schematically by Figure 6.5. 
Such a structure may arise, for example, as a consequence of M0 being the 
only predictive model thus far put forward in a specific decision problem context; 
or, as a consequence of the application of some kind of principle of simplicity 
or parsimony, as an attempt to "get away with" using M0, instead of using more 
complicated (but, in this context, unstated) alternatives. 
What perspectives might one adopt in relation to this, thus far clearly ill- 
defined, problem of model rejection? 
If we are concerned with coherent action in the context of a well-posed decision 
problem, we see from Figure 6.5 that we cannot proceed further unless we have 
some method for arriving at a value of u(mc0 | x) to compare with u(mo | a;). One 
way or another, we are forced to consider alternative models to Md. 
Let us suppose therefore that we have embedded M0 in some larger class 
of models M = {Mi, i e I}. This might be done, particularly where M0 has 
been put forward for reasons of simplicity or parsimony, by consideration of actual 
alternatives to M0 thought (by someone) to be of practical interest. Otherwise, it 
might be done by consideration of formal alternatives, generated by selecting, in 
some way, a "mathematical neighbourhood" of Mo (which might also, of course, 
contain alternatives of practical interest). For this redefined problem of model 
rejection within M, shown schematically in Figure 6.6, the hitherto undefined 
value of w(mg | a;) becomes 
u(ttiq I x) = maxu(mj | a;). 
iei' 
where I' — I - {0} indexes the models in M distinct from M0. 
For any specific decision problem, the calculation of u{rrii \ x), i E I proceeds 
as indicated in Section 6.1. Thus, if we adopt the M-closed perspective, evaluations 
are based on mixture forms involving prior and posterior probabilities of the Mif 
i G /; if we adopt the .M-completed perspective, the calculation is, in principle, 
well-defined, but may be numerically involved; if we adopt the .M-open perspective, 
we may use a cross-validation procedure to estimate the expected utilities. 
6.2 Model Rejection 411 
u(m0 | x) 
Figure 6.6 Model rejection within M = {M;, i £ 1} 
In the case where {Mi, i G /'} consists of actual alternatives to M0, we might 
regard the redefined model rejection problem as essentially identical to the model 
comparison problem, so that rejecting m0 corresponds to choosing the best of m*, 
i G /'. However, this would seem to ignore the fact that when M0 has been put 
forward for reasons of simplicity or parsimony there is an implicit assumption that 
the latter has some "extra utility", over and above the expected utility u(m0 | x). 
Thus, if u(wiq I x) — u(m0 | x) were positive, but not "too large", we might still 
prefer M0 because of the special "simple" status of M0. The same argument 
applies even more forcibly in the case where {Mi,i € /'} consists of formal 
alternatives to M0, since rejecting M0 may not lead obviously to an actual alternative 
model, and the "extra utility" of choosing M0 if at all possible may be greater. 
From this perspective, the redefinition of the problem of model rejection as one of 
model comparison corresponds to modifying slightly the representation given in 
Figure 6.6, by replacing u(m0 | x) by u(m0 | x) + e0(x) where e0(x) represents, 
given x, an implicit (but as yet undefined) extra utility relating to the special status 
of M0. (See Dickey and Kadane, 1980, for related discussion.) 
The formulation of the model rejection problem given above is rather too 
general to develop further in any detail. In order, therefore, to provide concrete 
illustrative analyses, we shall assume, for the remainder of this chapter that M = 
{M0, Mi}, where, for some parametric family {p(- \6),6 £ 9}, predictive models 
for x are defined, for some 9o C 9, by: 
M0: p0{x)= f p(x\0)p0{0)dG 
Mi : pi(x)= [ p(x\e)Pl(e)dG. 
Je 
Specially, M0 will correspond to either: 
(i) p(x | 60), a simple hypothesis that 6 = 00 (specified by a degenerate prior 
Po(0) which concentrates on 6q), or 
(ii) p(x | 4>0, A), a simple hypothesis on a parameter of interest <j> where A is a 
nuisance parameter (specified by apriorpo(#) = p(A | 0O) which concentrates 
on the subspace defined by <\> = 0O). 
412 
6 Remodelling 
The next three sections consider some detailed model rejection procedures 
within this parametric framework. 
6.2.2 Discrepancy Measures for Model Rejection 
Within the parametric framework described at the end of the previous section, 
model M0 corresponds to a form of parametric restriction (or "null hypothesis") 
imposed on model Mi. In such situations, it is common practice to consider the 
decision problem of model rejection to be that of assessing the compatibility of 
model M0 with the data x, this being calibrated relative to the wider model Mi 
within which Mo is embedded. We shall focus on this version of the model rejection 
problem with utilities denned by u(m0,0), u(mi,0), 6 G 9, and overall beliefs, 
p(61 x), 6 € 6, denned either by an M -closed form, 
P(M0 | x)p{0 | M0, x) + P(Mi | x)p{8 \Mux), 
with M = {M0, Mi}, or by the {Mi}-closed form, p(0 | Mi, x), the latter  
providing a kind of "adversarial" analysis, since it assigns M0 no special status. 
Noting that there are only two alternatives in this decision problem, it suffices 
to specify the (conditional) difference in utilities, say in favour of the larger model 
Mx, 
6(6) = u(m\,6) — u(m0,6), 
since the optimal inference will clearly be to reject model Mo if and only if 
/ [u(mu 0) - u(m0,0)} p(0 \ x) > e0(x), 
where e0(x) represents, as before, the utility premium attached to keeping the 
simpler model M0. We shall refer to 6(6) as a (utility-based) discrepancy measure 
between M0 and Mi when 6 e 9 is the true parameter. In terms of the discrepancy, 
the optimal action is to reject model M0 if and only if 
t(x) > e0(x), 
where 
t(x) = [ 6(8)p(01 x)dO. 
Je 
With a considerable reinterpretation of conventional statistical terminology, 
we might refer to t(-) as a test statistic, leading, for given data x, to the rejection 
of model M0 if the observed value of the test statistic exceeds a critical value, 
c(x) = e0(x). 
6.2 Model Rejection 
413 
How might c(x) be chosen? One possible approach could be to consider, prior 
to observing x and assuming M0 to be true, a choice of c() which would lead M0 
to only be rejected with low probability (a, say, for values of a of the order of 0.05 
or 0.01). Under this approach, we would choose c() such that 
P{t(x) > c(x) I M0) = a, 
thus obtaining the (1 — a)th percentage point of the predictive distribution of t(x), 
conditional on information available prior to observing x and assuming M0 to be 
true. Of course, this is just one possible approach to selecting c() and has no 
special theoretical significance. It is interesting that this choice turns out to lead 
to commonly used procedures for model rejection which have typically not been 
derived or justified previously from the perspective of a decision problem (see, 
for example, Box, 1980, for a non-decision-theoretic approach). Examples will 
be given in the following two sections. However, for criticism of the practice of 
working with a fixed a, see Appendix B, Section 3.3. 
6.2.3 Zero-one Discrepancies 
Suppose that the discrepancy measure introduced in Section 6.2.2 is denned to be 
ro if0 = 0O 
*(*)-\i if0^0o. 
Assuming the decision problem of model rejection to be denned from the M-closed 
perspective, we obtain 
t{x)= 16(0)p(O\x)dO 
= p(Mi | x) 
p(M])p{x\M{) 
p(M0)p(x | M0) + p{Mi)p(x | Mi) 
where 
, ,„>_ f p(x | 0O) if 0o specifies a simple hypothesis 
HX' °' ~ I / P(x | 0o, A)p(A | d>0)d\ if 0O = (0o, A), 
and 
p(x\M1)= /p(x\0)pi(0)dO. 
It follows from the analysis given in the previous section that M0 should be 
rejected if and only if, for specified critical value c(x), 
P(Mi\x)>c(x), 
414 
6 Remodelling 
i.e., if the posterior odds on M0 and against Mi are smaller than (1 — c(x))/c(x). 
In the case where the prior odds are equal, the rejection criterion for M0 in terms 
of the Bayes factor is given by 
Boi(x) < 
c(x) 
c{x) 
Example 6.10. (Null hypothesis for a binomial parameter). 
Suppose that x represents x successes in n binomial trials, with M0, Mi denned by 
M0: po(x) =p0(x\n) =Bi(x\n,60), 
Mi : pi (x) = pi (x | n) = / Bi(x | n, 0)Un(0 \0,l)d6, 
Jo 
and P(M0) = P(Mi) = |. Straightforward manipulation shows that 
Boi(x) = 
r(n + 2) 
T(x + l)r(n - x + 1) 
0J(1 - 60)n-x, 
which, assuming, for purposes of illustration, large x, n — x, and applying Stirling's formula, 
can be approximated by 
where 
B0i(x) 
X\x) = 
"|l/2 
exp{-fx2(:c)}, 
2n60(l - 60) 
(x-n60)2 (n - x - n(l - e0))2 
n0o 
n(l - 60) 
is the usual chi-squared test statistic. By considering —2 log Bi2(x), for given n, Oo, a value 
of c(x) = c(#o, n) which calibrates the procedure to only reject Mo with probability a when 
M0 is true, is defined by the equation 
2n60(l - 60) 
( c(0q, n) 
\l-c(60,n) 
exp{x?,Q}, 
where \\,a denotes the upper 100a% point of a x2 distribution. Of course, having decided 
on this particular approach to the choice of c(x), there is no real need to identify it! The 
rejection procedure is simply defined by comparing the test statistic value, x2(x)> with its 
tail critical value, x\,a- The reader might like to verify (perhaps with the aid of Jeffreys, 
1939/1961, Chapter 5) that similar results can be obtained for a variety of "null models" in 
more general contingency tables. 
6.2 Model Rejection 415 
6.2.4 General Discrepancies 
Given our systematic use throughout this volume of the logarithmic divergence 
between two distributions, an obviously appealing form of discrepancy measure is 
given by 
w = j«'W**l$i&*< 
where p(x | 6) is the general parametric form of model M\. 
In the case of a location-scale model, #o — (fM),&), 0 = (//, a), we might 
consider the standardised measure 
In any case, the general prescription will be to reject M0 if t(x) > c(x), for some 
appropriate c(x), where 
t{x)= I6{0)p{0\x)d0, 
withp(0 | x) derived either from an M-closed model, as illustrated in Section 6.2.3, 
or from the "adversarial" form corresponding to assuming model M\. 
Example 6.11. (Lindley's paradox revisited, again). In Examples 6.6 and 6.9 we 
considered the use of models Mi, M2 denned, for x = (x1:..., x„), by 
n 
M, : p(x) = Y\N(Xi\lM>,X), A<o>^ known, 
•=i 
M2: p(x) = / fj7V(a;j|/t,A),7V(/t|/ti,Ai)d/t, Mi, Ai, A known. 
•^ j=i 
Using the logarithmic divergence discrepancy, we obtain 
6(»)= fP(x\„,X)hgP^j^Xhx 
J p(x\(io,X) 
= n[N(x\„,X)\og^Xj'I'Xhx 
J N(x I no, A) 
which is just a multiple (by n/2) of a natural, standardised, measure (the non-centrality 
parameter) suggested by intuition as a discrepancy measure for a location scale family. 
416 
6 Remodelling 
Assuming the reference prior for \i derived from an {Mi}-closed perspective, which, 
as a special case of Proposition 5.24, is easily seen to be uniform, we have the reference 
posterior 
p(n\x) = N(n\x,n\), 
and hence 
f(x) = it / (M - Mo)2^(M I x, nX)d/j, 
^ J-oo 
J[l + *2(x)] 
2 
where, with a2 = A"1, the statistic z(x) = <Jn(x — Ho)/cr is seen to be a version of 
the standard significance test statistic for a normal location null hypothesis. With respect 
to p(x | Oo), this has an N(z | 0,1) distribution and the appropriately calibrated value of 
c(x) is thus implicitly defined, for example, by rejecting Mo if | .z(x) | exceeds the upper 
100(a/2)% point of an N(- | 0,1) distribution. 
The above analysis is easily generalised to the case of unknown A. Here, the reference 
posterior for (/j, A) from an {Mi}-closed perspective (see Example 5.17) is given by 
p((i,X\x) = N(ti\x,nX) Ga(A|i(n- 1), \ns2), 
where ns2 = T,(x, — x)2. It follows that 
t(x) = - I I nX(n - na)2N(n \ x, nX) Ga (A | |(n - 1), ^ns2) d/j,dX 
2 J-oo JO 
= - / [1 + nX(x - mo)2] Ga (A | i(n - 1), \ns2) d/j,dX 
2 Jo 
=Mi+S(3f-Mo)2] 
where, with s' — [ns2/(n — l)]1/2, we see that <Jn(x — Ho)/s' is a version of the standard 
significance test statistic for a normal location null hypothesis in the presence of an unknown 
scale parameter. With respect to p(x \ 60), this has a St(t | 0,1, n — 1) distribution and the 
appropriately calibrated value of c(x) is defined by the standard rejection procedure. 
The reader can easily extend the above analyses to other stylised test situations: 
for example, testing the equality of means in two independent normal samples, with 
known or unknown (equal) precisions. Rueda (1992) provides the general  
expressions for one-dimensional regular exponential family models. Multivariate normal 
location cases are also easily dealt with, the logarithmic divergence discrepancy in 
this case being proportional to the Mahalanobis distance (see Ferrandiz, 1985). We 
shall not pursue such cases further here, since it seems to us that detailed  
discussion of model rejection and comparison procedures all too easily becomes artificial 
6.3 Discussion and Further References 
417 
outside the disciplined context of real applications of the kind we shall introduce in 
the second and third volumes of this work. From the perspective of this volume, we 
have taken the analyses of this chapter sufficiently far to demonstrate the creative 
possibilities for model choice and comparison within the disciplined framework of 
Bayesian decision theory. 
6.3 DISCUSSION AND FURTHER REFERENCES 
6.3.1 Overview 
We have argued that both from the perspective of a sensitive individual modeller, 
and also from that of a group of modellers, there are frequently strong reasons for 
considering a range of possible models. 
This obviously leads to the problem of model comparison, or model choice, 
and our approach has been to consider formally a decision problem where the 
action space is the class of available models. In this setting, we have shown that 
"natural" Bayesian solutions, such as choosing the model with the highest posterior 
probability, are obtained as particular cases of the general structure for stylised, 
appropriately chosen, loss functions. 
We have also considered the generally ill-posed problem of model rejection, 
where the primary decision consists in acting as if the proposed model were true 
—without having specific alternatives in mind—and have shown that useful results 
may be obtained by embedding the proposed model within a larger class, and then 
using discrepancy measures as loss functions in order to decide whether or not the 
original simpler model may be retained after all. 
There is an extensive Bayesian literature directly related to the issues discussed 
in this chapter. Some authors adopt a purely inferential approach, by deriving either 
posterior probabilities, or Bayes factors for competing models; see, for example, 
Lindley (1965, 1972), Dickey and Lientz (1970), Dickey (1971, 1977), Leamer 
(1978), Bernardo (1980), Smith and Spiegelhalter (1980), Zellner and Siow (1980), 
Spiegelhalter and Smith (1982), Zellner (1984), Berger and Delampady (1987), Pet- 
tit and Young (1990), Aitkin (1991), Gomez-Villegas and G6mez (1992), Kass and 
Vaidyanathan (1992), McCulloch and Rossi (1992) and Lindley (1993). Others 
openly adopt a decision-theoretic approach; see, for example, Karlin and Rubin 
(1956), Raiffa and Schlaifer (1961), Schlaifer (1961), Box and Hill (1967), DeG- 
root (1970), Zellner (1971), Bernardo (1982, 1985a), San Martini and Spezzaferri 
(1984), Berger (1985a), Bernardo and Bayarri (1985), Ferrandiz (1985), Poskitt 
(1987), Felsenstein (1992) and Rueda (1992). 
418 
6 Remodelling 
6.3.2 Modelling and remodelling 
We have already argued that we see Bayesian statistics as a rather formalised 
procedure for inference and decision making within a well-defined probabilistic 
structure. 
Fully specified belief models are an integral part of this structure, but it would 
be highly unrealistic to expect that in any particular application such a belief model 
will be general enough to pragmatically encompass a defensible description of 
reality from the very beginning. 
In practice, we typically first consider simple models, which may have been 
informally suggested by a combination of exploratory data analysis, graphical  
analysis and prior experience with similar situations. And even with such a simple 
model, more formal investigation of its adequacy and the consequences of using 
it will often be necessary before one is prepared to seriously consider the model 
as a predictive specification. Such investigations will typically include residual 
analysis, identification of outliers and/or influential data, cluster analysis, and the 
behaviour of diagnostic statistics when compared with their predictive  
distributions. We shall not elaborate on this here. Some relevant references are Johnson 
and Geisser (1982, 1983, 1985), Pettit and Smith (1985), Pettit (1986), Geisser 
(1987, 1992, 1993), Chaloner and Brant (1988), McCulloch (1989), Verdinelli 
and Wasserman (1991), Gelfand, Dey and Chang (1992), Weiss and Cook (1992), 
Guttman and Peiia (1993), Peiia and Guttman (1993), and Chaloner (1994). 
Bayarri and DeGroot (1987, 1990, 1992a) provide a Bayesian analysis of 
selection models, where data are randomly selected from a proper subset of the 
sample space rather than from the entire population. 
As a consequence of this probing, mainly exploratory analysis, a class of 
alternative models will typically emerge. In this chapter we have discussed some 
of the procedures which may be useful in a formal comparison of such alternative 
models. The outcome of this strategy will typically be a more refined model for 
which a similar type of analysis may be repeated again. 
Naturally, a pragmatic combination of time constraints, data limitations, and 
capacity of imagination, will force this sequence of informal exploration and formal 
analysis to eventually settle on the use of a particular belief model, which hopefully 
can be defended as a sensible and useful conceptual representation of the problem. 
This remodelling process is never fully completed, however, in that either new 
data, more time, or an imaginative new idea, may force one to make yet another 
iteration towards the never attainable "perfect", all powerful predictive machine. 
6.3.3 Critical issues 
We shall comment further on six aspects of the general topic of remodelling under 
the following subheadings: (i) Model Choice and Model Criticism, (ii) Inference 
6.3 Discussion and Further References 
419 
and Decision, (iii) Overfitting and Cross-validation, (iv) Improper Priors, (v)  
Scientific Reporting, and (vi) Computer Software. 
Model Choice and Model Criticism 
We have reviewed several procedures which, under different headings such as model 
comparison, model choice or model selection, may be used to choose among a class 
of alternative models, and we have argued that, from a decision-theoretical point 
of view, the problem of accepting that a particular model is suitable, is ill-defined 
unless an alternative is formally considered. See, also, Hodges (1990, 1992). 
However, partly due to the classical heritage of significance testing, and given 
the obvious attraction of being able to check the adequacy of a given model without 
explicit consideration of any alternative, non-decision-theoretic Bayesians have 
often tried to produce procedures which evaluate the compatibility of the data with 
specific models. 
As clearly stated by Box (1980), the posterior distribution of the parameters 
only permits the 
estimation of the parameters conditional on the adequacy of the entertained model, 
while the predictive distribution makes possible criticism of the entertained model 
in the light of current data. 
Moreover, the predictive distributions which correspond to different models 
are comparable among themselves, while—in general—the posteriors are not. The 
use of predictive distributions to check model assumptions was pioneered by  
Jeffreys (1939/1961). Additional references include Geisser (1966,1971,1985,1987, 
1988,1993), Box and Tiao (1973), Dempster (1975), Geisser and Eddy (1979),  
Rubin (1984), Bernardo and Bermudez (1985), Clayton et al. (1986), Gelfand, Dey 
and Chang (1992) and Giron etal. (1992). 
The basic idea consists of defining a set of appropriate diagnostic functions 
t{ = ti(xn+i,..., xn+k) of the data and comparing their actual values in a sample 
with their predictive distributions p^. (• | x\,..., xn) based on a different sample 
from the same population. Possible comparisons include checking whether or not 
the observed tj's belong to appropriate predictive HPD intervals, or determining the 
predictive probability of observing t/s more "outlying" than those observed. The 
reader will readily appreciate that common techniques such as residual analysis, 
identification of influential observations, segregation of homogeneous clusters, or 
outlier screening, can all be reformulated as particular implementations of this 
general framework. 
As mentioned before, we see these very useful activities as part of the  
informal process that necessarily precedes the formulation of a model which we can 
then seriously entertain as an adequate predictive tool. However, it seems to us 
420 
6 Remodelling 
inescapable that if a formal decision is to be reached on whether or not to operate 
with a given model, then some form of alternative must be considered. 
For further discussion of model choice, see Winkler (1980), Klein and Brown 
(1984), Krasker (1984), Florens and Mouchart (1985), Poirier (1985), Hill (1986, 
1990), Skene et al. (1986) and West (1986). 
Inference and Decision 
Throughout this volume, we have emphasised the advantages of using a formal 
decision-oriented approach to the stylised statistical problems which represent a 
large proportion of the theoretical statistical literature. These advantages are  
specially obvious in model comparison since, by requiring the specification of an 
appropriate utility function, they make explicit the identification of those aspects 
of the model which really matter. 
We have seen, moreover, that the more traditional Bayesian approaches to 
model comparison, such us determining the posterior probabilities of competing 
models or computing the relevant Bayes factors, can be obtained as particular cases 
of the general structure by using appropriately chosen, stylised utility functions. 
Very often, the consequences of entertaining a particular model may usefully 
be examined in terms of the discrepancies between the prediction provided by the 
model for the value of a relevant observable vector, t, say, and its actual,  
eventually observed, value. Scoring rules, of the general type u(pt(-\xi,..., xn), t) 
provide natural utility functions to use in this context, by explicitly evaluating the 
degree of compatibility between the observed t and its predictive distribution. 
Overfitting and Cross-Validation 
If we are hoping for a positive evaluation of a prediction it is crucial that the  
predictive distribution is based on data which do not include the value to be predicted; 
otherwise, severe overfitting may occur. Pragmatically, however, although it is 
sometimes possible to check the predictions of the model under investigation by 
using a totally different set of data than that used to develop the model, it is far 
more common to be obliged to do both model construction and model checking 
with the same data set. 
A natural solution consists of randomly partitioning the available sample z = 
{xi,..., xn} say, into two subsamples {zx, z2} one of which is used to produce the 
relevant predictive distributions, and the other to compute the diagnostic functions; 
the procedure then being repeated as many times as is necessary to reach stable 
conclusions. This technique is usually known as cross-validation. For recent work, 
see Pena and Tiao (1992), Gelfand, Dey and Chang (1992), and references therein. 
For a discussion of how cross validation may be seen as approximating formal 
Bayes procedures, see Section 6.1.6. 
6.3 Discussion and Further References 
421 
A possible systematic approach to cross-validation starting from a sample of 
size n, z = {xi,..., xn}, and a model {p(x \ 0), p(9)}, involves the following 
steps: 
(i) Define a sample size k, where k < n/2 is large enough to evaluate the relevant 
observable function t — t(x\,..., x^). The observable function could either 
be that predictive aspect of the model which is of interest, as described by the 
utility function, or a diagnostic function, as described in the above approach 
to model criticism. 
(ii) Determine the set of all predictive distributions of the form 
Pj(*(*j) I *[;])' J = l.-••>($. 
where Zj is a subsample of z of size k and z^ consists of all the Xj's in z 
which are not in z j. 
(iii) Estimate the expected utility of the model by 
-l 
3 
Note that the last expression is simply a Monte Carlo approximation to the exact 
value of the expected utility. We also note that this programme may be carried out 
with reference distributions since the corresponding reference (posterior) predictive 
distributions Wj(t(zj) \z^) will be proper even if the reference prior w(6) is not. 
Improper Priors 
In the context of analysis predicated on a fixed model, we have seen in Chapter 5 
that perfectly proper posterior parametric and predictive inferences can be obtained 
for improper prior specifications. 
When it comes to comparing models, however, in general the use of improper 
priors is much more problematic. We first note that for models 
Mi{ft(x|0),p((0)}, i e /, 
the predictive quantities 
Pi{x) = / pi(x\0)pi(0)d6, i e /, 
typically play a key role in model comparisons for a range of specific decision 
problems and perspectives on model choice. But if one or more of the Pi{0) is not 
a proper density, the corresponding Pi{x)'s will also be improper, thus precluding, 
: 
422 
6 Remodelling 
for example, the calculation of posterior probabilities for models in an M-closed 
analysis. 
Essentially, with a formal improper specification for the prior component of 
a model an initial amount of data needs to be passed through the prior to posterior 
process before the model attains proper status as a predictive tool and can hence be 
compared with other alternative predictive tools. 
An exception to this arises when two models Mj,Mj, say, have common 0 
and improper p(0), in which case it can be argued that the ratio Pi(x)/pj(x), the 
Bayes factor in favour of Mi, does provide a meaningful comparison between the 
two models, However, there is an inherent difficulty with these methods when the 
models compared have different dimensions. 
Indeed, with the reference prior approach some models are implicitly  
disadvantaged relative to others; technically, this is due to the fact that the amount of 
information about the parameters of interest to be expected from the data crucially 
depends on the prior distribution of the nuisance parameters present in the model. 
Lindley's paradox (see, also, Bartlett, 1957), discussed earlier in this chapter, is a 
well known simple example of this behaviour. 
A possible solution consists in specifying the improper prior probabilities of 
the models—or, equivalently, weighting the Bayes factors—in a way which may be 
expected to achieve neutral discrimination between the models. Some suggestions 
along these and similar lines include Bernardo (1980), Spiegelhalter and Smith 
(1982), Pericchi (1984), Eaves (1985) and Consonni and Veronese (1992a). 
Another possible solution to the problem of comparing two models in the 
case of improper prior specification for the parameters is to exploit the use of 
cross-validation as a Monte Carlo approximation to a Bayes decision rule. 
As we saw in Section 6.1.6, for the problem of predicting a future observation 
using a log-predictive utility function the (Monte Carlo approximated) criterion for 
model choice involves the geometric mean of Bayes factors of the form 
where [x„-\ (j), Xj] denotes a partition of x = (x\,..., xn), and 
Pi(xj | X„_i(j)) = / Pi(xj | 0i)pi(0i | Xn-i(j)) dGi. 
Since, for sufficiently large n, Pi{0i \ xn-i(j)) will be proper, even for improper 
(non-pathological) priors Pi{0i), no problem arises. 
However, recalling from our discussion in Section 6.1.6 that the  
conventional Bayes factor is used to assess the models' ability to "predict x" from 
{Pi{x | 6i),Pi(6i)}, we see that the latter does run into trouble if Pi(6i) is  
improper, since, then, pt (x) is not proper. 
6.3 Discussion and Further References 
423 
Proceeding formally, if we were to take logpj(a;) as the utility of choosing 
Mi, the M-open perspective prefers Mt if 
/ 
hg^\p(x)dx>0, 
p2(x) 
where p(x) is not specified. Again, we can approach the evaluation of the left- 
hand side as a Monte Carlo calculation, based on partitioning x and averaging over 
random partitions. However, in this contect we want partitions where the proxy 
for the predictive part resembles data x and the proxy for the conditioning part 
resembles "no data". The closest we can come to this, and overcome the problem 
of the impropriety of Pi(Oi), is to take partitions of the form x — [xs(j), xn-s(j)], 
wheres(> 1) is the smallest integer such that bothpi(Bi \ xs(j)) andp2(#2 I xs(j)) 
are proper, and j = 1,..., Q). 
The proposal is now to select randomly k such partitions, and approximate the 
left-hand side of the criterion inequality by 
3=1 { 
Pl(xn-S(j)\xs(j)) 
k j^i^ \MXnsU) I XsU)) 
The (Monte Carlo approximated) model choice criterion then becomes, prefer 
Mi if 
k 
n 
3=1 
Pl(Xn-s(j)\Xs{j)) 
Pi(xn-S(j)\xs(j)) 
1/k 
■■Y[[B12(Xns(j),X.(j))]1,k >1, 
3=1 
where j512(x„_s(j), xs(j)) denotes the Bayes factor for Mx against M2, based on 
the versions 
\pi(x„-s(j)\oi),pi(ei\xs(j)) > 
of Mi, M2. Again, we see the explicit role of the geometric average of Bayes 
factors, but with the latter "reversing", in a sense, the role of past and future data 
compared with the form obtained in Section 6.1.6. 
At the time of writing we are aware of work in progress by several researchers who 
propose forms related to those discussed here. These include: J. O. Better and 
L. R. Pericchi (intrinsic Bayes factors), A. O'Hagan (fractional Bayes factors) 
and A. F. de Vos (fair Bayes factors). 
424 
6 Remodelling 
From a practical perspective, it might be desirable to trade-off, in utility terms, 
"fidelity to the observed data" and "future predictive power". This can be formalised 
by adopting a utility function of the form 
a \og\pi(x)\ + (1 - a) log\pi(y \ x)\, 0 < a < 1, 
which, in turn, leads to a criterion of the form, prefer Mi if 
{k 1 a ( k 1 1~a 
fl [Bn{xn-s{j),xa{j))}llk I I n [Bn{xhxn^{j))]llk \ > 1. 
Work in progress (by L. R. Pericchi and A. F. M. Smith) suggests that such a 
criterion effectively encompasses and extends a number of current criteria. 
We conclude by emphasising again that a predictivistic decision-theoretical 
approach to model comparison, where models are evaluated in terms of their  
predictive behaviour, bypasses the dimensionality issue, since posterior predictive 
distributions obtained from models with different dimensions are always directly 
comparable. 
Scientific Reporting 
Our whole development has been predicated on the central idea of normative  
standards for an individual wishing to act coherently in response to uncertainty. Beliefs 
as individual, personal probabilities are the key element in this process and, at any 
given moment in the learning cycle, are the encapsulation of the current response 
to the uncertainties of interest. 
However, while many are willing to concede that, in narrowly focused  
decision-making, such beliefs are an essential element, there has also been a  
widespread view (see, for example, Fisher, 1956/1973) that it would be somehow  
subversive to sully the nobler, objective processes of science by allowing subjective 
beliefs to enter the picture. As Dickey (1973) has remarked, rhetorically: 
But is not personal knowledge, or opinion, like superstition, non-objective and 
unscientific, and therefore to be avoided in science? Who cares to read about a 
scientific reporter's opinion as described by his prior and posterior probabilities? 
We have already made clear our own general view that objectivity has no 
meaning in this context apart from that pragmatically endowed by thinking of it as 
a shorthand for subjective consensus. However, there are clearly practical problems 
of communication between analysts and audiences which need addressing. 
The solution to such problems lies in combining ideas from Chapters 4 and 6. 
On the one hand, we have seen that shared assumptions about structural aspects 
6.3 Discussion and Further References 
425 
of beliefs (for example, exchangeability) can lead a group of individuals to have 
shared assumptions about the parametric model component, while perhaps differing 
over the prior component specification. On the other hand, we have seen, from 
several perspectives, that entertaining and comparing a range of models fit perfectly 
naturally within the formalism. There is nothing within the world of Bayesian 
Statistics that prohibits a scientist from performing and reporting a range of "what 
if?" analyses. To quote Dickey again: 
... communicating a single opinion ought not to be the purpose of a scientific 
report; but, rather, to let the data speak for themselves by giving the effect of 
the data on the wide diversity of real prior opinions. ... an experimenter can 
summarise the application of Bayes' theorem to whole ranges of prior  
distributions, derived to include the opinions of his readers. Scientific reports should 
objectively exhibit as much as possible of the inferential content of the data, 
the data-specific prior-to-posterior transformation of the collection of all  
personal probability distributions on the parameters of a realistically rich statistical 
model. 
We believe that this is the way forward—although, it has to be said, there 
is a great deal of work to be done in effecting such a cultural change. There are 
also some obvious technical challenges in making such a programme routinely 
implementable in practice. However, computational power grows apace, as does 
the sophistication of graphical displays. We shall return to this general problem in 
the second and third volumes of this work. 
For early thoughts on these issues, see Edwards et al. (1963) and Hildreth 
(1963); for technical expositions, see Dickey (1973), Roberts (1974) and Dickey 
and Freeman (1975); for a discussion in the context of a public policy debate, see 
Smith (1978). 
Computer Software 
Despite the emphasis in Chapter 2 and 3 of this volume on foundational issues— 
necessary for a complete treatment of Bayesian theory—we are well aware that 
the majority of practising statisticians are more likely to be influenced by positive, 
preferably hands-on, experience with applications of methods to concrete problems 
than they ever will be by philosophical victories attained through the (empirically) 
bloodless means of axiomatics and stylised counter-examples. We are also well 
aware that the availability of suitable software is the key to the possibility of  
obtaining that hands-on experience. 
But what are the appropriate software tools for Bayesian Statistics? What 
software? For whom? For what kinds of problems and purposes? 
A number of such issues were reviewed in Smith (1988), but at the time of 
writing, many still remain unresolved. Goel (1988) provided a review of Bayesian 
426 
6 Remodelling 
software in the late 80's. Examples of creative use of modern software in Bayesian 
analysis include: Smith et al. (1985, 1987) and Racine-Poon et al. (1986), who 
describe the use of the Bayes Four package; Grieve (1987); Lauritzen and Spiegel- 
halter (1988), on which the commercial expert system builder Ergo™ is based; 
Albert (1990), Korsan (1992), and Ley and Steel (1992), who make use of the  
commercial package Mathematical; Tierney (1990, 1992), who presents LISP-STAT, 
an object oriented environment for statistical computing and discusses possible 
uses of graphical animation; Cowell (1992) and Spiegelhalter and Cowell (1992), 
who, respectively, describe and apply the probabilistic expert system shell BAIES; 
Racine-Poon (1992), who discusses sample-assisted graphical analysis; Thomas 
et al. (1992), who describes BUGS, a program to perform Bayesian inference 
using Gibbs sampling; Wooff (1992), who describes [B/D], an implementation 
of subjectivist analysis of beliefs, as described by Goldstein (1981, 1988, 1991, 
and references therein); and Marriot and Naylor (1993), who discuss the use of 
MINITAB to teach Bayesian statistics. Further review and detailed illustration will 
be provided in the volumes Bayesian Computation and Bayesian Methods. 
427 
Appendix A 
Summary of 
Basic Formulae 
Summary 
Two sets of tables are provided for reference. The first records the definition, and 
the first two moments of the most common probability distributions used in this 
volume. The second records the basic elements of standard Bayesian inference 
processes for a number of special cases. In particular, it records the appropriate 
likelihood function, the sufficient statistics, the conjugate prior and  
corresponding posterior and predictive distributions, the reference prior and corresponding 
reference posterior and predictive distributions. 
A.1 PROBABILITY DISTRIBUTIONS 
The first section of this Appendix consists of a set of tables which record the 
notation, parameter range, variable range, definition, and first two moments of the 
probability distributions (discrete and continuous, univariate and multivariate) used 
in this volume. 
428 
A. Summary of Basic Formulae 
Univariate Discrete Distributions 
Bt(x I 9) Bernoulli (p. 115) 
0 < 6 < 1 x = 0,1 
P(x) = ex{i - ey-x 
e\x\ = e v[x] = e(i - e) 
Bi(a; | 6, n) Binomial (p. 115) 
0 <6< 1, n = 1,2,... z = 0,l,...,n 
p(a;)= fnV(l-0)"-x 
£[*] = n<? V[x] = n0(l - 0) 
Bb(a; | a, /3, n) Binomial-Beta (p. 117) 
a >0, (3 > 0, n= 1,2,... a; = 0, l,...,n 
p(x) = c(njr(a + a;)r(/3 + n-a;) 
T(a)r(/3)r(a + 0 + n) 
r , a na/3 (a + j3 + n) 
E[x]=nc7^3 V[x]=(a + W(a + P + 1) 
Hy(x | N, M, n) Hypergeometric (p. 115) 
N = l,2,... x = a,a+l,...,b 
M = l,2,... a = max(0, n - M) 
n= 1,...,N + M b = mm(n,N) 
*>->{%"-.) -r.T 
, , _ N _ nNM N + M-n 
^{Xl~nN + M V[XI~ (N + M^N + M-l 
A. 1 Probability Distributions 
429 
Univariate Discrete Distributions (continued) 
Nb(a; | 9, r) Negative-Binomial (p. 116) 
O<0<l,r = l,2,... 
^)=c(rt!71)(1~e)I 
E[x] = r 
1-9 
9 
a; = 0,1,2,... 
c = 9r 
1-9 
V[x] = r: 
02 
Nbb(a; | a,/3, r) Negative-Binomial-Beta (p. 118) 
a >0, /3>0, r = l,2... 
V + a;-l\ r(/3 + a;) 
p(x) = c 
a; = 0,1,2,... 
_ T(a + 0)T(a + r) 
r- 1 / r(a + /3 + r + x) 
r/3 
E[x] = r-£ V[x\-. 
a (a — lj 
a + /3 + r- 1 
+ 
r(a)r(/3) 
r/3 
(a-2) (a-l)(a-2) 
Pn(a;|A) Poisson (p. 116) 
A>0 
Ax 
p(a:) = c — 
£[a;] = A 
a; = 0,1,2,. 
c — e 
-A 
V[x] = A 
Pg(a; | a, j3, n) Poisson-Gamma (p. 119) 
a >0, /3>0, n = 1,2,... 
Y(a + a;) nx 
p(x) = c- 
x\ (/3 + n)a+x 
E[x]=n^ 
a; = 0,1,2,. 
pa 
c"r(a) 
T/r i na 
,1 + 5. 
430 
A, Summary of Basic Formula* 
Univariate Continuous Distributions 
Be(x\a,8) Beta (p. 116) 
a>0,/3>0 0 < x < 1 
= r ^-ifi _ry?-i 
p(x) = c xa (1 — x) 
r(g + /?) 
Y{a)Y{8) 
„r t a. Trr , aB 
E[x] = - V[x]~- H 
a + B l J (a + 0)2(a + B + l) 
Un(x | a, b) Uniform (p. 117) 
b> a a < x <b 
p(x) = c c = (b — a)~l 
E[x] = ±(a + b) V[x] = ±{b-aY 
Ga(x | a, 8) Gamma (p. 118) 
a > 0, /? > 0 x > 0 
8a 
p(x) = c xa~le~l)x r 
T(a) 
E[x] = aB'1 V[x] = aB~2 
Ex(x | 0) Exponential (p. 118) 
e>o x>o 
p{x) = ce~ex c = 0 
E[x] = 1/0 V[x] = I/O2 
Gg(x | a, 8, n) Gamma-Gamma (p. 120) 
a>0,B>0,n>0 x>0 
xn~l _ 8a r(a + n) 
p(x) = c 
(/? + x)a+n r(a) T(n) 
W "a-l K[XJ~ (a-l)2(a-2) 
A. 1 Probability Distributions 431 
Univariate Continuous Distributions (continued) 
X2(x \v) = xt Chi-squared (p. 120) 
v>0 x>0 
v(x) = cx("M-le-x/2 c = (ll2Y12 
P{x) ex r(^/2) 
E[x] = v V[x] = 2v 
X2{x | v, A) Non-central Chi-squared (p. 121) 
v>0, A > 0 x>0 
00 ( A\ 
p(x) = J2Pn[i ■2-)x2{x\v + 
2i) 
E[x] = u + X V[x] = 2(i/ + 2A) 
Ig(x | a, /?) Inverted-Gamma (p. 119) 
a > 0, /? > 0 x>0 
p(x) = cx-{a+^e-0lx c = J£-r 
E[x] = -?- V[x] 02 
a-I l J (a-l)2(a-2) 
X~x{%\v) Inverted-Chi-squared (p. 119) 
v>0 x>0 
P(x) = cx-W+%-^2 C=T^ 
E[x] = -^r V[x] 2 
v-2 l J (i/- 2)2(i/- 4) 
GaTl/2(x\a,f3) Square-root Inverted-Gamma (p. 119) 
a > 0, /? > 0 x>0 
p(x) = c x-(2a+i)e-/3/*2 c = _2£L 
432 
A. Summary of Basic Formulae 
Univariate Continuous Distributions (continued) 
Pa(x\a,p) Pareto (p. 120) 
a > 0, 0 > 0 
p(x) = c ar^+D 
E[x] = pa(a -1)-1 
0<x< +oo 
c = a0a 
V[x] = 02a(a - l)-2(a - 2)"1 
Ip(x | a, 0) Inverted-Pareto (p. 120) 
a > 0, 0 > 0 
p(x) = ci0"1 
£[s] = 0-xa(a +1)"1 
0 < x < p-1 
c = a0a 
V[x] = 0-2a(a + l)-2(a + 2)"1 
N(s | M, A) Normal (p. 121) 
—oo < m < +oo, A > 0 
p(x) = c exp {-^(a; - p)2} 
E[x] = p 
—oo < a; < +oo 
c = A1/2(2tt)-1/2 
V[x] = A"1 
St(x | p, A, a) Student t (p. 122) 
-oo < p < +00, A > 0, a > 0 
p(a:) = C[l + a-1A(x-M)T(a+1)/2 
E[x] = p 
—oo < x < +oo 
c_r(i(a + l))/Ay/2 
T(ia) \ax) 
V[x] = A-Xa(a - 2)"1 
F(s | a, )9) = Fa>/3 Snedecor F (p. 123) 
a > 0, 0 > 0 
j.a/2-l 
P(x) = C (/3 + ax)(a+W2 
25[s] = 
0 
0-2 
x>0 
r(l(a + /3))aa/2^/2 
r(ia)r(i/3) 
2/32(a + /3-2) 
c = 
V[c] = 
a(/3-2)2(/3-4) 
A. 1 Probability Distributions 433 
Univariate Continuous Distributions (continued) 
Lo(x | a, 0) Logistic (p. 122) 
—oo < a < +oo, 0 > 0 —oo < x < +oo 
p(x) = /T1 exp {-0~x{x - a)} [l + exp {-0~\x - a)}]"2 
E[x] = a V[x] = /32tt2/3 
Multivariate Discrete Distributions 
Muk(x\0,n) Multinomial (p. 133) 
0 = (0u...,0k) x = (xu--.,xk) 
O<0t<l, Etiet<l Ht=1xi<n 
n = 1,2,... xt = 0,1,2,... 
, fc+i fc fc 
llf=i x<- f=i f=i ^=1 
E[s4] = ndj V[xi] = n0i{l - 0t) C[xuXj] = -n0i0i 
Mdjt(a; | 0, n) Multinomial-Dirichlet (p. 135) 
a. = (ai,-..,ak+i) x = (xi,.. . ,xk) 
ai>0 Xi = 0,1,2,... 
n = 1,2,... £"=i z< < n 
t n\ l n! 
pw=cHtt 
X/l 
<*lsl = n;=i («+^ -1) *k+i=n - Eli ^ 
#N = ^ V[Xi] = ,±£1°* nft(1 - ih) 
1 + 2^=i a< 
r>\ i n + 2^=1 a* 
2^=i "/ 1 + E/=i ae 
434 
A. Summary of Basic Formulae 
Multivariate Continuous Distributions 
Dik(x | a) Dirichlet (p. 134) 
a = (oi,...,afc+i) x = (xu...,xk) 
a*>0 0<xi<l, £j=1s<<l 
p(.)=c(i-x:.o nv c=ng^ 
*=i a* 1 + 2^=i a< 1 + 2^=i at 
Ng(x, y | £t, A, a, /?) Normal-Gamma (p. 136) 
/x e 3?, A > 0, a > 0, /? > 0, xeK, y > 0 
p(x, y) = N(x | fi, Ay) Ga(y | a, 0) 
E[x] = ft E[y] = a0-1 V[x] = 0\-l(a - l)"1 V[y] = a0~2 
p(x) = St(x|//,a/T1A,2a) 
Njt(a; | (x, A) Multivariate Normal (p. 136) 
H = (/ii,...,/ifc) eft* « = (xu...,xk) e K* 
A symmetric positive-definite 
p(x) = c exp {-i(x - /*)'A(aj - /*)} c = ] A | 1/2(2tt)-':/2 
E[x] = H V[x] = A1 
Pa2(x, y | a, fio, A) Bilateral Pareto (p. 141) 
(/?o, A) e K2, #>< ft, a > 0 (x,y) e K2, x < /30, y > A 
p(s, y) = c(y- z)-(«+2) c = a(a + 1)(A - /?o)a 
4.7 Probability Distributions 
Multivariate Continuous Distributions (continued) 
435 
Ngk(x, y\fi,\, a, 0) Multivariate Normal-Gamma (p. 140) 
—oo < Hi < +oo, a > 0, 0 > 0 
A symmetric positive-definite 
(x,y) = (xi,...,xk,y) 
—oo < Xi < oo, y > 0 
p(x, y) = Nk(x | /i, Ay) Ga(y | a, 0) 
E[x, y] = (/*, a/3"1), V[aj] = (a - l)"1^"1, F[y] = a^"2 
p(x) = Stk{x | /i, Aa/T1,2a) p(y) = Ga(y | a, 0) 
Nwk(x, y\(x,\, a, 0) Multivariate Normal-Wishart (p. 140) 
—oo < Hi < +oo, A > 0 
2a > k - 1 
/3 symmetric non-singular 
p(as, i/) = Njfc(a; | /i, Ay) W\k(y | a, /3) 
E[a;,2/] = {/i,a/3-1} 
p(x) = Stjt(a; | /i, Aa/3"1,2a) 
a; = (xi,...,Xjt) 
-00 < X, < +00 
y symmetric positive-definite 
V[x] = (a - l)-1^"1 
p(y) = Wifc(y|a,/3) 
Stjt(a; | /i, A, a) Multivariate Student (p. 139) 
—oo < Hi < +oo, a > 0 
A symmetric positive-definite 
a; = (xi,...,£jt) 
— OO < Xi < +00 
l + -(x- tf\(x - J"^"^ = ^^SlAI1/2 
av ^; v ^;J r(ia)(a7r)*:/2 ' 
p(x) = c 
E[x] = n, V[x] = A"1 (a - 2)-1a 
Wi(fc(a; | a, 0) Wishart (p. 138) 
2a > k - 1 
0 symmetric non-singular 
p(x) = c|aj|a-(*+1)/2exp{-tr(J8x)} 
E[x] = a/3"1, ^[ar1] = (a - *±l)-\8 
a; symmetric positive-definite 
C"nt1r(|(2a+l-£)) 
436 
A. Summary of Basic Formulae 
A.2 INFERENTIAL PROCESSES 
The second section of this Appendix records the basic elements of the Bayesian 
learning processes for many commonly used statistical models. 
For each of these models, we provide, in separate sections of the table, the 
following: the sufficient statistic and its sampling distribution; the conjugate  
family, the conjugate prior predictives for a single observable and for the sufficient 
statistic, the conjugate posterior and the conjugate posterior predictive for a single 
observable. 
When clearly defined, we also provide, in a final section, the reference prior 
and the corresponding reference posterior and posterior predictive for a single 
observable. In the case of uniparameter models this can always be done. We recall, 
however, from Section 5.2.4 that, in multiparameter problems, the reference prior 
is only defined relative to an ordered parametrisation. In the univariate normal 
model (Example 5.17), the reference prior for (//, A) happens to be the same as that 
for (A, n), namely 7r(/Lt, A) — n(X, //) oc A"1, and we provide the corresponding 
reference posteriors for n and A, together with the reference predictive distribution 
for a future observation. 
In the multinomial, multivariate normal and linear regression models,  
however, there are very many different reference priors, corresponding to different 
inference problems, and specified by different ordered parametrisations. These are 
not reproduced in this Appendix. 
Bernoulli model 
z = {xu...,xn}, XiE {0,1} 
p(xi | 9) = Br(xi 10), 0 < 9 < 1 
p(r 19) = Bi(r 19, n) 
p(9)=Be(9\a,p) 
p{x) = Bb(r\a,/3,l) 
p(r) = Bb(r | a, /?, n) 
p(9 | z) = Be(01 a + r, p + n - r) 
p(x | z) = Bb(x | a + r, f3 + n - r, 1) 
7r(0) = Be(0|i I) 
7r(6> I z) = Be(6> \\ + r,\ + n-r) 
■k(x I z) = Bb(z | \ + r, \ + n - r, 1) 
A.2 Inferential processes 437 
Poisson Model 
z = {xi,...,xn}, Xi = 0,1,2,... 
p(Xi | A) = Pn(xi | A), A > 0 
t(z)=r = ZU*i 
p(r | A) = Pn(r | nX) 
p(A)=Ga(A|a,/3) 
p(x) = Pg(x|a,/?,l) 
p(r) = Pg(x | a, /?, n) 
p(A|z) = Ga(A|a+ r,/? + n) 
p(x | z) = Pg(x | a + r, 0 + n, 1) 
tt(A) a A~V2 
7r(A|z) = Ga(A|r + |,n) 
7r(x|z) = Pg(x|r + i,n, 1) 
Negative-Binomial model 
z = 
P(Xi 
t(z) 
p(s\ 
(xi, 
\0)- 
= s 
e) = 
• • • > xn), 
= Nb(xj | 
■■ Nb(s 10 
X\ 
e,r), 
,nr) 
= 0,1,2,... 
0<6>< 1 
p(6)=Be(0\a,l3) 
p(x) = Nbb(x | a, 0, r) 
p{s) = Nbb(s | a, 0, nr) 
p{91 z) = Be(0 | a + nr, /? + s) 
p(x | z) = Nbb(x\a + nr,0 + s,nr) 
Tr^ocfl-^l-0)^/2 
7r(6>|z) = Be(6>|nr,s + i) 
7r(x | z) = Nbb(x | nr, s + \, nr) 
438 A. Summary of Basic Formulae 
Exponential Model 
Z = {xi,...,X„}, 0 < Xi < 00 
p(xt I 9) = Ex{xt \9), 9>0 
p{t 19) = Ga(* | n, 9) 
p(9)=Ga(9\a,0) 
p(x) = Gg(x|a,/?,l) 
p(t) = Gg(t|a,/?,n) 
p(6> | z) = Ga(6> | a + n, 0 + t) 
p(x | z) = Gg(x \a + n,0 + t,l) 
n(9) oc 9~l 
tt(0 I z) = Ga(6> | n, t) 
7r(x|z) = Gg(x|n, t, 1) 
Uniform Model 
z = {x1,...,xn}, 0<Xi<9 
p(sj|0)=Un(a:j|O,0), 0 >0 
£(z) = £ = max{xi,..., x„} 
p(t\9) = lp(t\n,9-1) 
p(0) = Pa(0|a,/3) 
P(*) = ^lUn(x | 0,0), if x < /?, ^Pa(x | a, /?), if x > 0 
P(t) = ^p(t\n,0-1), ift<0, ^Pa(t\a,0), ift>0 
p(9\z) = Pa(9\a + n,0n), 0n = max{0,t} 
P(* I *) = ^?rUn(x | 0,0n), if x < /?„, ^rPaCx | a, 0n), iix>0n 
n(9) oc 9~l 
n{9\z) = Pa(9\n,t) 
■k(xI z) = ^-Ur^x 10,t), ifx<t, AjPa{x \n,t), if x > t 
A.2 Inferential processes 439 
Normal Model (known precision X) 
Z = {Xi,. . .,£„}, -00 < Xi < 00 
p(xt I n, A) = N(x, | n, A), -oo < /x < oo 
t(z) = J = n"1 £"=1 Xi 
p(x | /Li, A) = N(x | n, nX) 
p(n) = N(/x | /xo, A0) 
pCxHN^I/icAAoCAo + A)-1) 
p(x) = N (x | no, nX AoA"1), An = A0 + nX, 
p(fj, | z) = N(n | //„, A„), //„ = X~l(X0ii0 + nXx) 
p(x \z) = N(x\fin,X Xn(Xn + A)-1) 
7r(/x) = constant 
7r(/x|z) = N(/x|S,nA) 
■n(x\z) = N(x|S,An(n + l)-1) 
Normal Model (known mean \i) 
Z = {Xl,. ..,£„}, -OO < Xj < 00 
p(xj|/x,A) =N(xj|/x,A), A>0 
*(*)=* = £?=i(s«-/*)2 
p(i | ii, A) = Ga(t | in, §A), p(At) = X2(A* | n) 
p(A)=Ga(A|a,/?) 
p(x) = St(x|//,a/T1,2a) 
p(t)=Gg(s|a,2/?,±n) 
p(A|z) = Ga(A|a+±n,/?+±*) 
p(s | z) = St(x | /*, (a + |n)(/3 + it)"1,2a + n) 
tt(A) a A"1 
tt(A I z) = Ga(A | \n, \t) 
7r(x | z) = St(x I h, nt~l, n) 
440 A. Summary of Basic Formulae 
Normal Model (both parameters unknown) 
z = {x\,... ,xn}, —oo<Xi<oo 
p(xi | p, A) = N(a;j | p, A), — oo < p. < oo, A > 0 
<(z) = (x, s), nx = J2"=i xt, ns2 = Y!i=\{xi - xf 
p(x | /u, A) = N(a! | p, nX) 
p(ns2 | p, A) = Ga(ns2 | ±(n - 1), ±A), p(Ans2) = x2(Ans2 | n - 1) 
p(/t, A) = Ng(/x, A | /i0, n0, a, /3) = N(/x | /t0, n0A) Ga(A | a, (3) 
p(p) = St(/x | /i0, noa/?"1,2a) 
p(A) = Ga(A|a,/3) 
p(z) = St(x|/io,n0(n0 + l)"1a/3~1,2a) 
p(x) = St{x\ pjo,nan(no + n)~1a/9~1,2a) 
p(ns2) = Gg(ns2 | a, 2/3, ±(n - 1)) 
p(p | z) = St(/x | /xn, (n + n0)(a + \n)f3~l,2a + n), 
Hn = {no + ri)~1(n0Ho + nx), 
Pn = P + \ns2 + i(no + n)_1non(/i0 -x)2 
p{\\z) = Ga(A|a+±n,/?n) 
p(x | z) = St(x \pn,(n + no)(n + n0 + l)-1(a + ^n)^"1,2a + n) 
7r(/x, A) = 7r(A, /x) oc A-1, n > 1 
7r(/x | z) = St(/x | z, (n - l)s~2, n - l) 
7r(A|z) = Ga(A|i(n-l),ins2) 
n{x\z) = St(x\x,(n- l)(n + l)_1s~2,n- l) 
A.2 Inferential processes 441 
Multinomial Model 
z = {n,...,rk,n), n = 0,1,2,..., E*=i r* ^ n 
p(z | 0) = Mu*(z | 0, n), 0 < ft < 1, £*=1 ft < 1 
t(z) = (r,n), r = {ru...,rk) 
p(r\9) = M\ik{r\9,n) 
p(0) = Di*(0|a), a = {ai,...,at+i} 
p(r) = Mdfc(r |a,n) 
p(0 | z) = Difc ^0 | ax + n,..., ak + rk, ak+x + n - E*=i rtj 
p(x\z) = Mdk (x\ati + n,...,ak + rk,ak+i + n-ELir*>n 
Multivariate Normal Model 
Z — \X\,. . . , «Eti j, X{ kl Ji 
p(xi |/x, A) = Nfc(xi |/i, A), /x G 9?*, A A; x A; positive-definite 
t(z) = (x,S), x = n"1n=i^. S = n=i(^-*)(^-^ 
p(x | /i, A) = Nfc(x | /i, nX) 
p(S|A) = Wi,(S|i(n-l),iA) 
p(/i, A) = Nw*(/i, A | /i0, n0, a,/3) = N*(/i | /x0, n0A) Wifc(A | a, /3) 
p(x) = St* (x | /i0, (n0 + l)"1n0(a -\{k- lJJ/T1,2a - fe + l) 
p(/i | *) = St* (/i | /in, (n + n0)anf3-\ 2an), 
Mn = (no + n)-1(no/i0 + nx), 
f3n = (3 + ±S + ^(n + no^^noC/io-x)^-®)' 
p(A|z)=Wi*(A|a + ±n,/3n) 
p(x | z) = Stft(x | /i„, (n0 + n + l)"^ + n)an0-l,2an), 
an = a + \n-\{k-l) 
442 
A. Summary of Basic Formulae 
Linear Regression 
z = (y,X), y = {yu---,y„Y G 3?", xt = (xa,... ,xik) £?Rk, X = (a;0-) 
p(y\X,e,X) = Nn(y\X9,XIn), OeKk, A>0 
t(z) = (XtX,Xty) 
p{0, A) = Ng(0, X\0o,no,a,(3) = Nk(0\ 0o,n0A)Ga(A | a,(3) 
P{9) = Stk{e\90,n0al3-\2a) 
p(A) = Ga(A|a,/J) 
p(j/1 x) = St (i/1 x0o, /(xJa/T1,2a) 
/(x) = 1 - x(x'x + no)-1^', 
p(0 | z) = Stfc {01 0n, (n0 + X<X)(a + \n)fa\ 2a + n), 
0n = (n0 + X'XJ-^noOo + X'l/), 
A. = P + \{y - xenyy + \{oQ - ff„)«noff0 
p(A|z) = Ga(A|a+±n,/3n) 
p{y \x,z) = St{y | x0n, f„(x)(a + ^n)/?"1,2a + n), 
/„(x) = 1 + x(xlx + n0 + X'X)-1^' 
443 
Appendix B 
Non-Bayesian 
Theories 
Summary 
A summary is given of a number of non-Bayesian statistical approaches and 
procedures. The main theories reviewed include classical decision theory, fre- 
quentism, likelihood, and fiducial inference. These are illustrated and compared 
and contrasted with Bayesian methods in the stylised contexts of point and  
interval estimation, and hypothesis and significance testing. Further issues  
discussed include: conditional and unconditional inferences; nuisance parameters 
and marginalisation; prediction; asymptotics and criteria for model choice. 
B.1 OVERVIEW 
Bayesian statistical theory as presented in this book is self-contained and can be 
understood and applied without reference to alternative statistical theories. There 
are, however, two broad reasons why we think it appropriate to give a summary 
overview of our attitude to other theories. 
First, many, if not most, readers will have some previous exposure to  
"classical" statistics, and the material in this Appendix may help them to put the contents 
of this book into perspective. 
444 
B. Non-Bayesian Theories 
Secondly, our own experience has been that some element of comparative  
analysis contributes significantly to an appreciation of the attractions of the Bayesian 
paradigm in statistics. 
As a preliminary, we recall from Chapter 1 our acknowledgment that Bayesian 
analysis takes place in a rather/orma/ framework, and that exploratory data analysis 
and graphical displays are often prerequisite, informal activities. It is important, 
therefore, to be clear that in this Appendix we are discussing non-Bayesian formal 
procedures. 
We begin by making explicit some of the key differences between Bayesian 
and non-Bayesian theories: 
(i) As we showed in detail in Chapter 2, Bayesian statistics has an axiomatic 
foundation which guarantees quantitative coherence. Non-Bayesian statistical 
theories typically lack foundational support of this kind and essentially consist 
of a set of recipes which are not necessarily internally consistent. 
(ii) Non-Bayesian theories typically use only a parametric model family of the 
form {p(x | 0), x G X, 6 G 6}, ignoring the prior distribution p(0). The 
implications of this fact are so far reaching that sometimes Bayesian statistics 
is simplistically thought of as statistics with the "optional extra" of a prior 
distribution. In Chapters 2 and 3, we have argued that the existence of a prior 
distribution is a mathematical consequence of the foundational axioms. In 
Chapter 4, we stressed that predictive models, typically derived from  
combining p(x 10) and p(0), are primary. 
(iii) The decision theoretical foundations of Bayesian statistics provide a natural 
framework within which specific problems can easily be structured, with  
solutions directly tailored to problems. In contrast, most non-Bayesian theories 
essentially consist of stylised procedures, such as those for point or interval 
estimation, or hypothesis testing, designed to satisfy or optimise an ad hoc 
criterion, and often lacking the necessary flexibility to be adaptable to specific 
problem situations. 
(iv) We have argued that, from a Bayesian viewpoint, a decision structure is the 
natural framework for any formal statistical problem, and have described how 
a "pure" inference problem may be seen as a particular decision problem. 
Non-Bayesian theories depart radically from this viewpoint; classical  
decision theory is only partially relevant to inference, and non-Bayesian inference 
theories typically ignore the decision aspects of inference problems. 
In Section B.2, we will revise the key ideas of a number of non-Bayesian 
statistical theories, specifically reviewing Classical Decision Theory, Frequentist 
Procedures, Likelihood Inference and Fiducial and Related Theories. 
B.2 Alternative Approaches 
445 
In Section B.3, we will follow the typical methodological partition of non- 
Bayesian textbooks into the topics of Point Estimation, Interval Estimation,  
Hypothesis Testing and Significance Testing. Within each of those subheadings we 
will comment on the internal logic, the relevance to actual statistical problems, and 
the performance of classical procedures relative to their Bayesian counterparts. 
In Section B.4, we will discuss in detail some key comparative issues:  
Conditional and Unconditional Inference, Nuisance Parameters and Marginalisation, 
Approaches to Prediction, Aspects of Asymptotics and Model Choice Criteria. 
For readers seeking further comparative discussion at textbook level, we recall, 
from our discussion at the end of Chapter 5, the books by Barnett (1971/1982), Press 
(1972/1982), Cox and Hinkley (1974), Anderson (1984), DeGroot (1987), Casella 
and Berger (1990) and Poirier (1993). 
B.2 ALTERNATIVE APPROACHES 
B.2.1 Classical Decision Theory 
We recall from Section 3.3 the basic structure of a general decision problem,  
consisting of a set of a possible decisions D, a parameter space fl, a prior distribution 
p(cj) over fl, and a utility function u(d(u))) which we shall denote by u(d, cj) to 
conform more closely to standard notation in classical decision theory. We  
established that the existence of both the prior distribution p(u>) and the utility function 
u(d, cj) is a mathematical consequence of the axioms of quantitative coherence and 
that the best decision d* is that which maximises the expected utility 
u(d) = / u(d,u)p(u})dw. 
We established furthermore that, if additional information x is obtained which 
is probabilistically related to u by p(x \ u), then the best decision d*x is that which 
maximises the posterior expected utility 
u(d\x) = u(d,Lj)p(cj\x)dw, 
where 
p(u> | x) oc p(x | u>)p(u>). 
Some authors prefer to use loss functions instead of utilities. A regret function, 
or decision loss, is easily defined from the utility function (at least in bounded cases) 
by 
l(d,a;) = sup u{di,u)) — u(d,cj), 
AfiD 
446 
B. Non-Bayesian Theories 
which quantifies the maximum loss that, for each ui, one may suffer as a  
consequence of a wrong decision. Since supD u(d, a;) only depends on a;, the expected 
loss 
~l{d) = fl{d,u))p{cj)du; 
is minimised by the same decision d* which maximises u(d) and hence, from a 
Bayesian point of view, the two formulations are essentially equivalent. 
In contrast to this Bayesian formulation, the core framework of classical  
decision theory may be loosely described as decision theory without a prior distribution. 
A utility function (or a loss function) is accepted, perhaps justified by utility-only 
axiomatics of the type pioneered by von Neumann and Morgenstern (1944/1953), 
but a prior distribution for cj is not. 
Although some of the basic ideas in classical statistical theory were present 
in the work of Neyman and Pearson (1933), it was Wald (1950) who introduced 
a systematic decision theory framework, excluding prior distributions as core  
elements, but including a formulation of standard statistical problems within a decision 
framework. This work was continued by Girshick and Savage (1951) and by Stein 
(1956). An excellent textbook introduction is that of Ferguson (1967). 
Classical decision theory focuses on the way in which additional information 
x should be used to assist the decision process. Consequently, the basic space 
is not the class of decisions, D, but the class of decision rules, A, consisting of 
functions 6 : X —> D which attach a decision 6(x) to each possible data set x. It 
is then suggested that decision rules should be evaluated in terms of their average 
loss with respect to the data which might arise. Thus, the risk function r(8, u) of a 
decision rule 6 is defined as 
r(6,cj) — / l(8(x),u)p(x\<jj)dx 
and subsequent comparison of decision rules is based on their risk functions. 
The formulation includes, as a special case, the situation with no additional 
data (the no-data case), where the risk function reduces to the loss function. 
Example B.l. (Estimation of the mean of a normal distribution). 
Let x = {xi,..., x„} be a random sample from a N(x | //, 1) distribution, and suppose that 
we want to select an estimator for //, so that D = 3?, under the assumption of a quadratic 
loss function l(d, //) = (// — df. Some possible decision rules are 
(i) 6\{x) = x, the sample mean 
(ii) 62(x) = x, the sample median 
(iii) 63(x) = no, a fixed value 
(iv) <54(a;) = (n + no)_1(no/A) + nx), the posterior mean from an N(fi \ //0, "o) prior, 
centred on /zo and with precision no- 
B.2 Alternative Approaches 
447 
Using the fact that the variance of the sample median is approximately 7r/2n, the 
corresponding risk functions are easily seen to be 
(i) r(£i,/i) = l/n 
(ii) r(62, n) ^ 7r/2n, 
(iii) r(S3,fj) = {n- IA>? 
(iv) r(6i,n) = (n + no)-2{n + nl(fio-fJ-)2} 
r(63, n) 
Figure B.l Risk functions for /io = 0, n = 10 and no = 5 
Note that (iv) includes both (i) and (iii) as limiting cases when no —> 0 and no —> oo 
respectively. Figure B.l provides a graphical comparison of the risk functions. 
It is easily seen that, whatever the value of //, 62 has larger risk than Si but, otherwise, 
the best decision rule markedly depends on fi. The closer /zo is to the true value of fi, the 
more attractive <53 and <54 will obviously be. 
Admissibility 
The decision rule 62 in Example B.l can hardly be considered a good decision rule 
since, for any value of the unknown parameter /x, the rule #i has a smaller risk. 
This is formalised within classical decision theory by saying that a decision rule 6' 
is dominated by another decision rule 6 if, for all u, 
r{6',u) >r(6,ei) 
448 
B. Non-Bayesian Theories 
with strict inequality for some oj, and that a decision rule is admissible if there is 
no other decision rule which dominates it. A class of decision rules is complete if 
for any 6' not in the class there is a 6 in the class which dominates it, and a class 
is minimal complete if it does not contain a complete subclass. If one is to choose 
among decision rules in terms of their risk functions, classical decision theory 
establishes that one can limit attention to a minimal complete class. However, for 
guidance on how to choose among admissible decision rules, further concepts and 
criteria are required. 
Bayes Rules 
If the existence of a prior distribution p{oj) for the unknown parameter is accepted, 
classical decision theory focuses on the decision rule which minimises expected 
risk (or so-called Bayes risk) 
mm 
6 
i / r(6,u))p(oj)du} = min / / l(6(x),cj)p(x \cj)p(lj) dxdio, 
Jn s Jn Jx 
which it calls a Bayes decision rule. Note that, since 
p(x | u)p{u) = p(u | x)p{x), 
under appropriate regularity conditions we may reverse the order of integration 
above to obtain 
mm 
6 
in/ r(6(x),ui)p(u\x)p(x)dxdw 
5 JnJx 
= / p(x)min / l(d,Lj)p(u\x)dwdx, 
Jx 6 Jq 
so that the Bayes rule may be simply described as the decision rule which maps 
each data set x to the decision 6* (x) which minimises the corresponding posterior 
expected loss. Note that this interpretation does not require the evaluation of any 
risk function. 
It is easily shown that any Bayes rule which corresponds to a proper prior 
distribution is admissible. Indeed, if 6* is the Bayes decision rule which corresponds 
to p(u) and 6' were another decision rule such that r(6', u) < r(6*, a>) with strict 
inequality on some subset of tt with positive probability under p(oj), then one 
would have 
/ r(S',u)p((jj)dw < / r(6*,Lj)p(cj)dw, 
which would contradict the definition of a Bayes rule as one which minimises 
the expected risk. Wald (1950) proved the important converse result that, under 
rather general conditions, any admissible decision rule is a Bayes decision rule 
with respect to some, possibly improper, prior distribution. 
B.2 Alternative Approaches 
449 
There is, however, no guarantee that improper priors lead to admissible decision 
rules. A famous example is the inadmissibility of the sample mean of multivariate 
normal data as an estimator of the population mean, even though it is the Bayes 
estimator which corresponds to a uniform prior. For details, see James and Stein 
(1961). 
Minimax rules 
The combined facts that admissible rules must be Bayes, and that to derive the Bayes 
rule does not require computation of the risk function but simply the minimisation 
of the posterior expected loss, make it clear that, apart from purely mathematical 
interest, it is rather pointless to work in decision theory outside the Bayesian  
framework. Indeed, this has been the mainstream view since the early 1960's, with the 
authoritative monographs by DeGroot (1970) and Berger (1985a) becoming the 
most widely used decision theory texts. 
Nevertheless, some textbooks continue to propose as a criterion for choosing 
among decisions (without using a prior distribution) the rather unappealing minimax 
principle. This asserts that one should choose that decision (or decision rule) for 
which the maximum possible loss (or risk) is minimal. It can be shown, under rather 
general conditions, and certainly in the finite spaces of real world applications, 
that the minimax rule is the Bayes decision rule which corresponds to the least 
favourable prior distribution, i.e., that which gives the highest expected risk. 
The intuitive basis of the minimax principle is that one should guard against the 
largest possible loss. While this may have some value in the context of game theory, 
where a player may expect the opponent to try to put him or her in the worst possible 
situation, it has no obvious intuitive merit in standard decision problems. The idea 
that the minimax rule should be preferred to a rule which has better properties 
for nearly all plausible u> values, but has a slightly higher maximum risk for an 
extremely unlikely u> value seems absurd. Moreover, even as a formal decision 
criterion, minimax has very unattractive features; for instance, it gives different 
answers if applied to losses rather than to regret functions, and it can violate the 
transitivity of preferences (see e.g., Lindley, 1972). 
Thus, although in specific instances—namely when prior beliefs happen to 
be close to the least favourable distribution—the minimax solution may be  
reasonable (essentially coinciding with the Bayes solution), the minimax criterion seems 
entirely unreasonable. 
B.2.2 Frequentist Procedures 
We recall from Section 5.1 the basic structure of a stylised inference problem, where 
inferences about 0 G O are to be drawn from data x, probabilistically related to 0 
by the parametric model component {p(x \ 0), 0 G 6}. 
450 
B. Non-Bayesian Theories 
We established that the existence of a prior distribution p(9) is a mathematical 
consequence of the axioms of quantitative coherence, and that the required  
inferential statement about 9 given x is simply provided by the full posterior distribution 
p{9\x) = p{x\9)p{9)/p{x), 
where 
p(x)= Ip(x\9)p(9)d9. 
Frequentist statistical procedures are mainly distinguished by two related  
features; (i) they regard the information provided by the data x as the sole quantifiable 
form of relevant probabilistic information and (ii) they use, as a basis for both 
the construction and the assessment of statistical procedures, long-run frequency 
behaviour under hypothetical repetition of similar circumstances. 
Although some of the ideas probably date back to the early 1800's, most 
of the basic concepts were brought together in the 1930's from two somewhat 
different perspectives, the work of Neyman and Pearson, being critically opposed 
by Fisher, as reflected in discussions at the time published in the Royal Statistical 
Society journals. Convenient references are Neyman and Pearson (1967) and Fisher 
(1990). See, also, Wald (1947) for specific methods for sequential problems. 
Frequentist procedures make extensive use of the likelihood function 
lik(01 x) = p{x 19) 
(or variants thereof), essentially taking the mathematical form of the sampling 
distribution of the observed data x and considering it as a function of the unknown 
parameter 9. If z = z(x) is a one-to-one transformation of x, the likelihood in 
terms of the sampling distribution of z becomes (in the above variant) 
lik(01 z) = p{z 19) = p{x 19) 
dx 
dz 
lik(01 x) 
dx 
dz 
which suggests that meaningful likelihood comparisons should be made in the form 
of ratios rather than, say, differences, in order for such comparisons not to depend 
on the use of z rather than x. 
The basic ideas behind frequentist statistics consist of (i) selecting a function 
of the data t = t(x), called a statistic, which is related to the parameter 9 in a 
convenient way, (ii) deriving the sampling distribution of t, i.e., the conditional 
distribution p(t \ 9), and (iii) measuring the "plausibility" of each possible 9 by 
calibrating the observed value of the statistic t against its expected long-run  
behaviour given 9, described by p(t | 9). For a specific parameter value 9 = 90, if the 
observed value of t is well within the area where most of the probability density of 
B.2 Alternative Approaches 
451 
p(t 190) lies, then 0O is claimed to be compatible with the data; otherwise it is said 
that either 90 is not the true value of 9, or a rare event has happened. 
Such an approach is clearly far removed from the (to a Bayesian rather  
intuitively obvious) view that relevant inferences about 9 should be probability  
statements about 9 given the observed data, rather than probability statements about 
hypothetical repetitions of the data conditional on (the unknown) 9. This contrast 
is highlighted by the following example taken from Jaynes (1976). 
Example B.2. (Cauchy observations). Letx= {xi,x2} consist of two independent 
observations from a Cauchydistributionp(x 10) = St(x 10,1,1). Common sense (supported 
by translational and permutational symmetry arguments) suggests that 0 = (x\ + £2) /2 may 
be a sensible estimate of 0. Yet, the sampling distribution of 0 is again St(x 10,1,1) so that, 
according to a naive frequentist, it cannot make any difference whether one uses x\, x2 or 
0 to estimate 0. Clearly, there is more to inference than the choice of estimators and their 
assessment on the basis of sampling distributions. 
Sufficiency 
We recall from Section 4.5 that a statistic* is sufficient if p(x \t,9) = p(x \ t); i.e., if 
the conditional distribution of the data given t is independent of 9 (Proposition 4.11), 
and that a necessary and sufficient condition for t to be sufficient for 9 is that the 
likelihood function may be factorised as 
lik(0 | x) = p(x | 9) = h(t, 9)g{x), 
in which case, for any prior p(9), the posterior distribution of 9 only depends on 
x through t, i.e., p(9 \ x) = p{9 \ t). The concept of sufficiency in the presence 
of nuisance parameters is controversial; see, for example, Cano et al. (1988) and 
references therein. 
The sufficiency principle in classical statistics essentially states that, for any 
given model p(x \ 9) with sufficient statistic t, identical conclusions should be 
drawn from data X\ and x2 with the same value of t. The idea was introduced by 
Fisher (1922) and developed mathematically by Halmos and Savage (1949) and 
Bahadur (1954). 
From a Bayesian viewpoint there is obviously nothing new in this "principle"; 
it is a simple mathematical consequence of Bayes' theorem. 
However, from a "textbook" perspective, other frequentist developments of 
the sufficiency concept have little or no interest from a Bayesian perspective. For 
example: a sufficient statistic t is complete if for all 9 in ©, / h(t)p(t | 9)dt = 0 
implies that h(t) = 0. The property of completeness guarantees the uniqueness of 
certain frequentist statistical procedures based on t, but otherwise seems  
inconsequential. 
452 
B. Non-Bayesian Theories 
Ancillarity 
In Section 5.1 we demonstrated how a sufficient statistic t = t(x) may often 
be partitioned into two component statistics t(x) = [a(x),s(x)] such that the 
sampling distribution of a(x) is independent of 9. We defined such an a(x) to be 
an ancillary statistic and showed that, if a is ancillary, then 
p(9\x)=p(9\t)otp(s\a,9)p(9) 
so that, in the inferential process described by Bayes' theorem, it suffices to work 
conditionally on the value of the ancillary statistic. For further information, see 
Basu (1959). 
The conditionality principle in classical statistics states that, whenever there 
is an ancillary statistic a, the conclusions about the parameter should be drawn as 
if a were fixed at its observed value. The apparent need for such a principle in 
frequentist procedures is well illustrated in the following simple example. 
Example B.3. (Conditional versus unconditional arguments). A 0-1 signal comes 
from one of two sources 0\ or 62, and there are two receivers R\ and R2 such that 
p(x = O\Ri,0i) = p(x = l\Ri,e2) = 0.9 
p(x = 0\ R2,8i)=p(x = l\ R2, B2) = 0.2 
where the receiver is selected at random, with p{R\) = 0.99. If R2 were the receiver and 
x = 1 were obtained, the conditional likelihood would have been 
lik(0i \R2,x = l)= 0.8, lik(02 |R2,x = 1) = 0.2, 
suggesting 6\ as the true value of 6. On the other hand, the unconditional likelihood given 
x = 1 would have been 
lik(0i |x = 1) = 0.107, lik(02 \x = l) = 0.843, 
suggesting 02 instead. The conflict arises because the latter (unconditional) argument takes 
undue account of what might have happened (i.e., Ri might have been the receiver) but did 
not. 
A further example regarding ancillarity is provided by reconsidering  
Example B.2. The difficulty in this case disappears if one works conditionally on the 
ancillary statistic | x\ - xi \ /2. 
These examples serve to underline the obvious appeal of a trivial consequence 
of Bayes' theorem: namely, that one should always condition inferences on  
whatever information is available; the conditionality "principle" is just a small ad hoc 
B.2 Alternative Approaches 
453 
step towards this rather obvious desideratum (which is, in any case, "automatic" in 
the Bayesian approach). 
From a frequentist viewpoint, however, the conditionality "principle" is not 
necessarily easy to apply, since ancillary statistics are not readily identified, and are 
not necessarily unique. Moreover, applying the conditionality principle may leave 
the frequentist statistician in an impasse. For example, Basu (1964) noted that if x 
is uniform on [6, l+6[, then the fractional part of x is uniformly distributed on [0,1 [ 
and hence ancillary, but the conditional distribution of x given its fractional part is 
a useless one-point distribution! See Basu (1992) for further elegant demonstration 
of the difficulties with ancillary statistics in the frequentist approach. 
The Repeated Sampling Principle 
A weak version of the repeated sampling principle states that one should not follow 
statistical procedures which, for some possible value of the parameter, would too 
frequently give misleading conclusions in hypothetical repetitions. Although this 
is too vague a formulation on which to base a formal critique, it can be used to 
criticise specific solutions to concrete problems. 
A much stronger version of this "principle", whose essence is at the heart of 
frequentist statistics, states that statistical procedures have to be assessed by their 
performance in hypothetical repetitions under identical conditions. This implies 
that (i) measures of uncertainty have to be interpreted as long-run hypothetical 
frequencies, that (ii) optimality criteria have to be defined in terms of long-run  
behaviour under hypothetical repetitions and, that (iii) there are no means of assessing 
any finite-sample realised accuracy of the procedures. 
Example B.4. (Confidence versus HPD intervals). Let x = {xi,..., xn} be a  
random sample from N(z \n,l). It is easily seen that x is a sufficient statistic, whose sampling 
distribution is N(a; | /j,, n), a normal distribution centred at the true value of the parameter, 
with precision n. Since the sampling distribution of x concentrates around /j,, one might 
expect x to be close to fj, on the basis of a large number of hypothetical repetitions of the 
sample, so that x suggests itself as an estimator of fi. Moreover, conditional on fj,, 
P [x e m ± 1.96/s/n\ /j] = 0.95 
so that, if we define a statistical procedure to consist of producing the interval a; ± 1.96/y/n 
whenever a random sample of size n from N(x | fi, 1) is obtained, we are producing an 
interval which will include the true value of the parameter 95% of the time, in the long run. 
Note that this says nothing about the probability that fj, belongs to that interval for any given 
sample. In contrast, the superficially similar statement 
P[n€x± 1.96/ y/n | x] = 0.95 
which is derived from the reference posterior distribution of n given x, explicitly says that 
given x, the degree of belief is 0.95 that n belongs to x ± 1.96^, and is not concerned at 
all with hypothetical repetitions of the experiment. 
454 
B. Non-Bayesian Theories 
Invariance 
If a parametric model p(x \ 9) is such that two different data sets, X\ and Xi, 
have the same distribution for every 9, then both the likelihood principle and the 
mechanics of Bayes' theorem imply that one should derive the same conclusions 
about 9 from observing x\ as from observing X2. 
A more elaborate form of invariance principle involves transformations of 
both the sample and the parameter spaces. Suppose that, with X = ©, for all the 
elements of a group Q of transformations there is a unique transformation g such 
that #(0) = © andp(x | 0) = p(g(x) \ g(9)). Then the invariance principle would 
require the conclusions about 9 drawn from the statistic t(g(x)) to be the same 
as those drawn from g(t(x)). For example, in estimating 6 € 3? from a location 
model p{x \ 6) = h{x — 0) it may be natural to consider the group of translations. 
In this case, g(x) = x + a, a € 3?, and g{6) = 9 +a. The invariance principle then 
requires that any estimate t(x) of 6 should satisfy t(x + al) = t(x) + a, where 1 
is a vector of unities. 
Note that the argument only works if there is no reason to believe that some 9 
values are more likely than others. From a Bayesian point of view, for invariance to 
be a relevant notion it must be true that the transformation involved also applies to the 
prior distribution (otherwise, one may have a uniform loss of expected utility from 
following the invariance principle). Another limitation to the practical usefulness 
of invariance ideas is the condition that g(Q) =6. Thus, in the location/translation 
example, the invariance principle could not be applied if it were known that 6 > 0. 
A final general comment. Frequentist procedures centre their attention on 
producing inference statements about unobservable parameters. As we shall see in 
Section B.4, such an approach typically fails to produce a sensible solution to the 
more fundamental problem of predicting future observations. 
B.2.3 Likelihood Inference 
We recall from Section 5.1 the following trivial consequence of Bayes' theorem. 
Consider two experiments yielding, respectively, data x and z and with model  
representation involving the same parameter 9 € ©, the same prior, and proportional 
likelihoods, so that 
p(x\9) = h(x,z)p(z\9). 
Then the experiments produce the same conclusions about 9, since they induce 
the same posterior distribution. The likelihood principle suggests that this should 
indeed be the case, for the relative support given by the two sets of data to the 
possible values of 9 is precisely the same. 
Frequentist procedures typically violate the likelihood principle, since long 
run behaviour under hypothetical repetitions depends on the entire distribution 
{p(x | 9), x £ X} and not only on the likelihood. 
B.2 Alternative Approaches 
455 
As mentioned before, when common priors are used across models with  
proportional likelihoods, the Bayesian approach automatically obeys the likelihood 
principle and certainly accepts the likelihood function as a complete summary of 
the information provided by the data about the parameter of interest. With a uniform 
prior, the posterior distribution is, of course, proportional to the likelihood function. 
Proponents of the likelihood approach to inference go further, however, in their uses 
of the likelihood function, in that they regard it not only as the sole expression of 
the relevant information, but also as a meaningful relative numerical measure of 
support for different possible models, or for alternative parameter values within the 
same model. The basic ideas of this pure likelihood approach were established by 
Barnard(1949,1963),Barnardetal.  
(1962),Birnbaum(1962,1968,1972)andEdwards (1972/1992). They essentially argue that (i) the likelihood function conveys 
all the information provided by a set of data about the relative plausibility of any 
two different possible values of 9 and (ii) the ratio of the likelihood at two different 
9 values may be interpreted as a measure of the strength of evidence in favour of 
one value relative to the other. 
Both claims make sense from a Bayesian point of view when there are no 
nuisance parameters. Indeed, (i) is just a restatement of the likelihood principle 
and, moreover, it follows from Bayes' theorem that 
p(0i|z) ,_p(g|gi)p(gQ 
p{92\x) p(x\92)p(92)' 
so that the likelihood ratio satisfies (ii), since it is the factor which modifies prior 
odds into posterior odds. 
However, the pure likelihood approach, i.e., the attempt to produce inferences 
solely based on the likelihood function, breaks down immediately when there are 
nuisance parameters. The use of "marginal likelihoods" necessarily requires the 
elimination of nuisance parameters, but the suggested procedures for doing this 
seem hard to justify in terms of the likelihood approach. For early attempts, see 
Kalbfleish and Sprott (1970, 1973) and Andersen (1970, 1973). In recent years, 
work has focused on the properties of profile likelihood and its variants. Useful  
references include: Barnard and Sprott(1968),Barndorff-Nielsen(1980,1983,1991), 
Butler (1986), Davison (1986), Cox and Reid (1987,1992), Cox (1988), Fraser and 
Reid (1989), Bj0rnstad (1990) and Monahan and Boos (1992). Other references 
relevant to the interface between likelihood inference and Bayesian statistics  
include Hartigan (1967), Plante (1971), Akaike (1980b), Pereira and Lindley (1987), 
Bickel and Ghosh (1990), Goldstein and Howard (1991) and Royall (1992). See, 
also, Section 5.5.1 for a link with Laplace approximations of posterior densities. 
For further information on the history of likelihood, see Edwards (1974). 
The likelihood approach can also conflict with the weak repeated sampling 
principle, in that examples exist where, for some possible parameter values,  
hypothetical repetitions result in mostly misleading conclusions. Frequentist statistics 
456 
B. Non-Bayesian Theories 
solves the difficulty by comparing the observed likelihood function with the  
distribution of the likelihood functions which could have been obtained in hypothetical 
repetitions; Bayesian statistics solves the problem by working, not with the  
likelihood function, but with the posterior distribution defined as the weighted average 
of the likelihood function with respect to the prior. The following example is due 
toBirnbaum(1969). 
Example B.5. (Naive likelihood versus reference analysis). Consider the model 
p(x 10), x e {1,2,..., 100}, 0 e {0,1,2,..., 100}, where, for x = 1,2,..., 100, 
p(x\0 = O) = 1/100, 
p(x\0^O)=I{x=e){x) 
Then, whatever x is observed, if 0 = 0 the likelihood of the true value is always l/100th of 
the likelihood of the only other possible 0 value, namely 0 = x. 
From a Bayesian point of view, the answer obviously depends on the prior distribution. 
If all 0 are judged a priori to have the same probability, then one certainly has 
p(0 = O|z) = 1/101 
p(0 = x\x) = 100/101, i/O. 
However, if, say, 0 = 0 is considered to be special, as might well be the case in any real 
application of such a model, and is declared to be the parameter of interest, then the reference 
prior turns out to be 
p(0 = 0) = 1/2, p{0 = r) = 1/200, r = 1,..., 100, 
and a straightforward calculation reveals that this is also the posterior, given a single  
observation, x. Thus, with this prior, one observation from the model provides no information. 
Of course, for any prior, a second observation would, with high probability, reveal the true 
value of 0. 
Finally, as we shall discuss further in Section B.4, we note that, like frequentist 
procedures, the likelihood approach has difficulties in producing an agreed solution 
to prediction problems. 
B.2.4 Fiducial and Related Theories 
We noted in Section B.2.3 that frequentist approaches are inherently unable to  
produce probability statements about the parameter of interest conditional on the data, 
a form of inference summary that seems most intuitively useful. This fact, coupled 
with the seeming aversion of most statisticians to the use of prior distributions, 
has led to a number of attempts to produce "posterior" distributions without using 
priors. We now review some of those proposals. 
B.2 Alternative Approaches 
457 
Fiducial Inference 
In a series of papers published in the thirties, Fisher (1930, 1933, 1935, 1939)  
developed, through a series of examples and without any formal structure or theory, 
what he termed the fiducial argument. Essentially, he proposed using the  
distribution function F(t \ 9) of a sufficient estimator t € T for 9 € © in order to make 
conditional probability statements about 9 given t, thus somehow transferring the 
probability measure from T to 0. However, no formal justification was offered for 
this controversial "transfer". 
The basic characteristics of the argument may be described as follows. Let 
p(x 16), 6 £ (0o, #i) Q 3? be a one-dimensional parametric model and let t = t(x) 
be a sufficient statistic for 6. Suppose further that the distribution function of t, 
F(t 16), is monotone decreasing in 9, with F(t \ 60) = 1 and F(t \ 9]) = 0. Then, 
G{611) = 1 — F(t | 6) has the mathematical properties of a distribution function 
over (#o, #i) and, hence, 
ne\t) = -~F{t\9) 
has the mathematical structure of a "posterior density" for 6. This is the fiducial 
distribution of 6, as proposed by Fisher (1930,1956/1973). The argumentis trivially 
modified if F(t | 6) is monotone increasing in 6, by using G{6 \ t) = F(t \ 6). 
Example B.6. (Fiducial and reference distributions). Let x = {xi,..., xn} be a 
random sample from an exponential distribution p(x | 0) = Ex(x 10) = 0e~9x, with mean 
0~l. It is easily verified that x is a sufficient statistic for 0, and has a distribution function 
-J pnx9 
F(2|*)=(^T)U tn'le"dt' 
which is monotone increasing in 0. Hence, G(0 \ x) = F{x \ 0) is monotone increasing from 
0 to 1 as 0 ranges over (0, oo), and the fiducial distribution of 0 is obtained as 
f(0 \x) = -JI§F(x\0)= (^ijj){nx0fe-^. 
Note that this has the form f(0 \ x) oc p(x 10)n(0), with tt(0) = 0~l. Since ir(0) = 0~1 is 
in this case the reference prior for 0, it follows that, in this example, the fiducial distribution 
coincides with the reference posterior distribution. 
This last example suggests that the fiducial argument might simply be a re- 
expression of Bayesian inference with some appropriately chosen  
"non-informative" prior. However, Lindley (1958) established that this is true if, and only if, 
458 
B. Non-Bayesian Theories 
the probability model p{x \ 6) is such that x and 9 may separately be transformed 
so as to obtain a new parameter which is a location parameter for the transformed 
variable. See Seidenfeld (1992) for further discussion. 
In one-dimensional problems, the fiducial argument, when applicable, is more 
or less well defined and often produces reasonable answers, which are nevertheless 
far better justified from a Bayesian reference analysis viewpoint. However, it is 
by no means clear—and, in fact, a matter of considerable controversy—how the 
argument might be extended to multiparameter problems. The Royal Statistical 
Society discussions following the papers by Fieller (1954) and Wilkinson (1977) 
serve to illustrate the difficulties. Other relevant references are Brillinger (1962) 
and Barnard (1963). 
From a modern perspective, the fiducial argument seems now to have at most 
historical interest, and that mainly due to the perceived stature of its proponent. As 
Good (1971) puts it 
... if we do not examine the fiducial argument carefully, it seems almost  
inconceivable that Fisher should have made the error which he did in fact make. It is 
because (i) it seemed so unlikely that a man of his stature should persist in the 
error, and (ii) because, as he modestly says, his 1930 'explanation left a good 
deal to be desired,' that so many people assumed for so long that the argument 
was correct. They lacked the daring to question it. 
See, however, Efron (1993) for a recent suggested modification of the fiducial 
distribution which may have better Bayesian properties. 
Pivotal Inference 
Suppose that, for a given model p{x | 6), with sufficient statistic t, it is possible 
to find some function h(9, t) which is monotone increasing in 6 for fixed t, and 
in t for fixed 6, and which has a distribution which only depends on 6 through 
h{6, t). Then, h(9, t) is called a pivotal function and the fiducial distribution of 
9 may simply be obtained by reinterpreting the probability distribution of h over 
T as a probability distribution over 0. Fisher's original argument, as described 
above, is a special case of this formulation, since G{6 \ t) is a pivotal function with 
a uniform distribution over [0,1], which is independent of 6. 
Barnard (1980b) has tried to extend this idea into a general approach to  
inference. His basic idea is to produce statements derived from the distribution of an 
appropriately chosen pivotal function, possibly conditional on the observed values 
of an ancillary statistic a{x). 
Partitioning a pivotal function h(6, x) = [g(6, x), a(x)] to identify a  
possibly uniquely defined ancillary statistic a(x), and using the distribution of g(6,x) 
conditional on the observed value of a(x), does produce some interesting results 
in multiparameter problems where the standard fiducial argument fails. However, 
B.2 Alternative Approaches 
459 
the mechanism by which the probability measure is transferred from the sample 
space to the parameter space remains without foundational justification, and the 
argument is limited to the availability—by no means obvious—of an appropriate 
pivotal function for the envisaged problem. 
Structural Inference 
Yet another attempt at justifying the transfer of the probability measure from the 
sample space into the parameter space is the structural approach proposed by Fraser 
(1968, 1972, 1979). 
Fraser claimed that one often knows more about the relationship between data 
and parameters than that described by the standard parametric model p(x \9). He 
proposes the specification of what he terms a structural model, having two parts: 
a structural equation, which relates data x and parameter 0 to some error variable 
e; and a probability distribution for e which is assumed known, and independent 
of 0. Thus, the observed variable x is seen as a transformation of the error e, the 
transformation governed by the value of 0. The key idea is then to reverse this 
relationship, and to interpret 0 as a transformation of e governed by the observed 
x, so that 0 in a sense "inherits" the probability distribution. 
Example B.7. (Structural and reference distributions). Let x = {xx,..., xn} be a 
set of independent measurements with unknown location n and scale u. If the errors have a 
known distribution p(e), the structural equation is 
Xi = n + aei, i = l,...,n 
and the error distribution is, 
n 
p(e) =Y[p(ei). 
If p(e) is normal, this structural model may be reduced in terms of the sufficient statistics x 
and s2 to the equations 
x = /J, + ere, s = ase 
and error distributions 
e = z/y/n, z~N(z|0,l) 
se = {w/{n-l)yi\ w-xLr 
Reversing the probability relationship in the pivotal functions (n — l)s2/a2 ~ Xn-i ^d 
y/n(x — n)/s ~ St (t | 0,1, n — 1) leads to structural distributions for u and /x which, as is 
often the case, coincide with the corresponding reference posterior distributions. 
460 
B. Non-Bayesian Theories 
The general formulation of structural inference generalises the affine group 
structure underlying the last example, and considers a structural equation x = Oe, 
to be interpreted as the response x generated by some transformation 0 € G in 
a group of transformations G, operating on a realised error e, with a completely 
identified error distribution for e. It is then claimed that 0~l{x) has the same 
probability distribution e and, hence, this may be used to provide a structural 
distribution for 0. 
Here, the mechanism by which the probability measure on X is transferred to 
6 is certainly well-defined in the presence of the group structure central to Fraser's 
argument. However, the group structure is fundamental and the approach seems to 
lack general validity and applicability. As Lindley (1969) puts it 
... Fraser's argument [is] an improvement upon and an extension of Fisher's in the 
special case where the group structure is present but [one should be]... suspicious 
of any argument,..., that only works in some situations, for inference is surely a 
whole, and the Poisson distribution [is] not basically different in character from 
... the normal. 
When the structural argument can be applied it produces answers which 
are mathematically closely related to Bayesian posterior distributions with "non- 
informative" priors derived from (group) invariance arguments. In fact, in most 
examples, the structural distributions are precisely the posterior distributions  
obtained by using as priors the right Haar measures associated with the structural 
group, which in turn, are special cases of reference posterior distributions (see 
Villegas, 1977a, 1981, 1990; Dawid, 1983b, and references therein). 
B.3 STYLISED INFERENCE PROBLEMS 
B.3.1 Point Estimation 
Let {p(x | 0), 0 £ 6} be a fully specified parametric family of models and suppose 
that it is desired to calculate from the data x a single value 0(x) € 6  
representing the "best estimate" of the unknown parameter 6. This is the so-called point 
estimation problem. Note that, in this formulation, the final answer is an element 
of 6, with no explicit recognition of the uncertainty involved. Pragmatically, a 
point estimate of 6 may be motivated as being the simplest possible summary of 
the inferences to be drawn from x about the value of 0: alternatively, one may  
genuinely require a point estimate as the solution to a decision problem; for example, 
adjusting a control mechanism, or setting a stock level. 
We recall from Section 5.1.5 that, within the Bayesian framework, the problem 
of point estimation is naturally described as a decision problem where the set of 
possible answers to the inference problem, A, is the parameter space 6. Formally, 
B.3 Stylised Inference Problems 
461 
one specifies the loss function l(a, 6) which describes the decision maker's  
preferences in that context, and chooses as the (Bayes) estimate that value 6*(x) which 
minimises the posterior expected loss, 
l l{a,0)p{0\x)dO, 
where 
p(0\x)cxp(x\0)p(0). 
We have seen (Propositions 5.2 and 5.9) that intuitively natural solutions, such as 
the mean, mode or median of the posterior distribution of 0, are particular cases 
of this formulation for appropriately chosen loss functions. We also note that the 
definition of an optimal Bayesian estimator is constructive, in that it identifies a 
precise procedure for obtaining the required value. 
Classical decision theory ideas can obviously be applied to point estimation 
viewed as adecision problem. Thus, one may define admissible estimates, minimax 
estimates, etc., with respect to any particular loss function. From our perspective, 
the problems and limitations of classical decision theory that we identified in  
Section B.2.1 carry over to particular applications such as point estimation. Thus, 
admissible estimators are essentially Bayes estimators, but classical decision  
theory provides no foundational^ justified procedure for choosing among admissible 
estimators, with—as we noted—the general minimax principle being unpalatable 
to most statisticians. 
The frequentist approach proceeds by defining possible desiderata of the long 
run behaviour of point estimators, and, using these desiderata as criteria, proposes 
methods for obtaining "best" estimators, and identifies conditions under which 
"good behaviour" will result. The criteria adopted are typically non-constructive. 
The likelihood approach proceeds by using the likelihood function to measure 
the strength with which the possible parameter values are supported by the data. 
Hence, the optimal estimator is naturally taken to be that 6 which maximises the 
likelihood function. It is worth stressing that this is a constructive criterion, in 
that the very definition of a maximum likelihood estimator (MLE) determines its 
method of construction. 
Fiducial, pivotal and structural inference approaches all produce "posterior" 
probability distributions for 6. Hence, their "solution" to the problem of point 
estimation is essentially that suggested by the Bayesian approach; either to offer 
as an estimator of 6 some location measure of the probability distribution of 6 
or, more formally, to obtain that value of 0 which minimises some specified loss 
function with respect to such a distribution. 
462 
B. Non-Bayesian Theories 
Criteria for Point Estimation 
It should be clear from Sections 5.1.4 and B.2.2 that the search for good estimators 
may safely be limited to those based on sufficient statistics, for then, and only then, 
is one certain to use all the relevant information about the parameter of interest. 
However, the following two points introduce a note of caution. 
(i) Sufficiency is a global concept; thus, if 6(x) is sufficient for 0, it does 
not follow that 0i(x) is sufficient for a component parameter <?,, even if 0i(x) is 
sufficient for 0t when 0 — {0t} is known. For instance, with univariate normal 
data (x, s2) is jointly sufficient for (/j,, a2), but x is not sufficient for /x, nor is s2 
sufficient for a2. 
(ii) Sufficiency is a concept relative to a model; thus, even a small perturbation 
to the assumed model may destroy sufficiency. For example (x, s2) is not sufficient 
for (/x, <r) if the true model is St (x | /x, a, 1000) or the mixture form 0.999 x 
N(x | /x, <r) + 0.001 x N(x | 0,1), even though these two models are indeed very 
"close" to N(x | /x, a). 
The bias of an estimator 0(x) is defined to be 
6(0)= [ O(x)p(x\0)dx-0 
and its mean squared error (mse) to be 
mse(01 0) = j{9{x) - 0}2 p(x \0)dx = V(61 0) + {b(0)}2. 
From a frequentist point of view it is desired that, in the long run, 0 should be as 
close to 0 as possible; thus, if quadratic loss is judged to be an appropriate "distance" 
measure, a frequentist would like an estimator 0 with small mse(0 | 0) for almost 
all values of 0. A concept of relative efficiency is developed in these terms. An 
estimator 0\ is more efficient than 02, if, for all 0, mse(0i | 6) < mse(02 I #)• 
A simple theory is available if attention is restricted to unbiased estimators, 
i.e., estimators such that 6(0) = 0, since then we simply have to minimise V(010) 
in this unbiased class. However, although requiring the sampling distribution of 0 
to be centred at 0 may have some intuitive appeal, there are powerful arguments 
against requiring unbiasedness. Indeed: 
(i) In many problems, there are no unbiased estimators. For instance, r/n is an 
unbiased estimator of the parameter 0 for a binomial Bi(r | 0, n) distribution, 
but there is no unbiased estimator of Qll2. 
(ii) Even when they exist, unbiased estimators may give nonsensical answers, and 
no theory exists which specifies conditions under which this can be guaranteed 
not to happen. For example, the (unique) unbiased estimator of the parameter 
0 e (0,1) of a geometric distribution p(x \ 0) = 0(1 - 0)x, x = 0,1,..., 
B.3 Stylised Inference Problems 
463 
is 0(0) = 1, 9(x) = 0,x = 1,2,...; hardly a sensible solution! Similarly 
(see Ferguson, 1967), if 0 is the mean of a Poisson distribution, Pn(x | 9) = 
e~e9x/x\, x = 0,1,..., then the only unbiased estimator of e~e, a quantity 
which must lie in (0,1), is 1 if x is even and 0 if it is odd (again, hardly 
sensible); but—even more ridiculously—the only unbiased estimate of e~29 
is (—l)1, leading to the estimate of a probability as —1 (for all odd x)\ 
(iii) The unbiasedness requirement violates the likelihood principle, by making the 
answer dependent on the sampling mechanism. Thus, the unbiased estimator 
of /x from a N(x | /x, a) observation is x, but the unbiased estimator from 
p(x |/i, <r) = N(a; | /x, a), if x < 100 
= N(a; | 0,1) otherwise 
will be something else. Yet, if one is measuring n with an instrument which 
only works for values x < 100 and obtains x = 50, i.e., a valid measurement, 
it seems inappropriate to make our estimate of /x dependent on the fact that 
we might have obtained an invalid measurement, but did not. 
(iv) Even from a frequentist perspective, unbiased estimators may well be  
unappealing if they lead to large mean squared errors, so that an estimator with 
small bias and small variance may be preferred to one with zero bias but a 
large variance. 
For further discussion of the conflict between Bayes and unbiased estimators, 
see Bickel and Blackwell (1967). See, also, Wald (1939). 
Another frequentist criterion forjudging an estimator concerns the asymptotic 
behaviour of its sampling distribution. If we write 6n = 0{x\,..., xn) to make 
explicit the dependence of the estimator on the sample size, a frequentist would 
clearly like 6„ to converge to 6 (in some sense) as n increases. An estimator 6n 
is said to be weakly consistent if 6n —» 6 in probability, and strongly consistent if 
6n —> 6 with probability one. By Chebychev's inequality, a sufficient condition 
for the weak consistency of unbiased estimators is that V(6n) —> 0 as n —> 0. 
Obviously, a consistent estimator is asymptotically unbiased. 
For discussion on the consistency of Bayes estimators, see, for example, 
Schwartz (1965), Freedman and Diaconis (1983), de la Horra (1986) and Diaconis 
and Freedman (1986a, 1986b). For the frequentist properties of Bayes estimators, 
see Diaconis and Freedman (1983). 
"Optimum" Estimators 
We have mentioned before that minimising the variance among unbiased estimators 
is often suggested as a procedure for obtaining "good" estimators. Sometimes, this 
procedure is even further restricted to linear functions; thus, provided \x = E(x \ 9) 
and <72 = V(x \ 6) exist, x is said to be the best linear unbiased estimator (BLUE) 
of /x, in the sense that it has the smallest mse among all linear, unbiased estimators. 
It is easy, however, to demonstrate, with appropriate examples, that this is a 
rather restricted view of optimality, since non-linear estimators may be considerably 
464 
B. Non-Bayesian Theories 
more efficient. An "absolute" standard by which unbiased estimators may be judged 
is provided by the Cramer-Rao inequality. Let g = g(x) be an unbiased estimator 
of g(0) and define the efficient score function u(x \ 9) to be 
u{x\0) = -^\ogp{x\0). 
Then, under suitable regularity conditions, Ex \ e [u(x \ 0)} = 0, 
where 7W 
& 
de2u{x\0) 
1(0) = Exle [u\x | 0)) = -EX{1 
with equality if, and only if, 
u(x\0) = k(0){g(x)-g(0)}, 
where k(0) does not depend on x, in which case g is said to be a minimum  
variance bound (MVB) estimator of g(9). It follows that a minimum variance bound 
estimator must be sufficient, unbiased, and a linear function of the score function. 
We have already stressed that limiting attention to unbiased estimators may 
not be a good idea in the first place. Moreover, the range of situations where 
"optimal" unbiased estimators, i.e., the MVB estimators, can be found is rather 
limited. Indeed, if 0 is sufficient for 0 there is a unique function g(0) for which a 
MVB exists, namely that described above. For example, if x = {x\,..., x„} is a 
random sample from N(a; 10, a2), Ex2/n is a MVB estimator for a2, but no MVB 
estimator exists for <r! 
One might then ask whether it is at least possible to obtain an unbiased  
estimator with a variance which is lower than that of any other estimator for each 0, 
even if it does not reach the Cramer-Rao lower bound. Under suitable regularity 
conditions, the existence of such uniformly minimum variance (UMV) estimators 
can indeed be established. Specifically, Rao (1945) and Blackwell (1947)  
independently proved that if 0(x) is an estimator of 0 and t = t(x) is a sufficient statistic 
for 0, then, given the value of the sufficient statistic t, the conditional expectation 
of<?(a;), 
9{t) = E[011] = J 0(x)p(x | t)dx, 
is an improved estimator of 9, in the sense that, for every value of 0, mse(<? | 0) < 
mse(<? | 0), a result which can be generalised to multidimensional problems. 
A decision-theoretic consequence of the so-called Rao-Blackwell theorem is 
that any estimator of 0 which is not a function of the sufficient statistic t must be 
B.3 Stylised Inference Problems 
465 
inadmissible. However, as a constructive procedure for obtaining estimators this 
result is of limited value due the fact that it is usually very difficult to calculate the 
required conditional expectation. 
If 0(x) is unbiased and complete, and there is a complete sufficient statistic 
t = t(a;),then 0(t) is unbiased, and is the UMV estimator of 0. For example, r/n 
is the MVB estimator of the parameter 0 of a binomial distribution Bi(r 10, n), but 
there is no MVB estimator of 02. However, the result may be used to show that 
r(r - l)/[n(n — 1)] is a UMV estimator of 92. 
MLE estimators are not guaranteed to exist or to be unique; but when they 
do exist they typically have very good asymptotic properties. Under fairly general 
conditions, MLE's can be shown to be consistent (hence asymptotically unbiased, 
even if biased in small samples), asymptotically fully efficient, and asymptotically 
normal, so that, if n —> oo, the sampling distribution of 0 converges to the normal 
distribution N(010,1(0)) with mean 0 and precision 1(0), the information function. 
Bayesian estimators always exist for appropriately chosen loss functions and 
automatically use all the relevant information in the data. They are typically biased, 
and have analogous asymptotic properties to maximum likelihood estimators (i.e., 
from a frequentist perspective they are consistent, asymptotically fully efficient and, 
under suitable regularity conditions, asymptotically normal). A famous example is 
the Pitman estimator (Pitman (1939), which may be obtained as the posterior mean 
which corresponds to a uniform prior; see, also, Robert et al. (1993). 
Both the likelihood and the Bayesian solutions to the point estimation  
problem automatically define procedures for obtaining them; the frequentist approach 
does not (expect for special cases like the exponential family). In addition to the 
MLE approach, other methods of construction include minimum chi-squared, least 
squares, and the method of moments. However, these methods do not in themselves 
guarantee any particular properties for the resulting estimators, which usually have 
to be investigated case by case. Historically, all these construction methods have 
been used at various times within the frequentist approach to produce candidate 
"good estimators", which have then been analysed using the criteria described 
above. Nowadays, partly under the influence of classical decision theory, some 
frequentist statisticians pragmatically minimise an expected posterior loss to  
obtain an estimator, whose behaviour they then proceed to study using non-Bayesian 
criteria. 
For an extensive treatment of the topic of point estimation, see Lehmann 
(1959/1983). 
B.3.2 Interval Estimation 
Let{p(x\6),6 £ 6} be a fully specified parametric family of models and suppose 
that it is desired to calculate, from the data x, a region C(x) within which the 
parameter 6 may reasonably be expected to lie. Thus, rather than mapping X 
466 
B. Non-Bayesian Theories 
into 6, as in point estimation, a subset of 6 is associated with each value of x, 
whose elements may be claimed to be supported by the data as "likely" values of 
the unknown parameter 6. This is the so-called region estimation problem; when 
0 is one-dimensional, the regions obtained are typically intervals, hence the more 
standard reference to the interval estimation problem. 
Region estimates of 0 may be motivated pragmatically as informative simple 
summaries of the inferences to be drawn from x about the value of 6 or, more 
formally, as a set of 6 values which may safely be declared to be consistent with 
the observed data. 
We recall from Section 5.1.5 that, within a Bayesian framework, credible 
regions provide a sensible solution to the problem of region estimation. Indeed, for 
each a value, 0 < a < 1, a 100(1 — a)% credible region C, i.e., such that 
/ p(0 | x)d0 = 1 - a, 
Jc 
contains the true value of the parameter with (posterior) probability 1 - a and, 
among such regions, those of the smallest size, i.e., the highest posterior density 
(HPD) regions, suggest themselves as summaries of the inferential content of the 
posterior distribution. Note that this formulation is equally applicable to prediction 
problems simply by using the corresponding posterior predictive distribution. 
Confidence Limits 
For 0 < a < 1 and scalar 0 € 6 C R, a statistic 0a(a;) such that for all 0, 
Pt{0a(x) > 01 0} = 1 - a, 
and such that if ct\ > ct2 then <?ai < 0°"*, is called an upper confidence limit for 0 
with confidence coefficient 1 — a. Note that if g is strictly increasing, then <7(0a) 
is an upper confidence limit for g{0). The nesting condition is important to avoid 
inconsistency; see e.g., Plante (1984, 1991). 
Given x, the specific interval (—oo, 0a(x)\ is then typically interpreted as 
a region where, given x, the parameter 0 may reasonably be expected to lie. It 
is crucial however to recognise that the only proper probability interpretation of a 
confidence interval is that, in the long run, a proportion 1 — a of the 0a (x) values 
will be larger than 0. Whether or not the particular 0a(x) which corresponds to 
the observed data x is smaller or greater than 0 is entirely uncertain. One only has 
the rather dubious "transferred assurance" from the long-run definition. 
A lower confidence limit ^ (a;) is similarly defined as a statistic 9^ (x) such that 
Pr{9a(x) < 9 | 9} = 1 — a with the corresponding nesting property. Combining 
a lower limit at confidence level 1 — ai, with an upper limit at confidence level 
B.3 Stylised Inference Problems 
467 
1 - a2, we obtain a two-sided confidence interval [9^ (x), 9°"*(x)} at confidence 
level 1 - ai - a2 , such that, for all 9, 
Pr^j (x) < 9 < 9aHx) 19} = 1 - ai - a2. 
For two-sided confidence intervals, a convenient choice is a\ = a2, which 
produces central confidence intervals based on equal tail-area probabilities. There 
are, however, other alternatives, 
(i) Shortest confidence intervals. For fixed ai + a2 = a, a^ and a2 may be 
chosen to minimise the expected interval length Ex \ g[9a2(x) - 0^ (x) \ 9]. 
It must be realised, however, that shortest intervals for 9 do not generally 
transform to shortest intervals for functions g{9). It can be proved that intervals 
based on the score function 
u{x\9) = ^logp(a;|0) 
have asymptotically minimum expected length; moreover, the fact that u has 
a sampling distribution which is asymptotically normal N(u | 0,1{9)), with 
mean 0 and precision 1(9), may be used to provide approximate confidence 
intervals for 9. 
(ii) Most selective intervals. For fixed a\ + a2 = a, one could try to choose ai 
and a2 to minimise the probability that the interval contains false values of 9. 
However, such uniformly most accurate intervals are not guaranteed to exist. 
It is worth noting that, for a variety of reasons, the construction of confidence 
intervals is by no means immediate, 
(i) They typically do not exist for arbitrary confidence levels when the model is 
discrete. 
(ii) There is no general constructive guidance on which particular statistic to 
choose in constructing the interval. 
(iii) There are serious difficulties in incorporating any known restrictions on the 
parameter space, and no systematic procedure exists for incorporating such 
knowledge in the construction of confidence intervals. 
(iv) In multiparameter situations, the construction of simultaneous confidence  
intervals is rather controversial. It is less than obvious whether one should 
use the confidence limits associated with individual intervals, or whether one 
should think of the problem as that of estimating a region for a single vector 
parameter, or as one of considering the probability that a number of confidence 
statements are simultaneously correct. 
(v) Interval estimation in the presence of nuisance parameters is another  
controversial topic. Unless appropriate pivotal quantities can be found, the properties 
of various alternative procedures, typically based on replacing the unknown 
nuisance parameters by estimates, are generally less than clear. 
468 
B. Non-Bayesian Theories 
(vi) Interval estimation of future observations poses yet another set of difficulties. 
Unless one is able to find a function of the present and future observation 
whose sampling distribution does not depend on the parameters (and this is 
not typically the case), one is again limited to ad hoc approximations based 
on substituting estimates for parameters. 
But, even in the simplest case where 0 is a scalar parameter labelling a  
continuous model p(x | 9), the concept of a confidence interval is open to what many 
would regard as a rather devastating criticism. Namely the fact that the confidence 
limits can turn out to be either vacuous or just plain silly in the light of the observed 
data. We give two examples. 
(i) In the Fieller-Creasy problem, where the parameter of interest is the ratio of 
two normal means, there are values a < 1 such that, for a subset of possible 
data with positive probability, the corresponding 1 — a confidence interval is 
the entire real line. Solemnly quoting the whole real line as a 95% confidence 
interval for a real parameter is not a good advertisement for statistics. For 
Bayesian solutions, see Bernardo (1977) and Raftery and Schweder (1993). 
(ii) If xi and x2 are two random observations from a uniform distribution on the 
interval (0 - 0.5,0 + 0.5), and yi and j/2 are, respectively, the smaller and the 
larger of these two observations, then it is easily established that for all 9 
p{yi<e<m\e} = o.5 
so that (2/1,2/2) provides a 50%. confidence interval. However, if for the 
observed data it turns out that j/2 — j/i > 0.5 then certainly yi < 6 < j/2, so 
that we know for sure that 0 belongs to the interval (j/i, j/2), even though the 
confidence level of the interval is only 50%. 
These examples reflect the inherent difficulty that the frequentist approach to 
statistics has of being unable to condition on the complete observed data.  
Conditioning on ancillary statistics, when possible, may mitigate this problem, but it 
certainly does not solve it and, as discussed in Section B.2.2, it may create  
others. The reader interested in other blatant counterexamples to the (unconditional) 
frequentist approach to statistics will find references in the literature under the  
keywords relevant subsets, which refer to subsets of the sample space yielding special 
information and subverting the "long-run" or "on average" frequentist viewpoint. 
Two important such references are Robinson (1975) and Jaynes (1976); see, also, 
Buehler (1959), Basu (1964, 1988), Cornfield (1969), Pierce (1973), Robinson 
(1979a, 1979b), Casella (1987, 1992), Maatta and Casella (1990) and Goutis and 
Casella(1991). 
As a final point, we should mention that for many of the standard textbook 
examples of confidence intervals (typically those which can be derived from  
univariate continuous pivotal quantities), the quoted intervals are numerically equal 
B.3 Stylised Inference Problems 
469 
to credible regions of the same level obtained from the corresponding reference 
posterior distributions. This means that, in these cases, the intuitive interpretation 
that many users (incorrectly, of course!) tend to give to frequentist intervals of 
confidence 1 - a, namely that, given the data, there is probability 1 - a that the 
interval contains the true parameter value, would in fact be correct, if described, 
instead, as a reference posterior credible interval. 
A typical example of this situation is provided by the class of intervals 
x - t^s/yjn - 1 < /x < x + t^s/yjn - 1, a>0 
for the mean /x of a normal distribution with unknown precision. These are both the 
"best" confidence intervals for u, derivable from the sampling distribution of the 
pivotal quantity \Jn — l(x — /x)/s, and also the credible intervals which correspond 
to the reference posterior distribution for /x, 7r(/x | a;) = St(/x | x, (n - l)s~2, n — 1) 
derived in Example 5.17. Buehler and Feddersen (1963) demonstrated that relevant 
subsets exist even in this standard case. Indeed, if x = {xi,X2}, then C = 
{xmin, £max) is a 50% interval for /x, but if both observations belong to the set 
R={(xi,x2); \xi - x2\ > 4\x\/3} 
then Pr{C | x € R, /x, <r} = 0.5181. Pierce (1973) has shown that similar  
situations can occur whenever the confidence interval cannot be interpreted as a credible 
region corresponding to a posterior distribution with respect to a proper prior. Note 
that although this long-term coverage probability is not directly relevant to a Bayes- 
ian, the example suggests that special care should be exercised when interpreting 
posterior distributions obtained from improper priors. 
Casella et al. (1993) have proposed, for interval estimation, alternative loss 
functions to the standard linear functions of volume and coverage probability. 
B.3.3 Hypothesis Testing 
Let {p(x \6),6 € 6} be a fully specified parametric family of models, with 6, 
partitioned into two disjoint subsets 60 and 6j, and suppose that we wish to decide 
whether the unknown 6 lies in 60 or in 9X. If H0 denotes the hypothesis that 
0 e 60 and Hi the hypothesis that 6 € Gi, we have a decision problem, with only 
two possible answers to the inference problem, a0 = accept H0 or a\ = accept H\, 
where the choice is to be made on the basis of the observed data x. This is the 
so-called problem of hypothesis testing. In most such problems, the two hypotheses 
are not symmetrically treated; the working hypothesis H0 is usually called the null 
hypothesis, while Hi is referred to as the alternative hypothesis. Although the 
theory can easily be extended to any finite number of alternative hypotheses, we 
will present our discussion in terms of a single alternative hypothesis. 
470 
B. Non-Bayesian Theories 
We recall from Section 6.1 that, within a Bayesian framework, the problem of 
hypothesis testing, as formulated above, can be appropriately treated using standard 
decision theoretical methodology; that is, by specifying a prior distribution and an 
appropriate utility function, and maximising the corresponding posterior expected 
utility. We also recall that the solution to the decision problem posed generally 
depends on whether or not the "true" model is assumed to be included in the family 
of analysed models. Assuming the stylised A4-closed case, where the true model 
is assumed to belong to the family {p(x \0),G G 8} and the utility structure is 
simply 
u(au9)=0 Oeeu i = 0,l 
= -kj 6 G Bj, j + i, 
we have seen (Proposition 6.1) that the null hypothesis Hq should be rejected if, 
and only if, 
Boi{x) < Torn)' 
This corresponds to checking whether the appropriate (integrated) likelihood ratio, 
or Bayes factor, 
JBop(x\0)p(0)dO 
B0l{X)~ UlP(x\0)p(0)dO' 
is smaller than a cut-off point which depends on the ratio loi/ho of the losses 
incurred, respectively, by accepting a false null and rejecting a true null, and on the 
ratio of the prior probabilities of the hypotheses, 
p(Hi) = f p(G)dG, * = 0,1. 
From the point of view of classical decision theory, the problem of hypothesis 
testing is naturally posed in terms of decision rules. Thus, a decision rule for this 
problem (henceforth called a test procedure 6, or simply a test 6) is specified in 
terms of a critical region R$, defined as the set of x values such that Ho is rejected 
whenever iGflj. The most relevant frequentist aspect of such a procedure 6 is 
its power function 
pow(0 18) = Pr{x G R6 \ 0}, 
which specifies, as a function of 6, the long-run probability that the test rejects the 
null hypothesis Hq. Obviously, the ideal power function would be 
pow(0 | 6) = 0, 6 G Go 
= 1, 6 6 6, 
although, naturally, one will seldom be able to derive a test procedure with such 
an ideal power function. For any 6 G 0o, pow(018) is the long-run probability of 
B.3 Stylised Inference Problems 
471 
incorrect rejection of the null hypothesis; frequentist statisticians often specify an 
upper bound for such probability, which is then called the level of significance of 
the tests to be considered. The size of any specific test 8 is defined to be 
a = sup pow(0 | 8); 
9ee0 
thus, to specify a significance level a is to restrict attention to those tests whose 
size is not larger than a. 
Either 8o or 8i may contain just a single value of 6. In this case, the  
corresponding hypothesis is referred to as a simple hypothesis; if 8, contains more than 
one value of 6, then H, is referred to as a composite hypothesis. 
For any test procedure 8 one may explicitly consider two types of error;  
rejecting a true null hypothesis, a so-called error of type 1, and accepting a false null 
hypothesis, a so-called error of type 2. Let us denote by a(8 \ 6) and (3(8 \ 6) the 
respective probabilities of these two types of error, 
a(8 | 0) = Pt{x eRs\0} if 6 e 80) 
= 0 otherwise 
(3(81 6) = Pt{x 0 Rs | 6} if0G8i, 
= 0 otherwise. 
It would obviously be desirable to identify tests which keep both error  
probabilities as small as possible. However, typically, modifying R& to reduce one would 
make the other larger. Hence, one usually tries to minimise some function of the 
two; for example, a linear combination aa(8 \ 0) + b/3(6 \ 6). 
Testing Simple Hypotheses 
When both Ho and Hi are simple hypothesis, so that a(6 \ 6) = a(6 \ G0) = a(6) 
and P(61 6) = (3(8 \ 6{) = (3(8), it can be proved that a test which minimises 
aa(8) + b/3(8) should reject Ho if, and only if, 
P(x | flp) a 
P(z|6>i) 6' 
i.e., if the likelihood ratio in favour of the null is smaller than the ratio of the 
weights given to the two kinds of error. This can be seen as a particular case of 
the Bayesian solution recalled above, and is closely related to the Neyman-Pearson 
lemma (Neyman and Pearson, 1933,1967) which says that a test which minimises 
(3(8) subject to a(8) < a must reject H0 if, and only if, 
P(x\e1)<K 
472 
B. Non-Bayesian Theories 
for some appropriately chosen constant k. It has become standard practice among 
many frequentist statisticians to choose a significance level ao (often "conventional" 
quantities such as 0.05 or 0.01) and then to find a test procedure which minimises 
(3(6) among all tests such that a (6) < a0 (rather than explicitly minimising some 
combination of the two probabilities of error). The Neyman-Pearson lemma shows 
explicitly how to derive such a test, but it should be emphasised that this is not a 
sensible procedure. Indeed: 
(i) With discrete data one cannot attain a fixed specific size a(6) without recourse 
to auxiliary, irrelevant randomisation, whereas minimisation of a linear  
combination of the form aa(6) + b/3(6) can always be achieved. For a Bayesian 
view on randomisation, see Kadane and Seidenfeld (1986). 
(ii) More importantly, by fixing a (6) and minimising j3(6) one may find that, with 
large sample sizes, Ho is rejected when p(x \ H0) is far larger than p(x \ Hi), 
due to the fact that the minimising (3(6) may be extremely small compared with 
the fixed a(6). Although this can be avoided by carefully selecting a(6) as a 
decreasing function of the sample size, it seems far more natural to minimise 
a linear combination aa(6) + b(3(6) of the two error probabilities, in which 
case no difficulties of this type can arise. 
Other strategies for the choice of a (6) and (3(6) have been proposed. For 
example, a(6) = (3(6) corresponds to the minimax principle. However, it is 
important to note (see e.g., Lindley, 1972) that minimising a linear combination of 
the two types of error is actually the only coherent way of making a choice, in the 
sense that no other procedure is equivalent to minimising an expected loss. 
Composite Alternative Hypotheses 
In spite of the difficulties described above, frequentist statisticians have traditionally 
defined an optimal test 6 to be one which minimises (3(6 \ 6) for a fixed significance 
level ao- In terms of the power function, this implies deriving a test 6 such that 
pow(0 | 6) < a0, 6 G 60 
and for which pow(0 | 6) is as large as possible in 0i. A test procedure 6* is called 
a uniformly most powerful (UMP) test, at level of significance ao, if a(6* \ 6) < ao 
and, for any other 6 such that a(6 \ 6) < ao, 
pow(0 | 6) < pow(0 | 6*), for all 6 G 8i. 
It can be proved that, when 8 is one-dimensional, UMP tests often exist for  
onesided alternative hypotheses. 
A model {p(x \ 0), 9 G 8 C 3?} is said to have a monotone likelihood ratio in 
the statistic t = t(x) if for all 9\ < 02,p(x \62)/p(x \6\) is an increasing function 
of t. If p(x | 6) has a monotone likelihood ratio in t and c is a constant such that 
Pr{i >c\e0} = a0, 
B.3 Stylised Inference Problems 
473 
then the test 8 which rejects Ho if t > c is a UMP test of the hypothesis Ho = 9<9o 
versus the alternative Hi = 6 > 90, at the level of significance ao- However, UMP 
tests do not generally exist. 
Example B.8. (Non-existence of a UMP test). If a; = {x\,..., x„) is a random 
sample from a normal distribution N(x | fi, 1), then the test 6t defined by critical region 
Re = {x; x - im, > 1.282/v/n} is a UMP test for H0 = H < /to versus Hx= fi> no, with 
0.10 significance level. Similarly, the test <$2 defined by Rs2 = {x; /Jo — x > 1.282/y/n) is 
a UMP test for Ho = fi > fio versus Hi = fi < no, with the same level. Since these critical 
regions are different, it follows that there is no UMP test for fi = /to versus fi / /to. 
The fact, illustrated in the above example, that UMP tests typically do not 
exist for two-sided alternatives, suggests that a less demanding criterion must be 
used if one is to define a "best" test among those with a fixed significance level. 
Since the power function pow(0 16) describes the probability that the test 6 rejects 
the null, it seems desirable that, when Ho is true, pow(0 | 6) should be smaller in 
80 than elsewhere. A test 6 is called unbiased if for any pair 60 G 8o and 0i € 0i 
it is true that pow(0o | $) < pow(#i 16). 
Example B.9. (Comparative power of different tests). If x = {x\,...,x„} isaran- 
dom sample from a normal distribution N(x | fi, 1) then the test <53 defined by the region 
Rh = {x; \x-im>\ > 1.645/x/n} 
Figure B.2 Power of tests for the mean of a normal distribution 
Ho = i, n = 30, ci = 1.40, c2 = 2.05 
474 
B. Non-Bayesian Theories 
is an unbiased test for Ha = fi = ^ versus Hx = fi ^ fi0. Figure B.2 compares the power 
of this test with those defined in Example B.8, and with that of a typical non-symmetric test 
<54 of the same level, which has the critical region 
R6i = {x; x - no > ci/y/n, ot(m, - x > c2/-Jn} 
for suitably chosen constants C\ < c?. It seems obvious that £4, which is more cautious 
about accepting values of fi larger than ^ than about accepting values of fi smaller than /j0, 
should be preferred to the unbiased test £3 whenever the consequences of the first class of 
errors are more serious, or whenever the values of fi smaller than fia are considered to be 
more likely. 
It is clear from Example B.9 above that, even when they exist, unbiased 
procedures may only be reasonable in special circumstances. We are drawn again 
to the general comment that, in any decision procedure, prior information and utility 
preferences should be an integral part of the solution. 
Yet another approach to defining a "good" test when UMP tests do not exist is 
to focus attention on local power, by requiring the power function to be maximised 
in a neighbourhood of the null hypothesis. Under suitable regularity conditions, 
locally most powerful tests may be derived by using the sampling distribution of 
the efficient score function in a process which is closely related to that described 
in our discussion of interval estimation. However, the requirement of maximum 
local power does not say anything about the behaviour of the test in a region of high 
power and, indeed, locally most powerful tests may be very inappropriate when the 
true value of 6 is far from 80. 
Methodological Discussion 
Testing hypotheses using the frequentist methodology described above may be 
misleading in many respects. In particular: 
(i) It should be obvious that the all too frequent practice of simply quoting whether 
or not a null hypothesis is rejected at a specified significance level olq ignores 
a lot of relevant information. Clearly, if such a test is to be performed, the 
statistician should report the cut-off point a such that H0 would not be rejected 
for any level of significance smaller than a. This value is called the tail area 
or p-value corresponding to the observed value of the statistic. An added 
advantage of this approach is that there is no need to select beforehand an 
arbitrary significance level. As noted in the case of confidence intervals, there 
is a tendency on the part of many users to interpret a p-value as implying 
that the probability that H0 is true is smaller than the p-value. Not only, 
of course, is this false within the frequentist framework but, in this case, 
there is, in general, no simple form of reinterpretation which would have a 
Bayesian justification, so that, even numerically, p-values cannot generally 
B.3 Stylised Inference Problems 
475 
be interpreted as posterior probabilities. For detailed discussions see, for 
example, Berger (1985a), Berger and Delampady (1987) and Berger and Sellke 
(1987). See Casella and Berger (1987) for an attempted reconciliation in the 
case of one-sided tests. 
(ii) Another statistical "tradition" related to hypothesis testing consists of  
declaring an observed value statistically significant, implying that there exists  
statistical evidence which is sufficient to reject the null hypothesis, whenever the 
corresponding tail area is smaller than a "conventional" value such as 0.05 or 
0.01. However, since the classical theory of hypothesis testing does not make 
any use of a utility function, there is no way to assess formally whether or not 
the true value of the parameter 6, which may well be numerically different 
from a hypothetical value Go, is significantly different from 6q in the sense 
of implying any practical difference. Thus, a vote proportion of 34% for a 
political party is technically different from a proportion of 34.001%, but under 
most plausible utility functions the difference has no political significance. 
(iii) Finally, the mutual inconsistency of frequentist desiderata often makes it  
impossible, even in the theory's own terms, to identify the most appropriate 
procedure. For example, if a; is a random sample from N(x | p, mX) with  
precision mX determined by a random integer m, then m is ancillary and hence, 
by the conditionality principle, tests on \i or A should condition on the observed 
m. Yet, Durbin (1969) showed that, at least asymptotically, unrestricted tests 
may be uniformly more powerful. 
See Chernoff (1951) and Stein (1951) for further arguments against standard 
hypothesis testing. 
B.3.4 Significance Testing 
In the previous section, we have reviewed the problem of hypothesis testing where, 
given a family {p(x \ 0), 6 G 8}, a null hypothesis Hq = 6 G 8o is tested against 
(at least) one specific alternative. In this section we shall review the problem of 
pure significance tests, where only the null hypothesis Ho = {p(x \6),6 G 80} 
has been initially proposed, and it is desired to test whether or not the data x are 
compatible with this hypothesis, without considering specific alternatives. The null 
hypothesis may be either simple, if it completely specifies a density p(x \ Gq), or 
composite. 
We recall from Section 6.2 that, within the Bayesian framework, the problem 
of significance testing, as formulated above, could be solved by embedding the 
hypothetical model in some larger class {p(x\6),0 G 8}, designed either to 
contain actual alternatives of practical interest, or formal alternatives generated by 
selecting a mathematical neighbourhood of H0. For any discrepancy measure 
6(0) = u{Hc0,0}-u{H0,6}, 
476 
B. Non-Bayesian Theories 
describing, for each 6, the conditional utility difference, and for any function £q{x), 
describing the additional utility obtained by retaining Hq because of its special 
status, we showed that Hq should be rejected if, 
t(x) > e0{x), 
where 
t(x)= [6{0)p{0\x)d0 
Je 
is the expected posterior discrepancy. In particular, we proposed the logarithmic 
discrepancy 
as a reasonable general discrepancy measure. This (fully Bayesian) procedure 
could be described as that of selecting a test statistic t(x) which is expected to 
measure the discrepancy between Ho and the true model, and rejecting Ho if t(x) 
is larger than some cut-off point eo{x) describing the additional utility of keeping 
Ho if it were true, due to its special status corresponding to simplicity (Occam's 
razor), scientific support (or fashion), or whatever. 
From a frequentist point of view, a test statistic t = t(x) is selected with two 
requirements in mind. 
(i) The sampling distribution of t under the null hypothesis p(t | Ho) must be 
known and, if Ho is composite, p(t \ Ho) should be the same for all 6 G 8o- 
(ii) The larger the value of t the stronger the evidence of the departure from H0 
of the kind which it is desired to test. 
Then, given the data x, ap-value or significance level is calculated as the  
probability, conditional on Ho, that, in repeated samples, t would exceed the observed 
value t(x), so that p is given by 
/■OO 
p = / p(t\H0)dt. 
Jt(x) 
Small values of p are regarded as strong evidence that Ho should be rejected. 
The result of the analysis is typically reported by stating the p-value and declaring 
that Ho should be rejected for all significance levels which are smaller than p. 
Comparison with the Bayesian analogue summarised above prompts the  
following remarks. 
(i) The frequentist theory does not generally offer any guidance on the choice of 
an appropriate test statistic (the generalised likelihood ratio test, a disguised 
Bayes factor seems to be the only proposal). While in the Bayesian analysis 
B.3 Stylised Inference Problems 
477 
t(x) is naturally and constructively derived as an expected measure of  
discrepancy, the frequentist statistician must, in general, rely on intuition to select t. 
The absence of declared alternatives even precludes the use of the frequentist 
optimality criteria used in hypothesis testing. 
(ii) Even if a function t = t(x) is found which may be regarded as a sensible 
discrepancy measure, the frequentist statistician needs to determine the  
unconditional sampling distribution of t under H0; this may be very difficult, 
and actually impossible when there are nuisance parameters. Moreover, in 
the more interesting situation of composite null hypotheses, it is required that 
p(t | 6) be the same for all 6 in 8o, which, often, is simply not the case. 
(iii) If a measure of the strength of evidence against Hq is all that is required, 
the position of the observed value of t with respect to its posterior predictive 
distribution p(t \ x, Ho) under the null hypothesis seems a more reasonable, 
more relevant answer than quoting the realised p-value. Indeed, the  
compatibility of t(x) with H0 may be described by quoting the HPD intervals to 
which it belongs, or may be measured with any proper scoring rule such as 
A \ogp(t(x) | x, Ho) + B. Thus, in Figure B.3, t\ (x) may readily be accepted 
as compatible with Hq while t2(x) may not. 
h(x) t2(x) 
Figure B.3 Visualising the compatibility oft(x) with H0 
(iv) If a decision on whether or not to reject H0 has to be made, this should certainly 
take into account the advantages of keeping H0, i.e., defining the cut-off point 
in terms of utility. We described in Section 6.2 how this may actually be 
478 
B. Non-Bayesian Theories 
chosen to guarantee a specified significance level, but this is only one possible 
choice, not necessarily the most appropriate in all circumstances. 
We should finally point out that most of the criticisms already made about 
hypothesis testing are equally applicable to significance testing. Similarly,  
criticisms made of confidence intervals typically apply to significance testing, since 
confidence intervals can generally be thought of as consisting of those null values 
which are not rejected under a significance test. 
B.4 COMPARATIVE ISSUES 
B.4.1 Conditional and Unconditional Inference 
At numerous different points of this Appendix we have emphasised the following 
essential difference between Bayesian and frequentist statistics: Bayesian statistics 
directly produces statements about the uncertainty of unknown quantities, either 
parameters or future observations, conditional on known data; frequentist statistics 
produces probability statements about hypothetical repetitions of the data  
conditional on the unknown parameter, and then seeks (indirectly) ways of making this 
relevant to inferences about the unknown parameters given the observed data.  
Indeed, the problem at the very heart of the frequentist approach to statistics is that of 
connecting aggregate, long-run sampling properties under hypothetical repetitions, 
to specific inferences of a totally different type. Not only may one dispute the  
existence of the conceptual "collective" where these hypothetical repetitions might take 
place, but the relevance of the aggregate, long-run properties for specific inference 
problems seems, at best, only tangential. 
It is useful to distinguish between two very different concepts, initial precision 
sad final precision, introduced by Savage (1962). Thus, frequentist procedures are 
designed in terms of their expected behaviour over the sample space; they typically 
have average characteristics which describe, for each value of the unknown  
parameters, the "precision" we may initially expect, before the data are collected. Thus, 
for example, one might expect that, in the long run, the true mean \i will be included 
in 95% of the intervals of the form x ± 1.96/v/n which might be constructed by 
repeated sampling from a normal distribution with known unit precision. 
A far more pertinent question, however, is the following: given the observed 
x which derives from the observed sample (which typically will not be repeated in 
any actual practice), how close is the unknown \i to the observed x? Within the 
frequentist approach, one must rest on the rather dubious "transferred" properties 
of the long-run behaviour of the procedure, with no logical possibility of assessing 
the relevant^na/ precision. 
Thus, p-values or confidence intervals are largely irrelevant once the sample 
has been observed, since they are concerned with events which might have occurred, 
but have not. Indeed, to quote Jeffreys (1939/1961, p. 385) 
B.4 Comparative Issues 
479 
... a hypothesis which may be true may be rejected because it has not predicted 
observable results which have not occurred. This seems a remarkable procedure. 
The following example, taken from Welch (1939) further illustrates the  
difference between initial and final precision. 
Example B.10. (Initial and final precision). Let x = {x\,..., xn} be a random 
sample from a uniform distribution over )fi — \, fi +1 [. It is easily verified that the midrange 
ji = (zmin + Zmax)/2 is a very efficient estimator with a sampling variance of the order of 
1/n2, rather than the usual 1/n, so that, from large samples, we may expect, on average, 
very precise estimates of fi. Suppose however that we obtain a specific large sample with a 
small range; this is, admittedly, unlikely, but nevertheless possible. Given the sample and 
using a uniform (reference) prior for fi, we can only really claim that fi e]xmax — |, xmi„ + 5 [ 
(since the reference posterior distribution is uniform on that interval). Thus, if the actual 
data turn out this way, the final precision of our inferences about fi is bound to be rather 
poor, no matter how efficient the estimator fi was expected to be. 
The need for conditioning on observed data can be partially met in frequentist 
procedures by conditioning on an ancillary statistic. Indeed, we saw in Examples 
B.2 and B.3 that it is easy to construct examples where totally unconditional  
procedures produce ludicrous results. However, as pointed out in our discussion of the 
conditionality principle, there remain many problems with conditioning on  
ancillary statistics; they are not easily identifiable, they are not necessarily unique and, 
moreover, conditioning on an ancillary statistic, can yield a totally uninformative 
sampling distribution, and can conflict with other frequentist desiderata, such as 
the search for maximum power in hypothesis testing; see, for example, Basu (1964, 
1992) and Cox and Reid (1987). See, also, Berger (1984b). 
B.4.2 Nuisance Parameters and Marginalisation 
Most realistic probability models make the sampling distribution dependent not only 
on the unknown quantity of primary interest but also on some other parameters. 
Thus, the full parameter vector 6 can typically be partitioned into 6 = {(/>, A} 
where <p is the subvector of interest and A is the complementary subvector of 6, 
often referred to as the vector of nuisance parameters. 
We recall from Section 5.1 that, within a Bayesian framework, the presence 
of nuisance parameters does not pose any formal, theoretical problems. Indeed, 
the desired result, namely the (marginal) posterior distribution of the parameter of 
interest, can simply be written as 
p(</> I x) = J p(</>, AI x)d\ 
480 
B. Non-Bayesian Theories 
where the full posterior p(</>, A | a;) is directly obtained from Bayes' theorem. 
The situation is very different from a frequentist point of view. Indeed, the 
problem posed by the presence of nuisance parameters is only satisfactorily solved 
within a pure frequentist framework in those few cases where the optimality  
criterion used leads to a procedure which depends on a statistic whose sampling  
distribution does not depend on the nuisance parameter. Frequentist inferences about 
the mean of a normal distribution with unknown variance based on the Student-^ 
statistic, whose sampling distribution does not involve the variance, provides the 
best known example. In general, frequentists are forced to use approximate  
methods, typically based on asymptotic theory. Indeed, some statisticians see this as the 
main motivation for developing asymptotic results: 
a ... serious difficulty is that the techniques ... for problems with nuisance 
parameters are of fairly restricted applicability. It is, therefore, essential to have 
widely applicable procedures that in some sense provide good approximations 
when "exact" solutions are not available. ... the central idea being that when 
the number n of observations is large and errors of estimation correspondingly 
small, simplifications become available that are not available in general (Cox and 
Hinkley, 1974, p. 279, our italics) 
However, even the domain of "fairly restricted applicability" resulting from 
reliance on asymptotic methods can be problematic. In an early paper, Neyman and 
Scott (1948) illustrated such problems by considering models widi many nuisance 
parameters of the type 
n 
p(x\(j),\) = Y[p{xi\(j),Xi), 
t=l 
where a new nuisance parameter A* is introduced with each observation. Note 
that such models are not unrealistic: for example, xt may be a physiological  
measurement on individual i, which may have a normal distribution with mean A* and 
common variance (f>, the latter being the parameter of interest. Kiefer and Wol- 
fowitz (1956) and Cox (1975) proposed solutions for this type of problem based 
on treating the A/s as independent observations from some distribution. From a 
Bayesian viewpoint, this, of course, then becomes a case of hierarchical modelling 
as discussed in Section 4.6.5. 
The only general alternative strategy which has been proposed to avoid  
resorting to asymptotics when exact methods are not available is to use a modified form 
of likelihood, (estimated, conditional, or marginal), where the dependence on the 
nuisance parameters has been reduced or eliminated. 
An estimated likelihood is obtained by replacing nuisance parameters by (for 
example) their maximum likelihood estimates. This procedure does not take  
account of the uncertainty due to the lack of knowledge about the nuisance parameters, 
B.4 Comparative Issues 
481 
and may be misleading both in the precision and in the location associated with 
inferences about the parameters of interest. For example, in linear regression with 
many regressors, substitution of the regression coefficients by their mle's leads to 
an estimate of the variance which is misleadingly precise. 
Marginal and conditional likelihoods are based on breaking the likelihood 
function into two factors, either using invariance arguments or conditioning on 
sufficient statistics for the nuisance parameters. In both cases, one factor provides 
a likelihood function for the parameter of interest while the other is assumed "to 
contain no information about the parameter of interest in the absence of knowledge 
about the nuisance parameter". Key references to this approach are Kalbfleish and 
Sprott (1970, 1973) and Andersen (1970,1973). 
There are however two main problems with this type of approach. 
(i) They are not general and can only be applied under rather specific  
circumstances. 
(ii) They critically depend on the highly controversial notion of a "function not 
containing relevant information in the absence of knowledge about the  
nuisance parameters", for which no operational definition has ever been provided. 
In the cases where the techniques can be applied, and a consensus seems to 
exist about this vague information condition, the resulting forms tend to coincide, 
as one might expect, with the integrated likelihood 
/ p(ac | 0, A)7r(A| 4>)d\, 
integrated with respect to the conditional reference prior distribution n(X \ 4>) of 
the nuisance parameters given the parameter of interest. 
Profile likelihood provides a much more refined version of thi s approach, which 
often gives answers which closely correspond to Bayesian marginalisation results; 
the Fieller-Creasy problem concerning the ratio of normal means provides a typical 
example (see Bernardo, 1977). For further discussion and extensive references, see, 
for example, Barndorff-Nielsen (1983,1991), Cox and Reid (1987), Cox (1988) and 
Fraser and Reid (1989). Another suggestion, closely related to fiducial inference, 
is the implied likelihood (Efron, 1993). 
Liseo (1993) shows that reference posterior credible regions have better fre- 
quentist coverage properties than those obtained from likelihood methods. For a 
Bayesian overview of methods for treating nuisance parameters, see Basu (1977), 
Dawid (1980a), Willing (1988) and Albert (1989). 
482 
B. Non-Bayesian Theories 
B.4.3 Approaches to Prediction 
The general problem of statistical prediction may be described as that of inferring 
the values of unknown observable variables from current available information. 
Thus, from data x, usually a random sample {x\,..., #„}, inference statements 
are desired about, as yet, unobserved data y, often x„+i (the original problem 
considered by Bayes, 1763, in a binomial setting). 
We recall from Section 5.1.3 that, from a Bayesian point of view, with an 
operationalist concern with modelling uncertainty in terms of observables, Bayes' 
theorem, in its central role as a coherent learning process about parameters, is just 
a convenient step in the process of passing from 
P(x)= f f[P(xi\e)P(9)de 
to 
P{y\x)= J p(y\8)p{8\x)d8 
by means of p(8 \ x) ex. p(x \ 8)p(8). Since any valid coherent inferential  
statement about y given x is contained in the posterior predictive distribution p(y \ x), 
no special theory has to be developed. Of course, the inferential content of the 
predictive distribution may be appropriately summarised by location or spread 
measures, respectively providing "estimators" of y, such as the mean and the mode 
of p(y | x), or "interval estimates" of y such as the class of HPD intervals which 
may be derived from p(y \ x). Moreover, if one is faced with a decision problem 
whose utility function u(a, y) involves a future observable, then p(y \ x) becomes 
the necessary ingredient in determining the optimal action, a*, which maximises 
the appropriate (posterior) expected utility 
u{a \x)= u(a, y)p(y \ x) dy. 
The range of potential applications of these ideas is extensive. 
(i) Density estimation. The action space consists of the class of sampling  
distributions; the predictive distribution, which is the posterior expectation of the 
sampling distribution, is, for squared error loss, the optimal estimator of the 
sampling distribution, 
(ii) Calibration. Two observations (xu, %2i) are made on a set of n individuals 
using two different measuring procedures, and it is desired to estimate the 
measurement y2 that the second procedure would yield on a new individual, 
given that the measurement using the first procedure has turned out to be y\. 
The solution is a simple exercise in probability calculus leading to the required 
posterior predictive density p(y2 \ y\, X\, x2). 
B.4 Comparative Issues 
483 
(iii) Classification. This is a particular case of the problem of calibration where 
the £2t's (and y2) can only take on a discrete, usually finite, set of values. 
(iv) Regulation. In contexts analogous to (ii) and (iii), it is desired to select and fix a 
value of yi so that j/2 is as close as possible to a prescribed value. The solution 
is obtained by minimising the expectation of an appropriate loss function with 
respect to the predictive distribution p(j/2 I l/i, £Ci, X2). The particular case 
of optimisation obtains when it is desired to make 1/2 as large (or small) as 
possible. 
(v) Model comparison. In a setting with alternative models, the latter may be 
compared in terms of their predictive posterior probabilities (cf. Section 6.1). 
(vi) Model criticism. The compatibility of a given model with observed data may 
be assessed by comparing the realised value of a test statistic with its predictive 
distribution under that model (cf. Section 6.2). 
For further details of the systematic use of predictive ideas, the reader is 
referred, for example, to Roberts (1965), Geisser (1966, 1974, 1980b, 1988) and 
Zellner (1986b). The books by Aitchison and Dunsmore (1975) and Geisser (1993) 
contain a wealth of detailed discussion of prediction problems, including those  
involving decision making. Applications of predictive ideas to classification,  
calibration, regulation, optimization and smoothing are found, for instance, in Dunsmore 
(1966,1968,1969), Bernardo (1988), Racine-Poon (1988), Klein and Press (1992), 
Lavine and West (1992) and Zidek and Weerahandi (1992). See, also, Gelfand and 
Desu (1968) and Amaral-Turkman and Dunsmore (1985). 
It is important to recall here (see Section 5.1) that, by virtue of the  
representation theorems, parameters are limiting forms of observables and, hence,  
inference about parameters may be seen as a limiting form of predictive inference about 
observables. Although in practice it is usually convenient to work via parametric 
models, this point, stressed by de Finetti (1970/1974, 1970/1975) has considerable 
theoretical importance. Cifarelli and Regazzini (1982), among others, have  
continued this tradition by trying to develop a completely predictive approach which 
bypasses entirely the use of parametric models. 
We should emphasise again that the all too often adopted naive solution of 
prediction based on the "plug-in estimate" form 
p(y \x)= p(y\ 6)p(6 \ x)d0 ~ p(y | 0), 
effectively replacing the posterior distribution by a degenerate distribution assigning 
probability one to an estimator of 0, usually the maximum likelihood estimate, 
is bound to give misleadingly overprecise inference statements about y, since it 
effectively ignores the uncertainty about 6. The point is illustrated in detail by 
Aitchison and Dunsmore (1975). 
484 
B. Non-Bayesian Theories 
By comparison, the possibilities for frequentist-based prediction are fairly  
limited. They are essentially limited to producing tolerance regions, R(x), designed to 
guarantee that, in the long run, a proportion p of possible samples x would produce 
regions R(x) such that 
Pr[y G R{x) \0] = l-a, for all 8 G 6, 
i.e., regions which, for all parameters values, will contain a proportion 1 — a of 
future observations. If this sounds obscure, particularly in comparison with the 
simple idea of an HPD region from the predictive distribution p(y | a;), we can but 
agree! Moreover: 
(i) In order to construct a tolerance region it is essential to find a function of y and 
x with a sampling distribution which does not involve 8, something which is 
typically only possible in very simple stylised problems. 
(ii) The difficulties of "transferring" the long-run aggregate properties of  
confidence intervals into inference statements conditional on the observed data, are 
even more acute in a tolerance region setting. 
Descriptions of the frequentist approach to prediction are given in Cox (1975), 
Mathiasen (1979) and Barndorff-Nielsen (1980); Guttman (1970) provides a  
comparison between frequentist tolerance regions and HPD regions from predictive 
distributions. 
Kalbfleish (1971) was one of the first to examine likelihood methods for  
prediction. Essentially, with t denoting a sufficient statistic for 8, he proposed computing 
a predictive distribution of the form 
p(y\t)= [ P(y\8)f(8\t)d8 
Je 
whenever a fiducial distribution for 8, f(8\t), can be derived from the sampling 
distribution of t. Of course, the method is not always applicable; moreover, even 
when it is, it may lead to inconsistent results when the fiducial distribution is not a 
Bayes' posterior. For instance, in the discussion which follows Kalbfleish's paper, 
Lindley points out that if the model is 
(92 
p{x\8) = -—-{x+l)e~xe, x>O,0>O 
0 + 1 
and the method is applied both to obtain directly p(xn+\ \x1,... ,xn) and to obtain 
p(xn+i,xn | #i,... ,xn-i) and then p(xn+\ \x\,..., xn) from the joint predictive, 
one obtains different answers. This is an interesting example of the fact that fiducial 
distributions do not necessarily have basic coherence properties unless they are 
equivalent to Bayesian posterior distributions. 
B.4 Comparative Issues 
485 
Since the late 1970's a variety of more sophisticated "likelihood prediction" 
methods have been proposed, some sufficiency-based, others relating to profile 
likelihood ideas. Seminal contributions include those by Hinkley (1979), Lejeune 
and Faulkenberry (1982), Butler (1986) and Lane and Sudderth (1989). A review 
is given by Bj0rnstad (1990), and a further overview is provided by Geisser (1993). 
A more radical approach to prediction is set out in Dawid (1984), who sets 
out a theory of prequential analysis. This is closely related to our view that a 
model or theory is simply a probability forecasting system, but Dawid's theory is 
not predicated on such a system necessarily being Bayesian. Instead, the basic 
ingredients are simply two sequences; one a string of observations, the other a 
string of probability forecasts. Theoretical developments requiring an extension 
of the standard Kolmogorov (1933/1950) framework for probability are pursued 
in Vovk (1993a). See, also, Vovk (1993b) and Vovk and Vyugin (1993). Links 
with stochastic complexity (Solomonoff, 1978; Rissanen, 1987,1989; Wallace and 
Freeman, 1987) are reviewed in Dawid (1992). 
B.4.4 Aspects of Asymptotics 
In most statistical problems, a number of simplifications become available when 
the sample size becomes sufficiently large. In frequentist statistics, this is often 
the only way to obtain analytic results. From a Bayesian point of view, such 
simplifications are never theoretically necessary, although, of course, they often 
make computations easier and sometimes provide valuable analytic insight. 
We recall from Section 5.3 that, as the sample size increases, the posterior 
distribution of the parameter of interest 8 converges to a degenerate distribution 
which gives probability one to the true parameter value when the parameter of 
interest is discrete and, under suitable regularity conditions when the parameter of 
interest is continuous, converges to a normal distribution N(018n, H(8n)), with 
precision matrix 
H(8 ^ - ( d2\ogp(x\8)\ 
u^n>-{ mm, ) 8=8n 
The most frequently used asymptotic results in frequentist statistics concern 
the large sample behaviour of the maximum likelihood estimate 8n which,  
under suitable regularity conditions (mathematically usually closely related to those 
required to guarantee posterior asymptotic normality), may be shown to have an 
asymptotically normal sampling distribution N(0„ | 8, nl(8)), with precision  
matrix whose general element is 
For details, see, for example, LeCam (1956, 1970, 1986), and references therein. 
486 
B. Non-Bayesian Theories 
Since it is easily established that, for large n, H(8„) converges to nl{9), and 
since, asymptotically, the sampling distribution of 8n becomes a location model for 
8, it follows (Lindley, 1958) that the reference posterior distribution for 8 and the 
asymptotic fiducial distribution of 8 based on the sampling distribution of 8n are 
asymptotically equivalent. Moreover, the maximum likelihood estimator of 8 and 
the asymptotic confidence intervals based on 8n will be, respectively, numerically 
identical to the mode (or the mean) and the HPD intervals based on the reference 
posterior distribution (or any other posterior distribution based on a reasonably 
well-behaved prior). 
These results explain the fact that, for large samples (relative to the  
dimensionality of the parametric model component) there are typically very few numerical  
differences between Bayesian inferential statements and frequentist statements based 
on asymptotic properties. This asymptotic equivalence carries over, of course, to a 
number of applications. For example: 
(i) We showed (Corollary 2 to Proposition 5.17) that if 6 is asymptotically  
normal N(6\6n,L'l(6n)) then, under appropriate regularity conditions g(6) is 
asymptotically normal 
N(s(«)i4).m)[9'(i)r'). 
The frequentist equivalent (typically derived using the delta method for  
determining the asymptotic distribution of an estimator) is that if 9n has an  
asymptotic sampling distribution N(#„ 16, nl(9)), then g(9) has an asymptotic 
sampling distribution 
X(g(en)\g(6),nI(0)W(e)}-2). 
(ii) The predictive distribution p(y \ x) is asymptotically approached by p(y \8n). 
(iii) The action which maximises the posterior expected utility is, asymptotically, 
the same as that which maximises u(a, 8n). 
In the fictional world of unlimited data, numerical differences between  
frequentist and Bayesian solutions would tend to disappear with increasing sample 
size although, even then, differences in interpretation would persist. However, in 
the real world of limited data relative to the (often multiparameter) models required 
for realism, there is no reason to expect, in general, close coincidence of numerical 
solutions. 
B.4.5 Model Choice Criteria 
We have discussed earlier in Sections B.3.3 and B.3.4, the hypothesis and  
significance testing approaches to parametric hypotheses, but have noted that, in general, 
B.4 Comparative Issues 
487 
no satisfactory exact procedures exist. This may be because of a lack of  
simplification via sufficiency or invariance arguments, resulting in intractable distributions, 
or because a procedure cannot be found which has uniformly optimal properties 
through the range of parameter values under the alternative hypothesis. 
A procedure frequently adopted in such situations is the so-called general 
maximum likelihood ratio test, which we describe first for the case of a simple null 
hypothesis, 8 = 80 G 3?* and observations x = (x1,..., xn). The procedure is 
motivated by considering the ratio 
p{x\80) 
r(x) = ———~ > 
p(x\8) 
where 8 is the maximum likelihood estimate. Intuitively, small values of r(x) 
suggest rejection of the null hypothesis, but using this type of test requires deriving 
the distribution of r(x), which is, in general, not possible. However, a simple 
asymptotic argument (see, for example, Cox and Hinkley, 1974, Section 9.3) reveals 
that, for suitable regularity conditions, under the null hypothesis, as n —► oo, 
A(a;) = — 21ogr(a;) has a limiting \\ distribution. 
The procedure is easily extended to the case of a composite null hypothesis 
8 G ©o C 5t*. If the alternative hypothesis is 8 G ©i, and we define© = ©oU©i, 
we consider the ratio 
, , suPeee0P{x\8) 
sup„6e p(x\8) 
In this case, asymptotic analysis reveals that, for suitable regularity conditions, 
A(a;) = -21ogr(x) has a limiting xl distribution, where d is the difference in 
dimensionality, dim(0) — dim(©0), of the general and null hypothesis parameter 
spaces, respectively. 
It is interesting to compare this with a widely used Bayesian form of assessment 
of null and alternative models. Schwarz (1978) shows that, asymptotically, 
—21ogi?oi = A(x) — dlogn, 
where B0i is the Bayes factor (Section 6.1.4). 
We see, therefore, that the so-called Schwarz criterion for model choice adjusts 
the -2 log r(x) factor by a log n multiple of the dimensionality difference. 
An earlier proposal for adjusting the general likelihood ratio criterion is that of 
Akaike (1973, 1974, 1978b, 1987), whose so-called Akaike Information Criterion 
(AIC) takes the form 
AIC = A (a?) - 2d. 
See also, Akaike (1978b, 1979) for a Bayesian extension (BIC) of the AIC  
procedure. 
488 
B. Non-Bayesian Theories 
Yet another variant is found in Nelder and Wedderburn (1972), whose  
suggestion for goodness-of-fit comparisons of general linear models through plotting 
degrees of freedom against deviance is, in effect, the criterion 
\{x) - d. 
These and other related proposals are reviewed from a Bayesian perspective in 
Smith and Spiegelhalter (1980). See Stone (1977, 1979a) for further discussion 
and comparison. 
Roughly speaking, the Akaike criterion can be derived from a Bayes factor 
perspective as corresponding to a prior which concentrates on a neighbourhood of 
the alternative which is close, in an appropriate sense depending on n, to the null. 
The Schwarz criterion is derived from a Bayes factor perspective through a prior 
which does not depend on n. 
Finally, we note that the prequential theory of Dawid (1984)—see, also,  
Section B4.3—directly embraces the view that models are simply predictive tools and 
should be compared on mat basis, but does not necessarily use a Bayesian  
mechanism for such prediction. In Dawid (1992), it is shown that a particular form of 
so-called prequential assessment, based on the logarithmic scoring rule, leads to 
a model choice criterion which is asymptotically equivalent to the Schwarz  
criterion. It is also shown that this approach is essentially equivalent to model choice 
procedures arising in the stochastic complexity theory of Rissanen (1987). 
489 
References 
Abramowitz, M. and Stegun, I. A. (1964). Handbook of Mathematical Functions. New York: 
Dover. 
Achcar, J. A. and Smith, A. F. M. (1989). Aspects of reparameterisation in approximate 
Bayesian inference. Bayesian and Likelihood Methods in Statistics and Econometrics: 
Essays in Honor of George A. Barnard(S. Geisser, J. S. Hodges, S. J. Press and A. Zellner, 
eds.). Amsterdam: North-Holland, 439-452. 
Aczel, J. and Pfanzagl, J. (1966). Remarks on the measurement of subjective probability and 
information. Metrika 11, 91-105. 
Aitchison, J. (1964). Bayesian tolerance regions. J. Roy. Statist. Soc. B 26,161-175. 
Aitchison, J. (1966). Expected cover and linear utility tolerance intervals. J. Roy. Statist. 
Soc. B 28,57-62. 
Aitchison, J. (1968). In discussion of Dempster (1968). J. Roy. Statist. Soc. B 30, 234-237. 
Aitchison, J. (1970). Choice against chance. An Introduction to Statistical Decision Theory. 
Reading, MA: Addison-Wesley. 
Aitchison, J. and Dunsmore, I. R. (1975). Statistical Prediction Analysis. Cambridge:  
University Press. 
Aitken, C. G. G. and Stoney, D. A. (1991). The Use of Statistics in Forensic Science.  
Chichester: Ellis Horwood. 
Aitkin, M. (1991). Posterior Bayes factors. J. Roy. Statist. Soc. B 53,111-142 (with  
discussion). 
Akaike, H. (1973). Information theory and an extension of the maximum likelihood principle. 
2nd. Int. Symp. Information Theory. Budapest: Akademia Kaido, 267-281. 
490 
References 
Akaike, H. (1974). A new look at the statistical model identification. IEEE Trans. Automatic 
Control 19, 716-727. 
Akaike, H. (1978a). A new look at the Bayes procedure. Biometrika 65, 53-59. 
Akaike, H. (1978b). A Bayesian analysis of the minimum AIC procedure. Ann. Inst. Statist. 
Math. 30,9-14. 
Akaike, H. (1979). A Bayesian extension of the minimum AIC procedure of autoregressive 
model fifting. Biometrika 66, 53-59. 
Akaike, H. (1980a). The interpretation of improper prior distributions as limits of data 
dependent proper prior distributions. J. Roy. Statist. Soc. B 45, 46-52. 
Akaike, H. (1980b). Likelihood and the Bayes procedure. Bayesian Statistics (J. M.  
Bernardo, M. H. DeGroot, D. V. Lindley and A. F. M. Smith, eds.). Valencia: University Press, 
144-166 and 185-103 (with discussion). 
Akaike, H. (1987). Factor analysis and the AIC. Psychometrika 52, 317-332. 
Albert, J. H. (1989). Nuisance parameters and the use of exploratory graphical methods in 
Bayesian analysis. Amer. Statist. 43,191-196. 
Albert, J. H. (1990). Algorithms for Bayesian computing using Mathematica. Computing 
Science and Statistics: Proceedings of the Symposium on the Interface (C. Page and R. 
LePage eds.). Berlin: Springer, 286-290. 
Albert, J. H. (1993). Teaching Bayesian statistics using sampling methods and MINITAB. 
Amer. Statist. 47,182-191. 
Aldous, D. (1985). Exchangeability and Related Topics. Berlin: Springer. 
Allais, M. (1953). Le comportement de l'homme rational devant le risque: Critique des 
postulats et axiomes de l'ecole Am6ricaine. Econometrica 21,503-546. 
Allais, M. and Hagen, D. (1979). Expected Utility Hypotheses and the Allais Paradox. 
Dordrecht: Reidel. 
Amaral-Turkman, M. A. and Dunsmore, I. R. (1985). Measures of information in the  
predictive distribution. Bayesian Statistics 2 (J. M. Bernardo, M. H. DeGroot, D. V. Lindley 
and A. F M. Smith, eds.), Amsterdam: North-Holland, 603-612. 
Ameen, J. R. M. (1992). Non linear prediction models. J. Forecasting 11, 309-324. 
Amster, S. J. (1963). A modified Bayes stopping rule. Ann. Math. Statist. 34,1404-1413. 
Andersen, E. B. (1970). Asymptotic properties of conditional maximum-likelihood  
estimators. J. Roy. Statist. Soc. B 32, 283-301. 
Andersen, E. B. (1973). Conditional Inference and Models for Measuring. Copenhagen: 
Mental Hygiejnisk Foslay. 
Anderson, T. W. (1984). An Introduction to Multivariate Statistical Analysis. New York: 
Wiley. 
Angers, J.-F and Berger, J. O. (1991). Robust hierarchical Bayes estimation of exchangeable 
means. Canadian J. Statist. 19, 39-56. 
Anscombe, F. J. (1961). Bayesian statistics. Amer. Statist. 15, 21-24. 
Anscombe, F. J. (1963). Sequential medical trials. J. Amer. Statist. Assoc. 58, 365-383. 
Anscombe, F J. (1964a). Some remarks on Bayesian statistics. Human Judgement and 
Optimality (Shelly and Bryan, eds.). New York: Wiley, 155-177. 
Anscombe, F. J. (1964b). Normal likelihood functions. Ann. Inst. Statist. Math. 16, 1-41. 
References 
491 
Anscombe, F. J. and Aumann, R. J. (1963). A definition of subjective probability. Ann. Math. 
Statist. 34,199-205. 
Ansley, C. F, Kohn, R. and Wong, C.-M. (1993). Non-parametric spline regression with 
prior information. Biometrika 80, 75-88. 
Antoniak, C. (1974). Mixtures of Dirichlet processes with applications to Bayesian nonpara- 
metric problems. Ann. Statist. 2,1152-1174. 
Aoki, M. (1967). Optimization of Stochastic Systems. New York: Academic Press. 
Arimoto, S. (1970). Bayesian decision rule and quantity of equivocation. Systems,  
Computers, Controls 1,17-23. 
Arnaiz, G. and Rufz-Rivas, C. (1986). Outliers in circular data, a Bayesian approach. 
Questiio 10,1-6. 
Arnold, S. F (1993). Gibbs sampling. Handbook of Statistics 9. Computational Statistics 
(C. R. Rao, ed.). Amsterdam: North-Holland, 599-625. 
Arrow, K. J. (1951a). Alternative approaches to theory of choice in risk-taking situations. 
Econometrica 19, 404-437. 
Arrow, K. J. (1951b). Social Choice and Individual Values. New York: Wiley 
Arrow, K. J. and Raynaud, H. (1987). Social Choice and Multicriteria Decision Making. 
Cambridge, MA: The MIT Press 
Ash, R. B. (1972). Real Analysis and Probability. New York: Academic Press. 
Aumann, R. J. (1987). Correlated equilibrium as an expression of Bayesian rationality. 
Econometrica 55,1-18. 
Aykac, A. and Brumat, C. (eds.) (1977). New Developments in the Applications of Bayesian 
Methods. Amsterdam: North-Holland. 
Bahadur, R. R. (1954). Sufficiency and statistical decision functions. Ann. Math. Statist. 25, 
423-462. 
Bailey, R. W (1992). Distributional identities of Beta and chi-squared variates:a geometrical 
interpretation. Amer. Statist. 46,117-120. 
Balch, M. and Fishburn, P. C. (1974). Subjective expected utility for conditional primitives. 
Essays on Economic Behaviour under Uncertainty (M. Balch, D. McFadden and S. Wu, 
eds.). Amsterdam: North-Holland, 45-54. 
Bandemer, H. (1977). Theorie und Anwendung der Optimalen Versuchsplanung. Berlin: 
Akademie-Verlag. 
Barlow, R. E. (1989). Influence diagrams. Encyclopedia of Statistical Sciences Suppl. 
(S. Kotz, N. L. Johnson and C. B. Read, eds.). New York: Wiley, 72-74. 
Barlow, R. E. (1991). Introduction to de Finetti (1937). Breakthroughs in Statistics 1 (S. Kotz 
and N. L. Johnson, eds.). Berlin: Springer, 125-133. 
Barlow, R. E. and Irony, T. Z. (1992). Foundations of statistical quality control. Current 
Issues in Statistical Inference: Essays in Honor ofD. Basu. (M. Ghosh and P. K. Pathak 
eds.). Hayward, CA: IMS, 99-112. 
Barlow, R. E. and Mendel, M. B. (1992). De Finetti-type representations for lifetime  
distributions. J. Amer. Statist. Assoc. 87,1116-1123. 
Barlow, R. E. and Mendel, M. B. (1994). The operational Bayesian approach. Aspects of 
Uncertainty: a Tribute to D. V. Lindley (P. R. Freeman, and A. F M. Smith, eds.). 
Chichester: Wiley, (to appear). 
492 
References 
Barlow, R. E., Wechsler, S. and Spizzichino, F. (1988). De Finetti's approach to group 
decision making. Bayesian Statistics 3 (J. M. Bernardo, M. H. DeGroot, D. V. Lindley 
and A. F. M. Smith, eds.). Oxford: University Press, 1-15 (with discussion). 
Barnard, G. A. (1949). Statistical inference. J. Roy. Statist. Soc. B 11, 115-149 (with  
discussion). 
Barnard, G. A. (1951). The theory of information. J. Roy. Statist. Soc. B 13, 46-64. 
Barnard, G. A. (1952). The frequency justification of certain sequential tests. Biometrika 39, 
155-150. 
Barnard, G. A. (1958). Thomas Bayes, a biographical note. Biometrika 45, 293-295. 
Barnard, G. A. (1963). Some aspects of the fiducial argument. J. Roy. Statist. Soc. B 25, 
111-114. 
Barnard, G. A. (1967). The use of the likelihood function in statistical practice. Proc. Fifth 
Berkeley Symp. 1 (J. Neyman and E. L. Scott, eds.). Berkeley: Univ. California Press, 
27-40. 
Barnard, G. A. (1980a). In discussion of Box (1980). J. Roy. Statist. Soc. A 143, 404-406. 
Barnard, G. A. (1980b). Pivotal inference and the Bayesian controversy. Bayesian Statistics 
(J. M. Bernardo, M. H. DeGroot, D. V. Lindley and A. F. M. Smith, eds.). Valencia: 
University Press, 295-318 (with discussion). 
Barnard, G. A., Jenkins, G. M. and Winsten, C. B. (1962). Likelihood inference and time 
series. J. Roy. Statist. Soc. A 125, 321-372 (with discussion). 
Barnard, G. A. and Sprott, D. A. (1968). Likelihood. Encyclopedia of Statistical Sciences 9 
(S. Kotz, N. L. Johnson and C. B. Read, eds.). New York: Wiley, 639-644. 
Barndorff-Nielsen, O. E. (1978). Information and Exponential Families in Statistical Theory. 
New York: Wiley. 
Barndorff-Nielsen, O. E. (1980). Likelihood prediction. Symposium Mathematica 25,11-24. 
Barndorff-Nielsen, O. E. (1983). On a formula for the distribution of the maximum likelihood 
estimator. Biometrika 70, 343-365. 
Barndorff-Nielsen, O. E. (1991). Likelihood theory. Statistical Theory and Modelling. In 
Honour of Sir David Cox (D. V. Hinkley, N. Reid and E. J. Snell, eds.). London: Chapman 
and Hall, 232-265. 
Barnett, V. (1973/1982). Comparative Statistical Inference. Second edition in 1982,  
Chichester: Wiley. 
Barrai, I., Coletti, G. and Di Bacco, M. (eds.) (1992). Probability and Bayesian Statistics in 
Medicine and Biology. Pisa: Giardini. 
Bartholomew, D. J. (1965). A comparison of some Bayesian and frequentist inferences. 
Biometrika 52,19-35. 
Bartholomew, D. J. (1967). Hypothesis testing when the sample size is treated as a random 
variable. J. Roy. Statist. Soc. B 29, 53-82. 
Bartholomew, D. J. (1971). A comparison of Bayesian and frequentist approaches to  
inferences with prior knowledge. Foundations of Statistical Inference (V. P. Godambe and 
D. A. Sprott, eds.). Toronto: Holt, Rinehart and Winston, 417-434 (with discussion). 
Bartholomew, D. J. (1994). Bayes theorem in latent variable modelling. Aspects of  
Uncertainty: a Tribute to D. V. Lindley (P. R. Freeman, and A. F. M. Smith, eds.). Chichester: 
Wiley, (to appear). 
References 
493 
Bartlett, M. (1957). A comment on D. V. Lindley's statistical paradox. Biometrika 44, 533- 
534. 
Basu, D. (1959). The family of ancillary statistics. SankhyaA 21, 247-256. 
Basu, D. (1964). Recovery of ancillary information, SankhyaA 26, 3-16. 
Basu, D. (1969). Role of sufficiency and likelihood principles in survey sampling theory. 
SankhyaA 31, 441-454. 
Basu, D. (1971). An essay on the logical foundations of survey sampling. Foundations of 
Statistical Inference (V. P. Godambe and D. A. Sprott, eds.). Toronto: Holt, Rinehart and 
Winston, 203-242 (with discussion). 
Basu, D. (1975). Statistical information and likelihood. SankhyaA 37,1-71 (with discussion). 
Basu, D. (1977). On the elimination of nuisance parameters. J. Amer. Statist. Assoc. 72, 
355-366. 
Basu, D. (1988). Statistical Information and Likelihood: a Collection of Critical Essays (J. 
K. Ghosh, ed.). Berlin: Springer. 
Basu, D. (1992). Learning statistics from counter examples: ancillary statistics. Bayesian 
Analysis in Statistics and Econometrics (P. K. Goel and N. S. Iyengar, eds.). Berlin: 
Springer, 217-224. 
Basu, D. and Pereira, C. (1983). A note on Blackwell sufficiency and a Skibinsky  
characterization of distributions. SankhyaA 45, 99-104. 
Bauwens, L. (1984). Bayesian Full Information Analysis of Simultaneous Equation Models 
Using Integration by Monte Carlo. Berlin: Springer 
Bayarri, M. J. (1981). Inferencia Bayesiana sobre el coeficiente de correlaci6n de una 
poblaci6n normal bivariante. Trab. Estadist. 32, 18-31. 
Bayarri, M. J. and Berger, J. O. (1994). Applications and limitations of robust Bayesian 
bounds and type IIMLE. Statistical Decision Theory and Related Topics V(S. S. Gupta 
and J. O. Berger, eds.). Berlin: Springer, (to appear). 
Bayarri, M. J. and DeGroot, M. H. (1987). Bayesian analysis of selection models. The 
Statistician 36,137-146. 
Bayarri, M. J. and DeGroot, M. H. (1988). Gaining weight: a Bayesian approach. Bayesian 
Statistics 3 (J. M. Bernardo, M. H. DeGroot, D. V. Lindley and A. F. M. Smith, eds.). 
Oxford: University Press, 25-44 (with discussion). 
Bayarri, M. J. and DeGroot, M. H. (1989). Optimal reporting of predictions. J. Amer. Statist. 
Assoc. 84,214-222. 
Bayarri, M. J. and DeGroot, M. H. (1990). Selection models and selection mechanisms. 
Bayesian and Likelihood Methods in Statistics and Econometrics: Essays in Honor of 
George A. Barnard(S. Geisser, J. S. Hodges, S. J. Press and A. Zellner, eds.). Amsterdam: 
North-Holland, 211-227. 
Bayarri, M. J. and DeGroot, M. H. (1991). What Bayesians expect of each other. J. Amer. 
Statist. Assoc. 86,924-932. 
Bayarri, M. J. and DeGroot, M. H. (1992a). A 'BAD' view of weighted distributions and 
selection models. Bayesian Statistics 4 (J. M. Bernardo, J. O. Berger, A. P. Dawid and 
A. F. M. Smith, eds.). Oxford: University Press, 17-33 (with discussion). 
Bayarri, M. J. and DeGroot, M. H. (1992b). Difficulties and ambiguities in the definition of 
a likelihood function. J. It. Statist. Soc. 1,1-15. 
494 
References 
Bayarri, M. J., DeGroot, M. H. and Kadane, J. B. (1988). What is the likelihood function? 
Statistical Decision Theory and Related Topics IV1 (S. S. Gupta and J. O. Berger, eds.). 
Berlin: Springer, 3-27. 
Bayes, T. (1763). An essay towards solving a problem in the doctrine of chances.  
Published posthumously in Phil. Trans. Roy. Soc. London 53, 370-418 and 54, 296-325. 
Reprinted in Biometrika 45 (1958), 293-315, with abiographical note by G. A. Barnard. 
Reproduced in Press (1989), 185-217. 
Becker, G. M., DeGroot, M. H. and Marschak, J. (1963). Stochastic models of choice  
behavior. Behavioral Sci. 8,41-55. Reprinted in Decision Making (W. L. Edwards and A. 
Tversky, eds.) Baltimore: Penguin. 
Becker, G. M. and McClintock, C. G. (1967). Value: Behavioral decision theory. Annual 
Rev. Psychology 18,239-286. 
Bellman, R. E. (1957). Dynamic Programming. Princeton: University Press. 
Berger, J. O. (1979). Multivariate estimation with nonsymmetric loss functions. Optimizing 
Methods in Statistics (J. S. Rustagi, ed.). New York: Academic Press. 
Berger, J. O. (1980). A robust generalized Bayes estimator and confidence region for a 
multivariate normal mean. Ann. Statist. 8, 716-761. 
Berger, J. O. (1982). Bayesian robustness and the Stein effect. J. Amer. Statist. Assoc. 77, 
358-368. 
Berger, J. O. (1984a). The robust Bayesian viewpoint. Robustness of Bayesian Analysis 
(J. B. Kadane, ed.). Amsterdam: North-Holland, 63-144 (with discussion). 
Berger, J. O. (1984b). The frequentist viewpoint and conditioning. Proc. Berkeley Symp. 
in Honor ofKiefer and Neyman (L. LeCam and R. Olshen, eds.). Pacific Drove, CA: 
Wadsworth. 
Berger, J. O. (1985a). Statistical Decision Theory and Bayesian Analysis. Berlin: Springer. 
Berger, J. O. (1985b). In defense of the likelihood principle: axiomatics and coherence. 
Bayesian Statistics 2 (J. M. Bernardo, M. H. DeGroot, D. V. Lindley and A. F. M. Smith, 
eds.), Amsterdam: North-Holland, 33-65, (with discussion). 
Berger, J. O. (1986). Bayesian salesmanship. Bayesian Inference and Decision Techniques: 
Essays in Honor of Bruno de Finetti (P. K. Goel and A. Zellner, eds.). Amsterdam: 
North-Holland, 473-488. 
Berger, J. O. (1990). Robust Bayesian analysis: sensitivity to the prior. J. Statist. Planning 
and Inference 25, 303-328. 
Berger, J. O. (1993). The present and future of Bayesian multivariate analysis. Multivariate 
Analysis: Future Directions (C. R. Rao, ed.). Amsterdam: North-Holland, 25-53. 
Berger, J. O. (1994). A review of recent developments in robust Bayesian analysis. Test 3, 
(to appear, with discussion). 
Berger, J. O. and Berliner, L. M. (1986). Robust Bayes and empirical Bayes analysis with 
e-contaminated priors. Ann. Statist. 14, 461-486. 
Berger, J. O. and Bernardo, J. M. (1989). Estimating a product of means: Bayesian analysis 
with reference priors. J. Amer. Statist. Assoc. 84,200-207. 
Berger, J. O. and Bernardo, J. M. (1992a). Ordered group reference priors with applications 
to a multinomial problem. Biometrika 79, 25-37. 
References 
495 
Berger, J. O. and Bernardo, J. M. (1992b). Reference priors in a variance components 
problem. Bayesian Analysis in Statistics and Econometrics (P. K.GoelandN. S. Iyengar, 
eds.). Berlin: Springer, 323-340. 
Berger, J. O. and Bernardo, J. M. (1992c). On the development of reference priors. Bayesian 
Statistics 4 (J. M. Bernardo, J. O. Berger, A. P. Dawid and A. F. M. Smith, eds.). Oxford: 
University Press, 35-60 (with discussion). 
Berger, J. O., Bernardo, J. M. and Mendoza, M. (1989). On priors that maximize expected 
information. Recent Developments in Statistics and their Applications (J. P. Klein and 
J. C. Lee, eds.). Seoul: Freedom Academy, 1-20. 
Berger, J. O. and Berry, D. A. (1988). The relevance of stopping rules in statistical inference. 
Statistical Decision Theory and Related Topics IV1 (S. S. Gupta and J. O. Berger, eds.). 
Berlin: Springer, 29-72 (with discussion). 
Berger, J. O. and DasGupta, A. (1991). Multivariate Estimation, Bayes, Empirical Bayes 
and Stein Approaches. Philadelphia, PA: SIAM. 
Berger, J. O. and Delampady, M. (1987). Testing precise hypotheses. Statist. Sci. 2,317-352 
(with discussion). 
Berger, J. O. and Fan, T. H. (1991). Behaviour of the posterior distribution and inferences 
for a normal mean with t prior distributions. Statistics and Decisions 10,99-120. 
Berger, J. O. and Jefferys, W. H. (1992) The application of robust Bayesian analysis to 
hypothesis testing and Occam's razor. J. It. Statist. Soc. 1, 17-32. 
Berger, J. O. and Mortera, J. (1991a). Interpreting the stars in precise hypothesis testing. 
Internat. Statist. Rev. 59, 337-353. 
Berger, J. O. and Mortera, J. (1991b). Bayesian analysis with limited communication. J.  
Statist. Planning and Inference 28,1-24. 
Berger, J. O. and Mortera, J. (1994). Robust Bayesian hypothesis testing in the presence of 
nuisance parameters. Canadian J. Statist. 31, (to appear). 
Berger, J. O. and O'Hagan A. (1988). Ranges of posterior probabilities of unimodal  
priors with specified quantiles. Bayesian Statistics 3 (J. M. Bernardo, M. H. DeGroot, 
D. V. Lindley and A. F. M. Smith, eds.). Oxford: University Press, 45-65 (with  
discussion). 
Berger, J. O. and Robert, C. P. (1990). Subjective hierarchical Bayes estimation of a  
multivariate normal mean: on the frequentist interface. Ann. Statist. 18, 617-651. 
Berger, J. O. and Sellke, T. (1987). Testing a point null hypothesis: the irreconcilability of 
significance levels and evidence. J. Amer. Statist. Assoc. 82,112-133 (with discussion). 
Berger, J. O. and Srinivasan, C. (1978). Generalized Bayes estimators in multivariate  
problems. Ann. Statist. 6, 783-801. 
Berger, J. O. and Wolpert, R. L. (1984/1988). The Likelihood Principle. Second edition in 
1988, Hayward, CA: IMS. 
Berger, R. L. (1981). A necessary and sufficient condition for reaching a consensus using 
DeGroot's method. J. Amer. Statist. Assoc. 76,415-418. 
Berk, R. H. (1966). Limiting behaviour of the posterior distributions when the model is 
incorrect. Ann. Math. Statist. 37, 51-58. 
Berk, R. H. (1970). Consistency a posteriori. Ann. Math. Statist. 41, 894-906. 
Berliner, L. M. (1987). Bayesian control in mixture models. Technometrics 29,455-460. 
496 
References 
Berliner, L. M. and Goel P. K. (1990). Incorporating partial prior information: ranges of 
posterior probabilities. Bayesian and Likelihood Methods in Statistics and Econometrics: 
Essays in Honor of George A. Barnard(S. Geisser, J. S. Hodges, S. J. Press and A. Zellner, 
eds.). Amsterdam: North-Holland, 397^*06. 
Berliner, L. M. and Hill, B. M. (1988). Bayesian non-parametric survival analysis. J. Amer. 
Statist. Assoc. 83, 772-782 (with discussion). 
Bermudez. J. D. (1985). On the asymptotic normality of the posterior distribution of the 
logistic classification model. Statistics and Decisions 2, 301-308. 
Bernardo, J. M. (1977). Inferences about the ratio of normal means: a Bayesian approach to 
the Fieller-Creasy problem. Recent Developments in Statistics (J. R. Barra et al. eds.). 
Amsterdam: North-Holland, 345-349. 
Bernardo, J. M. (1978a). Una medida de la informaci6n util proporcionada por un experi- 
mento. Rev. Acad. Ciencias Madrid 72, 419-440. 
Bernardo, J. M. (1978b). Unacceptable implications of the left Haar measure in a standard 
normal theory inference problem Trab. Estadist. 29, 3-9. 
Bernardo, J. M. (1979a). Expected information as expected utility. Ann. Statist. 7,686-690. 
Bernardo, J. M. (1979b). Reference posterior distributions for Bayesian inference. J. Roy. 
Statist. Soc. B 41,113-147 (with discussion). 
Bernardo, J. M. (1980). A Bayesian analysis of classical hypothesis testing. Bayesian  
Statistics (J. M. Bernardo, M. H. DeGroot, D. V. Lindley and A. F. M. Smith, eds.). Valencia: 
University Press, 605-647 (with discussion). 
Bernardo, J. M. (1981a). Reference decisions. Symposia Mathematica 25, 85-94. 
Bernardo, J. M. (1981b). Bioestadistica, una Perspectiva Bayesiana. Barcelona: Vicens- 
Vives. 
Bernardo, J. M. (1982). Contraste de modelos probabilisticos desde una perspectiva  
Bayesiana. Trab. Estadist. 33,16-30. 
Bernardo, J. M. (1984). Monitoring the 1982 Spanish socialist victory: a Bayesian analysis. 
J. Amer. Statist. Assoc. 79, 510-515. 
Bernardo, J. M. (1985a). Andlisis Bayesiano de los contrastes de hip6tesis param&ricos. 
Trab. Estadist. 36,45-54. 
Bernardo, J. M. (1985b). On a famous problem of induction. Trab. Estadist. 36,24-30. 
Bernardo, J. M. (1988). Bayesian linear probabilistic classification. Statistical Decision 
Theory and Related Topics IV 1 (S. S. Gupta and J. O. Berger, eds.). Berlin: Springer, 
151-162. 
Bernardo, J. M. (1989). Analisis de datos y m&odos Bayesianos. Historia de la Ciencia 
Estadistica (S. Rfos, ed.). Madrid: Academia de Ciencias, 87-105. 
Bernardo, J. M. (1994). Optimal prediction with hierarchical models: Bayesian clustering. 
Aspects of Uncertainty: a Tribute to D. V. Lindley (P. R. Freeman, and A. F. M. Smith, 
eds.). Chichester: Wiley, (to appear). 
Bernardo, J. M. and Bayarri, M. J. (1985). Bayesian model criticism. Model Choice (J.- 
P. Florens, M. Mouchart, J.-P. Raoult and L. Simar, eds.). Brussels: Pub. Fac. Univ. Saint 
Louis, 43-59. 
Bernardo, J. M., Berger, J. O., Dawid, A. P. and Smith, A. F. M. (eds.) (1992). Bayesian 
Statistics 4. Oxford: University Press. 
References 
497 
Bernardo, J. M. and Bermudez, J. D. (1985). The choice of variables in probabilistic  
classification. Bayesian Statistics 2 (J. M. Bernardo, M. H. DeGroot, D. V. Lindley and 
A. F. M. Smith, eds.), Amsterdam: North-Holland, 67-81 (with discussion). 
Bernardo, J. M., DeGroot, M. H., Lindley, D. V. and Smith, A. F. M. (eds.) (1980). Bayesian 
Statistics. Valencia: University Press. 
Bernardo, J. M, DeGroot, M. H., Lindley, D. V. and Smith, A. F. M. (eds.) (1985). Bayesian 
Statistics 2. Amsterdam: North-Holland. 
Bernardo, J. M., DeGroot, M. H., Lindley, D. V. and Smith, A. F. M. (eds.) (1988). Bayesian 
Statistics 3. Oxford: University Press. 
Bernardo, J. M., Ferrdndiz, J. R. and Smith, A. F. M. (1985). The foundations of decision 
theory: an intuitive, operational approach with mathematical extensions. Theory and 
Decision 18,127-150. 
Bernardo, J. M. and Gir6n F. J. (1988). A Bayesian analysis of simple mixture problems. 
Bayesian Statistics 3 (J. M. Bernardo, M. H. DeGroot, D. V. Lindley and A. F M. Smith, 
eds.). Oxford: University Press, 67-88 (with discussion). 
Bernardo, J. M. and Gir6n F. J. (1989). A Bayesian approach to cluster analysis. Qiiestiio 5, 
97-112. 
Bernoulli, D. (1730/1954). Specimen theoriae novae de mensura sortis. Comment. Acad. Sci. 
Imp. Petropolitanae 5, 175-192. English translation as "Exposition of a new theory on 
the measurement of risk" in 1954, Econometrica 22, 23-26. 
Bernoulli, J. (1713/1899). Ars Conjectandi. Basel: Thumisiorum. Translated into German 
as Wahrscheinlichkeitsrechnung. Leipzig: Engelmann. 1899. 
Berry, D. A. (1994). Basic Statistics. Belmont, CA: Duxbury. (to appear). 
Berry, D. A. and StangI, D. K. (eds.) (1994). Bayesian Biostatistics. New York: Marcel 
Dekker. (to appear). 
Besag, J. (1986). Statistical analysis of dirty pictures. J. Roy. Statist. Soc. B 48, 259-302 
(with discussion). 
Besag, J. (1989). Towards Bayesian image analysis. J. Appl. Statist. 16, 395-407. 
Besag, J. and Green, P. J. (1993). Spatial statistics and Bayesian computation. J. Roy. Statist. 
Soc. B 55, 25-37. 
Bickel, P. J. and Blackwell, D. (1967). A note on Bayes estimates. Ann. Math. Statist. 38, 
1907-1911. 
Bickel, P. J. and Ghosh, J. K. (1990). A decomposition for the likelihood ratio statistic and 
the Bartlett correction—a Bayesian argument. Ann. Statist. 18,1070-1090. 
Binder, S. (1978). Bayesian cluster analysis. Biometrika 65, 31-38. 
Birnbaum, A. (1962). On the foundations of statistical inference. J. Amer. Statist. Assoc. 57, 
269-306. 
Birnbaum, A. (1968). Likelihood, lnternat. Encyclopedia of the Social Sciences 9,299-301. 
Birnbaum, A. (1969). Concepts of statistical evidence. Philosophy Science and Methods. 
(S. Morgenbesso, P. Suppes and M. White eds.) New York: St. John's Press. 
Birnbaum, A. (1972). More on concepts of statistical evidence. J. Amer. Statist. Assoc. 67, 
858-861. 
Birnbaum, A. (1978). Likelihood. International Encyclopedia of Statistics (W. H. Kruskal 
and J. M. Tanur, eds.). London: Macmillan, 519-522. 
498 
References 
Bj0rnstad, J. F. (1990). Predictive likelihood: a review. Statist. Sci. 5,242-265 (with  
discussion). 
Blackwell, D. (1947). Conditional expectation and unbiased sequential estimation. Ann. 
Math. Statist. 18,105-110. 
Blackwell, D. (1951). Comparison of experiments. Proc. Second Berkeley Symp. (J. Neyman 
ed.). Berkeley: Univ. California Press 93-102, 
Blackwell, D. (1953). Equivalent comparison of experiments. Ann. Math. Statist. 24, 265- 
272. 
Blackwell, D. (1988). In discussion of Diaconis (1988). Bayesian Statistics 3 (J. M.  
Bernardo, M. H. DeGroot, D. V. Lindley and A. F. M. Smith, eds.). Oxford: University Press, 
123-124. 
Blackwell, D. and Dubins, L. E. (1962). Merging of opinions with increasing information. 
Ann. Math. Statist. 33, 882-886. 
Blackwell, D. and Girshick, M. A. (1954). Theory of Games and Statistical Decisions. New 
York: Wiley. 
Blyth, C. R. (1972). On Simpson's paradox and the sure-thing principle. J. Amer. Statist. 
Assoc. 67, 364-366. 
Blyth, C. R. (1973). Simpson's paradox and mutually favourable events. J. Amer. Statist. 
Assoc. 68, 746. 
Booth, N. B. and Smith, A. F. M. (1976). Batch acceptance schemes based on an autogressive 
prior. Biometrika 63,133-136. 
Bordley, R. F. (1992). An intransitive expectations based Bayesian variant of prospect theory. 
J. Risk and Uncertainty 5,127-144. 
Borel, E. (1924/1964). A propos d'un traite" de probabilites. Revue Philosophique 98, 321- 
336. Reprinted in 1980 as "A propos of a treatise on probability" in Studies in Subjective 
Probability (H. E. Kyburg and H. E Smokier, eds.). New York: Dover, 45-60. 
Borovcnik, M. (1992). Stochastik im Wechselspiels von Intuitionen und Mathematik.  
Mannheim: Bl-Wissenschaftsverlag. 
Box, G. E. P. (1980). Sampling and Bayes' inference in scientific modelling. J. Roy. Statist. 
Soc. A 143, 383-430 (with discussion). 
Box, G. E. P. (1983). An apology for ecumenism in statistics. Science 151,15-84. 
Box, G. E. P. (1985). The Collected Works ofG. E. P. Box (G. C. Tiao, ed.). Pacific Drove, 
CA: Wadsworth. 
Box, G. E. P. and Cox, D. R. (1964). An analysis of transformations. J. Roy. Statist. Soc. B 26, 
211-252 (with discussion). 
Box, G. E. P. and Hill, W. J. (1967). Discrimination among mechanistic models. Techno- 
metrics9,51-7l. 
Box, G. E. P., Leonard, T. and Wu, C.-F. (eds.) (1983). Scientific Inference, Data Analysis 
and Robustness. New York: Academic Press. 
Box, G. E. P. and Tiao, G. C. (1962). A further look at robustness via Bayes' theorem. 
Biometrika 49,419-432. 
Box, G. E. P. and Tiao, G. C. (1964). A note on criterion robustness and inference robustness. 
Biometrika 51,169-173. 
References 
499 
Box, G. E. P. and Tiao, G. C. (1965). Multiparameter problems from a Bayesian point of 
view. Ann. Math. Statist. 36,1468-1482. 
Box, G. E. P. and Tiao, G. C. (1968). A Bayesian approach to some outlier problems. Bio- 
metrika 55,119-129. 
Box, G. E. P. and Tiao, G. C. (1973). Bayesian Inference in Statistical Analysis. Reading, 
MA: Addison-Wesley. 
Boyer, M. and Kihlstrom, R. E. (eds.) (1984). Bayesian Models in Economic Theory.  
Amsterdam: North-Holland. 
Breslow, N. (1990). Biostatistics and Bayes. Statist. Sci. 5, 269-298 (with discussion). 
Bretthorst, G. L. (1988). Bayesian Spectrum Analysis and Parameter Estimation. New York: 
Springer-Verlag. 
Bridgman, P. W. (1927). The Logic of Modern Physics. London: Macmillan. 
Brier, G. W. (1950). Verification of forecasts expressed in terms of probability. Month. 
Weather Rev. 78,1-3. 
Brillinger, D. R. (1962). Examples bearing on the definition of fiducial probability, with a 
bibliography. Ann. Math. Statist. 33,1349-1355. 
Broemeling, L. D. (1985). Bayesian Analysis of Linear Models. New York: Marcel Dekker. 
Brown, L. D. (1973). Estimation with incompletely specified loss functions. J. Amer. Statist. 
Assoc. 70, Ml-Ml. 
Brown, L. D. (1985). Foundations of Exponential Families. Hayward, CA: IMS. 
Brown, P. J., Le, N. D. and Zidek, J. V. (1994). Inference for a covariance matrix. Aspects 
of Uncertainty: a Tribute to D. V. Lindley (P. R. Freeman, and A. F. M. Smith, eds.). 
Chichester: Wiley, (to appear). 
Brown, P. J. and Makelainen, T. (1992). Regression, sequential measurements and  
coherent calibration. Bayesian Statistics 4 (J. M. Bernardo, J. O. Berger, A. P. Dawid and 
A. F. M. Smith, eds.). Oxford: University Press, 97-108 (with discussion). 
Brown, R. V. (1993). Impersonal probability as an ideal assessment. J. Riskand Uncertainty 7, 
215-235. 
Brown, R. V. and Lindley, D. V. (1982). Improving judgement by reconciling incoherence. 
Theory and Decision 14,113-132. 
Brown, R. V. and Lindley, D. V. (1986). Plural analysis: multiple approach to quantitative 
research. Theory and Decision 20,133-154. 
Brunk, H. D. (1991). Fully coherent inference. Ann. Statist. 19, 830-849. 
Buehler, R. J. (1959). Some validity criteria for statistical inference. Ann. Math. Statist. 30, 
845-863. 
Buehler, R. J. (1971). Measuring information and uncertainty. Foundations of Statistical 
Inference (V. P. Godambe and D. A. Sprott, eds.). Toronto: Holt, Rinehart and Winston, 
330-351 (with discussion). 
Buehler, R. J. (1976). Coherent preferences. Ann. Statist. 4, 1051-1064. 
Buehler, R. J. and Feddersen, A. P. (1963). Note on a conditional property of Student's t. 
Ann. Math. Statist. 34,1098-1100. 
Bunn, D. J. (1984). Applied Decision Analysis. New York: McGraw-Hill 
Butler, R. W (1986). Predictive likelihood inference with applications. J. Roy. Statist. 
Soc. B 48,1-38 (with discussion). 
500 
References 
Cano, J. A., Hernandez, A. and Moreno, E. (1988). On Kolmogorov's partial sufficiency. 
Bayesian Statistics 3 (J. M. Bernardo, M. H. DeGroot, D. V. Lindley and A. F. M. Smith, 
eds.). Oxford: University Press, 553-556. 
Carlin, B. P. and Gelfand, A. E. (1991). An iterative Monte Carlo method for nonconjugate 
Bayesian analysis. Statist. Computing 1, 119-128. 
Carlin, B. P. and Poison N. G. (1992). Monte Carlo Bayesian methods for discrete regression 
models and categorical time series. Bayesian Statistics 4 (J. M. Bernardo, J. O. Berger, 
A. P. Dawid and A. F. M. Smith, eds.). Oxford: University Press, 577-586. 
Carlin, J. B. and Dempster, A. P. (1989). Sensitivity analysis of seasonal adjustments:  
empirical case studies. J. Amer. Statist. Assoc. 84, 6-32 (with discussion). 
Carnap, R. (1950/1962). Logical Foundations of Probability. Chicago: University Press. 
Caro, E., Dommguez, J. I., and Giron, F. J. (1984). Compatibilidad del metodo de DeGroot 
para llegar a un consenso en la formula de Bayes. Trab. Estadist. 35,139-153. 
Casella, G. (1987). Conditionally acceptable recentered set estimators. Ann. Statist. 15, 
1364-1371. 
Casella, G. (1992). Conditional inference for confidence sets. Current Issues in Statistical 
Inference: Essays in Honor ofD. Basu. (M. Ghosh and P. K. Pathak eds.). Hayward, CA: 
IMS. 
Casella, G. and Berger, R. L. (1987). Reconciling Bayesian and frequentist evidence in the 
one-sided testing problem. J. Amer. Statist. Assoc. 82,106-135, (with discussion). 
Casella, G. and Berger, R. L. (1990). Statistical Inference. Pacific Drove, CA: Wadsworfh. 
Casella, G. and George, E. I. (1992). Explaining the Gibbs sampler. Amer. Statist. 46,167- 
174. 
Casella, G., Hwang, J. T. G. and Robert, C. P. (1993). A paradox in decision theoretic interval 
estimation. Statistica Sinica 3, 141-155. 
Chaloner, K. (1984). Optimal Bayesian experimental design for linear models. Ann. Statist. 
12, 283-300. 
Chaloner, K. (1994). Residual analysis and outliers in hierarchical models. Aspects of  
Uncertainty: a Tribute to D. V. Lindley (P. R. Freeman, and A. F M. Smith, eds.). Chichester: 
Wiley, (to appear). 
Chaloner, K. and Brant, R. (1988). A Bayesian approach to outlier detection and residual 
analysis. Biometrika 75, 651-659. 
Chan, K. S. (1993). Asymptotic behaviour of the Gibbs sampler. J. Amer. Statist. Assoc. 88, 
320-326. 
Chang, T. and Eaves, D. M. (1990). Reference priors for the orbit of a group model. Ann. 
Statist. 18,1595-1614. 
Chang, T. and Villegas, C. (1986). On a theorem of Stein relating Bayesian and classical 
inferences in group models. Canadian J. Statist. 14, 289-296. 
Chankong, V. and Haimes, Y. (1982). Multiobjective Decision Making. Amsterdam: North- 
Holland. 
Chao, M. T. (1970). The asymptotic behavior of Bayes' estimators. Manag. Sci. 41,601-608. 
Chateaneuf, A. and Jaffray, J. Y. (1984). Archimedean qualitative probabilities. J. Math. 
Psychology 28,191-204. 
References 
501 
Chen, C. F. (1985). On asymptotic normality of limiting density functions with Bayesian 
implications. J. Roy. Statist. Soc. B 97. 540-546. 
Chernoff, H. (1951). A property of some type A regions. Ann. Math. Statist. 22, 472-474. 
Chernoff, H. (1959). Sequential design of experiments. Ann. Math. Statist. 30,755-770. 
Chernoff, H. and Moses, L. E. (1959). Elementary Decision Theory. New York: Wiley. 
Chow, Y. S. and Teicher, H. (1978/1988). Probability Theory. Berlin: Springer. Second 
edition in 1988. Berlin: Springer. 
Chuaqui, R. and Malitz, J. (1983). Preorderings compatible with probability measures. Trans. 
Amer. Math. Soc. 279, 811-824. 
Cifarelli, D. M. (1987). Recent contributions to Bayesian statistics. Italian Contributions to 
the Methodology of Statistics (A. Naddeo, ed.). Padova: Cleub, 483-516. 
Cifarelli, D. M. and Muliere, P. (1989). Statistica Bayesiana. Pavia: G. Iuculano. 
Cifarelli, D. M. and Regazzini, E. (1982). Some considerations about mathematical statistics 
teaching methodology suggested by the concept of exchangeability. Exchangeability 
in Probability and Statistics. (G. Koch and F Spizzichino, eds.). Amsterdam: North- 
Holland, 185-205. 
Clarke, B. and Wasserman, L. (1993). Non-informative priors and nuisance parameters. 
J. Amer. Statist. Assoc. 88, (to appear). 
Clarke, R. D. (1954). The concept of probability. J. Inst. Actuaries 80,1 -31 (with discussion). 
Claroti, C. A. and Lindley, D. V. (eds.) (1988). Accelerated Life testing and Expert Opinion 
in Reliability. Amsterdam: North-Holland. 
Clayton, M. K, Geisser, S. and Jennings, D. E. (1986). A comparison of several model 
selection procedures. Bayesian Inference and Decision Techniques: Essays in Honor of 
Bruno de Finetti (P. K. Goel and A. Zellner, eds.). Amsterdam: North-Holland, 425-442. 
Clemen, R. T. (1989). Combining forecasts: a review and annotated bibliography. Int. J. 
Forecasting 5, 559-583. 
Clemen, R. T. (1990). Unanimity and compromise among probability forecasters. Manag. 
Sci. 36, 767-779. 
Clemen, R. T. and Winkler, R. L. (1987). Calibrating and combining precipitation probability 
forecasts. Probability and Bayesian Statistics (R. Viertl, ed.). London: Plenum, 97-110. 
Clemen, R. T. and Winkler, R. L. (1993). Aggregating point estimates, a flexible modelling 
approach. Manag. Sci. 39, 501-515. 
Cochrane, J. L. and Zeleny, M. (eds.) (1973). Multiple Criteria Decision Making. Columbia, 
SC: University Press 
Conlisk, J. (1993). The utility of gambling. J. Risk and Uncertainty 6, 255-275. 
Consonni, G. and Veronese, P. (1987). Coherent distributions and Lindley's paradox.  
Probability and Bayesian Statistics (R. Viertl, ed.). London: Plenum, 111-120. 
Consonni, G. and Veronese, P. (1992a). Bayes factors for linear models and improper priors. 
Bayesian Statistics 4 (J. M. Bernardo, J. O. Berger, A. P. Dawid and A. F. M. Smith, 
eds.). Oxford: University Press, 587-594. 
Consonni, G. and Veronese, P. (1992b). Conjugate priors for exponential families having 
quadratic variance functions. J. Amer. Statist. Assoc. 87,1123-1127. 
Cooke, R. M. (1991). Experts in Uncertainty. Opinion and Subjective Probability in Science. 
Oxford: University Press. 
502 
References 
Cornfield, J. (1969). The Bayesian outlook and its applications. Biometrics 25, 617-657. 
Cowell, R. G. (1992). BAIES, a probabilistic expert system shell with qualitative and  
quantitative learning. Bayesian Statistics 4 (J. M. Bernardo, J. O. Berger, A. P. Dawid and 
A. F. M. Smith, eds.). Oxford: University Press, 595-600. 
Cox, D. D. (1993). An analysis of Bayesian inference for nonparametric regression. Ann. 
Statist. 21,903-923. 
Cox, D. R. (1958). Some problems connected with statistical inference. Ann. Math.  
Statist. 29, 357-372. 
Cox, D. R. (1975). Partial likelihood. Biometrika 62, 269-276. 
Cox, D. R. (1988). Conditional and asymptotic inference. Sankhya A 50, 314-337. 
Cox, D. R. (1990). Models in statistical analysis. Statist. Sci. 5,169-174. 
Cox, D. R. and Hinkley, D. V. (1974). Theoretical Statistics. London: Chapman and Hall. 
Cox, D. R. and Reid, N. (1987). Parameter orthogonality and approximate conditional  
inference. J. Roy. Statist. Soc. B 49,1-39 (with discussion). 
Cox, D. R. and Reid, N. (1992). A note on the difference between profile and modified profile 
likelihood. Biometrika 79,408-411. 
Cox R. T. (1946). Probability, frequency and expectation. Amer. J. Physics 14,1-13. 
Cox R. T. (1961). The Algebra of Probable Inference. Baltimore: Johns Hopkins. 
Crowder, M. J. (1988). Asymptotic expansions of posterior expectations, distributions and 
densities for stochastic processes. Ann. Inst. Statist. Math. 40, 297-309. 
CsiszSr, I. (1985). An extended maximum entropy principle and a Bayesian justification. 
Bayesian Statistics 2 (J. M. Bernardo, M. H. DeGroot, D. V. Lindley and A. F. M. Smith, 
eds.), Amsterdam: North-Holland, 83-98, (with discussion). 
Cuevas, A. and Sanz, P. (1988). On differentiability properties of Bayes operators. Bayesian 
Statistics 3 (J. M. Bernardo, M. H. DeGroot, D. V. Lindley and A. F. M. Smith, eds.). 
Oxford: University Press, 569-577. 
Cyert, R. M. and DeGroot, M. H. (1987). Bayesian Analysis and Uncertainty in Economic 
Theory. London: Chapman and Hall. 
Daboni, L. and Wedlin, A. (1982). Statistica. Un'Introduzione all'ImpostazioneNeo-Baye- 
siana. Torino: UTET. 
Dalai, S. and Hall, W. J. (1980). On approximating parametric models by nonparametric 
Bayes models. Ann. Statist. 8, 664-672. 
Dalai, S. and Hall, W. J. (1983). Approximating priors by mixtures of natural conjugate 
priors. J. Roy. Statist. Soc. B 45, 278-286. 
Dale, A. I. (1990). Thomas Bayes: some clues to his education. Statist, and Prob. Letters 9, 
289-290. 
Dale, A. I. (1991). History of Inverse Probability: From Thomas Bayes to Karl Pearson. 
Berlin: Springer. 
Darmois, G. (1936). Sur les lois de probability a estimation exhaustive. C. R. Acad. Sci. 
Paris 200,1265-1266. 
DasGupta, A. (1991). Diameter and volume minimizing confidence sets in Bayes and  
classical problems. Ann. Statist. 19, 1225-1243. 
DasGupta, A. and Studden, W. J. (1991). Robust Bayesian experimental designs in normal 
linear models. Ann. Statist. 19,1244-1256. 
References 
503 
Davison, A. C. (1986). Approximate predictive likelihood. Biometrika 73, 323-332. 
Davison, D., Suppes, P. and Siegel, S. (1957). Decision Making: An Experimental Approach. 
Stanford: University Press. 
Dawid, A. P. (1970). On the limiting normality of posterior distributions. Proc. Camb. Phil. 
Soc. 67, 625-633. 
Dawid, A. P. (1973). Posterior expectations for large observations. Biometrika 60,644-666. 
Dawid, A. P. (1977). Invariant distributions and analysis of variance models. Biometrika 64, 
291-297. 
Dawid, A. P. (1978). Extendibility of spherical matrix distributions. J. Multivariate  
Analysis 14, 559-566. 
Dawid, A. P. (1979a). Conditional independence in statistical theory. J. Roy. Statist. Soc. B 41, 
1-31, (with discussion). 
Dawid, A. P. (1979b). Some misleading arguments involving conditional independence. 
J. Roy. Statist. Soc. B 41, 249-252. 
Dawid, A. P. (1980a). A Bayesian look at nuisance parameters. Bayesian Statistics (J. M.  
Bernardo, M. H. DeGroot, D. V. Lindley and A. F. M. Smith, eds.). Valencia: University 
Press, 167-203 (with discussion). 
Dawid, A. P. (1980b). Conditional independence for statistical operators. Ann. Statist. 8„ 
598-617. 
Dawid, A. P. (1982a). The well-calibrated Bayesian. J. Amer. Statist. Assoc. 77, 605-613. 
Dawid, A. P. (1982b). Intersubjective statistical models. Exchangeability in Probability and 
Statistics. (G. Koch and F. Spizzichino, eds.). Amsterdam: North-Holland, 217-232. 
Dawid, A. P. (1983a). Statistical inference. Encyclopedia of Statistical Sciences 4 (S. Kotz, 
N. L. Johnson and C. B. Read, eds.). New York: Wiley, 89-105. 
Dawid, A. P. (1983b). Invariant prior distributions. Encyclopedia of Statistical Sciences 4 
(S. Kotz, N. L. Johnson and C. B. Read, eds.). New York: Wiley, 228-236. 
Dawid, A. P. (1984). Statistical theory, the prequential approach. J. Roy. Statist. Soc. A 147, 
278-292. 
Dawid, A. P. (1986a). Probability forecasting. Encyclopedia of Statistical Sciences 7 (S. Kotz, 
N. L. Johnson and C. B. Read, eds.). New York: Wiley, 210-218. 
Dawid, A. P. (1986b). A Bayesian view of statistical modelling. Bayesian Inference and 
Decision Techniques: Essays in Honor of Bruno de Finetti (P. K. Goel and A. Zellner, 
eds.). Amsterdam: North-Holland. 391-404. 
Dawid, A. P. (1988a). The infinite regress and its conjugate analysis. Bayesian Statistics 3 
(J. M. Bernardo, M. H. DeGroot, D. V. Lindley and A. F. M. Smith, eds.). Oxford: 
University Press, 95-110 (with discussion). 
Dawid, A. P. (1988b). Symmetry models and hypotheses for structured data layouts. J. Roy. 
Statist. Soc. B 50,1-34 (with discussion). 
Dawid, A. P. (1992). Prequential analysis, stochastic complexity and Bayesian inference. 
Bayesian Statistics 4 (J. M. Bernardo, J. O. Berger, A. P. Dawid and A. F. M. Smith, 
eds.). Oxford: University Press, 109-125 (with discussion). 
Dawid, A. P. (1994). The island problem: coherent use of identification evidence. Aspects 
of Uncertainty: a Tribute to D. V. Lindley (P. R. Freeman, and A. F. M. Smith, eds.). 
Chichester Wiley, (to appear). 
504 
References 
Dawid, A. P. and Fang, B. Q. (1992). Conjugate Bayes discrimination with infinitely many 
variables. J. Multivariate Analysis 41, 27-42. 
Dawid, A. P. and Smith, A. F. M. (eds.) (1983). 1982 Conference on Practical Bayesian 
Statistics. Special issue, The Statistician 32, Numbers 1 and 2. 
Dawid, A. P. and Stone, M. (1972). Expectation consistency of inverse probability  
distributions. Biometrika 59,486-489. 
Dawid, A. P. and Stone, M. (1973). Expectation consistency and generalised Bayes inference. 
Ann. Statist. 1,478^185. 
Dawid, A. P., Stone, M. and Zidek, J. V. (1973). Marginalization paradoxes in Bayesian and 
structural inference. J. Roy. Statist. Soc. B 35,189-233 (with discussion). 
Debreu, G. (1960). Topological methods in cardinal utility. Mathematical Methods in the 
Social Sciences (K. J. Arrow, S. Karlin and P. Suppes, eds.). Stanford: University Press, 
16-26. 
Deely, J. J. and Lindley, D. V. (1981). Bayes empirical Bayes. J. Amer. Statist. Assoc. 76, 
833-841. 
de Finetti, B. (1930). Funzione caratteristica di un fenomeno aleatorio. Mem. Acad. Naz. 
Lincei 4, 86-133. 
de Finetti, B. (1937/1964). La provision: ses lois logiques, ses sources subjectives. Ann. 
Inst. H. Poincare 7,1-68. Reprinted in 1980 as 'Foresight; its logical laws, its subjective 
sources' in Studies in Subjective Probability (H. E. Kyburg and H. E Smokier, eds.). New 
York: Dover, 93-158. 
de Finetti, B. (1938). Sur la condition d'Equivalence partielle. Actualites Scientifiques et 
Industrielles 739. Paris: Herman and Cii. Translated in Studies in Inductive Logic and 
Probability 2 (R. Jeffrey, ed.). Berkeley: Univ. California Press, 193-206. 
de Finetti, B. (1951). Recent suggestions for the reconciliation of theories of probability. 
Proc. Second Berkeley Symp. (J. Neyman ed.). Berkeley: Univ. California Press, 217-226. 
deFinetti, B. (1961). TheBayesian approach to the rejection of outliers. Proc. Fourth Berkeley 
Symp. 1 (J. Neyman and E. L. Scott, eds.). Berkeley: Univ. California Press, 199-210. 
de Finetti, B. (1962). Does it make sense to speak of 'Good Probability Appraisers'? The 
Scientist Speculates: An Anthology of Partly-Baked Ideas (I. J. Good, ed.). New York: 
Wiley, 257-364. Reprinted in 1972, Probability, Induction and Statistics New York: 
Wiley, 19-23. 
de Finetti, B. (1963). La decision et les probabilities. Rev. Roumaine Math. Pures Appl. 7, 
405-413. 
de Finetti, B. (1964). Probability subordinate e teoria delle decisioni. Rendiconti Matemat- 
ica 23, 128-131. Reprinted as 'Conditional probabilities and decision theory' in 1972, 
Probability, Induction and Statistics New York: Wiley, 13-18. 
de Finetti, B. (1965). Methods for discriminating levels of partial knowledge concerning a 
test item. British J. Math. Statist. Psychol. 18, 87-123. Reprinted in 1972, Probability, 
Induction and Statistics New York: Wiley, 25-63. 
de Finetti, B. (1967). Logical foundations and measurement of subjective probability. Acta 
Psychologica 34,129-145. 
de Finetti, B. (1968). Probability: interpretations. Internal Encyclopedia of the Social  
Sciences, 12. London: Macmillan, 496-504. 
References 
505 
de Finetti, B. (1970/1974). Teoria delle Probability 1. Turin: Einaudi. English translation as 
Theory of Probability 1 in 1974, Chichester: Wiley. 
de Finetti, B. (1970/1975). Teoria delle Probability 2. Turin: Einaudi. English translation as 
Theory of Probability 2 in 1975, Chichester: Wiley. 
de Finetti, B. (1972). Probability, Induction and Statistics. Chichester: Wiley. 
de Finetti, B. (1978). Probability: interpretations. Internal Encyclopedia of Statistics (W. H. 
Kruskal and J. M. Tanur, eds.) London: Macmillan, 496-505. 
de Finetti, B. (1993). Induction and Probability. (P. Monori and D. Cocchi, eds.). Bologna: 
Clueb. 
DeGroot, M. H. (1962). Uncertainty, information and sequential experiments. Ann. Math. 
Statist. 33,404^119. 
DeGroot, M. H. (1963). Some comments on the experimental measurement of utility.  
Behavioral Sci. 8, 146-149. 
DeGroot, M. H. (1970). Optimal Statistical Decisions. New York: McGraw-Hill. 
DeGroot, M. H. (1973). Doing what comes naturally: interpreting a tail area as a posterior 
probability or as a likelihood ratio. J. Amer. Statist. Assoc. 68,966-969. 
DeGroot, M. H. (1974). Reaching a consensus. J. Amer. Statist. Assoc. 69,118-121. 
DeGroot, M. H. (1980). Improving predictive distributions. Bayesian Statistics (J. M.  
Bernardo, M. H. DeGroot, D. V. Lindley and A. F M. Smith, eds.). Valencia: University 
Press, 385-395 and 415-429 (with discussion). 
DeGroot, M. H. (1982). Decision theory. Encyclopedia of Statistical Sciences 2 (S. Kotz, 
N. L. Johnson and C. B. Read, eds.). New York: Wiley, 277-286. 
DeGroot, M. H. (1987). Probability and Statistics. Reading, MA: Addison-Wesley. 
DeGroot, M. H. and Fienberg, S. E. (1982). Assessing probability assessors: calibration 
and refinement. Statistical Decision Theory and Related Topics III 1 (S. S. Gupta and 
J. O. Berger, eds.). New York: Academic Press, 291-314. 
DeGroot, M. H. and Fienberg, S. E. (1983). The comparison and evaluation of forecasters. 
The Statistician 32, 12-22. 
DeGroot, M. H. and Fienberg, S. E. (1986). Comparing probability forecasters: basic binary 
concepts and multivariate extensions. Bayesian Inference and Decision Techniques:  
Essays in Honor of Bruno de Finetti (P. K. Goel and A. Zellner, eds.). Amsterdam: North- 
Holland, 247-264. 
DeGroot, M. H., Fienberg, S. E. and Kadane, J. B. (eds.) (1986). Statistics and the Law. New 
York: Wiley. 
DeGroot, M. H. and Kadane, J. B. (1980) Optimal challenges for selection. Operations 
Research 28, 952-968. 
DeGroot, M. H. and Mortera, J. (1991). Optimal linear opinion pools. Manag. Sci. 37, 
546-558. 
DeGroot, M. H. and Rao, M. M. (1963). Bayes estimation with convex loss. Ann. Math. 
Statist. 34, 839-846. 
DeGroot, M. H. and Rao, M. M. (1966). Multidimensional information inequalities and 
prediction. Multivariate Statistics (P. R. Krishnaiah, ed.). New York: Academic Press, 
287-313. 
506 
References 
de la Horra, J. (1986). Convergencia del vector de probabilidad a posterior bajo una  
distribution predictiva. Trab. Estadist. 1, 3-11. 
de la Horra, J. (1987). Generalized estimators: a Bayesian decision theoretic view. Statistics 
and Decisions 5, 347-352. 
de la Horra, J. (1988). Parametric estimation with L\ distance. Bayesian Statistics 3 (J. M. 
Bernardo, M. H. DeGroot, D. V. Lindley and A. F. M. Smith, eds.). Oxford: University 
Press, 579-583. 
de la Horra, J. (1992). Using the prior mean of a nuisance parameter. Test 1, 31-38. 
de la Horra, J. and Fernandez, C. (1994). Bayesian robustness of credible regions in the 
presence of nuisance parameters. Comm. Statist. Theory and Methods 23, (to appear). 
Delampady, M. (1989). Lower bounds on Bayes factors for interval null hypotheses. J. Amer. 
Statist. Assoc. 84,120-124. 
Delampady, M. and Berger, J. O. (1990). Lower bounds on Bayes factors for multinomial 
and chi-squared tests of fit. Ann. Statist. 18,1295-1316. 
Delampady, M. and Dey, D. K. (1994). Bayesian robustness for multiparameter problems. 
J. Statist. Planning and Inference , (to appear). 
Dellaportas, P. and Smith, A. F. M. (1993). Bayesian inference for generalised linear and 
proportional hazards models via Gibbs sampling. Appl. Statist. 42, 443-460. 
Dellaportas, P. and Wright, D. E. (1992). A numerical integration strategy in Bayesian  
analysis. Bayesian Statistics 4 (J. M. Bernardo, J. O. Berger, A. P. Dawid and A. F. M. Smith, 
eds.). Oxford: University Press, 601-606. 
De Morgan, A. (1847). Formal Logic. London: Taylor and Walton. 
Dempster, A. P. (1967). Upper and lower probabilities induced by a multivalued mapping. 
Ann. Math. Statist. 38, 325-339. 
Dempster, A. P. (1968). A generalization of Bayesian inference. J. Roy. Statist. Soc. B 30, 
205-247 (with discussion). 
Dempster, A. P. (1975). A subjective look at robustness. Internat. Statist. Rev. 46, 349-374. 
Dempster, A. P. (1985). Probability, evidence and judgement. Bayesian Statistics 2 (J. M.  
Bernardo, M. H. DeGroot, D. V. Lindley and A. F. M. Smith, eds.), Amsterdam: North- 
Holland, 119-132 (with discussion). 
DeRobertis, L. and Hartigan, J. (1981). Bayesian inference using intervals of measures. Ann. 
Statist. 9, 235-244. 
Devroye, L. (1986). Non-Uniform Random Variate Generation. Berlin: Springer. 
De Waal, D. J. and Groenewald, P. C. N. (1989). On measuring the amount of information 
from the data in a Bayesian analysis. South African Statist. J. 23,23-61 (with discussion). 
De Waal, D. J., Groenewald, P. C. N., van Zyl, D. and Zidek, J. (1986). Multi-Bayesian 
estimation theory. Statistics and Decisions 4, 1-18. 
Diaconis, P. (1977). Finite forms of de Finetti's theorem on exchangeability. Synthese 36, 
271-281. 
Diaconis, P. (1988a). Recent progress on de Finetti's notion of exchangeability. Bayesian 
Statistics 3 (J. M. Bernardo, M. H. DeGroot, D. V. Lindley and A. F. M. Smith, eds.). 
Oxford: University Press, 111-125 (with discussion). 
Diaconis, P. (1988b). Bayesian numerical analysis. Statistical Decision Theory and Related 
Topics IV1 (S. S. Gupta and J. O. Berger, eds.). Berlin: Springer, 163-175. 
References 
507 
Diaconis, P., Eaton, M. L. and Lauritzen, S. L. (1992). Finite de Finetti theorems in linear 
models and multivariate analysis. Scandinavian J. Statist. 19, 289-316. 
Diaconis, P. and Freedman, D. (1980a). Finite exchangeable sequences. Ann. Prob. 8,745- 
764. 
Diaconis, P. and Freedman, D. (1980b). De Finetti generalizations of exchangeability. Studies 
Inductive Logic and Probability (Jeffrey, R. C. ed.). Berkeley: Univ. California Press, 
223-249. 
Diaconis, P. and Freedman, D. (1983). Frequency properties of Bayes rules. Scientific  
Inference, Data Analysis and Robustness (G. E. P. Box, T. Leonard and C. F. Wu, eds.). New 
York: Academic Press, 105-116 
Diaconis, P. and Freedman, D. (1984). Partial exchangeability and sufficiency. Statistics: 
Applications and New Directions (J. K. Ghosh and J. Roy, eds.). Calcutta: Indian Statist. 
Institute, 205-236. 
Diaconis, P. and Freedman, D. (1986a). On the consistency of Bayes estimates. Ann. Statist. 
14,1-67, (with discussion). 
Diaconis, P. and Freedman, D. (1986b). On inconsistent Bayes estimates of location. Ann. 
Statist. 14, 68-87. 
Diaconis, P. and Freedman, D. (1987). A dozen de Finetti-style results in search of a theory. 
Ann. Inst. H. Poincare 23, 397-423. 
Diaconis, P. and Freedman, D. (1990). Cauchy's equation and de Finetti's theorem.  
Scandinavian J. Statist. 17, 235-274. 
Diaconis, P. and Ylvisaker, D. (1979). Conjugate priors for exponential families. Ann.  
Statist. 7,269-281. 
Diaconis, P. and Ylvisaker, D. (1985). Quantifying prior opinion. Bayesian Statistics 2 
(J. M. Bernardo, M. H. DeGroot, D. V. Lindley and A. F. M. Smith, eds.), Amsterdam: 
North-Holland, 133-156 (with discussion). 
Diaconis, P. and Zabell, S. L. (1982). Updating subjective probability. J. Amer. Statist. 
Assoc. 77, 822-830. 
Dickey, J. M. (1968). Three multidimensional integral identities with Bayesian applications. 
Ann. Math. Statist. 39,1615-1627. 
Dickey, J. M. (1969). Smoothing by cheating. Ann. Math. Statist. 40,1477-1482. 
Dickey, J. M. (1971). The weighted likelihood ratio, linear hypotheses on normal location 
parameteters. Ann. Math. Statist. 42, 204-223. 
Dickey, J. M. (1973). Scientific reporting and personal probabilities: Student's hypothesis. 
J. Roy. Statist. Soc. B 35,285-305. Reprinted in 1974 in Studies in Bayesian Econometrics 
and Statistics: in Honor of Leonard J. Savage (S. E. Fienberg and A. Zellner, eds.). 
Amsterdam: North-Holland, 485-511. 
Dickey, J. M. (1974). Bayesian alternatives to the F test and least-squares estimate in normal 
linear model. Studies in Bayesian Econometrics and Statistics: in Honor of Leonard 
J. Savage (S. E. Fienberg and A. Zellner, eds.). Amsterdam: North-Holland, 515-554. 
Dickey, J. M. (1976). Approximate posterior distributions. J. Amer. Statist. Assoc. 71, 680- 
689. 
Dickey, J. M. (1977). Is the tail area useful as an approximate Bayes factor? J. Amer. Statist. 
Assoc. 72,138-142. 
508 
References 
Dickey, J. M. (1980). Beliefs about beliefs, a theory of stochastic assessment of subjective 
probabilities. Bayesian Statistics (J. M. Bernardo, M. H. DeGroot, D. V. Lindley and 
A. F. M. Smith, eds.). Valencia: University Press, 471^187 and 504-512 (with  
discussion). 
Dickey, J. M. (1982). Conjugate families of distributions. Encyclopedia of Statistical  
Sciences 2 (S. Kotz, N. L. Johnson and C. B. Read, eds.). New York: Wiley, 135-145. 
Dickey, J. M. and Freeman, P. R. (1975). Population-distributed personal probabilities. 
J. Amer. Statist. Assoc. 70, 362-364. 
Dickey, J. M. and Kadane, J. B. (1980). Bayesian decision theory and the simplification of 
models. Evaluation of Econometric Models (J. Kwenta and J. Ramsey, eds.). New York: 
Academic Press, 245-268. 
Dickey, J. M. and Lientz, B. P. (1970). The weighted likelihood ratio, sharp hypotheses about 
chances, the order of a Markov chain. Ann. Math. Statist. 41, 214-226. 
Diebolt, J. and Robert, C. P. (1993). Estimation of finite mixture distributions through  
Bayesian sampling. J. Roy. Statist. Soc. B 55, (to appear). 
Doksum, K. A. (1974). Tailfree and neutral random probabilities and their posterior  
distributions. Ann. Prob. 2,183-201. 
Doksum, K. A. and Lo, A. Y. (1990). Consistent and robust Bayes procedures for location 
based on partial information. Ann. Statist. 18,443-453. 
Domotor, Z. and Stelzer, J. (1971). Representation of finitely additive semiordered qualitative 
probability structures. J. Math. Psychology 8,145-158. 
Draper, N. R. and Guttman, I. (1969). The value of prior information. New Developments in 
Survey Sampling (N. L. Johnson and H. Smith Jr., eds.). New York: Wiley. 
Dreze, J. H. (1974). Bayesian theory of identification in simultaneous equations models.  
Studies in Bayesian Econometrics and Statistics: in Honor of Leonard J. Savage (S. E. Fien- 
berg and A. Zellner, eds.). Amsterdam: North-Holland, 159-174. 
Dubins, L. E. and Savage, L. J. (1965/1976). How to Gamble if you Must: Inequalities 
for Stochastic Processes. New York: McGraw-Hill. Second edition in 1976. New York: 
Dover. 
DuMouchel, W. (1990). Bayesian metaanalysis. Statistical Methodology in the  
Pharmaceutical Sciences (D. A. Berry, ed.). New York: Marcel Dekker, 509-529. 
DuMouchel, W. and Harris, J. E. (1983). Bayes methods for combining the results of cancer 
studies in humans and other species. J. Amer. Statist. Assoc. 78, 293-315. 
Duncan, G. and DeGroot, M. H. (1976). A mean squared error approach to optimal design 
theory. Proc. 1976 Conf. Information Sciences and Systems. Baltimore: John Hopkins 
University Press, 217-221. 
Duncan L. R. and Raiffa, H. (1957). Games and Decisions. New York: Wiley. 
Dunsmore, I. R. (1966). A Bayesian approach to classification. J. Roy. Statist. Soc. B 28, 
568-577. 
Dunsmore, I. R. (1968). A Bayesian approach to calibration. J. Roy. Statist. Soc. B 30, 
396^05. 
Dunsmore, I. R. (1969). Regulation and optimization. J. Roy. Statist. Soc. B 31, 160-170. 
Durbin, J. (1969). Inferential aspects of the randomness of sample size in survey sampling. 
New Developments in Survey Sampling (N. L. Johnson and H. Smith Jr., eds.). New York: 
Wiley, 629-651. 
References 
509 
Durbin, J. (1970). On Birnbaum's theorem on the relation between sufficiency, conditionally 
and likelihood. J. Amer. Statist. Assoc. 65, 395-398. 
Dykstra, R. L. and Laud, P. (1981). A Bayesian nonparametric approach to reliability. Ann. 
Statist. 9, 356-367. 
Dynkin, E. B. (1953). Klassy ekvivalentnych slucainychy velicin. Uspechi. Mat. Nauk 54, 
125-134. 
Earman, J. (1990). Bayes' Bayesianism. Stud. History Philos. Sci. 21, 351-370. 
Eaton, M. L. (1982). A method for evaluating improper prior distributions. Statistical  
Decision Theory and Related Topics III1 (S. S. Gupta and J. O. Berger, eds.). New York: 
Academic Press, 
Eaton, M. L. (1992). A statistical diptych: admissible inferences, recurrence of symmetric 
Markov chains. Ann. Statist. 20, 1147-1179. 
Eaves, D. M. (1985). On maximizing the missing information about a hypothesis. J. Roy. 
Statist. Soc. B 47, 263-266. 
Edwards, A. W. F. (1972/1992). Likelihood. Cambridge: University Press. Second edition in 
1992. Baltimore: John Hopkins University Press. 
Edwards, A. W. F. (1974). The history of likelihood. Internat. Statist. Rev. 42, 9-15. 
Edwards, W. L. (1954). The theory of decision making. Psychological Bui. 51, 380-417. 
Edwards, W. L. (1961). Behavioral decision theory. Annual Rev. Psychology 12, 473-498. 
Edwards, W. L., Lindman, H. and Savage, L. J. (1963). Bayesian statistical inference for 
psychological research. Psychol. Rev. 70,193-242. Reprinted in Robustness of Bayesian 
Analysis (J. B. Kadane, ed.). Amsterdam: North-Holland, 1984,1-62. 
Edwards, W. L. and Newman, J. R. (1982). Multiattribute Evaluation. Beverly Hills, CA: 
Sage. 
Edwards, W. L., Phillips, L. D., Hays, W. L. and Goodman, B. C. (1968). Probability  
information processing systems: design and evaluation. IEEE Trans. Systems, Science and 
Cybernetics 4, 248-265. 
Edwards, W. L. and Tversky, A. (eds.) (1967). Decision Making. Baltimore: Penguin. 
Efron, B. (1973). In discussion of Dawid, Stone and Zidek (1973). J. Roy. Statist. Soc. B 35, 
219. 
Efron, B. (1982). The Jacknife, the Bootstrap and other Resampling Plans. Philadelphia, 
PA: SIAM. 
Efron, B. (1986). Why isn't everyone a Bayesian? Amer. Statist. 40,1-11 (with discussion). 
Efron, B. (1993). Bayes and likelihood calculations from confidence intervals. Biometrika 80, 
3-26. 
Efron, B. and Morris, C. N. (1972). Empirical Bayes estimators on vector observations—an 
extension of Stein's method. Biometrika 59, 335-347. 
Efron, B. and Morris, C. N. (1975). Data analysis using Stein's estimator and its  
generalisations. J. Amer. Statist. Assoc. 70, 311-319. 
Efron, B. and Tibshirani, R. J. (1993). An Introduction to the Bootstrap. London: Chapman 
and Hall. 
Eichhorn, W. (1978). Functional Equations in Economics. Reading, MA: Addison-Wesley. 
Eliashberg, J. and Winkler, R. L. (1981). Risk sharing and group decision making. Manag. 
Sci. 27,1121-1235. 
510 
References 
El-Krunz, S. M. and Studden, W. J. (1991). Bayesian optimal designs for linear regression 
models. Ann. Statist. 19, 2183-2208. 
Ellsberg, D. (1961). Risk, ambiguity and the Savage axioms. Quart. J. Econ. 75, 643-669. 
Erickson, G. J. and Smith, C. R. (eds.) (1988). Maximum Entropy and Bayesian Methods in 
Science and Engineering. (2 volumes). Dordrecht: Kluwer. 
Ericson, W. A. (1969a). Subjective Bayesian models in sampling finite populations. J. Roy. 
Statist. Soc. B 31,195-233. 
Ericson, W. A. (1969b). Subjective Bayesian models in sampling finite populations:  
stratification. New Developments in Survey Sampling (N. L. Johnson and H. Smith Jr., eds.). 
New York: Wiley, 326-357. 
Ericson, W. A. (1988). Bayesian inference in finite populations. Handbook of Statistics 6. 
Sampling (P. R. Krishnaiah and C. R. Rao eds.). Amsterdam: North-Holland, 213-246. 
Farrell, R. H. (1964). Estimators of a location parameter in the absolutely continuous case. 
Ann. Math. Statist. 35,949-998. 
Farrell, R. H. (1968). Towards a theory of generalized Bayes tests. Ann. Math. Statist. 39, 
1-/12. 
Fearn, T. and O'Hagan, A. (eds.) (1993). 1992 Conference on Practical Bayesian Statistics. 
Special issue, The Statistician 42, Number 4. 
Fedorov, V. V. (1972). Theory of Optimal Experiments. New York: Academic Press. 
Feller, W. (1950/1968). An Introduction to Probability Theory and its Applications 1.  
Chichester: Wiley. Third edition in 1968. 
Fellner, W. (1965). Probability and Profits: A Study of Economic Behavior along Bayesian 
Lines. Homewood, IL: Irwin. 
Felsenstein, K. (1988). Iterative procedures for continuous Bayesian designs. Bayesian  
Statistics 3 (J. M. Bernardo, M. H. DeGroot, D. V. Lindley and A. F M. Smith, eds.). Oxford: 
University Press, 609-613. 
Felsenstein, K. (1992). Optimal Bayesian design for discrimination among rival models. 
J. Comp. Statist, and Data Analysis 14, 427-436. 
Ferguson, T. S. (1967). Mathematical Statistics: a Decision Theoretic Approach. New York: 
Academic Press. 
Ferguson, T. S. (1973). A Bayesian analysis of some nonparametric problems. Ann. Statist. 1, 
209-230. 
Ferguson, T. S. (1974). Prior distributions on spaces of probability measures. Ann. Statist. 2, 
615-629. 
Ferguson, T. S. (1989). Who solved the secretary problem? Statist. Sci. 4, 282-296 (with 
discussion). 
Ferguson, T. S. and Phadia, E. G. (1979). Bayesian nonparametric estimation based on 
censored data. Ann. Statist. 7,163-186. 
Ferguson, T. S., Phadia, E. G. and Tiwari, R. C. (1992). Bayesian nonparametric inference. 
Current Issues in Statistical Inference: Essays in Honor of D. Basu. (M. Ghosh and 
P. K. Pathak eds.). Hayward, CA: IMS, 127-150. 
Ferrandiz, J. R. (1982). Una soluci6n Bayesiana a la paradoja de Stein. Trab. Estadist. 33, 
31^*6. 
References 
511 
Ferrandiz, J. R. (1985). Bayesian inference on Mahalanobis distance: an alternative  
approach to Bayesian model testing. Bayesian Statistics 2 (J. M. Bernardo, M. H. DeGroot, 
D. V. Lindley and A. F. M. Smith, eds.), Amsterdam: North-Holland, 645-654. 
Ferrandiz, J. R. and Sendra, M. (1982). Tobias de Bioestadistica. Valencia: University Press. 
Fieller, E. C. (1954). Some problems in interval estimation. J. Roy. Statist. Soc. B16,186-194 
(with discussion). 
Fienberg, S. E. and Zellner, A. (eds.) (1974). Studies in Bayesian Econometrics and Statistics: 
in Honor of Leonard J. Savage. Amsterdam: North-Holland. 
Fine, T. L. (1973). Theories of Probability: an Examination of Foundations. New York: 
Academic Press. 
Fishbum, P. C. (1964). Decision and Value Theory. New York: Wiley. 
Fishbum, P. C. (1967a). Bounded expected utility. Ann. Math. Statist. 38,1054-1060. 
Fishbum, P. C. (1967b). Preference-based definitions of subjective utility. Ann. Math. Statist. 
38,1605-1617. 
Fishbum, P. C. (1968). Utility theory. Manag. Set 14, 335-378. 
Fishbum, P. C. (1969). A general theory of subjective probability and expected utilities. Ann. 
Math. Statist. 40,1419-1429. 
Fishbum, P. C. (1970). Utility Theory for Decision Making. New York: Wiley. 
Fishbum, P. C. (1975). A theory of subjective expected utility with vague preferences. Theory 
and Decision 6,287-310. 
Fishbum, P. C. (1981). Subjective expected utility: a review of normative theories. Theory 
and Decision 13,139-199. 
Fishbum, P. C. (1982). The Foundations of Expected Utility. Dordrecht: Reidel. 
Fishbum, P. C. (1986). The axioms of subjective probability. Statist. Sci. 1, 335-358 (with 
discussion). 
Fishbum, P. C. (1987). Interprofile Conditions and Impossibility. London: Harwood. 
Fishbum, P. C. (1988a). Non-linear Preference and Utility Theory. Baltimore: John Hopkins 
University Press. 
Fishbum, P. C. (1988b). Utility theory. Encyclopedia of Statistical Sciences 9 (S. Kotz, 
N. L. Johnson and C. B. Read, eds.). New York: Wiley, 445^*52. 
Fisher, R. A. (1915). Frequency distribution of the values of the correlation coefficient in 
samples from an indefinitely large population. Biometrika 10, 507-521. 
Fisher, R. A. (1922). On the mathematical foundations of theoretical statistics. Phil. Trans. 
Roy. Soc. London A 222, 309-368. Reprinted in Breakthroughs in Statistics 1 (S. Kotz 
and N. L. Johnson, eds.). Berlin: Springer, 1991,11-44. 
Fisher, R. A. (1925). Theory of statistical information. Pmc. Comb. Phil. Soc. 22, 700-725. 
Fisher, R. A. (1930). Inverse probability. Proc. Comb. Phil. Soc. 26, 528-535. 
Fisher, R. A. (1933). The concepts of inverse probability and fiducial probability referring 
to unknown parameters. Proc. Roy. Soc. A 139, 343-348. 
Fisher, R. A. (1935). The fiducial argument in statistical inference. Ann. Eugenics 6,391-398. 
Fisher, R. A. (1939). A note on fiducial inference. Ann. Statist. 10, 383-388. 
Fisher, R. A. (1956/1973). Statistical Methods and Scientific Inference. Third edition in 
1973. Edinburgh: Oliver and Boyd. Reprinted in 1990 whithin Statistical Methods,  
Experimental Design, and Scientific Inference (J. H. Bennet, ed.). Oxford: University Press. 
512 
References 
Florens, J.-P. (1978). Mesures a priori et invariance dans une experience Bay6sienne. Pub. 
Inst. Statist. Univ. Paris 23, 29-55. 
Florens, J.-P. (1982). Experiences Bay6siennes invariantes. Ann. Inst. M. Poincare 18, 309- 
317. 
Florens, J.-P. and Mouchart, M. (1985). Model selection: some remarks from a Bayesian 
viewpoint. Model Choice (Florens, J.-R, Mouchart, M., Raoult J.-P. and Simar, L., eds.). 
Brussels: Pub. Fac. Univ. Saint Louis, 27-^*4. 
Florens, J.-P. and Mouchart, M. (1986). Exaustivite\ ancillarit6 et identification en statistique 
bay6sienne. Ann. Econ. Statist. 4, 63-93. 
Florens, J.-P., Mouchart, M., Raoult J.-P. and Simar, L. (eds.) (1985). Model Choice. Brussels: 
Pub. Fac. Univ. Saint Louis. 
Florens, J.-P, Mouchart, M., Raoult, J.-P, Simar, L. and Smith, A. F. M. (eds.) (1983). 
Specifying Statistical Models. Berlin: Springer. 
Florens, J.-P, Mouchart, M. and Rolin, J.-M. (1990). Elements of Bayesian Statistics. New 
York: Marcel Dekker. 
Florens, J.-P, Mouchart, M. and Rolin, J.-M. (1992). Bayesian analysis of mixtures: some 
results on exact estimability and identification. Bayesian Statistics 4 (J. M. Bernardo, 
J. O. Berger, A. P. Dawid and A. F. M. Smith, eds.). Oxford: University Press, 127-145 
(with discussion). 
Flournoy, N. andTsutakawa.R. K. (eds.) (1991). Statistical Multiple Integration. Providence: 
RI: ASA. 
Fougere, P. T. (ed.) (1990). Maximum Entropy and Bayesian Methods. Dordrecht: Kluwer. 
Fraser, D. A. S. (1963). On the sufficiency and likelihood principles. J. Amer. Statist.  
Assoc. 58, 641-647. 
Fraser, D. A. S. (1968). The Structure of Inference. New York: Wiley. 
Fraser, D. A. S. (1972). Bayes, likelihood or structural. Ann. Math. Statist. 43, 777-790. 
Fraser, D. A. S. (1979). Inference and Linear Models. New York: McGraw-Hill. 
Fraser, D. A. S. and McDunnough, P. (1984). Further remarks on the asymptotic normality 
of likelihood and conditional analysis. Canadian J. Statist. 12,183-190. 
Fraser, D. A. S. and Reid, N. (1989). Adjustments to profile likelihood. Biometrika 76, 
477^88. 
Freedman, D. A. (1962). Invariants under mixing which generalize de Finetti's theorem. 
Ann. Math. Statist. 33, 916-923. 
Freedman, D. A. (1963a). Invariants under mixing which generalize de Finetti's theorem: 
continuous time parameter. Ann. Math. Statist. 34,1194-1216. 
Freedman, D. A. (1963b). On the asymptotic behavior of Bayes estimates in the discrete 
case. Ann. Math. Statist. 34,1386-1403. 
Freedman, D. A. (1965). On the asymptotic behavior of Bayes estimates in the discrete case 
II. Ann. Math. Statist. 36, 454-456. 
Freedman, D. A. and Diaconis, P. (1983). On inconsistent Bayes estimates in the discrete 
case. Ann. Statist. 11,1109-1118. 
Freedman, D. A. andPurves, R. A. (1969). Bayes' method for bookies. Ann. Math. Statist. 40, 
1117-1186. 
References 
513 
Freeman, P. R. (1980). On the number of outliers in data from a linear model. Bayesian 
Statistics (J. M. Bernardo, M. H. DeGroot, D. V. Lindley and A. F. M. Smith, eds.). 
Valencia: University Press, 349-365 and 370-381 (with discussion). 
Freeman, P. R. (1983). The secretary problem and its extensions—a review. Internat. Statist. 
Rev. 51, 189-206. 
Freeman, P. R. and Smith, A. F. M. (eds.) (1994). Aspects of Uncertainty: a Tribute to D. V. 
Lindley. Chichester: Wiley, (to appear). 
French, S. (1980). Updating of beliefs in the light of someone else's opinion. J. Roy. Statist. 
Soc. A 143, 43^8. 
French, S. (1981). Consensus of opinion. Eur. J. Oper. Res. 7, 332-340. 
French, S. (1982). On the axiomatisation of subjective probabilities. Theory and Decision 14, 
19-33. 
French, S. (1985). Group consensus probability distributions: a critical survey. Bayesian 
Statistics 2 (J. M. Bernardo, M. H. DeGroot, D. V. Lindley and A. F. M. Smith, eds.), 
Amsterdam: North-Holland, 183-202 (with discussion). 
French, S. (1986). Decision Theory: an Introduction to the Mathematics of Rationality. 
Chichester: Ellis Horwood. 
French, S. (ed.) (1989). Readings in Decision Analysis. London: Chapman and Hall. 
French, S., Hartley, R„ Thomas, L. C. and White, D. J (eds.) (1983). Multiobjective Decision 
Making. New York: Academic Press. 
Friedman, M. and Savage, L. J. (1948). The utility analysis of choice involving risk. J. 
Political Econ. 56, 279-304. 
Friedman, M. and Savage. L. J. (1952). The expected utility hypothesis and the measurement 
of utility. J. Political Econ. 60, 463-474. 
Fu, J. C. and Kass, R. E. (1988). The exponential rate of convergence of posterior  
distributions. Ann. Inst. Statist. Math. 40, 683-691. 
Gamerman, D. (1992). A dynamic approach to the statistical analysis of point processes. 
Biometrika 79, 39-50. 
Gamerman, D. and Migon, H. S. (1993). Dynamic hierarchical models. J. Roy. Statist. 
Soc. B 55, 629-642. 
Gardenfors, P. and Sahlin, N.-E. (1988) (eds.) Decision, Probability, and Utility. Selected 
Readings. Cambridge: University Press. 
Garthwaite, P. H. and Dickey, J. M. (1992). Elicitation of prior distributions for variable 
selection problems in regression. Ann. Statist. 20, 1697-1719. 
Gatsonis, C. A. (1984). Deriving posterior distributions for a location parameter: a decision- 
theoretic approach. Ann. Statist. 12, 958-970. 
Gatsonis, C. A., Hodges, J. S., Kass, R. E. and Singpurwalla, N. (eds.) (1993). Case Studies 
in Bayesian Statistics. Berlin: Springer. 
Gaul, W. and Schader, M. (eds.) (1978). Data, Expert Knowledge and Decisions. Berlin: 
Springer. 
Geisser, S. (1964). Posterior odds for multivariate normal classification. J. Roy. Statist. 
Soc. B 26, 69-76. 
Geisser, S. (1966). Predictive discrimination. Multivariate Analysis (P. R. Krishnaiah, ed.). 
New York: Academic Press, 149-163. 
514 
References 
Geisser, S. (1971). The inferential use of predictive distributions. Foundations of Statistical 
Inference (V. P. Godambe and D. A. Sprott, eds.). Toronto: Holt, Rinehart and Winston, 
456-469. 
Geisser, S. (1974). A predictive approach to the random effect model. Biometrika 61,101- 
107. 
Geisser, S. (1975). The predictive sample reuse method, with applications. J. Amer. Statist. 
Assoc. 70, 320-328. 
Geisser, S. (1979). In discussion of Bernardo (1979b). /. Roy. Statist. Soc. B 41,136-137. 
Geisser, S. (1980a). The contributions of Sir Harold Jeffreys to Bayesian inference. Bayesian 
Analysis in Econometrics and Statistics: Essays in Honor of Harold Jeffreys (A. Zellner, 
ed.). Amsterdam: North-Holland, 13-20. 
Geisser, S. (1980b). A predictivist primer. Bayesian Analysis in Econometrics and Statistics: 
Essays in Honor of Harold Jeffreys (A. Zellner, ed.). Amsterdam: North-Holland, 363- 
381. 
Geisser, S. (1982). Bayesian discrimination. Handbook of Statistics 2. Classification (P. R. 
Krishnaiah and L. N. Kanal eds.). Amsterdam: North-Holland, 101-120. 
Geisser, S. (1984). On prior distributions for binary trials. J. Amer. Statist. Assoc. 38,244-251 
(with discussion). 
Geisser, S. (1985). On the prediction of observables: a selective update. Bayesian Statistics 2 
(J. M. Bernardo, M. H. DeGroot, D. V. Lindley and A. F. M. Smith, eds.), Amsterdam: 
North-Holland, 203-230 (with discussion). 
Geisser, S. (1986). Predictive analysis. Encyclopedia of Statistical Sciences 7 (S. Kotz, 
N. L. Johnson and C. B. Read, eds.). New York: Wiley, 158-170. 
Geisser, S. (1987). Influential observations, diagnostics and discordancy tests. Appl. Statist. 
14, 133-142. 
Geisser, S. (1988). The future of statistics in retrospect. Bayesian Statistics 3 (J. M.  
Bernardo, M. H. DeGroot, D. V. Lindley and A. F. M. Smith, eds.). Oxford: University Press, 
147-158 (with discussion). 
Geisser, S. (1992). Bayesian perturbation diagnostics and robustness. Bayesian Analysis 
in Statistics and Econometrics (P. K. Goel and N. S. Iyengar, eds.). Berlin: Springer, 
289-302. 
Geisser, S. (1993). Predictive Inference: an Introduction. London: Chapman and Hall. 
Geisser, S. and Cornfield, J. (1963). Posterior distributions for multivariate normal  
parameters. J. Roy. Statist. Soc. B 25, 368-376. 
Geisser, S. and Eddy, W. F (1979). A predictive approach to modelselection. J. Amer. Statist. 
Assoc. 74,153-160. 
Geisser, S., Hodges, J. S., Press, S. J. and Zellner, A. (eds.) (1990). Bayesian and  
Likelihood methods in Statistics and Econometrics: Essays in Honor of George A. Barnard. 
Amsterdam: North-Holland. 
Gelfand, A. E. and Desu, A. (1968). Predictive zero-mean uniform discrimination.  
Biometrika 55, 519-524. 
Gelfand, A. E. and Dey, D. K. (1991). On Bayesian robustness in contaminated classes of 
priors. Statistics and Decisions 9, 63-80. 
References 
515 
Gelfand, A. E., Dey, D. K. and Chang, H. (1992). Model determination using predictive 
distributions with implementation via sampling-based methods. Bayesian Statistics 4 
(J. M. Bernardo, J. O. Berger, A. P. Dawid and A. F. M. Smith, eds.). Oxford: University 
Press, 147-167 (with discussion). 
Gelfand, A. E., Hills, S. E., Racine-Poon, A. and Smith, A. F. M. (1990). Illustration of 
Bayesian inference in normal models using Gibbs sampling. J. Amer. Statist. Assoc. 85, 
972-985. 
Gelfand, A. E. and Smith, A. F. M. (1990). Sampling based approaches to calculating  
marginal densities. J. Amer. Statist. Assoc. 85, 398-409. 
Gelfand, A. E., Smith, A. F. M. and Lee, T.-M. (1992). Bayesian analysis of constrained 
parameter and truncated data problems using Gibbs sampling. J. Amer. Statist. Assoc. 87, 
523-532. 
Gelman, A. and Rubin, D. B. (1992a). Inference from iterative simulation using multiple 
sequences. Statist. Sci. 7, 457-511 (with discussion). 
Gelman, A. and Rubin, D. B. (1992b). A single series from the Gibbs sampler provides a 
false sense of security. Bayesian Statistics 4 (J. M. Bernardo, J. O. Berger, A. P. Dawid 
and A. F. M. Smith, eds.). Oxford: University Press, 625-631. 
Geman, S. (1988). Experiments in Bayesian image analysis. Bayesian Statistics 3 (J. M.  
Bernardo, M. H. DeGroot, D. V. Lindley and A. F. M. Smith, eds.). Oxford: University Press, 
159-171 (with discussion). 
Geman, S. and Geman, D. (1984). Stochastic relaxation, Gibbs distributions and the Bayesian 
restoration of images. IEEE Trans. Patt. Anal. Mach. Intelligence 6, 721-740. 
Genest, C. (1984a). A characterization theorem for externally Bayesian groups. Ann.  
Statist. 12,1100-1105. 
Genest, C. (1984b). A conflict between two axioms for combining subjective distributions. 
J. Roy. Statist. Soc. B 46, 403-405. 
Genest, C. and Zidek, J. (1986). Combining probability distributions: a critique and an 
annotated bibliography. Statist. Sci. 1, 114-148 (with discussion). 
George, E. I., Makov, U. E. and Smith, A. F. M. (1993). Conjugate likelihood distributions. 
Scandinavian J. Statist. 20,147-156. 
George, E. I., Makov, U. E. and Smith, A. F. M. (1994). Bayesian hierarchical analysis for 
exponential families via Markov chain Monte Carlo. Aspects of Uncertainty: a Tribute to 
D. V. Lindley (P. R. Freeman, and A. F. M. Smith, eds.). Chichester: Wiley, (to appear). 
George, E. I. and McCulloch, R. (1993a). Variable selection via Gibbs sampling. J. Amer. 
Statist. Assoc. 88, 881-889. 
George, E. I. and McCulloch, R. (1993b). On obtaining invariant prior distributions./ Statist. 
Planning and Inference 19, (to appear). 
Geweke, J. (1988). Antithetic acceleration of Monte Carlo integration in Bayesian inference. 
J. Econometrics 38, 73-90. 
Geweke, J. (1989). Bayesian inference in econometric models using Monte Carlo integration. 
Econometrica 57, 1317-1339. 
Geyer, C. J. (1992). Practical Markov chain Monte Carlo. Statist. Sci. 7, 473-511 (with 
discussion). 
516 
References 
Ghosh, J. K., Ghosal, S. and Samanta, T. (1994). Stability and convergence of the posterior 
in non-regular problems. Statistical Decision Theory and Related Topics V (S. S. Gupta 
and J. O. Berger, eds.). Berlin: Springer, (to appear). 
Ghosh, J. K. and Mukerjee, R. (1992). Non-informative priors. Bayesian Statistics 4 (J. M. 
Bernardo, J. O. Berger, A. P. Dawid and A. F. M. Smith, eds.). Oxford: University Press, 
195-210 (with discussion). 1 
Ghosh, M. (1991). Hierarchical and empirical Bayes sequential estimation. Handbook of 
Statistics 8. Statistical Methods in Biological and Medical Sciences (C. R. Rao and 
R. Chakraborty, eds.). Amsterdam: North-Holland, 441-458. 
Ghosh, M. (1992a). Hierarchical and empirical Bayes multivariate estimation. Current Issues 
in Statistical Inference: Essays in Honor ofD. Basu. (M. Ghosh and P. K. Pathak eds.). 
Hayward, CA: IMS, 151-177. 
Ghosh, M. (1992b). Constrained Bayes estimation with application. J. Amer. Statist.  
Assoc. 87, 533-540. 
Ghosh, M. and Pathak, P. K. (eds.) (1992). Current Issues in Statistical Inference: Essays in 
Honor of D. Basu. Hayward, CA: IMS. 
Gilardoni, G. L. and Clayton, M. K. (1993). On reaching a consensus using DeGroot's 
iterative pooling. Ann. Statist. 21, 391-401. 
Gilio, A. (1992a). Co-Coherence and extensions of conditional probabilities. Bayesian  
Statistics 4 (J. M. Bernardo, J. O. Berger, A. P. Dawid and A. F. M. Smith, eds.). Oxford: 
University Press, 633-640. 
Gilio, A. (1992b). Incomplete probability assessments in decision analysis. /. It. Statist. 
Soc. 1, 67-76. 
Gilio, A. and Scozzafava, R. (1985). Vague distributions in Bayesian testing of a null  
hypothesis. Metron 43,167-174. 
Gilks, W. R. (1992). Derivative-free adaptive rejection sampling for Gibbs sampling.  
Bayesian Statistics 4 (J. M. Bernardo, J. O. Berger, A. P. Dawid and A. F. M. Smith, eds.). 
Oxford: University Press, 641-649. 
Gilks, W. R., Clayton, D. G., Spiegelhalter, D. J., Best, N. G., McNeil, A. J., Sharpies, 
L. D. and Kirby, A. J. (1993). Modelling complexity: applications of Gibbs sampling in 
medicine. J. Roy. Statist. Soc. B 55, 39-52 (with discussion). 
Gilks, W. R. and Wild, P. (1992). Adaptive rejection sampling for Gibbs sampling. Appl. 
Statist. 41, 337-348. 
Gillies, D. A. (1987). Was Bayes a Bayesian? Hist. Math. 14, 325-346. 
Gilliland, D. C, Boyer, J. E. Jr. and Tsao, H. J. (1982). Bayes empirical Bayes: finite 
parameter case. Ann. Statist. 10, 1277-1282. 
Girelli-Bruni, E. (ed.) (1981). Teoria delle Decisioni inMedicina. Verona: Bertani. 
Gir6n, F. J., Martinez, L. and Morcillo, C. (1992). A Bayesian justification for the analysis 
of residuals and inference measures. Bayesian Statistics 4 (J. M. Bernardo, J. O. Berger, 
A. P. Dawid and A. F. M. Smith, eds.). Oxford: University Press, 651-660. 
Gir6n, F. J. and Ri'os, S. (1980). Quasi-Bayesian behaviour: a more realistic approach to 
decision making? Bayesian Statistics (J. M. Bernardo, M. H. DeGroot, D. V. Lindley 
and A. F. M. Smith, eds.). Valencia: University Press, 17-38 (with discussion). 
References 
517 
Girshick, M. A. and Savage, L. J. (1951). Bayes and minimax estimates for quadratic loss 
functions. Proc. Second Berkeley Symp. (J. Neyman ed.). Berkeley: Univ. California 
Press, 53-74. 
Godambe, V. P. (1969). Some aspects of the theoretical development in survey sampling. 
New Developments in Survey Sampling (N. L. Johnson and H. Smith Jr., eds.). New York: 
Wiley, 27-58. 
Godambe, V. P. (1970). Foundations of survey sampling. Amer. Statist. 24, 33-38. 
Godambe, V. P. and Sprott, D. A. (eds.) (1971). Foundations of Statistical Inference. Toronto: 
Holt, Rinehart and Winston. 
Goel, P. K. (1983). Information measures and Bayesian hierarchical models. /. Amer. Statist. 
Assoc. 78, 408-410. 
Goel, P. K. (1988). Software for Bayesian analysis: current status and additional needs. 
Bayesian Statistics 3 (J. M. Bernardo, M. H. DeGroot, D. V. Lindley and A. F. M. Smith, 
eds.). Oxford: University Press, 173-188 (with discussion). 
Goel, P. K. and DeGroot, M. H. (1979). Comparison of experiments and information  
measures. Ann. Statist. 7,1066-1077. 
Goel, P. K. and DeGroot, M. H. (1980). Only normal distributions have linear posterior 
expectations in linear regression. J. Amer. Statist. Assoc. 75, 895-900. 
Goel, P. K. and DeGroot, M. H. (1981). Information about hyperparameters in hierarchical 
models. J. Amer. Statist. Assoc. 76,140-147. 
Goel, P. K., Gulati, C. M and DeGroot, M. H. (1992). Optimal stopping for a  
non-communicating team. Bayesian Statistics 4 (J. M. Bernardo, J. O. Berger, A. P. Dawid and 
A. F. M. Smith, eds.). Oxford: University Press, 211-226 (with discussion). 
Goel, P. K. and Iyengar, N. S. (eds.) (1992). Bayesian Analysis in Statistics and Econometrics. 
Berlin: Springer 
Goel, P. K. and Zellner, A. (eds.) (1986). Bayesian Inference and Decision Techniques: 
Essays in Honor of Bruno de Finetti. Amsterdam: North-Holland. 
Goicoechea, A., Duckstein, L. and Zionts, S. (eds.) (1992). Multiple Criteria Decision  
Making. Berlin: Springer 
Goldstein, M. (1981). Revising previsions: a geometric interpretation. J. Roy. Statist. Soc. 
5 43,105-130. 
Goldstein, M. (1985). Temporal coherence. Bayesian Statistics 2 (J. M. Bernardo, M. H.  
DeGroot, D. V. Lindley and A. F. M. Smith, eds.), Amsterdam: North-Holland, 231-248 
(with discussion). 
Goldstein, M. (1986a). Separating beliefs. Bayesian Inference and Decision Techniques: 
Essays in Honor of Bruno de Finetti (P. K. Goel and A. Zellner, eds.). Amsterdam: 
North-Holland, 197-215. 
Goldstein, M. (1986b). Exchangeable belief structures. J. Amer. Statist. Assoc. 81,971-976. 
Goldstein, M. (1986c). Prevision. Encyclopedia of Statistical Sciences 7 (S. Kotz, N. L.  
Johnson and C. B. Read, eds.). New York: Wiley, 175-176. 
Goldstein, M. (1987a). Systematic analysis of limited belief specifications. The Statistician 
36,191-199. 
Goldstein, M. (1987b). Can we build a subjectivist statistical package? Probability and 
Bayesian Statistics (R. Viertl, ed.). London: Plenum, 203-217. 
518 
References 
Goldstein, M. (1988). The data trajectory. Bayesian Statistics 3 (J. M. Bernardo, M. H. De- 
Groot, D. V. Lindley and A. F. M. Smith, eds.). Oxford: University Press, 189-209 (with 
discussion). 
Goldstein, M. (1991). Belief transforms and the comparison of hypothesis. Ann. Statist. 19, 
2067-2089. 
Goldstein, M. (1994). Revising exchangeable beliefs: subjectivist foundations for the  
inductive argument. Aspects of Uncertainty: a Tribute to D. V Lindley (P. R. Freeman, and 
A. F. M. Smith, eds.). Chichester: Wiley, (to appear). 
Goldstein, M. and Howard, J. V. (1991). A likelihood paradox. J. Roy. Statist. Soc. B 53, 
619-628 (with discussion). 
Goldstein, M. and Smith, A. F. M. (1974). Ridge-type estimators for regression analysis. 
J. Roy. Statist. Soc. B 36, 284-319. 
G6mez-Villegas, M. A. and G6mez, E. (1992). Bayes factors in testing precise hypotheses. 
Comm. Statist. A 21,1707-1715. 
G6mez-Villegas, M. A. and Mafn, P. (1992). The influence of prior and likelihood tail 
behaviour on the posterior distributionXBayesiaw Statistics 4 (J. M. Bernardo, J. O. Ber- 
ger, A. P. Dawid and A. F. M. Smith, eds.). Oxford: University Press, 661-667. 
Good, I. J. (1950). Probability and the Weighing of Evidence. London : Griffin; New York: 
Hafner Press. 
Good, I. J. (1952). Rational decisions. /. Roy. Statist. Soc. B 14,107-114. 
Good, I. J. (1959). Kinds of probability. Science 127,443^*47. 
Good, I. J. (1960). Weight of evidence, corroboration, explanatory power and the utility of 
experiments. J. Roy. Statist. Soc. B 22, 319-331. 
Good, I. J. (1962). Subjective probability on the measure of a non-measurable set. Logic 
Methodology and Philosophy of Science (E. Nagel, P. Suppes and A. Tarski, eds.).  
Stanford: University Press, 319-329. 
Good, I. J. (1965). The Estimation of Probabilities. An Essay on Modern Bayesian Methods. 
Cambridge, Mass: The MIT Press. 
Good, I. J. (1966). A derivation of the probabilistic explanation of information. J. Roy. Statist. 
Soc. 5 28,578-581. 
Good, I. J. (1967). A Bayesian test for multinomial distributions. J. Roy. Statist. Soc. B 29, 
399^31. 
Good, I. J. (1969). What is the use of a distribution? Multivariate Analysis 2 (P. R. Krishnaiah, 
ed.). New York: Academic Press, 183-203. 
Good, I. J. (1971). The probabilistic explication of information, evidence, surprise, causality, 
explanation and utility. Twenty seven principles of rationality. Foundations of Statistical 
Inference (V. P. Godambe and D. A. Sprott, eds.). Toronto: Holt, Rinehart and Winston, 
108-141 (with discussion). 
Good, I. J. (1976). The Bayesian influence, or how to sweep subjectivism under the  
carpet. Foundations of Probability Theory, Statistical Inference and Statistical Theories of 
Science 2 (W. L. Harper and C. A. Hooker eds.). Dordrecht: Reidel, 119-168. 
Good, I. J. (1977). Dynamic probability, computer chess and the measurement of knowledge. 
Machine Intelligence 8, (E. W Elcock and D. Michie, eds.). Chichester: Ellis Horwood, 
139-150. Reprinted in Good (1983), 106-116. 
References 
519 
Good, I. J. (1980a). The contributions of Jeffreys to Bayesian statistics. Bayesian Analysis 
in Econometrics and Statistics: Essays in Honor of Harold Jeffreys (A. Zellner, ed.). 
Amsterdam: North-Holland, 21-34. 
Good, I. J. (1980b). Some history of the hierarchical Bayesian mehodology. Bayesian  
Statistics (J. M. Bernardo, M. H. DeGroot, D. V. Lindley and A. F. M. Smith, eds.). Valencia: 
University Press, 489-519. 
Good, I. J. (1982). Degrees of belief. Encyclopedia of Statistical Sciences 2 (S. Kotz, 
N. L. Johnson and C. B. Read, eds.). New York: Wiley, 287-292. 
Good, I. J. (1983). Good Thinking: The Foundations of Probability and its Applications. 
Minneapolis: Univ. Minnesota Press. 
Good, I. J. (1985). Weight of Evidence: a brief survey. Bayesian Statistics 2 (J. M. Bernardo, 
M. H. DeGroot, D. V. Lindley and A. F. M. Smith, eds.), Amsterdam: North-Holland, 
249-270 (with discussion). 
Good, I. J. (1987). Hierarchical Bayesian and empirical Bayesian methods. Amer. Statist. 41, 
(with discussion). 
Good, I. J. (1988a). Statistical evidence. Encyclopedia of Statistical Sciences 8 (S. Kotz, 
N. L. Johnson and C. B. Read, eds.). New York: Wiley, 651-656. 
Good, I. J. (1988b). The interface between statistics and philosophy of science. Statist. Sci. 3, 
386-398 (with discussion). 
Good, I. J. (1992). The Bayes/non-Bayes compromise: a brief review. J. Amer. Statist.  
Assoc. 87, 597-606. 
Good, I. J. and Gaskins, R. (1971). Non-parametric roughness penalties for probability 
densities. Biometrika 58, 255-277. 
Good, I. J. and Gaskins, R. (1980). Density estimation and bump hunting by the  
penalized likelihood method, exemplified by scattering and meteorite data. J. Amer. Statist. 
Assoc. 75,42-73. 
Goodwin, P. and Wright, G. (1991). Decision Analysis for Management Judgement. New 
York: Wiley. 
Gordon, N. J. and Smith, A. F M. (1993). Approximate non-Gaussian Bayesian estimation 
and modal consistency. J. Roy. Statist. Soc. B 55,913-918. 
Goutis, C. and Casella, G. (1991). Improved invariant confidence intervals for a normal 
variance. Ann. Statist. 19, 2019-2031. 
Grandy, W. T. and Schick, L. H. (eds.) (1991). Maximum Entropy and Bayesian Methods. 
Dordrecht: Kluwer. 
Grayson, C. J. (1960). Decisions under Uncertainty: Drilling Decisions by Oil and Gas 
Operators. Harvard, MA: University Press. 
Grenander, U. and Miller, M. I. (1994). Representations of knowledge in complex systems. 
J. Roy. Statist. Soc. B 56 (to appear, with discussion). 
Grieve, A. P. (1987). Applications of Bayesian software: two examples. The Statistician 36, 
283-288. 
Gu, C. (1992). Penalized likelihood regression: a Bayesian analysis. Statistica Sinica 2, 
255-264. 
Gupta, S. S. and Berger, J. O. (eds.) (1988). Statistical Decision Theory and Related Topics 
IV I. Berlin: Springer. 
520 
References 
Gupta, S. S. and Berger, J. O. (eds.) (1994). Statistical Decision Theory and Related Topics 
V. Berlin: Springer, (to appear). 
Guti6rrez-Pena, E. (1992). Expected logarithmic divergence for exponential families.  
Bayesian Statistics 4 (J. M. Bernardo, J. O. Berger, A. P. Dawid and A. F. M. Smith, eds.). 
Oxford: University Press, 669-674. 
Guttman, I. (1970). Statistical Tolerance Regions: Classical and Bayesian. London: Griffin. 
Guttman, I. and Pena, D. (1988). Outliers and influence. Evaluation by posteriors of  
parameters in the linear model. Bayesian Statistics 3 (J. M. Bernardo, M. H. DeGroot, 
D. V. Lindley and A. F. M. Smith, eds.). Oxford: University Press, 631-640. 
Guttman, I. and Pena, D. (1993). A Bayesian look at the question of diagnostics. Statistica 
Sinica 3, 367-390. 
Haag, J. (1924). Sur un probleme g6n6ral de probability et ses diverses applications. Proc. 
Internat. Congress Math. Toronto, 659-674. 
Hacking, I. (1965). Slightly more realistic personal probability. Philosophy of Science 34, 
311-325. 
Hacking, I. (1975). The Emergence^ Probability. Cambridge: University Press. 
Hadley, G. (1967). Introduction to Probability and Statistical Decision Theory. San  
Francisco, CA: Holden-Day. 
Hald, A. (1968), Bayesian single acceptance plans for continuous prior distributions. Techno- 
metrics 10, 667-683. 
Haldane, J. B. S. (1931). A note on inverse probability. Proc. Camb. Phil. Soc. 28, 55-61. 
Haldane, J. B. S. (1948). The precision of observed values of small frequencies. Bio- 
metrika 35, 297-303. 
Halmos, P. R. and Savage, L. J. (1949). Application of the Radon-Nikodym theorem to the 
theory of sufficient statistics. Ann. Math. Statist. 20, 225-241. 
Halter, A. N. and Dean, G. W. (1971). Decisions under Uncertainty. Cincinnati, OH: South- 
Western. 
Harrison P. J. and West, M. (1987). Practical Bayesian forecasting. The Statistician 36, 
115-125. 
Harsany, J. (1967). Games with incomplete information played by 'Bayesian' players. 
Manag. Sci. 14,159-182, 320-334,486-502. 
Hartigan, J. A. (1964). Invariant prior distributions. Ann. Math. Statist. 35, 836-845. 
Hartigan, J. A. (1965). The asymptotically unbiased prior distribution. Ann. Math. Statist. 36, 
1137-1152. 
Hartigan, J. A. (1966a). Estimation by ranking parameters. J. Roy. Statist. Soc. B 28, 32-44. 
Hartigan, J. A. (1966b). Note on the confidence prior of Welch and Peers. J. Roy. Statist. 
Soc. B 28, 55-56. 
Hartigan, J. A. (1967). The likelihood and invariance principles. J. Roy. Statist. Soc. B 29, 
533-539. 
Hartigan, J. A. (1969). Use of subsample values as typical values. J. Amer. Statist. Assoc. 104, 
1003-1317. 
Hartigan, J. A. (1971). Similarity and probability. Foundations of Statistical Inference 
(V. P. Godambe and D. A. Sprott, eds.). Toronto: Holt, Rinehart and Winston, 305-313 
(with discussion). 
References 
521 
Haitigan, J. A. (1975). Necessary and sufficient conditions for asymptotic normality of a 
statistic and its subsample values. Ann. Statist. 3, 573-580. 
Hartigan, J. A. (1983). Bayes Theory. Berlin: Springer. 
Hastings, W. K. (1970). Monte Carlo sampling methods using Markov chains and their 
applications. Biometrika 57, 97-109. 
Heath, D. L. and Sudderth, W. D. (1972). On a theorem of de Finetti, oddsmaking and game 
theory. Amer. Statist. 43, 2072-2077. 
Heath, D. L. and Sudderth, W. D. (1976). De Finetti's theorem for exchangeable random 
variables. Amer. Statist. 30, 333-345. 
Heath, D. L. and Sudderth, W. D. (1978). On finitely additive priors, coherence and extended 
admissibility. Ann. Statist. 6, 333-345. 
Heath, D. L. and Sudderth, W. D. (1989). Coherent inference from improper priors and from 
finitely additive priors. Ann. Statist. 17, 907-919. 
Hens, T. (1992). A note on Savage's theorem with a finite number of states. J. Risk and 
Uncertainty 5, 63-71. 
Herstein, I. N. and Milnor, J. (1953). An axiomatic approach to measurable utility. Econo- 
metrica 21, 291-297. 
Hewitt, E. and Savage, L. J. (1955). Symmetric measures on Cartesian products. Trans. Amer. 
Math. Soc. 80, 470-501. 
Hewlett, P. S. andPlackett, R. L. (1979). The Interpretation of Quantal Responses in Biology. 
London: Edward Arnold. 
Heyde, C. C. and Johnstone, I. M. (1979). On asymptotic posterior normality for stochastic 
processes. J. Roy. Statist. Soc. B 41,184-189. 
Hildreth, C. (1963). Bayesian statisticians and remote clients. Econometrica 31, 422-438. 
Hill, B. M. (1968). Posterior distributions of percentiles: Bayes' theorem for sampling from 
a finite population. J. Amer. Statist. Assoc. 63, 677-691. 
Hill, B. M. (1969). Foundations of the theory of least squares. J. Roy. Statist. Soc. B 31, 
89-97. 
Hill, B. M. (1974). On coherence, inadmissibility and inference about many parameters in 
the theory of least squares. Studies in Bayesian Econometrics and Statistics: in Honor 
of Leonard J. Savage (S. E. Fienberg and A. Zellner, eds.). Amsterdam: North-Holland, 
555-584. 
Hill, B. M. (1975). A simple general approach to inference about the tail of a distribution. 
Ann. Statist. 3, 1163-1174. 
Hill, B. M. (1980). On finite additivity, non-conglomerability, and statistical paradoxes. 
Bayesian Statistics (J. M. Bernardo, M. H. DeGroot, D. V. Lindley and A. F. M. Smith, 
eds.). Valencia: University Press, 39-66 (with discussion). 
Hill, B. M. (1986). Some subjective Bayesian considerations in the selection of models. 
Econometric Reviews 4, 191-288. 
Hill, B. M. (1987). The validity of the likelihood principle. Amer. Statist. 41, 95-100. 
Hill, B. M. (1988). De Finetti's theorem, induction and A(n), or Bayesian nonparametric 
predictive inference. Bayesian Statistics 3 (J. M. Bernardo, M. H. DeGroot, D. V.  
Lindley and A. F. M. Smith, eds.). Oxford: University Press, 211-241 (with discussion). 
522 
References 
Hill, B. M. (1990). A theory of Bayesian data analysis. Bayesian and Likelihood Methods 
in Statistics and Econometrics: Essays in Honor of George A. Barnard (S. Geisser, 
J. S. Hodges, S. J. Press and A. Zellner, eds.). Amsterdam: North-Holland, 49-73. 
Hill, B. M. (1992). Bayesian nonparametric prediction and statistical inference. Bayesian 
Analysis in Statistics and Econometrics (P. K. Goel and N. S. Iyengar, eds.). Berlin: 
Springer, 43-76. 
Hill, B. M. (1994). On Steinian shrinkage estimators: the finite/infinite problem and  
formalism in probability and statistics. Aspects of Uncertainty: a Tribute to D. V. Lindley 
(P. R. Freeman, and A. F. M. Smith, eds.). Chichester: Wiley, (to appear). 
Hill, B. M. and Lane, D. (1986). Conglomerability and countable additivity. Bayesian  
Inference and Decision Techniques: Essays in Honor of Bruno de Finetti (P. K. Goel and 
A. Zellner, eds.). Amsterdam: North-Holland, 45-57. 
Hills, S. E. (1987). Reference priors and identifiability problems in non-linear models. The 
Statistician 36, 235-240. 
Hills, S. E. and Smith, A. F. M. (1992). Parametrization issues in Bayesian inference.  
Bayesian Statistics 4 (J. M. Bernardo, 1J&. Berger, A. P. Dawid and A. F. M. Smith, eds.). 
Oxford: University Press, 227-246 (with discussion). 
Hills, S. E. and Smith, A. F. M. (1993). Diagnostic plots for improved parametrisation in 
Bayesian inference. Biometrika 80, 61-74. 
Hinkelmann, K. (ed.) (1990). Foundations of Statistics. An International Symposium in 
Honor of I. J. Good. Special issue, J. Statist. Planning and Inference 25. 
Hinkley, D. V. (1979). Predictive likelihood. Ann. Statist. 7, 718-728. 
Hipp, C. (1974). Sufficient statistics and exponential families. Ann. Statist. 2, 1283-1292. 
Hjort, N. L. (1990). Nonparametric Bayes estimator based on beta processes in models for 
life history data. Ann. Statist. 18, 1259-1294. 
Hoadley, B. (1970). A Bayesian look at inverse regression. J. Amer. Statist. Assoc. 65, 356- 
369. 
Hodges, J. S. (1987). Uncertainty, policy analysis and statistics. Statist. Sci. 2,259-291 (with 
discussion). 
Hodges, J. S. (1990). Can/may Bayesians use pure tests of significance? Bayesian and  
Likelihood Methods in Statistics and Econometrics: Essays in Honor of George A. Barnard 
(S. Geisser, J. S. Hodges, S. J. Press and A. Zellner, eds.). Amsterdam: North-Holland, 
75-90. 
Hodges, J. S. (1992). Who knows what alternative lurks in the hearts of significance tests? 
Bayesian Statistics 4 (J. M. Bernardo, J. O. Berger, A. P. Dawid and A. F. M. Smith, 
eds.). Oxford: University Press, 247-266 (with discussion). 
Hogarth, R. (1975). Cognitive processes and the assessment of subjective probability  
distributions. J. Amer. Statist. Assoc. 70, 271-294. 
Hogarth, R. (1980). Judgement and Choice. New York: Wiley 
Holland, G. D. (1962). The reverend Thomas Bayes, F.R.S. (1702-1761). J. Roy. Statist. 
Soc. A 125,421^61. 
Howson, C. and Urbach, P. (1989). Scientific Reasoning: the Bayesian Approach. La Salle, 
IL: Open Court. 
References 
523 
Hull, J., Moore, P. G. and Thomas, H. (1973). Utility and its measurement. J. Roy. Statist. 
Soc. A 136, 226-247. 
Huseby, A. B. (1988). Combining opinions in a predictive case. Bayesian Statistics 3 
(J. M. Bernardo, M. H. DeGroot, D. V. Lindley and A. F. M. Smith, eds.). Oxford: 
University Press, 641-651. 
Huzurbazar, V. S. (1976). Sufficient Statistics. New York: Marcel Dekker. 
Hwang, J. T. (1985). Universal domination and stochastic domination: decision theory under 
a broad class of loss functions. Ann. Statist. 13, 295-314. 
Hwang, J. T. (1988). Stochastic and universal domination. Encyclopedia of Statistical  
Sciences 8 (S. Kotz, N. L. Johnson and C. B. Read, eds.). New York: Wiley, 781-784. 
Hylland, A, and Zeckhauser, R. (1981). The impossibility of Bayesian group decision making 
with separate aggregation of beliefs and values. Econometrica 79, 1321-1336. 
Ibragimov, I. A. and Hasminski, R. Z. (1973). On the information in a sample about a 
parameter. Proc. 2nd Internat. Symp. Information Theory. (B. N. Petrov and F. Csaki, 
eds.), Budapest: Akademiaikiad6, 295-309. 
Irony, T. Z. (1992). Bayesian estimation for discrete distributions. J. Appl. Statist. 19, 533- 
549. 
Irony, T. Z. (1993). Information in sampling rules. J. Statist. Planning and Inference 36, 
27-38. 
Irony, T. Z., Pereira, C. A. de B. and Barlow, R. E. (1992). Bayesian models for quality  
assurance. Bayesian Statistics 4 (J. M. Bernardo, J. O. Berger, A. P. Dawid and A. F. M. Smith, 
eds.). Oxford: University Press, 675-688. 
Isaacs, G. L., Christ, D. E., Novick, M. R. and Jackson, P. H. (1974). Tables for Bayesian 
Statisticians. Ames, 10: Iowa University Press. 
Iversen, G. R. (1984). Bayesian Statistical Inference. Beverly Hills, CA: Sage 
Jackson, J. E. (1960). Bibliography on sequential analysis. J. Amer. Statist. Assoc. 55, 561- 
580. 
James, W. and Stein, C. (1961). Estimation with quadratic loss. Proc. Fourth Berkeley Symp. 1 
(J. Neyman and E. L. Scott, eds.). Berkeley: Univ. California Press, 361-380. 
Jaynes, E. T. (1958). Probability Theory in Science and Engineering. Dallas: Mobil Oil Co. 
Jaynes, E. T. (1968). Prior probabilities. IEEE Trans. Systems, Science and Cybernetics 4, 
227-291. 
Jaynes, E. T. (1971). The well posed problem. Foundations of Statistical Inference (V. P. Go- 
dambe and D. A. Sprott, eds.). Toronto: Holt, Rinehart and Winston, 342-356 (with 
discussion). 
Jaynes, E. T. (1976). Confidence intervals vs. Bayesian intervals. Foundations of Probability 
Theory, Statistical Inference and Statistical Theories of Science 2 (W. L. Harper and 
C. A. Hooker eds.). Dordrecht: Reidel, 175-257 (with discussion). 
Jaynes, E. T. (1980). Marginalization and prior probabilities. Bayesian Analysis in  
Econometrics and Statistics: Essays in Honor of Harold Jeffreys (A. Zellner, ed.). Amsterdam: 
North-Holland, 43-87 (with discussion). 
Jaynes, E. T. (1983). Papers on Probability, Statistics and Statistical Physics. (R. D. Rosen- 
krantz, ed.). Dordrecht: Kluwer. 
524 
References 
Jaynes, E. T. (1985). Highly informative priors. Bayesian Statistics 2 (J. M. Bernardo, 
M. H. DeGroot, D. V. Lindley and A. F. M. Smith, eds.), Amsterdam: North-Holland, 
329-359 (with discussion). 
Jaynes, E. T. (1986). Some applications and extensions of the de Finetti representation  
theorem. Bayesian Inference and Decision Techniques: Essays in Honor of Bruno de Finetti 
(P. K. Goel and A. Zellner, eds.). Amsterdam: North-Holland, 31^*2. 
Jeffrey, R. C. (1965/1983). The Logic of Decision. New York: McGraw-Hill. Second edition 
in 1983. Chicago: University Press. 
Jeffrey, R. C. (ed.) (1981). Studies in Inductive Logic and Probability. Berkeley: Univ. 
California Press. 
Jeffreys, H. (1931/1973). Scientific Inference. Cambridge: University Press. Third edition in 
1973, Cambridge: University Press. 
Jeffreys, H. (1939/1961). Theory of Probability. Oxford: University Press. Third edition in 
1961, Oxford: University Press. 
Jeffreys, H. (1946). Anjnvariant form for the prior probability in estimation problems. Proc. 
Roy.Soc. A 186,453-461. 
Jeffreys, H. (1955). The present position in probability theory. Brit. J. Philos. Sci. 5,275-289. 
Jeffreys, H. and Jeffreys, B. S. (1946/1972). Methods of Mathematical Physics. Cambridge: 
University Press. Third edition in 1972, Cambridge: University Press. 
Jewell, W. S. (1974). Credible means are exact Bayesian for simple exponential families. 
ASTINBulletin^, 77-90. 
Jewell, W. S. (1988). A heterocedastic hierarchical model. Bayesian Statistics 3 (J. M.  
Bernardo, M. H. DeGroot, D. V. Lindley and A. F. M. Smith, eds.). Oxford: University Press, 
657-663. 
Johnson, N. L. and Kotz, S. (1969). Discrete Distributions. New York: Wiley. 
Johnson, N. L. and Kotz, S. (1970). Continuous Univariate Distributions. New York: Wiley. 
Johnson, N. L. and Kotz, S. (1972). Continuous Multivariate Distributions. New York: Wiley. 
Johnson, R. A. (1967). An asymptotic expansion for posterior distributions. Ann. Math. 
Statist. 38, 1899-1906. 
Johnson, R. A. (1970). Asymptotic expansions associated with posterior distributions. Ann. 
Math. Statist. 41, 851-864. 
Johnson, R. A. and Ladalla, J. N. (1979). The large-sample behaviour of posterior  
distributions with sampling from muitiparameter exponential family models and allied results. 
SankhyaB 41,169-215. 
Johnson, W and Geisser, S. (1982). Assessing the predictive influence of observations. 
Statistics and Probability Essays in Honor ofC. R. Rao (G. Kallianpur, P. K. Krishnaiah 
and J. K. Ghosh, eds.). Amsterdam: North-Holland, 343-358. 
Johnson, W and Geisser, S. (1983). A predictive view of the detection and characterisation 
of influential observations in regression analysis. J. Amer. Statist. Assoc. 78,137-144. 
Johnson, W. and Geisser, S. (1985). Estimative influence measures for the multivariate 
general linear model. J. Statist. Planning and Inference 11, 33-56. 
Joshi, V. M. (1983). Likelihood principle. Encyclopedia of Statistical Sciences 4 (S. Kotz, 
N. L. Johnson and C. B. Read, eds.). New York: Wiley, 644-647. 
References 
525 
Justice, J. M. (ed.) (1987). Maximum Entropy and Bayesian Methods in Applied Statistics. 
Cambridge: University Press. 
Kadane, J. B. (1974). The role of identification in Bayesian theory. Studies in Bayesian 
Econometrics and Statistics: in Honor of Leonard J. Savage (S. E. Fienberg and A. Zell- 
ner, eds.). Amsterdam: North-Holland, 175-191. 
Kadane, J. B. (1980). Predictive and structural methods for eliciting prior distributions. 
Bayesian Analysis in Econometrics and Statistics: Essays in Honor of Harold Jeffreys 
(A. Zellner, ed.). Amsterdam: North-Holland, 89-109. 
Kadane, J. B. (ed.) (1984). Robustness of Bayesian Analysis. Amsterdam: North-Holland. 
Kadane, J. B. (1992). Healthy scepticism as an expected utility explanation of the phenomena 
of Allais and Ellsberg. Theory and Decision 32, 57-64. 
Kadane, J. B. (1993). Several Bayesians: a review. Test 2, 1-32 (with discussion). 
Kadane, J. B. and Chuang, D. T. (1978). Stable decision problems. Ann. Statist. 6,1095-1110. 
Kadane, J. B. and Larkey, P. (1982). Subjective probability and the theory of games. Manag. 
Sci. 28, 113-120. 
Kadane, J. B. and Larkey, P. (1983). The confusion of is and ought in game theoretic contexts. 
Manag. Sci. 29, 1365-1379. 
Kadane, J. B., Schervish, M. J. and Seidenfeld, T. (1986). Statistical implications of finitely 
additive probability. Bayesian Inference and Decision Techniques: Essays in Honor oj 
Bruno de Finetti (P. K. Goel and A. Zellner, eds.). Amsterdam: North-Holland, 59-76. 
Kadane, J. B. and Seidenfeld, T. (1990). Randomization in a Bayesian perspective. J. Statist. 
Planning and Inference 25, 329-345. 
Kadane, J. B. and Seidenfeld, T. (1992). Equilibrium, common knowledge and optimal 
sequential decisions. Knowledge, Beliefs and Strategic Information (C. Bicchini and M. 
L. Dalla Chiara, eds.). Cambridge: University Press, 27-45. 
Kagan, A. M., Linnik, Y. V. and Rao, C. R. (1973). Characterization Problems in  
Mathematical Statistics. New York: Wiley. 
Kahneman, D., Slovick, P. and Tversky, A. (eds.) (1982). Judgement under Uncertainty: 
Heuristics and Biases. Cambridge: University Press. 
Kahneman, D. and Tversky, A. (1979). Prospect theory: an analysis of decision under risk. 
Econometrica 47, 263-291. 
Kalbfleish, J. G. (1971). Likelihood methods in prediction. Foundations of Statistical  
Inference (V. P. Godambe and D. A. Sprott, eds.). Toronto: Holt, Rinehart and Winston, 
372-392 (with discussion). 
Kalbfleish, J. G. and Sprott, D. A. (1970). Application of likelihood methods to models 
involving large number of parameters. J. Roy. Statist. Soc. B 32, 175-208 (with  
discussion). 
Kalbfleish, J. G. and Sprott, D. A. (1973). Marginal and conditional likelihoods. Sankhya 
A 35, 311-328. 
Kapur, J. M. and Kesavan, H. K. (1992). Entropy Optimization Principles and Applications. 
New York: Academic Press. 
Karlin, S. and Rubin, H. (1956). The theory of decision procedures for distributions with 
monotone likelihood ratio. Ann. Math. Statist. 27, 272-299. 
526 
References 
Kashy ap, R. L. (1971). Prior probability and uncertainty. IEEE Trans. Information Theory 14, 
641-650. 
Kashyap, R. L. (1974). Minimax estimation with divergence loss function. Information 
Sciences?, 341-364. 
Kass, R. E. (1989). The geometry of asymptotic inference. Statist. Sci. 4, 188-234. 
Kass, R. E. (1990). Data-translated likelihood and Jeffreys' rule. Biometrika 77, 107-114. 
Kass, R. E. and Slate E. H. (1992). Reparametrization and diagnostics of posterior non- 
normality. Bayesian Statistics 4 (J. M. Bernardo, J. O. Berger, A. P. Dawid and A. F. M. 
Smith, eds.). Oxford: University Press, 289-305 (with discussion). 
Kass, R. E. and Steffey, D. (1989). Approximate Bayesian inference in conditionally  
independent hierarchical models (parametric empirical Bayes). J. Amer. Statist. Assoc. 84, 
717-726. 
Kass, R. E., Tierney, L. and Kadane, J. B. (1988). Asymptotics in Bayesian computation. 
Bayesian Statistics 3 (J. M. Bernardo, M. H. DeGroot, D. V. Lindley and A. F. M. Smith, 
eds.). Oxford: University Press, 261-278, (with discussion). 
Kass, R. E., Tierney, L. and Kadane, J. B. (1989a). The validity of posterior expansions based 
on Laplace's method. Bayesian and Likelihood Methods in Statistics and Econometrics: 
Essays in Honor of George A. Barnard (S. Geisser, J. S. Hodges, S. J. Press and A. Zellner, 
eds.). Amsterdam: North-Holland, 473^*88. 
Kass, R. E., Tierney, L. and Kadane, J. B. (1989b). Approximate methods for assessing 
influence and sensitivity in Bayesian analysis. Biometrika 76, 663-674. 
Kass, R. E., Tierney, L. and Kadane, J. B. (1991). Laplace's method in Bayesian analysis. 
Statistical Multiple Integration (N. Flournoy and R. K. Tsutakawa eds.). Providence: RI: 
ASA, 89-99. 
Kass, R. E. and Vaidyanathan, S. (1992). Approximate Bayes factors and orthogonal  
parameters, with application to testing equality of two binomial proportions. J. Roy. Statist. 
Soc. B 54,129-144. 
Keeney, R. L. (1992). Value-Focused Thinking. Harvard, MA: University Press. 
Keeney, R. L. and Raiffa, H. (1976). Decisions with Multiple Objectives: Preferences and 
Value Tradeoffs. New York: Wiley. 
Kelly, J. S. (1991). Social choice bibliography. Social Choice and Welfare 8, 97-169. 
Kempthorne, P. J. (1986). Decision-theoretic measures of influence in regression. J. Roy. 
Statist. Soc. B 48, 370-378. 
Kestemont, M.-P. (1987). The Kolmogorov distance as comparison measure between  
parametric and non-parametric Bayesian predictions. The Statistician 36, 259-264. 
Keynes, J. M. (1921/1929). A Treatise on Probability. London: Macmillan. Second edition 
in 1929, London: Macmillan. Reprinted in 1962. New York: Harper and Row. 
Khintchine, A. I. (1932). Surles classes d'6v6nements Equivalents. Mat. Sbornik39,40-43. 
Kiefer, J. and Wolfowitz, J. (1956). Consistency of the maximum likelihood estimator in the 
presence of infinitely many nuisance parameters. Ann. Math. Statist. 27, 887-906. 
Kim, K. H. and Roush, F. W (1987). Team Theory. Chichester: Ellis Horwood. 
Kimeldorf, G. S. and Wahba, G. (1970). A correspondence between Bayesian estimation in 
stochastic processes and smoothing by splines. Ann. Math. Statist. 41,495-502. 
References 
527 
Kingman, J. F. C. (1972). On random sequences with spherical symmetry. Biometrika 59, 
492-494. 
Kingman, J. F. C. and Taylor, S. J. (1966). Introduction to Measure and Probability.  
Cambridge: University Press. 
Klein, R. and Press, S. J. (1992). Adaptive Bayesian classification of spatial data. J. Amer. 
Statist. Assoc. 87, 844-851. 
Klein, R. W. and Brown, S. J. (1984). Model selection when there is 'minimal' prior  
information. Econometrica 52, 1291-1312. 
Kleiter, G. D. (1980). Bayes-Statistik: Grundlagen und Anwendungen. Berlin: W. de Gruyter. 
Kloek, T. and van Dijk, H. K. (1978). Bayesian estimates of system equation parameters: an 
application of integration by Monte Carlo. Econometrica 46,1-19. 
Klugman, S. A. (1992). Bayesian Statistics in Actuarial Science, with Emphasis on  
Credibility. Dordrecht: Kluwer. 
Koch, G. and Spizzichino, F. (eds.) (1982). Exchangeability in Probability and Statistics. 
Amsterdam: North-Holland. 
Kogan, N. and Wallace, M. A. (1964). Risk Taking.: A Study in Cognition and Personality. 
Toronto: Holt, Rinehart and Winston. 
Kolmogorov, A. N. (1933/1950). Grundbegriffe der Wahrscheinlichkeitsrechnung. Berlin: 
Springer. English translation in 1950 as Foundations of the Theory of Probability, New 
York: Chelsea. 
Koopman, B. O. (1940). The axioms and algebra of intuitive probability. Ann. Math.  
Statist. 41, 269-292. 
Koopman, L. H. (1936). On distributions admitting a sufficient statistics. Trans. Amer. Math. 
Soc. 39, 399-409. 
Korean, R. J. (1992). Decision analytica: an example of Bayesian inference and  
decision theory using Mathematica. Economic and Financial Modelling with Mathematica 
(H. R. Varian, ed.). Berlin: Springer, 407^58. 
Kraft, C, Pratt, J. W and Seidenberg, A. (1959). Intuitive probability on finite sets. Ann. 
Math. Statist. 30, 408-419. 
Krantz, D. H.,Luce, R. D., Suppes, P. andTvereky, A. (1971). Foundations of Measurement 1. 
New York: Academic Press. 
Krasker, W S. (1984). A note on selecting parametric models in Bayesian inference. Ann. 
Statist. 12,751-757. 
Kiichler, U. and Lauritzen, S. L. (1989). Exponential families, extreme point models, and 
minimal space-time invariant functions for stochastic processes with stationary and  
independent increments. Scandinavian J. Statist. 15, 237-261. 
Kuhn, T. S. (1962). The Structure of Scientific Revolutions. Chicago: University Press. 
Kullback, S. (1959/1968). Information Theory and Statistics. New York: Wiley. Second 
edition in 1968, New York: Dover. Reprinted in 1978, Gloucester, MA: Peter Smith. 
Kullback, S. andLeibler, R. A. (1951). On information and sufficiency. Ann. Math. Statist. 22, 
79-86. 
Kyburg, H. E. (1961). Probability and the Logic of Rational Belief. Middletown: Wesleyan 
University Press. 
Kyburg, H. E. (1974). The Logical Foundations of Statistical Inference. Dordrecht: Reidel. 
528 
References 
Kyburg, H. E. and Smokier, H. E. (eds.) (1964/1980). Studies in Subjective Probability. 
Chichester: Wiley. Second edition in 1980, New York: Dover. 
Lad, F. and Deely, J. J. (1994). Experimental design from a subjective utilitarian viewpoint. 
Aspects of Uncertainty: a Tribute to D. V. Lindley (P. R. Freeman, and A. F. M. Smith, 
eds.). Chichester: Wiley, (to appear). 
Lad, F, Dickey, J. M. and Rahman, M. A. (1990). The fundamental theorem of prevision. 
Statistica 50, 19-38. 
LaMotte, L. R. (1985). Bayesian linear estimators. Encyclopedia of Statistical Sciences 5 
(S. Kotz, N. L. Johnson and C. B. Read, eds.). New York: Wiley, 20-22. 
Lane, D. A. and Sudderth, W D. (1983). Coherent and continuous inference. Ann. Statist. 11, 
114-120. 
Lane, D. A. and Sudderth, W D. (1984). Coherent predictive inference. Sankhya A 46, 
/T66-185. 
Laplace, P. S. (1774/1986). M6moire sur la probability des causes par les 6venements. Mem. 
Acad. Sci. Paris 6, 621-656. English translation in 1986 as "Memoir on the probability 
of the causes of events", with an introduction by S. M. Stigler, Statist. Sci. 1, 359-378. 
Laplace, P. S. (1812). Theorie Analytique des Probabilites. Paris: Courcier. Reprinted as 
Oeuvres Completes de Laplace 7, 1878-1912. Paris: Gauthier-Villars. 
Laplace, P. S. (1814/1952). Essai Philosophique surles Probabilities. Paris: Courcier. The 
5th edition (1825) was the last revised by Laplace. English translation in 1952 as  
Philosophical Essay on Probabilities. New York: Dover. 
Lauritzen, S. L. (1982). Statistical Families as Extremal Families. Aalborg: University Press. 
Lauritzen, S. L. (1988). Extremal Families and Systems of Sufficient Statistics. Berlin: 
Springer. 
Lauritzen, S. L. and Spiegelhalter, D. J. (1988). Local computations with probabilities on 
graphical structures, and their application to expert systems. J. Roy. Statist. Soc. B 50, 
157-224 (with discussion). 
Lavalle, I. H. (1968). On cash equivalents and information evaluation in decisions under 
uncertainty. J. Amer. Statist. Assoc. 63, 252-290. 
Lavalle, I. H. (1970). An Introduction to Probability, Decision and Inference. Toronto: Holt, 
Rinehart and Winston. 
Lavalle, I. H. (1978). Fundamentals of Decision Analysis. Toronto: Holt, Rinehart and  
Winston. 
Lavine, M. (1991a). Sensitivity in Bayesian statistics: the prior and the likelihood. J. Amer. 
Statist. Assoc. 86, 396-399. 
Lavine, M. (199 lb). An approach to robust B ayesian analysis for multidimensional parameter 
spaces. J. Amer. Statist. Assoc. 86,400-403. 
Lavine, M. (1992a). Some aspects of Polya tree distributions for statistical modelling. Ann. 
Statist. 20,1222-1235. 
Lavine, M. (1992b). Sensitivity in Bayesian statistics: the prior and the likelihood. J. Amer. 
Statist. Assoc. 86, 396-399. 
Lavine, M. (1994). An approach to evaluating sensitivity in Bayesian regression analysis. 
J. Statist. Planning and Inference , (to appear). 
References 
529 
Lavine, M., Wasserman, L. and Wolpert, R. L. (1991). Bayesian inference with specified 
prior marginals. J. Amer. Statist. Assoc. 86, 964-971. 
Lavine, M., Wasserman, L. and Wolpert, R. L. (1993). Linearization of Bayesian robustness 
problems. J. Statist. Planning and Inference, (to appear). 
Lavine, M. and West, M. (1992). A Bayesian method for classification and discrimination. 
Canadian J. Statist. 20,451-461. 
Learner, E. E. (1978). Specification Searches: Ad hoc Inference with Nonexperimental Data. 
New York: Wiley. 
LeCam, L. (1953). On some asymptotic properties of maximum likelihood estimates and 
related Bayes' estimates. Univ. California Pub. Statist. 1, 277-329. 
LeCam, L. (1956). On the asymptotic theory of estimation and testing hypothesis. Proc. 
Third Berkeley Symp. 1 (J. Neyman and E. L. Scott, eds.). Berkeley: Univ. California 
Press, 129-156. 
LeCam, L. (1958). Les propietes asymptotiques de solutions de Bayes. Pub. Inst. Statist. 
Univ. Paris 7, 17-35. 
LeCam, L. (1966). Likelihood functions for large number of independent observations. 
Research Papers in Statistics. Festschrift for J. Neyman (F. N. David, ed.). New York: 
Wiley, 167-187. 
LeCam, L. (1970). On the assumptions used to prove asymptotic normality of maximum 
likelihood estimates. Ann. Math. Statist. 41, 802-828. 
LeCam, L. (1986). Asymptotic Methods in Statistical Decision Theory. Berlin: Springer. 
Lecoutre, B. (1984). L'Analyse Bayesienne des Comparaisons. Lille: Presses Universitaires. 
Lee, P. M. (1989). Bayesian Statistics: an Introduction. London: Edward Arnold. 
Lehmann, E. L. (1959/1983). Theory of Point Estimation. Second edition in 1983, New York: 
Wiley. Reprinted in 1991, Belmont, CA: Wadsworth. 
Lehmann, E. L. (1959/1986). Testing Statistical Hypotheses. Second edition in 1986, New 
York: Wiley. Reprinted in 1991, Belmont, CA: Wadsworth. 
Lehmann, E. L. (1990). Model specification. Statist. Sci. 5, 160-168. 
Lejeune, M. and Faulkenberry, G. D. (1982). A simple predictive density function. J. Amer. 
Statist. Assoc. 87, 654-657. 
Lempers, F. B. (1971). Posterior Probabilities of Alternative Linear Models. Rotterdam: 
University Press. 
Lenk, P. J. (1991). Towards a practicable Bayesian nonparametric density estimator. Bio- 
metrika 78, 531-543. 
Leonard, T. (1973). A Bayesian method for histograms. Biometrika 60, 297-308. 
Leonard, T. (1975). Bayesian estimation methods for two-way contingency tables. J. Roy. 
Statist. Soc. B 37, 23-37. 
Leonard, T. (1980). The roles of inductive modelling and coherence in Bayesian statisitcs. 
Bayesian Statistics (J. M. Bernardo, M. H. DeGroot, D. V. Lindley and A. F. M. Smith, 
eds.). Valencia: University Press, 537-555 and 568-581 (with discussion). 
Leonard, T. and Hsu, J. S. J. (1992). Bayesian inference for a covariance matrix. Ann.  
Statist. 20, 1669-1696. 
530 
References 
Leonard, T. and Hsu, J. S. J. (1994). The Bayesian analysis of categorical data: a  
selective review. Aspects of Uncertainty: a Tribute to D. V. Lindley (P. R. Freeman, and 
A. F. M. Smith, eds.). Chichester: Wiley, (to appear). 
Leonard, T., Hsu, J. S. J. and Tsui, K.-W. (1989). Bayesian marginal inference. J. Amer. 
Statist. Assoc. 84,1051-1058. 
Leonard, T. and Ord, K. (1976). An investigation of the F test procedure as an estimation 
short-cut. J. Roy. Statist. Soc. B 38, 95-98. 
Levine, R. D. and Tribus, M. (eds.) (1978). The Maximum Entropy Formalism. Cambridge, 
MA: The MIT Press. 
Ley, E. and Steel, M. F. J. (1992). Bayesian econometrics, conjugate analysis and rejection 
sampling. Economic and Financial Modelling with Mathematica (H. R. Varian, ed.). 
Berlin: Springer, 344-367. 
Lindgren, B. W. (1971). Elements of Decision Theory. London: Macmillan. 
Lindley, D. V. (1953). Statistical inference. J. Roy. Statist. Soc. B 15, 30-76. 
Lindley, D. V. (1956). On a measure of information provided by an experiment. Ann. Math. 
Statist. 27, 986-1005. 
Lindley, D. V. (1957). A statistical paradox. Biometrika 44,187-192. 
Lindley, D. V. (1958). Fiducial distribution and Bayes' Theorem. J. Roy. Statist. Soc. B 20, 
102-107. 
Lindley, D. V. (1961a). Dynamic programming and decision theory. Appl. Statist. 10,39-51. 
Lindley, D. V. (1961b). The use of prior probability distributions in statistical inference and 
decision. Proc. Fourth Berkeley Symp. 1 (J. Neyman and E. L. Scott, eds.). Berkeley: 
Univ. California Press, 453-468. 
Lindley, D. V. (1964). The Bayesian analysis of contingency tables. Ann. Math. Statist. 35, 
1622-1643. 
Lindley, D. V. (1965). Introduction to Probability and Statistics from a Bayesian Viewpoint. 
Cambridge: University Press. 
Lindley, D. V. (1969). Review of Fraser (1968). Biometrika 56,453^56. 
Lindley, D. V. (1971). The estimation of many parameters. Foundations of Statistical  
Inference (V. P. Godambe and D. A. Sprott, eds.). Toronto: Holt, Rinehart and Winston, 
435-453 (with discussion). 
Lindley, D. V. (1971/1985). Making Decisions. Second edition in 1985, Chichester: Wiley. 
Lindley, D. V. (1972). Bayesian Statistics, a Review. Philadelphia, PA: SIAM. 
Lindley, D. V. (1976). Bayesian Statistics. Foundations of Probability Theory, Statistical 
Inference, and Statistical Theories of Science 2 (W, L. Harper and C. A. Hooker, eds.), 
Dordrecht: Reidel, 353-363. 
Lindley, D. V. (1977). A problem in forensic science. Biometrika 44,187-192. 
Lindley, D. V. (1978). The Bayesian approach. Scandinavian J. Statist. 5, 1-26. 
Lindley, D. V. (1980a). Jeffreys's contribution to modern statistical thought. Bayesian  
Analysis in Econometrics and Statistics: Essays in Honor of Harold Jeffreys (A. Zellner, ed.). 
Amsterdam: North-Holland, 35-39. 
Lindley, D. V. (1980b). Approximate Bayesian methods. Bayesian Statistics (J. M.  
Bernardo, M. H. DeGroot, D. V. Lindley and A. F. M. Smith, eds.). Valencia: University Press, 
223-245 (with discussion). 
References 
531 
Lindley, D. V. (1982a). Scoring rules and the inevitability of probability. Internat. Statist. 
Rev. 50,1-26 (with discussion). 
Lindley, D. V. (1982b). Bayesian inference. Encyclopedia of Statistical Sciences 1 (S. Kotz, 
N. L. Johnson and C. B. Read, eds.). New York: Wiley, 197-204. 
Lindley, D. V. (1982c). Coherence. Encyclopedia of Statistical Sciences 2 (S. Kotz, N. L. 
Johnson and C. B. Read, eds.). New York: Wiley, 29-31. 
Lindley, D. V. (1982d). The improvement of probability judgements. J. Roy. Statist. Soc. A 
145,117-126. 
Lindley, D. V. (1983). Reconciliation of probability distributions. Operations Res. 31, 866- 
880. 
Lindley, D. V. (1984). The next 50 years. J. Roy. Statist. Soc. A 147, 359-367. 
Lindley, D. V. (1985). Reconciliation of discrete probability distributions. Bayesian  
Statistics 2 (J. M. Bernardo, M. H. DeGroot, D. V. Lindley and A. F. M. Smith, eds.), 
Amsterdam: North-Holland, 375-390 (with discussion). 
Lindley, D. V. (1986). The reconciliation of decision analyses. Oper. Research 14,289-295. 
Lindley, D. V. (1987). The probability approach to the treatment of uncertainty in artificial 
intelligence and expert systems. Statist. Sci. 2, 17^44 (with discussion). 
Lindley, D. V. (1990). The present position in Bayesian Statistics. Statist. Sci. 5,44-89 (with 
discussion). 
Lindley, D. V. (1991). Subjective probability, decision analysis and their legal consequences. 
J. Roy. Statist. Soc. A 154, 83-92. 
Lindley, D. V. (1992). Is our view of Bayesian statistics too narrow? Bayesian Statistics 4 
(J. M. Bernardo, J. O. Berger, A. P. Dawid and A. F. M. Smith, eds.). Oxford: University 
Press, 1-15 (with discussion). 
Lindley, D. V. (1993). On the presentation of evidence. Math. Scientist 18, 60-63. 
Lindley, D. V. and Deely, J. J. (1993). Optimal allocation of stratified sampling with partial 
information. Test!, 147-160. 
Lindley, D. V. and Novick, M. R. (1981). The role of exchangeability in inference. Ann. 
Statist. 9,45-58. 
Lindley, D. V and Phillips, L. D. (1976). Inference for a Bernoulli process (a Bayesian view). 
Amer. Statist. 30, 112-119. 
Lindley, D. V. and Scott, W. F. (1985). New Cambridge Elementary Statistical Tables.  
Cambridge: University Press. 
Lindley, D. V. and Singpurwalla, N. D. (1991). On the evidence needed to reach agreed 
action between adversaries, with application to acceptance sampling. / Amer. Statist. 
Assoc. 86, 933-937. 
Lindley, D. V. and Singpurwalla, N. D. (1993). Adversarial life testing. J. Roy. Statist. 
Soc. B 55, 837-847. 
Lindley, D. V. and Smith, A. F. M. (1972). Bayes estimates for the linear model. / Roy. 
Statist. Soc. B 34, 1-41 (with discussion). 
Lindley, D. V., Tversky, A. and Brown, R. V. (1979). On the reconciliation of probability 
assessments. J. Roy. Statist. Soc. A 142,146-180. 
Liseo, B. (1993). Elimination of nuisance parameters with reference priors. Biometrika 80, 
295-304. 
532 
References 
Liseo, B., Petrella, L. and Salinetti, G. (1993). Block unimodality for multivariate Bayesian 
robustness. J. It. Statist. Soc. 2, (to appear). 
Little, R. J. A. and Rubin, D. B. (1987). Statistical Analysis with Missing Data. New York: 
Wiley. 
Lo, A. Y. (1984). On a class of Bayesian non-parametric estimates: I. Density estimates. 
Ann. Statist. 12, 351-357. 
Lo, A. Y. (1986). Bayesian statistical inference for sampling a finite population. Ann.  
Statist. 14, 1226-1233. 
Lo, A. Y. (1987). A large sample study of the Bayesian bootstrap. Ann. Statist. 15,360-375. 
Lo, A. Y. (1993). A Bayesian bootstrap for censored data. Ann. Statist. 20, 100-123. 
Luce, R. D. (1959). Individual Choice Behaviour. New York: Wiley. 
Luce, R. D. (1992). When does subjective expected utility fail descriptively? J. Risk and 
Uncertainty 5, 5-27. 
Luce, R. D. and Krantz, D. H. (1971). Conditional expected utility. Econometrica 39, 253- 
271. 
Luce, R. D. and Narens, L. (1978). Qualitative independence in probability theory. Theory 
and Decision 9, 225-239. 
Luce, R. D. and Raiffa, H. (1957). Games and Decisions. Introduction and Critical Survey. 
Chichester: Wiley. 
Luce, R. D. and Suppes, P. (1965). Preference, utility and subjective probability. Handbook 
of Mathematical Psychology 3 (R. D. Luce, Bush and Galanter, eds.). New York: Wiley, 
249-410. 
Lusted, L. B. (1968). Introduction to Medical Decision Making. Springfield, IL: Thomas. 
Maatta, J. and Casella, G. (1990). Developments in decision theoretic variance estimation. 
Statist. Sci. 5, 90-120 (with discussion). 
Machina, M. (1982). 'Expected utility' analysis without the independence axiom.  
Econometrica 50, 277-323. 
Machina, M. (1987). Choices under uncertainty. Problems solved and unsolved. J. Econ. 
Perspectives 1,121-154. 
Main, P. (1988). Prior and posterior tail comparisons. Bayesian Statistics 3 (J. M.  
Bernardo, M. H. DeGroot, D. V. Lindley and A. F. M. Smith, eds.). Oxford: University Press, 
669-675. 
Makov, U. E. (1988). On stochastic approximation and Bayes linear estimators. Bayesian 
Statistics 3 (J. M. Bernardo, M. H. DeGroot, D. V. Lindley and A. F. M. Smith, eds.). 
Oxford: University Press, 697-699. 
Mardia, K. V., Kent, J. T. and Walder, A. N. (1992). Statistical shape models in image analysis. 
Computer Science and Statistics: Proc. 23rd. Symp. Interface (E. M. Keramidas, ed.). 
Fairfax Station: Interface Foundation, 550-557. 
Marinell, G. and Seeber, G. (1988). Angewandte Statistik. Munich: Oldenbourg Verlag. 
Maritz, J. S. and Lwin, T. (1989). Empirical Bayes Methods. London: Chapman and Hall. 
Marriott, J. M. (1988). Reparametrisation for Bayesian inference in ARMA time series. 
Bayesian Statistics 3 (J. M. Bernardo, M. H. DeGroot, D. V. Lindley and A. F. M. Smith, 
eds.). Oxford: University Press, 701-704. 
References 
533 
Marriott, J. M. and Naylor, J. C. (1993). Teaching Bayes on M1N1TAB. Appl. Statist. 42, 
223-232. 
Marriott, J. M. and Smith, A. F. M. (1992). Reparametrisation aspects of numerical Bayesian 
methodology for autoregressive moving-average models. J. Time Series Anal. 13, 327- 
343. 
Marschak, J. (1950). Statistical inference in economics: an introduction. Statistical Inference 
in Dynamic Economic Models. New York: Cowles Commission, 1-50. 
Marschak, J. and Radner, R. (1972). Economic Theory of Teams. New Haven: Yale University 
Press. 
Martin, J. J. (1967). Bayesian Decision Problems and Markov Chains. New York: Wiley. 
Martz, H. F. and Waller, R. A. (1982). Bayesian Reliability Analysis. New York: Wiley 
Masreliez, C. J. (1975). Approximate non-Gaussian filtering with linear state and observation 
relations. IEEE Trans. Automatic Control 20, 107-110. 
Mathiasen, P. E. (1979). Prediction functions. Scandinavian J. Statist. 6, 1-21. 
Mazloum, R. and Meeden, G. (1987). Using the stepwise Bayes technique to choose between 
experiments. Ann. Statist. 15, 269-277. 
McCarthy, J. (1956). Measurements of the value of information. Proc. Nat. Acad. Sci. USA 
42, 654-655. 
McCulloch, R. E. (1989). Local model influence. J. Amer. Statist. Assoc. 84,473-478. 
McCulloch, R. E. and Rossi, P. E. (1992). Bayes factors for non-linear hypothesis and 
likelihood distributions. Biometrika 79, 663-676. 
McCulloch, R. E. and Tsay, R. S. (1993). Bayesian inference and prediction for mean and 
variance shifts in autoregressive time series. J. Amer. Statist. Assoc. 88, 968-978. 
Meeden, G. (1990). Admissible contour credible sets. Statistics and Decisions 8, 1-10. 
Meeden, G. and Isaacson, D. (1977). Approximate behavior of the posterior distribution for 
a large observation. Ann. Statist. 5, 899-908. 
Meeden, G. and Vardeman, S. (1991). A non-informative Bayesian approach to interval 
estimation in finite population sampling. /. Amer. Statist. Assoc. 86, 972-986. 
Meinhold, R. and Singpurwalla, N. D. (1983). Understanding the Kalman filter. Amer.  
Statist. 37,123-127. 
Mendel, M. B. (1992). Bayesian parametric models for lifetimes. Bayesian Statistics 4 
(J. M. Bernardo, J. O. Berger, A. P. Dawid and A. F. M. Smith, eds.). Oxford: University 
Press, 697-705. 
Mendoza, M. (1994). Asymptotic posterior normality under transformations. Test 3, (to 
appear). 
Meng X.-L. and Rubin, D. B. (1992). Recent extensions to the EM algorithm. Bayesian 
Statistics 4 (J. M. Bernardo, J. O. Berger, A. P. Dawid and A. F. M. Smith, eds.). Oxford: 
University Press, 307-320 (with discussion). 
Merkhofer, M. W (1987). Quantifying judgemental uncertainty: methodology, experiences 
and insights. IEEE Trans. Systems, Science and Cybernetics 17. 741-752. 
Metropolis, N., Rosenbluth, A. W, Rosenbluth, M. N., Teller, A. H. and Teller, E. (1953). 
Equation of state calculations by fast computing machines. J. Chem. Phys. 21, 1087- 
1092. 
Meyer, D. L. and Collier, R. O. (eds.) (1970). Bayesian Statistics. Itasca, IL: Peacock. 
534 
References 
Mills, J. A. (1992). Bayesian prediction tests for structural stability. J. Econometrics 52, 
381-388. 
Mitchell, T. J. and Beauchamp, T. J. (1988). Bayesian variable selection in linear regression. 
/ Amer. Statist. Assoc. 83, 1023-1035 (with discussion). 
Mitchell, T. J. and Morris, M. D. (1992). Bayesian design and analysis of computer  
experiments: two examples. Statistica Sinica 2, 359-379. 
Mockus, J. (1989). Bayesian Approach to Global Optimization. Dordrecht: Kluwer. 
Mohammad-Djafari, A. and Demoment, G. (eds.) (1993). Maximum Entropy and Bayesian 
Methods. Dordrecht: Kluwer. 
Monahan, J. F. and Boos, D. D. (1992). Proper likelihoods for Bayesian analysis. Bio- 
metrika 79, 271-278. 
Morales, J. A. (1971). Bayesian Full Information Structural Analysis. Berlin: Springer. 
Moreno, E. and Cano, J. A. (1989). Testing a point null hypothesis: asymptotic robust 
Bayesian analysis with respect to priors given on a sub-sigma field. Internat. Statist. 
Rev. 57, 221-232. 
Moreno, E. and Cano, J. A. (1991). Robust Bayesian analysis with e-contaminations partially 
known. J. Roy. Statist. Soc. B 53, 143-155. 
Moreno, E. and Pericchi, L. R. (1992). Bands of probability measures: a robust Bayesian  
analysis. Bayesian Statistics 4 (J. M. Bernardo, J. O. Berger, A. P. Dawid and A. F. M. Smith, 
eds.). Oxford: University Press, 707-713. 
Moreno, E. and Pericchi, L. R. (1993). Prior assessments for bands of probability measures: 
empirical Bayesian analysis. Test 2, 101-110. 
Morgan, M. G. and Henrion, M. (1990). Uncertainty: a Guide to Dealing with Uncertainty 
in Quantitative Risk and Policy Analysis. Cambridge: University Press. 
Morris, C. N. (1982). Natural exponential families with quadratic variance functions. Ann. 
Statist. 10, 65-80. 
Morris, C. N. (1983). Parametric empirical Bayes inference: theory and applications. J. Amer. 
Statist. Assoc. 8, 47-59. 
Morris, C. N. (1988). Approximating posterior distributions and posterior moments.  
Bayesian Statistics 3 (J. M. Bernardo, M. H. DeGroot, D. V. Lindley and A. F. M. Smith, eds.). 
Oxford: University Press, 327-344 (with discussion). 
Morris, P. A. (1974). Decision analysis expert use. Manag. Sci. 20,1233-1241. 
Morris, W. T. (1968). Management Science, a Bayesian Introduction. Englewood Cliffs, NJ: 
Prentice-Hall. 
Mortera, J. (1986). Bayesian forecasting. Metron 44, 277-296. 
Mosteller, F. and Wallace, D. L. (1964/1984). Inference and Disputed Authorship: The 
Federalist. Reading, MA: Addison-Wesley. Second edition, published in 1984 as Applied 
Bayesian and Classical Inference, the Case of the Federalist Papers. Berlin: Springer. 
Mosteller, F. and Youtz, C. (1990). Quantifying probabilistic expressions. Statist. Sci. 5, 
2-24 (with discussion). 
Mouchart, M. (1976). A note on Bayes' theorem. Statistica 36, 349-357. 
Mouchart, M. and Simar, L. (1980). Least squares approximation in Bayesian analysis. 
Bayesian Statistics (J. M. Bernardo, M. H. DeGroot, D. V. Lindley and A. F. M. Smith, 
eds.). Valencia: University Press, 207-222 and 237-245 (with discussion). 
References 
535 
Muirhead, C. R. (1986). Distinguishing outlier types in time series. J. Roy. Statist. Soc. B 48, 
39-47. 
Mukerjee, R. and Dey, D. K. (1993). Frequentist validity of posterior quantiles in the presence 
of a nuisance parameter: Higher order asymptotics. Biometrika 80,499-505. 
Murphy, A. H. and Epstein, E. S. (1967). Verification of probabilistic predictions: a brief 
review. J. Appl. Meteorology 6,748-755. 
Murray, R. G., McKillop, J. H., Bessant, R. G., Hutton, I, Lorimer, A. R. and Lawrie, 
T. D. V. (1981). Bayesian analysis of stress thallium-201 scintigraphy. Eur. J. Nucl. Med. 
6,201-204. 
Myerson, R. B. (1979). An axiomatic derivation of subjective probability, utility and  
evaluation functions. Theory and Decision 11, 339-352. 
Nakamura, Y. (1993). Subjective utility with upper and lower probabilities on finite states. 
J. Risk and Uncertainty 6, 33-48. 
Narens, L. (1976). Utility, uncertainty and trade-off structures. J. Math. Psychol. 13, 296- 
332. 
Nau, R. F. (1992). Indeterminate probabilities on finite sets. Ann. Statist. 20, 1737-1767. 
Nau, R. F. and McCardle, K. F. (1990). Coherent behavior in non-cooperative games. J. 
Economic Theory 50, 242-444. 
Naylor, J. C. and Smith, A. F. M. (1982). Applications of a method for the efficient  
computation of posterior distributions. Appl. Statist. 31, 214-225. 
Naylor, J. C. and Smith, A. F. M. (1988). Economic illustrations of novel numerical  
integration methodology for Bayesian inference. J. Econometrics 38,103-125. 
Nelder, J. A. and Wedderburn, R. W. M. (1972). Generalised linear models. J. Roy. Statist. 
Soc. A 135, 370-384. 
Neyman, J. (1935). Sur un teorema concerte le cosidette statistiche sufficenti. Giorn. 1st. 
Ital. 6, 320-334. 
Neyman, J. and Pearson, E. S. (1933). On the problem of the most efficient tests of statistical 
hypothesis. Phil. Trans. Roy. Soc. London A 231, 289-337. 
Neyman, J. and Pearson, E. S. (1967). Joint Statistical Papers. Cambridge: University Press. 
Neyman, J. and Scott, E. L. (1948). Consistent estimates based on partially consistent  
observations. Econometrica 16,1-32. 
Nicolau, A. (1993). Bayesian intervals with good frequentist behaviour in the presence of 
nuisance parameters. J. Roy. Statist. Soc. B 55, 377-390. 
Normand, S.-L. and Tritchler, D. (1992). Parameter updating in a Bayes network. / Amer. 
Statist. Assoc. 87, 1109-1115. 
Novick, M. R. (1969). Multiparameter Bayesian indifference procedures. J. Roy. Statist. 
Soc. B 31,29-64. 
Novick, M. R. and Hall, W. K. (1965). A Bayesian indifference procedure. J. Amer. Statist. 
Assoc. 60,1104-1117. 
Novick, M. R. and Jackson, P. H. (1974). Statistical Methods for Educational and  
Psychological Research. New York: McGraw-Hill. 
O'Hagan, A. (1979). On outlier rejection phenomena in Bayes inference. J. Roy. Statist. 
Soc. B 41, 358-367. 
O'Hagan, A. (1981). A moment of indecision. Biometrika 68, 329-330. 
536 
References 
O'Hagan, A. (1988a). Probability: Methods and Measurements. London: Chapman and Hall. 
'Hagan, A. (1988b). Modelling with heavy tails. Bayesian Statistics 3 (J. M. Bernardo, 
M. H. DeGroot, D. V. Lindley and A. F. M. Smith, eds.). Oxford: University Press, 
345-359 (with discussion). 
O' Hagan, A. (1990). Outliers and credence for location parameter inference. J. Amer. Statist. 
Assoc. 85, 172-176. 
O'Hagan, A. (1991). Bayes-Hermite quadrature. J. Statist. Planning and Inference 29,245- 
260. 
O'Hagan, A. (1992). Some Bayesian numerical analysis. Bayesian Statistics 4 (J. M.  
Bernardo, J. O. Berger, A. P. Dawid and A. F. M. Smith, eds.). Oxford: University Press, 
345-363 (with discussion). 
O'Hagan, A. (1994a). Kendall's Advanced Theory of Statistics 2B: Bayesian Inference. 
London: Edward Arnold 
O'Hagan, A. (1994b). Robust modelling for asset management. J. Statist. Planning and 
Inference, (to appear). 
O'Hagan, A. and Berger, J. O. (1988). Ranges of posterior probabilities for quasimodal 
priors with specified quantiles. J. Amer. Statist. Assoc. 83, 503-508. 
O'Hagan, A. and Le, H. (1994). Conflicting information and a class of bivariate heavy-tailed 
distributions. Aspects of Uncertainty: a Tribute to D. V. Lindley (P. R. Freeman, and 
A. F. M. Smith, eds.). Chichester: Wiley, (to appear). 
Oliver, R. M. and Smith, J. Q. (eds.) (1990). Influence Diagrams, Belief Nets and Decision 
Analysis. Chichester: Wiley. 
Osiewalski, J. and Steel, M. F. J. (1993). Robust Bayesian inference in /,-spherical models. 
Biometrika 80,456-460. 
Osteyee, D. D. B. and Good, I. J. (1974). Information, Weight of Evidence, the Singularity 
between Probability Measures and Signal Detection. Berlin: Springer. 
Pack, D. J. (1986a). Posterior distributions. Posterior probabilities. Encyclopedia of  
Statistical Sciences 7 (S. Kotz, N. L. Johnson and C. B. Read, eds.). New York: Wiley, 
121-124. 
Pack, D. J. (1986b). Prior distributions. Encyclopedia of Statistical Sciences 7 (S. Kotz, 
N. L. Johnson and C. B. Read, eds.). New York: Wiley, 194-196. 
Padgett, W. J. and Wei, L. J. (1981). A Bayesian nonparametric estimator of survival  
probability assuming increasing failure rate. Comm. Statist. Theory and Methods 10,49-63. 
Page, A. N. (ed.) (1968). Utility Theory: A Book of Readings. New York: Wiley. 
Pardo, L., Taneja, I. J. and Morales, D. (1991). A-measures of hypoentropy and comparison 
of experiments: Bayesian approach. The Statistician 51, 173-184. 
Parenti, G. (ed.) (1978). I Fondamenti dell'Inferenza Statistica. Florence: Universita degli 
Studi. 
Parmigiani, G. and Berry, D. A. (1994). Applications of Lindley information to the design of 
clinical experiments. Aspects of Uncertainty: a Tribute to D. V. Lindley (P. R. Freeman, 
and A. F. M. Smith, eds.). Chichester: Wiley, (to appear). 
Pearson, E. S. (1978). The History of Statistics in the 17th and I8th Centuries. London: 
Macmillan. 
\f 
References 
537 
Peers, H. W. (1965). On confidence points and Bayesian probability points in the case of 
several parameters. J. Roy. Statist. Soc. B 27, 9-16. 
Peers, H. W. (1968). Confidence properties of Bayesian interval estimates. J. Roy. Statist. 
Soc. B 30,535-544. 
Peirce, C. S. (1878). How to make our ideas clear. Popular Science Monthly 12, 286-302. 
Peizer, D. B. and Pratt, J. W. (1968). A normal approximation for binomial, F, beta, and 
other common related tail probabilities. J. Amer. Statist. Assoc. 43, 24-26. 
Pena, D. and Guttman, I. (1993). Comparing probabilistic methods for outlier detection. 
Biometrika 80, 603-610. 
Pena, D. and Tiao, G. C. (1992). Bayesian robustness functions for linear models. Bayesian 
Statistics 4 (J. M. Bernardo, J. O. Berger, A. P. Dawid and A. F. M. Smith, eds.). Oxford: 
University Press, 365-388 (with discussion). 
Pereira, C. A. de B. and Lindley, D. V. (1987). Examples questioning the use of partial 
likelihood. The Statistician 37, 15-20. 
Perez, M. E. and Pericchi, L. R. (1992). Analysis of multistage survey as a hierarchical model. 
Bayesian Statistics 4 (J. M. Bernardo, J. O. Berger, A. P. Dawid and A. F. M. Smith, 
eds.). Oxford: University Press, 723-730. 
Pericchi, L. R. (1981). A Bayesian approach to transformations to normality. Biometrika 68, 
35^3. 
Pericchi, L. R. (1984). An alternative to the standard Bayesian procedure for discrimination 
between normal linear models. Biometrika 71, 576-586. 
Pericchi, L. R. (1993). Personal communication. 
Pericchi, L. R. and Nazaret, W. A. (1988). On being imprecise at the higher levels of a  
hierarchical linear model. Bayesian Statistics 3 (J. M. Bernardo, M. H. DeGroot, D. V. Lindley 
and A. F. M. Smith, eds.). Oxford: University Press, 361-375 (with discussion). 
Pericchi, L. R. and Perez, M. E. (1994). Posterior robustness with more than one sampling 
model. J. Statist. Planning and Inference , (to appear). 
Pericchi, L. R., Sans6, B. and Smith, A. F M. (1993). Posterior cumulant relationships in 
Bayesian inference involving the exponential family. J. Amer. Statist. Assoc. 88, 1419— 
1426. 
Pericchi, L. R. and Smith, A. F. M. (1992). Exact and approximate posterior moments for a 
normal location parameter. J. Roy. Statist. Soc. B 54, 793-804. 
Pericchi, L. R. and Walley, P. (1991). Robust Bayesian credible intervals and prior ignorance. 
Internal. Statist. Rev. 59, 1-23. 
Perks, W. (1947). Some observations on inverse probability, including a new indifference 
rule. / Inst. Actuaries 73, 285-334 (with discussion). 
Peskun, P. H. (1973). Optimal Monte Carlo sampling using Markov chains. Biometrika 60, 
607-612. 
Pettit, L. I. (1986). Diagnostics in Bayesian model choice. The Statistician 35,183-190. 
Pettit, L. I. (1992). Bayes factors for outlier models using the device of imaginary  
observations. J. Amer. Statist. Assoc. 87, 541-545. 
Pettit, L. I. and Smith, A. F M. (1985). Outliers and influential observation in linear models. 
Bayesian Statistics 2 (J. M. Bernardo, M. H. DeGroot, D. V. Lindley and A. F. M. Smith, 
eds.), Amsterdam: North-Holland, 473-494 (with discussion). 
538 
References 
Pettit, L. I. and Young, K. S. (1990). Measuring the effect of observations on Bayes factors. 
Biometrika 77,455-466. 
Pfanzagl, J. (1967). Subjective probability derived from the Morgenstem-von Neumann  
utility concept. Essays in Mathematical Economics (M. Shubik, ed.). Princeton: University 
Press, 237-251. 
Pfanzagl, J. (1968). Theory of Measurement. Chichester: Wiley. 
Pham-Gia, T. and Turkkan, N. (1992). Sample size determination in Bayesian analysis. The 
Statistician 41, 389-404. 
Phillips, L. D. (1973). Bayesian Statistics for Social Scientists. London: Nelson. 
Piccinato, L. (1973). Un metodo per determinare distribuzioni iniziali relativamente non- 
informative. Metron 31, 124-156. 
Piccinato, L. (1977). Predictive distributions and non-informative priors. Trans. 7th. Prague 
Conf. Information Theory (M. Uldrich, ed.). Prague: Czech. Acad. Sciences, 399-407. 
Piccinato, L. (1986). De Finetti's logic of uncertainty and its impact on statistical thinking 
and practice. Bayesian Inference and Decision Techniques: Essays in Honor of Bruno 
de Finetti (P. K. Goel and A. Zellner, eds.). Amsterdam: North-Holland, 13-20. 
Piccinato, L. (1992). Critical issues in different inferential paradigms. J. It. Statist. Soc. 2, 
251-274. 
Pierce, D. (1973). On some difficulties in a frequency theory of inference. Ann. Statist. 1, 
241-250. 
Pilz, J. (1983/1991). Bayesian Estimation and Experimental Design in Linear Regression 
Models. Leipzig: Teubner. Second edition in 1991, Chichester: Wiley. 
Pitman E. J. G. (1936). Sufficient statistics and intrinsic accuracy. Proc. Camb. Phil Soc. 32, 
567-579. 
Pitman E. J. G. (1939). Location and scale parameters. Biometrika 36, 391-421. 
Plante, A. (1971). Counter-example and likelihood. Foundations of Statistical Inference 
(V. P. Godambe and D. A. Sprott, eds.). Toronto: Holt, Rinehart and Winston, 357-371 
(with discussion). 
Plante, A. (1984). A reexamination of Stein's antiflducial example. Canad. J. Statist. 12, 
135-141. 
Plante, A. (1991). An inclusion-consistent solution to the problem of absurd confidence 
statements. Canad. J. Statist. 19, 389-397. 
Poirier, D. J. (1985). Bayesian hypothesis testing in linear models with continuously induced 
conjugate priors across hypotheses. Bayesian Statistics 2 (J. M. Bernardo, M. H. DeGroot, 
D. V. Lindley and A. F. M. Smith, eds.), Amsterdam: North-Holland, 711-722. 
Poirier, D. J. (1993). Intermediate Statistics and Econometrics: a Comparative Approach. 
Cambridge, MA: The MIT Press. 
Polasek, W and Potzelberger, K. (1988). Robust Bayesian analysis in hierarchical models. 
Bayesian Statistics 3 (J. M. Bernardo, M. H. DeGroot, D. V. Lindley and A. F M. Smith, 
eds.). Oxford: University Press, 377-394. 
Polasek, W and Potzelberger, K. (1994). Robust Bayesian methods in simple ANOVA 
problems. /. Statist. Planning and Inference, (to appear). 
Pole, A. and West, M. (1989). Reference analysis of the dynamic linear model. J. Time Series 
Analysis 10,13-147. 
References 
539 
Pole, A., West, M. and Harrison P. J. (1994). Applied Bayesian Forecasting and Time Series 
Analysis (with computer software). London: Chapman and Hall, (to appear). 
Pollard, W. E. (1986). Bayesian Statistics for Evaluation Research: an Introduction. Beverly 
Hills, CA: Sage. 
Poison, N. G. (1991). A representation of the posterior mean for a location model. Bio- 
metrika 78,426^130. 
Poison, N. G. (1992). In discussion of Ghosh and Mukerjee (1992). Bayesian Statistics 4 
(J. M. Bernardo, J. O. Berger, A. P. Dawid and A. F. M. Smith, eds.). Oxford: University 
Press, 203-205. 
Poison, N. G. and Tiao, G. C. (1994). Bayesian Inference. Aldershot: Edward Elger. (to 
appear). 
Poskitt, D. S. (1987). Precision, complexity and Bayesian model determination. J. Roy. 
Statist. Soc. B 49, 199-208. 
Potzelberger, K. and Polasek, W. (1991). Robust HPD regions in Bayesian regression models. 
Econometrica 59, 1581-1590. 
Pratt, J. W. (1961). Length of confidence intervals. J. Amer. Statist. Assoc. 56, 549-567. 
Pratt, J. W. (1964). Risk aversion in the small and in the large. Econometrica 32, 122-136. 
Pratt, J. W. (1965). Bayesian interpretation of standard inference statements. J. Roy. Statist. 
Soc. 5 27,169-203. 
Pratt, J. W., Raiffa, H. and Schlaifer, R. (1964). The foundations of decision under uncertainty: 
an elementary exposition. J. Amer. Statist. Assoc. 59, 353-375. 
Pratt, J. W., Raiffa, H. and Schlaifer, R. (1965). Introduction to Statistical Decision Theory. 
New York: McGraw-Hill. 
Press, S. J. (1972/1982). Applied Multivariate Analysis: using Bayesian and Frequentist 
Methods of Inference. Second edition in 1982, Melbourne, FL: Krieger. 
Press, S.J. (1978). Qualitative controlled feedback for forming group judgements and making 
decisions. J. Amer. Statist. Assoc. 73,526-535. 
Press, S. J. (1980a). Bayesian Inference in MANOVA. Handbook of Statistics 1. Analysis of 
Variance. (P. R. Krishnaiah, ed.). Amsterdam: North-Holland, 117-132. 
Press, S. J. (1980b). Bayesian inference in group judgement formulation and decision making 
using qualitative controlled feedback. Bayesian Statistics (J. M. Bernardo, M. H. DeG- 
root, D. V. Lindley and A. F. M. Smith, eds.). Valencia: University Press, 383-430 (with 
discussion). 
Press, S. J. (1985a). Multivariate Analysis (Bayesian). Encyclopedia of Statistical Sciences 6 
(S. Kotz, N. L. Johnson and C. B. Read, eds.). New York: Wiley, 16-20. 
Press, S. J. (1985b). Multivariate group assessment of probabilities of nuclear war. Bayesian 
Statistics 2 (J. M. Bernardo, M. H. DeGroot, D. V. Lindley and A. F. M. Smith, eds.), 
Amsterdam: North-Holland, 425-462 (with discussion). 
Press, S. J. (1989). Bayesian Statistics. New York: Wiley. 
Rabena, M. (1994). On the derivation of reference decisions. Test 3, (to appear). 
Racine-Poon, A. (1988). A Bayesian approach to non-linear calibration problems. J. Amer. 
Statist. Assoc. 83, 650-656. 
Racine-Poon, A. (1992). SAGA: Sample assisted graphical analysis. Bayesian Statistics 4 
(J. M. Bernardo, J. O. Berger, A. P. Dawid and A. F. M. Smith, eds.). Oxford: University 
Press, 389-404 (with discussion). 
540 
References 
Racine-Poon, A., Grieve, A. P., Fliihler, H. and Smith, A. F. M. (1986). Bayesian methods 
in practice: experiences in the pharmaceutical industry. Appl. Statist. 35, 93-150 (with 
discussion). 
Raftery, A. E. and Lewis, S. M. (1992). How many iterations in the Gibbs sampler? Bayesian 
Statistics 4 (J. M. Bernardo, J. O. Berger, A. P. Dawid and A. F. M. Smith, eds.). Oxford: 
University Press, 763-773. 
Raftery, A. E. and Schweder, T. (1993). Inference about the ratio of two parameters, with 
applications to whale censusing. Amer. Statist. 47, 259-264. 
Raiffa, H. (1961). Risk ambiguity and the Savage axioms. Comment. Quart. J. Econ. 75, 
690-694. 
Raiffa, H. (1968). Decision Analysis. Introductory Lectures on Choices under Uncertainty. 
Reading, MA: Addison-Wesley 
Raiffa, H. (1982). The Art and Science of Negotiation. Cambridge: University Press 
Raiffa, H. and Schlaifer, R. (1961). Applied Statistical Decision Theory. Boston: Harvard 
University. 
Ramsey, F. P. (1926). Truth and probability. The Foundations of Mathematics and Other 
Logical Essays (R. B. Braithwaite, ed.). London: Kegan Paul (1931), 156-198. Reprinted 
in 1980 in Studies in Subjective Probability (H. E. Kyburg and H. E Smokier, eds.). New 
York: Dover, 61-92. 
Ramsey, J. O. and Novick, M. R. (1980). PLU robust Bayesian decision theory: point  
estimation. J. Amer. Statist. Assoc. 75, 901-907. 
Randall, C. H. andFoulis, D. J. (1975). A mathematical setting for inductive reasoning.  
Foundations of Probability Theory, Statistical Inference, and Statistical Theories of Science 3 
(W. L. Harper and C. A. Hooker, eds.). Dordrecht: Reidel. 
Rao, C. R. (1945). Information and accuracy attainable in estimation of statistical parameters. 
Bull. Calcutta Math. Soc. 37, 81-91. 
Regazzini, E. (1983). Sulle Probabilita Coerenti nel Senso di de Finetti. Bologna: Clueb. 
Regazzini, E. (1987). De Finetti's coherence and statistical inference. Ann. Statist. 15, 845- 
864. 
Regazzini, E. and Petris, G. (1992). Some critical aspects of the use of exchangeability in 
statistics. J. It. Statist. Soc. 1, 103-130. 
Reichenbach, H. (1935). The Theory of Probability. Berkeley: Univ. California Press. 
Renyi, A. (1955). On a new axiomatic theory of probability. Acta Math. Acad. Sci. Hungar- 
icae 6, 285-335. 
Renyi, A. (1961). On measures of entropy and information. Proc. Fourth Berkeley Symp. 1 
(J. Neyman and E. L. Scott, eds.). Berkeley: Univ. California Press, 547-561. 
Renyi, A. (1962/1970). Wahrscheinlichkeitsrechnung. Berlin: Deutscher Verlag der Wis- 
senschaften. English translation in 1970 as Probability Theory. San Francisco, CA: 
Holden-Day. 
Renyi, A. (1964). On the amount of information concerning an unknown parameter in a 
sequence of observations. Pub. Math. Inst. Hung. Acad Sci. 9, 617-624. 
Renyi, A. (1966). On the amount of missing information and the Neyman-Pearson lemma. 
Research Papers in Statistics. Festschrift for J. Neyman (F. N. David, ed.). New York: 
Wiley, 281-288. 
References 
541 
Renyi, A. (1967). On some basic problems of statistics from the point of view of information 
theory. Proc. Fifth Berkeley Symp. 1 (J. Neyman and E. L. Scott, eds.). Berkeley: Univ. 
California Press, 531-543. 
Ressel, P. (1985). de Finetti type theorems: an analytical approach. Ann. Prob. 13, 818-922. 
Richard,J. F (1973). Posterior and Predictive Densities forSimultaneous Equations Models. 
Berlin: Springer. 
Rfos, D. (1990). Sensitivity Analysis in Multiobjective Decision Making. Berlin: Springer. 
Rfos, D. (1992). Foundations for a robust theory of decision making: the simple case. Test 1, 
69-78. 
Rios, D. and Martin, J. (1994). Robustness issues under precise beliefs and preferences. 
J. Statist. Planning and Inference , (to appear). 
Rfos, S. (1977). Andlisis de Decisiones. Madrid: ICE. 
Rfos, S., Rfos, S. Jr. and Rios, M. J. (1989). Procesos de Decision Multicriterio. Madrid: 
Eudema. 
Ripley, B. D. (1987). Stochastic Simulation. Chichester: Wiley. 
Rissanen, J. (1983). A universal prior for integers and estimation by minimum description 
length. Ann. Statist. 11, 416-431. 
Rissanen, J. (1987). Stochastic complexity. J. Roy. Statist. Soc. B 49, 223-239 and 252-265 
(with discussion). 
Rissanen, J. (1989). Stochastic Complexity in Statistical Enquiry. Singapore: World  
Scientific. 
Ritter, C, and Tanner, M. A. (1992). Facilitating the Gibbs sampler: the Gibbs stopper and 
the griddy-Gibbs sampler. J. Amer. Statist. Assoc. 87, 861-868. 
Rivadulla, A. (1991). Probabilidad e Inferencia Cientifica. Barcelona: Anthropos. 
Robbins, H. (1955). An empirical Bayes approach to statistics. Proc. Third Berkeley Symp. 1 
(J. Neyman and E. L. Scott, eds.). Berkeley: Univ. California Press, 157-164. 
Robbins, H. (1964). The empirical Bayes approach to statistical decision problems. Ann. 
Math. Statist. 35, 1-20. 
Robbins, H. (1983). Some thoughts on empirical Bayes estimation. Ann. Statist. 1,713-723. 
Robert, C. P. (1992). UAnalyse Statistique Bayesienne. Paris: Economica. 
Robert, C. P. (1993). A note on Jeffreys-Lindley paradox. Statistica Sinica 3, 603-608. 
Robert, C. P., Hwang, J. T. G. and Strawderman, W E. (1993). Is Pitman closeness a  
reasonable criterion? J. Amer. Statist. Assoc. 88, 57-76 (with discussion). 
Robert, C. P. and Soubiran, C. (1993). Estimation of a normal mixture model through Gibbs 
sampling and prior feedback. Test 2,125-146. 
Roberts, F. (1974). Laws of exchange and their applications. SIAM J. Appl. Math. 26, 260- 
284. 
Roberts, F. (1979). Measurement Theory. Reading, MA: Addison-Wesley 
Roberts, G. O. (1992). Convergence diagnostics of the Gibbs sampler. Bayesian Statistics 4 
(J. M. Bernardo, J. O. Berger, A. P. Dawid and A. F M. Smith, eds.). Oxford: University 
Press, 775-782. 
Roberts, G. O. and Smith, A. F. M. (1993). Simple conditions for the convergence of the 
Gibbs sampler and Metropolis-Hastings algorithms. Stoch. Proc. and their Applic. 44, 
(to appear). 
542 
References 
Roberts, H. V. (1963). Risk ambiguity and the Savage axioms. Comment. Quart. J. Econ. 
77, 327-342. 
Roberts, H. V. (1965). Probabilistic prediction. / Amer. Statist. Assoc. 60, 50-62. 
Roberts, H. V. (1966). Statistical Inference and Decision. Chicago: University Press. 
Roberts, H. V. (1967). Informative stopping rules and inferences about population size. 
J. Amer. Statist. Assoc. 62, 763-775. 
Roberts, H. V. (1974). Reporting of Bayesian studies. Studies in Bayesian Econometrics 
and Statistics: in Honor of Leonard J. Savage (S. E. Fienberg and A. Zellner, eds.). 
Amsterdam: North-Holland, 465-^183. 
Roberts, H. V. (1978). Bayesian inference. International Encyclopedia of Statistics (W. H. 
Kruskal, and J. M. Tanur, eds.). London: Macmillan, 9-16. 
Robinson, G. K. (1975). Some counter-examples to the theory of confidence intervals. Bio- 
metrika 62, 155-161. 
Robinson, G. K. (1979a). Conditional properties of statistical procedures. Ann. Statist. 7, 
742-755. 
Robinson, G. K. (1979b). Conditional properties of statistical procedures for location and 
scale parameters. Ann. Statist. 7, 756-771. 
Rodriguez, C. C. (1991). From Euclid to entropy. Maximum Entropy and Bayesian Methods 
(W. T. Grandy and L. H. Schick eds.). Dordrecht: Kluwer, 343-348. 
Rolin, J.-M. (1983). Non-parametric Bayesian statistics: a stochastic processes approach. 
Specifying Statistical Models (J.-P. Florens etal. eds.). Berlin: Springer. 108-133. 
Rosenkranz, R. D. (1977). Inference, Method and Decision. Towards a Bayesian Philosophy 
of Science. Dordrecht: Reidel. 
Royall, R. M. (1992). The elusive concept of statistical evidence. Bayesian Statistics 4 
(J. M. Bernardo, J. O. Berger, A. P. Dawid and A. F. M. Smith, eds.). Oxford: University 
Press, 405-418 (with discussion). 
Rubin, D. B. (1981). The Bayesian bootstrap. Ann. Statist. 9, 130-134. 
Rubin, D.B. (1984). Bayesianly justifiable and relevant frequency calculations for the applied 
statistician. Ann. Statist. 12, 1151-1172. 
Rubin, D. B. (1987). Multiple Imputation for Non-Response in Surveys. New York: Wiley 
Rubin, D. B. (1988). Using the SIR algorithm to simulate posterior distributions. Bayesian 
Statistics 3 (J. M. Bernardo, M. H. DeGroot, D. V. Lindley and A. F. M. Smith, eds.). 
Oxford: University Press, 395-402 (with discussion). 
Rubin, H. (1971). A decision-theoretic approach to the problem of testing a null hypothesis. 
Statistical Decision Theory and Related Topics (S. S. Gupta and J. Yackel, eds.). New 
York: Academic Press, 103-108. 
Rubin, H. (1977). Robust Bayesian estimation. Statistical Decision Theory and Related 
Topics //(S. S. Gupta and D. S. Moore, eds.). New York: Academic Press, 
Rubin, H. (1987). A weak system of axioms for 'rational' behaviour and the non-separability 
of utility from prior. Statistics and Decisions 5,47-58. 
Rubin, H. (1988a). Some results on robustness in testing. Statistical Decision Theory and 
Related Topics TV 1 (S. S. Gupta and J. O. Berger, eds.). Berlin: Springer, 271-278. 
Rubin, H. (1988b). Robustness in generalized ridge regression and related topics. Bayesian 
Statistics 3 (J. M. Bernardo, M. H. DeGroot, D. V. Lindley and A. F. M. Smith, eds.). 
Oxford: University Press, 403-410 (with discussion). 
References 
543 
Rueda, R. (1992). A Bayesian alternative to parametric hypothesis testing. Test 1, 61-67. 
Saaty, T. L. (1980). The Analytic Hierarchy Process. New York: McGraw-Hill. 
Sacks, J. (1963). Generalized Bayes solutions in estimation problems. Ann. Math. Statist. 34, 
787-794. 
Salinetti, G. (1994). Stability of Bayesian decisions. J. Statist. Planning and Inference , (to 
appear). 
San Martini, A. and Spezzaferri F. (1984). A predictive model selection criterion. J. Roy. 
Statist. Soc. B 46, 296-303. 
Sans6, B. and Pericchi, L. R. (1992). Near ignorance classes of log-concave priors for the 
location model. Test 1, 39-46. 
Sarndal C.-E. (1970). A class of explicata for 'information' and 'weight of evidence'. Inter- 
nat. Statist. Rev. 38, 223-235. 
Savage, I. R. (1968). Statistics: Uncertainty and Behavior. Boston: Houghton Miffin. 
Savage, I. R. (1980). On not being rational. Bayesian Statistics (J. M. Bernardo, M. H. De- 
Groot, D. V. Lindley and A. F. M. Smith, eds.). Valencia: University Press, 321-328 and 
339-346 (with discussion). 
Savage, L. J. (1954/1972). The Foundations of Statistics. New York: Wiley. Second edition 
in 1972, New York: Dover. 
Savage, L. J. (1962) (with others). The Foundations of Statistical Inference: a Discussion. 
London: Methuen. 
Savage, L. J. (1961). The foundations of statistics reconsidered. Proc. Fourth Berkeley 
Symp. 1 (J. Neyman and E. L. Scott, eds.). Berkeley: Univ. California Press, 575-586. 
Reprinted in 1980 in Studies in Subjective Probability (H. E. Kyburg and H. E Smokier, 
eds.). New York: Dover, 175-188. 
Savage, L. J. (1970). Reading suggestions for the foundations of statistics. Amer. Statist. 24, 
23-27. 
Savage, L. J. (1971). Elicitation of personal probabilities and expectations. J. Amer. Statist. 
Assoc. 66, 781-801. Reprinted in 1974 in Studies in Bayesian Econometrics and  
Statistics: in Honor of Leonard J. Savage (S. E. Fienberg and A. Zellner, eds.). Amsterdam: 
North-Holland, 111-156. 
Savage, L. J. (1981). The Writings of Leonard Jimmie Savage: a Memorial Collection. 
Washington: ASA/IMS. 
Savchuk, V. P. (1989). Bayesovskiye Metodi Statisticheskogo Otsenivaniya. Moscow: Nauka. 
Sawagari, Y, Sunahara, Y and Nakamizo, T. (1967). Statistical Decision Theory in Adaptive 
Control Systems. New York: Academic Press. 
Schervish, M. J., Seidenfeld, T. and Kadane, J. B. (1990). State-dependent utilities. J. Amer. 
Statist. Assoc. 85, 840-847. 
Schervish, M. J., Seidenfeld, T. and Kadane, J. B. (1992). Bayesian analysis of linear models. 
Bayesian Statistics 4 (J. M. Bernardo, J. O. Berger, A. P. Dawid and A. F M. Smith, 
eds.). Oxford: University Press, 419-434 (with discussion). 
Schlaifer, R. (1959). Probability and Statistics for Business Decisions. New York: McGraw- 
Hill. 
Schlaifer, R. (1961). Introduction to Statistics for Business Decisions. New York: McGraw- 
Hill. 
544 
References 
Schlaifer, R. (1969). Analysis of Decisions under Uncertainty. New York: McGraw-Hill. 
Schmitt, S. A. (1969). Measuring Uncertainty: an Elementary Introduction to Bayesian 
Statistics. Reading, MA: Addison-Wesley 
Schwartz, L. (1965). On Bayes procedures. Z. Wahr. 4,10-26. 
Schwarz, G. (1978). Estimating the dimension of a model. Ann. Statist. 6, 461-464. 
Scott, D. (1964). Measurement structures and linear inequalities. J. Math. Psychology 1, 
233-247. 
Scozzafava, R. (1989). La Probability Soggettiva e le sue Applicazioni. Milano: Veschi. 
Seidenfeld, T. (1979). Philosophical Problems of Statistical Inference. Dordrecht: Reidel. 
Seidenfeld, T. (1992). R. A. Fisher's fiducial argument and Bayes' theorem. Statist. Sci. 7, 
358-368. 
Seidenfeld, T., Kadane, J. B. and Schervish, M. J. (1989). On the shared preferences of two 
Bayesian decision makers. J. of Psychology 5, 225-244. 
Seidenfeld, T. and Schervish, M. J. (1983). A conflict between finite additivity and avoiding 
Dutch book. Philos. of Science 50, 398-112. 
Sen, A. K. (1970). Collective Choice and Social Welfare. San Francisco, CA: Holden-Day. 
Serfling, R. J. (1980). Approximation Theorems of Mathematical Statistics. New York: Wiley. 
Shafer, G. (1976). A Mathematical Theory of Evidence. Princeton: University Press. 
Shafer, G. (1982a). Belief functions and parametric models. J. Roy. Statist. Soc. B 44, 322- 
352 (with discussion). 
Shafer, G. (1982b). Lindley's paradox. J.Amer. Statist. Assoc. 77,325-351 (with discussion). 
Shafer, G. (1986). Savage revisited. Statist. Sci. 1,435-462 (with discussion). 
Shafer, G. (1990). The unity and diversity of probability. Statist. Sci. 5, 463-501 (with 
discussion). 
Shannon, C. E. (1948). A mathematical theory of communication. Bell System Tech. J. 27 
379-423 and 623-656. Reprinted in The Mathematical Theory of Communication  
(Shannon, C. E. and Weaver, W., 1949). Urbana, IL.: Univ. Illinois Press. 
Shao, J. (1989). Monte Carlo approximations in Bayesian decision theory. J. Amer. Statist. 
Assoc. 84, 727-732. 
Shao, J. (1990). Limiting behaviour of Monte Carlo approximation to Bayesian action. 
Statistics and Decisions 8, 85-99. 
Shao, J. (1993). Linear model selection by cross-validation. J. Amer. Statist. Assoc. 88, 
486-494. 
Shaw, J. E. H. (1988a). A quasi-random approach to integration in Bayesian statistics. Ann. 
Statist. 16, 895-914. 
Shaw, J. E. H. (1988b). Aspects of numerical integration and summarisation. Bayesian 
Statistics 3 (J. M. Bernardo, M. H. DeGroot, D. V. Lindley and A. F. M. Smith, eds.). 
Oxford: University Press, 411-428 (with discussion). 
Simon, J. C. (1984). La Reconnaissance des Formes. Paris: Masson. 
Simpson, E. H. (1951). The interpretation of interaction in contingency tables. J. Roy. Statist. 
Soc. 5 13,238-241. 
Singpurwalla, N. D. and Soyer, R. (1992). Non homogeneous autoregressive processes 
for tracking (software) reliability growth, and their Bayesian analysis. J. Roy. Statist. 
Soc. B 54, 145-156. 
References 
545 
Singpurwalla, N. D. and Wilson, S. P. (1992). Warranties. Bayesian Statistics 4 (J. M.  
Bernardo, J. O. Berger, A. P. Dawid and A. F. M. Smith, eds.). Oxford: University Press, 
435^446 (with discussion). 
Sivaganesan S. (1991). Sensitivity of some standard Bayesian estimates to prior uncertainty: 
a comparison. J. Statist. Planning and Inference 27, 85-103. 
Sivaganesan S. (1993). Robust Bayesian analysis of the binomial empirical Bayes problems. 
Canadian J. Statist. 21, 107-119. 
Sivaganesan S. and Berger, J. 0. (1989). Ranges of posterior measures for priors with 
unimodal contamination. Ann. Statist. 17, 868-889. 
Skene, A. M., Shaw, J. E. H. and Lee, T. D. (1986). Bayesian modelling and sensitivity 
analysis. The Statistician 35, 281-288. 
Skilling, J. (ed.) (1989). Maximum Entropy and Bayesian Methods. Dordrecht: Kluwer. 
Smith, A. F. M. (1973a). Bayes estimates in one-way and two way models. Biometrika 60, 
319-330. 
Smith, A. F. M. (1973b). A general Bayesian linear model. J. Roy. Statist. Soc. B 35,67-75. 
Smith, A. F M. (1978). In discussion of Tanner (1978). / Roy. Statist. Soc. A 141, 50-51. 
Smith, A. F M. (1981). On random sequences with centred spherical symmetry. J. Roy. 
Statist. Soc. B 43, 208-209. 
Smith, A. F M. (1983). Bayesian approaches to outliers and robustness. Specifying Statistical 
Models (J.-P. Florens, M. Mouchart, J.-R Raoult, L. Simar and A. F. M. Smith, eds.). 
Berlin: Springer, 13-55. 
Smith, A. F M. (1984). Bayesian Statistics. Present position and potential developments: 
some personal views. J. Roy. Statist. Soc. A 147. 245-259 (with discussion). 
Smith, A. F M. (1986). Some Bayesian thoughts on modeling and model choice. The  
Statistician 35, 97-102. 
Smith, A. F M. (1988). What should be Bayesian about Bayesian software? Bayesian 
Statistics 3 (J. M. Bernardo, M. H. DeGroot, D. V. Lindley and A. F M. Smith, eds.). 
Oxford: University Press, 429-435 (with discussion). 
Smith, A. F M. (1991). Bayesian computational methods. Phil. Trans. Roy. Soc. London 
A 337, 369-386. 
Smith, A. F M. and Dawid, A. P. (eds.) (1987). 1986 Conference on Practical Bayesian 
Statistics. Special issue, The Statistician 36, Numbers 2 and 3. 
Smith, A. F M. and Gelfand, A. E. (1992). Bayesian statistics without tears: a sampling- 
resampling perspective. Amer. Statist. 46, 84-88. 
Smith, A. F M. and Roberts, G. O. (1993). Bayesian computation via the Gibbs sampler 
and related Markov chain Monte Carlo methods. J. Roy. Statist. Soc. B 55, 3-23 (with 
discussion). 
Smith, A. F M., Skene, A. M., Shaw, J. E. H. and Naylor, J. C. (1987). Progress with 
numerical and graphical methods for Bayesian statistics. The Statistician 36, 75-82. 
Smith, A. F M., Skene, A. M., Shaw, J. E. H., Naylor, J. C. and Dransfield, M. (1985). 
The implementation of the Bayesian paradigm. Comm. Statist. Theory and Methods 14, 
1079-1109. 
Smith, A. F M. and Spiegelhalter, D. J. (1980). Bayes factors and choice criteria for linear 
models. J. Roy. Statist. Soc. B 42, 213-220. 
546 
References 
Smith, A. F. M. and Verdinelli, I. (1980). A note on Bayes designs for inference using a 
hierarchical linear model. Biometrika 47, 613-619. 
Smith, C. A. B. (1961). Consistency in statistical inference and decision. J. Roy. Statist. 
Soc. B 23, 1-37 (with discussion). 
Smith, C. A. B. (1965). Personal probability and statistical analysis. J. Roy. Statist. Soc. A128, 
469-199. 
Smith, C. R. and Erickson, J. G. (eds.) (1987). Maximum Entropy and Bayesian Spectral 
Analysis and Estimation Problems. Dordrecht: Reidel. 
Smith, C. R. and Grandy, W. T. (eds.) (1985). Maximum Entropy and Bayesian Methods in 
Inverse Problems. Dordrecht: Reidel. 
Smith, J. Q. (1988a). Decision Analysis, a Bayesian Approach. London: Chapman and Hall. 
Smith, J. Q. (1988b). Models, optimal decisions and influence diagrams. Bayesian Statistics 3 
(J. M. Bernardo, M. H. DeGroot, D. V. Lindley and A. F M. Smith, eds.). Oxford: 
University Press, 765-776. 
Smith, J. Q. (1992). A comparison of the characteristics of some Bayesian forecasting models. 
Internal. Statist. Rev. 60, 75-87. 
Smith, J. Q. and Gathercole, R. B. (1986). Principles of interactive forecasteing. Bayesian 
Inference and Decision Techniques: Essays in Honor of Bruno de Finetti (P. K. Goel and 
A. Zellner, eds.). Amsterdam: North-Holland, 405-^123. 
Smouse, E. P. (1984). A note on Bayesian least squares inference for finite population models. 
J. Amer. Statist. Assoc. 79, 390-392. 
Solomonoff, R. J. (1978). Complexity based induction systems: comparison and convergence 
theorems. IEEE Trans. Information Theory 24, 422-432. 
Spall, J. C. (ed.) (1988). Bayesian Analysis of Time Series and Dynamic Models. New York: 
Marcel Dekker. 
Spall, J. C. and Hill, S. D. (1990). Least informative Bayesian prior distributions for finite 
samples based on information theory. IEEE Trans. Automatic Control 35, 580-583. 
Spall, J. C. and Maryak, J. C. (1992). A feasible Bayesian estimator of quantiles forprojectile 
accuracy fori.d.d. data. J. Amer. Statist. Assoc. 87, 676-681. 
Spiegelhalter, D. J. (1987). Probability expert systems in medicine: practical issues in  
handling uncertainty. Statist. Sci. 2, 25-34 (with discussion). 
Spiegelhalter, D. J. and Cowell, R. G. (1992). Learning in probabilistic expert systems. 
Bayesian Statistics 4 (J. M. Bernardo, J. O. Berger, A. P. Dawid and A. F M. Smith, 
eds.). Oxford: University Press, 447-465 (with discussion). 
Spiegelhalter, D. J. and Knill-Jones, R. (1984). Statistical and knowledge-based approaches 
to clinical decision support systems with application in gastroenterology. / Roy. Statist. 
Soc. A 147, 34-77 (with discussion). 
Spiegelhalter, D. J. and Smith, A. F. M. (1982). Bayes factors for linear and log-linear models 
with vague prior information. J. Roy. Statist. Soc. B 44, 377-387. 
Stael von Holstein, C.-A. S. (1970). Assessment and Evaluation of Subjective Probability 
Distributions. Stockholm: School of Economics. 
Stael von Holstein, C.-A. S. and Matheson, J. E. (1979). A Manual for Encoding Probability 
Distributions. Palo Alto: CA.: SRI International. 
References 
547 
Steel, M. F. J. (1992). Posterior analysis of restricted seemingly unrelated regression equation 
models. Econometric Reviews 11,129-142. 
Stein, C. (1951). A property of some tests of composite hypotheses. Ann. Math. Statist. 22, 
475^76. 
Stein, C. (1956). Inadmissibility of the usual estimation of the mean of a multivariate normal 
distribution. Proc. Third Berkeley Symp. 1 (J. Neyman and E. L. Scott, eds.). Berkeley: 
Univ. California Press, 197-206. 
Stein, C. (1959). An example of wide discrepancy between fiducial and confidence intervals. 
Ann. Math. Statist. 30, 877-880. 
Stein, C. (1962). Confidence sets for the mean of a multivariate normal distribution. J. Roy. 
Statist. Soc. B 24, 265-296 (with discussion). 
Stein, C. (1965). Approximation of improper prior measures by proper probability measures. 
Bernoulli, Bayes, Laplace Festschrift. (J. Neyman and L. LeCam, eds.). Berlin: Springer, 
217-240. 
Stephens, D. A. and Smith, A. F. M. (1992). Sampling-resampling techniques for the  
computation of posterior densities in normal means problems. Test 1,1-18. 
Stewart, L. (1979). Multiparameter univariate Bayesian analysis. J. Amer. Statist. Assoc. 74, 
684-693. 
Stewart, L. (1983). Bayesian analysis using Monte Carlo integration, apowerful methodology 
for handling some difficult problems. The Statistician 32, 195-200. 
Stewart, L. (1985). Multiparameter Bayesian inference using Monte Carlo integration, some 
techniques for bivariate analysis. Bayesian Statistics 2 (J. M. Bernardo, M. H. DeGroot, 
D. V. Lindley and A. F M. Smith, eds.), Amsterdam: North-Holland, 495-510. 
Stewart, L. (1987). Hierarchical Bayesian analysis using Monte Carlo integration: computing 
posterior distributions when there are many models. The Statistician 36, 211-219. 
Stewart, L. and Davis, W. W. (1986). Bayesian posterior distributions over sets of possible 
models with inferences computed by Monte Carlo integration. The Statistician 35, 175- 
182. 
Stigler, S. M. (1982). Thomas Bayes' Bayesian inference. J. Roy. Statist. Soc. A 145, 250- 
258. 
Stigler, S. M. (1986a). The History of Statistics. Harvard, MA: University Press. 
Stigler, S. M. (1986b). Laplace's 1774 memoir on inverse probability. Statist. Sci. 1,359-378. 
Stigum, B. P. (1972). Finite state space and expected utility maximization. Econometrica 
40, 253-259. 
Stone, M. (1959). Application of a measure of information to the design and comparison of 
experiments. Ann. Math. Statist. 30, 55-70. 
Stone, M. (1961). The opinion pool. Ann. Math. Statist. 32, 1339-1342, 
Stone, M. (1963). The posterior t distribution. Ann. Math. Statist. 34, 568-573. 
Stone, M. (1965). Right Haar measures for convergence in probability to invariant posterior 
distributions. Ann. Math. Statist. 36,440-453. 
Stone, M. (1970). Necessary and sufficient conditions for convergence in probability to 
invariant posterior distributions. Ann. Math. Statist. 41, 1939-1953. 
Stone, M. (1974). Cross-validatory choice and assessment of statistical predictions. J. Roy. 
Statist. Soc. B 36, 11-147 (with discussion). 
548 
References 
Stone, M. (1976). Strong inconsistency from uniform priors. J. Amer. Statist. Assoc. 71, 
114-125 (with discussion). 
Stone, M. (1977). An asymptotic equivalence of choice of model by cross-validation and 
Akaike's criterion. J. Roy. Statist. Soc. B 39,44-47. 
Stone, M. (1979a). Comments on model selection criteria of Akaike and Schwarz. J. Roy. 
Statist. Soc. B 41, 276-278. 
Stone, M. (1979b). Review and analysis of some inconsistencies related to improper  
distributions and finite additivity. Proc. 6th Internal. Conf. Logic, Methodology and Philosophy 
of Science (L. J. Cohen, J. Los, H. Pfeiffer and K. P. Podewski, eds.). Amsterdam: 
North-Holland, 
Stone, M. (1986). In discussion of Fishburn (1986). Statist. Sci. 1, 356-357. 
Stone, M. and Dawid, A. P. (1972). Un-Bayesian implications of improper Bayesian inference 
in routine statistical problems. Biometrika 59, 369-373. 
Stroud, A. H. (1971). Approximate Calculation of Multiple Integrals. Englewood Cliffs, NJ: 
Prentice-Hall 
Sudderth, W. D. (1980). Finitely additive priors, coherence and the marginalization paradox. 
J. Roy. Statist. Soc. B 42, 339-341. 
Sugden, R. A. (1985). A Bayesian view of ignorable designs in survey sampling inference. 
Bayesian Statistics 2 (J. M. Bernardo, M. H. DeGroot, D. V. Lindley and A. F. M. Smith, 
eds.), Amsterdam: North-Holland, 751-754. 
Suppes, P. (1956). The role of subjective probability in decision making. Proc. Third Berkeley 
Symp. 5 (J. Neyman and E. L. Scott, eds.). Berkeley: Univ. California Press, 61-73. 
Suppes, P. (1960). Some open problems in the foundations of subjective probability.  
Information and Decision Processes (Machol, ed.). New York: McGraw-Hill, 162-170. 
Suppes, P. (1974). The measurement of belief. J. Roy. Statist. Soc. B 36, 160-175. 
Suppes, P. and Walsh, K. (1959). A non-linear model for the experimental measurement of 
utility. Behavioral Sci. 4, 204-211. 
Suppes, P. and Zanotti, M. (1976). Necessary and sufficient conditions for the existence of a 
unique measure strictly agreeing with a qualitative probability ordering. J. Philos. Logic 
5,431-138. 
Suppes, P. and Zanotti, M. (1982). Necessary and sufficient qualitative axioms for conditional 
probability. Z. Wahisch. verw. Gebiete 60,163-169. 
Susarla V. and van Ryzin, J. (1976). Nonparametric Bayesian estimation of survival curves 
from incomplete observations. J. Amer. Statist. Assoc. 71, 897-902. 
Sweeting, T. J. (1984). On the choice of prior distributions for the Box-Cox transformed 
linear model. Biometrika 71, 127-134. 
Sweeting, T. J. (1985). Consistent prior distributions for transformed models. Bayesian 
Statistics 2 (J. M. Bernardo, M. H. DeGroot, D. V. Lindley and A. F M. Smith, eds.), 
Amsterdam: North-Holland, 755-762. 
Sweeting, T. J. (1992). On asymptotic posterior normality in the multiparameter case.  
Bayesian Statistics 4 (J. M. Bernardo, J. O. Berger, A. P. Dawid and A. F. M. Smith, eds.). 
Oxford: University Press, 825-835. 
Sweeting, T. J. and Adekola, A. D. (1987). Asymptotic posterior normality for stochastic 
processes revisited /. Roy. Statist. Soc. B 49, 215-222. 
References 
549 
Tanner, M. A. (1991). Tools for Statistical Inference: Observed Data and Data Augmentation 
Methods. Berlin: Springer. 
Tanner, M. A. and Wong, W. H. (1987). The calculation of posterior distributions by data 
augmentation. J. Amer. Statist. Assoc. 82, 582-548 (with discussion). 
Teichroew, D. (1965). A history of distribution sampling prior to the era of the computer and 
its relevance to simulation. J. Amer. Statist. Assoc. 60, 27-49. 
Thatcher, A. R. (1964). Relationships between Bayesian and confidence limits for prediction 
J. Roy. Statist. Soc. B 26, 126-210. 
Thomas, A., Spiegelhalter, D. J. and Gilks, W. R. (1992). BUGS, aprogram to perform  
Bayesian inference using Gibbs sampling. Bayesian Statistics 4 (J. M. Bernardo, J. O. Berger, 
A. P. Dawid and A. F. M. Smith, eds.). Oxford: University Press, 837-842. 
Thorburn, D. (1986). A Bayesian approach to density estimation. Biometrika 73, 65-75. 
Tiao, G. C. and Box, G. E. P. (1974). Some comments on 'Bayes' estimators. Studies in 
Bayesian Econometrics and Statistics: in Honor of Leonard J. Savage (S. E. Fienberg 
and A. Zellner, eds.). Amsterdam: North-Holland, 620-626. 
Tibshirani, R. (1989). Noninformative priors for one parameter of many. Biometrika 76, 
604-608. 
Tierney, L. (1990). LISP-STAT. An Object Oriented Environment for Statistical Computing 
and Dynamic Graphics. Chichester: Wiley. 
Tierney, L. (1992). Exploring posterior distributions using Markov chains. Computer Science 
and Statistics: Proc. 23rd. Symp. Interface (E. M. Keramidas, ed.). Fairfax Station: 
Interface Foundation, 563-570. 
Tierney, L. and Kadane, J. B. (1986). Accurate approximations for posterior moments and 
marginal densities. J. Amer. Statist. Assoc. 81, 82-86. 
Tierney, L., Kass, R. E. and Kadane, J. B. (1987). Interactive Bayesian analysis using accurate 
asymptotic approximations. Computer Science and Statistics: 19th Symposium on the 
Interface (R. Heiberger, ed.). Alexandria, VA: ASA, 15-21. 
Tierney, L., Kass, R. E. and Kadane, J. B. (1989a). Fully exponential Laplace approximations 
to expectations and variances of nonpositive functions. J. Amer. Statist. Assoc. 84, 710- 
716. 
Tierney, L„ Kass, R. E. and Kadane, J. B. (1989b). Approximate marginal densities of 
nonlinear functions. Biometrika 76,425-433. 
Titterington, D. M., Smith, A. F. M. and Makov, U. E. (1985). Statistical Analysis of Finite 
Mixture Distributions. Chichester: Wiley. 
Torgesen, E. N. (1981). Measures of information based on comparison with total information 
and with total ignorance. Ann. Statist. 9, 638-657. 
Trader, R. L. (1989). Thomas Bayes. Encyclopedia of Statistical Sciences suppl. (S. Kotz, 
N. L. Johnson and C. B. Read, eds.). New York: Wiley, 14-17. 
Tribus, M. (1969). Rational Descriptions, Decisions and Designs. New York: Pergamon. 
van der Merwe, A. J. and van der Merwe, C. A. (1992). Empirical and hierarchical Bayes 
estimation in multivariate regression models. Bayesian Statistics 4 (J. M. Bernardo, 
J. O. Berger, A. P. Dawid and A. F. M. Smith, eds.). Oxford: University Press, 843-850. 
van Dijk, H. K., Hop, J. P. and Louter, A. S. (1987). An algorithm for the computation of 
posterior moments and densities using simple importance sampling. The Statistician 36, 
83-90. 
550 
References 
van Dijk, H. K. and Kloek, T. (1983). Monte Carlo analysis of skew posterior distributions: 
An illustrative econometric example. The Statistician 32, 216-223. 
van Dijk, H. K. and Kloek, T. (1985). Experiments with some alternatives for simple  
importance sampling in Monte Carlo integration. Bayesian Statistics 2 (J. M. Bernardo, 
M. H. DeGroot, D. V. Lindley and A. F. M. Smith, eds.), Amsterdam: North-Holland, 
511-530 (with discussion). 
Venn, J. (1886). The Logic of Chance. London: MacMillan. Reprinted in 1963, New York: 
Chelsea. 
Verbraak, H. L. F. (1990). The Logic of Objective Bayesianism. The Hague: CIP-DATA. 
Verdinelli, I. (1992). Advances in Bayesian experimental design. Bayesian Statistics 4 
(J. M. Bernardo, J. O. Berger, A. P. Dawid and A. F M. Smith, eds.). Oxford:  
University Press, 467-481, (with discussion). 
Verdinelli, I. and Kadane, J. B. (1992). Bayesian designs for maximizing information and 
outcome. J. Amer. Statist. Assoc. 87, 510-515. 
Verdinelli, I. and Wasserman, L. (1991). Bayesian analysis of outlier problems using the 
Gibbs sampler. Statist. Computing 1,135-139. 
Viertl, R. (ed.) (1987). Probability and Bayesian Statistics. London: Plenum. 
Villegas, C. (1964). On qualitative a-algebras. Ann. Math. Statist. 35, 1787-1796. 
Villegas, C. (1969). On the a priori distribution of the covariance matrix. Ann. Math.  
Statist. 40,1098-1099. 
Villegas, C. (1971). On Haar priors. Foundations of Statistical Inference (V. P. Godambe and 
D. A. Sprott, eds.). Toronto: Holt, Rinehart and Winston, 409-414 (with discussion). 
Villegas, C. (1977a). On the representation of ignorance. J. Amer. Statist. Assoc. 72,651-654. 
Villegas, C. (1977b). Inner statistical inference. J. Amer. Statist. Assoc. 72, 453^158. 
Villegas, C. (1981). Inner statistical inference II. Ann. Statist. 9, 768-776. 
Villegas, C. (1990). Bayesian inference in models with euclidean structures. J. Amer. Statist. 
Assoc. 85, 1159-1164. 
von Mises, R. (1928). Probability, Statistics and Truth. Reprinted in 1957, London: Macmil- 
lan. 
von Neumann, J. and Morgenstern, O. (1944/1953). Theory of Games and Economic  
Behaviour. 3rd. edition in 1953. Princeton: University Press. 
Vovk, V. G. (1993a). A logic of probability, with applications to the foundation of statistics. 
J. Roy. Statist. Soc.BSS, 317-352 (with discussion). 
Vovk, V. G. (1994). Forecasting point and continuous processes: prequential analysis. Test 3, 
(to appear). 
Vovk, V. G. and Vyugin V V. (1993). On the empirical validity of the Bayesian method. 
J. Roy. Statist. Soc. B 55, 253-266. 
Wahba, G. (1978). Improper priors, spline smoothing and the problems of guarding against 
model errors in regression. J. Roy. Statist. Soc. B 40, 364-372. 
Wahba, G. (1983). Bayesian confidence intervals for the cross-validated smoothing spline. 
J. Roy. Statist. Soc. B 45, 133-150. 
Wahba, G. (1988). Partial and interaction spline models. Bayesian Statistics 3 (J. M.  
Bernardo, M. H. DeGroot, D. V. Lindley and A. F M. Smith, eds.). Oxford: University Press, 
479-491 (with discussion). 
References 
551 
Wakefield, J. C, Gelfand, A. E. and Smith, A. F. M. (1991). Efficient generation of random 
variates via the ratio-of-uniforms method. Statistics and Computing 1,129-133. 
Wald, A. (1939). Contributions to the theory of statistical estimation and testing hypothesis. 
Ann. Math. Statist. 10, 299-326. 
Wald, A. (1947). Sequential Analysis. New York: Wiley. 
Wald, A. (1950). Statistical Decision Functions. New York: Wiley. 
Walker, A. M. (1969). On the asymptotic behaviour of posterior distributions. J. Roy. Statist. 
Soc. B 31, 80-88. 
Wallace, C. S. and Freeman, P. R. (1987). Estimation and inference by compact coding. 
J. Roy. Statist. Soc. B 49, 240-260 (with discussion). 
Walley, P. (1987). Belief function representations of statistical evidence. Ann. Statist. 15, 
1439-1465. 
Walley, P. (1991). Statistical Reasoning with Imprecise Probabilities. London: Chapman and 
Hall. 
Walley, P. and Fine, T. L. (1979). Varieties of modal (classificatory) and comparative  
probability. Synthese 41, 321-374. 
Wallsten, T. S. (1974). The psychological concept of subjective probability: a measurement 
theoretic view. The Concept of Probability in Psychological Experiments (C.-A. S. Stael 
von Holstein, ed.). Dordrecht: Reidel, 49-72. 
Wasserman, L. (1989). A robust Bayesian interpretation of likelihood regions. Ann. Statist. 
17,1387-1393. 
Wasserman, L. (1990a). Belief functions and statistical inference. Canadian J. Statist. 18, 
183-196. 
Wasserman, L. (1990b). Prior envelopes based on belief functions. Ann. Statist. 18,454^164. 
Wasserman, L. (1992a). Recent methodological advances in robust Bayesian inference. 
Bayesian Statistics 4 (J. M. Bernardo, J. O. Berger, A. P. Dawid and A. F M. Smith, 
eds.). Oxford: University Press, 483-502 (with discussion). 
Wasserman, L. (1992b). Invariance properties of density ratio priors. Ann. Statist. 20,2177- 
2182. 
Wasserman, L. and Kadane, J. B. (1990). Bayes' theorem for Choquet capacities. Ann. 
Statist. 18,1328-1339. 
Wasserman, L. and Kadane, J. B. (1992a). Computing bounds on expectations. J. Amer. 
Statist. Assoc. 87, 516-522. 
Wasserman, L. and Kadane, J. B. (1992b). Symmetric upper probabilities. Ann. Statist. 20, 
1720-1736. 
Wechsler, S. (1993). Exchangeability and predictivism. Erkenntnis 38, 343-350. 
Weerahandi, S. and Zidek, J. V. (1981). Multi-Bayesian statistical decision theory. J. Roy. 
Statist. Soc. A 144, 85-93. 
Weerahandi, S. and Zidek, J. V. (1983). Elements of multi-Bayesian decision theory. Ann. 
Statist. 11,1032-1046. 
Weiss, R. E. and Cook, R. D. (1992). A graphical case statistic for assessing posterior 
inference. Biometrika 79, 51-55. 
Welch, B. L. (1939). On confidence limits and sufficiency, with particular reference to 
parameters of location. Ann. Math. Statist. 10, 58-69. 
552 
References 
Welch, B. L. (1965). On comparisons between confidence point procedures in the case of a 
single parameter. J. Roy. Statist. Soc. B 27,1-8. 
Welch, B. L. and Peers, H. W. (1963). On formulae for confidence points based on intervals 
of weighted likelihoods. J. Roy. Statist. Soc. B 25, 318-329. 
West, M. (1981). Robust sequential approximate Bayesian estimation. J. Roy. Statist. Soc. 
5 43,157-166. 
West, M. (1984). Outlier models and prior distributions in linear regression. J. Roy. Statist. 
Soc. B 46,431-439. 
West, M. (1985). Generalized linear models: scale parameters, outlier accomodation and 
prior distributions. Bayesian Statistics 2 (J. M. Bernardo, M. H. DeGroot, D. V. Lindley 
and A. F. M. Smith, eds.), Amsterdam: North-Holland, 531-557 (with discussion). 
West, M. (1986). Bayesian model monitoring. J. Roy. Statist. Soc. B 48, 70-78. 
West, M. (1988). Modelling expert opinion. Bayesian Statistics 3 (J. M. Bernardo, M. H.  
DeGroot, D. V. Lindley and A. F. M. Smith, eds.). Oxford: University Press, 493-508 (with 
discussion). 
West, M. (1992a). Modelling agent forecast distributions. J. Roy. Statist. Soc. B 54,553-568. 
West, M. (1992b). Modelling with mixtures. Bayesian Statistics 4 (J. M. Bernardo, J. O. Ber- 
ger, A. P. Dawid and A. F M. Smith, eds.). Oxford: University Press, 503-524 (with 
discussion). 
West, M. and Crosse, J. (1992). Modelling probabilistic agent opinion..J. Roy. Statist. 
Soc. B 54, 285-299. 
West, M. and Harrison, P. J. (1986). Monitoring and adaptation in Bayesian forecasting 
models. J. Amer. Statist. Assoc. 81, 741-750. 
West, M. and Harrison, P. J. (1989). Bayesian Forecasting and Dynamic Models. Berlin: 
Springer. 
West, M. and Migon, H. S. (1985). Dynamic generalised linear models and Bayesian  
forecasting. J. Amer. Statist. Assoc. 80, 73-83. 
West, M., Mueller, P. and Escobar, M. D. (1994). Hierarchical priors and mixture models 
with applications in regression and density estimation. Aspects of Uncertainty: a Tribute 
to D. V. Lindley (P. R. Freeman, and A. F. M. Smith, eds.). Chichester: Wiley, (to appear). 
Wetherill, G. B. (1961). Bayesian sequential analysis. Biometrika 48, 281-292. 
Wetherill, G. B. (1966). Sequential Methods in Statistics. New York: Wiley. 
Wetherill, G. B. and Campling, G. E. G. (1966). The decision theory approach to sampling 
inspection. J. Roy. Statist. Soc. B 28, 381^*16. 
White, D. J. (1976a). Fundamentals of Decision Theory. Amsterdam: North-Holland. 
White, D. J. (1976b). A Decision Methodology. Chichester: Wiley. 
White, D. J. and Bowen, K. C. (eds.) (1975). The Role and Effectiveness of Theories of 
Decision in Practice. London: Hodder and Stoughton. 
Whittle, P. (1958). On the smoothing of probability density functions. J. Roy. Statist. Soc. B 
20, 334-343. 
Whittle, P. (1976). Probability. Chichester: Wiley. 
Wichmann, D. (1990). Bayes-Statistik. Mannheim: Bl-Wissenschaftsverlag. 
Wiener, N. (1948). Cybernetics. Cambridge, Mass.: The MIT Press. Reprinted in 1961. 
References 
553 
Wilkinson, G. N. (1977). On resolving the controversy in statistical inference. J. Roy. Statist. 
Soc. B 39,119-171 (with discussion). 
Wilks, S. S. (1962). Mathematical Statistics. New York: Wiley. 
Willing, R. (1988). Information contained in nuisance parameters. Bayesian Statistics 3 
(J. M. Bernardo, M. H. DeGroot, D. V. Lindley and A. F. M. Smith, eds.). Oxford: 
University Press, 801-805. 
Wilson, J. (1986). Subjective probabilities and the prisoners' dilemma. Manag. Sci. 32, 
45-55. 
Wilson, R. B. (1968). On the theory of syndicates. Econometrica 36,119-132. 
Winkler, R. L. (1967a). The assessment of prior distributions in Bayesian analysis. J. Amer. 
Statist. Assoc. 62, 776-800. 
Winkler, R. L. (1967b). The quantification of judgement; some methodological suggestions. 
J. Amer. Statist. Assoc. 62, 1105-1120. 
Winkler, R. L. (1968). The consensus of subjective probability distributions. Manag. Sci. 15, 
861-875. 
Winkler, R. L. (1972). Introduction to Bayesian Inference and Decision. Toronto: Holt, 
Rinehart and Winston. 
Winkler, R. L. (1980). Prior information, predictive distributions and Bayesian model  
building. Bayesian Analysis in Econometrics and Statistics: Essays inHonor of Harold Jeffreys 
(A. Zellner, ed.). Amsterdam: North-Holland, 
Winkler, R. L. (1981). Combining probability distributions from dependent information 
sources. Manag. Sci. 27, 479-488. 
Witmer, J. A. (1986). Bayesian multistage decision problems. Ann. Statist. 14,283-297. 
Wolpert, R. L. (1991). Monte Carlo importance sampling in Bayesian statistics. Statistical 
Multiple Integration (N. Flournoy, and R. K. Tsutakawa, (eds.). Providence: RI: ASA, 
Wolpert, R. L. and Warren-Hicks, W. J. (1992). Bayesian hierarchical logistic models for 
combining field and laboratory survival data. Bayesian Statistics 4 (J. M. Bernardo, 
J. O. Berger, A. P. Dawid and A. F. M. Smith, eds.). Oxford: University Press, 525-546 
(with discussion). 
Wong, W H. and Li, B. (1992). Laplace expansion for posterior densities of nonlinear 
functions of parameters. Biometrika 79, 393-398. 
Wooff, D. A. (1992). [B/D] works. Bayesian Statistics 4 (J. M. Bernardo, J. O. Berger, 
A. P. Dawid and A. F. M. Smith, eds.). Oxford: University Press, 851-859. 
Wright, D. E. (1986). A note on the construction of highest posterior density intervals. Appl. 
Statist. 35,49-53. 
Wrinch, D. H. and Jeffreys, H. (1919). On some aspects of the theory of probability. Phil. 
Mag. Ser. 6, 38, 715-731. 
Wrinch, D. H. and Jeffreys, H. (1921). On certain fundamental principles of scientific inquiry. 
Phil. Magazine 6,42, 363-390; 45, 368-374. 
Yaglom, A. M. and Yaglom, I. M. (1960/1983). Verojatnost i Informacija. Moscow: Nauka. 
English translation in 1983 as Probability and Information. Dordrecht: Reidel. 
Ye, K. (1993). Reference priors when the stopping rule depends on the parameter of interest. 
J. Amer. Statist. Assoc. 88, 360-363. 
554 
References 
Ye, K. and Berger, J. O. (1991). Non-informative priors for inferences in exponential  
regression models. Biometrika 78, 645-656. 
Yilmaz, M. R. (1992). An information-expectation framework for decision under uncertainty. 
J. Multi-Criteria Dec. Analysis 1, 65-80. 
Young, S. C. and Smith, J. Q. (1991). Deriving and analysing optimal strategies in Bayesian 
models of games. Manag. Sci. 37, 559-571. 
Yu, P. L. (1985). Multiple Criteria Decision Making. London: Plenum. 
Zellner, A. (1971). An Introduction to Bayesian Inference in Econometrics. New York: Wiley. 
Reprinted in 1987, Melbourne, FL: Krieger. 
Zellner, A. (1977). Maximal data information prior distributions. New Developments in the 
Applications of Bayesian Methods (A. Aykac and C. Brumat, eds.). Amsterdam: North- 
Holland, 211-232. 
Zellner, A. (ed.) (1980). Bayesian Analysis in Econometrics and Statistics: Essays in Honor 
of Harold Jeffreys. Amsterdam: North-Holland. 
Zellner, A. (1984). Posterior odds ratios for regression hypothesis: general considerations and 
some specific results. Basic Issues in Econometrics (A. Zellner, ed.). Chicago: University 
Press, 275-305. 
Zellner, A. (1985). Bayesian econometrics. Econometrica 53, 253-269. 
Zellner, A. (1986a). On assessing prior distibutions and Bayesian regression analysis with 
g-prior distributions. Bayesian Inference and Decision Techniques: Essays in Honor of 
Bruno de Finetti (P. K. Goel and A. Zellner, eds.). Amsterdam: North-Holland, 233-243. 
Zellner, A. (1986b). Bayesian estimation and prediction using asymmetric loss functions. 
J. Amer. Statist. Assoc. 81,446-451. 
Zellner, A. (1987). Bayesian inference. The New Palgrave: a Dictionary of Economics 1 
(J. Eatwell, M. Milgate and P. Newman, eds.). London: Macmillan, 208-218. 
Zellner, A. (1988a). A Bayesian era. Bayesian Statistics 3 (J. M. Bernardo, M. H. DeGroot, 
D. V. Lindley and A. F. M. Smith, eds.). Oxford: University Press, 509-516. 
Zellner, A. (1988b). Optimal information processing and Bayes' theorem. Amer. Statist. 42, 
278-284 (with discussion). 
Zellner, A. (1988c). Bayesian analysis in econometrics. J. Econometrics 37, 27-50. 
Zellner, A. (1991). Bayesian methods and entropy in economics and econometrics. Maximum 
Entropy and Bayesian Methods (W. T. Grandy and L. H. Schick eds.). Dordrecht: Kluwer, 
17-31. 
Zellner, A. and Siow, A. (1980). Posterior odds ratios for selected regression hypothesis. 
Bayesian Statistics (J. M. Bernardo, M. H. DeGroot, D. V. Lindley and A. F. M. Smith, 
eds.). Valencia: University Press, 585-603 and 618-647 (with discussion). 
Zidek, J. (1969). A representation of Bayes invariant procedures in terms of Haar measure. 
Ann. Inst. Statist. Math. 21,291-308. 
Zidek, J. and Weerahandi, S. (1992). Bayesian predictive inference for samples from smooth 
processes. Bayesian Statistics 4 (J. M. Bernardo, J. O. Berger, A. P. Dawid and A. F M. 
Smith, eds.). Oxford: University Press, 547-563 (with discussion). 
555 
Subject index 
A posteriori distribution 
(see Posterior distribution) 
A priori distribution 
(see Prior distribution) 
Absolutely continuous distribution 
multivariate 128,434, 435 
univariate 111, 430-433 
Absolutely continuous random quantity 
109-111 
Actions 5,13, 15-17 
in bounded decision problems x, 50-54 
in general decision problems x, 54-56 
set of x, 16,19-21 
Actuarial science 9,100, 373 
Additive decomposition of utility 66, 
149, 408 
Admissibility 447^49, 461 
and complete classes 448 
of Bayes rules 448, 449 
Admissible decision (see Admissibility) 
Akaike information criterion 487, 488 
Amount of information 6, 77-81, 
157-159, 303,422 
Analysis of variance 382 
Ancillarity xii, 7, 452,453, 475,479 
Approximations 
and discrepancy x, xi, 75-77,154-157, 
207-209 
asymptotic (see Asymptotic analysis) 
Gibbs sampling algorithm 353-355 
importance sampling xiii, 8, 264, 340, 
348-350 
iterative quadrature xiii, 264, 346-348 
Laplace approximation xiii, 8, 264, 
340-345, 455 
Markov chain Monte Carlo xiii, 8, 
264, 340, 353-356 
Metropolis-Hastings algorithm 
355, 356 
of a Binomial by a Poisson 76, 77 
of a Student by a Normal 156 
556 
Subject Index 
of moments of a transformed 
beta distribution 124, 125 
of prior distributions 283-285 
of the mean of a beta distribution 
344, 345 
sampling-importance-resampling xiii, 
8, 264, 340, 350-352 
to mean and variance 113 
to mean vector and covariance 132 
Ascending factorial function 135 
Asymptotic analysis xiii, 8, 235, 264, 
285-297, 445, 463^165, 480, 
485, 486 
continuous xiii, 287-297 
discrete xiii, 286, 287 
non-normal 296 
see also Asymptotic normality 
see also Reference distribution 
Asymptotic normality 
of posterior distributions 286-297, 
314,365 
for a ratio 297 
regularity conditions for 289-292 
under conjugate analysis 293, 294 
under transformations xiii, 
295-297,486 
Axiomatic approaches 5, 83-91 
relevance of 94, 95 
to degrees of belief 89, 90 
to utilities 90, 91 
see also Foundations 
Axioms of coherence 15, 16, 23-28 
Backward induction x, 59-63 
Bayes 
decision rule 448 
estimate (see Estimation) 
risk 448 
rules 448 
test 391-397, 412-417 
Thomas 1,2 
Bayes factor xiii, 389-395, 414,417, 
A22-A2A, 470, 476, 487, 488 
Bayes-Laplace postulate 357, 358 
Bayes' theorem xii, 2,4, 7, 162, 175, 180, 
241-255, 265, 269-285, 305, 306, 
340, 352, 368, 425, 451, 452, 454, 
455,480 
finite x, 2,38-40,42^15,47^19,68,77, 
80, 89, 94 
generalised xi, xiii, 127, 130 
Bayesian 
bootstrap 371 
computation vii, 5 
implementation 8, 263-356 
inference (see Inference, Bayesian) 
methods vii, 5 
paradigm xiii, 241-264 
reading list ix, 9-11 
software 425,426 
theory vii, ix, 5-9 
Belief functions 95, 99 
Beliefs 3-9,13-16, 33-49 
and actions x, 13-16 
and limiting frequencies 173-175, 
177, 179, 181 
and models xi, xiii, 165-167 
and probabilities x, 33-49 
conditional x, 38-49 
finite representation of x, 33-38 
general representation of xi, 105-109 
reporting of x,xi, 67-81,150-157,302, 
320, 360 
revision of x, 38^19, 127-130 
sequential revision of x, 47^19,56-67 
Bernoulli distribution, model 115, 428 
and exchangeability xi, 172-175, 
211-215 
asymptotic analysis for 294, 296 
"biased" stopping rule for 251, 252 
conjugate analysis of 270-272,436 
conjugate family for 267, 270 
inferential process for 248,249, 
270-274, 276, 277, 279-282,436 
reference distributions for 315, 316, 
337,436 
sufficient statistics for 195, 196, 436 
Subject Index 
557 
Beta distribution 116, 117, 124, 125, 134, 
135, 430 
as prior distribution 267, 270-273, 344 
mixtures of 279-281 
Beta-binomial distribution 
(see Binomial-beta distribution) 
Betting (see Monetary Bets) 
Bias 360, 462^65 
Bilateral Pareto distribution 141,434 
Binomial distribution, model 115, 134, 
172-175,211-215,428 
asymptotic analysis for 294, 296 
conjugate analysis of 270-272 
conjugate family for 267, 271 
inferential process for 248, 249, 
270-274,279-282,414 
reference distributions for 
315,316,337 
sufficient statistics for 195, 196 
see also Bernoulli distribution 
Binomial-beta distribution 117, 272, 428 
Bioassay 219, 220, 382 
Biostatistics 9 
Bootstrap 371 
see also Bayesian, bootstrap 
Borelset 110, 127 
Calibration 373, 482 
Called-off bets 87-89 
Canonical conjugate analysis 269-279 
Cauchy distribution 123, 349 
location parameter estimation 451 
Central limit theorem 126 
Centred spherical symmetry 183-186 
Characteristic function 114, 126, 
183-186,203 
Chi-squared distribution 118, 121, 123, 
124,138,330,361,414,431, 
459,487 
Choice of experiments 
see Design of experiments 
Classical decision theory 
xiv, 444-449,461 
Classification 373, 407,408, 483 
Closed under sampling 270, 281 
see also Conjugate analysis 
Cluster analysis 418 
Coding theory 358 
see also Stochastic complexity 
Coherence x, 23-33, 38^5, 83-91, 
94-99, 195 
and preferences x, 23-28 
and quantification x, 28-33, 83-85 
temporal 94 
see also Axiomatic Approaches 
Combining evidence 103-104 
Communication 92, 102-104, 166, 236, 
237 
Comparability of consequences 23, 24 
Comparability of events 33 
Comparative inference xiv, 376, 443^188 
Compatibility of probability and degrees 
of belief 35, 40 
Complete probability measures 108 
Complete symmetry 169 
Composite hypothesis (see Hypothesis) 
Computer software 425, 426 
Conditional 
beliefs (see Beliefs) 
density function 128-131, 166, 
172-226, 242-247, 353-355 
expectation 51, 52 
independence (see Independence) 
inference xiv, 374, 445, 452, 468, 475, 
478,479 
likelihood 48 
mass function 128-130 
preference 22, 85 
probability 38^19 
uncertainty relation 22, 38^0 
558 
Subject Index 
Confidence interval, region 359,445,453, 
465^69,478 
(see also Interval estimation) 
Confidence level, coefficient 466 
Confirmation theory 95 
Conjugate analysis xiii, 8, 9, 264-285 
(see also Canonical conjugate analysis) 
Conjugate prior family of distributions 
approximation with 279-285 
canonical form of 269 
convex combination (mixtures) of 
282, 283 
definition of 265, 266 
for regular exponential models 266-279 
limiting form of 276, 362 
logarithmic divergence between 278 
Consequences 16, 17, 19-21 
bounded set of 49-54 
extreme 49-54 
reference 54-56 
Consistency of preferences 26 
Consistent sequence of statistics, estimates 
312,320,326,353,405 
Constrained parameters 246, 247 
Contingency table 41, 216, 374, 414 
Continuity of uncertainty relation 107 
Control problems, theory 9, 374 
Convergence xi, 125-127 
almost surely 125, 126, 352 
in distribution 125, 126 
in expected utility 142-147 
in mean square 125,126 
in probability 125, 126 
Correlation 132, 231, 232 
inference for 337, 338, 363, 364 
reference distributions for 337, 338, 
363, 364 
Cost of observation, experiment 66, 67, 
149, 408,409 
Countable additivity xi, 6, 106-109, 
161, 162 
Covariance (matrix) 132 
Covariates 
and partial exchangeability xii, 7, 
209-211,219-222 
elaboration 232, 234 
selection of xiii, 407^1-09 
Coverage probability 365 
Credible regions 8, 259-262,466 
coincidence with confidence sets 
359, 468,469 
highest posterior density (HPD) 
260-262, 395,453,477,484,486 
intervals 259 
lack of invariance of HPD 262 
loss function for 259 
size of 259 
Critical value 412, 413 
Cross-validation xiii, 8, 403^07,409, 
420-424 
Data 2, 63, 147 
expected information from 78, 158 
expected utility of 78, 157 
expected value of 65,148 
reduction 190, 191 
"speaking for themselves" 
276, 298, 357 
structures xii, 209-226 
transformation 230, 231 
Decision 3-9, 13-22 
analysis 9, 10 
classical theory of xiv, 9, 444-449 
criterion 52, 56,146 
function 141-145 
rule 446 
theory 5 
tree 16, 17, 58, 59, 63, 147, 148, 
386-388,408,410,411 
see also Action 
see also Decision problem 
Subject Index 
559 
Decision problem x, 16-22 
basic elements of x, 16-18, 255, 256 
bounded x, 50-54 
formal representation of x, 18-22 
general x, 54-56 
sequential x, 56-67 
Degrees of belief 33-49 
and monetary bets 86-88 
and scoring rules 88, 89 
axiomatic approaches to 89, 90 
see also Beliefs 
Density estimation 482 
Density function 
joint 128,166 
marginal 128, 166 
multivariate 128 
univariate 111 
see also Conditional 
Descriptive theory 4, 23, 32, 95-98 
Design matrix 221 
Design of experiments x, 5, 63-67, 
147-149,159-160 
Deviation from uniformity model 313 
Diagnostic functions 418,419 
Digamma function 156 
Dirichlet distribution 134-135,434 
as prior distribution 441 
Discrepancy 
measures for model rejection 412-417 
of an approximation x, xi, 76, 77, 156, 
157, 207-209, 298 
Discrete distribution 
multivariate 128,433 
univariate 110, 428, 429 
Discrete random quantity 109-111 
Discrimination 373 
Distribution function 
multivariate 128 
univariate 110 
Dominance 447 
D-optimality 159 
Double-exponential distribution 
369, 379, 380 
Dutch book 86, 88,95 
Dynamic programming 
(see Backward induction) 
Econometrics 10, 374 
Economics 10 
Educational research 10 
Effects (main, interaction) 218, 234, 382 
Efficient score function 368, 464 
Empirical Bayes xiii, 226, 371-373 
Empirical distribution function 177-181, 
228,229,351 
Entropy 77, 79 
see also Maximum entropy 
Ergodic average 353 
Erlang distribution 118 
Errors of type 1 and 2 471 
Essentially bounded functions 142-144 
Estimation 
see Interval Estimation 
see Point Estimation 
Ethically neutral event 83, 84 
Events x, 16-18 
relevant 18-22,92, 95 
real-world 19 
significant 27, 28 
standard 29-32, 50-52, 54 
Exchangeability xi, 3, 101, 167-181, 
223-226,403,425 
and independence 167, 168 
and invariance 181-190 
and sufficient statistics 191-207 
extensibility of 171, 226, 227 
finite 169, 170, 226,227, 356, 357 
infinite 171, 172-226,357 
of parameters 222-226, 372 
partial xii, 7, 168-170, 209-222, 381 
role of 7 
unrestricted 211-216 
560 
Subject Index 
Exchangeable 
binomial parameters 223 
normal mean parameters 224-226 
Expectation (see also Mean) 
of a random quantity 112 
of a random vector 131, 132 
see also Conditional 
Expected 
information 80,91, 158, 159 
loss 75, 76, 154, 155 
utility 6, 51-81, 84, 90, 91, 141-147, 
155-157,299-302 
Experiment 
expected information from 80,158,159, 
299, 300 
expected value of 65-67, 148, 299, 300 
see also Design of experiments 
Expert System 426 
Exponential distribution, model 7, 118, 
187-189, 199,430 
and exchangeability xii, 187-189 
as maximum entropy choice 209 
conjugate analysis of 438 
conjugate family for 438 
inferential process for 438 
reference distributions for 438 
sufficient statistics for 197, 438 
Exponential family of distributions 
7,197-209 
and maximum entropy 207-209 
and exchangeability xii, 204-207 
as an approximation 207-209 
canonical form of 202-203 
conjugate analysis of 269-279 
conjugate family for 265-279 
first two moments of 203 
inferential process for 273-276 
information measures and xii, 207-209 
k-parameter201,202 
non-regular 198-200 
one-parameter 198-200 
regular 198, 199 
sufficient statistics for 197-207 
Extreme consequences 49-54 
F distribution (see Snedecor distribution) 
Fiducial inference xiv, 9, 444, 456-458, 
461,484,486 
Fieller-Creasy problem 468 
Final precision 
see Initial and final precision 
Finite additivity xii, 16, 36, 40, 105, 106, 
108,161,162 
Finite decision problems 16 
Finite exchangeability 169, 170, 226, 
227, 356, 357 
Finite horizon 59 
Finite population sampling 374 
Fisher distribution 
(see Snedecor distribution) 
Fisher information (matrix) 
expected 288, 314, 329, 331, 333, 335, 
336, 361 
observed 288 
Flatland paradox 365 
Forecasting 10, 374 
Forensic science 10 
Foundations x, 10, 13-104 
and coherence x, 23-33, 38^15, 83-91, 
94-99 
axiomatic basis of x, 5, 15, 16, 23, 
83-91,94,95,444 
critical issues x, xi, 92-104 
theories x, 83-92 
Fractiles (see Quantiles) 
Frame of discourse 18, 92-94 
Frequency (see Limiting frequency) 
Frequentist procedures xiv, 9, 371, 444, 
449-454,461 
Fuzzy logic 95 
Gambling (see Monetary bets) 
Game theory 5, 88, 104, 360 
Gamma distribution 118, 123, 124, 136, 
138,140,313,330,430 
Subject Index 
561 
as a prior distribution 267, 268, 274, 
301,302 
Gamma function 116 
Gamma-gamma distribution 120, 430 
as predictive distribution 438 
Gamma-Poisson distribution 
see Poisson-Gamma distribution 
Gauss-Hermite quadrature 264, 346 
Generalised gamma function 138 
Geometric distribution, model 
116,189,190 
and exchangeability xi, 189, 190 
conjugate analysis of 437 
inferential process for 437 
reference distributions for 437 
sufficient statistics for 437 
Gibbs sampling algorithm 353-355 
Group communication, decision making 
5,8,92,102-104,166,236,237,275, 
424,425 
Groups of transformations 362, 454,460 
Growth curves 220, 382 
Haar measure prior 362 
see also Noninformative prior 
see also Reference distribution 
Helly's theorem 126, 173, 213 
Hierarchical model, prior xiii, 7,222-226, 
371-373,383,480 
and binomial parameters 223 
and normal parameters 224-226, 383 
Highest probability density (HPD) region 
{see Credible region) 
Histogram 351 
History 10, 356, 357 
Hypergeometric distribution, model 
115,172,322,428 
Hypergeometric, function 338, 363 
Hyperparameter 224, 270, 271, 275, 
339, 372 
Hypothesis 2, 262, 263, 380-382, 390 
alternative 233, 380, 391,469,470 
composite 391, 392, 394, 471^175 
null 233, 394, 414, 416,469, 470 
simple 380, 391, 392, 394, 471, 475 
Hypothesis testing xiv, 8,9,445,469-475 
and Lindley's paradox 394, 406, 407, 
415,416,422 
composite versus composite 
392-394, 396, 397 
critical region 470 
in Bayesian inference 262, 263, 
391-397 
inadequacies of 472-475 
likelihood ratio test 476,487^88 
minimax 472 
one-sided tests 396, 397,472 
of point null hypothesis 394, 406, 407, 
414-^16 
power function 470,473 
simple versus composite 392, 394 
simple versus simple 392,471 
size of a test 471 
test procedure 470 
two-sided tests 473 
unbiased test 473 
uniformly most powerful test 472,473 
see also Bayes factor 
see also Significance testing 
Identifiability 239 
Image analysis 5, 374 
Implementation issues xii, 263, 264 
Importance sampling xiii, 8, 264, 340, 
348-350 
Improper prior distribution 276, 277, 321, 
357-367,421^24,449, 469 
see also Non-informative prior 
see also Reference distribution 
Improper probabilities 163 
Inadmissibility 367 
see also Admissibility 
Incoherence {see Coherence) 
562 
Subject Index 
Independence 
and dependence xi, 167, 168 
characterisation of 37-38 
conditional 45—47 
mutual 46 
of random quantities 113,130,172-190, 
194-197 
pairwise 28 
Induction, a problem of 322, 323 
Infinite exchangeability 
{see Exchangeability) 
Inference 
as a decision problem 6,67-81,92,102, 
420, 444 
Bayesian 241^126 
comparative 376, 445 
conditional and unconditional xiv, 374, 
445,452, 468,475,478,479 
critical issues xiii, 374, 375, 418^126 
fiducial xiv, 9, 444,456-458,461, 
484, 486 
likelihood xiv, 9, 444,454-456,461 
pivotal 458,459,461 
pure problem of 6, 67-81, 102 
statement, answer 255 
structural 459-461 
summaries xii, 255-263 
Inferential processes 
Bernoulli, binomial model 248, 249, 
270-274, 276, 277, 279-282,436 
exponential model 438 
linear regression 442 
multinomial model 441 
multivariate normal model 441 
summaries xiv, 436—442 
negative binomial model 437 
uniform model 438 
univariate normal model 439, 440 
Influential observations 418 
see also Robustness 
Information 6, 17, 77-81, 147-159, 
207-209, 298-300, 360 
amount of x, xi, 6, 77-81 
and the exponential family xii, 207-209 
expected 80, 91, 158, 159 
missing 304, 308, 324, 362 
perfect 65, 149,300 
theories of 10, 91,92 
value of 65, 147-149 
see also Logarithmic divergence 
Information matrix {see Fisher  
information) 
Initial and final precision 478,479 
Insufficient reason, principle of 
{see Bayes-Laplace postulate) 
Insurance 373 
Interquantile range 112 
Intersubjective 102-104, 166, 237 
see also Group communication 
Interval estimation (Bayesian) 
see Credible regions 
Interval estimation (non-Bayesian) 
xiv, 445, 465^169 
confidence limits for 465^169 
most selective interval 467 
shortest confidence interval 467 
Invariance 7, 454 
and Laplace approximation 344, 345 
and noninformative priors 358,362,366 
and representation theorems 
181-190,204 
in estimation 454 
of preferences 26 
Invariant prior distribution 358, 362 
Inverted chi-squared distribution 119,431 
Inverted gamma distribution 119, 431 
Inverted Pareto distribution 120, 432 
Iterative quadrature 
{see Numerical quadrature) 
Subject Index 
563 
Jacknife 371 
Jeffrey's rule 94 
Jeffreys' prior, rule 314, 315, 357-362 
see also Non-informative prior 
see also Reference distribution 
Kernel density estimate 351, 355 
Kernel of a density 129 
Lack of memory property 187, 190 
Laplace approximation xiii, 8, 264, 
340-345,455 
Laplace's rule of succession 272, 322,323 
Large sample approximations 
comparing Bayes to classical 486 
to posterior distribution 285-297, 
314,365 
see also Asymptotic analysis 
Law 10, 374 
Law of large numbers 126 
Law of the iterated logarithm 127 
Least favourable prior distribution 449 
see also Non-informative prior 
see also Reference distribution 
Lebesgueintegration, measure 111, 133, 
141,161-163 
Length, operational definition of 82, 83 
Likelihood function 7, 43, 173, 174, 177, 
185,243,450,480,481 
approximation for large samples 485, 
486 
conditioned 48 
integrated 245, 390, 470 
Likelihood inference xiv, 9, 444, 
454^56, 461 
Likelihood principle 249, 250, 454 
and noninformative priors 367 
and stopping rules 250-255 
derivation of 249, 250 
implies sufficiency 455 
satisfied by Bayesian analysis 249 
violation of 463 
Likelihood ratio 49, 392, 455, 470, 476, 
487 
Limit theorems xi, 125-127 
Limiting frequency 173, 174, 177, 179, 
450-454 
Lindley's paradox (see Hypothesis testing) 
Linear models 5, 10, 221, 222, 442 
(see also Regression) 
Linear theory (based on expectation  
operator) 162, 163 
Linear transformation of utilities 55 
Location-scale parameters 320, 362, 379, 
380,415,458 
Logarithmic divergence 76, 91, 278, 279, 
286,287,358,402,416,476 
Logistic distribution 122, 239, 240, 349, 
433 
Logistic model 220, 382 
Logit model 219, 220, 382 
Loss functions 256-263,445^149, 461 
absolute value 257 
for inference problems 256-263 
from uti lity functions 256,391,396,445 
linear 257, 396, 397 
logarithmic divergence 279 
quadratic 85, 88, 257, 300, 301, 397, 
401,406 
standardised quadratic 301 
zero-one 257, 405 
Lottery (see Option) 
canonical 85 
Mahalanobis distance 416 
Marginal distribution 128,166 
Marginalisation paradoxes 363, 364, 
479-481 
Marginalisation procedures xiv, 128, 445 
Markov chain Monte Carlo xiii, 8, 264, 
340, 353-356 
Marriage problem 61-63 
564 
Subject Index 
Mathematics, the role of xi, 30, 31,49,50, 
82, 85, 105, 106, 141, 142, 160-164, 
170,226-229 
Maximisation of expected utility 52, 56, 
90,91,146, 147,256,386,388 
Maximum entropy, procedures 
10, 209, 366 
Maximum likelihood estimate 
288,461,465 
asymptotic distribution of 485 
Mean 112, 131,357,258,461 
see also Expectation 
Mean squared error 462-465 
Measure theory 111, 133, 141, 
161-163,177 
Median 112, 257, 258,461 
Medical diagnosis 44, 45 
Meta-analysis 374 
Metropolis-Hastings algorithm 355, 356 
Minimax principle, procedures 360, 449, 
461,472 
Missing data 374 
Missing information 304, 308, 324, 362 
Mixtures (see Representation theorems) 
finite 279-283, 374, 462 
Mode 112,257,258,461 
Model comparison 
and Bayes factors xiii, 390-394, 405, 
414, 417, 422^24 
and covariate selection xiii, 407^t09 
approximation by cross validation xiii, 
403-407, 409,420-424 
as a decision problem xiii, 386-409 
general utilities for xiii, 395-402 
perspectives on xiii, 383-385 
zero-one utilities for xiii, 389-395 
Model comparison perspectives 
completed view 385 
closed view 384 
open view 385 
Model rejection 
discrepancy measures for xiv, 412-417 
through model comparison 
xiv, 409^12 
Modelling xi, 7,165-240 
and exchangeability 167-181 
and invariance 181-190 
and partial exchangeability 209-222 
and remodelling xiv, 377^126 
and sufficient statistics 191-207 
critical issues xii, 237-240 
scientific 237, 238 
technological 237, 238 
Models 7, 114 
and invariance xii, 181-190 
comparison of xiii, 377^t09,483 
choice of xiv, 8,377^t09,419,420,445, 
486-488 
criticism of 419,420, 483 
elaboration of xii, 229-232 
empirical 237, 238 
explanatory 237,238 
nonparametric xii, 228 
parametric xii, 7, 172-234, 242-255, 
380-383 
predictive 165-167, 243, 244 
ranges of xiii, 8, 277-283 
rejection of xiv, 409-417 
role of 237, 238 
simplification of xii, 233, 234 
Moment-generating function 114 
Moments 112, 113, 131, 132 
Monetary bets 86-88, 162 
Monotone continuity of uncertainty 
relation 107 
Monotonicity of uncertainty 
relation 27 
Subject Index 
565 
Monte Carlo methods 
(see Stochastic simulation) 
Multinomial distribution, model 
133,134,176, 177,216,433 
and exchangeability xi, 176, 177 
conjugate analysis of 441 
conjugate family for 441 
inferential process for 441 
reference distributions for 336, 441 
Multinomial-Dirichlet distribution 
135,136,433 
as predictive distribution 441 
Multiple regression 
221,222,383,442 
Multivariate analysis 5, 10, 374 
Multivariate distributions 
133-141,433-435 
Multivariate normal distribution, model 
136-138, 140,365,434 
and exchangeability xi, 185, 186 
as prior distribution 441,442 
as approximation to posterior  
distribution 286-297, 314, 365 
conjugate analysis of 441 
conjugate family for 441 
inferential process for 441, 449 
reference distributions for 441 
sufficient statistics for 441 
Multivariate normal-gamma distribution 
140,435 
as prior distribution 442 
Multivariate normal-Wishart distribution 
140,435 
as prior distribution 441 
Multivariate Student distribution 
139,140,435 
as marginal posterior distribution 441 
in linear model 442 
Mutual independence 46 
Negative-binomial distribution 
116,119,429 
conjugate analysis of 437 
conjugate family for 437 
inferential process for 437 
reference distributions for 437 
sufficient statistics for 437 
Negative-binomial-beta distribution 
118,429 
Neighbourhood of distributions 370 
Neyman factorisation criterion 193-195 
Neyman-Pearson lemma 471, 472 
No-data problems 446 
Non-Bayesian theories 
alternative approaches xiv, 445-460 
hypothesis testing xiv, 469-475 
interval estimation xiv, 465^t69 
point estimation xiv, 460-465 
significance testing xiv, 475-478 
Non-central chi-squared distribution 
121,431 
Non-informative prior 277, 298, 314, 
357-367 
invariant prior 358, 361, 362, 366 
Jeffreys' prior 314, 315, 357-362 
see also Reference distribution 
Nonparametric models, inference 
xii, 5, 228 
Normal distribution, model 7, 121, 123, 
136, 155-157, 181-185, 196, 216, 
432,459, 463,464,478, 480 
and exchangeability xii, 181-185 
as approximation to posterior  
distribution 286-297, 314, 365 
as maximum entropy choice 209 
as prior distribution 253, 300 
"biased" stopping rule for 253-255 
conjugate analysis of 274,439,440 
conjugate family for 268,439,440 
inferential process for 253-255, 297, 
361-365, 369, 394, 396, 397, 401, 
402, 406,407,415,416,439,440, 
446, 453, 473 
566 
Subject Index 
reference distributions for 328-333, 
439,440 
sufficient statistics for 196, 199, 439, 
440, 462 
Normal-gamma distribution 136, 434 
as prior distribution 268 
Nuisance parameter xiv, 245, 445, 
479-481 
Numerical approximations xiii, 339-356 
see also Approximations 
Numerical quadrature xiii, 8, 264, 
346-348 
Objectivity xii, 2, 3, 99-102, 236, 237, 
275, 298,424,425 
Observables xii, 7, 241-247 
Occam's razor 476 
Odds 49, 357, 390, 391 
Oil wildcatters 53, 54 
Operational perspective x, 18, 22, 51, 
81-83, 85, 87, 90, 94, 99, 100, 102, 
161,195,235,243,298 
Opportunity loss 65, 149 
Optimal stopping 59-63 
Optimisation 10 
Options 18, 19 
finite 23-33 
generalised xi, 141-149 
standard 29-33 
Ordered parametrisation 
323, 333, 364, 367 
Ordering (see Preference relation) 
Origin invariance 187-191 
Outlier 229, 230, 240, 370 
Overfitting 420, 421 
p-values 474-478 
Parameter 7,173-177,179-190,192-197, 
220, 235 
as label of distribution 114-125, 
133-141 
equality 233, 234 
nuisance 245, 445,479^t81 
of conjugate family 265, 266 
of interest 245 
see also Hyperparameter 
see also Location-scale parameters 
Parametric 
inference 243-247 
model xii, 228, 229 
sufficiency xii, 192, 193 
Pareto distribution 120, 141,432 
as prior distribution 438 
Partial exchangeability xii, 7, 168-170, 
208-222 
Pascal distribution 
(see Geometric distribution) 
Pattern recognition 10 
Pearson family of distributions 343 
Philosophy of science 10 
Pitman estimator 465 
Pivotal inference 458,459, 461 
Point estimation (Bayesian) 8 
Bayes estimate 257, 258, 461, 465 
for absolute error loss 257 
for logarithmic divergence loss 279 
for linear loss 257 
for quadratic loss 257 
for zero-one loss 257 
in inference 257, 258 
Point estimation (non-Bayesian) 
xiv, 9,445,460-465 
best linear unbiased (BLUE) 463 
bias 359, 462^t65 
consistent 312, 320, 326, 353, 405, 
463^65 
efficiency of 462 
maximum likelihood (MLE) 288, 359 
minimum variance bound (MVB) 
464,465 
unbiased 348, 462^t65 
uniform minimum variance (UMV) 
464,465 
Subject Index 
567 
Poisson distribution, model 
116,199,206,429 
as an approximation to the Binomial 
76,77 
characterisation via sufficiency 206 
conjugate analysis of 274, 437 
conjugate family for 267, 268,437 
inferential process for 437 
reference distributions for 437 
sufficient statistics for 437 
Poisson-gamma distribution 119,429 
as predictive distribution 437 
Posterior distribution 9, 43, 94, 130, 175, 
242-247 
approximation for large samples 
286-297,314,365 
convergence to correct value 285-287 
reference (see Reference distribution) 
Posterior odds ratio 49, 356, 390 
Power function 470-475 
Pragmatism 81 
Precise measurement 285 
Precision matrix 
of multivariate normal distribution 137 
of multivariate t distribution 140 
Precision, of normal distribution 121, 182 
Prediction 7, 10, 407, 408, 445, 454 
comparative issues xiv, 482-485 
with quadratic loss 300, 301, 397-399, 
401,402,406,407 
Predictive 
distribution 43, 243-247, 419, 483 
inference 243-247, 483 
model 167,419,444 
sufficiency 191-193 
Preference relation x, xi 
among options 17-33, 52, 56 
among decisions 141-147 
precise measurement of 31-33 
Prequential analysis 485,488 
Prescriptive theory 4, 23, 32, 95-98 
Prevision 162, 163 
Principle of optimality (Bellman) 61 
Prior distribution 9,43,94,130, 173-182, 
185, 186,189,190,234,235, 
241-247, 357-367, 444, 446, 450, 
470 
and modelling xii, 165-235 
approximation 279-285 
conjugate (see, Conjugate prior family 
of distributions) 
elicitation of 5, 160, 234,235, 370, 375 
hierarchical 222-226, 371, 372, 383 
invariant and relatively invariant 358, 
361,362 
Jeffreys' prior 314, 315, 357-362 
least favourable 449 
maximum entropy 365 
non-informative 357-367 
"objective" 275, 298, 357 
possible dependence on stopping rule 
252,255 
reference 298, 300-339, 361, 
363-365, 416 
robust 367-371 
'Vague" 8, 357-367 
see also Improper prior distribution 
see also Non-informative prior 
see also Reference distribution 
Prior ignorance xiii, 8, 264,298, 357-367 
see also Non-informative prior 
see also Reference prior 
Probability 1-3, 6, 7 
assessment 10,150, 234, 235, 370, 375 
classical (or symmetry) 3, 33, 99-102 
degrees of belief as 33-49 
frequentist 3, 33, 99-102 
logical 3, 99-102 
personal 33-49, 99-102 
review of mathematics of xi, 109-141 
upper and lower 99 
see also Subjective probability 
Probability density function 
(see Density function) 
568 
Subject Index 
Probability distribution 36, 37 
multivariate xi, 8, 127-141,433-435 
summaries of xiv, 112, 113, 131-133, 
427^t35 
univariate xi, 8, 109-125, 427^t33 
utility of x, xi, 69-75,151-154 
see also Distribution function 
Probability generating function 114 
Probability (mass) function 
multivariate xi, 128 
univariate xi, 111 
Probability space 108,109,141 
Probability theory, review of xi, 109-141 
Probit model 219, 220, 382 
Profile likelihood 343,455, 481 
Psychological research 10 
Quality assurance 374 
Quantification 28-33, 98, 99 
Quantile 112,257,370 
Quantitative coherence x, 22-33, 83-85 
Quasi-random numbers 350 
Radon-Nikodymderivative 111, 133 
Random 
characteristic function 183-185 
distribution function 179 
quantity xi, 7, 109-114, 165-167 
sample 7, 169, 173, 177,185, 189, 190 
vector xi, 127-133 
Randomisation 472 
Rao-Black well theorem 464 
Rationality 4, 5, 13-16 
Reference analysis xiii, 8, 9, 264, 298- 
339,458,459 
Reference analysis for 
an induction problem 322, 323 
binomial and negative binomial models 
315,316 
deviation from uniformity model 313 
exponential model 457 
infinite discrete case 338 
location model 320 
multinomial model 336, 337 
normal correlation coefficient 337-338, 
363, 364 
normal mean and standard deviation 
328-330, 361 
normal standardised mean 330-331, 
362, 363 
normal variance 301, 302 
prediction with quadratic loss 300, 301 
product of normal means 331-333 
several normal means 335, 364, 365 
uniform model 311, 479 
Reference decisions xiii, 299-302 
Reference distribution xiii 
and Jeffreys' prior 314, 315 
and maximum entropy 365 
and model criticism 313 
and ordered parametrisation 323, 333, 
364, 366 
and strong inconsistency 365 
and the likelihood principle 366, 367 
compatibility with sufficient statistics 
309-310 
explicit form of 307, 312, 319 
for hierarchical models 339 
for prediction 339 
given a consistent estimator 312,313 
given a nuisance parameter xiii, 
320-333 
independence of sample size 308, 309 
in the finite case 307, 308 
invariance of 310, 325, 326 
multiparameter xiii, 333-339 
one-dimensional xiii, 302-320 
posterior 300-339, 363-365,416 
prior 298, 300-339, 361, 363-365, 416 
restricted xiii, 316-320 
under asymptotic normality 314-316, 
326-333 
Regression 
coefficient 221 
equation 221 
inferential process for 442 
Subject Index 
569 
multiple 221, 222, 383 
polynomial 222 
simple 222 
trigonometric 222 
Regressor variables 221, 377 
Regulation problems 483 
Relevant event (see Event) 
Relevant subsets 468 
Reliability 10 
Remodelling xiv, 377^t26 
by model comparison 
(see Model comparison) 
by model rejection (see Model rejection) 
critical issues xiv, 418-426 
Reparametrisation 189, 220, 235, 344, 
345, 347, 348 
Repeated sampling 453,454,455 
Reporting beliefs 67-81,150-159, 302, 
316, 320, 357, 399-402,424,425 
Representation theorems xii, 3, 7, 
172-207, 235, 236 
for0-l random quantities 172-175 
for 0-1 random vectors 176, 177 
for several sequences of 0-1 random 
quantities 211-215 
general form of 177-181 
under centred spherical symmetry 
183-186 
under origin invariance 187-190 
under spherical symmetry 181-183 
under sufficiency 191-207 
Residual analysis 418 
Revision of beliefs x, 38^t5 
Riemannian metric 358 
Risk function 446 
Robustness xiii, 5, 99, 160, 235, 239, 
367-371 
Sample 
mean 190 
median 190 
range 190 
size 190 
space 454 
sum of squares 190 
total 190 
Sample survey 10 
Sampling cost (see Cost of observation) 
Sampling-importance-resampling 
xiii, 8, 264, 340, 350-352 
Sampling distribution 173, 177,450 
see also Likelihood function 
see also Model, parametric 
Schwarz criterion 487,488 
Scientific reporting 67-77, 150-157, 302, 
320, 360, 399-402,424,425 
Score function 6, 69-75,88-89, 151-157, 
399-409 
local 72-74, 153, 154 
logarithmic 74-81, 85, 91, 154-159, 
207-209, 302-339, 400-407, 
422^t24,488 
proper 71-74, 152, 399,477 
quadratic 71, 72, 88, 89, 152, 397^t07 
smooth 70, 151,399 
Scoring rules (see Score function) 
Sech-squared distribution 
(see Logistic distribution) 
Secretary problem 61-63 
Sensitivity analysis 160, 235 
see also Robustness 
Sequential 
analysis 5, 375 
decision problem 6, 56-57 
revision of beliefs 47-49 
Several Samples xiii, 211-216, 381 
and Bernoulli, binomial models 
211-215,216 
and multinomial models 216 
and normal models 216 
Significance testing xiv, 9, 262, 416, 445, 
475^t78 
inadequacies of 476,479 
pure significance test 475 
570 
Subject Index 
significance level (p-value) 
471,474-478 
see also Hypothesis testing 
Significant event 27, 28 
Simple hypothesis (see Hypothesis) 
Simpson's paradox 41,42 
Singular distribution 111 
Snedecor distribution 123, 124, 140,432 
Spectral analysis 10 
Spherical quadrature 348 
Spherical symmetry 181-183 
Splines 374 
Spun coin, the 279-281 
Squared root inverted gamma distribution 
119,431 
St. Petersburg paradox 87 
Standard deviation 112 
Standard events 29-32, 50-52, 54 
Standard normal distribution 121, 123 
Standard Student distribution 123 
Statistic 190 
Statistical decision problem 
(see Decision Problem) 
Statistical inference (see Inference) 
Statistical models (see Models) 
Stein's paradox 365 
Stirling's formula 414 
Stochastic approximation 374 
Stochastic assumptions 238, 239 
Stochastic complexity 485,488 
Stochastic simulation 264, 340, 348-356 
Stopping rule xii, 7, 247-255, 367, 375 
Straight-line model 220, 222, 382 
Strong law of large numbers 126,173,182, 
185,188,286,288 
Strong inconsistency 365 
Structural assumptions 238, 239 
Structural inference 459-461 
Structured layouts xii, 217, 218, 381, 382 
Student distribution 122, 123, 136, 139, 
239, 240, 329, 335, 369, 385, 416, 
432,459,462,469, 480 
Subjective probability ix, 2, 3,94,99-102 
measurement of 31, 32, 85 
uniqueness of 35, 37, 41 
see also Beliefs 
Subjectivity ix, xii, 2,3,99-102,195,236, 
237, 275, 424, 425 
Sufficiency 247-255,451 
parametric xii, 192, 193 
predictive xii, 191-193 
Sufficient statistics xii, 7, 190-207, 462 
definition of 191, 192 
factorization criterion for 193,194 
for exponential family xii, 197-207 
in conjugate analysis 264-285 
minimal 197 
Sure-thing principle 26 
Survival analysis 5 
Symmetry 3, 7, 33, 168-170 
centred spherical 183-186 
complete 169 
spherical 181-183 
t distribution (see Student distribution) 
Temporal coherence 94 
Test procedure (see Hypothesis testing) 
Test statistic 412-417 
Testing hypotheses 
(see Hypothesis testing) 
Time series 5, 374 
Tolerance region 484 
Transformation 
elaboration 230, 231 
of random quantities 111, 113,130-132, 
246, 295-297, 344, 347, 350, 
358, 360 
Transitivity 24-26, 84 
Subject Index 
571 
Truncated exponential distribution, 
model 200 
Two-way layout 217,218 
Type 1 and type 2 error probabilities 
(see Hypothesis testing) 
Unbiased estimation 348,462-465 
Uncertainty 2-5, 13-16 
precise measurement of 31-33 
relation 21-38, 106-109 
Uniform distribution, model 117, 349, 
380,430 
as prior distribution 308, 320, 356, 357 
conjugate analysis of 438 
discrete 79, 117,308,356 
inferential process for 468, 479,438 
reference distributions for 311, 438 
sufficient statistics for 199, 200, 438 
Uniformly most powerful test 472, 473 
Uniqueness 
of probability measures 37 
of conditional probability measures 41 
Utility x, xi, 6, 16, 49-75, 141-154 
and loss functions 256, 391, 396, 445 
axioms for 90, 91 
bounded 49-54 
canonical 50 
elicitationof51,53, 54, 91 
expected (see Expected, utility) 
for money 87 
general 54 
logarithmic 
(see Score function, logarithmic) 
of aprobability distribution x, xi, 69-75, 
151-154 
zero-one 389-395 
see also Score function 
Value of perfect information 65, 66, 149 
Variance 112, 113 
Weak law of large numbers 126 
Weight of evidence 390 
Weighted average 
form of posterior means 275, 276, 368 
form of prediction 384, 398 
of posterior distributions 281, 282, 387 
of prior distributions 280, 282 
Wishart distribution 138-140, 435 
as prior distribution 441 
Zero-one 
discrepancy xiv, 413,414 
utility xiii, 389-395 
573 
Author Index 
In an attempt to signal the contributions of authors who otherwise are subsumed 
anonymously in the "et al." of multi-author papers, we have included page 
references for all authors of such papers, even though only the first author's name 
actually appears on the page. 
Abramowitz, M. 156, 489 
Achcar, J. A. 345, 489 
Aczel.J. 91,489 
Adekola, A. D. 289,548 
Aitchison, J. 10, 99, 262, 483,489 
Aitken, C. G. G. 10, 489 
Aitkin, M. 417,489 
Akaike, H. 359,367,455,487,489,490 
Albert, J. H. 352, 426, 481, 490 
Aldous, D. 236, 490 
Allais, M. 96, 97, 98, 490 
Ameen, J. R. M. 374, 490 
Amaral-Turkman, M. A. 483,490 
Amster, S. J. 376, 490 
Andersen, E. B. 455, 481, 490 
Anderson, T. W. 376,445,490 
Angers, J.-F. 371,490 
Anscombe, F. J. 10,85,295,490 
Ansley.C.F 374,491 
Antoniak, C. 228,491 
Aoki,M. 10,374,491 
Arimoto, S. 91,491 
Arnaiz, G. 230,491 
Arnold, S. F. 11,491 
Arrow, K. J. 91,103,104,491 
Ash, R. B. 106,111,162,173,491 
Aumann, R. J. 85, 104,490, 491 
Aykac, A. 10,491 
574 
Author Inde. 
Bahadur, R. R. 451,491 
Bailey, R. W. 123,491 
Balch, M. 85,491 
Bandemer, H. 160, 491 Barlow, R. E. 
11,104,172,236,374,491,492,523 
Barnard, G. A. 2,95,250,270,362,376, 
455, 458,492 
Barndorff-Nielsen, O. E. 206,455,481, 
484, 492 
Barnett, V. 102, 376, 445, 492 
Barrai, I. 10, 492 
Bartholomew, D. J. 11, 376, 492 
Bartlett, M. 422,493 
Basu, D. 136, 250, 374, 376, 452, 453, 
468,479,481,493 
Bauwens, L. 10,493 
Bayarri, M. J. viii, 104, 243, 250, 313, 
338, 364, 371, 417, 418, 493, 494, 
496 
Bayes, T. 1,2, 89, 101, 102, 356, 357, 
482, 494 
Beauchamp, T. J. 232, 534 
Becker, G. M. 91,494 
Bellman, R. E. 61, 494 
Berger, J. O. viii, 9, 10, 11, 102, 250, 
258, 263, 302, 306, 307, 333, 334, 
337, 338, 362, 366, 370, 371, 373, 
375, 394, 417, 423, 449, 475, 479, 
490, 493, 494, 495, 496, 506, 519, 
520, 536, 554 
Berger, R. L. 104, 376, 445, 475, 495, 
500 
Berk, R. H. 287,495 
Berliner, L. M. 228,371,374,494,495, 
496 
Bermildez, J. D. 71,232, 289,419,496, 
497 
Bernardo, J. M. 9, 10, 11, 18, 71, 92, 
141, 156, 232, 302, 305, 306, 307, 
313, 323, 333, 334, 337, 338, 362, 
365, 366, 373, 374, 394, 417, 419, 
422, 468, 481, 483, 494, 495, 496, 
497 
Bernoulli, D. 87, 497 
Bernoulli,! 89,101,497 
Berry, D. A. 10,160,376,495,497,536, 
Besag, J. 353, 356, 374,497 
Bessant, R. G. 44, 535 
Best,N. G. 353,355,516 
Bickel, P. J. 455, 463, 497 
Binder, S. 373, 497 
Birnbaum, A. 11, 250, 455,456, 497 
Bj0rnstad, J. F. 455, 485, 498 
Blackwell, D. 91, 103, 159, 356, 463, 
464, 498 
Blyth, C. R. 42, 498 
Boos, D. D. 455, 534 
Booth, N. B. 374, 498 
Bordley, R. F. 96, 498 
Borel, E. 89, 498 
Borovcnik, M. 9, 498 
Bowen, K. C. 10, 104, 552 
Box,G.E.P. 9,10,44,92,93,230,231, 
258, 260, 263, 360, 361, 367, 370, 
376,413,417, 419, 498, 499,549 
Boyer, J. E. Jr. 373,516 
Boyer, M. 10,499 
Brant, R. 418, 500 
Breslow, N. 11,499 
Bretthorst, G. L. 10,499 
Bridgman, P. W. 82, 499 
Brier, G.W. 71,499 
Brillinger, D. R. 458,499 
Broemeling, L. D. 10, 499 
Brown, L. D. 10, 206, 258,499 
Author Index 
575 
Brown, P. J. 10, 373, 374, 499 
Brown, R. V. 100, 104, 499, 531 
Brown, S. J. 420, 527 
Brumat, C. 10,491 
Brunk, H. D. 162, 499 
Buehler, R. J. 88, 154, 468, 469,499 
Bunn,D.J. 104,499 
Butler, R. W. 455, 485,499 
Campling, G. E. G. 374, 552 
Cano, J. A. 371, 394, 451, 500, 534 
Carlin, B. P. 355, 374,500 
Carlin,J. B. 371,500 
Carnap, R. 90,100, 500 
Caro, E. 104, 500 
Casella, G. 353, 376, 445, 468, 469, 
475,500,519,532 
Chaloner, K. 160,418,500 
Chan, K. S. 353, 500 
Chang, H. 418,419,420,515 
Chang, T. 362, 500 
Chankong, V. 104,500 
Chao, M. T. 289,500 
Chateaneuf, A. 99, 500 
Chen, C. F. 289, 501 
Chernoff, H. 10,91, 160, 475,501 
Chow, Y. S. 178, 501 
Christ, D.E. 261,523 
Chuang.D. T. 371,525 
Chuaqui, R. 90, 501 
Cifarelli, D. M. 9, 10, 11, 483, 501 
Clarke, B. 367, 501 
Clarke, R. D. 100, 501 
Claroti, C. A. 10, 501 
Clayton, D. G. 353, 355, 516 
Clayton, M. K. 104,419,501,516 
Clemen, R. T. 104, 501 
Cochrane, J. L. 104, 501 
Collier, R. O. 10, 412 
Coletti,G. 10,492 
Conlisk, J. 87,501 
Consonni, G. 279,422, 501 
Cook, R. D. 418,551 
Cooke, R. M. 10,501 
Cornfield, J. 11,88,361,468,502,514 
Cowell, R. G. 426, 502, 546 
Cox, D. D. 374,502 
Cox,D. R. 68,231,237,238,343,368, 
376, 445, 455, 479, 480, 481, 484, 
487, 498, 502 
Cox, R. T. 90, 502 
Crosse, J. 375, 552 
Crowder, M. J. 289, 502 
Csiszar, I. 366, 502 
Cuevas, A. 371,502 
Cyert, R. M. 10,502 
Daboni, L. 9, 502 
Dalai, S. 228, 283, 502 
Dale, A. I. 2, 10,502 
Darmois, G. 203,502 
DasGupta, A. 10,160, 262,495, 502 
Davis, W. W. 350, 547 
Davison, A. C. 455, 503 
Davison, D. 91, 503 
Dawid, A. P. 10, 11, 47, 93, 162, 183, 
230, 236, 250, 279, 289, 362, 363, 
371, 373, 374, 460, 481, 485, 488, 
496, 503, 504, 545, 548 
Dean, G. W. 10,520 
Debreu, G. 91, 504 
Deely, J. J. 160, 373,504, 528, 531 
de Finetti, B. vii, 2, 3,9,10,11, 71, 77, 
86, 87, 88, 89, 101, 102, 106, 109, 
110, 162, 163, 172, 212, 230, 235, 
242, 375, 483,504, 505 
576 
Author Index 
DeGroot, M. H. viii, 9, 10, 11, 62, 91, 
102, 104, 133, 158, 160, 243, 250, 
258, 263, 276, 289, 362, 373, 375, 
376, 397, 417, 418, 445, 449, 493, 
494,497,502,505,508,517 
de la Horra, J. viii, 258, 371, 463, 506 
Delampady, M. 263,371,394,417,475, 
495, 506 
Dellaportas, P. 348, 355, 506 
Demoment, G. 10,534 
De Morgan, A. 89, 506 
Dempster, A. P. 99,371,419,500,506 
DeRobertis, L. 99, 371, 506 
Desu, A. 483,514 
de Vos, A. F. 423 
Devroye, L. 355, 506 
De Waal, D.J. 104,158,506 
Dey,D.K. 359,371,418,419,420,506, 
514,515,535 
Diaconis, P. viii, 94,188,204,206,207, 
226, 227, 236, 273, 276, 279, 283, 
348,463,506,507,512 
Di Bacco, M. 10,492 
Dickey, J. M. 11, 103, 123, 163, 228, 
263, 283, 375, 378, 411, 417, 424, 
425,507,508,513,528 
Diebolt, J. 374, 508 
Doksum, K. A. 228, 371, 508 
Domfnguez, J. I. 104, 500 
Domotor, Z. 90, 508 
Dransfield, M. 347, 426, 545 
Draper, N. R. 158,508 
Dreze, J. H. 239,508 
Dubins, L. E. 9, 104,162, 498, 508 
Duckstein, L. 104,517 
DuMouchel, W. 374,508 
Duncan, G. 160, 508 
Duncan, L. R. 10,508 
Dunsmore, I. R. 10, 373,483,489,490, 
508 
Durbin, J. 250,475, 508, 509 
Dykstra, R. L. 228, 509 
Dynkin, E. B. 235, 509 
Earman, J. 2,509 
Eaton, M. L. 89,227,362,367,507,509 
Eaves, D. M. 362, 422, 500,509 
Eddy.W. F. 419,514 
Edwards, A. W. F. 455, 509 
Edwards, W. L. 10, 11, 91, 103, 104, 
263,371,375,425,509 
Efron,B. 365,371,373,458,481,509 
Eichhorn, W. 37, 509 
Eliashberg, J. 104, 509 
El-Krunz, S. M. 160,510 
Ellsberg,D. 96,98,510 
Epstein, E. S. 71,535 
Escobar, M. D. 374, 552 
Erickson, G. J. 10,510,546 
Ericson, W. A. 11, 373, 374, 510 
Fan,T. H. 371,495 
Fang, B. Q. 373, 504 
Farrell, R. H. 258, 263, 510 
Faulkenberry, G. D. 485, 529 
Fearn,T. 10,510 
Feddersen, A. P. 469, 499 
Fedorov, V. V. 159,510 
Feller, W. 46, 510 
Fellner, W. 10, 510 
Felsenstein, K. 160,417,510 
Ferguson, T. S. 11, 62, 102, 228, 446, 
463,510 
Fernandez, C. 371,506 
Ferrandiz, J. R. 18,141,261,365,416, 
417,497,510,511 
Author Index 
577 
Fieller.E. C. 458,511 
Fienberg, S. E. 10, 104, 505,511 
Fine,T. L. 56,98,99,102,511,551 
Fishburn, P. C. 10, 11, 83, 84, 85, 90, 
91, 102,103,104,491,511 
Fisher, R. A. 68, 195, 314, 338, 420, 
424,450,451,457,460,511 
Florens, J.-P. 9, 10, 164, 239, 362, 374, 
420,512 
Flournoy, N. 348,512 
Fliihler.H. 11,426,540 
Fougere, P. T. 10,512 
Foulis, D. J. 95, 540 
Fraser, D. A. S. 250,289,455,459,460, 
481,512 
Freedman, D. A. 87,182,204,206,207, 
226, 227, 235, 236, 289, 463, 507, 
512,513 
Freeman, P. R. 10, 62, 103, 230, 425, 
485,508,513,551 
French, S. 5, 10, 90, 104, 375, 513,541 
Friedman, M. 91,513 
Fu,J. C. 289,513 
Gamerman, D. 374,513 
Gardenfors, P. 10,104,513 
Garthwaite, P. H. 375, 513 
Gaskins, R. 228,519 
Gathercole, R. B. 374, 546 
Gatsonis, C. A. 10,89,513 
Gaul,W. 103,513 
Geisser, S. 10, 11, 90, 316, 322, 361, 
362, 373, 405, 418, 419, 483, 485, 
501,513,514,524 
Gelfand, A. E. vii, 247, 350, 353, 355, 
418, 419, 420, 483, 500, 514, 515, 
545,551 
Gelman, A. 353,515 
Geman, D. 354, 374, 515 
Geman, S. 354,374,515 
Genest.C. 11,104,515 
George, E. I. 232, 353, 367, 373, 500, 
515 
Geweke,J. 349,515 
Geyer, C.J. 353,515 
Ghosal, S. 289,516 
Ghosh, J. K. 289, 324,455, 497, 516 
Ghosh, M. 10,11,258,373,516 
Gilardoni, G. L. 104,516 
Gilio, A. 89,263,371,516 
Gilks, W. R. 353, 355, 426, 516, 549 
Gillies, D. A. 2,516 
Gilliland, D. C. 373,516 
Girelli-Bruni, E. 9,516 
Gir6n, F J. viii, 99,104, 373, 374, 419, 
497,500,516 
Girshick, M. A. 91,446,498, 517 
Godambe, V. P. 10,374,517 
Goel, P. K. 10, 104, 158, 276, 371, 373, 
425,496,517 
Goicoechea, A. 104,517 
Goldstein, M. 11, 94, 163, 373, 426, 
455,517,518 
Gdmez,E. 417,518 
Gdmez-Villegas, M. A. viii, 371, 417, 
518 
Good, I. J. 9, 10, 11, 75,90, 91,94, 99, 
100, 102, 154, 228, 263, 360, 373, 
374,390,458,518,519,536 
Goodman, B. C. 375, 509 
Goodwin, P. 375, 519 
Gordon, N.J. 370,519 
Goutis, C. 468, 519 
Grandy, W. T. 10, 519, 546 
578 
Author Index 
Grayson, C.J. 10,53,519 
Green, P. J. 353, 356,497 
Grenander, V. 374,519 
Grieve, A. P. 11,426,519,540 
Groenewald, P. C. N. 104, 158, 506 
Gu, C. 374,519 
Gulati. C. M. 104,517 
Gupta, S. S. 10,519,520 
Gutiercez-Pena, E. 277,520 
Guttman, I. 158, 230, 418, 484, 508, 
520, 537 
Haag, J. 235, 520 
Hacking, I. 99,102, 520 
Hadley, G. 10, 520 
Hagen, D. 96,490 
Haimes, Y. 104, 500 
Hald, A. 374, 520 
Haldane, J. B. S. 315,362,520 
Hall, W. J. 228, 283, 502 
Hall, W. K. 362, 535 
Halmos, P. R. 451,520 
Halter, A. N. 10, 520 
Harris, J. E. 374, 508 
Harrison, P. J. 10, 374, 520, 539, 552 
Harsany, J. 104,520 
Hartigan, J. A. 9,99,130,163,164,250, 
289, 359, 360, 362, 371, 455, 506, 
520, 521 
Hartley, R. 10, 104, 513 
Hasminski, R. Z. 289, 523 
Hastings, W. K. 356,521 
Hays, W. L. 375,509 
Heath, D. L. 88, 90, 162, 172, 521 
Henrion, M. 375, 534 
Hens, T. 84, 521 
Hernandez, A. 451,500 
Herstein, I. N. 91,521 
Hewitt, E. 235, 521 
Hewlett, P. S. 219,521 
Heyde,C. C. 289,521 
Hildreth, C. 425, 521 
Hill, B. M. 11, 162, 227, 228,250, 371, 
373,420,496,521,522 
Hill, S. D. 362, 546 
Hill, W.J. 417,498 
Hills, S. E. 239,295,338,348,355,515, 
522 
Hinkelmann, K. 10, 522 
Hinkley, D. V. 368, 376, 445,480, 485, 
487, 502, 522 
Hipp, C. 203, 522 
Hjort, N. L. 228, 522 
Hoadley, B. 373, 522 
Hodges.J.S. 10,11,104,263,419,513, 
514, 522 
Hogarth, R. 104, 375, 522 
Holland, G. D. 2, 522 
Hop, J. P. 349, 549 
Howard, J. V. 455,518 
Howson, C. 10, 522 
Hsu, J. S. J. 345, 374, 375, 529, 530 
Hull, J. 87,91,523 
Huseby, A. B. 104, 523 
Hutton, I. 44, 535 
Huzurbazar, V. S. 203, 523 
Hwang, J. T. 258, 523 
Hwang, J. T. G. 465, 469, 500, 541 
Hylland, A. 104, 523 
Ibragimov, I. A. 289, 523 
Irony, T. Z. 11, 258, 374, 376,491, 523 
Isaacs, G. L. 261,523 
Isaacson, D. 371,533 
Iversen, G. R. 9, 523 
Iyengar, N. S. 10,517 
Author Index 
579 
Jackson, J. E. 375, 523 
Jackson, P. H. 10,261,523,535 
Jaffray, J. Y. 99, 500 
James, W. 449, 523 
Jaynes, E. T. 10, 90,227,357,363,365, 
366,375,451,468,523 
Jefferys, W. H. 371,495 
Jeffrey, R. C. 10, 94, 524 
Jeffreys, B. S. 73, 153, 208, 341, 524 
Jeffreys, H. 9,73,90,98,153,208,263, 
288, 314, 315, 322, 341, 358, 359, 
360, 361, 362, 366, 370, 414, 419, 
478, 524, 553 
Jenkins, G. M. 250, 455, 492 
Jennings, D. E. 419,501 
Jewell, W. S. 373, 524 
Johnson, N. L. 114,133,524 
Johnson, R. A. 289, 524 
Johnson, W. 418, 524 
Johnstone, I. M. 289,521 
Joshi,V. M. 11,250,524 
Justice, J. M. 10,525 
Kadane, J. B. 10, 11, 91, 98, 104, 130, 
160, 162, 239, 243, 250, 341, 344, 
371, 373, 375, 411, 472, 494, 505, 
508, 525, 526, 543, 544, 549, 550, 
551 
Kagan, A. M. 184, 525 
Kahneman, D. 96, 375, 525 
Kalbfleish, J. G. 455, 481, 484, 525 
Kapur, J. M. 10, 525 
Karlin, S. 417,525 
Kashyap, R. L. 360, 526 
Kass, R. E. 10,289, 341,348,358,361, 
373,417,513,526,549 
Keeney, R. L. 10, 91,104, 526 
Kelly, J. S. 104,526 
Kent, J. T. 374, 532 
Kempthome, P. J. 371, 526 
Kesavan, H. K. 10, 525 
Kestemont, M.-P. 228, 526 
Keynes, J. M. 9, 90, 98, 100, 526 
Khintchine, A. I. 235, 526 
Kiefer, J. 480,526 
Kihlstrom, R. E. 10, 499 
Kim, K. H. 104, 526 
Kimeldorf, G. S. 228, 526 
Kingman, J. F. C. 106, 182, 527 
Kirby.A. J. 353,355,516 
Klein, R. 483, 527 
Klein, R. W. 420, 527 
Kleiter, G. D. 9, 527 
Kloek, T. 349, 527, 550 
Klugman, S. A. 9, 527 
Knill-Jones, R. 95, 546 
Koch, G. 236, 527 
Kogan, N. 104, 527 
Kohn, R. 374, 491 
Kolmogorov, A. N. 485, 527 
Koopman, B. O. 98, 527 
Koopman, L. H. 203, 527 
Korsan, R. J. 426, 527 
Kotz, S. 114,133,524 
Kraft, C. 90, 527 
Krantz, D. H. 85,90, 104, 527, 532 
Krasker, W. S. 420, 527 
Kuchler, U. 236, 527 
Kuhn, T. S. 93, 527 
Kullback, S. 6, 76, 91, 208, 527 
Kyburg, H. E. 10, 87,99,102, 527,528 
580 
Author Index 
Lad,F. 160,163,528 
Ladalla, J. N. 289, 524 
LaMotte,L. R. 11,528 
Lane, D. A. 162,485,522,528 
Laplace, P. S. 2, 9, 89, 272, 288, 357, 
528 
Larkey P. 104, 525 
Laud, P. 228, 509 
Lauritzen, S. L. 95, 227, 236, 426, 507, 
527, 528 
Lavalle, I. H. 9, 10, 87, 528 
Lavine,M. 228,371,483,528 
Lawrie, T. D. V. 44, 535 
Le, H. 370, 536 
Le, N. D. 374, 499 
Learner, E. E. 10, 263,417, 528 
LeCam, L. 288,485, 528, 529 
Lecoutre, B. 10, 529 
Lee, P. M. 9, 529 
Lee, T. D. 420, 545 
Lee,T. M. 247,515 
Lehmann, E. L. 15, 68, 237, 238, 465, 
529 
Leibler, R. A. 76, 91, 527 
Lejeune, M. 485, 529 
Lempers, F. B. 10, 263, 529 
Lenk, P. J. 228, 529 
Leonard, T. 10,228,238,345,373,374, 
375, 407, 498, 529, 530 
Levine, R. D. 10, 530 
Lewis, S. M. 353, 539 
Ley, E. 426, 530 
Li, B. 341,553 
Lientz, B. P. 417, 508 
Lindgren, B. W. 10, 530 
Lindley, D. V. viii, 9,10,11,42, 62, 87, 
88, 90, 91, 103, 104, 158, 159, 160, 
261, 263, 272, 289, 338, 345, 359, 
364, 367, 373, 374, 375, 394, 417, 
449, 455, 457, 460, 472, 484, 486, 
496, 497, 499, 501, 504, 530, 531, 
537 
Lindman,H. 11,263,371,425,509 
Linnik, Y. V. 184, 525 
Liseo,B. 371,481,531,532 
Little, R. J. A. 374, 532 
Lo, A. Y. 228, 371, 374, 508, 532 
Lorimer, A. R. 44, 535 
Louter, A. S. 349, 549 
Luce, R. D 11, 85, 90, 91, 96,103, 104, 
527, 532 
Lusted, L. B. 10, 532 
Lwin, T. 373, 532 
Maatta, J. 468, 532 
Machina,M. 91,96,532 
Main, P 370,371,518,532 
Makelainen, T. 373, 499 
Makov, U. E. 373, 374, 515, 532, 549 
Mardia, K. V. 374, 532 
Malitz, J. 90, 501 
Marinell, G. 10, 532 
Maritz, J. S. 373, 532 
Marriott, J. M. 348, 426, 532, 533 
Marschak, J. 91,104, 494,533 
Martin, J. J. 10, 533 
Martin, J. 371,541 
Martinez, L. 419,516 
Martz, H. F. 10, 533 
Maryak, J. L. 258, 546 
Masreliez, J. L. 370, 533 
Matheson, J. E. 10, 375, 546 
Mathiasen, P. E. 484, 533 
Author Index 
581 
Mazloum, R. 160,533 
McCardle, K. F. 104, 535 
McCarthy,! 91,533 
McClintock, C. G. 91, 494 
McCulloch, R. E. 232, 367, 374, 417, 
418,515,533 
McDunnough, P. 289,512 
McKillop, J. H. 44, 535 
McNeil, A. J. 353,355,516 
Meeden,G. 160,366,371,533 
Meinhold, R. 374, 533 
Mendel, M. B. 236,491,533 
Mendoza, M. viii, 295, 307, 362, 495, 
533 
Meng X.-L. 374, 533 
Merkhofer, M. W. 375, 533 
Metropolis, N. 356, 533 
Meyer, D. L. 10, 533 
Migon,H. S. 374,512,552 
Miller, M. 374,519 
Mills, J. A. 374, 534 
Milnor,J. 91,521 
Mitchell, T. J. 160, 232, 534 
Mockus, J. 10, 534 
Mohammad-Djafari, A. 10, 534 
Monahan, J. F. 455, 534 
Moore, P. G. 87, 91, 523 
Morales, D. 160, 536 
Morales, J. A. 10, 239, 534 
Morcillo,C. 419,516 
Moreno, E. viii, 371, 394,451,500,534 
Morgan, M. G. 375, 534 
Morgenstern, O. 84, 90, 91, 446, 550 
Morris, C. N. 206, 343, 344, 373, 509, 
534 
Morris, M. D. 160,534 
Morris, P. A. 104, 534 
Morris, W. T. 10, 534 
Mortera,J. 104,371,374,394,495,505, 
534 
Moses, L. E. 10, 91, 501 
Mosteller.F 9,81,534 
Mouchart, M. 9,10,130,164,239, 373, 
374, 420, 512, 534 
Mueller, P. 374,552 
Muirhead, C. R. 230, 535 
Mukerjee, R. 324,359,516,535 
Muliere, P. 9, 501 
Mufioz, J viii 
Murphy, A. H. 71, 535 
Murray, R. G. 44, 535 
Myerson, R. B. 89, 535 
Nakamizo, T. 10, 374, 543 
Nakamura, Y. 99, 535 
Narens, L. 84, 90, 532, 535 
Nau, R. F. 104, 371, 535 
Naylor, J. C. 346, 347, 426, 533, 535, 
545 
Nazaret, W. A. 371, 537 
Nelder, J. A. 488,535 
Newman, J. R. 104, 509 
Neyman,J. 195,446,450,471,480,535 
Nicolau, A. 359, 535 
Normand, S.-L. 104, 535 
Novick, M. R. 10, 42, 261, 362, 368, 
523, 531, 535, 540 
O'Hagan, A. 9,10,230,348,370,371, 
423, 495, 510, 535, 536 
Oliver, R. M. 10,536 
Ord, K. 407, 530 
Osiewalski, J. 371, 536 
Osteyee, D. D. B. 10, 536 
582 
Author Index 
Pack, D.J. 11,536 
Padgett, W. J. 228, 536 
Page, A.N. 91,536 
Pardo, L. 160, 536 
Parenti, G. 10, 536 
Parmigiani, G. 160, 536 
Pathak,P K. 10,516 
Pearson, E. S. 2,446,450,471,536 
Peers, H. W. 359, 537, 552 
Peirce, C. S. 81,390,537 
Peizer, D. B. 124, 537 
Peiia, D. 230, 418, 420, 520, 537 
Pereira, C. A. de B. 136, 374, 455, 493, 
523, 537 
Perez, M. E. 371,373,537 
Pericchi, L. R. viii, 231, 369, 370, 371, 
373, 405, 407, 422, 423, 424, 534, 
537, 543 
Perks, W. 314,315,359,537 
Peskun, P. H. 356, 537 
Petrella, L. 371, 532 
Petris, G. 162, 540 
Pettit,L. I. 230,417,418,537,538 
PfanzagU. 84,91,489,538 
Phadia,E. G. 11,228,510 
Pham-Gia, T. 160, 538 
Phillips, L. D. 10, 272, 375, 509, 531, 
538 
Piccinato, L. 362, 376,538 
Pierce, D. 468, 469, 538 
' Pilz, J. 10,160, 538 
Pitman, E. J. G. 203, 465, 538 
Plackett, R. L. 219, 521 
Plante, A. 455, 466, 538 
Poirier, D. J. 376, 420, 445, 538 
Polasek, W. 371,538,539 
Pole, A. 10, 366, 374, 538, 539 
Pollard, W. E. 10,539 
Poison, N. G. 9,159,315,370,374,500, 
539 
Poskitt, D. S. 417, 539 
Potzelberger, K. 371,538.539 
Pratt, J. W. 9, 85, 87, 90,124, 250, 262, 
263, 376, 527, 537, 539 
Press, S.J. 9,10,11,104,376,445,483, 
514, 527, 539 
Price, R. 2 
Purves, R. A. 87,512 
Rabena, M. 302, 539 
Racine-Poon, A. 11,426,483,515,539, 
540 
Radner, R. 104, 533 
Raftery, A. E. 353, 468, 540 
Rahman, M. A. 163, 528 
Raiffa, H. 9, 10, 85, 91, 98, 103, 104, 
417, 508, 526, 532, 540 
Ramsey, F. P. 15, 68, 83, 84, 85, 102, 
540 
Ramsey, J. O. 85, 368, 540 
Randall, C. H. 95, 540 
Rao.C. R. 184,464,525,540 
Rao, M. M. 258, 505 
Raoult,J.-R 10,512 
Raynaud, H. 104, 491 
Regazzini, E. 9, 89, 162, 483, 501, 540 
Reichenbach, H. 100, 540 
Reid,N. 343,455,479,481,502,512 
Renyi, A. 77,79,91,102,158,163,540, 
541 
Ressel, P. 236, 541 
Richard, J. F. 10, 541 
Rios, D. 104, 371, 541 
Rios,M.J. 104,541 
Rfos,S. 10,99,104,516,541 
Rfos, S. Jr. 104, 541 
Ripley, B. D. 351,355,541 
Rissanen, J. 358,485, 488, 541 
Author Index 
583 
Ritter, C. 353, 541 
Rivadulla, A. 10, 541 
Robbins, H. 373, 541 
Robert, C. P. 9,373, 374, 394,465,469, 
495, 500, 508, 541 
Roberts, F. 10, 84, 104, 541 
Roberts, G. O. 353, 354, 356, 541, 545 
Roberts, H. V. 11,98,376,425,483, 
542 
Robinson, G. K. 468, 542 
Rodriguez, C. C. 362, 542 
Rolin, J.-M. 9, 164, 228,239, 374,512, 
542 
Rosenkranz, R. D. 10, 542 
Rosenbluth, A. W., 533 
Rosenbluth, M. N., 533 
Rossi, RE. 417,533 
Roush, F. W. 104, 526 
Royall, R. M. 455, 542 
Rubin, D. B. 10, 350, 353, 371, 374, 
419,515,532,533,542 
Rubin, H. 263,371,417,525,542 
Rueda, R. 416, 417, 543 
Rulz-Rivas, C. 230, 491 
Saaty, T. L. 104, 543 
Sacks, J. 258, 543 
Sahlin,N.-E. 10,104,513 
Salinetti,G. 371,532,543 
Samanta,T. 289,516 
San Martini, A. 417, 543 
Sans6,B. 370,371,537,543 
Sanz,R 371,502 
SamdalC.-E. 91,543 
Savage, I. R. 9, 96, 543 
Savage. L. J. 9, 10, 11, 71, 84, 85, 88, 
91, 92, 97, 98, 162, 235, 263, 285, 
371, 425, 446, 451, 478, 508, 509, 
513,517,520,521,543,544 
Savchuk, V. P. 9, 544 
Sawagari, Y. 10, 374, 544 
Schader, M. 103, 513 
Schervish, M. J. 91,104,162, 373, 525, 
543, 544 
Schick, L. H. 10,519 
Schlaifer, R. 9, 10, 85, 417, 539, 540, 
543, 544 
Schmitt, S. A. 9,544 
Schwartz, L. 463, 544 
Schwarz, G. 487,544 
Schweder, T., 468, 540 
Scott, D. 90, 544 
Scott, E. L. 480, 535 
Scott, W. F. 261, 531 
Scozzafava, R. 9, 263, 516, 544 
Seeber, G. 10, 532 
Seidenberg, A. 90, 527 
Seidenfeld, T. 10,90,91,104,162, 373, 
458, 472,525, 527, 543, 544 
Sellke, T. 263, 475,495 
Sen, A. K. 104, 544 
Sendra,M. 261,511 
Serfling, R. J. 295, 544 
Shafer, G. 84, 99,102, 263, 394, 544 
Shannon, C. E. 6, 80, 91, 102,159, 544 
Shao, J. 350, 405,544 
Sharpies, L. D. 353, 355, 516 
Shaw, J. E. H. 347, 348, 349, 350, 420, 
426, 544, 545 
Siegel, S. 91,503 
Simar,L. 10,373,512,534 
Simon, J. C. 10, 544 
Simpson, E. H. 42, 544 
Singpurwalla, N. D. 10, 104, 373, 374, 
513,531,533,544,545 
Siow, A. 417, 554 
584 
Author Index 
Sivaganesan, S. 371,545 
Skene, A. M. 347, 420,426, 545 
Skilling, J. 10, 545 
Slate E. H. 348,526 
Slovick, P. 96, 375, 525 
Smith, A. F. M. vii, 10, 11, 18, 103, 
141, 160, 183, 230, 247, 263, 295, 
345, 346, 347, 348, 350, 352, 353, 
354, 355, 356, 368, 369, 370, 373, 
374, 378, 417, 418, 422, 424, 425, 
426, 488, 489, 496, 497, 498, 504, 
506, 512, 513, 515, 518, 519, 522, 
531, 533, 535, 537, 540, 541, 545, 
546,547,549,551 
Smith, C. A. B. 99, 263, 394, 546 
Smith, C. R. 10, 510, 546 
Smith, J. Q. 10,104,374,536,546, 554 
Smokier, H. E. 10, 87,102, 528 
Smouse, E. P., 374, 546 
Solomonoff, R. J. 485, 546 
Soubiran, C, 374, 541 
Soyer, R., 374, 544 
Spall, J. C. 10, 258, 362, 546 
SpezzaferriF. 417,543 
Spiegelhalter, D. J. 95, 103, 353, 355, 
417, 422, 426, 488, 516, 528, 545, 
546, 549 
Spizzichino, F 104, 236, 492, 527 
Sprott, D. A. 10, 455, 481, 492, 517, 
525 
Srinivasan, C. 258, 495 
Stael von Holstein, C.-A. S. 10,375,546 
Stangl, D. K. 10, 497 
Steel, M.F.J. 371,374,426,530,536, 
547 
Steffey, D. 373, 526 
Stegun, I. A. 156, 489 
Stein, C. 333, 361, 365, 367, 446, 449, 
475, 523, 547 
Stelzer, J. 90, 508 
Stephens, D. A. 352, 547 
Stewart, L. 350, 547 
Stigler, S. M. 2, 356, 357, 547 
Stigum, B. P. 84, 547 
Stone, M. 103,160,162, 362, 363, 365, 
367, 405, 488, 504, 547, 548 
Stoney, D. A. 10, 489 
Strawderman, W. E. 465, 541 
Stroud, A. H. 348, 548 
Studden, W.J. 160,502,510 
Sudderth, W. D. 88, 90, 162, 172, 485, 
521, 528, 548 
Sugden, R. A. 160, 548 
Sunahara, Y. 10, 374, 543 
Suppes, P. 11, 84,90,91,104,503,527, 
532, 548 
SusarlaV. 228,548 
Sweeting, T. J. 231,289,548 
Taneja, I.J., 160,536 
Tanner, M. A. 353,541,549 
Taylor, S. J. 106, 527 
Teicher, H. 178, 501 
Teichroew, D. 350, 549 
Teller, A. H., 356, 533 
Teller, E., 356, 533 
Thatcher, A. R. 376, 549 
Thomas, A. 426, 549 
Thomas, H. 87,91,104,523 
Thomas, L. C. 10,513 
Thorburn, D. 228, 549 
Tiao, G. C. 9,230,258,260,360,361, 
367, 370, 419, 420, 498, 499, 537, 
539, 549 
Tibshirani, R. J. 359, 371, 509, 549 
Tierney, L. 341,344,353,356,426,526, 
549 
Author Index 
585 
Titterington, D. M. 374, 549 
Tiwari,R. C. 11,510 
Torgesen, E. N. 362, 549 
Trader, R. L. 11,549 
Tribus, M. 9,10, 530, 549 
Tritchler, D. 104, 535 
Tsao,H.J. 373,516 
Tsay, R. S. 374, 533 
Tsui, K.-W. 345, 530 
Tsutakawa, R. K. 348, 512 
Turing, A. 390 
Turkkan.N., 160,538 
Tversky, A. 10, 90, 96, 104, 375, 509, 
525,527,531 
Urbach, P. 10, 522 
Vaidyanathan, S. 417, 526 
van der Linde, A. viii 
van der Merwe, A. J. 373, 549 
van der Merwe, C. A. 373, 549 
van Dijk, H. K. 349, 527, 549, 550 
van Ryzin, J. 228, 548 
vanZyl,D. 104,506 
Vardeman, S. 366, 533 
Venn, J. 100, 550 
Verbraak, H. L. F. 10, 550 
Verdinelli, I. 160, 418, 546, 550 
Veronese, P. 279, 422, 501 
Viertl, R. 10, 550 
Villegas, C. viii, 362, 460, 500, 550 
vonMises, R. 100,101,550 
von Neumann, J. 84, 90, 91, 446, 550 
Vovk, V G. 485, 550 
Vyugin V. V 485, 550 
Wahba, G. 228, 374, 526, 550 
Wakefield, J. C, 355,551 
Wald, A. 375, 446, 448, 450, 551 
Walder, A. N. 374, 532 
Walker, A.M. 289,551 
Wallace, C. S. 485, 551 
Wallace, D. L. 9,534 
Wallace, M. A. 104,527 
Waller, R. A. 10, 532 
Walley, P. 99,102, 371, 537, 551 
Wallsten, T. S. 96,551 
Walsh, K. 91, 548 
Warren-Hicks, W J. 373, 374, 553 
Wasserman, L. 99, 130, 367, 371, 418, 
501,528,550,551 
Wechsler, S. 104, 236, 492, 551 
Wedderburn, R. W M. 488, 535 
Wedlin, A. 9, 502 
Weerahandi, S. 104, 483, 551, 554 
Wei, L. J. 228, 536 
Weiss, R. E., 418, 551 
Welch, B.L. 359,479,551,552 
West, M. viii, 10, 104, 230, 366, 370, 
374, 375, 420, 483, 520, 528, 538, 
539, 552 
Wetherill, G. B. 374, 375, 552 
White, D. J. 10, 104, 513, 552 
Whittle, P. 162,228,552 
Wichmann, D. 9,552 
Wiener, N. 91, 552 
Wild, P. 355,516 
Wilkinson, G. N. 458, 553 
Wilks, S. S. 133,553 
Willing, R. 481,553 
Wilson, J. 104, 553 
Wilson, R. B. 104, 553 
Wilson, S. P. 373, 545 
Winkler, R. L. 9, 104, 375, 397, 420, 
501,509,553 
Winsten, C. B. 250, 455, 492 
586 
Author Index 
Witmer, J. A. 376, 553 
Wolfowitz, J. 480, 526 
Wolpert, R. L. 10, 250, 350, 371, 373, 
374, 495,528, 553 
Wong, C.-M. 374, 491 
Wong, W H. 341, 353, 549, 553 
Wooff, D. A. 426, 553 
Wright, D. E. 262, 348, 506, 553 
Wright, G. 375, 519 
Wrinch, D. H. 90, 322, 553 
Wu, C.-F. 10,498 
Yaglom, A. M. 10, 553 
Yaglom, I. M. 10, 553 
Ye, K. 338, 367, 553, 554 
Yilmaz, M. R. 96, 554 
Ylvisaker, D. 188, 273, 276, 279, 283, 
507 
Young, K. S. 417, 538 
Young, S. C. 104, 554 
Youtz,C. 81,534 
Yu, P. L. 104, 554 
Zabell, S. L. 94, 507 
Zanotti, M. 90, 548 
Zeckhauser, R. 104, 523 
Zeleny, M. 104, 501 
Zellner, A. 10,11,43,90,263,361,362, 
417,483,511,514,517,554 
Zidek, J. V. 11,104, 362,363,374,483, 
499,504,506,515,551,554 
Zionts, S. 104, 517 
