lysis 
ssI 
A SECOND COURSE IN STATISTICS 
FREDERICK MOSTELLER 
Harvard University 
JOHN W. TUKEY 
Princeton University and Bell Telephone Laboratories 
& 
ADDISON-WESLEY PUBLISHING COMPANY 
Reading, Massachusett. .. Mn!c Ptrlc, C, dtfornia 
London � Amsterdam' Don Mills, Ontario' Sydney 
This book is in the 
ADDISON-WESLEY SERIES IN 
BEHAVIORAL SCIENCE: QUANTITATIVE METHODS 
Consulting editor: 
Frederick Mosteller 
The writing of this work, together with some of the research reported herein, was supported in part 
by the United States Government. 
Copyright � 1977 by Addison-Wesley Puolisiin; Company, Inc. Philippines copyright 1977 by 
Addison-Wesley Publishing Company, In.;. 
All rights reserved. No part of this publication 'may be reproduced, stored in a retrieval system, or 
transmitted, in any form oL' by any nean,, electrcn.:c, m.. echanical, photocopying, recording, or 
otherwise, without the !:tic'r wr'.tten rerrn'.ssicn of 'tEe publisher. Printed in the United States of 
America. Published simultaneously in Canada. Libtary bf"Congress Catalog Card No. 76-15465. 
ISBN 0-201-048�4-X 
BCDEFGH I J-HA- 7987 
To Virginia and Elizabeth 
Preface 
Two mainstreams intermingle in this treatment of practical statistics: (a)a 
sequence of philosophical attitudes the student needs for effective data 
analysis, and (b) a flow of useful and adaptable techniques that make it possible 
to put these attitudes to work. 
After a first course in statistics, the student needs further preparation for 
tackling the more difficult problems that commonly arise in data analysis, even 
some of those that appear simple. Many first courses concentrate on introduc- 
ing the student to notions of coni. rm. atory data analysis (primarily tests of 
significance), to some probability dstnbutions, and perhaps also to regression 
and analysis of variance. In a first course, the instructor rarely has time to 
discuss the philosophy of data analysis or of exploratory approaches, let alone 
time to bring out the vital ifs, ands, and buts of multiple regression. Much can 
be said about applying statistical methods to get effective and reliable answers 
from real data, and we take these matters up in some detail. 
We assume that the student has had a first course in statistics. (Having 
other courses can help, but nothing beyond a first course is assumed.) Depend- 
ing on the kind and length of preparation, a student will move faster or slower 
through this book's chapters, but most o.f the {naterial is likely to be new, 
whatever the preparation. Our emphasis s not mathematical, although some 
techniques are inevitably heavy with formulas. In most spots the mathematical 
details are handled by way of examples. 
Calculus is not often used. In the two or three spots where it arises, the 
reader can skip details and accept results without losing much. 
What a Reader can Get from this Book 
More important than the techniques that this book can teach are attitudes and 
approaches. As they work their way through, our readers can learn to identify 
at least the following attitudes, understandings, and approaches, which they 
will want to return to and ponder after reading parts of the book: 
o An approach to the formulation of statistical and data-analytical prob- 
lems such that, for example, (a) Student's shortcut to inference can be 
properly understood, and (b) the role of vague concepts becomes clear. 
vii 
viii Preface 
o The role of indications (of pointers to behavior not necessarily on 
prechosen scalesin contrast to conclusions or decisions about prechosen 
quantities or alternatives). 
o The importance of displays and the value of graphs in forcing the 
unexpected upon the reader. 
o The importance of re-expression (as well as how to do it). 
o The need to seek out the real uncertainty as a nontrivial task. 
o The importance of iterated calculation, where we go round and round 
till the answer settles down (a once-through calculation is fine, when one 
can be found to meet our needs). 
o How the ideas of robustness (specifically, robustness of efficiency) 
and resistance can change both what we do and what we think. 
o What regression is all about. 
o What regression coefficients can and can't do for us. 
o That the behavior of our data can often be used to guide its analysis 
(through both guided regression and re-expression, to name but two 
routes). 
o The importance of looking- at and drawing information from residuals. 
o The idea that data analysis, like calculations, can profit from repeated 
starts and fresh approaches; there is not just one analysis for a substantial 
problem. 
The reader who has not yet begun this book is likely to be unclear just what 
some of these items mean; the reader who has worked through the sixteen 
chapters ought to know what they mean, and will have developed views about 
their relative importance. 
Let us turn to the chapters themselves. In many cases, they need not be 
read in consecutive order, as the following description makes clear. The first 
two chapters offer some practical philosophy for data analysis and should lead 
off, almost certainly followed by Chapters 3 and 4 (unless the student has had 
preparation in exploratory data analysis, as most have not in 1977). (When we 
refer to the book Exploratory Data Analysis for fuller treatments, the refer- 
ences point to the 1977 first edition (Addison-Wesley Publishing Company, 
1977), and not to the limited preliminary edition). Chapter 4 provides a proper 
background for simple linear regression. 
Opinions seem to differ as to the nature and importance of re-expression. 
We put it where it is (Chapters 5 and 6) because to us it is both elementary and 
valuable. But it may be postponed, even placed well down among the regres- 
sion chapters. We have postponed most details of Chapter 6 to an appendix 
following Chapter 16. 
Preface ix 
Chapters 7 and 8 are extremely general in their philosophy and are impres- 
sive in the variety and difficulty of the problems their techniques tackle. The 
ideas (a) that finding the true uncertainty may not be easy and (b) that it can still 
be done, even in messy situations where classical mathematical attacks seem 
almost impossible, can illuminate all we may go on to do in the analysis of 
data and ought to have a chance to do this. The student who has mastered the 
techniques of Chapter 8 has made profound progress in confirmatory data 
analysis as well as in cross-validation. The techniques are as all-purpose for 
confirmatory data analysis as regression and analysis of variance are for their 
jobs. Chapter 8 illustrates their uses in multiple regression. If these chapters, 
which are not explicitly used in later chapters, are omitted, the reader will 
suffer a grave loss. 
Chapters 9 through 11 form a special bundle of three distinct techniques 
that any statistician or data analyst needs, though they are not widely available: 
the direct and flexible approach to two-way tables (Chapter 9); an up-to-date 
look at resistant/robust techniques in the simpler applications (Chapter 10); and 
a reasonably extensive account of standardization (Chapter 11). Many students 
pass through several statistics courses without learning about standardization, 
and then find themselves at a loss when they meet practical problems. We feel 
that the material of Chapter 11 (and Chapters 9 and 10) is an essential part of a 
statistician's or data analyst's education. Again, these chapters need not be 
read before the regression chapters, except that at least part of Chapter 10 is 
needed before or in conjunction withthe latter part of Chapter 14. 
Some subjects are naturally so interrelated and interlinked as to make a 
simple straight-through account at least difficult. We fear that today the bundle 
of issues and topics labeled "regression" provides an instance. We have tried 
to make things as orderly as we can. Chapter 12 covers the ABC's of regres- 
sion. Much of this material is often passed by without notice. We think it is 
important, deserving to come first as regression is'studied. Next comes Chapter 
13 on the woes of regression coefficients. Chapter 14 explains how the machin- 
ery underlying the usual fitting of regression works and how it can be used to 
make robust and resistant fits as well as the weak fits of usual least squares. 
Chapter 14 offers a mathematical approach to understanding regression 
that many studentg have found instructive and helpful. Not all will want to 
study it in full detail. Detailed study requires an ability to follow the summation 
notation closely and an appreciation that certain operations can be carried out, 
but not necessarily by the reader. The reader who is able to take statements of 
fact on faith can get much from this chapter without following all the formal 
details. The two preceding chapters give essential background, especially 
Chapter 12. 
The idea that there is only one regression that we could consider fitting to a 
given set of data is often false.' Chapter 15, which makes strong use of Chapters 
12 and 13, suggests ways to deal with a recognition that it is false for our 
specific data. Chapter 16 tells us something of how to hunt out additional fits to 
try next. 
x Preface 
For those who want a fast dash to the regression chapters, the route is 
Chaptei' 4, Chapter 10, and then on to the Chapter 12 through 16 sequence, 
backtracking sometime to Chapter 5, and to Chapters 1 and 2. 
Special Features 
Although some special features have already been alluded to, a more complete 
listing is useful. Among others, we present (in the order of their appearance): 
o An introduction to stem-and-leaf displays. 
o Use of running medians for smoothing. 
o The ladder of re-expressions for straightening curves. 
o Methods of re-expression for analysis, and a way to assess their 
worthwhileness in specific instances. 
o Special tables to make re-expression easy in hand calculations. 
o The method of direct assessment. 
o The jackknife, extensively illustrated. 
o Robust analyses of two-way tables. 
o Robust and resistant measures of location and scale. 
o Direct and indirect methods of standardization, as indication or as 
confirmatory data analysis. 
o Techniques of standardization that include allowance for broad 
categories. 
o A residual-emphasizing approach to regression. 
o Problems of collinearity in regression. 
o Regression with errors of measurement. 
o The interpretation of regression coefficients. 
o Effects of proxy variables. 
o Ordinary least squares. 
o Weighted least squares of several kinds. 
o Least absolute deviations. 
o Choosing among regression models, especially in stepwise fitting. 
o Choosing new variables and appraising old ones in multiple regres- 
sion. 
Preface xi 
Problems to be Done 
This book contains many homework problems and projects, all grouped at the 
end of the book and organized by section. 
Many of these problems, especially for the chapters on regression (and for 
Chapter 10), will take more time than one might like. Teachers should be 
sparing in their assignments; students should be brave in their efforts. 
Everyone should recognize that, as is true in practical data analysis, problems 
may often not have neat answers or a single correct solution. 
If you can be sure of what is happening, having the heavy calculations 
done on a computer can save you much effort. But be sure that the computer is 
doing exactly what you suppose. Not all of the many "packages" of statistical 
computer programs can be trusted to do exactly what one might hope they will 
do. More treacherous by far are isolated programs, often written by those who 
understand the computer and its programming far better than they understand 
the traps, classical or newly discovered, of statistical algorithms. Take extra 
care when using them. 
How this Book Came to be 
About 1963, Gardner Lindzey and Eliot Aronson, who were preparing to edit a 
new edition of the Handbook of Social Psychology (1968), invited the present 
authors to write a section to replace the Mosteller and Bush article in the first 
edition. Expansive ideas and editorial planning produced (a) "Data Analysis 
Including Statistics" as Chapter 10 (pages 80 to 203 of volume 2) of that revised 
handbook and (b) an understanding that we were free to use material from that 
article as part of a free-standing book. A variety of topics were then worked on, 
many reaching fairly polished form, but only when the goals reflected in the 
present volume were recognized and seized upon did the present book begin to 
coalesce. Chapters 1, 2, 7, and 8 herein cling closely to the Handbook article, 
while the others reflect some or all of the following insights. 
1. Students need a book that combines exploratory and confirmatory ap- 
proaches, and that is aimed, more or less directly, at analyzing data with a 
consciousness of the sorts of things that are being done in practice. 
2. Students need a book that tells the truth about regression, as to aims, as to 
techniques, as to what can be done and what can't. 
3. Students need materials that emphasize many of the important issues or 
techniques that so frequently have fallen between the cracks in statistical 
instruction. 
While we recognize that the continuing evolution of knowledge and insight 
makes it impossible to completely meet these three goals, we hope the reader 
will be pleased to discover how far it has been possible to go. 
xi Preface 
Acknowledgments 
We take pleasure in expressing our appreciation to the many people and 
organizations who have contributed to this venture, including those acknowl- 
edged in the Handbook of Social Psychology article (Mosteller and Tukey, 
1968). Among colleagues and students who have read drafts of chapters and 
helped with comments, special thanks go to David Hoaglin and Colin Mallows. 
The whole book and its examples and tables have been checked over by Anita 
Parunak. Earlier, Cleo Youtz and Gale Mosteller reviewed many examples and 
tables. We had considerable aid in identifying and drafting the problems from 
Miriam Gasko-Green, Keith Soper, Michael Stoto, and George Wong. Steven 
Fosburg programmed most of the extensive calculations of Chapter 8. We have 
had typing support from Holly Grano, Marjorie Olson, and Mary E. Bittrich 
(Chapter 10). 
Preparation of this work has been facilitated by National Science Founda- 
tion grants SOC72-05257 and SOC75-15702 (Harvard), Army Research Office 
(Durham) contracts and grants (Princeton) DA-31-124-ARO(D)-215, No. POll 
DAHC-04-73-C-0031, DAHC-04-74-G-0178, DAHC-04-75-G-0188, DAAG-29- 
76-G-0298, Energy Research and Development Administration grant E(11-1)- 
2310 (formerly AEC AT(11-1)-2310 (Princeton), and by Bell Telephone 
Laboratories, Inc. A substantial amount of the work was done during 1974-75 
at the University of California at Berkeley while Mosteller served as Miller 
Research Professor, a post supported by the Miller Institute for Basic Research 
in Science. 
All the exhibits from Exploratory Data Analysis are reprinted by permis- 
sion of the Addison-Wesley Publishing Company. 
We much appreciate the generosity of the many scholars and publishers 
who have given us permission to use their data as examples. Data from genuine 
investigations bring freshness and uncontrived challenge to the student of data 
analysis, and so the realism of the problem material is itself a contribution to 
our profession. 
Cambridge, Massachusetts F. M. 
Princeton, New Jersey J. W. T. 
February, 1977 
Contents 
I Approaching Data Analysis 1 
Introduction 1 
IA The staircase and the shortcut to inference 2 
lB Student's true contribution 4 
1C Distributions and their troubles 7 
1D A classical example: Wilson and Hilferty's analysis of 
Peirce's data 10 
1E Kinds of nonnormality and robustness 12 
1F The role of vague concepts 17 
l G Other vague concepts 19 
1H Indication, determination, or inference 21 
Summary: Data analysis 22 
2 Indication and Indicators 25 
2A The value of indication 25 
2B Examples of stopping with indication 27 
2C Concealed inference 31 
2D Choice of indicators 32 
2E An example of choice of indicator 34 
2F Indications of quality' Cross-validation 36 
Summary' Indication and indicators 40 
3 Displays and Summaries for Batches 43 
3A Stem-and-leaf 43 
3B Medians, hinges, etc. 43 
3C Mids and spreads 48 
3D Subsampling 49 
3E Exploratory plotting 49 
3F Trends and running medians 52 
3G Smoothing nonlinear regressions 61 
3H Looking for patterns 64 
3I Residuals more generally 70 
3J Plotting and smoothing 75 
Summary: Batch summaries and displays 77 
Xlll 
xiv Contents 
4 Straightening Curves and Plots 79 
The idea of straightening out curves 79 
4A The ladder of re-expressions 79 
4B Re-expressing y = x2 81 
4C The bulging rule 84 
4D More complicated curves 87 
4E Scatter plots 87 
Summary: Straightening curves 87 
The Practice of Re-Expression 89 
5A Kinds of numbers 89 
5B Quick logs 92 
5C Quick (square) roots and reciprocals 97 
5D Quick re-expressions of counted fractions, percentages, etc. 100 
5E Matching for powers and logs 102 
5F Re-expressions for grades 103 
5G Re-expressing ranks 108 
5H First aid in re-expression 109 
5I What to do with zeros and infinities 112 
Summary: Re-expression 114 
6 Need We Re-Express? 117 
General hints when re-expressed carrier is log x 118 
7 Hunting out the Real Uncertainty 119 
7A How cr// can mislead 121 
7B A further example of the need for direct assessment of variability 122 
7C Choosing an error term 123 
7D More detailed choices of error terms 125 
7E Making direct assessment possible 126 
7F Difficulties with direct assessment 129 
7G Supplementary uncertainty and its combination with 
internal uncertainty 129 
Summary: Hunting out the real uncertainty 131 
8 A Method of Direct Assessment 133 
8A The jackknife 133 
Appendix to 8A 137 
8B Examples with individuals 139 
8C Jackknife using groups: Ratio estimation for a sample survey 145 
8D A more complex example 148 
8E Cross-validation in the example 154 
8F Two simultaneous uses of"leave out one" 156 
Contents xv 
8G Dispersion of the/z's 160 
8H Further discussion of the example 161 
Summary: The jackknife 162 
9 Two- and More-Way Tables 165 
9A PLUS analyses 165 
9B Looking at two-way PLUS analyses 169 
9C Taking advantage of levels 172 
9D Polishing additive fits 178 
9E Fitting one more constant 190 
9F Using re-expression 194 
9G Three- and more-way analyses 200 
Summary: Two-way tables of responses 201 
10 Robust and Resistant Measures 203 
10A Resistance 203 
10B Robustness 205 
10C Robust and resistant estimates of location 206 
10D Robust estimates of scale 207 
10E Robust and resistant intervals 208 
10F Resistant and robust regression 209 
10G Multiple-component data 209 
1 OH Closing comment 218 
Summary: Resistant and robust techniques 218 
11 Standardizing for Comparison 221 
l 1A The simplest case 221 
1 lB Direct standardization 225 
11C Precision of directly standardized values 225 
11D Difficulties with direct standardization 230 
11E Indirect standardizing 233 
11F Adjustment for broad categories 240 
11G More than two broad categories 249 
Summary: Standardizing for comparison 256 
12 Regression for Fitting 259 
Introduction 259 
12A The two meanings of regression 262 
12B Purposes of regression 268 
12C Graphical fitting by stages 271 
12D Collinearity 280 
12E Linear dependence, exact and approximate 284 
12F Keeping out what is imprecisely measured Regression as exclusion 287 
xvi Contents 
*12G Which straight line? (Optional) 293 
12H Using subsamples 295 
Summary: Regression 295 
13 Woes of Regression Coefficients 299 
13A Meaning of coefficients in multiple regression 299 
13B Linear adjustment as a mode of description 303 
13C Examples of linear adjustment 305 
13D The relative unimportance of the exact carrier 315 
13E Proxy phenomena 316 
13F Sometimes x's can be "held constant" 318 
13G Experiments, closed systems, and physical versus social 
sciences, with examples 320 
*13H Estimated variances are not enough 328 
Summary: Woes of regression coefficients 331 
14 A Class of Mechanisms for Fitting 333 
14A Fitting lines--some through the origin 335 
14B Matching as a way of fitting 337 
14C Matchefs tuned to a single coefficient--and catchers 339 
14D Ordinary least squares 341 
14E Tuning for ordinary least squares 342 
14F Weighted least squares 346 
1,4G Influence curves for location 351 
14H Iteratively weighted linear least squares 356 
14I Least absolute deviations (Optional) 365 
14J Analyzing troubles 369 
14K Proof of the statement of Section 13B 374 
Summary: Mechanisms for fitting regression 376 
15 Guided Regression 381 
15A How can we be guided in what to fit? 381 
15B Stepwise techniques 387 
15C All-subset techniques 391 
15D Combined techniques 393 
15E Rearranging carriers judgment components 394 
15F Principal components 397 
15G How much we are likely to learn? 401 
15H Several y's or several studies 402 
15I Regression starting where? 402 
15J Arbitrary adjustment 404 
Summary: Guided regression 404 
16 Examining Regression Residuals 407 
16A Examining 9 407 
16B Variables and other carriers 419 
Contents xvii 
16C The next step: Looking with regard to an old variable, tod 423 
16D Looking with regard to a new variable, tnew 431 
16E Looking for additional product terms 435 
16F In what order? 446 
Summary: Examining regression residuals 448 
Appendix: Details About the Need to Re-Express 449 
Problems 467 
Data Exhibits for Problems 552 
Index 579 
Index for Chapter I 
Introduction 1 
1A The staircase and the shortcut to inference 2 
lB Student's true contribution 4 
1C Distributions and their troubles 7 
1D A classical example: Wilson and Hiiferty's analysis of 
Peirce's data 10 
1E Kinds of nonnormality and robustness 12 
1F The role of vague concepts 17 
1G Other vague concepts 19 
1H Indication, determination, or inference 21 
Summary: Data analysis 22 
References 23 
Chapter 1/Approach ing 
Data Analysis 
Introduction 
Every s.tudent of the art of data analysis repeatedly needs to build upon his 
previous statistical knowledge and to reform that foundation through fresh 
insights and emphases. This account of data analysis assumes that all of its 
readers are familiar with elementary statistical concepts and techniques; a few 
starred sections and paragraphs assume that some readers already have moder- 
ately advanced knowledge of both statistics and data analysis. 
Applications of mathematics are always complicated by the obligation to 
be true to the subject matter treated as well as to the mathematics. The 
structure of the art of data analysis makes its exposition especially difficult 
because it does not branch neatly like a tree, nor does it seem susceptible to an 
orderly progressive treatment. Consequently, we repeatedly deal with new 
insights and old fundamentals. Our purpose is to develop ideas profitable to 
both practitioners and critics of the analysis of data. 
Many mathematical and philosophical discussions begin with a general 
theory, from which are derived general principles; from these, in turn, specific 
procedures are produced and finally exemplified. In discussing data analysis, we 
find the following somewhat opposite order more practical. 
a) First, what to do. (What treatment to apply to the data of a given sort of 
problem, arithmetically or graphically.) 
b) Then, why choose that treatment. (What reasons are there for choosing 
this treatment, from among so many, to meet this sort of problem? Usually 
the explanation must be made in terms of specific mathematical models.) 
c) Next, what to include in the why. (What can be said about types of models, 
and classes of reasons, that have proved useful in carrying out this choice, 
and which types and classes, by contrast, have proved misleading.) 
d) Last, how to structure our thinking about the overall process of data 
analysis. (What general theories produce the reasons of (c) as deductions 
rather than as inductions from experience.) 
Great overturns at the (d) level have had little effect on what is done in 
practice, in comparison with the effects of less imposing changes at the (b) 
level. 
In this chapter, we strike a number of the themes: the staircase of primary, 
secondary, tertiary,... statistics and Student's shortcut to inference; the dis- 
tributional properties of observations and measurements; the need for vague 
1 
2 /1: Approaching data analysis 
concepts in the evaluation of more definite concepts and criteria; the distinc- 
tion between indication, determination, and inference. Each theme recurs. 
1A. The Staircase and the Shortcut to Inference 
Before Student's time, every analysis of data that considered "what might have 
been" resembled a long staircase from the near foreground to the misty 
heights. One began by calculating a primary statistic, a number that indicated 
quite directly what the data seemed to say about the point at issue. The 
primary statistic might, for instance, have been a sample mean. Then one faced 
the question of "How much different might its value have been?" and calcu- 
lated a secondary statistic, a number that indicated quite directly how variable 
(or perhaps how stable and invariable) the primary statistic seemed to be. The 
secondary statistic might have been an estimate of the standard deviation of 
such a sample mean. After this step, one again needed to face the question of 
"How much different?", this time for the secondary statistic, which again and 
again turned out to be less stable (of itself) than the primary statistic whose 
stability it indicated. In principle, one should have gone on to a tertiary 
statistic, which indicated the variability or stability of the secondary statistic, 
then to a quaternary statistic,..., and so on up and up a staircase which, since 
the tertiary was a poorer indicator than the secondary, and the quaternary was 
even worse, could only be pictured as becoming mistier and mistJer. In 
practice, workers usually stopped with primary and secondary statistics. 
Student (1908) broke new ground by asking essentially "What if I have n 
observations randomly drawn from a normal distribution about whose average 
and variance I wish to assume nothing?" Let ] be the sample mean, /z the 
population mean, yi a measurement, n the sample size. We shall think of ] and 
yi as random variables for the moment, rather than as the numerical values 
realized in an investigation. Student's name identifies the ratio: 
sample mean - distribution mean 
t = 
/(sample estimate of distribution variance)/n 
sample mean - distribution mean 
/sample estimate of variance of numerator 
whose distribution, based on normal theory, was found to depend only upon n. 
The staircase and the shortcut to inference 3 
Student calculated some numerical aspects of the distribution of t. Then, after 
applying empirical sampling to 3000 measurements of finger lengths and 3000 
heights, which had been collected as an aid in identifying criminals, he 
succeeded in correctly guessing the mathematical form of the distribution of t. 
R. A. Fisher (1925) verified Student's guess 17 years later. 
This approach cut off the misty staircase after the third step indeed, 
almost after the second step. For, in order to tell us about the population 
mean, the data were asked to provide only: 
1. the sample mean a primary statistic, 
2. the sample estimate of variance a secondary statistic, 
3. the sample size--a tertiary statistic, one that was easy to obtain and 
remarkably stable, at least so long as one compared this sample with other 
possible samples of the same size. 
All else was provided by the assumption of exact normality. 
Note that, while the sample mean, sample variance, and sample size are 
primary, secondary, and tertiary statistics for this purpose, they may play other 
roles in other circumstances. 
Given the normality assumption, these three numbers, and any contem- 
plated value tXc for the distribution mean, we can calculate 
sample mean - contemplated value  - tXc 
/sample estimate of variance of numerator s 
The contemplated value could be any number, and could be changed many 
times for a single set of data. It might be zero,if we were studying group 
differences and thought, either seriously or as a straw man, that we might find 
none. It might be 500, if we were comparing a group of students, perhaps the 
freshman class of some college, with the standard offered by a national average 
on a test standardized as the Educational Testing Service often does (mean 
500, standard deviation 100). It might take on, in turn, all possible values, as 
when we seek a confidence interval. 
Each time we insert a contemplated value into the formula for t, we take 
the first step in making a significance test. When the contemplated value 
exactly equals the population mean from which the yi are drawn, the distribu- 
tion of t is the one given by the usual tables. When the contemplated value is 
far from the true population mean, t is likely to yield a number large in 
absolute value. These ideas, applied to this and other key statistics, made a bit 
more precise with tables of critical values, and extended by the concept of 
operating characteristics or power of tests (see, for example, Mosteller and 
Bush, 1954), provide the whole machinery of significance testing and almost all 
the machinery used in practice to set confidence intervals. 
4 Exhibit 1/1: Approaching data analysis 
B, the 1930's and through the 1940's, people were learning to short-cut 
the staircase without making such strong assumptions. They introduced "non- 
parametric" or "distribution-free" procedures, thus eliminating dependence on 
the normal distribution for making "5%" really 5%, thus providing yet another 
stage in a continuing revolution. 
lB. Student's True Contribution 
In the --century following its introduction, Student's t has been used 
extremely often in practice, as have many and diverse techniques that have 
evolved out of the same chain of development. Its impact on the usage of 
words and on the development of statistical theory have been equally striking. 
The value of Student's work was not that it led to great changes in the 
numbers obtained in the analysis of data, because in the main it did not. To see 
this, look at Exhibit 1, where we show some % points that would be used to set 
Exhibit I of Chapter 1 
Standard Confidence Points for Student's t 
f = degrees 1/40 = 1/6 = 1/2 = 5/6 = 39/40 = 24/f (for 
of freedom 2.5% 16% 50% 83�% 97.5% interpolating) 
I -12.71 -1.73 0.00 1.73 12.71 
2 -4.30 -1.26 0.00 1.26 4.30 
3 -3.18 -1.15 0.00 1.15 3.18 
4 -2.78 -1.10 0,00 1.10 2.78 
5 -2.57 -1.07 0.00 1.07 2.57 
6 -2,45 -1,05 0,00 1,05 2.45 4 
8 -2.31 -1.03 0,00 1,03 2.31 3 
12 -2.18 -1.01 0.00 1.01 2.18 2 
24 -2.06 -.99 0.00 ,99 2.06 1 
c= -1.96 -.97 0.00 .97 1.96 0 
Notes: 
1. 2.5% and 97.5% points combine to give two-sided 95% confidence limits, or a two-sided 5% 
significance test. 
2. Interpolation in the reciprocal of the degrees of freedom gives good accuracy. For example, for 48 
degrees of freedom 24/f = 0.5. Consequently the corresponding 97.5% point is halfway between 2.06 
and 1.96 at 2.01. 
3. When we expect to use a t in a symmetrical two-sided way, it is often convenient to think of the 
distribution of Itl, the absolute value of t. We write Itl.g5 for the two-sided 95% point of Itl,which is 
given above in the column headed 39/40. We write It I=/a for the two-sided 2/3 point, which is given 
above in the column headed 5/6. We use similar notations for any two-sided % points of t. 
lB: Student's true contribution 5 
two-sided 95% = 19/20 = 38/40 confidence limits (and also those needed for 
two-sided 2/3 = 66% confidence limits, a less usual level selected for reasons 
we shall explain in Section 1C. One measure of the effect of Student's t is the 
ratio of the length of confidence interval obtained by using it to the length 
obtained when the standard normal distribution, which is equivalent to t for 
infinite degrees of freedom, is used. Long before Student came along, many 
people calculated the ratio now known as Student's t (except for a slight 
change in the definition of the sample standard deviation as discussed below), 
namely, 
sample mean - contemplated value 
sample standard deviation/f-n 
Not having Student's table, all they then knew how to do was to refer this ratio 
to a table of the standard normal distribution and to use varying amounts of 
verbal caution in interpreting the result. 
How great was the change? We find from Exhibit I that, for example, if 
95% confidence limits are set using the standard normal distribution, the 
multiplier of the standard error s is 1.96, while, using a t distribution with 12 
degrees of freedom, the multiplier is 2.18. Since 2.18/1.96  1.11, the use of 
the t distribution adds only 11% to the length of the 95% confidence interval 
when we have 12 degrees of freedom. (We use the symbol "" to stand for "is 
approximately equal to" or "approximates" or similar phrases implying ap- 
proximation ra. ther than strict equality.) If a slightly different definition of sy 
were used, again with the standard normal distribution, namely,J(y - y)2/n2, 
the ratio would rise to 1.15.) For levels of confidence less extreme, the effect is 
less marked. For example, for two-sided 2/3 confidence, we can go as low as 5 
degrees of freedom before the ratio of lengths of confidence intervals based on 
t to that based on the normal is as much as 1.! (,1.07/0.97). 
We are not trying to sweep under the rug the gigantic ratio of 6.5 - 
12.7/1.96 associated with 95% confidence and 1 degree of freedom. But most 
investigators use more degrees of freedom; indeed, an important impact of the 
t table has been to encourage investigators to get more degrees of freedom so 
as to avoid such terrible ratios as the 6.5, 2.2, and 1.6 offered at 95% by 1, 2, 
and 3 degrees of freedom, respectively. Note that, for 2/3 confidence, the 
corresponding ratios are only 1.8, 1.3, and 1.2. 
The value of Student's work lay not in a great numerical change, but in: 
orecognition that one could, if appropriate assumptions held, make allow- 
ance for the "uncertainties" of small samples, not only in Student's original 
problem but in others as well; 
oprovision of a numerical assessment of how small the necessary numerical 
adjustments of confidence points were in Student's problem and, as we have 
just seen, how they depended on the extremeness of the probabilities 
involved; 
6 Exhibit 2/1- Approaching data analysis 
o prsentation of tables that could be used--in setting confidence limits, in 
making significance tests to assess the uncertainty associated with even 
very small samples. 
Besides its values, Student's contribution had its drawbacks, notably: 
o it made it too easy to neglect the proviso "if appropriate assumptions 
held"; 
o it overemphasized the "exactness" of Student's solution for his idealized 
problem; 
o it helped to divert the attention of theoretical statisticians to the develop- 
ment of "exact" ways of treating other problems; and 
oit failed to attack "problems of multiplicity": the difficulties and tempta- 
tions associated with the application of large numbers of tests to the same 
data. 
The great importance given to exactness of treatment is even more 
surprising when we consider how much of the small differences between the 
critical values of the normal approximation and Student's t disappears (see 
Exhibit 2), especially at and near the much-used two-sided 5% point, when, as 
Exhibit 2 of Chapter 1 
Standard Confidence Points for Burrau's Modification of Student's t 
/f- 2 - / where f n - 1 
--.-- ..._. 
f = degrees 1/40 - 1/6 = 1/2 - 5/6 -- 39/40 = 24/f (for 
of freedom 2,5% 16J% 50% 83�% 97.5% interpolating) 
3 -1.84 -0.66 0.00 0.66 1.84 
4 -1.96 -0.78 0.00 0.78 1.96 
5 -1.99 -0.83 0.00 0.83 1.99 
6 -2.00 -0.86 0.00 0.86 2.00 4 
8 -2.00 -0.89 0.00 0.89 2.00 3 
12 - 1.99 -0.92 0.00 0.92 1.99 2 
24 -1.98 -0.95 0.00 0.95 1.98 1 
0o - 1.96 -0.97 0.00 0.97 1.96 0 
Notes: 
1. The variance of t is infinite for 2 or fewer degrees of freedom, so that Burrau's formula is inapplicable 
for f= I and 2. 
2. 2.5% and 97.5% points combine to give two-sided 95% confidence limits, or a two-sided 
5% significance test. 
(lB) 1C: Distributions and their troubles 7 
suggested by Burrau (1943), we multiply t by the constant required to bring its 
variance to 1. (Regardless of its practical pros and cons, Burrau's modification 
frees our insight from the possibly misleading effects of the quite nonconstant 
variance of t.) 
The time has long since come to pay attention to the advantages of 
Student's work and to recognize its drawbacks for what they are, skimping 
neither in comparison to the other. 
1C. Distributions and Their Troubles 
Student himself always remembered that observations and measurements were 
never distributed in magic bell-shaped curves, even when they were chemical 
determinations of commercial importance made under his own supervision 
(Student, 1927). The history of statistics and data analysis is a messy mixture of 
healthy skepticism and naive optimism about the exact shapes of the distribu- 
tions of observations. Such optimism has often been inflated by the wonderful 
properties of a single family of distributions, the "normal" distributions, whose 
probability densities are given by 
f(x) - 1 
- e for -oo < x < oo 
/2z-o- ' 
where/ and r are the population mean and standard deviation, respectively, e 
is the base of the natural logarithms 2.7182818 ..., and r is our old friend 
3.1415926 .... Exhibit 3 portrays three instances of normal distributions with 
differing combinations of p. and r. 
We use the adjectives "normal" and "Gaussian" interchangeably for 
distributions that fit this formula exactly. Neither.'term is wholly satisfactory. 
Some misinterpret the word "normal" to mean "the ordinarily occurring"-- 
but, so far as we know, distributions that exactly fit this formula never occur in 
practice not for individual observations, not for sample means, not for other 
derived quantities--though we have both theory and e. vidence. that many 
empirical distributions do approximate this shape, sometimes qmte usefully, 
but sometimes only apparently rather than meaningfully. (The characteristics 
that matter most are often those that are concealed, not revealed, by conven- 
tional histograms.) 
The connection of the distributions of derived statistics, such as sample 
means and Student's t, to the underlying distribution of individual values is 
often subtle, as we shall illustrate shortly. Since distributions of the derived 
statistics usually determine the adequacy of our statements of uncertainty, 
significance, and confidence, the particular aspects of approximation that 
matter in practice differ from situation to situation and are often not easily 
checked by examining a single sample, or even the whole body of data before 
us, which may consist of hundreds of observations. 
8 Exhibit 3/1: Approaching data analysis 
We say of the three distributions in Exhibit 3 that they all have the same 
"shape". Let us discuss the concept of shape. 
Suppose that we have many observations, so that the histogram represent- 
ing actual counts closely approximates the underlying distribution. Suppose 
that this histogram has been plotted on graph paper, but that someone has 
forgotten to put numbers along the axes. What can we learn from what we 
have? What have we lost? Without numbers on the vertical axis, we cannot tell 
how large the sample was. Since our interest centers on the distribution, not 
the sample, and the sample was large, we may be able to forget this. Without 
numbers on the horizontal axis, we cannot tell near what value the observa- 
tions fell, or how widespread or concentrated they were; we can tell nothing 
about location or about scale, to use technical terms. These are important 
things to have lost. What remains? 
By giving up location and scale, by classing together all distributions that 
differ only by a linear transformation, we have given up only 2 numbers; 
accordingly much remains: all, indeed, that is usually meant by the word shape. 
Exhibit 3 of Chapter 1 
A collection of three normal or Gaussian probability density functions, with 
differing means and differing standard deviations: I =-2,  = 1; 12 = 0, 
(r2 = 0.5; Ia = 4, a = 2. 
,4 
.z 
1C: Distributions and their troubles/Exhibit 4 9 
Exhibit 4 of Chapter I 
A collection of beta densities 
(I,4) 
10 /1: Approaching data analysis 
Distributions, even distributions belonging to the same mathematical family, 
can differ widely in shape. The family of beta functions, for instance, though 
still a very special case, includes distributions of many shapes, as Exhibit 4 
illustrates. It gives a collection of beta densities of the form 
[3(p) = cp'-(1 - p)'- (0 -<_ p <- 1), 
where c is the constant making the total area 1, and where the parameters 
(a, b) are indicated in the figure. 
Given the histogram, we can locate the central 2/3 of all cases and find the 
number of spaces along the horizontal axis required to cover them. We can do 
the same for the central 19/20, and compare the two lengths. If this ratio is 
almost exactly 2 to 1, we have confirmed one characteristic of a normal 
distribution. If it is appreciably larger than 2 to 1, we have evidence that the 
two-sided 5% points of our distribution are more widespread than we would 
have expected on the basis of how the central part of the distribution is spread 
out and how normal distributions behave. If the sample is large enough, this is 
already clear evidence that the population is not normal. (Other indications can 
be more revealing in small or moderate-sized samples, as we illustrate in 
Chapter 2.) 
We can look at such ratios farther and farther into the tails. If, when we do 
this, either for the whole population or for sufficiently large samples, these 
ratios eventually tend to be too large for normal distributions, we know that we 
are dealing with tails that are more straggling or more stretched out than those 
of a normal distribution. This statement has nothing to do with the overall 
spread of a distribution. Rather it concerns the spread of the extreme tails in 
comparison with the spread of the central body of the distribution. 
A glance. at Exhibit 1 shows that the distributions of Student's tap. pear 
more straggling than the Gaussian, except, of course, for the case wth oo 
degrees of freedom, which has exactly the Gaussian shape. The ratio just 
mentioned (length of middle 19/20 to length of middle 2/3), which is about 
2.02 for the Gaussian, is 2.16 for 12 degrees of freedom, 2.4 for 5, 3.4 for 2, 
and a whopping 7.3 for 1 degree of freedom. If you know that 2/3 of a 
Gaussian distribution lies in a certain interval, and you consider an interval 7 
times as long but with the same center, you know that all but an extremely tiny 
part (1 in a hundred billion) of that Gaussian distribution lies in the larger 
interval. If you know that 2/3 of a "Student's t with one degree of freedom" 
distribution lies in a certain interval, and you consider an interval 7 times as 
long but with the same center, we have just seen that more than 5% -- 1/20 of 
the distribution can fall outside the longer interval. When tails straggle a lot, 
the center of a distribution can fool the eye about the behavior of the tails. 
Real distributions often straggle a lot compared to a normal distribution. 
1D. A Classical Example: Wilson and Hilferty's Analysis of Peirce's Data 
An example investigating unusually extensive data for normality of shape of 
distribution may be illuminating. 
1D: A classical example/Exhibit 5 11 
In an empirical study intended to test the appropriateness of the normal 
distribution, C. S. Peirce (1873) analyzed the times elapsed between a sharp 
tone stimulus and the response by an observer, who made about 500 responses 
each day for 24 days. Though Peirce seems to have concluded that the normal 
shape of this distribution was on the whole verified, when Wilson and Hilferty 
(1929) reanalyzed Peirce's published material sixty years later, they came to 
very different conclusions. 
They calculated many different statistics from Peirce's data, treating each 
day's measurements separately. We select a few of the 23 statistics Wilson and 
Exhibit 5 of Chapter 1 
Daily Statistics from Wilson and Hilferty's Analysis of C. S. Peirce's Data 
(3) 
(1) (2) No. within (5) (6) 
, q- s 03- O 0.25s of, (4) Kurtosis Errors > 3.1s 
Day (milliseconds) 2(0.6745s) Obs. Exp. Skewness /2- 3 Neg. Pos. Total 
1 475.6 + 4.2 0.932 110 98 1.18 3.1 1 3 4 
2 241.5 � 2.1 0.842 113 97 0.43 0.9 1 0 1 
3 203.1 � 2.0 0.905 113 97 1.09 3.6 0 7 7 
4 205.6 q- 1.8 0.730 134 99 1.82 9.7 1 7 8 
5 148.5 q- 1.6 0.912 110 97 0.39 1.3 0 4 4 
6 175.6 q- 1.8 0.7a. a. 119 97 1.48 6.4 0 6 6 
7 186.9 � 2.2 0.753 132 98 2.96 24.9 0 6 6 
8 194.1 q- 1.4 0.840 12.0 97 0.48 4.1 2 6 8 
9 195.8 � 1.6 0.756 132 98 1.71 13.8 2 4 6 
10 215.5 q- 1.3 0.850 120 99 0.84 8.8 2 1 3 
11 216.6 q- 1.7 0.782 135 99 1.69 9.8 1 5 6 
12 235.6 q- 1.7 0.759 103 78 0.63 4.7 3 5 8 
13 244.5 � 1.2 0.922 101 97 -0.22 2.6 6 I 7 
14 236.7 � 1.8 0.529 192 99 5.74 63.6 2 3 5 
15 236.0 � 1.4 0.662 162 98 1.68 27.9 4. 4 8 
16 233.2 � 1.7 0.612 162 98 6.39 90.6 4 2 6 
17 265.5 � 1.7 0.792 123 100 0.21 4.3 3 5 8 
18 253.0 � 1.1 0.959 114 98 0.27 1.8 0 4 4 
19 258.7 � 1.8 0.502 187 99 10.94 143.9 1 3 4 
20 255.4 � 2.0 0.521 179 98 7.71 91.4 0 3 3 
21 245.0 q- 1.2 0.790 120 99 0.23 8.2 3 4 7 
22 255.6 q- 1.4 0.688 142 99 5.27 68.1 2 4 6 
23 251.4 � 1.6 0.610 158 98 2.73 31.1 0 3 3 
24 243.4 � 1.1 0.730 113 98 -0.02 5.4 3 3 6 
Averages 1.7 3.9 5.6 
12 /1: Approaching data analysis 
Hilferty reported and display them in Exhibit 5. Column (2) gives an estimate 
of the distance from the 25% (O) to the 75% point (03) of that day's 
distribution (based on the corresponding % points of the distribution of that 
day's sample), divided by a suitable multiple of the s computed from the very 
same observations. For a normal distribution, these numbers should vary 
around 1.00, roughly symmetrically. In Peirce's data, however, all 24 numbers 
are less than 1.00, many substantially. Column (6) shows that too many 
measurements deviate from their mean by more than 3.1 sample standard 
deviations (3.1s) in each direction. For a normal distribution, 1 observation in 
500 on the average would be more than 3.1s from g, half in either direction, 
whereas Peirce's data average 5.6 per 500. Similarly, measures of skewness and 
kurtosis are consistently positive instead of varying around zero, as they should 
for a normal distribution. Furthermore, according to column (3), too many 
measurements are near the mean for normally distributed data, again, by 
comparison with s. Thus Peirce's data clearly do not follow the normal or 
Gaussian "law". 
Some would try to ascribe this to the choice of reaction time, properly 
asserting that today an informed worker would not suppose that such times 
were likely to be normally distributed, and arguing that perhaps the logarithm 
of the reaction times might well be nearly normally distributed. A glance at the 
table in Exhibit 5 shows that this approach cannot succeed. Indeed, the number 
of negative errors beyond 3.1s is already 3 times that for a normal distribution, 
and a logarithmic transformation would increase this number. 
The important deviations from normality in Peirce's data are not matters 
of discreteness, nor are they matters of gross differences in shape. Columns (2) 
and (3) agree generally with each other both as to typical value and in 
day-to-day changes. As Winsor's "principle" predicts for the distributions that 
arise in practice, the distributions of these very large samples are, except for 
discreteness, reasonably "normal in the middle". However, the normal dis- 
tribution that would fit the center of the observed distribution would have a 
spread only about 3/4 of that of the distribution fitted to the observed value of 
the standard deviation. 
The median value in column (2), omitting day 1, is 0.756, which corres- 
ponds to a variance due to the "normal body" of the distribution of about 
(.756) 2  0.57 of that observed. Thus more than 40% (100% - 57%) of the 
observed variance comes from the fact that the tails straggle more than for a 
normal distribution. Again we see that Peirce's observations are glaringly far 
from being normally distributed. 
1E. Kinds of Nonnormality and of Robustness 
When a distribution does not have the Gaussian shape, its failure to be 
Gaussian may arise from: 
 1E: Kinds of nonnormality and robustness/Exhibit 6 13 
1. Discreteness and irregularity. (i) Ordinarily the value of an actual observa- 
tion or measurement can never be just any number from minus infinity to plus 
infinity--its possible values will be limited, often, in particular, to multiples of 
some least count. (For example, children and marriages come in whole num- 
bers, prices on many stock markets come in 1/8's, and many machine-shop 
measurements are in thousandths of an inch.) (ii) Beyond this, both real and 
theoretical distributio.ns are subject to further irregularities, such as those 
caused by observers' dgit preferences and those that appear even in theoretical 
distributions, such as the sampling distribution of the rank-correlation coeffi- 
cient in the null situation (when the variables being correlated are actually 
independent) (see Exhibit 6). To help guide the eye, the successive ordinates of 
this discrete distribution have been connected by straight lines. 
Exhibit 6 of Chapter 1 
Distribution of rank correlation coefficient for n = 7. 
Frequency' 
] I J ;,., 
-I -o5 o o.s' i 
To obtain probabilities, ordinates should be divided by 7!. Probabilities are zero 
I 2 
unless half the sum of squares of differences, d, is an integer. Example: Given 
1234567 the sum of squares d 2 = 22 + 02 + 22: 8 
the two matched rankings 3214567' 
and  2 
_,d - 4. (After Kendall, Kendall, and Smith, 1938.) 
14 Exhibit 7/1: Approaching data analysis 
2. Gros differences in shape. Among these we include those differences that 
can be reliably detected in samples of, say, 50 to 100 such as the gross skewness 
of X 2 (or F) with few degrees of freedom. The abrupt ends of a rectangu- 
lar distribution, which spreads its probability evenly over a fixed range, are 
barely this gross. Exhibit 7 represents the densities of )42 distributions with 1 and 
3 degrees of freedom; Exhibit 8 represents the density of a rectangular 
distribution; Exhibit 9 represents the density of a symmetrical triangular 
distribution, which, incidentally, represents the distribution of the sum of two 
independent and rectangularly distributed (uniform) random variables, each 
with range L/2, whose means sum to 
3. Minor dijerences in central shape. Difficult to separate from either of the 
first two and rarely of importance on their own. 
4. Behavior in the mils. Hard to detect, yet often important because a few 
straggling values scattered far from the bulk of the measurements can, for 
example, alter a sample mean drastically, and a sample s 2 catastrophically. 
Exhibit 7 of Chapter 1 
Probability density functions for the X 2 distribution with I degree of freedom and 
3 degrees of freedom, respectively. 
o.? 
01 
0.6 
o$ 
0.z 3dr 
1E: Nonnormality and robustness/Exhibits 8 and 9 15 
In the usual measurement situations, behavior in the tails is both the most 
vital and the least securely tied down of all. It is not well tied down because 
changes in tail size by large and important factors need involve only a few 
percent of all observations, and are therefore both difficult to detect by 
sampling and easy for transient causes to produce. It is vital because the 
extreme tails of the distribution of individual values are likely to influence large 
portions of the distributions of many derived quantities. 
Consider the situation where, perhaps because of human actions, there is 
one chance in a thousand of a huge deviation (an observation strikingly far 
from some central value like a mean or median). In samples of size 100, there 
will be one such huge deviation per 10 samples, or about 100 huge deviations 
Exhibit 8 of Chapter 1 
Probability density function for the rectangular distribution with range L and 
mean . 
Exhibit 9 of Chapter 1 
Probability density function of a triangular distribution. 
16 /1: Approaching data analysis 
per 1000 samples. Thus 10% of all sample means will be affected. This can be 
enough to distort 5% points seriously. If these "wild shots" are extreme 
enough, they will clearly perturb sample means seriously. Student's t, on the 
other hand, is a function of the observations that behaves in quite a different 
way than a sample mean does. The effect of one observation differing very 
greatly, both from the mean of the other observations and from the contem- 
plated value, is to make the value of t fall close to +1 or -1. When working at 
95%, this is going to cause reduction of % risk, thus "failing safe", but can 
make many instances that would otherwise be "significant" come out "not 
significant". Our real need is accurate answers, not just safe statements. Often 
the use of other significance tests will avoid such ill effects from outliers and 
bring us closer to describing the real state of affairs. 
Both of these desirable properties are kinds of robnstness, kinds of lack 
of susceptibility to the effects of nonnormality. The first property, tolerance of 
nonnormal tails, has been called robustness ot validity, and is exemplified by 
confidence intervals for Ix that have a 95% chance of covering Ix, whatever 
population may have been sampled. We get this sort of robustness exactly, for 
example, when.a confidence interval for the population median is based on the 
sign test. The second property, high effectiveness in the face of nonnormal tails, 
is called robustness of efficiency, and is exemplified by confidence intervals for 
Ix that tend to be almost as narrow, for any roughly normal distribution (that is, 
any distribution "approximately normal in the middle") as the best that could 
be done if the true shape of the distribution were known to a close approxima- 
tion. Such procedures are now available (e.g., Gross, 1976). 
You will find that most analytical studies of the effects of "nonnormality" 
consider only what happens when behavior in the tails of the distribution 
agrees with the results of naive extrapolation from the behavior of the body of 
the distribution. In the real world things are usually different, since many 
causes that could have contributed to each of the values that we had observed, 
like gross human error, act only infrequently. As a result, real tails rarely 
match the body of the distribution, and are likely, when very large samples 
begin to reveal their true behavior, to appear poorly pasted on. 
Both sorts of tail misbehavior can be of considerable importance. The 
difference between grossly nonnormal distributions and distributions "normal 
in the middle" but with straggling tails is not mainly in the consequences, which 
are rather similar, often equally unfortunate. Rather it is that gross nonnormal- 
ity is likely to be detected, or even brought forcibly to one's notice, but 
straggling tails tend to escape both notice and careful examination. 
One way to give specific examples of distributions with straggling tails is to 
consider mixtures of normal distributions. For example, we might consider a 
distribution formed from observations drawn from two normal distributions 
having identical means, but unequal standard deviations. Almost all of the 
measurements would come from the normal distribution with the smaller 
standard deviation, but a small fraction, say 1%, comes from a distribution 
(1E) 1F: The role of vague concepts 17 
with 3 times the standard deviation of the first. Such mixed distributions are 
sometimes called contaminated distributions. This one would be called 1% 
contaminated at scale 3. 
We shall also need the notion of relative efficiency. Roughly speaking, 
when two estimates of the same quantity have unequal variances, the ratio of 
the smaller variance to the larger is called the relative efficiency of the estimate 
with the larger variance. To be more careful in our definition, the distributions 
of the two statistics should have roughly similar shapes, and their variances 
should approximate constant multiples of 1In. Fortunately, very good estimates 
often do this. Under these conditions, the relative efficiency satisfactorily gives 
the ratio of the sample sizes required for two statistics to do the same job. As 
an example, for large samples from normal distributions, the sample median 
has variance approximately (,r/2)cr2/n, and the sample mean has variance cr2/n. 
We say that the median has efficiency 2/r  0.64. In other words, a sample of 
size 100 using the sample median estimates the population center about as well 
as a sample of 64 does using the sample mean. 
Consider an extreme case, where we compare a normal distribution with a 
distribution 1% contaminated at scale 3. This represents a lengthening of tails 
of the basic normal by so little as to require thousands of observations for 
reliable detection. Yet the effect on the distributions of derived quantities can 
be very serious. In large samples, we can compare two estimates of spread: (i) 
that based on the standard deviation, s, where squares of deviations are 
summed, and (ii) that based on the mean deviation, where absolute values of 
deviations are summed ([yi- y[/n). For the relative efficiencies we find 
(Tukey, 1960): 
For normality: Mean deviation 88% as good as s 
For 1% contamination: Mean deviation.. 144% as good as s 
Clearly, quite small differences in shape of distribution can greatly affect the 
relative efficiency and therefore the comparative palatability of different proce- 
dures. 
1F. The Role of Vague Concepts 
Effective data analysis requires us to consider t)ague concepts, concepts that can 
be made definite in many ways. To help understand many definite concepts, we 
need to go back to more primitive and less definite concepts and then work 
our way forward. Let us take a simple example. 
Most beginning students of statistics know that the standard deviation of a 
distribution is one of the useful things to estimate. The concept of standard 
deviation is hard enough to grasp and use; students have not, as a rule, had 
much time to consider why and when to use it. Let us discuss these questions. 
One approach from the vague toward the particular begins with the crude 
and qualitative idea that some distributions are more widely spread out, others 
18 /1: Approaching data analysis 
are more tightly packed, and with the insight that it might be well to measure 
this by some numerical measure of spread. This leads us to the sequence of 
ideas: 
oSpreads differ. 
o A numerical measure may be useful. 
o One numerical measure is the standard deviation. 
oHow do we judge whether this is a good choice? 
Note that we have so far been discussing the problem only for a complete 
distribution or population, and that no questions of sampling or estimation 
have yet been considered. We are still interested in what questions we want 
answered, not yet with how to get their approximate answers. 
The usual answers to the last question that are both acceptable and 
favorable to the standard deviation include the following: 
1. The definition of a standard deviation (of a complete distribution) is 
relatively clear and widely applicable. 
2. So long as one is concerned with simple or weighted sample means, or 
with many of the other conventional linear combinations of observations 
(such as 5 + 2x + 3y, where x and y are observations), the standard 
deviation (or its square, the variance) is a peculiarly useful measure 
because there are convenient relationships between the variance of the 
defined statistic and the variance of the underlying distribution that do not 
depend upon distributional shape. For example, with x and y as observa- 
tions, a and b as constants, try, Cry, and tr.+y the standard deviations of 
the indicated quantities, and p the correlation coefficient between x and y, 
we have the relation' 
2 a 2 2 
o'o+t,y cr + 2abpo'cry + b 2 2 
--- O'yo 
3. Certain algebraically convenient relations hold "on the average" between 
sample and population for the variance (and hence for its square root, the 
standard deviation); for example, the expected or average value of s 2 is 
2 S 2 
cr , where is the sample variance, s 2 = Y(yi - )2/( n - 1), and (r 2 is the 
population variance. 
4. If one can confine one's attention to distributions of given shape (all 
normal, all rectangular, all symmetric triangular, etc.), then any two 
measures of spread are connected by multiplication by fixed constants, and 
so it doesn't really matter which population measures of spread one 
chooses. (Recall from Section 1C that many larger families of closely 
related distributions, for example, all beta distributions, do not have a 
single shape.) Consequently, why not choose the standard deviation? 
(1F) 1G: Other vague concepts 19 
On the other hand, 
o Some distributions have infinite standard deviations; indeed, we have 
seen the examples of the t distribution with 1 and 2 degrees of freedom in 
the first footnote to Exhibit 2. When such a distribution is just twice as 
widely spread as another, their standard deviations do not tell us this, 
although a variety of other measures of spread, including the interquartile 
range, do. 
o Means are a poor way to summarize samples from distributions with 
infinite standard deviations, so that (2) is irrelevant when working with such 
distributions. 
o Even though the population variance is infinite, all the sample variances 
are finite, so that (3) buys us little or nothing in such cases. 
oil, for a distribution with finite variance, we change shape enough to 
make the variance infinite, the relation of the standard deviation to other 
measures of spread that remain finite changes drastically. 
o An imperceptible probability of sufficiently deviant values can make the 
variance infinite. 
o Consequently, the difference, in most practice, between distributions with 
infinite variances and distributions with finite variances and long straggling 
tails cannot be great, so that (4) buys us little or nothing in practice. 
We do not wish to conclude from these arguments that the standard de- 
viation is a poor choice. In many situations, it is still an ideal choice as a 
description of a population and in some it is far from ideal. 
We do wish to conclude: 
othat the standard deviation is a choice, and 
o that, in order to understand that it is a choice and to decide whether it is 
a good choice, we need the vaguer idea of measures o.f spread. 
We shall repeatedly need vague concepts like spread to help us understand 
and appraise the usefulness of definite concepts like standard deviation, range, 
and interquartile range. Sometimes the specific concepts come first, and the 
vague concept is sought out to help in dealing with them. More often, however, 
the vague concept comes first and guides us in identifying corresponding 
specific concepts. 
I G. Other Vague Concepts 
The student of elementary statistics may have learned to classify some experi- 
mental results as "significant" and others as "not significant". And he or she 
may have learned that observed sample means, as well as many other expres- 
20 /1: Approaching data analysis 
sions calculated from the sample, can generate "confidence intervals" for 
population means. These are specific ways of realizing another vague concept: 
"expressing amounts of uncertainty". 
Here, the sequence of ideas is: 
o Observed numerical summaries, such as sample means, from small or 
medium-sized samples do not coincide with what would have been found 
"by the same methods" from either extremely large samples or complete 
populations. 
owe do not know the difference between sample summary value and 
population summary value in any one instance (if we did, we would make 
the adjustment), but we may be able to assess about how large it might 
reasonably be. 
o The plausible size thus assessed will be different in different cir- 
cumstances; obviously, some sample summary values are subject to greater 
uncertainties than others. 
o Thus there is a place for ways of assessing and expressing amounts of 
uncertainty. 
Different specific ways of "expressing amounts of uncertainty" are in daily 
use. One whole class of choices, often closely related to one another, is 
provided by confidence intervals. (To give a 90% confidence interval is not the 
same as to give a 99% confidence interval, but in simple problems, the 
information conveyed by the one is often roughly translatable into that given 
by the other.) Besides confidence intervals, there are not only significance tests 
but a number of other ways of expressing extent of uncertainty. 
We cannot be effective in thinking comparatively about these ways, to say 
nothing of being effective in using them, without some attention to a more 
primitive vague notion: assessing the uncertainty of a numerical summary. 
We have introduced above, perhaps without your notice, an approxima- 
tion to another important but vague concept. The term "numerical summary" 
was illustrated by the example of a sample mean. It was hoped that the usual 
meanings of "numerical" and "summary" would be enough to seem to give it 
meaning for the moment. A still more general notion covers any of those things 
that have been drawn from a body of data to exhibit at least one aspect of what 
that body can suggest. This may be a numerical summary: :or a graphical 
summary. 
We are going to call any such summary an indication, and we shall insist 
on its meeting only two of the statistician's obligations' 
olt must differ from an anecdote by allowing each of the observations to 
contribute to it. (Anecdotes usually involve one or a few observations.) 
olt must be expressed in such a way that at least some of those who are 
interested in the subject can think about its interpretation. 
(1G) 1H: Induction, determination, or inference 21 
These conditions are important, but should not be interpreted too strin- 
gently. The mean of a sample of 15 is an indication, but so is the median of that 
sample, since each observation contributes to the median in the sense that, in 
the absence of ties at the median, if that one observation had been sufficiently 
different, the median would have been different. Each observation had its effect, 
even though small changes in a single observation (each of which would have 
an effect on the mean, smaller than on the individual value, but still present) 
need not change the median in the slightest. 
Both mean and median belong to a special class of indications one might 
call typical values, or centers, or in more technical language, measures of 
location. In particular, for nicely behaved data where the observations follow a 
crudely bell-shaped distribution, both mean and median usually indicate, 
roughly, where the center or body of that distribution falls. (If the tails are 
straggling and unsymmetrical enough, the mean may fail to do this.) 
Measures of spread, measures of association, all the results of most of the 
standard procedures of statistical analysis are indications, as are interesting 
bumps on curves, greater wiggliness of a graph on its lefthand side than on its 
right, or apparent segregation of a scatter diagram into clumps. 
Statistics courses tend not to emphasize indication; instead, they concen- 
trate upon how to express the uncertainties of particular kinds of indications. 
In impressing the student with the importance of assessing uncertainties when 
one can (and can afford the effort), it is perhaps inevitable that they may give 
the impression that indications whose uncertainties have not been assessed are 
worthless. This is, of course, just not so. We need to assess uncertainties 
vigorously and often, but we also need to look at indications of unassessed 
uncertainty, especially where assessment is impossible or uneconomical. 
I H. Indication, Determination, or Inference 
Since Student's revolution, many kinds of formal inference have been de- 
veloped; today we have our choice of three levels of statistical analysis. We can 
be concerned with any or all of the following: 
o pure indication in which, for example, we attend only to primary 
statistics, such as means and percentages, with no attempt to assess uncer- 
tainties; 
odetermination, or augmented measurement, where both primary and 
secondary statistics are to be calculated and considered, for example, a mean 
and an estimate of its standard deviation (and where, though nothing is said 
about inference, the secondary statistics are most often interesting only as 
tools for appraising uncertainty); 
o formal inference, where some relatively precise specification or descrip- 
tion of a manageable mathematical model allows us, at least apparently, to 
22 /1: Approaching data analysis 
tie up our uncertainties in a neat package--for example, through confidence 
limits, significance tests, fiducial inference, posterior distributions, or likeli- 
hood functions. 
In some specific situations, the third course is not open to us, perhaps for 
some of the following reasons: 
othe data do not allow some of the important causes of variation to show 
their effects; 
othe causes show themselves in a distinctly nonrandom way; or 
ono formal inference procedure is available because none has been de- 
veloped, or those that have been worked out all employ such stringent 
assumptions as to make their use unwise or misleading (or even perhaps 
because the computational effort required would be impractically great). 
Sometimes, indeed, we may appear to lack, as in the second of these, for 
instance, even any reasonable way to compute secondary statistics. Then we 
are likely to appear forced to resort to pure indication. 
When our concern is with indication alone, there is an important distinc- 
tion between indications that point toward something definite and those that 
point usefully but not toward anything, as when a graph causes its viewers to 
say "Look, there's a kink in the curve!" Indications of the former kind are also 
called estimates. Their indicators are also estimators. We discuss estimates and 
estimators further in Section 2D. 
This distinction between estimators and nonestimators is closely similar to, 
but definitely not the same as, the distinction between quantitative indicators 
and qualitative indicators. A large value of chi-square, for example, often 
indicates that some hypothesis fits poorly, yet the value of chi-square itself 
points toward no number that has a real meaning for the underlying situation. 
Summary: Data Analysis 
Student, in 1908, introduced an alternative to the infinite staircase in which 
assessing the variability of each statistic introduced a further statistic, whose 
variability was harder to assess. 
Three decades later, statisticians learned to shortcut the staircase without 
making such strong assumptions as Student's, thereby introducing the heyday 
of "nonparametric" or "distribution-free" procedures. 
The main value of Student's works lay in (i) showing that one could deal 
explicitly with small samples, (ii) giving numbers for how much this mattered in 
an important case, and (iii) providing tables for that case. Against this, his 
work, not by his own choice, made it too easy (iv) to neglect "if... assumptions 
hold," (v) to overstress the "exactness" in other problems, and (vi) to fail to 
attack the problems of multiplicity. 
(Summary) References 23 
We may have to be concerned with shapes of distribution whose tails are 
longer than might be anticipated from their middles or shoulders (we say 
"straggling tails" or "stretched-out tails"). Real data, as illustrated by C. S. 
Peirce's data on response time, often has stretched-out tails. 
Among the usual sorts of deviation from normality, we need to include: (i) 
discreteness and irregularity, (ii) gross differences in shape, (iii) minor differ- 
ences in central shape, (iv) behavior in the tails; while these are listed in order 
of increasing difficulty of detection, the most important is (iv), and the least 
(iii). 
We need to be concerned not only with Gaussian efficiency but even more 
with robustness of efficiency. 
Vague concepts often control the value and relevance of precise concepts, 
a point illustrated in some detail by discussing the standard deviation. 
Tracing back "significance" or "confidence" leads us to "expressing 
amounts of uncertainty" and to "a numerical summary" as key vague concepts, 
the latter leading to the concept of "indication." 
Three important levels of data analysis can reasonably be called "indica- 
tion", "determination", and "formal inference." 
References 
Burrau, 0 (1943). "Middelfejlen som Usikkerhedsmaal." Mat. Tidskr. B., 
1943, 9-16. (See Mathematical tables and other aids to computation, 2, 1946, 
74-75.) 
Cohen, J. (1969). Statistical power analysis for the behavioral sciences. New 
York: Academic Press. 
Fisher, R. A. (1925). "Applications of "Student's" distribution." Metton, 5(3), 
90-104. (References to this paper often use the date 1926, as Fisher did in his 
bibliography in Statistical methods for research workers; the journal gives 1 
Dec., 1925.) 
Gross, A.M. (1976). "Confidence interval robustness with long-tailed symmet- 
ric distributions." J. Amer. Statist. Assoc., 71, 409-416. 
Kendall, M. G., S. F. H. Kendall, and B. B. Smith (1938). "The distribution of 
Spearman's coefficient of rank correlation in a universe in which all rankings 
occur an equal number of times." Biometrika, 30, 251-273. 
Mosteller, F., and R. R. Bush (1954). "Selected quantitative techniques." In G. 
Lindzey (Ed.), Handbook of social psychology. Cambridge: Addison-Wesley; 
pp. 289-334. 
Odeh, R. E., and M. Fox (1975). Sample size choice: charts for experiments with 
linear models. New York: Marcel Dekker, Inc. 
Peirce, C. S. (1873). "Theory of errors of observations." Report of Superintendent 
of U.S. Coast Survey (for the year ending Nov. 1, 1870). Washington, D.C.: 
24 /1: Approaching data analysis 
Government Printing Office. Appendix No. 21, pp. 200-224 and Plate No. 27. 
"Student" (W. S. Gosset) (1908). "The probable error of a mean." Biometrilca, 
6, 1-25. Also in " Student' s" collected papers (edited by E. S. Pearson and J. 
Wishart), issued by the Biometrika Office, University College, London, 1942. 
Paper 2; pp. 11-34. 
, (1927). "Errors of routine analysis." Biometrika, 19, 151-164. Also in 
"Student's" collected papers (edited by E. S. Pearson and J. Wishart), issued by 
the Biometrika Office, University College, London, 1942. Paper 14; pp. 
135-149. 
Tukey, J. W. (1960). "A survey of sampling from contaminated distributions." 
In I. Olkin, S. G. Ghurye, W. Hoeffding, W. G. Madow, and H. B. Mann 
(Eds.), Contributions to probability and statistics. Essays in honor of Harold 
Hotelling. Stanford: Stanford Univ. Press; pp. 448-485. 
Wilson, E. B., and M. M. Hilferty (1929). "Note on C. S. Peirce's experimental 
discussion of the law of errors." Proc. Nat. Acad. Sci., 15(2), 120-125. 
Chapter 2 / I nd i cat ion 
an d Indicators 
Chapter index on next page 
Indication is elementary, important, and neglected. 
In beginning to remedy this neglect, we have to show indication as 
valuable and often the best that can be done. We have to illustrate how 
indicators may be reasonably chosen; how indication can require care, as 
illustrated by cross-validation; and how graphs are of great value, almost 
entirely because of their function as indicators. 
2A. The Value of Indication 
One hallmark of the statistically conscious investigator is a firm belief that, 
however the survey, experiment, or observational program actually turned out, 
it could have turned out somewhat differently. Holding such a belief and taking 
appropriate actions make effective use of data possible. We need not always 
ask explicitly "About how much differently?", but we should be aware of such 
questions. Most of us find uncertainty uncomfortable; the history of data 
analysis can be read as a succession of searches for certainty about uncertainty. 
All of us who deal with the analysis and interpretation of data must learn to 
cope with uncertainty in a way that meets our own needs. 
A caricature of one recipe might read: Apply a significance test to each 
result, believe the result implicitly if the conventional level of significance is 
reached, believe the null hypothesis otherwise. Such a complete flight from 
reality and its uncertainties is fortunately rare, but periodically considering its 
extremism may help us keep our balance. 
To the researcher, the primary value of data lies in what they indicate, 
what they appear to show. Such an appearance may be quite correct and exact, 
may be a matter of chance, and probably is a mixture. In some fields, when it is 
only a matter of counting noses or counting punched cards, assessing the 
indications tends to be overlooked or regarded as an unimportant detail, 
though demographic and population studies offer good exceptions. 
Nose-counting and reporting counts form only one extreme of indication. 
At the other, the most essential parts of our most complicated schemes of 
analyzing data complex analyses of variance, multiple regression analyses, 
factor analyses, latent structure analyses.. have as their main function the 
assessment of indication. 
The word indication is a vague concept intended to include, at one 
extreme, all of the classical descriptive statistics (for example: mean, median, 
25 
26 Index for Chapter 2 
2A The value of indication 25 
2B Examples of stopping with indication 27 
2C Concealed inference 31 
2D Choice of indicators 32 
2E An example of choice of indicator 34 
2F Indications of quality: Cross-validation 36 
Summary: Indication and indicators 40 
References 40 
(2A) 2B: Examples of stopping with indication 27 
mode, quantile, standard deviation, correlation), but also, at another extreme, 
to include any hints and suggestions obtained from data by an understandable 
process, suggestions that might prove informative to a reasonable man. Exam- 
ples are appearances of similarity (all the curves look S-shaped or ogival) or 
general behavior (the frequencies seem to be falling off roughly exponentially, 
or, again, although the means of the groups vary widely, the standard devia- 
tions are all close to those usually found in these experiments). Indication 
includes not only isolated numbers (differences, slopes, and other indices), but 
also inequalities and trends (the female appears to be more deadly than the 
male, blood pressure appears to rise with age), and appearances of graphs and 
diagrams (these scatter diagrams appear doughnut-shaped). 
What indication is not is inference or treatment of uncertainty; indication 
does not include confidence limits, significance tests, posterior and fiducial 
distributions, or even standard errors. 
The treatment of variation or uncertainty looms large and that of indica- 
tion small in most discussions of data analysis. This imbalance arises naturally: 
important considerations in assessing indications are often specific to a particu- 
lar problem and therefore difficult to generalize about; since the study of 
variation is often neglected because of the beginner's eagerness to find reg- 
ularities and simple appearances, we need to focus our attention on problems 
of uncertainty; and, conversely, for many, it is psychologically satisfactory to 
seek some kind of certainty. These are some of the many reasons that have 
combined to generate a bulky literature about measuring and allowing for 
variation, and to make most introductory statistics texts give their main 
attention to what is done once the indication is found. Naturally, we all desire 
an adequate assessment of both the indications and their uncertainties, but we 
shouldn't refuse good cake only because we can't have frosting too. 
2B. Examples of Stopping with Indication 
Often, the analyst of data stops calculating after reaching pure indication. 
(Some further thought may still give judgmental impressions of the uncertain- 
ties involved.) Let us illustrate a few such circumstances. Suppose, for example, 
that, in a haphazardly assembled remedial-reading class of 23 taught by a 
single teacher using one special method, the reading of 5 students substantially 
improved and that of 18 was substantially unchanged. The indication is that the 
treatment of the students improves perhaps 1/4 of the students. This figure is 
fraught with uncertainties about which the data give us little aid. All the same, 
if other teachers and methods are getting improvement for 85% of the 
students, we ignore at our peril the indication that this method is not as good as 
general practice. 
28 /2: Indication and indicators 
If we sought to assess the uncertainties of our 5/23, what could we do with 
such problems as' 
oFrom what population was this class something like a random sample? 
oIs that population something like those from which the samples taught by 
other methods were drawn? 
oHow much do the teacher's personality and tendencies to deviate from 
the assigned method matter? 
In this example, a good assessment of uncertainty seems most unlikely. 
Nevertheless, we have an indication that can be the basis for action. 
Matters need not be so simple as this example of indication suggests. To 
get data to yield a sensible indication may require subtle and complex analysis 
chosen with sophistication. As an example, let us simplify a problem that was 
much discussed in scientific quarters in the early 1960's. Consider choosing the 
adjustments required to take equitable and defendable account of background 
characteristics in a comparison of blacks and whites on a standard intelligence 
test. (To illustrate difficulties of indication, we need not raise either the hard 
issue of proper measures of intelligence or the biological problem of purity of 
strains.) Age, family size, socioeconomic status, population density, years of 
schooling, kind of schooling, strength of home life and its orientation toward 
things of the mind might provide a basis for handling this question. We still 
have the tough question of how these data shall be used. Obviously multiple- 
control methods are required, and we may be delighted to stop with indication, 
once these methods have been effectively applied. The techniques of indication 
often go far beyond the descriptive statistics found in first courses in quantita- 
tive methods. 
Problems of Multiplicity 
Another kind of problem arises when we want to distinguish two groups of 
individuals by a difference in variability for at least one of a large number, say 
100, of measured characteristics, having one measurement on each characteris- 
tic for each individual. We can, if we are careful, compare the variances on any 
one aspect of the two populations in a way that is reasonably reliable at, say, 
95% confidence. Even if the null hypothesis of equal variability were true, the 
average number of tests individually significant at 5% is just the product of 5% 
and the number of tests, namely (.05)(100) = 5. 
By the same token, the average number of tests individually significant at 
0.05% (1/20 of 1%) is (.0005)(100) -- 0.05, which can be interpreted to mean 
that one or more such will occur in about 5% of such situations (a situation 
means a repetition of the whole: here 100 tests each at a 0.05% level). This 
2B: Example of stopping with indication 29 
means that if the apparently most-extreme variance ratio is to be significant at 
the 5% level in its role as the largest of 100, it must be individually significant 
at about 0.05%! 
Such an extreme value must fall in a region where the exact but unknown 
shape of the underlying distribution may have large effects on the probability 
of our test ratio whether this be (s 2 2 2 2 
, 1/0'1)/($2/0'2) or something more robust, 
falling beyond a given value. No sort of statistical procedure is yet sure to 
deal well with this difficulty. The only way to obtain adequate control of such 
uncertainty may be to make a repeat study confined to those few characteristics 
that appeared in the first study to differ a lot from one group to another. In 
the interim, even though the data may have given the relevant sources of 
variation full opportunity to reveal themselves, it may be quite unwise to do 
anything beyond indication. 
An example farther along, but in a somewhat more familiar direction, 
arises when we are searching for interesting indications that may serve as hints 
in approaching further data. The intent here is not one of attaining a conclu- 
sion, nor of making measurements for record, but only of hunting out interest- 
ing indications. Suppose that we have looked at many aspects, say 1000 to 
10,000, instead of only 100, and have selected from this exploration a list of 
aspects whose values appear interesting. Now the dangers of implicit belief in 
very extreme % points are even greater. Here we must stop our calculations 
with indications and be careful to think of our results only as hints as to what to 
study next, rather than as established results. 
A seemingly slight modification arises when exploration and hint-searching 
has been carried out on a portion, say one-third, of the total data, with the 
intention of coming up with approximately a given number of hints, say 10 or 
20, which are to be tried out, either for measurement unaffected by selection or 
as subjects for conclusions, on the remaining two-thirds of the total data. Here, 
at the transition from exploration of the first third to confirmation by the other 
two thirds, we need nothing but indication; our problem is to pick the 
best-looking hints for immediate trial. (Compare this process with cross- 
validation discussed in Section 2F below.) 
(Indeed, on the closely analogous problem of selecting the "best" of many 
new strains of a crop, it is often wise to make the first selections in cir- 
cumstances where one knows that observed differences will almost certainly 
not be provably real (Yates, 1950). Some find this paradoxical. Breeders can 
measure accurately if they raise many specimens of a very few strains with, of 
course, little chance that an especially good strain has been included. Or they 
can raise many strains and measure them relatively inaccurately because they 
have few individuals of each strain. Following the latter procedure increases 
the chance of raising a good strain, but, because of poorer measurement, 
decreases the chance of detecting it. Attaining a desirable balance produces the 
paradox.) 
30 /2: Indication and indicators 
Some'of the important reasons for stopping with indications are: 
a) The form of the data (its structure) masks the important sources of 
variability. 
b) We have no good way to deal with what may be substantial differences in 
variability of this sort of indication, differences that experience suggests 
m.ay occur but to an extent not detectable in a single set of data. Usually 
ths difficulty arises from the multiplicity of variables or aspects being 
examined. 
c) Preliminary exploration of data of high multiplicity has led to selection of 
a few indications for later confirmation. 
Whatever the reason or reasons for stopping with indication, they should be 
reported. 
Consider, as a final example, comparing short and long forms of a 
projective test in order to study the effect of form length on reliability of 
scoring. Each form has 32 different ways of scoring a single protocol. Suppose 
that, as would be the case for the Rorschach test, the scores are available on a 
split-half basis, so that the calculation of reliability coefficients is quite feasible. 
To reduce the effects of sampling upon the final comparison, the investigator 
gives the short and long forms to the same people at the same time, perhaps by 
having the short form part of the long form. He finds it easy: to calculate 32 
reliability coefficients for the short form, and their average; to do the same with 
the reliability coefficients for the long form; and to look at the difference (or 
perhaps the ratio) of these average reliability coefficients. This indication is, he 
hopes, reasonably responsive to the original question of the comparative 
reliability of long and short forms. 
What if he seeks to pass from indication to inference? His statistical 
advisor is apt to tell him that each of his 64 basic scores is correlated, to 
unknown and probably differing extents, with each of the others. Here are 
�(63)(64) = 2016 correlation coefficients to consider. And each correlation 
coefficient between basic scores requires conversion, not a trivial task, to a 
correlation coefficient between the corresponding split-half reliabilities. When 
this is done, the variances and covariances of the reliabilities can be found, 
then those of the two average reliabilities, and finally the variance of their 
difference. If large-sample theory is adequate, as it may be if he happens to 
have 10,000 protocols, the uncertainty of the difference can now be assessed. 
After thinking about the effort involved, what would you do? 
Under the conditions described, indication would be the end of the road 
for most investigators. (We shall introduce in Chapter 8 a technique that makes 
inference feasible even in problems this gory.) 
Sometimes results that individually might reasonably have arisen from 
purely chance variation are worthy of report because they strengthen one 
(2B) 2C: Concealed inference 31 
another. Sometimes we have parallel results based on independent bodies of 
data, all contained in a single study where the assessment of mutual support is 
easy. Or there may be parallel results on overlapping or interrelated data, as 
where seven questions have all been answered by the same respondents, where 
the results agree in direction but the amount of mutual support is hazy. Or it 
may be that one result in today's study is likely to be paralleled in due course 
by other results in other studies. 
In all these situations, it can be quite wasteful to suppress results either 
because they are not individually significant or because their joint significance 
cannot be satisfactorily appraised. 
2C. Concealed Inference 
When we study data for the answers to a specific question, we sometimes find 
the evidence so strong as to obviously resolve the question. When matters are 
so clear-cut, quite informal inference is usually adequate, both in analysis and 
in communication. 
If the data are only obviously strong, the reader or listener is often asked 
to look at the indications, and "see" the situation. The investigator typically 
says "No statistics is necessary to see that...", meaning "simple ultraconserva- 
tive statistical methods are overwhelming". Back of such statements lies some 
notion of statistical analysis that is not being displayed. As examples: "85 
successes out of 100 cases could scarcely be compatible with a probability of 
success of 0.1" or "the standard deviation of the difference is obviously over 
100, but even if it were as small as 50, a difference in sample means of 10 
could scarcely be strong evidence in favor of either method". Although such 
discussions are common, considerable sophistication or experience may be 
required to be able to dispense with formal inference, and many of us have 
been brought up short by a neophyte's saying that he or she is not convinced, 
sometimes because the obvious has required a rather lengthy demonstration. 
If the data are much stronger than this, only the indications are given, and 
nothing at all is said to the reader or listener. 
Such instances are not instances of stopping with indication, as we use this 
term. They are merely cases of inference where there has been no need for any 
of the formalities of inference. Informal inference, perhaps expressed by 
"obviously", or by "looking at.. we see that" 
., , or by absence of any remark 
at all, is the only sort of inference felt necessary. Yet, even if no word be 
written or spoken, it is inference. 
When we speak of stopping with indication, we do not intend to include 
these pleasant cases of informal inference. Rather we refer, in the main, to 
cases of indication, without even the barest sort of appraisal of the indicator's 
stability, variability, or reliability to provide a basis for informal or formal 
inference. 
32 /2: Indication and indicators 
2D. Choice' of Indicators 
The analyst often chooses which indicator to use. He or she compares the 
responsiveness of the different indicators to the specific question reached. The 
analyst is likely to compare their ease of calculation and frequently asks 
whether the results of one will be more, or less, stable than those of another. 
Making such judgments takes more than looking things up in the proper 
book. Wholly inferior indicators are indeed subject to the struggle for exis- 
tence and tend to die out. But, as we saw at the close of Section 1C, a 
difference in distributional shape so small as to be difficult to detect in samples 
of substantial size (like a few thousand) can alter the balance between two 
indicators drastically. In the example cited, the indicators happened to be 
estimators. One, under ideal conditions, did only nine-tenths as well as the 
other; under more realistic conditions it did almost half-again better. 
Such difficulties and delicacies must be faced even, perhaps especially, if 
the indicator selected is to be used for pure indication alone. 
Before discussing choices among estimating indicators among esti- 
mators . we need to clarify the meaning of the verb "estimate". When does 
an indicator estimate something? What does it estimate? 
One naive answer is: An estimate estimates a parameter. Historically, the 
word "parameter" has meant two quite different things: 
o The numerical value of a particular symbol in a particular way of specify- 
ing a family of distributions, as when the family of normal distributions is 
parametrized either by /x and tr 2 or by /a and 
o A numerical characteristic of a distribution, as when any numerical 
population is partially characterized by its median. 
To restrict "estimation" to estimating a particular coefficient in a family of 
distributions would lose much of the usefulness of this vague concept. The idea 
of estimation should be as general as possible. Thus, while it is usually helpful 
to know about the comparative performance of two estimators when the 
individual fluctuations in the data follow distributions of Gaussian shape, it can 
be misleading not to also know how they behave for other shapes. 
Accordingly, even if the analysts know just what to estimate, a conting- 
ency not as frequent as is commonly thought, they need to be guided in 
choosing an estimate by all that is known or feltNto be true under a variety 
of alternative circumstances. 
An estimator is a function of the observations, a specific way of putting 
them together. It may be specified by an arithmetic formula, like  = Yxdn, or 
by words alone, as in directions for finding a sample median by ordering and 
counting. We distinguish between the estimator and its value, an estimate, 
obtained from a specific set of data. The variance estimator, s 2-- 
2C: Concealed inference 33 
 (xi- :)2/(rt -- 1), yields the estimate 7 from the three observations 2, 3, 7. 
We say s 2 is an estimator for tr 2 and we call tr 2 the estimand. In the numerical 
example, 7 estimates tr 2. 
Sometimes the estimator comes first, and we then ask what it points to. 
We speak of the estimator's target as an estimand (as something to be 
estimated) rather than as just a parameter. Our problem is to match estimands 
to estimators. When can we do this? How precisely? Are there general 
circumstances when we can expect a match? 
These questions have not had the research attention they deserve. The 
question of sample size plays a central role, although we might wish that it did 
not. In actual investigations, we use our estimator on a sample of a particular 
size. Yet today's approaches to the choice of matching estimands to estimators 
depend on what we would do with larger samples, a somewhat unsatisfactory 
state of affairs. 
As a device to aid discussion of this question at this time, we divide the 
estimators we use for a particular sample size into three classes' 
1. those we would use for all larger samples, 
2. those we would use for much larger, but not arbitrarily large, samples, 
3. those we would not care to use for much larger samples. 
Class 3 is hard to deal with in the present framework. It may never 
provide us with satisfying matches between estimator and estimand. 
Class 2 occurs frequently in actual practice. The sample range is often an 
example. Within our framework, we can often get practical answers by forget- 
ting that we would not use this estimator for arbitrarily large samples, by 
pretending that we have class 1. 
Class 1, then, unrealistic though it may be, is the class we now act as if we 
have. Even in this Utopia we cannot lay down simple rules that will pick an 
estimand to go with our estimator. All that we can do is to list some alternative 
circumstances where we are likely to be satisfied with the match between 
estimator and estimand. These include: 
o If the average value of the estimate (the result of averaging over all 
samples of a given size) has the same value for all sample sizes, this value is 
a good candidate for the matching estimand, as is the case for s 2 and cr 2 so 
long as tr 2 is finite. If, in addition, the distributions of the estimate condense 
around this estimand as the sample size grows, most would find the match 
more gratifying. Exceptions arise, for example, when the average value of 
the estimator is not at all typical of its distribution, as when considering s 2 
for a distribution with infinite tr 2. 
oIf the average values depend on sample size, but converge to a limit as the 
sample size increases, as is the case for s when tr is finite, much the same 
can be said. Most statisticians are more pleased when the dependence on 
34 /2: Indication and indicators 
sample size is slight, as in the s, rr example for normal distributions for 
n > 10, say. 
o In a similar vein, if the median of the distribution of sample estimates is 
the same for all sample sizes, or if its value converges to a limit as the 
sample size grows, this common or limiting value is likely to be a satisfactor- 
ily matching estimand. For, especially in large samples, the median of an 
estimate distribution rarely fails to be reasonably typical of that distribution. 
o The limit, as sample size grows, of any reasonable typical value (see 
Section 1G), other than the mean or median, of the distribution of estimates 
for a fixed sample size is also likely to be a satisfactorily matching estimand. 
These rules are not neat and detailed, but they do identify some matching 
estimands that many would regard as satisfactory. Our own diffident and 
tentative attitude toward this problem stems partly from the limited studies on 
which these remarks have been based, and partly from dissatisfaction with 
having the acceptability of estimands depend on properties of collections of 
sample sizes not present in the investigation. 
2E. An Example of Choice of Indicator 
Suppose that one can make repeated observations of some quantity, observa- 
tions which behave rather like a sample from a population with long, straggling 
tails. Having collected these observations, the observer wishes to summarize 
them in a way that indicates their location with as much precision as can be 
simply obtained. He or she considers the arithmetic mean of all the observa- 
tions but finds that the long straggling tails impart so much variance that this 
indicator is unduly imprecise. The analyst next considers using the sample 
median, because it recovers about 2/3 (actually, in large samples, as we noted 
in Section 1E, 2/r) of the information about location in a sample from a normal 
distribution and is likely to do even better than this in samples from distribu- 
tions with more straggling tails. 
The stability of the sample median depends on the height of the density 
near the population median. Exhibit 1 represents a density for which the 
median is a poor choice for measuring location. As long as the straggling-tailed 
distribution has a reasonable density in the middle, our observer prefers the 
median to the mean (might there be a still better choice?). Some may think the 
median unduly variable; it is not likely to be a very good candidate for use of 
the jacknife (Chapter 8); it may miss enough information to matter. 
An alternative to the median is the trimmed mean. The sample is trimmed 
of its possibly straggling tails by setting aside some fraction of the measure- 
ments from each tail of the sample. Specifically, let us suppose that we decide 
to set aside the lowest 10% and the highest 10% of the observations, and to 
take the arithmetic mean of the remaining 80%. 
2E: An example of choice of indicator/Exhibit I 35 
How well would this do for a normal distribution? Keeping only the me- 
dian gives an efficiency of about 2/3 (more precisely, 2/st  63.7%). Using the 
trimmed mean regains a fraction of the remaining 1/3 of the information about 
 contained in the sample. For symmetrical trimming, this fractional gain obvi- 
ously runs from 0% (for 50% trimmed from each tail, leaving onlly the sample 
median) to 100% (for 0% trimmed). Investigations we do not give here show 
that the gain is somewhat faster than linear, and so we will be conservative if 
we assign an efficiency for the proportion a trimmed from each tail of 
- + (1 - 2a) 1 =  + (1 - 2a), 
which gives, in our example, 
 + 0.8 = 0.93 = 93% 
of the information about location. In samples from distributions whose tails 
straggle moderately more than those of Gaussian distributions, trimmed means 
do even better than this. Trimmed means can be safely jacknifed, a technique 
described in Chapter 8. 
Our observer has chosen an indication. It indicates "location", for if we 
add a constant to all the observations, the trimmed mean changes by the same 
amount and in the same direction. Has there so far been a definite distribu- 
tional model? No, except for the existence of a pent distribution. Or has a 
Exhibit I of Chapter 2 
Example of a density for which the sample median would be an extremely poor 
choice for measuring location. 
If the area under each part is close to 1/2, then from sample to sample, the 
median will flop back and forth between the left and right half, producing 
substantial variability. 
In the figure, P is the probability to the left of a given value x. 
0 , , ) 
  thr0qthou 
36 /2: Indication and indicators 
definite parameter to be estimated been explicitly identified? Again, no, though 
a class is implied by the specific choice, namely, the typical values of means 
from samples of this size trimmed 10% on each tail. 
In our trimmed-mean example, as the sample grows larger and larger, the 
central 80% of the sample will more and more nearly match the central 80% 
of the population. This makes the natural choice of estimand the mean of the 
central 80% of the population. 
(Optional) 
We have reached our answer and expressed it understandably. The more 
mathematically inclined may like to see the result expressed more formally. Let 
0 be the parameter being estimated, the estimand; let F be the cumulative. 
Then 
0 = I_ yqb(F(y)). dF(y), 
where 
1.25 = 1/0.8 for 0.1 < u < 0.9, 
b(u) = 0 elsewhere. 
We gain even more from thinking about the finite-sample formulation: 
ave {10% trimmed mean} -- f-o yb,(F(y))- dF(y), 
where, writing [n/10] for the number of observations (approximately 10% of n) 
set aside from each tail: 
[n/zo] 
vanishes like u or (1 - u) ["/�] near 0 and 1, and 
4"(u) [is mainly concentrated over and near the interval (0.1, 0.9). 
This formulation can be used to tell how large a sample is needed to suitably 
reduce the contributions to this average value from the extreme tails of the 
distribution. 
We now understand what the estimand is, and a bit about when trouble from 
quite straggling tails is likely--provided we are dealing with a sample of 
independent observations. 
2F. Indications of Quality: Cross-Validation 
Even though they are not always used for prediction, the results provided by 
some statistical techniques can be loosely described as "predictive" or "fore- 
casting". A multiple regression of one variable upon a number of others, for 
2F: Indications of quality: cross-validation 37 
example, may set forth what is hoped to be a standard property that holds for 
other data. 
Users have often been disappointed by procedures, such as multiple 
regression equations, that "forecast" quite well for the data on which they were 
built. When tried on fresh data, the predictive power of these procedures fell 
dismally. 
Let us discuss this situation in terms of a single example, multiple 
regression, while understanding that our insights and conclusions apply to 
many other techniques as well. In dealing with multiple regression, when we 
speak of a "procedure", we mean, for example, a specific regression equation: 
z - 3.4x + 2.5y - 5.4. 
When we speak of the "form", we mean, for example, choosing which 
variables from among many shall enter the regression and deciding individually 
for each variable whether we use the original measure, its logarithm, square 
root, or other re-expression. We also have to choose how the variables shall be 
grouped and combined--sums, products, ratios, or how? In this discussion, 
then, a "procedure"consists of a "form" and numerical values for its coeffi- 
cients. 
When we use data to determine some procedure, we want to be able to 
answer the question: How well may I expect the chosen procedure to behave in 
use? Even when the specific variables to be used in a multiple regression have 
been picked in advance, so that the form is determined, the coefficients are 
chosen from infinitely many combinations of possibilities to make the results of 
substituting in the formula fit the data as closely as possible. Testing the 
procedure on the data that gave it birth is almost certain to overestimate 
performance, for the optimizing process that chose it from among many 
possible procedures will have made the greatest use possible of any and all 
idiosyncrasies of those particular data. Sometimes we say that "Optimization 
capitalizes on chance!" As a result, the procedure will likely work better for 
these data than for almost any other data that will arise in practice. The 
apparent degree of fit will be closer than the true fit, on the average. 
No one knows how to appraise a procedure safely except by using different 
bodies of data from those that determined it. In other words, appraisal requires 
some form of cross-validation. We recognize two levels of cross-validation, 
simple and double, the simple being more widely recognized. They are: 
o Simple cross-validation. Test the procedure on data different from those 
used to choose its numerical coefficients. 
 Double cross-validation. Test the procedure on data different both from 
those used to guide the choice of its form and from those used to choose its 
numerical coefficients. 
38 /2: Indication and indicators 
The second level of cross-validation, which, by analogy with the physi- 
cian's "double-blind" study, we have called "double cross-validation", is to be 
had only by going to fresh data. These fresh data are best gathered after 
choosing form and coefficients. When fresh gathering is not feasible, good 
results can come from going to a body of data that has been kept in a locked 
safe where it has rested untouched and unscanned during all the choices and 
optimizations (as in the studies of Macdonald and Ward, 1963, Miller, 1962, 
and Mosteller and Wallace, 1964). For the full validating effect, the data placed 
in the safe must differ from those used to choose the procedure in ways that 
adequately represent the sources of variation anticipated in practice. For 
example, they may need to involve distinct school systems, distinct inves- 
tigators, or distinct years of observation. Despite the high merit of double 
cross-validation, we cannot always afford it. 
Whether we are content with, or stuck with, simple cross-validation, today 
the computer offers us new freedom and power. In the classical approach to 
simple cross-validation, the available body of data was divided into two 
(sometimes more) pieces of similar size. One was used for optimization, the 
other for testing. Some energetic workers would then interchange the two 
pieces and repeat, thus gaining more information from the same data. Al- 
though they learned more by this process, a subtle miasma of suspicion arose 
from the unknown correlation between the two estimates of quality. 
Such suspicions show again how insistence on inference as the goal tends 
to distort attitudes toward indication. Unknown correlations among the com- 
ponent estimates do, indeed, destroy the possibility of using degree of mutual 
agreement to assess stability of the combined result precisely. On the other 
hand, whenever each estimate by itself is sound, a weighted combination of two 
or more, however much or little correlated, is both equally sound and more 
precise than the individual values. 
The man who has halved his data and cross-validated in both directions 
has used all of his data to assess the quality of what is to be had by optimizing 
upon a body of data half as big as his total collection. If he has so much data 
that halving or doubling has little effect on the quality of the optimized 
procedure, well and good. He can do little more than he has done. Few are so 
fortunate. 
When routine computation was expensive, even doing the cross-validation 
in both directions seemed an effort. Today, we can face much more. 
Frank Yates (1957) proposed and P. J. McCarthy (1976) has recently 
expounded the use of more than one way to halve the data to obtain additional 
information. Or suppose that we divide the data into ten parts of similar size. 
Then we can combine any nine parts, optimize the procedure for this nine- 
tenths, and validate on the remaining tenth. Once we have done this ten times, 
separating a different tenth each time, we have used all the data to assess the 
quality of what is to be had by optimizing upon a body of data nine-tenths the 
2F: Indications of quality: cross-validation 39 
size of the total collection. This often comes appreciably closer to answering 
the most usual question: Approximately what performance may I expect from 
the result of optimizing upon all of my data? 
With a computer, the ten calculations are often little more effort than the 
conventional one or two, because we have only to repeat the same pattern of 
computation, with little more programming and debugging time. 
Often we can profitably go much further. Suppose that we set aside one 
individual case, optimize for what is left, then test on the set-aside case. 
Repeating this for every case squeezes the data almost dry. If we have to go 
through the full optimization calculation every time, the extra computation 
may be hard to face. Occasionally, one can easily calculate, either exactly or to 
an adequate approximation, what the effect of dropping a specific and very 
small part of the data will be on the optimized result. This adjusted optimized 
result can then be compared with the values for the omitted individual. That is, 
we make one optimization for all the data, followed by one repetition per case 
of a much simpler calculation, a calculation of the effect of dropping each 
individual, followed by one test of that individual. When practical, this ap- 
proach is attractive. 
M. Stone (1974) gives a generalized form ot a cross-validation criterion 
applied to the choice and assessment of statistical prediction. 
One drawback of all kinds of single cross-validation is that the test sample 
is all too often much more like the optimization sample than is typical of the 
population of individuals or situations to which we wish our indication to refer. 
Accordingly, single cross-validation is all too often weaker, by an unknown 
amount, than it appears to be. 
The possibility of cross-validation is one of the main advantages of most 
automatic schemes of optimization. Approximate, cut-and-try, judgment- 
guided selection of procedures can often come very close in quality ot5 result, or 
even be superior, to those that formally optimize, but because one cannot fully 
specify what the procedure is and how it chooses the results, one cannot be 
sure that it is being tested on "independent" data. On the other hand, at 
the simple cross-validation level, every procedure generated by automatic 
optimization is easily tagged with the body of data used to determine the 
numerical values of its coefficients. 
The difficulty with determination by subjective judgment recurs at the 
level ot double cross-validation. When we choose the form of a procedure, the 
full body of data that was used may not be clear to anyone. Often the choice 
leans, sometimes very usefully, on a lifetime of experience, fact, and folklore, 
as well as on the convenience of the specific analysis. The experienced 
investigator will be especially hard-pressed to behave "double blind" because 
the form chosen may have been picked with the idiosyncrasies of the sets of 
data yet to be gathered already in mind. Satisfactory cross-validation on these 
next few sets will verify the investigator's foresight and satisfy us about its use 
40 /2: Indication and indicators 
on that sort of data, but can over-assure us about the cross-validation for uses 
further into the future that may not have been considered. Obviously we are not 
discussing an issue of "bad" versus "good". The investigator wants to be sure 
to consider as far as possible the whole range of uses, and thus be able to 
expect different performances under different circumstances. 
Summary: Indication and Indicators 
Indicate means appear to show; sometimes indication is as far as we need carry 
an analysis; indication may be numerical or qualitative. 
To go beyond indication requires an assessment of uncertainty of the 
indication, preferably a trustworthy assessment. 
We must be concerned about problems of multiplicity, because having 1 
result out of 1000 results tested turn out nominally just significant at 5% 
means something quite different from having 1 of 1 do this. 
In selection problems, a well-designed experiment may be one that almost 
always produces only indications, and where statistical significance of the 
results as a whole is only rarely to be anticipated. 
We often deal with concealed inference, where no formalism and no 
arithmetic need be used by a skilled analyst of data to see that either (i) there is 
no doubt of significance, or (ii) there is no hope of significance. 
One way to judge the quality of indicators is to consider how well they 
estimate what they are supposed to estimate. 
While sometimes we have something in mind for an indicator to "esti- 
mate" when we pick the indicator, it is often better to ask the indicator, in 
terms of the answers it gives, what it is trying to estimate--what is its estimand. 
One way to do this is to ask what happens when the indicator is applied to 
larger and larger samples. 
Cross-validation is a natural route to an indication of the quality of any 
data-derived quantity, be it estimate or pure indication or something else. 
Cross-validation can be single (on data not used to fit numerical coeffi- 
cients) or double (on data not used either to choose coefficients or to choose 
the form of the analysis or fit). Double is always safer. 
A major value of automatic schemes of optimization, fitting, and so forth, 
is that they can be reliably cross-validated. 
We take seriously indications or indicators not taken further down the 
inference trail; we take them seriously but with a pinch of salt labelled 
"indication only". 
We plan to cross-validate carefully wherever we can. 
References 
Macdonald, N.J., and F. Ward (1963). "The prediction of geomagnetic 
disturbance indices: 1. The elimination of internally predictable variations." J. 
Geophys. Res., 68, 3351-3373. 
(2F, Summary) References 41 
McCarthy, P. J. (1976). "The use of balanced half-sample replication in 
cross-validation studies." J. Amer. Star. Assoc., 44, 596-604. 
Miller, R. G. (1962). "Statistical prediction by discriminant analysis."Meteorol. 
Monogr., 4(25), 1-54. 
Mosteller, F., and D. L. Wallace (1964). Inference and disputed authorship: The 
Federalist. Reading, Mass: Addison-Wesley. 
Stone, M. (1974). "Cross-validatory choice and assessment of statistical predic- 
tions." J. Roy. Star. Soc., Series B, 36, 111-147. 
Yates, F. (1950). "Recent applications of biometrical methods in genetics: 1. 
Experimental techniques in plant improvement." Biometrics, 6, 200-207 (espe- 
cially pp. 204-205). 
Yates, F. (1957). Personal communication (to J. W. Tukey). 
42 Index for Chapter 3 
3A Stem-and-leaf 43 
3B Medians, hinges, etc. 43 
3C Mids and spreads 48 
3D Subsampling 49 
3E Exploratory plotting 49 
3F Trends and running medians 52 
3G Smoothing nonlinear regressions 61 
3H Looking for patterns 64 
31 Residuals more generally 70 
3J Plotting and smoothing 75 
Summary: Batch summaries and displays 77 
References 77 
Chapter 3/Displays and 
Summaries for Batches 
3A. Stem-and-Leaf 
Quick ways to write down batches of numbers are useful, especially if they 
reveal a fair amount about patterns of distribution. So are ways that let us 
"count in" easily, as when we are asked to find the 15th value from each end. 
All of these things can be done with stem-and-leaf displays, though we may 
need to use two, or more, styles of display to do two things well enough. These 
displays go further than the classical tallying devices for producing histograms. 
Consider the 1971 preliminary figures for dollar value of mineral 
production in the United States, by states. Alabama produced 291.4 millions of 
dollars worth. This we shall break into three parts as follows: 
Stem Leaf Forget (millions) 
Ala. 2 9 14 (291.4) 
Alask. 3 3 28 (332.8) 
Ariz. 9 8 10 (981.0) 
Ark. 2 5 32 (253.2) 
Calif. 19 2 06 (1920.6) 
where we have included, as well, the data for the next four states in alphabetic 
order. The result, for these five states only, is the skeleton stem-and-leaf 
display shown in Panel A of Exhibit 1. 
A number of ways to ring the changes on stem-and-leaf displays are 
discussed in EDA,* Chapter 1. The only other versions we illustrate here are 
shown in Exhibit 2. 
3B. Medians, Hinges, etc. 
When values are arranged in increasing order, the middle one is called the 
median, as has previously been mentioned. If we have already written the 
values out in order, the median is easy to find. Given, for example, 25 values 
(counting ties, if any), the 13th from either end is the median, which we will 
call M. Given 24 values in all, the median is the 12-}th from either end. (Here 
we define 12�th as halfway between the 12th and 13th.) The general rule is 
depth of median = �(1 + batch size). 
* Exploratory Data Analysis, J. W. Tukey (Reading, Mass.: Addison-Wesley, 1977). 
43 
44 Exhibit 1/3: Displays and summaries 
We can also readily find values at prescribed depths once we have a stem- 
and-leaf display. Its cumulative column shows us that Panel B of Exhibit 2 
contains 76 + 5 + 74 = 155 values. The median value, which is the 78th from 
each end, is to be found by counting within 18117001 as if we had it written 
down as 
(76 lower) 
180 
180 M 
181 
181 
187 
(74 higher) 
Exhibit 1 of Chapter 3 
A stem-and-leaf for value of mineral production in U.S. by state, 1971 preliminary 
(1 in a leaf is 10 million) 
A) SKELETON STEM-AND-LEAF B) ALL FIFTY STATES 
stem leaf 17 0 20229571997066398 
1 211 1 1261 
2 95 8 2 95286893 
3 3 21 3 3948 
4 17 40 
5 16 5 82 
6 14 6 405 
7 11 7'01 
8 8 
98 9 9'82 
10 7 (H) (192, 555, 104, 118, 114, 680, 
11 127) 
12 
13 Notes 
14 1. The 0-stem-0-1eaf entries are for 2.241 millions 
in Delaware and 4.299 millions in Rhode Island. 
15 2. The lefthand column contains cumulative 
16 counts of the number of states, counting in 
from the top and from the bottom, except that 
17 the 8 at the stem of 2 is the number of states 
18 with that stem. 
19 2 3. The line labeled (H) gives the numbers larger 
than 99. 
Interpretation. 9 at 2 is 29, 5 at 2 is 25; thus Interpretation. 3948 at 3 is one each 33, 39, 34, 38. 
2195means29and25eachonce. 8at9means98, 95286893 at 2 is 22, 23, 25, 26, 28, 28, 29, 29. 
and 2 at 19 means 192. 
S) Source 
The World Almanac, 1973, page 423. 
3B: Medians, hinges, etc./Exhibit 2(A) 45 
where the 78th value from either end is the value marked M. We call the 
median M a letter value. M tells us about the center of this batch of numbers in 
a way not much influenced by either one or a few "outliers", values much 
smaller or larger than most of the other values in the distribution. (We make a 
more specific study of this in Section 14G, especially Exhibit 1.) 
Other letter values. We often gain by defining not only a value halfway from 
each end, but values that we count in a quarter of the way, an eighth of the 
way, and so on. These values tell us about the variability of our numbers and 
the shape of the distribution. An easy way to remember just what we are to do 
is to note that our rule for taking a step from one letter value to another looks 
Exhibit 2 of Chapter 3 
Splitting stems and covering two decades to display telephones/city for North 
American cities with at least 100,000 phones in 1972 (1 in a leaf is 10,000 phones) 
A) UNSPLIT STEMS, but CHANGE IN STEM WIDTH (using roughly half of the 
cases that appear in the first two columns of the source) 
0 
1' 39716928601374621181074803106613136144781053214 
2 5312173869 
3 0142 
4 83200 
5*4 
6 823 
7 4 ' 
8 71 
9*6 
1''06, 37, 
2 35, 
3 
4 85, 
.** 
Note 
1. Leaves in top portion are 1-digit but are 2-digit in lower portion. The single asterisks following the 
numbers 1, 5, and 9 actually apply to all the stems in the top portion and indicate that we are dealing 
with a 2-digit number, a one-digit stem and a one-digit leaf. But the asterisks are given for only 1, 5, 
and 9 to avoid clutter. Similarly, the two asterisks in the lower portion remind us that we are dealing 
with 3-digit numbers, a one-digit stem and a two-digit leaf, and again only the 1 and 5 have been 
marked. 
2. The first entry 1'13 stands for 130,000 phones. 
3. The first entry in the bottom portion 1'* 106 stands for 1,060,000 phones. 
46 Exhibit 2(B-S)/3: Displays and summaries 
Exhibit 2 of Chapter 3 (continued) 
B) SPLIT STEMS, et al. (one digit per leaf, everywhere) 
0* 
14 10 13787253637324 
28 11 72416415612651 
36 12 23953110 
46 3706140514 
54 53289349 one to tWO Notes 
60 357939 hundred 1. Three digits carried in top section, one less in 
bottom section. Thus 19 1 131 means two 
68 16 11737448 thousand 191's (191,000to 191,999 phones) and one 
76 17 23717516 193, while 7*141110 means one 74 
5 18 17001 1 in leaf (740,000 to 749,999 phones) three 71's, 
and one 70. Moreover 4'xl8 means 
74[ 19 131 is 1,000 one 48x (4,800,000 phones). 
71 2* 31213332003434 
57 2 57869888759 two to nine 2. Two lines of 2 ], first for 0 to 4, second for 5to 
46 3* 014204246148 hundred 9. 
34 4 832000 thousands 
28 5' 4305625 
21 6 8239 1 in leaf 
17 7* 41110 is 10,000 
12 8 717 
9 9*6 
8 1 *x 03205 one to five 3. Counting in from each end gives rise to the 
3 2*x 3 millions leftmost column. Thus, for example, 
there is one 5*x and nothing more ex- 
3*x treme, hence a count of 1. There is one 
2 4*x 8 1 in leaf 4*x, which makes a total of 2 at 400 or 
1 5*x 9 is 100,000 higher. And so on, from each end. Note 
further that we want to count in from 
either end no more than half the total. 
The asterisks have an interpretation as given in Part A, Note 1 of this exhibit. The 
0* at the top of the stem column makes a place for 5-digit numbers starting with 
a zero and continuing with a one-digit leaf and three unspecified digits. 
S) Source 
The World Almanac, 1973, page 419. 
3B: Medians, hinges, etc. 47 
much like the rule that relates the median to the total count. What we shall do 
is to use 
next letter depth = �(1 + previous letter depth*), 
where the * reminds us to throw away any excess over the next smaller integer. 
If the previous letter depth is 12�, we use 12, and the next letter depth is 
�(1 + 12)= 6�. 
We label the successive letter depths (after M for median) as H (for hinge, or 
use Q for quarter if you prefer), E (for eighth), and then D, C, B, A, Z, Y, X, 
W... (for inverse alphabetic labeling). 
For our telephone example, we have 
Batch Count 155 Calculation 
M depth 78 78 = -}(1 + 155) 
H depth 39� 39-} -- -}(1 + 78) 
E depth 20 20 = -}(1 + 39) 
D depth 10-} 10� = -}(1 + 20) 
etc. 
Note the use of 1 + 39 and not 1 + 39� at depth E. 
Next we look up the 78th (already done), 39�th, 20th, and 10�th from each 
end. Where is the 39�th from the bottom? The 36th is 129, the highest value 
for stem 12, and the next 10 go 130, 130, 131, 131, 133, 134, 134, 135, 136, 
137. The 39�th will be halfway between the 39th and the 40th, or the average 
of the 3rd and 4th above the 36th, hence 131. 
We also want the 39�th highest. There are 34 occurrences of 40 or more, 
and moving down shows us 38, 36, 34, 34, 34, 34, 32, 32, 31, 31, 30, 30, 
where we need to go halfway between the third and fourth 34 to reach 34 at 
depth 39� (really 340,000 to 349,999). .. 
Depth 20 is fairly easy to find. We have 
(21st) 62 
(20th) 63 
(19th) 68 
(18th) 69 
(17 larger) 
so that the 20th from the top is 63. 
Exhibit 3 shows a simple standard form that includes the depth asked for 
so that these can be checked if desired. 
Even if we had done no more than write down the letter values, as in 
Exhibit 3, our results would show something of the shape of distribution of 
these 155 numbers. At the upper end, the letter values keep on rising rapidly 
as we go from E to D to C and on. At the lower end, the corresponding letter 
values hardly move at all. Clearly, the distribution of this batch of 155 is quite 
skew. 
48 Exhibit 3/3: Displays and summaries for batches 
3C. Mids and Spreads 
If we want to work further with our letter values, convenient statistics, based 
on pairs of letter values at the same depth, are their means (here also medians) 
and the differences of such pairs. Thus, at depth D, where we had 
87x and 106�, 
we now go on to find 
48x = �(87x + 106�) (the "mid") 
and 
76x = 87x - 106� (the "spread"). 
The spread calculations tell us about the "spread" of numbers about the 
median. We can make such calculations for any letter. Ways of using these 
further will be found in EDA, Chapter 19. 
When, as in Exhibit 21, Panel J, of Chapter 9, we wish to give the spreads 
as part of the letter-value display, we write our values down like this: 
# 
M 78 180 spread 
H 39� 34x 131 209 
E 20 63x 112 527 
D 10� 87x 106� 767 
Exhibit 3 of Chapter 3 
Skeleton letter-value display for the data of Exhibit 2 
Letter values 
# 155 (in thousands) 
From From 
Depths top bottom 
M 78 180 
H 39� 34x 131 
E 20 63x 112 
D 10� 87x 106� 
c 5� 125x 103 
B 3 23xx 102 
A 2 48xx 102 
1 
Z 1 �35x 101� 
Y 1 59xx 101 
Here x stands for an unspecified digit. The actual numbers are 340 thousand for 34x, 639 thousand for 
63x, 873.5 thousand for 87x, and so on, as can be checked by going to the source. 
(3C, 3D) 3E: Exploratory plotting 49 
The spreads shown were obtained from the original numbers in the source, 
rather than the abbreviated ones using the x in the third digit. 
3D. Subsampling 
Sometimes we have such large sets of data that sheer bulk stands in the way of 
careful analysis. In such straits, which have their pleasant side, subsampling, or 
even sequences of subsamples together with successive analyses, may be both 
economical and instructive. Section 12H discusses this for regression, but the 
idea is equally applicable for other techniques, beginning with stem-and-leaf 
analysis. 
3E. Exploratory Plotting 
If we want to diagnose relations further, we may be aided by graphical work. In 
this section we describe a flexible approach that may be profitable when we 
have a substantial amount of data. The general idea is to obtain a fairly smooth 
regression relating the response variable y to its matched x-values without 
making strong assumptions about the form of the relation. 
When we are lucky enough to have a very large number of data points, we 
may not wish to plot them all, and subsampling may be useful. Sometimes 
other omissions may be more helpful. 
For 20 to, say, 400 data sets, a good shortcut is to plot the k points with 
the highest y's and the k points with the lowest y's, where 
k = the larger of 10 and x/ 
where n is the number of data sets. 
For n > 400, k will be over 20, and we wou!d like to cut it down a little. 
Starting with the largest k and smallest k values and then taking about 10 out 
of each of the two sets of k, roughly uniformly spread by y value, may be 
effective. 
The 1962 County and City Data Book provides information for 88 
unincorporated urban places (with populations of at least 25,000), including 
the 1959 median family income for each. Exhibit 4 gives a variety of 
information for (a) the 10 places (of 88) with the highest family income and (b) 
the 10 places with the lowest family income. Exhibit 5 shows plots, for these 20 
places, of family income against 4 of the 8 quantities given in Exhibit 4. 
The plot against median age shows strong dependence, and a few strays, 
which have been identified. The plot against percent using public 
transportation may surprise us, if it means, as it seems to indicate, that the 
more affluent use more public transportation. The plot against percent in the 
same house after 5 years shows a modest relation, with one New Jersey 
extreme stray. The plot against percent of housing units in single-unit 
structures shows at most a weak relationship. 
50 Exhibit 4(A)/3: Displays and summaries 
Exhibit' 4 of Chapter 3 
The 10 urban unincorporated places, 1960, with highest and 10 with lowest 
incomes, 1959, for families living there in 1960. 
A) Headings are column numbers in County and City Data Book, 1962, Table 
5, pages 468-475. 
Median 
family 
Place income (212)(229)(244)(246)(248)(249)(256)(264) 
New Hanover, N.J. 4572 21.8 1.1 42.2 0.4 4.7 78.8 82.2 24.1 
Florence-Graham, Calif. 4904 25.7 40.2 17.1 16.8 3.9 86.6 41.2 1.3 
Kannapolis, N.C. 5182 29.0 54.5 20.4 6.1 4.6 96.7 27.1 6.5 
Brownsville, Fla. 5306 22.6 35.3 39.1 3.2 4.6 93.5 46.4 26.0 
East Los Angeles, Calif. 5439 25.1 44.8 26.7 19.7 4.2 79.8 37.1 6.0 
Bell Gardens, Calif. 5567 24.4 26..1 67.6 1.5 3.8 89.8 59.6 30.3 
Hempfield, Pa. 5909 29.3 58.4 37.0 5.3 5.2 95.0 24.9 3.0 
South San Gabriel, Calif. 6076 29.3 40.9 36.9 6.3 4.3 90.1 41.5 10.4 
Essex, Md. 6160 24.8 46.7 34.5 6.7 5.4 78.5 37.5 12.0 
Methuen, Mass. 6278 34.6 63.1 38.2 6.7 5.3 72.6 19.9 3.6 
Needham, Mass. 9282 32.5 56.0 69.3 14.0 6.2 92.3 22.5 11.2 
Teaneck, N.J. 9518 33.0 63.0 62.9 29.4 6.1 80.6 17.9 26.9 
Silver Springs, Md. 9540 31.7 48.6 76.2 11.0 5.7 68.9 33.7 40.6 
Greenwich, Conn. 9588 35.6 55.8 54.5 9.7 5.8 69.9 20.7 11.6 
West Hartford, Conn. 9712 37.4 50.4 72.1 13.5 6.2 79.8 22.1 16.0 
Cheltenham, Pa. 9985 36.6 57.9 75.0 24.0 6.5 73.5 23.8 41.8 
Mount Lebanon, Pa. 11108 36.9 49.1 62.8 25.5 6.2 81.4 25.4 14.0 
Wellesley, Mass. 11478 31.3 52.4 70.8 15.4 6.7 94.7 23.2 9.8 
Lower Merion, Pa. 12204 32.6 57.4 69.0 18.8 7.1 78.2 21.2 37.9 
Bethesda, Md. 12357 31.4 36.3 82.6 8.5 6.5 82.3 37.4 41.7 
I) Column identification 
(212) = median age, 
(229) = % in same houses, 1955 and 1960, 
(244) = % in white collar occupations, 
(246) = % using public transportation, 
(248) = median rooms/unit, 
(249) = % units in one-unit structures, 
(256) = % moved in, 1958 to 1960, 
(264) = % with air conditioning. 
3E: Exploratory plotting/Exhibit 5 51 
Exhibit 5 of Chapter 3 
Plots, for the 10-PLUS-10 places with high or low family income, of family 
income against each of four other quantities 
Median ;ncome Medrn ;ncome 
(dollars) (do lars) 
Ig, 00 - Bethesda, Md.. � 12,00 - 
/ellesljt He. � 
IO, 000 - [0,000 - 
� � 
� � � � 
g,000-  ' efhuen, Ma. 6,000- '' 
� � 
� � ' � as- L,A., Ca. 
� � � � 
� � Florence-raharrl, 
,ooo- 4,000 - 
(dol lets) 
12 000 - ' 12 000 
I 
� � 
lO, 000 - . !0,000 - . 
� � 
� � � � � 
,ooo I t > $,ooo,  t � t 
o o o 70 8 90 
'% Same l'use 5..yeas % Housing rl single units 
6,000 - * , 6/000 - ' 
� 
� 
� � � � � 
� � 
� 
� 
tew Ianover I. Y. 
4,000 - 4,000 - 
52 /3: Displays and summaries for batches 
We have explored only a few of the 67 columns of data given in our 
source. Even so, we can see that, if we wished to describe median family 
income in terms of other variables, we would have a variety of places to begin. 
Let us begin with median age, where the median family income might rise 
$300 for each year of median age. Once we know this, the y we ought to 
consider is changed, for we have prepared to divide median family income into 
two parts 
fit PLUS residual. 
Here 
fit - ($300)(median age in years), 
and so fit has a clear description. The second part is 
residual = (median family income) - ($300)(median age in years) 
which remains undescribed and ought to serve us as our new y for further 
exploration. 
It would be attractive to calculate all 88 values of the new y and pick out a 
new 10 highest and a new 10 lowest. When the data are in a computer where 
this is easy, we gladly do this. Otherwise, we might use the same 10 high and 
10 low for two or three steps of exploration and then recompute. 
3F. Trends and Running Medians 
In relating a response variable y to its matched x value, we may want to know 
the shape of the relationship primarily for its own sake, or for a further analysis 
of departures from the observed regression line, of residuals. Sometimes the 
values of x form a grid of equally spaced points. But we may have other 
circumstances as well. When we have no very strong views about the form of 
the relation and do not expect it to be linear, many procedures suggest 
themselves. Among these are: 
1. drawing a freehand curve through the points to give an eye-satisfying fit; 
2. chopping up the x-axis into chunks and taking medians or means of 
y-values and of x-values within chunks, and then somehow passing a curve 
near the resulting points, and 
3. using running means or medians. 
In this section we discuss running medians, and in later sections we discuss 
other possibilities. 
Let us orient our discussion toward time, but only to fix ideas, not to 
restrict the use of the method. Let us suppose that an observation is composed 
of two parts, the regression curve we wish we knew and an independent error. 
3F: Trends and running medians 53 
Thus the response at time t (= 1, 2, 3,..., T) can be represented as 
response = regression + error, 
or, if f(t) is the value of the regression at time t, we can write more formally 
y (t) = f(t) + errort. 
If the error is substantial compared to the variation in f, then it is tempting 
to estimate f(t), not just by y(t), but by the average of the y's in the 
neighborhood of t. This average gives a better estimate of [(t) than does y(t) 
alone, provided the errors are not much correlated with one another. Indeed, 
to the extent that the values of f(t) fall along a straight line, not necessarily a 
horizontal one, the arithmetic average of values centered on t is an unbiased 
estimate of f(t). We have then a strong urge to average results for several 
response values of time near and centered at t. Against this, insofar as f(t) is 
not linear, we are, through this averaging or smoothing as we shall call it, 
muddling up the regression. If we average three adjacent values, y(t - 1), y(t), 
y(t + 1), then we are estimating 
f(t- 1) + f(t) + f(t + 1) 
instead of f(t). 
3 
We must not extend the number of points being used too far because, if we do, 
we will gradually lose the character of the function we are trying to estimate. 
Such averaging has been widely used in time-series work. Naturally, many 
sorts of averages are used, often with unequal weights. In this section, we 
emphasize running medians where three adjacent values of t are employed. 
Our primary purpose here is to give some idea of what happens when such 
smoothing is used. In a later section, we use such smoothing on a real problem. 
We are not especially recommending the particular brand of smoothing 
described here, and other better methods are available in EDA, Chapters 7 
and 16, and in Velleman (1975), to which we refer the reader. 
The two main things we plan to illustrate in this section are what happens 
when running medians are applied to the simplest ideal situation, and what 
happens when they are applied in more complicated situations. We hope that 
not a great deal is lost when they are applied, and we hope that they will 
respond sensitively in circumstances where the regression function, f(t), is 
curved. or jumps. In the course of exploring these issues through examples, 
we also see special features that we need to be aware of. 
Running medians. We plan to compute running medians and then to continue 
computing them on the resulting sequence of medians until they stabilize 
(repeat themselves completely). We illustrate this with adjacent sets of three, 
but for any odd number we compute in a similar manner. (For even numbers, 
recall that we take the average of the two most middling values of y in a set: 
thus, the median of 7, 2, 10, 3 is (7 + 3)/2 = 5.) 
54 /3: Displays and summaries for batches 
Example.. Computing running medians. Exhibit 6 shows 20 observations, one 
associated with each of the times 1, 2,..., 20. These happen to be random 
normal deviates drawn from a random-number table. Indeed, we secretly know 
that their population mean is 0 and their variance 1. Accordingly, for the 
process generating these y's, f(t)= 0 for all 20 values of t. Thus, we are 
dealing with a situation where the true regression is a straight horizontal line. 
In a practical situation we would rarely know all this. 
We plan to smooth by running medians and then to fit a least-squares line 
to the resulting set of smoothed values. Then we will compare the result with 
what would have been obtained if we had directly fitted a least-squares straight 
line to the data, as we would if we believed in linearity and normality but did 
not know the slope .and 1�v�1 of the line. Finally, we will be using these same 
data twice more in illustrating what happens when we fit in situations where the 
underlying regression is ar from linear. 
In Exhibit 6 the first column gives the time period and the second the 
associated values o y. Since we have to do something about the first position, 
we merely recopy it in taking the running median of three. Then we take the 
values for time 1, 2, and 3 and take their median (the median of-0.423, 
-0.602, and 1.703 is -0.423). This is written on the second line of the third 
column, and then running medians of each successive set o three are given on 
successive lines, until the line for t = 20, where again we copy the original 
number. This process is repeated until no more changes occur. 
We can shorten the writing; it is enough to write down only the changing 
values rather than every value. Then the 0.401 of line t -- 9 would be written 
only in the y(t) column. The final smoothed results would be given by the 
rightmost number on each line. 
Exhibit 7 shows plots of both the original set of data and the smoothed 
result. As one would expect, the smoothed data vary less than the raw. One 
way to think about smoothness is to count turning points. Given three 
successive numbers, if the middle one is largest or smallest, it is called a turning 
point. In a random sequence of length n, the expected number of turning 
points is (n - 2), assuming that each point in the triplet has a distinct y-value. 
(Out of the 6 possible orders of three different numbers, 4 have their largest or 
smallest number in the middle; therefore the probability of a turning point in a 
set of three is - = -. The -2 in the formula comes from end effects.) 
In sample No. l's raw data we find 10 instead ot the expected 12. In the 
smoothed numbers we have to deal with ties, adjacent ties, but it seems 
reasonable to say that there are only 3 turning points, if we interpret them as 
local maxima or minima, corresponding to positions 4, 5, to 15, 16, and to 17, 
18, 19. (In these fiat spots, the issue is not the exact location of the turning 
point with respect to t. What we care about is that a turn is made.) 
Exhibit 8 shows the results of smoothing for the first ten random samples 
of size 20 that we drew. They give some notion o the variability that actually 
3F: Trends and running medians/Exhibit 6 55 
occurs in the smoothed running medians. We secretly know that what we 
would like to see is a flat curve running along the horizontal axis, as the 
estimate of [. We see a warning in Sample No. 4, where the rise at the 
beginning and the drop at the end are not real in the original function [, though 
we might mistakenly tend to believe them because the rest of the points do 
hover so close to the horizontal axis. 
Exhibit 6 of Chapter 3 
Smoothing a set of 20 response values by repeatedly computing running 
medians of three adjacent values. These y(t) are Sample No. I of 10 samples of 
size 20. 
Running Running medians 
medians of 3 medians 
y(t) of 3 
random random 2nd 
normal normal 1 st iteration 
bJ deviates deviates iteration y' 
1 -.423 -.423 -.423 -.423 
2 -.602 -.423 -.423 -,423 
3 1.703 1.703 1,703 1.703 
4 1,887 1.887 1.887 1.887 
5 2.049 1,887 1.887 1.887 
6 1,127 1.127 1.127 1.127 
7 ,651 .651 .651 ,651 
8 -.836 ,401 .401 .401 
9 .401 .401 .401 .401 
10 .906 .410 .410 .410 
11 .410 .410 .410 .410 
12 .221 .410 ,410 .410 
13 ,968 .426 .426 ,426 
14 .426 .426 ,426 .426 
15 -.844 -.844 -.844 -.844 
16 - 1.290 -.844 -.844 -.844 
17 .657 .063 .063 .063 
18 .063 .657 .063 .063 
19 1.283 .063 .063 .063 
20 - 1.563 - 1.563 - 1.563 - 1.563 
S) Source 
Rand Corporation (1955). A Million Random Digits with 100,000 Normal Deviates. New York: The Free 
Press. Page 154 in Table of Gaussian Deviates; first 20 values from first 4 columns. 
56 Exhibit 7/3: Displays and summaries 
In sex?eral of the examples we see flat tops or bottoms of length 2. For 
example, in Sample No. 1 the opening two points have a flat bottom of length 
one unit and there is another flat bottom between 15 and 16. This is a common 
feature of running medians of length three that often bothers us. Other 
methods in the reference already given (EDA, Chapter 7) take steps toward 
removing this feature. 
Exhibit 7 of Chapter 3 
Comparison of the raw data of Sample No. I and the data smoothed by repeated 
running medians of 3. 
.y 
� 
0 i I I I l_ I I I I I i J i I I I ?  T I ' t 
I I ! I I I � � I I I I ] I I I I 
5 I0 15 20 
-0,5-  ' 
-0,5' - 
MOOTNEI) DATA 
-2.0 - 
Exhibit 8 of Chapter 3 
Smoothed plots of 10 samples of size 20 random normal deviates equally spaced 
horizontally. (The small x's indicate where a least-squares line fitted to the data 
passes.) 
I- I '1 ! '1 I I I I ! I I I I i ! I I I I ') 
.- 
0- : $arnple I 
Sample ?_. 
Sample 3 
t ample 4 
_ X Sample 5 
x Sample 8 
I- 
O--  : 5ample IO 
l_. I I I I I I I I I I I I I I I I I I I >, 
I 5' I0 ly o 
58 /3: Displays and summaries for batehes 
In Sample No. 4, the rise at the beginning and fall at the end are 
impressive to the eye, the more so because of the flatness from t = 2 to 
t=18. 
Similarly, the hump in the middle of Sample No. 5 stands out because it 
disturbs an otherwise eye-soothing smoothness. 
Sample No. 7 has an impressive drop at the beginning and a sag at the 
end, almost compelling one to believe in the reality of these effects in the 
process generating the data. 
Samples No. 2, 6, and 10 have a rather sinusoidal character. 
Let us remember that all these results are supposed to be estimating points 
on the horizontal axis. And so the message is that indications of shape in 
smoothed data must be viewed with reserve. When we look back at the raw 
data in Exhibit 7, we are not much impressed with any feature of it because the 
variability of the points is substantial. It gives the appearance of trending 
downhill from left to right, but the three high points in the upper left and the 
two low ones in positions 15 and 16 don't impress us much because of the 
considerable overall variability. When we look at the smoothed data in Exhibit 
7, we are more likely to be impressed with the bulge up on the left and the dip 
at 15 and 16, because the basic variation in the points is much less. 
Consequently one must be restrained in interpreting shapes from smoothed 
curves, because the restraints imposed by observing natural variability are no 
longer visually available, as we now explain. 
Running medians give rather lumpy looking resultsoften a little more 
lumpy than running means. Since a particular piece of input data affects only a 
few values of a running median (or running mean), if the input data bobbles up 
and down around some value, we expect its running medians to do the same. 
But they do not do this as irregularly as the data. When a running median of 3, 
for instance, is quite high, this is because two of the three values are at least 
correspondingly high. Often these two will be adjacent to each other. When 
this happens, at least two successive running medians of 3 will have to be high. 
Thus, high values tend to come in pairs (or triples, or more). 
This is particularly noticeable when we use repeated medians of 3, for if an 
early medianing leaves an isolated high value, the next medianing will 
eliminate it. Thus the peaks and valleys left after repeated smoothing by 
medians tend to be rather distinct lumps. Essentially, the high-frequency noise 
has been smoothed out, leaving the low-frequency noise behind, and looking 
more impressive. 
Fitting straight lines. Next, let us see what happens when we fit straight lines 
by equally weighted least squares (ordinary least squares) to both the raw 
samples and the smoothed data. As an example, for Sample No. 1, 
Uricentered form Centered form 
raw aata: � - -0.0588x + 0.9776 - -.0588(x - 10.5) + 0.360, 
smoothed data: y' -- -0.0834x + 1.187 - -.0834(x - 10.5) + 0.311. 
3F: Trends and running medians/Exhibits 9 and 10 59 
Exhibit 9 shows the (slope, centered intercept) pairs for all ten samples, and 
Exhibit 10 shows the same results graphically. (We use the centered intercept 
since, in this ideal situation, it is distributed independently of the slope when 
we use the raw values.) Theory says the distribution of the smoothed results 
Exhibit 9 of Chapter 3 
Comparisons of slopes and intercepts of lines fitted to raw data and smoothed 
data from Exhibit 8. 
Centered intercept 
I Slope I (at t= 10.5) 
I Sampel I Raw ! [Smoothed t Raw ! l Smoothed ! 
1 -.0588 -.0834 .360 .311 
2 .0054 .0138 -.148 -.148 
3 -.0498 -.0306 -. 188 -. 184 
4 -.0352 -.0082 -. 125 -.255 
5 -.0054 -.0133 .195 .373 
6 -.0011 .0052 -.136 -.226 
7 -.0720 -.0753 -.590 -.486 
8 .0373 .0256 .120 -.030 
9 .0505 .0448 .140 .095 
10 .0154 .0209 -.059 -. 144 
Ideal 0.0 0.0 0.0 0.0 
Exhibit 10 of Chapter 3 
Graphical comparison of slopes and intercepts for raw and smoothed data. 
-qmooh Srnoot:he 
0.08- / 0,8 
/ / 
/ 
/ 0. - / 
0,06- / C[:NTER 
SLOPE5 
/ 0,4 / 
0,04- / . / 
O, OZ ,/ O,Z. // 
-0,08-0.0-o, 04-0.0a ,/o.oz 0.04 0.06 0.08 -0,8 -0.6-0,4 -0, //'0; 0.4 0. 0,8 
I I I t / 
, , 
� 
24o-- f --o.z 
/ � 
/ -0,04- / --0,4 
/ 
/ � / 
/ 
/ -O.Ol - / . 
/ / 
/ / 
/ , -0,08- - / - --0.8 
60 Exhibit 11/3: Displays and summaries 
Exhibit'11 of Chapter 3 
Stem-and-leaf comparing raw and smoothed slopes and centered intercepts in 
terms of both values and sizes. 
A) VALUES 
Slopes Centered intercepts 
Raw ! I Smoothed I { Raw !Smoothedl 
.06 
0 .05 
.04 4 6 17 
7 .03 
.02 50 924 
5 .01 3 9 
5 .00 5 --- 
5 3 
15 -.00 8 3284 448 
-.01 3 25 
-.02 
5 -.03 0 8 
9 -.04 9 
8 -.05 
-.06 
2-.07 5 
-.08 3 
B) SIZES 
l Slop es 
IRawi ,ISmoothedj Raw i JSm��thedl 
.08 3 .6 
2 .07 5 9 .5 
.06 .4 8 
80 .05 6 .3 17 
9 .04 4 .2 25 
57 .03 0 3284924 .1 448 
.02 50 5 .0 
5 .01 33 
155 .00 58 
(3F) 3G: Smoothing nonlinear regressions 61 
should be slightly more variable than those for the raw values. Incidentally, in 
Exhibit 8 the small crosses at each end of each figure and in the middle show 
where the least-squares line fitted to the smoothed points would pass. 
Finally, Exhibit 11 shows the stem-and-leaf diagrams for slope and for 
centered intercept, giving a direct comparison of the observed marginal 
distributions, and we see again that the variability of the results is much the 
same, even though theory tells us that the raw fit is preferable in this ideal 
situation (though not when the data are far from Gaussian). 
This completes our illustration of what smoothing does in an ideal null 
situation, that is, one with no trend and with identically normally distributed 
error at each point. We turn now to illustrations where the regression curve [ is 
nonlinear. 
3G. Smoothing Nonlinear Regressions 
Next we illustrate the smoothing of nonlinear regressions where the variation 
in the random component is nonnegligible, but not overwhelming. Exhibit 12 
shows a regression function f with its 20 points connected by a dotted line. 
When the curve oscillates rapidly, we cannot expect to pick up these 
oscillations if our grid of t values is coarse. Furthermore, if we smooth our 
data, we will to some extent smooth the basic function f. Exhibit 12 shows, as 
the heavy line, the smoothed function f* using running medians of length 3 
(smoothed until they repeat). It is this running median function f* that we will 
actually be looking toward rather than the original function, when we smooth 
data in which f is perturbed by random errors. To repeat, smoothing the data 
smooths the function being estimated as well. Ess. entially it has smoothed out 
two turning points and lost what might be the scientifically important jag down 
at t = 6 and the spike at t = 7. Smoothing, then, can smooth away important 
effects. 
Exhibit 13 shows what happens when the unsmoothed normally 
distributed errors in the samples of Section 3F are added to f, and then 
running medians are computed for the ten samples. 
Except for Sample Nos. 5, 6, 7, and 8, the general shapes of the smoothed 
data agree with that of the smoothed regression function f* shown in Exhibit 
12. These four graphs smoothed away the smaller real hump at the left. All the 
smoothed results have trouble displaying the roundness of the righthand end. 
By comparing the graphs of Exhibit 8 with the f* function in Exhibit 12, 
one can, to some extent, forecast what happens in Exhibit 13. For example, 
in Exhibit 8 the initial drops in Sample Nos. 5, 6, 7, and 8 tend to destroy 
the hump of Exhibit 12. 
62 Exhibits 12 and 13/3: Displays and summaries 
Exhibit '12 of Chapter 3 
The regression function f, shown dotted, and its smoothed version, , 
smoothed by running medians of length 3. Note that f =  except at t = 5, 6, 
7. 
o 5' io t5 20 
Exhibit 13 of Chapter 3 
The smoothed data when the unsmoothed normally-distributed errors of the 
samples of Section 3F are added to the irregular function f shown in Exhibit 12 
and then smoothed by repeated running medians of length 3. 
j/ Y 
$AMPL c i SAMPLF Z 
4- ,4- 
0 5 10 15 20 0 ' IO 15' 20 
3G: Smoothing nonlinear regressions/Exhibit 13(continued) 63 
Exhibit 13 of Chapter 3 (continued) 
SAMPL 3 J SAMPLe' 4 
-'o .. I I ,,._ . 
$ t0 15 0 0 5' 10 15 0  
y ..Y 
S'.AAAPL 5  SAMPLE 6 
4.- 4.- 
,- 
-Z t,tL,,J.rtt t It tttlttttl- t I ,  J  It t t I I>- 
O ' rO I ZO o 5 IO I 20 
.y .Y 
SAA/1PL[' 7 SAMPLL:' 8 
64 Exhibit 13(concluded)/3: Displays and summaries 
3H. Looking for Patterns 
When we examine departures from the observed regression line, the residuals, 
we have many ways to detect patterns. Trends, bends, oscillations, and wedge 
shapes (pictured shortly) may be directly interpretable. We may want to 
re-express to eliminate the wedge. Perhaps patterns of parallel diagonal bars 
may reflect concentration on "permitted" numbers, or perhaps they reflect only 
a curiosity. 
Sections 3F and 3G discussed the use of running medians to see 
relationships and deviations for linear and nonlinear regressions. This section 
explains another means of detecting patterns, the procedure suggested at the 
beginning of Section 3F, namely, passing a curve through the means or 
medians of the chunks of x-values and of y-values resulting from chopping up 
the x-axis. 
To illustrate a number of these points, we study empirically some counts 
related to a famous mathematical problem that many mathematicians have 
worked on, among them L. Euler and G. H. Hardy. Hardy's work (1906) 
investigates the counts we describe. 
The Goldbach counts. A deep problem in mathematics, still unsolved after 
more than 200 years, is the Goldbach Conjecture. It states that every even 
number, starting with 6, can be represented as the sum of two odd primes 
Exhibit 13 of Chapter 3 (concluded) 
3H: Looking for patterns 65 
(6 = 3 + 3). (A prime is an integer that can be divided exactly only by itself 
and 1. The number 1 itself is not counted as a prime.) Much progress has been 
made on this conjecture. However, what we will think about is not its proof, 
but the number o[ ways it is possible to represent an even number, E, as the 
sum of two odd primes (including 0 as a possible count). We call this number 
the Goldbach count. Examples are: 
20 = 3 + 17 = 7 + 13 (Goldbach count = 2) 
30 = 7 + 23 = 11 + 19 = 13 + 17 (Goldbach count = 3) 
68 = 7 q- 61 = 31 q- 37 (Goldbach count = 2) 
Exhibit 14 displays selected patches of Goldbach counts for the even numbers 
� from 2 to 508 and from 9500 to 10,000. The even numbers are listed by tens 
under the heading T; and under T + 0, T + 2,..., T + 8, the Goldbach 
counts for the corresponding even numbers are given. 
In Exhibit 15 we plot the Goldbach counts against half the even number to 
be represented, to see what sort of pattern emerges. Thus the number we plot 
..horizontally is n - �E. 
First we see a general trend up and to the right. 
Second we see the counts spreading vertically as we move to the right, 
forming a wedge shape. We might ask whether we could transform the 
..numbers so as to stabilize the variability, but we set this aside for the moment 
in favor of searching for other patterns. 
We also see that some values of N have particularly low counts and others 
particularly high ones as we move to the right. Can we detect a pattern in this? 
We would do well to get a regression line or curve to act as a base line. Then 
:.we can look hard at deviations from the line. We can choose many ways to 
'make clumps and get average horizontal values and average vertical values. We 
could take successive sets of 10 values of N: 0 to 9, 10 to 19, and so on, and 
� Center the horizontal value at the midpoint. As our vertical coordinate, we 
i. Could take the median or the mean of the y-values, here Goldbach counts. We 
":.Will call such points the centroids, though in physics that term would be used 
;Only for the point whose coordinates are the mean of the x's and the mean 
of the y's. The observations seem fairly well behaved in this region of N, and 
so let us take the arithmetic mean within each set. The points corresponding to 
.:the midpoint for N and the mean for y, the Goldbach count, are marked with a 
Cross on Exhibit 15. The crosses are connected by line segments to give a crude 
regression curve. 
::":" The advantage of this crude regression is that it gives us a way of 
separating large counts from small ones. Until we get as far right as about 
....N = 36, the large values do not stand out, but starting about here they do. 
And they tend to form a recognizable pattern in the graph. Every third point 
� has a high count. 
66 Exhibit 14/3: Displays and summaries 
Exhibit 14 of Chapter 3 
Goldbach counts for selected even numbers E, where E = T + O, T + 2,..., T + 8. 
T'T + 0 2 4 6 8 T'T + 0 2 4 6 8 
0 ,, 0 0 I I 250 9 16 9 8 14 
10 2 I 2 2 2 260 10 9 16 8 9 
20 2 3 3 3 2 270 19 7 11 16 7 
30 3 2 4 4 2 280 14 16 8 12 17 
40 3 4 3 4 5 290 10 8 19 8 11 
50 4 3 5 3 4 300 21 9 10 15 8 
60 6 3 5 6 2 310 12 17 9 10 15 
70 5 6 5 5 7 320 11 11 20 7 10 
80 4 5 8 5 4 330 24 6 11 19 9 
90 9 4 5 7 3 340 13 17 10 9 16 
100 6 8 5 6 8 350 13 10 20 9 10 
110 6 7 10 6 6 360 22 8 14 18 8 
120 12 4 5 10 3 370 14 18 10 11 22 
130 7 9 6 5 8 380 13 10 19 12 9 
140 7 8 11 6 5 390 27 11 11 21 7 
150 12 4 8 11 5 400 14 17 11 13 20 
160 8 10 5 6 13 410 13 11 21 10 11 
170 9 6 11 7 7 420 30 11 12 21 9 
180 14 6 8 13 5 430 14 19 13 11 21 
190 8 11 7 9 13 440 14 13 21 12 13 
200 8 9 14 7 7 450 27 12 12 24 9 
210 19 6 8 13 7 460 16 28 12 13 24 
220 9 11 7 7 12 470 15 13 23 14 11 
230 9 7 15 9 9 480 29 11 14 23 9 
240 18 8 9 16 6 490 19 22 13 13 23 
500 13 15 27 15 14 
T'T + 0 2 4 6 8 T'T + 0 2 4 6 8 
9500 135 100 209 117 97 9750 286 108 99 194 124 
9510 253 97 105 211 86 9760 135 185 98 90 219 
9520 170 199 101 109 188 9770 132 117 191 108 99 
9530 123 94 232 97 104 9780 260 103 99 230 98 
9540 257 104 103 202 120 9790 151 205 102 110 200 
9550 130 199 104 102 193 9800 147 118 205 95 101 
9560 123 121 191 94 110 9810 258 101 118 206 94 
9570 295 97 98 245 90 9820 135 206 98 100 259' 
9580 132 194 93 94 199 9830 124 98 219 96 91 
9590 159 109 221 96 97 9840 264 129 104 196 100 
3H: Looking for patterns/Exhibit 14(continued) 67 
Looking more carefully, we see that 
39, 42, 45, 48, 51, 54, 57, andso on 
all have values above the line, and so we have detected a pattern. N's divisible 
by 3 seem to have larger Goldbach counts than those that are not divisible by 
3. At any rate, they fall above the fitted regression. 
Now that we have this hypothesis, we could look back and see whether we 
can detect it for earlier N. Note that 30, 27, 24, and 21 are all above the line 
segments we have fitted. 
Admittedly, sometimes N's not divisible by 3 also have high Goldbach 
counts. They deserve exploration, but now that we have had success with large 
counts, let us look for especially small counts. One way to search might be to 
set aside the N's divisible by 3, fit a new curve, and then look carefully at the 
large and the small counts. The next exhibit does this. 
Exhibit 16 shows a much more homogeneous set of vertical values than 
does Exhibit 15, still rising as we go to the right, but the vertical variation is 
much reduced. Can we detect any further pattern in the remaining points? 
We might look again to see which ones are much above and which below a 
new regression curve and consider these. We take successive sets of 9 N's for 
this. Each set of 9 N's has 6 counts left. We use these to form the crude 
regression as before. 
Exhibit 14 of Chapter 3 (continued) 
9600 261 77 127 197 91 9850 130 195 103 130 202 
9610 137 190 117 93 234 9860 144 104 204 86 99 
9620 135 100 194 93 106 9870 316 102 104 208 110 
9630 264 112 97 212 91 9880 156 200 117 101 196 
9640 126 191 93 124 202 9890 146 103 214 96 118 
9650 123 105 192 99 114 9900 301 98 102 211 94 
9660 324 101 110 194 97 9910 134 233 100 109 223 
9670 140 220 119 98 191 9920 141 112 200 122 108 
9680 140 101 197 96 118 9930 266 105 103 202 103 
9690 284 93 101 193 106 9940 162 200 113 113 196 
9700 121 254 98 104 184 9950 126 95 248 98 105 
9710 134 99 192 117 102 9960 269 113 99 217 120 
9720 254 96 128 195 103 9970 139 194 93 104 195 
9730 161 193 103 101 196 9980 136 135 211 103 110 
9740 133 101 235 113 93 9990 269 102 98 255 99 
10000 127 
S) Source 
Frederick Mosteller (1972). " data-analytic look at Goldbach counts." Statistica Neerlandica, 26, No. 3, 
227-242. Reprinted by permission of the editors of the journal. 
68 Exhibit 15/3: Displays and summaries 
Let us first look at those with high counts--high by more than one unit, 
just to pick a number. We find the N-axis values: 
17, 32, 50, 56, 65, 71, 77, 80 
We know this problem deals with number theory, and so a pattern might 
depend upon which numbers divide the N's, as we have already observed with 
3's. We found that divisibility by 3 seems to raise the size of the count. Are 
these new N's divisible by anything other than by 2 or 3, which we have 
already considered? Beneath each number N, let us write the divisors other 
than 2 or 3, or N itself. 
17, 32, 50, 56, 65, 71, 77, 80 
-- 5 7 5 - 7 5 
13 11 
Exhibit 15 of Chapter 3 
Goldbach counts for even numbers--2N, N = 1, 2, 3,... The line segments con- 
nect the centroids of successive bursts of 10 points, N = 0- 9, 10- 19, etc. 
14- 
� 
� � 
-- � �1 I I t I I I I I I 
4- .. - . -. ' 
0 0 20 30 40 0 0 70 80 
3H: Looking for patterns/Exhibit 16 69 
We note that $, 7, 11, and 13 are all primes, the next small primes. And they 
all appear as divisors. Indeed, they are the successive primes after 3. Thus the 
suspicion is that divisibility of N by small odd primes raises the size of the 
Goldbach count on the average. In the spirit of mathematical experimentation, 
let us suspend judgment on this and look at the small counts, those low by 
more than one unit, to see what their N's reveal. They are, with their divisors 
other than 2 and 3, 
34, 49, 61, 64, 74, 76, 79 
17 7 37 19 
We find these N's for lower counts are divisible by fewer small primes (no 5's, 
one 7, no 11's, no 13's) than the higher numbers. We have the general idea 
now that N's divisible at least once by small odd primes produce larger counts. 
Perhaps the first divisibility by each small prime is more important than 
repeated divisibility. 
I this were true, then we could, by multiplying successive distinct small 
odd primes together, construct a number that should have a high Goldbach 
count relative to its neighbors. Then we would see whether its Goldbach count 
were high compared with those of its neighbors. Let us do this. 
Exhibit 16 of Chapter 3 
Goldbach counts after removing the N's divisible by 3. The trend continues, as 
does the increase of spread with N. 
count 
C = 
70 /3: Displays and summaries for batches 
Let'N = 3 x 5 x 7 = 105, so that the even number is 2N -- 210. Then 
the counts near and at 210 are: 
200, 202, 204, 206, 208, 210, 213, 214, 216, 218, 220 
8 9 14 7 7 19 6 8 13 7 9 
and we see that we have constructed a number that produces a high count 
relative to its neighbors. 
Although we could go on enjoying this example, what we have found is 
that, by looking carefully at the departures from an observed regression 
function, we could begin to detect patterns. Of course, we have not proved 
anything mathematically here. That was not our job. What we wanted to do 
was to attack a set of data empirically by looking at residuals and see whether 
we could detect any patterns. So far we have detected the idea that when the 
even number is divisible by several different small primes, we tend to get 
higher Goldbach counts. We also found that the regression curve rises as N 
increases and that the variability of the counts increases as N increases. 
It is not our intention to press this issue further, though it could be studied 
through residuals and through extensive exploratory data analysis as in 
Mosteller (1972). But we have made an important point with this example. 
Rather simple data analysis can illuminate deep problems, as mathematical giants. 
like Euler and Gauss well knew. 
31. Residuals More Generally 
Now that we have wrestled informally with residuals in an example and seen a 
bit of what can be learned from studying residuals, we consider here, and again 
in Chapter 16, other approaches to analyzing residuals. And let us begin 
generally. 
We can examine residuals arithmetically or graphically. Careful arithmetic 
examination is tedious by hand and could be easy by computer, but still we 
usually should have a graphical examination. In the Goldbach example, we did 
not actually graph the residuals, although we graphed the original data. But the 
residuals were clear in the graph. (For careful arithmetic examination, we refer 
the reader to Anscombe and Tukey, 1963; and to Anscombe et al., 1974.) 
Calculation of letter values and identification of "outside" and "far out" values 
(see EDA, Chapter 5) may help, especially if the residuals come from a 
resistant fit. 
A graphical look at residuals almost always reduces to making (x, y) plots, 
where the y's are the residuals. The main questions are thus: "Which x's?" and 
"Which plotting techniques?" The technique question relates to how we are to 
apply our efforts efficiently, since halving the effort per plot will often 
encourage us to do more than twice as many different plots, often a good 
tradeoff. That a single technique would serve us well is not to be expected. 
Sometimes the indications we seek are coarse and obvious. At another extreme 
we want to find even delicate and indistinct appearances. 
31: Residuals more generally/Exhibit 17 71 
We discussed exploring with economy in Section 3E, plotting for the high 
and low y's only, a dozen or so points each. This plot will help find gold 
� nuggets at the grass roots. What should be done about the finer screenings? 
Example. Temperatures and geography. We explore the residuals of maximum 
January temperatures against latitude. Exhibit 17 shows, for cities of the 
United States, their maximum January temperature plotted against latitude. The 
':relation is fairly close, although a few cities stand out, especially Jacksonville, 
Fla., Seattle, Wash., and Juneau, Alaska. Essentially, they all look warm for 
their latitude. 
Exhibit 17 of Chapter 3 
=Maximum January temperature against latitude (x indicates two coincident 
observations) 
Ten, perjure 
� . COF) 
I 
: * 3'acsonvHie, 
 �- � 
X � 
� 0 - � � 
X* * o.eale, Wash. 
� X 
� � 
0 - � Juneau, Alaska 
5- � 
'":" 201 I I I I !  - I I I 
72 Exhibit 18/3: Displays and summaries 
A least-squares line was fitted to these points; its slope is b = -1.94. Then, 
letting temperature be y and latitude x, and letting  and g be their means, 
the residuals 
y - y- b(x- :) 
were calculated. These residuals in turn were plotted in Exhibit 18 against the 
longitude of the city. The diurnal rotation of the earth would, of course, have 
an averaging effect on temperature for various longitudes, but the land masses 
and water bodies (which affect temperature) bear a relation, albeit complicated, 
to longitude. So longitude is useful as abscissa. The cities that were outliers 
Exhibit 18 of Chapter 3 
Residuals from (max [January temperature vs. latitude]) against longitude. 
\o Tacbonville FI], 
80- 
� CJeaL-'cle Wash. 
t_K- 
L.A., Cal. � �'Portland, Ore. 
t0 - Phoenix, Ariz. � 
� San Frandsco, Cal. 
31: Residuals more generally/Exhibit 19 73 
before are outliers again in the residual plot. But, in addition, we note, as we 
go from 100 � to 130 �, that the points take a general drift upward. What are 
these high-residual, far west cities? Examples are Los Angeles and San 
Francisco, Calif., Portland, Ore., Seattle and Spokane, Wash. Four of the five 
are on the Pacific. This suggests a warming effect from the ocean. 
For a further indication, we can take medians for longitude of successive 
groups of seven places and the medians of their corresponding residuals. This 
yields the summary points shown by the circles in Exhibit 18. The slight rise of 
the median temperature around longitude 75 � may be caused by the accident of 
having a number of southern cities clumped near that longitude. Even though 
the linear effect of the latitude has been removed, clumps of points can display 
seeming effects. And, of course, the warming effects of being near the o.cean 
may in part account for the rise. Exhibit 19 shows the original data for the 
cities as well as the altitude and the residual from fitting temperature by 
latitude. 
Exhibit 19 of Chapter 3 
Maximum January temperatures in degrees Fahrenheit, from 1931-1960, for 
Cities in the U.S., with latitude, longitude, and altitude. 
Max. Jan. x x x3 Residuals 
:1 Cities of the U.S. I temperature Lat. � Long. � AIt. (ft.) �- - b(Xl- ) 
Mobile, Ala. 61 30 88 5 2.0 
Montgomery, Ala. 59 32 86 160 2.9 
:Juneau, Alaska 30 58 134 50 24.3 
Phoenix, Ariz. 64 33 112 1090 9.8 
Little Rock, Ark. 51 34 92 286 -1.2 
Los Angeles, Calif. 65 34 118 340 12.8 
San Francisco, Calif. 55 37 122 65 8.6 
Denver, Col. 42 39 104 5280 -.5 
:;.New Haven, Conn. 37 41 72 40 -1.7 
� Wilmington, Del. 41 39 75 135 -1.5 
Washington, D.C. 44 38 77 25 -.5 
':Jacksonville, Fla. 67 38 81 20 22.5 
� iilKey West, Fla. 74 24 81 5 2.4 
;'Miami, Fla. 76 25 80 10 6.3 
::Atlanta, Ga. 52 33 84 1050 -2.2 
iHonolulu, Hawaii 79 21 157 21 1.5 
BOise, Idaho 36 43 116 2704 1.2 
:Chicago, III. 33 41 87 595 -5.7 
:?Indianapolis, Ind. 37 39 86 710 -5.5 
74 Exhibit 19(S)/3: Displays and summaries 
Exhibit 19 of Chapter 3 (continued) 
Des Moines, iowa 29 41 93 805 -9.7 
Dubuque, Iowa 27 42 90 620 -9.7 
Wichita, Kansas 42 37 97 1290 -4.4 
Louisville, Ky. 44 38 85 450 -.5 
New Orleans, La. 64 29 90 5 2.1 
Portland, Maine 32 43 70 25 -2.8 
Baltimore, Md. 44 39 76 20 1.5 
Boston, Mass. 37 42 71 21 .3 
Detroit, Mich. 33 42 83 585 -3.7 
Sault Ste. Marie, Mich. 23 46 84 650 -6.0 
Minn.-St. Paul, Minn. 22 44 93 815 -10.8 
St. Louis, Missouri 40 38 90 455 -4.5 
Helena, Montana 29 46 112 4155 0.0 
Omaha, Nebraska 32 41 95 1040 -6.7 
Concord, N.H. 32 43 71 290 -2.8 
Atlantic City, N.J. 43 39 74 10 .5 
Albuquerque, N.M. 46 35 106 4945 -4.3 
Albany, N.Y. 31 42 73 20 -5.7 
New York, N.Y. 40 40 73 55 -.6 
Charlotte, N.C. 51 35 80 720 .7 
Raleigh, N.C. 52 35 78 365 1.7 
Bismarck, N.D. 20 46 100 1674 -9.0 
Cincinnati, Ohio 41 39 84 550 -1.5 
Cleveland, Ohio 35 41 81 660 -3.7 
Oklahoma City, Okla. 46 35 97 1195 -4.3 
Portland, Ore. 44 45 122 77 13.1 
Harrisburg, Pa. 39 40 76 365 -1.6 
Philadelphia, Pa. 40 39 75 100 -2.5 
Chariestown, S.C. 61 32 79 9 4.9 
Rapid City, S.D. 34 44 103 3230 1.2 
Nashville, Tenn. 49 36 86 450 .6 
Amarillo, Tx. 50 35 101 3685 -.3 
Galveston, Tx. 61 29 94 5 -.9 
Houston, Tx. 64 29 95 40 2.1 
Salt Lake City, Utah 37 40 111 4390 -3.6 
Burlington, Vt. 25 44 73 110 -7.8 
Norfolk, Va. 50 36 76 10 1.6 
Seattle-Tacoma, Wash. 44 47 122 10 17.0 
Spokane, Wash. 31 47 117 1890 4.0 
Madison, Wisc. 26 43 89 860 -8.8 
Milwaukee, Wisc. 28 43 87 635 -6.8 
Cheyenne, Wyoming 37 41 104 6100 -1.7 
San Juan, Puerto Rico 81 18 66 35 -2.3 
S) Source 
Temperatures are from page 263, and the geographical positions from pages 704-705 of The World 
Almanac and Book of Facts, 1974 edition; Copyright � Newspaper Enterprise Association, 
New York. 1973. Reprinted by permission of the publisher. 
3J: Plotting and smoothing/Exhibit 20 75 
3J. Plotting and Smoothing 
How can we accommodate the extremes of economical and delicate explora- 
tion along with the intermediate cases? Exhibit 20 shows the data for 88 
Unincorporated urban places from the 1962 County and City Data Book. The 
percent of housing units in one-unit structures is associated with the median 
family income for 1959. We plan to regress the income on the housing. We 
have chopped the "% one unit" data into 20 groups of 4 or 5, but sometimes 
:as few as 1 or as many as 7. In chopping up the variable, some attempt was 
made to keep the groups nearly the same size without making the intervals 
terribly different in size. The chopping is similar to that in the Goldbach 
example, but we have not retained equal intervals. 
For each group we have computed the median. And now, instead of 
merely connecting these medians, we smooth them by the smoothing methods 
described in Section 3F, repeated running medians of 3. 
The general impression in plotting the smooth points and medians in 
Exhibit 21 is of remarkably little relation between median family income and 
Exhibit 20 of Chapter 3 
� One choice of about 20 groups, and the corresponding median family incomes, 
raw, medianed, and smoothed 
' 23t0 30 7151,8380, 6910 7151 25 
40 7003 7003 7151 40 
 56 tO 59 7538, 8372 7955 58 
� 68 tO 70 9588, 7451,9540 9540 7955 69 
� i' 72 tO 75 7113, 6908, 6693, 7495, 6278, 9985 7010 74 
�  78 6160, 4572, 6806, 12264 6483 7010 78 
79 tO 80 5434, 6539, 9712, 9518, 8088 8088 7662 79 
� �� 81 tO 82 6522, 12357, 7662, 7550, 11108 7662 82 
� �: 83 tO 85 7741,7371,7003, 8863, 8895, 7413 7577 7577 84 
: 86 8596, 4904, 7475, 6489 7276 7577 86 
�  87 8123, 6338, 7973, 7753 7863 7494 87 
88 7474, 8561,7494, 7276 7494 88 
89 5567, 8265, 7260, 6908, 8.'!. 6 7260 7420 89 
90 tO 91 6613, 6076, 8685, 7597, 7243, 8728 7420 90 
� 92 tO 93 7869, 7782, 7978, 5306, 9282, 8368 7880 92 
� �: 94 7189, 11478, 8888, 6922 8039 7880 94 
� i 95 8191 7908, 5909, 7169 7538 95 
: 96t0 97 9043, 8336, 5182, 6602, 6615 6615 7538 96 
98 8602, 7186, 7365, 8363, 8671,9236 8482 7656 98 
99 7936, 7467, 8998, 6987, 7656 7656 99 
:: * By repeated running medians of 3; only changes shown. 
76 Exhibit 21/3: Displays and summaries 
% of housing units in one-unit structures. The present picture is more sensitive 
than a plot of the 10 high and 10 low points alone, and more sensitive than a 
plot of the 88 individual points would have been. If we examine it carefully, we 
are almost tempted to believe in a rise in median family income from perhaps 
7200 at the left to 7600 at the right. But our experience with smoothing warns 
us not to take this seriously. 
Effort. Putting Exhibit 21 together is not free of effort. But it is not much, if 
any, more work than plotting all the points would have been. 
If we had had a deck of 88 index cardsone for each unincorporated 
urban placedbearing the data, doing the equivalent work directly from cards 
would have been easy. All that we would have done would be to order the 
cards by % one-unit, then to pick up in groups of reasonable size, writing down 
group medians as we go. Only the smoothing and plotting would remain. With 
Exhibit 21 of Chapter 3 
The 10 highest and 10 lowest income communities and the smoothed median 
from the 20 groups of Exhibit 20 of % housing units in single-unit structures and 
median income for the 88 unincorporated urban places. 
edian 
]C a rn i l.v 
income 
(,d011rs) 
� 
� 
12,000 - 
� 
� 
Io, ooo - � 
� 
� 
(;,000 - � . 
� 
� 
� 
� 
4,0000  I I : I l I I I I I 
20 40 70 8O 100 
% I. fouing in sin!-unit srud. ure 
(3J, Summary) References 77 
card decks, to which successive residuals are added as found, this type of 
Careful plotting is workable even for fairly substantial bodies of data. (For more 
than a few hundred data sets, subsampling is likely to be desirable.) 
Summary: Batch Summaries and Displays 
We write down batches of numbers in two or three of the simplest stem-and- 
leaf forms. 
Medians can be naturally supplemented by other letter values, whose 
depths are defined, one after another, in almost the same way that the depth of 
the median is related to the batch size. 
� ..: With letter values we made letter-value displays, both skeleton displays, 
..and those including one or both of spreads or mids. 
"' In handling larger bodies of data, subsampling offers convenience and 
flexibility. Its use can be crucial. Without it, analysis might be defeated by 
sheer bulk of work. 
i:.':.: We saw some of the advantages and disadvantages of one resistant 
Smoothing procedure, chosen for simplicity. 
:.'' To explore (x, y) data, we can do different things, including (i) plotting all 
points (often inadequate alone); (ii) plotting the 10 to 20 points with highest 
Y'values and a similar number with the lowest y's; (iii) fitting a straight line and 
examining the residuals (perhaps as in (ii)); (iv) taking running medians of the 
y-values (after re-ordering by the x-values); (v) dividing the data into groups 
:according to their x-values, finding medians of these groups, smoothing the 
medians (as in (iii), say) and plotting the results; and (vi) combinations of the 
above. 
iReferences 
Anscombe, F. J., and J. W. Tukey (1963). "The examination and analysis of 
residuals." Technometrics, 5, 141-160. 
:."Anscombe, F. J., D. R. E. Bancroft, and J. G. Glynn (1974). "Tests of 
residuals in the additive analysis of a two-way table--a suggested computer 
program." Technical Report No. 32, November, 1974. Department of Statis- 
tics, Yale University. 
:EDA = Tukey, J. W. (1977). Exploratory Data Analysis. Reading, Mass.: 
Addison-Wesley. 
Hardy, G. H. (1906). Messenger Math., 35, p. 145. 
:Mosteller, F. (1972). "A data-analytic look at Goldbach counts." Statistic 
'.Neerlandica, 26, 227-242. 
:.Velleman, P. F. (1975). "Robust nonlinear data smoothing." Technical Report 
.:iNo. 89, (Series 2), Department of Statistics, Princeton University (AEC). 
78 Index for chapter 4 
The idea of straightening out curves 79 
4A The ladder of re-expressions 79 
4B Re-expressing y = x = 81 
4C The bulging rule 84 
4D More complicated curves 87 
4E Scatter plots 87 
Summary: Straightening curves 87 
Chapter 4/Straightening 
Curves and Plots 
The Idea of Straightening out Curves 
When we deal with closely related variables, some advantages occur if we can 
express their relationship linearly. Interpolation and interpretation are rela- 
tively easier, and departures from the fit are more clearly detected. In this 
chapter, we offer ways of straightening out curves. 
When we have empirically determined relations between variables, we 
cannot hope that straightening the relation within the range of the observed 
data will also straighten it far beyond the observed range. Although luck might 
help us, ordinarily we need some sort of theory or previous experience to do 
that. 
This chapter helps us re-express one or both of a pair of variables so that 
relations originally curved are straighter. If y and x are the variables, we 
consider re-expressing y or x or both. The primary tools are: 
1. a ladder of re-expressions, and 
2. rules for determining which direction to move on the ladder. 
Learning the techniques will be simplified if at first we concentrate on 
straightening out a functional relation, and then later extend the idea to scatter 
plots and other empirical data. 
4A. The Ladder of Re-expressions 
Since we need a systematic set of re-expressions, the powers of a variable 
naturally suggest themselves. As a start, let the power p take the values 
1 1 
-3, -2, -1, -, #,- 1, 2, 3 
27 ' 
(More about # later.) Let us think about only positive values of the variable, 
which for convenience we call t. 
First, we want a set of re-expressions each of which is monotonic in the 
same direction. When p > 0, as t increases, t p increases. When p < 0, as t 
increases, t  decreases. To make them all increase as t increases, we can use 
-t  when p is negative. 
Second, what shapes do these curves have? When p > 1, they are hollow 
upward /. When p = 1, the curve is straight. When p < 1, the curves are 
79 
80 Exhibit 1/4: Straightening curves and plots 
hollow downward F-'. Exhibit 1 shows the shapes of these curves, although 
they have been rescaled as in Exhibit 8 of Chapter 5 and pulled apart by 
additive constants, so that they can be seen more clearly. 
What do we choose for #? The value p = 0 leads to a constant, and so we 
cannot usefully choose it. Instead we choose log t. (We might think of these 
powers of t as coming from  t v- dt. When p = 0, we get log t.) Some may want 
to do something else, and they might get a different answer. The log t curve fits 
in well and we wouldn't want to leave out the logarithm because it is the 
Exhibit I of Chapter 4 
Shapes of curves z = t p for p = -3, -2, #, 1, 2, 3. 
p 3 
P= iole  
p=3 
p=?_ P =-?. 
p=-3 
p=l 
(4A) 4B: Transforming y= x 2/Exhibit 2 81 
transformation most commonly used. Consequently, at # we do not use t �, but 
log t. 
When we are trying re-expressions, we will move up and down the ladder 
.of re-expressions given here, searching for one that straightens well. To aid our 
intuition about which direction to move on the ladder, we will do one example 
where we have complete command of the information. Then we will give a set 
of rules. 
4B. Re-expressing y = x 2 
As our instructive example, we choose the functional relation 
y=x 2, xO. 
Its graph is shown in Exhibit 2, and we note that it is hollow upward and 
increasing as x increases. 
1. What re-expression o y would straighten out the curve? Let us consider 
What would happen if we replaced y by either y: or y m. We would be helped 
by considering points in two intervals 0 to I and 1 to 4, because all t  are 
equal at t = 1. 
Using y2. If y is replaced by y2, then all the points for 0 < x < 1 will be lower 
than they were before, because squaring a number between 0 and 1 makes it 
:Smaller. Squaring the y's where x > 1 makes the y's larger. We have, all told, 
then, bent the curve more than before. 
:"" Exhibit 2 of Chapter 4 
Graph of y = x = hollow upwards, y increasing as x increases. 
15 
IO 
CO I 2 3 4 > x 
82 /4: Straightening curves and plots 
Three points, two slopes. Let's make this more quantitative by a rough device. 
We could compute the slope of the chord from the origin to the value at x = 1 
and the slope from the point at x = 1 to x = 4 for both the original curve and 
the transformed one. Ideally the slopes. are equal for the two intervals. 
Original slopes: 0 < x < 1, 1/1 - 1; 1 < x < 4, 15/3 = 5; 
New slopes: 0 < x < 1, 1/1 - 1; 1 < x < 4, 255/3 = 85. 
The original slopes are ! and 5, a ratio of 5, and the new slopes are 1 and 
85, a ratio of 85. Using y2 has not moved the slopes closer together but farther 
apart. We are moving in the wrong direction. 
Next let us try yl/2= y. Now, points in the interval 0 < x < 1 are 
increased, because the square root--indeed, any root with power between 0 
and 1 raises such values; for example, 
40.01 = 0.1, /0.001 = 0.1. 
In the interval I < x < 4, the numbers become smaller. Thus, we are raising 
the lefthand set of numbers and decreasing the righthand ones, a possible step 
in the correct direction. Thus, if we replace y by y* = 4y we get the relation 
* 
y = 
or y* -- x, x >0, 
and this is an equation of a straight line through the origin. 
The suggestion we want to draw from this example is that for hollow- 
upward, monotonically increasing curves, if we want to replace y, we should 
move down the ladder to a p smaller than 1. We do not draw the lesson that 
this will work well or that we know how far to go. In the present example, the 
idea of either squaring or square-rooting sticks out because we knew the 
formula. 
Let us now try for a second lesson by going back to the original curve 
y=x 2. 
2. What re-expression o[ x would straighten out the curve? Again, our know- 
ledge of the functional form suggests that we replace x by either x* = x 2 or 
x* -- x/. If we replace x by x* = x 2, then we will have points (x 2, y), and 
since y = x 2, we again get a straight line because the points are (x 2, x2). We 
have moved up the ladder. We are tacitly assuming that all powers p are 
available, but relatively few are used. Let us suppose that only the powers -3, 
-2, -1, #, 1, 2, 3 had been available. What would we try for y? 
3. Trying log y. Since we want to go down the ladder for y, let us replace y by 
log y and see what happens. We use Ioge. We have log 0 =-co, log 1 = 0, 
log 4 - 1.39. 
Original slopes: 0 < x < 1, 1; 1 < x < 4, 5; 
New slopes: 0 < x < 1, co; 1 < x < 4, 0.46. 
4B: Re-expressing y= x2/Exhibit 3 83 
The logarithm increased the slope for the lefthand interval and decreased it for 
the righthand one, both moves in the correct direction, but it overcorrected. 
Starting. We could have avoided the infinities here if we had added a constant 
to y before we started. Let's ask what constant we could have added to make 
the slopes of the two chords equal. We want c so that 
log (1 + c) - log c log (16 + c) - log (1 + c) 
1 3 
log - log 
Trying a few values suggests that c = 0.95 gives a close approximation. 
Ordinarily, we would round this c off to 1, but let us go ahead with 0.95. We 
are ready to replace 
y by y*= log (y + 0.95). 
Tabular values to two decimals for x = 0, 1, 2, 3, 4 are 
x 0 1 2 3 4 
y* -0.05 0.67 1.60 2.30 2.83 
The graph is shown in Exhibit 3. On the one hand, the curve is not straight, but 
on the other hand it is much straighter than it was to begin with, and it might 
Exhibit 3 of chapter 4 
Graph of y* = log (x = + 0.95). 
I 
I ?. 3 4 
84 Exhibit 4/4: Straightening curves and plots 
serve us very well..A straight line co. uld be fitted to this curve that would not be 
off more than 0.1 zn the vertical dzrection over the range 0 < x < 4. 
The suggestion here is that perfection may not be necessary. A coarse grid 
on p .might be quite satisfactory. Usua!ly we include p = � and p =.-.; 
someUmes p--�, and even other fracu.onal powers. How much detail s 
worthwhile depends upon the example being treated. 
Although we have not proved it, the direction of the hollowness. upward 
or downward. and the direction of the monotonicity lead us to a set of rules 
for making the fit. We give these rules without further discussion in the next 
section. 
4C. The Bulging Rule 
The fundamental rule is to move on the ladder in the direction in which the 
bulge points. The bulge points separately for x and for y. Exhibit 4 is a 
reminder of how to use the ladder of powers to aid re-expression. The arcs illus- 
trate four combinations of slope and curvature, four kinds of bulging. 
Let us try another example. Data for it are given in Exhibit 5, and these 
data are plotted in Exhibit 6. 
Exhibit 4 of Chapter 4 
Directions to move. The arrows point in the direction of the bulge for each type of 
curve and for each variable separately. 
 down x 
 , 
. down ,, 
4C: The bulging rule/Exhibits 5 and 6 85 
Example 1. Use the ladder to straighten out the curve in Exhibit 6 by 
re-expressing y. 
Solution. From the point of view of y, the curve bulges upward, and we want 
to go up the ladder from p .ln d compute the pairs of slopes. Let us choose 
Let us pick out three points 
x = 0.1, 3, 9. The original slopes of the chords are 
2.88 - 0.93 4.16 - 2.88 
0.1 < x < 3, = 0.67; 3 < x < 9, = 0.21; 
3-0.1 9-3 
Ratio of slopes: 0.67/0.21 = 3.2 
Exhibit 5 of Chapter 4 
Data relating y and x for Example 1, Section 4C 
IX J! Y l Ixl L..Yl 
.1 .93 I 2.00 
.3 1.34 3 2.88 
.5 1.59 5 3.42 
.7 1.78 7 3.83 
.9 1.93 9 4.16 
Exhibit 6 of Chapter 4 ' 
Plot of data for Example 1. 
/ 
0  4 6 8 10 
86 /4: Straightening curves and plots 
Let us now move up the y ladder to y2. 
X 0.1 3 9 
y,= y2 0.86 8.29 17.31 
The new slopes are 
8.29 - 0.86 17.31 - 8.29 
0.1 < x < 3, 3 - 0.1 = 2.56; 3 < x < 9, 9 - 3 = 1.50; 
Ratio of slopes: 2.56/1.50 = 1.7. 
The move has reduced the ratio, but we have not gone far enough. Let us try 
3 
y. 
x 0.1 3 9 
y** = y3 0.80 23.89 72.0 
23.89 - 0.80 72.0 - 23.9 
0.1 < x < 3, = 7.97; 3 < x < 9, = 8.01; 
3-0.1 9-3 
Ratio of slopes: 7.97/8.01 - 0.995. 
3 
This is within rounding error of 1.00, and we conclude that replacing y by y 
will straighten out the curve. 
Since we secretly know that the original data were rounded values of 
y = 2, we can see that the movement on the ladder has actually found 
exactly the right re-expression. Nevertheless, it will be instructive to see what 
would have happened if we had tried instead to re-express x. 
Example 2. Straighten out the curve of Exhibit 6 by re-expressing x. 
Solution. The curve bulges left, and so we want to go down the ladder to log x 
or -l/x. Let us try these values, using the same 3 choices of x' 
y 0.93 2.88 4.16 
x 0.1 3 9 
x* = log x -1 0.48 0.95 
x** = -1/x -10 -0.33 -0.11 
As the calculations below show, the ratio of the slopes increases steadily. 
(4C, 4D, 4E) Summary: Straightening curves 87 
Left interval Right interval Ratio log ratio 
2.88 - 0.93 4.16 - 2.88 
- 0.67 = 0.21 0.31 -0.51 
x 3-0.1 9-3 
2.88 - 0.93 4.16- 2.88 
x* = 1.32 = 2.72 2.06 0.31 
0.48 - (-1) 0.95 - 0.48 
2.88 - 0.93 4.16 - 2.88 
x** = 0.20 = 5.82 29.1 1.46 
-0.33 - (-10) -0.11 - (-0.33) 
Moving to the logarithm was already too far. 
::: If we plot the logs of the ratios against p for the three points, we can 
interpolate to get an estimate of the re-expression that straightens most. The 
estimate is 0.38, or about � (the exact result). 
4D. More Complicated Curves 
If a curve had a lazy-S shape, we would not be likely to remove the curvature 
::by the process we have described. We might conceivably break it up into two 
Pieces at the inflection point  and fit the two pieces separately. More 
COmplicated curves could be treated similarly. Sometimes we can do better 
than this. 
: Sometimes we may have theory to guide us, and then we would expect to 
: use it. For example, if we knew that the number of primes among the integers 
less than x was x/loge x, approximately, then plotting the observed number y 
against x/loge x would be likely to produce the desired linearity. 
:4E. Scatter Plots 
:When the data are not as smooth as those we have dealt with, we replace 
Values in a narrow array by some average .the median or the mean of both y 
and of x, and then we work with three of these points at a time, as before. The 
median has some advantages here. The median of the re-expressed values is 
the re-expressed value of the original median, whereas the mean of the 
re-expressed values is not the re-expressed mean. Ordinarily it is the mean that 
possesses such attractive commutative properties, but nonlinearity is not one 
of those situations. 
Summary: Straightening Curves 
Straightening out the relat!on of y to x ov. er the range where we have data 
::need not ensure straightening out the relatton outside that range. 
::: "Straightening" is to be attacked initially in terms of three well-selected 
points. 
88 /4: Straightening curves and plots 
We' use the ratio of the slopes of the two segments connecting the middle 
(according to x) point to the upper and lower points, planning to bring this 
ratio close to 1.0 by trying various rungs on the ladder of re-expression, and, 
where helpful, interpolating. 
If a point-cloud (scatter plot) appears otherwise too fuzzy for effective 
straightening, we divide the (x, y) points into groups according to their x- 
values, then find (x-median, y-median) for each group, and work with these 
latter points. 
The simplest re-expressions for amounts form a ladder, mainly consisting 
of t p for various p, but with log t taking the place that would otherwise be 
reserved for t o (which is everywhere = i and thus is unhelpful). 
When our curve or point-cloud is still curved, we move on the ladder of 
re-expression in the direction that the bulge points, a rule that can be applied, 
with different consequences, to re-expressing either x or y. 
Chapter 5/The Practice of 
Re-expression 
Chapter index on next page 
Numbers are primarily recorded or reported in a form that reflects habit or 
convenience rather than suitability for analysis. As a result, we often need to 
re-express data before analyzing it. This chapter suggests re-expressions and 
time-saving ways to make them, and then offers some general guidelines as to 
what to try first. 
The aims of re-expression will not appear in this chapter. Some of them 
have already appeared (particularly in Chapter 4); others will appear later (for 
example, in Chapters 9 and 12). For all these particular aims, there are general 
principles about what sorts of re-expressions are likely to be effective with what 
kind of numbers. These are the subject of the present chapter. 
Some readers may prefer to read only the first few sections (5A and 5B 
are a minimum) on first reading and refer to the others as need arises. 
$A. Kinds of Numbers 
Numbers come to us expressed in several different ways, but most kinds fall 
into a few broad classes. Assisted by rules of thumb about choosing familiar 
analogs when we have an unfamiliar expression, these broad classes give us a 
start on most data we want to analyze. 
The broad classes are: 
amounts and counts, neither of which can be negative. 
obalances, which can have either sign and can be almost arbitrarily large, 
either positively or negatively. 
ocounted fractions, as in "I saw 37 magpies and 4 had yellow bills, so 4/37 
belonged to the yellow-billed species." 
oranks, where either 1 = largest, 2 = next to largest,..., or 1 = smallest, 
: 2 = next to smallest,... 
<}grades .ordered labels, as from A, B, C, D, E, F or from * ** *** **** 
(Note that all these expressions are ordered. Some data come as names 
(perhaps robin, blackbird, sparrow, and so forth), but these names are not put 
� 'in numbers-. though COUNTS of that occurrence may be so we need not 
re-express the names. Numbers on professional football players' jerseys are 
� really "names" also. Just writing in digits does not make a number.) 
89 
90 Index for Chapter 5 
5A Kinds of numbers 89 
5B Quick logs 92 
5C Quick (square) roots and reciprocals 97 
5D Quick re-expressions of counted fractions, per- 
centages, etc. 100 
5E Matching for powers and logs 102 
5F Re-expressions for grades 103 
5G Re-expressing ranks 108 
5H First aid in re-expression 109 
51 What to do with zeros--and infinities 112 
Summary: Re-expression 114 
References 115 
5A: Kinds of numbers 91 
We try the analog rules that follow only when dealing with numbers that do 
not fit in any of the broad classes: 
o if the number is free to move in one direction, but has a definite bound, 
say A, in the other, a natural analog is an amount or count. The rule is: 
Take y - A, if A is a lower bound, or A - y, if it is an upper bound, and 
treat it like an amount. 
o if the number is definitely constrained in both directions, say B <. y - C, 
a natural analog is the counted fraction. The rule is: Treat (y - B)/(C - B) 
as if it were a counted fraction. 
Re-expressing amounts or counts. If y is an amount or count, the more 
frequently used re-expressions are 
log (y + c) 
or 
(y + c) 
where c and p are constants and c is often zero. When. not zero, c avoids 
special difficulty in the logarithm when y = 0 and helps smilarly with powers 
when p < O--for example, when p = -1. We call such a c the "start"; and, 
when c  0, we speak of "started logs" or of "started powers". 
The most common powers are p = �, p = -1, p = � in descend!ng fre- 
quency of use. See Section 4A and the ladder of re-expression in Exhibit 1 of 
Chapter 4. (p = 1 is just the original expression.) 
If the ratio of largest to smallest value of y is substantial, we usually begin 
by looking at log y. 
Re-expressing balances. We re-express balances themselves rather infre- 
quently. Much more often, we plunge deeper into the data and find that the 
balance y arises as a difference 
y----Z--g, 
where z and u are amounts or counts. Then we look at a new number, y*, 
often a balance of the form 
y* = (a re-expression of z) - (a re-expression of u). 
Since knowing the numerical value of y alone does not fix the numerical value 
of y*, these steps do more than just re-express y. 
.... If V is an amount, then log V is a balance, because logs of numbers 
'"between 0 and i are negative, and logs are unbounded in both directions. Thus 
0ne possible kind of re-expression for balances, y, is 
92 /5: The practice of re-expression 
Since 
e  = (e) y, 
we could take logs to the base e, and write 
log e y = y, 
and thus e y is an amount whose log (to the base e) is the given balance. Some 
of the rather infrequent re-expressions of balances follow this pattern. 
Re-expressing counted fractions. To analyze counted fractions, we often treat 
the fraction in one class, say class A, and those in class not-A comparably 
symmetrically EXCEPT for direction. To illustrate one method, we can use for 
our example of 4 in 37 magpies: 
(re-expression of ) - (same re-expression of ). 
We call such expressions "foldings". Some foldings use the sorts of re- 
expressions described above for amounts and counts. More complex foldings, 
which usually have the folding built into the expression, are sometimes useful, 
though we will say almost nothing about them here. Those already familiar 
with such devices may recognize such names as probits, normits, standard 
normal (or Gaussian) deviates, anglits, angular transforms, and so on. Logits, 
which might seem to belong to the complex case, are just folded logarithms, 
because the logit is 
log 1 p = log p - log (1 - p) 
= (re-expression of p) - (same re-expression of (! - p)) 
Re-expressing ranks. As we will see later, folding a started log is one plausible 
and effective re-expression o ranks. 
Re-expressing grades. The most useful idea here seems to be to fix on some 
standard distribution, and ask: "If we divide up the standard distribution 
according to the observed fractions, where does the center of gravity of each 
piece fall?" More in due course. 
Note. We return to each of these points later in this chapter. 
5B. Quick Logs 
Although we live in the high-speed computer age, many will be astonished at 
how much can be done by hand in less time than it takes to debug a 
complicated program. Especially in the exploration of data, multiple analyses 
and reanalyses are the rule, with earlier tries frequently discarded. Thus, unless 
5B: Quick logs 93 
one has strong skill as a programmer and considerable resources in subroutines 
and computer access, a fair amount of pilot work by hand can be profitable 
before programming the computer with the selected plan. Consequently, even 
liking high-speed computation as well as we do, we still also appreciate the 
value of handwork. Therefore we provide materials to speed it. 
If we have a hand-held electronic calculator that goes beyond +, -, x, :-, 
we have only to press one button to turn the number we have entered into a 
log. Moreover, we have a choice as to how many decimals we are to keep. 
Usually we keep more than we need, slowing down our calculations. One good 
practice is to start by keeping two decimals, which is often one more than we 
need, planning to go back and redo the work if the analysis suggests that two 
are not enough. If our calculator takes two button presses for logo and only 
one for log = ln, let us use log to 2 decimals. 
For those without a hand-held calculator, a table can help greatly. Exhibit 
1 gives a table of convenient form, taken from EDA, Chapter 3 (to which we 
refer you for further details, although the table is self-contained). 
Using Exhibit 1 for quick logs to the base 10. After determining the charac- 
teristic, obtain the mantissa by locating the number between the two numbers 
in the first column, and choosing the number in the second column. 
We call this a "break" table because it breaks the x-variable into intervals 
such that all numbers in the interval are assigned the same value f(x) for any x 
in the interval. It differs, therefore, substantially from the usual table where, 
for a set of x's, values of f(x) are given, and then we interpolate in the table to 
get f(x). 
Example. Thus 82.1 has characteristic i from the remainder triangle of Panel 
B (the left right triangle) of Exhibit 1. We now regard the number as a 
four-digit one, 8210, and read from the last column 0.91, and so the logarithm 
is 1.91. Had we had 82.23, we would have chosen 0.92, because our rule for 
ties is to choose the even answer. 
Started logs. For 
log (y + c), c ) 0, 
we add c to y, for y an amount, and use Exhibit 1. For y a count, we may take 
advantage of having only integers for y's. Three cases arise, c small, middling, 
and large' 
c small: can b6 neglected for y > 1. 
c middling: use a standard fraction, say 1/6. 
c large: if large enough, c can be taken to be an integer. 
94 Exhibit 1/5: The practice of re-expression 
Exhibit I of Chapter 5 
Break table for two-decimal logs to the base 10 
A) MAIN BREAK TABLE for mantissas 
Break log Break log Break log Break log Break log 
9886 1567 2484 3936 6238 
.00 .20 .40 .60 .80 
1012 1603 2540 4027 6382 
.01 .21 .41 .61 .81 
1036 1641 2601 4121 6532 
.02 .22 .42 .62 .82 
1059 1678 2660 4216 6683 
.03 .23 .43 .63 .83 
1084 1718 2723 4316 6840 
.04 .24 .44 .64 .84 
1109 1757 2786 4415 6998 
.05 .25 .45 .65 .85 
1136 1799 2852 4519 7162 
.06 .26 .46 .66 .86 
1161 1840 2917 4623 7328 
.07 .27 .47 .67 .87 
1189 1884 2986 4732 7499 
.08 .28 .48 .68 .88 
1216 1927 3054 4841 7673 
.09 .29 .49 .69 .89 
1245 1973 3127 4955 7853 
.10 .30 .50 .70 .90 
1273 2018 3198 5069 8035 
.11 .31 .51 .71 .91 
1302 2066 3274 5187 8223 
.12 .32 .52 .72 .92 
1333 2113 3349 5308 8413 
.13 .33 .53 .73 .93 
1365 2163 3428 5433 8610 
.14 .34 .54 .74 .94 
1396 2213 3507 5559 8810 
.15 .35 .55 .75 .95 
1429 2265 3590 5689 9016 
.16 .36 .56 .76 .96 
1462 2317 3672 5821 9225 
.17 .37 .57 .77 .97 
1495 2372 3759 5957 9441 
.18 .38 .58 .78 .98 
1531 2426 3845 6095 9660 
.19 .39 .59 .79 .99 
1567 2484 3936 6238 9886 
When in doubt, use an even answer; thus, 1462 gives .16 and 1.495 gives .18. 
B) SE3-rlNG DECIMAL POINTS 
I 1 
+0 -1 
10 0.1 
+1 -2 
10o 0.Ol 
+2 -3 
lOOO o.001 
+3 -4 
10,o0o o.oool 
+4 -5 
100,000 0.00001 
+5 -6 
1,000,000 0.000001 
c) EXAMPLES 
Number B A log number 
log 137.2 2 + .14 = 2.14 
log 0.03694 -2 + .57 = -1.43 
log 0.896 -1 + .95 = -0.05 
log 174,321 +5 + .24 = 5.24 
5B: Quick logs/Exhibit 2 95 
For small and large c, we have only (except for the smallest y's in the first 
case) to refer to a conventional log table, which is always effective in supplying 
logs of integers. The following supplementary table of log (y + c) shows why: 
Short table of log (y + c), c > 0 
y=O 1/=1 1/=2 1/=3 y=4 
(c = O) (-oo) (0.00) (0.30) (0.48) (0.60) 
c-- 0.01 -2.00 0.00 0.30 0.48 0.60 
c = 0.03 -1.52 0.01 0.31 0.48 0.61 
c = 0.1 - 1.00 0.04 0.32 0.49 0.61 
(c = 0.25) (-0.60) (0.10) (0.35) (0.51) (0.63) 
..Clearly we hardly need this table for y _> 1 for c = 0.01 and 0.03, and for 
� .y _> 2 for c = 0.1 and even for c = 0.25. The entries scarcely vary down a 
".Column for such y's. 
For a moderate c, it is convenient to standardize on a single c. The choice 
:c = 1/6 has rather recondite reasons in its favor. More importantly, it seems to 
work just about as well as the more classical 1/10 or 1/4. So Exhibit 2 gives 
some 2-decimal values of log (y + ). Once y > 10, log (y + ) matches log y 
so closely we might as well move to Exhibit 1. 
Some readers may now wish to skip to Chapter 7, returning to the skipped 
:imaterial when necessary. 
Exhibit 2 of Chapter 5 
:Some two-decimal values of Iogo (y + ) 
::'::.Tens 0 1 2 3 4 5 6 7 8 9 
� :.'..'...' O0 -,78 ,07 .34 ,50 ,62 ,71 ,79 ,86 ,91 .96 
10 1,01 1,05 1,09 1,12 1,15 1.18 1,21 1,23 1.26 1,28 
20 1.30 1,33 1,35 1,36 1,38 1,40 1,42 1,43 1,45 1,46 
:Example 
:The cell in the row labeled 10 and column labeled 2 represents y = 10 + 2 = 12, 
?he entry in the cell is log 12 = 1.09, 
96 Exhibit 3/5: The practice of re-expression 
Exhibit 3 of Chapter 5 
Break table for (square) roots 
A) EXAMPLES 
Starting from the decimal point, divide the number into periods of two digits. 
Thus, 124.2 is 1 24 2, but 1242 is 12 42. Likewise, 0.00654 is 00 65 4 or 65 4. 
I Number I 1 Periods I I From B I I From C J ,I Number I 
124.2 1 24 2 ab. 112 11.2 
1242 12 42 ab. 35 35. 
.00654 00 65 4 .0x 80 .080 
B} BREAK TABLES to SET DECIMAL POINT--enter between bold figures, leave 
with light figures. 
I 1 
a. .x 
I oo .ol 
ab. ,Ox 
I 00 00 .00 01 
abc. .00x 
I O0 O0 O0 .00 O0 01 
abcd. .000x 
I O0 O0 O0 O0 .00 O0 O0 01 
C) MAIN BREAK TABLE � in and out as in panel B 
I Breakll Root] I Break I I Rootl J Break ! t Rooti 1Breakt I RootJ I Breakll Root i 
98 01 2 49 64 5 66 15 60 35 40 
100 160 240 40 60 
I 02 01 2 62 44 5 90 16 40 37 21 
102 164 246 41 62 
I 06 09 2 75 56 6 20 17 22 39 69 
104 168 252 42 64 
I 10 25 2 89 O0 6 50 18 06 42 25 
106 172 258 43 66 
I 14 49 3 02 76 6 81 18 92 44 89 
108 176 264 44 68 
1 18 81 3 16 84 7 13 19 80 47 61 
110 180 270 45 70 
1 23 21 3 31 24 7 45 20 70 50 41 
112 184 276 46 72 
I 27 69 3 45 96 7 78 21 62 53 29 
114 188 282 47 7 4 
I 32 25 3 61 00 8 12 22 56 56 25 
116 192 288 48 76 
I 36 89 3 76 36 8 47 23 52 59 29 
118 196 294 49 78 
I 41 61 3 92 04 8 82 24 50 62 41 
120 200 30 50 80 
I 48 64 4 08 04 9 30 25 50 65 61 
124 204 31 51 82 
I 58 76 4 24 36 9 92 26 52 68 89 
128 208 32 52 84 
I 69 O0 4 41 O0 10 56 27 56 72 25 
132 212 33 53 86 
I 79 56 4 57 96 11 22 28 62 75 69 
136 216 34 54 88 
I 90 44 4 75 24 11 90 29 70 79 21 
140 22(} 35 55 9(} 
2 01 64 � 92 64 12 60 30 80 82 81 
144 224 36 56 92 
2 13 16 5 10 76 13 32 31 92 86 49 
148 228 37 57 94 
2 25 O0 5 29 00 14 06 33 06 90 25 
152 232 38 58 96 
2 37 16 5 47 56 14 82 34 22 94 09 
156 236 39 59 98 
2 49 64 5 66 44 15 60 35 40 98 01 
5C: Quick (square) roots and reciprocals 97 
5C. Quick (Square) Roots and Reciprocals 
After logs, we want most frequently to use (square) roots or reciprocals. Again 
the hand-held calculator can do the trick. On the other hand, we might not 
have one, and would then want a table. 
For roots we have to be a little more careful since x/0 = 4.47 does not 
look like / = 1.41 or  = 14.1. The details are given and illustrated by 
examples in Panel A of Exhibit 3. 
Examples. Both 124.2 and 1242 point off into two two-digit periods to the left 
of the decimal. Each period "produces" one digit in the root, indicated as a 
and b here. 
The pattern of 124.2 is 1 24.2; therefore, it goes with an isolated digit in 
its leftmost period, and so we locate it in the first break column of Panel C. 
Since 1242 has two digits in its leftmost period (12 42), we locate it in the third 
break column. 
The number 0.00654 points off as .00 65 4 and so has two digits in its 
leftmost nonzero period and is located in column 5. Each 00 period contributes 
a 0 to the answer. And so .00 00 00 65 4 would give .00080. 
The compact table for (square) roots shown in Exhibit 3 is taken from 
EDA, Chapter 3, which discusses its use in rather more detail. 
Reciprocals. When we work with reciprocals, it is often convenient to use 
D D 
, or C , 
y y 
where C and D are positive constants, so that our working values increase as 
the raw values increase. Exhibit 4, also from EDA, Chapter 3, gives a compact 
table of - 1000/y. 
Started roots and reciprocals. Again, when dealing with counts, we are likely 
to "start" our roots or reciprocals. Again, if the start is either negligible or an 
integer, we can profit from conventional tables of (square) roots or reciprocals. 
For small values of c, we can see. from this short table of started (square) 
roots of y + c; 
Short table of started (square) roots of y + c 
y=O y=l y=2 y=3 y=4 y=5 
(c = O) (0.00) (1.00) (1.41) (1.73) (2.00) (2.24) 
c -- 0.01 O. 10 1.00 1.42 1.73 2.00 2.24 
c - 0.03 O. 17 1.01 1.42 1.74 2.01 2.24 
c = 0.1 0.32 1.05 1.45 1.76 2.02 2.26 
(c = 0.3) (0.55) (1.14) (1.52) (1.82) (2.07) (2.30) 
98 Exhibit 4/5: The practice of re-expression 
Exhibit 4 of Chapter 5 
Break table for (negative) reciprocal (using -1000/number) 
A} BREAK TABLES for SETTING DECIMAL POINT 
.Break Setting Setting Break 
1000 IOO0 
.X a. 
10,ooo lOO 
.ox ab. 
100,000 10 
.00x abc. 
1,000,000 1. 
.000x abcd. 
10,000,000 0.1 
.0000x abcde. 
100,000,000 0.01 
B) MAIN BREAK TABLE -digits of negative reciprocal 
IBreakllValue| IBreakl(Value( IBreakllValuet (BreakllValuei tBreaktlValue t 
990 1639 2469 4115 617 
-100 -60 -40 -240 -160 
1010 1681 2532 4202 633 
-98 -59 -39 -236 -156 
1031 1709 2597 4274 649 
-96 -58 -38 -232 -152 
1053 1739 2667 4348 667 
-94 -57 -37 -228 -148 
1075 1770 2740 4425 685 
-92 -56 -36 -224 -144 
1099 1802 2817 4505 704 
-90 -55 -35 -220 -140 
1124 1835 2899 4587 725 
-88 -54 -34 -216 -136 
1149 1869 2985 4673 746 
-86 -53 -33 -212 -132 
1176 1905 3077 4762 769 
-84 -52 -32 -208 -128 
1205 1942 3175 4854 794 
-82 -51 -31 -204 -124 
1235 1980 3279 4950 820 
-80 -50 -30 -200 -120 
1266 2020 3367 505 640 
-78 -49 -294 -196 -118 
1299 2062 3436 515 855 
-76 -48 -288 -192 -116 
1333 2105 3509 526 870 
-74 -47 -282 -188 -114 
1370 2151 3584 538 885 
-72 -46 -276 -184 -112 
1408 2198 3663 549 901 
-70 -45 -270 -180 -110 
1449 2247 3745 562 917 
-68 - -264 -176 -108 
1493 2299 " 3831 575 935 
-66 -43 -258 -172 -106 
1538 2353 3922 588 952 
-64 -42 -252 -168 -104 
1587 2410 4016 602 971 
-62 -41 -246 -164 -102 
1639 2469 4115 617 990 
Examples 
Number A B -1000/number 
124.2 a. -80 -8.0 
.04739 a bcde. -212 -212'*. 
1242. .x -80 -.80 
5C: Quick (square) roots and reciprocals/Exhibits 5 and 6 99 
or this, of started (negative) reciprocals: 
Short table of started (negative) reciprocals of y + c 
y=O y=l y=2 y=3 y=4 y=5 
(C = O) (-oo) (-1000) (-500) (-333) (-250) (-200) 
C = 0,01 -100000 -990 -498 -332 -249 -200 
C = 0.03 -33333 -971 -493 -330 -248 -199 
C = O. 1 - 10000 -909 -476 -323 -244 - 196 
(C = 0.3) (-3333) -769 (-435) (-303) (-233) (- 189) 
--that we only have to bother with very small c's for quite small y's. 
The situation for c = -} is set out in Exhibits 5 and 6. Again we can use 
rather small tables (sometimes with a unit change in the last place) for larger y. 
Exhibit 5 of Chapter 5 
1 
Some two-decimal values of /y + =. 
Units 
Tens 0 1 2 3 4 5 6 7 8 9 
O0 .41 1,08 1,47 1.78 2.04 2.27 2.48 2.68 2.86 3,03 
10 3.19 3.34 3,49 3.63 3.76 3.89 4,02 4.14 4.26 4.38 
20 4.49 4.60 4.71 4.81 4,92 5,02 5.12 5,21 5.31 5.40 
For 30 -< y -< 288, add 0.01 to /; for y > 288, use  as in Exhibit 3. 
Exhibit 6 of Chapter 5 
'$ome two-decimal values of -1000/(y + ) 
� Units 
:Tens 0 1 2 3 4 5 6 7 8 9 
: O0 -6000 -857 -462 -376 -240 -194 -162 -140 -122 -109 
10 -98.4 -89.6 -82.2 -75,9 -70.6 -65.9 -61.9 -58,3 -55,0 -52.2 
20 -49,6 -47.2 -45.1 -43.2 -41.4 -39.7 -38.2 -36.8 -35.5 -34.3 
iFor 34 -< y -< 56, decrease -1000/y by 0.1; for 57 -< y, use -1000/y, as in Exhibit 
100 Exhibit 7(A)/5: The practice of re-expression 
5D. Quick Re-expressions of Counted Fractions, Percentages, Etc. 
We have proposed to fold our re-expressions of fractions, so that 50% will 
always be expressed as 0.00. How much further can we carry this agreement? 
The size of the plurality is p - (1 - p) = 2p - 1. Can we have 51% as 0.02 
and 48% as -0.04 for all the re-expressions we want to consider? As Exhibit 7 
shows, we can do such matching, to the two decimals we are routinely keeping, 
from 38% to 62%, after which the re-expressions diverge, at first slowly and 
then more rapidly. 
Exhibit 7 of Chapter 5 
Re-expressions of fractions, matched at 50% 
Pluralities, folded roots, folded logarithms .Alternative expressions for counted 
fractions (take sign of answer from head of column giving %). 
A) MAIN TABLE 
+ Plur. froot flog - + Piur. froot flog - 
50% use .00 use 50% 85% .70 .76 .87 15% 
51 ; .02 <-- 49 86 .72 .78 .91 14 
52 .04 48 87 .74 .81 .95 13 
53 .06 47 88 .76 .84 1.00 12 
54 .08 46 89 .78 .87 1.05 11 
55% use .10 use 45% 90.0% .80 .89 1.10 10.0% 
56 ; .12 < 44 90.5 .81 .91 1.13 9.5 
57 .14 43 91 .82 .92 1.16 9 
58 .16 42 91.5 .83 .94 1.19 8.5 
59 .18 41 92 .84 .96 1.22 8 
60% use .20 use 40% 92.5% .85 .97 1.26 7.5% 
61 -- .22 < 39 93 .86 .99 1.29 7 
62 .24 38 93.5 .87 1.01 1.33 6.5 
63 .26 .26 .27 37 94 .88 1.02 1.38 6 
64 .28 .28 .29 36 94.5 .89 1.04 1.42 5.5 
65% .30 .30 .31 35% 95.0% .90 1.06 1.47 5.0% 
66 .32 .32 .33 34 95.5 .91 1.08 1.53 4.5 
67 .34 .35 .35 33 96 .92 1.10 1.59 4 
68 .36 .37 .35 32 96.5 .93 1.12 1.66 3.5 
69 .38 .39 .40 31 97 .94 1.15 1.74 3 
5D: Counted fractions, percentages, etc./Exhibit 7 101 
Exhibit 7 of Chapter 5(continued) 
70% .40 .41 .42 30% 97.2% .94 1.16 1.77 2.8% 
71 .42 .43 .45 29 97.4 .95 1.17 1.81 2.6 
72 .44 .45 .47 28 97.6 .95 1.18 1.85 2.4 
73 .46 .47 .50 27 97.8 .96 1.19 1.90 2.2 
74 .48 .50 .52 26 98.0 .96 1.20 1.95 2.0 
75% .50 .52 .55 25% 98.2% .96 1.21 2.00 1.8% 
76 .52 .54 .58 24 98.4 .97 1.22 2.06 1.6 
77 .54 .56 .60 23 98.6 .97 1.24 2.13 1.4 
78 .56 .59 .63 22 98.8 .98 1.25 2.21 1.2 
79 .58 .61 .66 21 99.0 .98 1.27 2.30 1.0 
80% .60 .63 .69 20% 99.2% .98 1.28 2.41 0.8% 
81 .62 .66 .73 19 99.4 .99 1.30 2.55 0.6 
82 .64 .68 .76 18 99.6 .99 1.32 2.76 0.4 
83 .66 .71 .79 17 99.8 1.00 1.35 3.11 0.2 
84 .68 .73 .83 16 100.0% 1.00 1.41 oo 0.0 
B} SUPPLEMENTARY TABLE--for flogs of fractions beyond 1% or 99% 
+ flog - + flog - 
99.0% 2.30 1.0% 99.80% 3.11 .20% 
� 1 2.35 .9 .82 3.16 .18 
.2 2.41 .8 .84 3.22 .16 
.3 2.48 .7 .86 3.28 .14 
.4 2.55 .6 .88 3.36 .12 
99.50 2.65 .50 99.90 3.45 .10 
 .52 2.67 .48 .91 3.51 .09 
.54 2.69 .46 .92 3.57 .08 
.56 2.71 .44 .93 3.63 .07 
.58 2.73 .42 .94 3.71 .06 
99.60 2.76 .40 99.95 3.80 .05 
.62 2.78 .38 .96 3.91 .04 
.64 2.81 .36 .97 4.06 .03 
.66 2.84 .34 .98 4.26 .02 
.68 2.87 .32 .99 4.61 .01 
99.70 2.90 .30 Examples 
.72 2.94 .28 
.74 2.97 .26 99.29% gives 2.47 
 .76 3.01 .24 0.37% gives-2.80 
 .78 3.06 .22 
102 /5: The practice of re-expression 
All' we have to do is to take as the definition of froots (folded roots) 
instead of 
-f 
and to take as the definition of flogs (folded logs) 
�(loge f - loge (1 - f)) = 1.1513 (log10 f - loglo (1 - f)) 
instead of either 
f 
loge f- log (1 - f) = loge 1 _ f 
or 
f 
loglo f- loglo (1 - f) = loglo i -- f' 
For total counts up to a thousand or so, two-decimal values are satisfac- 
tory. For larger values, it may be worthwhile going to more decimals. 
Starting counted fractions. To start re-expressions of counted fractions, we 
begin with (let one count be x, the other count (n - x), and c the start) 
(one count) + (start) x + c 
(total count) + (start) + (start) n + 2c' 
so that this fraction is identical with 
(the other count) + (start) n - x + c x + c 
1- =1- = . 
(total count) + (start) + (start) n + 2� n + 2� 
1 
Then in the original folded-log expression, if c = g, we have 
log (one count) - log ((the other count) 
re-expressed as 
log (one count + -}) - log (the other count + ). 
Note that we started with p = x/n and (1 - p) = (n - x)/n, and that the two 
terms in log n added to zero. In the same way, in the started logs, the terms in 
log (n + 2c) added to zero. Consequently, we can use Exhibit 2 for our values 
of logs of y + -}, where y is an integer. If we gain by matching at 50%, we put 
in the factor 1.1513, to make the match, even when we start the counts in our 
counted fractions. We forget about this factor when it makes little difference. 
5E. Matching for Powers and Logs 
Having matched at 50%-50% when re-expressing counted fractions, one asks 
about matching powers and logs when re-expressing amounts or counts. All we 
have to do is to decide at just what value A of y, we want the match to come. 
(5E) 5F: Re-expressions for grades 103 
Once we have done this, we can use any of the following six expressions (see 
Exhibit $): 
= + 
2 2 
(p =�) 2A - A 
y 
(p = pseudo O) A loge + A 
:: (p = -1) -A + 2A 
::or, more generally, 
P 
lor any p  0, as a re-expression of y which is matched to y at and near 
For a convenient example, we can take A = 300, obtaining the results 
Shown in Exhibit 8. The values of p go smoothly from left to right across the 
columns. The fourth column of Exhibit 8 shows clearly how, for powers 
matchyd to y at y y 300, the matched logarithm takes the place left open by 
the fafiure of yP wth p = 0 to be a useful re-expression. 
Exhibit 9 tabulates these same expressions more finely, and to two more 
'decimals, in the neighborhood of y = 300, so as to bring out the closeness of 
:the matching near that value. 
SUppose that we have individuals classified as A, B, C, D, or E, and that the 
frequencies of occurrence are as in Exhibit 10. For each grade we can calculate 
:::CUmulative proportions, as in that exhibit: 
o a fraction p of individuals beyond the grade, 
o a fraction P of individuals beyond and including the grade, 
::and then use either the table or formula of Exhibit 11 to calculate the 
104 Exhibit 8/5: The practice of re-expression 
corresponding values of qb(). The tentative re-expression of each grade is then 
the value of 
P-p 
where P and Bp are the two fractions. 
Panels and C of Exhibit 10 show the same calculation for hypothetical 
groups of "good" and "poor" students, which we suppose to have been sorted 
out without any use of the grade we are re-expressing. It would be well if the 
re-expressions suggested by these groups agreed with each other (and with the 
Exhibit 8 of Chapter 5 
Some values of re-expressions of logs and powers matched at and near 
y = A = 300. 
Values of p 
 [pseudo 0] -1 -2 
2 1  
y A  y) A  A  3A 
-- + -- y J4J4J4J4J4J4J4J4J4J - A A Iog e I + A + 2A y2 t 
2A 2 y 2 2 
150 0 -300 -oo -oo - 
151 25 -127 -445 -3000 -21150 
154 50 -55 -238 - 1200 -4950 
167 100 46 -30 -300 -900 
217 200 190 178 150 112 
254 250 248 245 240 234 
281 280 280 279 279 278 
290 290 290 290 290 289 
299 299 299 299 299 299 
300 300 300 300 300 300 
31 301 301 301 301 301 
3:1 320 320 319 319 318 
354 350 348 346 343 340 
417 400 393 386 375 366 
567 500 475 453 420 396 
1817 1000 795 661 510 436 
6817 2000 1249 869 555 447 
oo oo oo oo 600 450 
 [pseudo-zero] -1 -2 
p=2 I  
5F: Re-expressions for grades/Exhibit 9 105 
figures for the whole group). Agreement here would be excellent if the 
re-expressions suggested by the two groups were to agree up to an additive 
constant. Nor would we be bothered if it took a multiplicative constant, 
instead, before the re-expressions agreed. So our question is: Is the one 
suggested re-expression nearly a linear function of the other? 
Exhibit 12 shows the plot of one suggested re-expression against the other. 
The result is not a straight line, but neither is it badly bent or jagged. 
Accordingly, we think either re-expression is reasonable--and, from an excess 
of caution, plan to use the mean of the two, namely: 
A B C D E 
-4.48 -2.54 0.08 2.80 5.20 
Exhibit 9 of Chapter 5 
Finer tabulation of Exhibit 8 near y -- A = 300, where the matching is closest. 
- + ' Y ',/4AY - A A Ioge + A t- 2A �2  
� 2 2 
280.67 280 279.66 279.30 278.57 277.81 
285.38 285 284.81 284.61 284.21 283.80 
 290.17 290 289.92 289.83 289.66 289.48 
1292.11 292 291.95 291.89 291.78 291.67 
294.06 294 293.97 293.94 293.88 293.82 
296.03 296 295.99 295.97 295.95 295.92 
298.01 298 298.00 297.99 297.99 297.98 
299.00 299 299.00 299.00 299.00 299.00 
300.00 300 300.00 300.00 300.00 300.00 
301.00 301 301.00 301.00 301.00 301.00 
302.01 302 302.00 301.99 301.99 301.98 
304.03 304 303.99 303.97 303.95 303.92 
306.06 306 305.97 305.94 305.88 305.82 
308.11 308 307.95 307.90 307.79 307.69 
310.17 310 309.92 309.84 309.68 309.52 
315.38 315 314.82 314.64 314.29 313.95 
320.67 320 319.68 319.36 318.75 318.16 
 ! [pseudo-zero] -1 -2 
p=2 1 2 
106 Exhibit 10/5: The practice of re-expression 
Actually, these are nearly enough equally spaced for us to expect to do well by 
using -2, -1, 0, 1, 2. We can see this by first adding -0.09 and then dividing 
by 2.6, to get: 
A B C D E 
-1.75 -1.01 0.00 1.05 1.97 
Exhibit 10 of Chapter 5 
Example of scoring grades 
A) OVERALL EXAMPLE 
# = p = p = <h(P)- 4(p) 
1 Gradel count fraction> fraction> i<1 
A 127 .0000 .0304 .0000 -.1361 -4.48 
B 497 .0304 .1496 -.1361 -.4220 -2.40 
C 3243 .1496 .9269 -.4220 -.2616 .21 
D 231 .9269 .9823 -.2616 -.0889 3.12 
E 74 .9823 1.0000 -.0889 .0000 5.02 
(Total) (4172) 
Notes: .0304 = 127/4172, .1496 = (127 + 497)/4172, etc. Values of (p) 
and (P) from Exhibit 11. 
B) HYPOTHETICAL GOOD STUDENTS 
A 64 .0000 .0792 .0000 -.2768 -3.49 
B 127 .0792 .2364 -.2768 -.5469 -1.72 
C 560 .2364 .9295 -.5469 -.2549 .42 
D 54 .9295 .9963 -.2549 -.0244 3.45 
E 3 .9963 1.0000 -.0244 .0000 6.59 
(808) 
C) HYPOTHETICAL POOR STUDENTS 
A 12 .0000 .0114 .0000 -.0623 -5.46 
B 53 .0114 .0617 -.0623 -.2316 -3.37 
C 821 .0617 .8406 -.2316 -.4387 -.27 
D 107 .8406 .9421 -.4387 -.2212 2.14 
E 61 .9421 1.0000 -.2212 .0000 3.82 
(1054) 
5F: Re-expressions for grades/Exhibits 11 and 12 107 
Exhibit 11 of Chapter 5 
Values--and formula--for (p) or (P) 
A) Values for selected p's or P's 
q (last digit) = 
::p or P 0 I 2 3 4 5 6 7 8 9 
.000q .0000 -.0010 -.0019 -.0027 -.0035 -.0043 -,0051 -.0058 -.0065 -.0072 
.00q .0000 -.0079 -,0144 -.0204 -,0261 -.0315 -.0367 -.0417 -.0466 -.0514 
.Oq .0000 -.0560 -.0980 -,1347 -.1679 -.1985 -.2270 -.2536 -.2788 -.3025 
.1 q -.3251 -.3465 -.3669 -.3864 -,4050 -.4227 -,4397 -.4559 -.4714 -.4862 
.2q -.5004 -.5140 -.5269 -.5393 -.5511 -,5623 -.5731 -.5833 -.5930 -.6022 
: .3q -.6109 -.6191 -.6269 -.6342 -.6410 -.6474 -.6534 -.6590 -.6641 -.6687 
.4q -.6730 -.6769 -.6803 -.6833 -.6859 -.6881 -.6899 -.6913 -.6923 -.6929 
.Sq -.6931 -.6929 -,6923 -.6913 -.6899 -.6881 -.6859 -,6833 -.6803 -.6769 
:: .6q -.6730 -.6687 -.6641 -.6590 -.6534 -.6474 -.6410 -.6342 -.6269 -.6191 
.7q -.6109 -.6022 -.5930 -.5833 -.5731 -.5623 -.5511 -.5393 -.5269 -.5140 
::: .8q -.5004 -.4862 -.4714 -.4559 -,4397 -.4227 -.4050 -.3864 -.3669 -.3465 
.9q -.3251 -.3025 -.2788 -,2536 -.2270 -.1985 -.1679 -.1347 -,0980 -.0560 
 .99q -.0560 -.0514 -.0466 -.0417 -.0367 -,0315 -.0261 -.0204 -.0144 -.0079 
999q -0079 -.0072 -,0065 -.0058 -.0051 -.0043 -.0035 -.0027 -.0019 -.0010 
 B) Formula for (/)(p) 
::: (p) = p Iog p + (1 - p)log. (1 - p) 
:Exhibit 12 of Chapter 5 
:The two suggested re-expressions interrelated 
� :From 
"A om panel C, xhibi !O 
108 Exhibit 13/5: The practice of re-expression 
5G. Re-expressing Ranks 
When we want to re-express ranks, we may gain by treating them like counted 
fractions. If an observation is 5th from one end of 37, let us consider that 
dividing all 37 observations in two parts by comparing them to a nearby 
division (cutting) value will give: 
04 in one class, 33 in the other, if the division (cutting) value is close on 
one side of the fifth observation, 
05 in one class, 32 in the other, if the division (cutting) value is close on 
the other side. 
These two counted-fraction situations lead to started-and-folded logs 
log (4 + ) - log (33 + ) 
and 
log (5 + ) - log (32 + ) 
so that a natural related expression for rank 5 uses the average of the 
corresponding two arguments, and gives 
log (4� + -) - log (32� + ) 
or, equivalently, 
log (5 - �) - log (33 - ) 
where rank 5 from one end is rank 33 from the other. 
Exhibit 13 of Chapter 5 
Some two-decimal values of log (i - )for integers i < 100. 
q(last digit) = 
i 0 1 2 3 4 5 6 7 8 9 
oq - -.18 .22 .43 .56 .67 .75 .82 .88 .94 
1 q .99 1.03 1.07 1.10 1.14 1.17 1.19 1.22 1.25 1.27 
2q 1.29 1.32 1.34 1.36 1.37 1.39 1,41 1.43 1.44 1.46 
3q 1.47 1.49 1.50 1,51 1.53 1.54 1.55 1.56 1.58 1.59 
4q 1.60 1.61 1.62 1.63 1.64 1.65 1.66 1.67 1.68 1.69 
5q 1.70 1.70 1.71 1.72 1.73 1.74 1.75 1.75 1.76 1,77 
6q 1,78 1.78 1.79 1.80 1.80 1,81 1.82 1,82 1.83 1.84 
7 q 1.84 1.85 1.86 1.86 1.87 1.87 1.88 1.88 1.89 1.90 
8q 1.90 1.91 1.91 1.92 1.92 1.93 1.93 1.94 1.94 1.95 
9q 1.95 1.96 1.96 1.97 1.97 1.98 1.98 1.99 1.99 1.99 
For i _> 30, these can also be used as values of log i if desired. 
'*;'.. (5G) 5H: First aid in re-expression 109 
i . More generally, we can use 
log (i - �) - log (n + 1 - i - �) = log i -  
1 
:: n+l-i- 
for a rank of i (from the chosen end). Calculations are facilitated by using 
': EXhibit 13 to provide the values of logs of integers less 1/3. Clearly, we can 
1 
.:i forget about the - as soon as i is greater than 30, so long as two decimals 
. suffice. 
,.15 H. First Aid in Re-expression 
Choosing exactly the right re-expression for a particular quantity may not be 
'easy. To try to do a good job, we may have to (1) sense rather weak indications 
from the data in hand, (2) draw on experience with other bodies of data, or (3) 
:lean on subject-matter knowledge. Even all three may not suffice. Both 
� because we may not be prepared to try hard to choose our re-expression, or 
: because we have too little information for anyone to choose reliably, we need 
;rUles of thumb that can provide "first aid", that can lead us to re-expressions 
� that are almost always not badand usually pretty good. 
;: Four rules will deal quite effectively with most of our needs, namely: 
:1. Take logs of an amount or count (if there are zeros or infinities, we may 
' need to deal with them; see the next section). 
: 2. Take logits or folded logs of fractions or percents; use some multiple of 
( ) 
log 1 p 
: (zeros or ones call for special treatment). (If the data are restricted so that 
: A -< x --- B for some A  0 and B  1, use 
: :::3. Transform a rank i out of n, by 
log n (3n + i- 1 
...: i - log 
4. Let a balance stand. 
These rules are not supposed to be a final answer .just as first aid for the 
� .'injured is no substitute for a physiciansbut they offer a safe beginning. 
:':Se:eond aid. What if first aid is not good enough, and no careful guide can be 
.fOUnd? In such cases we may revert to "second aid," along the following lines: 
110 /5: The practice of re-expression 
If We started with amounts or counts, first aid would have us take logs. We 
might need to undo this, going back from 
x* - log x 
to 
x - antilog x*. 
If we are prepared to do this to x*, we may also be prepared to do it, instead, 
to cx* reaching 
x** = antilog cx* 
which, in view of 
log (x c) = c � log x = cx* 
will give us 
:g, c 
X -- X 
for some exponent c. 
If x is a fraction or a percent, so that first aid gives 
x 
x* = log 1 _ x 
backtracking from cx* gives 
** 1 -- X X c 
 + (1 - x)' 
with whose use we have little experience. A related possibility is to start 
from 
x 
= = log x - log (1 - x) 
x* log 1 - x 
and undo the two logs separately, reaching 
x** -- x c - (1 - x) c, 
1 
which has proved useful, for example, for c = 3. 
For ranks we can do similar thingsmagain there is little experience. If we 
think of 
log = Gol , 
n-i+ + 
where 
Gol (p) = log p - log (1 - p) 
5H: First aid in re-expression 111 
is the inverse of the cumulative logistic distribution, we might also consider 
where Aug is the inverse of the cumulative Gaussian (normal) distribution. 
(The result is quite close to what have been known as "rankits" or "normal 
scores".) 
For balances, we can go via ranks, taking the n values of our balance, 
ordering them, assigning ranks, and forming the same re-expression of the 
ranks 
3i-1 i-g 
log 3(n + 1) (3i + 1)= log 
- n-i+l-g 
= log (i - �) - log (n - i + 1 - �) 
that we discussed in Section 5G. 
None of these are guaranteed to organize the data well, but one or another 
often does. If none meets our needs, we may have to fit a sum of two or more 
terms, each involving a different re-expression of our x. 
Ordered values. On occasion, we have a variable whose given values are not 
numbered but come in an order, as when grades A, B, C,... are assigned to 
performances. To bring such variables into a regression, we need to give 
numerical values to the levels. For each grade, we can regard the percentage 
assigned to that grade as a slice out of a logistic distribution. Then we assign 
each grade the numerical value of the corresponding center of gravity (CG) for 
the logistic. 
On a computer, we take the CG of the slice from p = A to p = B to be 
B log B + (1 - B) log (1 - B) - A log A - (1 - A) log (1 - A) 
B-A ' 
or, equivalently, we can use the analysis of Section 5F and the table of Exhibit 
11. 
For hand work we can often do well enough with 
1( A (A + B)/2 B ) 
 log 1 - A ' 4 log 1 - (A  B)/2 + log 1 - B ' 
which is easier when we have only a table of 
log (1 - A 
We can do this either for the data as a whole or on each of several quite 
;...differently behaved parts of the data. In the latter case, we first check to see the 
results for the parts that behave similarly (see Exhibit 10, and its discussion in 
text, for an example), and then combine the results for the various parts. 
112 /5: The practice of re-expression 
51. What. to do with Zeros--and Infinities 
If rules (1) or (2) in Section 5H tell us to take the log of zero, what should we 
do? To some extent, the answer depends on just what we are going to do with 
the variable--and on how many zeros we face. 
If we fac.e only a few zeros in y (or a small percent of zeros) AN.D we are 
to make a resistant fit so that any sufficiently discrepant value will be gven zero 
weight, we can use log 0 - L, 
where L is taken to be less than any number (but larger in absolute value). This 
approach gives the few zeros zero weight, without regard to how they relate to 
the previous fit. Although sometimes acceptable for a few zeros, this approach 
may not be acceptable for a substantial percentage of zeros. 
Another simple solution is to "start" our logs, using 
log (x; + c) or log (y + c) 
instead of log xj or log y. When dealing with counts, popular values for c are 
1.00 and 0.25, although both smaller and larger starts are sometimes used. The 
analyst who wants to use only one adjustment, ranks and all included, can use 
1/6 for a start, not 1/3, as we have explained. When dealing with amounts that 
include zeros, we have no clear practice, and it is hard to be sure what to do, 
although setting log 0 -- L may still work. 
Although starting is neat, we do not know whether it is better to deal with 
counts by starting or by assigning a suitable negative value in place of 
log 0 (since log 1 = 0, any negative value goes in the right direction). If we 
wish to pick a value "out of a hat" to replace log 0, there is something to be 
said for such choices as 
-log 4 or -log 6 or -log 8. 
(Note: -log 6 takes the logarithm of the average of the arguments of -log 4 
and -log 8.) If we want to be more careful about matters, we can let the value 
we assign depend upon the frequency of zeros. Bohidar, Gruber, and Tukey 
have studied some aspects of this choice, and find that it is reasonable to use, 
(log p)/(1 - p), where p is estimated by the proportion of zero cells (if all cells 
are 0, calculate p as if one cell were not 0). Different portions of a table might 
use different estimates of p. 
Infinities. Sometimes we have amounts that can be infinite, either actually or 
for practical purposes. The amount of time a rat takes to run a maze may be 
infinite, when the rat refuses to run. The amount of time taken by an 
apprentice to be promoted may be infinite, when he quits the job first. The 
amount of time taken by a fish to die in the presence of a certain amount of 
pollution may be practically infinite, when the fish is still alive after a substan- 
tial period and the results of the study must be pulled together, especially if 
most fish have died rather quickly. 
51: What to do with zeros--and infinities 113 
An easy cure for such infinities is to re-express the result, not as logs but 
rather as reciprocals. The reciprocal time for the rat who does not run is merely 
zero. Indeed, in such an example we are now analyzing speed of running and 
not time to run. 
Our speeds are now amounts safely away from infinity, but they do include 
some zeros. If we want to use the first-aid rules, we can now do any of the 
things described earlier in this section, including using log (speed PLUS start), 
that are appropriate. 
Both zeros and infinities. What if we have both zeros and infinities? If we start 
first and then take reciprocals, we use 
1 
amount PLUS start' 
and if we then want to go to logs, we can start again, possibly using a new 
value, start*, reaching 
log( 1 PLUS start*) 
amount ILUS start ' 
Signs. To avoid negative signs, some use positive reciprocals, and, if the times 
or amounts are mostly or alwaysgreater than one and our start is small, 
they use 
-log (amount 1 
ILUS start')' 
since otherwise most logs would be negative. If the data measure "time to", 
they analyze speeds for raw reciprocals and slownesses for started log 
reciprocals. 
Others like to keep re-expressions going in the same direction. If we began 
by thinking about slowness, their choice would be to use negative reciprocals 
1 
amount 
and to always use the minus sign in 
+ start*), 
-log (amount 1 
iLUS stari ) or -log (amount LU S start 
whether this leads to plus signs or to minus signs. 
Choice of sign does not affect the results of most specified computations. It 
can affect one's sensitivity to the results of intermediate steps or to hints arising 
in the computation. 
114 /5: The practice of re-expression 
Fractions, ranks, and related quantities. If we begin with a fraction, and then 
go to 
x* = log ! P 
as first aid, we will have a zero-inanity problem when either p = 0 or p = 1. 
Because we can write 
. p k 
� = log , 
x = 1�gl-p n- k 
where k were observed out of n, it is natural to go to 
k + start 
x* = log 
n- k + start 
with the sort of values for start that we use for counts. 
Summary: Re-expression 
We distinguish the following kinds of values: (i) amounts and counts, (ii) 
balances, (iii) counted fractions, (iv) ranks, (v) grades. 
The most usual re-expressions, of an amount or a count, y, are log (y + c) 
and (y + c) p for various values of p and c. 
Ordinarily we do not re-express balances, finding it wiser to re-express two 
(or more) quantities of which the balance is a difference (in Section 9F we will 
meet a contrary instance, where re-expression by eCY seems natural). 
Counted fractions seem well treated in terms of folded quantities, often 
folded logs (including logits) and folded roots. 
Break tables can make re-expression quick and easy. 
Our break tables give 2-decimal logs, roots, and reciprocals. 
Matching at and near 50% can make pluralities (folded %), folded roots, 
and folded logarithms nearly the same from 38% to 62% and reasonably the 
same from 25% to 75%. 
A natural approach to re-expression for grades is to focus on the interval 
covered by a given grade (the % below, the % at, and the % above) and then 
to assign a center of gravity to that grade either on all the data or on a 
well-defined part of it. 
Exhibit 11 of this chapter helps assign centers of gravity, applicable to a 
part (or all) of the data, for any grade. 
We combine information from doing this for several parts, in order to get 
a single, broadly-useful re-expression. 
We may re-express ranks in terms of a "folded logarithm" with both 
counts started by -1/3. 
We exercise "first aid to unexamined numbers" by (i) taking logs of 
amounts or counts, (ii) taking folded logs of fractions, percents, or other 
(Summary) References 115 
counted fractions, (iii) treating ranks as just explained, (iv) letting any balance 
stand. 
We pick natural starts when necessary or desirable as part of first aid. 
First-aid rules for re-expressing data modulate into second-aid re- 
expressions that are appreciably more flexible. 
We extended the idea of starting to deal simultaneously with both "zeros" 
and "infinities." 
We "start" our roots or logs by taking c > 0, then in particular, by taking 
c = 1/6. 
We may "start" counted fractions by adding 1/6 to both counts. 
We see how to match any two of the simple re-expressions both at and 
near any given A. (The same simple linear re-expression will match re- 
expressions with a common c at and near (A - c). The reader should be able 
to work out how to match re-expressions with different values of c.) 
References 
Bohidar, N. R., D. G. Gruber, and J. W. Tukey, "Efficacy estimates for 
parasite-count data, including zero counts." Submitted to Experimental 
Parasitology. 
EDA = J. W. Tukey (1977). Exploratory Data Analysis. Reading, Mass.: 
Addison-Wesley. 
116 Index for Chapter 6 
General hints when re-expressed carrier is log x 118 
(For further details see Appendix, following Chapter 16.) 
Chapter 6/Need We 
Re-express? 
We may know, from experience with the type of data being analyzed, that a 
specific re-expression of the raw x's is helpful. Or we may have other reasons 
for believing that the re-expression is appropriate. We still might prefer not to 
bother, either because of the extra trouble, because of the difficulty of 
explaining and defending the analysis of the re-expressed data, or because little 
would be gained. We care, though, about the possible losses incurred by not 
re-expressing. 
Although we have given considerable attention to tools for re-expression 
(in Chapter 5), sometimes the effort of re-expression is not likely to be 
rewarded--for instance, when choosing either between fitting y by 
bx or b* log x 
or between fitting y by 
a + bx or a* + b* log x 
when 1 -< x -< 1.001. 
These are special cases of choosing between the raw carrier, x in the 
example above, and the re-expressed carrier, log x. Throughout, we will be 
considering only x's that are everywhere positive. The assumption in this 
chapter is that the re-expressed carrier will do a better job; the question is: 
"Will it be ENOUGH better to be worth the trouble of re-expression?" Here 
quality of performance is to be measured in terms of residuals. A poorer fit is 
exposed as such by the fact that it leaves unnecessarily large residuals. 
To study this question, we would like to use as little information about the 
data as we can, thus simplifying our task. We provide a method of guidance 
that requires at the start three pieces of information: 
o the number of data points, 
othe closeness of relation between the raw and re-expressed carriers, and 
o the closeness of relation between the raw carrier and the response. 
To describe the two closenesses, we use correlation coefficients. It is useful 
to recall that 
variance {residuals} = 1 - r 2 
variance {response} ' 
117 
118 /6: Need we re-express? 
where r '2 is the correlation coefficient between the response and the carrier 
used in a simple regression. 
While it is reasonable to suppose that we have at least plotted y against 
Xaaw, the raw carrier, so that we can assess, by eye if need be, the closeness of 
relation of the response to the raw carrier, we may well have related neither y 
nor XRaw to XStraightened , the re-expressed carrier. We can perhaps avoid the 
effort of re-expression if we can judge the closeness of Xaw and X stightened, 
and thus the value of straightening, in some simple way. A most useful aspect 
of the data for judging the value of straightening is given by the ratio of the 
largest Xaw to the smallest Xaw. In brief, if the ratio is large compared with 1, 
we will be more likely to want to re-express, and if it is near 1, we may well not 
bother. 
We give here only some general hints about how the answer comes out 
when the properly re-expressed carrier is the logarithm of the raw carrier, 
leaving to the Appendix (following Chapter 16) most details for this case and 
all details for the cases where the re-expressed carrier is the square root or the 
(negative) reciprocal of the raw carrier. 
General Hints when Re-expressed Carrier is log x 
If the largest value of x is twice the smallest value of x, we will usually 
need to re-express x if the correlation between the response and x is high 
(say >0.90). (If we are being very careful, we will re-express for smaller 
correlations; if we dare be offhand, we may not bother for correlations less 
than 0.95.) 
o If the ratio of the largest value of x to the smallest value of x is less than 
2, we will not re-express unless the correlation between response and raw x 
is even higher than 0.95. 
o If the largest value of x is twenty or more times the smallest value, we are 
likely to want to re-express almost every time that the relation between 
response and raw x seems at all helpful. 
olf we are in doubt, either we should read the Appendix, or we should try 
re-expression and see how much better it works, or both. 
Chapter 7/Hunting Out 
the Real Uncertainty 
Chapter index on next page 
To go beyond indication, we need to assess the uncertainty of our indications. 
Although precision of assessment has value, reality of assessment is more basic, 
because we can be easily misled by variables not represented or recognized in a 
study. 
We assign contributions to uncertainty to two sources: those that might be 
judged from the data at hand--internal uncertainty; and those that come from 
causes whose effects are not revealed by the data supplementary uncertainty. 
:Thus internal and supplementary uncertainty are two vague concepts intended 
to aid our understanding of uncertainty, variation, and stability. Failure to 
attend to both sources can lead to serious underestimates of uncertainty and 
consequent overoptimism about the stability of the indication. To avoid these 
:traps, we need to choose a satisfactory error term from the data, and we need 
to allow for sources of variation that are present but not made visible by the 
data-gathering process. 
Good design in observational programs and experiments can reduce the 
impact of all kinds of variation upon the uncertainty of our results. Design can 
be especially valuable in helping to make sure that major sources of variation 
are introduced into the investigation. It is often wise to "broaden the base" of 
a narrowly focused investigation so that the internal uncertainty can properly 
represent the real variation and the supplementary uncertainty can be reduced. 
:. Once a good design has been executed, we want to get a sound estimate of 
internal uncertainty from the data at hand--direct assessment of uncertainty. 
Beyond this, we still need to assess the likely magnitude of effects that cannot 
be examined in the data. Some sources might be' systematic errors of measure- 
ment (example: tendency to omit young children from a census); mismatch 
.between sampled and target population (example: in public-opinion polling, 
the sampled population is not the voting population, and even if it were, the 
population of opinions prior to the official balloting need not be the population 
:of opinions held in the voting booth); halo effect, where in repeated 
measurement of the same object the observer tends to agree too closely with 
his first measurement. 
::. We begin our discussion of internal uncertainty by illustrating how the 
Classical formula 
120 Index for Chapter 7 
7A HOW /V can mislead 121 
7B A further example of the need for direct assessment 
of variability 122 
7C Choosing an error term 123 
7D More detailed choices of error terms 125 
7E Making direct assessment possible 126 
7F Difficulties with direct assessment 129 
7G Supplementary uncertainty and its combination with 
internal uncertainty 129 
Summary: Hunting out the real uncertainty 131 
References 132 
�  7A: How /q can mislead 121 
:rnay serve us poorly. Next, we note that real or conceptual randomness of some 
accessible subdivision is, in practice, the basis for the assessment of internal 
uncertainty. Different levels of grouping offer a basis for the direct assessment 
of variability, and we offer some guidelines for choosing the appropriate level. 
After discussing an example, we suggest some major difficulties with direct 
assessment and point to some solutions. 
": Turning to supplementary uncertainty, we stress its importance, consider 
its appraisal, and discuss its combination with internal uncertainty. 
..::7A. How // Can Mislead 
.Both nonmathematical and mathematical introductory treatments of statistics 
':take pairis to emphasize that the standard deviation of the sample mean is 
::.:!tr/r-, where tr is the population standard deviation and n is the sample size. 
.This idea is most important, and it is part of the basis for the theory of 
..sampling, but it leans, as an introduction must, on an oversimplified view of 
What is going on. Later on, the analysis of variance may introduce the idea of 
'idiverse sources of variation, but we should emphasize early and often the need 
:.:::.to allow the data themselves to speak quite directly for their own variation. 
Peirce's study of reaction time (see Section 1D) again provides an example with 
both substantive and methodological interest. 
From Exhibit 5 of Chapter I we draw the following values: 
.. Day: (1) 2 3 4 5 6 to 24 
Observed mean: (475.6) 241.5 203.1 205.6 148.5 175.6 to 265.5 
s/,n: (4.2) 2.1 2.0 1.8 1.6 1.1 to 2.2 
':::Setting aside the first day's results, which are obviously discrepant, we observe 
':!that s/d-fi varies from 1.1 to 2.2. If the values of s/x/- measured the standard 
.deviations of the observed means, the variance of a difference from one day to 
the next would be between 
(1.1) 2 + (1.1) 2 = 2.42 for the smallest variability 
'and 
(2.2) 2 + (2 2) 2 = 9.68 for the largest. 
These limits correspond to standard deviations of the difference between 
::means for pairs of days of d2.42 - 1.6 and 9.68- 3.1. If these standard 
.deviations based upon tr/4 were appropriate, then the magnitude of most day- 
:tO-day differences would have to be less than two standard deviations, or less 
"'than 3.2 to 6.2, and that of almost all differences would have to be less than 
122 /7: Hunting out the real uncertainty 
three standard deviations, or less than 4.8 to 9.3. The actual differences for 
adjacent days, not including the first day (see Exhibit 5 of Chapter 1)' 
-38, +2,-57, +27, +11, +7, +2, +20, +1, +19, +9, 
-8,-1,-3, +32,-12, +6,-3,-10, +11,-4,-8 
impolitely pay little attention to such limitations. 
In the language of analysis of variance, Peirce's data show considerable 
day-to-day variation. In the language of Walter Shewhart, such data are "out 
of control" .the within-day variation does not properly predict the between- 
days variation. Nor is it just a matter of the observer "settling down" in the 
beginning. Even after the 20th day he still wobbles. 
The wavering in these data exemplifies the history of the "personal 
equation" problem of astronomy. The hope had been that each observer's 
systematic errors could be first stabilized and then adjusted for, thus improving 
accuracy. Unfortunately, attempts in this direction have failed repeatedly, as 
these data suggest they might. Thus the observer's daily idiosyncrasies need to 
have account taken of them, at least by assigning additional day-to-day 
variation. (What about hour-to-hour? Or week-to-week? We can only guess 
here, since these particular data are tabulated by whole days and do not stretch 
over many weeks.) The big change from first to all later days is also a common 
feature of many kinds of data, whose reduction in the main experiment by pilot 
work and training is often most important. 
Wilson and Hilferty (see Section 1D) made it clear that Peirce's data 
illustrate "the principle that we must have a plurality of samples if we wish to 
estimate the variability of some statistical quantity, and that reliance on such 
formula as tr/4 is not scientifically satisfactory in practice, even for estimating 
unreliability of means." 
Even in dealing with so simple a statistic as the arithmetic mean, it is often 
vital to use as direct an assessment of its internal uncertainty as possible. 
Obtaining a valid measure of uncertainty is not just a matter of looking up a 
formula. 
7B. A Further Example of the Need for Direct Assessment of Variability 
Let us turn from a measurement problem to one of counting, where the 
binomial distribution suggests itself. We tend to think automatically of the 
binomial variation of counts, with standard deviation for the observed fraction 
tr --- x/Pq/n, where n is the sample count and p = 1 - q is the proportion of 
successes in the population. Again, the idea of this microcosmic standard 
deviation is important for many purposes, yet it may underestimate actual 
variation considerably. Let us turn to mass production for an example involving 
mixing individual differences with the behavior of machines. 
(7B) 7C: Choosing an error term 123 
If among thousands of manufactured piece parts the observed fraction of 
defective piece parts is p, and 1000 pieces are produced by one operator on 
one machine in one shift, it is risky business to suppose that the long-run 
average proportion of defectives will be between p- 3/pq/1000 and p + 
3,,/pq/1000. The fraction defective is likely to depend on many things: the day 
of the week (Mondays being notorious), the operator, the machine, the shift, 
the supervisor, the inspector, and other plant matters we should not detail 
here. Appreciating this bramble of sources of variability led Walter Shewhart 
(1931) to devise methods of quality control with limits p + 3x/pq/n as an ideal 
:: to be nearly achieved only after the most strenuous and sophisticated engineer- 
:.ing efforts. What mass production with all its control and measuring ability 
::cannot attain, other fields cannot expect formulas to give. Belief in such 
?.:'formulas may produce fancied security and sad surprises. 
 Nothing can substitute for relatively direct assessment of variability. In 
� ..:.:direct assessment, we base differences on large observational groups, differ- 
' 'ences more nearly representative of the many sources of variation that we must 
face. In the manufacturing example, we might look at numbers of defectives for 
:.':.several combinations of operators, shifts, machines, and days to get a notion of 
:::the variability actual manufacturing produces. (As we discuss in Section 7G, we 
?cannot ordinarily expect the data to tell us about all sources of variation.) In 
:::icomplicated investigations, the many sources of variability oblige us to assess 
:.?..variability directly. 
Even if such a multiplicity of sources did not exist, the lack of an 
:appropriate mathematical formula connecting micro-differences to macro- 
';.'i'.'Variability would often drive us to direct assessment. Even when one makes 
;i.drastic oversimplifications (perhaps assum. ing inde. pen. dence, absence of many 
i:!known sources of variability, and Gaussian distributions), the corresponding 
::mathematical formula may never have been derived, may require an impracti- 
� :�'.'Cal effort to derive or locate, and may be misleading if found. The need for 
diverse and complex analyses also forces us to direct assessment. 
:7C. Choosing an Error Term 
� A large body of data offers considerable freedom for measuring its internal 
::uncertainty. In a study of a group of educational tests, for instance, we may 
� ':'have had all ninth-grade students in a city school system as subjects. There is a 
::natural hierarchy: 
� ?" a) student b) class c) school d) city. 
.:..;::.... We could, if we chose, regard those students who were, in the year in 
"'::"question, in the ninth grade in a particular school as a random sample of those 
:i:Who "might" have been there, considering, for instance, the socioeconomic, 
:.i'.:...ethnic, and criminological background of the area from which this school draws 
its pupils. If we did this for each school in a city and regarded both the set of 
124 /7: Hunting out the real uncertainty 
school and the city as fixed, we would have an adequately specific model to 
support an assessment of internal uncertainty. Here we would turn to pupil-to- 
pupil differences within class as the basis of our measures of stability. 
To the extent that our concern is with exactly these schools in this one city, 
such an assessment may be quite satisfactory. Alternatively, to the extent that 
broader, more encompassing, assessments are impossible (as when only one 
school in the city has ninth-grade pupils), such an assessment may be the best 
that we can do. 
If our concern is with the general nature of a broad urban milieu, one in 
which no particular distribution of, say, socioeconomic and ethnic backgrounds 
has a distinguished role, then we will do better--in the sense of giving 
ourselves more useful information--by focusing our attention on school-to- 
school differences as a basis for assessing internal uncertainties. To do this 
means, in practice, to act as if the schools studied were a random sample from 
a larger population of schools. 
By so doing, we bring into the assessment at least part of the 
neighborhood-to-neighborhood differences of our broad urban milieu. If there 
are regional differences, and "our" city belongs to a distinctive region, we will 
not have adequately represented neighborhood-to-neighborhood differences 
across regions in our assessment of instability. If, on the other hand, 
socioeconomic class is the dominant influence, and the fractions of 
socioeconomic classes for entire cities are nearly constant from city to city, then 
we may have overrepresented neighborhood-to-neighborhood variation. Our 
assessment may have imperfections of several kinds, but with data from only 
one city, it may be the best that can be done. 
While it is easy to write down formulas based on other kinds of assump- 
tions, assessment of variability in practice seems to be universally based upon 
treating suitable units--students, classes, schools, cities, or even the collection 
of tests being studied--as if they were a random sample. Recognizing this is 
important, both for making certain general techniques of assessing variability 
broadly applicable (see Chapter 8) and in making it clear that the practical 
choice is usually between "treat it as random" and "forget it, sweep it under 
the rug." 
We have been discussing, in a very pragmatic vein, what the analysis-of- 
variance oriented worker usually calls "the choice of error term." 
One might argue that it would be well to restrict the calculation of an 
indication of instability "as if so-and-so were random" to those cases where so- 
and-so was indeed random. The writers believe this position often leads to 
artificially lowered estimates of instability because of the exclusion of sources of 
variability that were sampled, though perhaps not very randomly or com- 
pletely. Consequently, we encourage treating effects as randomly sampled in 
many circumstances where the randomness is at best dubious. 
It would be a disservice to leave the impression that the most all- 
encompassing assessment of internal uncertainty is always the best. Our 
purposes may make a less encompassing assessment more relevant. If we have 
' i. (7C) 7D: More detailed choices of error terms 125 
Studied the only three large cities in some state, for instance, then so far as 
decision about policies that apply uniformly to all that state's large cities are 
Concerned, an appropriately weighted average of the results for these three 
:::cities is the natural guide. Only the uncertainties of the individual studies 
:contribute uncertainty to this guide; city-to-city differences do not contribute 
further uncertainty because all the cities to be affected have been considered. 
i::: :: (For a nationally oriented survey, conducted in Boston, New Orleans, and 
Seattle, the opposite might be likely.) 
Then, too, in many instances, large-scale differences involve so few com- 
:P'arisons as to make any assessment that includes these differences quite 
Unstable. If a study is made in exactly two cities, for instance, it may be best to 
give assessments of internal uncertainty for the results of each city separately, 
i:::and for the results for the two cities combined, allowing only for "sampling" 
Within the cities, but stating pointedly that further allowance for city-to-city 
Variation must be made. 
::'. By following this plan, different readers can combine both informed 
judgment and the observed city-to-city differences differently so as to assess an 
iappropriate city-to-city contribution to uncertainty. There need not be any one 
: :.:.:...right answer. Different purposes may well demand different assessments of 
instability. 
7D. More Detailed Choices of Error Terms 
 �Given a number of groups (cities, years, etc.) the differences between which are 
::to provide an error term, and given a result assessed separately for each, we 
are likely to proceed by using Student's t to get limits on the effects of 
:::internally expressed variability. 
::':::'i'" Often, we have little choice as to the number of groups to use, but 
g:ometimes we are freer to choose. If our principles for the selection of an error 
term leave us with 100 groups, all equivalent, we could consider assigning these 
: 1"00 randomly into 20 supergroups of five each or randomly into five super- 
groups of 20 each. Making fewer supergroups saves computational effort; what 
: Will it cost elsewhere? 
:::: How many groups should one take? Generally speaking, the more the 
:better, but usually economic and data pressures force one to take few. A look 
at Exhibit 1 showing the two-sided 5% levels for the t table is enlightening. 
: Note that, when we get to three degrees of freedom, we are already about 89% 
on the way to an infinite number of degrees of freedom for the 5% point as 
: measured in the t scale. And by 10 dr, we are only about 10% worse off on this 
::::::Scale than at oo dr. (Actually the loss in variance terms, which may be more 
:relevant, nearly corresponds to the square of the ratio of t-table entries.) 
Consequently, numbers of groups from 4 to 10 are quite practical. Subject to 
::::Calculational difficulties, larger numbers are preferable. We often use 10 as a 
rule of thumb. 
126 Exhibit 1/7: Hunting out the real uncertainty 
If groups of the right sort were always evident in the data, and if it were 
always easy to calculate results based on each group alone, and always efficient 
to use such results, the discussion of internal uncertainty could end here. 
7E. Making Direct Assessment Possible 
When the choice of independent groups of data is not automatic, it is often 
helpful to deliberately construct equivalent subsamples. For example, in mak- 
ing a sample survey from a list, we might draw several systematic subsamples 
(each consisting of every ruth individual), having starting points randomly 
chosen from the first m on the list. To estimate the population mean/z, we first 
compute the mean y for each separate subsample. The means of each of these 
k subsamples are then treated as single independent measurements. Their 
mean ] = Y,]/k estimates the population mean, /z. The estimated variance of 
, s = ( - )2/k(k - 1), makes it easy to compute confidence limits for 
 + It_xl s, (a) 
where Itk_l comes from the Student t table with k - 1 degrees of freedom for 
whatever two-sided confidence level has been chosen. In the limits given by 
expression (a), we have used a direct assessment of the variability of the . 
Do not suppose that s is equivalent to the s/n deprecated in the Peirce 
example (Section 7A). True, we are still using the notion of variance to 
measure variability, but the appraisal of that variability in s comes from 
differences between good-sized chunks, perhaps "days" in the Peirce data, 
while that in the Peirce s/n comes from differences between single measure- 
ments. 
In the same vein, several small stratified samples could be drawn, each a 
replicate sample drawn from the whole population by a method that specified 
a certain sort of stratification. Here, to estimate the population mean, a 
different y, a weighted estimate for the population mean, might be constructed 
Exhibit I of Chapter 7 
Two-sided 5% Levels for the t Table, 
1 12.71 6 2.45 
2 4.30 7 2.36 
3 3.18 8 2.31 
4 2.78 9 2.26 
5 2.57 10 2.23 
o 1.96 
� :i ::.:.'. 7E: Making direct assessment possible 127 
::::for each of the stratified samples. Then one would compute confidence limits 
.::...on the population mean tx based on these i, just as for the limits given by 
::':expression (a) above. One advantage of the use of such equivalent subsamples 
.:.: (they are often called "interpenetrating") is the ease with which they can be 
i..?:Used to allow for the variation represented, for example, by different interview- 
::11ers and supervisors, whose services can be assigned to different samples. 
:.i:i.".. Exactly similar techniques apply to many statistics. For an example dealing 
':;with estimated slopes (regression coefficients, here linear combinations of 
� /.:Observed y's with coefficients depending on the x's), let us consider part of an 
":experiment by Johnson and Tsao (1944; Johnson, 1949). 
?/:Johnson-Tsao experiment. The subject holds a ring pulled upwards by one of 
::seven initial weights (100, 150, 200, 250, 300, 350, or 400 grams). By means 
ii:.;'..:11.'of liquid and valves, the pull is increased at one of four rates (100, 200, 300, or 
:400 grams per minute). The subject announces "now" when he notices the 
:........;increased heaviness. The observation (that is, the just noticeable change) is 
:taken as the change in pull up to the instant the subject reports. Preliminary 
.?..practice runs accustom the subject to the apparatus and procedure. Johnson 
� :'"'?and Tsao used 8 subjects, 4 sighted and 4 blind, 4 male and 4 female. Each 
..subject executed the experiment twice. The experimenters randomized the 
'..:'.:'0rder of procedure for 5 replications of each of the 28 (= 4 x 7) measurements 
:i:i:(for each of 8 observers at each session). 
....;'....::' A graphical examination of variability as a function of level will show that 
the logarithms of the observations seem more suitable for the analysis than do 
:..::.!:?,the raw observations. The data suggest that, for a fixed rate of change, the 
:':i:observat ons may not depend much upon the initial weights. Let us look 
..further at this. 
.'.'. For the four sighted subjects and the rate 300 g/min, let us compute the 
.islopes of the regression lines of the common logarithms of the observations 
).:.Upon the coded initial weights (1 corresponds to 100 grams 7 to 400 grams) 
:::::iExhibit 2 shows the raw and logarithmic observations. The slopes on x for the 
:.'::..four subjects, measured in logo units per 50g change in initial weight, are' 
'i:::0.0029, 0.0154, -0.0064, 0.0021. The average slope is  = 0.0035, and 
::.,...::.iss = 0.0045. Consequently, entering a t table with three degrees of freedom, 
?:we find the 95% confidence limits for the slope to be 0.0178 and -0.0108. 
For the whole change from 100 grams to 400 grams, we would multiply 
:;::?ithese numbers by 6 (= 7 - 1) to get, for example, 6 = 0.0210, 95% interval 
:::from -0.065 to +0.107. This corresponds to an estimated change in the raw 
� .::�bservation by only about 5% (a factor of antilog0 0.0210 - 1.05) with a 95% 
if:Confidence interval from -14% to +28% (a factor between 0.86 and 1.28). In 
..:.view of the variation in the data from other sources, there are many purposes 
'::.":'::'for which the variation owing to initial weights is not only not significantly 
::: different from zero but clearly not very important 
128 Exhibit 2/7: Hunting out the real uncertainty 
Note that the results here obtained by direct assessment of internal 
uncertainty are exactly those that would have followed from a straightforward 
complete analysis of variance, in which "slope x subjects" was used as the 
error term for "slope". Aside from the systematic attitudes it produces, 
perhaps the greatest single virtue of the analysis of variance is its provision of 
direct assessments of internal uncertainty. 
We can carry out simple direct assessment of internal uncertainty for 
results that depend on the data in more complex ways than as linear combina- 
tions with fixed weights. In some problems such an approach is quite satisfac- 
tory; in others the difficulties about to be discussed are serious. 
Exhibit 2 of Chapter 7 
Johnson-Tsao Experiment 
Coded (1) (2) (3) (4) 
weight Male Male Female Female 
Original data: sighted subjects, 300g/min 
l Grams added before subject says "now" I 
I 15.8 35.0 27.2 12.2 
2 18.6 39.3 41.1 9.6 
3 12.2 47.8 32.2 11.7 
4 12.8 38.2 21.3 12.4 
5 16.5 57.7 33.7 11.9 
6 15.8 39.7 28.2 12.8 
7 17.0 44.8 29.6 10.5 
Transformed data Iogo: 
I x I I y = logto (grams added before subject responds) 
I 1.20 1.54 1.43 1.09 
2 1.27 1.59 1.61 0.98 
3 1.09 1.68 1.51 1.07 
4 1.11 1.58 1.33 1.09 
5 1.22 1.76 1.53 1.08 
6 1.20 1.60 1.45 1.11 
7 1.23 1.65 1.47 1.02 
Y.x -- 28 y = .32 11.40 10.33 7.44 
Y. xy 33.36 46.03 41.14 29.82 
Y.x 2= 140 (Yx) 2= 784 n = 7 
 (7F) 7G: Supplementary uncertainty 129 
:7F. Difficulties with Direct Assessment 
:Besides the question of supplementary variability treated in the next section, 
:::the major difficulties with direct assessment of variability, as just described, are 
:these' 
a) it may not be feasible to calculate meaningful results from such small 
: amounts of data as properly chosen groups would provide, or 
: b) although the results would be meaningful, they would be so severely 
: biased as to make their use unwise. 
: None of these difficulties arises in the examples of the last section, because 
the results there considered were all linear combinations of the observations 
With fixed weights. Thus, had we analyzed the same Johnson-Tsao data without 
i four subj first act would have been to form arithmetic 
separat ng the ects, our 
means over subjects, and our resulting regression coefficient would have been 
:the arithmetic mean of those for the four individuals. Similar results hold for 
the particular types of interpenetrating subsampling described in Section 7E. 
So long as we have linearity with fixed weights, everything is simple. 
: In Chapter 8, we explain how to handle the more complex cases where 
:i:difficulties (a) and (b) arise. 
: Beyond this sort of difficulty, the most prominent problem arises in the 
:presence of two or more separately isolated measures of variability, all of 
which should contribute to a proper error term. Suppose that we have 
conducted a study of student reaction to world news in each of 20 schools, 
widely spread across the country, in each of 10 years. School-to-school 
differences, embodying regional differences as they do, are surely likely to be 
substantial and certainly reflect an important kind of variability. Year-to-year 
changes in the impact of world news cannot be neglected. 
 Happily, the effects of both of these major sources of uncertainty can be 
assessed within the data; we would not like to have had to assess either one 
purely as a matter of jud.gment. We must face the question of how to use two 
error terms at the same tme. (Accounts of what to do are harder to find than 
::they should be. Those that can be found refer to analysis-of-variance calcula- 
tions rather than to grand means, but are routinely translatable. For formulas, 
:see Scheft6 (1959). For a worked example illustrating the arithmetic, see 
Cooper (1969). For a rather more complicated example, see Anderson (1947).) 
It does not suffice to say that we have 200 groups, each made up of one 
 school for one year, and that we need only use the variation among results for 
these groups to assess the variability of overall results. 
:7G. Supplementary Uncertainty and its Combination with Internal Uncertainty 
We want to be ready to make allowance for the effects of systematic error and 
of sources of variability excluded from our assessment of internal uncertainty. 
130 /7: Hunting out the real uncertainty 
If our' observations are confined to one city, city-to-city variation is not 
revealed in our data and cannot contribute to any assessment of internal 
uncertainty. (If only two cities are involved, we have already seen that it may 
be wise to leave city-to-city variation to judgment and include its effects in 
supplementary uncertainty.) Similar statements can apply to years, regions, and 
many other aspects of our data. 
Besides the variations that might have been evident from more extensive 
data, some deviations are intrinsic in the way that the data were gathered. The 
instruments used, whether paper-and-pencil tests, questionnaires, or mercury- 
in-glass thermometers, are subject to imperfections of calibration and to 
responsiveness to other variables than the one they seek to measure. 
As a homely example, consider the market-research analyst who plans to 
ask his respondents to perform an extra task beyond the initial questioning. In 
the pilot work, he finds that the interviewers report no difficulty about 
persuading the respondents to do the extra task. Indeed, of the respondents 
thus far approached, only 2 of 50 have failed to respond to either the 
questionnaire or the task. How shall he suppose matters are going to work out 
in the actual survey? He can be helped if he knows, for example, that, in his 
sort of survey, easy questionnaires alone lead to 15% nonresponse even with 
several callbacks. From his 96% indication, he is reduced at once to 85%. 
Next, he must consider a further discount for the extra task, but how much may 
depend heavily upon the enticeme_nts offered for the respondent's cooperation. 
At any rate, he should probably not be surprised by at least a further 15% loss. 
(If the response rate is important, the analyst may design the pilot study 
with randomly chosen respondents and superimpose different incentives to 
discover what response rates they yield.) 
Such sources of supplementary uncertainty should not be neglected. The fact 
that they have to be assessed by judgment, sometimes tempered by data from 
other sources, is no excuse for pretending they do not exist. Nor is the evidence 
that they are usually underestimated (even by physicists assessing their most 
fundamental constants; see DuMond and Cohen, 1958) any reason not to try 
to do as well with them as considered judgment permits. 
How are we to do all this? As something wholly separate from our 
assessment of internal uncertainty? Or as something to be combined with the 
latter? The writers would like to be able to combine assessments of supplemen- 
tary uncertainties and systematic errors with those of the internal uncertainties. 
In the end, all are matters of judgment. The investigator may find it worthwhile 
to communicate the internal uncertainty as well as the combined uncertainty; 
when the combining is something that only others can do, it may be better 
merely to communicate the separate components. 
How is combination done in practice? 
It is easy to expand an estimated standard deviation to an estimated root- 
mean-square error. One squares the standard deviation and adds on the square 
of the bias, and then takes the square root. Sometimes one can reduce an 
actual sample size to an effective sample size. 
,i  Summary: Hunting out the real uncertainty 131 
We can relatively easily combine supplementary uncertainties with results 
expressed as statements of confidence intervals. Suppose that we wish to add a 
!,COokie cutter" type of uncertainty, such as "anything between -4% and + 1% 
COUld be present". If our confidence interval already runs from 62% to 70%, 
:we have only to move our cutting points outward to 58% and 71%. Similarly, 
if we want to make a significance test, we may wish to move the null hypothesis 
first one way and then the other, by a corresponding amount. If we wish to add 
"distributional" type of uncertainty, such as "the systematic error tends to 
follow a normal distribution with average -1% and a standard deviation 3% ", 
We will have to paste this onto our model and adjust the results somewhat 
more subtly. If the confidence interval is already based on a standard error of 
2%, as in our 62% to 70% example, we could add the variances 2 2 q- 3 2 = 13, 
i:and get a pooled estimate of 3.6. The new center is 68% - 1% = 67%. Our 
final "confidence limits adjusted for supplementary uncertainty" would be 
about 67% + 7.2%, or an interval from 59.8% to 74.2%. 
If the supplementary uncertainty to be assessed by judgment is appreciable 
C�mpared to the variability to be expected in repetitions of the overall study, 
then precisely what our internal "probability statements" are about is no 
longer important. Here, it is essential to describe both combined and internal 
Uncertainty. Readers and hearers are entitled to see what their own judgments 
WOuld lead to. When we wish to communicate an overall uncertainty, we ought 
:to do so, and our use of probability statements should be kept flexible enough 
to enable us to do so. 
However such details are to be handled, investigators and writers, and 
their statistical advisors, have a continuing and serious obligation to plan to 
assess supplementary uncertainties every time they assess an internal uncer- 
tainty. The results may come out in terms of "words of warning', rather than 
in terms of numbers. Words are often an acceptable minimum, but all of us 
Should try to do better wherever we can. We owe supplementary and overall 
assessments of uncertainty to our readers, and to the researchers that come 
after us, even when we have made a direct assessment of internal uncertainty. 
::::: Although good methods of assessing supplementary uncertainties seem 
:deeply bound to the subject matter of the analysis, extensive discussions by 
SUbject-matter experts may help statisticians find further methods of broad 
applicability. The area needs development. 
In closing, let us emphasize that, for very different reasons, both the tightly 
COntrolled laboratory study, and the large study that has had to have play in its 
methods before it can be done at all, have special need for the appraisal of 
SUpplementary uncertainty. 
Summary: Hunting out the Real Uncertainty 
All results involve two kinds of uncertainty: internal uncertainty, uncertainty 
that can be assessed from the data, and external uncertainty, uncertainty that 
cannot. 
132 /7: Hunting out the real uncertainty 
(r/x/ may not be the correct standard error when n observations, of 
apparent variance tr 2, contribute equally to an arithmetic mean, and the same 
can be said for pvt-Pq/n. 
A large, well-structured body of data offers a variety of measures of 
internal uncertainty based on comparisons among more or less closely related 
portions of the data. These measures are often of differing size. The choice 
among them (i) can be an important matter, and (ii) can be thought about quite 
rationally. 
We can use equivalent subsamples, sometimes identified after the fact, as a 
basis for assessing internal uncertainty. 
By using fewer comparisons and thus fewer degrees of freedom, we incur a 
cost, in increased variability of our assessment, when assessing internal uncer- 
tainty. 
The major difficulties with equivalent subsample assessment of uncertainty 
(possible lack of definition, probable bias) are associated with small subsam- 
ples. 
Assessing internal uncertainty when a result summarizes over two or more 
kinds of classification is not a trivial matter; in particular it will often neither 
suffice to add up the uncertainties exposed by the classifications separately, 
nor to treat the combination of classifications as a single classification. 
It is appropriate to assess sup.plementary (external) uncertainty b.y in- 
formed judgment, and to combine nternal and external uncertainties nto a 
single figure, doing both as carefully as we reasonably can. 
We frequently need to assess supplementary uncertainty, whether the data 
be experimental or observational. 
References 
Anderson, R. L. (1947). "Use of variance components in the analysis of hog 
prices in two markets." J. Amer. $tat. Assoc., 42, 612-634. 
Cooper, B. E. (1969). Statistics for Experimentalists. Elmsford, N.Y.: Pergamon 
Press, Inc.; p. 167. 
DuMond, J. W. M., and E. R. Cohen (1958). "Fundamental constants of 
atomic physics." In E. U. Condon and H. Odishaw (Eds.), Handbook of 
physics. New York: McGraw-Hill (LC: 57-6387); pp. 7-143 to 7-173. 
Johnson, P.O. (1949). Statistical methods in research. New York: Prentice- 
Hall; p. 299. 
Johnson, P. O., and F. Tsao (1944). "Factorial design in the determination of 
differential limen values." Psychometrika, 9, 107-144. 
Scheft6, H. (1959). The Analysis of Variance. New York: John Wiley and 
Sons; p. 247. 
Shewhart, W. A. (1931). Economic Control of Ouality of Manufactured Product. 
New York: Van Nostrand; p. 361. 
ha pter 8/A Met hod of 
Direct Assessment 
:Chapter index on next page 
8A. The Jackknife 
� For statistics more complicated than weighted averages, we are likely to find 
:difficulties in assessing stability, even when we have moderately large amounts 
of. data. Thus, for example, in fitting a multiple regression on k independent 
variables, one needs at least k + 1 data points, and not many people would be 
Pleased to work with so few. Consequently, if a substantial body of data is 
needed in each group, the number of possible groups of data available for 
direct appraisal of variability by the standard method described in Section 7E 
may be severely restricted. Second, many statistics based on small samples give 
biased estimates; typically the leading term in the bias is proportional to I/n, 
where n is the sample size. Consequently, the mean of results based on several 
SUbsamples is likely to be more biased than is a single result based on all the 
data, at least to the extent that the individual samples are small. A method with 
Wide application, intended to ameliorate these problems, is the jackknife. 
The name "jackknife" is intended to suggest the broad usefulness of a 
technique as a substitute for specialized tools that may not be available, just as 
the Boy Scout's trusty tool serves so variedly. The jackknife offers ways to set 
:sensible confidence limits in complex situations. The basic idea is to assess the 
effect of each of the groups into which the data have been divided, not by the 
result for that group alone, which we used in Section 7E, but rather through 
the effect upon the body of data that results from omitting that group. 
An illustr.ation will speed understanding. For a simple mean of five 
numbers, any sngle value can be easily expressed as the weighted difference of 
two means, the mean of all five values and the mean of the four other than the 
selected value. Thus, for example, for the values 3, 5, 7, 10, 15, we can 
represent 7 by 
 5 .3 + 5 + 7 + 10 + 15 + 15 
 _4.3 5+10+ 
This result is not only trivial to prove, but appears to have trivial consequences. 
For means of equally weighted numbers, the consequences are indeed trivial. 
But when we deal with more complex statistics, the analogous computation 
does not give us the same result as we get by applying "the same" calculation 
to the individual pieces. Instead, it gives us something much more useful. In 
particular, as we see later, such complex statistics might even be regression 
equations rather than mere numbers. 
133 
134 Index for chapter 8 
8A The jackknife 133 
Appendix to 8A 137 
8B Examples with individuals 13} 
8C Jackknife using groups: Ratio estimation for a 
sample survey 145 
8D A more complex example 148 
8E Cross-validation in the example 154 
8F Two simultaneous uses of "leave out one" 156 
8G Dispersion of the i's 160 
8H Further discussion of the example 16 
Summary: The jackknife 162 
References 162 
::: 8A: The jackknife 135 
The two bases of the jackknife are that we make the desired calculation 
for all the data, and then, after dividing the data into groups, we make the 
Calculation for each of the slightly reduced bodies of data obtained by leaving 
.out just one of the groups. 
::: Let, then, y() be the result of making the complex calculation on the 
Portion of the sample that omits the }th subgroup, that is, making it on a pool 
of (/c - 1) subgroups. Let y, be the corresponding result for the entire sample, 
and define pseudo-values by 
y./ = kyn- (k- 1)y(), ] = 1,2,..., k. (1) 
These pseudo-values now play the role originally played in Section 7E by the 
results of making the computation for each group separately. Note that, as in 
the example with five numbers, when the calculation reduces to forming a mean 
Using equal weights, we have y, = yj, where y is the result for the ]th piece 
alone. Accordingly, for simple means, the use of the jackknife technique 
reduces to the technique of Section 7E, as we would hope. 
::: The final accuracy required for the y.j is just about what would be needed 
for the y. Because of the multiplications by k and by k - 1, which may be 
large, one usually needs to compute yll and the y() to more decimals than 
would be needed if they were to be used directly. Accordingly, the y. are 
particularly sensitive to computational errors or rounding changes in ylx and 
y%, although their sensitivity to data variability is ordinarily, if anything, a little 
less than that of the y they replace. 
The key idea is that, in a wide variety of problems, the pseudo-values can 
be used to set approximate confidence limits, using Student's t, as if they were 
the results of applying some complex calculation to each of k independent 
pieces of data. The words "as if" are vital here; Student's t still performs well 
in many circumstances where the y. deviate substantially from independence. 
The jackknifed value y., which is our best single result, and an estimate s$ 
of its variance are thus given by: 
: 1 
::: Y* = k (y*l +''' + Y*), 
1 
2 2/ 
S*=S . 
 (When the Y0) are rounded, or otherwise quantized, after or during 
alculation, Tukey (unpublished manuscript) has suggested the conservative 
Practice of increasing s. 2 by k2'r 2, where 72 may often be taken as the variance 
of a uniform distribution over the rounding or quantizing interval. Thus, if we 
rounded y(i) to 3 decimals, the corresponding displacement would have a 
uniform distribution over a range of 0.001, that is, from x- 0.0005 to 
136 /8: A method of direct assessment 
x + 0.0005, where x is the rounded value. Applying the usual formula L2/12 
for the variance of a rectangular distribution of length L, we would have 
.2 = (0.001)2/12 = 0.00000008. Unless k is large, or s. 2 is very small, we need 
pay little attention to the k2'r 2 term for so small a 2. It costs little to compare 
k2'T 2 with s-2.) 
Adjusting degrees of freedom 
Difficulties with the jackknife seem to arise most frequently from two causes- 
o excessively straggling tails, 
o discreteness of the values produced. 
No one has found a good solution for excessive straggling that applies in any 
circumstances or for any method of analysis; if we are willing to make a 
corresponding change in what we are estimating, the methods discussed in 
Chapter 10 may suffice to handle the difficulty. If we must stay with a more 
classical estimate, for example, that pointed to by the arithmetic mean, no one 
has found anything to help. 
We can do something about discreteness. Let us look at a natural and very 
extreme case. Suppose that we jackknife the median of a sample of even size, 
say k = 2m. If we delete any observation in the upper half of these 2m values, 
the median of what is left will be the ruth value, counting up from the lowest. If 
we delete any observation in the lower half, the median will be the (m + 1)st 
value in the original sample. 
Accordingly the y(j) have but two different values, taking on each m times, 
and the same holds for the pseudo-values. The stability of s2. is not greater than 
that corresponding to a squared difference between 2 values. This would lead 
us to use only 1 degree of freedom for t's involving such an s. 
Can we obtain a more stable s.? By jackknifing in groups, to be discussed 
in Section 8C, we can arrange to have more than 2 different pseudo-values 
appear. This can be very valuable, since the only advantage to using individuals 
in ordinary situations is to preserve degrees of freedom, something that cannot 
actually be done in this example. 
Clearly a rule of thumb for reducing degrees of freedom is needed. The 
following simple rule should be helpful: 
a) Count the number of different numbers appearing as pseudo-values, sub- 
tract one, and use the result as degrees of freedom. 
This rule is to be used only when the sameness of the pseudo-values arises 
from the definition of the computation, as in the case of a median or a range, 
and not when it arises from the nature of the basic observations or the conduct 
of our arithmetic. Specifically: 
.i. : (8A) Appendix: Combinations and re-expressions 137 
:! i v..'.: 
'.b)i.:.i If slight changes in the basic observations as when values that by their 
/.:' :;:'::'::nature are either 0 or 1 are made -0.001, +0.002, 0.997, or 1.004- 
would make two pseudo-values different, they should not be considered 
: "the same" in applying rule (a). Example: % of successes in binomial obser- 
. vations. 
c) I:':' If carrying more decimals in the computation would have made two 
;::::..':..'::pseudo-values different, they should not be considered "the same" in 
pplyi () 
a ng rule a. 
APPENDIX TO 8A: Combinations and Re-expressions 
Theresuts of data analysis are not always single numbers. When we deal with 
!seve'ral numerical results, say y, z, v, and w, we can use a single choice of k 
Pieces and go through the jackknife computation separately for each result. 
'HaVing matched sets of pseudo-values (y.j, z.j, v.i, w.i) for each ], we can easily 
find...a set of k illustrative values for any combination or function of these 
results, forming, for example, y. + z. 
'i:  :"" V*j + W*j 
::to:i'tell us something about the estimand corresponding to 
.: : y-z 
similarly, we consider the log y.j as telling us about the estimand correspond- 
ing:::.to log y. This procedure extends to combinations that depend on an 
auxiliary variable or variables, as in 
ye 
'..i .... y ' Xl 4- Z ' X2 4- 1) ' X3 q- W ' X4, 
:f�iWe can consider such illustrative functions as 
y.je 
and 
 ::.i:' y*i' Xl + Z, i ' X2 + l),j' x3+ W* i � X4. 
 ;.... In thinking about such questions of combination and transformation, we 
nee'd to bear in mind that the order of doing things will generally matter. Thus, 
f�r..:.'example, we will almost always have 
:.:. (log y).j  log (y.), 
although these two expressions are likely to be rather similar. We must, by 
definition, have 
:: (log y)(j) = log (y()). 
138 /8: A method of direct assessment 
This identity, however, does not help, because 
(log y).j = k � (log Y)all -- 
= k- log (y,u) - (k - 1) log (y(j)) 
= k.log(yn)-(k-1)log(, k'y-y*i) 
which need not equal log(y.). When, for example, k = 2, Yah = 4, and 
y. = 3, the displayed expression becomes 2 log 4 - log 5 -- log 3.2, which is not 
the same as log 3. Two corollaries are worth noting' 
oThere may be some advantage in jackknifing one expression of a given 
result rather than another (as when we jackknife log y or y2 instead of y). 
olf we deal with a linear combination of results, as in 
y - 3z + 2v, 
or in 
y + Zt + Vt 2 + wt 3, 
where t is an auxiliary variable, or in 
y ' X 1 -1 t- Z ' X 2 -[- I) ' X 3 '}' W ' X4, 
where xx, x2, x3, and x4 are auxiliary variables (regression variables), the order 
of operation will not matter, so that jackknifing y, z, v, w separately, using the 
same pieces, is enough to deal with all combinations. Consequently when 
considering many possible sets of x's, jackknifing the coefficients is economical. 
We know little about which choices of expression tend to polish up the 
behavior of the jackknife. What evidence we have suggests that: 
olt is very desirable to avoid situations where the sampling distribution of 
the quantity jackknifed has an abrupt terminus or where the possible values 
of its estimand are restricted to an interval or half-line. For example, if one 
jackknifes estimates of probabilities, he might find a few final results 
negative or greater than 1. One possible approach would be to jackknife the 
logit, log[p/(1- p)], and then form the antilogit of the final result. This 
would keep the numbers in bounds. 
oIt is desirable to avoid sampling distributions with one or more straggling 
tails. 
oIt is probably desirable to avoid markedly unsymmetrical sampling dis- 
tributions. 
:: (8A) 8B: Examples with individuals 139 
: 
In summary we can use the jackknifing of several numerical results to tell 
ii":::s about any combination of these results. Our conclusions will usually differ 
somewhat from those reached by jackknifing that combination directly. This 
: Offers us choices that can sometimes allow us to improve our conclusions. If we 
deal with linear combinations, as, for example, in the relation of multiple 
?egression equations to the coefficients that appear in them, these differences 
i:idisappear. 
i.?...:..i::::: The last two paragraphs of Chapter 2 may now be profitably reread, with 
::'i"i'j'ackknifing" substituted for "cross-validation". 
8B. Examples with Individuals 
:.'W"'e plan four examples. The first is a toy to get ideas in mind and deals with 
:inference about the standard deviation from a skewed and straggling-tailed 
.qistribution. The second example is rather clean and simple, the problem it 
treats is nonstandard, and theory for treating it is not generally available. On 
the other hand it is not a problem where the jackknife does its best work, but 
'Without it... ? The third deals with a small survey example where it is 
:Convenient to use groups, each consisting of several units. The fourth treats a 
c6mplicated multiple-response problem in some detail. It offers more than one 
i:ustration of the power of the method in a problem where indication would 
:..'Otherwise have been the most we would have hoped to seek. 
Example (first of 4). Confidence limits on a standard deviation. A sample 
fr�m a distribution produced the 11 values 
There is no reason to suppose that the distribution is normal and some reason 
':::t0...'....suppose it is not. Set confidence limits on the standard deviation 
:.First solution. Since the data are few, let us use each measurement as a group 
:0f' size 1. Call the measurements x, x2,..., x x. Since each group is of size 1, 
:we could not compute a sample standard deviation for a group. We compute 
Standard deviations first for all the measurements together that is, for all the 
groups together: 
Yn = (x - )2/10, 
:Sh�wn at the head of column 3 of Exhibit 1. Then, leaving the jth measure- 
ment out, that is, leaving the jth group out, we compute 
y() = Xi- :(D)2/9, 
Where g(;)is the mean of the 10 x's that exclude the jth, and the summation in 
::Y'iJi omits (x;- g(i)) 2. We compute this for each of the 11 measurements 
140 Exhibit 1/8: A method of direct assessment 
(groups). These values are also shown in the third column of Exhibit 1. From 
these we then compute pseudo-values 
y. = 11 yn - 10y0), 
shown in the fourth column of the table. 
Our estimate of tr is y., the average of these pseudo-values, which turns 
out to be about 1.49. The details are shown in Exhibit i, together with the 2/3 
confidence interval for rr: from 0.85 to 2.13, and the 95% confidence interval: 
from 0.10 to 2.88. Since we happen to secretly know that these data came from 
an exponential distribution with mean and standard deviation both unity, we 
need not be displeased with these limits. 
Second solution. We are not compelled to jackknife s itself, as we just did. We 
might, for example, jackknife log s instead. Thinking about this possibility, we 
are encouraged to do just this, because the sampling distribution of log s is 
Exhibit I of Chapter 8 
Jackknife for a sample standard deviation, first solution (y,. = sample stan. 
dard deviation = 1.34347) 
Standard deviations 
omitting x i 
ELI I x, l y,,, Iv', = 11 y.,, - a oy,,, I 
1 .1 1.36382 1.1400 
2 .1 1.36382 1.1400 
3 .1 1.36382 1.1400 
5 .5 1.39539 .8243 
6 1.0 1,41457 .6325 
7 1.1 1.41578 .6204 
9 1.9 1.39427 .8355 
10 1.9 1.39427 .8355 
11 4.7 .70742 7.7040 
13.1 y. = 1.4894 
s. = 0.6244 
Two-sided confidence limits on 
2/3:1.4894 + it0[2/3s. = 1.4894 + 1.02(0.6244), or the interval from 0.85 to 2.13o 
95%: 1.4894 � Itol.,ss, = 1.4894 � 2.23(0.6244), or the interval from 0.10 to 2.88. 
8B: Examples with individuals/Exhibit 2 141 
� 'i:.known to be better behaved than that for s. Specifically, the distribution of the 
logarithm of s is usually more nearly symmetrical and has less straggling tails 
than the sampling distribution of s. The logarithm might be more biased; but 
:unbiasedness is not of major importance, particularly in view of the bias- 
:.reducing feature of the jackknife. 
The details and results are shown in Exhibit 2. We have introduced �'s for 
the logged values of the y's from Exhibit 1. The 2/3 confidence interval for cr 
now runs from 0.94 to 3.29, while the 95% interval runs from 0.44 to 6.93. 
..:.:Unsatis[actory solutions. By way of contrast, let us display some solutions 
which treat the data as if they came from a normal distribution. The overall s 2 
is 1.805; a chi-square table gives the following % points for 10 degrees of 
: freedom' 
1 1 5 
2.5%    97.5% 
Values 3.25 5.78 9.34 14.15 20.48 
Exhibit 2 of Chapter 8 
! 
Jackknifing of Iogo s for the data treated in Exhibit I (Y,, = Iogo y,, = 0.12823) 
[/] l Y(i) = loglo Yli, I I Y'i = 11 Ioglo �,,, - 10 Iog,o y,,)I 
"':" 1 � 13476 ,06293 
2 � 13476 .06293 
3 . 13476 .06293 
4 � 14266 -.01607 
..::. 5 � 14470 -.03647 
"' 6 � 15062 -.09567 
7 � 15100 -,09947 
8 � 15095 -.09897 
9 � 14435 -.03297 
10 � 1 4435 -.03297 
11 -, 15032 2.91373 
Y. = 0.24454 = Mean 
s.  = 0.071605, s. = 0.2676 
::2/3 limits for IOglo or: 0.2445:1: 1.02(0.2676), or the interval from -0.028 to 0.517 
95% limits for lOglot r: 0.2445 + 2.23(0.2676), or the interval from -0.352 to 
0.841 
',2/3 confidence interval for : from 0.94 to 3.29 
95% confidence interval for : from 0.44 to 6.93 
142 /8: A method of direct assessment 
Accordingly the usual symmetric 95% confidence limits for cr 2 are 
1.805 1.805 
= 0.88 and = 5.55, 
20.48/10 3.25/10 
while the 2/3 limits are, similarly, 1.28 and 3.12. The corresponding confidence 
intervals for tr run from 0.94 to 2.36 and from 1.13 to 1.77. These limits may 
be optimistically short. 
Similarly, using the range w - 4.7 - 0.1 = 4.6 and % points for w/o- in 
normal samples of 11, namely: 
1 1 5 
2.5% g  g 97.5% 
Values 1.78 2.41 3.12 3.93 4.86 
we find intervals for o- from 0.95 to 2.58 at 95% and from 1.17 to 1.91 at 2/3 
confidence. 
Comments. The four sets of solutions compare as follows: 
Source 2/3 limits 95% limits 
jackknifing s 0.85 <- o- <- 2.13 0.10 -< o- <- 2.88 
jackknifing log s 0.94 <- o- <- 3.29 0.44 _< o- <_ 6.93 
$2/0'2 1.13 <- o- < 1.77 0.94 <- o- <- 2.36 
w/o' 1.17 <- o- <- 1.91 0.95 <- o- <- 2.58 
The comparison between the two sets of jackknife limits shows fair 
similarity for the 2/3 limits and considerable difference for the 95% limits. A 
moment's reflection on the fact that the lower limit for o- based on jackknifing 
log s cannot be negative, while that based on jackknifing s can, offers one 
reason for preferring to jackknife log s. Nevertheless, jackknifing s itself did 
nicely in this example. Jackknifing x/s or 3v is another possibility. 
For the exponential distribution that we actually sampled, Var s 
2(o-2/n), whereas, for the normal distribution, Var s  �(o-2/n). The ratio of 
these quantities measures the relative variabilities of sample standard devia- 
tions drawn from the different distributions. Consequently, the variability in 
variance terms for the kind of sample we are dealing with is four times that 
given by normal theory. We might expect, then, a factor of roughly x/ = 2 in 
the lengths of the confidence intervals. Consequently, normal theory directly 
applied cannot possibly give nearly valid confidence limits, and we must discard 
the last two solutions as unsatisfactory. 
Example (second of 4). Estimating the 10% point o a pool of popu- 
lations. Each individual in a universe has associated with it a population of 
measurements. For each of a sample of 11 individuals, a group of 5 
measurements is taken. Exhibit 3 shows the measurements (hypothetical) 
arranged from greatest to least for each individual. Estimate the upper 10% 
point of the total population of measurements. 
8B: Individuals/Exhibits 3 and 4 143 
Solution. A variety of ways could be used to estimate the 10% point. One 
method pools the groups, estimates that the space between a pair of measure- 
ments contains 1/(n + 1) of the total distribution, and interpolates to 10%. For 
example, 5 measurements divide a population into 6 parts, each of which 
contains, on the average, 1/6 of the probability. With only 5 measurements, 
however, the upper 10% point is hard to estimate, because the estimate falls 
naturally into the block above the largest observation, which is often infinitely 
long. Here is another example where the pooling inherent in combining all 
pieces but one helps. 
Our numerical work can be simplified by collecting, as in Exhibit 4, a few 
of the largest observations. First, let us estimate the 10% point for the full 
sample of 55 points. Fifty-five points yield 56 blocks, and we want to include 
56/10 = 5.6 blocks, counting from the top. In Exhibit 4, we want Yah to be 0.6 
of the way from the 5th to the 6th measurement. Thus, 
5.172 - 5.137 = 0.035, 0.6 x 0.035 = 0.021, 5.172 - 0.021 
= 5.151 = Yn. 
Exhibit 3 of Chapter 8 
Measurements for 11 individuals 
Individual 
$.880 4.660 6.950 4.756 4.411 4.257 2.642 12.541 8.404 3.262 3.286 
5.172 4.522 3.948 3.792 4.357 3.572 2.276 4.081 5.137 2.874 2.858 
:3.598 3.403 3.062 2.458 3.571 1.809 2.007 3.853 3.172 2.120 2.787 
� 3.034 3.211 2.906 .412 2.983 1.801 1.922 .364 1.432 1.456 2.752 
'.628 .070 .482 -.458 1.825 1.480 1.588 -2.945 -1.415 .780 2.047 
Exhibit 4 of Chapter 8 
Largest 8 measurements ranked and identified 
2nd 8.404 9 
4th 6.880 1 
6th 5.137 9 
:"::Sth 4.660 2 
144 Exhibit 5/8: A method of direct assessment 
When the first individual is omitted, we have to compute a y(1) based on 50 
points or 51 blocks. We thus want to count down 51/10 = 5.1 blocks. Omitting 
individual 1 eliminates two measurements, 6.880 and 5.172, from Exhibit 4, 
because both measurements come from that group. We interpolate between 
the 5th and 6th remaining measurements, finding 
4.756 - 4.660 = 0.096, 0.1(0.096) = 0.0096  0.010, 
and 
y() = 4.756 - 0.010 = 4.746. 
Exhibit 5 shows the resulting Y0), Y'J, Y*, s., and the 95% confidence 
limits, where 2 degrees of freedom have been used since only 3 different values 
arise. Note that the estimate y. = 5.874 is considerably larger than that 
obtained directly from Yah = 5.151. In this particular problem, we secretly 
know that the 10% point is 5.773, because the populations associated with 
individuals were constructed to be normal with/ = 2, 3, or 4 with probability 
1/3 and independently  = 1, 2, 3 with probability 1/3. Thus, we had nine 
Exhibit 5 of Chapter 8 
Jackknife quantities 
[ Y.,!I 5.151 [ I Pseudo-values [ 
y() 4.746 y. = 9.201 ( = 11(5.151)- 10(4.746)) 
Y(2} r.j. 168 y.2 = 4.981 
Y(3) 5.099 y.3 = 5.671 
Y(4) 5.168 Y*4 = 4.981 
iris) 5.168 �*5 = 4.981 
Y(s) 5.168 �-8 = 4.981 
y(?) 5.168 Y*7 = 4.981 
Y(s) 5.099 y.s = 5.671 
Y(9) 4.746 Y*e = 9.201 
Y(1o) 5.168 I/-1o = 4.981 
�(1) 5.168 �.11 = 4.981 
Total 55.866 Total -- 64.611 
Total/11 5.0787 Total/11 = 5.874 = y. 
Check: y. = 11 (5.151 ) - 10(5.0787) = 5.874 
s 2 = 2.78024 
s.  = 2.78024/11 - 0.25275 
s. = 0.503 
y. -I-!t21.95S , -- 5.874 + 4.30(0.503), or 95% confidence interval from 3.71 to 8.04 
(8B) 8C: Jackknife using groups/Exhibit 6 145 
different equally likely normal populations. We chose one at random with 
� :replacement for each of the 11 individuals. 
This example is one for which the jackknife is not especially well suited, 
?for it deals rather repeatedly with single order statistics. The repetitions of the 
Value 4.981 in Exhibit 5 are symptomatic of the difficulty. Generally speaking, 
:.the variations of maxima and minima and of ranges depend heavily on the 
exact shape of the underlying distribution. Accordingly, it is probable that 
robustness of validity cannot be had for any confidence procedure concerning 
� "their values. Even in such circumstances, the jackknife is often as good a 
..Procedure as we have available. An approximate idea of uncertainty is better 
"than none. 
:8C. Jackknife Using Groups: Ratio Estimation for a Sample Survey 
.'.:In practice, we usually divide our data less completely, working with and 
comparing groups made up of more than one individual or case. 
;Example (third of 4). Ratio estimate. In expounding the use of ratio esti- 
.mates, Cochran (1953, p. 113; 1963, p. 156) gives sizes (number of inhabi- 
'rants) in 1920 and 1930 for each city in a random sample of 49 drawn from a 
Population of 196 large U.S. cities. Exhibit 6 repeats his values, and totals 
� 'them, first by 7's and then overall. For computation on such an example by 
Paper, pencil, and book of tables, it is worth noting that, so far, no extra work 
is involved, since these short additions are so much easier to check than the 
'adding for all 49 cases would be. 
.:::::i.: Exhibit 6 of Chapter 8 
population of 49 large cities in 1920 (x) and 1930 (y) in thousands of people. 
1st 7 2nd 7 3rd 7 4th 7 5th 7 6th 7 7th 7 Summary 
76 80 120 115 60 57 44 58 38 52 71 79 36 46 (751) (915) 
.:.i'i::". 138 143 61 69 46 65 77 89 136 139 256 288 161 232 (977) (1122) 
67 67 387 459 2 50 64 63 116 130 43 61 74 93 (965) (1243) 
29 50 93 104 507 634 64 77 46 53 25 57 45 53 (385) (553) 
:'.:: 381 464 172 183 179 260 56 142 243 291 94 85 36 54 (696) (881) 
23 48 78 106 121 113 40 60 87 105 43 50 50 58 (830) (937) 
..i:..:: 37 63 66 86 50 64 40 64 30 111 298 317 48 75 (450) (611) 
TOtals 751 915 977 1122 965 1243 385 553 696 881 830 937 450 611 5054 6262 
146 /8: A method of direct assessment 
The formula for the ratio estimate of the 1930 population total is 
(1930 sample total) 
x (1920 population total), 
(1920 sample total) 
so that the logarithm of the estimated 1930 population total is given by 
log , total / - og total ] + log total ' 
Consequently, we find it natural to work with, and jackknife, 
z = log (1930 sample total) - log (1920 sampl total), 
since this choice minimizes the number of multiplications and divisions. 
Further computation is shown in Exhibit 7, where, in the "All" column, the 
numbers 5054 and 6262 come direct from the previous exhibit and, in the 
"i = 1" column, the numbers 4303 = 5054- 751 and 5347 = 6262- 915 
are the results of omitting the first seven cities; and so on for the other 
columns. Five-place logarithms have obviously given more than sufficient 
precision, so that the pseudo-values of z are conveniently rounded to 3 
decimals. To be able to continue easily with hand calculation, an arbitrary 
central value of 0.100 was subtracted from each z. and the result multiplied by 
1000. These working values are used for the calculation of numerical values for 
95% limits = mean + allowance. 
Exhibit 8 gives all the remaining details. The resulting point estimate is 28,300, 
about 100 lower than the unjackknifed estimate. (Since the correct 1930 total 
is 29,351, the automatic bias adjustment, which is effective only on the 
average, did not help in this instance.) The limits on this estimate are ordinarily 
somewhat wider than we would get if we had used each city as a separate 
group, since 
it6[.95 '- 2.447, It47{.95 = 2.012. 
The standard error found here was e.0125 in logarithmic units, which converts 
to about +830 in total (antilog 0.0125  1.0292, 0.0292 x 28,300  830). 
The agreement of this value, on only 6 degrees of freedom, with Cochran's 
value of 604 (1953, p. 119; 1963, p. 163) is fair. 
As we promised in Section 2B, we shall now develop a method that will 
allow the investigator who is comparing two forms of a projective test to do 
better than indication. 
The experiment must involve a number of subjects; we want to generalize 
to a large class of similar subjects. We need only divide the subjects into a 
suitable number of groups, and jackknife the whole calculation, obtaining a 
jackknifed estimate, and jackknifed confidence limits for the difference of the 
averaged reliability coefficients according to the two scorings. While a moder- 
8C: Jackknife using groups/Exhibit 7 147 
148 Exhibit 8/8: A method of direct assessment 
ate amount of computation is involved, the application of the jackknife is 
routine. 
The ability to work with groups as well as individuals is a crucial advantage 
of the jackknife, not only as a way of keeping computation to a reasonable 
volume but, more importantly, as a way of ensuring the use of an appropriately 
large error term. In particular, the way the sample was drawn controls the 
proper assessment of stability of any survey result. If the units were drawn in 
clusters, the correct error term involves cluster-to-cluster variation, and we 
must be sure that each piece is made up of one or more whole clusters. If the 
units were stratified and stratum sizes were known, stratum-to-stratum varia- 
tion must be excluded from the error term, a condition that can sometimes be 
ensured by the choice of pieces (and always by the choice of basic computa- 
tion). 
Exhibit 8 of Chapter 8 
Final computations for the ratio estimate (Base data: 1920 total = 22,919; 
log (1920 total) = 4.360; log total = log (1920 total) + log ratio) 
[ Ouantityconsidered 1[ Resultsfound 
I In words I I In formulas i i Good estimate i 195% confidence intervals l 
Working units 1000(z. - 0.100) about -7.9 -38.5to 22.7 
log ratio z. about 0.092 0.062 to 0.123 
log total Iog(1920total)+ z. about 4.452 4.422to 4.483 
total antilog (log total) about 28,300 26,000 to 30,400 
8D. A More Complex Example 
We now turn to an example of jackknifing where we can offer no other 
bearable approach. The similarity between the "leave-one-out" form of cross- 
validation discussed at the end of Section 2F and the jackknife has, without 
doubt, occurred to the reader. The example we discuss next involves both the 
cross-validation issue and the complex stability issue. Thus, it should come as 
no surprise that we are going to use both a "leave-one-out" (that is, jackknife) 
assessment of stability and "leave-one-out" cross-validation. Indeed, as we 
shall see when we come to further discussion, there are questions about this 
example where it is natural to apply "leave-one-out" techniques not merely 
"two deep" but "three deep". 
Example (fourth of 4). Discrimination. This example deals with an author- 
ship problem. Alexander Hamilton and James Madison wrote during the same 
period about similar political matters, and their personal histories were also 
8D: A more complex example 149 
:Similar. One way of trying to ascribe authorship of certain of their writings is 
based upon the rates with which each used high-frequency words. Much more 
:extensive and effective studies of this problem have been made (Mosteller and 
Wallace, 1964, 1963) than we are about to make. But since investigators 
frequently find themselves involved in problems with the same basic difficulty 
many variables, few data we set forth here, in a way that clearly reveals the 
jackknife methodology of assessing variability, a new small study of this 
problem. 
: We shall try to discriminate between some writings by Hamilton and some 
by Madison on the basis of the five words they used most frequently. Then we 
Shall see how well the method works. An advantage of choosing the five most 
:frequent words, from the point of view of the example, is that their choice is 
not based upon any prior estimate of whether these words are good or poor at 
Separating these authors' writings, an advantage for simplification, though not 
necessarily for discrimination. The decision to use 5 words is arbitrary: we 
wanted an example complicated enough to imitate reality, but modest enough 
in size that neither we nor the reader need make a career of it. Another 
advantage is that, because one more occurrence, or one fewer, changes these 
rates so slightly, the rates of use of such high-frequency words should behave 
Smoothly, making all standard techniques for measurement data sure to be 
effective. 
:: We chose 11 papers known to have been written by each author, mainly 
from among the Federalist papers. These particular 22 papers were chosen, 
because among the 100 or so papers we had available, their lengths were 
nearest to 2500 words, running from about 2200 to about 2800. For some 
Purposes, it would have been better to have chosen randomly. For convenience 
in applying the jackknife, each Hamilton paper was randomly paired with a 
Madison paper. Perhaps we could have paired them more meaningfully by 
Order of publication, but we did not. The number k = 11 was chosen partly 
because it is one more than the round number 10, and we frequently need to 
multiply or divide by k- 1. Also 10 is only about twice as big as 5, the 
'number of variables chosen for analysis, and one of our purposes is to illustrate 
the variability that may occur in a study of several weakly discriminating 
Variables when we have only a modest set of data available for establishing the 
technique of making distinctions. In our discussion, we work only with papers 
Whose authorship is known, though the method of approach illustrated should 
Produce a means of distinguishing unknown papers as well. 
We did the pairing to save calculation. We could as well have removed one 
Paper at a time and done 22 calculations. The pairing has no intrinsic merit and 
may be distracting if not regarded as an economy measure. 
 The standard device that seems most reasonable here is the linear discrim- 
inant function. When we have two classes of individuals measured on vari- 
:ables x, x2,..., xk, the linear discriminant function is a linear function 
 = A(bx + b2x2 +'" + b,x,)+ B. 
150 Exhibit 9(A)/8: Direct assessment 
The coefficients bi for the x's are chosen so as to separate the observed sample 
values of the two classes of individuals as widely as we can, considering the 
internal variation within the two classes. The numbers A and B are merely 
scale factors chosen for convenience, either of the investigator or of computa- 
tion. 
When convenient, as it is here, we may regard each Hamilton paper as 
having a y-value of 1, and each Madison paper as having a y-value of 0. The 
corresponding discriminant function is just the linear function obtained by 
fitting a multiple-regression equation with the values of the dependent variable 
y (to be forecast) assigned 0 or 1 according as Madison or Hamilton is the 
author. As a consequence, the free coefficients A and B are automatically so 
chosen that the value  forecast by the discriminant function averages 1 for 
those Hamilton papers in the subset for which a particular discriminant 
function is constructed, while the average of 9 for the corresponding Madison 
papers is 0. We shall--arbitrarily, but naturally--decide that a discriminant 
score of over 0.5 is a Hamilton indicator, one of less than 0.5 a Madison 
indicator. This decision simplifies our work by making it unneccessary for us to 
estimate the optimum point of division for the scores and may weaken our 
discrimination. 
Exhibit 9 shows the rates per thousand words for the five high-frequency 
words and (xO, in (x2), of (X3) , the (x4), and to (xs) in each of the 22 papers. 
The numbers assigned to the papers are those assigned in Mosteller and 
Wallace (1964, pp. 12-14, 269-270). Exhibit 9 also shows the sums of squares 
Exhibit 9 of Chapter 8 
Rates per thousand for the 5 words for each of the Hamilton and Madison papers 
used (sums of squares and the cross-products for each author are also given) 
A) Hamilton papers 
Words, i 
Group and in of the to 
! Number ! j 1 2 3 4 5 [ TotalJ 
o  . . . . . .o 
 2 . 24. ._ 0.0 .4 . 
36 3 24.3 23.5 64.7 90.8 42.3 245.6 
73 4 18.0 27.2 59.6 86.8 35.9 227.5 
2  20. 2. .4 8. 3. 3.0 
7 6 21.8 17.4 73.1 90.4 35.6 238.3 
  2. 3. . 8.4 .3 2. 
11 8 28.5 26.1 71.3 74.5 33.3 233.7 
3 B 28.0 20.9 56.9 82.7 d.9 234.3 
34 10 21.3 25.0 60.4 82.2 47.7 236.6 
66 11 18.5 30.7 72.7 109.3 36.6 267.8 
Totals 258.1 280.6 724.1 994.0 426.9 
 8D: Complex example/Exhibit 9(B-F) 151 
Exhibit 9 of Chapter 8 (continued) 
: B) Madison papers 
Words, i 
:JNumberJ i  2 3 4  JTmalJ 
40 1 31.6 19.9 54.8 93.8 38.6 238.7 
37 2 37.3 23.3 56.8 84.2 31.0 232.6 
:: 133 3 21.2 17.5 58.2 97:6 39.9 234.4 
14 4 27.9 19.1 55.8 93.1 33.5 229.4 
 122 5 40.7 9.3 59.0 71.5 33.6 214.1 
39 6 24.4 27.9 60.0 115.3 34.8 262.4 
46 7 27.7 17.7 61.1 115.3 32.7 254.5 
 .. 8 28.1 22.3 57.0 110.9 29.7 248.0 
47 9 30.6 23.6 68.3 118.6 23.2 264.3 
42 10 33.9 21.8 64.9 93.7 33.6 247.9 
: 132 11 23.3 31.4 34.8 94.3 49.6 233.4 
Totals 326.7 233.8 630.7 1088.3 380.2 
C) Hamilton' Sums of squares and of cross-products of deviations 
LWo, rdsJ [ 1 1 [ 2 I I 3 il 4 i J 
1 275,985 
 2 -139,756 226.069 
3 95.181 -12.473 471.102 
4 -67.655 181.6'!. . 455.111 1267.465 
5 -31.396 -34.801 -260.843 -227.696 244.069 
D) Madison' Sums of squares and of cross-products of deviations 
351.920 
-173.050 334.287 
169.590 -212.312 719.265 
-514.910 381.708 364.715 2146.385 
-173.710 109.502 -479.175 -315.635 442.865 
E) Pooled sums of squares and of cross-products of deviations 
:  1 627.905 
2 -312.806 560.356 
3 264.771 -224.785 1190.367 
4 -582.565 563.352 819.826 3413.850 
5 -205.106 74.701 -740.018 -543.331 686.934 
: F) Hamilton sum minus Madison sum 
-68.6 46.8 93.4 -94.3 46.7 
152 Exhibit 10(A)/8: Direct assessment 
of deviations and the sums of cross-products of deviations of these quantities 
for Hamilton and Madison (deviations taken from respective means). The 
pooled sums of squares and of cross-products of deviations are used to obtain 
the coefficients of the discriminant function D,n. The sums of columns xi from 
Exhibit 9 for Madison are subtracted from the corresponding sums for Hamil- 
ton, and these mean differences are recorded in Exhibit 9. Let us first study the 
variability of the discriminant function in terms of the variability of its 
individual coefficients. Exhibit 10 shows, in its top portion, the coefficients for 
the five variables and the constant term--first when all 22 papers are used for 
the fitting and then when each group, here each matched pair, of Hamilton and 
Madison papers is omitted, one at a time, and the fitting executed for the other 
20. Here the whole discriminant function is jackknifed column by column--- 
that is, coefficient by coefficient--computing 
1 l(coefficient for all) - 10(coefficient with ]th pair omitted). 
For example, the pseudo-coefficient for x3 when we omit the fourth pair is 
given as (more decimals retained, both here and in the actual computation, 
than in Exhibit 10): 
11(0.0526442)- 10(0.0563169) = 0.015917. 
Exhibit 10 of Chapter 8 
Original discriminant function D..., and the 11 discriminants, D(j. constructed by 
omitting each pair of Hamilton and Madison papers in turn: from these are 
constructed the 11 pseudo-discriminants d.j and their average D.. which are also 
given 
A) Original discriminant function D.. and 11 discriminants D(i ) 
I Coefficient of ] Constant 
D.. -0.01902 0.02851 0.05264 -0.01642 0.04056 -2.83668 
D() -0.01904 0.03032 0.05295 -0.01660 0.04120 -2.89257 
D(2) -0.02747 0.02559 0.04532 -0.01964 0.03163 -1.47660 
D(3) -0.02884 0.01467 0.04928 -0.01479 0.03962 -2.14043 
D(4) -0.00716 0.02874 0.05631 -0.01248 0.05243 -4.21632 
D(5) -0.01790 0.02789 0.05348 -0.01642 0.04172 -2.94733 
D(6) -0.01695 0.03151 0,05182 -0.01455 0.04145 -3.10996 
D(7) -0.02053 0.03063 0.05166 -0.01757 0.03681 -2.55988 
D(.) -0.01648 0.03338 0.05660 -0.01979 0.04184 -2.99265 
D(9) -0.02350 0.02910 0.05002 -0.01670 0.03074 -2.20700 
D(1o) --0.01093 0.03047 0.05406 --0.01559 0.04523 --3.40123 
D() -0,01983 0.02953 0.05521 -0.01616 0.04188 -3.06431 
8D: Complex example/Exhibit 10(B) 153 
This result appears in the second portion of Exhibit 10 as the fourth value in 
the third column. Entries have been truncated to 5 decimals. These 11 new 
discriminant functions, together with the 12th formed by averaging them, are 
the....pseudo-discriminants and the jackknifed discriminant, respectively. Note 
that whole functions are being jackknifed, not just values of functions, not just 
Coefficients in functions, something we can do because the coefficients appear 
linearly in the values. 
::i.. Let us look at D., the jackknifed discriminant function, and the standard 
errors of its coefficients, calculated according to the formula of Section 8A. 
D* - -3.0141 - 0.0195x + 0.0301x2 + 0.0547x3 - 0.0167x4 + 0.0420xs 
Standard errors 
(jackknifed so,) 0.0193 0.0149 0.0105 0.00645 0.0181 
critical 
ratio [b[/s, 1.0 2.0 5.2 2.6 2.3 
These results suggest that the only coefficient seriously different from zero 
might be the third, which multiplies the frequency for of, a suggestion we shall 
not follow up here. 
Exhibit 10 of Chapter 8 (continued) 
B} Pseudo-discriminants D.j and their average D. 
:: I Coefficient of I Constant 
I x, I [.. x= .11 I I x, I I x, I term 
:D*i -0.01886 0.01041 0.04956 -0.01463 0.03422 -2.27783 
D,; 0.06548 0.05770 0.12584 0.01582 0.12988 -16.43747 
:D,3 0.07914 0.16695 0.08621 -0.03268 0.04996 -9.79921 
D,, -0.13765 0.02623 0.01591 -0.05584 -0.07806 10.95966 
D,s -0.03020 0.03477 0.04425 -0.01639 0.02902 - 1.73025 
D,6 -0.03967 -0.00149 0.06081 -0.03513 0.03166 -0.10390 
:::D,7 -0.00389 0.00736 0.06242 -0.00490 0.07810 -5.60468 
D.. -0.04443 -0.02015 0.01308 0.01731 0.02776 -1.27696 
D,9 0.02582 0.02262 0.07885 -0.01364 0.13878 -9.13354 
D,o -0.09988 0.00897 0.03843 -0.02470 -0.00606 2.80880 
D, -0.01090 0.01831 0.02697 -0.01905 0.02736 -0.56044 
D, -0.01955 0.03015 0.05476 -0.01671 0.04205 -3.01416 
154 Exhibit 11/8: A method of direct assessment 
When we apply the discriminants Dau and D. to the 22 Hamilton and 
Madison papers used to construct these functions, we get the results shown in 
Exhibit 11, which have been, arbitrarily, truncated to three decimals. First we 
note that the results for Dn and D. are very similar. Second, when we use the 
value 0.5 as a cutoff, so that higher discriminant function scores are regarded as 
Hamilton forecasts and lower ones as Madison forecasts, we note that all 22 
papers are correctly classified by both Dn and D.. 
Surprisingly good? Hard to be sure, since we used these same papers to 
select this discriminant. Cross-validation is needed. 
Exhibit 11 of Chapter 8 
Value of discriminants D,. and D. when applied to each of the 22 papers 
Group 
of i D,, I I D. I 
papers I HI t M JIll i M I 
1 1.170 0.039 1.206 0.024 
2 0.833 -0.017 0.859 -0.033 
3 1.001 0.338 1.023 0.333 
4 0.764 -0.055 0.777 -0.075 
5 .ooo -o.o5 .o2o -o.o8o 
6 1.052 0.171 1.073 0.172 
7 0.822 -0.209 0.836 -0.227 
8 1.246 -0.351 1.275 -0.374 
9 0.668 -0.157 0.673 -0.167 
10 1.235 0.380 1.263 0.381 
11 1.203 -0.089 1.243 -0.107 
Average 0.999 0.000 1.023 -0.014 
8E. Cross-validation in the Example 
So far the jackknife has provided us with an honest estimate of the variability of 
the coefficients. Let us try next to get an honest estimate of the ability of the 
final discriminant to sort out Hamilton and Madison writings. This is a new and 
major task. 
To do just this, without any assessment of stability, we would have to 
cross-validate. With 11 pairs and 5 variables, we clearly dare not divide the 
data in halves. But, as suggested in Section 2F, we could set aside each pair in 
turn and apply the discriminant based on the remaining 10 pairs to the 
set-aside pair. To do this, we need exactly the discriminants we have just 
8E: Cross-validation/Exhibit 12 155 
calculated for a different purpose. When we apply each D() to the rates for the 
corresponding papers by Hamilton and by Madison, Hi and M, we find the 
Values set out in the left side of Exhibit 12. There is now one misclassification, 
paper M3, whose value of 0.510 just barely assigns it to Hamilton. 
Actually, the behavior of these discriminants, involving 5 coefficients fitted 
to 10 pairs of observations, is surprisingly good. The indicated separation of 
Hamilton and Madison subgroup means amounts to 0.975 -0.015 = 0.960. 
This is a surprisingly large value, but one whose stability we have not, as yet, 
introduced a way to estimate. (If 22 papers have been divided into 11 and 11 at 
random, the average indicated separation of such means would be zero because 
the signs of the differences would tend to be + and - at random.) 
The spread of each group around its observed mean is not small; the 
sample standard deviation falls between 0.28 and 0.31 for both Hamilton and 
Madison. This time, not only do we fail to have an indication of the stability of 
the result, but we are not at all sure that it is not misleading. For instance, all 
the data enter both into the value of D(3) at H3 and the value of D(7) at H7. 
Exhibit 12 of Chapter 8 
Results of applying discriminant functions D( of 
Exhibit 11 to the omitted papers, thus cross- 
validating the discriminants. 
applied to IShift* from Da,, I 
1 1.205 -.044 +.035 -.005 
2 .642 -.004 -.191 -.013 
3 1.025 .510I' +.024 -.172 
4 .592 -.130 -.172 +.075 
5 .993 -.034 -.007 -.017 
6 1.018 .230 -.034 -.059 
7 .792 -.253 -.030 +.044 
8 1.363 -.438 +, 117 +,087 
9 .567 -.091 -.101 -.066 
!0 1 269 .459 +.034 - 079 
11 1.256 -. 124 +.053 +.035 
Average .975 +.015 -.024 -.015 
* + means better discrimination; - means worse. 
t This paper misclassified. 
156 /8: A method of direct assessment 
Accordingly we have no assurance that these values are not correlated in some 
way that makes the average square of their difference substantially different 
from the sum of the corresponding variances. 
By taking the mean-squared deviations of the Hamilton papers about 1, 
and those of Madison about 0 (instead of about the observed means), we may 
obtain indications that, at the price of combining assessment of spread and 
shrinkage, are not exposed to this difficulty, since they are sums of terms each 
of which involves only a single value. (In this example, the numerical values are 
almost the same.) These measures are thus legitimate indications, but they still 
lack any measure of stability. 
By omitting one pair at a time, then, we have had our choice: 
1. estimated stability for overall discriminant function (no estimate of quality 
of performance), as studied in Section 8D; 
2. estimated performance by cross-validation (no estimate of stability for this 
estimate), as studied in the present section. 
8F. Two Simultaneous .Uses of "Leave out One" 
If both quality of performance and stability are to be estimated, we must 
combine cross-validation and jackknifing. This means dropping out one pair for 
each purpose. Accordingly we must find discriminant functions based upon sets 
of 9 = 11- 2 pairs of papers. We denote, indifferently, the discriminant 
obtained when both pair i and pair  are omitted by 
Dco(j ) = Dcots = Dts(i). 
What we are going to do, from either point of view (that of the jackknifing 
or that of the cross-validation), is to set one pair aside and redo the whole 
analysis with the remaining 10 (instead of 11) pairs of papers. 
With pair i set aside for cross-validation, the jackknife leads to pseudo- 
discriminants 
D.(i) = 10D(i) - 9D�:(i) = 10Do- 9Dcoc . 
For each i there are ten D.j(i), and one D*i which is the average of the D.j(i). 
No one of these discriminant functions used papers Hi and Mi in its formation. 
Therefore, when we apply the discriminants to Hi and Mi, we get 2 x 
(10 + 1)= 22 honest cross-validations. 
Exhibit 13 gives the results, cut to three decimals. (Two decimals would 
serve most purposes.) Look first at group 1 in Exhibit 13. The first entry 1.250 
is gotten from D-2(1) applied to H1. Note that all Hamilton values exceed 0.5, 
though the 10th, gotten from D-11(1), had a close call at 0.535. Note that one 
Madison value crosses 0.5. we label this a mistake by the pseudo-discrimin- 
ants. For these two papers, then, the error rate by the pseudo-discriminants 
is 5%. For all papers the error rate by pseudo-discriminants is about 16%. 
8F: "Leave out one"/ Exhibit 13 157 
158 Exhibit 13 (continued)/8: Direct assessment 
o doooddooo d od 
o 
 o 
ooood  
o 
__ oodoo-d o-oo 
 8F: "Leave out one"/ Exhibit 14 159 
For each paper, we would like to know how firmly a decision in favor of 
:Hamilton or Madison is made. Let us conceive of an infinite number of pairs of 
Hamilton and Madison papers, all different from pair i, on which a discriminant 
function D.(i) might be based, and regard the 10 pairs actually used as a 
sample from this' infinite population. The results of applying the infinite- 
Population D.(i) to H and M would be/m and t, the true values for these 
:papers. These are the values toward which our jackknifed answers are aimed. 
The jackknifed values for these papers, say Yi and y for the ith pair, 
combined with the variability of these jackknifed values, assessed from the 
individual pseudo-values by way of s. 2, can be used to set confidence limits on 
the ti and t at any chosen level. Insofar as these limits do not include 0.5, 
:We have clear evidence, at that level, for Hamilton or for Madison. Naturally, 
:then, we want to know how many standard errors each Ym or y is from the 
Cutoff 0.5. 
: In Exhibit 14, which includes values of s. 2, we summarize the results for 
::the 22 papers. 
Note that 8 of Hamilton's papers and 8 of Madison's are beyond two 
standard errors from the cutoff, 1 Madison is at 1.7, while the other 5 have 
only relatively narrow margins. 
We now have cross-validated estimates, combined with an assessment of 
:their stability. Our results, tho.ugh much more useful, are still far from perfect. 
:We have assessed stability, chmbing to the second step of the staircase, but, 
since we have not assessed the stability of this stability, we have not reached 
:the third step. It is not surprising, in view of the small number of papers 
considered, that we have also failed to gain any appreciable information about 
the shape of the distribution of the/i-t (or of the/). 
Exhibit 14 of Chapter 8 
Departures of observed means from 0.5, using s. as the unit 
1 .753 .188 4.0 .490 .128 3.8 
2 � 105 .300 0.3 .563 � 184 3.0 
3 .555 .134 4.1 .006 .137 0.0 
4 .048 .186 0.3 .685 .148 4.6 
5 .529 .159 3.3 .554 .236 2.3 
6 .565 .222 2.5 .437 .119 3.7 
7 .308 .130 2.4 .765 .240 3.2 
8 .905 .217 4.2 .944 .126 7.5 
9 .080 � 123 0.6 .559 � 176 3.2 
: 10 .799 .142 5.6 .062 .084 0.7 
:: 11 .834 .160 5.2 .754 .432 1.7 
160 /8: A method of direct assessment 
8G. Dispersion of the tx's 
But we can learn a little more. If we had an infinite supply of papers to fix the 
discriminants, dropping one pair would not change the discriminants, or their 
coefficients. Accordingly, the tzHi must average 1, and the tzMi must average 0, 
since these constraints apply to the discriminant values before one pair is 
dropped. Thus, on the basis of quantities like 
,(Ym- 1) 2= 1.011, 
we could construct rough variance-component estimates of the standard devia- 
tion of the distribution (over i) of/zui, the value that would be associated with 
Hamilton paper i if we had infinite material for constructing the discriminant 
function. 
On the average, over papers and discriminants, (Ys- 1) 2 will equal 
2 + rr 2 where rr2 is the variance of the/zm, and rr 2 the variability of assessing 
any single paper with a discriminant function based on only a finite number o 
pairs of other papers. We estimate rr 2 by s2.. We estimate the population 
average value of (Ym- 1) 2 directly, pooling the results for the two authors, 
and then estimate rr2 by subtraction. 
We find the average sum of squares of deviations (from 1 or 0, respec- 
tively, for Madison and Hamilton) to be �(1.011 + 0.865) = 0.938. To estimate 
the population average value of (YI - 1) 2, we divide by 11 (instead of the 10 
that would have been appropriate if we had fitted a mean), finding 0.0853, thus 
confirming the possibly biased prediction (in Section 8E) of about 0.30 for the 
standard deviations. 
For the 22 values of s2. we get a total of 0.8364 and an average of 0.0380, 
which estimates rr 2. Subtracting gives 0.0853- 0.0380 = 0.0473 as the esti- 
mate of (r2. Consequently we estimate rr to be about 0.22. For both Madison 
and Hamilton, the mean /z appears to be a shade more than two standard 
deviations from the cutoff of 0.5. Thus, on Gaussian theory and for many other 
distributions, roughly 2.5% of each author's papers would be incorrectly 
assigned if we had only these 5 words on which to base the discrimination bu 
infinite material to determine the discriminant function used. (Our estimate of 
cr is far from precise, being worth perhaps a dozen degrees of freedom.) 
There is now some interest in comparing the mean differences and 
standard deviations associated with different sorts of discriminants and the 
Hamilton and Madison subgroups. We have found these estimates: 
Separation Standard 
of group deviation of 
. means distribution Ratio 
Discriminant based on 10 other pairs 0.96 0.29 3.3 
Discriminant based on oo other pairs 1.00 0.22 4.5 
where again we are dealing with indications of unknown stability. The im- 
provement from basing the discriminant on many more pairs of papers appears 
(8G) 8H: Further discussion of the example 161 
to be rather less than one might have supposed, although the indicated gain is 
substantial. 
Taken separately and as a whole, these results give us a picture of the 
strength and weakness of the proposed discriminant function when it is put into 
practice. Being able to carry it out required a high-speed computer and double 
use of "leave-one-out" procedures, one a jackknife, one a cross-validation. 
Again, let us emphasize that the whole effort here is designed to illustrate 
the jackknife in a complicated problem with modest amounts of data, rather 
than to illustrate the full nature of the Federalist problem with its crucial 
problems of word selection. 
8H. Further Discussion of the Example 
The question of the stability of the estimate of cr might be worth investigating. 
We have here a number calculated by a definite procedure; we seek to assess 
its stability. We could jackknife again. We could drop out one more pair, 
calculate 11 x 10 x 9/6 = 165 discriminant functions, one for each set of 8 
pairs of papers, and work back to 11 more jackknifed estimates of 
Combining these in another grand jackknife, with the one we already have, 
gives us what we sought: an estimate of cry, and an estimate of the stability of 
this estimate. What other technique offers even a crude approximation to an 
assessment of the stability of an estimate of cr ? 
 As still a further step, consider the person who has read the sentence 
(toward the end of Section 8D) suggesting that one rate, that for of, is doing all 
the work, and who asks: what evidence do these data give about the superiority 
or inferiority of the 5-variable discriminant function to the use of of alone? 
To answer such a question thoroughly requires double cross-validation 
(see Section 2F), rather than single. We ought to study the question's answer 
ion other data than those that suggested it. 
 But if this cannot be done, and we must peer into the same 22 papers 
again, there is no difficulty in attaining a single cross-validation with assessed 
Stability. We have only to turn to each set of 9 pairs and determine two 
discriminant functions, one based upon all 5 words (already done), one based 
upon of alone (very easy). Turning to each omitted pair in turn, we can 
calculate, for each paper, 
: 1. the behavior of the first discriminant, 
:: 2. the behavior of the second discriminant, 
3. the difference in behavior, an indicator of which is better. 
The jackknifing can then proceed on quantities of type (3), producing, for 
each paper, both an estimate of improvement, and an assessment of the 
Stability of this estimate. If our original question is really about how the two 
discriminants would compare if each could be based on a very large population 
of papers, this would give a relevant and useful answer. 
162 /8: A method of direct assessment 
Further references on the jackknife are Quenouille (1956), Tukey (1958), 
Durbin (1959), Mickey (1959), Kendall and Stuart (1961), Brillinger (1964), 
Miller (1964), Robson and hitlock (1964), Jones (1965), Gray and Adams 
(1972), Gray, Watkins, and Adams (1972), Egman, Meyers, and Bendel (1973), 
Frawley (1974), Jones (1974), Miller (1974a), 45 references in a review by 
Miller (1974b), and Wainer and Thissen (1975). There have also been generali- 
zations to the jackknife (see Gray and Shucany, 1972). 
Summary: The Jackknife 
The jackknife is designed to do any of many jobs fairly well, but it should be 
remembered that a special method may do better for a specific kind of analysis. 
We use the jackknife both for estimation and to estimate variance. 
We can learn which pieces it is natural and reasonable to divide a given 
body of data into. 
We can calculate pseudo-values, jackknifed value, and s2., and go on to set 
(confidence) limits on the corresponding estimand. 
The major difficulties with the elementary jackknife arise (i) when the 
distributions of interest have excessively straggling tails, and/or (ii) when only a 
few different numbers appear as pseudo-values. (Occasionally the methods ot 
Chapter 10, to come, will let us deal with difficulty (i). Rearranging the size and 
mutual relation of the pieces involved will often deal with (ii).) 
Where jackknifing of various values comprising an overall result proceeds 
in a parallel way, we can frequently think of the jackknife as applying to a 
whole function or a whole situation (rather than to one class of "equivalent 
numbers" at a time). 
It is desirable to avoid (in jackknifing) sampling distributions with (i) 
abrupt ends and (ii) one or more straggling tails, and it is probably desirable to 
avoid those that are strongly unsymmetrical (often avoidable by re-expression). 
We can use the jackknife to provide estimates of the uncertainties at- 
tached to the performance of a discriminant function, as well as those of its 
coefficients, particularly by combining leave-out-one jackknifing and leave-out- 
one cross-validation. (The same would apply to results of many other analytical 
techniques, as well as to discriminant functions.) : 
We could, by bringing in a third leave-out-one operation, specifically 
another jackknife, add, to the two assessments just mentioned, an assessment 
of the uncertainties in our assessed quality of performance for the discriminant 
function. (This requires, overall, a "leave-out-three" technique.) 
We followed through the jackknife calculations in a variety of small 
examples and in one of greater complexity. 
References 
Brillinger, D. R. (1964). "The asymptotic behavior of Tukey's general method 
of setting approximate confidence limits (the jackknife) when applied to 
maximum likelihood estimates." Rev. Int. Statist. Inst., 32, 202-206. 
References 163 
Cochran, W. G. (1953), Sampling techniques. New York: Wiley. (2nd ed., 
1963.) 
Durbin, J. (1959). "A note on the application of Quenouille's method of bias 
reduction to the estimation of ratios." Biometrika, 46, 477-480. 
lEgman, R. K. C E. Meyers, and R. Bendel (1973). "New methods for test 
selection and reliability assessment, using stepwise multiple regression and 
jackknifing." Educ. Psychol. Meas., 33, 883-894. 
Frawley, W. H. (1974). "364: Using the jackknife/in testing dose responses in 
proportions near zero or one ..... revisited." Biometrics, 30, 539-545. 
(3ray, H. L., and J. E. Adams (1972). "Jackknifing stochastic processes." Texas 
J. Sci., 23, 559. 
Gray, H. L., and W. R. Shucany (1972). The generalized jackknife statistic. 
New York: Marcel Dekker, Inc. 
Gray, H. L., T. A. Watkins, and J. E. Adams (1972). "Jackknife statistic, its 
:extensions, and its relation to e-transformations." Ann. Math. Star., 43, 
1-30. 
Jones, H. L. (1965). "The jackknife method." In Proc. IBM Scientific Com- 
puting Symposium on Statistics, October 21-23, 1963. White Plains, New York: 
IBM Data Processing Division; pp. 185-201. 
(1974). "Jackknife estimation of functions of stratum means." Biomet- 
rika, 61, 343-348. 
Kendall, M. G., and A. Stuart (1961). The advanced theory of statistics. Vol. 2: 
Inference and relationship. London: Charles Griffin & Company; pp. 5-7. 
Mickey, M. R. (1959). "Some finite population unbiased ratio and regression 
estimators." J. Amer. Statist. Assoc., 54, 594-612. 
Miller, R. G., Jr. (1964). "A trustworthy jackknife." Ann. math. Statist., 35, 
1594-1605. 
(1974a). "A unbalanced jackknife." Ann. of Star., 2, 880-891. 
(1974b). "The jackknifea review." Biometrika, 61, 1-15. (45 refer- 
ences) 
Mosteller, F., and D. L. Wallace (1963). "Inference in an authorship problem." 
J. Amer. Statist. Assoc., 58, 275-309. 
(1964). Inference and disputed authorship' The Federalist. Reading, 
Mass.' Addison-Wesley. 
Quenouille, M. H. (1956). "Notes on bias in estimation." Biometrika, 43, 
353-360. 
Robson, D. S., and J. H. Whitlock (1964). "Estimation of a truncation point." 
Biometrika, 51, 33-39. 
Tukey, J. W. (1958). "Bias and confidence in not-quite large samples." 
Abstract in Ann. math. Statist., 29, 614. 
 (unpublished). "Data analysis and behavioral science." Princeton Uni- 
versity and Bell Telephone Laboratories. 
Wainer, H., and D. Thissen (1975). "When jackknifing fails (or does it?)." 
Psychometrika, 40, 113-114. 
164 Index for Chapter 9 
9A PLUS analyses 165 
9B Looking at two-way PLUS analyses 169 
9C Taking advantage of levels 172 
9D Polishing additive fits 178 
9E Fitting one more constant 190 
9F Using re-expression 19, 
9G Three- and more-way analyses 200 
Summary: Two-way tables of responses 201 
References 202 
Chapter 9/Two- and 
More-Way Tables 
A large and important class of analyses starts from two or more "handles" on 
the data. Here we start with the two-way case and regard the data as responses, 
so to speak, to two factors. The simplest case arises when the two handles can 
be grasped separately. Here the handles are expressed by the location of each 
response in a row and a column and thus by the "values" of the two factors. 
We attend briefly also to the case of three or more handles that can be grasped 
separately and thought of as rows, columns, and layers- by the "values" of 
three or more factors. (We can, if we are careful, still write out the data on a 
single sheet of paper in such a way as to make its structure clear.) The 
extension to larger numbers of handles that can be grasped separately is not 
difficult. 
Let us suppose that the fit in a two-factor analysis has the form 
fit -- common PLUS row PLUS column 
where "common" stands for a value applied to every cell, "row" stands for a 
value depending only upon the cell's row, and "column" stands for a value 
depending only upon the cell's column. As always, we have 
residual = data MINUS fit 
where "data" is the value of the response in the cell. Thus we can write exactly 
data -- common PLUS row PLUS column PLUS residual. 
We also take one short step beyond this, but for still other fits, we refer the 
reader to EDA, Chapters 11 and 12, and to McNeil and Tukey (1975). 
9A. PLUS Analyses 
Let us start with a one-way example. Climatography of the U.S. (#60-44, 
Climatography of the States: Maryland; page 9) reports the monthly mean 
temperatures Fahrenheit in Washington, D.C., for January to July to be 
Jan. Feb. March April May June July 
36.2 37.1 45.3 54.4 64.7 73.4 77.3 
165 
166 /9: Two- and more-way tables 
Their median is 54.4. Their mean is 388.4/7 = 55.5. We can now write down 
the data and a summary as either 
36.2 37.1 45.3 54.4 64.7 73.4 77.3 54.4 
or 
36.2 37.1 45.3 54.4 64.7 73.4 77.3 I 55.5 
with the form 
data values I summary 
where we have used a single vertical bar to cut off our summary from the 
values it summarizes. 
The next step is to use the summary as a fit, and change the data values 
into residuals, by subtracting the fit from them. Since the relation of the two 
parts is now changed they now have to be added together to reproduce the 
datarowe use a double line to cut them off. For our median and mean 
examples, we get 
-18.2 -17.3 -9.1 0 10.3 19.0 22.9 II 54.4 
or 
-19.3 -18.4 -10.2 -1.1 9.2 17.9 21.8 [[ 55.5 
accordingly as we take out the median or the mean. 
We may, if we wish, take out any other number, whether or not it is a 
well-defined summary. Thus, we might like to take out 55, or 50 (as a round 
number), obtaining 
-18.8 -17.9 -9.7 -0.6 9.7 18.4 22.3 II 55 
or 
-13.8 -12.9 -4.7 4.4 14.7 23.4 27.3 II 50 
respectively. 
In each illustration, we have separated the given values into a sum of two 
contributions 
data = fit PLUS residual 
or, as we could write, in these especially simple problems, 
data = common PLUS residual 
We want next to do the same thing, apportion the data into a sum of more than 
two terms, when we have two handles--two factors whose values or names 
describe each of the numbers each of the responses to be analyzed. We turn 
to such an example. 
9A: PLUS analyses 167 
Two-[actor descriptions. If, for each of three East Coast places, we start anew 
with mean monthly temperatures from January through July, we can do what 
we just did separately for each place, as shown in Panels A and B of Exhibit 1. 
Once we have done this, we see, in much more detail, how the data behave. 
Looking at Panel A, it is clear that it is warmer in July than in January, and 
warmer in Laredo (Texas) than in Caribou (Maine). We can now see, in Panel 
B, that the change from January to July (in �F) is larger in Caribou than in 
Laredo, and that Washington falls in between. 
We have taken a place effect out of each rowwe can go ahead and take a 
month effect out of each column. Panel C shows the result of writing down the 
medians of the columns, Panel D that of taking them out. 
We now have a 
two-way analysis 
where we have broken down the data as 
common PLUS row PLUS column PLUS residual, 
expressing each value as the sum of four terms. For January in Caribou, for the 
analysis of Panel D, this gives 
(54.4) + (-19.7) + (-18.3) + (-7.7) = 8.7 (check!) 
The sum gives the original measurement of 8.7, as it should. For May in 
Washington, Panel D gives 
 (54.4) + (0.0) + (10.3) + (0.0) = 64.7 (check!) 
and it adds to 64.7 as it should. 
Just as we had many choices in a one-way analysis taking out whatever 
value we want to remove so in the two-way analysis we have many, many 
choices--taking out whatever row effects (here place effects) and whatever 
Column effects (here month effects) we wish. 
Panel E shows an alternative analysis of the same data, taking out effects 
that are multiples of 5�F. The "residuals" are not so small, but the terms are 
especially easy to talk about and summarize. (Note that, for Caribou in 
January, we have (55)+ (-20)+ (-20)+ (-6.3), which still adds up to 8.7, 
just as it should.) 
We plan to use the pattern 
row 
residuals effects 
column effects common 
as a standard effective way to break down a two-way table. We emphasize such 
points as these: 
1. The breakdown does, carefully and quantitatively, what we often try to do 
 qualitatively, when we "eyeball" the table. 
168 Exhibit 1/9: Two- and more-way tables 
Exhibit I of Chapter 9 
A two-way example: median monthly temperatures. Medians are used for the fit 
in Panels A through D. 
A) The DATA and some medians: data lmedians 
I Jan. I Feb. i I Marc-hi lAprill I Uayll Junell July l 
Caribou 8.7 9.8 21.7 34.7 48.5 58.4 64.0 34.7 
Washington 36.2 37.1 45.3 54.4 64.7 73.4 77.3 54.4 
La redo 57.6 61.9 68.4 75.9 81.2 85.8 87.7 75.9 
B) The same ANALYZED ONE WAY: residuals ii medians 
Caribou -26.0-24.9-13.0 0 13.8 23.7 29.3 II 34.7 
Washington -18.2 -17.3 -9.1 0 10.3 19.0 22.9 54.4 
Laredo -18.3 -14.0 -7.5 0 5.3 9.9 11.8 75.9 
C) And some medians the OTHER WAY: residuals medians for rows 
median of grand median 
column residuals 
Caribou -26.0 -24.9 -13.0 0 13.8 23.7 29.3 34.7 
Washington -18.2 -17.3 -9.1 0 10.3 19.0 22.9 54.4 
Laredo -18.3 -14.0 -7.5 0 5.3 9.9 11.8 75.9 
-18.3 -17.3 -9.1 0 10.3 19.0 22.9 54.4 
D) And with these TAKEN OUT also: row terms 
(residual 
residuals row medians) 
column terms common 
(medians of (median of row 
column residuals) medians) 
Caribou -7.7 -7.6 -3.9 0 3.5 4.7 6.4 -19.7 
Washington .1 0 0 0 0 0 0 0.0 
Laredo 0 3.3 1.6 0 -5.0 -9.1 -11.1 21.5 
-18.3 -17.3 -9.1 0 10.3 19.0 22.9 54.4 
E) An ALTERNATIVE ANALYSIS 
Caribou -6.3 -10.2 -3.3 -.3 3.5 3.4 4.0 -20 
Washington 1.2 -2.9 .3 -.6 -.3 -1.6 -2.7 0 
Laredo 2.6 1.9 3.4 .9 -3.8 -9.2 -12.3 20 
-20 -15 -10 0 10 20 25 55 
(9A) 9B: Looking at two-way PLUS analyses 169 
! 
2. Not only does the breakdown usually do it better, but it exposes the 
 residuals to our view now we can "eyeball" them to see what else is 
going on moreover, we can, and usually should, subject them to further 
quantitative analysis. 
3. We now have more numbers in our table than we started with, a general 
: phenomenon, essential if we are to both analyze and preserve the original 
data. At some stage we may replace either all residuals or each of several 
groups of residuals by summary descriptions, thus decreasing the number 
of numbers we attend to, but we should do this only after we have looked 
hard at the residuals. 
: 4. The approach offers great freedom of choice of analysis. Standard versions 
have important uses, but we may choose the analysis that helps us most. 
:: When we deal with any such breakdown, we can always call the parts 
� ::; "contributions," or "terms" .neutral words. Sometimes, especially when we 
have been careful in making the fit, we call them "effects". To use "effects" 
gives notice that we are trying to go beyond describing the data, trying to 
estimate as best we can something that lies behind the particular data before 
:us, possibly even arguing for causation, or suggesting that we have in mind a 
::structural model that the fitting is being used to estimate. 
9B. Looking at Two-Way PLUS Analyses 
Given the two-way PLUS analysis just presented, we need to answer 
<,what is the fit like? 
:: <,what are the residuals like? 
:iWe will soon see how answering the first helps answer the second. 
:: Consider the alternative analysis of Panel E in Exhibit 1, namely 
(Residuals omitted) -20 
= 0 
: -20 -15 -10 0 10 20 25 55 
The same fit can be written in many ways, including: 
(Residuals omitted) 35 
75 
-20 -15 -10 0 10 20 25 Not used, or O, 
whichever inter- 
pretation is 
preferred 
170 Exhibit 2/9: Two- and more-way tables 
which shows the fit for any month-place combination as the sum of only two terms, 
one determined by the place (here, 35, 55, or 75) and one determined by the 
month (here, -20 to +25). 
Let us start a plot with a month-axis and a place-axis. The beginning looks 
like Exhibit 2. Because we have used the same scale (in �F) on the two axes, we 
can now look at the effects of changing place or of changing month (the 
differences in place contribution or in month contribution) and see them as 
pictorial sizes, not just as numbers. 
We could therefore enter the values of the residuals on the picture of 
Exhibit 2, as shown in Exhibit 3. You may well be able to see the general 
features of the table of residuals by studying these numbers. Probably you can 
bring them out better by coding them and making them more visual. Just what 
coding seems best may vary with the problem, but 5 to 7 classes seems a good 
number. We have made a stem-and-leaf of the residuals and found the median 
-0.3, and the difference between the upper and lower 25% points, I = 5.2. 
Exhibit 2 of Chapter 9 
Starting to picture the fit from Panel E of Exhibit I 
3[ 4 5F 5 7Y 
go ..... 3'une 
tO ....... a.y 
0 .... / pril 
-I0 , March 
- Feb 
-2o .Ten 
Caribou W'ashinco Laredo 
 9B: Two-way PLUS analyses/Exhibit 3 171 
We chose, as a central interval, M + I, then, as the next interval, items falling 
between there and M + -}I (beyond the central interval and within the hinges), 
"ext M +/, and beyond that. We also assigned the following symbols 
to to to to Above 
i Below M - I M - -}1 M - -}1 M - -}1 to M + �1 M + -}1 M + I M + I 
The symbols are probably satisfactory for other occasions, but the intervals 
:Chosen might often be varied. 
:: When we use these cutoffs, we get Exhibit 4. It shows strikingly, if Exhibit 
:3. did not, that opposite corners of the rectangle have residuals of the same sign 
Exhibit 3 of Chapter 9 
Picture of the fit with residuals from Panel E of Exhibit 1 
: 4.0 
:0 . 3.,� -I, 6 -0.2 June 
:10-- 3. S =3 -3.8 --/da3 
172 Exhibit 4/9: Two- and more-way tables 
and that the middle of the table has been fit rather closely. This is a fairly 
common pattern for residuals after fitting row PLUS column PLUS common. 
Exhibit 4 of Chapter 9 
Residuals (of Exhibit 3) coded 
-to o � + , March 
() o + lreb 
� . + Jan 
Caribou Was hln,tn Largo 
9C. Taking Advantage of Levels 
Our fit is still of the form 
common PLUS row PLUS column 
(where we have made the common zero). Let us consider all combinations of 
place and month that give a chosen value to the fit, say 55�F. As Exhibit 5 
shows, these combinations fall exactly on a diagonal straight line. 
;y.:�.... 9C: Taking advantage of levels/Exhibit 5 173 
1. Draw a selection of constant-fit lines for other constants as well as for 
55�F � 
� ?.'".2. Put in +'s for the actual combinations of places and months; 
� './.3. Look at the result see how it would be simplified by being turned 
:".':'.> throu 45 �- and draw it so, 
:i::i:4. Pull back the constant-fit lines to the sides of the new picture; 
/...:.".::5. Replace the many crosses by the intersections of a smaller number of lines, 
:.:": one for each row and one for each column. 
:::.:::::i. Exhibit 5 of Chapter 9 
Exhibit 2 with 55�F line added. 
... Feb 
20 -- :ran 
� .: I I I I 
::::.� r b Wash;nton Largo 
174 Exhibit 6/9: Two- and more-way analyses 
Exhibit 6 shows the results after steps (1) and (2), and Exhibit 7 after all five 
are completed. 
The final picture differs from many graphs in that we do not try to 
interpret horizontal position; only vertical position, which shows the fitted 
value, is of direct importance to us. 
Residuals. Now that the fit has been conveniently arrayed, it offers a natural 
place to put the residual for each combination of row and column in our 
example, each combination of place and month. Exhibit 8 shows the residuals 
left by our alternative fit, each located at the place where the corresponding 
place and month lines in Exhibit 7 cross. We have now put the residuals at 
sensible places, and we get a good visual feel from them. 
Exhibit 6 of Chapter 9 
Exhibit 5 with more constant-fit lines and the data points individually marked. 
?_0 -- .Tune 
-I0 Marct 
  Feb 
CerC ,ou Was hnn Lar edo 
9C: Taking advantage of levels/Exhibit 7 175 
i..i.i Some may prefer to see these residuals coded in a different way. We can 
Code our values and show them on a figure like Exhibit 7. Let us use the same 
::..Coding and intervals as in Exhibit 4, except that, when we turn the picture of 
the fit through 45 � , we turn the crosses also: 
:::. � O o � x 
These symbols, applied to Exhibit 7, appear in Exhibit 9. 
Exhibit 7 of Chapter 9 
Exhibit 6 with added changes: (3) turned through 45 �, (4) the constant lines 
'Shown at edges, (5) the 3 x 7 = 21 crosses replaced by 3 + 7 = 10 lines, one for 
each place and for each month. 
--7 � 
75-� . 
35'-- 35 � 
176 Exhibit 8/9: Two- and more-way tables 
Because this coding shows our residuals to be highly structured, in that 
alternate corners have alternate signs, 
we need to carry the fitting further. 
Exhibit 8 of Chapter 9 
Exhibit 7 with the residuals plotted vertically 
75' -2,7 -3,8 ,: 75' 
.9 
- 4,0 
3.4 
3.5 
-.3 -2.5 
-3.3 
-10,2 
-, 
9C: Taking advantage of levels/Exhibit 9 177 
In thinking over this display of residuals, bear in mind that (1) these data 
will eventually be seen to be very well fitted rather simply, and (2) we started 
On these data knowing a lot about the effects of places and months (Laredo 
hot, Caribou cool; January to July warming, most rapidly near April)--we 
Exhibit 9 of Chapter 9 
Exhibit 7 with coded residuals 
o 
/: 
x 
x 
178 /9: Two- and ,more-way tables 
could have put places and months in order without seeing the data, and thus 
gone a long way toward Exhibit 9. For most sets of data we will not know so 
much and we may need the plot of the two-way fit, as in Exhibit 7, to organize 
a picture of the residuals. Vertical position lets us see easily what happens if we 
change months or change places or both. We have a good hold on the entire 
behavior of the fit. 
We did this for one fit to one set of data. We can do it for any fit of the 
form 
(common PLUS row) PLUS column 
to any set of data, since we can convert to 
(common PLUS row) PLUS column 
and do exactly what we have done in this example. 
9D. Polishing Additive Fits 
When we use means to carry out the fitting in a two-way table, the effects for 
rows and columns can be found in one "iteration." Further iterations leave the 
results unchanged. When we use other kinds of averages, such as medians, the 
first iteration may not be adequate. In this section, we illustrate this point. 
Along the way, we encourage the reader to compute residuals and examine 
them as a routine action in data analysis. In exemplifying the iterative approach 
and attention to residuals, we consider how means and medians treat residuals 
in a table with an outlier and how they treat a table with an empty cell. 
Although it may sometimes seem like extra computing trouble, routine 
good practice requires the calculation and analysis of residuals, 
oto appreciate the fit itself, 
o to look for peculiarities and interpretations, 
o to make comparisons with other analyses, 
o to check for blunders. 
For exploratory data analysis, we recommend starting with medians, but 
of course, other averages such as the mean may be especially appropriate in a 
particular problem. Similarly, re-expressions such as logarithms may improve 
the analysis. 
The basic approach. Up to this point in analyzing two-way tables, we have 
taken out row effects, then taken out column effects, and stopped. Ordinarily 
this one full step of iteration will do a great deal of fitting, but it may not finish 
the job, perhaps because the method used basically requires iteration, perhaps 
because of rounding problems when we calculate to a given number of 
decimals, or perhaps because of holes in the table. 
9D: Polishing additive fits 179 
For the steps in an iteration, we use single lines to separate data at one 
!::?.Stage from summaries. Then we use double lines to separate the summaries 
i:.:i:.'.'..from the residuals. Thus, the sequence would go: 
First Second Third Fourth Fifth 
: original row or residuals column or new 
i...data in block column summary in block row summary residuals 
and so on. When we use rows first and then columns, the physical layout for 
:::.two-way tables is convenient if we move snakewise left to right, down, left, 
.:down, right, as in this pattern: 
First Second II Third 
row residuals 
original block summary from rows 
Fourth 
:ii.:.. column summary 
Seventh Sixth Fifth 
:"' row row new residuals 
residuals summary from columns 
'""' Eighth... 
To illustrate this method, Exhibit 10 shows an example whose median fit 
:::requires more than one iteration to be complete. Recall that the original 
'::'.'example in Exhibit 1 was completed after a single cycle. It will be simplest to 
think of it as another set of monthly temperature data for the same three 
::::11. places. Although with the previous explanation, Exhibit 10 largely spells itself 
:i;'..'0ut, it may be worth mentioning the steps and details. 
iFirst, start with the temperature data I 
Second, get row medians [I 
:.: Third, subtract row medians from cells in their row 
Fourth, get column medians 
Fifth, subtract column medians from residuals in columns I 
:Sixth, get row medians and use check mark (/) if median is zero II 
180 Exhibit 10/9: Two- and more-way tables 
9D: Polishing additive fits 181 
Seventh, get row residuals 
:Eighth, get column medians, use check marks 
Ninth, get column residuals I 
: Tenth, get row medians and stop, 
because row medians are all checks and step 9 made all column medians 
:: Checks. This completes Panel A. 
In Exhibit 10 we introduce the idea of a half-step. We call finding 
(computing and writing down) a row (or column) median and then finding the 
:::residuals a half-step. Thus, pairs of instructions listed above represent half- 
steps, i.e., the second and third, the fourth and fifth, and so on. For Panel B, 
We assemble the place (city) fit by adding the corresponding terms for each 
iteration, here the first and third half-steps. The third row was the only one 
:changed in third half-step. We could stop there, or we could break up these 
:i:row medians into the sum of two parts; the first is a median of row medians, 
and the second is the row effects. To do this, we first take out the median of the 
:row medians, 54.4. That median plays the role of a grand mean, of a common 
term, and of a baseline value. 
The pieces of the assembly of the monthly fit for Panel C are a little harder 
to find because they don't line up as neatly as the row effects do; only May and 
June were affected in the fourth half-step. 
In our example, we want four half-steps. Our experience is that four to 
eight half-steps are ordinarily enough. Since arithmetic errors in this iterative 
process are easy to make and not self-correcting, it is well, whenever reasona- 
ble, to have a computer do this work and print it out. Although it is possible to 
get slow drifts that could seem to be asking for many half-steps, it may not be 
wise to follow them to the bitter end. For a complete table, one may want to 
pick some number of half-steps, 4, 5, or 6, say, and quit after that. (A table 
with holes may require several more half-steps.) 
Fitting by means. We can use the same approach with means--or with any 
other kind of summary. Exhibit 11 shows first mean polish and then median 
polish for another set of temperature data. We make two half-steps before 
quiescence (nearest 0.1�F) using means, and four half-steps using medians. 
This result is in agreement with the theoretical demonstration that analysis 
by means requires only two half-steps. It may surprise the reader to note that 
sometimes analysis by means takes longer than two half-steps. The reason that 
this "contradiction" occurs is that our theoretical demonstration applies to 
arithmetic using exact values infinite decimal places or exact fractions. It does 
not apply when we calculate to a fixed number of decimals and round our mean 
accordingly. Although we rounded in Exhibit 11, analysis by means still 
required only two half-steps, as analysis by means often will. 
182 Exhibit 11/9: Two- and more-way tables 
 9D: Polishing additive fits/Exhibit 12 183 
:: Exhibit 11's purpose is to allow the comparison of the two sets of 
:residuals. If we look at the final residuals of Panel A (analyzed by means), we 
see an undistinguished mess, in which 13 of the 21 residuals are greater than or 
equal to 1 in absolute value. Only with the utmost concentration might we see 
informative patterns among these residuals. The mean spreads the deviations 
around, thus making the sum of their squares small, as we illustrate again later 
in this section. 
The final residuals of the analysis by medians, as we have already noticed, 
are much more interpretable' Only 6 of the 20 residuals are greater than 1 in 
absolute value, and all 6 of these, together with one zero, make up the 7 
residuals for Laredo. Thus we learn that Caribou and Washington are in very 
good agreement, and we should start thinking about Laredo. 
If we draw the line at a size of �0.5, we still have 7/21 for the analysis by 
medians, but now count 17/21 for the analysis by means. 
Exhibit 12 shows the stem-and-leaf displays for the two sets of final 
::residuals from Exhibit 11, compared with each other and with the final 
Exhibit 12 of Chapter 9 
Stem-and-leaf displays for three analyses of the same data (two analyses are 
from Exhibit 11) 
Analysis by Analysis by Analysis by 
means medians midrange 
6 6 6 
5 5 5 
4 4 4 
3 36 3 
2 128 2 09 2 5414 
I 276 I I 37 
0 41872 0 004070001 0 0666 
-0 694 -0 104400 -0 356 
-1 023409 -1 9 -1 7053 
-2 -2 -2 1535 
-3 7 -3 2 -3 
-4 -4 -4 
-5 -5 6 -5 
-6 -6 -6 
High 2.8 3.6 2.5 
Low -3.7 -5.6 -2.5 
Range 6.5 9.2 5.0 
Sum of squares 51.04 71.57 58.57 
Sum of absolute 
values 27.2 21.3 30.5 
184 /9: Two- and more-way tables 
residuals obtained by taking midranges (mean of the highest and lowest) as 
chosen summaries. In addition to the stem-and-leaf displays, we also give three 
measures of spread: range, sum of squares, and sum of absolute values. Each of 
the three analyses beats the other two according to exactly one of these 
criteria, just as they should. 
Unusual talues. To continue our discussion of means and medians, we look at 
their comparative performance in a very pure case--in a two-way table that has 
all values identical except exactly one. We do this in a 3 x 7 table. We lose 
nothing in the illustration by making all values 0 except the center one and by 
making it 21 (so it is divisible by 3 and 7). Putting the odd value in the center 
promotes symmetry and visual contrast but makes no other difference. 
0 0 0 0 0 0 0 
0 0 0 21 0 0 0 
0 0 0 0 0 0 0 
1. Median analysis. The medians of all rows and columns are zero and the 
original table is also the residual table. (It is not the speediness of this analysis 
that recommends it, for that criterion would leave all tables untouched.) 
2. Mean analysis. Here we find 
0 0 0 0 0 0 0 x/ [[ 0 0 0 0 0 0 0 
0 0 0 21 0 0 0 3 -3 -3 -3 18 -3 -3 -3 
0 0 0 0 0 0 0 x/ 0 0 0 0 0 0 0 
-1 -1 -1 6 -1 -1 -1 
/ 1 1 1 -6 1 1 1 
4 -2-2-2 2-2-2-2 
4 i 
Thus what started as a table well summarized as 2! values, 20 of one kind and 
! of another, winds up with 4 kinds of values, distributing the deviations all 
over the table. 
One might say, of course, that, if one had such a table, the fact would be 
recognized and means would not be used. Such a plan for recognizing outliers 
is more easily requested than executed. When the data are many and compli- 
cated, situations like the one revealed so starkly in the example may be well 
camouflaged, and a great deal of sophisticated scanning may be required to 
discover outliers. 
9D: Polishing additive fits/Exhibit 13(A) 185 
Another position would be that part of the purpose of an analysis is to 
reallocate the deviations and that the pattern of residuals produced by the 
analysis by means is one satisfactory analysis. And an analysis that doesn't 
reallocate variation at all may not be appealing. 
Our purpose here is not so much to settle this sort of argument as to make 
clearer how the two methods respond to outliers, the median tending to leave 
them as spikes, the mean tending to reduce them and spread the deviations 
around. The mean analysis has a final panel of residuals where absolute values 
sum to 48. Thus, using the mean analysis led to more than doubling the total 
absolute deviation (48 versus 21), at the same time that the sum of squared 
residuals was reduced from 441 --- (21) 2 to 252 = (15.9) 2. If we were sure that 
we wanted to minimize the sum of squares, we would use the mean; if we want 
to minimize the sum of absolute values, we would gain much more, in this 
instance, by using the median (48/21 = 2.3 but 21/15.9 = 1.3). 
The example suggests that it takes only a few large outliers in a table to 
allow an analysis by means to conceal what would otherwise be a rather easily 
recognized structure structure on a scale intermediate between that of the 
outliers and that of the usual irregularities. 
Holes. Sometimes tables have empty cells, cells with no observation, or some- 
times we wish to set an observation aside and see what would have been 
estimated in its absence. When we do the two-way iteration with some empty 
cells, we treat each row or column according to the number of cells it has 
present. We take the summary for the filled cells. Usually this situation leads to 
more iterations. 
We analyze Exhibit 13 both with and without the entry 1035, using both 
medians and means, four analyses in all. Panel B of Exhibit 13 shows the mean 
coping with the outlier by spreading error over the table, and the median 
leaving the spike pretty well in place. When the outlier cell is removed, Panel C 
shows that the residuals for the two methods are much more nearly compara- 
ble, though the median has 10 residuals smaller than 10 in absolute value and 
Exhibit 13 of Chapter 9 
Analysis of a set of data on wheat, ratio of dry to wet grain (x 1000), by 
medians and means, with and without one cell value, 1035, present. 
A) Original data 
Ielockt INonel ! Early t ! Middlel lLate ! 
I 718 732 734 792 
2 725 781 725 716 
3 704 1035 763 758 
4 726 765 738 781  
186 Exhibit 13(B-S)/9: Two- and more-way tables 
the mean has only 6. Of course, this is a feature of the median to give more 
very small and more very large residuals than the mean does. 
Even more important are the changes of column (treatment) effect due to 
omission of the anomalous value. These are 0, -12, 0, 14 for median fitting, 
but 17, -51, 17, 17 for mean fitting. This shows how much less sum of 
squared changes of 313 instead of 3468--the one anomalous value perturbs 
the column (treatment) effects when medians are used. 
Fitting by means and medians together. We have been comparing the virtues 
of mean, median (and midrange) analyses. Sometimes a combination of 
methods may be used, as the following example illustrates. 
Example. Seasonal adjustment. Exhibit 14 shows the logarithm of the indices 
of department-store sales, by months, from 1941 through 1945, with the mean 
of 1935 through 1939 set at 100. The entries under the years in the left half of 
Exhibit 13 (continued) 
B) With cell present 
Medians Means 
-1 -38 1 33 -3 18 -78 12 48 -18 
7 12 -7 -42 -4 32 -22 10 -21 -25 
-30 250 15 -16 12 -67 154 -30 -57 53 
I -11 -1 16 3 18 -53 8 29 -10 
' 
-27 24 -13 13 749 -44 66 -22 0 762 
C) With one cell (value = 1035) omitted 
Medians Means 
-1 -26 1 19 0 1 -27 -5 31 -1 
7 24 -7 -56 -1 15 29 -7 -38 -8 
0 I-I 45 0 -15 -16 I-] 21 -6 2 
0 0 -2 1 7 0 -3 -10 11 8 
-27 12 -13 27 II 746 -27 15 -5 17 745 
S) SOURCE 
W. G. Cochran (1947). "Some consequences when the assumptions for the analysis of variance are not 
satisfied." Biometrics, 3, p. 27. With permission from the author and from the Biometric Society. 
* 9D: Polishing additive fits/Exhibit 14 187 
 Exhibit 14 are actually 10001ogo(index/100). (We use logarithms because 
financial movements are most often multiplicative rather than additive in their 
effects. Considerable experience encourages using the logarithm, even though 
the fit to raw values would be about as satisfactory in this example.) Returning 
to Exhibit 14, we observe that the numbers are growing larger as the years go 
:by. To see the degree of periodicity by years, we rank the values for months 
within each year in Exhibit 15, and compute the median rank for each month 
and the range of that month's ranks. 
For six of the months, the range of the ranks is 2 or less, October, 
November, and December being especially notable for the stability of their 
:ihigh ranks. January and July compete for the lowest sales. February, March, 
and August have the largest ranges. Notice especially the sharp break between 
each December and the following January. This means that we cannot expect 
to fit a smooth curve across years to data that have not been seasonally adjusted. 
::. To fit the original log data of Exhibit 14, we first remove the 5-year 
median for each month and take the residuals in each row, as shown in the 
righthand side of Exhibit 14. The regression model we plan to fit is, except for 
Exhibit 14 of Chapter 9 
Removing the median monthly effect from the log values of the department- 
Store sales index 
1941 1942 1943 1944 1945 Residua! = 
I :M�nths Ii log y ! [ Median* I Iog y - median 
Jan. -41 93 111 140 193 111 -152 -18 0 29 82 
Feb. -13 68 193 152 233 152 -165 -84 41 0 81 
Mar. 45 146 158 230 328 158 -113 -12 0 72 170 
APr. 114 149 215 238 243 215 -101 -66 0 23 28 
May 121 124 193 250 262 193 -72 -69 0 57 69 
June 93 93 190 212 270 190 -97 -97 0 22 80 
July 4 29 104 152 215 104 -100 -75 0 48 111 
Aug. 117 104 146 196 225 146 -29 -42 0 50 79 
Sept. 179 207 241 292 320 241 -62 -34 0 51 79 
Oct. 140 230 272 320 364 272 -132 -42 0 48 92 
Nov. 199 274 330 394 438 330 -131 -56 0 64 108 
Dec. 364 418 438 507 548 438 -74 -20 0 69 110 
Mean residual in column -102 -51 3 44 91 
* Median over five years for each month. 
188 Exhibit 15/9: Two- and more-way tables 
error'terms, 
y = constant + month/+ year }, 
where month i and year  stand for the month and year effect, i-Jan., 
Feb., ..., Dec., and } - 1941,..., 1945. When we remove a median for each 
month, as we do in Exhibit 14, we are taking out an estimate of the "month 
effect." (We might alternatively have taken out a mean for each month.) What 
is left can be used to estimate an average effect for each year separately. 
The reader will observe that the average residuals by year, given at the 
foot of the righthand side of Exhibit 14, are nearly linear in the years. It is 
tempting then to fit a straight line to the five points. Arguing against this is our 
knowledge that recessions and prosperous times make economic series jump 
around in nonlinear ways. And so, although a straight line might be useful for 
some purposes, in general we will want to take out an effect for each year, or 
possibly fit some more complicated curve, or use some smoothing device. 
Therefore, we choose to remove the column means from the first residuals and 
to examine them further. (Means instead of medians were taken only for 
variety.) 
In Exhibit 16, the months for the five years are assigned the numbers 
t = 1, 2,..., 60. The year effects from the bottom of Exhibit 14 are subtracted 
from the first residuals given in the righthand side of that exhibit, to produce 
the final residuals shown in Exhibit 16. These have been smoothed using 
Exhibit 15 of Chapter 9 
Ranks of months within years for the original entries of Exhibit 14, together with 
the median rank and range of ranks for each month 
1941 1942 1943 1944 1945 
[Months i IRanks of months within years I IMedian i i Range I 
Jan. 1 3� 2 1 1 1 2� 
1 
Feb. 2 2 6� 23 4 2� 4� 
Mar. 4 7 4 6 9 6 5 
Apr. 6 8 8 7 5 7 3 
May 8 6 6� 8 6 6� 2 
June 5 3� 5 5 7 5 3� 
1 
July 3 1 1 2 2 2 2 
Aug. 7 5 3 4 3 4 4 
Sept. 10 9 9 9 8 9 2 
Oct. 9 10 10 10 10 10 1 
Nov. 11 11 11 11 11 11 0 
Dec. 12 12 12 12 12 12 0 
9D: Polishing additive fits/Exhibit 16 189 
190 Exhibit 17/9: Two- and more-way tables 
running medians of length three. Then the final smoothed residuals have been 
plotted against t, !he number of the .m.onth, !n Exhibit 17. 
The outstanding pattern of Exhibit 17 s that of small variability in the 
middle years and of larger variability for t in the first two or last two years. We 
know, from both Exhibits 14 and 15, that the middle year was fitted extremely 
closely, because for eleven of the twelve months, the median value was in the 
middle year. Inevitably, then, the more extreme years will fail to fit as well. 
Why the first two years should produce more residual variability after smooth- 
ing than the last two is not apparent, but the graph strongly suggests this� 
Conceivably, during the early war years, people could buy more or less at will, 
while during the later years they bought what was available. 
9E. Fitting One More Constant 
Although our PLUS fit to the table of mean monthly temperatures (3 places x 
7 months) helped our understanding, we concluded it did not go far enough. 
The smallest further step we can take would be to fit one more constant. And 
what constant should it be? 
Exhibit 17 of Chapter 9 
Smoothed residuals2 from Exhibit 16, where residuals2 = residuals-column 
means of residualsq. 
lesidua12 
9E: Fitting one more constant/Exhibit 18(A) 191 
To bring in a multiplicative effect, we propose to add terms proportional 
to the product of the row and column effects. We write them as 
(row) (column) 
constant , 
common 
where the "constant" is the constant we spoke of fitting. This move makes our 
fit 
(row) (column) 
common PLUS row PLUS column PLUS constant . 
common 
To execute the fit, we can recall that, for a given cell of the table, 
first residual = data MINUS (common PLUS row PLUS column). 
We plan to use the new term constant (row)(column)/common to fit these first 
residuals. Excluding the constant, we can call the remaining part of the new 
term a comparison value, where 
(row) (column) 
comparison value - , 
common 
the quantities on the right being the ones we have already computed. We can 
exhibit this relation by making a graph--plotting first residuals against com- 
parison values or by going to some more formal calculation, even a least- 
squares line. Exhibit 18, Panel A, shows the calculation of comparison values 
for the alternative analysis given in Panel E of Exhibit 1, and the left side of 
Exhibit 18 of Chapter 9 
Calculation of comparison values for the alternative fit of Panel E of Exhibit I and 
its use to array the first residuals. 
A) Calculation of COMPARISON VALUES 
I Month I 
l Jan. 1 [Feb. jiMarchi IApr. I lUayl IJunel 1 July l 
Column -20 -15 -10 0 10 20 25 
Column/common -.364 -.273 -.182 0 .182 .364 .454 
Row 
Comparison values -20:-7.28 -5.46 -3.64 0 3.64 7.28 9.08 
(row)(column) 0: 0 0 0 0 0 0 0 
common 20: 7.28 5.46 3.64 0 -3.64 -7.28 -9.08 
192 Exhibit 18(B)/9: Two- and more-way tables 
Panel B shows the results of ordering the pairs of comparison values and first 
residuals on the basis of their comparison values. The similar arrangement of 
positive and negative values for the pairs indicates the dependence of compari- 
son values and first residuals. For each group of equal comparison values, the 
medians of the corresponding first residuals are written on the right side of 
Exhibit 18, Panel B. Clearly the dependence is strong. 
Exhibit 19 shows the corresponding diagnostic plot, with comparison value 
as abscissa and first residual as ordinate. As is shown there, a line of slope 
+1.00 seems a reasonable fit, if we decide to stop with a line. 
Panels A and B of Exhibit 20 show an analysis in which 
(row)(col) 
1.00 
common 
has also been fitted, by subtracting it from the first residuals of Exhibit 1, Panel 
E. We see (1) that the sizes of the new residuals are considerably smaller, and 
(2) that their signs are heavily negative (16 out of 21). The stem-and-leaf 
display of the new residuals in Panel C suggests that an even better fit (in the 
sense of reducing largest residuals) would be found by subtracting -1.0 (or 
even -2.0) from the new residuals and adding -1.0 (or -2.0) to the common, 
giving the fit 
(row) (column) 
54 PLUS column PLUS row PLUS 
55 
or 
(row) (column) 
53 PLUS column PLUS row PLUS . 
55 
Exhibit 18 of Chapter 9 (continued) 
B} COMPARISON VALUE FIRST RESIDUAL pairs ORDERED according to their 
COMPARISON VALUES 
l Comp. I First I i [Comp. I I First ] l Comp. Media.n 
value resid. i value resid. value first resd. 
9.08 4.0 0 -0.6 9.08 4.0 
7.28 2,6 0 -1.6 7.28 3.0 
7.28 3,4 0 -2.7 5.46 1.9 
5.46 1.9 0 -2,9 3.64 3.4 
3.64 3.4 -3.64 -3.8 0 -0.3 
3.64 3.5 -3.64 -3.3 -3.64 -3.6 
0 1.2 ' -5.46 -10.2 -5.46 -10.2 
0 0.9 -7.28 -9.2 -7.28 -7,8 
0 0.3 -7.28 -6.3 -9.08 - 12.3 
0 -0.3 -9.08 - 12.3 
0 -0.3 
 9E: Fitting one more constant/Exhibit 19 193 
For the latter, the largest residuals would be about 3.2 in absolute value, in 
contrast to 5.1 (in Exhibit 20) or 12.3 (in Exhibit 1). Fitting one more constant 
has considerably improved the adequacy of our partial description. 
Additive and multiplicative fitting. When we make this further fit, the values 
of the constant multiplying the comparison values can often be given a helpful 
:interpretation (see EDA, Chapter 12). This particular extended additive fit can 
be equally well described as a multiplicative fit, because 
common PLUS row PLUS column PLUS (row) (column) (,) 
(common) 
is identical to 
( row ](column) (**) 
common 1 +  1 + , 
common/ common 
as we see by multiplying (**) out. Here the quantities in parentheses could be 
renamed "row*" and "column*" since the value in the first parenthesis 
depends on only the row and the second on only the column, giving 
common TIMES row* TIMES column*. 
Exhibit 19 of Chapter 9 
Diagnostic plot of first residuals against comparison values for data of Exhibit 18, 
Panel B. Circled point indicates two observations. 
Fir 
O- 
-8- 
� 
I [ I I t Compa,,,?5orl value 
194 Exhibit 20/9: Two- and more-way tables 
9F. Using Re-expression 
In this section we illustrate the use of re-expression to make a fit involving one 
additional constant. This fit is tuned to a particular set of data, with no 
implication that the particular constant appearing in the re-expression would 
be good for another set of similar data. (This is quite different from the choice 
between raw values and log values, where a good choice for one set of data 
may well be a good choice for similar sets of data.) As in other cases of fitting a 
single constant, we feel free to fit "any old value." If we have no better way to 
go, we proceed by trying a variety of values for the constant and then fixing on 
the one that seems best. (When we tinker with the expression of the response, 
we shall use matched re-expressions as introduced in Section 5E.) 
Exhibit 20 of Chapter 9 
Fitting the extra constant to the first residuals in Exhibit 1, Panel E. 
A) The PATTERN 
New residuals Row contribution 
(row)(column) 
1.0 x -. 
common 
Column contribution Common contribution 
B) RESULTS 
Jan. Feb. March Apr. May June July 
Caribou 1.0 -4.7 .3 -.3 -.1 -3.9 -5.1 -20 
Washington 1.2 -2.9 .3 -.6 -.3 -1.6 -2.7 0 
La redo -4.7 -3.6 - .2 .9 -.2 - 1.9 -3.2 20 
Caribou -7.3 -5.5 -3.6 0 3.6 7.3 9.1 
Washington 0 0 0 0 0 0 0 
Laredo 7.3 5.5 3.6 0 -3.6 -7.3 -9.1 
-20 -15 -10 0 10 20 25 55 
C) STEM-and-LEAF DISPLAY for NEW RESIDUALS 
I 20 
0 933 
-0 122336 Median -0.6 
-1 69 Upper hinge -0.1 
-2 79 Lower hinge -3.2 
--3 269 Interquartile range 3.1 
-4 77 
-5 1 
9F: Using re-expression 195 
": Here we combine the use of re-expression of response with the use of 
two-way analysis. The most used re-expressions are logarithms of amounts or 
:Counts, which, in view of 
::"': log ((common)(row*)(column*)) = log common + log row* + log column*, 
::' converts situations adapted to multiplicative fitting into situations adapted to 
:additive fitting. 
� i':: ': Effective re-expression of the response by a logarithm corresponds to a 
::slope of plus one on our diagnostic plot (Exhibit 19). As a consequence, we 
:,imay take other positive slopes as suggesting re-expressions in the general 
direction of the use of logs. The slopes for square and cube roots go in the 
isame direction, but are less vigorous than logs. The slopes for reciprocals and 
reciprocal roots go in the same direction but are more vigorous. But what if the 
:slope of the diagnostic plot is negative something less common but possible? 
:'Or what if the responses to be re-expressed take negative values as well as 
::positive ones, as our Fahrenheit temperature example in Exhibit 1 could, and 
.:would if we extended our places some hundreds of miles north? 
In many of the cases, where the slope is negative and the responses are 
� routinely nonnegative, we can effectively re-express our response by squaring 
it--or by taking some other power greater than one. For potentially negative 
responses, however, squaring is unlikely to be satisfactorysince two values of 
y, one positive and one negative, correspond to each single value of y2 (except 
:zero). Confusion of values leads almost inevitably to confusion of result. 
The simplest re-expressions that apply equally to negative and positive 
values, and whose plots, like that of y2, are hollow upward, are probably the 
exponentials, introduced in Section 5A: 
Y ._.> �cy= eY/d 
whose matched formsmatched at yo--corresponds to replacing y by 
de (y-y�>/a + (y0- d), 
which is the same as 
(de-Y�/d)eY/d + (Y0- d). 
Thus, if in our temperature example of Exhibit 1, Panel A, we choose yo = 55 
(close to the center of the data, which was 54.4) and d = 80, we would use 
(80e-SS/8�)e y/S� + (55 - 80) = 40.23e y/8�- 25, 
something that can be evaluated in a few key strokes on a hand-held calculator. 
Exhibit 21 shows the results of making a variety of such exponential 
re-expressions on our East Coast temperature data of Exhibit 1, Panel A. Note 
that, as d changes, there is a gradual change of the fitted values and of the 
residuals. 
The chosen values of d are either multiples of 10 or values for which 
120/d are simple, or both. The number 120 is not actually used in the 
196 Exhibit 21 (A-D)/9: Two- and more-way tables 
9F: Using re,-expression/Exhibit 21(E--H) 197 
198 Exhibit 21(I-L)/9: Two- and more-way tables 
9F: Using re-expression 199 
re-expressions or calculations of Exhibit 21. The advantage to using 120 and 
having 120/d simple is that this makes plotting against the reciprocal of d easy, 
:where we expect--and in this instance confirm--simpler behavior in terms of 
(any multiple of) the reciprocal of d than in terms of (any multiple of) d itself, 
::Particularly since large values of d correspond to no re-expression, to using the 
:raw val-ues with which we started (as can be verified by taking limits). 
When we look at the selected letter-value displays for residuals from fits 
:with selected d's in Panel J, it is clear--since we used matched re- 
expressions that d = 60 is better than either d = 120 or d = 40. What value 
of d is best? To approach an answer, Exhibit 22 plots the H-spreads, E- 
spreads, 2-spreads (2 in from each end), and 1-spreads (ranges) for all available 
Values of d. In every case we see a minimum in the spread for d near 60-70, 
near 60 for the three outer spreads, and near 70 for the H-spread, which 
appears to depend much more irregularly on d. 
/ Why do we have a minimum just here? Turning back to the residuals, 
Which were, for d = 60, 
Caribou .4 0 0 0 0 0 0 
Washington 0 -.3 0 0 .1 1.4 .7 
Laredo -3.7 0 1.5 3.1 0 -2.3 -5.3 
and, for d = 70, 
Caribou 0 - 1 0 0 .4 0 .7 
Washington 0 0 0 -.4 -.4 .1 0 
Laredo -1.9 2.1 2.9 3.6 0 -3.2 -5.6 
we see that the residuals for Caribou and Washington are very small, only 
tenths of a degree, while those for Laredo are not. However, looking up and 
down Exhibit 21, we see no closer agreement than for d = 60 and d = 70 
between Laredo and either Washington or Caribou. Thus we conclude that the 
best exponential re-expression 
ohas d close to 60 or 70. 
 brings Washington and Caribou into close agreement. 
o does reasonably well in bringing Laredo into agreement with the other 
two, but is not as successful. 
Since it is easy to argue that Laredo, though not far from the Gulf of Mexico, is 
not genuinely "East Coast," it would be natural for us to next examine a 
larger variety of genuine East Coast places and see how well d - 70 does. 
Note that in such an exponential re-expression, we are again fitting 
one more constant 
although this time the constant is in the re-expression of the response, rather 
than in the fit itself (as it was in Section 9E, when the constant times a 
comparison value was added to the fit). 
200 Exhibit 22/9: Two- and more-way tables 
9G. Three- and More-Way Analyses 
If we wish to analyze tables with more than two ways, the arithmetic inevitably 
gets heavier. The general technique is much like that of the two-way attack. 
For a three-way analysis, let us call the ways A, B, and C. Then the paired 
variables are AB, BC, and CA. Let us begin with AB. For each AB pair, there 
is a line of entries differing only in C. We take a half-step, finding and 
removing a summary for every such line. Then we turn to the BC pairs, and 
take a half-step for the residuals in the lines where only A changes. Then we 
turn to the CA pairs--and continue after that with AB again. 
The approach is illustrated and treated more extensively in EDA, Chapter 
13. 
Exhibit 22 of Chapter 9 
Dependence on d of various spreads of residuals. The spreads are calculated 
from the values of deCV-VoU+ (Yo- d), the re-expressions of the mean monthly 
temperatures in Exhibit 21. 
. k 
18 - /- epra (rang,) 
16- 
14- 
t~ lread 
12- 
I0- 
oo 17o 6 d 
0 I 15 ,7 t iS' 3 4. 
 - H- spreed 
O- 
Summary 201 
� ..Summary: Two-Way Tables of Responses 
"Analyses of tables of responses that come out in the form: common PLUS row 
:.::.:.PLUS column PLUS residual offer a general pattern within which there can 
� '"'be important details of fitting. 
These analyses extend what we do when we "eyeball" a table of responses. 
:i;:.. Having more numbers in a full analysis than in the data is a normal 
i:phenomenon, often necessary for a careful examination of what appears to be 
:.:11.going on; such an examination does not bar further summarization. 
Starting from scratch (or from some other analysis), successive removal of 
:'.medians, alternately by rows and by columns (median polish), produces com- 
'': rnon PLUS row PLUS column fits. 
..:......: Such analysis by medians leaves unusual perturbations in the data much 
?more clearly delineated in the residuals than does analysis by means, which 
itends to smear the consequences of isolated perturbations quite widely. 
� .'? Such an analysis by medians works whether the table is complete or 
iwhether a certain number of values are either missing or chosen to be set aside 
.....for a particular analysis. 
Any row-PLUS-column-PLUS-common fit can be graphically displaye, d in 
...!terms of two families of parallel lines, with the vertical coordinate of the 
?intersections giving the values of the fit. 
Coded symbols, showing both their signs and rough sizes, let us make 
effective pictures of residuals. 
The appearance of matching signs of residuals in opposite corners, of 
...ieither a well-rearranged numerical table or a coded plot of residuals, gives a 
:::basis for trying a specific sort of further fitting--fitting one more constant. 
:'. Fits of the form 
 (row) (col) 
common PLUS row PLUS column PLUS 1.0 
: (common) 
can also be written as 
common TIMES row* TIMES column*. 
Plotting residuals against comparison values allows us to choose the 
"constant" in a term 
:'::: (row)(col) 
PLUS (constant) 
(common) 
':'"to be added to our previous common-PLUS-row-PLUS-column fit. 
We can often re-express y as a convenient alternative to fitting the one 
more constant. To see which re-expression allows common-PLUS-row-PLUS- 
column to fit very well, we can try varied re-expressions of the responses. 
The qualities of such different fits may be compared by using re- 
expressions matched near the center of the values of our responses, and 
:.:...:looking at the behavior of various simple measures of spread for residuals. 
202 19: Two- and more-way tables 
References 
McNeil, D. R., and J. W. Tukey (1975). "Higher-order diagnosis of two-way 
tables, illustrated on two sets of demographic empirical distributions." Biomet- 
rics, 31, 487-510. 
EDA = Tukey, J. W. (1977). Exploratory Data Analysis. Reading, Mass.: 
Addison-Wesley. 
Chapter 10/Robust and 
Resistant Measures of 
Location and Scale 
Chapter index on next page 
In this chapter we press beyond the ideas of robust and resistant methods given 
i n Chapter 1 and introduce a number of ideas lightly. We referred to them in 
Chapter 9 and will return to them in more detail in Sections 14G, 14H, and 
14I. 
10A. Resistance 
Resistance is a property we like summary statistics to have. If changing a small 
part of the body of data, perhaps drastically, can change the value of the 
summary substantially, the summary is not resistant. Conversely, if a change of 
a small part of the data, no matter what part or how substantially, fails to 
Change the summary substantially, the summary is said to be resistant. 
The arithmetic mean is the prototype of a nonresistant summary. If, in 
1 + 2 + 2 + 3+..-+ 23 
= 9.58, 
101 
We change the second measurement of 2 to 101002, the arithmetic mean will 
change to 
1 + 2 + 101002 + 3 +..- + 23 
= 1009.58. 
101 
Changing 1/101st of the data has changed the mean tremendously. 
The median is the prototype of a simple resistant summary. In our 
example of 101 values, suppose we have 
50 values {1, 2, 2, 3,..., 8, 9} 
1 value {9} 
50 values {9.5, 10,..., 23} 
where the median is 9. Then changing the second 2 to 101002 changes the 
body of data to 
50 values {1, 2, 3, ..., 8, 9, 9} 
1 value {9.5} 
50 values {10,..., 23, 101002} 
with median 9.5 not far from 9. 
203 
204 Index for Chapter 10 
10A  Resistance 203 
10B Robustness 205 
10C Robust and resistant estimates of location 206 
10D Robust estimates of scale 207 
10E Robust and resistant intervals 208 
10F Resistant and robust regression 209 
10G Multiple-component data 209 
10H Closing comment 218 
Summary: Resistant and robust techniques 218 
References 219 
 (10A) 10B' Robustness 205 
 No change of a single value could move the median any further than this. 
Clearly the median is resistant. 
: Other kinds of summaries are also resistant. Many important ones are 
defined implicitly and have to be calculated by iteration, as in the following 
example, where iteration is required because we cannot compute y* until we 
::know the w's and we cannot compute the w's until we know y*: 
, 
Y Y. 
where 
: wi = 'S when .yi - Y* 2 
<1, 
0, otherwise, 
land 
S = median {ly - y*l} 
or, perhaps, 
S = �(spread between hinges), 
and c is a constant, often taken as 9 or 6. (It may help to think of S as 
estimating roughly 2 
 cr, so that, with c = 6, we allow the residuals to count up 
to about 4or, where the spread is being measured more by the middle observa- 
tions than by the large ones.) The estimate thus defined is often called a 
biweight estimate. (See Chapter 14 for more about such estimates.) 
10B. Robustness 
Why not be satisfied with the median? Why squander the considerable amount 
of arithmetic needed to calculate a more complex resistant estimate of location 
like the biweight? 
We are concerned with efficiency with using a summary that is thorough 
in extracting from bodies of data most of what we can learn about the aspect 
being summarized. 
How can we measure thoroughness of extraction? We must 
o decide how to measure effectiveness, and 
o decide in what body of data the extraction will take place. 
Except for small samples from distributions with very stretched tails, it is 
usually satisfactory to measure effectiveness in terms of the variance of the 
summary (assuming that, on the average, the summary comes close to assessing 
what we .would like assessed--otherwise special circumstances, such as special 
loss functions, might rule). Using variances, a natural measure of efficiency is 
lowest variance feasible 
Efficiency = actual variance ' 
206 Exhibit 1/10: Robust and resistant measures 
We usually report efficiency as a percentage. What then should we think 
about 90% efficiency? The answers are that 
o it is very good. 
othis summary's behavior could be distinguished from that of a 100% 
efficient summary. 
o but it would be a massive job to do this in practice. 
Gains of a single percent of efficiency are essentially never detectable in 
practice. 
What we want is 
robustness of efficiency. 
We want to have high efficiency in a variety of situations, rather than in any 
one situation. Our attention naturally shifts to the poorest efficiency in a 
reasonable collection of situations. If this is high, we can rightfully feel that we 
have a good summary. 
10C. Robust and Resistant Estimates of Location 
How do the three estimates of location mentioned above survive the new test 
of robustness of efficiency? Exhibit 1 has the story. 
The conclusion is that, except for quite small samples, the biweight 
estimate has all the desired properties. (For the smallest samples, say those of 
count three, four, and five, we might do better with the median.) 
Exhibit 1 of Chapter 10 
Resistance and robustness of efficiency of some location estimators 
Stretched- Robustness 
Sample Resis- Gaussian tail of 
l Estimat�r i size rant? efficiency efficiency efficiency 
Arithmetic Small No 100% Poor Poor 
mean Large No 100% Very poor Very poor 
Median Small Yes High Higher High 
Large Yes 62% Higher Moderate 
Biweight Small Reason- Highish Higher Highish 
with c = 6 ably 
or 9 Large Yes >90% >90% High 
(10C) 10D: Robust estimates of scale 207 
In practice, then, we tend to: 
o use the median in exploration, and in other circumstances where moder- 
ate efficiency in a wide variety ot situations is enough. 
ouse the biweight, or one of its relatives, when high performance is 
needed. 
ouse the arithmetic mean only after careful study: when traditions or 
meaningfulness in the field of exploration of application require it; when the 
cost of redoing computer programs is exorbitant; when better confidence or 
significance techniques are available only tor means; when the convenience 
of linearity is required; or when the type of data has short tails and no 
outliers, as we explain next. 
Sometimes distributions tend to have tails tighter than or about like a 
normal distribution, rather than more straggling. Under these circumstances, 
means are more efficient than biweights, and, for very short tails, special 
location statistics can be developed that give more weight to the far-out 
observations than to those in the center of the sample. The uniform distribu- 
tion has especially short tails for a continuously spread-out distribution. For 
samples from the uniform, theory tells us that the average of the two extreme 
observations uses all the sample information. This illustrates how, for short- 
tailed distributions, the extreme observations should get more weight. 
10D. Robust Estimates of Scale 
Here we do not know as much as we would like. Either the 
MAD = Median Absolute Deviation = median 
where y* is a resistant estimate of location, or the interquartile range I (dis- 
tance between the hinges or 25% points), 
I = H-spread = upper hinge - lower hinge, 
is a resistant estimate of scale (of spread). We recognize these two as barely 
robust of efficiency. 
Ranges, and range-based estimates of scale, are perhaps a shade less 
robust than s = (sample variance) /-. Neither is competitive with MAD or I. 
An alternative. David Lax (1975) has appraised a rather complicated statistic 
that does more than twice as well as MAD in some fairly realistic situations. 
Let 
I 
y = median of y's, 
I 
Yi- Y 
9(MAD) 
208 /10: Robust and resistant measures 
Then .he uses a measure of scale derived from the asymptotic variance of a 
biweight, namely, 
[Y'.' (1 - u2)(1 - 5u2)] 2' 
where Y_'. indicates sum.mation for u2- < 1 only. Roughly speaking, th.e u's 
define weights. When u 2 s small, the weights are about equal, the denominator 
reduces to n, and the whole expression becomes Y'. (y - )2/r, something that 
looks like a reasonable variance estimate. 
A modification that reduces to 
Z(y - 
$ -- 
(n- ) ' 
and is known to perform somewhat better, is 
[' (1 - u2)(1 - 5u2)][-1 + Y.' (1 - u2)(1 - 5u2)] 
Note here that (1 - u2) 4 = w 2, where w was introduced in Section 10A. 
Inevitably other estimates will be developed. 
10E. Robust and Resistant Intervals 
The word "robust" has been used in statistics with many different meanings it 
will probably be used with many more. Until about 1970, it usually meant 
robustness of validity that what was nominally 5% was in fact 5% for a wide 
variety of situations. This is separate from the meaning here--separate but not 
necessarily contradictory. 
Indeed, Alan Gross (1976) has shown us that we can have both robustness 
of efficiency and robustness of validity in a simple procedure for setting 
confidence limits for centers of symmetric situations. In later work, Gross tells 
us, he has shown that one can do essentially as well with an interval based upon 
a biweight. A modified procedure uses, for , the biweight estimate, namely 
y- y 
9(MAD) 
and then takes, as the estimated variance of , the (asymptotically correct) 
expression (cf. Section 10D): 
 X' (y- )(- u-)  
vary = $i = 
[Z' - - + Z' - - sa)] ' 
(Whether we u. se the biweight  or the median  is not important.) Then use 
Student's t wth 0.7 times the usual number of degrees of freedom, ,-- 
(10E, 10F) 10G: Multiple-component data 209 
/ 0.7(n - 1), to give the interval 
 t/   
 y + var . 
This works well for n > 8. 
101:. Resistant and Robust Regression 
For the iterative use of biweights in regression, see Sections 14G, 14H, and 141. 
10G. Multiple-Component Data 
When we have multiple-component data often called vectors we are likely 
: to want: 
: <}resistant-and-robust location estimates--which we can get by applying 
::: biweights to each component separately. 
o resistant-and-robust substitutes for variances, covariances, and correla- 
tions. 
i How are we to find the latter? 
Classically, the most frequent use for calculated estimates of variances and 
covariances was to compute linear regressions. We now turn this around, and 
:Use resistant-and-robust linear regressions as a tool in calculating resistant-and- 
robust estimates of variances and covariances. 
The fundamental fact about variances and covariances is their quadratic 
nature, best shown forth by the identities they satisfy, such as 
var (fu + gv) .= f2 var (u) + 2/g cov (u, v) + g2 var (v), 
and, more generally, 
cov (fu + gv, ]u + kv) -- f] var (u) + ( + gj) cov (u, v) + gk var (v), 
and not at all by how they are calculated. Our usual estimates -ar and ov of 
variances and covariances, of course, satisfy the same identities. 
If we are to define estimates of analogs of variances and covariances, we 
want them to satisfy the same identities. Since they will not be quadratic 
� 'functions of the data, there is only one easy way to satisfy the identities: First, 
define estimates for special components, and then use the identities to extend 
the definition. 
Suppose, then, that we are willing to begin with specially chosen compo- 
nents x, y - bx, z - cy - dx, ..., and that we are willing 
1. to take zero as the estimate of the analog of covariance, which we will 
denote ob rather than ov, between each pair of these special compo- 
nents; and 
2. to take the square of a resistant estimate of scale as the analog of the 
variance, which we will denote bar rather than ar, for each of these 
special components. 
210 /10: Robust and resistant measures 
We use b for robust, since r (= "rar") would be far from euphonious--note, 
too, that "b" sounds a lot like "v". 
The identities teach us, in particular, that: 
bar (y) = bar ((y - bx) + bx) 
= $ar (y - bx) + b 2 far (x), 
aob (x, y) =- Sob (x, (y - bx) + bx) 
= b bar (x), 
$ar (z) -- lar (z - cy - dx) + c 2 $ar (y - bx) + (d + bc) 2 f>ar (x); 
bar (2y + 3z) =- 4 bar y + 6 aob (y, z) + 9 bar z 
= 9 $ar (z - cy - dx) + (9c 2 + 6c + 4)lar (y - bx) 
+ (9(d + bc) 2 + 6(d + bc)b + 4b 2) f>ar (x), 
so that bar and aob are defined for all linear combinations of x, y, z,... Thus it 
remains only for us to: 
1. choose the constants b, c, d, ... 
2. decide which resistant-and-robust estimates of scale are to define Sat (x), 
bar (y - bx), bar (z - cy - dx), ... 
The simple choices seem to be quite satisfactory for the present, namely: 
1. y- bx, z- cy- dx, ... shall be the residuals from our conventional 
resistant-and-robust regression of y on x, and of z on x and y, and 
2. we will take 
$ar (x) = rtsi (for the values of x), 
$ar (y - bx) = ns,i (for the value of y - bx), 
and so on, where the ns2i are the biweight variances defined in Section 10D. 
Other robust variables might be chosen, such as the square of the median 
deviation from the median, or even the square of the difference between two 
order statistics. 
A comment. We have chosen to keep the property of resistance for bar and 
ob, namely that' 
ochanging a small part of the data, no matter how much, makes only a 
small change in the results. 
We have chosen to give upmin part because we do not know how to keep 
both this and resistancema property that some might have thought equally 
important, namely that: 
o changing the order of the components, say from (x,y,z,...) to 
10G: Multiple component data 211 
(z, x, y, ...), or replacing one set of components by another linearly equivalent 
to it, say from (x, y, z,...) to (x + y, x - y, 2y - z,...), does not change the 
� results. 
� We are acting as if invariance were nice, but can be dispensed with, while 
 resistance and robustness of efficiency cannot be spared. And that is just what 
we think and recommend. 
::..i Alternatives. If we have only three components, they can be written down in 
..only six orders. Those who wish to have the order of their components not 
 matter could start from each of the six permutations, go all the way to the bars 
':':and obs of the original components .and then combine the six sets of values, 
::taking either the arithmetic mean or some robust summary. With 10 compo- 
..nents there are far too many orders to do anything like all of them, but one 
.:. could do the calculations for a random sample of 10 or 20 of the permutations 
:(cp. Moses and Oakford, 1963). While the means or the summaries--of the 
10 or 20 results would not be free of all effects of where we started, the 
� ..corresponding spreads would show, by their sizes, just about how much effect 
where we started has on the results. We call attention to these possibilities 
without recommending their use. 
(Optional) Invariance under permutation, or an estimate of how much permu- 
 tation matters, may not satisfy a few. They would like invariance under all rota- 
tions, or even under all affine transformations. If they were willing to be satisfied 
with an estimate of how much difference rotation, or even affine transformation, 
matters, they can get this by drawing a random sample of rotations, or of aftinc 
.transformations, applying each to the given form of the data, carrying�hrough 
:�� the calculations to identify special components, and then establishing bars and 
obs for the original data as found, starting from each transformed set of initial 
components. Again, the spreads among corresponding results would show how 
large are the differences that arise from the original choice of components. 
Consequences. Now that we are not going to use the estimates of analogs of 
variance and covariance to calculate linear regressions, one of theix most 
frequent uses (deplored by some) will be to calculate estimated analogs of 
correlation coefficients, for example: 
Sob (x, y) 
8orr (x, y) --- 
/lar (x) ar (y) 
b bar (x) 
qar (x)[ar (y - bx) + b 2 ar (x)] 
where the sign in the last form is that of b. 
212 /10: Robust and resistant measures 
Another frequent use will be in choosing linear combinations of our 
original components--principal components, for instance or in assessing prop- 
erties of such newly chosen components canonical correlations, for instance. 
In such uses, we may find it important to protect ourselves from the conse- 
quences of giving up invariance. There is a simple way to do this: 
1. Calculate bar and ob values as above. 
2. Use these as if they were -ar and ov values, to calculate the new 
components. 
3. Return to step (1), using the new components instead of the original 
components. 
4. Repeat step (2), finding, usually, minor modifications of what we found at 
step (2). 
5. Use the result. 
This plan does not eliminate all the consequences of loss of invariance, but it 
does protect us from most of them. 
Example 1. Andrews case. Exhibit 2 gives 11 x and y pairs and t.he corres- 
ponding values of Oar, ov, bar, and gob. The conventional correlation is 
(classical) = -0.7333. 
The resistant and robust analog is 
gorr (x, y) = 0.99992. 
We see how great a difference doing things resistantly can make. 
Example 2. Ten-in-a-box. Exhibit 3 gives 17 (x, y, z) triples, and goes on to 
find: 
i) as column 6, the residuals of y for a resistant straight-line fit to x; 
ii) as column 8, the residuals of z for a resistant straight-line fit to x; 
iii) as column 9, the residuals of column 8 for a resistant straight-line fit to 
column 6; and 
iv) as column 10, the residuals of x for a resistant fit of a constant. 
The values of Sb2 and ns2i can then be calculated for columns 6, 9, and 10, since 
we know that the biweight fit to each of these columns is zero (to a more than 
adequate approximation). 
We have now calculated bot.h.the b, c, and d in x, y - bx, z - cy - dx 
and the ns,i for these three quantities (which are also their bars). We go on to 
calculate the complete table of bars and obs and, for comparison, the 
complete table of Oars and 6ovs. The two tables show no trace of resemblance. 
10G: Multiple-component data/Exhibit 2: 213 
: Exhibit 2 of Chapter 10 
:Andrews case; resistant correlation example 
i: A) The DATA 
.02 .04 .02 .0093 -4.98 -4.62 
..: .99 1.03 .04 .0349 -4.01 -3.65 
2.01 1.97 - 04 -.0391 -2.99 -2.63 
2.98 2.96 -.02 -.0135 -2.02 -1.66 
4.03 3.97 -.06 -.0474 -.97 -.61 
5.01 4.98 -.03 -.0117 .01 .37 
6.05 6.07 .02 .0443 1.05 1.41 
6.98 7.03 .05 .0797 1.98 2.34 
8.07 8.00 -.07 - .0340 3.01 3.37 
9.03 8.96 -.07 -.0284 4.03 4.12 
25.00 -25.00 -50.00 -49.8658 20.00 20.36 
H .02 .0221 2.50 2.86 
H -.05 -.0366 -2.50 -2.14 
$ .035 .0294 2.50 2.50 
cS .315 .2641 22.50 22.50 
B) RESULTS of VARIOUS COMPUTATIONS 
Biweight fit of y- x to x: 0.0108 - 0.0058x 
Column (4): (y - x) - (0.0108 - 0.0058x) -- y - 0.0108 - 0.9942x 
2 (for col. (4), using cS = 0.2641) = 0.00019136 
:::'nsi (for same) = 0.0021050 = ar (y- bx) 
Biweight fit of x - 5 to a constant: 0.36 
Column (5): (x - 5) - (-0.36) = x - 4.64 
si (for x- 4.64, using cS = 22.5) = 1.13587 
2 (for same) = 12.4945 = ar x 
rJSbi 
I .0021050 
t:orr= 1+ (0.99945)J =0.99992 
(-46.7004) 
'(classical) = � = -0.7333 
,/(46.4410)(87.3414) 
214 Exhibit 3 (A)/10: Robust and resistant measures 
I I 
- I I 
10G: Multiple-component data/Exhibit 3(B-D) 215 
Exhibit 3 of Chapter 10 (continued) 
B) STRAIGHT-LINE FITS, COLUMNS OF PANEL A 
Biweight fit of y- x to x: .079 - .001261x 
Column (5): y - x - (.079 - .001261x) = y - .079 - .998739x 
:Biweight fit of (5) to x: .001 - .000067x 
Column (6): column (5)- (.001 -.000067x)- y- .080- .998672x 
Biweight fit of z- x to x: -.200- .001235x 
:Column (7): z- x - (-0.200- .001235x) = z + .200- .998765x 
Biweight fit of (7) to x: -0.039- .000003x 
Column (8): column (7)- (-.039- .000003x) = z + .239- .998762x 
Biweight fit of (8) to (6): -.007 + .000132 (column 6) 
:Column (9): column (8)-(-.007 + .000132 �v .080- .998672x)) 
= z + .246- .000132y- .998630x 
Biweight fit of x to a constant: .117 
:Column (10): x- .117 
C) CALCULATION OF AR and OB 
Special combinations 
bar 
x 434.914 
y- bx 3.0577 
z- cy- dx 17.0556 
where 
b = .998672 
c = .000132 
d - .998630 
so that 
Iar y = 3.0577 + (.998672)2(434.914) = 436.758 
t;ob (x, y) = (.998672)(434.914) = 434.336 
Iar z = 17.0556 + (.000132)2(3.0577) + (.998672')2(434.914) = 450.815 
,ob (x, z) = (.998672')(434.914) = 434.336 
,ob (y, z) = (.00132)(3.0577) + (.998672)(.998672')(434.914) = 433.760 
* These are d + bc = .998672 and not b = .998672 
D) COMPARISON OF /AR/OV and AR/;OB 
AR AND 3OV AR AND 3OB 
500362.43 -81.49 -97.70 x 434.914 434.336 434.336 
500221.61 332.59 � 436.758 433.760 
500217.57 z 450.815 
216 Exhibit 4/10: Robust and resistant measures 
How can the $ar-ob table be so different from the ar-ov table? 
Essentially because our example has been carefully loaded so that the eight 
outer points, which come very close to the (�1000, �1000, �1000) corners of a 
large cube, will dominate the ar-ov entries and have very little effect on the 
bar-ob entries. When they dominate, the variances are large--close to half a 
million--and the shape appears very nearly spherical (or cubical). When the 
corners of the large cube drop away, the bars are small--around 435mand the 
shape corresponds to a flattened cigar, to an ellipsoid with semiaxes of 
roughly, 20.9, 4.1, and 1.8. The latter shape is determined almost entirely by 
the locations of the nine inner points. (The sizes are enhanced over what the 
nine points alone would have given, because each rtsi is increased when a 
point gets very little or no--weight--when it appears so large as to deserve 
"skipping". This happens when the size of y - ] approaches or exceeds cS.) 
Comment 
A Roman-numeral jack-in-the-box is about to pop out at you! Plotting--or 
thinking hardinabout the unperturbed points, as in Exhibit 4, for the ten-in-a- 
box example, as given in Exhibit 3, could persuade the ten-in-a-box configura- 
tion to show itself to the reader almost as suddenly. 
Exhibit 4 of Chapter 10 
Ten-in-a-box example, unperturbed form 
1000 1000 - 1000 
1000 -1000 -1000 
14 14 32 
-7 -7 -16 
1000 1000 1000 
- 1000 1000 - 1000 
22 22 22 
11 11 11 
0 0 0 
-11 -11 -11 
-22 -22 -22 
1000 - 1000 1000 
- 1000 - 1000 - 1000 
7 7 16 
-14 -14 -32 
- 1000 1000 1000 
-1000 -1000 1000 
lOG: Multiple-component data/Exhibit 5 217 
Eample 3. Alternative solution to the Andrews case. Let us return to the first 
example, and open the resistant analysis of the Andrews case from another 
start, taking y + x as a residual, rather than y - x. Exhibit 5 has the first few 
Steps of the arithmetic leading to a slope of 
� -1.0023 
(which is close, compared with the -1.0056 of the classical analysis). We thus 
see that, for this set of data, we can use the same resistant straight-line fitting 
procedur� to obtain two quite different fits, one with slope 0.9942 (Exhibit 2) 
and one with slope -1.0023 (Exhibit 5). 
 It would often be claimed that it was regrettable that a single fitting 
procedure would give either of two such different answers, and that it ought to 
give only one. However, if we plot the data of Exhibit 2, even roughly, it is 
Exhibit 5 of Chapter 10 
Another look at the Andrews configuration 
! I � I I� + xl I( 4).1 (s) l I (6) J 
� 02 .04 .06 -7.50 -8.06 -8.118 
� 99 1.03 2.02 -5.54 -6.10 -6,155 
2.01 1.97 3,98 -3.58 -4.14 -4,193 
2,98 2.96 5.94 -1.62 -2.18 -2,230 
4.03 3.97 8.00 .44 -. 12 -. 169 
5.01 4.98 9.99 2.43 1.87 1.824 
6.05 6.07 12.12 4.56 4.00 3.956 
6.98 7.03 14.01 6.45 5.89 -5.848 
8.01 8.00 16.01 8.45 7.89 7.850 
9.03 8.96 17.99 10.43 9.87 9.833 
25.00 -25.00 0 -7.56 -8.12 -8.121 
: H 13.06 5.50 4.94 4.902 
H 3.00 -4.56 -5.12 -5.174 
S 5.03 5.03 5.03 5.038 
cS 45.27 45.27 45.27 45.34 
Biweight of y + x: 7.56 
Column (4): � + x- 7.56 
Biweight of column (4) (using cS = 45.27): .56 
Column (5): (� + x - 7.56) - .56 = y + x - 8.12 
Biweight fit of column (5) on x (using cS = 45.2): .058 + .0023x 
Column (6): (� + x- 8.12)- (.058- .0023x) - �- 1.0023x- 8.062 
218 /10: Robust and resistant measures 
clear that both fits are sensible, and that either might be the one we need in a 
specific situation. Which is which depends on our attitude toward the "odd 
point" (25,-25). What we really need, in such situations, is to be presented 
with both fits and be told "think hard, and see if you can choose". 
Regrettably, no one has yet designed a computer program to find all 
plausible distinct solutions and tell us to choose. 
1OH. Closing Comment 
We have now seen that resistant and robust techniques are already available 
for a wide variety of questions. Further extensions and more polished tech- 
niques for use in place of today's suggestions are by now inevitable. But enough 
is already known to make resistant and robust techniques widely usable. 
Summary: Resistant and Robust Techniques 
Both resistance and robustness of efficiency are concepts important to the 
practice of data analysis, as well as to its theory. 
We can do well using the median for exploration and the biweight for 
careful work, reserving the arithmetic mean for very special situations. 
If we have reason to believe that tails less straggling than those of a 
Gaussian ("normal") distribution are a solid possibility, the choice of an 
estimate is a new question, not discussed here. 
The classical estimates of spread (of scale) are not very robust of efficiency, 
with the MAD (median absolute deviation from the median) or the H-spread 
(or the interquartile range) among the best of an inferior lot. 
A somewhat more complex measure of spread (of scale), denoted "rtsi" in 
this book, is robust of efficiency and highl. y resistant. 
We can, using a method of Gross's, combine biweight and s2i to obtain 
confidence intervals that are resistant, robust of efficiency, and robust of 
validity. 
If we want analogs of variances and covariances for multiple-component 
data, we can follow a three-step path: 
1. Find components we are willing to treat as orthogonal (obtained as 
residuals from successive robust-and-resistant regressions). 
2. Assess a spread for each (we suggest using /ISb2i). 
3. Apply the usual identities for vars and covs to extend the definition of bar 
and ob to all linear combinations (of either the original or the new 
components). 
We could assess the extent of the dependence of the resulting bars and 
6obs on the choice of components from which to start. 
When the purpose of finding substitutes for Oars and ovs is to pick out 
certain newly chosen components, we may repeat the whole bar-ob process, 
(1OH) References 219 
Starting from the first set of newly chosen components as if they were the 
original components. This will greatly reduce any serious dependence of the 
final answers on how we chose the original components. 
References 
Gross, A.M. (1976). "Confidence-interval robustness with long-tailed symmet- 
ric distribution," J. Amer. Statist. Assoc., 71, 1976, 409-416. 
Lax, D. A. (1975). "An interim report of a Monte Carlo study of robust 
estimators of width." Technical Report No. 93, (Series 2). Department of 
Statistics, Princeton University. 
Moses, L. E., and R. V. Oakford (1963). Tables of Random Permutations. 
Stanford, California: Stanford University Press. 
220 Index for Chapter 11 
11A The simplest case 22 
11B Direct standardization 225 
11C Precision of directly standardized values 225 
11D Difficulties with direct standardization 230 
11E Indirect standardizing 233 
11F Adjustment for broad categories 24(} 
11G More than two broad categories 249 
Summary: Standardizing for comparison 256 
References 257 
C h apter 11/Standard izing 
for Comparison 
When responses can be expressed as rates or proportions, we want to compare 
the effects of various treatments. Once the overall rates are computed and 
compared, we usually ask for more information. For example, if we find that 
one form of teaching makes students spell better than another, the skeptic 
immediately asks whether the groups tested were comparable. Similarly, if a 
new chemical for preventing wound infection in abdominal operations shows a 
lower rate of infection than the old treatment, we want to make sure that the 
patients getting the new treatment were not better off initially. 
The use of randomized controlled trials goes a long way toward providing 
such controls. Even so, chance occasionally plays sad tricks, and it may be 
useful to make adjustments for this. Of more importance are adjustments to 
equate the performance of groups in observational or poorly controlled studies. 
Comparisons frequently can be improved by using one of the methods of 
standardization described in this chapter. 
Although we do not illustrate this here, essentially similar problems arise 
and essentially similar solutions are sometimes useful when dealing with means 
or medians of measurements instead of with rates or proportions. 
11A. The Simplest Case 
Example 1. Two treatments, dichotomous populations. Let us begin with the 
problem in its simplest form: Two treatments are applied, one to one group 
and one to the other; then the groups are split into two strata, those Easy and 
those Hard to achieve success with, as determined by a criterion other than the 
present treatments. The report of the overall investigation is that Treatment I 
got 60% success and Treatment II got 44% success (Panel A of Exhibit 1). The 
field of vital statistics would call these numbers "crude success rates." They are 
crude because they do not consider the comparability of the groups taking the 
different treatments, because they essentially ignore the additional information 
provided by the classification into strata. 
In our example, let us suppose that the numbers came out as in Panel A of 
Exhibit 1. The crude success rate is computed for Treatment I, for example, 
thus: 
0.7 x 800 + 0.2 x 200 600 
= = 0.60, 
1000 1000 
or 60%. 
We note an alarming thing. Although the crude rate is higher in Treatment 
I, Treatment II performs better on both kinds of people, the Easy and the 
221 
222 Exhibit 1 / 11: Standardizing for comparison 
Hard (see Panel C). In this example, the comparison of the crude success rates 
is misleading about the relative value of the treatments. Treatment II was 
almost forced to have a lower crude success rate because it was applied to a 
much larger proportion of people who are hard to get success with. 
Once this is pointed out, we can consider what to do instead. Naturally, 
the set of four percentages of success gives us a good summary of the situation, 
and knowing them, we can answer various questions. Four possible questions 
are: 
1. What would the comparison of rates be if both treatments were to be 
applied to a population with the composition of Treatment I's group? 
2. Same question, about a population like Treatment IFs group? 
3. What about the average population? (Half that of Treatment I, half that of 
Treatment II.) 
Numbers exposed 
to Treatment 
I II 
Easy 450 450 
Hard 550 550 
Exhibit I of Chapter 11 
Calculation of crude success rates, two treatments, two strata. 
A) Number of successes 
I Treatment ] 
I Stratum I I I I I'i 
Easy 560 80 
Hard 40 360 
Total 600 440 
B) Numbers exposed 
Easy 800 100 
Hard 200 900 
Total 1000 1000 
C) Success rates 
Easy 70% 80% 
Hard 20% 40% 
Crude rate 60% 44% 
11A: The simplest case 223 
. What about an arbitrary population? 
:: Percentage of successes 
[Treatment I I I, Treatment II I Diff. (II- I) I 
Answers 
Question 1 60 72 12 
Question 2 25 44 19 
Question 3 42.5 58 15.5 
Question 4. When we come to question 4, Panel C shows that the difference is 
somewhere between 10 percent in favor of Treatment II (80- 70) and 20 
percent in favor of Treatment II (40 - 20), depending on the mixture of those 
Easy and Hard to succeed with. When all are Easy, Treatment II wins by 10 
percent; when all are Hard, Treatment II wins by 20 percent. A 50-50 mixture 
of Easy and Hard gives 15 percent (60- 45), just midway between. 
Which of all these populations should we choose? The answer depends on 
our purpose, of course, which might be merely reviewing possibilities as we 
have just done, to see how sensitive the results are to the change in mix. If so, 
we have done enough already. But we may want to report for a specific 
situation. 
We would then make our best estimate of the kind of population we were 
expecting to have to treat, and perform the calculation on that basis, with 
perhaps some consideration given to deviations from that chosen population. 
The chosen population is called the standard population; that is, it is the mix 
for which the comparison is standardized. Any mix can be chosen as the 
standard. Now consider our second example. 
Example 2. Crossing percentages. Let us suppose that the rates of success for 
the Easy and Hard groups are as follows: 
Successes with 
treatment 
Easy 70% 80% 
Hard 40% 10% 
Now when we have all Easy subjects, Treatment II wins by 10%, but when 
they are all Hard, Treatment I wins by 30%, and therefore it makes quite a 
difference to us what the standard population is. In Example 1, we merely 
missed the difference a bit, but now if we can use only one treatment, our 
choice will depend upon the mixture that exists in the population we plan to 
treat. Above 75% Easy we choose Treatment II, and below that we choose 
Treatment I. 
224 /11: Standardizing for comparison 
Of course, if one can identify the subject's class beforehand and give the 
more successful treatment for that class, we are in the best of all possible 
worlds. In these cheery circumstances, the interest in standard populations may 
well wane. 
Needless to say, this approach can be generalized to handle several 
treatments and to handle many classes. The method is usually called direct 
standardization. 
The results of direct standardization are often computed--just as we have 
done so far--with no attention to the indicated variability, but we may wish 
some indication of variability. In question 1 above, for Example 1, we have 
PStd ----- 0.8pn. asy + 0.2pHard 
as our computed success rate. Treating 0.8 and 0.2 as fixed, and using 
^ Pq 
vat p = -- 
N' 
where q - 1 - p and N is the group size, and letting u and v be the observed 
proportions, we have 
ar (0.8u + 0.2v) = 0.64 ar u + 0.32 aov (u, v) + 0.04 ar v, 
in which we can calculate the appropriate variances. 
Let Psta,i be the observed proportion of success when the standard 
population sizes (0.8 Easy and 0.2 Hard) are applied to the percentages of 
success from Treatment I. Similarly, psta,ti is the observed proportion of success 
when the standard population (same mixture) is applied to the percentages of 
success of Treatment II. Then PStd,I- Pstd,1I is the difference in the observed 
proportion, and we want to compute its variance. We get, assuming zero 
correlation between results for the Easy and Hard strata: 
ar pstd. = 0.64 (0.70)(0.30) + 0.04 (0.20)(0.80) = 0.000200 - (0.014) 2 
800 200 
= (1.4%)2; 
9ar PStd.n = 0.64 (0.80)(0.20) + 0.04 (0.40)(0.60) 
100 900 = 0.001035 = (0.032) 2 
= (3.2%)2; 
.64((0.70)(0.30) (0.80)(0.20)) 
,ar (PStd,I- PStd,It) = 0 \ - 4- 100 ' 
((0.20)(0.80) (0.40)(0.60)) 
+ 0.04 \, 2- + 900 
= 0.001235 = (0.035) 2= (3.5%)2; 
or, more simply, since there is no correlation across treatments, 
'ar (Psta,t- PStd,II) = (1.4�/o) 2 + (3.2%) 2 = (3.50/0) 2. 
(11A, 11B) 11C: Precision of directly standardized values 225 
Thus, our difference of 12% (under Question 1) has a stan.dard error of 3.5% 
and can be regarded as certain of sign and as known to within about a factor of 
two. 
11B. Direct Standardization 
Let us suppose that there are J treatments and K strata. In the standard 
population, we have the proportions Wk, k = 1, 2,..., K, which might well be 
thought of as weights, so that  W = 1. And we have success rates corres- 
ponding to the ]th treatment in the kth stratum Pjk, where ] = .1, 2, ..., J. 
In Example 1, the strata were Easy and Hard and the Pjk appear n the bottom 
panel of Exhibit 1. 
Then Di, the directly standardized success rate for Treatment ], is given by 
Z=l Wk ' 
which simplifies, when  W  1, to 
Di=  WP. 
kl 
Example 3. Death rates in Maine and South Carolina. Theodore D. Woolsey, 
in his description of adjusted death rates, gives a beautiful example where the 
crude death rates produce a misleading result in comparing the death rates for 
Maine and South Carolina in 1930. Exhibit 2 gives the basic data. The point of 
the example is similar to that of our Example 1: South Carolina had a higher 
death rate than Maine in all but one age class, and even there they were nearly 
equal. Nevertheless, the crude death rate gave South Carolina the better 
overall showing. The reason was that Maine's population was generally much 
older than that of South Carolina. Maine had a crude death rate of 1390.8 per 
100,000, while South Carolina had only 1288.8 per 100,000. 
To get a better comparison of the overall death rates, Woolsey did not 
choose the age distribution of either of the states. Instead, he took as his 
standard population, the age distribution of the whole United States, and 
applied the death rates for each age interval--in Maine, PEk, and in South 
Carolina, Pscto this standard population, which he represented as 1 million 
people spread across the age groups. The state corresponds to a treatment, the 
age group to a stratum. His calculation is shown in Exhibit 3. 
11C. Precision of Directly Standardized Values 
We want some idea of the possible precision of the directly standardized 
numbers of deaths. We may not need to be as precise as can be. So let us 
develop both a rough calculation (to be defined below) and a crude calculation, 
as well as a relatively precise one. 
226 Exhibit 2/11: Standardizing for comparison 
11C; Precision/Exhibit 3 (through S) 227 
228 / 11: Standardizing for comparison 
. Let N be the sample size and p the proportion dying. Then, assuming 
bnomial variation and taking advantage of p being small, so that q -- 1 - p is 
near 1, we have 
var (Actual deaths) = Npq  Np  Actual deaths. 
Method 1. Crude binominal. For South Carolina, there are 22,401 actual 
deaths. This estimates both the mean Np and the variance Npq, because the 
latter is close to Np. 
For Maine the corresponding estimate of mean and crude variance is 
11,082, the actual deaths. 
Method 2. Standardized binomial (rough standard error). If D is the number 
of deaths, let KD be the standardized deaths, regarding K as a constant. Then 
the mean is KNp and the variance K2Npq  K2Np. We can take 
Standardized deaths Standardized deaths 
K=  
Actual deaths Np 
Thus, an alternative, improved approximation for the variance of standard 
deaths is 
(Standardized deaths) 2 
since q  1. 
Actual deaths ' 
To take still more care, we would multiply by q. 
For South Carolina, the estimate with q set at 1, is 
(17,163) ' 
= 13,150. 
22,401 
For Maine, the corresponding estimate is 
(12,033) 2 
= 13,066. 
11,082 
Using these estimates for the variances, the standardized difference, 
South Carolina MINUS Maine = 17,163 - 12,033 = 5,130, 
the number of excess deaths for South Carolina, gets variance (for this 
estimate of excess deaths) 
13,150 + 13,066 = 26,216 = (162) 2. 
Thus we see that the difference is close to 30 times its approximate standard 
error. We are unlikely, in such a case, to need a better standard error, but we 
shall go ahead, as an illustration of the method. 
11C: Precision of directly standardized values 229 
Mehod 3. Stratified binomials. To improve our approximate calculation, we 
?.eed only apply the 
?:'":..""i' ( S tan d ar dized d e aths) 2 
� .?: Actual deaths 
iapproximation to each stratum (age group, here) separately, starting with, for 
:SOuth Carolina, 
(1916) 2 (150) 2 (164) 2 
+ + +... 
4905 446 410 
'':.'.'.'748.4 + 50.4 + 65.6 + 176.7 + 300.7 + 1043.8 
� "':.':': + 1257.7 + 1928.0 + 2653.0 + 3560.2 + 3414.2 = 15 198.7, 
::land, for Maine, 
:1758.0 + 154.1 + 150.2 + 285.5 + 474.4 + 973.3 
+ 1043.6 + 1666.6 + 1811.1 + 2622.6 + 2462.1 = 13,401.5, 
:So that our rough approximation for the variance of the difference is now 
� '::'.'". 15,198.7 + 13,401 5 = 28,600.2 = (169) 2 
.i'less than 5% greater than the rougher approximation. 
:Allowing [or q. To improve the approximation still further, we must re- 
member the q in Npq and use 
:: (Standardized deaths) 2 
Actual deaths (1 - probability of death) 
??for each stratum, starting with 
� .. (1916) 2 (150) ' 
..?.' (1 - 0.02392) = 730.5 and (1 - 0.00185) = 50.3 
ii.:. 4905 446 
South Carolina, and reachirig 
730.5 + 50.3 + 65.5 + 175.9 + 298.8 + 1034.7 
: + 1242.1 + 1889.6 + 2565.1 + 3341.4 + 2931.6 = 14,325.5 
for that state, and for Maine 
!'i"'.i: 1721.9 + 153.8 + 150.0 + 284.9 + 472.6 + 969.5 
+ 1037.9 + 1648.5 + 1774.2 + 2485.7 + 2126.1 = 12,825.1, 
:i::giving, for the estimated variance of the difference, 
14325.5 + 12825.1 = 27150.6 = (165) :z. 
230 / 11: Standardizing for comparison 
Comparison of Approximations 
In the example, it matters little whether we take the standard error of the 
difference of 5130 standardized deaths as +162 or +169 or +165, particularly 
since sources of error beyond binomial variation usually need to be considered. 
For example, year-to-year variation in the South Carolina-Maine difference 
might dwarf the variation we have carefully calculated. In more extreme 
circumstances, the difference between the various approximations and the full 
stratified Npq form can be more important. 
When we deal with small numbers of deaths or small numbers of survivors, 
we may need to go a little further. In the most extreme case -when either 
deaths or survivors are zero where pq, as usually calculated, vanishes so that, 
if we are not more careful, we assess a zero estimated variance we may 
especially want to enlarge the estimated variance because we are confident it is 
larger than 0. We have no way to do anything exactly although, in more 
complicated problems, Sutherland, Holland, and Fienberg (1974) have ways to 
offer some help. We offer here a method that does fit in with the rest of our 
treatment of variables. But this consistency does not, so far as we know, 
guarantee that our correction is nearer the true value than was the original raw 
estimate. 
The proposed adjustment is to add  to the count of every category, having 
1 
p, _ actual deaths +  and q, _ actual survivors +  
cell count + � cell count + . 
for each cell for which an estimated variance would be required, thus leading to 
the use of 
Estimated variance 
\' Txp--sed / (actual deaths + ) 1 - ci'cuu - . 
for the contribution from each cell in which there are no deaths. 
11D. Difficulties with Direct Standardization 
We began as if there were no uncertainty about the P's and went on to 
calculate some uncertainties. In Example 3, the death rates at each age, called 
age-specific rates, are fairly well determined. But often our rates are based 
upon the investigation under discussion, and then it is a common experience 
that some cells (treatment-stratum pairs) may have small numbers of cases and 
so the P's may be estimated with considerable uncertainty. When this sort of 
uncertainty is combined with a substantial weight, then the result of the entire 
calculation may be unduly sensitive to the specific outcomes. 
?.. 11D: Difficulties/Exhibit 4 231 
ilexample 4. Ill-determined rates. Let us give an example where such an effect is 
easy to demonstrate. If we work with death rates associated with an elective 
bPeration done under two different treatments, we might have three classes of 
Patients, those in Excellent health, those in Fair health, and those in Poor 
health. The distribution of patients might fall as in Exhibit 4. 
Let us note that the one patient in cell [Poor x Treatment I] died. Had she 
Survived, the cell rate would have been 0/1 = 0, the death rate/1000 would 
have gone down from 26.48 to 1.47, and Treatment I would have looked very 
favorable for the standard population. Instead of looking about 6 times as bad, 
it would have looked 3 times as good as Treatment II. 
Exhibit 4 of Chapter 11 
Table with heavily weighted unreliable cell 
A) Deaths 
: Treatment I 
iStratum I I I I II I 
Excellent 10 4 
Fair 9 20 
Poor 1 2 
B) Numbers exposed (cell counts) 
Excellent 10,000 5,000 
Fair 3,000 4,000 
Poor 1 20 
C) Rates/1000 
Standard I Expected deaths I 
population I I ! III I 
Excellent 1 0.8 14,500 14.5 11.6 
Fair 3 5 5,000 15.0 25.0 
Poor 1000 100 500 500.0 50.0 
Totals 20,000 529.5 86.6 
Rate/1000 26.48 4.33 
Had the Poor-Treatment I patient survived: 
Rate/1000 1.48 4.33 
232 / 11: Standardizing for comparison 
How can this be fixed? Some things just can't be fixed. In the example, we 
can, of course, investigate to try to find out whether the lone patient who died 
suffered some sort of horror story, was doomed from the start, and possibly was 
out at the "hopeless" edge of the category Poor. But so, of course, may have 
been the deaths in this category for the other treatment. Such troubles require 
careful consideration and cannot all be provided for by technical statistical 
devices. 
What we are illustrating is that, when basically small probabilities acciden 
tally produce high estimates, possibly by factors of 10 or 100, and when they 
are given substantial weight by the standard population, then the direct method 
of standardizing can give rather wild results. Consequently, it is important to 
review any direct standardizing calculation for this sort of difficulty. One grave 
danger is that the number of cells may be very large and that a computer is 
going to compute the final result without displaying any intermediate calcula- 
tions; then one can be badly fooled. Whenever a direct standardization gives a 
wild value, the possibility should be reviewed that this was caused by a small 
probability having been heavily weighted. Whenever such a wild value doesn't 
occur, we may wonder whether it should have. In either case, it will help us to 
know estimates of the relevant standard deviations. 
If we estimate the standard error by the revised formula, using p* and q*, 
we find, for the expected deaths for the Poor-Treatment I cell, the following: 
p, = 7/6 7 1/6 1 
4/3 =  and q* = =- 
4/3 8' 
Estimated variance- (500)2 7 (1 7)=(191)2 
1 6 8 ' 
which contributes + 9.55 to the estimated standard deviation of the 26.48 rate 
which may be enough to warn us. (The difference is only 2.3 times its standard 
error.) 
If we had had a computer program that reported (a) estimated variance of 
total expected deaths and (b) largest contribution from a single cell, we would 
have been adequately warned, since these figures would have been 
36505 = (191) 2 and 36458 = (191) 2. 
Here, 
1  + [3-") 91 - 3000� 
+(400 (_ 0 
0-) (10-}) 1(-00/ 
= 36458.33 + 25.39 + 21.35. 
11E: Indirect standardizing 233 
(Had we added �, as is sometimes done, instead of , we would have found 
3_ 1 
: p, = _2 = 3 q. . 
2 4 and = 1 p 4 
Estimated variance = (500)2 (23-)(1 - ) = (306) 2, 
: 
:increasing the estimated contribution to the standard error by almost a factor 
of rig. The difference between using  and �, though sometimes large, 
:rarely matters. If we should have been adding 0.01 instead of either of these, 
the estimated variance would have been (49) 2 and that might matter.) 
: None of these remarks or calculations is intended to deny the basic 
:uncertainty created in this technique by having one cell with one observation, 
Which then is given a large weight. It could, of course, have been even 
worse no observation at all. 
This feature of direct standardization may have been largely responsible 
:for the development of another method of standardization, to which we now 
turn. 
11E. Indirect Standardizing 
For direct standardizing, we had to reach outside the system for a standard 
Population, or invent it as an average of some sort from the two populations we 
had at hand. We applied the specific rates associated with each treatment to the 
Standard population. In indirect standardization, we go the other way round. 
:We reach out for a set of standard specific rates and apply them to the 
:Population associated with each treatment, and see how the resulting frequency 
of success compares with the crude rate for the population. Then we compare 
::the two ratios. Fleiss (1973) offers a good discussion of the dangers and 
anomalies associated with direct and indirect standardization, as well as medi- 
cal and birth- and death-rate examples. 
Where shall we get our standard rates for indirect standardization? In a 
problem like the Maine-South Carolina comparison that we treated in Exam- 
ple 3, Woolsey chose the specific death rates for the given age groups in the 
:United States as a whole. This uses the U.S. experience as a baseline, just as he 
used the sizes of U.S. national age groups as the standard population in the 
direct attack. 
Sometimes in direct standardization, people choose the population formed 
by pooling the populations from the treatments. The corresponding choice for 
indirect standardization would be to use as standard rates some weighted 
combination of the treatment rates for the several treatments, and, of course, 
one attractive set of weights for that purpose is "weighted proportional to 
count of stratum." Using these weights is equivalent to using the rates pooled 
across treatments. Let us return to the data of Example 1 and carry through 
with this particular weighting. 
234 Exhibit 5(A-C)/11: Standardizing for comparison 
Example 5. Indirect standardization applied to Example 1 in Exhibit 5. First, 
we compute the standard rates for each stratum separately. Then we apply the 
standard rates to the treatment counts in the two strata, for each treatment 
separately. This gives us the reference percent of success for each treatment. 
(These numbers are not to be compared with one another! They are to be 
compared with their own crude success rates.) 
Exhibit 5 of Chapter 11 
Indirect standardizing for Example 1 
A} Numbers of successes 
.I Treatment 
Stratum] Ill II ITotals I 
Easy 560 80 640 
Hard 40 360 400 
Total 600 440 
B) Numbers exposed 
Easy 800 100 900 
Hard 200 900 1100 
Total 1,000 1,000 
C) Success rates Standard 
rates 
Easy 70% 80% 71 1% (= 64o 
� 9-3-6) 
Hard 20% 40% 36.4% (= --o) 
Crude rate 60% 44% 
Applying standard rates to: Reference 
Success 
rates 
71.1 x 800 + 36.4 x 200 
Treatment I: = 64.2% 
1,000 
71.1 x 100 + 36.4 x 900 
Treatment I1: = 39.9% 
1,000 
crude rate 
Standard success ratio = reference rate 
60% 44% 
Treatment I: - 0.93; Treatment I1: - 1.10 
64.2% 39.9% 
'":"': 11E: Indirect standardizing 235 
:'"':':': The results suggest that Treatment II has a relatively higher success rate 
:than does Treatment I. Relative to what? Treatment II is 10% favorable 
:compared to the standard, and Treatment I is 7% unfavorable compared to its 
:..Standard. 
i"'.':' The principles of the rough approximation to the (binomial) variance, used 
earlier for direct standardization, are easily applicable here. The most direct 
:application, treating nstd as known, would be to compute either 
/'gObs -- /'[Std or /4nobs + 2 -- /4 FISt d q- 1 
i .:......: Std 
':::'::as an approximately standard normal deviate. Actu, except for rtObs = 0, 
:..nObs + 2 is a good enough approximation to v nob + /n-  }, which is 
?approximately standard normal [see Freeman and Tukey (1950)]. For nob = 
'0, replace /4-0 + 2 by 1. In our example, we have 600 successes = nob, and 
::642 standard successes = /Std, and 
"?.' 600 - 642 -42 
:.'"':" Z = = = -1.66 or = /2402 - x/69 
i': ,/642 25.3 
= 49.01 - 50.68 = -1.67. 
The minus sign reflects the failure of the observed number of successes to come 
up to the standard. 
:'" For Treatment II, we have 
'Zii: hobs- nStd __ 440- 399_ 41 - 2.05 or /1762 x/1597 = 2.02. 
%/r-Std %/ 20.0 
Then the quantity we are concerned about is 
Zi- Zn -1.65 - 2.05 -1.67 - 2.02 
= = -2.62 or = -2.61 
:.: / 1.414 x/ ' 
which suggests that Treatments I and II do perform differently, because the 
final quantity, if they were identical, would be a unit-variance, approximately 
normal random variable. 
.:...Examlle 6. Another set o[ standard rates. The standard rates need not be the 
weighted averages; instead we might have weighted by treatment totals, which 
would give, as standard rates, 
Easy 75% (= [(1000 x 0.70) + (1000 x 0.80)]/2000)) and Hard 30%. 
Applying these rates gives 
Treatment I: 66% Treatment lI: 34.5% 
crude 600 440 
Ratios: = 0.91 = 1.28 
standard 660 345 
and again Treatment II shows the higher rate. 
236 / 11: Standardizing for comparison 
Discussion 
The net difference in Example 6 is: 
(-9%) - (28%) = -37%, 
instead of 
(-7%)- (10%) = -17% 
(as in Example 5). (And even (Zx-Zxi)/x/ is nearly double its previous 
value.) Without regard to the assessment of uncertainty, how are we to think 
about these two answers, both negative but considerably different in size? If we 
do have strong external evidence about standard rates, we should use that 
evidence. What if we do not? Clearly, the first thing to do is to understand, if 
we can, why the strata were so unbalanced for the two treatments. Sometimes 
this can tell us something about which standardized rates to use. But what if 
this does not decide for us either? 
The most extreme standard rates that are "consistent" with the data are as 
follows, and lead to the following ratios: 
crude successes 
Ratio: 
standard successes 
70% 20% 1.00 1.76 0.76 
70% 40% 0.94 1.02 0.08 
80% 20% 0.88 1.69 0.81 
80% 40% 0.83 1.00 0.17 
All these agree that Treatment I seems, on balance, to have a lower death rate 
than Treatment II in this regard, any of these is much better than the crude 
comparison, which gave the wrong sign. They do differ quite considerably 
among themselves, however, reminding us that it often matters appreciably 
which standard rates we use. 
This uncertainty is over and beyond the uncertainty assessed above, which 
treats the standard rates as known. 
Often the unbalance is not nearly so great as here. Accordingly, the exact 
choice of standard rates is often not so impor-tam. In some studies, we can find 
reasons for a choice of standard rate. In the others, one plausible policy might 
be to use the median rate across.all treatments (including control treatments). 
In our example, of course, this would again lead to the 75% and 30% that we 
got by using treatment totals for weights. 
Example 7. Indirect approach with an ill-determined rate. Let us apply the 
indirect method to Example 4, which had one seriously ill-determined rate. Let 
us arbitrarily use the pooled counts to get standard rates, as shown in-Exhibit 6. 
11E: Indirect standardizing/Exhibit 6 237 
Exhibit 6 of Chapter 11 
 Indirect standardization with an ill-determined rate 
 A) Numbers of successes 
Treatment 
Excellent 10 4 14 
Fair 9 20 29 
Poor I 2 3 
Total 20 26 
: B) Numbers exposed 
Excellent 10,000 5,000 15,000 
Fair 3,000 4,000 7,000 
Poor I 20 21 
C) Rates/1,000 Standard 
rates 
14 
Excellent I 0.8 
15,000 
29 
Fair 3 5 
7,000 
3 1 
Poor 1,000 100 = - 
21 7 
D) Standard successes 
Excellent 9.33 4.67 
Fair 12.43 16.57 
Poor .14 2.86 
Total 21.90 24.10 
Crude successes 
Ratios' � 
Standard successes 
0.913 1.08 
Rough strength: 
/(20) + 2 - (21.90)+ I = 9.06 - 9.41 = -0.35' -0.35 - 0.43 
4(,/-' 7  'v/';  ' 10.30 9.87 0.43) / = -0.55 
if we had 10 times the data, the final result would be multiplied by about 
  3.16, and, if we had 100 times the data by about 10. (Thus we would need 
vastly more data to have a reasonable chance of reaching significance.) 
238 / 11: Standardizing for comparison 
This approach suggests that Treatment II has the higher death rate but 
that we can't be sure. The approach suppresses the effect of the single case in 
the 1-Poor cell, so far as indication goes, but preserves it in assessing uncer- 
tainty. 
In Exhibit 6 we also look to see what 10 or 100 times as much data, 
divided in exactly the same way, would provide in the way of strength of 
evidence. 
Discussion 
We mention two awkwardnesses with the indirect approach. 
1. Even if the treatments have equal rates for each group, if the groups have 
different proportions, then the indirect method will produce a difference in 
overall success rates, usually slight, but annoying. 
2. Woodworth offers an example to show that, when one compares 3 or more 
treatments, it is possible for one treatment to have higher rates in each 
group than a second treatment and yet wind up with a lower standardized 
rate. This seems more distressing than the previous difficulty. 
Exhibit 7 shows Woodworth's example. We note that, in Stratum 1, 
Treatment I has a higher success rate than Treatment II, and the same is true 
in Stratum 2. But the indirectly standardized rates for Treatments I and II are 
1.02 and 1.08, respectively. Thus, Treatment I, which starts out with a 
relatively higher success rate of about 10 or 11%, winds up with about a 6% 
lower rate. Woodworth reports that, if all comparisons are made by pairs, this 
effect does not occur. 
This example deserves some further attention. We face a quite inconsistent 
situation: Two treatments give higher rates in Stratum 1, while the third shows 
a higher rate in Stratum 2. If we analyze the table of rates in the manner of 
Chapter 9, we find: 
0 0 -22 10 
0 0 22 -10 
10 0 -10 100 
for (1000 times) the rates themselves, and 
0 0 -10 [I 4 
0 -1 10 -4 
4 0 511 -100 
for (100 times) their logarithms. Either table shouts for an explanation as to 
why Treatment III is different. 
It is probably even more important to understand this difference between 
Treatment III and the other two, than to make the comparison of Treatments I 
and II come out right. 
:::..:: 11E: Indirect standardizing/Exhibit 7 239 
: 
....' Exhibit 7 of Chapter 11 
Woodworth's example of reversal of rates through indirect standardization 
" A) Occurrences 
::i: I Treatment ] 
1 12 99 39 150 
1.2 90 9 51 1 50 
:Totals 102 108 90 300 
B) Numbers exposed 
: 1 100 900 500 1,500 
2 900 100 500 1,500 
: Total s 1,000 1,000 1,000 
C) Rates/1,000 
Standard 
rates 
:1 .120 .110 .078 .1 
2 .100 .090 .102 .1 
D) Standard occurrences 
1 10 90 50 
2 90 10 50 
Totals 100 100 100 
Crude occurrences 
Ratios: : 
Standard occurrences 
1.02 1.08 o.go 
Differences between crude and standard occurrences, in standard deviation units, 
using /4nob, + 2 -/4nstd + 1: 
.22 .81 --1.00 
S) SOURCE 
Woodworth, G. G. (1971). Standardized Mortality Comparisons: Sketch of a Review. Unpublished 
manuscript. Used with permission of the author. 
240 / 11: Standardizing for comparison 
Had we used the median rule in this example, our standard rates would 
have been 0.115 and 0.095, so that standardizing would have produced a 
correct sign for the comparison of Treatments I and II. This works often but, 
sad to say, not always. (As we can see if we make up a new example, replacing 
Treatment III by three Treatments, say IIIa, IIIb, IIIc that seem to behave 
exactly the same, with the same total counts as for Treatment III. In such a 
case, the two-way analysis would shout even more loudly.) There is no 
substitute for looking at the two-way analysis of the individual rates! 
Once again the basic message is that 
1. One number cannot honestly do the work of many. 
2. We should look as carefully as we can at the basic tables. 
3. Sometimes we are, nevertheless, forced to summarize statistics in a single 
number, or find it desirable to do so. 
11F. Adjustment for Broad Categories 
We have so far treated our strata as they stood. We need to be concerned 
about the possibility that our strata are wider than we wish. 
Let us go back to Example 1 (in Exhibit 5, in particular), where we have 
only two strata: Easy and Hard. While we may not be able either from lack of 
records or lack of insight--to classify the cases more finely, we can hardly 
doubt that some Easy cases are easier than others or that some Hard cases are 
harder than others. 
Whatever mechanism determined that Treatment I should be used on 
80% Easy cases and Treatment II on 90% Hard ones is unlikely to be blind to 
shades of ease or hardness. If we could cut the data into four strata, rather than 
two, we might expect something like Exhibit 8. The difference between 
treatments looks a little greater than it did in Exhibit 5. If we calculate the 
indirectly standardized data, they, too, will now look more different. Having 
four strata instead of two can lead to an appreciably greater correction. What 
are the morals of this story? 
First and foremost, we must not feel that standardization for the strata that 
we happen to have on hand has eliminated the influence on whatever variable 
underlies our stratification. We have gotten rid of some, but not all, of that 
variable's effects. We must be prepared for a need for further adjustment. 
Second, we need to seek out a plausible method of further adjustment. 
Such a method may be useful, even if its formal development calls on unlikely 
assumptions. Its u. se helps, even if i! gives us only an order-of-magnitude 
indication of the sze of the further adjustments required. (And we can usually 
do better than just this.) 
Adjusting ]or effects o[ broad categories. When we adjust for broad categories, 
as in the "Easy"- and "Hard"-to-cure example in Section 11A, we think of 
� 11F: Adjustment/Exhibit 8 241 
:these two categories as broad categories on a continuum representing degree of 
difficulty of success. And we think of the individuals as spread along this 
continuum. In the example, under Treatment I, 80% were easy to cure, 20% 
hard. And so we think of the distribution as in Exhibit 9. The cutting point of 
::the distribution splits the easy cases from the hard ones. The areas under the 
:Curve to the left and right equal the percentages in the two groups, 80% and 
20%, respectively. If we have such a difficulty scale and such a distribution, 
then, in principle, we can compute the center of gravity for the easy cases and 
for the hard ones. In Exhibit 9, these are represented by the small fulcrums � 
beneath the horizontal axis. These centers of gravity can be regarded as 
measuring the average difficulty of successful treatment in the two groups. 
Don't worry for a moment about where that scale comes from or where the 
distribution comes from. We supply it later. 
Next, we have a corresponding picture for Treatment II of the example, 
:10% easy, 90% hard, as shown in Exhibit 10. Again we have centers of gravity 
for the two groups. 
Exhibit 8 of Chapter 11 
A hypothetical further breakdown of Exhibit 5. 
A) Successes 
Treatment 
Very easy 360 20 380 
Semi-easy 200 60 260 
Semi-hard 35 250 285 
Very hard 5 110 115 
B) Numbers exposed 
Very easy 400 20 420 
Semi-easy 400 80 480 
Semi-hard 150 500 650 
Very hard 50 400 450 
C) Rates 
Very easy 90% 100% 90.5% 
Semi-easy 50% 75% 54.2% 
Semi-hard 23.3% 50% 43.8% 
Very hard 10% 27.5% 25.6% 
242 Exhibits 9 and 10/11: Standardizing for comparison 
We want to see how the rates of success (bottom panel of Exhibit 1) relate 
to the centers of gravity for the two treatments and two classes of individuals 
treated, and so we form Exhibit 11, where the x's show rates of success plotted 
against centers of gravity for the four groups. An important finding is that the 
two Easy groups are not matched on the difficulty scale. Similarly, neither are 
the two Hard groups. Our purpose is to adjust for these inequities in difficulty. 
To do this, we need to pick an average difficulty and adjust the success 
rates to this average difficulty. We might choose the mean or median (if there 
are several treatments or several groups), possibly weighted by numbers of 
cases. ff the lines connecting the points in Exhibit 11 are approximately 
parallel, it will make little difference which average we choose. For the 
moment, let us adjust to the average difficulty for the Easy groups and the 
average difficulty for the Hard groups. The geometry is shown in Exhibit 12, 
where the abscissas of the open circles represent the average standard difficulty 
for the Easy groups and for the Hard ones. 
Exhibit 9 of Chapter 11 
An illustrative distribution of individuals along the scale of difficulty, with 80% 
easy to cure, 20% hard to cure, as in Treatment I in Section 11A. The 
represent centers of gravity for the two groups. 
Easy tard  .._  
� Cutino. � tandard dificuit. 
Exhibit 10 of Chapter 11 
Hypothetical distribution of difficulty for the individuals undergoing Treatment II, 
from Section 11A. 
oint 
11F; Adjustment for broad categories/Exhibit 1 1 243 
To get a final summary of the difference in success rates for the two 
i:treatments, adjusted for difficulty, we might average the differences for the 
Easy and Hard groups, perhaps weighting by the numbers in the original 
:groups, or by the numbers expected in a population to be treated. 
With one omission, we have given the overall plan for the two-treatment, 
Wo-category problem. The omission is the distribution and its association with 
ihe scale of difficulty. Although many distributions might be chosen, we would 
lilce one with an easily computed center of gravity for segments. It should not 
matter a great deal just what distribution we choose, provided we take a 
 distribution whose shape roughly agrees with our ideas of the distribution of 
 difficulty in a population. The logistic recommends itself. It has an attractive 
Shape, symmetric with a single mode that is not unduly peaked. Its formulas 
are convenient and we have a table that helps us compute the centers of gravity 
(]xhibit 11 of Section $F). The idea of scoring grades by way ot centers of 
 gravity has already been presented in Section 5H, together with the needed 
ormulas. 
Exhibit 11 of Chapter 11 
Plot of success rates against centers of gravity (for two Easy and two Hard 
groups). 
The x's show rates of success plotted against centers of gravity for the four 
groups. We observe that the Easy groups are not matched for difficulty, nor are 
the Hard groups. 
Iat� of 
success 
as mnt  
50 - Treatment  
I 
244 Exhibits 12 and 13/11: Standardizing for comparison 
Exhibit 12 of Chapter 11. 
Plot of average standard difficulty versus success rates for Easy and Hard groups. 
The abscissas of the open circles represent the average standard difficulty for the 
Easy groups and for the Hard ones. The arrows show the movement required to 
adjust the success rates for each of the four groups. The lengths of the vertical 
line segments represent the estimated difference in success rates for the two 
treatments after being adjusted for difficulty. 
le f 
success 
I01 Selecfed 
comparison 
I 
I 
I 
Treab?k.r K I 
Eas, I 
I 
I 
'tO - I Hard 
,, 
comparison 
Hard 
0  Stanard dfficult.v 
Exhibit 13 of Chapter 11. 
The standard logistic density function. The abscissa is log [p/(1 - p)], where the 
cumulative is p. 
.... I 0 I 2 3 4 > x 
 11F: Adjustment for broad categories 245 
The logistic. The standard logistic distribution has a probability density with 
the general shape shown in Exhibit 13. It is symmetric about zero. The defining 
property is that the abscissa at the point where the area to the left is p has the 
:value 
� 
x = d = log 1 p ' 
We note that when p = �, d = O, we use d as a mnemonic for standard 
difficulty. 
We choose the logistic distribution and translations of it to represent 
distributions of degrees of difficulty in treatment populations. We will want the 
cutting points moved to zero, and so we shall have to translate the distributions 
by the distance from log[p/(1 - p)] to O; or, equivalently, we measure the 
distance of the center of gravity from log [p/(1 - p)]. 
The general idea is that the difficulty of treating a population successfully 
can be approximately represented by a bell-shaped curve whose center moves 
to the right as the population becomes harder to treat successfully. 
To get an idea of the scale of the standard logistic, the central interval 
containing 95% of the density runs from -3.23 to 3.23. 
Finally, we need the center of gravity for such an interval. From Section 
5H we have that, if the upper cumulative level is B and the lower is A, the 
center of gravity of the interval is 
[B log B + (1 - B)log (1 - B)] - [A log A + (1 - A)log (1 - A)] 
CG= 
B-A ' 
When. we have only two classes, one runs from 0 to p, the other from p to 1.. 
Substituting A = 0, B = p in the formula, we get the center of gravity of ths 
interval for the standard logistic as 
p log p + (1 - p)log (1 - p) 4(p) 
0 to p: CG = = . 
P P 
Then, letting A = p, B = 1, gives, for the other segment, 
p log p + (1 - p)log (1 - p) -4'(P) 4,(1 - p) 
p to 1: CG = - = = , 
1-p 1-p 1-p 
where q(p) is tabled in Exhibit 11 of Section 5F. Recall that to translate the 
distributions, we measure the distance of these centers of gravity from the 
cutting point 
P 
log 1 - p' 
thus effectively translating the cutting point to zero. We now carry out, as a 
numerical example, the example given at the beginning of the chapter. 
The basic data we need are shown in Exhibit 14. 
246 Exhibit 14/11: Standardizing for comparison 
Numerical example. Let us carry out the calculations first for Treatment I. For 
the Easy group, p - 0.8, and so the cutting point on the standard logistic is 
0.8 
loge = loge 4 = 1.386. 
1 -0.8 
To compute the two centers of gravity, we read, for p = 0.8 from Exhibit 11 of 
p + -p)loge (1 p) is -0.5004. The centers of 
Section 5F, that p log (1 - 
gravity for the standard logistic then are 
-0.5004 
0 to 0.8: CG - = -0.6255  -0.626. 
0.8 
-(-0.5004) 
0.8 to 1: CG = - +2.502. 
0.2 
We want to find the positions of the centers of gravity when the cutting 
point is translated to zero, and so we must subtract 1.386 from each, to get: 
Treatment I. Results for shifted distribution, zero at p = 0.8: 
Easy 0 to 0.8 -2.012 - (-0.626- 1.386) 
Hard 0.8 to 1 1.116 = (2.502 - 1.386) 
Cutting point 0 = (1.386 - 1.386). 
The corresponding calculations for Treatment II, where 10% were in the 
Easy category and 90% in Hard, are, using the numerator -0.3251 from 
Exhibit 11 of Section 5F to get the cutting point -2.187 and the centers of 
gravity' -3.250 and 0.361. 
Exhibit 14 of Chapter 11 
Crude success rates: two treatments, two strata (repeated from Panels B and C of 
Exhibit 1 ). 
A) Numbers exposed B) Success rates/1000 
[ Treatment I Treatment 
Stratum I'll !, I Stratum I I] I !1 
Easy 800 100 Easy 70% 80% 
Hard 200 900 Hard 20% 40% 
Total 1,000 1,000 Crude rate 60% 44% 
11F: Adjustment/Exhibit 15 247 
Treatment lI. Results for shifted distribution, zero at p = 0.1: 
Easy 0 to 0.1 -1.063 = (-3.250 + 2.187) 
Hard 0.1 to 1 2.548 = (0.361 + 2.187) 
Cutting point 0 = (-2.187 + 2.187), log (0.1/0.9) = -2.187. 
The average difficulty for the two groups in each category is: 
Easy groups: �(-2.012 - 1.063) = -1.538, 
Hard groups' �(1.116 + 2.548) = 1.832. 
We can read the adjusted numbers off the graph shown in Exhibit 15, or 
interpolate after we get the slopes of the lines, as follows. 
Exhibit 15 of Chapter 11 
Success levels after adjustment for broad categories (indicated by heavy 
arrows). 
Rate of 
SUCCESS 
,. 
Ioo - 
= 40- 80 
80 ,et = -tl.06% 
40 - /ard 
ZO - Ha, 
o_ -z -I o I 2 3 
ctan carc d/'f'f;cultj 
248 / 11: Standardizing for comparison 
Adjusted values of success rates in percents 
For the Easy groups: 
Treatment I 70 - 15.98(-1.538 - (-2.012)) = 62.4% 
Treatment II 80 - 11.08(-1.538 - (-1.063)) = 85.3% 
For the Hard groups: 
Treatment I 20- 15.98(1.832- 1.116) = 8.6% 
Treatment II 40- 11.08(1.832- 2.548) = 47.9% 
The gains, Treatment II minus Treatment I, are then 
Easy 85.3 - 62.4 = 22.9 
Hard 47.9- 8.6 = 39.3 
Average gain 31.1 
Our calculation of the average gain going from Treatment I to Treatment II is 
31.1%, compared with the unadjusted crude increase 16% -- 60% - 44% and 
the simply adjusted value of �(10% + 20%) = 15%. 
The example brings out the point that adjusting for broad categories can 
make a substantial difference. 
Standard population. If we wanted to take as the standard population 45% 
Easy and 55% Hardinwhere "Easy" is a compromise between "Easy as given 
Treatment I" and "Easy as given Treatment II" and "Hard" is a similar 
compromise then we could weight the adjusted differences 0.45 and 0.55, and 
we would get 
0.45(22.9) + 0.55(39.3) = 31.9%, 
compared with the original values (simple adjustment): 
0.45(0) + 0.55(20) = 
Comment 
The adjustment we have made would be exact (for huge samples) if: 
1. the percent success depends linearly on the CG of the corresponding 
difficulty distribution, and 
2. the difficulty distributions have the same spread. 
The first of these assumptions is exact only when percent success is linear in 
standard difficulty. Since only approximate good behavior is needed to make 
the adjustment, this issue seems not to be so important for practice. 
: (11F) 11G: More than two broad categories 249 
Acting as if the difficulty distributions have the same spread seems rather 
more likely to cause trouble. We can avoid this by calculating %'s of success at 
the cutting point, which only requires us to proceed as though % success is 
linear in the standard difficulty CG's. For our example, at the cut between Easy 
and Hard, this gives: 
1.116 2.012 
For Treatment I: (70%) - (20%) = 37.84%, 
3.128 3.128 
2.548 1.063 
For Treatment II: (80%) + (40%) = 68.22% 
3.611 3.611 
:::(where 3.128 = 1.116 + 2.012 and 3.611 = 2.568 + 1.063). This difference 
of 30.38%, quite close to the difference obtained in the other way above, is 
pictured in Exhibit 15 as the difference between Treatment I and Treatment 
II at the point where the standard difficulty equals zero for both treatments. 
11G. More than Two Broad Categories 
The case treated in the last section is actually the most difficult one just 
because there is only one cutting point. Once we have two or more cutting 
points, our problem separates into two quite different parts: 
<>All but two categories are internal categories, bounded by two cutting 
points. These we can tackle quite directly and effectively. 
<>Two categories are external categories, which often involve only small 
fractions of the cases, and hence, for a reason described below, do not cause 
great difficulty. 
While we have more alternatives to consider, working with more than two 
categories is actually much more robust, and its results are correspondingly 
more satisfactory. 
Let us recapitulate some crucial aspects of the overall situation: 
ocases are of varying "difficulty". 
<> skilled judgment has divided them into three or more strata on the basis 
of difficulty. 
<>we believe that difficulty actually varies continuously, so that it is reason- 
able to treat the strata we actually have as if there were a continuous scale 
of difficulty that had been cut in a few places. 
o one way to put numbers on difficulty without any attention to observed 
results -is to assume a shape for some or all of each difficulty distribution. 
The logistic shape is convenient. 
250 / 11: Standardizing for comparison 
o For internal categories, as we shall see, the shape chosen is not very 
important. 
oFor external categories, as we shall see, the logistic shape is often neutral 
in behavior. 
Underlying these facts are two questions. (1) How well do we expect the 
logistic scale to work? If the logistic scale works well enough, (2) how do we 
think percent success should be related to it.9 We turn now to these points. 
If only a small amount of selectivity has crept in, the two difficulty 
distributions (on the unknown but most relevant scale) will be fairly similar. In 
this case, a way of attaching numbers to difficulties that makes one distribution 
reasonably closely logistic is likely to do the same for the other also. Con- 
versely, if there is a great difference in difficulty for the populations, given the 
different treatments, so great that the overlap is small, the results of making 
each apparently exactly logistic can conflict only in this short overlap, so we can 
again make both nearly enough logistic on a single scale. If there is any trouble, 
it will arise, between these two extremes, for cases combining largish difference 
with largish overlap. 
The question of relation of response to numerical difficulty is taken up 
below. 
Internal Categories and CG'S 
Saying at least something about the center of gravity (CG) of an internal 
category is easy and is not heavily dependent on our choice of standard 
distribution. Let us think for a bit about categories 30% wide. If such a 
category extends from 35% to 65% on a scale of qualitative difficulty, we will 
have little hesitation in putting its CG at its midpoint, especially since this will 
be exact for every symmetrical distribution of quantitative difficulty. 
If the category extends from 40% to 70%, or from 50% to 80%, we will 
have little doubt that the CG is toward the 40%-point or 50%-point end from 
the midpoint. This has to happen for every symmetrical single-peaked distribu- 
tion. Thus the quality of location of the CGleft of mid, close to mid, or right 
of midis very clear. 
Trial of a variety of alternative distribution shapesGaussian, logistic, 
even Cauchyshows that the numerical location of the CG is rather closely the 
same for internal categories, so that any one of these will do and we might as 
well use the easiest. 
Now we investigate the second question. Estimating the rate at which 
percent success varies with CG location is neither so easy nor so crucial as 
locating the CG is. Suppose only that our correction, for a given category, is 
always in the right direction and between 0.5 and 1.5 of the right size. Then, 
after correction, 
[remaining errorl < �[initial error ! 
and we have made real progress. 
11G: Broad categories/Exhibit 16(A) 251 
Let us plan to adjust to category midpoints. To get the direction right, we 
need only be right about the quality of location of the CG and the direction of 
change of percent success with difficulty. These are just exactly the simplest 
things to deal with. We can get them right almost all the time. Accordingly, we 
do not need high precision in either numerical location of CG or slope of 
percent success against quantitative difficulty. So let us look at the arithmetic. 
For the example given in Exhibit 8, when we have four categories and two 
treatments, Exhibit 16 shows the calculations leading to the centers of gravity 
of the segments. They are calculated using our usual formula with the help of 
the table of Exhibit 11 of Section 5F. 
Exhibit 16 of Chapter 11 
Calculations of centers of gravity for the grades of Exhibit 8 
A} Calculations of the Location of Centers of Gravity 
Standard , Value** [ i Adjust 
center ! of Translated for 
Grade I I Cut II r  I o, gravity I Diff. I log (p/(1- p)) (extremes) 
0% 0 Treatment I 
Very easy .4 -1,682 -1.276 -, 134 
40% -,6730 2.114 -,406 
Semieasy .4 .432 (.490) -,058 
80% -.5004 1,581 1,386 
Sam iha rd ,15 2.013 (2.165) -. 152 
95% -,1985 1,957 2.944 
Very hard ,05 3,970 1.026 -.125 
100% 0 
Treatment II 
0% 0 
Very easy .02 -4.900 -1,008 +.134 
2% -,0980 2,061 -3.893 
Semieasy .08 -2.839 (-3.045) +.206 
10% -,3251 2.143 -2,197 
Semihard .50 -.696 (-.896) +,200 
60% -,6730 2,378 .405 
Very hard .40 1.682 1.276 +.125 
100% 0 
Median 
Very easy -1.142 + .134 
Semieasy 
Semihard 
Very hard 1.151 � .125 
* (p) = p Iogop + (1 - p)logo (1 - p). 
** Values in ( ) are midway between adjacent values of log (p/(1 - p)). 
*** For extreme categories: values of standard center of gravity MINUS median of same (across 
treatments). For internal categories: values of std. CG MINUS midvalue (in parenthesis). 
252 Exhibit 16(B)/11: Standardizing for comparison 
As an example, let us run across the Semieasy line for Treatment I. That 
80% of the area is to one side of the cut is determined from the middle panel 
of Exhibit 8. Exhibit 11 of this chapter gives -0.5004 for 80%. We need the 
difference, -0.5004- (-0.6730), divided by 0.4 to get the center of gravity 
0.432, as referred to the standard logistic. The column of differences comes in 
handy later in getting the slopes. 
From the bottom panel of Exhibit 8, we copy the rates of success onto 
Panel B of Exhibit 16. Then we can plot success rates against difficulty as 
measured by centers of gravity, as is done in Exhibit 17 for Treatment I. The 
figure is of some help because it reminds us which chord we are interpolating 
on and in which direction. 
For Treatment I and the Semieasy category, we have these standard 
difficulties: 
One end: -0.406 
CG: 0.432 mid 0.490 adjust by 0.058 
Other end: 1.386 
where 0.490 = �(-0.406 + 1.386), and the direction of +0.058 is very clear. 
Exhibit 16 of Chapter 11 (continued) 
B} Percentage of Success by Treatment 
Rate of success and slopes 
Adjusted 
Treatment percentage 
Pooled 
percentages I Categoryl I I I II I I Diff. I I I I I! i Diff. 
21% Very easy 90% 100% 10% 87.5 101.6 14.1' 
Slope -18.92 -12.13 
24% Semi-easy 50% 75% 25% 49.0 77.4 28.4 
Slope -16.89 -11.67 
32.5% Semi-hard 23.3% 50% 26.7% 20.7 52.3 31.6 
Slope -6.80 -9.46 
22.5% Very hard 10% 27.5% 17.5% 9.2 28.7 19.5 
Weighted 20.7 Weighted 24.4 
average average 
*Note that only 2% of those receiving Treatment I fall in this line. 
..;'.. 11G: Broad categories/Exhibit 17 253 
?..:Since the CG of the Semihard category falls at 2.013, the adjustment is by 
0.058 
= 0.0367 
. 2.013 - 0.432 
::'.of the 50%- 23.3% = 26.7% difference in success. This adjustment gives 
(Treatment I) success at Semieasy midpoint - 50% -0.98% = 49.02%, re- 
c�rded in Panel B of Exhibit 16 and shown by the circle in Exhibit 17. For 
Treatment II, a similar adjustment gives 75% + 2.39% = 77.39%. 
We can write the first adjustment as: 
(+0.058 of standard difficulty)(16.9% per unit of standard difficulty), 
where 
50% - 23.3% 
16.9% = 
2.013 - 0.432' 
the slope of the dotted line in Exhibit 17. Here the +0.058 would change very 
little for different assumed distributions which matter over only this one 
interval so far as +0.058 goes, while the 16.9 would change somewhat, not 
Exhibit 17 of Chapter 11 
Plot of rates of success for Treatment i against centers of gravity measuring 
difficulty. 
Vertical bars indicate the midpoints of the internal intervals, but medians (overthe 
two treatments) of the standard CG for external treatments. Circles show the 
adjusted values of the success rate. 
Ie of 
Ioo - 
254 /11: Standardizing for comparison 
greatly, for different assumed distributionsmwhich now matter over only two 
categories (here Semieasy and Semihard). We would be surprised by anything 
that deviated as much as either 0.05 and 12%, giving a 0.6% adjustment, or 
0.065 and 20%, giving a 1.3% adjustment. An adjustment of 0.98% is not large, 
but it will help us a lot. That is what we want from a procedure for adjusting 
for broad groups. 
We are attracted to this type of adjustment for every internal category 
because it has no need for approximate equality of spread for the distribution 
of standard difficulties for different treatments--and because it requires reason- 
able behavior only within one category, or within two adjacent ones. 
Because the external categories present a slightly different situation (to be 
discussed subsequently), we cannot rely much on the slope between the 
internal and external categories to calculate the adjusted percentage for the 
internal group, in this case, the Semihard group. Instead, it is safer to use the 
slope between the internal groups to adjust internal percentages. This slope is 
also pictured in Exhibit 17. 
External Categories 
External categories may still give us difficulty, however. We are not very well 
tied down when we look at only one cut. The best we can hope to do, usually, 
is to work with two cuts by concentrating on two categories, the extreme 
category and the adjacent category. 
Things are a little better than one might fear, however. The distance of the 
CG of an extreme category from the cut that defines that category behaves in a 
way that is often quite helpful, as Exhibit 18 suggests. We see that, for extreme 
categories that contain no more than 10% of the cases, the CG is just about 
1.0 unit outside the defining cut. 
Thus, if the spreads of the tails of the distributions of quantitative 
difficulties are the same for various treatments, and all extreme categories are 
small, all CG's will be about 1.0 unit from the defining cuts, and cannot differ 
much from one another, so that the required adjustment will be small. 
This is a special property of distributions, like the logistic, that have nearly 
exponential tails. This sort of "neutral" behavior, leading to small adjustments 
when all extreme categories are small, seems a safe choice when as is almost 
always true we are uncertain about exact shapes of distribution of quantita- 
tive difficulty. 
Sometimes, as in our standard example, the extreme categories are not all 
small. In these cases we can apply either of the methods used above in the 
two-category case the case where all categories are extreme. We can use the 
less robust approach and find adjusted percents of success for the extreme 
categories working from the nearest cut, or we can, rather more robustly, 
interpolate to that cut. (It is sometimes handy to give adjusted percents of 
success for every cut and for the midpoint of every internal category, although 
Broad categories/Exhibit 18 255 
we shall not present them.) We notice that we got an adjusted value of over 
100% in one cell. 
Finally, we need a standard set of weights. We chose the pooled counts in 
the groups for the two treatments, and they appear in the leftmost column of 
Panel B of Exhibit 16. Using these weights, we compute the weighted average 
of the differences in success rates for the raw percentages and the adjusted 
ones. There is a difference of 20.7 in the raw and of 24.4 in the adjusted. This 
is a smaller adjustment than we got for the two-group case, but it is still 
substantial. Exhibit 19 summarizes the differences (Treatment II minus Treat- 
ment I) in the success rates already calculated for the two- and four-group 
examples, as well as the one-group example, the example that ignores the 
classification into easy and hard and yields only the crude success rate. The 
group adjusted weighted average entry in the two-group example was discussed 
in Section 11F in the paragraph on standard population. 
Direct standardization. The sort of adjustment we have just been making 
corresponds to direct standardization, in that we have tried to calculate a 
success rate appropriate to each treatment (for each category) and have then 
Exhibit 18 of Chapter 11 
Separation of CG from defining cut (in standard difficulty) for extreme categories 
Content of 
extreme Separation of 
category CG from cut 
99% 4.652 
98% 3.991 
95% 3.153 
90% 2.557 
80% 2.012 
60% 1.627 
40% 1.277 
20% 1.116 
10% 1.054 
5% 1.026 
2% 1.008 
1% 1.005 
256 Exhibit 19/11: Standardizing for comparison 
looked at differences and, finally, combined the differences with weights. Had 
we combined the adjusted success rates with weights and then taken differ- 
ences, an arithmetically equivalent procedure, we would have been proceeding 
strictly parallel to direct adjustment, except for the allowance for category 
breadth. 
Summary: Standardizing for Comparison 
Responses expressed as rates or proportions can often be more properly 
compared after standardization for some background variable. 
We can directly standardize rates or proportions for background variables. 
We can calculate, using various compromises between reduced effort and 
quality of approximation, estimated standard errors for such directly standard- 
ized rates. 
Ill-determined rates--perhaps in only one cell can cause trouble for 
direct standardization and we need (i) a warning bell when this is likely to 
occur, and (ii) a method of standardization that avoids this difficulty. 
Exhibit 19 of Chapter 11 
Differences (Treatment I! minus Treatment I) in percentage of success. 
("Standardized only" here means that inequalities in difficulty among Easy 
groups ancl among Hard groups have not been taken into account; "Broad-group 
adjusted" means that they have been.) 
One group example: 
44% - 60% = -16% 
Two-group example 
Stand'ized Broad-group 
only adjusted 
Average 15% 31.1% 
Weighted average 15.5% 31.9% 
Four-group example 
Stand'ized Broad-group 
only adjusted 
Average 19.8% 23.4% 
Weighted average 20.7% 24.4% 
References 257 
Standard error calculations ..perhaps overall, but better in terms of the 
:1iargest contributions from one or a few cellswarn of the presence of ill- 
determined rates and the need for a different approach. 
Indirect standardization is available whenever it is more appropriate than 
direct standardization. 
Indirect standardization offers advantages and has a reasonable logic and 
straightforward calculations. 
Standardization for a background variable given in broad catetories is never 
enough to eliminate all the biases associated with that background variable, 
and often requires supplementation. 
To adjust further the results of standardization for broad categories, we 
can use one technique for end categories (hence only this technique is needed 
when but two categories are used for the background variable) and another for 
intermediate categories. 
The logistic distribution offers a convenient way to assign centroids to 
broad categories, a way that generally reflects the true differences in center of 
gravity for the parts of the populations (for specific treatments) to be com- 
pared. 
References 
Fleiss, J. L. (1973). Statistical Methods for Rates and Proportions. New York: 
Wiley and Sons. Chapter 13. 
Freeman, M. F., and J. W. Tukey (1950). "Transformations related to the 
angular and the square root." Ann. Math. Stat., 21, 607-611. 
Sutherland, M., P. Holland, and S. E. Fienberg (1974). "Combining Bayes and 
frequency approaches to estimate a multinomial parameter." In S. E. Fienberg 
and A. Zellner (Eds.), Studies in Bayesian Economics and Statistics. Amster- 
dam: North-Holland; p. 585-6 I7. Or see Y. M. M. Bishop, S. E. Fienberg, and 
P. Holland (1975). Discrete Multivariate Analysis. Cambridge, Mass.: MIT 
Press; pp. 429-433. 
258 Index for Chapter 12 
Introduction 259 
12A The two meanings of regression 262 
12B Purposes of regression 268 
12C Graphical fitting by stages 27 
12D Collinearity 280 
12E Linear dependence, exact and approximate 28 
12F Keeping out what is imprecisely measured 
regression as exclusion 287 
'12G Which straight line? (Optional) 293 
12H Using subsamples 295 
Summary: Regression 295 
References 296 
Chapter 12 / Regression 
for Fitting 
Introduction 
The ABC's of a subject are supposed to be simple, but the ACD's of the 
relationship between two or more variables the meanings of Association, 
Causation, and Dependence, and especially the distinctions among them--are 
complex. Therefore, after beginning with explanations, distinctions, and defini- 
tions, we illustrate them by a variety of examples, some where actual numbers 
are available, and some where we all know enough to be confident about what 
would be found if the numbers were gathered. 
French dictation test. Let us consider a test of children's ability to write French 
text correctly from dictation. For simplicity we may assume that the spoken 
French is standardized by using a tape recorder, that the scoring has been made 
uniform, and that the scorers can decipher all the handwritings involved. Let y 
be the score on this dictation test, and let x be the weight of the child. What is 
the relation of y and x? 
In addition to sounding a bit facetious, with nothing more said, this 
question is almost meaningless. 
Widely varying ages. Are we considering children over a substantial range of 
ages, say 5 to 15, or are we considering only children of age exactly 15 years, 
plus or minus a few weeks? If all ages are mixed together, heavier children will 
in general be older and thus, at least where French is taught or spoken, more 
likely to do well. For such a mixed group there is likely to be a strong positive 
relation between weight x and dictation score y. 
Nearly constant age. If all children are 15 years old, plus or minus a few 
weeks, there are other questions. If nothing else, there is the difference 
between girls and boys, which is likely to show the lighter children performing 
better (since, at this age, girls are better in languages). 
A mix. But what if we mix 15-year-olds from different countries where 
different amounts of French are taught before age 15, for example, in France, 
Holland, and the U.S.A? If the French are lighter than the Dutch who, in turn, 
are lighter than the Americans, we would get a strong negative relation 
between weight and French dictation. 
These illustrations show us that to discuss the relation of x and y we need 
to specify the circumstances perhaps even a specific population of people or 
259 
260 /12: Regression for fitting 
instances that would be sampled. With this need in. mind, we can turn to some 
concepts and their definitions. 
Some Statistical Concepts 
Association. The weakest concept is association--do the values of x and 
seem to be paired in a somewhat related way in the population (presence o 
association), or do they seem to be paired in a completely unrelated way (lack 
of association)? We have discussed a few ways that weight and French dictation 
co.uld appear associated, s. ometimes .positively with y increasing as x increases 
(&fierent ages) and sometimes negatively with y decreasing as x increases (girls 
vs. boys, children from three different countries mixed). 
Independence. To illustrate independence, if we had a population of 15-year- 
old girls, all of whom had been equally exposed to the teaching of French and 
all of whom came from homes where their food had been equally nourishing, 
we might have a population where weight and score on French dictation were 
not associated, where their numbers seemed to behave independently. To get 
this to happen exactly rather than approximately, using the full mathematical 
idea of statistical independence might be hard. (Strictly, X and Y are indepen- 
dent random variables if and only if 
Pr(X -< a, Y < b) = Pr(X -< a)Pr(Y < b) 
for all a and b; and similar definitions hold for discrete unordered variables.) 
When, more generally, we find an association between weight and French 
dictation, do we believe that increased weight "causes" French dictation to be 
better or worse (no matter which direction the relation may be).9 Not if we 
understand the situation as we have just described it. We would be willing to 
accept amount of training, native ability, sex, and amount of French spoken at 
home as possible causes, perhaps even age, but we would be unlikely to accept 
weight as causing better or worse scores at French dictation. Why? 
Causation. Two or three sorts of ideas usually are required to support the 
notion of "cause": 
1. Consistency. 
o that when other things are equal in the population we examine, the 
relation between x and y is consistent across populations in direction . 
perhaps even in amount. 
2. Responsiveness. 
othat if we can intervene and change x for some individuals, their y's will 
respond accordingly. 
Introduction 261 
3. A mechanism. 
othat there is a mechanism, which someone might sometime understand, 
through which the "cause" is related, often step by step, with the "effect" 
the sort of mechanism where, at each step, it would be natural to say "this 
causes that". 
Of course, none of these applies to the relation between weight and perfor- 
mance in French dictation. 
Of these three, only one consistency--can be confirmed by observation 
alone. We might look at a variety of different populations and see whether the 
relationship between x and y is consistent in direction or in direction and 
amount. 
The second.-responsiveness---can be confirmed by experiment, if an ex- 
periment is feasible. If we can intervene and change x, we can see if y then 
changes. Occasionally natural experiments as well as manmade ones offer some 
evidence here, the natural ones being almost always less valuable. (Beware 
especially of "natural experiments" where no change has been made.) 
The third--the mechanism can be confirmed only by constructing a 
detailed mechanism, and supporting the correspondence between each step in 
the mechanism and that in the process under study. 
Causation, though often our major concern, is usually not settled by 
statistical arguments (indeed, is usually not settled at all in social problems), 
though statistics has many inputs that can help. Causation is made more 
plausible by considerations drawn from data, most effectively perhaps when 
three essentials are provided: 
o a clear and consistent association between x and y; 
oa showing that either there are no plausible common causes of x and y or 
that the quantitative relationships between the plausible common causes and 
x and y are inadequate to explain an observed clear and consistent associa- 
tion. (The frequent existence of partial causes, as in the classical nature- 
nurture and heredity-environment arguments, can haunt attempts to make 
such showings. Clearly both heredity and environment cause stature in men 
and women, but how much of the effect to assign to each--or whether such 
an assignment based on a single number makes sense is a difficulty.) An 
example of common-cause creating is that inflation increases both interest 
rates and prices; another example, the Industrial Revolution produced a 
larger population and hence both more scotch whisky imported into New 
York and more ministers there as well! 
oa showing that it is unreasonable for y to cause xo which is often far from 
easy. (In our example, suppose fond parents give more candy to children 
who do well in French and even send those who do poorly to bed without 
their supper!) 
262 /12: Regression for fitting 
In these chapters, then, our concern has to be with the presence, absence, 
or amount of association. We should, and will, leave the question of causation 
aside. We have to be careful with our language and give repeated warnings 
about what we are not doing. 
Dependence. It is not enough to straighten out association and causation. 
We must be clearer about that abused word "dependence" and its relatives. 
When we say "y depends on x", sometimes we intend exclusive dependence, 
meaning that, if x is given, then the value of y follows, usually because the 
existence of a law is implied. In mathematics if y is the area of a circle and r 
the radius, then y = 7rr 2 illustrates this exclusive dependence. (This use is 
found in mathematics courses, and is frequently used in the more exact parts of 
the physical sciences.) 
At other times, "y depends on x" means failure of independence, usually 
in the sense of "other things being equal" as in "the temperature of the hot 
water depends on how far the faucet is from the heater." Clearly it may depend 
on other things such as the setting of the heater and the building's temperature 
as well, to say nothing of whether the long pipe between is on a cold outside 
wall or a warm inside one. These are two quite different ideas of dependence, 
and the use of one word for both has often led to troublesome, if not 
dangerous, confusion. 
Then there are the mathematical usages "dependent variable" and "inde- 
pendent variable". These have been extremely effective in producing confusion 
when dealing with data. We will try to avoid them here entirely. 
12A. The Two Meanings of Regression 
Regression methods bring out relations between variables, especially between 
variables whose relation is imperfect in that we do not have one y for each x. 
To choose physical variables as examples of imperfect relations, we could cite 
the relations in man between weight and height, or between weight and height 
and girth. Such inquiries were common in physical science long before the 
name "regression" emerged from Galton's studies of inheritance in biology. In 
his particular example, Galton noted that tall fathers had tall sons, but not as 
tall on the average as the fathers. Similarly, short fathers had short sons, but 
not as short as the fathers. These tendencies of the average characteristics of 
selected groups in the next generation to move toward the average of the 
population, rather than reproduce the averages of their parents, Galton called 
regression regression toward the mean. We mention this bit of history be- 
cause without it the name regression is rather a puzzle. 
The ]irst meaning o regression: column (local) averages. We study the more 
general ideas of regression and correlation. What is regression? To begin, 
suppose that we have two variables, illustrated by height x and weight y for a 
large population of men. Then for each small interval of x (say, of length an 
inch or a centimeter), we have a distribution of weights y. We could compute a 
summary of these weights for that interval. The summary might be the 
12A: The two meanings of regression/Exhibit I 263 
arithmetic mean, the median, or even the geometric mean. Imagine the chosen 
summary weight being computed for each successive one-inch interval from, 
say, 5' 2" to 6' 4". Then the points (xi, ]i), where x, is the center of the ith height 
interval and ]i is the average weight for that interval, will likely fall close to a 
curve that could summarize them, po.ssibly close to a straight l!ne. Such a 
smooth curve approximates the regression curve called the regression of y on 
x. We give a more mathematical description later. 
Example. Age at notable contribution (y) versus age at death (x). Lehman 
(1953) has explored the distribution of ages when people make notable 
contributions for ten fields of endeavor. To avoid exaggerating the effects of 
youth on creativity, since early death precludes further contributions, he 
grouped his data according to age at death of the person making a contribu- 
tion. Exhibit 1 gives, for each class interval of age of death, the distribution of 
age of making contributions. It parallels the height-weight description above 
with age of death being x, held approximately fixed, and y being the age at 
contribution. 
Exhibit 1 of Chapter 12 
Percentages of notable contributions produced in each decade of life, among 980 
individuals who died at various ages. Total percentage of notable contributions 
for each longevity group is 100. !n all, 1,540 notable contributions. 
[ x = Age at time of death I 
Age at which 
contribution made I Under 70- 
.11 ii ii 
_411 
Under 20 5 1 2 
20-29 32 23 17 8 15 10 12 8 
30-39 50 39 32 38 36 28 32 29 
40-49 14 28 27 28 28 27 28 26 
50-59 9 20 16 13 20 15 22 
60-69 4 10 6 10 9 9 
70-79 4 3 3 
80-89 1 2 
Totals* 101 99 101 100 100 99 100 99 
Medians** 32.7 36.8 40.2 41.4 36.4 44.3 42.1 44.8 
* Totals may not add to 100 because of rounding. 
** Interpolated linearly. Thus the died-under-50 category, where 37% contributed before age 30 
and 87% before age 40, interpolates to 50%, the median, by age 32.7. 
s) SOURCE 
Lehman, H. C. (1953). Age and Achievement, Princeton U.niversity Press; p.. 317. Copyright 
1953 by the American Philosophical Society. Reprinted by permission of Princeton Umversity Press. 
264 /12: Regression for fitting 
Once we have such a table, we want to boil it down. Why? To clarify or 
accent the relation between the specific values of one variable and the 
corresponding values of the other. A drastic summary might note that the 
modal age-interval for contribution for every death interval provided in the 
table is 30-39. These seem to be highly productive years. 
If we want an average, we might compute the mean or median for each 
column. Then to get a regression we might plot these averages against the 
midpoints of the class intervals for age at death. (It is not immediately clear 
what age at death to give to the died-under-50 class or the lived-past-85 class.) 
If we use the median for age at contribution, we will thereby evade the 
problem of what age to assign to contributions under age 20. 
The medians are shown at the bottoms of the columns of Exhibit 1. Then 
Exhibit 2 shows a plot of the points, together with a fitted line and an equation 
for it. Apparently each additional 5 years of life adds about 1 year (5(0.19)) 
to the median age at contribution for people dying between ages 50 and 85. The 
two endpoints have arbitrarily been given ages at death of 45 and 87�, 
respectively. 
Formal delinition o[ first meaning o[ regression. In the mathematical case 
where for each value of x there is a distribution of Y, with density f(y I x) (read 
f of y given x), the regression of y on x is defined as follows. For each x, 
compute the mean value of Y for that x, namely 
y(x) = f, yf(y [ x)dy. 
Then the function defined by the set of ordered pairs (x, y(x)) is called the 
regression of y on x. We have followed the custom, in this case, of choosing the 
mean as the average for defining the curve (we could have used the median, for 
example). 
Example. Mean. Suppose that the density function is 
�(ylx) = 2y 0 < < < 1 
X 2 , y--x-- . 
For a given value of x, 0 < x < 1, the mean is 
= y. y ay 
And so  is a linear function of x, passing through the origin. 
Let us look at the median using the same conditional distribution of y. 
12A: The two meanings of regression/Exhibit 2 265 
Example. Median. The median ymea(X) is obtained by finding what value splits 
the density in half. Thus we require 
2 [ Ymcd(X) 1 
y dy = -, 
2 [ym(X)T = ! 
x 2 2 2' 
This yields 
1 
and so in this example, using the median as the average also gives a straight 
line through the origin, but with a slightly different slope, about 0.71 instead of 
O.67. 
Sometimes for each y there is also a distribution of X, with density 
g(x I y), a. nd then we can define (y) =  xg(x [y)dx, and the regression of X 
on y is gven by the set of ordered pairs ((y), y). 
Exhibit 2 of Chapter 12 
Regression of median age of contribution on age at death; line is based on 
middle 6 points equally weighted; x values of two endpoints arbitrarily assigned 
to 45 and 87�, respectively. 
hiedian ate at 
contribution 
45 
$5- 
Aie at deatl Cmiadle of class interval) 
30 I I I I I I I I   
45' 5'0 5' 60 $5 '/0 1 80 85 
266 /12: Regression for fitting 
In practical work, we ordinarily do not have continuous populations with 
known functional forms. But the data may be very extensive. When they are, 
we can break one of the variables into small intervals and compute averages for 
each interval and, without severe assumptions about the shape of the curve, 
essentially get a regression curve, as we did in the age-of-innovation example. 
What the regression curve does is give a grand summary for the averages 
for the distributions corresponding to the set of x's. We could go further and 
compute several different regression curves corresponding to the various per- 
centage points of the distributions and thus get a more complete picture of the 
set. Ordinarily this is not done, and so regression often gives a rather incom- 
plete picture. Just as the mean gives an incomplete picture of a single 
distribution, so the regression curve gives a corresponding incomplete picture 
for a set of distributions. This was the first meaning of regression. 
When the data are more sparse, we may find that sampling variation 
makes it impractical to get a reliable regression curve in the simple averaging 
manner described. 
The second meaning o regression: fitting a [unction. One device occasionally 
used is to introduce a smoothing procedure, applying it either to the column 
summaries or to the original y's (once these have been ordered in terms of 
increasing x). For an introduction to such methods, see Sections 3F and 3G, 
and EDA, Chapters 7 and 16. Using that approach gives a curve that may be 
smooth, but not necessarily of any simple functional form. Sometimes such a 
result is good enough in itself; sometimes it serves to suggest a simple 
functional form that we can then fit. 
By force when the data is still sparser, by suggestion when we have looked 
at and thought about smoothed results, by lack of thought in very many other 
circumstances, we usually come to a second approach. We assume a shape for 
the curvelinear, quadratic, logarithmic, or whateverand fit the curve by 
some statistical method, often least-squares. 
In doing this, we do not even pretend that the resulting curve has the 
shape of the regression curve that would arise if we had unlimited dataonly 
that we have an approximation. 
Since conditions leading to this second approach to regressionsforced 
fitting of a functional form arise so often, we tend to forget the first and more 
fundamental meaning discussed above--curve connecting averages of column 
distributions. The second approach considerably extends the usefulness of 
regression methods, for we often have comparatively modest amounts of data, 
rather than the hundreds or thousands of (x, y) pairs often needed to make 
narrow columns work well, or the rather smaller number required for effective 
smoothing. 
We ordinarily choose for the curve a form with relatively few parameters. 
And we have to choose how to fit it. (This can be done by choosing some 
criterion for fittingsleast-squares, least first powers, least pth powers 
whichever we can stand and afford. Or it can be done by describing how to 
12A: The two meanings of regression 267 
recognize a fit in terms of the residuals that it leaves. It can also be done by 
describing the steps we are to take in constructing a fit or by some combina- 
tion of two or three of these approaches.) 
By fitting a simple shape, one may be able to detect the need for 
complications. For example, when we fit to data the horizontal line y = c 
where c is a constant, we may at once see the need for some slope to the line. 
Or, having fitted a sloping straight line, we may then see the need for 
curvature. 
Such insights may come more easily by looking at plots of residuals 
(y - fitted y). They may also come as a result of smoothing such plots, either 
by formal methods or by passing a smooth curve by eye through the scatter of 
points. 
e$idual Residual 
More Than One Carrier 
Up to this point, we have mainly, but not entirely, emphasized regressing one y 
variable (response) against one x variable, or as we shall call it, a carrier. But 
the whole discussion can be extended to the case of more than one carrier 
(other than the constant). The move to two carriers (beyond the constant) is an 
important step, and it has the advantage that the geometry can be kept to three 
dimensions, leaving us feeling that we can have a strong intuitive grip on the 
situation. Once we are comfortable with two nonconstant carriers, the next 
important move is to a large but fixed number of carriers. A third move is to 
treat several, but an undecided number. Both choosing the set of carriers from 
which a final subset is to be drawn and choosing that subset can be most 
disconcerting processes. 
268 /12: Regression for fitting 
12B. Purposes of Regression 
A simple, important use of regression is: 
1. to get a summary, a use we illustrated in Section 12A with the data on age 
at contribution. 
That same example illustrated another use, 
2. to set aside the effect of a variable that might confuse the issue there, age 
at death. 
3. Contributions to attempts at causal analysis are a popular use for regres- 
sion, and our example illustrated that as well. Although age alone does not 
cause contributions, death nearly always stops them. 
4. Sometimes, as a corollary to item 3, we want to measure the size of the 
effect through a regression coefficient, as we did in the age-at-contribution 
example. As we mention in Chapter 13, this use is fraught with difficulties 
when there are multiple causes and when various noncausal variables are 
associated with other causal ones. 
5. An extreme instance of the causal approach occurs when we use it to try to 
discover a mathematical or empirical law. 
A more common use for regression is: 
6. for prediction, as when we use information from several weather stations 
to predict the probability of rain at a particular area several hours hence, 
or use information about composition of the population and measures of 
pollution in various cities to forecast the death rate from a class of diseases 
believed caused by such pollution. 
Such predictive uses are widespread and diversified. Sometimes, indeed, a 
predictive use may be intended to be causative as in item 4 if we reduce a 
particular sort of pollution, we estimate the effect on the death rate (a possibly 
misleading use); or the regression equation may be used to estimate the 
anticipated death rate in a new place and thus to see whether that place is 
better off than "average", as in item 2, an "other things being equal" 
prediction. 
We can try to fit 
y = f(x), where f is empirically determined, 
y = bx, 
y = a+bx, 
y = a + bx + 
y = a + bx + ct, 
and their generalizations to more carriers (more coefficients, more variables, or 
12B: Purposes of regression 269 
both) for such purposes. The problems we need to solve can be different in the 
different cases. 
Regression as exclusion. Sometimes we make a fit, perhaps by a quadratic, say 
y = a + bx + cx 2 
with a general purpose, 
7. of getting x "out of the way." We may know that x affects y, perhaps 
substantially, and be curious as to whether t affects y, too. (Here we use 
"affect" as a shorthand for "is associated with, possibly, but not certainly, 
through a causal mechanism." We will also use the "effects of x" in a 
similar way.) 
For example, we might like to remove the effects of years of education from 
measures of performance on knowledge of current events, and then see what is 
left to be explained by frequency of media readership. 
One approach would take the effects of x out of y and see if what remains 
is associated with t. To try to do this, we might first fit. 
y = a + bx + cx 2 
and then continue with y., where 
Y.x = Y - a- bx- cx ' 
is a residual after fitting x, a quantity freer of the effects of x than y was. 
In other circumstances, we can estimate, for each x, the distribution of y 
given x and then replace y by a measure of where y stands in the distribution 
of y given x, such as the number of standard deviations above the mean. This 
is, of course, easier when x takes on only a few values, as when x is sex (male 
or female), or a few class intervals, as in our age-at-contribution example. In 
this class of problem, we compare y with what we would predict knowing x, or 
knowing the values of several variables (x's). 
Those who are primarily concerned with getting x out of the way need not 
interpret or give meaning to the coefficients, formulas, and tables they use to 
do this. They should look hard at what carriers are used to see that they are 
reasonable--not an easy task--but the coefficients need not have meaning in 
themselves. If an investigation shows that the score, y, on French dictation is 
related to age x by 
y - 50 + 7(x - 10) - 0.5(x - 10) 2, 
we do not expect the coefficients 50 (of 1), 7 (of x- 10), and -0.5 (of 
(X- 10) 2) to be the same in other investigations. Others may choose other 
curves. If we want to brush x out of the way while carrying this investigation 
further, we can use 
y - 50 - 7(x - 10) + 0.5(x - 10) 2 
without prejudice to what we might do some other time. 
270 /12: Regression for fitting 
More than this, we would like a good fit, without regard to whether it is 
the best kind of fit. In this example, some other function of age might be better 
than a polynomial. Thus, if French is not taught below age 7, the time 
dependence of. y on x must be very close to y = 0 (if 0 means no skill) for all 
x < 7, something that a polynomml finds very hard to approximate. But if our 
study includes no children under 9, the quadratic may be quite good enough. 
And in general Such a fit is a practical tool. A simple expression is a fairly 
good description. It may also be the right way to express a relationship. 
In the next chapter, we will look at a question of intermediate difficulty: If 
we accept the form of dependence, what ought we to make of the coefficients? 
The purpose there getting coefficients that can be interpreted--is quite differ- 
ent from the purposes in the rest of the present chapter: getting a good fit, 
either for getting the x's out of the way or for predicting y from the x's. 
These last two purposes sound almost like versions of the same thing. 
They are similar, but some differences can matter. 
Let us look a little harder at a concrete prediction problem. Consider a 
large state university, so large that for most applicants the decision on admis- 
sion has to depend on a formula that forecasts degree of success. How should 
the variables and the coefficients in such a formula be chosen? Clearly the 
prediction needs to be at least partly successful. 
Should father's occupation be an x? If it is, what should its coefficient be? 
If it isn't available, then there is no choice. If it is, moral and political 
considerations may control whether it is used. (At a smaller institution, one 
that prides itself on a balanced student body, x may be used for quite other 
reasons than prediction of performance.) 
Within the constraints of availability (and cost) and moral and political 
pnnciples, probably some set of x's can be used. We will want to combine 
them into our prediction formula, but we ought not to care much about the 
values of the coefficients. What is important is the fit -what sort of y do we 
predict for these x's? And how close will prediction come to performance? 
Many different formulas will do about equally well. We would like one of 
them--it doesn't matter much which. (If we were trying to understand what 
mattered in college success, we would have to take a very different attitude.) 
A word more about the political side. When the formula is chosen, since 
many formulas will be about equally good at forecasting, it would be well to 
note whether the chosen one has any bad quirks that will lead to hilarious 
newspaper columns or to the appearance of injustice for reasons that might be 
anticipated from its form or values. One must assume that the formula will 
become public property and will be criticized, at best, by neutral and, more 
likely, by unsympathetic observers. Consequently, if it has features such as that 
the outcome on one true-false spelling item is worth more than a B in two 
years' study of the calculus, it is going to be hard to explain that it wouldn't 
matter much which formula was used, however true that might be. Conse- 
quently, among the many formulas that might be used, it would be well to 
(12B) 12C: Graphical fitting by stages 271 
choose one that is reasonably presentable and defensible if other people choose 
to make interpretations. 
In this chapter, we treat the easier problems of finding a good fit: 
something usable either for getting x out of the way or for predicting y from x, 
but something whose structure and coefficients are not or at least not yet--to 
be taken seriously. 
12C. Graphical Fitting by Stages 
Let us suppose that we wish to fit y with x and x2 in the form /3x +/32x2. 
Although we can fit 
y = lXl - /2X2 
directly, we may improve our understanding by a more sequential approach. In 
this second approach we represent the fit not in terms of x and x2, but in terms 
of x andx2-adjusted-for-xl, which we label x;. Specifically, we construct 
X2;1  X2- d2;IXl 
where d2; is chosen to free x2 of Xl, at least approximately. Specifically d2; is 
the slope of a line through the origin in the x-vs.-x2 plane, chosen to fit the 
scatter of points well, especially to predict x2 from x. Then x2; is the set of 
residuals of x2 based on the forecast d2;lx.. In this section we do not use least 
squares explicitly to get d2; but elsewhere we may. We are doing for x what 
we often do to free y of x when we have only y and x. 
The overall plan is first to fit y to 7x and get 
y = ,Xl 'JU residual. 
Once this is done we compute residuals 
y;x- y- ;/x, 
:where the subscript ;1 reminds us that we are computing a residual for y and 
'the 1 indicates that we have, in a specific sense, removed the effect of x. 
Second, we use these residuals to fit 
Y = /2x2 + residual. 
::That is, we take the part of y that was free. of xt and relate it to the part of x2 
:::that is free of x. Then we assemble the peces to see that 
:.:':. Y = 3,x + Y; 
::is fitted by 
i'::. 'y1X1 n t- 2X2;1 - 
:::'If we wish, this can be converted to a form that is identically equivalent, where 
i.:.11..::!... y is fitted by/x + /2x2 
::":provided we set 
272 Exhibit 3 / 12: Regression for fitting 
We have illustrated the point that removing the effects of variables can be done 
one variable at a time. 
Here the removals symbolized by the semicolon in the subscript may be 
formal and carefully executed or they may be very informal. In this section, we 
are rather informal, fixing our coefficients graphically, but carefully,-in two 
stages. 
Exhibit 3 gives the U.S.A. data for a simple example where 
y = total derived employment - 65317 (in 1000 jobs) 
x = GNP price deflator - 101.7 (as a %) 
x: = gross national product - 387,698 (in $1000) 
for the years 1947 to 1962. (The subtracted constants 65317, 101.7, and 
Exhibit 3 of Chapter 12 
Employment, price deflator, and gross national product by years. 
Gross 
Price national 
Employment, deflator, product, 
[Year[ � x, x2 
1947 -4,994 - 18.7 - 153,409 
1948 -4,195 -13.2 -128,272 
1949 -5,146 - 13.5 - 129,644 
1950 -4,130 - 12.2 - 103,099 
1951 -2,096 -5.5 -58,723 
1952 -1,678 -3.6 -40,699 
1953 -328 -2.7 -22,313 
1954 -1,556 -1.7 - 24,586 
1955 702 -.5 9,771 
1956 2,540 2.9 31,482 
1957 2,852 6.7 55,071 
1958 1,196 9.1 56,848 
1959 3,338 10.9 95,006 
1960 4,247 12.5 114,903 
1961 4,014 14.0 130,475 
1962 5,234 15.2 167,196 
Note. The precision given is that of the original sources. 
S) SOURCE: 
A. E. Beaton, D. E. Rubin, and J. L. Barone (1976). "1'he acceptability of regression solutions: another 
look at computational accuracy." J. of Amer. Statist. Assoc., 71, 158-168. 
Quoted from J. W. Longley (1967). "An appraisal of least-squares for the electronic computer from the 
point of view of the user." J. of Amer. Statist. Assoc., 62, 819-841. 
12C: Graphical fitting/Exhibit 4 273 
387,698 used in these definitions are, to the accuracy given, the means over 
this sequence of years.) 
Exhibits 4 and 5 show the two steps of fitting by a constant times x. The 
first, found by eye, gives a trial slope of +300. We use a prime to indicate a 
! 
trial value. And so y;, x2;, and so on, are trial residuals. From the first line of 
Exhibit 3 we get 
y' - -4,994 - 300(-18.7) = 616 
A further fit of xx to y = y - 300x, shown in Exhibit 5, then finds a slope of 
5, giving a final slope of 305 and a y; of y - 305x. Then for 1947 
y; = -4,994- 305(-18.7)  710 
from the values in Exhibit 3. (The arithmetic is shown in Exhibit 6.) 
Exhibit 4 of Chapter 12 
First try at regression of y on x,. 
4,000 - 
x 
x 
x 
- ,ooo - 
Iopt = + $00 
-4,000 - 
x 
-,ooo - 
I I I I 1 
-o - IO o Io 
274 Exhibit 5/12: Regression for fitting 
Exhibit 5 of Chapter 12 
Second try at regression of y; on x,. 
! 
J/;t 
z, ooo 
x 
1,500 - 
1,000 - 
x x 
x 
.500- x 
x x 
-500 - 
-I, 000 - x x 
I I I I I ";l 
-o -Io o  20 ' 
12C: Graphical fitting/Exhibit 6 275 
Exhibit 6 of Chapter 12 
First tries and fitted values of regressions (noted at bottom of exhibit), using the 
values from Exhibit 3. 
First 
First tries I I Fitted values I try, 
1947 -18.7 616 33,591 710 1,801 584 
1948 -13.2 -235 3,728 -169 -18,712 1,141 
1949 - 13.5 - 1,096 5,356 - 1,028 - 17,594 204 
1950 -12.2 -470 18,901 -409 -1,839 -280 
1951 -5.5 -446 -3,723 -418 -13,073 497 
1952 -3.6 -598 -4,699 -580 -10,819 177 
1953 -2.7 482 4,687 496 97 489 
1954 - 1.7 - 1,046 -7,586 - 1,038 - 10,476 -305 
1955 -.5 852 14,771 854 13,921 -120 
1956 2.9 1,670 2,482 1,656 7,412 1,137 
1957 6.7 842 -11,929 808 -539 846 
1958 9.1 -1,534 -34,152 -1,580 -18,682 -272 
1959 10.9 68 - 13,994 14 4,536 -304 
1960 12.5 497 - 10,097 434 11,153 -347 
1961 14.0 -186 -9,525 -256 14,275 -1,255 
1962 15.2 674 15,196 598 41,036 -2,275 
Notes. 
�/ = �- 300x,, 
� = �/ - 5x, = �- 305x,, 
x;1 = x2- 10,000x, 
x2; = x� + 1700x = x2- 8300x 
�/2 = �, - 0.07x� 
276 Exhibit 7/12: Regression for fitting 
Exhibits 7 and 8 go on to do a similar thing for x2 on x. This time the 
second term is 17% of the first. (Arithmetic is also shown in Exhibit 6.) Exhibit 
3 shows for 1947 
! 
x2; =-153,409- 10,000x = 33,591, 
and then 
X2;1-- 33,591 + 1,700xt = 1801. 
Exhibit 7 of Chapter 12 
First try at regression of x= on x,. 
_ooooo t 
x 
I00,000 
x 
-1o, ooo 
-i00,000 x 
I I I I " '1 
-00, 000 i0 -I0 0 i0 20 ' 
12C: Graphical fitting/Exhibit 8 277 
We are ready to go ahead with the final step, the regression of y;, on x2;. 
Exhibit 9 has the first step. It is not always easy to judge what would be a 
good regression coefficient; we decided to try 0.07. The next exhibit, Exhibit 
10, shows that this may have been too much, but we decided to stop here, in 
view of the generally horizontal nature of all but two of the sixteen points, 
namely those for the last two years, 1961 and 1962. (Some analysts might have 
come back by a slope of as much as -0.015 or -0.02.) 
Exhibit 8 of Chapter 12 
Second try at regression of x';, against x,. 
! 
40'��0Ix 
30,000 
20,000 x 
I0,000 
x 
o 
x 
-I0,000 x x 
-20,000 
Slope = - 1,700 
-OtOO0 
00050 I I I  ,,. ,, 
-40, _ -{0 0 I0 20 ' 
278 Exhibit 9/12: Regression for fitting 
In summary, then we have found the following roulgh fits: 
y by 305xt, so that y;, = y - 305x; 
x2 by 8300x, so that x2;, = x2 - 8300Xl 
y; by .07x2;, so that y;2 = y;- 0.07X2;, 
where y;12 means residuals of y after removing both xa and x.;, or, more 
briefly, x l and x.. Combining, we find 
Y;2 = Y;1 -- 0.07X2; 
= (y - 305x)- O.07(x- 8300x) 
= y + 276x- O.07x, 
Exhibit 9 of Chapter 12 
First try at the regression of it;, against 
z,000 
x 
_ e = 0,07 
0 
x 
-I,000 7 x 
-2,ozo, ooo o o, ooo 4o, ooo 
12C: Graphical fitting/Exhibit 10 279 
so that our fit to y is 
-276x + O.07x. 
Had we come back by 0.01 in the slope of ' 
y2 on x2; our fit would have 
been 
-199x + 0.06x2, 
showing how much change in coefficients can sometimes be associated with a 
very small adjustment in fit (only one point, that for 1962, is moved more than 
+200, out of a trend amounting to 10,000). 
Exhibit 10 of Chapter 12 
Basis for a second new try at the regression of y;;,. against 
! 
X  
I000 - 
X  
x 
- 1,000 - 
-,ooo - 
_ J I ] I   , 
� :'Z; I 
-0,000 0 10,000 40,000 
280 /12: Regression for fitting 
12D. Collinearity 
Many opportunities for trouble arise in regression problems. One well known 
difficulty goes under the heading of collinearity. In the simplest case, the 
difficulty arises from measuring essentially the same quantity under different 
names, and then trying to use the several measures to get a regression relation 
between all these nearly-equivalent carriers and the response variable. Essen- 
tially, the idea is that we get into trouble when we try to treat one piece of 
information as if it were several pieces. This inevitably leads to arbitrariness 
about the allocation of the weights to be given the several pieces. This situation 
can arise very readily in social science work and in economics, when we have 
many variables that may enter a regression relation and clumps of these 
variables measure much the same thing. We illustrate this in the church- 
membership example at the close of this section. 
Example. Chest and heart measurements. A biostatistician had two measure- 
ments of chest size in boys about 12 years old taken a few months apart, as 
well as a direct measure of heart size. He decided to use each boy's two chest 
measures to develop a regression equation for estimating heart size. His 
equation was something like the following, where H - heart size, and C and 
C2 are the first and second chest measurements, 
H = 10 + 8C + 3C2. (1) 
His equation was based on least squares from one group of boys. He had an 
additional set of data taken on boys of the same age in the same part of the 
country, and when he fitted his equation for the second group, he got 
something like 
H = 10 + 5 C + 6C2. (2) 
Since the two groups of boys were expected to be quite comparable and in both 
groups the number was substantial, he was bothered by the large differences 
between the coefficients of the C's in the two equations. Which equation 
should he believe? 
What is going on here is that, for each boy, C and C2 are nearly identical, 
differing because of slight growth, and possibly differing more because of 
measurement error. Thus C and C2 might well have been averaged, producing 
a single value for chest measurement , leading to a regression equation of 
about 
H-- 10 + 11. (3) 
Note that the coefficient 11 is the sum of the coefficients of C and C., and that 
both Eqs. (1) and (2) had coefficients of C's adding to 11 (the sums of these 
coefficients were approximately, but not exactly, identical in the actual 
example). 
12D: Collinearity 281 
Geometrical clarification. From a geometric point of view, in the three- 
dimensional space of (C, C2, H), the points are strung out almost along a 
straight line. This straight line is being asked to determine a plane. Insofar as 
the points are on the line, this is an impossible task. When the points are close 
to the line, it is an uncertain one; a slight change in the positioning of a point 
can tip the plane just like a seesaw. 
Estimation. Even though the two estimates of H, 
10 + 8C + 3C and 10 + 5C + 6C2, 
were very different in the coefficients of the C's, their estimates of H were, in 
the application, very near one another. From the point of view of the geometric 
explanation, the reason is that any point (C, C2) had to be dose to the 
C = C2 line in the (Cx, C.) plane. That point, projected vertically to the 
regression plane, then, will be close to the original line of points in the 
three-dimensional space for a great variety of planes through the line. All these 
planes will then give much the same fit or prediction for the point. 
All told then, when two reliable, identically distributed measures of the 
same quantity are used as predictors in a multiple regression equation, we find: 
a) the sum of their regression coefficients is relatively stable, and 
b) the prediction of the dependent variable is much the same because of the 
stability mentioned in (a). 
Naturally the very special case we have just treated has implications for 
other problems, or we would not have brought it up. Whenever variables used 
for prediction are highly correlated with one another, or when two or more are 
estimating essentially the same thing, the sizes of the corresponding coefficients 
are likely to be uncertain indicators of the "importance" of the predictor. We 
must therefore anticipate that, at the very least, it is difficult to tell what the 
effect might be on the response variable if we were able to change the value of 
one of the predictor variables. Note that, in these high-correlation situations, 
we cannot change the value of one carrier substantially without changing the 
other at the same time. 
Example. Church membership. If one thinks of number M of individual 
church members as the consequence of a church's activity, using salaries L as a 
measure of labor (actually salaries plus 10% of the value of its parsonages), 
and of the value of its buildings less debt as a measure of capital, K, then an 
economist might wish to relate these by 
M = 10��L-K �K. 
Taking logarithms gives 
log M =/30 +/3,. log L +/3 log K. 
282 Exhibit 1 1 / 12: Regression for fitting 
Exhibit 11 gives the logarithms of M, L, and K for one state from each 
region for the Methodist Episcopal Church, according to the 1936 religious 
census. Before fitting me proposed regression, the plot of log L against log K 
was made, as shown in Exhibit 12. The plot shows a nearly linear relation with 
slope near 1 between log L and log K, giving warning that the chest- 
measurement sort of collinearity has struck again. This means that /3L and 
would be poorly determined by the data. Still, estimating log M can be readily 
done from log L, log K, or a combination. Let us plot log M against the sum of 
log L and log K. 
Exhibit 13 shows the result of the plot. Again the relation is close and 
1 
near-linear with slope close to . 
Exhibit 11 of Chapter 12 
Logarithms of membership, salaries, and capital for one state from each region 
in the Methodist Episcopal Church in 1936. 
I State I I Membershipl I salaries [ [Capitall 
Maine 4.30 5.33 6.20 
New York 5.48 6.47 7.60 
Ohio 5.58 6.34 7.44 
Minnesota 4.87 5.75 6.77 
Delaware 4.41 5.30 6.33 
Kentucky 4.38 5.19 6.13 
Arkansas 3.62 4.49 5.45 
Montana 4.12 5.05 5.93 
Washington 4.61 5.44 6.33 
Median 4.41 5.33 6.33 
7th minus 3rd 0.57 0.56 0.64 
S) SOURCE. 
G. Mosteller, personal communication, drawn from Religious Bodies: 1956, Vol. II, Part 2, Bureau of the 
Census, United States Department of Commerce, U.S. Government Printing Office; pp. 1086-1096. 
12D: Collinearity/Exhibits 12 and 13 283 
Exhibit 12 of Chapter 12 
Plot of logarithm of salaries against logarithm of value of property (less debt) for 
the Methodist Episcopal Church, 1936, in one state from each of nine geographi- 
cal regions. 
logarithm 
7- 
Iolrith  of capel 
,, I I > 
4. 8 7 8 
Exhibit 13 of Chapter 12 
Plot of logarithm of membership against (logarithm of salaries) + (logarithm of 
value of property) for the Methodist Episcopal Church, 1936, in nine states. 
c rnemhip 
1, I I I I 
9 I0 I! IZ. 15 14. 
(' Iilaritf, rn o capital) + (io{!ritlrn of .saladrs) 
284 /12: Regression for fitting 
12E. Linear Dependence, Exact and Approximate 
When we have a set of carriers and plan to use the regression on them for 
prediction, we frequently call the carriers predictors. 
Let us push the collinearity idea further and deal with predictors that are 
functionally related, with no error. In a most extreme situation, two or more 
predictors may be measured with exactly a linear relation between them. This 
could happen if a number of percentages adding to 100% are included as 
separate variables (such as compositions of alloys in metals, or percentages of 
families having different kinds of housing). Or we might have redundancy, for 
example, for variables having to do with a child in a completed family: the 
child's birth order B, the family size N, and number of younger children, Y. 
These yield the exact relation 
B+Y=N. 
Other examples of special interest to social and clinical psychologists are certain 
tests whose parts are scored in such a way as to add up to a constant, as in the 
Allport-Vernon-Lindzey Study of Values. 
How do such dependencies affect the fitting of multiple-regression equa- 
tions? They create difficulties equivalent to those that arise from trying to 
divide by zero in ordinary arithmetic. Let us take the simplest case, where the 
variables x and x2 add to a constant k, x + x2 = k, or where one is a linear 
function of the other, x2 = bx + c. 
Let x and x2 be predictors for the variable y. Then the observed points in 
three dimensions (xi, x2i, yi) are all in the plane perpendicular to the (x, x2) 
plane through the line relating x and x2. This goes to the extreme suggested in 
the chest-heart example. The graphs tell us that we do not have two predictors, 
only one. We can choose a variety of representations for it. For the case where 
x + x2 = k, we could get along with only x or only x2 or any linear function 
of x or of x2. Indeed, we can combine x and x2, provided their coefficients are 
not identical: ax + bx2 + c, a  b, because as x changes so does x2, and by 
the same amount but with the opposite sign. If their coefficients were identical, 
the function would be constant and we would lose the information in both 
predictors. 
12E: Linear dependence, exact and approximate 285 
In the second illustration, we can also use any linear function of xx and x2, 
provided it is not of the form a(x2- bx) + d, because the latter would be a 
constant. 
The only easy remedy, then, for the forms of exact collinearities discussed 
in this subsection is to abandon the attempt to use the two predictors 
separately. Instead, replace them by a single carrier. This will fix the 
arithmetic--and may or may not help us with the prediction problem. 
Which single carrier we use is not determined by anything discussed here. 
Convenience or relations with further carriers might determine the answer for 
regression as prediction. If we are trying for regression as measurement or 
regression as a causal indicator, we are likely to be in deep trouble because 
there the coefficients are usually required to have meaning. 
The simplest polynomial; a nearly linear relation. One form of near collinear- 
ity generated by functional relations arises when we try to fit a regression 
involving a linear function of 1, x, and x2--to fit a quadratic a + bx + cx  to a 
set of data. 
Since x and x ' are closely related, we may have computational difficulties 
leading to the need for many places of accuracy to keep any of the precision we 
actually have. In computing a mean, we are used to the idea of picking a 
central value, obtaining deviations from it, averaging these deviations, and then 
recapturing the mean. Example: Average 13879, 13881, 13864. It is conve- 
nient to choose 13870, say, as an arbitrary value, obtain deviations, 9, 11, -6, 
sum to get 14 and the average 4, and a final average 13874. People who do 
the calculation in their heads usually use some form of this approach. The main 
point is that the original 5-digit numbers led to 1- and 2-digit deviations, and 
the arithmetic was easier. 
In multiple regression, especially with highly correlated variables, a related 
device--similar to choosing a central value can be even more helpful in 
keeping numbers of decimal places down, and is often even more needed. 
Fitting a + bx + cx  is equivalent to fitting 
a* � b*x � c(x - A - Bx) 
for any A and B, where a = a* - Ac and b = b* - Bc, and the coefficient, 
c, of the quadratic term is unchanged. If an x0 were centrally located in the 
data, then (x - Xo? would be small compared with x :. Also 
(g- X0) 2 = g 2-- 2XoX + x 
has the form 
x 2- Bx - A 
with A = -x and B = 2Xo. Thus it could be advantageous to fit 
a* + b*x + c(x - Xo) 2 
or even 
a ** + b* (x - Xo) + c(x - Xo?. 
286 /12: Regression for fitting 
In addition to the point about keeping down the arithmetic, this form tells us 
more about the uncertainty of the estimates of the coefficients. If all x's are 
close to xo, the carrier (x - Xo) 2 is very small in comparison with x - x0 or 1, 
and so, as we next explain, we cannot expect to estimate c with much 
numerical precision. 
As discussed in Chapter 14, the denominator of the variance of  will be 
the sum of the squares of what is left after (x - Xo) 2 has been regressed on 1 
(important) and x - x0 (usually less important) (that is, we fit /o + /l(X - xo) 
to (x - x0) 2 and get the sum of the squared residuals). So we are in somewhat 
more trouble than the smallness of  (x - x0) 2 suggests. This doesn't matter as 
much for our present "good fit" aim, since the values of (x - x0) 2 we substitute 
in are also likely to be small compared to I and x- Xo. (Extrapolation is 
another matter; there parabolic fits are far more dangerous than straight-line 
fits because the contribution from the quadratic term can swamp the rest of the 
fit.) 
Returning to practical arithmetic, we note that it is often important to 
avoid trying to fit 
a + bx + cx 2 
instead of 
a,, + b*(x - Xo) + c(x - Xo) . 
A few "canned" computer programs fit the second form automatically and then 
reconvert and report a, b, and c. If yours does not, users may get badly 
misleading results, unless they have changed to a good set of carriers in 
advance. 
Even for what appear to be long intervals of x, the correlation of x and x 2 
may be considerable. For example, if x is uniformly distributed over some 
interval (0, A), then the correlation between x and x 2 is 0.97 for any choice of 
A. Many would associate this degree of relation with the idea of a practically 
perfect linear relation. For this situation the variance of our coefficients is 
increased by a factor of about 
1 
1 -r: = 12 
over what it would be if we sought to fit either x or x 2 alone. And so again we 
find ourselves close to an exact linear relation. 
A t (/I, A ) 
o A 
(12E) 12F: Regression as exclusion 287 
In spite of all these troubles, it may nevertheless be useful to detect and 
remove a quadratic term from a scatter diagram. 
As one would expect, the computational difficulties and the uncertainty in 
the coefficients become even more critical when fitting a cubic or higher-degree 
polynomial to any fund of data. 
Near-linear and even exact dependencies are also common between vari- 
ables with different-seeming names, although we have used x and x 2 as an easy 
example. The examples at the beginning of the section illustrate this. 
12F. Keeping Out What is Imprecisely Measured Regression as Exclusion 
(Before reading this section, the reader may wish to read the introduction and 
the first section of Chapter 14.) 
To do a fair job of excluding the effect of a variable (the seventh purpose 
of regression, mentioned in Section 12B) that cannot be assessed directly, but 
is related to something we can measure, requires additional data and tech- 
niques. An example is maturity in children, which is related to chronological age. 
Essentially, we need at least two measures of such a latent variable to get a 
handle on its effect. 
The case o[ no deviations. Let us review, first, our usual approach. When x is 
measured precisely and we want to take its linear contribution out of y, we can 
do this by looking at either the residual 
y - bx 
or the residual 
y-a-bx, 
where a + bx is as good a linear fit to y as we can get. So long as x measures 
exactly what we want to eliminate, keeping x out is exactly like predicting 
from x. 
Because the estimate b will not exactly coincide with the true /3, our 
elimination of x's linear contribution will not be perfect. We can write 
y - bx = (y - t3x) - (b - t3)x, 
or 
y - a - bx = (y - a -/3x)- (a- a)- (b- /3)x, 
where )-/3x (or y-a- /3x) is what we wish we had. The deviation 
-(b- x is unbiased on the average because (b-/3)x behaves, from one 
data set to another, just as b - /3 does. If we do a good job of fitting, (b -/3)x 
is sometimes positive and sometimes negative. 
If other contributions from x are important, we may want to go to 
additional terms or forms of x and use, say, 
y - bx - cx 2, 
where bx + cx 2 is a better fit to y. 
288 /12: Regression for fitting 
The customary situation. But what if x is a useful measure of what we want to 
keep out, but is not an exact measure? Perhaps we are studying children and 
wish to keep out--adjust for--the effect of maturity. We are likely to have 
available chronological age, which tells us a lot about maturity, but not the 
whole story. We have no difficulty using chronological age as an x in predic- 
tion, but how should we use it in adjustment? 
More generally, then, we have an x which measures a z only approxi- 
mately; how can we use it to remove the effect of z from y? It turns out that 
we at least need an extra variable u, called an instrumental variable, which also 
measures z, like the score on a maturity test. 
Suppose that we have an instrumental variable u, meaning by this: 
1. that u is also associated with z, 
2. that the residual of u, after subtracting its true regression on z, is uncorre- 
lated (has zero covariance) with the residuals of x and y. 
More precisely, the statistical model is 
x = a +/3z + residuals, 
y = a +/3z + residuals, 
u = a +/3z + residuals, 
/3,/3,/3 are regression coefficients of z, 
coy {residuals, residuals} = 0, 
coy {residuals, residuals} = 0. 
We might try to use a linear function of either u or x to eliminate z from 
y. If we want to exclude z from y by subtracting a multiple of x, since we have 
to remove the term/3z from y, we need to multiply x by/3//3 so that the x's 
13z term will also contribute /3z. We get 
 /3y/3z /3 residual + C 
y x = /3yz + residualy /3 
/3 residual + C 
= residualy 
where C = ay - (13y/13)a,. (The value of C is not important in this discussion.) 
To do this, we need an estimate of /3y//3, something we cannot get directly, 
because By and/3 are regression coefficients on z, and we do not know z. 
In this ideal situation, the covariances of x and y with u are 
cov {x, u} =cov {/3z,/3z} = /3/3. var {z}, 
coy {y, u} =cov {/3vz,/3z} = /3/3 var {z}, 
as we see by substituting for x, y, and u in the first equalities and noticing that, 
in both lines, three of the four resulting covariances vanish. Accordingly, from 
12F: Regression as exclusion 289 
the least-squares definition for the population, the regression coefficients of x 
and y on u are 
cov {x, u} /3 vat {z} 
-t3' 
vat {u} var {u} 
and 
coy {y, u} /3 var {z} 
var{u} vat{u} ' 
so that 
Now we can estimate/3y., by byu, since we know both y and u, and /3x by 
bx. This lets us take 
bxu 
as a reasonable estimate of/3J/3 and 
byu 
Y b, x 
as a reasonably adjusted y--adjusted for z, not only for x. All this is sound so 
long as u satisfies the zero-covariance conditions for the residuals, and 
cov (x, u)  O. (Economists might use the language that says that we have 
adjusted for the structural regression instead of for the predictive regression.) 
The u's that we have available are often crude. If they are, we still use 
them rather than nothing. Even a u that takes two or three values--that 
divides the data into two or three groups is quite usable if we can believe that 
it satisfies the zero-covariance conditions for residuals and the nonzero 
covariance condition. 
Note that b 
is ordinarily larger than by would be. To see this, suppose first that 
cov {residuaL, residuals} = O. 
then 
cov {y, x) _ /3y/3 var {z) 
var {x} /32 var {z} + var {residuaL}' 
while we can rewrite /3J/3 as 
= /3/3 vat {z} 
/3}var {z} ' 
a fraction with the same numerator but a smaller denominator than/3y. If the 
290 /12: Regression for fitting 
covariance between the residuals of x and y does not vanish, we can still 
estimate/3y//3--though it may be even harder to find a u but we cannot be as 
sure that/3y//3x is farther from 0 than/3yx is. 
Although we shall not prove it, the variability of 
Y bu x 
will ordinarily be larger than the variability of 
y - bvx 
because trying to predict 
y adjusted for z 
must be harder than trying to predict 
y adjusted for x, 
since we know so much more about x than about z. 
We now have a technology for removing the effects of a latent variable, 
here z, from a response variable y, provided that we have two measures that 
depend on the latent variable and meet other conditions. Let us apply it. 
When illustrating a new statistical technology, perhaps the most helpful 
example is one where the structure is secretly known, so that we can see how 
well the method behaves and find the sources of deviations. Let us display first 
a set of data as it might appear in a real-life example where the structure is not 
known. 
Example. Known structure. Exhibit 14 shows the original data for y, x, and u, 
and we want to remove, even if we cannot quite succeed, the effect of z where, 
as a special case of what was described above, we suppose that 
y = /3yz + residualy, x = /3z + residuals, u = /3uz + residuals, 
and that the residual terms have zero covariances. 
Solution. Exhibit 14 shows also the calculation of bydb, = 2.505. Using this 
result, we can compute the residuals shown in the first column of Exhibit 15. 
(The true values are plotted against the estimated values in Exhibit 17.) Note 
that they follow the true residualy's in a general way, though by no means 
perfectly. 
Discussion 
We secretly know exactly the structure of this example because we built it to fit 
the model perfectly. We chose 10 values of z (= -5, -4, -3, -2, -1, 1, 2, 3, 
4, 5)--(Note. Zero happens not to be present)--and /y - $, x = 2, u -- 3o 
We chose as, ay, c = 0. The residual terms were chosen to be independently 
normally distributed with means 0 and standard deviations , 1, and 3 for y, x 
12F: Regression as exclusion/Exhibits 14 and 15 291 
Exhibit 14 of Chapter 12 
Data for removal of z effect from y, where u is the instrumental variable. 
y x u 
-26 -8 -16 
-21 -7 -14 
-14 -7 -8 
-5 -4 -2 
-8 -2 -2 
7 3 3 
8 5 2 
13 6 11 
16 8 9 
25 11 14 
Totals -5 5 -3 
 yu = 1522 
 xu = 605 
by. 152.2- (-.5)(-.3) = 2.505 
b,,. 60.5- (.5)(-.3) 
Exhibit 15 of Chapter 12 
Estimated and true residuals. 
Estimated 
residualy True 
y- 2.505x residualy 
-6.0 -1 
-3.5 -1 
3.5 1 
5.0 5 
-3.0 -3 
-.5 2 
-4.5 -2 
-1.5 -2 
-4.0 -4 
-2.6 0 
Totals -17.1 -5 
292 Exhibits 16 and 17/12: Regression 
and u, respectively. (Note. These are not standard deviations of y, x, and u, but 
of their residual terms.) Using random normal deviates, 10 values of residual 
terms were found for each variable, and, for convenience, these were rounded 
to the nearest integer. The actual values and components leading to the y's, 
x's, and u's are shown in Exhibit 16. In real life only the y, x, and u columns 
would be available. 
Exhibit 16 of Chapter 12 
Construction of the y, x, u values 
I z /vz Residualv I i y /z Residual I !x /z Residuall I  I 
-5 -25 -1 -26 -10 2 -8 -15 -1 -16 
-4 -20 -1 -21 -S 1 -7 -12 -2 -14 
-3 -15 1 -14 -6 -1 -7 -9 1 -8 
-2 -0 5 -s - 0 - - 4 -2 
- -5 -3 -8 -2 0 -2 -3  -2 
1 5 2 7 2 1 3 3 0 3 
2 10 -2 8 4 1 5 6 -4 2 
3 15 -2 13 6 0 6 9 2 11 
4 20 -4 16 8 0 8 12 -3 9 
5 25 0 25 10 1 11 15 -1 14 
Exhibit 17 of Chapter 12 
Plotted from values of Exhibit 15. 
Tru 
ridui.v 
oZ- 
I I ; I--I I 0 I I I t I ! 
� �  ;stiimat reslduel X 
(12F) 12G- Which straight line? 293 
Estimating the residuals. Our estimate of /y// is 2.505, extremely--and 
unusually.. close to the true value 2.5. Nevertheless, we cannot retrieve the 
residualv's exactly because, even with the ratio almost exactly right, x is still 
not z. Since x is not z, the errors in such estimates of the residual of y will have 
size about 2.505 x residuaL, as algebra will show. In the example, the re- 
siduaL's are all �2, �l, or 0, and so the error in estimating the residual y is 
about �5, +2.5, or 0. The only residual of 2 is the first one, and that gives an 
error of 5, to one decimal place. In addition to this, had the estimate of the 
ratio /y// been farther from the true value, the deviations from the true 
residuals would have had a larger contribution from that error in slope. 
Some readers may have noted that we might also have used x as the 
instrumental variable for the regression of y on z, estimating/3// by by/b,. If 
we had chosen to do that, our estimate of residualy would be 
y - 
Since/3//3 is 5/3, the new estimated residualy's would be in error by roughly 
5/3 residuaL. 
Both methods could be used, and some weighting scheme could combine 
them to make different estimates of the residual's. We do not pursue that line 
of discussion here. 
12G. Which Straight Line? (Optional.) 
Suppose, for the moment, that we have just one carrier xmand that y is well 
fitted by a straight line 
y----A+Bx 
in x. Does this meanmir there is a "right" straight linethat we have the 
: "right" straight line? Perhapsand perhaps not. 
Suppose that Y(y) is a function of y that increases when y increases; then 
Y(y) = Y(A + Bx), 
so that 
Y(y)- A* + B*X(x), 
where 
X(x) = Y(A + Bx)- A* 
.which is a function of x alone, once the constants A, B, A*, and B* are 
:':::known. Thus any such Y has a whole family of X's (A* and B* are at our 
choice) such that Y(y) is well fitted by a straight line in X(x). How then are we 
:to pick out the right straight line? 
If there appears to be a single choice for which the two re-expressions 
one of y and one of x both seem simple, perhaps we can believe that this is 
::::'the fight straight line. (A nice special case arises when the first-aid rules lead to 
294 /12: Regression for fitting 
a straight line.) But what if there is no such guidance? Or if two or three pairs 
of relatively simple re-expressions all seem to fit relatively well? (If this seems 
unlikely, recall that if y = bx then log y - log b + log x, and that if log y - 
log b + k. log x, then y = bxk.) 
It is easy to say, especially for a physical scientist, that theory should tell us 
what to choose. But if theory says that bx, also says that log y 
y= it = 
log b + log x. Theory is no help to a single set of data. 
Sometimes parallel cases are sufficient to settle a choice. If 
y = a + b,x 
in the ith case, where the ai and bi vary rather unpredictably, then having 
log (y - ai) = log b + log x 
with different values of at, is far from being attractive. After all, log (y - a,) is 
a different function of y for each a. We have lost consistency of description by 
"taking logs" when this means something different for each parallel case. 
Making parallel cases come out parallel can be a good reason for choosing 
(y, x) instead o.f (Y, X). 
Beyond ths, only one other criterion sees some general use and has some 
arguments of plausibility in its form. This criterion says choose (y, x) or (Y, X) 
so that the spread of the data around the straight line (or around the more 
general fit) is roughly constant. 
By this criterion, for instance, if we can fit y = bx, and note that the 
spread of y's for a given x is roughly proportional to that given x (and hence to 
the center of the y-distribution for that x), we find ourselves urged to go over 
to log y = log b + log x 
for which the residuals will be more nearly of constant width. This choice 
depends on the behavior of imperfections; we can only do it if the fit is not 
perfect. 
Why might this criterion be plausible? First, because it makes the situation 
easier to explain and think about. One can say that 90% or 95% limits on a 
single new observation are about so-and-so wide, without having to bother 
about x. One can look at a residual and assess its plausible relevanceswithout 
bothering about x. 
Second, we often fit y against x and then go on to analyze the residuals, 
perhaps by regression on a further carriersusually more usefully, of course, by 
regression on the residuals of the new carrier on x. In this process, it helps if 
the variables used at the second or later stage are at least moderately 
compatible with the residuals; a good standard for both is roughly constant 
variance when displayed against x. 
Without other dominating considerations, in choosing between (y, x) and 
(Y, X), we appear to have two good criteria, which may apply: 
o making parallel situations become well analyzed in parallel ways, 
omaking the imperfections of fit of approximately constant size. 
(12G, 12H)Summary: Regression 295 
We cannot expect both of these to be available for the same data. When they 
are both available, they often agree; when they disagree, follow the first of the 
two. 
12H. Using Subsamples 
Regression is applied to varying amounts of data as few as 3 or 4 points (data 
sets) and, perhaps, as many as 5 million. In any ordinary circumstances, starting 
one's regressions with more than 1000 points is almost certainly bad practice, 
and starting with more than 200 can well be. Regression should ordinarily be a 
flexible process. Residuals ought to be looked at in several ways. Starting in on 
an oversized collection of data sets severely discourages flexibility. 
If we are lucky enough to have 10,000 data points, it is almost certain that 
we should begin our work on them by preparing subsamples, perhaps a 
subsample of 1000 and a sub-sample of 100. Perhaps a subsample of 2500, a 
sub-sample of 625, and a sub-sub-subsample of 160 or so. In either case, we 
would then begin the analysis by working with the smallest subsample until we 
had gone as far as we could, and then move up to the next subsample. 
A simple consequence is that large data files should be so stored that it is 
easy to extract subsamples of various sizes. When the order of data sets is 
unimportant, the easiest way to arrange for this may be to pull successive 
subsamples out, putting each ahead of the sample or subsample from which it is 
drawn. 
Taking one from every 2, 4, 8, 16 or, more generally, 2 k (integer k > 0) 
successive values is easily accomplished, using uniform random numbers to 
pick the one value. At worst, such a random stratified sample will be almost as 
good as a simple random sample. At best, as when the original file is quite 
:structured, it will be much better. If we need more careful sampling, perhaps 
:::itaking explicit account of some stratification of the file, C. T. Fan, Mervin 
Muller and Ivan Rezucha (1962) have prepared programs that make such 
sampling easy on IBM equipment. 
Another use of subsamples, of course, is to assess the stability of answers, 
::either directly or via the jackknife (see Chapter 8). 
Summary: Regression 
We have recognized the difficulties of establishing causation, the pressures for 
::belief in establishment of causation that needs for decisions impose upon us, 
and the consequent dangers of serious error when established association is 
wrongly interpreted as causation. 
We have reviewed the different meanings of dependence (and of related 
:::Words) and the advantages of avoiding the use of such words. 
"Regression" has two quite different meanings, one a curve of typical 
values of y for (nearly) fixed x (or, more generally, a surface of such values) 
296 /12: Regression for fitting 
and the other a choice of a collection of possible fits distinguished by values of 
constants and a fitting of these constants. 
Every regression (according to either definition) is an incomplete descrip- 
tion. Thus, though our regression is often extremely convenient or extremely 
useful, it has no claim either to begin in "the right form" or to be telling us the 
whole story. 
Regression has several different purposes, including summarization, meas- 
urement (of the coefficients in a specified regression), exclusion (of the effects 
of interfering variables), and prediction--and what is good practice for one 
purpose need not be good practice for another. 
Near or exact collinearity, either o two carriers or of several, (i) is 
something we must be prepared for, (ii) is something that can destroy (one or 
both o) precision of estimation and simplicity of interpretation of coefficients 
while leaving the quality of fit high, (iii) is something that has caused many 
misinterpretations and error, and (iv) is something that we can learn to deal 
with. 
Polynomials--even quadratic ones--are likely instances of near collinear- 
ity (unless we are careful). 
A good fit of the form y = a + bx may, or may not, mean that this is the 
"right" straight line. 
We can fit regressions by stages, removing each carrier in turn .to an 
amount assessed by either graphical or arithmetical techniques- not only from 
the response but also from each other carrier not so far fitted. 
We replace bundles of closely associated (nearly collinear) carriers by 
either a single composite or by a single composite together with the residuals of 
the individual carriers ater regression on that composite (or together with only 
those residuals that appear enough larger than measurement error to be likely 
to be useful). 
In regression as exclusion, we can do better than use regression coefficients 
appropriate for prediction (we need to use larger coefficients, specifically 
structural ones). 
Rough constancy of size of residual is a guide in choosing among 
alternative good fits by straight lines (where the alternatives involve re- 
expressions, sometimes of both x and y). Consistent description of parallel sets 
of data is a better guide. 
To deal effectively with any significantly large set of data, plan to sub- 
sample and to work first on the smallest sample that is reasonable for what we 
are doing, stepping up (by a factor of 2, 3, $, or 10) to successively larger bodies of 
data as our analysis progresses. 
References 
Allport, G. W., P. E. Vernon, and G. Lindzey (3rd ed., 1960). Study of Values. 
Boston: Houghton Mifflin. 
References 297 
EDA--Tukey, J. W. (1977). Exploratory Data Analysis. Reading, Mass.: 
Addison-Wesley. 
Fan, C. T., M. E. Muller, and I. Rezucha (1962). "Development of sampling 
plans by using sequential (item-by-item) selection techniques and digital com- 
puters." J. of Amer. Statist. Assoc., 57, 387-402. 
Riley, M. W., and A. Foner (1968). Aging and Society, Vol. 1. New York: 
Russell-Sage Foundation. p. 437. (Shows Exhibit 1, page 263.) 
298 Index for Chapter 13 
13A Meaning of coefficients in multiple regression 299 
13B Linear adjustment as a mode of description 303 
13C Examples of linear adjustment 305 
13D The relative unimportance of the exact carrier 315 
13E Proxy phenomena 316 
13F Sometimes x's can be "held constant" 318 
13G Experiments, closed systems, and physical 
versus social sciences, with examples 320 
13H Estimated variances are not enough 328 
Summary: Woes of regression coefficients 331 
References 332 
Chapter 13/Woes of 
Regression Coefficients 
We know that regression coefficients can suffer from serious difficulties. Let us 
look systematically at what can be done about them. We discuss what troubles 
arise, how they happen, and to what extent we can prepare for them, avoid 
them, or become warned of their presence. 
� Coefficients that we can apply in new situations may be the most useful 
knd of knowledge we can try to gain or reasonably hope to have (except in a 
few highly structured situations). 
13A. Meaning of Coefficients in Multiple Regression 
Form ol the variable. As a first problem in the meaning of a regression 
coefficient, we consider a quadratic function, which can be thought of as having 
structure /30' 1 + x + 2x2, where x2 --- x2. We consider different ways of 
writing the basic variable x (dropping the subscript) and their effect 
upon the size of the coefficients. 
If 
117- 3x + 2X 2 
is a good fit for -2 <- x <- 5, what meaning do we give x's coefficient, -3? 
Each of the following expressions is numerically identical with 117- 3x + 
2x 2, namely: 
115 + x + 2(x - 1) 2 
109 + 5x + 2(x - 2) 2 
117 + x + 2x(x - 2) 
117 + 5x + 2x(x -4). 
Whatever interpretation we can properly give to -3, the coefficient of x in our 
original formula, there must be a parallel interpretation that we can give to 1 
or 5, each the coefficient of x in two of our later formulas, because all these 
formulas produce identical results. 
If we had started with 
12- 3x + 5x2 
and gone to 
12- 8x + 5x2' 
where x2* = x + x2, we might be tempted to talk about the coefficient "with 
299 
300 /13: Woes of regression coefficients 
xx held constant," because the remaining coefficients in the two expressions 
have the same value. But recalling that x2 might be x 2 note that for positive x's 
we cannot hold x 2 constant and vary x very much. This cannot be the answer. 
The only general fact is that the coefficient of any one carrier in our 
example, x--depends on which other carriers are offered for fitting at the same 
time--in this example, 1 and x 2, or 1 and (x - 1) 2, or 1 and x(x - 2), or 1 and 
x(x - 4). 
As the differences, in our simple example, among 
-3 = coefficient of x when 1 and x 2 are also offered 
and 
+1 = coefficient of x when 1 and (x - 1) 2 are also offered, 
+5 = coefficient of x when 1 and (x - 2) 2 are also offered, 
emphasize, a coefficient in a multiple regression. either in a theory or in a 
fit depends on MORE than just: 
o the set of data and the method of fitting. 
o the carrier it multiplies. 
It also depends on: 
o what else is offered as part of the fit. 
Subsets o[ variables. Part of the difficulty of giving meaning to regression 
coefficients arises because the coefficients themselves change depending upon 
which variables are present in the regression. This could be regarded as an 
extension of the points made above. Since we frequently do not know which 
ones to put in, this leaves us in an awkward spot. To drive this point home, let 
us illustrate with an easily verified numerical example. 
Example. Plane through the origin. Let us suppose that, unknown to us, it is 
exactly true that 
y = + +/33x3, 
but that we plan to fit by least squares 
Y = /1X1 '- t2X2 
or 
y '-- lXl + 3X3 
or 
y = lXl 
instead. Let us assume that fl = 2, fl: = 4, f13 = 10, and, for arithmetic 
simplicity, that we have complete cross-product symmetry among our variables 
so that, or our (x, x:, x3) triples, 
13A: Meaning of coefficients in multiple regression 301 
What are /1, /il, and el, and how do they compare with /317 
Solution. For least-squares fitting a plane through the origin, 
(Z xy)(Z - (Z xy)(Z 
(Z x)(Z - (Z 
and the formula for 15 is the same except that x2 is replaced by x3 throughout. 
We can readily get  xy, i = 1, 2, 3, from 
 xy = 13  xx, + 132  x2x + 133  x3x, i= 1,2, 3, 
and we find for the numerical case chosen that  x ly = 9,  x2y = 10, 
 x3y = 13. 
We then have or compute 
= 2, = 5�, = 3�. 
And so, even in this very symmetrical problem the coefficient of x varies 
substantially depending on which variables are present. If we fit only y - exl, 
the coefficient of x is 
1=9. 
When we treat this example, as we have, as a straight problem in 
arithmetic, our first reaction may well be, "Whoever said there was to be a 
simple relation or invariance when the variables changed?" That is a fair 
enough comment. But in real-world problems, we do ransack the variables, 
keeping some and throwing others away. In the end we will have a strong urge 
to interpret the chosen coefficients in a physical manner, such as "If we change 
xl by a given amount, we will change y by a certain amou.nt, and ther. efor. e 
public policy should be...". Our example shows that even m a determnistlc 
system (the original y -- /3x + /32x2 + 3X3 had no error), different subsets of 
the variables used for regression can give substantially different coefficients for 
the same variable. Indeed, even the sign can be reversed from one set to 
another. 
We need then to speak of the coefficient of xl 
0 when x2 and x3 are also offered, 
0 when x2 is also offered, 
0 when x3 is also offered, 
0 when nothing else is offered, 
and appreciate that these four ordinarily give substantially different results. 
.:...A variety o x's. This point is so important that we make it again more 
:algebraically. 
302 /13: Woes of regression coefficients 
Given several x's, the coefficientsc, c*, c**, c***--of x when we fit 
ClX 1 q- C2X 2 -1- C3X 3 q- �4X4 
CX + C2X2 + C3X3 
c**x (1) 
1 -It- C2*X2 -It' C6*X6 
+ 
are usually different. Indeed, rarely will they be the same. 
We are now looking at four different fits � in the sense that the set of 
possibilities that can be represented by c*x + c2'x2 + c3'x3 (for some values of 
c*, c2', c3') is a very different set of possibilities from all those that can be 
represented by c***xx + c****x? (for all possible values of c*** and c****). 
Thus there is no way to go from a fit of (c, c2, c3, c4) without further 
information to a fit of (c*, c2', c3') or of (c**, c2 , c6'*) or of *** *** 
(c , c7 ) and 
no way to use a fit to one of these sets of coefficients without further 
information to get (c, c2, Ca, c4) or even the individual c's. We would need to 
know much more about the data (or population) involved to make such 
conversions. In our deterministic example we used all the sums of squares and 
cross-products. 
(The case of 12 - 3x + 5x2 and 12 - 8x + 5x2', with x2* = x + x2, is a 
different matter, where we deal with the same fit expressed in different terms. 
Here, knowing that x2* = xx + x2 enables us to convert one set of coefficients 
into the other. This sort of possibility, although often useful computationally, 
does not often arise in subject-matter oriented analyses.) 
Stocks. Each fit is selected from a set of possible fits. In expressions (1), we 
gave four such sets. We call such a set a "stock" because we want to think 
about it as a collection of potential fits from which one can be selected by 
choosing the values of the c's, just as men's shirts can be ordered by giving 
neck and arm measurements. They are the set of possible fits chosen for the 
regression. 
Any sum-of-terms stock -the only kind we discuss in this chapter can be 
written 
C1X1 -'[- C2X2 -'[- ' ' ' -'[- CkXk 
if we are willing to admit that any or all of x2, x3,..., xk may also involve x. 
(Thus, x2-= x, x3-- x3*, x4-- x + x4*, where x3* and x4* do not involve x, 
would define a perfectly good stock of the form cx + C2X 2 q- C3X3 -{- C4X4-) 
It is important to know what other carriers are present in addition to the 
carrier whose coefficient concerns us. Are the details of the form of expression 
of a stock important? Not so long as we can get back and forth algebraically 
from one form to the other. The stocks 
C1X1 q- C2X 2 q- . . � _.[_ CkX k 
and 
CtX q- C2X2 + ''' + �X 
(13A) 13B: Linear adjustment as a mode of description 303 
are the same, if the smaller stocks, called costocks of x (to have a convenient 
way of talking about them), one made up of everything of the form 
c2x2 +'" + CkXk (Any c's), 
and the other of everything of the form 
c2 x2 + � � � + ckxk (Any c*'s), 
are identical. These costocks include everything in the whole stock with c = 0. 
The values of c in the two fits will be the same provided the fitting 
criteria: (i) are the same, (ii) depend only upon residuals; for example, if both 
forms are fitted by least squares. Thus, in a sum-of-terms fit, what matters in 
fixing the coefficient of x are: 
o the set of data and the method of fitting (including any weights), 
o the stock defined by the other terms that are also being offered. 
13B. Linear Adjustment as a Mode of Description 
In Section 12C, we introduced and illustrated graphically a technique for 
removing one variable at a time; we called it fitting by stages. In this section we 
develop this idea algebraically, partly 
a) because we wish to use it further, 
b) because we want to show that the technique does produce a least-squares 
solution, and 
c) because it emphasizes what we have just said by observing that the 
least-squares coefficient of each carrier is the regression of the response on 
this carrier linearly adjusted for this carrier's costock. 
A word about notatio. n. When .we have y and several x's, x, x2,..., xk, we 
have introduced notation for residuals exemplified by y;2 and x3;x2 to indicate 
that x and x2 have been removed from y and x3, respectively. When we use 
least squares we replace the semicolon by a period, and the least-squares 
residuals above are written y.2 and x.2. Sometimes x is a constant, frequently 
x -- 1. The example we are about to discuss has three variables y, x, and t, and 
a constant is also to be fitted. In the x notation our variables are x = 1, 
x2 = x, x3 -- t. It will be convenient in this example to let 1, x, and t stand for 
themselves instead of the subscripts. Thus y.x will mean that the constant and 
x have been removed from y by least squares. 
In this discussion we describe a step-by-step. procedure for 'itaking out" 
the linear effect of variables, one variable at a tme. Let us begn with only 
three variables, y, x, and t and the constant 1 one response and three carriers, 
304 /13: Woes of regression coefficients 
respectively. Suppose we have regressed y on 1 and x, finding the fit y = 
a + bx. Then we can compute the residuals, which in this context are 
y-adjusted-linearly-for-l-and-x = y.x -- y - a - bx, 
and suppose that this adjusts y adequately for x. If then we want to ask 
whether the adjusted y seems to depend on t as well, we could start by plotting 
the adjusted y against t and looking. 
What if t were rather like x? We have tried to free y.x of x, so it cannot 
look like the part of t that behaves like x. We will get a much clearer picture if 
we regress t on x also, thus largely freeing t of x and finding 
t-adjusted-linearly-for-l-and-x --- t. =- t- c - dx. 
Now we can plot y. against t.. Suppose we find that there is a clear 
dependence. It is natural for us now to fit a linear regression of y.l on t., say 
y. -- e + ft.x. 
What can we say about e and f when we do everything by least squares? 
We will show, at the end of the next chapter, that this approach gives us, in an 
unfamiliar form, exactly the least-squares fit of y to /31 + /32x + 3t. More 
specifically 
a + bx + f(t- c - dx) 
is the least-squares fit in question, as proved in Section 14K. 
The general case. We can have several x's instead of 1, as explained in Section 
14K. The general result is that if, for example, 
y - a + bx + ct + du + ev + fw, 
then: 
1. b is the regression coefficient of x when y, linearly adjusted for 1, t, u, v, 
and w, is regressed on x, linearly adjusted for 1, t, u, v, and w. 
2. c is the regression coefficient of t when y, linearly adjusted for 1, x, u, v, 
and w, is regressed on t, linearly adjusted for 1, x, u, v, and w. 
6. a is the regression coefficient of 1 when y, linearly adjusted for x, t, u, v, 
and w, is regressed on 1, linearly adjusted for x, t, u, v, and w. 
When we deal with a particular situation, we want to write this sort of 
statement out for each coefficient that we want to interpret. There is no 
substitute for facing the facts in detail. 
(13B) 13C: Examples of linear adjustment 305 
When we want a statement to remember, we can condense a little, and 
say: 
The (least-squares) coefficient of each carrier is the regression of the 
response, linearly adjusted for this carrier's costock, on this carrier also 
linearly adjusted for this carrier's costock (where the costock contains all pos- 
sibilities in the whole stock for which the corresponding carrier has coefficient 
zero). 
13C. Examples of Linear Adjustment 
Using fitting methods to recover meaningful coefficients from data is one of the 
jobs often asked of regression. Let us look at an example where we assume the 
correct functional form, but where the data have involved rounding. We 
assume that a + bx + cx 2 is the correct form to fit. The true quadratic is 
y --- -1 + x + 0.5x 2. When we computed the values of y for a set of x's, we 
rounded the term 0.5x 2 to two decimals, rounding to the nearest even second 
decimal when necessary. Thus for x- 0.9 we replaced 0.5(0.81) by 0.40 
instead of the correct 0.405. We are starting with values close to the correct 
quadratic. Let us see how nearly we recover it by regression. 
Beginning with the constant. If, for computational convenience, we wanted to 
take out I first, the calculation analogous to that given in Section 13B would 
begin 
Y-1 -- Y -- , X. = X- , t. = t- t; 
then taking out x would give 
Y.lx  Y-1- gx.1, t.s -- t.x- hx.x; 
and finally 
y., = y. -ft.; 
and the fit would be 
Yfitted - Y -[- gx.1 +ft. lx. 
We use this approach in the examples. 
Example 1. Quadratic. Exhibit 1 gives data for y and x and we will take as t 
the quadratic x 2. We want the multiple regression of y on 1, x, and t. 
Solution. From y, x, and x 2 we removed their averages 0.94, 1.2, and 1.48 to 
get 
Y.1 = Y -- Y, X.1  X -- , t-1 --- (X2)-I -- X 2-- -. 
(Note that  is the average of the squares.) 
306 Exhibit 1/13: Woes of regression coefficients 
Next we adjust y. and t.x by removing x. to get 
y.x m y. __ gX.x = y.- 2.2X.x, 
t.x --= t.X- hx. = t.x- 2.4x.. 
Then at last we regress y. on t. lx to get 
y.,  y. - ]'t.t = y.x -.48t.. 
Exhibit I of Chapter 13 
Quadratic regression using stepwise approach. 
y. =-- x. ---- tq ---- (x2)q 
I I l"x"l I x I i I 
0.30 -.64 .192 0.9 -.3 .09 .81 -.67 .201 
0.50 -.44 .088 1.0 -.2 .04 1.00 -.48 .096 
0.70 -.24 .024 1.1 -.1 .01 1.21 -.27 .027 
0.92 -.02 0 1.2 0 0 1.44 -.04 0 
1.14 .20 .020 1.3 .1 .01 1.69 .21 .021 
1.38 ... .088 1.4 .2 .04 1.96 .48 .096 
1.62 .68 .204 1.5 .3 .09 2.25 .77 .231 
r/= 0.94 .616  = 1.2 .28 x -= = 1.48 .672 
g y.x.=..: t.x. ..882 
---- T, x. = 2.2, h ---= �x. T = = 2.4 
/ I 
_ 2 
lx = t. lx  t-lx  
" I I"i -x., i " I! " I '-' I'x'[ IIx>"l x'i 
-.64 -.66 .02 -.67 -.72 .05 .0010 .0025 -.004 
-.4 -. 0 -.48 -.48 0 0 0 .000 
-.24 -.22 -.02 -.27 -.24 -.03 .0006 .0009 -.006 
-.02 0 -.02 -.04 0 -.04 .0008 .0016 -.001 
.20 .22 -.02 .21 .24 -.03 .0006 .0009 -.006 
. .44 0 .48 .48 0 0 0 .000 
.68 .66 .02 .77 .72 .05 .0010 .0025 -.004 
.0040 .0084 
 Y.xt.x .0040 
f=-- = -0.48. 
t.x .0084 
Ytt = ' + gx., + f(x2).,x = 0.94 + 2.2x., + 0.48(x2).x 
Yfd = � - (g - fh) - f- + (g - fh )x + fx  = -1.03 + 1.05x + 0.48x  
13C: Examples of linear adjustment 307 
That result, shown in the final column of Exhibit 1, gives y.x, as zero to 
two decimals, when we round 0.Sx 2 to the nearest even second decimal. 
The final fit can be written 
Yttea = 0.94 + 2.2x.x + 0.48(x2).lx, 
or rewritten in terms of 1, x, and x 2 as 
3/fitted = -1.03 + 1.05x + 0.48X 2. 
The result is most encouraging when we recall that we started out with 
y = -1 + x + 0.Sx 2, but that we round.ed the 0.Sx 2 term. We started very 
close to a true quadratic and came recognzzably close to recovering it. We note 
that introducing error into one term disturbed all three coefficients slightly. 
The calculations have been carried to the extent shown and the various 
rounding errors in the calculations have some effect. In the top panel, second 
column, the y.'s do not sum to zero as they should in theory. We observe that 
the final residuals add up to a negative value rather than zero as theory says 
they must. But the theory is for exact calculations with unlimited decimals. The 
magnitude of the residuals y.,, as calculated in the last column of Exhibit 1, 
is misleading because, when we use the desired formula, we get smaller 
residuals, as we now show. 
Let us look at the difference between the original unrounded function and 
the fitted function. It is 
Yte- Yntea = -1 + x + 0.5x 2- (-1.03 + 1.05x + 0.48x 2) 
= 0.03 -- 0.05X + 0.02X 2. 
By completing the square we have 
0.0[x 0.0 1 (0.0s, 
.- x + .02/ J- 4(0.02) + 0.03 
= 0.02(x- 1.25) 2 + 0.03 - 0.03125 
-- 0.02(x - 1.25) 2 - 0.00125. 
Between 0.9 and 1.5, the largest departures of x from 1.25 are at x = 0.9 and 
x = 1.5. At x = 0.9 we find the largest error is about 0.0012, and at x = 1.5 it 
is 0. Therefore, the residuals in the final column exaggerate the actual errors in 
the final fit if it were calculated to unlimited decimals. In the next similar 
example, we will be just a little more careless, and observe the consequences. 
Example 2. Rounded quadratic. When the quadratic y = 1 + x 2 is rounded to 
one decimal place for x - 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, find what quadratic is 
recovered when we fit a general quadratic. 
Solution. The results are laid out in Exhibit 2 in a manner completely parallel 
to that for Exhibit 1. The graph of Exhibit 3 exhibits the irregularities 
introduced by the rounding. 
308 Exhibit 2/13: Woes of regression coefficients 
We have recovered 
Yfitted = 0.95- 0.02x + 1.01X 2, 
which again gives coefficients close to those of the true function. The error 
-0.05 - 0.02x + 0.01x 2 = -0.06 + 0.0l(x - 1) 2 takes values close to -0.06 
for x's between 0.9 and 1.5. We are, of course, aided by the lack of any 
random error beyond the rounding and by the fact that the function we happen 
to be fitting has exactly the algebraic form that we are trying to retrieve. The 
Exhibit 2 of Chapter 13 
Recovering y = I + x = rounded to one decimal. 
1.8 -.67 .201 0.9 -.3 .09 .81 -.67 .201 
2.0 -.47 .094 1.0 -.2 .04 1.00 -.48 .096 
2.2 -.27 .027 1.1 -.1 .01 1.21 -.27 .027 
2.4 -.07 0 1.2 0 0 1.44 -.04 0 
2.7 .23 .023 1.3 � 1 .01 1.69 .21 .021 
3.0 .53 .106 1.4 .2 .04 1.96 .48 .096 
3.2 .73 .219 1.5 .3 .09 2,25 .77 .231 
2.47 .670 g = 1.2 .28  = 1.48 .672 
.670 
g = = 2.4, h = 2.4 
.28 
-,67 -.72 .05 -.67 -,72 -.05 .0025 .0025 0 
-.47 -.48 ,01 -.48 -.48 0 0 0 .01 
-.27 -.24 -.03 -.27 -.24 -.03 .0009 .0009 0 
-.07 0 -.07 -.04 0 -.04 .0028 .0016 -.03 
.23 .24 -.01 .21 .24 -.03 .0003 .0009 .02 
.53 .48 .05 .48 .48 0 0 0 .05 
",74 .72 .04 .77 .72 .05 .0020 .0025 -.01 
.0085 .0084 
.0085 
f = = 1.01 
.0084 
I/fitte d = ' q-gx. +ft. lx = 2.47 + 2.4x. + 1.01hx 
/fitted -" 0.95 -- O.02x + 1,01x 2 
13C: Linear adjustment/Exhibit 3 309 
residuals are not as near zero as they were for Example 1. But we note that the 
residuals are smaller than the error, for example, in the constant term, which 
shows again that a function can fit well over a range, even though the 
coefficients do not match the true ones very closely. We can suppose then that 
even when we have the wrong form for the function, fitting may be fairly close. 
Example 3. Rounding to the nearest integer. What happens when we try to 
recover y = 1 + x 2 from data at the same values of x as in Example 2 after we 
round y to the nearest integer? 
Solution. Exhibit 4 shows the calculations, which, except for y.,, are essen- 
tially the same on the righthand half of the page as in the previous two tables. 
The final result is 
Yfitted = 3.31 - 3.67x + 2.38x 2, 
and even a motherly eye will find it very difficult to recognize 1 + x 2 hidden on 
the righthand side of this equation. 
Exhibit 3 of Chapter 13 
Plot of Ynuod versus x (small x's) for data shown in Exhibit 2 and Ytrue = I '' X 2 
(indicated by curve). 
5.0- 
Z.8:- 
I. 3 1.0 I. 1.4 I.t; 
310 Exhibit 4/13: Woes of regression coefficients 
From the difference 
Ytrue- Yntted = 1 + X 2-- (3.31 - 3.67x + 2.38x2), 
we find that the error is now, completing the squares as before, 
0.12- 1.38(x- 1.33) 2, 
whose largest absolute value, for x between 0.9 and 1.5, is 
10.12- 1.38(-0.43)21  [-0.41 = 0.14. 
Exhibit 4 of Chapter 13 
Retrieving y = I + x 2 from integer rounding. 
l I I-,I I i i<x-,>=i 
2 -.43 .129 0.9 -.3 .09 .81 -.67 .201 
2 -.43 .086 1.0 -.2 .04 1.00 -.48 .096 
2 -.43 .043 1.1 -.1 .01 1.21 -.24 .027 
2 -.43 0 1.2 0 0 1.44 -.04 0 
3 .57 .057 1.3 .1 .01 1.69 .21 .021 
3 .57 .114 1.4 .2 .04 1.96 .48 .096 
3 .57 .171 1.5 .3 .09 2.25 .77 .231 
� = 2.43 .600  = 1.2 .28 - 1.48 .672 
.600 
g = = 2.14, h = 2.4 
.28 
11x.,I l.,l I I 
-.43 -.64 .21 -.67 -.72 .05 .0105 .0025 .09 
-.43 -.43 0 -.48 -.48 0 0 0 0 
-.43 -.21 -.22 -.27 -.24 -.03 .0066 .0009 -.15 
-.43 0 -.43 -.04 0 -.04 .0172 .0016 -.33 
� 57 .21 .36 .21 .24 -.03 -.0108 .0009 .43 
� 57 .43 .14 .48 .48 0 0 0 .14 
� 57 .64 -.07 .77 .72 .05 -.0035 .0025 -.19 
� 0200 .0084 
.0200 
f = = 2.38 
.OO84 
' + gx. + ft.x = 2.43 + 2.14x. + 2.38t.x 
3.31 - 3.67x + 2.38x 2 
13C: Linear adjustment/Exhibit 5 311 
When we have rounded to units, moving values as much as 0.5, a maximum 
shift of 0.15 in the fit does not seem unreasonably large. The values of the 
function fit decently well in the relevant range of x, even though the coefficients 
of the fitted function are distant from those of the true function. 
We might return to the beginning of this section to see how closely we 
recovered the coefficients in the first example, and to contrast this with how 
poorly we just now recovered them in the third example. The difference is 
striking, and we may well ask "Why such a difference?" 
Let us recall that the scale of the disturbance in Example 3 was 100 times 
that in Example 1. To reduce the consequences of a random disturbance 
100-fold would take a sample 10,000 times as big. Such a change ought to 
correspond to a big difference in behavior. 
The general lesson here contains no surprises, but it is one we must 
continually keep in mind: As the data themselves become more and more 
degraded, the chance that the coefficients will reflect the true effects of their 
carriers becomes less and less even when we have exactly the correct model. The 
reader will want to consider whether Examples 1, 2, or 3 or something worse 
more properly represents the problems he or she ordinarily deals with. 
Example 4. Race relations, expectations of biracial living, and previous ex- 
perience. Exhibit 5 displays the expectations of blacks and whites for the 
outcome of biracial living as the expectation depends on previous experience, 
which Herbert Hyman (1965) gives. We wish to treat the variables "race" and 
"previous experience" as adjustment variables for the "expectations" variable. 
Exhibit 5 of Chapter 13 
Previous experience, race, and expectations. (Numbers in parentheses are counts 
of cases.) 
t: I = Have previously t: 0 = Have not previ- 
lived in ously lived in 
biracial area biracial area 
i y: Expectations ii x:l= black I l0 = white I Ix: 1= black i l0 =white 
2 -Integrationofraces 11%(21) 9%(10) 5%(1) 5%(5) 
1 = Accommodation 
of races 69%(133) 72%(83) 50%(9) 39%(3.5) 
0 = Conflict between 
races 20%(39) 19%(22) 45%(8) 56%(50) 
193 115 18 90 
S) SOURCE 
Hyman, Herbert (1965). Survey Design and Analysis. New York: The Free Press; p. 291. 
The data are reprinted with permission of the author and of Macmillan Publishing Co., Inc. Copyright 
1955 by The Free Press, a Division of The Macmillan Company. 
312 /13' Woes of regression coefficients 
Consequently, we have arbitrarily assigned scores for 
Variable x: Race black = 1 
[white = 0 
Variable t: Previous experience Ilived in biracial area = 1 
[not lived in biracal area - 0 
fintegration of races = 2 
Variable y: Expectations laccommodation of races = 1 
[conflict between races = 0 
The choice of scores for variables x and t does not matter because there are 
on. ly two categories. The choice of scores for y does matter a bit, and some 
mght prefer other spacings than the equal spacing we have chosen. If so, it will 
be easy to redo our analysis. 
Arithmetic is simplified if we recall that the least-squares fit to an array of 
numbers is their mean. Furthermore, when a line can be passed through the 
mean of y for each x, then that line is the least-squares fit. Because we have 
only two values of x, we can readily compute the regression lines by taking 
means of y and of t for each x. The intercept will be the mean value for x = 0, 
and the slope will be the mean value at x -- 1 minus the mean value at x = 0 
divided by 1 [the "run" of (rise/run) is (1 -0)]. 
Exhibit 6 shows the calculations for getting the regression equations for 
estimat!ng y and t from x and tables of the residuals y.x and t.x. The final 
regression equation is obtained from calculations outlined in Exhibit 7 and the 
result is 
Yfitted -- 0.514 + 0.029X + 0.371 t. 
The cell with the largest number of cases is that for y = 1, x = 1, t = 1, .and 
there the final equation estimates Yfittcd = 0.914 for a residual y. -- 1.09. 
The overall swing from inexperienced whites to experienced blacks is a change 
in estimated score from 0.514 to 0.914, or 0.4 score points. Thus experience 
and race make some difference in expectations, but not a great deal. We must 
remember that we study here populations as they stand. The events leading to 
experience or lack of it may also be related to expectations. Previous expecta- 
tions themselves could be part of the cause of degree of familiarity with 
integrated housing. Therefore we must be careful to remember that we are 
primarily describing a situation rather than proving cause and effect. 
Example 5. Linear constraints on variables. We mentioned in Section 12E the 
questions arising from 
B+Y=N 
where B = birth order, Y = number of younger children, and N--family 
13C: Linear adjustment/Exhibit 6 313 
size. The investigator studying birth order has a choice between fitting 
response - a + bBB, 
or 
response = a + b*B + byY, 
or 
response = a + b*B + bNN, 
or something more complex. Anything that can be put in the form 
response = a + bBB + byY + bNN 
can be reduced, identically, to either of the two two-term forms we have just 
given. To interpret the coefficient of B, which measures the effect of birth 
Exhibit 6 of Chapter 13 
Freeing y and t of x. 
A) Counts fv B) Counts ft 
I x I x 
2 15 22 37 I 115 193 308 
1 118 142 260 0 90 18 108 
0 72 47 119 
205 211 416 
Totals 205 211 416 
T. yfy = 148 186 Yfft = 115 193 
?x=o = 0.722, x= = 0.882 =o = 0.56, =1 = 0.915 
x = 0.722 + 0.160x { = 0.56 + 0.354x 
2 1.278 1.118 I 0.439 0.085 
I 0.278 0.118 0 -0.561 -0.915 
0 -0.722 -0.882 
314 Exhibit 7/13: Woes of regression coefficients 
order, he or she must decide among bB, b[, and b*. The real basis for 
this decision has to be the costock, which here is either 
{all constants} 
or 
{all a + byY} 
or 
{all a + brN}. 
Which should the investigator choose? 
Exhibit 7 of Chapter 13 
Freeing Y.x of t.x for the expectations for the integration example. 
counts 
I '.,x 
21 1.118 .085 2.0 1.09 1.2 
I 1.118 -.915 -1.0 1.46 1.2 
10 1.278 .439 5.6 1.12 1.2 
5 1.278 -.561 -3.6 1.49 1.2 
I 1 1 133 � 118 .085 1.3 .09 .2 
1 I 0 9 .118 -.915 -1.0 .46 .2 
1 0 I 83 .278 .439 10.1 .12 .2 
1 0 0 35 .278 -.561 -5.5 .49 
0 1 1 39 -.882 .085 -2.9 -.91 -.8 
0 I 0 8 -.882 -.915 6.5 -.54 -.8 
0 0 I 22 -.722 .439 -7.0 -.88 -.8 
0 0 0 50 -.722 -.561 20.3 -.51 -.8 
24.8 
fyx, t. = 66.9 
24.8/66.9 = 0.371 
Y..d = 0.722 + 0.160X + 0.371t.x 
Y.t.d = 0.514 + 0.029X + 0.371t 
(13C) 13D: Relative unimportance of exact carrier 315 
Note that the choice cannot be 
{all a + by Y + brN} 
for any c + dB can be written in this form as 
c +dY-dN, 
so that the costock of B covers the whole stock, leaving nothing to use to 
estimate a coefficient for B. 
If the investigator believes that Y matters only because of B and N, he or 
she should accept 
a + b*B + bNN. 
Similarly, if one is convinced that N does not matter except through Y, one will 
accept 
a +b[B + b.Y. 
In either case, we need to report (a) that this choice was a matter of judgment, 
and also whose judgment it was, and (b) that data with Y = N + B can never 
shed any light on the adequacy or wisdom of this judgment. 
Of course, if all the investigator wanted was a fit to the data, either of 
these two latter choices gives the same fit as the fit (with its indeterminacy of 
coefficients) of 
a + bB + bN + byY. 
Here no problem arises until the moment when we begin to think about the 
coefficients, something that is often hard to avoid. 
13D. The Relative Unimportance of the Exact Carrier 
We have seen that, in reporting a regression coefficient, it is not enough to 
report its carrier. If we are to understand what contributes to the coefficient of 
the carrier, we must know the carrier's costock also. It is now time to go 
further. 
Consider two investigators. One fits, by least squares, 
y --- a + bx + ct + du. 
The other fits, by least squares, 
y --- A + Bx* + Ct + Du, 
where 
x* = kx + a linear combination of (1, t, u). 
How will their b's and B's compare? 
316 /13: Woes of regression coefficients 
If they do this for only one set of data, which we call a situation, they will 
have 
b = kB, 
as we see by substitution. If we don't know k, and we rarely do in social and 
economic problems, we have trouble in relating the two reports. 
If they do do this in a whole variety of situations: 
situation m , situation (2) , ..., situation (H) , 
finding 
b() b (2) b() 
, , ' � �  , 
and 
B () g (2) 
,  � � � , , 
then we will have 
b(O= kB o) 
with the same k for all i. This means that 
b(X) b (2) b (-I) 
B() B (2) B(H) � 
Thus, except for a scale factor, the b's tell us just the same things about the 
various situations that the B's do. 
If our purpose is comparative- and it almost always is, ultimatelywit 
doesn't matter much whether we use x or x*. What really matter are: 
1. the costock of x (or of x*), and 
2. the whole stock that is fitted. 
(They were the same for 1, x, t, u as for 1, x*, t, u.) 
Except for scale factors it is exactly the costock (all of the fit that doesn't 
involve the coefficient of interest) and the whole stock (the collection of all fits 
considered possible) that matter in the result, not the specific carrier used. 
13E. Proxy Phenomena 
Suppose x2 makes a serious contribution to a regression equation- to reducing 
the residual variance. Suppose further that x: is highly correlated with x:, so 
that 
X2 '- �X22 
for some e. (The presence or absence of a constant term can be important, but 
introducing it does not add any essential new features to the discussion.) Then 
we can account for nearly as much variance by putting 
b2(ex22) 
in the regression as we can by putting in 
b2(x2). 
(13D) 13E: Proxy phenomena 317 
This is an inevitable consequence of the close correlation and is not at all 
affected by the presence or absence of an established or believable relationship 
between x2 and x22. 
In such a case we often say, especially when x2 is part of the regression but 
x.2 is not, that 
X 2 is a proxy for x22. 
Sometimes this is good, but often it is bad. 
Channeling through a proxy. If x2 and x2: are closely correlated, if x2 is not 
related to what we are studying, if x.. is quite strongly related, if x2 IS in the 
regression, but x:: is NOT in, then we are likely to find x: carrying an 
appreciable part of our regression. When this happens, we are tempted to 
believe that x: is "relevant," but a more appropriate interpretation would be 
that "x2 appears relevant because it is a proxy for x::, which I am sure ought to 
be relevant because..." 
A very simple example comes from geometry. Suppose that, without 
knowing the actual structure of our problem, we actually are dealing with 
squares whose sides are x: = 4, 5, 6 (unknown to us) and we are interested in 
estimating circumferences which here are y = 16, 20, 24, respectively. If, as 
might happen, we had a variable "related to x:," namely x  x22 = 16, 25, 
36, respectively, then we could get the least-squares regression line through the 
origin of 
y = 0.745x:: 
with circumference estimates of 12, 19, 27. (Had we allowed a constant term, 
_ X 2 
we could do much better using x::.) Yet x2:  does not even have the right 
dimensions for the problem we are working on. We ought to have a term linear 
in x:, not a squared term. The coefficient is absorbing the dimensional problem. 
But x2: is strongly correlated with x2 here. 
We may often have to face circumstances similar to this one. 
Distraction by a proxy. If, on the other hand, we include both b2x2 and b22x22 
terms in our regression, where x2 is not related to .y but x22 is, we have a 
tendency toward dilution or distraction, just as in the chest-and-heart- 
measurements example, Section 12D. Both 
b2x2 and b22x22 
terms will appear in the fit, and both will be serving essentially the same 
purpose as far as numbers go, so that 
b2 + eb22 
plays the role that we would have assigned to b2 if only we had been able to get 
the necessary insight and data to leave x22 out. AS a result, b2 may be smaller, 
perhaps far smaller than we would have wished, and perhaps even of the wrong 
sign. 
318 /13: Woes of regression coefficients 
Extreme distraction. A bad case of distraction occurs when x22 is so competi- 
tive as to reduce b2 essentially to zero. This may not happen too often with one 
x2., but when there are several variables--say x2., x3, x4,..., x27---each of 
which is highly correlated with x2, we are almost sure to come close to 
knocking b out of the ring. (Again, we can even make b go through zero and 
come out with the wrong sign.) A single x.2 can also make b come out with 
the proper sign, but twice too large. 
More variables, in general All the complexities we have ascribed to high 
correlation of two x's can arise when there is weaker linear dependence of 
three or more variables. Suppose, for example, that u, v, and w are indepen- 
dent, conceptually and statistically, and that 
x = u + v + smidgen, 
x2 -- v + w + smidgen2, 
x3 = w- u + smidgen3, 
where "smidgen" represents something quite small. Then x:- x is very 
highly correlated with x3, and the same sort of difficulty cannot help arising. 
Example. During World War II, in investigating aiming errors made during 
bomber flights over Europe, one of the research organizations developed a 
regression equation with several carriers. Among its nine or so carriers were 
altitude, type of aircraft, speed of the bombing group, size of group, and the 
amount of fighter opposition. On physical grounds, one might expect higher 
altitudes and higher speeds to produce larger aiming errors. It would not be 
surprising if different aircraft differed in performance. What the effect of size of 
group might be can be argued either way. But few people will believe that 
additional fighter opposition would help a pilot and bombardier do a better 
job. Nevertheless, amount of fighter opposition appeared as a strong term in 
the regression equation -the more opposition, the smaller the aiming error. 
The effect is generally regarded as a proxy phenomenon, arising because the 
equation had no variable for amount of cloud cover. If clouds obscured the 
target, the fighters usually did not come up and the aiming errors were 
ordinarily very large. 
13F. Sometimes x's can be "Held Constant" 
We have been careful to point out using x and t = x2--that it does not 
generally make sense to try to interpret the coefficients of x in terms of what 
"would happen if the other x's were held constant". In this section, we try to 
go ahead a little, sounding a few of the most necessary warnings. 
(13E) 13F: Sometimes x's can be "held constant" 319 
Polynomial fits. When it comes to fitting polynomials, whether as simple as 
bx + b2x 2 
or as complex as 
Do + bxx + b2 X2 q- b3 X3 q- b4x 4 q- bsx s, 
it rarely pays to try to interpret coefficients. Pictures of the fits or of the 
difference in two fits to two sets of data--can be very helpful, but the 
coefficients themselves are rarely worth a hard look. 
Unrelated x's. If the x's are not closely related, either functionally or statisti- 
cally, we may be able to get away with interpreting b as the "effect of xi 
changing while the other x's keep their same values." If we want to tap expert 
judgment about the value of bi, some set of words like those in quotes may be 
the best we can use. 
In practical or policy situations, however, we need to recognize how large 
a difference there can be between: 
1. x changing while the other x's are not otherwise disturbed or clamped, 
and 
2. changing xi while holding the other x's fast. 
Such differences are not only possible but likely in social and economic 
problems, because the x's we are working with there are usually neither the 
most fundamental variables in the situation nor the complete set of variables. 
Consider the example of performance on tests of cognitive achievement as 
related to parents' education, socioeconomic status, and years of schooling. 
Note that we have no measures of innate intelligence, attention paid in school, 
parental or teachers' encouragement, or even hours spent on the subject matter 
being tested, to say nothing of physical handicaps. Our regression works so 
long as x's and y's together are driv_en by the fundamental variables acting as 
they. had acted., at particul.ar. times and places, before we collected the data. If 
we interfere wth their activity, we are likely to change the regression and find 
that the effect of the change cannot be predicted by either of the two 
regressions listed above. 
Holding all but one fixed and changing that one, if we can indeed do this, 
is likely to interfere with the underlying pattern of variability and covariability, 
thus changing the regression. In most policy situations this danger is very real. 
Multicollinear carriers. We have already explained that the only hope in 
dealing with sets of carriers where certain linear combinations are nearly 
constant is to change our coordinate system, bringing in, on the one hand, 
carriers whose coefficients we can assess reasonably solidly, and on the other 
hand carriers that we can say little about. The coefficients of the latter carriers 
scarcely need interpretation. 
320 /13: Woes of regression coefficients 
The former carriers often have a general interpretation about what.must 
be constant; thus, looking ahead, in the wide-receiver example in Section 15E 
we might well be able to understand the consequences of one wide receiver 
being 10% bigger than another in all lengths. The coefficient of x2...9o, 
however, would not always measure such a thing. In Section 15E, all three of 
X123...90 , X567890 , and x79o would increase by 10% under such a change, so 
that b23-..90, b567890, and b789o would all contribute to the fitted effect of a 
general 10% increase in size. 
13G. Experiments, Closed Systems, and Physical versus Social Sciences, with 
Examples 
George Box has [almost] said (1966): "The only way to find out what will 
happen when a complex system is disturbed is to disturb the system, not merely 
to observe it passively." These words of caution about "natural experiments" 
are uncomfortably strong. Yet in today's world we see no alternative to 
accepting them as, if anything, too weak. 
Regression is probably the most powerful technique we have for analyzing 
data. Correspondingly, it often seems to tell us more of what we want to know 
than our data possibly could provide. Such seemings are, of course, wholly 
misleading. Some examples of what can happen may help us all to understand 
Box's point, which covers these examples as wed as many others. 
First, suppose that what we would like to do is measure people (or items) 
in a population and use the regression coefficients to assess how much a unit 
change in a background variable (say x x) will change a response variable (say 
y). Since the regression coefficient of x depends upon what other variables are 
used in the forecast, we cannot hope to buy the information about the 
quantitative effect of x so cheaply. These remarks do not deny the potential 
use of forecasting the value of y from several variables x, x2, and so on, in the 
population as it now exists. What they do cast grave doubts on is the use to 
forecast a change in y when x is changed for an individua. 1 (class, city, state, 
country), without verification from a controlled trial making such a change. 
(Strictly speaking, but unrealistically and impractically, if we want to verify 
what happens when only x changes, the controlled trial should be made so 
that x x changes and there is no chance for the other variables to change the 
way they naturally would when the underlying variables are manipulated to 
change x. This sort of study may not be feasible, and it may not yield what we 
need to know. We ordinarily want to know what will actually happen when we 
change Xl.) 
When such issues are raised, proponents of observational studies plus 
regression analysis are likely to cite the physical sciences for illustrations of the 
success of the method. 
The idea that such regression-as-measurement methods are successful in 
the physical sciences is seriously misleading for a variety of reasons. First, because 
13G: Experiments, with examples 321 
so many physical-science applications of regression-as-measurement are to 
experimental data. And second, because the relatively few useful applications 
that remain involve systems in which "the variables" are: 
o few in number, 
o well clarified, and 
o measured with small error. 
Example 1. Heat capacity. When an engineer fits 
A + BT + CT 2 + DT 3 
to data on heat capacity of a well-defined substance as a function of tempera- 
ture, for example, he or she takes advantage (1) of knowledge that only 
temperature is involved, (2) of knowledge that temperature and only tempera- 
ture is changing, (3) of many decades of work that have gone into the definition 
of the temperature scale, and (4) of measurement precision that is very high, 
compared to the changes in T. Analogs of any of these four supports, to say 
nothing of all four, are not common in social, economic, or medicosurgical 
analyses. 
Some researchers who have a keen appreciation of certain of these 
difficulties, try to use regression in a more qualitative way. They admit they 
cannot really use it to measure the size of the effects involved, but they hope 
that they can use it to show that some effect or other actually exists. They, too, 
are likely to fail often without knowing that they have failed. 
Why might they fail? We raised this point in the discussion of the example 
of expectations in integrated housing, where we noted that previous expecta- 
tions (nearly the response variable) may have partly caused the experience or 
lack of it with such housing. Let us say it more generally. If we fear that what 
we measure with error as x2 may also contr. ib.ute to the "causation of y" 
alongside what we measure with error as x, t s not enough to do multiple 
regression of y on x and x2. The coefficient of x may be reliably different 
from zero when what we measure with error as x has no effect on y 
whatsoever (see Tukey, 1973 and 1974). 
Let us illustrate this paragraph further. 
Example 2. Child development. Suppose we are studying some aspect, y, of 
children's development as a response to some out-of-home variable, x, and we 
want to separate any response to x from the varied responses to home 
atmosphere, either material or mental. First, we choose as appropriate a 
measure of home atmosphere as we can, call it x2, and carry out multiple 
regression of y on x and x2. If we still find that x clearly contributes to the 
fitting, should we believe that "even with home conditions held constant" x 
contributes to y? 
Regrettably, we cannot be confident. First, while we may have used much 
wisdom and insight in choosing our x2, only a limited number of measures of 
322 /13: Woes of regression coefficients 
home atmosphere could be considered for measurement, so that, at best, the x2 
we choose is an imperfect measure of the aspect of home atmosphere that 
matters. And, second, x, whose effect we are probing, and x2, which falters 
under the burden of carrying all relevant home atmosphere, may turn out to be 
correlated in the population our data comes from. Under such circumstances, 
a substantial coefficient for x does not ensure that x has an effect "with home 
atmosphere held constant." 
(Part of Section 13G above has been adapted from Gilbert et al., 1977.) 
Example 3. Long-run rs. specific educational gains. At the risk of over- 
explaining the point, suppose that the performance on a test in some units is 
approximately related to the amount of education (measured in school years), 
x, one has had and to the amount of special training (measured in weeks), x2, 
one has had in the following way: 
y = 6 + x + 2X2. 
The coefficients 1 and 2 of the carriers x and x2 might be used to estimate the 
average value of additional years of schooling or of additional weeks of special 
training. This sort of use is one that those social scientists are likely to want. 
When the original observations come from an experiment, such an in- 
terpretation may be valid, but when the interpretation comes from an observa- 
tional study, it is not so likely to hold. For example, let us suppose that we have 
looked at a population of individuals having various values of the x's and that 
we have related their y's to the x's as we have described here. We are justified 
in saying that, on the average, people with two more years of schooling score 
two units higher on the test, and that three more weeks of special training go 
with people getting 6 more units in the performance test. 
What we are not so justified in saying is that if we took a random person 
who had, say, 9 years of schooling and gave him 2 more years, he would, on 
the average, gain two points on the test. This is not because of the lack of 
reliability in determining the equation. We are thinking of this additional 
schooling being given to many people. The difficulty arises because the data are 
not based on an experiment. It may be that the reason the people with 9 years 
of schooling didn't get more schooling in the first place is part of the reason 
that they did not score higher on the test. And if they are somehow given more 
schooling, we may not get the results implied by the equation. When we do an 
experiment, we may be able to apply the change in treatment to all the kinds of 
people we have in mind, or restrict our prediction to the kinds of people who 
are studied. A classical example is the Salk vaccine experiment where volun- 
teers were more likely to contract polio than nonvolunteers. An observational 
study therefore would show the effectiveness of the vaccine as less than its 
actual worth if outcomes of volunteers all vaccinated were compared with those 
of nonvolunteers all unvaccinated. 
In spite of this danger, the interpretation of the effect of changing the 
variables is very frequently made, and with little warning for the reader. The 
13G: Experiments, with examples 323 
idea that increased schooling probably increases the score is likely correct, 
though not because of these data. (For a serious attempt to prove this see 
Hyman, Wright, and Reed (1975).) Our general knowledge of the world may 
well be worth much more than the data. In particular we often can't count 
much on the magnitude of the fitted regression coefficient. 
In physical science, we may be just trying to get an expression for an 
approximate law relating y and the x's. The coefficients may also have some 
special meaning there, such as the acceleration of gravity. 
In physical science we often are in a position where we think we know the 
variables that "cause" the dependent variable. For example, once we know the 
radius of a circle, it is the "cause" of the area of a circle. At least, knowing the 
radius, there is no escape from the area. But in many social problems, where 
the causes are both multiple and entangled, if indeed there are identifiable 
causes, it is hard to choose the variables. 
Example 4. Poverty and birth de[ects. What are the variables that "cause" 
poverty? To get at this, one has to have some sort of view of an economic or 
social process. Or to take another tack, what is the cause of birth defects? We 
have to pick and choose among variables. One view is that it is some problem 
in the biological process leading to the birth of the baby. That is a good causal 
approach for a biologist. It might also do for a medical student until he begins 
to say something like "better prenatal care would have reduced the proportion 
of birth defects in this class of individuals." Now inadequacy of care becomes a 
cause. This groping for causes is a process that is endless, and one person's 
causes are another's consequences. 
Example 5. Measuring health. When we measure health for groups of people, 
we might use death rate, morbidity rate (days of illness), and amount spent for 
food, as a few relevant variables. We could also use last year's measure of 
health for the same individuals to help measure this year's health. After all, 
what "caused" much of this year's health was the state prior to this year. If we 
put all these variables in the equations to forecast health, it may be hard to 
interpret the coefficients since all these variables are intended to estimate the 
same thing. For the purpose of estimating fitness, as appraised by the physician, 
it still may be all right to use all these variables, but we may not have much of 
an interpretation appropriate for any particular coefficient. Once we see that 
the matter has this fuzziness, we also see that, when we try to insert variables 
to describe the causes, we are likely to have several variables that measure 
much the same thing. 
Example 6A. Egg data with lull linear model As a way of rounding out our 
discussion of the more exact parts of the sciences, begun by our discussion of 
quadratics, let us return to physical measurements for a further illustration. 
A. P. Dempster measured the diameters (long and short) of some hen's eggs and 
their volume V with a view to estimating the relation between these measures. 
324 Exhibit 8/13: Woes of regression coefficients 
If an egg were an ellipsoid of revolution, we would expect its volume to be 
V = kLW 2, 
where L is the long diameter, W the diameter of the largest circular cross 
section (how do hens make eggs circular in cross section?), and k a constant, 
� r/6. Dempster got the diameters with calipers and the volume in the same way 
you or Archimedes would have measured the volume of a lobster. The data 
appear in Exhibit 8. Dempster fitted the equation 
KV = cL � 
by taking logarithms to the base 10 and then using least squares. Thus he fitted 
(letting v = log KV, x = 1, x2 = log L, x3 = log W, K = 6/r), 
1) '-- 1' l -}-' /2X2 q- 3X3, 
where log c = /3x. 
If his measurements were exact and the ellipsoid law were, too, then 
/3 = 0, /32 = 1, and /33 = 2. The actual fit he got was approximately 
v = 0.320 + 0.728x2 + 1.812x3. 
Exhibit 8 of Chapter 13 
Data for the egg example: x2 = log L, xa = log W, v = log (6V/w); logarithms to 
the base 10. 
X2 Xa V 
I 0.7659 0.6360 2.031 
2 0.7353 0,6198 1.982 
3 0.7416 0.6280 1.995 
4 0.7600 0.6280 2.019 
5 0.7861 0.6239 2.031 
6 0,7539 0.6156 1.956 
7 0.7747 0.6156 2.007 
8 0.7718 0.6239 1.995 
9 0.7889 0.6114 1.995 
10 0.7659 0.6072 1.995 
11 0,7689 0,6156 1.995 
12 0.7478 0.6239 2.007 
S) SOURCE 
Dempster, A. P. (1969). Elements of Continuous Multivariate Analysis. Reading, Mass: Addison-Wesley; 
p. 151. 
Reproduced with the permission of the author and the publisher. 
13G: Experiments, with examples 325 
This result does not look very close to 
V = 0 + lx2 q- 2X3, 
but both equations fit the data fairly well, just as our experience with quadra- 
tics might lead us to expect. Because x2 is near 0.75 and x3 is near 0.62, the 
contribution of xx (= 1) to v is worth, on the average, about l�x2 or l�x3. Also, 
the three coefficients add up to 2.860, or nearly the correct 3. One way to get 
closer to 3 would be to add about half of 0.320 to the righthand side. Thus a 
roughly appropriate sort of trading-off is going on. This helps prediction, but 
leaves it devilishly hard to get the correct constants. This illustrates that even in 
a physical-science example with a closed system and an almost correct form of 
relation, the coefficients can appear to be far from the "true" values. (The 
values found are, however, NOT significantly different from 0, 1, and 2.) We 
say almost correct and put quotes around true because eggs are ovoid rather 
than elliptical in one cross section, and this might make a little difference. 
(Historians say that this difference between an ovoid and an ellipse held the 
astronomer Kepler up for several years.) 
Example 6B. Egg dam with ellipsoidal model Had we set/J to its true value 
for the ellipsoid, namely zero, we would have fitted the equation 
3 = 0.858x2 + 2.168x3. 
This result looks a bit more like 
V = lx2 + 2x3, 
and we note that the coefficients add to 3.026, extremely close to 3. 
Not all examples turn out to be contrary in their results: 
Example 7. Steam consumption, weather, and operating days. Draper and 
Smith (1966) give an example of steam consumption in a factory. 
y = number of units of steam used per month, 
x8 = average atmospheric temperature for month in degrees Fahrenheit, 
x6 = number of operating days per month. 
Their fitted equation was 
y = 9.1 - 0.0724x8 + 0.2029x6. 
Note that the coefficient of x8 is negative, which agrees with our expectation. If 
the outside temperature is higher, we need to generate less heat. If the factory 
runs more days, it is likely to use more heat, so we expect the coefficient of x6 
to be positive, as it turned out to be. If x8 = 60�F and x6 = 20 days, then the 
estimated y is 8.8. The signs are sensible, but the sizes are uncertain. 
326 /13: Woes of regression coefficients 
Example 8. Yield, Jertilizer, and rainfall. Wonnacott and Wonnacott (1969) 
give an example of yield of wheat: 
y = yield in bushels/acre, 
x = fertilizer in pounds/acre, 
z = rainfall in inches, 
which gives 
y = 11.33 + 0.0689x + 0.6038z. 
Both coefficients are positive, which we expect as long as we don't overdo the 
fertilizer and as long as the fields don't become too soggy. The values of the 
variables used to produce the equation were in the ranges 
y: 40 to 80 
x: 100 to 700 
z' 32 to 37 
But we illustrate again the contrariness of many social-science examples. 
Example 9. Pupils' achievement, home, school and teacher variables. A sample 
of information on 20 schools from the northeast and middle Atlantic states 
drawn from the population of the Coleman Report is tucked into the computer. 
Its 5 variables are: 
y = verbal achievement score (6th graders) 
X l = staff salaries per pupil, 
x2 = 6th grade % white collar (father), 
x3 = SES (socioeconomic status), 
x4 = teachers' average verbal scores, 
xs = mothers' average education (1 unit = 2 school years), 
and the regression that comes out is 
y = 19.9 - 1.79Xl + 0.0432x2 + 0.556x3 + 1.11x4- 1.79xs. 
The coefficients of x and x5 are unexpected, certainly in sign and probably in 
magnitude. 
When we have lots of variables, especially variables competing to measure 
the same thing, as variables 1, 2, 3, and 5 all do, it is very hard to interpret the 
coefficients. One way around this is to try to make one variable out of all the 
variables supposed to measure the same thing, here some combination of 
socioeconomic status and interest in schooling. 
Here the SES variable x3 is already an attempt to weight together several 
economic variables. It might be wise, alternatively, to use it and x4 without the 
other variables. 
13G: Experiments, with examples 327 
The general position is roughly as follows: 
 when we have several variables, all trying to measure the same thing, they 
are likely to be highly correlated; 
owhen this happens each is a proxy for all; 
 if they are expressed in equivalent units, the sum of their coefficients is 
fairly well determined, though no individual coefficient is; 
 the data are not going to tell us what linear combination will serve us 
well in particular, they are not going to tell us which of these closely 
related variables is important; 
 so we will do better by using good judgment in condensing these variables 
into a composite, and fitting one regression coefficient for the composite and 
other regression coefficients for other variables, if any. In doing this, we 
would give no detailed attention to the response and certainly no detailed 
attention to the apparent relationships of these variables to the response. 
Sometimes, especially in borderline cases, we want to take this approach a 
stage further, as follows: 
o pick a judgment composite of the j constituents (essentially without 
regard to the response or to their relationship to it), 
o find the residuals after regressing each constituent variable on the 
composite--this gives us j residual variables; 
o study these residual variables, particularly in their relation to measure- 
ment errors, known, evaluated, or guessed (if only a few percent of their 
values are large enough to be clearly not routine measurement error, we will 
have to do what we can to assess why the few are large--was it unusual 
measurement error, or are these unusual individuals? only in the latter case 
will such almost-small residual variables be worth using); 
o put into a regression (a) the composite and (b) those residual variables 
found worthy of further use. 
When we do this, even the surviving residual variables will tend to be 
small--and their small sums of squares will be reflected in large estimated 
variances for their coefficients regrettable, but inevitable, and a proper reflec- 
tion of what we do not know. This sort of approach saves what information we 
have about which variables among this group seem important. Occasionally we 
are able to learn a lot this way. 
Just dumping in a lot of closely correlated variables, and expecting a fit to 
the data to tell us, directly and simply, which one or ones are important usually 
expresses unjustified optimism. Any appearance this approach produces has a 
good chance of being misleading. It is far better to know what "these data 
cannot tell us" rather than erroneously to believe the results when they seem to 
have told us more than they actually can. 
328 /13: Woes of regression coefficients 
13H. Estimated Variances Are Not Enough 
An easy way to try to pass off the woes of regression coefficients is to say: "But 
I always calculate, and report, standard errors for my regression coefficients; 
surely that is enough to protect me!" We are about to learn, on the contrary 
that 
o in many of these problems, a standard error is no help; 
o in many others, standard errors do too much. 
This "easy way out" doesn't work. 
External proxies and correlation among variables measured with error. Two 
troublesome situations arise when' 
o a carrier is really performing as a proxy for a variable not present in the 
regression [example of 13E, 3 of 13G (this could fall here or below), and 
4 of 13G]; 
o two carriers measure with error two correlated variables of primary 
interest [examples 4 of 13C, 2 of 13G, 3 of 13G (could fall here or above), 5 
of 13G, and 6 of 13G]. 
In either situation, having even an arbitrarily large amount of data does 
nothing to resolve the difficulty. Accordingly, standard errors, which tell us 
how close our finite-data numbers come to their infinite-data ideal, cannot be 
expected to show us the indeterminacies and uncertainties we face. To avoid 
these kinds of troubles, if this is possible, we require some means other than 
getting more data. And we need warning from something other than such 
measures of statistical uncertainty as standard errors. 
Proxies within the ]it. In a third important situation, enough data would 
resolve our dilemmas, so that measures like standard errors have a chance to 
help us. (Example 9 of 13G probably falls here.) The simplest case arises when 
two carriers xt and x2, say, are highly correlated. This results in bt and b2 being 
poorly determined, but some combination of them being much better deter- 
mined. If x and x2 are in equivalent units, then b + b2 will be well deter- 
mined if they point in the same way, while b- b2 will be well determined 
when they point oppositely. (Large negative correlations make exactly as much 
trouble as large positive ones of the same magnitude.) 
If we only look at 
the estimated variance of bl 
and 
the estimated variance of b2 
in such a situation, we will see only two very large numbers. Unless we look 
* This section may be skipped on a first reading. 
13H: Estimated variances not enough 329 
further, say at the estimated covariance of b and b2, we will receive no 
warning that at least one linear combination of b and b2 is well determined. 
When we have only two carriers, looking at 1 covariance, as well as 2 
variances (for the coefficients) is quite possible. With 5 carriers, though, it is 10 
covariances for 5 variances, and for 10 carriers it is 45 for 10. It is desirable to 
have some better way of detecting what is going on. 
Beaton and Tukey (1974) have proposed one way to do this, first finding 
and reporting the c's such that each of 
b + c262 + c363 + ''' + Clkbtc, 
c2b + b2 + c2363 +''' + c2tbtc, 
ckb + ck2b2 + c363 + '" + bk 
has as small an estimated variance as possible and then also reporting what that 
minimized estimated variance is. 
If, for example, a calculation showed that the original coefficients, 
minimized-variance residuals, and their estimated variances are: 
Linear 
oefficient rariance combination Qariance 
b (47.2) b - 0.1lb2- 0.0763 (44.0) 
b2 (3.0) -0.04b + b2- 0.0lb3 (2.9) 
b3 (25.3) -0.08b - 0.2362 + b3 (21.4) 
we would feel that things were almost perfect, so far as dependence interfering 
with the interpretation of variances is concerned. 
If, on the other hand, we found the following: 
b (47.2) b - 0.1lb2 + 1.3763 (1.54) 
b2 (3.0) -0.06bx + b2 - 0.0363 (2.7) 
b3 (25.3) 0.67b -- 0.0862 + b3 (0.47) 
we would recognize that b + 1.463 is relatively very much better determined 
than either b or b3 (because x and x3 are closely correlated). Triple-. and 
higher-order dependences between the b's (which express closeness of the x 
to planes, etc.) will also be diagnosable if this approach is used. 
To do a good job of detection, we need to take matters a step further. 
After having found the 
bi + cijbj 
(where E' indicates summation over all j  i), which minimizes the estimated 
variance, we would like to drop as many terms out of the summation as we can 
(readjusting the c's for the others freely, of course) without much raising the 
estimated variance of the result. 
330 /13: Woes of regression coefficients 
For such a purpose, the "backwards" techniques to be discussed in 
Chapter 15 seem quite suitable. How far should we go? Two standards suggest 
themselves, one more conservative, one more liberal, namely: If V = 
est var {b} and Vmi n --- estvar {b + ' cibi}, we agree, 
If Vmi n < to allow increase up to Vmxn; 
K ' 
Vn V- Vmi V- Vmi 
If .< -< Vmin, to allow increase to � 
K K K ' 
V- Vmi n Vmin Vrnin 
If _< to allow increase to 
K K ' K ' 
where K - 10 for a conservative rule and K = 5 for a liberal one. Note that the 
rule can be written' 
Allow an increase of up to the median of 
This allows us a reasonable increase, both when Vmi << V and when 
V -- Vmin << V. 
In our two examples above, this might lead, in the first case, to 
bl (47.2) b (47.2) 
(3.0) (3.0) 
b3 (25.3) -0.2762 + b3 (23.0) 
which tells us that the only dependence of even small-to-moderate conse- 
quence involves b3 and b2, and, in the second case, to 
bt (47.2) b + 1.3863 (1.97) 
(3.0) (3.0) 
b3 (25.3) 0.71bx + b3 (0.52) 
which really brings out the dependence between bx and b3 and the fact that an 
appropriate linear combination is well-determined. 
Comment 
Other special cases may require appropriately matched treatment, but we have 
covered the major points: 
osome kinds of problems are not at all touched by standard errors; 
o in other kinds, standard errors, taken alone, can mask knowledge we 
have actually gained; 
o there are steps we can take routinely to have the latter situation brought 
to our attention; 
Summary: Woes of regression coefficients 331 
o we know of no mechanistic way to tackle the possibility of the former 
situation; so far we can only recommend careful and deep thought. 
Summary: Woes of Regression Coefficients 
It is not enough to know what a regression coefficient multiplies; we must know 
what other carriers are offered. (The same carrier can appear with two or more 
different costocks even when one and the same stock is presented in two or 
more different ways.) 
The coefficient of a carrier can always be judged from a simple linear 
regression, that of the response upon the carrier, provided both the response 
and the carrier have first been linearly adjusted for the carrier's costock. 
If we represent the same stock in two ways by changing one carrier while 
keeping the others the same, the change in the fitted coefficient of the 
changeable carrier is by a fixed factor, so that, if we compare results for several 
"populations," the coefficients of the initial carrier will tell us essentially the 
same story as the coefficients of the changed carrier. (All this is true so long as 
we keep the costock the same, whether or not the other carriers are kept the 
same.) 
Combining the last points, we recognize the cosrock of the carrier which a 
regression coefficient multiplies as more important than the carrier itself. 
We must be prepared for one variable---or carrier to serve as a proxy for 
another, and worry about the possible consequences, in particular, whether the 
proxy's coefficient siphons off some of the coefficient we would like to have on 
the proper variable, or whether a variable serves us well only because it is a 
proxy. (In either case, interpretation of the regression coefficient requires very 
considerable care.) 
Two kinds of circumstances need to be distinguished. There may be a large 
difference between the consequences of xi changing (i) while the other x's are 
not otherwise disturbed or clamped, or (ii) while the other x's are held fast. 
When, as so often happens, we collect data according to (i) and wish for results 
applicable to (ii), the discrepancy can be misleading. 
Internal standard errors can do nothing to warn us about any of the 
difficulties considered in this chapter. 
Looking at estimated variances of regression coefficients, without consid- 
ering covariances, can often leave us with impressions of overlarge variability. 
This leads us to the following sequence: 
o Find the modifications bi + ' qibj (where j  i) of the regression coeffi- 
cients bi whose estimated variances are least; 
o Compare the variances of these modifications with those of the bare 
regression coefficients; 
332 /13: Woes of regression coefficients 
o Ask to what extent we can simplify the expression of these modifications, 
by dropping out terms and readjusting the remaining coefficients, without too 
greatly increasing the estimated variance of the result; 
o Look at these remaining modifications as telling us about important 
dependences (in estimation) among the estimated regression coefficient. 
References 
Beaton, A. E., and J. W. Tukey (1974). "The fitting of power series, meaning 
polynomials, illustrated on band-spectroscopic data." Technometrics, 16, 147- 
185. 
Box, G. E. P. (1966). "Use and abuse of regression." Technometrics, 8, 
625-629. 
Coleman, J. S., E. O. Campbell, C. J. Hobson, J. McPartland, A.M. Mood, 
F.' D. Weinfeld, R. L. York (1966). Equality of Educational Opportunity. 2 
volumes. Washington, D.C.: Office of Education, U.S. Department of Health, 
Education, and Welfare, U.S Government Printing Office. [OE-38001; 
Superintendent of Documents Catalog No. FS 5.238:-38001.] 
Draper, N., and H. Smith (1966). Applied Regression Analysis. New York: 
John Wiley and Sons, Inc.; p. 352. 
Gilbert, J.P., F. Mosteller, and J. W. Tukey (1977). "Steady social progress 
requires quantitative evaluation to be searching," in Chapter 4 of The 
Evaluation of Social Programs (C. C. Abt, Ed.). Beverly Hills, Ca.: Sage 
Publications, Inc.; pp. 295-312. 
Hyman, H. H., C. R. Wright, and J. S. Reed (1975). The Enduring Effects of 
Education. Chicago: University of Chicago Press. 
Tukey, J. W. (1973). "The zig-zagging climb from initial observation to 
successful improvement." Frontiers of Educational Measurement and Infor- 
mation Systems (W. E. Coffman, Ed.). Boston: Houghton Mifflin; pp. 113-120. 
Tukey, J. W. (1974). "Instead of Gauss-Markov least squares, what?" In R. P. 
Gupta (Ed.), Applied Statistics. Proceedings of a Conference at Dalhousie 
University, Halifax, Nova Scotia, May 2-4. North-Holland Publishing Co.; pp. 
351-372. 
Wonnacott, T. H., and R. J. Wonnacott (1969). Introductory Statistics. New 
York: John Wiley and Sons, Inc.; p. 255. 
Chapter 14/A C lass of 
Mechanisms for Fitting 
Chapter index on next page 
In exploring this chapter, the reader may want to keep in mind that it discusses 
basic ideas in regression without giving all details of execution. In developing 
these important ideas, we exploit a rather different approach and language 
from the usual ones. If readers are not always ready to construct the functions 
or weights described here, they need not be dismayed. If, instead, they assume 
that such constructions are possible, they can use the ideas to appreciate the 
subtleties of fitting functions, particularly in several variables. The ideas 
presented can, should, and will be used for data analysis, but in this chapter 
their practical and conceptual implications, not their execution, are what count. 
Additivity is the hallmark, both of the most classical techniques of fitting 
and of the individual steps of more flexible ones' If ] is the fit to some values...y, 
and if these values had instead been z or z + y, with fitted values 2 or z + y 
respectively, then it may be that 
z+y=2+y. 
When this happens for all y's and all z's, this fit, or this step of a more complex 
fit, is said to be additive. 
If a fitting method is additive in this sense, it is possible to write it in terms 
of adding up over data sets. For if 
Y = Y() + Y(2) +''' + Y(,), 
the fits must satisfy 
y = y() + 2) +'" + 
Let us then define 
y(i>(i) = Value of y(j> for data set i 
= { y(i) when i = ], 
0 else. 
Then ](i) depends only on y at data set i, so the fit  comes about by adding up 
across data sets. 
Fitting processes that can be thought of as working by adding up informa- 
tion across data sets are important' 
obecause they are simple, 
obecause they include the classical techniques, 
obecause they can provide great flexibility (particularly by taking several 
333 
334 Index for Chapter 14 
14A Fitting lines--some through the origin 335 
14B Matching as a way of fitting 337 
14C Matchers tuned to a single coefficient--and 
catchers 339 
14D Ordinary least squares 341 
14E Tuning for ordinary least squares 342 
14F Weighted least squares 346 
14G Influence curves for location 351 
14H iteratively weighted linear least squares 356 
141 Least absolute deviations (Optional) 365 
14J Analyzing troubles 369 
14K Proof of the statement of Section 13B 374 
Summary: Mechanisms for fitting regression 376 
References 379 
14A: Fitting lines---some through the origin 335 
steps and using the result of each step to determine the way things are added 
up for the next). 
How general is this sort of fitting? Although not universal, it can be made 
nearly so. All linear least-squares fits are additive, and so is every fitting 
technique that can be reduced to least squares, in one step or iteratively. Both 
nonlinear least squares and modern techniques of resistant or robust fitting can 
be treated as iterations of appropriately changing linear least-squares proce- 
dures. 
How should we think about these kinds of fitting? This chapter offers a 
treatment using the concept of "matchers" that deals mathematically and 
heuristically with many ideas of fitting. The matching idea applies widely to 
thinking about fitting, covering all the cases just mentioned, provided we are 
willing to change matchers whenever this is appropriate as after every step in 
an iterative fit. 
The generality of iteratively modified least squares can also be hinted at by 
mentioning that we treat least absolute deviation fitting as a special case in 
Section 14I. 
14A. Fitting Lines--Some Through the Origin 
If we fit /3x to y with no constant term, ordinary least squares leads, as we 
verify below, to the estimate of /3 
Y. xy 
EX 2 ' 
Here x is called the "carrier" if the sum of several ter. ms were fitted, as in 
/x + ?x: + 8x, then x, x, and x would each be carners. (Throughout the 
chapter we shall not bother to remind the reader that dividing by zero is illegal, 
but we act as if denominators are nonzero unless the matter needs special 
emphasis.) Hats ^ over quantities mean estimates, in this chapter some kind of 
least-squares estimates. 
If the variance of each y is r: and all covariances of the y's with each 
other vanish, we have 
^  x2Er 2 0 -2 
var  -- 
(Residual variance) 
EX � 
This is the basic relation, and we can frequently reduce other variance results 
to a similar form. 
If we fit a +/3x to y, which means taking I and x as "carriers" (a. 1 + 
13' x), least squares gives 
(x - 
336 /14: A class of mechanisms for fitting 
Again, if the variance of y is cr 2, and covariances of the y's vanish, we get 
Residual variance 
var/ = 
Z (x -- ) ' 
a form which suggests that fitting 
+ t3(x- 
where tx -- a + t3, is equivalent to fitting 
We now notice that, for least squares (done as if all yi have equal variances), 
the presence or absence of or /3. has no influence on our estimate of the 
other. That is, the value of in fitting 
t3(x- 
is identical to that in fitting 
+ t3(x- 
(This is also true for/3 in a +/3x, since the estimate is the same.) Furthermore, 
the value of t in fitting Ix alone, namely , is identical to that in fitting 
+ t3(x - 
though this is not true for a in a +/3x. 
The lack of influence of the presence of Ix and/3 on one another, together 
with the previous results, means that 
Residual variance 
var/J = 
E (X -- ) 
and 
Residual variance 
var t = var {y t{xi}} = , 
where {xi} stands for the given set of values of x. 
Note that in fitting Ix � 1, the carrier is 1 for each x, and that the n in the 
denominator is the sum of squares of n values, each value being 1. To 
summarize, 
owhen we fit/3x, the denominator for var/j is  x2; 
owhen we fit Ix + /3(x - g), the denominator for var/ is  (x - )2, and 
owhen we fit Ix, the denominator for var t is n (=  12). 
Leaning on our previous work (in Chapter 13), we see that the form 
t +/J(x - g) could come smoothly from taking 1 out of both y and x, thus 
fitting t and getting the residual y., and then fitting x- g to y.. Fitting 
a + /3x does not have this take-one-variable-out-at-a-time interpretation (un- 
less g = 0). 
14B: Matching as a way of fitting 337 
14B. Matching as a Way of Fitting 
Given a variety of carriers, 
the x's now being different variables, no two identical, each taking on a set of 
values, let us think of fitting 
a linear multiple regression on our carriers. Some of the x may be functions of 
others. One of the x's may be a constant; for example, x will often have the 
single value ! for all da[a sets.  
To get values /3,/32,...,/3, we need some equations. Life is simplest 
when these are linear equations in the /3's. Let 
] = + +". + 
be the fit, once we have reached it. 
Let us now lead up to a method of getting simple linear equations for the 
If we have a set of coefficients {h(i)} (where the letter i corresponds to the 
ith set of observations, i = 1, 2,..., n), we will call the set a matchef, if and 
only if it produces the iollowing equality for any data and the corresponding fit 
] h(i)y(i) =  h(i)(i). (*) 
Thus the matcher-weighted sum of the observed values is required to equal the 
matcher-weighted sum of the fitted values of the response variable. Every such 
set of coefficients that satisfies (*) for a specific kind of fit we call a 
matcher 
for that fit. 
What are some simple examples? When we fit y = /3x by ordinary 
least-squares, x is a matcher, for to require 
 x(i)y(i) =  x(i)29(i) -  x(i)x(i) = l  x(i)x(i) 
is to require 
'.x(i)y(i) = lJ  x(i)x(i). 
This is equivalent to the equation for /3 we used earlier 
Yxy 
-,X 2 - 
338 /14: A class of mechanisms for fitting 
When we fit = a + fix or, equivalently, y = t +. /3(x- g), then 1 (a 
constant), x, and x y all matchers. The corresponding conditions are for 
- g are 
12 and /J: 
1 as matcher: 
y(i) = 
ny = n(a + fi)= 
x as matcher: 
 x(i)y(i) =  x(i):(i) = & Y, x(i) + fi Y', x(i)x(i) 
=   x(i) + l  x(i)(x(i) - g); 
(x - g) as mateher: 
 (x(i)- )y(i) -  (x(i)- :)(i)--  (x(i)- :)( + [J(x(i)- )) 
= (x(i) - 
which also give the familiar results for fi, &, and fl. 
Some algebra [or marchers. Matchers come in bundles. Suppose h = {h(i)} and 
k = {k(i)} are matchefs. Then 
Y h(i)y(i) = Y h(i)](i), 
and 
Z k(i)y(i) = Y k(i)](i). 
Adding with various weights gives 
Weights 1 and 1: Y, [h(i)+ k(i)]y(i)=  [h(i)+ k(i)]](i), 
Weights 2 and -3:  [2h(i) - 3k(i)]y(i) =  [2h(i) - 3k(i)]](i), 
Weights c and cu:  [cuh(i) + ckk(i)]y(i) =  [cah(i) + c,k(i)](i). 
Thus all weighted sums or linear combinations of matchefs are themselves 
matehers. 
Our problem is twofold: 
oto get enough matchefs so that we have enough independent equations to 
solve, at least in principle, for all of our unknowns (fi, fi2,..., 
oto choose a set of matchers such that solving these equations will not get us 
into numerical difficulty--and such that the equations will be easy to solve. 
Fitting y = tlx. When fitting y = fix by least squares, only constant multiples 
of x are matchers. One is enough. Two would be redundant. Arithmetic is 
usually minimized by using 1 as the constant multiplier of x. (If x was in 
half-integers, we might like 2 or 4.) 
14C: Matchers tuned to a single coefficient 339 
Fitting y = a + fix. When fitting y = a +/3x by least squares, all. expressions 
of the form c + dx are matchers. Two will be enoughwunless one s a multiple 
of the other, and thus linearly dependent. Three would be redundant. 
But if all x's are small, trying to use 
1000000 + x and 1000001 + x 
will lead to a pair of equations so delicately related that, because of rounding 
errors or numbers of significant figures required, we will soon wish we did not 
have to solve them. Almost the same is true if 1950 -< x _< 1975 and we pick 1 
and x as our matchers. Instead, choosing x -  as a matchef leads, after a little 
algebra, to 
 Ix(i) - ]y(i) -- /J  [x(i) - ]2 (a pure slope equation), 
which is easy to solve and for which the residual form x(i)- : has already 
smoothed the computational difficulty. And using I + cx, where c = 
- x(i)/ (x(i)) 2, leads to 
Y', (1 + cx(i))y(i) = i  (1 + cx(i)) (a pure intercept equation), 
which reduces to 
ny + c x(i)y(i) = ri(n + cn) (a pure intercept equation), 
again an equation easy to solve for 5. 
If we wish to fit y = y, + /3(x - g), matters are even simpler, because 
taking 1 and (x - ) as matchers gives 
 y(i) = n 
and 
 Ix(i) - g]y(i) = /  Ix(i) - 
If we have k coefficients to fit, we need k equations--k equations that are 
not linearly dependent. This means that the family of matchers has to be at 
least k-dimensional. It cannot have larger dimension, either, because k + 1 
linearly independent equations in k unknowns would be incompatible. 
14C. Matchers Tuned to a Single Coefficient--and Catchers 
We next develop a special set of matchers that make it easy to solve for the/3's 
and that catch all the information the data have about/3. Suppose we are fitting 
y = /iX1 q- /32X 2 q-''' q- [kXk 
by a process that can be described by matchers (all kinds of linear least squares 
are included). How about a /3 found by a single matcher? If we can find a 
matchef h = {h(i)} such that 
0 = = = ... = (*) 
340 /14: A class of mechanisms for fitting 
we are in luck so far as /3 goes. For if this holds 
 h(i)y(i) = 1  h(i)xx(i), (**) 
so that 
= y" h(i)y(i) h(i)x(i) O. 
E 
We may quite properly say that h is tuned to , since /J2, /J3,...,/Jk do not 
appear in (**). As "radio stations" they are "tuned out"; they are not "heard." 
If we can tune out all but one /3, we can readily solve for it. 
We have already seen simple examples. Thus, if we are fitting a +/3x, 
.then x - i is tuned to/3 and tunes a out--while, for a very special c, 1 + cx 
xs tuned to a--and tunes /3 out. . 
More complex examples are avafiable: Suppose that x--1, x2 = x, 
x3 = x 2, that is, the three variables are the 0th, 1st, and 2nd powers of x, 
where x takes the integer values 1, 2, 3, 4, 5, 6, 7, 8, 9, 10. Then it turns out 
that 
c[22- 11x + x ] 
is tuned to /33, for any c. 
(To verify that 
10 10 
0 =  (22- 11x + X2)l =  (22- 11x + X2)X, 
1 1 
it may help to recall that 
 . n(n + 1) .x2 n(n + 1)(2n + 1) 
l=r, x =  = , 
 x3= n(n + 1 2 
If we have enough matchefs to ensure a unique solution, we will soon 
show that there is always a matchef tuned to whatever/3M interests us. For let 
h, h2, ..., hk be k linearly independent matchefs; then all we need is to find a 
set of d's to satisfy the k - I equations 
5'. (dxh(i) + d2h2(i) +'.' + &,h,(i))xs(i) = O, $  M (and 1 < S < k). 
i 
These are equivalent to (*), in that they tune out all but 
While we are at it, let us fix the common multiplying constant that would 
otherwise be free by asking that 
 (dh(i) + d2h2(i) +''' + d,h,(i))xM(i) = 1. 
i 
(14C) 14D: Ordinary least squares 341 
We need the coefficients in this set of k (= (k - 1) + 1) equations in the d's to 
have a nonzero determinant and hence for the equations to have a unique 
solution. More explicitly, we need the determinant 
... 
not to vanish. 
Put 
cM = dhx + d2h2 +'" + d:h,. 
The mateher c is not only a matcher tuned to/3, as any constant multiple of 
c would be, it is more; it is a 
catcher 
for/3M as we soon explain. When we match y and  using this matchef, we get 
cM(i)y(i) = 13' 1, 
which is 
[J =  c(i)y(i). 
In view of this relation--and the nature of cM depending only on the x's 
and not at all on the y's we have 
vat/J =  (c(i)) 2 var y(i) 
= cr 2  (c(i)) 2. 
The last form holds provided that all vat y(i) = cr 2. 
Accordingly, all that the data are allowed to tell us about/3, when fitted 
to satisfy a set of matehers in the presence of the other x's, is to be found from 
a listing of the pairs of values: y(i) and c(i). Thus c catches all the 
information about /3. Knowing a catcher reduces finding /3 to a one- 
parameter regression problem. (We will reduce it to an even more helpful 
one-parameter regression problem in Section 14E.) 
14D. Ordinary Least Squares 
We now show that when xx, x2,..., x are chosen as matchers for the fit, they 
produce the least-squares fitthe fit that minimizes ] (y - ])2. 
Suppose now that we are fitting 
y = /3xx + f12x2 4-...-{-- flkX k 
342 /14: A class of mechanisms for fitting 
and that we have--so far more or less arbitrarily ...chosen x x, x2,..., xk as 
matchefs for our fit. This means that we have also chosen 
gxxx + g2x2 +''' + gkxk 
for any {gi} not all zero, as a matcher because as we have seen, linear 
combinations of matchefs are again matchers. (We use the subscript j here 
because i is already busy, being used for the ith set of observations.) If  and  
are two fits of this form. however chosen then each is also a matcher for our 
fit, and so is y -, because. , , and  -  are each a linear combination of 
matchers. Recall that choos.ng a set of t3's chooses a fit. 
We now let ] be a special fit, the one obtained with our selected matchers 
x, x2,..., xk. Then, 
y - + (y - 
whence, as is easy to check, 
The middle term on the right is, setting the factor of 2 aside for a moment, 
which vanishes because ] -  is a matcher for the fit of . ( - ] could be used 
to help determine  in terms of y.) 
Thus 
and, since the first term on the right is positive or zero, 
so that  makes the sum 
 (observed - fitted) 2 
as small as possible. Hence the term "least squares" for the fits 
in which all 
gX + g2X2 + ''' + gkXk 
are matchers. 
14E. Tuning for Ordinary Least Squares 
In multiple regression when we are good at predicting xi from the other x's, 
then the variance of /3 will be large, because its denominator is the residual 
sum of squares of xi - i, that is, 
2 
var/3 = 
 (Xi -- i)2, 
14E: Tuning for ordinary least squares 343 
where ii is the least-squares estimate of xi based on the other x's and cr 2 is the 
common variance of y. We use the method of matchers to prove this in this 
section. 
^ 
We also show that all the information about /3i is contained in the 
combination of (a) y residuals after taking out of y the costock of xi and (b) the xi 
residuals after taking out of x the least-squares fit of the costock of xi. 
Estimating xi. If we are in the ordinary least-squares case, fitting 
y = /3 x +/32x2 + � � � +/3x 
with x, x2,..., xk and all their linear combinations as matchers, we can take 
z = x, and fit 
Z --' 'Y12X2 "{"'Y13X3 -+' ' ' ' '+- lkXk 
by ordinary least squares. We can take x2, x3,..., x as the working matchers 
for this fit, so that the conditions on the fit are 
 xj(i)z(i) =  xj(i)(i) for J = 2, 3,..., k, 
which can be written 
 xx(i)(z(i)- (i)) = 0 for J = 2, 3,..., k 
or 
 xj(i)(xx(i)- '2x2(i)- '13x3(i) ..... 'xkxk(i)) = 0 
for J = 2, 3,..., k. 
Now let us look at the residual of x after fitting x2,..., xk, namely, 
X1-23..-k m X1  12X2  13X3 ..... "YlkXk --' X1  X1. 
(In factor analysis, x.23..., is sometimes called the anti-image of x in x, 
x2,..., x and the summation of {jxj is called the image, rather attractive 
memory aids.) The k - 1 conditions we just established are just what is needed 
to show that 
Xl.23...k '-- Xl- "}/12X2- 13X3 ..... 
is tuned to /x because when we multiply y - j/3jxj through by X1-23-.-k and 
sum both sides over i, the sums Y. xjx.23..., (for j g i) vanish, as we have 
already arranged. Thus we are left with 
' YX-23-..k = / Z X1Xl'23"'k, the tuned equation for 
How can [ arise? Since 
 X1'23'" 
E X1.23--- 
344 /14: A class of mechanisms for fitting 
and since 
X 1 = X1.23... k -[- 'y12X2 -{- � � � q- tlkXk , 
we must have 
 X1.23...kX1 '--  X1.23...kX1.23...k. 
Similarly, if we fit y by 2X 2 q- ''' n t- kXk, finding y.23...k as the residual, we 
have 
Y = Y-23---k + 2X2 '' ''' q- kXk, 
so that 
 X1.23---ky =  X1-23--. kY-23.-.k. 
So we must have 
^ E X1'23'" kY'23'" k 
/31 = 
E � 
This says that/32 can be found from the one-parameter regression problem 
involving the n pairs of values 
Y-23.-- k and x.2... k. 
Because y.23... k is likely to be much smaller in magnitude than y itself, a plot 
for this latter regression problem will help us much more than a plot of 
y against x.23... k 
or of 
y against the catcher for/3. 
(These two will look exactly alike, since the catcher for 18 has to be a multiple 
of x.23...k. Thus Y.23...k can be equally usefully plotted against either.) 
We might think that calculating y.23...k and then y.3...  (leaving out x2), 
then the residual of y after the fit leaving out x3, and so on to leaving out x 
was more work than we want to face; but it need not be, because the/3's in 
IX1 q- /2X2 q-''' q- kXk 
and in 
+ +'" 
are the same, as we can easily see by substituting in the value of X1.23... k. This 
means that 
Y-123-.- = Y-23.-. t, -- /3xXl.23... k. 
So the plot of 
Y-23 �. � k against Xl.23... k 
is just the same as the plot of 
Y-23--.  - /3xx.23.. - k against x.23... , 
14E: Tuning for ordinary least squares 345 
something that is easy to make, given the final y-residual Y.123..-k and the 
x-residual, x x.23... k. This ismthe one-parameter regression problem that tells us 
the most about estimating/3x when we are fitting/31xx + 2X2 q- ''' q- kXk-- 
not only what the estimate is but how hard each data set is working to fix the 
estimate. (See Larsen and McCleary (1972).) 
Residuals get smaller. Since ordinary least squares minimizes 
we have 
 (X1.23...k) 2   (Xl.fcwer) 2   X, 
where "fewer" stands for any proper subset of the variables 2, 3,..., k. 
Unhappiness in many fitting problems comes about when 
Z << Z 
where "<<" means "much less than". Thus when 
x x = 1, x2 = x, x3 = x 2, x4 = x  and xs = x 4 all for 
x = 1,2,3,..., 10, 
we find  (X1.2345) 2 
= 9.16 x 10 -3 
 (x.)  
= 1.87 x 10 -4 
Zx ' 
 (x.4)  
= 2.38 x 10 -s 
Zx ' 
 (x.s)  
2 = 1.70 x 10 -5, 
x4 
E (x.)  
= 9.82 x 10 -6. 
Many, many decimal places that one might have naively thought had been 
captured have to go down the drain in such situations. 
Recall that, since 
= 
  (x.... (i)) , 
the variance of B1 is 
 x 2 
.23... k(i) var {y(i)} 
va {d = 
( (x....  (i))) 
2 
 (x.,... (i)) ' 
the last provided every var {y(i)} = 2. 
346 /14: A class of mechanisms for fitting 
Step up or down. Notice also that if we write 
IX1 q- /2X2 q- ' ' ' q- kXk 
as 
lXl-23...k q- X2 q-''' q- 
the /*'s are just what we would get by fitting 
+... + 
leaving x out. This follows because (1) x2, x,..., xk are matchefs for both fits 
and (2) they tune x.2... out. 
14F. Weighted Least Squares 
In surveying and in astronomy, where least squares originated, investigators 
long ago recognized that some observations are "better" or "stronger" than 
others and took appropriate action. This action often assigned differing weights 
to different observations, either for objective reasons or as a matter of 
judgment. Thus the history of weighted least squares is almost as extensive as 
that of ordinary least squares. Today, as we shall see in Section 14H, we have 
further uses for weighted least squares, further reasons for knowing how it 
works. 
Suppose that we become more arbitrary and pick almost any nonnegative 
weights w = {w(i)} we wish, one for each data point 
{x(i), x(i),..., x(i)), 
and decide that wx, wx2,..., wxk are to be our matchefs. For example, 
w(1)x2(1), w(2)x2(2), w(3)x2(3), ..., w(n)x2(n) 
are then the values of a matcher, namely wx2. 
If, again, ] and ] are to fits of the form cx + c2x2 +' + ckxk, 
however chosen, then wy, wy, and w(y- 9) are all matchers, 'is linear 
combinations of the wxi. Let 9 now be the fit obtained with our selected 
matchers, wx, wx2,..., wx. We still have 
(y - )=- (y - ) + ( - ), 
whence 
As in Section 14D, because w(9- ) is now a matcher, the middle term 
vanishes when  is the fit fixed by our new set of matchers, the wxi's. By an 
argument similar to that at the close of the previous section, 
E w(y - ):>- Y, w(y - )', 
14F: Weighted least squares 347 
so that our new ] minimizes 
 w(observation - fit) 2. 
Unweighting. We have done weighted cases separately from unweighted 
meaning "equally weighted"ones. We did this to get into a frame of mind 
o where weights are then taken seriously, 
owhere we ask what weights to use. 
So far as proofs and mathematics go, however, we can always reduce 
weighted cases to unweighted ones. Suppose we are fitting 
y  iX1 q- 2X2 q-''' q- kXk 
with weight w. The matching equations are 
 w(i)x(i)y(i) = Y'.w(i)x(i)](i) 
for any linear combination x(i) of the xs(i). We may write them as 
 [,/x(i)][.,/w(i)y(i)] =  [4w(i)x(i)][.,/w(i):(i)], 
which corresponds to fitting 
with matchefs 
x/�x = x/w(any linear combination of x, x2,... xk) 
= any linear combination of x/-wx, x/-x2,... x/-xk), 
the last form showing that our fit of (***) is unweighted (equally weighted). 
Note that, in particular, 
wyx.... = t w(x.3...)' 
where x.23... is the residual corresponding to the varying weights w. Since 
this can also be written 
= 
the estimating formula for  can be written, not only as 
  wyx....  
= 
E w(x.... ) 
but also as 
 z (4wy)(w.:... 
= 
Z (4--�wx..... ): 
Accordingly, if we have weights, the plot of 
x/wy against 
348 /14: A class of mechanisms for fitting 
or better 
x/Wy.23 � �. t against x/WX1.23 - � � k 
has to tell the whole story. 
Tuning in weighted least squares. The device of unweighting shows us that if w 
has been so chosen that 
w var {y = r 2, 
which implies 
var {x/y} = cr 2, 
then 
2 
var/31 = 
E 
for weighted least squares, where x.23...k is the residual after fitting x with 
x2, x3,..., xk using weight w. 
A Comment 
Often we want most weights more or less the same size, interspersed with a few 
small ones--as if we had a few high-variability observations among a mass of 
others. When we do have a few high-variability observations, moving the few 
weights down to the low values they should have corresponds to facing the 
facts, to taking away "information" that really wasn't there. 
In a very extreme case, where three observations out of 100 are extremely 
variable while the others are much less variable (but equally variable among 
themselves), keeping all the weights the same will make the answers horribly 
variable, while making it seem as if we have 100 observations. Weighting the 
three zero will give good results, though we must now admit to only 97 
observations. Putting in three small weights will do a little better maybe we 
will get the equivalent of 97.13 observations. 
The three cases look like this: 
Equivalent number of equivariable observations* 
Weight for the 
three bad values We think we have We actually have 
1 100 Few (maybe 3 or 4 or even 5) 
0 97 97 
0.041 97.13 97.13 
We never really had an equivalent of 100 equally good observations. Any 
attempt to argue for 99 = 100 - ! degrees of freedom for tr 2 is a sham once 
* If the third set of weights is optimum. 
14F: Weighted least squares 349 
we know that three observations are extremely variable. Either 96 = 97 - 1, 
when we set the three wholly aside, or 96.13 - 97.13- 1, when we down- 
weight them, may be legitimate numbers of degrees of freedom. But if we leave 
the three in, we will be in much worse shape. Indeed, if they are variable 
enough, our rr 2 may be worth as little as 3 degrees of freedom, and our other 
estimates will be very bad. Large weights that should be small can burn us 
badly the results both will in fact be bad and will appear better than they 
actually are (often better than they could possibly be). 
rA Generalization (Optional) 
Occasionally we may want to be even more general. If we have a two-way 
array of "weights" {wij}, how shall we do the fitting? We might minimize 
  wii(y(i)- ](i))(y(j)- 9(j)). (*) 
i 
We can have symmetry in the weights without loss of generality, because 
replacing w# by 
' + w#) 
W ii -- 
would provide the symmetry w' i = w'#, without changing the value of the 
double sum. 
After arranging that w i = w#, let us take 
h(i) = 
h2(i) =  wiix2(j), 
hk(i) =  wixk(j) 
as the matchers defining . Further, let  be another fit of the same form 
blX + b2x2 +'" + bkx, 
that is, another choice of the /3's with the same carriers. Then 
h(i) = 
and 
are also matchers, and so, of course, is their difference 
h'(i) =  wi[(j)- (j)]. 
350 /14: A class of mechanisms for fitting 
We plan to show that the 's minimize the sum labelled (*). To do this we 
look at 
  ,[y0)- ()][y()- ()]. (**) 
i i 
As before, we write 
y -  -- ( - )+ (y - ) 
and apply it in (**) to get: 
+   w,[0)- ()][y()- (i)] 
 j (***) 
+   w[y(j)- (i)][(i)- (i)] 
+   wi[y0')- 0)][Y(i)- (i)]. 
In the second line of (***), 
 i[(i)- (i)] 
is p matchyr for  and o this term vanishes. Because we ade wi = w, the 
third line s identical wth the second, and therefore it yamshes as well. 
We need to introduce a special condition to assure nonnegativeness of the 
original expression (*). When we deal with sums of squares, the positivity 
comes on a silver platter, but weighted sums of cross-products can be negative. 
The condition on the w's we require is that 
o (****) 
for any {u}. (In matrix language, we require the matrix of weights to be 
positive semidefinite.) Frankly, this condition is not an easy one to check. If the 
w's happened to be elements of the inverse of a variance-covariance matrix, 
this condition would be automatically satisfied. At any rate, let us assume that 
our w's have the nonnegativity property (****). 
Then the first line of (***) must be 0. Our situation now is that we have 
the equality 
(**) = first line of (***) + last line of (***). 
The nonnegativity assumption ensures that all these parts are 0, but we now 
emphasize in particular that the first line of (***) is 0. When we replace it by 
zero we have 
(**)  last line of (***), 
or, written more fully, 
i j i j 
(14F) 14G: Influence curves for location 351 
which is the desired further generalization of the result of Section 14D: 
If the two-way array of weights {wj} yields a nonnegative quadratic form 
(condition (****)), then the matchers 
h,(i) =  wqx,..(i), m = 1, 2,..., k, 
provide the estimate  which is the corresponding doubly-generalized 
least-squares estimate among all fits to y of the form 
/lXl -'[- /2X2 -+''''-+- [kXk. 
14G. Influence Curves for Location 
To appreciate how a few statistics respond to data, let us see how they behave 
when one observation in the sample travels smoothly through all possible 
values. For convenience, let us consider a sample of 11 measurements in all. 
Let 10 of the measurements have the values 
10, 7, 3, 3, 3,-2,-5,-5,-6,-8, 
so that they sum conveniently to zero, and the 1 l th measurement, x, will move 
from very negative values to very positive ones. The example, although numeri- 
cal, should display the general shape, called the influence curve, of the effect of 
the value of one x on each statistic. 
A. The Mean, . 
Because the other 10 measurements sum to zero, the grand total must be x 
itself. Consequently, the sample mean 
 xi x x 
X TM --  - . 
n n 11 
Exhibit 1 shows in the top panel the straight line with slope 1/11 that indicates 
how g responds to changes in x in this example. 
B. The Median, x. 
Because the sample size is odd, the middle observation is the median, here the 
sixth from either end. The median is 
-2 when x -2, 
x when -2 < x < 3 
3 when 3 =< x. 
352 Exhibit 1/14: Mechanisms for fitting 
Consequently, the influence curve as shown in the middle panel of Exhibit 1 is 
composed of three straight-line segments, two parallel to the horizontal axis, 
and a third with a slope 1 connecting the other two. The distance between the 
parallel lines equals the distance between the two middle observations in the 
basic set of 10. 
Exhibit 1 of Chapter 14 
Influence curves. 
'[50 -to -o o o o so 
! 
o 
30 -lO -I0 0 i0 0  
I ), 
-SO -;'0 -iO 0 0 lO 0 40 
14G: Influence curves for location 353 
C. The Biweight, . 
Biweight is an abbreviation for bisquare weight. Let us weight observations 
according to 
= - iui--< 
0 elsewhere, 
with 
x - i 
cS ' 
choosing somewhat arbitrarily c = 6 and $ = � x (interquartile range). For 
simplicity we have counted in 3 from each end and taken the difference of 
these observations to get essentially the interquartile range. For distributions 
4 
near the normal, the interquartile range averages to near (r, and so 6S will 
average about 4or; this gives a way of thinking roughly about our choices. 
Finally the estimate is defined as 
' 
Consider first the value of S. To get the interquartile range, we compute 
the distance, /, between the observations 3 in from each end: 
For: 
-m<_-x < 6, =9, 
=- S = 4.5; 
3-X 
<S<4.5 S= 
-6 < x <-5, I = 3- x, 4.0 = � 
2 
-5 -< x < 3, I = 8, S = 4; 
x+5 
3 < <7, 4<S<6 S= � 
--x= I=x+5, ' 2 
7<x<+o 1'=12, S=6 
Because  depends on the weights and the weights depend on , we have 
to iterate. Exhibit 2 shows illustrative calculations used to find the curve shown 
in the bottom panel of Exhibit 1. 
Note that the bottom panel of Exhibit 1 shows a behavior that we will 
often like. As the moving observation x walks too far either to the left or to the 
right, its influence goes to zero, and we get a result based entirely on the other 
10 measurements. Approximately, x gets zero influence when x <-27 or 
x ->_ 36, and, of course, zero effect when x  0. 
These influence curves show: 
ohow the mean walks off toward +o or -o when one measurement is 
sufficiently wild; 
o how the median ignores the change in the measurement once it gets 
outside a narrow range determined by the middle measurements; and 
ohow the biweight similarly ignores the changes in the measurement 
outside a substantial range, and yet responds sensitively to it in the middle 
portion of this range. 
354 Exhibit 2(A-B)/14: Mechanisms 
Exhibit 2 of Chapter 14 
Illustrative calculations for biweight influence curve x =-21, 6S = 27, 
21 
 = - =-1.9091. 
A) First iteration 
u = w ( u ) = 
x- ) (1 - u)  
10 11.9091 .4411 .1945 .8055 .6488 6.4875 
7 8.9091 .3300 .1089 .8911 .7941 5.5587 
3 4.9091 .1818 .0331 .9669 .9350 2.8049 
3 4.9091 .1818 .0331 .9669 .9350 2.8049 
3 4.9091 .1818 .0331 .9669 .9350 2.8049 
-2 -.0909 -.0034 .0000 1.0000 1.0000 -2.0000 
-5 -3.0909 -.1145 .0131 .9869 .9740 -4.8698 
-5 -3.0909 -.1145 .0131 .9869 .9740 -4.8698 
-6 -4.0909 -.1515 .0230 .9770 .9546 -5.7277 
-8 -6.0909 -.2256 .0509 .9491 .9008 -7.2065 
-21 -19.0909 -.7071 ,4999 .5001 .2501 -5.2511 
Total 9.3014 -9.4640 
,(2)  wix -9.4640 -1.0175 
 wi 9.3014 
B} Second iteration: 2 (2)-- -1.0175 
! ! !. l", ! -u,I w, ! I 1 
10 11.0175 .4081 .1665 .8335 .6947 6.9471 
7 8.0175 .2969 .0882 .9118 .8314 5.8200 
3 4.0175 .9488 .0221 .9779 .9562 2.8686 
3 4.0175 .1488 .0221 .9779 .9562 2.8686 
3 4.0175 .1488 .0221 .9779 ,9562 2.8686 
-2 -.9825 -.0364 .0013 .9987 .9974 -1.9947 
-5 -3.9825 -.1475 .0218 .9782 .9570 -4.7848 
-5 -3.9825 -.1475 .0218 .9782 .9570 -4.7848 
-6 -4.9825 -.1845 .0341 .9659 .9331 -5.5983 
-8 -6.9825 -.2586 .0669 .9331 .8707 -6.9657 
-21 -19.9825 -.7401 .5477 .4523 .2045 -4.2954 
Total 9.3144 -7.0509 
-7.0509 
= -0.7570 
9.3144 
14G: Influence curves/Exhibit 2(C-E) 355 
Exhibit 2 of Chapter 14 (continued) 
C) Third iteration: 2<3)= -0.7570 D) Fourth iteration: 2 (4)= -0.6821 
3 .9617 2,8850 3 .9632 2,8895 
3 .9617 2.8850 3 .9632 2.8895 
3 .9617 2.8850 3 .9632 2,8895 
-2 .9958 -1.9915 -2 .9952 -1.9905 
-5 .9512 -4.7561 -5 .9495 -4.7475 
-5 ,9512 -4.7561 -5 ,9495 -4,7475 
-6 .9260 -5,5560 -6 ,9239 -5,5435 
-8 .8613 -6.8900 -8 ,8585 -6.8678 
-21 .1917 -4.0267 -21 .1881 -3.9504 
Total 9.3117 -6.3519 Total 9.3103 -6,1517 
-6.3519 -6.1517 
2 (4) = = -0.6821 2 (5) = = -0.6607 
9.3117 9.3103 
E) Fifth iteration: 2 (5) = -0.6607 
10 .7125 7.1251 
7 .8455 5.9183 
3 .9636 2.8907 
3 ,9636 2.8907 
3 ,9636 2.8907 
-2 .9951 -1.9902 
-5 .9490 -4.7450 
-5 .9490 -4,7450 
-6 .9233 -5.5399 
-8 .8577 -6.8614 
-21 .1871 -3.9287 
Total 9.3100 -6.0947 
-6.0947 
2 (6) = = -0.6546 
9.3100 
356 Exhibit 3/14: Mechanisms for fitting 
To see how such a small change in the measure of scale chosen in the 
biweight matters, we have also computed the influence curve replacing S in u 
by the median absolute deviation from the median. We get the result shown in 
Exhibit 3. It is extremely close to that shown for the biweight in Exhibit 1. 
The reader may wish to refer to an article by F. E. Hampel (1974); see 
Reference list at end of chapter. 
Exhibit 3 of Chapter 14 
Influence curves using median absolute deviation from the median as the 
measure of spread S. 
Median of 
? The median absolute deviation 
$ _ from the median / 
-30 -ZO -I0 0 I0 gO t0 40 
-o -gO -tO 0 !0 20 0 40 
14H. Iteratively Weighted Linear Least Squares 
Notation As before, we use  for anything that we could get from ordinary 
least squares, from weighted least squares, or from the generalization of 
weighted least squares discussed at the end of Section 14F. We wish to extend 
this idea to a sequence of fits where 
i) each is least squares, and 
ii) the weights change. 
As long as we are thinking of a step at a time, we continue to use ],/3j, and so 
on for least-squares results, reserving the double-hats ,/3j, and so on, for fits 
14H: Iteratively weighted linear least squares 357 
not involving least squares. For the final result of a sequence of least-squares 
fits with changing weights, we use y*, /3, and so on. We also use e for a 
residual following either a fit or a step of a fii. 
We remarked, at the beginning of the section on weighted least squares, 
that weights could be and have been used to reflect the strengths or weaknesses 
of individual observations. In this section, we learn how weights can be used 
toward a second end, essentially to ensure the kind of influence curves we 
want, and thus to ensure the high performance in estimation that goes with 
such curves. 
This secon. d kind of weighting differs f.rom the more historical use in 
technique and nterpretation though not in ultunate purposesbetter estimates. 
We can use both ideas in the same analysis, and when we do, we need to 
multiply together a weight of one kind and a weight of the other to find the 
"weight" that enters the final calculation. 
Suppose, to begin with, that the weights of the first kind are all the same. 
We recognize both the advantages and disadvantages of least-squares fitting. It 
is easy to describe in terms of matchers, is reducible to a single solution of a set 
of simultaneous equations, and is the subject of much experience and many 
computer programs (not all trustworthy, but some very safe). Or we may be 
able to afford to tune the matchefs, and so use the solutions to a set of simple 
equations. All these are very good things. On the other hand, least-squares 
fitting is relatively inflexiblethough a priori weights can and do help - and, as 
our examples show, is quite susceptible to serious perturbation of results when 
a few wild values are present. How can we have the advantages without the 
disadvantages? 
Suppose we consider: 
ochoosing a w = {w(i)} and finding the corresponding fit (); 
othen choosing a w2 - {w2(i)} which may depend on how the first fit came 
out, and finding the corresponding least-squares fit (2); 
o then choosing a w3... and so on. 
We can either go a prechosen number of steps or stop once the change in fit 
from one step to the next seems small enough. (We can think about going on to 
the limit, but we would not, in practice, wish to wear out either pencil or 
computer, to say nothing of the user.) 
The real question is how to choose the successive weights. The first set 
must be chosen as a matter of initial judgment. For the present, let us assume 
that the initial weighting is equal weighting for all i. 
A very useful way to choose the later weights is to let the ith weight 
depend on the ratio of the ith residualin the previous iteration to some 
measure of the general size of residuals in that iteration, namely on 
y(i)- ((i) 
358 /14: A class of mechanisms for fitting 
where c is a numerical constant, and .Sk is a .measure of spread for all the 
residuals left by the kth fit and (k)(i) s the estimate of y(i) produced by the 
kth fit. 
Biweighting. What dependence of w(i) on ui should we use? Again there can 
be a variety of good choices. On a computer, we like to take the bisquare 
weighting, abbreviated biweighting, 
W(U) = (1- U2) 2 U 2 < 1, 
=0 it2 ,>--- 1. 
The general idea is that deviations that are small get larger weights and, as 
the deviations grow, their weights get smaller and finally there is a rather 
smooth but complete cutoff when u >_- 1. 
We often use 
Sk = median of the iy(i) - 
c = 6or9, 
but lots of other choices work well, too. Note that Sk is obtained by 
computing the absolute value of each residual for y in the kth fit and then 
taking the median of these absolute ;values. Thus Sk measures variation, 
essentially around the middle of the data set. Then ui looks at each deviation in 
terms of this measure of variation. In particular, c determines how soon the 
weight becomes zero. The choice of c = 6 gives 0 weight to deviations more 
than 6S from the latest estimate of location. 
Example. Outlier. We can expect to do only the simplest examples by hand, so 
let us start with fitting a line through the origin. Exhibit 4 shows a plot of five 
points. Exhibit 5 applies the w(u) = (1 - u2) 2 fitting procedure. Since the fifth 
point is rather out of line with the others, the first task of the fitting procedure 
is to modify /J() = 1.13 until this point has little weight. Here this takes two 
steps of fitting, after which the successive estimates are /j(4) = 1.02, /3 (s) = 
1.00, and/(6) _ 0.99. If the reader will carry the calculation one step farther, 
he will find 0.99 again. To our 2-decimal precision the calculation has con- 
verged. 
The number of steps required in this example is more than we would like. 
Is there any easy way to deal with this problem? In our example, yes in 
others, sometimes. We can here look at Exhibit 4 and say to ourselves that the 
fifth point is out of line, so we will start with weights 1 on the first four, but 0 
on the fifth. 
Setting the weight initially to zero does not set the point permanently 
aside. A point can, in principle, regain a nonzero weight in the course of the 
iteration. Here we find /()= 29.6/30 = 0.987, which one further step of 
biweight fitting moves hardly at all. In general, we cannot expect to know in 
advance which points should get low weight, since we cannot expect so simple a 
plot. 
14H: Iterative weighting/Exhibits 4 and 5(A) 359 
Exhibit 4 of Chapter 14 
Line through the origin. 
7 
�- 
0 [ 1,,. I I I 
0 I z 3 4 
Exhibit 5 of Chapter 14 
The arithmetic of biweight fitting a line through the origin. 
A} Data and simple sums 
1 1.1 (1.0) 1.1 1 
2 2.0 (1.0) 4.0 4 
3 3.1 (1.0) 9.3 9 
4 3.8 (1.0) 15.2 16 
5 6.5 (1.0) 32.5 25 
62.1 55 
62.1/55 = 1.13 
360 Exhibit 5(B-D)/14: Mechanisms 
Exhibit 5 of Chapter 14 (continued) 
B) Second step -median residual in boldface type; two decimals only 
W(1) ---_ 
fit ()= el = lul = (1 - u2) 2 
l I l 
I 1.1 1.13 .03 ,02 1.00 1.10 1.00 
2 2.0 2.26 .26 � 15 .96 3.84 3.84 
3 3.1 3.39 .29 .17 .94 8.74 8.46 
4 3.8 4.52 .72 .41 .69 10.49 11.04 
5 6.5 5.65 .85 .49 .58 18.85 14.50 
43.02 38.84 
(2) = 1.11 
C) Third step. ,likewise 
W(2) .-- 
x I i � J 1.11x 
I 1,1 1,11 .01 ,01 1,00 1.10 1.00 
2 2,0 2.22 .22 .16 .95 3.80 3.80 
3 3.1 3.33 .23 � 17 ,94 8,74 8.46 
4 3.8 4.44 .64 .46 ,62 9.42 9.92 
5 6.5 5.55 ,95 .69 .27 8.78 6.75 
31.84 29.93 
/(3) = 1.06 
D) Fourth step---likewise 
W(3) -- 
x I i � I 1.06x IY 06xl lel/6(.12) or O i w(3'xy J i w(3'x i 
1 1.1 1.06 .04 .06 .99 1.09 .99 
2 2.0 2.12 .12 .17 .94 3.76 3.76 
3 3.1 3.18 .08 � 11 .98 9.11 8.82 
4 3.8 4.24 .44 .61 .39 5.93 6.24 
5 6.5 5.30 1.20 1.67 0 0 0 
20.28 19.81 
/(4) = 1.02 - 
14H: Iterative weighting/Exhibit 5(E-F) 361 
Stepweighting. For hand work we might like to save arithmetic by introducing 
a discrete set of weights. Hand work is thinkable only for the simplest problems. 
We suggest the weights 
4 lu[-< 0.2, 
3 0.2 < lul-< 0.4, 
w(u) = 2 0.4 < lul--< 0.6, 
1 0,6 < l ul-<- 0.8. 
o 0.8 < lul 
Exhibit 6 has the arithmetic for the example of the line through the origin 
pictured in Exhibit 4. 
Exhibit 5 of Chapter 14 (continued) 
F.) Fifth step likewise 
Ix I y [ 1.02x [y .02x[ [el/6(08) or O i w,.,x.YiI w,,,x= I 
1 1.1 1.02 .08 .17 .94 1.03 .94 
2 2.0 2.04 .04 .08 .99 3.96 3.96 
3 3.1 3.06 .04 .08 .99 9.20 8.91 
4 3.8 4.08 .28 .58 .44 6.69 7.04 
5 6.5 5.10 1.40 2.92 0 0 0 
20.88 20.85 
1.00 
F) Sixth step--likewise 
W (s) = 
= l ul = ( - u=)  
1 1.1 1.0 .10 .17 .94 1.03 .94 
2 2.0 2.0 .00 .00 1.00 4.00 4.00 
3 3.1 3.0 .10 .17 .94 8.74 8.46 
4 3.8 4.0 .20 .33 .79 12.01 12. 
5 6.5 5.0 1.50 2.50 0 0 0 
25.78 26.04 
(6) - 0.99 
362 Exhibit 6/14: Mechanisms 
Exhibit 6 of Chapter 14 
Stepweight fitting the example of Exhibit 4 (with c = 6). 
A) Panel A as in Panel A of Exhibit 5. 
B} Second step 
fit ("= lei: l l.l= 
1 1 i i I 
1 1.1 1.13 .03 .02 4 4.4 4. 
2 2.0 2.26 .26 .15 4 16.0 16. 
3 3.1 3.39 .29 � 17 4 37.2 36. 
4 3.8 4.52 .72 .41 2 30.4 32. 
5 6.5 5.65 .85 .49 2 65.0 50. 
153.0 138. 
/(2) = 1.11 
C) Third step 
1 1.1 1.11 .01 .01 4 4.4 4. 
2 2.0 2.22 .22 .16 4 16.0 16. 
3 3.1 3.33 .23 � 17 4 37.2 36. 
4 3.8 4.44 .64 .46 2 30.4 32. 
5 6.5 5.55 .95 .69 1 32.5 25. 
120.5 113.0 
1.07 
D) Fourth step 
fit (3) = ie[ = iul = 
x fiy I 1.07x lY - 1.07X' ' lei,6(.14) l w"'! !.w(:)x.yl I w(')x' 
1 1.1 1.07 .03 .04 4 4.4 4. 
2 2.0 2.14 .14 .17 4 16.0 16. 
3 3.1 3.21 � 11 � 13 4 37.2 36. 
4 3.8 4.28 .48 .57 2 30.4 32. 
5 6.5 5.35 1.15 1.37 0 0 0 
88.0 88. 
1.00 
14H- Iterative weighting / Exhibit 7(A-B) 363 
We have now reduced the arithmetic till most of it is involved in finding 
the residuals, something we cannot do without. 
If we now put c - $ in the stepweight procedure, we can reduce the 
arithmetic still further, since the rule for weights becomes: 
w i = 4 if Jell --< Sk, 
wi = 3 if Sk < le, I < 2Sk, 
wi = 2 if 2Sk <]ei[--< 3Sk, 
wi = 1 if 3Sk < [el =< 
wi = 0 if 4Sk < levi. 
The arithmetic reduces now to that of Exhibit ?. At each step, beyond finding 
residuals we have to find Sk, the median Iresidual[, and its multiples by 1, 2, 3 
and 4; to see whether wi is 4, 3, 2, 1, or 0 (by comparing ]eli with these 
multiples in the (*) column); to form simple integer multiples of xy and x2; and 
to find the corresponding sums and/(k+) as their ratio. If we can afford to do a 
basic least-squares fit .which ought always involve getting residuals--by hand, 
we can almost always afford going on to a stepweighted fit. 
Exhibit 7 of Chapter 14 
The arithmetic for stepweight fitting with c = 5. 
A) Panel A in Exhibit 5. 
B) Second step 
1 [ 71__ I 
l x �' I 
4 
1 1.1 1.13 .03 (.29) 3 4 4.4 4. 
2 2.0 2.26 .26 (.58) 2 4 16.0 16. 
3 3.1 3.39 .29 (.87) 1 4 37.2 36. 
4 3.8 4.52 .72 (1.16) 0 2 30.4 32. 
5 6.5 5.65 .85 2 65.0 50. 
153.0 138. 
j(2) __ 1.11 
364 Exhibit 7(C-D)/14: Mechanisms 
We will sometimes do better with c = 9 and a somewhat different set of 
steps, so that 
wi = 4 if leil--< 2.4S,, 
wi = 3 if 2.4Sk < leil _--< 4.2S, 
wi = 2 if 4.2Sk < le,[ ----< 5.6&, 
wi = 1 if 5.6Sk < le,! -< 7.4Sk, 
w = 0 if 7.4Sk < le,!. 
Once these 4 multiples of Sk are calculated, we proceed as before. 
Exhibit 7 of Chapter 14 (continued) 
C) Third step 
4 
I 1.1 1.11 .01 (.23) 3 4 4.4 4. 
2 2.0 2.22 .22 (.46) 2 4 16.0 16. 
3 3.1 3.33 .23 (.69) 1 4 37.2 36. 
4 3.8 4.44 .64 (.92) 0 2 30.4 32. 
5 6.5 5.55 .95 0 0 0 
88.0 88. 
1.00 
D) Fourth step 
ixlll .ooxl iwll I = 
4 
1 1.1 1.00 .10 (.10) 3 4 4.4 4. 
2 2.0 2.00 .00 (.20) 2 4 16.0 16. 
3 3.1 3.00 .10 (.30) 1 4 37.2 36. 
4 3.8 4.00 .20 (.40) 0 3 45.6 48. 
5 6.5 5.00 1.50 0 0 0 
103.2 104. 
/<4) = .99 
(14H) 141: Least absolute deviations (Optional) 365 
Weights on weights. Suppose we had advance knowledge that different data 
points had different precision, so that we would want to start with different 
w(i)'s. What changes would we need to make? 
If we had weighted in inverse proportion to anticipated variance, as we 
would in the simplest case, we would want to look at suitably weighted residuals, 
namely the. 
x/-(i)(y(i)- (i))'s, 
the sum of whose squares we had in fact, minimized. Taking, at the kth step 
_ x/w(i)[y<>(i)- <>(i)], 
S = median of the 
c as before, 
w(u) as before, 
we would want to take as weights for the next step, not just w(u )) but the 
product 
w(u?)) � w(i) 
of the new weights and the initial weights. These are readily computed. Note 
that w(i) appears in e? ) but w(u? -)) does not. 
.141. Least Absolute Deviations (Optional) 
Sometimes, instead of least-squares fitting, we wish to minimize 
constant  
(constant)  ! y(i) - (i)[ =  iY'(- 2 -)l (y(i) - 9(i)) 
to obtain a 
least absolute dviation fit, 
which we will refer to as a least-absolutes fit. In one way this is better than 
least squares: It does not pay excessive attention to large residuals. In another 
it is worse, since it pays undue attention to very small ones. (We will 
see how to avoid this difficulty shortly.) 
If we try the iterative fitting process of Section 14G with 
constant 
= 
y(i)- ()(i) 
and the process converges to (i), we must have minimized 
constant 
5'. w.,t(i)(y(i)- (i))2 = 5'. [y(- _ ii) I (y(i)- if(i))2. 
366 Exhibit 8 /14: Mechanisms for fitting 
Hence (i) is a least-absolutes fit. Thus we see that iterative weighted least 
squares can lead to least-absolute deviation fits. We also have had brought to 
our attention an unfavorable feature of least-absolute deviations: great weight 
on the observations with smallest residuals, because a small ly(i) - (i)l in the 
denominator produces a great weight. 
Example 1. One outlier. Given the five points listed in Exhibit 8, obtain the 
least-absolute deviations fit of the form y = /3x. 
Discussion. The figure at the bottom of Exhibit 8 suggests one attractive line 
as y = x, or/ = 1. If we start our interpretation there, we fit the four points 
nearest the origin exactly, and this gives us difficulty in taking reciprocals. The 
interpretation is that infinite weight should be given to four of the points and 
only a finite weight to the fifth, which thus has zero relative weight. This does 
not bother us because we like/3 = 1. But suppose we had put the line through 
Exhibit 8 of Chapter 14 
Fitting a slope near/1 = 1. 
I I 1 0 l+d d 
2 2 2 0 2 + 2d 2d 
3 3 3 0 3 + 3d 3d 
4 4 4 0 4 + 4d 4d 
5 6 5 1 5 + 5d 1 - 5d 
Total I + 5d 
m � 
IL I I I I 
�0 I z 3 4 
141: Least absolute deviations (Optional) / Exhibit 9 367 
the fifth point instead. Then it would get infinite weight, the other four would 
not matter, and the iteration would be stuck again, but this time in what we 
would all agree was the wrong place. Computationally this is unacceptable. 
If we wish to find the least-absolutes fit, we can take t3 as 1 + d, where d 
is a small number (less than 0.2). The residuals are computed for this fit in 
Exhibit 8 and we see that the sum of their absolute values is I + 5d. Therefore 
we can minimize the sum of absolute deviations by setting d = 0, and the 
desired minimizing/3 is 1. 
Example 2. Difficulties. We could illustrate the iterative procedure either on 
this example or on the example in Section 14G. The illustration of Section 14G 
is carried out in Exhibit 9 for the interested reader. We started with /() = 
1.02. 
The main point is that very heavy weights given to points with small 
residuals not only are troublesome computationally, but are also unsatisfactory 
Exhibit 9 of Chapter 14 
Fitting a line through the origin using least absolute deviations. 
A) Least-squares estimate: 62,1/55 = 1.13 
1 1.1 1.02 .08 1.0 1.10 1,00 
2 2.0 2.04 .04 2,0 8.00 8.00 
3 3.1 3.06 .04 2.0 18.60 18.00 
4 3,8 4.08 .28 .286 4.35 4.58 
5 6.5 5.10 1.40 .0571 1.86 1.43 
1.84 33,91 33.01 
33.91 
/(2) = = 1.0273, round up to 1.028 
33.01 
= Iv- 1.028xl Weight 
y 1.028x =e ,072/e l wxy wx= ! 
1.1 1.028 ,072 1,00 1.10 1.00 
2.0 2.056 .056 1.285 5,14 5.14 
3.1 3.084 .016 4.5 41,85 40,50 
3.8 4.112 .312 .231 3.51 3.69 
6.5 5.140 1.360 .0529 1.72 1.32 
1.812 53.32 51.65 
53.32 1,0323 
51.65 
368 Exhibit 10/14: Mechanisms for fitting 
from a statistical view. Even if (i) we want to minimize a sum of terms, and (ii) 
we like absolute deviations as a measure of size for large residuals, we need to 
use a more reasonable measure of size for small residuals. We want to give the 
middle measurements substantial weight and then reduce the weights as 
deviations get large. 
Flattened weights. We can get such a measure by using. 
1, if [y(i) - ](h)(i) i _--< k, 
= k 
otherwise 
ly(i)- (h)(i)[ ' 
where k is some moderate value, perhaps the median absolute residual, called 
Sk in Section 14H. 
The result is to minimize 
where 
u where this is __-< k 2 (for-k _<- u _<- k), 
'I,(u) = klul elsewhere. 
Example 3. Weights. Let us apply the suggestion just given to the data of 
Example 2. 
Discussion. Let us begin with a rounded version of our latest estimate of /, 
namely 1.03. From the fourth column of Exhibit 10, we see that the median of 
the errors is 0.07, and we set this equal to k. This leads to unit weights for the 
three points nearest the origin, a modest weight for the fourth point, and a 
small weight for the last point. This last point matters even with this small 
weight because the difference between its weighted xy and x 2 accounts for a 
notable share of the difference of the final slope from 1.000. 
Exhibit 10 of Chapter 14 
Flattened weights related to absolute deviations. 
1 1.1 1.03 .07 1.00 1 1.1 1.0 
2 2.0 2.06 .06 1.17 I 4.0 4.0 
3 3.1 3.09 .01 7.00 I 9.3 9.0 
4 3.8 4.12 .32 .22 0.22 3.34 3.52 
5 6.5 5.15 1.35 .05 0,05 1.63 1.25 
19.37 18.77 
/2 = 19.37 1.032. 
18.77 
(141) 14J: Analyzing troubles 369 
Since the movement from a slope of 1.03 to 1.032 is so slight, it does not 
seem worth iterating further. 
Other powers. If we want to minimize 
ly - 
for any p, we see that we have only to use 
constant 
w+(i) = 
iY(i)- Y' '(i)l v 
Again it would pay us to flatten off the peak so as not to weight the closely 
fitted values so heavily. 
Weights on weights. Suppose we want to minimize, not just 
7ly - 
but rather 
 w(i)[y(i) - (i)[.s, 
where the w(i) are given and do not depend on the values y - . To do this, 
we need only change, as suggested in Section 14H, 
constant 
w+(i) = 
[y(i)_ 
into 
constant 
= w(i)' 
lY(i)- f()(i)[o.s 
as the reader can verify. 
14J. Analyzing Troubles 
When Y. X.23... k is small, the variance of/J, is large. There is nothing that can 
be done to stoo this happening. All that ordinary least squares can tell us about 
/3 is present in a simple picture, the plot of y.23... k against xx.23...k. If, as may 
happen, we find, out of 200 values of i, 
-0.0002 -< X.23...k(i) --< 0.0003 for 197 values of i, 
1.97 --< X.23...k(i) --< 2.01 for 2 values of i, 
x.23... k(i) = 47.34 for 1 value of i, 
we can be fairly sure there have been three large errors--perhaps in measure- 
ment, perhaps in copying, perhaps in computation and for what i's these have 
occurred. Moreover we can be sure that if we insist on fitting 
y = /3xx + 2X2 nt'''' q- kXk 
370 /14: A class of mechanisms for fitting 
instead of 
y -- 2X2 q- � � � kXk, 
then what we find for /3 and, regrettably, but also automatically, for 
/J2,/J3"'/Jk--will be almost entirely determined by these three identifiable 
data-sets with the large errors. This usually ought to be quite unacceptable--so 
we ought to know when it is happening. To find out, we need routinely to look 
at the distribution of the values of x.23... k (and at those of its brothers and 
sisters). 
At first blush, we might hope to escape by setting aside these three points. 
It will be a good thing to set them aside--at least our results will not be 
controlled by errors. But we still have serious trouble. For our assumed values 
of x.23...k we have 
)2 < 0.00000009 < 10 -7, 
X1-23 .-- k 
so that 
2 
 er 10er  
var/3 > = 1 000,000 times , 
n. 10 -7 ' n 
which is likely to be devastatingly large. 
Moreover, the variances of the other/j, if found for a fit including a 
will be correspondingly large. 
If only a very few values of X1-23--.k are large enough to be useful, a 
least-squares fit of 
IX1 q" 2X2 +''' q- kXk 
is not likely to be what it seems. 
Occasionally we know that these few unusual values are correct. Then 
writing the fitted regression as 
lX1.23...k -'[- X 2 q-''' q- Xk, 
where the /3"s are the coefficients of the fit with x left out (see the last 
section), makes it easier to warn ourselves what we do and do not know. 
Although /3x still depends almost entirely on a few data points, we can, by 
looking at x2.3... k and its brothers and sisters, find out which/*'s depend on a 
lot of data points and which on a few. 
AJter setting some aside. If we believe these few large values come from errors 
which may be in any or several of the xi's, then we need to set aside these data 
points and fit the others. We are not likely to be happy about the smallness of 
 (Xl-23 � -. k) 2 
and the corresponding largeness of var {/}, but that is just a fact of life. 
If again we put 
X1-23--.k -- X1  '�12X2 ..... lkXk 
14J: Analyzing troubles 371 
we have 
and if simultaneously (1) ;/j is not small and (2) var {/J} is not large, then 
var {;/lj/J} >> var {}, 
so that var {1} and all other var {} will still be large. It may still pay to look 
at 
+ +... + 
instead of 
The other coecients, ,...,  may still be well determined in one form, 
and if we reveal how small the sum of squares of x.3...  now is (after setting 
aside the few unusual points), we will have made clear which coecients we 
know little about (anyone that involves more than ,..., ) and why 
(x.3... )2 is too small. 
General Comments 
Rewriting regression expressions so that some coefficients are well determined 
and others less may, therefore, sometimes be desirable. The new coefficients 
will have new meanings, and this can be true even if their numerical values 
happen not to change. The hope is that we will have learned a bit more about 
what the data tell us and what they don't. 
One way to rewrite regression expressions is by dropping out one or more 
terms. This may be wise or it may be an evasion of responsibility. When we fit 
a regression without interpreting the coefficients, not even to compare them 
with coefficients arising from a similar fit to other data, then pulling out a 
variable whose coecient is ill determined is likely good practice. If we fit a 
regression whose coefficients are to be interpreted, and we felt there was 
reason to include x, then taking out x, perhaps without comment, may be 
irresponsible. It may be better to leave Xl in, in the form Xl-Z3---k, and 
recognize what is not known about its coefficient. 
Which variable should be special? Whether we go to a "fully dotted" variable 
like x.23... k or leave x out, we are likely to have a choice as to which x is to 
play the special role. We may be able to choose among 
+ + 
and 
+ + 
and 
372 /14: A class of mechanisms for fitting 
or among 
and 
and 
+ 
How shall we choose? We suggest that you write down the alternatives, 
see what you would say about each, and then try to see which story is most 
helpful. Unfortunately, fixing up one variable may not be enough; we may need 
to handle two or more x's specially. 
Weights. As we noticed, we can extend the definition of Xl-23---k to a case with 
weights w(/)---either weights inherent in what we believe about how var {y(i)} 
depends on or, or weights chosen iteratively to reach more desirable fits or 
combinations of both. Here, too, if all but a few values of x.23...k are very 
small, we are in trouble. 
The only way that we can get out of trouble with small values of 
 (X1.23...k) 2 is therefore to get additional information. 
An inequality. If we fit, successively, 
y = /3x (to find ()), 
y = IX1 q- /2X2 (to find (2)), 
y = iIX1 q- /2X2 q- /3X 3 (and so on), 
y = fiX1 q- f2X2 q-''' nt- lkXk, 
then (in addition to the great changes in the meaning of/3 as we go from one 
fit to another) we have 
Z t Z( 
g > X1. 2 --- X1.23 
.._-- ---. 
We do not know var {y}. All we can do is to estimate it. We usually do this 
from the values of y - i--more precisely from the values of whichever of 
y_ (1) y_ (2) Y_ (3) is appropriate. Because we are doing least 
squares, 
14J: Analyzing troubles 373 
Now to estimate var {y}, we usually divide 
(y_ {i>)2 by (n- i), 
so our estimates of var {y} can grow slowly as we fit more terms. In fact, 
however, they are not likely to grow. 
If our estimates of var {y} were all the same, which is what would happen, 
on average, if x2, x3,..., xk had only random explanatory power for y, we 
would have 
Oar {/ [/3x} < Oar {/J [/3x +/32x2} <-" 
< Oar {/x i/3x +/32x2 +-" +/3uxk} 
Thus, in this situation, the more terms we add to the fit, the more variable the 
coefficients entered earlier become. 
This is not what we are used to thinking about. Often we find adding more 
terms make. s the y -  much smaller. So we happily take credit for this with a 
smaller estimate of cr 2 for the long fit than for the short. Let us think of a 
10ngish list of terms, where we can stop fitting after varying numbers of terms. 
What is going on is that 
involves first, systematic variations (because we haven't yet put in enough 
variables to take them away), and second, some relatively random variation 
that doesn't depend very much, if at all, on how much we fit. Suppose, then, 
that 
est var {y} = systematics 2 + variability, 
where systematics 2, coming from what we have not yet fitted, goes down as we 
fit longer expressions. The apparent variance of/3x, say, is 
est var {y} (systematics after some) 2 + variability 
X 1-some  X 2  
1-some 
where the first term in the numerator is decreasing so that the numerator can 
race the denominator to see which shrinks fastest. Often the numerator will 
win for a while, but this will come to an end. Eventually the denominator 
becomes smaller faster, and the fraction starts to rise again. 
Thus we can go too far. When we recognize that adding x's always 
redefines the meanings of the coefficients of the x's we already had, it is dearly 
easy to go too far without recognizing it, if we need meaningful coefficients. 
We can also go too far toward making the average of vat {]} larger instead 
of smaller, although overshooting happens slowly, so that results are less 
sensitive to the exact choice. (See the discussion in Section 15A and the 
references given at the close of Chapter 15 for the relevant techniques.) 
374 /14: A class of mechanisms for fitting 
14K. Proof of the Statement of Section 13B 
We now return to the proof that if 
y. = y - a - bx 
t.x = t- c - dx 
and the least squares fit to y.x is 
y.  e + ft., 
then 
a + bx + f(t- c - dx) 
is the least-square fit of y to 1, x, and t. We complete the proof by building up 
the result from scratch. Because y.x is a residual from a least-squares fit that 
includes 1 (we fitted a � 1 + bx), 1 is a matchef, so that we also know that the 
sum of the y-residuals vanishes, so that 
Y. = 0 
and so, for the same reason, must 
= 0. 
Consider the "first" normal equation when we regress y.x on t.x and 1, 
 l(y. -(e + ft.x)) = O, 
whence 
E Y-t -e E 1 + fE t.x = O; 
and since the sums of the y-residuals and of the t-residuals vanish, we find that 
e has to be zero. Our last fit must therefore be 
y. -- ft.. 
What happens if we start putting things together? We first fitted 
a+bx 
to y, and last fitted 
to what was left. We ought to look at the combination 
a + bx + fi., 
which can also be written in more extended form as 
a + bx + f(t- c - dx) = (a - fc) + (b - fd)x + ft. 
What can we say about it? 
14K: Proof of the statement of Section 13B 375 
We know that 
Y, x(y):  X(a + x), 
and we have just seen that 
Y, (t.) = 0. 
Thus we could write, by "adding zero" in the form f Y t., 
 l(y)= Y, l(a + bx + ft.,). 
This shows that 
 l(y - a - Ox - ft.) = O. (*) 
We know that 
 (y) =  ( + bx) 
where "x(y)" means "x times y", and 
 (t.) = 0. 
Thus again "adding zero" in the form f xt., we get 
 (y)=  (, + v + ft.l). 
This shows that 
 (y -, - v - ft.) = 0. (**) 
We know that 
 (t.) = 0, 
 (t.) = 0, 
so that 
 t.(a + bx) = O. 
Hence, because y. = y- a- bx, 
 t.(y.) =  t.(y - a - bx) =  t.y. 
Next we show that, because the value of f was chosen to make the second of the 
following expressions equal to zero, 
 t.(y - a - bx - ft.) =  t.(y. - ft.) = O. 
Because t. = t- c - dx, we have t = c + dx + t.l, and because we have 
shown that (*) and (**) hold, we have now proved that 
 t(y - a - bx- ft.)= 0. (***) 
Thus we have shown the vanishing of all three sums 
(*) 
 (y -  - v - ft.) = 0, (**) 
(***) 
376 /14: A class of mechanisms for fitting 
so that the form we get from adding the step-by-step fits 
a + bx + ft.x = (a - fc) + (b - fd)x + ft 
has been proved to be the least-squares multiple regression of y on 1, x, and t. 
How should we interpret the coefficient of t? The construction we have 
just gone through shows that it is the regression of 
y-adjusted-linearly-for-x -= y. 
on 
t-adjusted-linearly-for-x --- t.x. 
A more detailed description for f is that it is 
"the regression coefficient obtained when y (linearly adjusted for 1 and x) 
is regressed on t (linearly adjusted for 1 and x)." 
General case. We could have left out 1, doing 
y -- Bx, 
t '- Dx, 
Y.l " Ft., 
without any change in the argument. 
We could have had several x's instead of one, with only the simplest 
change in the argument (putting in each xi in turn in place of x alone). Such an 
approach is part of some computational methods for stepwise regression. The 
general result is that quoted at the end of Section 13B. 
Summary: Mechanisms for Fitting Regression 
Many mechanisms of fitting, either as a whole or stage by stage (i) involve 
adding up across data sets and (ii) can be defined in terms of a family of 
marchers, for each of which the relation 
 (matcher)y =  (matcher)], 
involving summation across data sets, is a condition that can help to define the 
fit. 
All forms of least squares, including varied forms of weighting, which may 
be either fixed or changing from stage to stage of an iteration, can be described 
in terms of matchers. 
When we fit a general straight line, selected from the stock {all a +/3x}, 
by ordinary least squares the matchers are {all c + dx}. 
Summary: Mechanisms for fitting regressions 377 
For any coefficient/3i, appearing in any expression of a stock {all 5'./3ixi} to 
be fitted by ordinary least squares, there is a special matcher worthy of being 
called a catcher because it satisfies not only 
 (catcher)y --  (catcher)] 
but also 
/J, =  (catcher)y, 
where the summations are over the data sets. 
When we have {all /3ix} as the stock, we can take, as the collection of all 
matchers, all the potential fits in this stock. If we do, the sum of squares of the 
deviations,  (y - 9)2, will be a minimum. Thus it will be the fit by ordinary 
least squares. 
To understand many of the troubles that may be concealed in fitting from 
{all Y. Tx} it is useful to look hard at the values of 
gi. '-- Xi -- Xi 
for each and every i, as well as at the y-residuals, y - 9. 
The variance of fitting/3 in {all Y./3xi} by ordinary least squares is 
2 
var [3i Y (Xi -- i)2 
when each y satisfies 
� y = unknown fit + perturbation, 
provided that the perturbations are uncorrelated and have a common vari- 
ance. cr 2, and tha. t i is the ordinary least-squares fit to x in terms of the other 
carriers (the ordinary least-squares fit from the costock of x). 
Since we have taken a fit from the costock out of x, we may as well do the 
same for y, forming residuals y.a, but , after which an ordinary least-squares fit 
of a straight line to the (x - �i, Y-dl but i) pairs will give us /3. 
Labor in calculating Y-an but need not be great, since we may write 
Y-allbuti -- y-all q- fli(Xi -- ii), 
something that saves effort when we are using this approach to understand (i) 
how the value of ft is related to the details of the data and (ii) how we might 
alter the mechanism of our fit to better match the idiosyncrasies of our data. 
Since 
Y'.x2>Z  >Z 2 >... 
i m Xi-1 '-- Xi'12 m 
and since these decreases may be by large factors, the precision of fitting/3 can 
only decrease as more and more carriers are added to the stock concerned. 
This decrease can be covered up, for a certain number of increases, by a 
decrease in the s 2 we calculate from Y (y- )2 as more flexible fits offer a 
better chance to match the actual behavior of ave {y} as a function of the x's. 
378 /14: A class of mechanisms for fitting 
If we fit from {all /3ix} by choosing a weight for each data set and taking 
{all (weight) ( Tixi)} as our matchers, the resulting fit will minimize 
 (weight)(y - ])2 
and will thus be the fit by weighted least squares with the chosen weights. 
This result means that weighted least-square fitting of y from {all Y./3ixi} 
is equivalent to ordinary least-square fitting of y /weight from 
{all dweight Y./3ix}. 
Accordingly, if we choose weights so that 
(weight) x var {y} = cr 2, 
then 
2 
var/3i = 
2  
Y. (weight.)xi dot 
where Xidot is the residual after fitting, with the chosen weights, x from its 
costock. 
If we introduce a "matrix weight" {wj} and choose as our matchers all the 
linear combinations 
{matcher at i =  wij(potential fit at ])), 
one for each potential fit, we will minimize 
thus obtaining a generalized weighted least-square fit. 
If we are to consider how expressions calculated from data respond to the 
values involved, we can do well by looking at the influence curve generated 
when all but one of the data values are constant and that one varies as it 
wishes. 
The influence curve of the biweight shows just the behavior we would like 
to have in a harsh, realistic world, namely' (i) near straight-line behavior when 
the wandering value is near the middle of the other values, (ii) flattening off as 
it moves further away, and (iii) a final turn over and decrease to the point 
where the presence or absence of the value matters not, once it has moved 
sufficiently far away. 
Using a sequence of fittings in which each stage is an instance of weighted 
least squares, but where the weights in each step depend upon the residuals in 
the previous stage, is a very flexible method of fitting regressions. 
Using a biweight procedure, say with S = median absolute deviation from 
the median (or, alternatively, perhaps half the spread between the hinges 
defined in Chapter 10) and c = something like 9 to perhaps 6, offers an 
effective fitting procedure that is resistant and robust of efficiency. 
References 379 
When we want both (i) to weight to compensate for understood changes in 
variability of y from data set to data set and (ii) to biweight to provide 
resistance and robustness of efficiency, then we should: 
a) take (weight) x (biweight) as our working weight, 
b) take as the numerator of our biweights the product x/-ght (y-  at 
last stage). 
Even least absolute deviation fits--or modifications (flattened weights) of 
the.m to .allow more effective use of data sets with small y - 's can be made 
by teratve least-squares fitting with weights based upon the residuals of the 
previous stage. 
We can: 
omake biweight fits using the computer, 
oapproximate biweight fitting by hand, using a stepweighting scheme, 
oinclude weights compensating for understood changes in variability from 
data set to data set in such fittings. 
References 
Hampel, F. E. (1974). "The influence curve and its role in robust estimation." 
J. Amer. Statist. Assoc., 69, 383-393. 
Larsen, W. A., and S. J. McCleary (1972). "The use of partial residual plots in 
regression analysis." Technometrics, 14, 781-790. 
380 Index for Chapter 15 
15A How can we be guided in what to fit? 381 
15B Stepwise techniques 387 
15C All-subset techniques 391 
15D Combined techniques 393 
15E Rearranging carriers--judgment components 394 
15F Principal components 397 
15G How much are we likely to learn? 401 
15H Several y's or several studies 402 
151 Regression starting where? 402 
15J Arbitrary adjustment 404 
Summary: Guided regression 404 
References 405 
Chapter 15/Guided 
Regression 
Frequently problems arise where 
owe have several, even many, carriers; 
owe do not expect to use all of them; 
owe are content, after some preliminary modification, to let the eventually 
chosen stock be defined by a subset, conceivably any subset, of our carriers. 
Possibly more than one subset could be available at the close of the 
operation. 
We could not be much concerned with the coefficients of our fit, because if 
we .were, we would have to worry about just which stocks made sense. The 
purpose must then be either regression as description or regression as exclu- 
sion. 
In this chapter, we describe only techniques based entirely on least 
squares, either equally weighted or with some fixed set of weights. We include 
some resistant versions. This does not mean that we are opposed to methods 
other than least squares. 
We do want methods that let the data guide its own analysis, at least as far as 
selecting the stocks. In practice, this means guiding the computer, because the 
required volume of arithmetic soon drowns the hand calculator. The combina- 
tion computer-graphical display-human operator has not yet been followed 
through far enough to let us judge how much it can do. 
15A. How Can We Be Guided in What to Fit? 
Because we consider many alternative stocks, we must choose among them. 
Naturally we want to measure how well each alternative performs so that our 
choice can be guided. To develop such a measure, let us begin with some "ideal 
conditions" because we plan to compute as if these were true. As usual, the ith 
data point is (xi,-x_i,...., xi, yi). 
Ideal Conditions 
1. The model The actual values of y can be expressed as 
Y = t + errors. 
2. Unbiasedness. Each errori has average (expected value) zero. 
381 
382 /15: Guided regression 
3. Homogeneity. Each error has the same variance tr 2. 
4. Lack of correlation. Each distinct pair of errors, errors, and errors, for i  /, 
has zero covariance. 
5. The structure. The "true" values can be expressed linearly in the x's as 
i 
(Thus the stock is adequate to describe all systematic effects). 
Although these ideal conditions may well be far from true, they are often a 
reasonable starting point. Indeed, working as if they were true often gives us a 
useful answer. (Making a reasonable fit, whether or not we have these ideal 
conditions, is ordinarily the first step in looking to see how reasonable they 
are.) 
If these conditions hold for any stock, they also hold for any larger stock 
containing the first stock. When we want to avoid unnecessarily large stocks, it 
is appropriate for us to consider the problem of choosing among stocks, for all 
of which the ideal conditions hold. Note carefully that we are choosing between 
stocks rather than fits; for this chapter we agreed earlier to use least squares 
with a fixed set of weights. 
In any event, the convenience and simplicity of these ideal conditions, 
when combined with an ideal minimand, lead us to a classical basis for guiding 
our fitting. This ideal minimand is the expected sum of squared residuals 
squares of fitted value MINUS true value, 
ave (i - *li) 2 
which measures an ideal, average (not directly measurable) lack of fit. It gives 
the long-run average sum of squares of the deviations of the estimates  from 
the true values *h for a particular stock satisfying the ideal conditions. 
A practical question arises at once: We are not likely to know the 
indeed finding out something about them is ordinarily why we are fitting. We 
must turn, then, to estimating this ideal lack of fit--to replacing the ideal 
minimand with a minimand we can handle. 
Estimating the lack of fit. Let us recall that under the ideal conditions  is an 
unbiased estimate of h, that is, 
ave y = 
As a result, the mean square error of  from t is the variance of 
ave (9i - ?li) 2 = var 
Using this result, the ideal minimand can be written 
var y. 
i 
15A: How can we be guided? 383 
What we plan to show is, first, that the ideal minimand is: 
2 
cr x (# of carriers)* 
and, second, that an estimate of this is 
s 2 x (# of carriers), 
where s 2 is the familiar estimate of the common 
# of data points MINUS # of carriers' 
For example, . when we fit y. = .o' 1 + /31x, we us.e two carriers, 1 and x, and 
so the denominator on the right s n - 2, where n s the number of data points. 
We turn now to the derivation. 
Derivation. We are still talking about one stock, and so we can change the xj 
without changing the stock, so that 
Y', x#xj = O,   J, 
We suppose the change in x's has been made and that (1) now holds. Since 
stays the same function of y, ..., y, this does not change  var ]i. Since the 
x's are fixed, we recall first that 
and second that 
li -- E x#yi. 
i 
Using these facts, we have a sequence of further results. First, 
cov (t3,/3j) = O, j J, 
and 
var {/} = o -a. 
* The symbol # stands for "number." 
384 / 15: Guided regression 
Furthermore, 
2 
var i =  xji var 
2 
-- 0'2 Z X ji 
so that 
2 
= a x ( of carriers). 
Moreover, because least squares leads, under our ideal conditions, to 
coy (y - , ) = 0, 
we have 
vat yi = vat (yi- i) + vat i 
= ave (yi - )2 + var i, 
which becomes 
2   2 
Then, transposing ves 
av (y- ,)= ( -  x) , 
so that 
since  x = 1, which gives 
 ave (y - )2 = ( of data points MINUS  of carriers) 2. 
i 
Since we assumed that the y's have a common a2, these results lead us to 
the usual estimate 
  Z(y- ) 
 of data points MINUS  of carriers 
and to an estimate of our actual minimand as 
s 2 x ( of carriers), 
which is what we started out to show. 
15A: How can we be guided? 385 
You may want to reconsider the merits of this after we have reached our 
discussion of C}, below. 
If we like either minimand, then we dislike unnecessary carriers because 
they increase their size. 
If we like our actual minimand, either for itself or as an estimate of the 
ideal one, then we prefer among all stocks with a given number of carriers .. 
those that make s 2 smallest. Such stocks produce the closest fits to the data. (If 
the ideal assumptions weren't true, some stocks might have smaller tr2's than 
others. Then a stock that made rr 2 smallest would produce the closest fits to the 
true values. But we cannot know this.) Such a stock appears to come closest, 
among all with the given number of carriers, to meeting the ideal assumption 
that we can express 
/i = ave {yi} 
exactly. 
The idea we require may be more familiar for sample and population 
means than for sums of squares of deviations. If we have many samples from 
many different populations all with about the same true mean, then the chances 
are good that the population with the largest observed mean does not have the 
largest true mean. Yet if we are to pick one we hope has a large mean, we will 
pick the one with the largest observed mean. In both situations we have a 
problem of multiplicity, more specifically of competition. Turning now to the 
stocks, the corresponding problem is that the stock that actually fits best among 
all our stocks with k carriers may well not be the one that apparently fits best 
with our data. When we fit a great many alternative stocks, many fits may have 
ideal minimands (expected values for the real minimands) that are close 
together, and chance will then help determine which stock will be observed to 
fit best in the sample. If many stocks are nearly equally close in ideal terms, 
then sampling variation could make it almost certain that the apparently best 
fitting one is not the ideally best fitting one. 
Choice among k-carrier stocks. Lacking other information, however, we are 
likely to chooseamong all our k-carrier stocks, or among all the k-carrier 
stocks we have tried the one that yields the smallest s 2. 
Some may wish to pick out several of the k-carrier stocks with small s 2, 
and then go on to other considerations. 
Choice of k. How then do we choose k, the number of carriers? A variety of 
criteria have been suggested, often with immediate words of warning that it 
would be well not to follow them blindly. We mention three here as reasonable 
possibilities: 
oMallows's 
oAnscombe's (Tukey's) s2/(n -- k), 
oAllen's PRESS. 
386 /15: Guided regression 
In the context of a rather predetermined sequence of stocks, each includ- 
ing the previous ones, Anscombe (1967) produced a criterion for the degree of 
fit which Tukey, in discussing the same paper, simplified. The quantity they 
want to be small is 
residual sum of squares s 2 mean square 
(n- k) 2 rt- k residual df 
Several investigators have developed an approach to selecting sub-sets of 
predictor variables, usually labeled PRESS, meaning prediction sum of squares. 
Anderson, Allen, and Cady (1972) and Allen (1974) are good sources. 
Their context is that of forward and backward stepwise regression, but 
with modification. In the forward stepwise approach, after a new variable is 
entered into a subset, then each old variable is reconsidered for dropping. 
Their criterion is most simply understood as 
PRESS,tock  (y  2 
= - Y(i)stock) � 
i=1 
This quantity is computed for each stock under consideration. The residual is 
the observed yi minus the estimated yi using the given stock, but with the ith 
observation omitted from the calculations leading to the estimate. If this 
formula is used directly, the calculations can be rather heavy. It turns out that 
alternative ways of writing this expression show PRESSstock to be a weighted 
sum of squares of the ordinary residuals, y - (i)stock, and the calculations of 
the weights reduce the work. 
In the context of reviewing methods of subset selection, C. L. Mallows 
(1973) offers a most instructive set of examples and further references. His 
index for appraising stocks is 
1 
Cp =  (residual sum of squares for stock) - n + 2p, 
when n is the number of observations, p the number of carriers in the stock, 
and 'some unbiased estimate of the true variance. We use p here instead of 
our k because Mallows' Cp has come to have that name. And so p -- k in these 
discussions. Mallows is careful to avoid using Cp to determine stopping or 
choices; he emphasizes the desirability of often looking at more than a few fits, 
including two or more using the same number of carriers. In a personal 
communication he says "that inspection of a C-plot may sometimes suggest 
that a particular k be chosen, but equally it may indicate that any one choice 
would be foolish." Others who want a rule might wish to use C. 
He also points out that C is derived in a way similar to that of the 
estimate we gave earlier for the ideal minimand. We could write 
J -- ave (i- i)2 = bias + variance, (*) 
where 
bias = Y'.(ave i - Ti) 2, 
variance = (n - p) cr 2. 
(15A) 15B: Stepwise techniques 387 
Further, we have the average value of the residual sum of squares (RSS) as 
ave RSS = bias + per 2. 
By substituting RSS - ptr 2 for bias in (*) and s 2 for tr 2, we get, as an estimate 
of J, 
^ S2 
J = RSS- (n- 2p) . 
This can be rewritten as 
ps 2 + RSS - (n - p)s 2. 
The first term is the measure we gave earlier as the ideal minimand and the last 
two terms form an estimate of the bias from the specific stock used. And this 
bias may be nonzero when the stock chosen is not adequate. The reader may 
then prefer C v to ps 2 for guidance. 
The whole area of guided regression is fraught with intellectual, statistical, 
computational, and subject-matter difficulties. 
We cannot expect to know just which fit would prove most useful to us; 
this often applies to which carriers appear in the fit, as well as just what 
coefficients they appear with. 
Letting the formal procedures produce a number of fits, and then deciding 
among them on the basis of both apparent quality of fit and other considera- 
tions, is often a wise thing to do. 
It is now time to see whether we can drop one carrier of our triple with 
little cost. If so, we do, and then tr3/adding a new third to our new pair. The 
new triple cannot be worse than the first triple and is sometimes much better. 
Thus moving both forward (more carriers) and backward (fewer) can help. 
15B. Stepwise Techniques 
What shall we do if we face 10 or 100 or 1000 possible carriers and a y ? 
If we must use only one carrier, we have little choice. We try each carrier 
separately, compare the s2's they leave, and take the carrier that leaves the 
smallest one. Equivalently, we would look at the reduction in sum of squares 
due to fitting each carrier alone, 
RSS = j = 1, 2, , k. 
i 
and pick the carrier with the largest RSS. (Either choice picks the carrier 
having the highest squared correlation with y.) 
If we plan to use exactly two carriers, we might try all pairs. For k - 10, 
with 45 pairs, we might well do just this. For k = 1000, where there are 
499,500 pairs, we cannot expect to look at them all. 
388 /15: Guided regression 
One approach would be to try all 1000 alone, see which does best, try that 
one in combination with all 999 others, and see which of these 999 pairs does 
best. There is no guarantee that any of these 999 pairs is even dose to best 
among all 499,500 pairs, but we may not do too badly. How can we try to do a 
little better? 
Having found our first pair, we can try all 998 ways of expanding it to a 
triple, and then try all pairs contained in the triple. This may do better -it 
cannot do worse. 
Standard stepwise procedures. Standard programs for stepwise regression usu- 
ally operate in the following way: 
oForward step: Try every carrier not now in the regression, select the one 
with the greatest reduction in residual sum of squares, then test; if test is 
favorable, add this carrier to the regression (else halt); go to either another 
forward step or a backward step (depending on program). 
oBackward step: Try removing from the regression every carrier now in, 
select the one yielding the least increase in residual sum of squares, then 
test; if test is favorable, delete this carrier from regression; go to forward 
step. 
Here the usual tests calculate "F-statistics", 
change in residual sum of squares 
F= 
s 2 for larger stock ' 
and compare each such value with value(s) arbitrarily selected by the user (if 
backward steps are taken, the test F for them can usually be chosen sepa- 
rately). If possible, we recommend a procedure that halts only after such a test 
has been "failed" for several--say at least three successive forward steps. In 
any case, the output should show all useful results for the regression found 
after each step, forward or backward. 
The statistic used is analogous to formal tests of significance for the 
presence of a carrier with a nonzero coefficient in a nonstepwise problem. 
Naturally in the sequential situation of stepwise analysis, the statistic remains a 
useful measure, but its nominal level of significance loses its exact frequency 
interpretation. Although the classical levels of significance such as 5% and 1% 
are often used in stepwise fitting, it may be better to move up to, say, 10% or 
20%, to avoid missing good candidates when the effectiveness of many of the 
variables is small. 
The user's responsibility. With such output, the user is prepared to use one or 
more of the criteria mentioned in Section 15A to help decide when to stop. 
Mechanizing this step seems dangerous. The user needs some contact with 
what is going on. 
15B: Stepwise techniques 389 
Desirable supplements. What more might a reasonable user ask of a stepwise 
program? 
The user should ask for information about the residuals after each selected 
fit. A minimum would be (a) information about their distribution and (b) a 
listing of the dozen or so largest (in absolute value). If only a few substantially 
exceed the others, the investigator will probably want (a) to return the data 
with these few data-sets omitted and (b) to study these unusual data-sets 
carefully. He or she might find some mistakes or some really new 
phenomenon. 
Displaying what the coefficients of each carrier are (or would be if the 
carrier were to be added to the stock) at each step costs little and may save 
some grief. (The necessary calculation is a natural step toward finding the 
reduction in residual-sum-of-squares, already needed for guidance.) Sudden 
changes .or even slower drfftswarn us about near depcndencies multicol- 
linearities, among the variables. 
Example. Macdonald and Ward (1963) studied the internal structure of the 
magnetic-character figure, C, one o several measures of disturbance of the 
earth's magnetic field. These disturbances are clearly related to solar activity, 
almost certainly through the arrival of energetic particles from the sun. As 
might be expected, results for one day considerably resemble those of another. 
Days roughly 27 days apart.-27 days being roughly the time for one rotation 
of the sun's atmosphere also resemble each other. 
With a view both to predicting and to producing relatively structureless 
residuals, Macdonald and Ward studied the regression of 
y = C for day t + k 
on any subset of 
x = C for day t, 
x2 = C for day t- 1, 
x3 = C for day t- 2, 
x98 = Ci for day t- 97. 
Using a stepwise program with a particular stopping rule, they applied the 
technique to separate five-year blocks of data, each involving either 1826 or 
1827 daily values of t. 
For k = 1, predicting tomorrow, they found 
y = 0.3746 + 0.570x - 0.174x2 + 0.113x27 
+ 0.071x64- 0.063x86 - 0.057x6 
for the period 1915-1919. Here x and x2 are Ci for the previous two days, 
390 Exhibit 1/15: Guided regression 
their coefficients accounting for some persistence in both level and slope. The 
next two terms, involving x27 and x54, refer to times one and two solar 
revolutions earlier. Finally the x86 and x6 terms do not have a clear meaning. 
The results for four five-year periods are compared in Exhibit 1. The 
exhibit shows considerable consistency in the regressions selected for the four 
periods. To summarize: we should use (a) nearby days, (b) days about 27 days 
back, (c) possibly a day or days about 54 days back. 
That we can give fairly convincing reasons why particular terms appear 
makes this an unusual application of regression, and especially of stepwise 
regression. (But we did not try to interpret the sizes of the coefficients. In such 
unusual examples, where the stocks are fairly definitely fixed, we might try 
this.) 
The weighted case. Most stepwise programs are written to cover only 
equiweighted fits, but there is no difficulty---either in theory or in practice in 
Exhibit I of Chapter 15 
The results of stepwise autoregression of C (prediction I day ahead) 
A) The TERMS from NEARBY DAYS 
1915-19 .570x - .174x2 - .057x6 
1920-24 .513x - .114x2 
1925-29 .586x - .137x2 
1930-34 .527x - .123x2 
B) The TERMS for LAGS near 27 DAYS (one solar rotation) 
1915-19 + .113x27 
1920-24 +. 110x6 +. 140x7 
1925-29 +. 102x6 
1930-34 + .181x2s + .109x2 + ,070X28 
C) The TERMS for LAGS near 54 DAYS (two solar rotations) 
1915-19 + .071x54 
1920-24 nil 
1925-29 nil 
1930-34 + .056x54 
D) OTHER TERMS 
1915-19 - .063x8s 
1920-24 nil 
1925-29 nil 
1930-34 nil 
(15B) 15C: All-subset techniques 391 
extending them to cover arbitrary weights. Simple sums of squares--and their 
reductions--are replaced by weighted sums of squares--and their reductions. 
(The interpretation of the F values changes somewhat, but ordinarily not 
enough for us to change our choice of critical values.) 
Working with variance-dependent weights makes no essential change, 
then, except that point (4) of the ideal conditions becomes: 
4. Variance proportionality. For each i, the product of the weight Wiandthe 
variance of errori is the same. tr 2. 
Resistant stepwise fitting. We know little about resistant stepwise fitting, but 
we can suggest techniques from which we can anticipate a satisfactory perfor- 
mance. Since the simplest resistant fits known involve repeated changes of 
weight, we can revise our stepwise calculation to allow for this. Our suggestion, 
chosen from many without much regard for computer time, should be relatively 
reliable. 
The suggested procedure is: 
1. Choose weights, if desired. 
2. Run a conventional stepwise regression using these as fixed weights, and 
choose a stock. 
3. Refit this stock iteratively and resistantly, ending up with (a) a fit, (b) 
residuals, and (c) second weights, which are products of initial weights and 
weights emerging from the resistant technology. Examine the residuals and 
resistant weights carefully. If they seem reasonable, then: 
4. Repeat step (2) using second weights; 
5. Refit as in step (3), reaching third weights; 
6. Repeat step (2) using third weights; 
7. Refit as in step (4), reaching fourth weights. 
How many repetitions are likely to be needed will have to be learned by 
experience for each type of problem. 
15C. All-subset Techniques 
If we have exactly three carriers, we can fit the 8 - 2 3 possible regressions 
(including the empty one) where we "take out" 
Nothing 
x1 
x and x2 
X2 
x2 and x3 
X3 
x3 and x x 
x3 and xx and x2 
392 /15: Guided regression 
Thus we consider all subsets of the variables in the original state. In this 
unusual ordering, moving from one to the next always takes either a standard 
forward step (moving one carrier in) or a standard backward step (moving one 
carrier out). Thus, if we had access to the pieces of a stepwise regression 
program, we could instruct those pieces to get us all eight regressions with no 
waste motion. 
Such arrangements can be constructed for more than three carriers. Daniel 
and Wood (1971) report the existence and availability of a program to do all 
4096 = 2 TM regressions where the carriers come from a chosen set of 12. 
Because we do not ordinarily want all 4096 sets of answers, Daniel and Wood's 
program reports a selected few, using Mallows' Cv to guide the selection. 
When we can afford this approach, we need not worry about the quality of 
our guidance--we used none. (We could miss something valuable only in the 
unlikely event that Cp did not work well for us.) For examples of using this 
technique, see the Daniel and Wood book (1971). 
An algorithm by Furnival (1971) allows the extension of this approach to 
18 or 20 carriers. Even more important is the development, by Furnival and 
Wilson (1974), that allows the streamlined calculation of the m best subsets, 
each of k carriers, for various values of k. This seems applicable to sets of as 
many as 35 carriers or more. 
The weighted case. Again we have no difficulty in inserting any fixed set of 
weights, specifically variance-dependent weights, into the calculations. 
Equiweighted sums of squares and cross-products are replaced by generally- 
weighted sums of squares and cross-products. If possible, a display of residuals 
against weights (probably in the form "log Iresiduali against log weight") should 
be made and examined for satisfactory behavior. We would like something like 
the plot of "2 log Iresidual I + log weight" against "log weight" not very tilted. 
This would correspond roughly to having successfully weighted inversely as the 
variance. We also look for outliers, special patterns, and other troublesome or 
helpful features. (If desired, this could be applied for the fit by all carriers 
concerned.) 
Resistant versions. It is not so easy to convert all-subsets fits from fixed least 
squares to iteratively reweighted least squares. Until we know more about 
various alternatives, however, a reasonably safe program, when resistant fits 
are in order and there are not too many carriers, would seem to be: 
1. Fit all the carriers together resistantly and record the final weights from 
the resistant technology. 
2. Examine the final weights from the resistant technology and residuals for 
reasonableness. 
3. If the examination uncovers no serious abnormalities, run a weighted 
(15C) 15D: Combined techniques 393 
all-subset procedure.. or a Furnival and Wilson procedure--using the final 
weights of the resistant technology from (1) throughout. 
If variance-dependent weights are desired, the modifications are not hard to 
add. 
15D. Combined Techniques 
What plan should we use when we have many carriers available? 
oone regression with all carriers in? 
o all regressions on all subsets? 
oa stepwise calculation, using a path of selected subsets? 
If no one approach looks promising, we often combine approaches. 
Many times when we look hard at our carriers, we can profitably sort them 
something like this: 
oKey carriers: a few (none to six) that we want to include in any 
regression; 
o Promising carriers: a second set, perhaps up to twelve, or more if we use 
Furnival and Wilson, that deserve somewhat special attention; 
o The haystack: a motley collection of other carriers that deserve limited 
attention. 
If this sorting seems appropriate, we can arrange the procedure in a few large 
steps: 
1. Remove the key carriers. Take the key carriers and regress them out of b. oth 
y and al! other carriers by doing this resistantly. Plan that all further w. ork deals 
with residuals from these regressions. Examine, if possible, the distribution of 
each kind of residual and decide whether it is variable enough that we can use 
it in further work and/or whether we have few enough large residuals to call for 
their special examination and, possibly, a reanalysis with the corresponding 
data-sets put aside. 
2. Choose the promising set and select a promising subset. Take the residuals 
from the first step, and pick out 0 to 12 or more carriers (other than those used 
in the first step) that seem to deserve special attention. Apply an all-subsets 
procedure, perhaps Furnival and Wilson. Select a promising subset as a basis 
for further work. (When in doubt, include one or two more carriers.) Regress 
this subset, also, out of (the residuals_of) y and (the residuals f! the remaining 
carriers. Examine and act upon the new residuals as in step 
394 /15: Guided regression 
3. Stepwise search in the haystack. Run a stepwise regression of the y- 
residuals after step 2 on the x-residuals after step 2. Use this search to select a 
few more carriers to be added to the collection. Examine the residuals of y 
very carefully, and act on them as in step ! when required. 
4. All-subsets check o[ nonkey carriers. Decide whether to stop or check once 
more. To check, take up to 12 or more of 
the carriers selected at step two, 
the carriers selected at step three, 
the carriers considered, but not selected, at step two, 
using this priority order, and run an all-subsets analysis (on residuals from step 
one). 
Nothing can be guaranteed, but this offers a fairly thorough program that 
includes the contributions of the subject matter and exploits the freedom of 
exploration that stepwise regression offers. 
The weighted case. All this can be equally well done with any fixed weights. 
(We have discussed the simplicity of the required changes in the all-subset and 
stepwise procedures above.) 
The resistant case. Here the first step would naturally be a resistant fit of the 
key and promising carriers together, so as to fix a set of weights from the 
resistant technology to be used through the fitting and refitting of these 
carriers. 
The stepwise calculation would then proceed as described at the end of 
Section 15B, using all key, selected promising, and so-far-stepwise-included 
carriers in each resistant refit. 
Another alternative would be to confine the resistant calculations to steps 
1 and 4. If this is done, we will need to at least try adding each unincluded 
carrier singly and resistantly. 
15E. Rearranging Carriers-Judgment Components 
To use many carriers burdens us with two costs: 
olooking at more carriers takes more computing; 
ofitting more coefficients leaves the 9's more variable (if the fit to  was 
adequate with fewer). 
How can we reduce these costs? 
If we can "boil down" our carriers so that we use a smaller number, we 
can save on both costs. 
If we can modify our carriers, while preserving their number (and often 
their overall stock), so that fewer of them appear in our final regression, we will 
(15D) 15E: Rearranging carriers 395 
not only save on computing, but we will also save on the variances of the ]'s. 
Let us see how. 
Modi[ying carriers. Often we have a variety of carriers that measure much the 
same thing. Thus we might, when studying wide receivers for professional 
football teams, be likely to include a variety of carriers that measure size, 
including, perhaps: 
1. standing height 6. length of longer arm 
2. height to extended right arm 7. length of right hand 
3. height to extended left arm 8. length of left hand 
4. length of legs 9. length of fingers of right hand 
5. length of shorter arm 10. length of fingers of left hand 
In addition, we might include carriers for speed, experience, weight, 
jumping ability, pupillary reflex, and the like. 
If we thought we were trying to settle which of the size variables mattered, 
we probably should stop before we begin. The various size measures will be 
closely correlated (because professionals tend to be large, the closeness will be 
reduced because of cutoff or "attenuation"; with high-school players the 
correlation might be higher). But if we want to do nearly as well as we can in 
predicting wide-receiver performance from physique, we can go ahead. 
What about the ten size carriers we listed? Should we use them as they 
stand? Surely not. Indeed, most of the simple correlations among them will be 
high. We might take logarithms of the measurements and combine the logs. Let 
then xj be the log of the ]th measure in our list. 
Eight of the ten come in pairs. We are likely to gain by going to sums and 
differences, using 
X23 -' �(X2 q- X3) and x2- x3= x32 
xs6 = �(xs + x6) and x5 --x6 = x65 
x78 = -}(x? + Xs) and x7- x8 = x87 
c90 = �(x9 + Xo) and x9- Xo -- x09 
Next, we have now six measures of overall size: x x, X23 , X4, X56, X78, X90- Let us 
combine these into three measures, one of overall size, one of arm length, one 
of hand size, as follows: 
X1234567890 = -(X1 - X23 -[- X 4 - X56 q- X78 q- X90), 
X567890 = '(X56 q- X78 q- X90), 
X7890 = �(X78 q- X90 ). 
396 /15: Guided regression 
Then we can pick up the pieces with what is left of seven of the other carriers 
after regression on these three. 
An alternative, possibly more reasonable, would be to use 
in place of xs6 
X567890 
on the ground that we suspect that, where arm length matters, we lose, not 
gain, by including hand and finger size. We only suspect, we do not know. 
In any event, we have a reasonable hopebased on our general insight 
about how people vary and how the sizes of wide receivers affect their 
performance .that if we regress performance on our new set of size carriers we 
will need no more carriers than beforesperhaps we can get along with 
fewer .-to come close to the best regression. 
If, for example, we came out with 
1.74Xs67890 + 0.12X1234567890 
as a satisfactory regression, we would be paying--in terms of var {9(i)}--for 
fitting only 2 coefficients. The fact that this regression can be written, first, as 
1.74 0.12 
(X56 q- X78 -- X90 ) -1 (X 1 q- X23 -- X 4 q- X56 q- X78 q- X90 ) 
3 6 
=0.02xl + 0.02X23 + 0.02x4 + 0.60X56 + 0.60X78 + 0.60X9o 
and then as 
0.02x + 0.0IX2 + 0.01X3 + 0.02x4 + 0.30x5 + 0.30X6 
+ 0.30X7 + 0.30X8 + 0.30X9 + 0.30Xzo 
is irrelevant to what we have to pay for fitting coefficients. We fitted only 2, not 
10. Of course, if one did a lot of preliminary fitting, plotting, and stepwise 
regressions looking for small variances before choosing such a plan, it might be 
possible to choose a remarkably good subset on a basis that sounds like the 
present analysis but that was actually guided by the data rather than judgment. 
If that approach was used, one should be charged for using all the variables; 
there is no reduction in the variance of the coefficients. The variance will be 
larger, and we will not know what it is. 
Judgment components. We need a name for purely judgment-constructed 
combinations of the carriers with which we started. By analogy with the 
"principal components" we will meet in the next section, we will call them 
judgment components. 
When volunteered by persons of sound insight, they may be better than those 
that mechanical processes construct on the basis of the data being analyzed. 
The good judge may succeed sometimes, but not always, because he bases his 
judgment on many other similar sets of data carefully analyzed in the past. 
(15E) 15F: Principal components 397 
Thus the judge may not always be using only intuition: sometimes his judgment 
will be broadly based upon data, even hard data. 
Nonlinear combinations. As we have observed, sometimes a number of carriers 
tend to move together- a number of measures of general economic conditions 
is perhaps the most plausible case. If 
owe can scale these carriers so their values are very nearly alike, and 
owe fear that unusual phenomena, perhaps like oil embargoes, are likely to 
make a few of them much less effective for our purposes, 
we may want to make up a judgment component in a way that will be 
insensitive to the localized distortions that we fear. To do this we might adjust 
the values of the different carriers (mainly by scaling) to come out almost the 
same. Then, treating these values like a sample, we replace them by a single 
value, using either median or bisquare rather than mean to form our first 
judgment component. (One thing to say for the mean .... both good and badmis 
that it combines different values while paying attention to them all and, 
sometimes to our sorrow, neglecting none.) 
15F. Principal Components 
How can we do something with sets of carriers whose mutual relations we must 
assess from the numbers in the data before us, that is, when we lack the 
insights such as we had about the sizes of wide receivers? Given a stock 
described by specific carriers, how can we find linear combinations of the given 
carriers that will be likely, or unlikely, to pick up regression from some as yet 
unidentified y? Can we find a way that pays for fewer fitted coefficients? 
As before, we cannot offer a guaranteed answer. A malicious person who 
knew our x's and our plan for them could always invent a y to make our 
choices look horrible. But we don't believe nature works that way more 
nearly that nature is, as Einstein put it (in German), tricky, but not downright 
mean." And so we offer a technique that frequently helps. 
One approach that seems likely to work is called 
principal �ompononts. 
If we begin by scaling our x's in a natural way (details later), various linear 
combinations 
will have various variances. If we look, instead, at the ratio of such a variance 
to a measure of scale of the cj's, more specifically at 
var (cx + C2X 2 q- ... q- CkXk) 
+ +... + ' 
398 /15: Guided regression 
or, more precisely, at our estimate 
ar(cx + cx2 +... + CkXk) 
+ +... + 
of this ratio, we might believe that linear combinations with higher variances 
will be likely to attract more regression than those with low variances, 
especially if those with low variances are really almost entirely made up of 
"errors and fluctuations". Those with high variances may be responding to 
some common cause, one that may also help to drive the y's up and down. 
The �omponemts. What we propose to do is to compare two quadratic forms in 
the ci's, namely 
+ +... + 
which is the variance of the weighted sum when the x's are uncorrelated and 
have a common scale, and 
ar{cx + cx +... + cx) = c 2 ,ar x + c ar x +... + c,ar x 
+ 2cc2 5ov {x, x2} + 2CLC3 .OV {Xl, 
-{- ... -{- 2Ck-lCk OV{Xk-1, 
the natural estimate of the variance when we do not assume zero correlations. 
It is a well-known result that such a comparison leads naturally to a 
sequence 
of sets of coefficients in which the relative size 
one quadratic form 
other quadratic form 
grades steadily rom one end to the other. 
If A and B are the symmetric matrices representing the quadratic forms in 
some coordinate system, the desired components are determined by the 
solutions of 
In our case, we can take 
/ ar xt 5ov (x, x2) ... 5ov (x, xk) \ 
vlir X2 ...... 
A= 
vlr Xk 
15F: Principal components 399 
and tl 0 0 O 
1 0 0 
B= I 0 
\ 1 
where all below the main diagonal is to be filled in to mirror what is above that 
diagonal. 
Solving such determinantal equations can be tiresome, but computer 
programs fortunately are now widely available for getting these sets of coeffi- 
cients. 
Choice oJ  scale. If we know nothing about the x's, it is often especially 
convenient to scale them so that 
ar xj = 1 for all j. 
This choice puts l's on the main diagonals of A and correlation coefficients 
elsewhere. 
If we can estimate the measurement errors of the x's, we may do con- 
siderably better if we scale the x's so that 
Judged measurement variance of xj = 1. 
Having done this, we can look at all components with too small a variance, say, 
Car {cx + C2X 2 q- ... q- CkXk} < k (or perhaps k/2 or perhaps 2k) 
with a jaundiced eye. A linear combination that is no more variable than would 
be accounted for by independent, judged measurement errors is not likely to be 
telling us much unless its excess size is concentrated on a few data sets. 
If an individual number is mostly measurement error, its value is not likely 
to help us very much. If the average squared deviation of all values is mainly 
made of measurement error, t. he only time we are likely to be told anything 
helpful by the array of values is when a few of them are much bigger than the 
general size of all the rest. Only for such unusual values can we be relatively 
sure that only a small part of their value is measurement error. Only these will 
be telling us something helpful. 
Measurement �ovariances. Tacitly, this argument assumes that the 
MEASUREMENT covariances among the x i  O. (*) 
(Otherwise we ought to put the measurement correlations in the off-diagonal 
elements of B.) In problems where such correlations are modest, as they 
frequently are, we often get a reasonable approximation; but it can be 
unsatisfactory. 
Assumption (*) above differs substantially from 
OBSERVED covariances among the xi  0. (**) 
400 /15: Guided regression 
If we do a good job of separately measuring things that are naturally corre- 
lated, we can easily have assumption (*) hold when assumption (**) does not. 
Thus (*) has a better chance of being nearly true. 
Small components. When assumption (*) holds, we can be relatively sure that 
omitting the small components from further analysis loses us little. Even if we 
did not have any reasonable basis to judge measurement variances, some of the 
small components often look small enough that we are willing to set them 
aside. In either case, we are using principal components as a boiling-down 
process - as a way to replace more carriers by fewer. 
An application. If we had a friend who was about to start stepwise regression 
in a situation involving 20 variables concerned with each of home environ- 
ment, school environment, and community environment, we would feel bad to 
have him start with those 60 (-20 + 20 + 20) variables as his initial carriers. 
If all 60 seemed reasonable variables, well measured, we would much rather 
see him start with 1, 2, or 3 principal components for each bunch of 20 this is 
perhaps as well as we know how to help him in general. 
If, after taking out what he could with these selected combinations, he 
insisted on trying again combining the selected components and all the indi- 
vidual variables, we would tend to discourage himsbut we would admit, if 
pressed, that this would be better than starting with only individual variables. 
If only some of each set of 20 looked good, we would certainly urge our 
friend to start with principal components for the subbunches of good variables, 
certainly doing these first. (Once they had been tried, a trial with a few largest 
components from each good subbunch and a very few from each bad subbunch 
might be reasonable.) 
Factor analysis. Psychometricians and economists have developed a variety of 
techniques, other than principal components, that can be used for boiling-down 
stocks that require few to many carriers to describe. Some were designed to be 
easy by hand calculation; some take advantage of resistant techniques. Most 
have been created as the first step in a "factor-analysis" procedure, whose final 
aim is to identify some linear combinations thought to be more legitimate than 
others in serving as a coordinate system for the boiled-down stock. 
I we follow Section 15D. If we follow the general approach of Section 15D, 
and want to boil down our carriers by applying principal components to either 
all our carriers together or to each of several groups of carriers, the pressures 
on us are relaxed, because we can draw from each use of principal components 
a fourfold partition of these components: 
o a few (may be none), to be "must" carriers; 
oa few more, to be "all subsets" carriers; 
oa few to several more, to be "stepwise" carriers; 
othe remainder, to be set aside. 
(15F) 15G: How much are we likely to learn? 401 
With four parts, of which only the last is set aside, we may do a fair job of 
partitioning. 
Reshaping. Separating big components from little components can be 
effective although middle components are not likely to be sharply separated 
from either. The distinctive names of 
o the largest component, 
othe second-largest component, and 
othe third-largest component, 
may be merely pseudonyms. Even when we analyze a large amount of data, the 
largest component often fluctuates considerably in direction. and so do the 
others. 
As a result, reshaping the large components by 
o choosing new linear combinations of the few largest principal components 
so as to make them more interpretable, 
odropping carriers that appear with relatively small coefficients, 
o adjusting coefficients to well-selected standard values (see References for 
recent work of B. F. Green), 
should not make our components much less useful and may make them 
considerably more interpretable. Thus, we can profitably reshape in these ways. 
15G. How Much Are We Likely to Learn? 
We started this chapter with a limited aim: to do a reasonable job of regression 
as description or regression as exclusion. In the regression chapters, we have 
warned repeatedly that we usually dare not take the coefficients seriously. We 
warn again. Nevertheless, let us sketch some circumstances where we might 
pay attention to some of the coefficients. 
Stable �oe�ients. Earlier chapters showed that we cannot count on the 
coefficient b3 of xj in a fit involving many x's to resemble, either in size or sign, 
the coefficient bj of x when x is fitted either alone or accompanied by a 
different set of carriers. If, nevertheless, we carry out a stepwise regression and 
find, when we watch b3, that bs changes relatively little all the way from 
beginning to end, we are appropriately tempted to think that some useful 
meaning may be given to its value. 
If this constancy starts only after a certain set of other carriers is fitted, we 
will usually benefit from thinking hard about the interpretation of that coeffi- 
cient when fitted in the presence of those other carriers. If the interpretation 
thus reached is likely to be relevant to our problems, we can afford to pay heed 
to this coefficient. 
402 /15: Guided regression 
Judgment or principal components 
The nice situations just described are more likely when we have used judgment 
components or principal components. They are also likely to be flagged by 
coefficients relatively large for the first component (or first few components) 
and dropping rapidly for the others. Such coefficient patterns often signal both 
a well selected set of components and at least one possibly meaningful 
coefficient. 
Warning. Without such conditions, interpreting coefficients can be most mis- 
leading. 
15H. Several y's or Several Studies 
Given several responses in a single study . or several studies with the same 
response--or both - we have a choice of trying to do well with each separately, or 
to do well with all together. 
To treat things together, we allow ourselves separate regression coefficients on 
any xj in the regression--for each y, for each study, for both but we add up 
(after the most reasonable scaling we know how to do) the sums of squares of 
residuals, or the reductions in such sums of squares. Except for this, and the 
need to have parallel pictures to examine separately the possible needs to 
re-express different y's in our study or studies, all is as before. (We should not 
expect to use different expressions for the same response in different studies 
using the same carriers. Should the data suggest this, we ought to examine it 
very carefully indeed.) 
151. Regression Starting Where? 
To suppose that everything has to be discovered anew in each new data set 
breeds waste and loses efficacy of analysis. Given the idea of a single analysis, 
which is laid down in concrete in advance no matter how inappropriate the 
data may show it to be, we would have some excuse for forgetting the past. 
Legal constraints may occasionally produce such straitjackets, but those will be 
rare exceptions. In most applications of regression we did not control the 
values of the x'sNand no one analysis is forced onto the data. Thus we ought 
to expect to look both to the past and to parallel bodies of data in search of 
information, hints, and intuitions. 
At the earliest stage, such sources often equip us with a list of x's that 
ought to be considered, come what may. Often, too, we can have a list of likely 
candidates--and a list of those not so likely. 
Next, the past can often give us guidance whether the first-aid expressions 
of our variables are the best we can do at least before questioning carefully 
the data before us. 
(15H) 151: Regression starting where? 403 
Next, the past may well be able to tell us what values certain coefficients in 
our regression are expected to take--before we look at the present data. 
If we are taking a flexible approach, where we let the data guide us as to 
which carriers stay in the regression (and which are allowed to fall by the 
wayside), we can use our advance information--or appropriate theory, 
trustworthy or dubious to set initial values for certain coefficients. Thus if we 
expected 
y --- constant + 17x + ?x2 + 5X3- ?X4 -{- 3xs, 
we would do well to analyze 
y- (17x + 5x3 + 3xs), 
rather than analyzing y. 
Suppose that we find this composite variable to be fitted by 
(1.2 + 1.5)x + (4+ 1)x2 + (-3 + 2)x3 + (7 + 3)x4 + (1 + 0.1)xs, 
where the numbers after the + are estimated standard deviations of the 
estimated coefficients before the same +. We might well reduce this fit to 
4X2 + 7X4 + 
or, more precisely, to 
(4 d- 1)X2 + (7 + 3)x4 + (1 q- 0.1)xs. 
If we did, then the fit to y is reasonably reported as 
y --- 17x + 4X2 + 5X3 + 7X4 + 4X5, 
where two terms come from our original subtraction, two from the fit, and the 
fifth, more clearly written as 
4x5 = x5 + 3xs, 
from both. 
When it comes time, as it should, to quote uncertainties for these coeffi- 
cients, it seems reasonable to consider that when 
b + st, 
is replaced by zero, we ought perhaps to consider a mean-square error of 
b 2 when Ibl >-Isol, 
$2 
b when Ibl < Isol, 
so that the result would be 
y --- (17 q- 1.5)x + (4 q- 1)x2 + (5 d- 3)x3 + (7 + 3)x4 + (4 � 0.1)xs, 
where, for x3 the +3 is q-b, though �1.5, +1, +3, and q-0.1 are +sb. Note that 
at this stage we attend to the b and s of the fitted part rather than the sum of 
404 /15: Guided regression 
the "advance" coefficient and the fitted coefficient. If we had not reduced x 
and x3 out of the fit, the result would have been 
y -- constant + (18.2 + 1.5)x + (4 + 1)x2 
+ (2 + 2)x3 + (7 + 3)x4 + (4 + 0.1)xs. 
(Throughout this discussion we have acted as if the correlations between the 
estimated coefficients can be neglected. This is, of course, not always the 
case � some would even say "not commonly the case".) 
(An early form of this approach appeared in a presentation by E. C. 
Harrington at a November 1956 symposium at North Carolina State College.) 
The broad philosophy is clear--science builds on previous work. Estima- 
tion of coefficients often needs to do the same. 
15J. Arbitrary Adjustment 
Sometimes we are concerned with regression as removal and we find that the 
precision of the important coefficients, when estimated from the data before us, 
is not as high as we need. At such a time, we should ask ourselves whether 
external coefficients intuited from past knowledge does not prescribe some or all of 
the relevant coefficients with greater precision than do the data before us. If 
they do, we ought to take seriously the possibility of adjusting our y's 
according to such external coefficients. Sometimes, indeed, either theory or 
semiquantitative understanding gives us coefficients that are better than even 
the consolidation of past knowledge. When either is so, we ought not to use the 
coefficient or coefficients estimated from our particular piece of data--unless 
the estimates are demonstrably significantly different from those provided by 
the past, by theory, or by semiquantitative understanding. (For one example 
see Cox (1957).) 
Again, not everything must be reproved in every study. 
Summary: Guided Regression 
One approach to the question of how many carriers to include in our stock 
leads to choosing to minimize: 
E(Y- ) 
# of data sets MINUS # of carriers' 
Better considered approaches lead to the minimization of such expressions 
as Mallows' Cp, Allen's PRESS, or the additionally divided ratio (mean 
square)/(degrees of freedom). 
One way to avoid the penalties incurred by fitting coefficients of many 
carriers is to recombine our carriers into linear combinations, either by pure 
judgment or by the determinantal equations of the method of principal 
components. 
(Summary) References 405 
We can apply stepwise techniques of fitting in which carriers are added 
to or removed from the fit one by one, using any of a variety of standard 
computer programs. 
Some possible approaches are: 
o a plausible pattern of calculation when we want a resistant stepwise fit. 
o Daniel and Wood's all-subsets procedure to up to 12 carriers. 
oFurnival and Wilson's "best m subsets for each k" procedure to up to 35 
carriers. 
omodifications of these procedures to accommodate variance-com- 
pensating weights. 
o a plausible pattern of calculation when we want resistant analogs of the 
"all-subsets" or "best m for every k" procedures. 
oa combined approach to data with many potential carriers, for which the 
first step is to divide the carriers into (i) key carriers, (ii) promising carriers, 
(iii) the haystack, and in which the successive steps make careful use of this 
separation. 
oreplacing bundles of significantly intercorrelated carriers by linear combi- 
nations selected by judgment, of which there may be either fewer or an 
equal number. 
oreplacing bundles of significantly intercorrelated carriers by linear combi- 
nations selected by applying principal components to these carriers (not 
including the response). 
ocombining either of these latter approaches with the key-promising- 
haystack approach. 
oobserving how a "given" coefficient evolves through the steps of a 
stepwise procedure. 
ostarting our regression analyses, not from all coefficients zero, but rather 
from each coefficient at its best value, before looking at the data at hand. 
References 
Allen, D. M. (1971). "Mean square error of prediction as a criterion for 
selecting variables." Technometrics, 13, 469-475. 
Allen, D. M. (1974). "The relationship between variable selection and data 
augmentation and a method of prediction." Technometrics, 16, 125-127. 
Anderson, R. L., D. M. Allen, and F. B. Cady (1972). "Selection of predictor 
variables in linear multiple regression," in Bancroft, T. A. (Ed.), Statistical 
Papers in Honor of George W. Snedecor, Ames, Iowa: Iowa University Press. 
Anscombe, F. J. (1967). "Topics in the investigation of linear relations fitted by 
the method of least squares." J. Roy. Statist. Soc., Series B, 29, 1-29. 
Cox, D. R. (1957). "The use of a concomitant variable in selecting an 
experimental design." Biometrika, 44, 150-158. 
406 /15: Guided regression 
Daniel, C., and F. S. Wood (1971). Fitting Equations to Data. New York: 
Wiley and Sons. 
Furnival, G. M. (1971). "All possible regressions with less computation." 
Technometrics, 13, 403-408. 
Furnival, G. M., and R. W. Wilson, Jr. (1974). "Regression by leaps and 
bounds." Technometrics, 16, 499-511. 
Green, B. F., Jr. "Parameter sensitivity in multivariate methods?' Submitted 
for publication in Psychometrika. 
Macdonald, N.J., and F. Ward (1963). "The prediction of geometric distur- 
bances indices: 1. The elimination of internally predictable variations." J. 
Geophys. Res., 68, 3351-3373. 
Mallows, C. L. (1973). "Some comments on Cp." Technometrics, 15, 661-675. 
Tukey, J. W. (1967). "Discussion of Anscombe's paper." J. Roy. Statist. Soc., 
Series B, 29, 47-48. 
Chapter 16/E xam ining 
Regression Residuals 
Chapter index on next page 
In Section 3J we treated economical plots of residuals against any chosen x. 
Such plots are intended for diagnosis, not for definitive results. 
We need diagnosis after fitting a regression of prechosen form to data. We 
ask "Are we through? How can we look ahead and guess which possible 
additions to our regression may be important?" These are good questions, but 
we may not be able to address them directly. 
We ask first about y, ], y - , and then about other variables. Because the 
y and ] problem is more general than regression, we discuss it first. 
16A. Examining 
Suppose that we had just y and x, and had fitted bx, finding 
ave (y - bx) 2 = s 2. 
To find out what is left, we plot the residuals y - bx against x or bx. We do not 
plot them against y, because this is likely to be misleading, as we now explain. 
Suppose y is highly variable, as illustrated in the leftmost panel of Exhibit 
1, but bx varies only modestly, staying close to the constant value c; then 
Y = (y - bx) + bx  (y - bx) + c, 
and all the points (y, y- bx) are close to the line containing the points 
Exhibit I of Chapter 16 
The rightmost panel illustrates the misleading character of the y-  versus y 
plot when the relation between y and x is slight. 
.y y -y y -y 
20 - 1 10:- 
O- I. I I ... t I   -IO- I  I I t � a: -I00 I I I I,>.y 
I 2 3 4  " i 2  4.   IO I' 20 
407 
408 Index for Chapter 16 
16A Examining p 407 
16B Variables--and other carriers 419 
16C The next step: Looking with regard to an old 
variable. to, 423 
16D Looking with regard to a new variable. t,o 431 
16E Looking for additional product terms 435 
16F In what order? 446 
Summary: Examining regression residuals 448 
16A: Examining  409 
(t + c, t). This is illustrated in the rightmost plot of Exhibit 1. The tightness and 
strong slope of this line would seem to indicate that it is possible to improve 
the original linear fit. However, here the original linear fit, though poor, is the 
best linear fit possible, so the indication from the plot of the residuals against y 
is fallacious. When we plot y - bx against x, as in the middle panel of Exhibit 
1, we have a vertically-wide blur and no indication of tilt, thus giving us correct 
insight into both the adequacy of the choice of b and the substantial size of the 
residuals. 
Suppose, instead, that we thought x would do in place of bx. Then we 
would plot y - x against something. The slope of y - x against y would show, 
by a slope close to 1, when the fit is poor, but not whether the slope is right or 
wrong. The slope of y - x against x would show, by its slope of b - 1, when 
we need to use some bx instead of x. Therefore we plot residuals y - ] against 
x, not y. 
The next question we ask, once we have begun with a fit , is "Are we 
getting full value from ]? More specifically, will a function of ] fit y better?" If 
the answer is yes that a modification will fit better then we usually have two 
choices: 
o fit the original y with a function of , 
o re-express y so that it can be fitted more easily by something new drawn 
from the stock that produced y. 
Let us illustrate with an example. 
Example. Scattered bank deposits. Exhibit 2 gives one amount of loans out- 
standing for each of 16 quarters for the years 1923 through 1927. Each 
amount is for all banks in one Federal Reserve district, the districts varying 
haphazardly among Boston, New York, Philadelphia, and Richmond. A fit, of 
the form 
linear in time PLUS district effect 
is shown at the bottom of Exhibit 2, together with the residuals in the 
rightmost column. 
Exhibit 3 shows the results of ordering on ] and then smoothing both.  
and y - y. After this has been done, these two can be added together agmn, 
and the result, h(]), smoothed, giving a possible function of ]. 
Exhibit 4 shows the smoothed y -  against , which is clearly concave- 
upward--we call  or J or % concave-upward and r- or c- or  convex- 
upward--suggesting either: 
using a concave-upward function of  with the present y, or 
using a convex-upward function of y and fitting it with a new ]. 
410 Exhibit :2/16: Examining regression residuals 
One natural choice for the convex-upward function of y would be log y. We 
explore this shortly. This corresponds to using a concave-upward function of  
of the form 
a� . 
Exhibit 5 shows our smoothed h(]) in comparison with 
1500e�'���19 = . 
Exhibit 2 of Chapter 16 
16 scattered values of loans outstanding (all banks in a Federal Reserve L:trict at 
a call date near the close of the quarter indicated; in millions of dollars) 
Quarter � = 
I District I name and # Loans 1'�i I y - l 
Boston 4Q23(-8) 3146 2740 406 
New York 1Q24(-7) 8229 9251 - 1022 
Phila. 2Q24(-6) 1940 1624 316 
Richmond 3Q24(- 5) 1751 1269 482 
New York 4Q24(-4) 9119 9578 -459 
Boston 1 Q25(-3) 3487 3285 202 
Richmond 2Q25(-2) 1804 1596 208 
Phila. 3Q25(-1) 2294 2169 125 
Phila. 4Q25 (0) 2368 2278 90 
Richmond 1 Q26 (1) 1873 1923 -50 
Boston 2Q26 (2) 3796 3830 -34 
3G26 (3) 
New York 4Q26 (4) 10976 10450 526 
Richmond 1Q27 (5) 1829 2359 -530 
Phila. 2Q27 (6) 2509 2932 -423 
New York 3Q27 (7) 11731 10777 954 
Boston 4Q27 (8) 4031 4484 -453 
* Fit used: 
3612 (Boston) 
10014 (New York) 
 = 109 x (Quarter #) + 
2278 (Phila.} 
1814 (Richmond) 
s) SOURCE 
Several annual reports of the Federal Reserve Board. 
16A: Examining 1?/Exhibit 3 411 
Our interest here is directed toward the residuals, rather than toward 
the fitting procedure that produced the particular values of the coefficients 
a and b. Thus we do not present the fitting process here, only the final results. 
Clearly, while the fit is not perfect, it is encouraging. 
Exhibit 6 gives the numerical values of 15008 �'���9 and the corresponding 
residuals y - y. Values of log y are gven n Exhibit 6, as well as the fitted 
values and residuals obtained from the particular fit displayed at the bottom of 
the exhibit. Exhibit 7 shows the smoothing of both sets of residuals, after 
sorting on the fit. Exhibit 8 plots the residuals of the form 
y- 1500eO-OOm9 - y- y. 
They show no noticeable structure. 
Exhibit 3 of Chapter 16 
Smoothing y-  against . 
y - 
ordered smoothed* y - C/ I smoothed I h()** J smoothed 
1269 T 482 1751 
1596 208 316 1912 1832 
1624 h 316 208 1832 1912 
1923 -50 125 2048 
2169 125 90 2259 
e 
2278 90 90 2368 
2359 -530 90 2449 
2740 406 -423 90 2830 
2932 -423 202 -34 2898 
3285 s 202 -34 3251 
3830 -34 3796 
4484 a -453 4031 
9251 -1022 -459 8792 
9578 m -459 9119 
10450 526 10976 
10777 e 954 11731 
* Smoothing is by running medians of three until no more changes occur. 
** h() = smoothed  + smoothed (y - ). 
412 Exhibit 4/16: Examining regression residuals 
Exhibit 4 of Chapter 16 
Smoothed (y- ) plotted against  for the values from Exhibit 3. 
16A: Examining /Exhibit 5 413 
Exhibit 5 of Chapter 16 
Smoothed h() compared with 1500e �-����, both plotted against y. Dots are 
o. ooo[9.y 
ooo 
0 ! ! ] I j/ 
5000 I0,000 
414 Exhibit 6/16.' Examining regression residuals 
16A: Examining /Exhibit 7 415 
416 Exhibit 8/16: Examining regression residuals 
Exhibit 9 plots the residuals of the form 
logo y - los0 y. 
We see that their median is 0.003, a constant we might add to lo; but 
beyond this, as with the residuals from the exponential fit, we see very little 
new. Havg no. ticed t.he. unstructured appearance of theresiduals when t1otted 
againit lgo m lExhbt 9, we plot the smoothed h(ig) against lo in 
Exhibit 10 and recognize that we again have a plot close to a straight line, with 
a hint of a spur at the lower left. 
The conclusion of this example is that we can fit raw values of loans 
outstanding with the form 
where  is of the form 
(straight line in date) PLUS (district effect), 
or that we can use this form directly to fit log (loans outstanding). 
Exhibit 8 of Chapter 16 
The smoothed residuals y-1500e �'���9 plotted against 
.V-  oo e �'��m- 
smoothed 
\ 
\ 
400 - \\ 
\ 
\ 
\ 
\ 
ZOO - \ 
0- '/ \\ 
J \\\ 
I ,  
-ZOO0 5000 I0,00 
16A: Examining /Exhibits 9 and 10 417 
Exhibit 9 of Chapter 16 
The smoothed residuals Iogo y go Y P ot against Iogo y 
to,o.Y- 10,lo J/ 
Exhibit 10 of Chapter 16 
The smoothed function h(Io%o plotted against Iogoy. 
smoked 
418 Exhibit 11/16: Examining regression residuals 
More points? The example we have just seen, involving 16 data points, was 
small enough for us to work point by point. Even 40 pointa would be enough to 
make an individual-point plot of 
(9, Y - 
less than satisfactory for answering our question about full value from 9. 
In Section 3J, we looked deeper at such questions o large bodies o data by 
grouping the y.- values according to t. he values of , summarizing the 
groups, smoothing the summaries, and plotnng the result. Exhibit 11 shows the 
corresponding numbers for our last example. We try both (1) to keep the sizes 
of the groups of residuals, the y -  groups, somewhat the same size, and (2) to 
keep the lengths (and separations) of the  intervals also about the same. As the 
example illustrates, we oten cannot do both perfectly. The resulting plot is 
displayed in Exhibit 12. From this picture we can perceive much o the basic 
structure shown in the slightly more time-consuming analysis of Exhibit 7 
pictured in Exhibit 8. The final rise at the end has, however, disappeared. (It 
would have required very small groups to preserve it.) 
What then are the possibilities or examining y-  with respect to 97 
Arranged in rough order o increasing eitort, we can: 
1. Make a plot of extremes, 10 high and 10 low points. 
2. Smooth the data and plot the smoothed result. Of course, if we have a lot 
of points, we can use the device of Section 3J. 
3. Combine (1) and (2). 
4. Plot all the points. 
5. Combine (2) and (4). 
6. Do (2); then plot all deviations from this smooth. 
Exhibit 11 of Chapter 16 
Grouping and medianing the residuals of the raw y (from Exhibit 7). 
!  Jl Residual = z ] ! Median ] lSmoothedll Median ? ] 
(in hundreds) = y- 1500e �'���9 resid.-- z' z' (in hundreds) 
12-19 -158, -221, -100, -289 -190 16 
21-29 29, 56, -519, 621, - 109 29 25 
32-38 687, 691 689 515 35 
44 515 515 44 
92-95 -470, - 137 -304 80 94 
104-107 52, 107 80 106 
(16A) 16B: Variables--and other carriers/Exhibit 12 419 
Once there are more than a few points, we prefer any of the others to (4), and 
all but (4) to (5). Since (4) and (5) are relatively more work than most, we don't 
expect to do them often. If we want to work hard, (6) is often the thing to do. 
16B. Variables--and Other Carriers 
We are used to re-expressing variables. For example, we may replace t (t > 0) 
by log t, or replace s by s 3 or -1/s. What are the essentials of re-expression? 
For us they are: 
oability to calculate the values of either expression from the other; 
o(usually) a monotone relationship between the expressions, so that when 
one goes up so does the other, and when one goes down so does the other. 
What happens when a function of a variable is not a re-expression? 
Most often, probably, it involves some kind of folding. Thus t 2 is folded 
once at zero, with -t and t being put together, since (-0 2= t 2. If 0, for 
-,r --- 0 -< +,r, is our variable, then cos 0 is folded once (at zero) while sin 0 is 
folded twice (at ,r/2 and -,r/2). If we consider t for all real values, then cos t is 
folded at every integer multiple (positive, zero, or negative) of ,r, while sin t is 
folded at every t of the form "(,r/2)+ an integer multiple of ,r". In more 
general cases, of course, the folding involves stretching or shrinking of one side 
of the fold as compared to the other. 
Exhibit 12 of Chapter 16 
Smoothed medians z  of residuals grouped according to [r plotted against 
median fr (in hundreds) for values from Exhibit 11. 
\ 
i 
I 
40l - I 
\ 
! 
! 
/ 
! 
00 - ! 
I 
/ 
0- / 
/ 
/ 
/ 
/ 
-200- ' 
I I I I . 
IO o ;o 70 90 
420 /16: Examining regression residuals 
Besides re-expressing single variables, we could concern ourselves with 
re-expressing pairs of variables. Now things are less simple. In particular, there 
seems to be no handy package of re-expressions corresponding to the one we 
use for single variables: started powers (and their logarithmic and exponential 
limiting forms). We are not yet ready to treat the re-expression of pairs of 
variables generally and simply. 
Carriers and variables. When we fit a quadratic, for example, we have choices 
as to how to write it. We can write 
13 + 3t- 2t 2, 
or, equivalently, 
18 + 0(t - 12) - (2t 2 - 3t + 5). 
In the first expression, we say the carriers are 1, t, and t 2, and in the second, 1, 
(t- 12), and (2t 2-- 3t + 5). More generally, we could speak of the set of all 
quadratics in t with carriers 1, t, and t 2 as 
{all a + bt + ct 2} 
or, equivalently, of 
{all d + e(t- 12) +/(2t 2- 3t + 5)} 
with carriers 1, (t- 12), and (2t 2- 3t + 5). 
We now want to relate stocks or fits to variables. Let us begin by relating, 
to certain variables, a stock or fit- as in this example: 
{all a + btl + ct + dt2 + et32}. 
Here the carriers are 1, t, t, t2, t, each of which is fixe.d either by a value of t 
or by a value of t2 (or, in the case of 1, by a value of ether). We can take the 
variables involved as (t, t2) or (t, t2) or (tl, t}), all simple re-expressions of one 
another. (We cannot take t and t as the variables if negative t2 can occur, 
since the value of t is not determined by the value of t22.) 
Simplicity would ordinarily lead us to say merely that the variables are tl 
and t2, a choice that is not forced but is likely to be helpful. 
If we change how the stock or the fit--is carried, it can be natural to 
change what we think of as the variables. In 
{all a + bt + ct2} 
or 
19 - 5 t + 25 t2, 
it is natural to think of t and t2 as the variables. In the expressions equivalent 
to those just given, 
{all a + d(t + t2)+ e(t2- t)} 
or 
19 + 10(t + t2) + 15(t2- t), 
16B: Variables--and other carriers 421 
it is natural, but not necessary, to think of t + t2 and t2 - t as the variables. 
Are these different? 
In one sense, they are the same. After all, (t + t2, t2 - t) is a very simple 
re-expression of (t, t2). In another sense, they are very different. In analogy to 
Lincoln and Barnum's famous saying, perhaps, most of us always think one 
variable at a time and all of us do this in most of our thoughts. So a 
re-expression that mixes up variables is not trivial in its impact on our thinking. 
Why should we make such re-expressions? 
Basically, we re-express to make matters simpler. To write down, look at, 
or think about 
1000(t + s)- 0.004(t- s) 
is simpler than to write down, look at, or think about 
999.996t + 1000.004s. 
Similarly 
{all a + b(t + s) + c(t + s) 2 + d(t + s) 3 + e(t- s)} 
is simpler than 
{all a + b(t + s) + c(t 2 + 2st + s 2) + d(t 3 + 3t2s + 3ts 2 + s 3) + e(t - s)}, 
especially if the first is written 
{all a + bu + cu 2 + du 3 + ev}, 
where u = t + s and v = t- s. 
Habits o[ mind. In every field where data are collected and analyzed, we have 
habits of mind about which are "the" variables. These habits change with time, 
and data have their impact on these changes. For the last few decades "Gross 
National Product" has been "a variable" for economists, businessmen, and 
newspaper readers. Yet not too long ago, long explanations would have been 
necessary both about how values that were separately observed were put 
together and about what the result might mean. For another example, turn 
back to a century and a half ago when Benjamin Rumford, the German count 
from New England, studied the behavior of "heat" and separated the ideas of 
"temperature" and "quantity of heat," thus introducing variables that, on the 
one hand, made the behavior of the data simpler and, on the other, laid the 
foundation for modern thermodynamics. 
A reasonable rule for variables is: Use the variables currently popular, 
unless some new variables make the behavior of the data noticeably simpler; 
never stick with popular variables if others make the data much simpler. 
After all, different people are entitled to have different ways of thinking 
about the same problem sometimes one way may be much better than the 
others, though perhaps only time will tell. 
422 /16: Examining regression residuals 
Naturaln�ss o: variables. In practice, our understanding of the world often 
prescribes how natural we will judge certain variables to be. The combination 
Age of mother PLUS # of living children 
is not likely to be accepted willingly as a natural variable. On the other hand, 
the pair 
Age of husband + Age of wife 
seems, d anything, more acceptable than the separate variables 
Age of husband, 
Age of wife, 
especially since the difference in ages is much commented on in daily li[e. The 
commoner pair o carriers are 
Age of husband, 
Age of husband - Age of wife, 
or, perhaps, 
Age of wife, 
Age of wife - Age of husband. 
This is not just a matter of algebraic structure. While we would often accept 
# of boys PLUS # of girls 
as a natural family variable, we are much less likely to accept 
# of boys MINUS # of girls. 
Nonlinear steelcs. In the linear cases discussed so far, we note that the carriers 
in an expression can be obtained by taking the partial derivatives with respect 
to the coefficients. Thus the carriers in 
f(t) = a + bt 
are 
of 
Oa Ob 
If we use this approach, what happens when we have nonlinear stocks or fits? 
If, for example, our stock is 
{all ae-}, 
then if the carriers are the derivatives with respect to the parameters, we get 
e -' (derivative w.r.t. a) 
-ate - (derivative w.r.t. b) 
(16B) 16C: Looking at an old variable, tol d 423 
(equivalently, e - and te-'). In general, then, in the nonlinear case the carriers 
depend upon where we are in the stock; here, on the value of b. 
Similarly, if the fit is 
13e -?t, 
we take the carriers as 
e -?t and 13 te -?t 
or, equivalently, as 
�--7t and t� -7t. 
16C. The Next Step: Looking with Regard to an Old Variable, told 
We can go on with the diagnosis of residuals from regression more easily ff we 
have a standard setup. So let us suppose that we have fitted 
 -- bo + bXl +"' + 
where each x is a function of exactly one of the variables, any of which we can 
call toa, 
tl, t2,..., th (for h < k). 
There are likely to be other variables around as well, so let any one such be 
Some examples (of everything but thaw) may help. One, with k -- 3 and 
h = 1, is 
 = 13 - 2t + 3t 2-- 4t 3, 
where the apparent x's are 1, t, t 2, t 3, and we need only the one variable t. 
Another, with k = 5 and h = 3, is 
 = 8 - 3t + 4t 2- 3s + 782- st, 
where the apparent x's are 1, t, t 2, s, S 2, St, and we need three variables, most 
naturally, t, s, and st. (We need st because we are sticking to "each carrier as a 
function of one variable".) Still another example, with k - 5 and h = 5, is 
9 = 4 + 3Xl + 2X2 + X3- 7X4- 6x5, 
where the apparent x's are 1, x, x2, x3, x4, xs, and the natural t's are 
XI X2 X3 X4 X5. 
Looking at a lold � How do we find out whether we are getting full value from 
one of the variables already represented in the fit? Whichever of t, t2,..., th 
this variable may be, let us call it told- What it is reasonable to do depends in 
* It is not important whether bo is included; we have put it in because it appears more often 
than not in practice. 
424 /16: Examining regression residuals 
part on how frequently tola is represented--or nearly represented--among the 
carriers of our fit. 
If told is represented only once, or perhaps twice, we may be able to 
proceed very much as we did with ]. Ordering the y - ] according to total and 
th. en smoothing the y - ] values may be revealing. If we have many points, we 
wfil want to take medians of groups before smoothing. 
To do this, we do not need to care much about how tola is initially 
expressed. If we change expression, the sorting order and the smoothing of 
y-  will be unaffected. All that can happen to our plot is a tendency to 
"crowd up" or spread out somewhere. If our plot does crowd up, we re-express 
told and re-plot without any need for recomputing the smoothed y -  values. 
(We need to re-plot to avoid crowding.) 
When will this simple and direct approach be satisfactory? Basically, when 
we can get enough well-determined points to plot, either initially or by taking 
medians of groups. If we have three or four times as many points as times when 
the systematic behavior appears to change sign, and if the perturbations to 
which each point appears to be subject are small compared to the systematic 
behavior, we will likely see what is going on. If we have only one point for each 
interval (now unrevealed) of constant sign of the systematic behavior, we will 
not see enough. Equally, if there are three or four points in each interval of 
constant sign but the apparent perturbations are large compared to the systema- 
tic behavior, we will not see much either. We are forced to do something better 
than "smooth and plot" when the data are not generous with information 
about what to do next. 
Next carrier. If we need to squeeze the data for information about 
unaccounted-for dependence on told, which often happens when we already 
have several representations of told in our fit, we will have to work with a 
specific carrier or two specific carriers. Our first task is to pick this carrier, or 
these carriers, by picking specific functions of told. 
Suppose, for example, that tola = t, and that 
 = a + btl + ct + dt2 + et3. 
It might now seem natural to add t as the next carrier involving t. Taken 
properly this is quite correct, but we dare not take it too literally. Plotting 
y - ] against t3, for example, is not going to show us anything much, even if 
there is a lot there. For t is just a re-expression of tx, and we held out little 
hope for good results from plotting against t. 
Suppose the fit were 
 = 1 + 23t- 5t 
(where we drop the t2 and t3 terms for a moment for simplicity) and we choose 
t] as a carrier. The new fit would be something of the form 
] = 1 + 23t 5t2 + h(t3 � 2 
-- -- 12t -- jt - jo), 
16C: Looking at an old variable, to,d 425 
where the expression in parentheses 
Xdot '-' t - ]2t- jz t,- jo, 
is the result of "eliminating" the earlier carriers from t. ("Eliminating" here 
means fitting 
t by j2t + jtx + jo 
using the same sort of fitting procedurespossibly least squares- and forming 
the difference. Such elimination is sometimes called orthogonalization.) If the 
change in the fit is by a multiple of Xdot, then we should use Xdot, and not t, in 
examining the behavior of the y - . 
If we go back to the original, slightly more complicated example, with the 
t2 an.d t3 terms included, we will, of course, have to include these terms in Xdot, 
making it 
Xdot = t3-- t?, 
where 
+ j,t, + jo + + 
Thus our routine procedure, if toga appears more than once or perhaps 
twice among the carriers, begins with these steps: 
,pick a next carrier (or two next carriers problems do occur, for example, 
where adding t] has no effect but adding t helps). 
o find the corresponding Xdot (or the two corresponding Xdot'S) as the result 
of orthogonalizing the next carrier(s) to all the carriers we have already 
used. 
osort the data points on Xot and smooth the sorted y - 's (or do this first 
for one Xaot and then for the other). 
It is possible that finding just one Xaot and plotting the 10 high and 10 low 
extremes against Xaot will be revealing; but if it is not, we go on to do the 
appropriate smoothing and afterward, as seems necessary and effective, to: 
2. plot the smoothed result; 
3. plot the extremes and the smooth, or 
6. do (2); then plot all deviations from this smooth. 
The extra work of (6) often does pay off. 
Note that we have not considered plotting the deviations of the residuals 
from the smooth against the selected carrier. Plotting such deviations would 
not tell us about such matters as how the spread of the apparent fluctuations 
seems to depend on toga. To ask such questions we would plot against toga and 
not against Xot. 
426 Exhibit 13/16: Examining regression residuals 
Example. Fitting an unperturbed exponential. The left section of Exhibit 13 
shows !0 values of an exponential function, at equally spaced values of toa, the 
least-squares quadratic fit, and the corresponding residuals. These residuals are 
plotted against told in Exhibit 14, and will be plotted against Xaot in Exhibit 15. 
Since we are working with unperturbed values whose natural behavior is very 
smooth, we have no difficulty in seeing the nature of the behavior in Exhibit 
14, as well as in Exhibit 15. (In both exhibits we have shown a dotted trace 
through the results of repeated median smoothing of adjacent sets of 
three points, called 3R smoothing. For Exhibit 15, the residuals from the 
quadratic fit are ordered on Xdot and then smoothed.) 
What additional fit do these exhibits suggest? In Exhibit 14 we appear to 
have zeros at toga values of 0.8, 4.6, and 8.3, suggesting using a multiple of 
P3 = (toga- 0.8)(tod- 4.6)(told- 8.3). 
Exhibit 13 of Chapter 16 
Residuals when an exponential, with and without perturbations, is fitted by a 
quadratic, and Xdot based on (told) a. 
Exp. PLUS 
i_ Exp�nential() I Perturbati�ns(3) pertur- 
bation; 
Quadratic] ]Quadratic I IResid- i resid- !(s) 
l to,ai ! Raw I fit'2' [Residuals II Raw I I fit("' I uals uals IXdot 
0 1.000000 1.006394 -.006394-.0035 -.0006 -.0029 -.0093 -42 
I 1.105171 1.103277 .001894 .0012 -.0007 .0019 .0038 14 
2 .22o .23 .oo2o .ooo -.ooo .oos .oo 3s 
3 .3 .3o .oo4so-.oo -.ooo -.oo -.oo2 3 
 .2s .n .oo2a3 .oo -.ooo .ooso .oo7  
5 1.648721 1.650336 -.001615-.0056 -.0006 -.0050 -.0067 -12 
6 1.822119 1.826982 -.0063-.0040 -.0004 -.0036 -.0085 -31 
 2.o, 2.o,s -.oos,, .oos -.ooo2 .ooss .oooo -, 
 2.2ss 2.22 -.oo2ss -.oo2o .oooo -.oo -.oo4 -4 
9 2.459603 2.452636 .006967 .0008 .0003 .0005 .0074 42 
Notes 
(1) Exponential is e to the power tot/10. 
(2) 9 = 0.0079763 + 0.0889069x + 1.0063940, and for the normal equations, 
 x2= 45,   = 285,  x a = 2025,  x  = 15333,  xy = 86.778199, 
 x  = 589.15937,  y = 16.337995. 
(3) Peurbations are from A Million Random Digits with 100,000 Normal De- 
viates, by the RAND Corporation; The Free Press, N.Y., 1955, p. 47, last 
column, upper o blocks. 
(4)  = -0.00002- 0.0001x- 0.0006. 
(5) Xaot is here taken as a convenient multiple (; see Fisher, R. A., and F. Yates, 
(1953). Statistical Tables for Biological, Agricultural and Medical Research, 4th 
edition. London: Oliver and Boyd. Table XXlII, p. 80) of the result of 
ohogonalizing t to 1, to and t. 
16C: Looking at an old variable/Exhibit 14 427 
At the extremes and the local maximum and local minimum, this takes the 
following values, which we compare with those of the residuals by taking 
ratios: 
told: 0 2.5 7.0 9.0 
y- ' -0.0064 0.0057 -0.0059 0.0070 
P3: -30.544 20.706 -19.344 25.256 
Ratio: 0.00021 0.00028 0.00030 0.00028 
Since these ratios indicate what multiple of P3 might be a good fit, this simple 
approach would lead us to think (a) that a multiple of P3 might be a good thing 
to add to , and (b) that 0.00027 would be a good first guess for the multiplier. 
Turning now to Exhibit 15, we see a very clear dependence of y -  on gdo t. If 
we take the endpoints, for which 
(0.0070) - (-0.0064) 
= 0.00016, 
(42)- (-42) 
we are led to think of 
0.00016Xdot 
as our choice to supplement . 
Exhibit 14 of Chapter 16 
Plot of y -  against tod for the unperturbed exponential of Exhibit 13. Smoothed 
points (changes shown by small x's where different) are connected by dotted 
line. 
0.00; - 
0.001 - 
/ 
(0.0, 0) (4., 0) I0.3,0) 
0- 
-0.00 
/ 
/ 
-0,006 
L __ I I >. telj 
0 5 10 
428 Exhibit 15/16: Examining regression residuals 
Clearly, using either Exhibit 14--plotting against told or Exhibit 15-- 
plotting against Xdot ' has worked well in this example, although the latter has 
given an even stronger regression. 
Example. Fitting a perturbed exponential The middle section of Exhibit 13 
gives some possible perturbations, drawn from a table of random numbers, for 
the 10 values of toa, fits a quadratic in told, and finds the residuals. Since we 
are, in these two examples, fitting by ordinary least squares, we have 
fit to (exponential PLUS perturbations) 
EXACTLY EQUALS 
(fit to exponential) PLUS (fit to perturbations), 
and hence 
residuals for (exponential PLUS perturbations) 
EXACTLY EQUALS 
(residuals for exponential) PLUS (residual for perturbations). 
The residuals for the perturbed exponential are shown on the right side of 
Exhibit 13. They are plotted against tola in Exhibit 16 and will be shown 
Exhibit 15 of Chapter 16 
Plot of y-  against Xdo t for the unperturbed exponential. Smoothed points 
(shown by small x's where different) are connected by dotted line. 
0.00 / 
/ 
/ 
/ 
/ 
/ 
0,003 - / 
/ 
Xe 
/ 
/ 
/ 
/ 
0- / 
/ 
/ 
/ 
� 
I 
I 
-0,00 - / 
/ 
/ 
/ 
/ 
/ 
/ 
-0.001; - 
/ 
� 
l I I i I ) ot 
-40 o 40 
16C: Looking at an old variable/Exhibits 16 and 17 429 
plotted against Xdo t in Exhibit 17. When we look at the points alone in Exhibit 
16, we can hardly detect any systematic behavior. Using the (3R) smoothed 
values would leave us with a picture suggesting sign-changes near 0.4, 5.2, and 
7.8, not far from those found for the unperturbed exponential. 
In Exhibit 17, we have only to look at the points to see a clear dependence 
of y -  on Xaot. (If we look at the smooth, the appearance is strengthened.) 
Exhibit 16 of Chapter 16 
Plot of y-  against t,d for the perturbed exponential of Exhibit 13. Smoothed 
points are connected by dotted line. 
0.0; 
Exhibit 17 of Chapter 16 
Plot of y-  against xot for the perturbed exponential of Exhibit 13. Smoothed 
points are connected by dotted lines. 
0,0i - 
-40 0 40 
430 Exhibits 18 and 19/16: Examining regression residuals 
Example. Doubly perturbed exponential To continue the process of perturba- 
tion to larger disturbances, Exhibits 18 and 19 plot the residuals for 
(exponential) + 2(perturbation) 
against tod and Xdot, respectively. We can see nothing in the plot a. gainst toa, 
but the plot against Xaot still gives a reasonable suggestion of dependence one 
Exhibit 18 of Chapter 16 
Plot of y- against t.,d for the exponential (of Exhibit 13) PLUS 
2 x perturbation. Smoothed points are connected by dotted line. 
O.OZ - 
� 
� 
/ \ / 
o - /(o.s, o)  (,a, o) //7.4, o) 
/ \ / 
/ \ / 
/ . L. / 
/ ' 
� _ _ 
-0,02 - 0  9 
Exhibit 19 of Chapter 16 
Plot of y- against Xdot for the exponential (of Exhibit 13) PLUS 
2 x perturbation. Smoothed points are connected by dotted line. 
r  
/ 
/ 
e____e/ � 
-D.O,- -40 0 0  dot 
(16C) 16D: Looking at a new variable, t.ew 431 
somewhat weaker than Exhibit 17--whether we look at only the points or at 
their 3R smooth. 
Conclusions [rom the three examples. These examples suggest solving some 
problems by plotting y -  against told, but also suggest that hard problems can 
be dealt with only by plotting against a suitable Xdot. Smoothing has made the 
plots more effective, and allowed problems of intermediate difficulty to be 
handled by plotting against told, where otherwise, we would have had to plot 
against Xaot. 
In addition to better detectability, the plot against Xdot has one further 
advantage: We can find the coefficient (of Xdot or of toa; they are the same) 
directly as the apparent slope of y-  against Xdo t. Note that even when 
fitting a polynomial, as in the analysis based on Exhibit 14, getting at the 
coefficient of told from a plot of y -  against told is not straightforward. 
If we have kept the values of the coefficients of the other carriers in Xdot, 
and found the coefficient c of Xdot, to get the new fit we have only to form 
y q- CXdot, 
expanding Xaot in terms of the original carriers if we wish. 
16D. Looking with Regard to a New Variable, tne, 
What should we do to ask whether a tnw will help us? We did two different 
things when responding to this question about a told: 
O direct plotting against toa, 
o selection of a "next carrier", conversion to an Xdot, then plotting against 
Xdot. 
We also found that the second approach may well work when the first does not. 
We ought to expect to have the same two possibilities when asking about 
tn,, although we ought not to expect the question "Will direct plotting work?" 
to necessarily have the same sort of answer. Let us turn first to an example, and 
then go on to discussion. 
Example. Railroad returns. Exhibit 20 sets out 20 years of values for (y) 
dollars of freight revenue to U.S. railroads per ton of freight hauled and (tncw) 
average miles of haul. A cubic fit in time (t) is shown for each, as are the 
residuals. Exhibit 21 plots y - 9 against tnew, and shows at most a very weak 
tendency to dependence. Exhibit 22 plots y - 9 against Xdot, here the residuals 
of tnew after fitting a cubic in t. The dependence here is now quite clear. There 
is more that we ought to take out with a further regression. Obviously we have 
learned much more from the plot against Xdot. 
432 Exhibit 20/16: Examining regression residuals 
Exhibit 20 of Chapter 16 
Dollars per ton hauled and average length of haul for all U.S. (class I, Ii, and III) 
railroads combined. 
Dollars per ton ,I Miles hauled 
Year ! Actual Cubic Residuals Actual Cubic Residuals 
! y fit y- 2 t,e, fit Xdo t 
1957 6.26 6.11 .15 429.20 441.60 --12.40 
1956 5.97 6.18 --.21 428.08 429.60 --1.52 
1955 5.95 6.18 --.23 430.67 421.65 9.02 
1954 6.19 6.14 .05 431.65 417.18 14.47 
1953 6.27 6.05 .22 420.66 415.57 5.09 
1952 6.16 5.92 .24 426.93 416.25 10.68 
1951 5.66 5.75 --.09 419.99 418.60 1.39 
1950 5.58 5.56 .02 416.32 422.04 --5.72 
1949 5.57 5.34 .23 412.02 425.98 --13.96 
1948 5.12 5.12 .00 405.64 429.81 --24.17 
1947 4.43 4.88 --.45 407.82 432.95 --25.13 
1946 4.10 4.64 --.54 415.48 434.79 --19.31 
1945 4.43 4.41 .02 458.14 434.75 23.39 
1944 4.53 4.19 .34 473.28 432.23 41.05 
1943 4.41 3.99 .42 469.07 426.64 42.43 
1942 4.02 3.81 .21 427.76 417.37 10.39 
1941 3.48 3.67 --.19 368.54 403.84 --35.30 
1940 3.35 3.56 --.21 351.13 385.45 --34.32 
1939 3.45 3.49 --.04 351.21 361.61 --10.40 
1938 3.54 3.48 .06 356.05 331.72 24.33 
S) SOURCE 
y is series Q85, t,,w is series Q83, both from page 431 of the Historical Statistics of the United States, 
Colonial Times to 17. U.S. Bureau of the Census. 
16D' Looking at a new variable/Exhibits 21 and 22 433 
Exhibit 21 of Chapter 16 
Plot of y- , against t. ow for freight revenues (3R smoothed values shown by 
dotted line). 
� 
I I ) I � 
330 ?0 410 450 40 
Exhibit 22 of Chapter 16 
Plot of y-  against Xot for freight revenues (3R smoothed values indicated by 
dotted line). 
0.40 t 
I 
/ 
� � / 
/ 
/-I \ // 
/  f \ / 
/  I .-- 
/ '  I 
O- * I 
I *\ 
/ x _,_1 
I 
/ 
%---/ � 
� 
� 
-30 -20 -i0 0 I0 0 30 40 
434 Exhibit 23/16: Examining regression residuals 
Discussion and Comments 
Why has Xdo t been more useful? Exhibit 2.3 shows the time histories of y - , 
t, ew, and Xdot. Here we can see more. Fitting the cubic in time has taken out 
wha.tever slow motion there was in y; what remains in y-  is oscillatory, 
wawng rapidly up and down around zero. When we look at tnew, we see a 
combination of things: a bulge during World War II (1942-1945), combined 
with a general upward trend. When we look at Xdot, we see that the removal of 
a cubic in t has also made Xdo t oscillatory. 
Exhibit 23 of Chapter 16 
Plots of y- , tn.w, and Xot against date. 
! 
4g0 - / 
i 
! 
! 
400 
/ 
/ 
/ 
t40 1945' 1050 1);5' '" 
(16D) 16E: Looking for additional product terms 435 
The ex!stence of a trend in tnew makes it not directly useful in fitting y - , 
because ]lS a trend. Only after we have taken trend out of tnew by fitting a 
cubic in t, to obtain Xdo t as the residual, do we have something that can directly 
fit y- y. 
Another way to say the same thing is: We cannot gain much by adding a 
multiple of tnw to ] unless we are free to readjust the coefficients in ] 
appropriately. 
In this example, the similarity between tnw and the stock leading to  was 
more easily visible because it consisted of slowly changing variation, while the 
similarity between tnw and y - ], which is the same (except for a multiplying 
constant) as the similarity between Xdo t and y - ], is rapidly changing. To have 
this happen makes the example easier to understand, which is good. Beware, 
however, of thinking that such similarities always are slowly changing and 
rapidly changing; they often differ in other ways. 
The central points remain: 
1. A similarity between tv and the stock already fitted stands in the way of 
adding f. tnew to  unless we can readjust the constants in . 
2. The easiest way to allow for such readjustment is to think of adding f. Xdot 
to ]. Since the similarity between tw and  has been eliminated in 
forming Xdot, we can do this freely and easily. Thus, the plot of y- ] 
against Xdot is directly useful. 
Earlier, when we discussed improving a fit of the form 
 = a + bh + ct2 + dt2 + et3 
by including further dependence on h, we were careful to insist that Xdo t was to 
have "eliminated" from it 
,>not only 1, h, t, for the reasons discussed in Section 16C, 
,>but also t2 and t3. 
We now see the reasons that "elimination" of t2 and t3 may be important; they 
are those we have just been discussing. 
16E. Looking for Additional Product Terms 
Multiplicative adjustments. So far we have looked for things that could be 
added to our fit. Sometimes we look for things that might be multiplied. We 
first consider a method that is not likely to help. If the fit is already reasonably 
close, then a change from 
 to (1 + u), 
where u is, for instance, of the form 
CO-' C1X1 -[- C2X2-[- ' ' ' -'[- C k X k 
436 /16: Examining regression residuals 
and relatively small, corresponds to a change on the log scale from 
log  to log ((1 + u)) = log  + log (1 + u) 
= log  + (log e) log (1 + u) 
'- log  + (log e) u, 
where e = 2.71828... (When we write the formula this way, the logarithms 
can be taken to any desired base. When natural logs are used, in the 
approximation the second term on the righthand side becomes u.) Thus we can 
look for a gain from such a fit by relating 
log y - log  
either to old t's or a tev. 
Unfortunately, the resemblance between y- and log y- log  will 
usually be high enough for such an approach to be mainly looking again for 
what we already sought. 
A different possibility. The possibility of gaining by modifying  to 
med q- (- med)( 1 q- 
where mea is the median of the values of ], is different in character. It leads to 
Y -- (med q- ( -- med)(1 q- I1)) = y --  -1- (med- 
and to making inquiry into the possible regression of 
y-  on (- med)U. 
This is a fresh inquiry, since  - med takes both positive and negative signs, 
about equally often. 
We may well set aside data points for which  - med is small, because the 
product ( - n�d)U will be even smaller for such points. We shall locate the 
hinges (or quartiles) of  and set aside data points for whic.h  falls between 
them. Let us consider two approaches, one through replacing y-  by Q 
and one through replacing y -  by q, where the new quantities are defined as 
Q = { (y - ])/(]- mcd) (top and bottom quarters of ), 
(nothing) (middle half of ]), 
and as 
'+ (y - ) (top quarter of ]), 
q = (nothing) (middle half of ]), 
- (y - ) (bottom quarter of ]). 
The latter uses only the sign of  - med and not its magnitude; thus it is a little 
easier to construct. It often suffices. 
16E: Looking for additional product terms 437 
It now makes good sense for us to examine the relation of either Q? or q9 
to either old or new t's. 
Example. Honolulu high tides. Our next example deals with the heights and 
times of high tides predicted for Honolulu in 1969. To make the data more 
manageable, we will confine our analysis to high tides falling on the 2nd, 4th, 
6th, 8th, 10th, 12th or 14th of the months of January, February, March, April, 
May, and June. This yields 77 tides. We take time of high tide as x, predicted 
height of high tide as .y, and date-hour as t.. 
To do anything hke a thorough job wth Honolulu tides would require 
more space and more of your time than this example deserves in this 
chapter of this book. In particular, it would be desirable to take at least all the 
700-odd high tides in one year (more years would be still better). It would be 
desirable, too, to give reasonable attention to the physics of the problem as 
always, subject-matter knowledge should be considered. But we are trying to 
illustrate a couple of special techniques and so we do not want to go more 
deeply into this example than it is worth to the reader. We will try only to show 
how q's or O's can help us cut into what starts as apparently a mass of messy 
data, and how they can lead to quite specific phenomena that deserve further 
attention. 
Exhibit 24 gives, for these tides, the following: 
othe predicted height, y, in tenths of a foot (above mean lower low water), 
otime of day, x, both in tenths of an hour and in degrees (where 
360 � = 24 hours), 
othe epoch, t, as an integer for the month number plus a fraction for the 
day and hour converted into thousandths of a 31-day month (all multiplied 
by 1000). 
Here a tide at 3:48 ^.u. (written as 038, since 48 minutes is 0.8 hour) on 2 
January would occur 
38 
(,2 - 1) +  = 1.158 days 
240 
after the beginning of January, since January begins on the midnight preceding 
the first day of the month. This corresponds to 
1.158 
- 0.037 of a 3I-day month, 
31 
so the entry is 
1000(1 + 0.037) = 1037. 
This sets midnight before New Year's Day at 1000 (rather than at 0000). 
We were lazy in using fractions of a thirty-one day month. This means that 
3.2 days, for example, contributes an equal fraction in any month, no matter 
438 Exhibit 24/16: Examining regression residuals 
what its length. (It also means that 0.96 of the way through February, for 
example, never occurs. We thought the simplification worth such complexities 
for our present purposes. In a more careful analysis we would have to admit 
that months were of different lengths, something that would probably force us 
to work in days or in fractions of a year.) 
As a first step, we divide the heights according to the angle of the day. 
Exhibit 25 gives the results of using 20 � intervals. The smoothed medians of 
Exhibit 24 of Chapter 16 
Predicted high tides, Honolulu, 1969 (Days 2, 4, 6, 8, 10, 12, 14 of Jan., Feb., Mar., 
Apr., May, June). 
Ht. Time Epoch Ht. Time Epoch Ht. Time Epoch 
II***l 1*'11 II***l I****l I**'11 
22 038 57 � 1037 20 034 51 o 3037 8 037 56 � 5037 
5 153 230 � 1053 9 156 234 � 3053 22 168 252 � 5055 
23 048 72 � 1103 18 043 64 � 3103 5 052 78 � 5104 
5 166 249 � 1119 12 167 250 � 3119 23 184 276 � 5122 
21 059 88 � 1169 14 052 78 � 3168 3 075 112 � 5171 
6 181 271 � 1186 15 181 272 � 3186 21 204 306 � 5189 
18 069 104 � 1235 9 062 93 � 3234 6 111 166 � 5241 
9 201 302 � 1253 17 200 300 � 3253 18 225 338 � 5256 
13 080 120 � 1301 18 226 339 � 3321 11 132 198 � 5308 
13 226 339 � 1321 4 122 183 � 3371 12 011 16 � 5356 
8 098 147 � 1368 21 017 26 � 3422 16 144 216 � 5374 
22 016 24 � 1422 7 142 213 � 3438 9 024 36 � 5423 
5 128 192 � 1436 14 037 56 � 4037 20 156 234 � 5440 
22 044 66 � 2038 17 164 246 � 4054 4 052 78 � 6039 
7 164 246 � 2054 10 047 70 � 4103 24 182 273 � 6057 
20 153 79 � 2104 20 178 267 � 4121 4 077 116 � 6107 
9 177 266 � 2121 6 060 90 � 4169 21 199 298 � 6124 
15 062 93 � 2170 20 197 296 � 4187 8 107 160 � 6176 
12 194 291 � 2187 3 085 128 � 4237 16 216 324 � 6190 
10 072 108 � 2235 19 221 332 � 4256 14 127 190 � 6243 
15 218 327 � 2255 6 125 188 � 4307 11 234 351 � 6257 
22 013 020 � 2357 18 011 16 � 4356 9 002 3 � 6291 
4 131 196 � 2372 11 141 212 � 4374 19 141 212 � 6309 
24 029 43 � 2423 15 025 38 � 4423 6 018 27 � 6357 
6 150 225 � 2..0 15 153 230 � 4440 21 153 230 � 6375 
5 032 48 � 6424 
22 164 246 � 6441 
(*) Height (above mean lower low water) in tenths of a foot. 
{**) Time (24-hour clock) in tenths of an hour. 
(***) Time (360 � per day). 
{****) Date in thousandths of a month (see text). 
16E: Looking for additional terms/Exhibit 25 439 
440 Exhibit 26/16: Examining regression residuals 
the corresponding groups of heights are plotted using small circles in Exhibit 
26. The prominent features of this plot are: 
opeaks near 60 � and 270 � (= 180 � + 90 �) . 
ovalleys near 170 � (= 90 � + 80 �) and 360 � (= 270 � + 90�). 
Let us remove a rough fit involving .sines or cosines of 20, because these 
will repeat every 180 �. If we use a multiple of cos 2(0 + phase constant), we 
can fit any linear combination of cos 20 and sin 20 exactly. This function has a 
peak when 2(0 + phase constant) is 0 �. The data do not peak at 0 �, and 
therefore, we need a nonzero phase constant. Let us choose 75 �, so that the 
carrier of our fit is cos 2(0 - 75�). The peaks of the smoothed medians rise to 
17 and 18, an average of 17.5 or about 18, and go as low as 12 and 7, 
averaging 9.5, or about 10. So an overall level of 14, which is close to the 
median 13.0 of the smoothed medians, offers a start for a first fit. Thus we are 
fitting 14 + a cos 2(0- 75�). The distance between the peaks and the valleys 
is about 18 - 10 = 8, and so a reasonable value for a is 4. The final choice is 
14 + 4 cos 2(0- 75�). 
Exhibit 26 of Chapter 16 
Smoothed medians from Exhibit 25 plotted against 0 and a first fit against 0 
(14 + 4 cos 2( 0- 75�), shown dotted). 
StaShed 
medians 
$ . ! I I I I >; 
o' 90' 18o' ?7o' o' 
16E: Looking for additional terms/Exhibit 27 441 
This fit is shown in column (**) of Exhibit 25, and subtracted from the 
medians of 20 � slices. The new residuals are plotted in Exhibit 27. Most of 
what we now see is a single valley and a single peak, so we fit a cosine or sine 
of 0 itself. The same rough-and-ready technology we used before suggests 
6 cos (0- 310 �) as the second fit. The slightly high fitted points around 
0 - 310 � originate in the use of the coefficient 6, rounded from 5.5. 
This second fit is also shown and subtracted (still from the medians of 20 � 
slices) in Exhibit 25. We could improve our fit, now of the form 
14 + 6 cos (0 - 310 �) + 4 cos 2(0 - 75 �) 
somewhat further, but we turn instead to the O and q analyses. 
Analysis by q. Exhibit 28 extends Exhibit 25, where the tidal heights of 
Exhibit 24 were classified by mean angles of the earth's rotation, by giving the 
more precise values of  evaluated. at the specific angles. 
The values of y at the specific angles are recorded in Exhibit 29 in 
chronological order (of increasing t = epoch) and no longer in a classification 
ordered by angles. Here y -  is shown, as are the values of q and Q, which 
are defined only for the outer quarters of , where  < 928 or > 17.6. For 
facilitating the computation of Q?, the relevant values of y - Ymed are noted. 
The median med of  is 15.2. 
Exhibit 27 of Chapter 16 
The first residuals plotted against 0 and a second fit against 0 (6 cos(O - 310�), 
shown dotted) 
442 Exhibit 28/16: Examining residuals 
16E: Looking for additional terms/Exhibit 29 443 
Exhibit 29 of Chapter 16 
Values of y (from Exhibit 25, the tidal example), , and y - , at specific epochs 
(from Exhibit 28); values of q9 and Q9. 
t 
epoch 
1037 22 15.2 6.8 
1053 5 17.2 -12.2 
1103 23 14.7 8.3 
1119 5 20.7 -15.7 -15.7 5.5 -2.8 
1169 21 12.9 8.1 
1186 6 22.0 -16.0 -16.0 6.8 -2.4 
1235 18 10.8 7.2 
1253 9 19.6 -10.6 -10.6 4.4 -2.4 
1301 13 8.2 4.8 -4.8 -7.0 -.7 
1321 13 15.5 -2.5 
1368 8 5.6 2.4 -2.4 -9.6 -.2 
1422 22 14.8 -14.8 
1436 5 9.2 -4.2 4.2 -6.0 .7 
2038 22 14.3 7.7 
2054 7 20.2 -13.2 -13.2 5.0 -2.6 
2104 20 15.1 4.9 
2121 9 22.2 -13.2 -13.2 7.0 -1.9 
2170 15 12.3 2.7 
2187 12 20.6 -8.6 -8.6 5.4 -1.4 
2235 10 10.1 -.1 
2255 15 16.7 -1.7 
2357 22 14.7 7.3 
2372 4 10.0 -6.0 
2423 24 15.3 8.7 
2440 6 16.3 -10.3 
3037 20 15.2 4.8 
3053 9 18.0 -9.0 -9.0 2.8 -3.2 
3103 18 14.2 3.8 
3119 12 20.8 -8.8 -8.8 5.6 - 1.6 
3168 14 15.1 -1.1 
3186 15 22.0 -7.0 -7.0 6.8 - 1.0 
3234 9 12.3 -3.3 
3253 17 19.8 -2.8 -2.8 4.6 -.6 
3321 18 15.5 2.5 
3371 4 7.6 -3.6 3.6 -7.6 .5 
3422 21 14.9 6.1 
3438 7 13.7 -6.7 
4037 14 15.2 - 1.2 
4054 17 20.2 -3.2 -3.2 5.0 -.6 
4103 10 14.6 -4.6 
4121 20 22.2 -2.2 -2.2 7.0 -.3 
444 Exhibit 29 (continued)/16: Examining regression residuals 
Exhibit 30 plots the values of q? against t. The six months appear as six 
groupings, as they must. Even a casual look shows: 
o a general upward trend, and 
owider spreads toward the left, except for February. 
Exhibit 29 of Chapter 16 (continued) 
4169 6 12.6 -6.6 
4187 20 20.2 -.2 -.2 5.0 0 
4237 3 7.2 -4.2 4.2 -8.0 .5 
4256 19 16.2 2.8 
4307 6 8.5 -2.5 2.5 -6.7 .4 
4356 18 14.7 3.3 
4374 11 13.5 -2.5 
4423 15 15.2 -.2 
4440 15 17.2 -2.2 
5037 8 15.2 -7.2 
5055 22 21.2 .8 .8 6.0 .1 
5104 5 15,1 -10.1 
5122 23 21.8 1.2 1.2 6.6 .2 
5171 3 9.5 -6.5 6.5 -5.7 1.1 
5189 21 19.1 1.9 1.9 3.9 .5 
5241 6 5.7 .3 -.3 -9.5 0 
5256 18 15.6 2.4 
5308 11 10.3 .7 
5356 12 14.7 -2,7 
5374 16 14.4 1.6 
5423 9 15.2 -6.2 
5440 20 18.0 2.0 2.0 2.8 .7 
6039 4 15.1 11.1 
6057 24 21.9 2.1 2.1 6.7 .3 
6107 4 8.8 4.8 -4.8 -6.4 -.8 
6124 21 20.0 1.0 1.0 4.8 .2 
6176 8 5.2 3.2 -3.2 -10.0 -3.2 
6190 16 16.0 0 
6243 14 8.8 5.2 -5.2 -6.4 .8 
6257 11 15.3 -4.3 
6291 9 14.6 -5.6 
6309 19 13.5 5.5 
6357 6 14.9 -8.9 
6375 21 17.2 3.8 
6424 5 15.3 - 10.3 
6441 22 20.2 1.8 1.8 5.0 .4 
16E: Looking for additional terms/Exhibit 30 445 
Furthermore, early months (Jan., Feb., Mar., Apr.) show a very steady trend 
within months. 
The purpose of any routine process of exploration, like the use of q--or 
O or q, or O, is not to check if a particular simple addition to the fit will help 
(we can check this by just going and fitting). It is rather to find out whether a 
general direction offers opportunities. Here the general direction, before the q 
analysis, was toward: 
onot too complicated combinations of ] and t or 0 and t. 
We have been told, quite correctly, by the q plot, that this general direction is 
promising. It is now up to us to look harder and identify more specific 
directions we can explore. 
The plot of q against t tells us that we can now improve our fit by 
combining 
0, which describes time of day, and 
t, which describes time of year (within 1969). 
Exhibit 30 of Chapter 16 
q from Exhibit 29 against date-hour combination t. The dotted lines connect 
results for alternate days of a month. 
446 /16: Examining regression residuals 
This is rather striking since up to this point had depended on 0 alone. The 
most useful combination is unlikely here to e of the simple form 
d( - e)(t- ta) 
corresponding to a single straight line in Exhibit 30, though an additional term 
of this form would be of some help. (To see roughly how much, plot Q against 
y-) 
Parting comment. We have used these data to illustrate various techniques for 
breaking into a nonobvious structure. We have not tried to take the analysis of 
these data nearly as far as we could have. 
Other products. To assess products of carriers, one of which is already a carrier 
in the fit, we choose an appropriate xj, call it x, find its hinges (or quarters), set 
aside data points for which xj falls between its quartiles, and form, for the 
other half of the data sets, either or both of 
Q = (y - )/(x- Xmed), 
q = (y -- ) sgn (x -- Xmea), 
where 
+1 when z > O, 
sgn z = -1 when z < O, 
0 when z = O. 
We can then explore the apparent behavior of Q or q just as we could 
that of y - 9, of O9 or of q. 
If we wish to consider products involving carriers representing two tnew'S, 
say, Xnew and Xnwr, we have only to take x - Xw and relate Q.w or qw to 
Xnewer. 
16F. In What Order? 
We have provided several ways to explore possibilities for further reducing the 
size of residuals by increasing the complexity of the fit. We cannot expect to try 
all of them on every fit we make. It would be good to have a (1) general 
indication of which to try first, which second,... and (2) a general indication of 
where to stop. The first of these needs we can meet fairly well, but experience 
does not yet seem to tell about the second. 
The following order of examination is suggested: 
1. y- ] in relation to 9. 
16F: In what order? 447 
2. y- in relation. to told'S , sometimes in relation to the appropriate 
Xdot'S, that seem amportant. 
3. y- in relation to .tnew'S, orthogonalized to become Xdot'S , that we 
strongly suspect of being important. 
4A.* .y- in relation to tola'S (still often as Xaot'S) not thought to be 
mportant. 
4B.* O or q in relation to told'S (still often as Xdot'S ) that seem important. 
4C.* Ot or qt for t an important told, and other important told'S (still often as 
XdotS). 
5A. y - ] in relation to t,,'s (as Xdot'S) not thought to be important. 
$B. Q or q in relation to tew'S (as Xdot'S) not thought to be important. 
While guidance as to where to stop the overall examination is not at hand, 
we can offer guidance about how to behave at various stages, namely: 
olf we find an appreciable apparent dependence at stage (1), we either 
re-express y or fit further terms to absorb this appearance into the fit. (We 
can then start again.) 
oil we find a large apparent dependence in one of individual comparisons 
of stages (2), (3), (4A), (4B), or (5A), we proceed immediately to refitting 
and starting again. 
olf we find a moderate or small dependence in an individual comparison of 
one of these stages, we complete all the individual comparisons of that stage 
before choosing which one or few carriers to add to the new fit, refitting and 
restarting. 
olf we fi. nd a high or mode. rate dependence in stages (4C) or (5B), we 
proceed mmediately to refittang and starting again. 
olf we find a small dependence in either of these stages, we think hard 
about what to do next. 
Note that: 
oThe Xdot for a particular t, whether a told or a tnew, is the same for y -  or 
any Q or any q (as is the need/no need decision about going to an Xdot). 
oq and the q,'s are easy to find (set aside half the data sets, and change 
the sign of half the y - 's that remain). 
Accordingly, having done stage (2), it is easy--and often worthwhile--to do 
stage (4B) for q and stage (4C) for at least some q's. 
* The order of choice among 4A, 4B and 4C (or between 5A and 5B) depends on the situation 
in ways about which we cannot yet give general indications. 
448 /16: Examining regression residuals 
Summary: Examining Regression Residuals 
Displaying y -  against y is misleading. 
Displaying y - against y is often helpful. 
We may have a variety of choices as to which are "the variables" once the 
stock, or even the carriers, are fixed. 
The carriers in a nonlinear fitting situation are defined by the partial 
derivatives of the fit with respect to the coefficients. 
A useful sequence of steps is: 
oto display y -  against . 
oto smooth y-] against  and to add it to smoothed , in order to 
suggest an h() that might be a better fit than . 
oto reflect our understanding of such an h(]) by either (i) re-expressing  
or (ii) re-expressing x. 
oto work with groups of points falling in (narrow ranges of) y when we 
have enough points to make this useful. 
We may have to use one or two selected "next carriers" when examining 
the need to use further a variable already represented by, say, two or more 
carriers, and to regress present residuals against such next carriers. 
We may use either display against tnew or display against Xdot (here the 
residuals of tnew after fitting with the carriers in the present fit) to enquire into 
the apparent importance of adding some function of tew as an additional 
carrier. (Here we expect Xaot to be more powerful, especially if tw was suitably 
re-expressed before Xaot was formed). 
We hunt for additional product terms by displaying y -  against either 
Q or q (see early Section 16E for formulas). 
We hunt for other additional product terms by displaying against Q or q, 
where x is a well-chosen carrier already in the fit (see late Section 16E for 
formulas). 
Appendix/Details about 
the Need to Re-express 
A. The General Case 
The question before us is: 
"Do we need to change from a less satisfactory expression of x (described 
below as 'the raw carrier') to a more satisfactory expression (described 
below as 'the straightened carrier')?" 
Our assumption is that we know the straightened carrier to be better, that the 
question is only "Is it enough better to be worth the trouble where the 
trouble may be a matter of calculation, but is more likely to be a matter of 
exposition and description?" We often want to answer this question on the 
basis of what we have learned by fitting a regression on the raw carrier to 
answer it without going to the trouble of fitting the (admittedly somewhat 
better) regression on the straightened (more satisfactory) carrier. 
Then our basic data have to be: 
o How much are we willing to worsen the fit by using the raw carrier? 
oHow alike are the raw and straightened carriers? 
oHow good was the fit using the raw carrier? 
oHow many data points were there? 
We measure the first three of these as follows: 
o worsening of the fit by /, where the mean-square failure of fit (a 
modification of the mean-square error of fit, to be explained in Section E) is 
(1 + 3) times as large when the raw carrier is used as when the straightened 
carrier is used. 
osimilarity between raw and straightened carriers, either by rca,iers, the 
correlation coefficient between them, or by e, where 
. 2 
2 
rcicrs = correlation (Xraw, Xstraightened) 
1 
1+ 2 
(e is often an easier basis for tabulation or discussion. Its interpretation is 
discussed in Section E.) 
449 
450 /Appendix: Details about the need to re-express 
othe quality of fit, using the raw carrier, by the corresponding correlation 
coefficient rrawfit, where 
2 = correlation 2 (y, Xw) 
rrawfit  
1 2 variance of residuals from fit of Xra w 
 /'raw fit  ' 
variance of y before regression 
Three questions now arise: 
o How do we judge what $ is acceptable? 
oHow are we to appraise  (or e) in practice? 
/'carriers 
2 
How do we assess the actual , using, say, n, rrawfit, and ? 
As mentioned above, we answer the second question for the most 
frequently-arising re-expressions by relating 2 
/'carriers to: 
o(mainly) the ratio "largest x/smallest x" for the raw carrier. 
o(small amount of modulation) whether the x-values are, compared to the 
normal, stretch-tailed, neutral, or squeezed-tailed (as they, for example, will 
be if they are uniformly spaced). 
The most important relation is of 2 
roa,ers to the ratio of largest to smallest x, but 
we should allow a small amount of modulation from the shape of the x-values. 
We return to the discussion of the ratio in Section C and of the spacing in 
Section D. 
2 
We can express /5 as follows in terms of e 2, n, and rrawnt. 
e 2 (.(1 + (n 1)2) 2 
 -- /'rawfit  1 
/'rawfit 
We plan to work in terms of three values of fi, namely, 
fi = 1, where we care only moderately about fit quality and do not mind 
doubling the mean-square failure; 
fi = 0.1, where we care quite a lot about fit quality but are willing to let the 
mean-square failure rise by 10%; 
fi = 0.01, where we care extremely strongly about fit quality, and are only 
willing to let the mean-square failure rise by 1%. 
We think of the two end values,/5 = 1 and/5 = 0.01, as rather extreme, and the 
middle one /5 = 0.1 as a moderate value. 
Sometimes we will not want to calculate /5, preferring a quick look at a 
table of rr2awfit for the three particular values of/5. Exhibit 1 gives these values. 
Some examples. Suppose we have n 100 and 2 
= rcers = 0.9412, SO that e = 
0.25. (Both 1 2 E2 
--rcarrirs and show that about 6% of the variance of the raw 
A: The general case/Exhibit I 451 
carrier would be left over if we fitted the raw carrier with the straightened 
carrier.) Suppose, further, that in the population, 2 � 
rrawfit 1S about 0.50. What can 
we find from this exhibit? 
If we look at Exhibit 1, for n = 10 and e = 0.25, we find that: (A) it 
would take an 2 
/'rawfit of 0.68 to make = 1 and 1 +  = 2; (B) 2 
an /'rawfit of 
0.27 to make/5 - 0.1 and i + /5 -- 1.1. If we were willing to allow an increase 
in mean-square failure of up to/5: 1, we need not re-express; but if we do not 
wish to allow an increase beyond  - 0.1, we must re-express. 
Exhibit I of Appendix 
Values of (true) 2 
r;,,t between response and raw carrier such that mean-square 
failure in finding slope is (A) doubled, (B) increased by 10%, (C) increased by 1%. 
=.6 =.5 =.4 =.3 =.25  =.2  =.15 =.1 =.06 =.03 =.01 
A) Mean-square failure times 2(= 1 + &) 
n = 5 N N N (.84) (.88) .923 .956 .980 .9928 .9982 .999800 
10 N (.56) (.69) .81 .87 .917 .954 .980 .9928 .9982 .999800 
30 N (.38) .54 .73 .82 .893 .945 .978 .9925 .9982 .999800 
100 N (.18) .31 .54 .68 .82 .916 .971 .9916 .9981 .999799 
300 N N .14 .31 .46 .66 .841 .953 .9891 .9980 .999797 
1000 N N N .12 .22 .39 .654 .895 .9804 .9974 .999790 
B) Mean-square failure--times 1.1 (= 1 + &) 
n = 5 N N N N N N (.82) (.906) .963 .9902 .99890 
10 N N N N (.61) (.71) .81 .902 .962 .9902 .99890 
30 N N N (.37) .48 .61 .76 .888 .960 .9901 .99890 
100 N N N (.18) .27 .41 .62 .84 .952 .9895 .99889 
300 N N N N (.12) .22 .41 .73 .930 .9879 .99887 
1000 N N N N N .08 .19 .50 .860 .9825 .99880 
C) Mean-square failure--times 1.01 (= I + ) 
n = 5 N N N N N N N N N .923 .9901 
10 N N N N N N N (.64) .78 .922 .9901 
30 N N N N N (.36) .46 .61 .77 .921 .9901 
100 N N N N N (.20) .31 .50 .73 .917 .9900 
300 N N N N N N .16 .33 .64 .904 .9898 
1000 N N N N N N (.06) .15 .45 .864 .9891 
Entries of N almost always, and entries in parentheses usually, indicate that the raw fit is never really 
appropriate (either because it is too weak (nonsignificant) or because the fit with the straightened carrier is 
better). (See "Relation to significance/nonsignificance" near the end of this section, for further explanation.) 
452 /Appendix: Details about the need to re-express 
If we choose to calculate /5, we have 
0.0625 ((1 + (100- 1)(0.0625))(0.50) 1) = 0.39. 
/5 - 1.0625 ]  -1 � 0.0625)(0.5) - 
For even a moderate quality of fit (one that practitioners from some fields 
might think good, while those from others would think horribly poor), we 
cannot accept � = 0.25 if we want to do a reasonably careful job of fitting. 
What if � were 0.1 and the observed 2 
rrawfit were still 0.50? For n = 100, 
2  
we find from Exhibit 1 that true rrawfit S of 0.971, 0.84, and 0.50, respectively, 
correspond to/5 = 1, 0.1, and 0.01. Thus our supposed observed 2 _ 
/'rawfit- 0.50 
falls right at/5 = 0.01. If we do not wish to worsen our fit by more than 1%, 
use of the raw carrier is just acceptable. (If our original fit was somewhat 
2 
better, rraw�it--0.6 or 0.7, we would fall in between two of these values, 
worsening by more than 1% and less than 10%, by using the raw carrier; if 
2 
rrawfit = 0.84, we would worsen by 10%.) 
The closer the fit to the raw carrier, the better carriers we need to make 
the fit reasonably effective. 
Relation to Significance/Nonsignificance 
We understand that raising the quality of the fit, as measured by 2 
/'rawfit, all else 
being the same, raises the value of/5. For given 2 
rcarr,r. or , and given n, then, 
making 2 
/'rawfit large enough will make the use of the raw carrier unacceptable. 
Other things happen as we start from 2 
/'rawfit 0 and make 2 
= rraw�it larger and 
larger. In particular, the fit originally nonsignificantly different from zero 
first becomes significant at 5% and then significant at 1%. 
If we do not take fits seriously until they reach or nearly reach significance, 
the sequence as fits become closer can be the following: 
o first, fits not taken seriously, because they are not beyond or close to 
significance. 
o then, fits taken seriously but not worth straightening 
ofinally, fits worth straightening. 
2 
These three cases will arise if significance comes for a lower /'rawfit than does the 
profitability of straightening. 
If, on the other hand, the need for straightening manifests itself at a 
smaller 2 .... 
rrawfit than does sgmficance, the mddle case does not arise, and our 
choice is only between "don't take it seriously" and "straighten it at once". 
2 . 
Thus it matters quite a lot which happens for a lower rraw�it. significance or 
a need for straightening. If the need for straightening comes first, every fit that 
we take at all seriously should be redone using the straightened carrier. 
Let us review our main ideas. Let us fix, for a moment, (i) sample size and 
2 2 
(ii) 2 or reartiers and consider what happens as rrawfit changes. If 2 
rraw�it is small 
enough, the fit with the raw carrier will not be significant, suggesting that either 
A: The general case 453 
we forget this kind of fit or try the straightened carrier to see whether it might 
be significant. If, on the other hand, 2 
rrawfit is large (close to 1), the value of/5 
will also be large and there will be a value of 2 
/'rawfit SO large that the 
2 
corresponding value of 15 is intolerable. Thus, the range of values of rrawfit for 
which it is reasonable to use the fit by the raw carrier is limited at both 
ends at one by nonsignificance, at the other by intolerable 15. As we stiffen our 
standards of tolerability, decreasing the 15 we are able to accept, this usable 
range of values shrinks, and eventually disappears. 
2 
Cases where no useful values of rrawfit are possible, using significance at 
5% to limit the range of useful values, are shown in Exhibit 1 by the entry of 
an "N" to stand for "Nonsig.nificant or intolerable". We set ( )'s around 
additional values for which the nterval of useful values vanishes when signifi- 
cance at 1% is taken to set its lower limit. 
In practice, of course, we have only a sample value of 2 
rrawfit and we use 
this to enter, for example, Exhibit 1, whose entries are population (true) values 
of 2 
/'rawfit- It has seemed accurate enough to do this, especially since our choice 
of $ is rarely really precise. We recommend ignoring this further complication 
in using Exhibit 1. 
We can extract some rough guidance in simple form from the values in 
Exhibit 1 and the appearances of N's and ( )'s: 
1. If doubling of the mean-square failure by using the raw carrier is 
allowable, 
o and if detection of a marginal dependence between response and raw 
carrier is sought, one barely significant at 5%, we may get away with � 
2 
almost as large as 0.6 (which means an rcriers of about 0.74) (this takes n's 
like 10 to 100); 
obut if we want to use a much closer raw fit, a larger 2 
/'rawfit we cannot use 
nearly so large an , and must have 2 
rc..ers much closer to 1; 
oand if the fit we seek is of the quality regarded as 'not so good" for many 
experiments in a freshman physics laboratory, corresponding, say, to residu- 
2 
als about 1% as large as (y - )'s, so that 1 - rrawtit - 0.0001, we will need 
to hold � down to something like 0.007 (an 2 
rce of 0.99995). 
2. If we want a fit of reasonable quality (/5 = 0.1), 
owhen the closeness between response and raw carrier is marginally signifi- 
cant at 5%--just enough to detect the fit we may be able to go as far as e a 
to an rcarr of 0.92 (this takes n s like 30 to 100); 
little smaller than 0.3, 2 , 
o where the closeness is much higher, we will need a smaller �, perhaps 0.1 
(an 2 
rcarriers of 0.99 between raw and straig. htened); 
454 /Appendix: Details about the need to re-express 
o when the closeness is that of a nice experiment in freshman physics, 
corresponding, say, to residuals about 0.1% as large as (y- ?)'s, we will 
2 
need to hold e down to 0.0003 or less (to an roartiers of 0.999999 or more 
between raw and straightened). 
3. If we want a very careful fit (15 = 0.01), 
owhere the closeness is marginal, we may be able to go as far as something 
2  
like e = 0.15, reorders = 0.98 (again for n s like 30 to 100); 
o when the closeness is much higher, we may need to stay below  - 0.05, 
and above 2 
rcaies = 0.997. 
The better we try to do--that is, the smaller $ we demand--and the closer 
the fit (the higher the we start with, the more careful we must be..We 
--rarricrs) smaller for the relation 
then need to keep e smaller or keep (1 2 
between raw and straightened carriers, if we are to decide reasonably that the 
raw carrier will do. 
B. The Case of a Very Good Fit 
trawlits S), and hence with small e, the ratio 
If we deal with very good fits (large 2 , 
1 2 
-- /'rawfit 
1 2 
-- /'carriers 
is nearly constant, given 
Exhibit 2 shows the values of this ratio for small e and a variety of n's, still 
for 15 = 1, 0.1, and 0.01. To a very adequate approximation: 
o to do no worse than doubling by using the raw carrier, we have to make 
sure that (1 2 
-- /-carriers) is no more than one-half of (1 - ; 
oto do no worse than 10% excess, we have to hold (1 - to no more 
rc2arriers) 
than one-tenth of 1 - 2 (really about one-eleventh); 
/'rawfit 
o to do no worse than 1% excess, we have to hold 1 2 
-- /-carriers to no more 
than about one-hundredth of (1 - YSwfit). 
In the range of close fits, matters are easily described. We see that we 
often need 
1 2 
 /'carriers 
that is substantially or very considerably smaller than 
1 2 
 /'rawfit 
if we are not to be compelled to change from Xaw to Xstraightcncd. 
C: Do we need to take Iogs?/Exhibit 2 455 
C. Do We Need to Take Logs? 
So far we have answers in terms of what 2 
roaiers we can stand. We need answers 
in even simpler terms, such as these: If we believe log x is right, will x do? If 
we believe / is right, will x do? If we believe -1/x is right, will x do? 
Relatively simple answers to these questions rest on the ratio 
largest x 
smallest x 
at least for well-behaved sets of x's. 
2 
To be sure of this, we have only to calculate the rcarriers between a set of 
n x's and the n corresponding values of log x. This needs to be done over a 
range of values of n--and a variety of shapes of batch. We have chosen to 
work with 3 basic shapes of batch: 
oequispaced (squeezed-in tails); 
oroughly normal (spacing between the ith and (i + 1)st ordered value 
proportional to 1/i(n- i)); 
o stretched-out tails (values of the tangent of a suitable equispaced batch). 
Each of these can be used to describe either the shape of x or the shape of log 
x, thus somewhat exploiting the effect of skewness. 
Exhibit 2 of Appendix 
Ratios of (1 z 
- - r_arriers) for small 
r;.w.t) to (1 2 
= .0a I = .0031 = .00al 
A) for doubling, & = 1 
n = 5 2.00 2.00 
1000 2.10 2.01 2,00 
3000 2.23 2.02 
B) for 1.1 times, & = 0.1 
n = 5 10.98 11.00 
1000 11.97 11.09 11.00 
3000 13.97 11.27 
C) for 1.01 times, 6 = 0.01 
n = 5 99.0 100.8 
1000 108,7 101,8 101.0 
3000 128,0 103.6 101.2 
456 Exhibit 3/Appendix' Details about the need to re-express 
Exhibit 3 displays the batches of sizes 10 and 20. The diversity of shape 
involved is quite clear. The extremes are probably more extreme than most 
examples that will arise in practice. 
2 
Exhibit 4 shows the apparent dependence of /'carriers on the ratio 
largest x 
smallest x 
for three groups of situations, only the outermost curves being shown for 
each--namely, from NW to SE: 
Exhibit 3 of Appendix 
The three shapes of batch for batches of 10 and 20. (Relative scales not impor- 
tant.) 
queezed- n Stretch,l- o Squeezed- in Stretched- cx/t 
� � 
� � 
� � 
� 
� 
� 
� 
� � 
� 
� 
� 
� � 
� 
� � 
� � � � 
� : 
� � t/lOre. 
� � 
� � � � 
� 
� 
� 
� � � � 
� 
� 
� 
� � 
� 
� 
� 
� � 
� 
� 
� 
� 
� 
� � 
� 
� � 
LIni) c. 0au tan unilq Crau tan 
atches of 10 6a[ches of 20 
See continuation on next page. 
C: Do we need to take Iogs?/Exhibit 3 457 
ostretched-out tails for log x symmetric and n = 10, and for x symmetric 
and n -- 10, 20, 40 (dashed boundaries) 
oneutral (-normal) tails for log x symmetric and n = 10, and for x 
symmetric and n = 5, 10, 20, 40 (solid boundaries) 
osqueezed-in tails for log x symmetric and n = 10, and for x symmetric and 
n = 10, 20, 40 (dotted boundaries). 
2 
Exhibit 4 covers ratios of about 15 down to 1.05 and corresponding /'carriers 
values from about 0.85 to 0.9999. The values of 2 
re.tiers are sufficiently similar 
for different n and their change from n = 20 to n = 40 is so small -that we 
feel relatively happy using Exhibit 4 for any value of n that comes along. 
Exhibit 3 of Appendix (continued) 
Column one: x's are equally spaced. 
Column two: x's are spaced proportionally to: 
Differences 1/9, 1/16, 1/21, 1/24, 1/25, 1/24, 1/21, 1/16, 1/9 
Values -.2829 I-.17181-.1093 I-.0617 I-.02001.0200 [ .0617 I .1093 I .1718 I .2829 
Column three: x's are proportional to: 
Values tan 85 � , tan 66.11 � , tan 44.22 � , tan 28.33 � , tan 9.44 � 
Values 11.4301 2.2578 1.0807 .5392 .1663 
and their negatives 
Column four: x's are equally spaced. 
Column five: x's are spaced proportionally to: 
Diff. 1/100 1/99 1/96, 1/91, 1/84, 1/75, 1/64, 1/51, 1/36, 1/19 
Values +.005001 +.015101 �.02552 ] �.036511 +.048411 +.061741 +.077371 +.096981 �-12476 } +.17739 
Column six: x's are spaced like: 
Values tan 85 �, tan 85�,... 
Values �11.4301, �4.0265, �2.3679, �1.6102, �1.1589, �.8470 
Values of points missing in the figure (�.6084, d=.4115, �.2386, �.0782) 
Note that overall scale is neither fixed nor relevant. 
458 Exhibit 4/Appendix: Details about the need to re-express 
For the particular illustrations of worsening (dependent on $) and of 
rr2awfit) 
closeness of fit (measured by considered in Section A, we find, if log x is 
the good carrier to use: 
1. and if doubling is permitted, 
2 
o and marginal fits are to be considered, then (since we saw that roarrirs 
might go down to 0.74) we are off the top of Exhibit 4 and might get along 
with (largest x)/(smallest x) ratios of somewhat more than 20 and not need 
to re-express; 
Exhibit 4 of Appendix 
largest x 
Dependence of 4arriers on when log x is the straightened carrier. (Solid 
smallest x 
lines = neutral tails; dashed lines = stretched tails; dotted lines = squeezed 
tails; see text for details.) 
.90' ' 
.9995 , ' ' 
.9998 // , ' -- 9998 
// , 
,9999 %' ' ,9999 
D: And what about v and l/x? 459 
o but we want to use our raw carrier for closer fits, we will have to have 
2 
(see Section A) an rca,ers of perhaps 0.8 or 0.9, and will be in the upper part 
of Exhibit 4 where one can accept (largest x)/(smallest x) of perhaps 5; 
o and we have quality of fit that would be "not so good" for many 
freshman physics experiments, calling for 2 
roarnets = 0.99995 (see Section A), 
then we are off the bottom of Exhibit 4 and have to be uncomfortable at 
(largest x)/(smallest x) values of 1.05 or 1.1. 
2. and if reasonable efficiency ($ = 0.1) is desired, 
o for marginal detectable fits, we are in the upper portion of Exhibit 4 
2 
(res = 0.92 or 0.93) and can live with (largest x)/(smallest x) ratios of 
perhaps 8 (neutral), 10 (squeezed-in to equispacing), or 4 (highly stretched); 
o for much closer fits, we are nearer the middle of Exhibit 4 2 
rcarriers -- 
0.99) and can live with (largest x)/(smallest x) ratios of perhaps 2 (neutral), 
2.5 (squeezed-in to equispacing), or 1.8 (highly stretched); 
o when the closeness is that of a nice experiment in freshman physics, so 
2 
that (see Section A) we need an roe. of perhaps 0.999999, we are far off 
Exhibit 4 to the left and can hardly expect to use Xaw, even if 
(largest x)/(smallest x) is as small as 1.02. 
3. and if we want a very careful fit (/5 = 0.01), 
o where the closeness is marginal, we are a little below the top on Exhibit 4 
2 
(rrr = 0.98) and can live with (largest x)/(smallest x) ratios of perhaps 
2.5 (neutral), 3 (squeezed-in to equispacing), or 2 (highly stretched); 
o with closeness much better than marginal, we are below the middle of 
Exhibit 4 ( 2 
rcarriers =- 0.997) and can live with (largest x)/(smallest x) ratios of 
perhaps 1.4 (neutral), 1.5 (squeezed-in to equispacing), or 1.3 (highly 
stretched). 
Clearly the ratio of largest x/smallest x we can live with depends very 
much upon how good a fit we have--and, to an only somewhat lesser extent, 
on how careful we wish to be, that is, on how small we want to keep 
D. And What About / and -l/x? 
We have answered in some detail when log x "should" be used. What if x 
"should" be used? Or-I/x? 
Exhibit 5 shows a set of curves for all three cases. For simplicity, only the 
squeezed-in and stretched-out curves are shown. In view of our finding for 
460 Exhibit S/Appendix: Details about the need to re-express 
log x, that batch size does not matter very much, only the values for n - 10 are 
shown. To make the relation to Exhibit 1 and its accompanying discussion as 
close as possible, the vertical scale is given in terms of e (rather than 
2 
roers- 1/(1 + e2)). This exhibit, when combined with Exhibit 1 and its 
accompanying discussion, tells us how important it may be to re-express an x, 
once we know what re-expression would be sound. 
Exhibit 5 of Appendix 
Relation between e and (largest x)/(smallestx) when -l/x, log x, or  is the 
correct expression. (Drawn for n = 10; useful for most reasonable n.) 
--0,5 
--0.2 
O. 
o,o.  . o.o 
o.o 0.02 
Largest 
0,01 J I I I I I > 
I,I I,Z I,S'  5 [ 0 2.0 Sinai lest 
(D) E: Derivation 461 
Thus, for one example, if e = 0.1 is the quality we require, we can 
probably stand omitting re-expression for ratios of (largest x)/(smallest x) no 
larger than: 
ofrom 1.3 (stretched) to 1.4 (squeezed) if -1/x is sound; 
ofrom 1.7 (stretched) to 2.0 (squeezed) if log x is sound; 
o from 3.5 (stretched) to 5 (squeezed) if x is sound. 
Turning back to Exhibit 1 we see that E = 0.1 corresponds to: 
odoubling the mean-square failure by using the raw carrier above 2 
'rawfit '-" 
0.98 (n  5 or 10), 0.97 (n - 100), or 0.95 (n  300). 
o adding 10% to the mean-square failure above 2 
rrawt -- 0.90 (n  10), 
0.84 (n  100), or 0.73 (n  300). 
2 
o adding 1% to the mean-square failure above rrawt = 0.60 (n- 30), 
0.50 (n - 100), or 0.33 (n  300). 
Proceeding similarly, we can get reasonable guidance in almost any situation. 
In particular: 
o although the method was derived for single-carrier regressions, we have 
no qualms about using this guidance for multiple regression. 
oif we need an E < 0.01, we would re-express without stopping to look at 
Exhibit 5. 
o while some extreme circumstances may appear to let us use e's of more 
than 0.5, we do not expect to try to take advantage of themBif we don't 
have e < 0.5, we will re-express anyway. 
o when looking at Exhibit 5 still leaves us in doubt, we go ahead and 
re-express. 
E. Derivation 
We return now to our prototype situation, and develop the formulas. Let us 
suppose that: 
1. x and z have zero means, equal variances, say 1, and zero covariance; 
2. x should be the carrier used, but we are planning to use x + Ez instead, 
where e measures the amount of irrelevance introduced. Thus, x = 
Xstraightcncd , X "{- Z = Xraw, in our previous language. Then 
3. how much do we lose in quality of estimation of a regression coefficient b 
by using x + ez instead of x? 
462 /Appendix: Details about the need to re-express 
We lose no generality by assuming that the mean, O, and variance, 1, is 
common to both x and z, and, further, that the response we are regressing is x 
itself, perturbed by independent errors rei of mean 0 and variance cr 2 so that 
the desired value of the regression coefficient, b, is 1.00. That is, the response is 
yi = x + o'er, i = 1, 2, ..., n. 
We shall judge the quality of estimate of b, not by the mean square error, 
which is 
(bias of b) 2 + (actual variance of b), 
but rather by a quantity we call the mean-square failure 
(bias of b)2 + ave(apparent variance of b), 
because we lose when we think the variance of b is greater than it actually is. 
By ave(), we mean the average (expected value) of the quantity in parenthesis. 
We use the average of the apparent variance because any systematic 
dependence of y on x that cannot be accounted for by a linear dependence has 
to contribute to the residuals, y - , and hence, to the sum of their squares. 
Thus, the s 2 upon which our conventional estimate of vat {b} is based will have 
1 
ave {s 2} = cr 2 + ', (systematic deviations) 2, 
n--1 
and we will thus ordinarily have 
ave {s 2} > rr 2, 
where cr 2 is the average variance of y (around the sum of straight-line fit and 
systematic deviation). 
Since we do not know, in practice, how the average of s 2 is divided 
between cr 2 and the systematic part, we have to suffer with an apparent 
variance for b whose average value is greater than the actual variance of b. The 
mean-square failure takes some account of this suffering (while the mean- 
square error does not). 
We now derive an expression for the mean-square failure under our 
hypotheses. By definition, we have 
](x + rrei)(xi + ezi) 
b= 
Z (x, + ' 
We have a fixed set of x's and z's, so that our assumptions about means, 
variances, and covariances imply that 
2 2 
Z = Z = 0, = = 1, Z = O. 
n-1 n-1 
To get the expected value of b, we merely multiply out and sum up terms in 
numerator and denominator, and then take expectations in the numerator. 
E: Derivation 463 
Thus 
aveb= ave[ 'x2 ] 
 + cr xe +   xz + ere  ez 
2 
Y.x 1 
2 2 2 2' 
x + e Yz 1 + e 
Because we have selected the true coefficient as 1, 
2 
bias in b = 1 - ave b = 
2' 
As we know from Chapter 14, the variance of the regression coefficient is 
the residual variance divided by the sum of squares of the carrier, and so 
2 2 
varb = = 
] (xi + Ezi) 2 (n- 1)(1 + e2)- 
The residuals. The residuals become 
x + 0-ei - b(x + ez) = (1 - b)x + 0-ei - bez. 
We need to square, take expected values, and sum over i. Squaring gives 
22 b22 2 
(1 -- b)2xi 2 + 0- ei + 5 z + 20.(1 - b)xei - 2(1 - b)bexiz - 2o'bEz. 
(1.) 
Taking expected values and summing the first three terms of (1') gives: 
2 __ 0.2 2 52 2 b 2. 
 x i ave (1 b) 2 + ave  e +  z ave (2.) 
Among the final three terms of (1'), the middle term sums to zero because 
Y xiz = 0, according to our covariance assumption. The remaining terms can 
be regrouped as: 
2crxiei -- 20-(x + ez)be. 
The term 2o'xei has expected value zero because ave ei = 0 and x is fixed. We 
need, then, deal only with 
-20.(xi + ezi)be. 
We can write the expected value of the sum as: 
-20'  (xi + Ezi) cov (ei, O), (3 *) 
where the term for the product of the means of e and b vanishes because 
ave(ei) = 0. 
Assembling our results, we get the expected sum of squares as: 
'X 2 _ 0.2 2 
i ave (1 b)2 + ave  e i 
- 20. (x+ ezi)coy (ei, b)+ E2( z)ave b 2. (4*) 
464 /Appendix: Details about the need to re-express 
Let us take up the terms one by one. In the first term, x 2 = n - 1, and 
we already know the expected value of b and the variance of b. 
ave (1- b) 2 = {ave (1- b)) 2 + var b 
= - +varb 
l+e 2 
And so the first term of (4,) becomes 
z . (1) 
(n- l) , 7  + (n- )( +  ) 
2 
In the second term of (4,), ave e = 1, and so the term has value 
ntr . (2) 
To evaluate the third term of (4,), we need an expression for cov (e, b). 
When we expand the sum in the numerator of b, we have the sum of terms like 
x + o'ex + xz + o-ez, j = 1, 2,..., n. 
These must be multiplied by e, and the expected value taken. The only value 
of j that makes a contribution is j = i, and only the second and fourth terms 
contribute. They give, since ave e 2 = 1, 
tr(x + z), 
and so the required covariance is 
(x + z)cr 
coy (e,, b) = 
Z (x, + z,) ' 
We use this to reduce the third term of (4,), and we get: 
--20- 2 (3) 
The fourth term of (4,) gives 
rr '1 T- e 2 
(n- 1)e 2 (n- 1)(1 + e 2) + ' (4) 
Adding up (1), (2), (3), and (4) and simplifying gives, as our expected sum 
of squares of residuals: 
( ) 
 2 
(n-) 7_e + . 
If s 2 is the sample variance of the residuals, then its expected value is 
2 
ave $2 ._ 0.2 q.. E 
l+e 2' 
E: Derivation 465 
The expected value of the estimated variance of b is 
2 
2  
s2 cr -t l+E2 
ave z (xi + ezi) 2 = (n - 1)(1 + 2)- 
And so, at last, the 
mean-square failure = (bias of b) 2 q- ave (estimated variance of b) 
2 
0 -2 -- 
= 1 - e 2' + (n - 1)(1 + e2) ' (MSF) 
Size o1' the irrelevant contribution. One way to think about the mean-square 
failure is as a proportional change from what we would have had under 
perfection, namely, when  = 0. Then the mean-square failure is cr2/(n- 1). 
We can write the mean-square failure as (1 + 8) times this quantity, and then 
measures the proportional increase. 
Writing 
2 
0.2 
+ a)- = - 
n - I I - 2 + (n 
we can solve for r  and simplify, to get: 
1 
n-l+ 
cr = (1-+ 1 E 2' 
Correlations. We want to find out the proportion of the variance of y ex- 
plained by regression given a value for E 2. To make the relation, first recall 
that, without error, the variance of x is 1 and that, with error, it is (1 + (r2). 
And so we write 
mean-square residual (1 2 )(variance of y) 
---  Frawfit 
or 
2 
-- rrawfit)(1 q- 0'2). 
or2+ 1 + 2 (1-- 2 
Solving this for 2 
/'rawfit gives: 
2 __ 1 
rrawfit (6 * ) 
(1 + E-)(1 + Or2) ' 
466 /Appendix: Details about the need to re-express 
where cr 2 is a function of n, /5, and e, as shown in (5,). 
Expression [or 15. Solving (5,)for /5 gives 
8= .... e 1+ 
e 2 {<n_.-__l>e2+_ i ) 
= 1 - e2[ (1 + e2)cr 2 - I . 
Solving (6,) for 2 gives 
2 
1 2 
-- rawfit 2 
2 
2 ' 
Frawfit 
So 
2 
 Frawfit  2 
l+e 
(1 + e) = (1 + e ) 
Frawfit 
 (1 +e )  
 Frawfit 
2 � 
Frawfit 
Therefore we can write 
rrawfi t 
2 - 1 , (7,) 
/'rawfit 
which can be converted to 
2 
2 /'carriers) 2 
/'careers) '' .... 2 /'rawfit. 
1 + (n- 
/'arriers- /'rawfit / 
Summary: When is Re-expression Profitable? 
We can assess whether to bother to go from x to log x (or to x, or to -l/x) 
when we know that re-expressing the carrier is a good thing. The questions that 
have to be answered are: How good a thing? Is the good worth the effort, often 
mostly the effort of explanation? 
We approach the first question in terms of two correlations: ro..cr, the 
correlation between x and log x (or the other preferred carrier); and rrawfit, the 
correlation between x (or the several x's) and y. 
Over much of the range, the answer is: "It will pay considerably to change, 
unless 1 2 2 ,, 
rc.rri� is only a small fraction of 1 - 
 /'raw fit. 
Exhibit I gives more detail, and may help us decide when this rule is not 
good enough. 
We assess the size of 2 
ro,er well enough by noticing the value of 
(largest x)/(smallest x) and turning to Exhibit 4 (for log x only) or Exhibit 5. 
Problems 
Problems are numbered by chapter and section, and by problem number within 
the section. Th.us 7C4 indicates Problem 4 of Section 7C. 
A superscript C as in 2B4 � means that if this problem is assigned it might 
be helpful for the group to discuss its solution. 
A starred problem * means that the solution may require the student to 
have more than the minimal preparation for the course. 
A P as in 16P4 means the problem is a project. 
A 0 as in 14Q1 is a problem directly tied to another chapter. 
Exhibits for problems for example, Exhibit 1 for Problem 1E3 often 
appear near the problems. We also have several sets of data in the section 
entitled Data Exhibits for Problems that appears at the end of the problems. 
Chapter 1 
1A1) State four stages in organizing data analysis, and suggest some pros and 
cons of traversing through these stages in opposite directions. 
1A2) a) What staircase of steps did Student shortcut, and 
b) What did he use in place of data? 
1A3) Discuss whether, and when, Student's shortcut was a good idea. 
1A4) Give three different instances, putting each in the framework of a specific 
application, of Student's t. Discuss the possible "contemplated values" in 
each case. (Make clear what a contemplated value is.) 
1A5') Distinguish "nonparametric" and "distribution-free" procedures. 
1B1) For about how many degrees of freedom does Student's t give a 95% 
2 
confidence limit 2.29 times as long as a  confidence limit? 
1B2) Find the smallest value of 
Length of 95% confidence limit 
Length of 2/3 confidence limit 
offered by Student's t. What sort of degrees of freedom make this ratio 
largest? What are its three largest values (for integer numbers of degrees of 
freedom)? 
467 
468 / Problems 
1B3) Find the standard confidence points, based on Student's t, for (a) 15 
degrees of freedom, (b) 60 degrees of freedom (see Note 2 of Exhibit I of 
Chapter 1). 
1B4) In what did the great value of Student's work lie? In notable changes in 
critical values? 
1B5) Discuss the drawbacks then, now, and in the future--of Student's step 
ahead. 
1C1) Is a Gaussian distribution "normal"? Why and why not? 
1C2) Do all normal distributions have the same "shape"? 
1C3) Sketch roughly the probability density function of the normal distribution 
with/ = 1, (r = 2. Be sure to label the axes. (Exhibit 3 of Chapter 1 should 
help.) 
1C4) Name a family of distributions that contains distributions with widely 
differing shapes. 
1C5) When are the tails of a distribution said to be "more stretched out"? When 
"more straggly"? What implicit standard of comparison underlies such 
verbal expressions? 
1C6) Can we rely on either the center or the tails of a distribution to tell us about 
the other? Why/why not? When yes/when not? 
1D1) For the normal distribution what are Q1 and Q37 For a uniform distribution 
running from 0 to 17 
1D2) Precisely what are skewness and kurtosis? 
1D3) In large samples, about what fraction of observations lie within 0.25s of ? 
Beyond 3.1 s? 
1D4) Class project. Find a large sample of measurements and try out some 
techniques used by Wilson and Hilferty on Peirce's data. (A source is: 
Michelson, A. A., F. H. Paese, and F. Pearson (1935). "Measurement of the 
velocity of light in a partial vacuum." Astrophysical Journal, Vol. 82, 26-61. 
See Table VI, 51-54.) 
1E1) Do you think the irregularities in Exhibit 6 of Chapter 1 are important? 
Why/why not? When/when not? 
1E2) What happens at the ends of the curves in Exhibit 7 of Chapter 17 Answer 
the same question for Exhibit 8 of Chapter 1. 
1E3) Why may deviations from the normal shape be important in the tails? 
1E4) What do we prefer to safety in statistical analyses? 
1E5) Name and explain at least two kinds of robustness. 
1E6 c) What is a "wild shot"? 
1E7) How is behavior not in the tails related to behavior in the tails? Is this 
important? Why/why not? 
Chapter 2 469 
1E8 c) How much difference do you think 1% in efficiency makes? 
1E9) A neurotic throws away half the values in his/her sample at random. How 
efficient is the mean of the half that is left relative to the mean of those 
thrown away? Relative to the mean of all the observations? 
1F1) List and discuss three vague statistical concepts, at least one of which is 
not mentioned in Section 1F or 1G. 
1G1 c) List the pros and cons of the sample median as a numerical summary. 
1G2) Do 1G1 for the sample mean instead of the sample median. 
1G3) List three instances of indications, two of which are not discussed in 
section 1G. 
1H1) Discuss the value of data analysis taken only as far as indication. 
1H2) Can chi-square, used as an indication of poor fit, be converted into an 
estimator? 
1H3) Distinguish between indication, determination, and inference. 
Chapter 2 
2A1) Why is the recipe of the second paragraph of Section 2A "a complete flight 
from reality"? 
2A2) List with explanations three instances of indication not covered, directly or 
implicitly, in Section 2A. 
2B1) Find three selected instances of indication in your daily paper. Discuss the 
importance or lack of importance of not stopping with indication in each 
case. 
2B2) Find, and discuss, two instances of problems of multiplicity that are of 
particular interest to you. 
2B3) A careful investigator tested each of 137 chemicals (in mice) for their 
possible effect in making the occurrence of one kind of cancer less 
frequent. Each chemical was compared separately with an inert material, 
so that 2 x 137 = 274 groups of mice were involved. The results were 15 
chemicals at least significant at 5%, 2 at least at 1%, 1 at least at 0.1%. 
Discuss the investigator's results. What do they seem to indicate? 
2B4 c) An agronomist tested out 11,273 new strains of a plant, obtained by 
hybridization. When arranged in order, the individual significance of cer- 
tain strains were: best 0.03%, 5th best 0.17%, 25th best 0.72%, 125th best 
2.8%, 625th best 10.3%. Another agronomist tested 1492 strains of another 
plant with substantially the same results for the significance of the best, 
5th best, .... If you had charge of deciding (a) whether either agronomist 
were to retest some selected strains of his plant and (b) how many were 
to be fetested, what would you do? 
2B5) Given a single body of data to be used for both exploration and confirma- 
tion, would you choose one-third for exploration and hint-searching, two- 
thirds for confirmation as a proper balance? Why/why not? 
470 /Problems 
2B6) Ten investigators study the same problem; no one finds an individually 
significant result, but 9 of the 10 lean in the same direction. What would 
you conclude? 
2C1) Find at least three instances of concealed "informal" inference in print. 
Copy out the relevant sentences and discuss why you do or do not find the 
inference "obvious". 
2D1) Is the mean of a population a parameter? In which sense? 
2D2) Can the family of GaussianQ9_al) distributions be parametrized by/ze =2 
and log ? By /e 2 and //2 + <27 Why/why not? 
2D3) If you knew just what question you wanted to ask, and that each of two 
estimators would provide estimates responsive to this question, what else 
would you like to know to guide your choice between these two estimates? 
Would the relative amounts of computing effort required influence your 
decision? Why/why not? When/when not? 
2D4) Which of these functions of X 2, used as a measure of goodness of fit (for a 
total of n counts) to a model of equal numbers in each of k-  + 1 
categories, has an estimand: 
x x/k, - 
Why/why not/which estimand? 
2D5) Suppose an estimator: (1) in samples of two always gives either the 
answer "0" or the answer "oo" while (2) in largish samples (say, _100) it 
gives answers close to /e 2. What is its estimand in samples of two? In 
samples of 2000? Explain your answers. 
2E1) Suppose the midmean (a trimmed mean with 25% trimmed from each 
end) is used instead of the 10% trimmed mean discussed in Section 2E. 
What Gaussian efficiency would we expect? What is the ratio of efficiency 
for the midmean to efficiency for a 10% trimmed mean in the Gaussian 
situation? 
2E2) (Continues 2E1) If the situation deviates from Gaussian shape in the 
direction of more straggling tails, what do you think will happen to the 
distribution of efficiencies? 
2E3 c) Find a table of (uniform) random numbers and use Exhibits I and 2 for 
Problem 2E3 to generate samples of 20 from both the extremely-stretched- 
tail distribution of Exhibit 1 for Problem 2E3 and the stretched-tail distribu- 
tion of Exhibit 2 for Problem 2E3. (Each individual participating should 
prepare an agreed-upon number of samples, at least 3, and should use a 
different part of the random-number table.) For each sample calculate (a) 
its median, (b)its midmean (see Problem 2E1), (c) its 10% trimmed mean. 
Collect the class's results for each of the six combinations of two tail 
stretchings with three estimators. Discuss the results and decide what com- 
parative performance of the three estimates is indicated for each degree of 
tail stretching. (SAVE BOTH SAMPLES AND ESTIMATES, tagged by indi- 
vidual responsible, FOR LATER USE.) 
Chapter 2/Exhibit I (A-D) 471 
Exhibit I for Problem 2E3 
Table for constructing samples from an extremely stretched-tailed distribution 
from random strings of digits 
A) MODE OF USE 
Draw digits from a table of uniform random numbers until the string is neither all 
nines nor all zeros. Example (not very random): 1, 7, 2, 4, 90, 001, 4, 6, 94, 98, 07, 
0000001, 999992. Here the table below is entered first with 1 giving -220, then 
with 7 giving .24, then with 2, then with 4, then with 90 giving 2.2 x 102, then 001 
giving L, and so on. 
B) TABLE 001 to 998 (values marked "L" are -< -101��, those marked "H" are 
-> + 10��). 
[Enter I I Enter J IENTERJ I Enter i I Enter J 
0007 L <-- <-- 90 2.20 x 10 = 990 2.69 X 10 41 
001 L 01 -2.69 x 104 1 -220. 91 6.69 x 10 = 991 1.80 x 10  
002 L 02 -5.18 x 10 TM 2 -1.45 92 2.68 x 103 992 1.94 x 10 s= 
003 L 03 -3.00 X 10 TM 3 -,24 93 1.60 x 104 993 1.10 x 10 "� 
004 L 04 -7.20 x 108 4 -.07 94 1.73 x 105 994 2.41 x 10 TM 
005 -7,33x 1084 05 -4.85x107 5 ,00 95 4.85x 107 995 7,33x 1084 
006 -2.41 x 10 TM 06 -1.73 x 105 6 .07 96 7.20 x 108 996 H 
007 -1.10 x 106� 07 -1.60 x 104 7 .24 97 3,00 x 10 = 997 H 
008 -1.94 x 1052 08 -2.68 x 103 8 1.45 98 5.18 x 10 TM 998 H 
009 -1.80x 10  09 -6.69x102 --> --> 9997 H 
C) EXTREME VALUES--rarely needed 
If numerical values are needed for cases marked "L" or "H" in panel B, convert 
strings of digits to a fraction u, and use 
Value -' -e/U/100 = -10 ('434/u)-2 for u -< .004, 
Value '-- e/(-")/100 = 10 ('"/(-)- for u -> .996, 
where exponents can be taken to the nearest integer with adequate accuracy. 
Thus: 
Rounded 
I Stringl I Fraction u [ .434/[u or I - u] I MINUS 2 
001 .001 434.0 432 
0000001 .0000001 434000.0 433998 
99999 .99999 4340.0 4338 
003 .003 144.7 143 
D) UNDERLYING FORMULA using fraction "u" 
Value -- (e - ) 
100 
472 Exhibit 2/Problems 
2E4 c) Make up a further table, like that of Exhibits I and 2 of Problem 2E3, but 
less extreme. 
2E5 c) (Continues 2E4) Use the table constructed in 2E4 in the same way that 
Exhibits 1 and 2 were used in 2E3. (SAVE RESULTS.) 
2F1) A sports writer claims that, to predict the difference in the number of runs 
between two teams in a particular game, you should take the mean 
difference for all the other games between these two teams played in the 
same season. Given a season's detailed record, can we cross-validate this 
procedure? If so, what exactly would we do? If not, why not? 
2F2) Another sports writer proposes to use the same procedure for the differ- 
ence in points between two football teams. Same questions. 
2F3) What differences would it make in 2Flmor in 2F2mif only games already 
played were used for prediction? 
Exhibit 2 for Problem 2E3 
Table for constructing samples from a stretched-tailed distribution from random 
strings of digits 
A) MODE OF USEsee Panel A of Exhibit I for Problem 2E3. 
B) TABLE 0001 to 9998 (for extreme values see panel C). 
t Enter I I Enter I I Enter I IENTERI I Enter I I Enter J [ Enter J 
00007 L <-- <-- ,-- 90 .98 990 100 9990 1.00 x 104 
0001 -1.00 x 106 001 -10000 01 -100. 1 -.98 91 1.22 991 123 9991 1.23 x 104 
0002 -2.50 x 105 002 -2500 02 -25.0 2 -.23 92 1.55 992 156 9992 1.56 x 104 
0003 -1.11 x 105 003 -1111 03 -11.1 3 -.09 93 2.03 993 204 9993 2.04 x 104 
0004 -6.25 x 104 004 -625 04 -6.25 4 -.03 94 2.77 994 278 9994 2.78 x 104 
0005 -4.00 x 104 005 -400 05 -3.99 5 .00 95 3.99 995 400 9995 4.00 X 10 4 
0006 -2.78 x 104 006 -278 06 -2.77 6 .03 96 6.24 996 625 9996 6.25 x 104 
0007-2.04x104 007 -204 07 -2.03 7 .09 97 11.1 997 1111 9997 1.11 x 10 s 
0008 -1.56 x 104 008 -156 08 -1.55 8 .23 98 25.0 998 2500 9998 2.50 x 10  
0009 -1.23 x 104 009 -123 09 -1.22 --> --> --> 99997 H 
C) EXTREME VALUES RARELY NEEDED 
Strings 0000...: Multiply values in lefthand column of panel B by one factor of 
100 for each additional 0 beyond three. 
Strings 9999...: Multiply values in righthand column of panel B by one factor of 
100 for each additional 9 beyond three. 
D) FORMULA 
With u, the corresponding fraction between 0 and 1, we have 
1 (1-)2 2 
Value = 1)0 u 
Chapter 3 473 
2F4 c) What would double cross-validation mean in the examples of 2F1 and 
2F27 
2F5) Collect 10 (x, y) pairs of numbers that interest you (not perfectly linearly 
related). Fit y - a -I- bx by the best method you know. Then divide the 10 
pairs, randomly, into halves, fitting y = a + bx to each half and cross- 
validating it on the other. How much has '-(Yobserved- Yfitted) 2 changed, in 
each half, from one situation to the other? How would you explain this? 
2F6) (Continues 2F5 but with more extensive calculations) Fit y = a -I- bx to 
each set of 9 out of 10 of the pairs of 2E5. Cross-validate each on the pair 
left out. Compare results with those of 2F5. 
2F7) Do the same as in 2F5, where (x, y) now equals (i, (i- 5.5) 2) for i = 
1,2,..., 10. 
2F8) (Continues 2F7, but with heavier calculations) Do the same as in 2F6 where 
(x, y) now equals (i, (i - 5.5) 2) for i = 1,2,..., 10. 
2F9 c) Suppose we have eight values a, b, c,..., h; in how many different ways, 
each division symmetrically related to each of the others, can you divide 
them 4 and 4 (into halves)? 
2F10) What advantages are shared by all objectively formalized methods of 
fitting and by no methods that fail to be "objective"? Discuss. 
Chapter 3 
3A1) In 
2 01 
3 
4 67 
what are the stems? What are the leaves? What set of values is rep- 
resented? 
3A2, 3A3, 3A4) Make up the stem-and-leaf display corresponding to the popula- 
tion of the 50 U.S. states plus the District of Columbia in (3A2) 1960, (3A3) 
1965, and (3A4) 1970 (Data Exhibit 3 for Problems). 
3A5) (Continues 3A2-3A4) Compare the stem-and-leaf displays of 3A2, 3A3, and 
3A4. 
3A6) Construct the stem-and-leaf displays corresponding to births and to deaths 
in the 50 U.S. states from 1960-1970 (Data Exhibit 3 for Problems). 
3A7) Compare the stem-and-leaf displays of 3A6 with each other and with those 
of 3A2-3A4, if you did them, expressing similarities and differences in 
words. 
474 Exhibit 1/Problems 
3A8) Exhibit I for Problem 3A8 shows the sizes of some of the cities in the U.S. 
with an estimated population of more than 250,000 as of July 1, 1973. Make 
up the corresponding stem-and-leaf display with unsplit stems, but changes 
in stem depth (as in Exhibit 2A of Chapter 3). 
3B1) What is a median? 
3B2) What is the median value of 24 values? of 25 values? 
3B3) Find the median values of the batches in Panel B of Exhibit I and Panel B of 
Exhibit 2 of Chapter 3. 
3B4) Find the hinges and eighths of the batches in Problems 3A2 and 3A4. 
3B5) Complete a skeleton letter-value display (like Exhibit 3 of Chapter 3) for the 
data of Exhibit I of Chapter 3. 
3B6, 3B7, 3B8) (continues 3A4) Complete a skeleton letter-value display for the 
data (3B6) of 3A2, (3B7) of 3A3, and (3B8) of 3A4. 
Exhibit I for Problem 3A8 
Population of U.S. cities above 250,000 in 1973. 
Population Population 
Metropolitan July 1, 1973 Metropolitan I July 1, 1973 
area (in thousands) area (in thousands) 
Akron, Ohio 677 Hartford, Connecticut 733 
Albany, New York 800 Honolulu, Hawaii 686 
Albuquerque, N. Mex. 376 Houston, Texas 2168 
Atlanta, Georgia 1748 Indianapolis, Ind. 1137 
Austin, Texas 375 Jackson, Mississippi 275 
Baltimore, Maryland 2128 Jacksonville, Florida 661 
Birmingham, Alabama 787 Jersey City, N.J. 598 
Boston, Mass. 2898 Kansas City, Mo.-Kans. 1299 
Bridgeport, Connecticut 397 Lancaster, Pa. 335 
Buffalo, New York 1345 Las Vegas, Nevada 308 
Charleston, S.C. 352 Lexington, Kentucky 282 
Charleston, W. Va. 256 Los Angeles, California 6924 
Chicago, Illinois 7002 Louisville, Ky.-Ind. 886 
Cincinnati, Ohio 1383 Madison, Wis. 301 
Cleveland, Ohio 2006 Miami, Florida 1370 
Columbia, S.C. 349 Milwaukee, Wis. 1417 
Columbus. Ohio 1057 Minneapolis-St. Paul, Minn. 2000 
Dayton, Ohio 848 Nashville, Tenn. 732 
Denver, Colorado 1377 Nassau-Suffolk, N.Y. 2630 
Detroit, Michigan 4446 New Haven, Connecticut 415 
El Paso, Texas 390 New Orleans, La. 1083 
Erie, Pennsylvania 273 New York, N.Y.-N.J. 9739 
Fort Lauderdale, Florida 756 Newark, N.J. 2053 
Fresno, California 435 Philadelphia, Pa.-N.J. 4806 
Harrisburg, Pa. 425 Phoenix, Arizona 1127 
Pittsburgh, Pa. 2365 
Chapter 3 475 
3C1) Give a letter-value display, including spread, for the U.S. population in 
1965 and the births and deaths during 1960-1970 (see batches in 3A2 and 
3A4). 
Re 3D. Data Exhibit I for Problems (at the end of Problems) gives various useful 
kinds of data for 152 cities of 25,000 or more (in 1960) in three census 
regions--West North Central, West South Central, and Mountain of the 
United States. Each solver's next task will be to select his or her own 
subsamples of (about) 50, of (about) 20, and of (about) 5 from this 
collection. Before doing this, however, a few questions are best answered. 
3D1 c) If you have access to a table of random numbers (random decimal digits), 
how can you use such a table to draw out the subsamples just mentioned? 
3D2 c) If all you have are index cards and implicit confidence (probably unwar- 
ranted) in your own ability to shuffle cards until they are random, how 
could you prepare to get subsamples as nearly random (still of sizes close 
to 50, 20, and 5) as your shuffling will permit? 
3D3) Two analysts, who lacked both random numbers and confidence in their 
shuffling, went about things in the following way: First, keeping the order 
of the cities in Data Exhibit 1 for Problems, they divided the 152 into 51, 50, 
and 51. Then they divided each 51, still in the same order, into three 17's, 
and each 50 into 17, 16, and 17. Then they thirded these into either 6, 5, 
and 6, or 6, 5, and 5. 
One of them claimed that taking the third 51, the fifth 16, and the 7th 6 was 
a satisfactory way to pick out subsamples. The other argued that there 
were obviously too many Texas cities in the subsample of 49, and that 
everyone knew that Texas was different. A better approach to getting a 
subsample of 50 or 51, she claimed, was to begin with the 1st, 2nd, or 3rd 
city in the list, choosing among these "at random" and then to take this 
city and every third city thereafter. 
You are now to give judgments stating not only which you prefer, but 
what pros and cons you can see for each--and whether you would prefer 
still another approach. 
3D4 c) Given two different coins, say a penny and a nickel, how can we flip them 
and pick out one of three alternatives with substantia_lly equal probability? 
(Optional: Could three different coins be used to do this more safely?) 
3D5 c) Would it have helped the two analysts mentioned in Problem 3D3 to have 
had a copy of Moses and Oaldord's (reference after Chapter 10) table of 
random permutations? 
3D6) Draw your own subsamples of about 50, about 20, and about 5 from the 
152 cities of Data Exhibit 1 for Problems, using the best technique availa- 
ble to you. SAVE these SAMPLES for LATER USE for example, by clearly 
marking your copy of Data Exhibit 1 for Problems. 
3D7 c) A third analyst said, "1 want to save duplication, so I want my sample of 
about 5 to be part of my sample of about 20, and my sample of about 20 to 
476 /Problems 
be part of my sample of about 50." Discuss the pros and cons of accepting 
such a "box-within-box" requirement. 
3E1 c) (A variable, not median family income, to be assigned to each solver.) For 
the 152 cities in Data Exhibit 1 for Problems, k = /152 = 12 +. Focus your 
attention on (a) the 12 cities with the highest values of the variable 
assigned to you, (b) the 12 cities with the lowest values of the variable 
assigned to you. Thin down to 10 each, by omitting the 5th and 9th most 
extreme in each set. For the two sets of 10 remaining, make plots like 
those in Exhibit 5 of Chapter 3. Plot your assigned variable against two 
others, one being median family income. (Class) Collect all the plots; 
discuss what they seem to show. 
3E2) (Continues 3E1, with same assignment of variables to solvers.) Take the 
sample of about 50 cities you selected in 3D6. Plot your assigned variable 
against median family income for these "50." How well did the corres- 
ponding 10-and-10 plot of 3E1 allow you to forecast the fifty? And vice 
versa? 
3E3) Do 3E2 again, but using your sampie of about 20. Answer the same 
question. 
3F1) (Requires assignment of variable, possibly the same as in 3E1, not median 
family income, to each solver.) Using your sample of 50 cities, make a 
stem-and-leaf of the 50 values of median family income, and then use this 
to help you write out the 50 cities in order of median family income. If any 
cities are tied, bracket them. Then turn to your assigned variable and write 
down its values (in the same order, replacing values for bracketed cities by 
their median) as �. You now have either 50 (if there were no ties) or 
 somewhat fewer than 50 (if there were ties) values of y'. Now smooth this 
sequence by running medians of 3 according to the pattern of Exhibit 6 of 
Chapter 3. Plot the result. Discuss what the result means to you. 
3F2) Do the same as 3F1 with a different variable as �. 
3F3) Do the same as 3F1, choosing a variable given in Data Exhibit 1 for 
Problems (at the end of Problems) other than either median family income 
or the variable assigned to you (or the variable chosen in 3F2) as the basis 
for ordering your 50 cities, but taking the same �'s. Discuss the similarities 
and differences, both as to appearance and as to meaning, between this 
result and that of 3F1. 
3F4) Combine the changes in 3F2 and 3F3. Same instructions as 3F3. 
3F5) "Age heaping." It is a well known demographic phenomenon that people 
tend to report their ages rounded to the nearest multiple of 5 years. The 
reported female age distribution for Mexico in 1960 (Source: Mexico. 
Direction General de Estadistica, VIII censo general de poblacion, Cuadro 7, 
1962) for ages 0 to 75 is given in Exhibit 1 on page 477. Analyze this data 
by means of running medians. Does it make any difference whether 
running medians of length 3, 5, or 7 are used? What patterns show up in 
the residuals? Do they support the age-heaping hypothesis? 
Chapter 3/Exhibit I 477 
3F6) Repeat Problem 3F5 working with the square roots of the numbers report- 
ing each age. 
3G1) (Continues 3F1.) Eye-fit a straight line 
y- a + b x (median family income) 
to the smoothed results of 3F1. Adjust each of the 50 individual y's by 
subtracting this straight line. Smooth the resulting residuals by repeated 
medians of 3. Plot the result. Do you believe the straight line captured all 
the nonrandom relation of y to median family income? Why/why not? Add 
back the same straight line to the smoothed residuals, and plot on the same 
graph with the results of 3F1. How do the two sets of results compare? Why 
do you think this happens? 
3G2) (Continues 3F2.) Do the same for the data of 3F2 (instead of 3F1). 
Exhibit 1 for Problem 3F5 
RepoSed female age distribution for Mexico, 1960 (in thousands) 
lAgel I# at Age l lAgel I# at Age I Age I# at Age! 
0 558 
1 513 26 243 51 42 
2 582 27 220 52 86 
3 604 28 283 53 61 
4 584 29 182 54 66 
5 566 30 412 55 148 
6 562 31 113 56 69 
7 524 32 208 57 46 
8 529 33 163 58 83 
9 430 34 146 59 48 
10 497 35 310 60 245 
11 369 36 168 61 20 
12 455 37 130 62 43 
13 398 38 224 63 32 
14 404 39 129 64 32 
15 382 40 331 65 103 
16 366 41 51 66 29 
17 346 42 136 67 23 
18 403 43 91 68 40 
19 300 44 77 69 16 
20 409 45 231 70 109 
21 226 46 90 71 9 
22 325 47 77 72 25 
23 294 48 148 73 15 
24 289 49 78 74 14 
25 380 50 281 75 48 
478 /Problems 
3G3) Exhibit 16 of Chapter 3 (in Section 3H) omits (in comparison with Exhibit 15 
of Chapter 3) the 60 points corresponding to even numbers divisible by 6. 
Write the Goldbach counts for these 60 numbers down (in order of the even 
numbers) and smooth them by repeated running medians of 3. Subtract the 
solid "curve" from Exhibit 16 and plot the differences. 
3G4) (Continues 3G3.) Take the differences just formed and smooth them once 
again by repeated running means of 3. Plot the result. Can you find a 
simple approximation7 
3H1) (3H1 to 3H3 can also be usefully shared across the class.) Make a plot like 
Exhibit 15 of Chapter 3, using the Goldbach counts from 252 to 430 (instead 
of from 2 to 180). 
3H2) Do the same from 9502 to 9680. 
3H3) Do the same from 9752 to 9930. 
3H4 to 3H6) (Continues corresponding problem above.) Do the same for Exhibit 
16 of Chapter 3, including a broken line for the result of smoothing (as in 3F 
or 3G) the Goldbach counts for even numbers divisible by three (as in 3G3). 
311) (Class exercise.) The table from which Exhibit 19 of Chapter 3 was taken, 
which also appears at page 248 of the 1973 World Almanac, shows, for the 
same 30-year period: (a) normal minimum January temperatures; (b) 
normal July maximum temperatures; (c) normal July minimum tempera- 
tures; (d) highest temperature recorded; (e)lowest temperature recorded; 
(f) normal annual precipitation. These seven variables should be shared 
out, about one-seventh of the class receiving each. Then plots like Exhibit 7 
of Chapter 3 should be made. 
312) (Continues 311.) Eye-fit a slope to the plot just made and plot the residuals 
against longitude as in Exhibit 18 of Chapter 3. (Those who like the west on 
the left may run their horizontal scale from right to left.) Discuss your resllt. 
313) (Continues 312.) Setting aside any cities for which the residuals of 312 
seemed clearly unusual, seek out the best .most useful, most regression- 
producing--variable you can find against which to plot the remaining 
residuals. Make the plot and discuss your results. 
3J1) For the about 50 cities in your own sample, relate median family income to 
percent housing in single-unit structures (a) in terms of 10-plus-10, and (b) 
in terms of the smoothed median of the medians of successive blocks of 
(about) 3 cities (16 blocks). Plot the analog of Exhibit 21 of Chapter 3 and 
discuss your results (i) by themselves and (ii) in comparison with that 
exhibit. 
3J2) Do the same (except for (ii)) relating median family income to the variable 
assigned to you. 
Chapter 4 479 
Chapter 4 
4A1) Where would z = t /3 fit in the ladder of powers? Or would it? 
4A2) Where would z = /t + 4 fit in the ladder of powers? Or would it? 
4A3) Which rungs in the ladder of powers are also usefully defined for t  07 
4B1) Consider the curve defined (in part, but well enough for our purposes) by 
the following table: 
x: .73 1.27 1.73 2.31 2.50 2.72 2.91 
y: .05 .26 .65 1.54 1.95 2.52 3.08 
What re-expression of y would straighten out the curve? (Explore as in part 
I of Section 4B.) 
4B2) For the same curve of Problem 4B1, what re-expression of x would 
straighten out the curve? (Explore as in part 2 of Section 4B.) 
4B3) For the data in Problem 4B1, try log y and "starting" as in part 3 of 
Section 4B. 
4B4) Re-express y to straighten out the curve implied by the following table: 
x: .73 1.27 1.73 2.31 2.50 2.72 2.91 
y: 1.11 1.33 1.48 1.63 1.67 1.72 1.76 
4B5) Re-express x to straighten out the curve of the table of Problem 4B4. 
4B6) Try log y and "starting" to straighten out the curve of the table of Problem 
4B4. 
4B7) Re-express x to straighten the curve implied by the following table: 
x: .73 1.27 1.73 2.31 2.50 2.72 2.91 
y: 2.22 2.34 2.44 2.55 2.59 2.63 2.67 
4B8 c) Find the best re-expression you can for the table of 4B7. 
4C1) Exhibit 1 shows various curves. Tell which way in y and, alternatively, 
which way in x, we should move along the ladder of powers to straighten 
(at least partially) the solid curve in the NW panel (see next page). 
4C2) Same for the dashed curve in the NW panel. 
4C3) Same for the dotted curve in the NW panel. 
4C4/5/6) Same for each of the three curves in the NE/SW/SE panel. 
4C7) Given below are the "qx" values for the United States in 1970. These are 
the probabilities that a person, aged x, will die in the next five years. 
480 Exhibit 1/Problems 
What re-expression of x, qx, or both seems to straighten this curve best? 
(x -- age, qx = probability of dying) 
I x 
.ooo 
20 .0074 60 .096 
25 .0072 65 .138 
30 .0097 70 .198 
35 .012 75 .290 
40 ,019 80 ,407 
85 .555 
Exhibit I for Problem 4C1 
,i"';' "", /// 
/ 
/ 
\, / \ 
',, ! 
Chapter 4/Exhibit I 481 
4D1) The following table gives the number of primes less than n for selected 
values of n: 
n 4 16 64 256 1024 4096 16384 
# 2 6 18 54 172 564 1900 
Plot log # against log n. What does the plot suggest for straightening out 
the relation between # and n? 
4D2) The following table gives values of cot e for selected values of e: 
e: 0.1 � 0.2 � 0.5 � 1 � 2 � 5 � 10 � 20 � 50 � 
cot e: 1145.9 286.3 114.59 57.29 28.63 11.43 5.67 2.75 .84 
Plot log cot e against log e. What does the plot suggest for straightening 
out the relation7 
4D3) Using the table of Problem 4D1 as a base, plot #/n against n. What does 
this suggest for straightening out the relation? Then choose between 
plotting log (#/n) against n and plotting #/n against log n. What does 
your chosen plot suggest7 
4D4) Using the table of Problem 4D2 as a base, plot e cot e against e. What does 
this suggest? Choose re-expressions of e against which to plot e cot e, 
Which gives the most helpful result? What would you plot next? 
4E1 c) (One eighth of the class to be assigned each variable other than median 
family income.) Go back to Exhibits 4 and 5 of Chapter 3, and add the 
ten urban unincorporated places (given in Exhibit I for Problem 4E1) 
with median family incomes at and near the medians of all 88 places. 
Exhibit I for Problem 4E1 
10 urban unicorporated places near median for median income, 1959 (see Exhibit 
4 of Chapter 3 for further detail) 
Median 
family 
I Place income 1221 2291 I.1  2tAt 2 49 25J 2L J 
Braintree, Mass. 7474 31.0 60.7 52.2 7.7 5.8 88.8 19.4 6,7 
Ross, Pa. 7475 31.1 52.6 4.0 12.9 5.7 86.1 28.6 6.5 
Elmont, N.Y. 7494 31.1 68.2 44.2 33.2 5.6 88.5 16.6 19.0 
Framingham, Mass. 7495 29.1 44.5 53.0 5.9 5,6 79.8 32,7 21.0 
Arlington, Mass. 7538 34.8 61.8 60.4 27.7 5.8 56.8 22.3 7.8 
Natick, Mass. 7550 28.7 57.8 55.7 9.1 5.9 81.6 23.7 5.4 
Ewing, N.J. 7597 31.1 56.0 48,7 6.8 5.6 91.5 23.0 24.0 
Middletown, Pa. 7656 22.6 19.2 56.3 7.9 6.1 99.2 35.8 24.3 
Catonsville, Md. 7662 32.0 50.5 64.5 14.9 6.0 82.3 25.9 14.8 
Hamden, Conn, 7741 35.5 59.4 55.3 13.7 5.7 84.5 21.4 10.6 
482 / Problems 
Replace each set of ten places by the point (median of variable assigned to 
you, median of median family income). Seek a re-expression that will put 
these three points nearly along a single line. Discuss. (Bring all results for 
each of the 8 variables together in class, and discuss.) 
Chapter 5 
5A1 c) Label each of the following according to whether they are amounts, 
counts, balances, counted fractions, ranks, or grades: the concentration of 
salt in sea water, the concentration of carbon dioxide in the atmosphere, 
the fraction of persons of Hispanic descent in Manhattan, the number of 
tons of salt in the sea, the number of molecules of carbon dioxide in the 
atmosphere, the average number of years of life (the expectation of life) 
for females born in Germany in a given year, the income for tax purposes 
of a large corporation, the income for tax purposes of an individual, the 
class of a baseball league, the "points" by which a football game is won, 
the position of a football team in the standings (how many ahead and how 
many tied), the letter grades given in a mathematics course, how far "east 
of Boston" is a town in Vermont, how far above water is a submerged 
volcano, or a volcanic island, how many fingers an injured man has lost, 
how many tires on an old car are badly worn. 
5A2) If you wanted to re-express each number in 5A1, what re-expression would 
you try first? 
5B1) Explain in detail how to use Exhibit 1 of Chapter 5 to obtain a two-decimal 
log of 321.98. Of 0.000098321. 
5B2) Two analysts were debating whether to use log (� + 0.01) or log (y + 0.02). 
For which �'s would their decision really matter? 
5B3) Another pair were debating between log (y -I- 1) and log (y + 3). For which 
y's would this matter? How could log (y + 1) or log (y + 3) be easily found? 
5B4) Evaluate log 17-; log 23.. 
5C1) Explain how to use Exhibit 3 of Chapter 5 to obtain a two- or three-digit 
square root for 321.98, and for 0.000098321. 
5C2 c) Describe the pros and cons of working with each of 1000/y, l/y, -l/y, 
-1000/�. 
5c3) Yet another pair of analysts were arguing about choosing /y + .05 or 
/y + .1. For what y's would this choice matter? 
5C4) A fourth pair of analysts were torn between choosing /y + 1 and /y + 2. 
For what y's would this choice matter? 
5C5) Describe how to use Exhibit 4 of Chapter 5 to obtain a two- or three-digit 
negative reciprocal for 321.98, and for 0.000098321. 
5C6) What are the values of 1/, 2/-, 4, and 2? 
Chapter 5 483 
5C7) What are the values of -1000/17-, -1000/23-, -1000/47-, and -1000/289-? 
5D1) What are the matched re-expressions of 86.5%7 Of 13.5%7 Of 98.9%7 Of 
1.1%? 
5D2) Describe in detail how it is easy to calculate 
log (one count) - log (the other count) 
when the counts are both started by { and the raw counts were 17 + 133 = 
150. 
5D3) Do the same as in Problem 5D2 for 
v/one count - /the other count 
for the same start and raw values. 
5D4 c) Discuss the pros and cons for having various re-expressions of fractions 
matched near 50%. 
5E1) Why do we have to pick a value at which to match powers and logs when 
we didn't seem to do anything like this when matching re-expressions of 
fractions? 
5E2) If we know that a matched re-expression of y does not differ from y by 
more than �1 for all y between 280 and 320, what matched powers could 
be providing such a re-expression? 
5E3) Same question as Problem 5E2 with agreement to �10 for 2800-< y-< 
3200. 
Warning: Problems in section F require substantial calculations. 
5F1) Hartman (quoted by K. A. Mather, 1949, Statistical Analysis in Biology, 
page 196) gave counts of the number of men and the number of women 
who could not taste phenylthiocarbamide in various strengths. The counts 
were 
Men: 15 A 35 B 46 C 31 D 23 E 13 F 9 G 7 H 10 I 13 J 25 K 63 Total 290 
Women: 42 A 52 B 38 C 30 D 19 E 17 F 6 G 5 H 10 I 19 J 33 K 43 Total 314 
Here A is the strongest solution, which 15 of the men could not taste, while 
35 more could taste A but not B, and so on to the 63 men who could taste 
the weakest solution. Guided by Exhibit 10 of Chapter 5, use Exhibit 11 of 
that chapter to obtain two re-expressions for A, B,..., K, one for men and 
one for women. 
5F2) Davis and Richards (Journal of Ecology, 21:350-384 and 22: 106-155) give 
counts of trees of seven species (one of the seven is two species hard to 
distinguish) by six size classes i.n a mixed rain forest (jungle) at Moraballi 
Creek, British Guiana. The size classes were measured, but you are not 
being told what diameters cut off the classes. The counts, from smallest 
484 / Problems 
class to largest class are as follows: Eschweilera (decolorans or pallida) 
(128, 19, 14, 7, 1, 1); Eschweilera sagotiana (48, 5, 6, 14, 13, 1); l_icania 
laxifiora (24, 18, 13, 5, 6, 0); Licania heteromorpha (176, 29, 12, 0, 0, 0); 
Licania venosa (128, 26, 16, 9, 10, 0); Ocotea rodiaei (16, 8, 2, 5, 10, 3); 
Pentaclethra macroloba (80, 31, 20, 16, 6, 0). Find a re-expression for each 
size class (omitting empty size classes) using, in turn, each species. Do the 
seven re-expressions seem to agree fairly wel? 
5F3) (Continues 5F2) For each size class in 5F2, find the mean re-expression for 
the three smallest-size classes (chosen as occupied for all species) and 
subtract the result for all values of that re-expression. For each size class 
you will now have three to seven such adjusted re-expressions. Take their 
median as a first approximation to an overall re-expression for that size 
class. Plot each original re-expression against this approximate overall 
re-expression. Do the plots look reasonably like straight lines? 
5F4) (Continues 5F3) Eye-fit straight lines to each of the seven plots with which 
Problem 5F3 closed. Calculate the values of 
Residual from line 
Second adjustment -- 
Slope of line 
for each combination of size class and species. Take medians over species. 
Do these seem meaningfully large? Form an improved overall re- 
expression for each size class as 
approximate overall PLUS median second adjustment. 
How nearly equally spaced do the size classes seem to be? 
5G1) Re-expression of ranks is sometimes useful when the data given is numeri- 
cal. The 1960 population (POP) and median family income (MFI) of the 
counties of Arizona, of the counties of each of the (1964) Congressional 
districts of Arkansas, and of each of three "stripes" along California are 
given in Exhibit I for Problem 5G1, ordered by 1960 population. For the 
state or portion assigned to you, find the re-expression log (i--) MINUS 
log (n + 1 - i -�) for both the POP's and MFI's. Plot both raw MFI against 
raw POP and re-expressed MFI against re-expressed POP. Which plot 
seems more useful? 
5G2) Take 3 points from each of the eight county groupings given in Exhibit 1 for 
Problem 5Gl--the second (in smaller groupings) or the third (in larger 
groupings) from each end and the middle point are convenient choices 
and explore to see what power (or log) of the 1960 population appears 
to behave in a reasonably .straight-lin. e way with the log (i--) MINUS 
log (n + 1 - i-�) re-expression (used n 5G1). 
5G3) (Continues 5G2) For the county groupings assigned to you, plot your 
selected power re-expression of the 1960 population against its rank-based 
re-expression. 
Chapter 5/Exhibit I 485 
Exhibit I for Problem 5G1 
Populations and median family incomes (MFI) of counties in selected areas 
I Arizona 1960 I First CD Arkansas 1960 
Con, II II II POP il 
Maricopa 663,510 5896 Mississippi 70,174 2725 
Pima 265,660 5690 Crittenden 47,564 2506 
Pinal 62,673 4412 Craighead 47,303 3408 
Cochise 55,039 5107 Phillips 43,997 2360 
Yuma 46,235 5360 St, Francis 33,303 1973 
Coconino 41,857 5398 Poinsett 30,834 2591 
Navajo 37,994 4237 Greene 25,198 2654 
Apache 30,438 2832 Clay 21,258 2633 
Yavapai 28,912 5197 Lee 21,001 1710 
Gila 25,245 5087 Cross 19,551 2480 
Graham 14,045 4593 
G reenlee 11,059 5168 
Santa Cruz 10,808 4620 
Mohave 7,736 5111 
[ Second CD Arkansas 1960 II Third CD Arkansas 1960 
[ County I POP 11MFI I County l POP I! MFI 
Pulaski 242,980 4935 Sebastian 66,685 3089 
White 32,795 2893 Washington 55,797 3683 
Lonoke 24,551 2708 Benton 36,272 3180 
Faulkner 24,303 2968 Crawford 21,318 3122 
Arkansas 23,355 3348 Pope 21,1 77 3046 
Jackson 22,843 2995 Boone 16,116 2837 
Independence 20,048 2502 Logan 15,957 2376 
Monroe 17,327 2162 Johnson 12,421 2484 
Lawrence 17,267 2255 Yell 11,940 2600 
Conway 15,430 2751 Carroll 11,284 2555 
Woodruff 13,954 1902 Franklin 10,213 2611 
Randolph 12,520 2497 Baxter 9,943 2800 
Prairie 10,515 2853 Madison 9,068 1928 
Cleburne 9,059 2137 Searcy 8,124 2066 
Izard 6,766 2699 Scott 7,297 2168 
Fulton 6,657 1886 Van Buren 7,228 1968 
Sharp 6,319 1902 Marion 6,041 2210 
Stone 6,294 1740 Newton 5,963 1666 
Perry 4,927 2217 .. 
486 Exhibit I (continued) / Problems 
5G4) (For those who wish to explore further, and who have access to a 1962 
County and City Data Book.) Other columns that might be interesting to 
examine, separately or together, are (4) population density, (5) population 
% growth, (13) median age, and the ratio of (107) number of food stores to 
(86) number of manufacturing establishments. 
5H1) How much does section 5H tell you that the earlier sections have not told 
you? 
5H2) You have a large number of triangles, each of which has one side 10 units 
long and one side 5 units long. Under first-aid rules, how would you 
re-express the length of the third side? 
Exhibit I for Problem 5G1 (continued) 
Fourth CD Arkansas 1960 I Tidw* California 1960 I 
I County I POP II MFI II County II POP I MFI I 
Jefferson 81,373 3671 Los Angeles 6,038,771 7046 
Union 49,518 4361 San Diego 1,033,011 6545 
Garland 46,697 3511 Alameda 908,209 6786 
Miller 31,686 3372 San Francisco 740,316 6717 
Ouachita 31,641 3686 Santa Clara 642,315 7417 
Saline 28,956 4483 Sacramento 502,775 7100 
Columbia 26,400 3438 San Mateo 444,387 8103 
Ashley 24,220 3432 Contra Costa 409,030 7327 
Hot Springs 21,893 3881 San Joaquin 249,919 5889 
Clark 20,950 3127 Ventura 199,138 6466 
Desha 20,770 2430 Monterey 198,351 5770 
Hempstead 19,661 2676 Santa Barbara 168,962 6833 
Chicot 18,990 2013 Solano 134,597 6190 
Drew 15,213 2614 Humboldt 104,892 6282 
Lincoln 14,447 1911 Santa Cruz 84,219 5325 
Bradley 14,029 3069 Napa 65,890 6524 
Polk 11,981 2694 Mendocino 51,058 5803 
Lafayette 11,030 2245 Del Norte 17,771 6277 
Howard 10,878 3033 
Nevada 10,700 2538 
Dallas 10,522 2809 
Sevier 10,156 3089 
Little River 9,211 2725 
Grant 8,294 2985 
Pike 7,864 2614 
Cleveland 6,944 2363 
Calhoun 5,991 2394 
Montgomery 5,370 2572 -' 
Chapter 5/Exhibit I (continued) 487 
5H3 c) A maker of quality resistance units selects from all units produced those 
with resistance values between 99% and 101% of nominal. How would you 
re-express the measured resistances of nominal 500-ohm units? Explain 
your answer. 
5H4) Standard patterns of mortality. William Brass ("On the Scale of Mortality", 
pp. 69-110 in Biological Aspects of Demography (W. Brass, ed.), London: 
Taylor and Francis, Ltd., 1971) has proposed that there is a "standard" 
pattern of mortality which, when modified by only two parameters, ade- 
quately describes most human mortality experiences. Given in Exhibit 1 
Exhibit I for Problem 5G1 (continued) 
Adj** California 1960 ! Remaining California 1960 
Orange 703,925 7219 Tulare 166,403 481 5 
San Bernardino 503,591 5998 Butte 82,030 5408 
Fresno 365,945 6603 Shasta 59,468 5989 
Riverside 306,191 5693 M adera 40,466 4596 
Kern 291,981 5933 Yuba 33,859 5031 
Stanislaus 157,294 5260 Nevada 20,911 5419 
Sonoma 147,325 5725 Tuolumne 14,404 5602 
Marin 146,820 8110 Lassen 13,597 5861 
Merced 90,448 4806 Colusa 12,075 5604 
San Luis Obispo 81,011 5659 Inyo 11,689 5837 
imperial 72,105 5507 Plumas 11,260 5834 
Yolo 65,727 6240 Modoc 8,308 5709 
Placer 56,468 5989 Mariposa 5,064 4704 
Kings 49,954 4957 Sierra 2,247 5863 
Sutter 33,380 5670 Mono 2,213 6321 
Siskiyou 32,885 5558 Alpine 397 
El Dorado 29,390 6603 
Tehama 25,305 5589 
Glenn 17,245 5290 
San Benito 15,396 5538 
Lake 13,786 4438 
Calaveras 10,289 5824 
Amador 9,900 5636 
Trinity 9,706 6210 
* Tidw = tidewater = all counties with largest city or town on tidal water 
** Adj = adjacent = all counties bounded by tidewater counties 
s) SOURCE 
1962 County and City Data Book. 
488 Exhibit 1/Problems 
are three modern "life tables", the expected proportion of people in each 
population who live until the indicated age, and Brass's proposed standard. 
Brass has noted that plots of the logits of each life table against the logits of 
the standard are nearly linear. Dealing with each country separately, is the 
logit the best re-expression? Which others could be tried7 Considering them 
jointly, how well does the logit hold up? Do you believe there is a "standard" 
pattern of mortality, based on these data? 
511) Two analysts had data giving the number of traffic accidents hour by hour 
for three nearby large cities for the 168 hours of a particular week. One 
wanted to analyze the logs of the counts; the other objected because there 
were so many zeros. What two quite different proposals could the first 
analyst make that would avoid all difficulty with logs of zero? 
512) An investigator, studying slopes of curves made by a recording device, 
thinks logs would make sense in part because running the paper faster 
would divide all slopes by the ratio of the paper speeds. You are asked how 
to avoid trouble with both horizontals and verticals with both zero slopes 
and infinite slopes. What do you say? 
513) In 512, would your answer depend on whether the investigator had 2 zeros 
and 3 infinities in 1000, or 97 zeros and 43 infinities in 10007 Why/why not? 
Exhibit 1 for Problem 5H4 
Proportion surviving to indicated age (Brass) 
Brass's [ Sweden I [ Italy [ Japan 
I Agel Standard J Females, 1959 1 I Females, 1901-111 I Males' 19591 
1 .850 .987 .848 .964 
5 .769 .984 .739 .952 
10 .750 .982 .721 .947 
20 .713 .979 .699 .938 
30 .652 .974 .641 .917 
40 .590 .965 .592 .892 
50 .511 .943 .541 .846 
60 .396 .891 .466 .738 
70 .238 .757 .317 .524 
80 .076 .447 .107 .214 
(Data reprinted by permission of the author and of the publisher.) 
Chapter 6/Exhibit I 489 
Chapter 6 
6-1) If we would gain somewhat from re-expression, is it always worth the 
bother? Why/why not? 
6-2) If you had observations 1, 1.5, 3, 5,..., 63, 89, 121, and believed that taking 
logs would give a better expression, is it likely to be worthwhile to do this? 
Why/why not? When/when not? 
6-3) Answer Problem 6-2 if the values range from 23 and 29 to 365 and 513. 
6-4) Answer Problem 6-2 if the values range from 51 and 54 to 473 and 551. 
6-5) Answer Problem 6-2 if the values range from 1.04 and 1.09 to 1.73 and 1.81. 
Exhibit I for Problem 6-6 
Proportions of Congressional seats and popular vote (Democratic), 1900-1972 
Democrat Democrat Democrat Democrat 
Year ! % Votes % Seats 1 Year I % Votes % Seats 
1900 46.60 43.59 1936 58.48 78.91 
1902 48.68 46.23 1938 50.82 60.79 
1904 43.66 35.23 1940 52.97 62.24 
1906 46.55 42.49 1942 47.66 51.51 
1908 48.11 43.99 1944 51.71 56.12 
1910 50.50 58.46 1946 45.27 43.32 
1912 57.11 69.54 1948 53.24 60.60 
1914 50.34 54.48 1950 50.04 54.04 
1916 48.88 49.30 1952 49.94 49.08 
1918 45.10 44.63 1954 52.54 53.33 
1920 37.67 30.56 1956 50.97 53.79 
1922 46.40 47.92 1958 56.10 64.91 
1924 42.09 42.56 1960 54.97 59.95 
1926 41.57 45.14 1962 52.42 59.45 
1928 42.84 37.91 1964 57.50 67.82 
1930 45.87 49.77 1966 51.33 57.01 
1932 56.87 72.79 1968 50.92 55.86 
1934 56.18 75.76 1970 54.32 58.62 
S) SOURCE 
See E. R. Tufte, "The relationship between seats and votes in two-partysystems." American PoliticalScience 
Review, 68, June 1974, 540-554. (Data reprinted by permission of the author and of the journal.) 
490 /Problems 
6-6) Seats and Votes (Project) In a two-party parliament or congress, one does 
not expect that, if a party gets x percent of the aggregate vote, it will get x 
percent of the seats. In fact, various models have been proposed to 
describe this relationship. The simplest model is known as the "Cube 
Law", i.e., 
1-S 1-V 
where S is the proportion of seats, and V is the proportion of votes. A 
more complicated "logit" model is: 
log 1 - S = /30 +/3 log 1 - V ' 
E. R. Tufte proposes a simpler model: 
s- .5 =/30 + .5), 
where /3o has a simple interpretation as the "bias" of a system, and /31 is 
described as the "swing ratio", i.e., the percent of seats gained for one 
percent more votes. What theoretical objections are there to the first and 
third model7 What is the relationship between the Cube Law and the logit 
model? The proportions of seats and votes for 36 U.S. Congressional 
elections in the 20th century are given in Exhibit 1. Based on these data, 
and bearing in mind the theoretical aspects, which model seems most 
appropriate? In light of its simple interpretation, is the linear model 
"accurate enough" for most purposes? Can you propose a better model? 
Chapter 7 
7A1) Go back to Exhibit 5 of Chapter 5 and collect, for days 2 to 24, the apparent 
variance of a daily mean (1) as based on differences between the daily 
means and (2) as the mean of the interval estimates, (s.) 2, of the variances 
of daily means. If 
S 2 
True variance of daily mean -- , 
neff 
then 
n Variance based on differences (external) 
nef Variance based on (s) 2 (internal) 
Find an estimate of n/neff. In view of n --- 500, about how big is ne, What 
does this mean? 
7A2 c) Why is it impossible that a valid measure of uncertainty be routinely found 
by looking up a formula? 
7B1) F. A. Palumbo and E. S. Strugala (Industry Quality Control, November 
1945, 6-8) give data on the fraction defective (in vulcanization) of battery 
adapters used in Handle-Talkies on 32 consecutive lots. Each data pair 
Chapter 7 491 
gives (sample size, number defective). (140,77), (140,19), (140,24), 
(140,20), (140,27), (155, 0), (155,0), (210, 0), (155, 0), (210, 0), (50,50), (50,4), 
(50, 17), (90,0), (105,0), (105,4), (155,8), (155,2), (155, 0), (210,4), (155,5), 
(155, 7), (105, 3), (210, 12), (190, 9), (125, 7), (125, 5), (125, 2), (75,0), 
(75,4), (125, 1), (125, 2). There were 9 lots with sample sizes of 50 to 105, 10 
with sample sizes of 125 or 140, 13 with sample sizes of 155 or more. For 
each of these three groups of lots, calculate an internal estimate (average of 
pq/n) for the variance of a lot p, and an external estimate (the usual s 2 
based on the p values for those lots). Compare and discuss the comparison. 
7Cl) In a study of National Football league wide receivers, various physical 
dimensions are to be collected for later comparison with pass-catching 
performance. Incidentally, we want to study differences in wide-receiver 
heights among the four conferences into which the NFL is divided. Limita- 
tions on effort restrict us to two teams per conference. For some reason or 
other it has been decided to study wide receivers in both the most success- 
ful team in each conference and the least successful team in each confer- 
ence. What is the most nearly appropriate error term to use in comparing 
conferences? Why? If this error term is biased, in which direction is it likely 
to be off? 
7C2) In a study of 5 New England regional cookbooks, one aspect to be studied 
was a score for ease of preparation. Two proposals were made for the 
sampling of recipes to be kitchen-tested. In one, 20 recipes were to be 
selected from each book at random; in the other the recipes were to be 
divided into 5 to 10 categories: meat, fish, vegetables, sauces, and gravies, 
etc., and a fixed number of recipes were to be selected at random from each 
category. What would you consider the correct error term if each of these 
plans were adopted? 
7C3) (Continues 7C2) What if it were necessary to use several combinations of 
cooks and kitchens, which could, however, be balanced across cookbooks? 
7C4) A comparison of 6 supermarkets on prices for food not on special sale 
involved dividing the food sold into 20 categories, and selecting 10 rela- 
tively standard items in each category. These were then priced twice a week 
(on rotating days) for a year, so that 124,800 prices were collected. What 
would you recommend as the proper basis for an error term? How would 
your answer depend on whether the results were supposed to be for the 
year of observation, or to be used to guide further purchases? 
7D1) Two analysts provided alternative plans for a study. These plans allocated a 
fixed total amount of money differently. One collected more data in fewer 
independent groups--a total of 1000 observations in five groups. The other 
collected less data in more groups--a total of 700 observations in ten 
groups. If the number of groups used affects only the number of degrees of 
freedom available not the size of group-to-group variation, not the approp- 
riateness of group-to-group variation--which of these plans will, on average, 
produce the shorter 95% interval? 
492 /Problems 
7D2) (Continues 7D1) And what of a plan that collects 900 observations in 9 
groups? 
7E1) F. Proschan (Technometrics, 5:376 (1963)) gives time intervals between 
failures in the air-conditioning system of thirteen type 720 jet aircraft as 
follows (we give airplane # before a ";", followed by successive times 
between failures, stopping at any major overhaul): (7907; 194, 15, 41, 29, 
33, 181), (7908; 413, 14, 58, 37, 100, 65, 9, 169, 447, 184, 36, 201,118), (7909; 
90, 10, 60, 186, 61, 49, 14, 24, 56, 20, 79, 84, 44, 59, 29, 118, 25, 156, 310, 76, 
26, 44, 23, 62), (7910; 74, 57, 48, 29, 502, 12, 70, 21, 29, 386, 59, 27), (7911; 55, 
320, 56, 104, 220, 239, 47, 246, 176, 182, 33), (7912; 23, 261, 87, 7, 120, 14, 
62, 47, 225, 71,246, 21, 42, 20, 5, 12, 120, 11, 3, 14, 71, 11, 14, 11, 16, 90, 1, 
16, 52, 95), (7913; 97, 51, 11, 4, 141, 18, 142, 68, 77, 80, 1, 16, 106, 206, 82, 
54, 31,216, 46, 111, 39, 63, 18, 191, 18, 163, 24), (7914; 50, 44, 102, 72, 22, 
39, 3, 15, 197, 188, 79, 88, 46, 5, 5, 36, 22, 139, 210, 97, 30, 23, 13, 14), (7915; 
359, 9, 12, 270, 603, 3, 104, 2, 438), (7916; 50, 254, 5, 283, 35, 12), (8044; 487, 
18, 100, 7, 98, 5, 85, 91, 43, 230, 3, 130), (8045; 102, 209, 14, 57, 54, 32, 67, 
59, 134, 152, 27, 14, 230, 66, 61, 34). For each airplane calculate (a) median 
time between failures, (b) fraction of failure intervals less than 20, (c) 
median length of intervals less than 20, (d) median length of intervals equal 
to or larger than 20, and apply Student's t to set limits on the averages of 
each of these four over time and airplanes. 
7E2) R. L. Anderson (Journal of the American Statistical Association, 42:612-634 
(1947)) studied the logs of the ratios of hog prices at Cincinnati and 
Louisville for 2 weight classes by 12 months, by 5 days of the week, by 5 
years (1937-41). Among his summary figures are these: Months (January to 
December; 147.4, 194.3, 190.5, 208.9, 215.6, 200.9, 192.1,185.9, 146.6, 137.6, 
144.9, 162.8). Days (Monday to Friday; 175.2, 174.0, 180.0, 173.6, 183.8). 
Years (1937 to 1941; 169.5, 195.3, 220.4, 168.7, 132.6). Treating each of 
months, days, and years in turn as a basis for direct error assessment, use 
Student's t to set limits on the overall mean. On abstract principles, which 
ought to be the most appropriate choice as an error term? Explain your 
answer. After seeing the data, what changes would you make in your 
answer? Why/why not? 
7F1) (Teaser = means for solution not given in text.) Anderson's hog-price data 
(see 7E2 above) include the following averages for day of the week and year 
combinations, given here (rounded) in the form (year; Monday, Tuesday, 
Wednesday, Thursday, Friday) for one size class: (1937; 156, 168, 173, 158, 
173), (1938; 202, 192, 200, 195, 201), (1939; 207, 210, 211,214, 219), (1940; 
144, 118, 132, 131, 151), (1941; .125, 130, 131, 123, 132). Pretend, if neces- 
sary, that both years and days of the week should contribute to the variance 
assigned to the grand mean. Show the details of a calculation in which an 
estimated variance allows properly for both of these specific sources of 
variation, as well as for residual variation. Compare the corresponding 95% 
confidence intervals with those found when either one of the sources of 
variation is allowed for alone. 
Chapter 8 493 
7G1) A certain chemical analysis is known, from long historical studies, to give 
answers that vary from day to day within a week (2= 2 x 10-s), from 
week to week within a month ((r 2 = I x 10-5), and from month to month 
within a year (r 2= 3 x 10-5). Sixteen determinations were made on one 
day and gave s 2= 0.0000173. What variance should be assigned the mean 
of these sixteen as reflecting (a) a mean over a month or (b) a mean over a 
year? 
7G2) (Continues 7G1) Another sixteen determinations were made in a day and 
gave s 2 (not s) - 0.000279. What variance should be assigned the grand 
mean of the two sets of 16 (under both (a) and (b) above) if (i) the two days 
were the same, (ii) the two days were different, but in the same week, (iii) 
the two days were not in the same week, but in the same month, (iv) the 
two days were not in the same month, but in the same year7 
7G3) (Continues 7 G2) Should your answer to 7G2 depend on the target to which 
the estimate is regarded as aimed--for example, a weekly mean, a 
monthly mean, a yearly mean, a long-run mean? Illustrate your answer 
numerically for case (ii) of 7G2. 
Chapter 8 
8A1) Name two important uses of the jackknife. Why is the method called the 
jackknife? 
8A2) (a) What is Ya.? (b) What is yi)? (c) What is Y.i? (d) What is 
8A3) Examine equation (1) of Section 8A and explain how a pseudo-value can be 
regarded as analogous to a value of y. 
8A4) Let the expected value of a particular statistic based on a sample of size n 
be/ -I- (a/n). Thus the statistic is a biased estimate of/. When n = k, find 
the expected value of �,j in equation 1. (Note that E(Yi )) = p, + [a/(k - 1)].) 
Explain how this result supports the second remark of the first paragraph of 
Section 8A. 
8A5) When the statistic is the sample mean, derive �.. (That is, when Ya. = 
T, y/k, and 
8A6) We are to estimate the variance of a normal distribution with unknown 
mean. Show that the bias in the estimator y;.. = (x - :)2/n is of order 1/n. 
is the bias in y bigger than that in �a.? Show that �. is unbiased. 
8B1) The geometric mean of k measurements is defined as gk = ''�Y'='" 
Four measurements have been drawn: 1, 2, 2, 4. Use the jackknife to 
estimate the population geometric mean and to set - confidence limits on it. 
494 /Problems 
8B2) Use logs in Problem 8B1 to get confidence limits on log gk, and thence for 
gk. 
8B3) For fitting a line through the origin, an investigator has 6 points (xi, y), 
i -- 1,2,..., 6. They are 
(2, 1) (2, 2) (4, 2) 
(2,2) (3,2) (5,4) 
To estimate the slope, he uses the estimate 
Use the jackknife to set { confidence limits on the true slope. 
8B4) (Continues 8B3) As an alternative method of estimating the slope, the 
investigator considers 
ml= -F---t-'..-F-- . 
x2 Xk 
Jackknife this estimate for the data in Problem 8B3 and compare the s[ for 
the two methods. Of the two estimators, which seems preferable? 
8Cl) In Exhibit 7 of Chapter 8, check the i = 3 column. 
8C2) In Exhibit 7 of Chapter 8, why is the line "1000 (rounded z,i- 0.100)" 
introduced? 
8C3) Use the individual groups given in Exhibit 3 of Chapter 8 as a basis for 
jackknifing the 20% trimmed mean, and set confidence limits on its es- 
timand. Recall that this mean is obtained by deleting the largest 20% and 
the smallest 20% of the observations and taking the average of the remain- 
ing observations. 
8C4) What does this trimmed mean estimate? 
8C5) If we doubled the number of observations per individual but kept the 
number of individuals the same in Problem 8C3, would the expected value 
of y, be changed7 Why/why not? 
8D1) What is the purpose of the discriminant function in the Federalist example 
of Section 8D? 
8D2) How does the discriminant function here relate to an ordinary regression 
equation? Especially explain what y is. 
8D3) In the discriminant function, why doesn't it matter what the values of A and 
B are (A  0)? What would happen if A were doubled? 
8D4) Usually there is a cutoff point in discriminant problems. What is a natural 
one to use here? 
Chapter 8 495 
8D5) Sometimes the distributions associated with the two groups of observa- 
tions in a discriminant problem look like this' 
Typ [ 
Might you consider cutoffs other than midway between the means? What 
might you gain? 
8D6) If the means for groups 1 and 2 were 0 and M and the standard deviations 
cr and or2, and the distribution of the discriminants is approximately 
normal, find a formula for the cutoff point, C, that makes the number of 
errors in classification equal for the two groups. 
8D7) Usually in jackknifing we leave one observation out at a time, but in the 
Federalist problem a Hamilton paper was paired with a Madison paper 
rather arbitrarily and the pair was omitted. Why was this done? Was it 
necessary? Was it desirable? Was something lost? 
8D8) Use the rates of the word "of" (Exhibit 9 of Chapter 8) to discriminate 
between Hamilton and Madison's authorship. Essentially set up y'-- 
Ab3X3 + B to estimate 0 and 1 for Madison and Hamilton, respectively. 
8D9) (Continues 8D8) Jackknife your discrimination method, continuing through 
the equivalent of Exhibit 11 of Chapter 8. 
8D10) Use the words "of" and "and" to set up a discriminant function for the 
Hamilton and Madison problem. Essentially, carry out Exhibit 10 of Chap- 
ter 8 for these two words. 
8Dli) (Continues 8D10) Set up Exhibit 11 of Chapter 8 for "of" and "and". 
8E1) (Continues 8D9) Now cross-validate your estimates. 
8E2) (Continues 8E1) Compare the performance of "of" alone with that of the 5 
words in the text. 
8E3) (Continues 8Dll) Carry out Exhibit 12 of Chapter 8 for the "of" and "and" 
approach. 
8E4) (Continues 8E3) Compare the performance of "of" and "and" with the 5 
words and with that of "of" alone. 
8E5) From Exhibit 12 of Chapter 8 get the estimated values of the misclassifica- 
tion probability for Hamilton and that for Madison papers. 
496 /Problems 
8F1) (Project) (Continues 8D8, 8D9, 8E1, 8E2) Carry out the two simultaneous 
uses of "leave-out-one" given in Section 8F for the discriminant, using 
"of" in the Federalist example. 
Chapter 9 
Warning. These problems require heavy calculations and careful checking. It is 
wise to choose a problem (1, 2, 5, 6, 9, or 10) and plan to carry it 
throughout the chapter. Problems 9A1 and 9A2 have advantages. 
9A1) U. Behn (Annalen der Ph�sik, (4) 1:257-269 (1900)) gives the mean 
"atomic heat" for a number of elements, over three temperature ranges. 
We now give (element; for -180 � to -79 � , for -79 � to +18 � , for +18 � to 
+ 100 �) in order of decreasing atomic weight: (Pb; 6.0, 6.2, 6.4), (Pt; 5.4, 6.1, 
6.3), (Sb; 5.5, 5.8, 6.0), (Sn; 5.8, 6.1, 6.5), (Cd; 5.6, 6.0, 6.3), (Ag; 5.4, 5.9, 
6.0), (Pd; 5.2, 6.0, 6.3), (Zn; 5.2, 5.8, 6.1), (Cu; 4.5, 5.6, 6.0), (Ni; 4.3, 5.8, 6.4), 
(Fe; 4.0, 5.6, 6.3), (AI; 4.2, 5.3, 6.0), (Mg; 4.6, 5.7, 6.1). Analyze the resulting 
3 x 13 table along the lines suggested in Section 9A. Present the results in 
standard form. What can you see? 
9A2 c) P. B. Stam, R. F. Kratz, and H. J. White (Textile Research Journal, 22: 
448-465 (1952)) give the fractional diameter increase of dry human hairs 
as the relative humidity is first increased and then decreased. We give now 
(sample #; at 10%, at 40%, at 60%, at 90%, at 100%, at 90%, at 60%, at 
40%, at 10%, at 0%), where "at x%" indicates the increase in diameter (in 
thousandths of the original diameter) after stabilization at x% relative 
humidity. (#9; 25, 57, 63, 89, 142, 83, 62, 50, 20, 9), (#9A; 25, 43, 57, 84, 
115, 87, 61,--, 21, 6), (#11; 54, 72, 110, 133, 171,120, 79, 69, 42, 0), (#12; 
10, 29, 44, 80, 116, 100, 54, 38, 16, 1), (#14; 30, 75, 101,142, 164, 141,109, 
84, 40, 3). Analyze the resulting 5 x 10 table as suggested by Section 9A. 
Present your results in standard form. What can you see? 
9A3) Repeat 9A2, using the square roots of the values given there. 
9A4) Repeat 9A2, using the logs of I PLUS the values given there. (Thus, for 
example, 25 becomes log 26 - 1.41.) 
9A5 and 9A6) H. M. Brown and J. S. Graham (Textile Research Journal, 20: 
418-425) present a table of the fineness (/g/in) of cotton as a function of 
variety and % mature fibers. Selected from this comes the following, in the 
form (variety; at 65%, at 70%, at 75%, at 80%): (Half & Half; 5.10, 5.40, 
6.00, 6.81), (Deltalpine 11; 3.95, 4.25, 4.60, 5.07), (Stoneville 5; 4.35, 4.55, 
4.68, 4.78), (Stoneville 2B (8275); 3.40, 3.82, 4.22, 4.65), (Hibred; 4.33, 4.73, 
5.14, 5.60), (Qualla; 4.63, 4.88, 5.10, 5.28), (Starrex; 4.45, 4.82, 5.30, 6.05), 
(Mexican Big Bowl; 4.10, 4.32, 4.59, 4.88), (Farm Relief; 4.68, 4.95, 5.15, 
5.32), (Cleveland (Wannamaker); 4.90, 5.16, 5.30, 5.36), (Arkansas; 3.85, 
4.08, 4.34, 4.67), (Cook 912; 4.25, 4.52, 4.81, 5.10), (Rogers Acala; 3.72, 3.95, 
4.13, 4.30), (Triumph (759); 4.56, 4.85, 5.15, 5.48), (Triumph (44); 4.47, 4.83, 
5.23, 5.70). Analyze the 5 x 4 table from the first five varieties as suggested 
by Section 9A. Present your results in standard form. What can you see? 
Chapter 9/Exhibit I 497 
9A6) Analyze the full 15 x 4 table of the data of Problem 9A5 and tell what you 
see. 
9A7) Repeat 9A6 using logs of values rather than raw values. 
9A8) (Continues 9A6 and 9A7) How would you compare and relate the analysis 
of 9A6 and 9A77 
9A9) W. J. Youden (Analytical Chemistry', 19: 946-950) reports the final crucial 
digits in a precision measurement of the ratio of iodine to silver (one basis 
for atomic-weight calculations), where 2 iodine preparations were used in 
most, but not all combinations, with 5; silver preparations. We give me- 
dians of comparisons in the form (Iodine sample used; for silver A, for 
silver B, for silver C, for silver D, for silver E) in units of the 7th significant 
digit: (Iodine I; 24', 41', 29'*, 50', 55), (Iodine II; 18'*, 18', ,61, m) 
where * = median of 2 observations, ** = median of 3 observations. 
Make the best two-way analysis you can of this 2 x 5 table with 2 holes. 
Present in standard form. Comment. 
9A10) The Report of the Royal Society, IGY Antarctic Expedition to Halley Bay'. 
etc., (Sir David Brunt, ed. 1962) gives (in table 14, page 191) the mean 
monthly temperatures at various heights (more precisely at the heights 
where the air pressure had fallen to specified pressures) for balloon 
ascents made at noon Greenwich meridian time. Exhibit I for Problem 
9A10 extracts a 14 x 12 table of these temperatures. Analyze this table as 
suggested in Section 9A, and present the results in standard form. How 
successful do you think this analysis is in itself? As a basis for further 
analysis? 
Exhibit I for Problem 9A10 
Mean monthly temperatures (all negative) in tenths of a degree C (all in 1958) 
Pressure ] 
level(rob) IFI IMI IAI IMI IJ,I IJI L&_I ISl L9_I INI IDI 
30 343 423 529 687 787 870 917 885 827 651 394 326 
40 354 425 530 665 779 849 901 891 852 704 443 348 
50 368 425 517 640 778 837 888 886 850 720 464 350 
60 375 428 509 636 761 822 870 877 839 728 483 371 
80 385 431 498 610 719 789 843 851 829 731 520 392 
100 396 435 496 585 696 761 811 831 820 740 597 403 
150 412 435 472 557 644 717 768 792 789 740 601 446 
200 423 435 468 551 630 701 750 762 759 720 626 461 
250 453 456 492 584 652 666 701 719 708 677 628 540 
300 502 496 548 571 614 616 639 654 644 616 586 556 
400 430 427 463 472 509 500 521 525 512 499 465 459 
500 334 329 361 373 411 396 417 417 399 385 354 355 
700 179 186 212 227 270 254 257 272 248 225 204 209 
850 93 123 139 177 2:08 2:06 207 218 2:11 154 128 128 
498 Exhibit 1/Problems 
9B1) (Continues 9A1) Make a plot like Exhibit 2 of Chapter 9 for the results of 
analysis in Problem 9A1. 
9Bi) (Continues 9Ai, i- 2,..., 10) Do the same as Problem 9B1 for a 9A 
problem you have been assigned. 
9B(10 + i)) (Continues 9Bi, i = 1,2,..., 10) Make a plot like Exhibit 3 of Chapter 9 
and like Exhibit 4 of Chapter 9, using the residuals previously found for 
your assigned problem. What can you see? 
9Ci) (i- 1,2,..., 10) Rotate the plot of Problem 9Bi through 45 � and add 
"water-level" indications, as in Exhibit 7 of Chapter 9, for the problem 9Bi 
you were assigned. 
9C(10 + i)) (i = 1,2,..., 10) (Continues 9Ai) Make a plot like that of Exhibit 8 of 
Chapter 9 based on the residuals of the analysis of Problem 9Ai, whichever 
you were assigned. What do you see? 
9Di) (Continues 9Ai, i = 1,2,..., 10) Polish the analysis of Problem 9Ai as 
indicated in Section 9D. Use medians. How much has this analysis been 
improved? 
9D(10 + i)) (Continues 9Ai, i = 1,2,..., 10) As in 9Di, but use something other 
than medians. 
9D21) Infant mortality rates, the number of deaths per 1000 live births, are given 
in Exhibit I for Problem 9D21 for some regions of the United States, 
broken down by education of the father. Analyze these data by means of 
an additive fit, and analyze the residuals. 
9Ei) (Continues 9Ai or 9Di, i - 1,2,..., 10) Produce a plot like Exhibit 19 of 
Chapter 9 based on the analysis of Problem 9Di or 9Ai. What do you 
conclude? 
Exhibit I for Problem 9D21 
Infant mortality rates, U.S., 1964-66 
Education of father in years 
Region I l-<81 I-11t 112 13-1Sl 1>-161 
Northeast 25.3 25.3 18.2 18.3 16.3 
North Central 32.1 29.0 18.8 24.3 19.0 
South 38.8 31.0 19.3 15.7 16.8 
West 25.4 21.0 20.3 24.0 17.5 
S) SOURCE 
"Infant Mortality Rates: Socioeconomic Factors, United States," U.S. Department of HEW, NCHS, Vital 
and Health Statistics, Series 22, Number 14. 
Chapter 9/Exhibit I 499 
9E(10 + i)) (Continues 9Ei, i = 1,2,..., 10) If there appeared to be any slope in 
the plot of Problem 9Ei, calculate a further analysis as in Exhibit 20 of 
Chapter 9. 
9F1) Carry Problem 9A3 through the chapter if you haven't already done it. 
9P1) Parity Progression Ratios (Project). Parity progression ratios are defined as 
the proportion of women at a given parity level (number of previous live 
births) who eventually go on to have another birth. Bean and Wood give 
estimates of these ratios for three ethnic groups in the Southwestern United 
States in 1960 and 1970. These ratios are shown in Exhibit 1 for Problem 
9P1. 
These data can be treated in two ways. First, treat the six rows as three 
different populations and do a two-way PLUS analysis of the entire table. 
Alternatively, we can do separate analyses, one on the 1960 data and one 
on the 1970 data. How do the column (or parity) effects compare in three 
analyses? What about the row (or population) effects? Does the second 
method give sufficiently cleaner residuals to warrant the extra trouble? 
What do the residuals and diagnostic plots indicate about re- 
expression? Do they point to the same thing in each of the subtables as in 
the entire table? 
9P2) Infant Mortality Rates (Project) Exhibit I for Problem 9P2 gives infant 
mortality rates (deaths in first year/1000 live births) broken down by age of 
mother and the birth order (number of older siblings + 1) of the children. 
Which re-expressions seem helpful in fitting this data? Is there any structure 
remaining in the residuals? 
9P3) (Teaser perhaps a class project) P. B. Maek, J. A. Balog, and M. N. Jordon 
(Textile Research Journal, 22: 30-42) reported on the percentage changes 
Exhibit I for Problem 9P1 
Parity progression ratios 
I Parity I 101 L21 131 141 151 
1960 
Anglos .879 .797 .558 .484 .480 ,531 
Mexicans ,939 .920 .818 .796 .789 .774 
Blacks .748 .778 .787 .789 .774 ,759 
1970 
Anglos .929 ,882 .646 ,531 .476 .496 
Mexicans .961 .943 .839 .803 .748 .713 
Blacks .866 .877 .794 .761 .723 ,725 
S) SOURCE 
F. D. Bean and C. H. Wood, "Ethnic variations in the relationship between income and fertility," 
Demography, November 1974, 629-640. (Data reprinted by permission of the authors and of the 
journal.) 
500 Exhibit 1/Problems 
in breaking strengths of 6 fabrics, 2 types of washing, and 5 numbers of 
launderings. Their results for wet-strength changes can be given as follows, 
in tenths of a %, in the form: (fabric, type of washing; warp after 1, warp 
after 5, warp after 10, warp after 15, warp after 20; filling after 1, filling after 
5, filling after 10, filling after 15, filling after 20): (Salyna, sonic; -40, -1, 
-104,-100,-108; -54,-34, -75,-109,-128), (Salyna, hand; -71,-99, 
-129,-109,-129; -143,-110,-177,-51,-38), (Gabardine I, sonic; 158, 
106, -49, -24, 69; -36, -23, 13, 95, 10), (Gabardine I, hand; -213,-291, 
-286, -420, -379; -5, -50, -69, -104, -114), (Poplin, sonic; 17, -34, -72, 
-78, -115; -24, - 123, - 11, 55, -2), (Poplin, hand; -202, -345, -438, -523, 
-523; 65, 173, 32, 162, 144), (Gabardine II, sonic; 185, 212, 224, 166, 310; 
200, 200, 276, 187, 250), (Gabardine II, hand; -130, -78, -146, -138,-246; 
169, 127, 122, 344, 74), (Taffeta, sonic; 0, -58, 10, -48, -58; 0, 260, 150, 30, 
120), (Taffeta, hand; -11,-112,-135, -124,-180; -32,-24, 71,-48, 16), 
(Satin, sonic; 115, 135, 62, 94, 83; 78, 117, 109, 86, 78), (Satin, hand; -197, 
-193,-160,-181,-354;-190,-95,-48,-114, 152). Notice that the 
changes extend from -523 (a loss of 52.3% of the initial strength) to 344 (a 
gain of 34.4% above the initial strength). Go as far as you can with an 
analysis of this 6 x 2 x 5 x 2 array of data, display the results of your 
analysis as well as you can, and discuss their appearances. 
Exhibit I for Problem 9P2 
(Deaths in first year)/(1000 live births) 
Birth order 
 ]1 21 LJ [4] [5 I 1>6 
15-19 26.1 42.7 54.7 63.4 96.9 140.0 
20-24 17.2 21.8 27.3 35.1 45.2 58.7 
25-29 17.5 17.3 18.9 22.4 28.5 39.7 
30-34 24.1 19.3 18.2 20.2 23.6 33.5 
35-39 27.7 22.8 21.0 20.9 22.0 32.0 
40-44 33.4 31.2 26.8 24.1 25.0 35.3 
S) SOURCE 
" study of infant mortality from linked records by age of mother, total birth order, and other 
variables." United States, 1960 Live Birth Cohort, National Center for Health Statistics, Vital and Health 
Statistics, Series 20, Number 14, 1973. 
Chapter 10/Exhibit I 501 
Chapter 10 
10A1) Calculate the mean of the days of survival for the control group, with and 
without the outlier, and for the experimental group, for the data in Exhibit 
1 for Problem 10A1. 
10A2) Calculate two types of resistant summary, the median and the biweight 
(for c = 6), for both the control and the experimental groups in the 
guinea-pig study of Vitamin C. 
Exhibit 1 for Problem 10A1 (also used for Problems 10A2, 10D1, 10E1, 10E6) 
Survival times (beyond day 10) of infant guinea pigs born of dams in experimen- 
tal and control groups 
Norkus and Russo used guinea pigs to see if high Vitamin C intake by expectant 
mothers caused ascorbic-acid dependency, and eventually scurvy, in the off- 
spring. The expectant guinea pigs are divided into two groups. The experimental 
group (4 animals) is maintained on an ascorbic-acid level equivalent to 1500 mg 
for a 70-kg man, and the control group (5 animals) on one-tenth the amount. The 
8 offspring in the experimental group and the 14 in the control are given the 
same dosage as their mothers for ten days. On the eleventh day all offspring are 
restricted to essentially no Vitamin C. 
The experimenters compare the number of days of survival after day 10 for 
the two groups by means of a separate standard error for each group. The days 
of survival, in stem-and-leaf format, are: 
(Low level of (High level of 
Vitamin C) Vitamin C) 
1 1 4466 
2 1345 2 0446888 
3 133 3 012 
4 4 
54 5 
They conclude that, when this essential nutrient is removed from the diet, pups 
from dams receiving high levels of vitamin C die sooner than those on the control 
diet. 
S) SOURCE 
Norkus, E. P., and P. Russo (1975). "Changes in ascorbic acid metabolism of the offspring following high 
maternal intake of this vitamin in the pregnant guinea pig," Annals of the New York Academy of 
Sciences, 258, Second Conference on Vitamin C, 401-409. 
502 /Problems 
10A3) Clarke ("The data of geochemistry", Bulletin 3304, The U.S. Geological 
Survey, 1908, page 608) gives analyses (in %) of 8 samples of native 
platinum, as given below, after rounding, for five of the more important 
constituents. For the constituent assigned to you, find (a) the mean, (b) the 
median, (c) the midmean, and (d) the biweight. Compare the results. 
Sample 
lC�nstituentl [AI L ICI IDI Il I1 LD IHi 
Platinum 86.2 85.5 82.8 76.4 49.0 68.2 78.4 73.0 
Palladium .5 .6 3.1 1.4 .2 .3 .1 21.8 
Rhodium 1.4 1.0 .3 .3 3.3 3.1 1.7 ? 
Copper .6 1.4 .4 4.1 1.6 3.1 3.9 0 
iron 7.8 6.8 11.0 11.7 18.9 7.9 9.8 0 
10A4) Clarke also (page 523) gives analyses of seven serpentinous rocks. Again 
rounded, they are, for the more important constituents: 
SiO2 40.4 39.1 41.9 40.5 31.0 44.9 13.1 
AI203 1.9 2.1 .7 .8 1.0 5.5 1.6 
Fe203 2.8 4.3 0 4.0 4.9 1.8 1.2 
FeO 4.3 2.0 4.2 2.0 2.0 3.5 .2 
MgO 36.0 39.8 38.6 37.4 38.4 25.6 58.4 
H O (I) 10.7 12.7 14.2 13.8 20.8 5.8 24.8 
Imitate 10A3 for the constituent assigned to you. 
10A5) Clarke also (page 377) gives analyses of seven leucite rocks. Again 
rounded, they are, for the most important constituents: 
SiO= 51.9 44.4 50.2 46.5 46.0 42.6 46.1 
AI203 20.3 20.0 11.2 11.9 17.1 9.1 16.0 
Fe203 3.6 5.2 3.3 7.6 4.2 5.1 3.2 
FeO 1.2 2.8 1.8 4.4 5.4 1.1 5.6 
MgO .2 1.8 7.1 4.7 5.3 10.9 14.7 
CaO 1.6 8.5 8.0 7.4 10.5 12.4 10.6 
Na=O 8.5 6.5 1.4 2.4 2.2 .9 1.3 
K20 9.8 8.1 9.8 8.7 9.0 8.0 5.1 
H=O(I) 1.1 1.4 2.6 3.6 .4 4.2 1.4 
Imitate 10A3 for the constituent assigned to you. 
Chapter 10/Exhibit I 503 
10A6) Clarke also gives (page 317) analyses of 6 samples of augite, a mineral of 
varying composition. Again rounded, they are, for the most important 
constituents: 
Constituent] I,I 1"1 ICl I LU 
Si02 45.2 49.1 47.5 48.7 54.9 47.1 
Ti02 4.3 0 3.0 0 0 1.8 
AI203 7.7 8.0 4.1 9.3 6.3 7.8 
Fe203 3.0 0 ,5.6 3.8 2.9 1.3 
FeO 4.1 8.3 6.4 6.4 4.6 8.2 
MgO 12.2 12.4 10.0 14.7 14.5 13.5 
CaO 23.4 22.6 21.6 16.8 15.9 19.3 
Imitate 10A3 for the constituent assigned to you. 
10A7) Landolt-Bornstein's Physikalisch-Chemische Tabellen of 1923 (Volume 2, 
page 254) collects the results of 9 investigators for the specific heat of 
water at various temperatures. The results at temperatures from 5�C to 
30�C are given (at 5 � intervals) in Exhibit 1 for Problem IOA7. Most 
experimenters based their measurements on the same kind of standard 
thermometer, but three used one of two other standards, as indicated in 
the last column. Imitate 10A3 for the temperature assigned to you. Discuss 
the results in view of the additional information above about the ther- 
mometric standard used. 
10A8) The Landolt-Bornstein Tabellen also collect (Volume 2, pages 801-802) 14 
values for the Planck's constant h. As multiples of 1029 erg-seconds, 
rounded, these are: 667, 662, 656, 655, 652, 658, 654, 650, 653, 657, 656, 
653, 656, 654. Imitate 10A3 for these. 
Exhibit I for Problem 10A7 
Specific heat of water at various temperatures according to nine investigators 
Different 
I Investigat�r I I 5�c I I 10�C I I 15�c I 1 20�C I ! 25�C J I 30�C I thermometer 
Regnoult .9994 .9997 1.0000 1.0004 1.0008 1.0012 (A) 
Liidin 1.0027 1.0010 1.0000 .9994 .9993 .9996 
Dieterici 1.0050 1.0021 1.0000 .9987 .9983 .9984 
Bonsfreld 1.0039 1.0016 1.0000 .9991 .9989 .9990 
Callendor 1.0042 1.0019 1.0000 .9988 .9980 .9975 (T) 
Ronland 1.0054 1.0019 1.0000 .9979 .9972 .9969 
Bartoilis 1.0041 1.0017 1.0000 .9994 1.0000 1.0016 
Janke 1.0040 1.0016 1.0000 .9991 .9987 .9988 
Jaeger 1.0030 1.0013 1.0000 .9990 .9983 .9979 (T) 
504 Exhibit 1/Problems 
10A9) Calculate means, medians, and biweights (c = 6) for subjects F and G at 
Lausanne (Exhibit I for Problem 10A9). Find the three differences. Com- 
ment on their similarity or dissimilarity. 
10A10) Do the same for subject F at Lausanne and Zuoz (Exhibit I for Problem 
10A9). 
10All) Repeat the biweights in 10A3 using c = 9. Comment on size of effect. 
10A12) Repeat the biweights in 10A4 using c = 4. Comment on size of effect. 
10A13) Calculate means, medians, and biweights (c- 6) for the 1939 uterus 
weights at .20 and .287/rat of stilbestrol in Exhibit I for Problem 10A13. 
Find the three differences, comment on their similarity or dissimilarity. 
10A14) Do the same for the 1939 and 1940 weights at .287/rat. 
10A15) Do the same for the toodull of elasticity for trees at sites 5 and 7 in Exhibit 
I for Problem 10A15. 
Exhibit I for Problem 10A9 (also used for 10A10, 10D2, 10E3, 10E4) 
Pateliar reflexes at two elevations 
Linder** reports the results of measurements, on 8 successive days, of the 
pateilar reflex of several subjects at Lausanne (550-meter elevation) and then, 
after a day for adjustment, for 8 days at Zuoz (1750-meter altitude). Part of the 
data, expressed in logarithmic units (values are 20 times the excess of the 
common logarithms over 2) is given below. 
Subject F I I Subject G I 
I Lausannei i Zuoz I I Lausanne i l Zuoz i 
77 58 79 59 
82 63 8O 59 
78 81 65 56 
75 63 78 56 
76 76 78 56 
75 71 83 65 
74 95 84 66 
73 57 79 84 
** Linder, A. (1950), "Statistical analysis of some physiological experiments," Sankhy&, 10, 1-12. 
(Used by C. I. Bliss and D. W. Calhoun 1954, An Outline of Biometry, Yale Cooperative 
Corporation, page 100.) Data reprinted by permission of the author and of the editors of 
Sankhyi. 
Chapter 10/Exhibits I (10A13) and I (10A15) 505 
Exhibit I for Problem 10A13 (and also used for 10A14, 10D3, 10D4, 10E3, 10E4, 
10E8, 10E9) 
Uterus weights of immature rats after stilbestroi feedings (in 
1000(log wt. - 1.4)) 
Lee, Robbins and Chen*** gave varied doses of stilbestrol to immature rats and 
then weighed their uterus with, among others, the following results (expressed as 
1000 (log of wt.- 1.4)): 
I December 1939 I I April 1940 I 
I '207/rat I [.28//rat i [ .287/rat I 
-38 83 -2 
100 138 122 
122 173 130 
139 197 191 
144 197 321 
149 232 
214 251 
*** Lee, H. M., Robbins, E. B., and Chen, K. K. (1942). "The potency of stilbestrol in the immature 
female rat," Endocrinology 30: 469-473. (Quoted by C. I. Bliss and D. W. Calhoun 1954. An 
Outline of Biometry, Yale Cooperative Corporation, New Haven, at page 103.) Data reprinted 
with permission of journal and author. 
Exhibit I for Problem 10A15 (and also used for 10D5, 10D6, 10E5, 10E10) 
Red pine elasticity in trees from Connecticut plantation 
Kraemer** measured the modulus of elasticity of the outside pieces of red pine 
trees from different sites in a 25-30-year-old plantation in Connecticut. Some of 
his results were as follows (expressed as 1/10 the modulus): 
I site1 i i Site 5 J ! Site 7 i 
136 68 95 
138 114 117 
140 120 118 
143 132 124 
146 147 126 
149 150 132 
150 159 133 
157 163 146 
159 164 150 
178 152 
197 161 
** Kraemer, J. H., Dissertation, Yale University, 1943. (Quoted by C. I. Bliss and D. W. Calhoun, 
1954. An Outline of Biometry, Yale Cooperative Corporation, New Haven, at page 59.) 
506 Exhibit 1/Problems 
10A16 c) Martin* measured the length of survival of mice infected with tubercle 
bacilli. Bliss and Calhoun* combined Martin's experiments into two 
groups, A and B, with the results shown in Exhibit 1 of Problem 10A16. 
Treating 14 + as 14.5 days, etc., calculate as many as you can of mean, 
median, biweight, midmean, and trimean for each group. Make as many 
comparisons as you can. Discuss your results. 
10A17) Would you be able to calculate more of the measures of location 
specified in 10A16 if you worked with 1/(time of survival) instead of 
(time of survival)? 
10A18) Make the computations of 10A16 for -100/(days of survival). 
10B1) Define robustness of efficiency. (Refer to Section 1E as well.) 
10B2 c) One estimate is 92% as efficient as another. Suppose both have esti- 
mates of their variances that we can afford to calculate and that the 
sample size is so large that we can neglect the variability of these 
estimates can treat them as being without error. What will the ratio of 
their estimated variances be? What will the ratio of the lengths of the 
corresponding confidence intervals be? Is this enough to care about? To 
worry about? Why/why not? 
10B3/4/5) Same as 10B2 for 50%/80%/10% relative efficiency. 
10C1) For small samples from approximately normal distributions without 
stretched tails, how would you choose among the arithmetic mean, the 
median, and the biweight? 
10C2) To keep the amount of calculation down in large samples, choose 
among the 3 location estimates of Exhibit 1 of Chapter 10. 
Exhibit I for Problem 10A16 
Mouse survival (in days after inoculation) 
I Survival (in days) 
IGroup i14+i[ 15+ I 16+1[ 17+[Lls+ 119+ 120+11211[ 22+[ 
A 7 6 12 23 23 43 37 23 17 
B 1 -- -- 3 2 7 12 14 16 
Group I 23+ii 24+ i 25+ 26+11 27+i 28+1 29+1 30+ i >-31 i 
A 6 7 8 5 1 2 1 -- 3 
B 16 24 12 4 0 0 4 4 19 
* Martin, A. R. (1946). "The use of mice in the examination of drugs for chemotherapeutic activity 
against mycobacterium tuberculosis," J. Pathol. Bacteriol., 58: 500-585. (Used by C. I. Bliss and D. 
W. Calhoun, An Outline of Biometry, Yale Cooperative Corporation, New Haven, at page 62.) 
Chapter 10 507 
10C3) For good defense against stretched tails in large samples, what is the 
location estimate of choice? 
10C4 c) A city hires policemen subject to lower and upper limits on their height. 
A study is being made to compare the heights of traffic-control officers 
and detectives. Which measure of location would you recommend 
using? 
10C5 c A golf club conducts a hole-in-one tournament each year, participated in 
by a large number of experts and a smaller number of optimistic dubs. 
The resting place of every ball has been carefully mapped each year. It is 
desired to summarize each year's experience (both near-to-far and right- 
to-left) so that year-to-year variations in the apparent center of impact 
can be related to year-to-year variations in the placement of the hole on 
the green. Which measure of location would you recommend using? 
10C6) As part of a campaign to strengthen an after-11 P.M. noise ordinance in a 
middle-sized city, measurements of average noise power for 10-minute 
intervals have been made in 27 sites in and near the downtown area. 
These measurements have been made from 11 P.M. to 5 A.M. orl each of 
53 consecutive days. How would you recommend summarizing the 53 
measurements at any one ten-minute interval and place? 
10C7) (Continues 10C6.) Would your answer to 10C6 be altered if (a) you were 
advising the anti-noise committee, (b) you were advising those opposed 
to stronger noise controls, (c) you knew that a particular site was on a 
route heavily used by ambulances and police cars? How/why? 
10D1) Using the median as the resistant estimate of location for the robust 
estimate of scale, MAD, complete the following chart for the Vitamin C 
2 
data given in Exhibit I for Problem 10A1; sb, here indicates the modifica- 
tion of the Lax estimate of scale at the close of Section 10D. 
Control Experimental 
Biweight 
Median 
MAD 
10D2) Fill out a chart analogous to that of 10D1 for subjects F and G at Lausanne 
(pateliar reflex data--see Problem 10A9). 
10D3) Do the same for .20 and .287/rat in December 1939 (uterus weight 
dataMsee Problem 10A13). 
10D4) Do the same for .287/rat in December 1939 and April 1940 (same). 
10D5) Do the same for sites 5 and 7 (pine elasticity modulus data--see Problem 
10A15). 
508 / Problems 
10D6) Do the same for sites I and 5 (same). 
10D7 c) Compute as many of hinge-spread (-- interquartile difference), s 2, and 
si for the two groups of Problem 10A10 as you can. 
10D8) Would you be able to calculate more of the measures of spread specified 
in Problem 10D7 if you worked with 1/(time of survival) instead of (time 
of survival)? 
10D9) Make the computations of Problem 10D7 for -100/(days of survival). 
10E1) (Continues 10D1.) Use the results of 10D1 to compare, resistantly, the 
biweight values for the Vitamin C data from Exhibit I for Problem 10A1. 
Give a 95% confidence interval for the difference. 
10E2/3/4/5) (Continues 10D2/3/4/5.)As 10E1, for 10D2/3/4/5. 
10E6) (Continues 10E1.) Make a (nonresistant) comparison of the means in the 
Vitamin C data using Student's t to set a 95% confidence interval. 
Compare with the results of 10E1 and discuss. 
10E7/8/9/10) Continue Problem 10E2/3/4/5, as in Problem 10E6. 
10Ell) For the same constituent as in 10A3, use Student's t both (a) based on  
and s 2 and (b) based on biweight and s 2, to set 95% confidence limits on 
a typical value for that constituent. Discuss the difference in the results. 
10E12/13/14) Imitate 10Ell for the constituent assigned to you with the data of 
10A4/5/6. 
10E15/16) Imitate 10Ell, for the temperature assigned to you, with the data of 
10A7/8. 
Re lOG) For the problems on this section see Problems 14Q1 to 14Q4. 
10P1) National Assessment of Educational Progress (Project) Sometimes em- 
pirical studies can tell us what location estimates are preferable in a 
given situation. Such opportunities arise especially for large sets of data 
with repeated use of the same analysis. An example is the study of 
means, medians, and biweights for data by the National Assessment of 
Educational Progress, whose researchers set up several different popula- 
tions based on their empirical findings. They drew 70 observations from 
a population and computed the three measures of location for that 
sample. They repeated this operation 400 times for each population. We 
provide in Exhibit 1 for Problem 10P1 the stem-and-leafs for the 
biweight, the median, and the mean. The population here was that of the 
changes in percent right on 70 multiple-choice questions for 13-year- 
olds for the nation, between the first and fourth year examination in the 
subject Science. Use the stem-and-leaf plots in the exhibit to report on 
the properties of the distributions of these three measures of location 
and to help decide which measure of location seems preferable for this 
particular population. 
Chapter 10/Exhibit I (A) 509 
Exhibit I for Problem 10P1 
Stem-and-leaf plots 
A) For mean 
154 
14 
134 
12 
11 369 
10 OO2456 
9 122248 
8 0125677889 
7 01124447 
6 133567777 
5 001122344445678 
4 000112444556667899 
3 011123?,?.55666778999 
2 00000111123445555567777888899 
1 001222333334667777788999 
0 0122223333444555666667777778888999 
-0 8883887777665544443333222222211 
-1 999987776655544433332222211111000 
-2 9999998877766555'!....322222111000 
-3 999998766666555.'!.'!..333332211100 
-4 88777665544333221100 
-5 9988877666555433321100 
-6 88764332100000 
-7 954321110 
-8 85221000 
-9 9863 
-10 9641 
-11 72 
-12 82 
-13 
-14 
510 Exhibit I (B) / Problems 
Exhibit I for Problem 10P1 (continued) 
B) For median 
21 6 
20 379 
19 2278 
18 
17 
164 
15 
14 
13 000 
12 9999 
11 11111111666666677778 
10 5555 
9 000 
8 333333338999 
7 
6 0000000000000000000000000111111111111111111111111 
5 
4 77777777777777777777 
3 00111111111144......,!.,!. 
2 
1 777778888888 
0 000000000000000001111111111111117 
-0 666111111111111 
-1 
-2 '!.'! .  . .'!. 444444444333 
-3 977777666 
-41666666666666666554 
-5 
-6 9777722222200000000000000 
-7 777777777 555555555553333333333333333 
-8 8888888888885554333333333333111111 
-9 444333322222111100000000000 
-10 9831 
-11 911111 
-12 88 
-13 0 
-14',954 
-15 000 
-16 1 
-17 
-18 31 
Chapter 10/Exhibit 1 (C) 511 
Exhibit I for Problem 10P1 (continued) 
C) For biweight 
16 2 
159 
14 
13 
12 12 
11 013499 
10 002444 
9 011367 
8 0223455568 
7 001123466 
6 023333444449 
5 00112223334455567778 
4 00011223456678999 
3 012233333556677889 
2 12222334467889 
1 00001123344445555566667788889999 
0 0000123333444455556667888888999 
-0 9998887777766555444432222111111 
- 1 999777776666666555543332222221111110000 
-2 98877777766555533332222 
-3 999988888876665555544332221 O0 
-4 999887776665544333321110 
-5 99888654444332211000 
-6 9999876655533331111000 
-7 876555310 
-8 87520 
-9 86200 
-10 520 
-11 54 
-12 80 
-13 
-14 4 
s) SOURCE 
Robert Larson and Don SearIs, National Assessment of Educational Progress, personal 
communication. 
512 /Problems 
Chapter 11 
11-1) When should standardizing for comparison be used? 
11-2) How do randomized trials provide controls? 
11A1) What are the distinctions between "crude" and "adjusted" rates? 
11A2) What is the standard population (in numbers) for a 50-50 mixture of Easy 
and Hard in Example I of Section 11A? 
11A3) Show the calculations for the percentage difference (11- I) with a 50-50 
mixture of Easy and Hard. 
11A4) What is the standard population (in numbers) for 45% Easy and 55% 
Hard in Example I of Section 11A? 
11A5) Let the probabilities of success for Treatment I be: p in the Easy stratum 
and P2 in the Hard stratum. What would be the crude success rate when 
Treatment I is applied to a population that is composed of a mixture with 
proportions t of Easy and 1 - t of Hard? 
11A6) Let the probabilities of success Pi be as follows 
I Treatments I 
Easy P P2 
Hard P2 P2 
with p > P12 and P2 < P22 What mixtures of Easy and Hard in the 
population will lead Treatment I to be preferred to Treatment I17 
11A7) Using Data Exhibit 2 for Problems (at the end of the problems) on 
smoking and health, compute the crude death rates for the nonsmokers 
and the three smoking groups. 
11A8) (Continues 11A7) Consider the nonsmoking population as standard and 
compute standardized death rates Rno, for nonsmokers, Rop for cigar and 
pipe smokers only, Re for cigarette smokers only. 
11A9) Compare the results of 11A7 and 11A8. 
11^10) Consider the cigarette smokers and nonsmokers in Data Exhibit 2 for 
Problems. Use ages 0--59 and ages 60 + as two strata. Calculate the 
difference, smokers minus nonsmokers, for a 50--50 mixture of the two 
strata. 
11A11) What is direct standardization? 
11A12) Assuming no correlation between results for Easy and Hard strata, find 
Var (Pst, - Pstd..) when the standard population is as in Question 2 in the 
text. 
Chapter 1 1 513 
11B1) In Section 11A, what are the W's for the directly standardized rates in 
Questions 1-3 in the text? 
11B2) Explain why the greater age of the population in Maine caused its crude 
death rate to be higher, even though its specific rates in each age class 
are lower than in South Carolina in Exhibit 2 of Chapter 11. 
11B3) In Exhibit 1 of Chapter 11, the crude difference in success rate (Treatment 
I - Treatment il = 60% - 44%) is different from what it would be if the 
comparability of the groups were taken into account. In Exhibit 2 of 
Chapter 11, the crude rates also produce misleading results. Explain how 
the difficulties are overcome in each example. 
11B4) Use the following numbers in each age group for a standard million in 
the U.S. in 1975 to adjust the death rates in Exhibit 3 of Chapter 11 for 
Maine and South Carolina by the direct method. 
Age Standard million Age Standard million 
(in years) for U.S. in 1975 (in years) for U.S. in 1975 
0-4 74,400 25- 34 144,600 
5-9 81,000 35-44 107,200 
1 0-14 95,500 45-54 111,400 
1 5-19 98,700 55-64 92,200 
20-24 90,300 65-74 64,600 
75+ 40,200 
11B5) (Continues 11B4.) Compare the results of 11B4 for a 1975 standard 
million to those for a 1940 standard million in Exhibit 3 of Chapter 11. 
11B6) In Exhibit I for Problem 11B6, Panel B gives the distribution of word 
lengths for texts of 15 to 18 thousand words from Hamilton and Madi- 
son. Panel A gives the frequency (occurrences per 1,000 total words) for 
one particular word of each length. Combining these, we can find, for 
each word length, the chance that either author will use the particular 
word of that length. Panel C gives a standard distribution of word lengths 
based on Melville's Mob)/Dick. Use it to calculate the chance that any of 
the three authors will use one of the particular words, standardized for 
the distribution of word length. 
(Exhibit 1 for Problem 11B6 appears on the next page.) 
514 Exhibit 1/Problems 
Exhibit I for Problem 11B6 
Word lengths and word frequency distributions 
A) Frequency distributions of words of 1-12 letters 
I Rates per 1000 words* 
I Word of length k i I Hamilton [ Madison 
k-- 1 (a) 22.85 20.22 
2 (of) 64.85 57.80 
3 (our) 2.27 1.11 
4 (what) 1.38 1.15 
5 (among) .39 .84 
6 (second) .18 .37 
7 (whether) .49 .97 
8 (language) .04 .21 
9 (direction) .22 .03 
10 (throughout) .04 .17 
11 (destruction) .13 .01 
12 (consequently) .03 .48 
B) Distribution of word lengths C) Distribution of word lengths 
Standard thousand 
I Word length k J ! Ramilt�nl ! Madis�n I k I from Moby Dick** 
1 423.2 396.1 1 45.0 
2 3531.6 3834.7 2 162.0 
3 2925.0 3644.7 3 228.1 
4 2042.4 2204.9 4 196.5 
5 1580.0 1720.2 5 122.3 
6 1116.1 1396.4 6 79.5 
7 1026.7 1298.7 7 66.0 
8 824.5 1027.4 8 45.3 
9 805.7 888.1 9 27.9 
10 617.6 743.4 10 15.1 
11 396.6 450.4 11 8.1 
--> 12 385.6 483.0 >_ 12 4.3 
15,675.0 18,088.0 
* Source: Mosteller, F., and David L. Wallace (1964). Inference and Disputed Authorship: The 
Federalist. Addison-Wesley: Reading, MA, pp. 244-248, 258, 260. Reprinted by permission of 
publisher and author. 
** Nowlin, A. G. (1973}. "Statistical analysis of linguistic word-frequency distributions and 
word-length sequences," Ph.D. thesis, Princeton University. Reprinted by permission of author. 
Chapter 11/Exhibit I 515 
11B7) In Exhibit I for Problem 11B7 the number of deaths after operations from 
two areas in the country are displayed by age and by sex. For each of the 
two areas compare the directly standardized death rates for males and 
females, taking the average male-female population as standard. 
Exhibit I for Problem 11B7 
Surgical deaths 
The following counts come from two areas in the United States and refer to a 
5-year period. The population is all patients who underwent surgery, and the 
deaths include all patients who died in the hospital following surgery. 
I Areal J 
Population I I I 
lAgel I Males I I Females I [ Males J I Females J 
0-4 2104 1952 34 22 
5-14 4272 3911 9 11 
1 5-24 2835 2989 23 5 
25-34 2785 2606 19 8 
35-. . 1930 1886 16 15 
45-54 1497 1524 59 40 
55-64 960 1013 101 52 
65-75 652 855 185 118 
76-83 186 287 97 108 
84 + 69 125 68 103 
Area II I 
I Population I I Deaths I 
lAgel I Males I Females I I Males I ! Females { 
0-4 703 689 12 3 
5-14 1739 1758 5 2 
1 5-24 1233 1244 14 1 
25-34 989 1004 8 3 
35-44 897 922 9 13 
45-54 921 961 28 15 
55-64 686 739 68 37 
65-75 611 784 159 73 
76-83 189 290 86 88 
84 + 52 124 70 119 
516 /Problems 
11B8) For Exhibit 1 for Problem 11B7, compare the directly standardized death 
rates in Areas ! and I! with a 50-50 mixture of the two populations taken as 
standard (males and females pooled together in each area). 
11C1) Using the standardized binomial, calculate the variance for the standar- 
dized difference in word rates from 11B6. 
11C2) Use the stratified binomial to improve the calculation of the variance from 
11C1. 
11C3) Compute, using the improved stratified binomial, the standard error of the 
differences: Rnon - Rcp, Fnon - Fc for the smoking and health data of 11A8. 
Based on these standard errors, what can be said about the different 
rates? 
11C4) Are the crude or standardized or stratified binomials appropriate for 
calculating the standard error? 
11D1) In view of Exhibit 4 of Chapter 11, explain why it is important to review 
any standardizing calculation. What are the potential difficulties? 
11D2) A computer program calculates standardized rates. What displays would 
you like the program to provide to warn you of a wild result? 
11D3) In Exhibit 1 for Problem 11B7 on surgical deaths, the estimated death rate 
for females ages 5-24 is almost zero. What sort of problems does this 
impose? 
11E1) How does indirect standardization differ from direct standardization? 
11E2) in Example 5 of Section 11E, why are the reference percent success rates 
not to be compared with each other but, rather, with their crude success 
rates? 
11E3) Do Treatments I and II perform differently in Example 2 of Section 11A for 
indirect standardization? 
11E4) Given the weighting of treatment totals of Example 6 in Section 11E, are 
Treatments land II identical? 
11E5) In the calculation of the rough approximation to the variance, when would 
the standardized number nst d not be considered known? 
11E6) Based on the data presented in Exhibit 2 of Chapter 11, compute the 
indirectly standardized death rates for Maine and South Carolina. 
11E7) For Data Exhibit 2 for Problems, compute the indirectly standardized 
lung-cancer ratios for smokers and nonsmokers. 
11E8) Describe two awkwardnesses with the indirect approach. 
11F1) Why is adjustment necessary7 
11F2) In general, what standard difficulties might be selected to adjust for 
comparison of the Easy and of the Hard groups7 
11F3) Why is the logistic used as the distribution of standard difficulty7 
Chapter 11 / Exhibit I 517 
11F4) With the standard population as 55% Easy and 45% Hard, what is the 
difference (Treatment II- Treatment I) for the adjusted groups? 
11F5) Does the adjustment work for both directly and indirectly standardized 
populations? 
11F6) Consider the cigarette smokers and nonsmokers in Data Exhibit 2 for 
Problems. Calculate the centers of gravity for each for the two strata ages 
0-59 and ages 60 + . 
11F7) Calculate the average difference, smokers minus nonsmokers, for 11F6. 
1 1 F8) Compare the results of 1 1 F7 with 1 1A10. 
11F9) Using a mixture of 45% ages 0-59 and 55% ages 60 + as the standard, 
find the adjusted difference (smokers minus nonsmokers). 
11F10) What advantage is there to calculating percentages of success at the 
cutting point? 
1 1 P1) Standardized Fertility Indices (Project) Age distribution of women in the 
childbearing ages and age-specific fertility rates for 3 modern European 
countries are given in Exhibit 1 for Problem 11P1. Pooling the data to get 
distributions and rates, obtain the directly and indirectly standardized 
fertility rates. Demographers frequently standardize fertility rates indi- 
rectly on the fertility of the Hutterites, a religious sect in the western 
United States and Canada known for their exceptionally high fertility. 
Compare the three countries based on these rates, given below. By 
inspection, it seems clear that fertility is highest in France and lowest in 
the U.K. Do the three methods of standardization bear this out? Based on 
each method, separately and jointly, would you say that Norway is closer 
to France or the U.K. in its fertility experience? 
Exhibit I for Problem 11P1 
Age distributions and fertility rates 
Women Women Women 
I Age I (in 1000's) I Rate (in 1000's) IRatel (in 1000's) 1RateJ [Rate 
119 2070. .0255 150.8 .03 1696. .0432 .300 
224 1851. .1580 145,7 .1500 1707. ,1309 ,550 
229 1382. .1636 150.9 .1373 1799. .1354 .502 
3 1514. .0985 111.8 .0732 13. .0635 .7 
339 1646. .79 96.5 .0309 1387. .0246 .406 
4 1657. .0146 101.2 .0073 1424. .0061 .222 
449 1561. .0013 113.0 .0004 1497. .0004 .061 
S) SOURCE 
U.N. Demographic Yearbooks, 1972 and 1974; for the Hutterite data, A. J. Coale, "Factors associated 
with the development of low fertility" in U.N. World Population Conference, 1965, Vol. II, U.N.: New 
York, 1967. 
518 Exhibit 1/Problems 
Why do demographers like to use a certain fixed standard with which 
to compare all populations? Why should they choose the Hutterites, who 
have the highest reliably recorded fertility known? 
Chapter 12 
12-1) Compare and distinguish the concepts of asociation, independence, and 
causation. 
12-2) What are 3 sorts of ideas necessary to support the notion of cause? 
12-3) Contrast the mathematical and statistical meanings of "dependence". 
12-4) Exhibit I for Problem 12-4 gives the mean annual level of Lake Victoria 
Nyanza (x) relative to a fixed standard, and the number of sunspots (�) for 
the years 1902-1921. Plot � vs. x, and do a simple regression of � on x. 
Suggest a mechanism that might explain the association of x and �. 
12A1) What is the first meaning of regression? 
12A2) Describe some situations where one would prefer summaries other than 
the mean. 
Exhibit I for Problem 12-4 
Mean annual level of Lake Victoria Nyanza and number of sunspots, 1902-1921. 
1902 -10 5 1912 -11 4 
1903 13 24 1913 -3 1 
1904 18 42 1914 -2 10 
1905 15 63 1915 4 47 
1906 29 54 1916 15 57 
1907 21 62 1917 35 104 
1908 10 49 1918 27 81 
1909 8 44 1919 8 64 
1910 1 19 1920 3 38 
1911 -7 6 1921 -5 25 
S) SOURCE 
Sir Napier Shaw, ManualofMeteorology, Vol. 1- Meteorology in history; Cambridge University Press, 
London 1942, p. 284. Data reprinted by permission of publisher. 
Chapter 12/Exhibit I 519 
12A3) What is the second meaning of regression? 
12A4) What are some advantages and disadvantages of each type of regression? 
12A5) Plot length of stay, �, on physical status, x, for the herniorrhaphy data 
(Data Exhibit 8 for Problems at the end of the book). 
a) Compute the mean of � at each physical status, and plot these 
points on the graph. In what sense is this a regression? How good a 
summary do you think it provides? 
b) Repeat part (a) for the median of y at each physical status, and 
compare. How might the addition of other percentage points improve the 
regression as a summary? 
12A6) Draw a straight line through the points in the graph from the previous 
problem. What is an equation for your line? Interpret the equation. What 
advantages and disadvantages does this kind of summary have compared 
with those of 12A57 
12A7) Exhibit I for Problem 12A7 gives the number of icebergs sighted south of 
Newfoundland (x) and south of the Grand Banks (y) for each month of 
1928. 
a) Plot � on x, and draw a line through the points. What is an equation 
for your line? 
b) Compute the residuals and plot them on x. What can you say about 
the errors you expect in predictions of � as a function of x? 
c) How do the ideas of association, dependence, and causation apply 
to this problem? 
12B1) Describe five different uses for regression. 
12B2) Why don't we care about regression coefficients when fitting for exclu- 
sion? What do we care about? 
Exhibit I for Problem 12A7 
Numbers of icebergs sighted monthly south of Newfoundland, x, and south of 
the Grand Banks, y, for 1920. 
[Monthl JL_J IFI IMI IAI IMI IJI IAI Isl IOl INI 
x 3 10 36 83 130 68 25 13 9 4 3 2 
y' 0 I 4 9 18 13 3 2 I 0 0 0 
S) SOURCE 
Sir Napier Shaw, Manual of Meteorology, Vol. 2, p. 407, Cambridge University Press, London, 1942. Data 
reprinted by permission of publisher. 
520 Exhibit 1/Problems 
12B3) Why is it important to remember that many different sets of variables 
often predict about equally well? 
12B4) A regression in the sense of local averages appears in Exhibit I for 
Problem 12B4 for eventual number of children anticipated by women 
interviewed in national samples taken in 1955, 1960, and 1965. Do 
women's expectations appear to change from 1950 to 19657 Regress, for 
the 14 points, the anticipated number of children on year of interview 
(coded: -1, 0, 1; no constant term) and compute anticipated numbers of 
children with this trend subtracted. 
12B5) Pint length of stay (y) on age in years (x) for the herniorrhaphy data (Data 
Exhibit 8 for Problems). 
a) Draw a line through the points and interpret its equation. Compute 
lengths of stay adjusted for age. 
b) Do you think it wise or unwise to use a single regression formula to 
adjust for age when the range is from 2 to 80 years, as here? 
12B6) Pint appropriations for research to the Air Force (y) against year (x) for 
Armed Forces Research data (Data Exhibit 10 for Problems). 
a) Draw a line through the points and use its equation to get appropri- 
ations "adjusted for linear trend." 
b) Pint "adjusted y" by year. Have you removed differences over 
time? What kind of function might work better? 
12C1) What does x2; mean? 
Exhibit I for Problem 12B4 
Anticipated eventual number of children 
l Year of interview 
!.Age at I 
.nterview [ 1955 [ 11960 [ 11965 
20-24 3.20 3.07 3.26 
25-29 3.32 3.46 3.53 
30-34 3.32 3.49 3.58 
35-39 3.16 3.35 3.59 
40-44 -- 3.46 3.54 
S) SOURCE 
N. B. Ryder and C.-F. Westoff, Reproduction in the Uniteel States 1965, Princeton University Press, 
Princeton, N.J., 1971, p. 42. 
Chapter 12 521 
12C2) What, in words and formula, is 
12C3) What is x2; 17 
12C4) Exactly what are Y;2 and 
12C5) Fitting x2 to y; is algebraically equivalent to fitting x2; to y;. What is the 
advantage of the latter method? 
12C6) We learned in Section 12B that very different sets of variables can often 
produce nearly the same fit. Explain why we often find that very different 
sets of coefficients for the same variables can produce nearly the same fit 
as well. 
12C7) Use the graphical method of this section to fit age (first) (x) and physical 
status (second) (x2) to length of stay (y) for the herniorrhaphy data (Data 
Exhibit 8 for Problems). (If you have done Problem 12B5, the first step has 
already been done.) Compare your equations 
Y = bo + blX 
and 
y --- Co - ClX 1 -- C2X 2. 
12C8) Regress savings on income and interest rate (Moody's Aaa long-term 
bonds), using the Economics data set (Data Exhibit 4 for Problems). Save 
your work for later problems. 
12C9) a) Which single variable estimates pupil's verbal performance best in the 
Coleman data (Data Exhibit 7 for Problems)? 
b) What second variable seems to do the best job together with your 
choice in (a)? Does it seem worth having? 
12D1) If we wish to adjust y for z and have several measures of z, why may it not 
help to use all of them in a regression? (More on this topic in Sections 12F 
and 13G and Chapter 15 including the possible use of composites with 
coefficients selected by personal judgment.) 
12D2) How might two clearly disparate carriers come to be collinear? (See, for 
example, Problem 12-4.) Thus, collinearity often cannot be predicted from 
the nature of the variables. 
12D3) A "least-squares" fit of y on x, x2, x3, x4, x 5 for the Coleman data (Data 
Exhibit 7 for Problems) yields the formula 
y = 19.9- 1.79x + .0432x2 + .556x3 + 1.11x4- 1.79xs. 
a) What is apparently surprising about some of the coefficients? Why is 
this not unexpected for this stock? 
b) Which variables seem to be collinear? Why/why not? Replace the set of 
collinear variables with one carrier, and refit, using the reduced stock. 
Discuss how your residuals compare with residuals from the fit with the 
complete stock. 
522 /Problems 
12D4) For the Economics data (Data Exhibit 4 for Problems), let 
y = savings, 
x = income, 
x2 = long-term interest rate (Moody's Aaa), 
x3 = long-term interest rate (Moody's Bbb), 
x4 = short-term interest rate. 
Compare the regressions: 
a) y = bo + bx + b2x2; 
b) y = bo + bx + b2x2 -F b3x3; 
c) y = bo + bx + b2x2 -F b3x 3 -F b4x4. 
12D5) Economists have known for a long time about the empirical relation which 
we approximate as: 
(Inflation) = a + b(unemployment rate). 
This is known as the Phillips Curve. For the Economics data (Data Exhibit 4 
for Problems), let 
y = inflation if Pt = consumer price index in year t, let � = t- ' 
x = overall unemployment, 
x2 = unemployment for men over 20 years old, 
x3 = unemployment for women over 20 years old. 
a) For the carrier xi and pair of carriers (xi, x i) assigned to you, fit the 
regressions 
y = bo + bi x 
and 
y = co + ci xi + ci x i . 
b) Compare the coefficients and the fits. 
12D6) Compare your results in 12D5 with a fit of the form 
y = do + dx + d2x2 -F d3x3, 
12D7) Drops in barometric pressure are associated with bad weather, In Data 
Exhibit 6 for Problems, let 
y = (station pressure)- (yesterday's station pressure), 
x = relative humidity, 
x2 = fog, 
xe = precipitation. 
Chapter 12 523 
a) For the carrier xi and pair of carriers (xi, x i) assigned to you, fit the 
regressions 
Y = bo + bx 
and 
y = Co + cx + cx. 
b) Compare the coefficients and the fits. 
12D8) Compare your results in Problem 12D7 with a fit of the form 
y = do + dx + d2x, + d3x:. 
12D9) Which of the variables in the ancient warfare data (Data Exhibit 12 for 
Problems) seem to be collinear? Outline the steps you might follow to 
construct a regression model to estimate the number of months at war. 
12E1) Why can two polynomial equations with very different coefficients often 
fit the same data well7 What does this imply about extrapolating beyond 
the range of the data? 
12E2) What happens when a multiple regression is fit to data with collinear 
carriers? What needs to be done? 
12E3) Consider women's anticipated parity from Problem 12B4. Using the 
parities adjusted for interview year, what can be said about (a) whether 
women in different births cohorts (1916-20, 1921-25, ..., 1941-45) have 
different expectations, and (b) whether there is a trend in expectations by 
age after date of birth is removed7 
12E4') Compute the correlation between X and X 2 if X is uniformly distributed 
between 0 and A. 
12E5) Why may it be advantageous to fit 
a* + b*x + c(x- Xo) 2 
instead of 
a + bx + CX2, 
even though these are mathematically equivalent? 
12E6) Is Problem 13C4, predicting temperatures from date and yesterday's temp- 
erature, an example of complete dependence? 
12F1) What do we need in order to control for the effect of a variable that is not 
directly measurable? 
12F2) What assumptions do we need to make to do this7 
12F3) What is an instrumental variable? 
12F4) How does the variability of y' - (bv,/b<,)x compare with that of y - bvxX? 
Why is this reasonable? 
524 / Problems 
12F5) (Continues 12C8) Suppose we now wish to control for the effect of 
inflation. The Phillips Curve (inflation = bo + b(unemployment rate)) 
means that the unemployment rate is a useful measure of inflation. For a 
more direct measure, we can use the consumer price index (CPI): if the CPI 
in year t is Pt, let our measure of inflation be i = (Pt - P-)/Pt-1. Use i as 
an instrumental variable to correct the regression of Problem 12C8 for 
"true" inflation. How do the coefficients for income and interest rates 
change? 
12F6) (Continues 12F5) Which of the assumptions of this method are likely to be 
wrong? 
12F7) (Continues 12F5) Regress the residual savings from Problem 12C8 on i 
adjusted for income and interest rate. How do your results compare with 
those for Problem 12F57 
12F8) (Continues 12D4). A common model used in economics is 
Savings - bo + b(income) + b2 (interest rate) + b3 (last year's inflation). 
To adjust the regression in Problem 12D4 for last year's inflation, we can 
use, as an instrumental variable, the inflation in the consumer price index; 
the inflation It for year t is 
It = (CPIt - CPI_)/CPIt_. 
The Phillips Curve 
Inflation = Co + c(unemployment rate) 
suggests that unemployment be used as a second measure of inflation. 
a) Adjust the regression for last year's "true" inflation. Discuss the 
validity of the assumptions in this method. 
b) Adjust the regression for It_ only. How do your results compare? 
12F9) In what ways do the data for the United States 1955-74 support or 
contradict the economic models outlined in Problem 12F87 
12G1) When x and y may be rescaled in different ways, all producing a more or 
less linear fit, and when reality doesn't care which we choose, how can we 
decide upon a scale? 
12G2) Why do we use these rules when the physical constraints of the problem 
don't force a choice? 
12G3 Which rule takes precedence and why? 
12G4) Noyes and co-workers (Journ. chim. phys. 6:505 (1908) and Zeits. phys. 
Chem. 70:350 (1910))gave equivalent conductivities at various tempera- 
tures for solutions of several substances of differing strength, as at the top 
of page 525. 
Chapter 12/Exhibit I 525 
I Concen- at at at at at at at ((o at 
t ratio n 18�C 100�C 306�C 18�C 100�C 306�C 18�C 1 C 06�C 
0.0005 128.1 1044 107.5 355 1003 375 835 374.0 
.002 126.3 393 1008 105.4 349 955 373.6 826 371.2 
� 01 122.4 377 910 102.0 335.5 860 368.1 807 365.0 
� 08 113.5 341.5 720 93.5 301 680 353.0 762 353.7 
.1 112.0 336 92.0 290 350.6 754 
a) For the substance and temperature assigned to you, fit both 
Equivalent conductivity = a* + b*(concentration) 
and 
Equivalent conductivity = a** + b**/concentration. 
Which gives the better fit? 
b) What would you estimate the equivalent conductivity to be for a very 
small concentration? 
c) What limits can Student's t give you for that answer? 
12G5) Klemenc and Remi (Mon. Chem. 44:307 (1924)) gave viscosities for 
various mixtures of (a) hydrogen and propane and (b) hydrogen and NO. 
Find a reasonable fit, in some form, for the case assigned to you. For (a) 
the (% propane, viscosity) pairs are, rounded: (0, 86), (3.1, 89), (7.8, 94), 
(8.9, 95), (15, 97), (22.2, 96), (32.7, 92), (51.8, 87), (69.8, 81), (80.4, 77), (100, 
75), For (b) the (% NO, viscosity) pairs are (0, 85), (19.8, 142), (23.0, 145), 
(28.4, 147), (45.1, 160), (70.4, 172), (85.0, 175), (100, 180). 
12G6) Exhibit 1 for Problem 12G6 gives the length x, width of square cross 
section x2, and volume y of 5 bricks. The relation y' = xx 2 holds exactly. 
How well does the regression 
y = bo + bXl + b2x2 
fit? What properties of this data set make a good fit possible? 
Exhibit 1 for Problem 12G6 
Dimensions of bricks 
10 5 250 
10 6 360 
10 4 160 
11 5 275 
9 5 225 
526 / Problems 
12H1) What are the advantages for beginning analysis of a large body of data 
using subsamples? 
12H2) Which of these advantages remains even when we have a high speed 
computer to do all the arithmetic? 
Chapter 13 
13A1) What does the identity 
117 - 3x + 2x 2 = 109 + 5x + 2(x- 2) 2 
tell us about interpreting the regression coefficient of x? 
13A2) Suppose we fit the regression 
(Inflation) = bo + b(unemployment rate) 
+ b2(last year's inflation) + b3(interest rate). 
What is wrong with the statement "the effect of interest rates on inflation 
is b3"? How should b3 be described? 
13A3) What is a stock? 
13A4) What are the major factors influencing b in the fit y = bo + bx + b2x2 + 
b3x37 
13A�) Suppose that a chemist wishes to compare the effects of chlorine, 
bromine, and iodine (halogens) on the boiling points of some alkyl halides. 
Any alkyl group (e.g., CH) can combine with any halogen (here Cl, Br, or 
I) to make an alkyl halide (CH,Cl, CH.Br, or C,H,I). We want to study how 
boiling points vary. The boiling points (in �C) appear in Exhibit 1 for 
Problem 13A, with the molecular weights of the alkyl groups. Let 
� = boiling point; x = molecular weight of alk�1 group; 
'1 chlorine halogen 
x2 = ,0 no chlorine halogen; 
and let x and x be similarly defined for bromine and iodine. 
a) Graphically or otherwise, fit the regression 
y -- blx1 Jr- b2x 2 Jr- b3x 3 -F b4x4. 
(A single plot of y on x will yield good enough estimates for all parame- 
ters.) 
b) What are "the effects of the halogens"? 
13A6) A second chemist may prefer to adjust for the molecular weight of the 
entire alkyl halide. So set x5 = molecular weights of alkyl halide. Since the 
atomic weights of CI, Br, and I are, respectively, 35.5, 80, 127, we have 
x = x + 35.5x2 + 80x + 127x4. 
a) Rewrite your fit as 
y - C5X 5 ' C2X 2 -+- C3X 3 -- C4X 4, 
b) What are "the effects of the halogens"? 
Chapter 13/Exhibit I 527 
13A7) This question uses the Municipal Bond Data (Data Exhibit 9 for Problems). 
a) Fit the regression y = bo + b2x..,. 
b) Fit the regression y -- Co + ClXl + c..,x2, and compare your coefficients 
with those in (a). 
c) Fit the regression y = do + dx. Do the results agree with your intui- 
tion based on the results from (a) and (b)? 
13A8) In Data Exhibit 5 for Problems, let 
y = E (educational expenditure per public-school pupil), 
Xl -- SBG (size of Massachusetts state block grant), 
x2 - W (taxable property value per public-school pupil). 
a) Fit the regressions y = bo + bXl and y = Co + c2x2. 
b) Fit y = do + dx + d2x2 and compare with the fits of (a). 
13B1) What do we mean by x;25 and by y; and by Y.2 and by x2.? 
13B2) Suppose we want to regress y on Xl and x2 by stages. As a first step, x is 
fitted to y, leaving y. Which of the following approaches can be per- 
suaded to produce a 2-carrier fit of x and x2 to y7 Of the approaches that 
will, which is preferable7 Why? 
a) Fit x to x2, leaving X2; 1. Fit x2; 1 to Y;1. 
b) Fit x2 to y. 
c) Fit Xl to x2, leaving x;. Fit x2; to y. 
d) Fit x to Y;1. 
13B3) a) How should the coefficients in Problem 12C7 be understood? 
b) How should the coefficients in Problem 12C8 be understood? 
Exhibit I for Problem 13A5 
Molecular weights of alkyl groups a.d boiling points of halogens 
Alkyl Molecular I, Halogen 
group weight I CI II Br II I I 
C2H 29 12.5 38 72 
n-C3H7 43 47 71 102 
n-C4H9 57 78.5 102 130 
n-CsH 71 108 130 157 
n-C6H 3 85 134 156 180 
n-C H  99 160 180 204 
n-CsH7 113 185 202 225.5 
13B4) In the herniorrhaphy data (Data Exhibit 8 for Problems), let 
y = length of stay, 
X 1 -- age, 
x2 - physical status. 
a) Using least-squares at each stage, follow the graphical method to fit 
� = bo + bXl + b,x2, fitting Xl before x2. 
b) Compare the least-squares estimates of bo, b, b2 with those found 
graphically in Problem 12C7. Compare the fits. 
13C1) We have seen the effect of mild disturbances (through rounding) on 
regression fits, where every point has a chance to be disturbed about the 
same amount. What is likely to happen if an important variable is left out? 
If the model is wrong? 
13C2) In Example 5 of Chapter 13, we found that there is a "best" least-squares 
fit, but that the regression coefficients are indeterminate. How can this 
be? What does this imply about using fits versus interpreting regression 
coefficients? 
13C3) Note that there is no constant term in the regressions of Problem 13A5. If 
Xo -- 1 were included for a constant term, what exact collinearity among 
x0, Xl, x2, x3, and x4 has been introduced? 
13C4) Suppose we want to predict temperatures for April 2-30 in the weather 
data (Data Exhibit 6 for Problems). One way to start is to take out a trend 
by date, since winter is ending, and to use yesterday's temperature as a 
carrier, since warm spells and cold spells tend to last several days. 
a) Use a stepwise regression to do this, taking out the date effect first. 
What has happened? 
b) Does the problem disappear if we try to predict temperatures only on 
rainy days7 Why/why not? 
13C5) Discuss how the exact dependencies found in Problems 13C2 and 13C3 
affect interpretation of the effects we were trying to measure. 
13C6) Exhibit 1 for Problem 13C6 gives values of y = -1 + x -!- 0.5x 3, rounded 
to 2 decimals, for the x's in Exhibit 1 of Section 13C. Fit a regression 
y = bo + bx + b2 X2 and discuss the results. 
13C7) Repeat Problem 13C6 after rounding y to 1 decimal place. 
13C8) Fit a regression y = bo + blX '-F b2 X2 -F b3x 3 to the data in Problem 13C6 
and discuss the results. 
13C9) Repeat Problem 13C8 after rounding y to 1 decimal place. 
13C10) a) Find the least-squares estimates for the regression 
Y = bo + bx + b2x2 
using the data of Exhibit 3 of Chapter 12. 
Chapter 13/Exhibit I 529 
b) Add "rounding errors" of �.2 to x, �500 to x2, and �100 to �, using a 
random-number table, and compare the coefficients of 
y* bo + + 
c) How has the fit changed? 
13C11) Repeat Problem 13C10 using "rounding errors" of +1 for x, �2000 for 
and +300 for �. How much error do you think these variables have, either 
due to measurement error or due to measuring something different than 
what we'd like? 
13D1) Suppose that we fit a k-carrier regression 
y = bo + bx +... + bkxk 
and suppose we re-express the k-carriers by linear combinations to z, 
z2,..., zk without altering the stock of possible fits. If we now fit a new 
regression 
y = C O Jr' ClZ 1 J- ' ' ' J- CkZ k 
have the coefficients changed? Has the fit changed? 
13E1) What do we mean by the phrase "Xl is a proxy for X2"? 
13E2) Suppose that an investigator finds that scores y' on a general knowledge 
test are correlated strongly with number of years of education, E. Eager 
to find what kinds of courses were most helpful, the investigator defines 
new variables x,..., x of the flavor "number of humanities courses", 
"number of statistics courses", etc. But the new regression 
� = bo-{- blX1 +'"-F bkxk 
has no coefficients very far from O. What has happened? 
Exhibit 1 for Problem 13C6 
Rounded values of y = -1 + x + .Sx 3 
.9 .26 
1.0 .50 
1.1 .77 
1.2 1.06 
1.3 1.40 
1.4 1.77 
1.5 2.19 
530 /Problems 
13E3) Suppose that we regress "number of tackles in a season", T, on height H 
(in inches) and weight W (in pounds) of defensive tackles in professional 
football, finding T = bo + .50W-.10H. Does this mean it helps to be 
shorter? What can we say about the relative importance of height and 
weight? 
13E4) Let y = municipal bond yield, x = block offer size, and x5 = college 
students/population in the municipal bond data (Data Exhibit 9 for Prob- 
lems). 
a) Fit the regressions y = ao + ax, and y - bo + bsxs, and y = Co + 
ClX 1 -{- CsX 5. 
b) Do college students lobby for higher yields on their municipal bonds7 
What might explain your finding? 
13E5) Discuss which variables in the Economics data (Data Exhibit 4 for Prob- 
lems) might be proxies (a) for unincluded or unmeasurable other vari- 
ables, or (b) for other variables in the data set. 
13E6) Do Problem 13E5 for the Coleman data (Data Exhibit 7 for Problems). 
13E7) Do Problem 13E5 for the herniorrhaphy data (Data Exhibit 8 for Problems). 
13F1) Can you distinguish "the effect of. xi on y when all other xj are. held 
constant" from "the effect of xi, adjusted for all other x's, on y, adjusted 
for all other x's"? Why/why not? 
13F2 If a policy change shifts xj, when is it often inadequate to substitute 
the new xj into the old regression? Why is it often inadequate to substitute 
all the new x,..., xk into the old regression? 
13G1) How are physical science applications of regression often different from 
applications in other fields? 
13G2) Why is regression analysis unable to guarantee control of background 
variables in an observational setting? Give an example of how this might 
happen in some field of application you are familiar with. 
13G3) Explain why multiple regression with correlated carriers may produce 
poorly determined individual coefficients even when the fit to the data ;s 
close. 
13G4) Compute and compare the residuals (v - ) for the egg data from Exhibit 
8 of Chapter 13 when: 
a)  = x, + 2x3; 
b) ' = .320 + .728x, + 1.812x3. 
13G5) If we have several variables all measuring aspects of the same thing (like 
"atmosphere at home"), why is it unwise to put them individually into the 
same regression? How can we use such multiple information? 
Chapter 14 531 
13G7) For the Economics data in Data Exhibit 4 for Problems, let 
y - savings, 
x -- unemployment (all), 
x2 -- Moody's Aaa interest rate, 
x3 -- short-term interest rate, 
x4 -- consumer price index, 
x5 = (year- 1965). 
a) Regress � on x through xs, and try to interpret the resulting equation. 
b) Which carriers are acting as proxies for which other carriers in the 
regression? for variables not in the regression? Remember that every 
carrier is increasing over time. 
c) Can you guess the effect of a change in interest rates from this 
equation? 
13G8) Do Problem 13G7 for the Education data (Data Exhibit 5 for Prob- 
lems) where � = expenditure per public-school child and all variables are 
included in the fit. Should some carriers be logarithms of variables? How 
are wealthier towns likely to differ on all carriers from poorer towns? 
13G9) In Problem 13G7 or 13G8, why has the regression equation not gained in 
accuracy and validity due to control for so many background variables? 
13H1) Describe a problem with carriers which cannot be resolved by even an 
arbitrarily large amount of data. 
13H2) Explain how it can happen that coefficients b and b2 are very poorly 
determined, but that b -F b2 is very precisely determined. 
13H3) What is the general approach to finding difficulties as in Problem 13H27 
13H5) Regress � on x, x2, and year in the data from Exhibit 3 of Section 12C. 
How do the techniques of this section help to illuminate the difficulty in 
getting stable coefficients? 
13H6) Use the techniques of this section (a) to better understand the fit and (b) to 
simplify the regression equation in Problem 13G7. 
13H7) Do Problem 13H6 using the regression from Problem 13G8. 
13H8) How can the techniques of this section help to uncover when (a) several 
carriers are measuring essentially the same thing? (b) when 2 carriers are 
wholly different but strongly interrelated in the population under study? 
(c) when 3 or more carriers are measuring different mixes of two things? 
Chapter 14 
14A1) Show that the least-squares estimate of = in y =  +/3x is & = ?-/. 
14A2) Show that 
0'2 Z X2 
Var (&) = 
nZ (x - 
532 /Problems 
Check this formula in the special case when each x is either 0 or 1 (not 
necessarily all alike). 
14A3) For the fit / +/(x- ,) using least-squares, prove that the presence or 
absence of/ in the model has no influence on our estimate of/3, and vice 
versa. 
14A4) Suppose we are fitting y - c + ,x 2 by equally weighted least squares. 
What is the variance of 77 How can we rewrite c + 7x 2 so that the two 
coefficients can be fitted separately? What will the variances of the esti- 
mated coefficients be? 
14A5) Suppose we are fitting y = c + /3x and want to rewrite c +/3x in terms of 
z and a new coefficient  so that c and & can be fitted separately. Express 
the condition for this in terms of the variances and covariances of c and . 
Find a rewriting that satisfies this condition. 
14A6) Suppose we are fitting y =  cos e + /sin e where the values of e are 
irregularly scattered. Can we rewrite  cos e + /sin e in terms of ) and / 
so that ) and / can be estimated separately? What is the natural condi- 
tion? What rewriting satisfies it? 
14A7) Suppose we are fitting y - KAe ^t + Kse t where A and B are given. Can 
we rewrite KAe t + Kse  in terms of K and, say, 's, so that K and ?s can 
be separately estimated? How? Can we rewrite it so that Ks and, say, -^ 
can be separately estimated? How? What are the variances of KA after the 
first rewriting and of Ks after the second? What were they in the begin- 
ning? 
14A8) (Uses 14A7) Suppose B is close enough to A so that 
e (s-A)t -' 1 + (B - A)t 
is a satisfactory approximation; what is the approximate variance of K? 
What would happen if B were very close to A? 
14B1) What is a matchar? 
14B2) Why is the set of ones a matchar in fitting y = c +/3x by least squares? 
14B3) Show that in fitting y = /3x by least squares, multiples of x are matchars. 
Are they the only matchars? 
14B4) Show that in fitting y = c + /3x by least squares, {x(i)- �} is a matchar 
even if z and /3 are linearly dependent. 
14B5) Why is it that in fitting y-- c +/3x by least squares, two matchars are 
needed if a and /3 are linearly independent? 
14B6) Let/3 be the least-square estimator for/3 in the model ./3(x - ,). Let _bxe ' 
another estimator for /3 not a least-square estimator, and let hi = x 
Prove that if, for a given set of data,  ihi >  ihi, then / > /3 for the 
same data. 
14B7) Suppose that we are fitting y = /3x, where the y's are integers and the x's 
come in eighths (fractions of an inch? stock prices?) What matchar will 
Chapter 14 533 
keep our arithmetic simplest? (Here, simplest means "no fractions" and 
"numbers" not unnecessarily large.) 
14BCJ) Suppose that x = O, 1, 2, 3, 4, 5 each once, and that we are fitting 
y --  + x. What two matchers does the discussion in the text suggest 
as particularly convenient? Why are they convenient? 
14B9) Suppose that we are fitting y =5 + x + 3,x 2 and that someone has 
proposed 
1 + x + 3x , 1 + x + 5x , I + x + 11x , 1 + x + 17x , I + x + 29x , 
and 2 + x + 1023x  
as matchefs. How many do we need to choose? (Call this number k.) 
Which of the () sub. sets of k can we choose? What is the simplest one? 
Do we like it? Suggest a better choice (not all from these 6 candidates)? 
14B10) What would happen if we tried to fit 
y = z + /3x + /x  + r(3x - 17x + 12)? 
How many matchefs would we need? How could we find them? 
14Bll) Suppose we want to fit y = ae  + 1', and that we have an approxima- 
tion,/3o, to/3. What is now the natural approximation to ae  + 77 What 
are a corresponding set of marchers? Can we use the same set of 
marchers for more than one value of/30? Why? Why not? 
14C1) What is meant by a fit to the model y 
14C2) What is the value of c so that the marcher {1 + cx(i)} tunes to  and tunes 
out/3 in fitting y = z +/3x by least squares? 
14C3) What is meant by a catcher for a fitted coefficient? 
14C4) Show that x(i) - 2 
T, (x(i) - 
is a catcher for/ in fitting y = / + /(x - �). 
14C5) Suppose we are fitting y =  -t-/x. What is the catcher for ? For =? For 
/ (in the form / +/(x- 2))? 
14C6) Suppose the values of x are O, 1, 3, 6 and 10; what numerical forms do 
the catchers take that we have just asked for? 
14C7) Suppose (i) the values of x are symmetric about zero, and (ii) we are 
fitting 
z -t- ,BX 2 -F -yx4 -I - X s -I- x ? -F ?X 8 -I- X TM. 
What is the catcher for ? What is its arithmetic expression if x = �2, �5, 
�8, �9, and �10 (each once)? 
14C8) How much would the last answer be changed if x = �2 (each seven 
times), �5 (each five times), �8 (each twice), �9 (each twice), and �10 
(each once)? 
534 Exhibit 1/Problems 
14C9) if {c(i)} is the catcher for  when we are fitting 
OlX 1 - O2X 2 -- 3X3, 
and if {c2(i)} is the catcher for (z2, and {c3(i)} that for (3, what will happen 
if we fit only 
1Xl -- 3X3 ? 
14C10) If {d(i)} is the new catcher for = and {d(i)} that for  in this latter fit, 
explain how to calculate the d's from the c's. 
14Cll) (For those with access to a computer only.) The 1962 County and City 
Data Book gives (among others) values of xo (total population, 1960), 
x= (% foreign-born), x2=3 (% completed less than 5 years of school), x23 
(% male in labor force for unincorporated urban places of 25,000 or more 
population). In Pennsylvania, the names and values are given in Exhibit 1 
for Problem 14Cll. 
Suppose that we are fitting 
0X203 -' {311X213 '' {312X22 3 -- 3X233 . 
What are the catchers for (Zo, (z, 2, and  (give answer in numerical 
form, one value for each of the 15 unincorporated places)? 
Exhibit I for Problem 14Cll 
Data for large unincorporated urban places in Pennsylvania 
[ Xo j t x23 j [x223 I i x3 1 
Abington 55,831 5.7 2.3 70.0 
Bristol 59,298 3.4 2.3 74.6 
Cheltenham 35,990 7.8 2.6 69.4 
Falls 29,082 2.5 1.8 74.2 
Haverford 54,019 6.7 2.8 71.3 
Hempfield 29,704 3.8 7.8 74.6 
Lower Merion 59,420 7.3 2.9 64.6 
Middletown 26,894 3.9 1.1 77.5 
Millcreek 28,441 2.9 3.8 72.5 
Mount Lebanon 35,361 3.7 1.1 72.5 
Penn Hills 51,512 4.3 2.9 75.3 
Ridley 35,738 4.0 3.9 73.5 
Ross 25,952 4.0 1.8 73.6 
Springfield 26,733 4.7 1.8 73.7 
Upper Darby 93,158 6.7 2.8 66.5 
Chapter 14 535 
14C12) (Computer not required) The 1962 County and City Data Book also gives 
the same data for 13 places in the state of New York. If we were again 
fitting 
0X203 -- Qi1X213 -+' 2X223 -+' 3X233 
to these 13 points, how would we expect the catchers for New York to be 
related to those for Pennsylvania? And for the 12 places in California? 
14C13) (Computer needed.) Delete the one of the 15 data sets in Problem 14Cll 
assigned to you and repeat the calculation. How would you have ex- 
pected the two sets of catchers to be related? How were they related? 
14C14) (Class exercise; uses 14C13.) Collect the various results obtained for 
leaving out the various places and (i) examine as a whole, (ii) discuss in 
class. 
14D1) Define the least-squares fit to y = ,iixi in terms of residual sum of 
squares and matchefs. 
14D2) The set of matchers {x,..., x} yields the least-squares fit of y = T,/xi. 
Is this set unique? If not, give another set. 
14D3) Why is cx +... + cx a matcher when fitting y = i/3ixi by least 
squares? 
14D4) Find a catcher for , the least-squares estimate of/ in y =  +/x. 
14D5) Use the idea of matchefs to show that , ( - y) - 0, where  is a fit of 
y -  ix with matchers which are the linear combinations of e carriers 
X,... ,X. 
14D6) Find the least-squares estimate of/3 and/3= in y = /3x -k/3zx=, assuming 
x;) - x,x) o. 
14D7) (For those used to calculus only.) Starting from the representation of the 
sum of squared deviations as 
..... 
use calculus to find a set of conditions such that /,/,...,/ give the 
unrestricted minimum to this sum, Convert your conditions into state- 
ments that certain things are marchers. Relate your result to that in the 
text of Section 14D, 
14D8) When will the result of ordinary least squares be unique? When not? 
14D9) Why is Y. (y- C/)  never negative? 
14E1) Why is x.... a marcher for fitting y = Z=/x by least squares? 
14E2) Is x.... a catcher for/,? 
14E3) What are some catchers for /3? 
2 < Z X12-fewer ? 
14E4) Why is  x.... - 
536 /Problems 
14E5) Suppose we are fitting y -- /o + /3x +/2 sin x where 
x = �1 x 10 -k, +2 x 10 -k, �3 x 10 -k, and �4 x 10 -, each once. 
Using the approximation sin x -' x - -x 3, calculate x3.2 where sin x -- x 
to a corresponding approximation, x---x2, and 1----x. How does 
 (x3.2)  compare with  (x3)27 For how large a value of k do you think 
you could do a respectable fit? How would you do the arithmetic for k 
relatively large? 
14E6) Suppose that, as in the last example, we are fitting y = oXo +/x + 
/32x, where  XoX. = 0 and  XoX, = 0. If  (x.) is very small in com- 
parison with  (x3) 2, what can we say about  (x2.)  in comparison with 
: (X2)27 Why? 
14E7) (Uses 14E6) Suppose we are fitting � = /oXo + %x + /2x3.2 under the 
same conditions as those of Problem 14E6. How will the variance of  
compare with the variance of /? Why? What does this mean7 
14E8) Suppose we are fitting � = /o + /3x +/32 sin x +/3 tan x, where 
x - �1 x 10 -, �2 x 10 -, �3 x 10 -k, and +4 x 10 - each once, 
Using the approximations 
__ I 3 
sin x '- x x -- 1--X 5, tan x -' x + �x 3 + x 5, 
what simple rewriting of/o +/x +/= sin x +/3 tan x in the form /o + 
,x + ,x  + 7sx s seems natural and revealing? How can we use this to 
get upper bounds for  (X=.o3) 2 and  (xa.o=)=? What do these bounds tell 
us about var/= and var/a? 
14E9) (Continues 14E8) How do var/= and var/ in Problem 14E8 compare with 
(a) varg2 in fitting y = o + =sinx and (b) var in fitting y = 
1o -I- 1 tan x? Comment and discuss. 
14E10) Suppose we are fitting y = /o + x +/2 sin x, where y -- tan x and 
x = +1 x 10 -k +2 x 10 -, �3 x 10 -, and �4 x 10 - each once 
Using the approximations listed in 14E8, find the fit. To this approxima- 
tion, what is X=.o (for Xo = 1, x = x, x= = sin x)? And Y.o? Draw a 
picture of the points (X=.o, Y.o). What do they tell you about =? 
14Ell) (Continues 14E10) Suppose instead that 
y = tan x + small random errors. 
About how large can the "small" random errors be and still have the sign 
of/3= reasonably well determined? For general k? For k -- 5? For k = 107 
14E12) A chemist, fitting y - /o +/x +/3=x= + ... +/xs, finds the following 
values for (xs, x.s=0, Y.s20): (1, .002, .0007), (4,-.001, .0001), (9, 
.001, -.0002), (16, .003, -.0004), (25, -.004, -0002). Plot (a) Y.o 
against xs, (b) Y.s,32o against xs.s3=o. What does the comparison of the 
Chapter 14 537 
two plots tell you? Do you think the sign of/6 is at all well determined? 
Suppose one were to fit Y.o234s = 3/6xs; would the sign of s be reasona- 
bly well determined? How can all this be? What is your advice to the 
chemist? 
14E13) (For computer users only) Go back to the data in Problem 14Cl 1 and take 
y -- X203, X 0 --' 1, X 1 = X213, X 2 --' X223, X 3 --' X233, and find the values of 
X0.123, X1.023, X2.013, X3.012, and Y.0123. Make the four plots of �'0123 against 
each of the other four, identifying any points that appear important. What 
does this tell you about which unincorporated places have a relatively big 
influence on which of /o, /, /2, or /37 (Assume a fit of the form 
+ 
14F1) What is a weighted least-squares fit to y =  x? 
14F2) Show that ordinary least-squares result when 
1 i=j, 
wiJ = o i  j. 
14F3) If the precision of each measurement �i is proportional to 1/xi in y = 
so that an appropriate weight for the ith observation is 1/x, show that 
the weighted least-squares estimate of / is ?/,. 
14F4) !n Problem 14F3, if the precision of each measurement y is proportional 
to 1/x 2, show that the weighted least-squares estimate of / is 
14F5) An astronomer wishes to fit � = /x with weight w to the following data, 
given as (w, x, �): (4, 0, 0.13), (9, 1,0.27), (4, 2, 0.43), (1,3, 0.69), 
(16, 4, 0.91), (25, 5, 1.32), (9, 6, 1.50), (1,8, 2.03). Write down an equivalent 
set of equally weighted data. Then plot it. 
14F6) (Continues 14F5) From the plot of Problem 14F5, pick an approximate 
value B for/ and replot the residuals from y -- Bx in a similar way (still 
appropriate for an unweighted fit). What does this tell you? Does any one 
of the points seem out of line? How much would you have to change its 
weight to bring it back more or less in line? 
14F7) (For computer users only) Go back again to the data given in 14C7 and, 
taking w = x203, � = x233, x I -- x213, x 2 --' x223, fit y = /o + /x + 2X2 � 
14F8) (For anyone) Make the plots of /-y against /-Xl and v-x2, for the data 
specified in 14F7. What conclusions do you reach? 
14F9) (Continues 14F8) Make a visual estimate C of 7 in � = 3/o + %x and 
plot  (y- Cx) against /-x 2. What do you conclude? 
538 /Problems 
14F10) An analyst once fitted � = /30 +/3x to 47 data points using the following 
weights: thirty-two 1's, five 3's, five 10's, one 100, two 1000's, and two 
1,000,000's. If you had the (x, y) pairs, how could you approximate his fit 
most easily7 
14Fll) in the example of the 100 points (3 sour) discussed in the text, suppose 
that x's went with the y's, that we wanted to fit � - /3x, and that the three 
points that deserved the low weights had x's of just about the typical 
size. How much are we likely to reduce the variance of/3 by including the 
sour points (with correct weights)? Would the confidence interval for /3 
shorten appreciably? Why/why not? 
14F12) (Continues 14Fll) What if the x's for the three "sour" points were about 
100 times all the other x's? Answer the questions of Problem 14F11 then. 
How is it easy to see the result? 
14G1) For the same 10 fixed values and one moving one, calculate the influence 
curve for the midmean, defined as the mean of the middle half of the 
values, taking this to mean (a) the middle five of eleven, (b) the middle 
five-and-a-half (the middle five with unit weight and the next on each side 
with weight �). How much do these two curves differ? Generally? At 
most? 
14G2) Do the same as Problem 14G1 for two other trimmed means, (a) the 
mean of the middle three, and (b) the mean of the middle seven. 
14G3) Do the same as Problem 14G1 for the trimean, 
Lower hinge + 2(median) + upper hinge 
4 
(Recall that in batches of 11, the hinges are at depth 3.5--are halfway 
between the 3rd and 4th from each end.) 
14G4) Do the same as Problem 14G1 for the singly and doubly Winsorized 
means, where in singly Winsorized means (a) the highest value is re- 
placed by a repetition of the 2nd highest (in doubly Winsorized means the 
two highest are replaced by repetitions of the 3rd highest) and (b) the 
lowest by a repetition of the 2nd lowest (in doubly Winsorized means, the 
two lowest are replaced by repetitions of the 3rd lowest)� 
14G5) (Uses 14G1, 14G2, 14G3, 14G4.) It is easy to classify influence curves by 
location into six categories, according as (a) they are or are not straight in 
the middle and as (b) they rise (fall) indefinitely, boundedly without 
returning, boundedly with returning, with the general properties shown 
in Exhibit I for Problem 14G5. Make a copy of the 2 x 3 table with 
marginal labels, and fill in, each in the appropriate cell, the mean, 
median, biweight (2 versions), the four trimmed means, the trimean, and 
Winsorized means just considered. Discuss what your result seems to 
suggest about which of these location indicators to use when. 
Chapter 14/Exhibits I (14G5) and I (14H3) 539 
14H1) Why is iterative least-squares fitting desirable? 
14H2) Do the same analysis as that given in Example I (p. 356) but using 
w(U) = {1 -lul I,'1 < 1, 
14H3) An analyst was trying to fit 
y =/o +/,x +/x +/x 
and tried to have a biweighted fit made by iteration, with the results in 
Exhibit I for Problem 14H3. What do you think happened during the 
calculation? 
Exhibit I for Problem 14G5 
Performance of estimates of location 
Bounded rise, Bounded rise, 
Indefinite rise no return with return 
Not Unsafe; efficiency not Safe against medium- Very safe; but efficien- 
straight very high for any stretched tails; c� cannot be very 
in reasonable distri- efficiency as high for any reason- 
middle bution < able distribution 
Straight Unsafe; efficiency very Safe against medium- Very safe; efficiency 
in high for some reason- stretched tails; effi- veryhighforsomeand 
middle able distributions, ciency very high for may be high for a 
but not for others some, and may be variety of distri~ 
high for a variety of butions 
distributions 
Exhibit I for Problem 14H3 
Biweighted fit by iteration 
llterati�nt l/�,] [LJ [/! I/l [' 
Start 0 0 0 0 7.5 
One 3.1 2.4 1.9 7.3 3.0 
Two 2.9 2.5 2.0 7.2 2.8 
Three 4.9 2.5 2.1 7.1 4.0 
Four 3.0 2.5 2.1 7.1 2.9 
Five 2.9 2.5 2.1 7.1 2.7 
Six 2.9 2.5 2.1 7.1 4.7 
Seven 3.0 2.4 2.2 7.0 2.9 
Eight 3.0 2.5 2.1 7.1 2.8 
Nine 3.9 2.5 2.1 7.1 2.8 
540 /Problems 
14H4) Make a plot of the weights against u given by the simple step-weights with 
c - 5 and on the same graph a plot of the bisquare weights with c = 5. 
(Multiply the latter by a constant to make it look more than the former!) 
How closely do the two versions compare? 
14H5) Do the same for the (slightly more complicated) step-weighting with c = 9 
and a bisquare weighting with c = 9. 
14H6) What might happen because a particular (x, y) pair moved back and forth 
across a break in the step-weighting? 
14H7) (For computer users.) Turn to the data in 14Cll. This time take x - X2o3 
and y = x233, and fit y = /3x (a) by least squares and (b) by biweight 
iteration with c = 4 starting with /3 = O. Discuss the difference. (Plot the 
residuals if it is helpful.) 
14H8) (For computer users; continues 14H7.) Do the same abl/biweight iteration, 
starting with /3 - 0 and c - 6, 8, and 10. Corn. pare five sets of results 
and discuss differences. (Plot the residuals if t is helpful!) 
14H9) (For computer users; continues 14H7 and 14H8.) Do the same by biweight 
iteration, starting from / - -.0002 and using c -- 4 and c = 10. Compare 
with earlier results and discuss. (Plot the residuals if it is helpful!) 
1411) Compare the pros and cons of least-squares fitting and least absolute 
deviation fitting. 
1412) What is an unfavorable feature of least absolute deviation fitting? 
1413) Let the matchers vv,xl,..., w,x k for the th iteratively weighted linear 
least-squares fitting of y = =/3x i converge 
..., wastxk. Show that wastx,..., w,txk are marchers for the least absolute 
deviation fitting of y = =/3x. 
1414') If we have only y's, no x's, what kind of a fit does least absolute deviations 
reduce to? In the light of 14G5, what might we fear for least absolute 
deviations? 
1415') Go back to the example of 14G with 10 fixed points=and one moving one, 
and find what value of 2 minimizes ',(xi - ,) where (u) is u 2 or k lul 
according as lul -< or -> k for a variety of values of the 11th point (what 
iterative weights are to be used?) What kind of influence curve does this 
kind of fitting seem to have? 
1416') (For computer users only; a town to be assigned to each solver.) Go back 
to the data in 14Cll, taking x = X2o3, /t- x233' C -' 9, and initial  -- O. 
Make flattened-least-absolute fits for (a he data as is, (b) y for the town 
assigned to you altered by �10, �8, �6, �4, and �2 (with unaltered y's for 
the other 11 towns). Plot the corresponding influence curve for /. Com- 
pare the results for various towns in class. 
1417') (For computer users only; a town to be assigned to each solver, con- 
tinues 1416.) Do the same with y--x233, x I --1, x2 = x223, x3  X213, 
Chapter 14 541 
W = X2o3 (the weight of the first kind), and the same c, initial /'s, and 
changes for one y. 
14J1) An analyst fitted (or tried to fit) y = /o + 1X1 + 2X2 Jr- 3X3 . All the 
/'s came out fairly large, something anticipated, so instead of thinking of 
plotting, for example, Y. o23 = Y.o23 +/X.o3 against X.o..,:, it was better to 
think of plotting Y.o23 (the final residual) against xl.o2. For 237 data points 
(y, x1, x2, x:), the analyst found the following pattern- 
Number Values of ! 
of data Value of 
points Y.o23 I 2-o1 I I I 
211 small small small small small 
13 small small large small small 
10 small small small large small 
1 small small small small large 
1 small large small small small 
I large large large large small 
What questions ought to be asked? Focusing on which data points? Which 
of/o,/,,/=,/ would you suggest believing? Which data points would you 
suggest setting aside? Which/3's would be most likely to be changed when 
this was done? 
14J2) An analyst had carefully thought through reasons for fitting n data points 
with 
where n/10 _< x[ _< 10n for each of the xi's. Before looking at y at all, 
however, she found that 
, (xo.45) 2 = .27n,  (x.o23)  = .38n,  (X2.o45) 2 = .0003n, 
 (X.o5)  = .79n, : (x.os)  = .0001 n,  (xs.0, z4)  = 1.23n. 
She was unhappy about the two small sums of squares and wondered 
what kind of trouble she had. What ought you to tell her? (First, about the 
trouble. Second, about what can be done about it.) Do either of these 
depend on what the y's are? 
14J3) A sports analyst had data on 1279 basketball players with y = lifetime 
average, x = length of legs (inches), x, = length of arms (inches), x3 = 
height (inches), x4 = weight. Exploring the x's he found 
: (x.34)  = 1279(.01 inch) 2, Y, (x2.,,)  = 1279(.008 inch) , 
 (x3.,)  = 1279(.012 inch) ,  (x.,) 2 = 1279(13.2 lbs) . 
He decided he would have to drop one or more x's. Was he right? 
Why/why not? If one or more x's are to be dropped, what is a good list to 
pick from? Can the numbers tell us which of these to try dropping first? 
Can common sense tell us? Which would you drop first? 
542 /Problems 
14J4) Another sports analyst had data on 534 tennis players with y any of 
various performance measures, Xl = mean velocity (miles per hour) of 
first serves when crossing the net, x2 = % of first serves in, x3 = mean 
velocity of second serves, x4 = time to run 40 yards, xs = time to run the 
marathon (26 miles, a few hours). On examining the x's for the first time, 
she found 
 (Xl.234s) 2 = 534(.07 mph) 2, T. (x2.1) 2 = 534(11%) 2, 
 (x.124s) 2 -- 534(17 mph) 2,  (x.12s) 2 = 534(0.47 seconds) 2, 
 (x5.123) 2 = 534(32 minutes) 2. 
Do you think these were reasonable answers? Why/why not? 
14J5) Suppose we are considering only Xl and x2. If Xo = I and r is the Pearson 
product-moment correlation between Xl and x2 [means allowed for, so 
that r  = (T. (X1.oX2.o))21(T. (xl.o)2)(T. (X2.o)2)], show that 
T. (xl.o2) =- (1 - r2) (xl.o) = and T (X2.ol) 2 = (1 - r )  (x2.o) 2. 
What does this mean about 
 (X.o=) = . (X=.o) = 
and 
 (X,.o)   (X.o)  
14,J6) (Continues 14J5) What about 
T (x,.=) =  (x=.,) = 
 (x) 2 and T. (x2) = ? 
14,17) (Continues 14J6)What about 
 (Xl-2346) 2  (X2.1345) 2 
and ? 
 (x.,,,) '  (x,..,) ' 
And so on? Why/why not? 
14J8) Are the following ratios possible together? 
 (X1-234) 2 ' (X2.134) 2 
= 10 - = 10 -2, 
 (x) ' ' T (x,) ' 
 (x.,)   (x.,)  
= 10 -1 = 1 
 (x)   (x)  
Why/why not? 
Re14Q. These problems, appropriate for Section 10G, require the techniques 
of Sections 14G, 14H, and 141. 
14Q1) Take your own sample of 20 cities (drawn in Problem 3D6) and 3 
variables from Data Exhibit 1 for Problems, of which two should be 320 
(median family income) and 327 (% of college graduates) and go through 
the Oob-bar computations of Section 10G. 
Chapter 15 543 
14Q2) Do the same with three variables, two of which are 331 (% in same house 
as in 1955) and 358 (moved in between 1958 and 1960). 
14Q3) Do the same for variables x, xs, and y from Data Exhibit 7 for Problems. 
14Q4) Do the same for variables x3, x4, and x of Data Exhibit 9 for Problems. 
Chapter 15 
15-1) What do we mean by guided regression? 
15-2) Why does guided regression make sense only when we do not need to 
interpret individual coefficients? What modification seems at first to con- 
quer this difficulty? Does it/doesn't it? 
15A1) What are the five "ideal conditions" we assume for guided regression? If 
we're worried about whether they hold, what is usually the first step in 
checking them? 
15A2) Explain what a stock is. 
15A3) What is a minimand? 
15A4) When there are many alternative stocks, several may turn out to have 
minimands nearly as small as the observed smallest minimand. How can 
this be exploited in practice? 
15A5) What does PRESS stand for? 
15A6) What is Anscombe's (Tukey's) s2/(n - k)? 
15A7) What is Mallows' Cp? 
15A8) How are the quantities in Problems 15A5 through 15A7 used in practice? 
15B1) What is stepwise regression? Why do we often need to do this rather than 
trying all subsets of carriers? 
15B2) What are forward and backward steps? Why do we need a backward step? 
15B3) How can step-by-step output of fits and residuals tell us about what's 
happening in the stepwise procedure? What if the coefficient for a carrier 
of interest changes a lot with each new step? 
15B4) How might a resistant stepwise regression be done? 
15B5) In "Woes of regression coefficients" (Chapter 13) we have warned against 
some interpretations of coefficients that Macdonald and Ward seem to be 
using successfully. Why may things work out for them and not for some 
other sorts of data? 
15B6) Suppose we wish to predict expenditure per pupil in the Education data 
(Data Exhibit 5 for Problems). There are 6 carriers listed; for the 5 carriers 
assigned to you, do the following; 
544 /Problems 
a) How many possible stocks of size k = 2 are there? Choose the stock 
you expect to be best. 
b) Use a stepwise procedure to look for a "best" stock of this size; be sure 
each carrier has a chance to be deleted. How does your solution compare 
with (a)? 
15B7) Compute all 10 regressions of stocks of size k = 2 in Problem 15B6. How 
many are nearly optimal? Did you find the optimal solution in parts (a) or 
(b)? 
15B8) Repeat Problem 15B6 with stocks of size k = 3. 
15B9) Compute all 10 regressions of stocks of size k = 3 in Problem 15B8. How 
many are nearly optimal? Did you find the optimal solution in parts (a) or 
(b)? 
15B10) Use Mallows' Cp to guide choice of k in Problem 15B6, and use a stepwise 
procedure to yield a "best" stock. How does your answer compare with 
work done in the last 4 problems? 
15B11-15B15) Repeat Problems 15B6-15B10 for a data set of your own. 
15Cl) What is the advantage of all-subset techniques over guided regression? 
15C2) Can we gain by looking at stocks in any particular order? What order? 
15C3) How does the work of Daniel and Wood or of Furnival and Wilson enlarge 
the applicability of this method? 
15C4) How might a resistant version of all-subset regression techniques be 
done? 
15C5) (Heavy computation.) For the Economics data (Data Exhibit 4 of Problems), 
let y = log of personal consumption, x -log of income, x 2 = long-term 
interest rate (Moody's Aaa), and x3 - short-term interest rate. Regress y 
for all possible stocks of x's, and discuss your results. What interpretation 
can be made of the regression coefficients? 
15D1) How might we sort many carriers into 3 categories of importance for 
fitting? 
15D2) What are the main steps of an analysis of this sort? 
15D3) When removing key carriers, why is it especially unlikely that the devia- 
tions (y,- - ,-) are independent? uncorrelated? 
15D4) The Boston weather data, Data Exhibit 6 for Problems, has at least 6 
carriers to predict daily temperature. Others that may be important are, for 
example: date, yesterday's temperature, or change in station pressure, 
etc. List a set of carriers of possible interest for predicting temperature (y). 
Choose 2 carriers x and x2, and fit them resistantly to � using the method 
of Section 12C. What are your values of Y;2, and x2;? (Do not include 
carriers involving precipitation, pressure, relative humidity, or fog, as they 
are needed in later problems.) 
Chapter 15 545 
15D5) Sort the ancient-warfare data, Data Exhibit 12 for Problems, into 3 
categories of interest. Fit at most 2 of the key carriers, and compute 
residuals, using number of months at war as the y variable. 
15D6) (Heavy computation) This is the first of a series of problems to predict 
inflation using the Economics data (Data Exhibit 4 of Problems). (See 
Problem 12F5 for a definition of inflation in terms of the consumer price 
index.) Choose 1 or 2 key carriers and divide the remaining ones into 
"interesting" and "long shots". You may wish to define some new carriers 
such as "last year's inflation" or "change in GNP". 
15E1) What are the 2 costs of using many carriers in a regression? 
15E2) What is the intuitive motive for combining the "football" carriers (modify- 
ing carriers in Section 15E) in the logarithmic scale? 
15E3) If variables are combined after careful study of the data, the estimated 
residual variance is likely to be less than if judgment components are used 
a priori. Explain why this does not contradict the statement in Section 12E 
that the variance of  will be larger. 
15E4) Why are linear combinations often sensible ways to combine carriers? 
What kinds of circumstances make nonlinear combinations preferable? 
15E5) (Continues Problem 15D4) Create a judgment component for fitting the 
temperature residuals with a measure of "wetness" from precipitation, 
pressure, relative humidity, and fog. Add this component to the fit. 
15E6) (Continues Problem 15D5) Create judgment components for the several 
categories of carriers in the ancient warfare data (Data Exhibit 12 for 
Problems). Discuss why we are forced to combine carriers in this problem. 
Fit these components to the residual months at war from Problem 15135. 
15E7) (Continues 15D6) Create judgment components for "interest" and for 
"unemployment" and add these to your regression. 
15F1) What are principal components? Discuss the most important differences in 
analysis between using the principal component .5x- 2x2 + xs or the 
judgment component .5x - 2x2 + xs. 
15F2) Should principal components ever be used instead of a priori judgment 
components? What questions are raised about (a) the data or (b) the 
judgments behind the a priori components? 
15F3) What is the distinction between ar (xj) and the judged measurement 
variance of x in Section 15F? 
15F4) For principal components, what are the 4 categories of carriers for study? 
How do they compare with the 3 categories of Section 15D? 
15F5) Why do we feel justified in reshaping principal-component carriers to be 
more interpretable? How much are we likely to lose? 
15F6) (Continues Problem 15D4) What are the principal components for 
precipitation, pressure, relative humidity, and fog? How does the most 
546 /Problems 
important one compare with your judgment component? How do the fits 
compare? 
15F7) (Continues Problem 15D5) Compare principal components with some of 
your judgment components. If we fit several principal components, how 
ought we respond to a markedly better fit than with judgment 
components? 
15F8) (Continues Problem 15E7) How do principal components compare with 
judgment components for the "interest rate"? Which do you prefer? 
15G1) Discuss the coefficients you have obtained in a regression analysis in the 
light of the remarks of this section. 
1511) What is the role of past knowledge and current analyses? 
1512) Since we are doing linear regression, fitting x, x2, x3, x4, x5 to y is 
algebraically equivalent to fitting x, x2, x3, x4, xs to (y- 17Xl- 5x3- 
3xs). What do we gain by using the latter approach? 
1513) Suppose you were about to analyze the weather for Boston in April of 
1976. How would you use the results from the corresponding 1975 data 
(Data Exhibit 6 for Problems) to strengthen your analysis? How do the 
ideas in Section 15H apply here as well? 
Chapter 16 
16A1) Why should regression analysis always include an examination of 
residuals? 
16A2) Why is plotting (y- ) against y a bad idea? What is a better idea? 
16A3) If we find a pattern in a residual plot, there are at least two approaches to 
improving the fit. What are they? 
16A4) What are convex-upward functions? Concave-upward functions? 
16A5) What are some different ways of examining (y - 32) with respect to 7 
16A6) How would your choice among the possibilities in Problem 16A5 change if 
you had 10, 40, 100, or 1000 points? 
16B1) What do we mean by a "re-expression" of x as opposed to the more 
general term a "function" of x? 
16B2) What is folding? 
16B3) What is the distinction between a carrier and a variable? 
16B4) When two different sets of variables give rise to the same stock, identical 
fits will be obtained. Why then does the choice of variables matter? 
16B5) Are we restricted to use variables in the currently most popular form? 
What do we lose by using new re-expressions? What might we gain? 
16C1) When plotting (y- ) against to, does it matter if to is re-expressed? 
Chapter 16 547 
16C2) What is Xdot? 
16C3) What can we do when there is not enough data to tell us about depen- 
dence of (y- ) on to.d? 
16C4) For what purposes would we want to plot residuals against tod and not 
against Xdot, even if Xdot is on hand? 
16C5) How do we smooth residuals? When can this help? 
16C6) If a plot of (y - ) against Xdot shows that to]u (or, equivalently, Xdot) should 
be added to the regression, how can we easily do this? 
16C7) Completeness of Death Registration. In any population, there is a 
tautology: 
Death rate - Birth rate - growth rate. 
If we think of the series of populations defined as that part of a population 
who are older than x, we have the series of relations 
Death rate(x) -- Birth rate(x) - growth rate(x), 
where 
# of deaths above age x 
Death rate(x) = 
# of people above age x 
and 
# of people "passing" age x 
Birth rate(x) - , 
# of people above age x 
additional # of people above age x 
Growth rate(x) = # of people above age x ' 
Demographers frequently use the concept of a "stable population," i.e., 
one in which each segment is growing at the same rate, so that 
growth rate(x) does not depend on x. Stable populations result from a 
long period of unchanging fertility and mortality. 
In a survey of rural China, taken in 1930, it is suspected that fertility 
and mortality have been unchanged for a number of years, but that only a 
certain percentage, c, of deaths have been reported, this percentage not 
depending on age. 
a) How would this affect the series of relations described above? How can 
linear regression be used to estimate c? Given in Exhibit I for Problem 
16C7 are the birth and death rates, above age x, for males in the survey. 
What is your estimate of the "completeness of registration", c7 
b) Does the population seem to be stable? Why/why not? Does complete- 
ness seem to be independent of age? Why/why not? Do you prefer to plot 
residuals against age or birth rate? Why/why not? Will both plots tell us 
more than one? Why/why not? 
16C8) Let � - bond yield, 
x3 = population of issuer, 
x = total net debt, 
for the Municipal Bond data (Data Exhibit 9 for Problems). 
548 Exhibit 1/Problems 
a) Regress y on x3, y on x4, and y on x3 and x4. 
b) Use plots of residuals to diagnose what has happened, and the quality 
of the fit by y = bo + b3x3 + b,x,. 
c) How would you continue your analysis? 
16D1) When adding a new variable tnew to a regression fit, why will the carrier 
tnew often fail to improve the fit, while Xdot greatly improves the fit? 
16D2) (Continues Problem 16C8) Use residual plots to add the best new variable 
to your best fit from Problem 16C8. Why will residual plots help in this 
situation when nearly any mechanical stepwise algorithm is likely to fail? 
16D3) (Continues Problem 16D2) Finish developing a model for bond yield with 
the Municipal Bond data (Data Exhibit 9 for Problems). Discuss why you 
took the approach you chose, and discuss the meaning of your model. 
16E1) Why do we consider muitiplicative fits of the form med -{- (/ -- /rned)( 1 ' U) 
instead of the form (1 + u)? 
16E2) What is (347 Why do we use it? 
16E3) What is q? How does it compare to Q? 
Exhibit 1 for Problem 16C7 
Reported birth and death rates (in 1000's) 
lAgel I Birth ratel I Death ratel 
5 29.5 16.2 
10 30.1 15.8 
15 31.8 16.8 
2O 33,6 18.2 
25 36.7 19.6 
30 40.3 21.8 
35 45.3 24.9 
40 54.7 29.0 
45 63.4 33.8 
50 77.9 41.6 
55 91.4 52.1 
6O 12O.8 64.6 
65 143.5 82.8 
70 169.4 111.8 
75 231.6 129.4 
S) SOURCE 
Data reprinted by permission of M. A. Stoto. For discussion of data, see G. W. Barclay, A. J. Coale, M. A. 
Stoto, and T. J. Trussel (1976). "A reassessment of the demography of traditional rural China," 
Population Index, 42, 606-635. 
Chapter 16 549 
16E4) Why do we plot residuals, rather than simply trying a new fit? 
16E5) How do we assess whether to add products of carriers? 
16G1) (Continues Problem 15F8) Complete a model describing how economic 
variables vary with the rate of inflation. 
a) "Lagged" variables might allow a prediction of next year's inflation. 
How good a prediction can be made without knowing the values of other 
carriers in next year's fit? What risks are involved in such a prediction? 
b) To what extent can the coefficients in your regression be interpreted? Of 
those variables that could be manipulated, what policy changes would you 
recommend to reduce inflation? 
c) Discuss how the changes suggested in (b) might affect values of other 
carriers. 
16P1) (Project) Redo the analysis of Problem 9A10, using regression to study the 
relationship of pressure and temperature throughout the year. Clearly, 
"month" is not a suitable carrier for regression without re-expression 
(why?). One way to start is to consider the problem as 12 smaller 
problems, pulling analyses together later (cf. Sections 12G, 12H, 15H, 151). 
16P2) (Project) Analyze the heart-transplant data (Data Exhibit 11 for Problems). 
Questions of most interest to a surgeon may include: 
a) What factors seem important for longer survival? 
b) How useful is the mismatch score for predicting survival time? Risk of 
rejection? 
c) How would you advise a physician about trade-offs between accepting 
a poor mismatch score or increasing waiting time? Is age a factor? 
d) What are the risks involved in changing policy on the basis of (c)? How 
would you explain this by example to a physician? 
e) Do you need to control for a time trend in these data? 
16P3) (Project) Analyze the Armed Forces Research data (Data Exhibit 10 for 
Problems). How does the amount appropriated compare with the amount 
requested? The amount requested next year? How is the process changing 
over time? How do the three branches of the military fare during this 
period? 
16P4) (Project) (Continues Problems 15F7, 15D5, etc.) Complete your analysis of 
the ancient warfare data (Data Exhibit 12 for Problems). What do you 
conclude about the effectiveness of military strength as a deterrent in 
history? 
16P5) (Project). Demographic Transition. Switzerland, in 1888, was entering a 
period known as the "demographic transition"; i.e., its fertility was begin- 
ning to fall from the high level generally found in undeveloped countries to 
the lower level it has today. If we agree to look at changes in "lg", a 
common standardized fertility measure, they can be related to changes in 
various socioeconomic indicators. Ig and 5 of these indicators collected by 
Francine van de Walle for a book on the demographic transition in 
Switzerland, are given in Exhibit I for Problem 16P5 for 47 French-speak- 
Exhibit 1/Problems 
ing provinces at about 1888. Analyze these data. Which variables seem most 
important? Comment on the "static" nature of this approach to a 
"dynamic" problem. 
Exhibit I for Problem 16P5 
fertility and socioeconomic indicators 
ID no. 
province  I x, I I I I 
1 .802 .170 .15 .12 9.96 .222 
2 .831 .451 .06 .09 84.84 .222 
3 .925 .397 .05 .05 93.40 .202 
4 .858 .365 .12 .07 33.77 .203 
5 .769 .435 .17 .15 5.16 .206 
6 .761 .353 .09 .07 90.57 .266 
7 .838 .702 .16 .07 92.85 .236 
8 .924 .678 .14 .08 97.16 .249 
9 .824 .533 .12 .07 97.67 .210 
10 .829 .452 .16 .13 91.38 .244 
11 .871 .645 .14 .06 98.61 .245 
12 .641 .620 .21 .12 8.52 .165 
13 .669 .675 .14 .07 2.27 .191 
14 .689 ,607 .19 .12 4.43 .227 
15 .617 .693 .22 .05 2.82 .187 
16 .683 .726 .18 .02 24.20 .212 
17 .717 .340 .17 .08 3.30 .200 
18 .557 .194 .26 ,28 12.11 .202 
19 .543 .152 .31 .20 2.15 .108 
20 .651 .730 .19 .09 2.84 .200 
21 .655 .598 .22 .10 5.23 .180 
22 .650 .551 .14 .03 4.52 .224 
23 .566 .509 .22 .12 15.14 .167 
24 .574 .541 .20 .06 4.20 .153 
25 .725 ,712 .12 .01 2,40 .210 
26 .742 .581 .14 .08 5.23 .238 
27 .720 .635 .06 .03 2.56 .180 
28 .605 .608 .16 .10 7.72 .163 
29 .583 .,268 .25 .19 18.46 .209 
30 .654 .495 .15 .08 6.10 .225 
Chapter 16/Exhibit I (continued) 551 
Exhibit I for Problem 16P5 (continued) 
ID no. 
of province I/.I I ! I I 
31 .755 .859 .03 .02 99.71 .151 
32 .693 ,849 .07 .06 99.68 .198 
33 .773 .897 .05 .02 100.00 .183 
34 .705 .782 .12 .06 98.96 .194 
35 .794 .649 .07 .03 98.22 .202 
36 .650 .759 .09 .09 99.06 .178 
37 .922 .846 .03 .03 99.46 .163 
38 .793 .631 .13 .13 96.83 .181 
39 .704 .384 .26 � 12 5.62 .203 
40 .657 .077 .29 � 11 13.79 .205 
41 .727 .167 .22 .13 11.22 .189 
42 .644 .176 .35 .32 16.92 .230 
43 .776 .376 � 15 .07 4.97 .200 
44 .676 .187 .25 .07 8.65 .195 
45 .350 .012 .37 .53 42.34 .180 
46 .447 .466 .16 .29 50.43 .182 
47 .428 .277 .22 .29 58.33 .193 
D) DEFINITIONS 
x Proportion of population involved in agriculture as an occupation; 
x2 Proportion of "draftees" receiving highest mark on army examination; 
X 3 Proportion of population whose education is beyond primary school; 
x4 Proportion of population who are Catholic; 
x, Infant morality: proportion of live births who live less than 1 year. 
S) SOURCE 
Data used by permission of Francine van de Walle. Office of Population Research, Princeton University, 
1976. 
Unpublished data assembled under NICHD contract number No 1-HD-O-2077. 
552 Exhibit 1/Data exhibits for problems 
Problems where data exhibits are used 
1. Middle-U.S. cities: 3D6 through 3F4, 3G1, 3G2, 14Q1, 14Q2 
2. Smoking and health: 11A7 through 11A10, 11C3, 11E7, 11F6, through 11F9 
3. U.S. population 1960-1970, by state: 3A2 through 3A7, 3B4, 3B6 through 3B8, 
3C1 
4. Economics: 12C8, 12D4, 12D5, 12D6, 12F5 through 12F9, 13B3, 13E5, 13G7, 
13G9, 13H6, 15C5, 15D6, 15E7, 15F8, 16G 1 
5. Education: 13A8, 13G8, 13G9, 13H7, 15B6 through 15B10 
6. Meteorology: 12D7, 12D8, 12E6, 13C4, 15D4, 15E5, 15F6, 1513 
7. Coleman: 12C9, 12D3, 13E6, 14Q3 
8. Herniorrhaphy: 12A5, 12A6, 12B5, 12C7, 13B3, 13B4, 13E7 
9. Municipal bonds: 13A7, 13E4, 14Q4, 16C8, 16D2, 16D3 
10. Armed Forces Research: 12B6, 16P3 
11. Heart transplants: 16P2 
12. Ancient warfare: 12D9, 15D5, 15�6, 15F7, 16P4 
13. Suicide (unused) 
Data Exhibit I for Problems 
Middle-U.S. cities 
This exhibit gives various facts about the 152 cities of 325,000 population in 1960 
in the three "middle" U.S. census regions: West North Central, West South 
Central, and Mountain. The variables listed are: 
# Serial number in this list (alphabetic by state and city), 
301 Land area (sq. miles, rounded), 
302 Rank among U.S. Cities (1- largest), 
303 1960 population (in hundreds), 
306 % of population nonwhite in 1960, 
313� % of population either foreign-born or with at least one foreign-born 
parent (sum of columns 313 and 314), 
320 Median family income, 1959, 
325 % of persons (_>25 years of age) who completed less than 5 years of 
school, 
327 % of persons (->25 years of age) who are college graduates, 
331 % of persons (>-5 years of age) living in same home 1955 and 1960, 
352 % of persons (->5 years of age) in one-unit housing structures (includes 
row houses), 
357 % occupied housing units with ->1.01 persons per room, 
358 % of occupied housing units moved into by head of household during 
1958 to 1960. 
These are from a few of the 160-odd columns of information given by the 1962 
County and City Data Book. 
Exhibit I 553 
554 Exhibit I (continued)/ Data exhibits for problems 
Exhibit I 555 
556 Exhibit I (continued) / Data exhibits for problems 
Exhibit I 557 
558 Exhibit I (concluded)/Data exhibits for problems 
Exhibit 2 559 
Data Exhibit 2 for Problems 
Smoking and health 
These data come from "A Canadian study of smoking and health Second 
report," by E. W. R. Best and C. B. Walker, published in the Canadian Journal of 
Public Health, 55: 1-1, 1964. 
The study took the form of a questionnaire survey of Department of Veterans' 
Affairs pensioners with a followup of deaths of pensioners who responded to the 
questionnaire. The study lasted 6 years from July 1, 1956, to June 30, 1962. 
(Those who had smoked at least a total of 100 cigarettes or 10 cigars or  
pipefuls of tobacco during their life-time were classified as smokers.) The study 
group consists of World War I and II pensioners and Korean War pensioners and 
their male dependents (sons and fathers). The age groups are based on age in 
1956. 
Cigar and Cigarette 
I Nonsmokers I pipe only and other I Cigarette only 
I Age I I Pop. J J Deaths J J Pop. J Deaths J I Pop. I Deaths J Pop. ]J Deaths 
40-44 656 18 145 2 4531 149 3410 124 
45-49 359 22 104 4 3030 169 2239 140 
50-54 249 19 98 3 2267 193 1851 187 
55-59 632 55 372 38 4682 576 3270 514 
60-64 1067 117 846 113 6052 1001 3791 778 
65-69 897 170 949 173 3880 901 2421 689 
70-74 668 179 824 212 2033 613 1195 432 
75-80 361 120 667 243 871 337 436 214 
80 + 274 120 537 253 345 189 113 63 
S) SOURCE 
Data reprinted by permission of the author and of the journal. Study conducted by the Department of 
National Health and Welfare and the Department of Veterans Affairs, Canada, and the Canadian Pension 
Commission. 
560 Exhibit 3/Data exhibits for problems 
Data Exhibit 3 for Problems 
The U.S. population 1960-1970 by state and region 
Change in population 
1960 to 1970 
Population I 
(in thousands) Net increase I 
I Regi�n I Nett�ta' I 
and state 11960 J [ 1965 J r1970 1 Number II Percent I] Births I l Deaths I .migration 
U.S. 179,979 193,526 203,806 23,912 13.3 39,033 18,192 3,070 
N.E.* 10,532 11,329 11,883 1,338 12.7 2,169 1,147 316 
Maine 975 997 997 24 2.5 203 109 -69 
N.H. 609 676 742 131 21.5 133 71 69 
Vt 389 404 446 55 14.1 85 45 15 
Mass 5,160 5,502 5,706 541 10.5 1,040 574 74 
R.I. 855 893 951 90 10.5 171 93 13 
Conn 2,5 '1. . 2,857 3,041 497 19.6 537 255 214 
M.A.* 34,270 36,122 37,274 3,034 8.9 6,725 3,749 59 
N.Y. 16,838 17,734 18,384 1,458 8.7 3,361 1,852 -51 
N.J. 6,103 6,767 7,193 1,101 18.2 1,259 645 488 
Pa 11,329 11,620 11,813 475 4.2 2,105 1,252 -378 
E.N.C.* 36,291 38,406 40,313 4,028 11.1 7,832 3,652 -153 
Ohio 9,734 10,201 10,664 946 9.7 2,047 975 - 126 
Ind 4,674 4,922 5,202 531 11.4 1,023 475 - 16 
III 10,086 10,693 11,128 1,033 10.2 2,153 1,077 -43 
Mich 7,834 8,357 8,890 1,052 13.4 1,754 729 27 
Wis 3,962 4,232 4,429 466 11.8 856 395 4 
W.N.C.* 15,424 15,819 16,518 930 6,0 3,133 1,604 -599 
Minn 3,425 3,592 3,815 391 11.5 744 327 -25 
Iowa 2,756 2,742 2,832 68 2.4 541 291 - 183 
Mo 4,326 4,467 4,688 358 8.3 857 502 2 
N. Dak 634 649 620 -15 -2.3 135 55 -94 
S. Dak 683 692 668 -14 -2.1 146 65 -94 
Nebra 1,417 1,471 1,488 72 5.1 291 146 -73 
Kens 2,183 2,206 2,249 70 3.2 419 218 -130 
S.A.* 26,091 28,743 30,805 4,700 18.1 5,965 2,598 1,332 
Del 449 507 551 102 22.8 109 45 38 
Md 3,113 3,600 3,938 822 26.5 740 303 385 
D.C. 765 797 756 -7 -1.0 182 89 -100 
Va 3,986 4,411 4,659 682 17.2 909 369 141 
W. Va 1,853 1,786 1,751 -116 -6.2 339 190 -265 
N.C. 4,573 4,863 5,098 526 11.5 1,032 412 -94 
S.C. 2,392 2,494 2,597 208 8.7 573 216 -149 
Ga 3,956 4,332 4,607 646 16.4 975 379 51 
Fie 5,004 5,953 6,848 1,838 37.1 1,107 596 1,326 
E.S.C.* 12,073 12,627 12,839 754 6.3 2,665 1,213 -698 
Ky 3,041 3,140 3,231 181 6.0 647 313 -153 
Term 3,575 3,798 3,937 357 10.0 755 353 -45 
Ale 3,274 3,. . 3 3,451 177 5.4 729 319 -233 
Miss 2,182 2,246 2,220 39 1.8 534 228 -267 
Exhibit 3 561 
Data Exhibit 3 for Problems (continued) 
Change in population 
1960 to 1970 
Population 
(in thousands) l Net increase 
i Regi�n I I Nett�ta' I 
and state 11960J I 1965 I 1970 I l Number I I Percent I IBirths I I Deathsl migration 
U.S. 179,979 193,526 203,806 23,912 13.3 39,033 18,192 3,070 
W.S.C.* 17,010 18,209 19,388 2,371 14.0 4,012 1,599 -42 
Ark 1,789 1,894 1,932 137 7.7 401 193 -71 
La 3,260 3,496 3,652 386 11.9 832 316 -130 
Okia 2,336 2,440 2,567 231 9.9 461 244 13 
Tex 9,624 10,378 11,236 1,617 16.9 2,318 847 146 
Mt.* 6,916 7,740 8,348 1,429 20.8 1,724 602 307 
Mont 679 706 698 20 2.9 144 66 -58 
Idaho 671 686 718 46 6.9 146 58 -42 
Wyo 331 332 334 2 0.7 70 28 -39 
Colo 1,769 1,985 2,223 453 25.8 401 163 215 
N. Mex 954 1,012 1,023 65 6.8 263 68 -130 
Ariz 1,321 1,584 1,792 470 36.1 365 122 228 
Utah 900 991 1,066 169 18.9 245 65 -11 
Nev 291 444 493 203 71.3 91 31 144 
Pac.* 21,368 24,464 26,600 5,328 25.1 4,808 2,028 2,547 
Wash 2,855 2,967 3,413 556 19.5 591 284 249 
Oreg 1,772 1,937 2,101 323 18.2 346 182 159 
Calif 15,870 18,585 20,007 4,236 27.0 3,634 1,511 2,113 
Alaska 229 271 304 76 33.6 73 13 16 
Hawaii 642 704 774 137 21.7 164 37 11 
* Region composed of states that follow. 
S) SOURCE 
Statistical Abstract of the United States, 1975, pp. 12-13. 
562 Exhibit 4/Data exhibits for problems 
Exhibit 5 563 
Data Exhibit 5 for Problems 
Education expenditure 
The following data was contributed by the economist M. S. Feldstein. A more 
detailed reference is "Wealth, neutrality, and local choice in public education" by 
Martin S. Feldstein, The Amer/can Economic Review, 65, No. 1, March 1975, pp. 
75-89. Data reprinted by permission of the author and of the American Economic 
Association. 
Variables: 
MFI Median family income, 
SBG State block grant, 
FG Federal grants, 
RES % of local tax base in residential property, 
PSC Public-school students per capita, 
EEP Educational expenditure per public-school pupil, 
TVP Taxable property value per public-school pupil, 
P The price to the town of obtaining 100 dollars for education. 
Towns whose schools are partially supported by matching state funds need pay 
less than a dollar to obtain a dollar for educational expenses. 
l MFIJ I SBGI  I RESl l PSCI I EEPI I TVPI P 1 
9890 181 308 68 0.180 873 19560 100 
12247 65 84 82 O. 180 864 34311 1 O0 
10904 1 102 71 0.185 874 24418 75 
11292 I 84 76 0.208 758 26824 74 
13030 1 99 80 0.256 841 28044 82 
10377 146 111 64 0.212 719 17453 100 
9815 47 146 45 0.103 1184 48012 100 
9738 150 149 60 0.197 622 15950 100 
14958 66 155 90 0.273 1008 32105 100 
12516 1 101 76 0.211 842 25743 74 
10086 68 117 35 0.175 949 50993 100 
9881 1 220 70 0.294 859 39036 87 
11982 1 158 71 0.248 903 19296 70 
11550 1 128 70 0.232 782 16941 63 
9750 96 144 62 0.200 894 26419 100 
9756 1 196 29 0.181 861 22569 70 
11031 113 101 85 0.256 871 25478 100 
11645 162 106 60 0.220 574 187 59 100 
11278 1 164 79 0.539 854 22904 75 
10871 1 70 55 0.228 850 23858 73 
17558 112 136 88 0.299 1027 24570 100 
9739 1 90 59 0.167 742 31847 80 
11020 1 126 65 0.246 772 16347 66 
10665 I 91 66 0.205 805 24561 7 5 
12424 1 56 84 0.227 761 24857 77 
564 Exhibits 5 (concluded) and 6/Data exhibits for problems 
Data Exhibit 5 for Problems (continued) 
I MFll I SBGI  I RESI I PSCl I EEP 1 I TVPI i_ P I 
9638 156 119 73 0.226 674 17915 100 
1 2580 1 63 50 0.300 745 17616 68 
12656 91 321 81 0.215 720 27058 100 
13144 1 122 80 0.273 844 26744 82 
9992 130 316 69 0.200 858 20570 100 
8924 1 162 50 0.197 878 19348 67 
12629 I 107 75 0.286 783 14790 63 
12606 I 114 68 0.228 867 24950 74 
10621 215 88 67 0.294 760 10848 100 
11629 157 185 68 0.207 749 21630 100 
9510 60 83 67 0.523 917 40645 100 
11094 142 108 68 0.189 954 25794 100 
13434 1 93 88 0.244 763 23991 74 
10067 66 79 88 0.178 810 52291 100 
11541 1 125 85 0.240 815 26512 82 
14805 I 172 84 0.302 948 19009 71 
9594 1 78 50 0.153 761 22433 67 
9752 131 65 60 0.239 634 18598 100 
12281 I 105 83 0.231 795 26689 77 
9957 162 137 67 0.150 751 22710 100 
12412 1 89 82 0.222 808 30002 83 
9802 1 136 51 0.230 786 25084 74 
9418 1 188 70 0.146 769 36782 81 
12837 1 169 56 0.224 903 22132 74 
11631 1 105 55 0.240 688 29349 82 
9279 1 140 69 0.250 822 12658 61 
11685 1 92 90 0.177 731 23856 71 
10038 I 125 44 0.165 955 20115 66 
Data Exhibit 6 for Problems 
Data about the weather in Boston during April 1975 
This data comes from the Preliminary Local Climatological Data and Surface 
Weather Observations of the National Weather Service, Forecast Office in Boston, 
Mass. 
Any positive precipitation under .01 inches is rounded up to .01 inches. "Sun- 
shine" is measured by a photoelectric cell and is the number of hours above a 
threshold brightness level. The pressure and relative humidity readings are not 
averages; they are all taken between 12:50 and 1:00 P.M. Fog is coded as 0 if there 
1 
is no fog, I if fog is moderate, 2 if visibility drops below  mile. 
Exhibit 6 565 
o 
  oo.ooooooooooooooo. ooo 
Exhibit 7/Data exhibits for problems 
Data Exhibit 7 for Problems 
Random sample of 20 schools from The Coleman Report, for Mid-Atlantic and 
England states 
Variables: 
verbal mean test score (all sixth graders), 
Staff salaries per pupil, 
6th grade per cent white-collar fathers, 
Socioeconomic status composite deviation: 6th grade means, for family size, 
family intactness, father's education, mother's education, per cent white-collar 
fathers, and home items, 
Mean teacher's verbal test score, 
6th grade mean mother's educational level (1 unit -- 2 school years) 
School 
Number IX, i ! Ixi I J 
1 3.83 28.87 7.20 26.60 6.19 37.01 
2 2.89 20.10 -11.71 24.40 5.17 26.51 
3 2.86 69.05 12.32 25.70 7.04 36.51 
4 2.92 65.40 14.28 25.70 7.10 40.70 
5 3.06 29.59 6.31 25.40 6.15 37.10 
6 2.07 44.82 6.16 21.60 6.41 33.90 
7 2.52 77.37 12.70 24.90 6.86 41.80 
8 2.45 24.67 -0.17 25.01 5.78 33.40 
9 3.13 65.01 9.85 26.60 6.51 41.01 
10 2.44 9.99 -0.05 28.01 5.57 37.20 
11 2.09 12.20 - 12.86 23.51 5.62 23.30 
12 2.52 22.55 0.92 23.60 5.34 35.20 
13 2.22 14.30 4.77 24.51 5.80 34.90 
14 2.67 31.79 -0.96 25.80 6.19 33.10 
15 2.71 11.60 -16.04 25.20 5.62 22.70 
16 3.14 68.47 10.62 25.01 6.94 39.70 
17 3.54 42.64 2.66 25.01 6.33 31.80 
18 2.52 16.70 -10.99 24.80 6.01 31.70 
19 2.68 86.27 15.03 25.51 7.51 43.10 
20 2.37 76.73 12.77 24.51 6.96 41.01 
Exhibit 8 567 
Data Exhibit 8 for Problems 
Herniorrhaphy data 
The following table presents data on the experience of 32 patients undergoing an 
elective herniorrhaphy (there were no deaths). The outcome measures are: 
LEAVE (condition leaving the operating room): 
1. routine recovery, 
2. went to intensive care unit for observation overnight, 
3. went to intensive care unit; moderate care required, 
4. went to intensive care unit; intensive care required. 
NURSE (level of nursing care required 1 week after operation): 
1. intense, 
2. heavy, 
3. moderate, 
4. light. 
LOS (length of stay in hospital after operation, in days) 
Variables describing the patient's preoperative condition are, where not self- 
explanatory: 
PSTAT (physical status, discounting that associated with the operation) on a scale 
of 1-5, 1 being perfect health and 5 being very poor health. 
BUILD (body build): 
1. emaciated, 
2. thin, 
3. average, 
4. fat, 
5. obese. 
CARDIAC or RESP. (preoperative complications): 
1. none, 
2. mild, 
3. moderate, 
4. severe. 
568 Exhibit 8 (continued)/Data exhibits for problems 
Data Exhibit 8 for Problems (continued) 
Age ! Preoperative 
complications ! 
IPatientJ I(years) I ISexl IPSTATI IBUILDII CARDIACI I RESP. I ,ILEAVE I ILOS! INURSEI 
1 78 M 2 3 I I 2 9 3 
2 60 M 2 3 2 2 2 4 -- 
3 68 M 2 3 1 I I 7 4 
4 62 M 3 5 3 I I 35 3 
5 76 M 3 4 3 2 2 9 4 
6 76 M 1 3 I I I 7 -- 
7 64 M 1 2 1 2 I 5 -- 
8 74 F 2 3 2 2 1 16 3 
9 68 M 3 4 2 I I 7 -- 
10 79 F 2 2 1 I 2 11 3 
11 80 F 3 4 4 I 1 4 -- 
12 48 M I 3 I 1 I 9 3 
13 35 F I 4 1 2 I 2 -- 
14 58 M I 3 I 2 I 4 w 
15 40 M 1 4 I I 1 3 -- 
16 19 M I 3 I I I 4 -- 
17 79 M 3 2 3 3 3 3  
18 51 M 1 3 I I I 5 -- 
19 57 M 2 3 2 1 1 8 3 
20 51 M 3 3 3 2 I 8 4 
21 48 M I 3 I I 1 3  
22 48 M 1 3 I 1 1 5 -- 
23 66 M 1 3 I I 1 8 4 
24 71 M 2 3 2 2 2 2 -- 
25 75 F 3 I 3 I 2 7 -- 
26 02 F 1 3 1 I 1 0 - 
27 65 F 2 3 1 I 2 16 3 
28 42 F 2 3 1 1 2 3 t 
29 54 M 2 2 2 2 2 2  
30 43 M 1 2 1 1 1 3 t 
31 04 M 2 2 2 1 I 3 - 
32 52 M 1 3 1 1 I 8 3 
S) SOURCE 
B. McPeek and J.P. Gilbert of the Harvard Anesthesia Center. Data reprinted by permission of the 
contributors 
Exhibit 9 569 
Data Exhibit 9 for Problems 
Municipal bond data for 20 cities 
Variables: 
Bond yield, 
Block offer size (no. of $1000 bonds), 
Term to maturity (100's of months), 
Population of issuer (100,000's of people), 
Total net debt, 
College students/population. 
No. i i I I ! II 
1 Birmingham 30 1.81 3.61 .280 1.03 335 
2 Oxnard 10 1.93 .29 .012 .00 365 
3 Salinas 30 2.79 .24 .023 4.29 315 
4 Danbury 15 1.81 .40 .036 2.38 325 
5 New Haven 15 1.87 1.65 .186 6.54 283 
6 Norwalk 40 2.17 .59 .145 .15 300 
7 New Orleans 1 5 2.34 6.40 .710 1.91 327 
8 Baltimore 10 1.85 9.74 1.827 2.24 290 
9 Detroit 10 2.09 19.25 1.703 1.81 317 
10 St. Louis 55 2.03 8.73 .533 3.09 273 
11 Clifton 5 2.37 .81 .088 .00 356 
12 New York City 5 2.33 82.00 20.720 2.25 314 
13 North Hempstead 35 1.93 2.05 .075 2.10 345 
14 Tulsa 25 2.53 2.54 .193 2.57 315 
15 Philadelphia 80 2.14 22.00 3.861 2.36 305 
16 Memphis 90 1.93 4.53 .465 1.55 285 
17 Hopewell 15 2.16 .22 .023 .00 350 
18 Norfolk 10 1.90 2.99 .356 .47 320 
19 Madison 100 1.93 1.17 .205 21.09 270 
20 So. Milwaukee 25 1.81 .17 .027 .00 305 
570 Exhibit 1n/Data exhibits for problems 
Data Exhibit 10 for Problems 
Armed Forces Research and Development (in millions of dollars) 
The attached table gives the amounts (in millions of dollars) requested by the 
Army, the Navy, and the Air Force for research and development for the years 
1953 through 1973, and the corresponding amounts appropriated by the U.S. 
Congress. 
I -rll r I !1 ,r II I 
1953 450.0 440.0 75.7 70.0 525.0 525.0 
54 475.0 345.0 74.9 58.6 537.0 .0.0 
55 355.0 345.0 61.0 419.9 431.0 418.1 
56 333.0 333.0 439.2 439.2 570.0 570.0 
57 410.0 410.0 477.0 492.0 610.0 710.0 
58 400.0 400.0 505,0 505.0 661.0 661,0 
59 471.0 498.7 641.0 821.2 719.0 743,0 
60 1046,5 1035.7 970.9 1015.9 750.0 1159,9 
61 1041.7 1041.2 1169,0 1218.6 1334.0 1552.9 
62 1130,4 1203.2 1267.0 1301.5 1637.0 2403.2 
63 1329.0 1319.5 1474.0 1475,9 3439.0 3632.1 
64 1474.6 1390.2 1578,4 1530.5 3627.9 3458.7 
65 1401.5 13.'!.. 1 1456.3 1377.5 3210,9 3117.3 
66 1442.7 1410,6 1478.1 1444.2 3153.9 3109,4 
67 1522.2 1531.9 1752.5 1762,4 3058.1 3116.8 
68 1544.0 1514.2 1863,9 1826.5 3293.6 3251.2 
69 1661,9 1522.6 2146,4 2141.3 3364,7 3570,3 
70 1849,5 1596.8 2211.5 2186.4 3561.2 3060.6 
71 1717.9 1618,2 2197.3 2165.1 2909.7 2762.1 
72 1951.5 1839.5 2431.4 2372.3 3017.0 2912.9 
73 2122.7 1829.0 2813.8 2545.3 3262.2 3122.5 
s) SOURCE 
James R. Capra, "Analysis of data describing Congressional responses to DOD budget requests," Ph. D. 
thesis, Naval Postgraduate School, Monterey, California, June, 1974. 
Exhibit 1 1 571 
Data Exhibit 11 for Problems 
Heart transplants 
After a patient is admitted to the Stanford program, a donor heart, matched on 
blood type, is then sought. 
We chose to present here only patients who have been followed until their death, 
because that avoids some problems arising from dealing with censored data. (For 
patients still alive, we know only that their survival time will be longer than it is to 
date, so the observation is incomplete or "censored".) The data reported here 
cover the deaths following heart transplantation during the period January, 1968 
through April, 1974. 
The variables in the table are: 
Survival time Number of days the patient survived after the opera- 
tion, 
Age Age at time of operation, 
Waiting time Number of days from entry into the program until the 
operation was performed, 
Calendar time Number of days after January 1, 1968 that the opera- 
tion was performed, 
Mismatch T5 score A measure of the degree to which donor and recipient 
are mismatched for tissue type. 
Survival time Reject? Mismatch Age Waiting time Calendar 
(days) Yes = + T5 Score (years) (days) time 
15 1.11 54.3 0 6 
3 1.66 40.4 35 123 
624 + 1.32 51 50 244 
46 + 0.61 42.5 11 235 
127 0.36 48 25 253 
64 + 1.89 54.6 16 279 
1350 + 0.87 54.1 36 300 
280 + 1.12 49.5 27 327 
23 2.05 56.9 19 325 
10 + 2.76 55.3 17 412 
1024 + 1.13 43.4 7 405 
39 + 1.38 42.8 11 454 
730 + 0.96 58.4 2 469 
136 + 1.62 52 82 565 
1 0.47 54.2 70 594 
836 + 1.58 45 15 612 
(Continued) 
572 Exhibit 1 1 (continued)/Data exhibits for problems 
Data Exhibit 11 for Problems (continued) 
Survival time Reject? Mismatch Age Waiting time Calendar 
(days) Yes = + T5 Score (years) (days) time 
60 + 0.69 64.5 16 623 
54 + 2.09 49 45 870 
47 + 0.87 61.5 18 864 
44 0.0 36.2 0 1101 
994 + 0.81 48.6 1 1107 
51 + 1.38 47.2 20 1149 
253 + 1.08 48.8 31 1210 
51 + 1.51 52.5 9 1326 
322 + 1.82 48.1 20 1382 
65 + 0.66 49.1 2 1420 
551 0.12 48.9 32 1525 
66 + 1.12 51.3 11 1538 
65 + 1.68 45.2 2 1561 
25 + 1.68 53 4 1634 
63 + 2.16 56.4 26 1742 
12 0.61 29.2 4 1727 
29 + 1.08 54 66 1862 
48 3.05 53.4 31 1882 
297 + 0.60 42.6 36 1893 
50 + 2.25 46.4 59 1966 
68 + 1.33 51.4 138 2060 
26 0.82 52.5 159 2082 
161 + 1.2 43.8 3 2087 
S) SOURCE 
Stanford Heart Transplantation Program. 
Described in more detail in Rupert G. Miller, Jr., "Least-squares regression with censored data," 
Biometrika, 63, 449-464, 1976. Data reprinted by permission of the author and of the journal. 
Exhibit 12 573 
Data Exhibit 12 for Problems 
Ancient warfare: Does military deterrence work? 
These data consist of a sample of influential higher civilizations* throughout 
history, for which adequate records remain to determine nearly all the carriers 
below. For each civilization chosen, a randomly chosen decade within several 
centuries is chosen. The Swiss confederation was added so that the data set 
would include some republics. 
Variable descriptions (deleting those defined as functions of other variables): 
Column 
War 1 Months at war in second sampled decade, 
Territory 2 Territory gained by conspicuous state 
(% of original area), 
Stance 3 Defensive military stance, 
Strength 4 Superior numbers in army, 
Mobility 5 Superior horses or ships, 
Quality 6 Army known superior by historians, 
Fortifications 7 Fortified border with rival, 
Prestige 8 Everybody agrees army better, even rivals, 
Propinquity 9 Has common border with rival, 
Barriers 10 At least - of common border has natural barrier, 
Capital city 11 Capital city of conspicuous state within 300 miles 
of border, 
Benefit; 12 One-sided benefits conferred by one side on 
the other, 
Culture 13 Cultural exchange common, 
Trade 14 Trade exchange common. 
576 Exhibit 13 (A)/Data exhibits for problems 
Data Exhibit 13 for Problems 
Suicide and death rates 
A) Male suicide rates 
I Age 
i Country I I 15-24 I 25-34 I l 35-44 I I 45-54 155-64 J 165-74 
Bulgaria 8.3 7.9 10.9 19.1 25.2 40.2 
Finland 15.5 35.2 53.2 62.2 68.6 68.9 
Hungary 30.5 44.1 56.0 65.2 70.7 76.4 
Israel 4.8 10.2 11.4 16.4 18.2 23.0 
N eth e rla nd 4.1 7.5 8.4 13.9 18.8 24.5 
Sweden 13.8 28.3 40.8 51.0 50.5 47.4 
U.S.A. 9.9 17.3 22.4 28.4 36.0 35.7 
S) SOURCE 
D. S. Hamermesh and N.M. Goss (1974). "An Economic Theory of Suicide," Journal of Political 
Economy, 82, p. 83. Data reprinted by permission of author and of the publisher, the University of 
Chicago Press. � 1974 by The University of Chicago. All rights reserved. 
See opposite page for panel B. 
Exhibit 13 (B) 577 
Index 
1) References to chapter summaries have been distinguished by terminal "s". 
2) References to exhibits are not distinguished (only page number) 
3) Words and terms not part of the language of statistical methodology usually re�er to 
examples (pages 1-466) or to problems (pages 467-577). 
Abt, C. C., 332 
accessible subdivision, basis for 
assessment, 121 
Adams, J. E., 162, 163 
adding up across data sets, 333ff 
additivity 
of fitting, 333 
and linear least squares, 335 
for resistance and robustness, 
335 
adjusted-linearly-for, 304ff 
adjustment, 
"arbitrary", 404 
for broad strata, 240ff, 257s 
linear, 303, 305ff, 374 
multiplicative, 435 
affine transformation, 211 
age at death, 263 
age at notable contribution, 263ff 
age heaping, 476 
air-conditioning, 492 
alkyl halides, 527 
Allen, D. M., 386, 405 
Allen's PRESS, 385, 404s 
Allport, G. W., 284, 296 
all-subset regression, 391,405s 
resistant versions, 392 
weighted case, 392 
alternate corners, alternate signs, 
170ff 
amounts, 89 
log of, = balances, 91 
re-expressing, 91, 114 
analog rules, 91 
analysis, 
factor, 25, 400 
latent structure, 25 
multiple regression, 25 
of variance, 25, 121,124 
analyzing data, complicated 
schemes of, 25 
Anderson, R. L., 129, 132, 386, 
405, 492 
Andrews case, 212ff 
anecdotes, 20 
anglits, 92 
angular transforms, 92 
Anscombe, F. $., 70, 77, 385, 
386, 405 
Anscombe's (Tukey's) criterion, 
385, 404s 
anti-image, 343 
Archimedes, 324 
arithmetic mean, 203 
internal uncertainty of, 122 
armed forces research, 520, 549 
552, 570 
assessment of uncertainty, impor- 
rance, 21 
association, 260 
astronomy, 122 
atomic weights, 497 
augmented measurement, 21 
autoregression, stepwise, 390 
auxilliary function, logistic, 107 
backward step, 388 
balances, 89 
balances, re-expressing, 91if, 114s 
Balog, $. A., 499 
Bancroft, D. R. E., 70, 77 
bank deposits, 409, 410 
ar - ob vs. ,ar - ov, 216 
Jar, 209 
Barclay, G. W., 548 
Barnum, P. T., 421 
Barone, $. L., 272 
batches, 
displays for, 43ff 
summaries for, 43ff 
Bean, F. D., 499 
Beaton, A. E., 272, 329, 332 
behavior, general, 27 
Behn, U., 496 
Bendel, R., 162, 163 
bends, 64 
Best, E. W. R., 559 
beta densities, 9, 10 
better rates deserve thought, 236 
between-days variation, 122 
binomial variation of counts, 122, 
123 
binomials, stratified, 229 
biracial living, expectations of, 
311 
birth and death rates, 548 
birth defects, 323 
birth order, 284, 312 
Bishop, Y. M. M., 257 
bisquare weight, 205, 218s, 358, 
378s 
influence curve for, 353 
biweight, 
estimate, 2(}5, 218s, 358, 378s 
fit, 539 
interval, 208 
location, 353 
biweighting, 358ff 
blacks and whites, comparison of, 
28 
Bliss, C. I., 504-506 
Bohidar, N. R., 112, 115 
boiling points, 526ff 
bombing errors, 318 
Boston, 409 
both fits, need for, 218 
Box, G. E. P., 320, 332 
Brass, W., 487 
break table, 93, 114s 
logs, 94 
reciprocals, 98 
roots, 96 
breaking strengths, fabrics, 499 
bricks, dimensions of, 525 
Brillinger, D. R., 162 
broad categories, adjustment for, 
240, 257s 
broad categories, more than two, 
249 
Brown, H. M., 496 
Brunt, Sir David, 497 
bulging rule, 84ff 
Bullough, V. L., 575 
Burrau, 0., 6, 7, 23 
Burrau's modification, 6 
Bush, R. R., 3, 23 
Ct,, Mallows's, 385, 404s 
Cady, F. B., 386, 405 
Calhoun, D. W., 504-506 
call date, 410 
Campbell, E. Q., 326, 332 
canonical correlations, 212 
Capra, J. R., 570 
Caribou, Maine, 167 
carriers, 267ff 
as derivatives, 422ff 
as predictors, 284 
and variables, 420 
next, 424 
catchers, 339, 341ff, 377s 
categories, see also strata 
broad, 240, 257s 
external, 249, 254ff 
internal, 249, 250ff 
more than two, 249 
causation, 260, 323 
causes, partial, 261 
center of gravity, 
for difficulty, 241ff 
for re-expressing grades, 92, 
111,114s 
of logistic, 243, 257s 
centers, 21 
centroid, 65, 257s 
Chen, K. K., 505 
chest measurements, 280 
child development, 321 
China, rural, 548 
chi-square, 22 
X 2 distribution, for variance, 141 
choice, 19 
choosing an error term, 123 
chords, slopes of, 82, 85 
579 
580 Index 
chunks, medians of, 52, 64ff 
church membership, 281 
circumstances, alternative, 32 
cities, 
mid-U.S., 552ff 
only three, 125 
city-to-city differences, 124 
Clarke, 502ff 
Climatography of the U.S., 165 
Coale, A. J., 548 
ob, 209 
Cochran, W. G., 145, 146, 163, 
186 
coded residuals, 170if, 201s 
coefficients, rank correlation, 13 
coefficients, regression, 299, 335 
advance, 404 
ill-determined, 371 
indeterminacy of, 315 
large variance, 369 
linear equations for, 337 
magnitude, 323 
of toi d, 431 
stable, 401 
variances, 328, 335, 369, 377s 
variances for coslock, 345 
weighted, 347, 372 
Cohen, E. R., 130, 132 
Cohen, J., 23 
Coleman data, 521ff 
Coleman, J. S., 326, 332, 530, 
552 
Coleman Report, 566 
collinearity, 280ff, 296s 
column, 165ff 
column effects, 167ff 
combinations and re-expressions, 
137ff 
common, 165ff 
community environment, 400 
comparison value, 191if, 201s 
competition, 385 
complex system, 320ff 
components, 
judgment, 369, 394, 404s 
principal, 398ff, 404s 
computers, 92 
concave-upward, 409 
concealed inference, 31, 40 
concepts, vague, 1 ff, 17if, 19if, 
469 
Condon, E. U., 132 
confidence intervals, 3, 20, 23s 
in formal inference, 21-22 
normal vs. Student's t, 5 
plus supplementary uncertainty, 
131 
with pseudo-values, 135, 139 
for systematic subsample, 126 
for variance, 139-142 
confidence points, for Student's t, 
4 
congressional seats, proportions 
of, 489ff 
consistency, 260 
constant-fit lines, 173ff 
constructing samples, 471ff 
contaminated distributions, 17 
contribution, largest from single 
cell, 232ff 
contributions, 169 
controlled trials, randomized, 221 
convex-upward, 409 
Cooper, B. E., 129, 132 
orr, 211 
correct model, 311 
correction, real progress, 250 
correlation, 27 
among variables measured with 
error, 328 
canonical, 212 
high, 281,316, 327 
lack of, 382 
correlation coefficients, 
analogs of, 211 
for uniform distribution, 286 
in re-expression, 117, 449 
cosinusoidal fit, 440 
Costa, A. J., 517 
coslocks, 303ff, 331s 
coefficients from, 343,377s 
variance, 345 
weighted, 347, 372 
cotton fineness, 496 
counted fractions, 89, 115s 
folded, 92, 114s 
re-expressing, 91 
starting, 102 
zero-infinity problem, 114 
counts, 89 
binomial variation of, 122 
re-expressing, 91, 114s 
County and City Data Book, 49, 
50, 75, 485ff, 534ff, 552 
covariance, analog of, 209 
Cox, D. R., 404, 405 
critical values, tables of, 3 
cross-validation, 25, 36ff, 139 
better ways, 38ff 
double, 37, 40s, 161 
generalized, 39 
and jackknife, 139, 148, 154 
for quality of performance, 156, 
159 
simple (single), 37, 40s 
and stability, 156, 159 
troubles, 39 
crude success rates, 221 
cubic fit, 431 
curves, straightening, 79ff 
cutting points, between strata, 
249 
Daniel, C., 392, 405, 406 
data analysis, lff 
data, hold back in safe, 38 
Davis, T. A. W., 483 
death rates, age-specific, 225, 226 
death registration, completeness 
of, 547 
decimals 
in multiple regression, 285 
number of, 93 
for pseudo-values, 135 
degrees of freedom, 
for jackknife, 136 
rule for reducing, 136 
for weighted least squares, 348 
5, what acceptable?, 450ff 
demographic transition, 549 
Dempster, A. P., 323, 324 
density, fooling median, 35 
department-store sales, 187ff 
dependence, 259, 
exclusive, 262 
linear, 284 
dependent variable, 262 
depth, letter, 47, 77s 
derived statistics, distributions of, 
7 
descriptive statistics, classical, 25 
determinantal equations, 399 
determination, 2, 21ff 
deterministic system, 301 
deviation, mean vs. standard 17 
deviation, standard, 17-19, 23 
diagnostic plot, 192ff, 193 
difference, variance of, 121 
different, could have been, 24 
difficulty, 241ff 
centers of gravity, 242 
distribution, 241 
spread, 248 
standard, 245 
digit preferences, 13 
direct assessment, 
bias from, 129 
difficulties with, 129 
for linearity with fixed weights, 
129 
making it possible, 126 
a method of, 133ff 
and random sample, 124 
sample size for, 129 
of stability, 148, 156 
of uncertainty, 119 
of variability, 122ff 
direct standardization, 224 
and adjustment, 255 
difficulties with, 231 
variances, 224, 227-229, 232 
discreteness, 12, 13, 23s 
discreteness and the jackknife, 
136 
discriminant function, 152ff 
jackknifed, 152, 153 
discrimination, between authors, 
148ff, 162s 
displays, for batches, 43ff 
distraction, extreme, 318ff 
distribution shapes, alternative, 
250 
distribution-free procedures, 4, 
22s 
distributions, 
beta, 9, 10 
characterized by parameters 
X 2, 14, 141 
Index 581 
contaminated, 17 
derived statistics, 7 
of difficulty, 241 
efficiency, 470 
exponential, 195, 196-198, see 
also exponential distribu- 
tion 
Gaussian, 7if, see also Gaussian 
distribution 
logistic, 243, 244, see also logis- 
tic distribution 
more straggling, 10 
more stretched out, 10 
normal, 7, 8, see also normal 
distributions 
posterior, 22 
range, confidence interval, 142 
rank-correlation coefficient, 13 
rectangular, 14, 15 
t, 2, 4, 16, 125, 126 
triangular, 14, 15 
uniform, 286 
for variance, 141 
diurnal rotation of the earth, 72 
double cross-validation, 37, 40s 
double-blind study, 38ff 
Draper, N., 325, 332 
DuMond, J. W. M., 130, 132 
Durbin, J., 162, 163 
earth's magnetic field, 389 
east coast temperature, 167, 168 
Economic Report of The 
President, 562 
economics data, 522ff, 530, 531, 
552, 562 
EDA, Exploratory Data .4 nalysis, 
43, 53, 70, 77, 93, 97, 115, 
165, 202, 266, 297 
education, 531,543,552, 563 
expenditure, 563ff 
educational gains, 322 
Educational Testing Service, 3 
educational tests, group of, 123 
effects, 169 
column, 167 
multiplicative, 191 
row, 167 
efficiency, 205 
Gaussian, 23s, 206 
relative, 17 
robustness of, 16, 23s, 106if, 
218s 
egg data, 323ff 
Egman, R. K., 162, 163 
eliminate variable, 425, 435 
ellipse, 325 
employment, 272 
empty cells, 
approximately large, 148 
two at once, 129 
in two-way table, 185, 186, 201 
engineering efforts, 123 
environment, 400 
equivalent number of observa- 
tions, 348 
error term, 
appropriately large, 148 
choice of, 123ff 
of measurement, systematic, 
119 
two at once, 129 
errors, standard, 153, see also 
standard errors 
estimands, 33ff, 40s 
estimate, 22, 32ff, 40s 
biased, 133 
of lack of fit, 382 
estimator, 22, 32ff 
sample size for, 33 
Euler, L., 64, 70 
even numbers, 65 
exact and approximate linear 
dependence, 264 
exact carrier, relative unimpor- 
tance of, 315ff 
examining , 407ff 
more points?, 418ff 
exclusion, regression as, 269, 
271,287, 296s 
exploration, 29, 445 
exponential distribution, 195, 
196-198 
doubly perturbed, 430 
fitting quadratic to, 426ff, 428ff, 
430ff 
jackknifed, 140, 142 
perturbed, 426, 428 
unperturbed, 426, 427 
variance for, 142 
external categories, 249, 254 
extrapolation, dangers, 266 
F-statistic, in stepwise regression, 
388 
factor analysis, 25, 343, 400 
failure, mean-square, 449ff, 451, 
462 
failures, 492 
family income, median, 49, 50, 
51, 75ff, 76 
Fan, C. T., 295, 297 
Federal Reserve Board, deposits, 
409, 410 
Federal Reserve district, 409 
Federalist papers, authorship, 149, 
161,495ff 
Feldstein, M. S., 563 
fertility, Swiss, 550 
and socioeconomic indicators, 
550 
fertilizer, 326 
fiducial info. fence, 22 
Fienberg, S. E., 230, 257 
finger pull, 127 
first aid in re-expression, 109, 
114s 
first aid, role in second aid, 109 
Fisher, R. A., 3, 23, 426 
fit, 302, 420 
additivity in, 333 
cosinusoidal, 440 
empty cells, 185, 186, 201s 
exponential, 426-430 
with half steps, 181 
ideal conditions for, 381 
iterative least squares, 335, 356 
least absolute deviations, 335, 
365, 367, 379s 
mean polish, 181, 182 
median polish, 180 
by medians, 178 
multiplicative effect, 191 
one more constant, 199, 201s 
order of, 446, 448s 
outliers from, 183,201s 
with product terms, 435, 448s 
quality of (rrawfit) , 450ff 
re-expression for, 194, 201s 
resistant, 70 
significance, 452 
sinusoidal, 440 
sounding, effect on, 307 
very good, 454, 455 
worsening of (8), 449 
fitted coefficients, 
what determined, 303 
interpretation dangerous, 301 
fitting a function, 266ff 
fitting, by stages, 271ff 
fitting lines, 335 
fitting two-way tables, 178ff 
flattened weights, 368, 279s 
Fleiss, J. L., 233, 257 
Flieger, W., 577 
flogs (folded logs), 92, 102, 114s 
folded counted fractions, 114s 
folded }ogs (flogs), 92, 102, 114s 
folded roots (froors), 102, 114s 
foldings, 
anglits, 92 
angular transforms, 92 
for counted fractions, 72, 114s 
logits, 92, 114s 
normits, 92, 
of variables, 419 
probits, 92 
standard normal deviates, 92 
Foner, A., 297 
forecasting, 36 
forecasting formulas, criticism ex- 
pected, 270 
form, 
of procedure, how chosen, 39 
of regression, 37 
formal inference, 21, 31 
of procedure, how chosen, 39 
of regression, 37 
forward step, 388 
Fox, M., 23 
fraction defective, 123 
fractions, 
counted, 89, 114s 
logs of started, 114 
re-expressing, 92, 100ff 
started, I02 
zero-infinity problem, 114 
Frawley, W. H., 162, 163 
582 Index 
freedom, degrees of, 
for jackknife, 136 
rule for reducing, 136 
for weighted least squares, 348 
freeing y of x, 268, 271,287, 304, 
374 
Freeman, M. F., 257 
freight, amounts and revenue, 
431 
French dictation, 259 
freshman physics experiment, 454 
froots (folded roots), 102, 114s 
functional relation, straightening, 
79ff 
Furnival, G. M., 392, 405, 406 
Gaiton, F., 262 
Gauss, K. F., 70 
Gaussian distributions, 7if, see 
also normal distribution 
Gaussian efficiency, 23s, 206 
Gaussian inverse cumulative, 111 
geochemistry, data of, 502 
geometric mean, 493 
Ghurye, S. G., 24 
Gilbert, J.P., 322, 332 
Glynn, J. G., 70, 77 
Goldbach Conjecture, 64 
Goldbach counts, 65ff, 478 
good cake, shouldn't refuse, 27 
Goss, N.M., 576 
Gosset, W. S., 1, 7, 22s, 24 
grades, 89 
grades, re-expressing, 92, 103, 
106, 107 
Graham, J. S., 496 
Grand Banks, 519 
graphical fitting, by stages, 271ff 
gravity, center of, 241 
for re-expressing grades, 92, 
111, 114s 
of logistic, 243,257s 
Gray, H. L., 162, 163 
Green, B. F., Jr., 401,406 
Gross, A.M., 16, 23, 208, 218s, 
219 
gross national product, 272, 421 
grouping, appropriate level, 121 
groups, how many?, 125 
Grove, R. D., 226, 227 
Gruber, D. G., 112, 115 
guided regression, 
combined techniques, 393 
fraught with difficulties, 387 
ideal conditions, 381 
several studies, 402 
several y's, 402 
h(9) smoothed, 409 
habits of mind, 421 
hair diameter, 496 
half steps, number, 181 
halo effects, 119 
halving, multiple, 38 
Hamilton, Alexander, 148ff 
Hamilton indicator, 150 
Hammermesh, D. S., 576 
Hampel, F. E., 356, 379 
handwork, 93,361 
Hardy, G. H., 64, 77 
Harrington, E. C., 404 
Hartman, G., 483 
haystack, 393, 405s 
stepwise search in, 394 
health, measuring, 323 
heart measurements, 280 
heart-transplant, 549, 552, 571 
heat capacity, 321 
heat of water, specific, 503 
herniorrhaphy, 519ff, 528, 530, 
552, 567ff 
high tides, Honolulu, example, 
437ff, 438 
highest k and lowest k, 49 
highest ten and lowest ten, 76 
high-frequency words, 149 
Hilferty, M. M., 10, 11, 12, 24, 
122, 468 
hinges, 47ff 
hint-searching, 29 
Hobson, C. J., 326, 332 
Hoeffding, W., 24 
hog prices, 492 
Holland, P., 230, 257 
hollow downward, 80 
hollow upward, 79, 81 
home conditions, 321 
home, school, and teacher vari- 
ables, 326 
homogeneity, 382 
Honolulu high tides, 437ff- 
Hotelling, H., 24 
housing, % single-unit, 49ff, 76 
H-spread, 207 
human error, gross infrequent, 16 
Hyman, H. H., 311,323, 332 
hypothesis, poor fit, 22 
icebergs sighted, 519 
ideal conditions for fitting, 381 
ideal minimand, 382ff 
ideal minimand, estimate of, 
383ff 
idiosyncrasies, daily, 122 
ill-determined cell rates, and in- 
direct standardization, 
231ff, 236ff, 256s 
image, 343 
independence, failure of, 262 
independence, statistical, 260, 
295s 
independent variable, bad term, 
262 
indeterminancy, in regression, 
315 
index cards, 76ff, 475 
indication, 20if, 25ff 
hints, 29 
not inference, 27 
stopping with, 30 
uncertainty of, 119 
indicators, 25ff 
choice of, 32ff 
indirect standardization, 233ff, 
257s 
difficulties with, 236 
inequalities, 27 
infant mortality, 498, 500 
inference, 1, 30, 38 
concealed, 3 lff, 40s 
fiducial, 22 
formal, 21 
informal, 31 
shortcut to, 2 
infinite population, and jackknife, 
159, 160 
infinities, 112ff 
infinities, ant! zeroes, 113 
influence curve, 538 
for biweight, 353ff, 378s 
for location, 351ff, 352 
for MAD, 356 
for mean, 351ff 
for median, 351ff 
informal inference, 31 
instrumental variable, 288ff 
instruments, 
dependence on calibration, 130 
sensitivity to the unwanted, 130 
intelligence test, for blacks and 
whites, 28 
intercept equation, 339 
intercepts, 59 
intercepts, centered, sizes, 60 
internal categories, 249 
and centers of gravity, 250 
internal uncertainty, 119, 131s 
educational test, 123 
plus supplementary uncertainty, 
129, 132s 
interpenetrating subsamples, 127 
interquartile range, 19, 207, 218s, 
353 
interval, biweight, 208 
interval estimates, 
robust/resistant, 208ff 
invariance, 
can be dispensed with, 211 
under arline transformations, 
211 
under permutation, 211 
under rotation, 211 
vs. resistance and robustness, 
211 
investigator, statistically cons- 
cious, 25 
irrelevant contribution, size of, 
465 
iteratiee least squares, 335, 356, 
376s 
jack-in-the-box, 216 
jackknife, 34, 133ff, 162s 
jackknifed confidence limits, 146 
jackknifed estimate, 146 
jackknifing, 
and bias reduction, 141 
coefficients, 138 
Index 583 
for combinations, 137, 138, 
162s 
and cross-validation, 139, 148, 
156 
examples with individuals, 139 
in groups, 136, 148, 162s 
and infinite population, 159 
and large error term, 148 
median, 34 
order statistics, 145 
references for, 162 
a standard deviation, 139 
trimmed mean, 35 
what to avoid, 138 
Jacksonville, Florida, 71 
January temperature, maximum, 
71if, 72-74 
Johnson, P. O., 127, 128, 132 
Jones, H. L., 162, 163 
Jordon, M. H., 499 
judged measurement variance, 
399 
judgment components, 396ff, 402 
don't cheat, 396 
judgment composite, 327 
judgment composite, advantages, 
327 
Juneau, Alaska, 71 
k, choice of, 385 
exact, relative unimportance of, 
315 
held constant, 318ff, 331s 
key, 393,405s 
modifying, 395 
more than one, 267ff 
mul ticollinear, 319ff 
naturalness of, 422 
nearly-equivalent, 280 
number of, 385 
offered for regression, 301, 
331s 
promising, 393, 40 5s 
ratio, largest x/smallest x, 118 
raw, relation to re-expressed, 
450 
raw (Xraw), 117, 449 
rearranging, 394 
re-expressed (Xstraightened) , 117 
k-carrier stocks, choice among, 
385 
Kendall, M. G., 13, 23, 162, 163 
Kendall, S. F. H., 13, 23 
Kepler, J., 325 
key carriers, 393, 405s 
Keyritz, N., 577 
Klemenc, A., 525 
Kraemer, J. H., 505 
Kratz, R. F., 496 
L, less than any number, 112 
lack of fit, estimating the, 382ff 
ladder, motion on, rules, 79 
ladder, of re-expressions, 79ff, 
88s 
Lake Victoria Nyanza, level, 518 
Landolt-Bornstein, 503 
Laredo, Texas, 167 
large data file, structures, 295 
largest component, misleading, 
401 
largest observed mean, 385 
largest x/smallest x, 118, 450ff 
largest/smallest, substantial, 91 
Larsen, W. A., 345, 379 
Larson, R., 511 
latent structure analyses, 25 
latent variable, 287 
latitude, 71 
Lausanne, Switzerland, 504 
Lax, D. A., 207, 219 
least absolute deviations fit, 365ff 
flattened weights, 368 
least absolutes fit, 365 
least pth deviations, 369 
least squares fit, 357, 377s 
and costock, 303, 374 
generalized weighted, 349, 378s 
iteratively modified, 335 
iteratively weighted, 356ff 
linear and additive, 335 
by matchefs, 341ff, 376s 
matrix weighted, 349 
ordinary, 341ff 
to raw and smooth data, 58ff 
tuning, 342 
weighted, 346, 357ff, 378s 
leave-out-one-procedures, 38ff 
cross-validation, 161, 162s 
jackknife, 135, 156, 161 
two simultaneous uses of, 156 
leave-out-two procedure, in 
Federalist example, 161 ff 
Lee, H. M., 505 
Lehman, H. C., 263 
length, of arm or leg, 395 
letter depth, 47, 77s 
letter values, 45, 77s 
depth of, 47 
displays, 48 
hinges, 45 
medians, 45 
likelihood functions, 22 
limens, difference, 127 
Lincoln, A., 421 
Linder, A., 504 
Linder, F. E., 226, 227 
Lindzey, G., 284, 296 
linear adjustment, 303, 374 
constraints on variables, 312 
examples, 305 
linear combinations of observa- 
tions, 18 
linear regression, 
dependencies, 284 
equations for coefficients, 337 
resistant and robust, 209 
linear relations, advantages, 79 
location, 8, 21 
estimates of, 539 
indicators of, 35 
influence curves for, 351 
measures, 21,203ff 
performance of estimates of, 
539 
robust and resistant, 206 
log of amount --- balance, 91 
log x, need to take?, 459 
logistic distribution, 243ff, 244, 
245 
for center of gravity, 243 
cumulative, 111 
slice out of, 111 
usefulness of, 250 
logit, 92, 114s 
logs, 
matching for, 102if, 104, 105 
need for, 455ff, 458,460 
quick, 92ff 
in relation to powers, 80 
two decimal, 93ff 
longitude, 72, 73-74 
Longley, J. W., 272 
Los Angeles, California, 73 
lowest ten and highest ten, see 
highest 
Macdonald, N.J., 38, 40, 389, 
406, 543 
macrovariability, micro- 
differences to, 123 
MAD, (median absolute devia- 
tion), 207, 218s, 356 
MAD, influence curve, 351 
Madison indicator, 150 
Madison, James, 148ff 
Madow, W. G., 24 
Maek, P. B., 499 
magnetic character figure, 389 
magnetic field, earth, 389 
main diagonal, 399 
Maine, death rates, 225ff 
Mallows, C. L., 385, 386, 392, 
404, 406 
Mallows's Co, 385, 404s 
Mann, H. B., 24 
market-research analyst, 130 
Martin, A. R., 506 
mass production, 123 
matched re-expressions, 194 
matchefs, 335, 337, 376s 
algebra for, 338 
as catchers, 341 
examples, 338 
for least squares, 341 
tuned to a coefficient, 339 
working, 343 
matching estimands to estimators, 
33 
matching, re-expressions, 102if, 
115s 
for exponentials, 195, 196-198 
for fractions, 100-101 
for logs and powers, 104, 105 
mathematical model, description 
of, 21 
Mather, K. A., 483 
McCarthy, P. J., 38, 41 
584 Index 
McCleary, S. J., 345, 379 
McNeil, D. R., 165, 202 
McPartland, J., 326, 332 
mean, 25, 206, 207, 218s 
influence curve for, 351 
in regression definition, 264 
relative efficiency, 17 
trimmed, 34-35 
variance of, 17 
Winsorized, 12 
mean deviation, 17 
mean polish, 181, 182, 183 
vs. median polish, 181, 182 
mean-square error, 130, 382, 
449ff 
as minimand, 382 
mean-square failure, 449ff, 451, 
465 
definition, 462 
measurement, 
augmented, 21 
covariances, 399 
errors, 399 
regression as, 268, 320 
systematic errors of, 119 
variance, judged, 399 
measurements, chest and heart, 
280 
measures of location, 21,203 
mechanism, 261 
median absolute deviation 
(MAD), 207, 218s, 356 
median age, 49, 50, 51 
median family income, 49-51, 75, 
76, 476ff, 484ff 
median polish, 180, 182, 201s 
vs. mean polish, 181ff 
medians, 25, 34, 43ff, 206, 207, 
218s 
confidence interval for, 16 
efficiency, vs. mean, 17 
influence curve for, 351 
for jackknife, 34 
in regression definition, 265 
repeated running, 53, 55 
running, 53, 54, 56, 57, 411 
smoothed, 75 
for two-way table, 178 
variance of, 17 
Melville, H., 513ff 
Methodist Episcopal Church, 282 
Mexico, age distribution for, 
476ff 
Meyers, C. E., 162, 163 
Michelson, A. A., 468 
Mickey, M. R., 162, 163 
raids, 48ff 
mid-U.S. cities, 542ff, 552 
miles of haul, 431 
Miller, R. G., Jr., 38, 41,162, 
163, 572 
mineral production, U.S., 43 
minimand, ideal, 382 
minimum estimated variance, 329 
rainvat, modifications, 329 
mode, 27 
model, correct, 311 
model, for regression, 381 
modifications, minvar, 329 
modified carriers, 395 
molecular weights, 526ff 
monotonic, 79 
Mood, A.M., 326, 332 
Moraballi Creek, British Guiana, 
483 
more coefficients, more variable, 
373ff 
more generally, residuals, 70 
more numbers, 169 
more-way tables, 165ff 
mortality, pattern of, 487ff 
Moses, L. E., 211, 219 i 
Mosteller, F., 3, 23, 38, 41, 67, 
70, 77, 149, 150, 163, 322, 
332, 514 
Mosteller, G., 282 
mouse survival, 506 
Muller, M. E., 295, 297 
multicollinear carriers, 319 
multiple components, 209, 218s 
multiple regression, 25, 36, 133 
and discriminant function, 150 
meaning of coefficients, 299ff 
number of decimals for, 285 
re-expression for, 461 
variables used in, 301 
multiple-component data, 
analysis, robust and resis- 
tant, 209ff, 218s 
multiple-control methods, often 
required, 28 
multiplicative effect, 191 
multiplicity, 385 
holding back data, 29 
problems of, 6, 28, 40s 
may demand repeat study, 18 
with Student's t, 6, 22 
municipal bonds, 530, 547, 552, 
569 
Naroll, F., 575 
Natoil, R., 575 
National Assessment of Educa- 
tional Progress, 508 
need, see re-expression 
neighboring points, borrowing 
from, 53 
neurotic, 469 
new variable, (there), 431 
New York, 409 
noise, 58 
non-linear regression, 61 
non-linear stocks, 422, 448s 
nonnormality, 12ff, 16 
non-parametric procedures, 4, 22s 
non-response, allowance for, 130 
Norkus, E. P., 501 
normal distributions, 7, 8 
inverse cumulative, 111 
in the middle (Winsor), 16 
mixtures of, 16 
random deviates, 54, 55-57 
standard deviates, 92 
and the trimmed mean, 35 
variance of, compared with ex- 
ponential, 142 
normal, not the ordinarily occur- 
ring, 7 
normal scores, 111 
normits, 92 
nose-counting, 25 
notation, dot, 303ff 
Nowlin, A. G., 514 
Noyes, A. A., 524 
nsb2 i, 210 
number of carriers, 383, 385 
number of children, anticipated, 
510 
numbers, 
doubly constrained, 91 
isolated, 27 
kinds of, 89 
singly constrained, 91 
numerical summary, 20, 23s 
Oakford, R. V., 212, 219 
observations, equivalent number, 
348 
Odeh, R. E., 23 
Odishaw, H., 132 
old variable, (told) , 423 
coefficient of, 431 
Olkin, I., 24 
on the average, holding, 18 
one more constant, 19911' 
one number, cannot do the work 
of many, 240 
one variable at a time, 303ff 
operating characteristic, 3 
optimization, 37, 39, 40s 
order of fitting, 446, 448s 
order statistics, with jackknife, 
145 
ordered labels, 89 
ordered values, logistic re- 
expression, 111 
orthogonalization, 425 
oscillations, 64 
outliers, 16, 45 
in least-absolutes fit, example, 
366 
in least-squares fit, example, 
358 
in two-way tables, 183, 201s 
oversimplifications, drastic, 123 
ovoid, 325 
Paese, F. H., 468 
Palumbo, F. A., 490 
papers, pairing of, 149 
parameter, two meanings of, 32, 
36 
parity progression ratios, 499 
partial derivatives, for nonlinear 
stock, 422, 448s 
past (the), 402 
pateliar reflex, 504, 507 
patterns, looking for, 64ff 
Index 585 
Pearson, E. S., 24 
Pearson, F., 468 
Peirce, C. S., 11ff, 23, 24, 121 
data, 11,121,122, 468 
Pennsylvania, 534ff 
performance, quality of, by 
cross-validation, 156 
and stability, 156, 159 
permutations, random, 211 
personal equation, 122 
phenylthiocarbamide, 483 
Philadelphia, 409 
Phillips Curve, 522, 524 
physics laboratory, freshman, 453 
piece parts, defective, 123 
Planck's constant, 503 
plane through the origin, 300ff 
plant breeding, selection, 29 
plots, straightening, 79ff 
plots, which!, 70 
plotting and smoothing, 75ff 
plotting, exploratory, 49ff 
for non-linear data, 52-64 
omissions, 49 
PLUS analyses, 165ff 
polish median, 180, 182, 201s 
mean, 181, 182, 183 
polishing additive fits, 178ff 
political aspects of choice, 270 
polynomial fits, 319ff 
polynomials and collinearity, 
265ff 
pool of populations, 10% point of, 
142 
population, counties, 484ff 
population, U.S., 552, 560 
populations, 20 
of counties, 484ff 
infinite, and jackknife, 159 
mean, 126 
pooled, 233 
standard, 223, 248 
and success rate, 222 
Portland, Oregon, 73 
possible fits, set of, 302ff 
posterior distributions, 22 
poverty, 323 
power curves, shapes, 80 
power of test, 3 
powers, matching for, 102ff 
powers, started, 91 
prediction, 36 
for multiple regression, 37 
regression as, 268, 270 
vs. regression coefficients, 281 
prediction sum of squares 
(PRESS), 386 
predictive techniques, 36 
predictors, 284ff 
PRESS, Allen's, 385, 404s 
previous experience, 311 
price deflator, 272 
primary statistic, 1, 21 
primes, 65ff, 87 
principal components, 212, 397, 
402, 404s 
choice of scale, 399 
probability density functions, 14, 
15 
probits, 92 
problems of multiplicity, 28, 40s 
with Student's t, 6, 22 
procedure --- form and numbers, 
37 
product terms, effective use of, 
435ff, 446 
projective test, 30 
jackknifing, 146 
two forms of, 146 
promising sets of carriers, 393 
proportions, 221ff 
Proschan, F., 492 
proxies, 531 
proxy, 317, 331s 
channeling through, 317 
distraction by, 317 
external, 328 
within the fit, 328 
proxy phenomena, 316ff 
pseudo-coefficient, 152 
pseudo-discriminants, 153 
pseudonyms, 401 
pseudo-values, 135ff, 162s 
for confidence limits, 135 
sensitivity to error, 135 
public transportation, percent us- 
ing, 49, 50, 51 
pull (ring on finger), 127 
pupils' achievement, 326 
pupil-to-pupil differences, 124 
quadratic fit, after perturbation, 
426 
quadratic form, 398 
nonnegative, 351 
quality, 
of fit and significance, 452 
of fit (rrawfit) , 450 
of fit, very good, 454, 455 
indications of, 36ff 
of performance, and stability, 
156, 159 
of performance, by cross- 
validation, 156 
quantile, 27 
Quenouille, M. H., 162, 163 
quick logs, 93 
quick re-expressions, of fractions, 
100ff 
quick (square) roots, 96ff 
break table, 96 
folded, 100-101 
started, 97, 99 
rcarriers , 449ff 
rrawfit , 450 
race relations, 311ff 
railroad returns, 43 lff 
rainfall, 326 
random normal deviates, 54 
smoothing, 55 
random, treating as if, 123 
randomized controlled trials, 221 
range, 19, 207 
confidence intervals, forms, 
142 
interquartile, 19, 207 
rank correlation coefficient, distri- 
bution of, 13 
rankits, 111 
ranks, 89 
re-expressing, 92, 108ff 
rates, 221ff 
ill-determined, 231 
standard specific, 244 
ratio estimates (sample survey), 
145ff 
raw carrier, 117, 449ff 
largest x/smallest x, 118, 450 
relation to straightened carrier, 
449 
reaction time, 121 
real data, 23s 
real tails, rarely match body, 16 
reciprocals, 97 
need for, 459, 460 
quick, 97 
started, 97, 99 
rectangular distribution, 14, 15 
red pine elasticity, 505 
Reed, J. S., 323,332 
re-expressed carrier (Xstraightened) , 
117, 449 
relation to Xra w (rc2arriers) , 450 
re-expressing, 
amounts or counts, 91, 102 
balances, 91ff 
counted fractions, 92, 100 
grades, 92, 103 
ranks, 108ff 
re-expression, 79, 418 
a class of mechanisms, 333ff 
exponential, 195ff 
first aid in, 109 
for fuzzy scatter plot, 87, 88s 
hollow upward, 195 
for inflection point, 87 
ladder of, 79ff, 80, 84, 88s 
logistic, for ordered values, 111 
matched, 102ff 
need for, 117, 449ff 
for pairs of variables, 420 
practice of, 89 
reward for, 117 
second aid in, 109 
to straighten, 81 
for two-way tables, 194, 201s 
use of signs, 113 
when profitable, 117, 449ff 
regression, 262, 320, see also re- 
gression :�efficients, step- 
wise regression 
anticipated, 403 
backwards for simplification, 
330 
a class of mechanisms, 333ff 
as exclusion, 269, 287, 296s 
586 Index 
indeterminacy of, 315 
magnitude of, 323 
as measurement, 268, 320ff 
from reduced data, 344 
relative comparison, 316 
stable, 401 
sum of, 281 
variances, 328, 335 
regression curve, 
examining residuals, 407ff 
as grand summary, 266 
pointing out high values, 65 
regression expressions, 
for fitting, 259ff 
formal definition, 264 
rewriting, 371ff 
regression function, 53, 61ff 
guided, 381 ff 
linear, resistant and robust, 209 
in median terms, 262 
in mean terms, 262 
meanings of, 262, 266, 295s 
multiple, 25, 36ff, 133 
non-linear, 61 
predictive, 269 
purposes of, 268, 296s 
re-expression for, 461 
resistant-and-robust, 210 
second meaning of, 266ff 
starting where, 402 
stepwise, 387ff, 404s 
structural, 289 
toward the mean, 262 
variables offered for, 301,331s 
with subsamples, 295 
relation of y to x, 52 
relative efficiency, 17 
reliability coefficients, 146 
remedial-reading class, 27 
Remi, W., 525 
residual variance, 335 
residuals, 52, 70, 170 
analysis of, 178 
coded, 170if, 201s 
examination, in what order, 
446 
get smaller, 345ff 
smoothed, 189ff 
spread around line, 294, 296s 
two-way, 166ff 
resistance, 203ff, 210 
and additivity, 335 
resistant and robust techniques, 
already available, 218 
resistant smoothing, 77s 
resistant stepwise regression, 391, 
405s 
response rates, 130 
responsiveness, 260 
results, lumpy looking, 58 
reward for re-expression, 117 
Rezucha, I., 295, 297 
Richards, P. W., 483 
Richmond, Va., 409 
Riley, M. W., 297 
Robbins, E. B., 505 
Robson, D. S., 162, 163 
robust and resistant measures, 
203ff 
robustness, 16, 205, 208 
and additivity, 335 
kinds of, 12ff 
of efficiency, 16, 23s, 206ff, 
218s 
of validity, 16, 145, 208, 218s 
roots, 
roots, quick (square), 97 
folded, 100-101 
need for, 459, 460 
started, 97 
Rorschach test, many scores, 30 
rotation of earth, 72 
rotations, 211 
rounding, effect on fit, 307ff 
row, 165ff 
row effects, 167ff 
RSS, largest, 387 
Rubin, D. E., 272 
rule, bulging, 84ff 
Rumford, Count B., 421 
running means, 52 
running medians, 52ff, 411 
repeated, 53 
Russo, P., 501 
Ryder, N. B., 520 
safe statements, often inadequate, 
16 
Salk vaccine, 322 
same house, percent in, 49 
sample estimate of variance, 3 
sample mean, 3 
standard deviation of, 121ff 
sample size, 3, 33, 36, 40s 
for subsampling, 77 
much larger, 33 
small, 129, 132s 
sample survey, 126 
ratio estimation in, 145 
sampled and target populations, 
mismatch, 119 
samples, interpenetrating, 127 
sampling, theory of, 121 
San Francisco, California, 73 
scale, 8 
measures of, 203ff 
robust estimates of, 207ff 
scatter diagrams, doughnut- 
shaped, 27 
scatter plots, straightening, 79, 
87, 88s 
Scheffe, H., 129, 132 
school environment, 400 
school-to-school differences, 124 
Seads, D., 511 
seasonal adjustment, 187ff 
Seattle, Washington, 71, 73 
second aid, (in re-expression), 
109 
secondary statistic, 1, 21 
selection, by stages, 29 
semidefinite, positive, 350 
SES (socioeconomic status), 326 
shape, 7if, 23, 58, 74 
Gaussian failure, 12 
gross differences in, 14 
Shaw, Sir N., 518, 519 
Shewhart, W. A., 122, 123, 132 
Shuc,ny, W. R., 162, 163 
r//n can mislead, 121ff 
sign test, 16 
significance, 19, 23s 
for concealed inference, 40s 
for multiplicity, 28, 31 
for well-designed experiment, 
40s 
individual vs. simultaneous, 29 
test of, 3, 20, 22, 25 
signs, in re-expression, 113 
similarity, appearances of, 27 
single unit, % housing in, 49 
sinusoidal fit, 440 
size, see sample size 
skeleton letter-value display, 48 
slope equation, 339 
slopes, 59 
ratio of, in straightening, 82ff 
sizes, 60 
slowness, 113 
small cells, in direct standardiza- 
tion, 230ff 
Smith, B. B., 13, 23 
Smith, H., 325, 332 
smoking and health, 552 
Canada, 559 
smoothing, 53, 54 
examples, 54ff, 61ff 
for exponential, 426 
least squares fit to, 38 
of medians, 75 
and plotting, 75 
resistant, 77s 
and shape, 58 
and turning points, 54 
South Carolina, death rates, 225ff 
specification, 21 
speed vs. time, 113 
Spokane, Washington, 73 
spread, 18, 48ff 
display (letter-value), 48 
of difficulty distribution, 248 
measures of, 17ff 
minimized, in two-way table, 
199-201 
of residuals around line, 294, 
296s 
square roots, 
folded, 100-101 
need for, 459, 460 
quick, 97 
started, 97 
stability, 38, 119 
with jackknife, 148 
and quality of performance, 156 
Stam, P. B., 496 
standard deviation, 17, 18, 27 
compared with mean deviation, 
17 
Index 587 
confidence limits for, 139 
danger of normal theory, 142 
expanded to root-mean-square, 
130 
as indication 
infinite, 19 
jackknifing, 139 
microcosmic, 122 
misleading, 121, 132s 
standard difficulty, 245 
standard errors, 
for discriminant coefficients, 
153 
inadequacy, 328, 330-331 
standard million, U.S., 513 
standard normal deviate, 92 
approximate, 235 
standard population, 223,248 
standard stepwise procedures, 388 
standardization, 223, 256s 
adjustment for broad 
categories, 240 
dangers and anomalies, 233 
direct, 224ff, see also direct 
standardidization 
indirect, 233ff, see also indirect 
standardization 
never complete, 240 
standardized comparison, 223 
standardized difference, 228ff 
standardized fertility, 517 
standardized values, 
standard errors and variances, 
228ff 
Stanford, 571ff 
started counted fractions, 102, 
114 
started logs, 83, 91, 93ff, 95, 112 
started powers, 91 
matched, 102, 104, 105 
started reciprocals, 97, 99 
started roots, 97, 99 
started zeros and infinities, 113, 
115s 
statistical independence, 260 
statistics, 
more complicated, 133ff 
order, with jackknife, 145 
primary, l, 21 
secondary, 1, 21 
staircase of, lff 
tertiary, 1 
statistics courses, 21 
steam consumption, 325 
stem-and-leaf displays, 43ff, 61 
stepweighting, 361ff, 362, 363 
stepwise fitting, resistant, 391 
stepwise regression, 387ff, 404s, 
528 
all-subsets, 391,405s 
backwards, 330, 388 
desirable supplements, 389 
forward, 388 
proof of, 374 
resistant, 391-394, 405s 
standard, 388 
Student's 
Student's 
467 
Student's 
weighted, 390 
stilbestrol feeding, 505 
stocks, 
and variables, relation, 420 
many alternative, 384 
nonlinear, 422, 448s 
sum-of-terms, 302ff 
Stone, M., 39, 41 
stopping with indications, reasons 
for, 30 
Storky problems, 318 
Stoto, M. A., 548 
straggling tails, 10, 16, 23s, 34, 
318s 
and confidence limits on stan- 
dard deviation, by jack- 
knife, 139 
excessively, 136 
long, close to infinite variance, 
19 
straight line, which?, 293 
straight lines, 
centered from, 58ff 
fitting, 58 
straightened (re-expressed) car- 
rier, 449ff 
straightening curves and plots, 
79ff 
profitability of, 449ff 
strata, see also categories 
adjustment for broad, 240, 257s 
cutting point between, 241,242 
hard and easy, 221ff 
stratified sampling, 126ff, 295 
stretched-out tails, 10, 23s, 450, 
456 
efficiency for, 206 
structural regression, 289 
Strugala, E. S., 490 
Stuart, A., 162, 163 
Student, 1, 2, 3, 4if, 24 
student reaction to world news, 
129 
shortcut, 467 
t, 2, 4, 5, 16, 126, 135, 
t, failing safe, 16 
Student's true contribution, 4ff 
subsample, 48, 77, 132s 
for regression, 295, 296s 
interpenetrating, 127 
systematic, 126 
success rates, 243 
crude, 221ff 
directly standardized, 225ff 
and population, 222 
suicide, 552 
sum of squares of x's, 345, 369, 
372, 377s 
summaries, for batches, 43ff 
summary values, population, 20, 
23s 
summary values, sample, 20, 23s 
sun's atmosphere, rotation of, 
389 
sunspots, number of, 518 
supplementary uncertainty, 119, 
129. 132s 
different target and sample po- 
pulations, 119 
halo effect, 119 
plus internal uncertainty, 129, 
132s 
systematic errors, 119 
surgical deaths, 515ff 
survey, 
ratio estimate in, 145 
sample, 126 
Sutherland, M., 230, 257 
Switzerland, 549 
systematic errors of measure- 
ments, 119 
systematic subsamples, 126 
systematic variations, 373 
t-distribution, 2, 4, 16 
two-sided 5% level, 125, 126 
t-table, 127 
the w, 431 
told, 423 
tol d , how often represented?, 424 
tails, 14 
changes in size, 15 
see also straggling tails 
neutral, 450, 456 
normal, 450, 456 
squeezed, 450, 456 
straggling, 10, 18, 23s, 34, 318s 
stretched-out, 10, 23s, 206, 
450, 456 
take out variable, 269, 272, 298, 
303, 336, 374, 425, 435 
telephones/city, 45 
temperatures, 
antarctic, 497 
east coast places, 167, 168 
extreme, 478 
maximum January, 71, 72-74 
re-expressed, 180 
ten-in-a-box, 212, 214, 216 
terms, 169 
tertiary statistic, 1 
tests, 
of hypothesis, 3, 20, 22, 25 
of significance, 3, 20, 22, 25 
sign, 16 
thermodynamics, foundation of, 
421 
Thissen, D., 162, 163 
three points, two slopes, 82 
three-way analyses, 200ff 
tides, Honolulu high, 437, 438 
time series, 53 
transformations, affine, 211 
trends, 27, 52ff, 64 
trials, randomized controlled, 221 
triangular distribution, 14, 15 
trimmed mean, 34 
estimand of, 36 
formalization, 36 
jackknifing, 35 
for normal distribution, 35 
588 Index 
relative efficiency, 35 
troubles in regression, diagnosis 
of, 369ff 
Trussel, T. James, 548 
Tsao, F., 127, 128, 132 
Tufte, E. R., 489 
Tukey, J. W., 17, 24, 41, 43, 70, 
77, 112, 115, 135, 162, 163, 
165, 202, 257, 297, 321, 
322,, 329, 332, 385, 386, 
406 
tuning out, 34011' 
turning points, 54 
two-factor descriptions, 167ff 
two-way analysis, 167ff 
additive and multiplicative 
fitting in, 193ff 
fitting one more constant for, 
199 
holes, 185, 186, 201s 
by means, 181 
by medians, 179, 180 
outliers from, 183, 201s 
re-expression in, 194, 201s 
unusual values in, 183 
two-way (matrix) weight, 349, 
378s 
two-way plots, I75ff 
two-way tables, 165ff 
typical values, 21 
unbiasedness, 381 
uncertainty, 20, 23s, 25, 27, 119 
assessment of, 21 
assessment of supplementary, 
owed, 131 
certainty about, 25 
combination of two kinds of, 
130ff 
direct assessment of, 119 
internal, 119, 131s 
in a neat package, 22 
overall, 131 
of small samples, allowance for, 
5 
supplementary, 119, 132s, see 
also supplementary uncer- 
tainty 
treatment of, 27 
two sources of, 119 
uncomfortable, 25 
uniform distribution, 286 
unincorporated urban places, 49, 
50, 75, 76, 481,534 
university admission, 270 
unweighting, 347 
urban places, unincorporated, 49, 
50 75, 76, 481,534 
U.S. cities, population of, 474 
U.S., population, 560ff 
U.S. railroads, 431 
Utopia, 33 
vague concepts, lff, 469 
other, 19ff 
role of, 17ff 
vague toward the particular, 17 
validity, 16 
robustness of, 16, 145 
values, 
far out, 70 
outside, 70 
van de Walle, F., 549ff 
variability, as a function of level, 
127 
two internal sources, 129 
variables, 
closely correlated, 316, 327 
coefficient of old, 431 
composite, 327 
currently popular, 421 
dependent, 262 
eliminate, 425,435 
held constant, 318, 331s 
independent, 262 
instrumental, 288 
latent, 287 
linear constraints on, 312ff 
naturalness of, 422 
new, (tnew), 431,448s 
offered for regression, 301, 
331s 
old, (to/d), 423 
powers of, 79ff 
re-expressing pairs of, 420 
which special, 371ff 
subsets of, 300ff 
take out, 269, 272, 287, 303, 
336, 374 
variance, 17, 33 
analog of, 209 
analysis of, 25 
for direct standardization, 224, 
227-229, 232 
for indirect standardization, 235 
infinite, 19, 33 
judged measurement, 399 
large, 369 
population and sample, 18 
for regression coefficients, 328, 
331s, 335,345, 369 
residual, 335 
resistant and robust, 209 
(zero), 230 
variation, 119 
binomial, 122ff 
treatment of, 27 
Velleman, P. F., 53, 77 
Vernon, P. E., 284, 296 
vital statistics, 221ff 
Vital Statistics Rates in the United 
States,, 226, 227 
vitamin C intake, 501 
Wainer, H., 162, 163 
Walker, C. B., 559 
Wallace, D. L., 38, 41, 149, 150, 
163, 514 
Ward, F., 38, 40, 389, 406, 543 
warfare, ancient, 545, 549, 552, 573 
Washington, D.C., temperatures, 
165 
Watkins, T. A., 162, 163 
weather, 325, 528, 544ff, 564ff 
Boston, 56411' 
wedge shapes, 64, 65 
weighted least squares, 346 
weighted regression, reduced 
form, 348 
weights, 346, 378s 
flattened, 368, 379s 
on weights, 365, 369 
two kinds, 357 
two-way (matrix), 349, 379s 
Weinfeld, F. D., 326, 332 
Westoff, C. F., 520 
wheat yield, 186 
White, H. J., 496 
whites and blacks, comparison, 28 
Whitlock, J. H., 162, 163 
wide-receivers, 320, 395, 491 
wild shots, 16 
Wilson, E. B., 10, 11, 24, 122, 
468 
Wilson, R. W., Jr., 392, 405, 406 
Winsor, C. P., 12 
principle, 12 
Wishart, J., 24 
within-day variation, 122 
woes of regression coefficients, 
299ff 
Wonnacott, R. J., 326, 332 
Wonnacott, T. H., 326, 332 
Wood, C. H., 499 
Wood, F. S., 392, 405, 406 
Woodworth, G. G., 238,239 
Woolsey, T. D., 225-227, 233 
word frequency, 514 
word lengths, 514 
words, 
high frequency, 149ff 
of warning, 242 
world news, student reaction to, 
129 
worsening of fit, 449 
Wright, C. R., 323, 332 
Xdo t, 425, 434ff 
usefulness, 431, 434 
Y - :9 versus y, misleading, 407 
Yates, F., 29, 38, 41 
yield of wheat, 326 
York, R. L., 326, 332 
Youden, W. J., 497 
zero counts, and standard errors, 
230 
zeros, 
and infinities, 113-115 
what to do with, 112ff 
Zuoz, Switzerland, 504 
