\pdfoutput=1
%% Author: PGL  Porta Mana
%% Created: 2019-08-08T08:04:04+0200
%% Last-Updated: 2020-05-19T21:35:18+0200
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newif\ifarxiv
\arxivfalse
\ifarxiv\pdfmapfile{+classico.map}\fi
\newif\ifafour
\afourfalse% true = A4, false = A5
\newif\iftypodisclaim % typographical disclaim on the side
\typodisclaimtrue
\newcommand*{\memfontfamily}{zplx}
\newcommand*{\memfontpack}{newpxtext}
\documentclass[\ifafour a4paper,12pt,\else a5paper,10pt,\fi%extrafontsizes,%
onecolumn,oneside,article,%french,italian,german,swedish,latin,
british%
]{memoir}
\newcommand*{\firstdraft}{8 August 2019}
\newcommand*{\firstpublished}{18 August 2019}
\newcommand*{\updated}{\today}
\newcommand*{\propertitle}{Models and hypotheses, log-likelihoods,\\ and cross-validation log-scores%\\{\large}%
}% title uses LARGE; set Large for smaller
\newcommand*{\pdftitle}{Models and hypotheses, log-likelihoods, and cross-validation log-scores}
\newcommand*{\headtitle}{Hypotheses, log-likelihood, cross-validation}
\newcommand*{\pdfauthor}{P.G.L.  Porta Mana}
\newcommand*{\headauthor}{Porta Mana}
\newcommand*{\reporthead}{\ifarxiv\else Open Science Framework \href{https://doi.org/10.31219/osf.io/k8mj3}{\textsc{doi}:10.31219/osf.io/k8mj3}\fi}% Report number

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Calls to packages (uncomment as needed)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\usepackage{pifont}

%\usepackage{fontawesome}

\usepackage[T1]{fontenc} 
\input{glyphtounicode} \pdfgentounicode=1

\usepackage[utf8]{inputenx}

%\usepackage{newunicodechar}
% \newunicodechar{Ĕ}{\u{E}}
% \newunicodechar{ĕ}{\u{e}}
% \newunicodechar{Ĭ}{\u{I}}
% \newunicodechar{ĭ}{\u{\i}}
% \newunicodechar{Ŏ}{\u{O}}
% \newunicodechar{ŏ}{\u{o}}
% \newunicodechar{Ŭ}{\u{U}}
% \newunicodechar{ŭ}{\u{u}}
% \newunicodechar{Ā}{\=A}
% \newunicodechar{ā}{\=a}
% \newunicodechar{Ē}{\=E}
% \newunicodechar{ē}{\=e}
% \newunicodechar{Ī}{\=I}
% \newunicodechar{ī}{\={\i}}
% \newunicodechar{Ō}{\=O}
% \newunicodechar{ō}{\=o}
% \newunicodechar{Ū}{\=U}
% \newunicodechar{ū}{\=u}
% \newunicodechar{Ȳ}{\=Y}
% \newunicodechar{ȳ}{\=y}

\newcommand*{\bmmax}{0} % reduce number of bold fonts, before font packages
\newcommand*{\hmmax}{0} % reduce number of heavy fonts, before font packages

\usepackage{textcomp}

%\usepackage[normalem]{ulem}% package for underlining
% \makeatletter
% \def\ssout{\bgroup \ULdepth=-.35ex%\UL@setULdepth
%  \markoverwith{\lower\ULdepth\hbox
%    {\kern-.03em\vbox{\hrule width.2em\kern1.2\p@\hrule}\kern-.03em}}%
%  \ULon}
% \makeatother

\usepackage{amsmath}

\usepackage{mathtools}
\addtolength{\jot}{\jot} % increase spacing in multiline formulae
\setlength{\multlinegap}{0pt}

\usepackage{empheq}% automatically calls amsmath and mathtools
%\newcommand*{\widefbox}[1]{\fbox{\hspace{1ex}#1\hspace{1ex}}}

%\usepackage{fancybox}

%\usepackage{mdframed}

% \usepackage[misc]{ifsym} % for dice
% \newcommand*{\diceone}{{\scriptsize\Cube{1}}}

\usepackage{amssymb}

\usepackage{amsxtra}

\usepackage[main=british,french,italian,german,swedish,latin,esperanto]{babel}\selectlanguage{british}
\newcommand*{\langfrench}{\foreignlanguage{french}}
\newcommand*{\langgerman}{\foreignlanguage{german}}
\newcommand*{\langitalian}{\foreignlanguage{italian}}
\newcommand*{\langswedish}{\foreignlanguage{swedish}}
\newcommand*{\langlatin}{\foreignlanguage{latin}}
\newcommand*{\langnohyph}{\foreignlanguage{nohyphenation}}

\usepackage[autostyle=false,autopunct=false,english=british]{csquotes}
\setquotestyle{british}

\usepackage{amsthm}
\newcommand*{\QED}{\textsc{q.e.d.}}
\renewcommand*{\qedsymbol}{\QED}
\theoremstyle{remark}
\newtheorem{note}{Note}
\newtheorem*{remark}{Note}
\newtheoremstyle{innote}{\parsep}{\parsep}{\footnotesize}{}{}{}{0pt}{}
\theoremstyle{innote}
\newtheorem*{innote}{}

\usepackage[shortlabels,inline]{enumitem}
\SetEnumitemKey{para}{itemindent=\parindent,leftmargin=0pt,listparindent=\parindent,parsep=0pt,itemsep=\topsep}
% \begin{asparaenum} = \begin{enumerate}[para]
% \begin{inparaenum} = \begin{enumerate*}
\setlist[enumerate,2]{label=\alph*.}
\setlist[enumerate]{label=\arabic*.,leftmargin=1.5\parindent}
\setlist[itemize]{leftmargin=1.5\parindent}
\setlist[description]{leftmargin=1.5\parindent}
% old alternative:
% \setlist[enumerate,2]{label=\alph*.}
% \setlist[enumerate]{leftmargin=\parindent}
% \setlist[itemize]{leftmargin=\parindent}
% \setlist[description]{leftmargin=\parindent}

\usepackage[babel,theoremfont,largesc]{newpxtext}

\usepackage[bigdelims,nosymbolsc%,smallerops % probably arXiv doesn't have it
]{newpxmath}
\linespread{1.083}%\useosf
%% smaller operators for old version of newpxmath
\makeatletter
\def\re@DeclareMathSymbol#1#2#3#4{%
    \let#1=\undefined
    \DeclareMathSymbol{#1}{#2}{#3}{#4}}
%\re@DeclareMathSymbol{\bigsqcupop}{\mathop}{largesymbols}{"46}
%\re@DeclareMathSymbol{\bigodotop}{\mathop}{largesymbols}{"4A}
\re@DeclareMathSymbol{\bigoplusop}{\mathop}{largesymbols}{"4C}
\re@DeclareMathSymbol{\bigotimesop}{\mathop}{largesymbols}{"4E}
\re@DeclareMathSymbol{\sumop}{\mathop}{largesymbols}{"50}
\re@DeclareMathSymbol{\prodop}{\mathop}{largesymbols}{"51}
\re@DeclareMathSymbol{\bigcupop}{\mathop}{largesymbols}{"53}
\re@DeclareMathSymbol{\bigcapop}{\mathop}{largesymbols}{"54}
%\re@DeclareMathSymbol{\biguplusop}{\mathop}{largesymbols}{"55}
\re@DeclareMathSymbol{\bigwedgeop}{\mathop}{largesymbols}{"56}
\re@DeclareMathSymbol{\bigveeop}{\mathop}{largesymbols}{"57}
%\re@DeclareMathSymbol{\bigcupdotop}{\mathop}{largesymbols}{"DF}
%\re@DeclareMathSymbol{\bigcapplusop}{\mathop}{largesymbolsPXA}{"00}
%\re@DeclareMathSymbol{\bigsqcupplusop}{\mathop}{largesymbolsPXA}{"02}
%\re@DeclareMathSymbol{\bigsqcapplusop}{\mathop}{largesymbolsPXA}{"04}
%\re@DeclareMathSymbol{\bigsqcapop}{\mathop}{largesymbolsPXA}{"06}
\re@DeclareMathSymbol{\bigtimesop}{\mathop}{largesymbolsPXA}{"10}
%\re@DeclareMathSymbol{\coprodop}{\mathop}{largesymbols}{"60}
%\re@DeclareMathSymbol{\varprod}{\mathop}{largesymbolsPXA}{16}
\makeatother
%%
%% With euler font cursive for Greek letters - the [1] means 100% scaling
\DeclareFontFamily{U}{egreek}{\skewchar\font'177}%
\DeclareFontShape{U}{egreek}{m}{n}{<-6>s*[1]eurm5 <6-8>s*[1]eurm7 <8->s*[1]eurm10}{}%
\DeclareFontShape{U}{egreek}{m}{it}{<->s*[1]eurmo10}{}%
\DeclareFontShape{U}{egreek}{b}{n}{<-6>s*[1]eurb5 <6-8>s*[1]eurb7 <8->s*[1]eurb10}{}%
\DeclareFontShape{U}{egreek}{b}{it}{<->s*[1]eurbo10}{}%
\DeclareSymbolFont{egreeki}{U}{egreek}{m}{it}%
\SetSymbolFont{egreeki}{bold}{U}{egreek}{b}{it}% from the amsfonts package
\DeclareSymbolFont{egreekr}{U}{egreek}{m}{n}%
\SetSymbolFont{egreekr}{bold}{U}{egreek}{b}{n}% from the amsfonts package
% Take also \sum, \prod, \coprod symbols from Euler fonts
\DeclareFontFamily{U}{egreekx}{\skewchar\font'177}
\DeclareFontShape{U}{egreekx}{m}{n}{%
       <-7.5>s*[0.9]euex7%
    <7.5-8.5>s*[0.9]euex8%
    <8.5-9.5>s*[0.9]euex9%
    <9.5->s*[0.9]euex10%
}{}
\DeclareSymbolFont{egreekx}{U}{egreekx}{m}{n}
\DeclareMathSymbol{\sumop}{\mathop}{egreekx}{"50}
\DeclareMathSymbol{\prodop}{\mathop}{egreekx}{"51}
\DeclareMathSymbol{\coprodop}{\mathop}{egreekx}{"60}
\makeatletter
\def\sum{\DOTSI\sumop\slimits@}
\def\prod{\DOTSI\prodop\slimits@}
\def\coprod{\DOTSI\coprodop\slimits@}
\makeatother
\input{definegreek.tex}% Greek letters not usually given in LaTeX.

%\usepackage%[scaled=0.9]%
%{classico}%  Optima as sans-serif font
\renewcommand\sfdefault{uop}
\DeclareMathAlphabet{\mathsf}  {T1}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsf}{bold}{T1}{\sfdefault}{b}{sl}
%\newcommand*{\mathte}[1]{\textbf{\textit{\textsf{#1}}}}
% Upright sans-serif math alphabet
% \DeclareMathAlphabet{\mathsu}  {T1}{\sfdefault}{m}{n}
% \SetMathAlphabet{\mathsu}{bold}{T1}{\sfdefault}{b}{n}

% DejaVu Mono as typewriter text
\usepackage[scaled=0.84]{DejaVuSansMono}

\usepackage{mathdots}

\usepackage[usenames]{xcolor}
% Tol (2012) colour-blind-, print-, screen-friendly colours, alternative scheme; Munsell terminology
\definecolor{mypurpleblue}{RGB}{68,119,170}
\definecolor{myblue}{RGB}{102,204,238}
\definecolor{mygreen}{RGB}{34,136,51}
\definecolor{myyellow}{RGB}{204,187,68}
\definecolor{myred}{RGB}{238,102,119}
\definecolor{myredpurple}{RGB}{170,51,119}
\definecolor{mygrey}{RGB}{187,187,187}
% Tol (2012) colour-blind-, print-, screen-friendly colours; Munsell terminology
% \definecolor{lbpurple}{RGB}{51,34,136}
% \definecolor{lblue}{RGB}{136,204,238}
% \definecolor{lbgreen}{RGB}{68,170,153}
% \definecolor{lgreen}{RGB}{17,119,51}
% \definecolor{lgyellow}{RGB}{153,153,51}
% \definecolor{lyellow}{RGB}{221,204,119}
% \definecolor{lred}{RGB}{204,102,119}
% \definecolor{lpred}{RGB}{136,34,85}
% \definecolor{lrpurple}{RGB}{170,68,153}
\definecolor{lgrey}{RGB}{221,221,221}
%\newcommand*\mycolourbox[1]{%
%\colorbox{mygrey}{\hspace{1em}#1\hspace{1em}}}
\colorlet{shadecolor}{lgrey}

\usepackage{bm}

\usepackage{microtype}

\usepackage[backend=biber,mcite,%subentry,
citestyle=authoryear-comp,bibstyle=pglpm-authoryear,autopunct=false,sorting=ny,sortcites=false,natbib=false,maxcitenames=2,maxbibnames=8,minbibnames=8,giveninits=true,uniquename=false,uniquelist=false,maxalphanames=1,block=space,hyperref=true,defernumbers=false,useprefix=true,sortupper=false,language=british,parentracker=false]{biblatex}
\DeclareSortingScheme{ny}{\sort{\field{sortname}\field{author}\field{editor}}\sort{\field{year}}}
\iffalse\makeatletter%%% replace parenthesis with brackets
\newrobustcmd*{\parentexttrack}[1]{%
  \begingroup
  \blx@blxinit
  \blx@setsfcodes
  \blx@bibopenparen#1\blx@bibcloseparen
  \endgroup}
\AtEveryCite{%
  \let\parentext=\parentexttrack%
  \let\bibopenparen=\bibopenbracket%
  \let\bibcloseparen=\bibclosebracket}
\makeatother\fi
\DefineBibliographyExtras{british}{\def\finalandcomma{\addcomma}}
\renewcommand*{\finalnamedelim}{\addspace\amp\space}
%\renewcommand*{\finalnamedelim}{\addcomma\space}
\setcounter{biburlnumpenalty}{1}
\setcounter{biburlucpenalty}{0}
\setcounter{biburllcpenalty}{1}
\DeclareDelimFormat{multicitedelim}{\addsemicolon\addspace\space}
\DeclareDelimFormat{compcitedelim}{\addsemicolon\addspace\space}
\DeclareDelimFormat{postnotedelim}{\addspace}
\ifarxiv\else\addbibresource{portamanabib.bib}\fi
\renewcommand{\bibfont}{\footnotesize}
%\appto{\citesetup}{\footnotesize}% smaller font for citations
\defbibheading{bibliography}[\bibname]{\section*{#1}\addcontentsline{toc}{section}{#1}%\markboth{#1}{#1}
}
\newcommand*{\citep}{\footcites}
\newcommand*{\citey}{\footcites}%{\parencites*}
\newcommand*{\ibid}{\unspace\addtocounter{footnote}{-1}\footnotemark{}}
%\renewcommand*{\cite}{\parencite}
%\renewcommand*{\cites}{\parencites}
\providecommand{\href}[2]{#2}
\providecommand{\eprint}[2]{\texttt{\href{#1}{#2}}}
\newcommand*{\amp}{\&}
% \newcommand*{\citein}[2][]{\textnormal{\textcite[#1]{#2}}%\addtocategory{extras}{#2}
% }
\newcommand*{\citein}[2][]{\textnormal{\textcite[#1]{#2}}%\addtocategory{extras}{#2}
}
\newcommand*{\citebi}[2][]{\textcite[#1]{#2}%\addtocategory{extras}{#2}
}
\newcommand*{\subtitleproc}[1]{}
\newcommand*{\chapb}{ch.}
%
% \def\arxivp{}
% \def\mparcp{}
% \def\philscip{}
% \def\biorxivp{}
% \newcommand*{\arxivsi}{\texttt{arXiv} eprints available at \url{http://arxiv.org/}.\\}
% \newcommand*{\mparcsi}{\texttt{mp\_arc} eprints available at \url{http://www.ma.utexas.edu/mp_arc/}.\\}
% \newcommand*{\philscisi}{\texttt{philsci} eprints available at \url{http://philsci-archive.pitt.edu/}.\\}
% \newcommand*{\biorxivsi}{\texttt{bioRxiv} eprints available at \url{http://biorxiv.org/}.\\}
\newcommand*{\arxiveprint}[1]{%\global\def\arxivp{\arxivsi}%\citeauthor{0arxivcite}\addtocategory{ifarchcit}{0arxivcite}%eprint
\texttt{\urlalt{https://arxiv.org/abs/#1}{arXiv:\hspace{0pt}#1}}%
%\texttt{\href{http://arxiv.org/abs/#1}{\protect\url{arXiv:#1}}}%
%\renewcommand{\arxivnote}{\texttt{arXiv} eprints available at \url{http://arxiv.org/}.}
}
\newcommand*{\haleprint}[1]{%\global\def\arxivp{\arxivsi}%\citeauthor{0arxivcite}\addtocategory{ifarchcit}{0arxivcite}%eprint
\texttt{\urlalt{https://hal.archives-ouvertes.fr/#1}{HAL:\hspace{0pt}#1}}%
%\texttt{\href{http://arxiv.org/abs/#1}{\protect\url{arXiv:#1}}}%
%\renewcommand{\arxivnote}{\texttt{arXiv} eprints available at \url{http://arxiv.org/}.}
}
\newcommand*{\mparceprint}[1]{%\global\def\mparcp{\mparcsi}%\citeauthor{0mparccite}\addtocategory{ifarchcit}{0mparccite}%eprint
\texttt{\urlalt{http://www.ma.utexas.edu/mp_arc-bin/mpa?yn=#1}{mp\_arc:\hspace{0pt}#1}}%
%\texttt{\href{http://www.ma.utexas.edu/mp_arc-bin/mpa?yn=#1}{\protect\url{mp_arc:#1}}}%
%\providecommand{\mparcnote}{\texttt{mp_arc} eprints available at \url{http://www.ma.utexas.edu/mp_arc/}.}
}
\newcommand*{\philscieprint}[1]{%\global\def\philscip{\philscisi}%\citeauthor{0philscicite}\addtocategory{ifarchcit}{0philscicite}%eprint
\texttt{\urlalt{http://philsci-archive.pitt.edu/archive/#1}{PhilSci:\hspace{0pt}#1}}%
%\texttt{\href{http://philsci-archive.pitt.edu/archive/#1}{\protect\url{PhilSci:#1}}}%
%\providecommand{\mparcnote}{\texttt{philsci} eprints available at \url{http://philsci-archive.pitt.edu/}.}
}
\newcommand*{\biorxiveprint}[1]{%\global\def\biorxivp{\biorxivsi}%\citeauthor{0arxivcite}\addtocategory{ifarchcit}{0arxivcite}%eprint
\texttt{\urlalt{https://doi.org/10.1101/#1}{bioRxiv doi:\hspace{0pt}10.1101/#1}}%
%\texttt{\href{http://arxiv.org/abs/#1}{\protect\url{arXiv:#1}}}%
%\renewcommand{\arxivnote}{\texttt{arXiv} eprints available at \url{http://arxiv.org/}.}
}
\newcommand*{\osfeprint}[1]{%
\texttt{\urlalt{https://doi.org/10.17605/osf.io/#1}{Open Science Framework doi:10.17605/osf.io/#1}}%
}

\usepackage{graphicx}

%\usepackage{wrapfig}

%\usepackage{tikz-cd}

\PassOptionsToPackage{hyphens}{url}\usepackage[hypertexnames=false]{hyperref}

\usepackage[depth=4]{bookmark}
\hypersetup{colorlinks=true,bookmarksnumbered,pdfborder={0 0 0.25},citebordercolor={0.2667 0.4667 0.6667},citecolor=mypurpleblue,linkbordercolor={0.6667 0.2 0.4667},linkcolor=myredpurple,urlbordercolor={0.1333 0.5333 0.2},urlcolor=mygreen,breaklinks=true,pdftitle={\pdftitle},pdfauthor={\pdfauthor}}
% \usepackage[vertfit=local]{breakurl}% only for arXiv
\providecommand*{\urlalt}{\href}

\usepackage[british]{datetime2}
\DTMnewdatestyle{mydate}%
{% definitions
\renewcommand*{\DTMdisplaydate}[4]{%
\number##3\ \DTMenglishmonthname{##2} ##1}%
\renewcommand*{\DTMDisplaydate}{\DTMdisplaydate}%
}
\DTMsetdatestyle{mydate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Layout. I do not know on which kind of paper the reader will print the
%%% paper on (A4? letter? one-sided? double-sided?). So I choose A5, which
%%% provides a good layout for reading on screen and save paper if printed
%%% two pages per sheet. Average length line is 66 characters and page
%%% numbers are centred.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\ifafour\setstocksize{297mm}{210mm}%{*}% A4
\else\setstocksize{210mm}{5.5in}%{*}% 210x139.7
\fi
\settrimmedsize{\stockheight}{\stockwidth}{*}
\setlxvchars[\normalfont] %313.3632pt for a 66-characters line
\setxlvchars[\normalfont]
\setlength{\trimtop}{0pt}
\setlength{\trimedge}{\stockwidth}
\addtolength{\trimedge}{-\paperwidth}
% The length of the normalsize alphabet is 133.05988pt - 10 pt = 26.1408pc
% The length of the normalsize alphabet is 159.6719pt - 12pt = 30.3586pc
% Bringhurst gives 32pc as boundary optimal with 69 ch per line
% The length of the normalsize alphabet is 191.60612pt - 14pt = 35.8634pc
\ifafour\settypeblocksize{*}{32pc}{1.618} % A4
%\setulmargins{*}{*}{1.667}%gives 5/3 margins % 2 or 1.667
\else\settypeblocksize{*}{26pc}{1.618}% nearer to a 66-line newpx and preserves GR
\fi
\setulmargins{*}{*}{1}%gives equal margins
\setlrmargins{*}{*}{*}
\setheadfoot{\onelineskip}{2.5\onelineskip}
\setheaderspaces{*}{2\onelineskip}{*}
\setmarginnotes{2ex}{10mm}{0pt}
\checkandfixthelayout[nearest]
\fixpdflayout
%%% End layout
%% this fixes missing white spaces
\pdfmapline{+dummy-space <dummy-space.pfb}\pdfinterwordspaceon%

%%% Sectioning
\newcommand*{\asudedication}[1]{%
{\par\centering\textit{#1}\par}}
\newenvironment{acknowledgements}{\section*{Thanks}\addcontentsline{toc}{section}{Thanks}}{\par}
\makeatletter\renewcommand{\appendix}{\par
  \bigskip{\centering
   \interlinepenalty \@M
   \normalfont
   \printchaptertitle{\sffamily\appendixpagename}\par}
  \setcounter{section}{0}%
  \gdef\@chapapp{\appendixname}%
  \gdef\thesection{\@Alph\c@section}%
  \anappendixtrue}\makeatother
\counterwithout{section}{chapter}
\setsecnumformat{\upshape\csname the#1\endcsname\quad}
\setsecheadstyle{\large\bfseries\sffamily%
\centering}
\setsubsecheadstyle{\bfseries\sffamily%
\raggedright}
%\setbeforesecskip{-1.5ex plus 1ex minus .2ex}% plus 1ex minus .2ex}
%\setaftersecskip{1.3ex plus .2ex }% plus 1ex minus .2ex}
%\setsubsubsecheadstyle{\bfseries\sffamily\slshape\raggedright}
%\setbeforesubsecskip{1.25ex plus 1ex minus .2ex }% plus 1ex minus .2ex}
%\setaftersubsecskip{-1em}%{-0.5ex plus .2ex}% plus 1ex minus .2ex}
\setsubsecindent{0pt}%0ex plus 1ex minus .2ex}
\setparaheadstyle{\bfseries\sffamily%
\raggedright}
\setcounter{secnumdepth}{2}
\setlength{\headwidth}{\textwidth}
\newcommand{\addchap}[1]{\chapter*[#1]{#1}\addcontentsline{toc}{chapter}{#1}}
\newcommand{\addsec}[1]{\section*{#1}\addcontentsline{toc}{section}{#1}}
\newcommand{\addsubsec}[1]{\subsection*{#1}\addcontentsline{toc}{subsection}{#1}}
\newcommand{\addpara}[1]{\paragraph*{#1.}\addcontentsline{toc}{subsubsection}{#1}}
\newcommand{\addparap}[1]{\paragraph*{#1}\addcontentsline{toc}{subsubsection}{#1}}

%%% Headers, footers, pagestyle
\copypagestyle{manaart}{plain}
\makeheadrule{manaart}{\headwidth}{0.5\normalrulethickness}
\makeoddhead{manaart}{%
{\footnotesize%\sffamily%
\scshape\headauthor}}{}{{\footnotesize\sffamily%
\headtitle}}
\makeoddfoot{manaart}{}{\thepage}{}
\newcommand*\autanet{\includegraphics[height=\heightof{M}]{autanet.pdf}}
\definecolor{mygray}{gray}{0.333}
\iftypodisclaim%
\ifafour\newcommand\addprintnote{\begin{picture}(0,0)%
\put(245,149){\makebox(0,0){\rotatebox{90}{\tiny\color{mygray}\textsf{This
            document is designed for screen reading and
            two-up printing on A4 or Letter paper}}}}%
\end{picture}}% A4
\else\newcommand\addprintnote{\begin{picture}(0,0)%
\put(176,112){\makebox(0,0){\rotatebox{90}{\tiny\color{mygray}\textsf{This
            document is designed for screen reading and
            two-up printing on A4 or Letter paper}}}}%
\end{picture}}\fi%afourtrue
\makeoddfoot{plain}{}{\makebox[0pt]{\thepage}\addprintnote}{}
\else
\makeoddfoot{plain}{}{\makebox[0pt]{\thepage}}{}
\fi%typodisclaimtrue
\makeoddhead{plain}{\scriptsize\reporthead}{}{}
% \copypagestyle{manainitial}{plain}
% \makeheadrule{manainitial}{\headwidth}{0.5\normalrulethickness}
% \makeoddhead{manainitial}{%
% \footnotesize\sffamily%
% \scshape\headauthor}{}{\footnotesize\sffamily%
% \headtitle}
% \makeoddfoot{manaart}{}{\thepage}{}

\pagestyle{manaart}

\setlength{\droptitle}{-3.9\onelineskip}
\pretitle{\begin{center}\LARGE\sffamily%
\bfseries}
\posttitle{\bigskip\end{center}}

\makeatletter\newcommand*{\atf}{\includegraphics[%trim=1pt 1pt 0pt 0pt,
totalheight=\heightof{@}]{atblack.png}}\makeatother
\providecommand{\affiliation}[1]{\textsl{\textsf{\footnotesize #1}}}
\providecommand{\epost}[1]{\texttt{\footnotesize\textless#1\textgreater}}
\providecommand{\email}[2]{\href{mailto:#1ZZ@#2 ((remove ZZ))}{#1\protect\atf#2}}

\preauthor{\vspace{-0.5\baselineskip}\begin{center}
\normalsize\sffamily%
\lineskip  0.5em}
\postauthor{\par\end{center}}
\predate{\DTMsetdatestyle{mydate}\begin{center}\footnotesize}
\postdate{\end{center}\vspace{-\medskipamount}}

\setfloatadjustment{figure}{\footnotesize}
\captiondelim{\quad}
\captionnamefont{\footnotesize\sffamily%
}
\captiontitlefont{\footnotesize}
\firmlists*
\midsloppy
% handling orphan/widow lines, memman.pdf
% \clubpenalty=10000
% \widowpenalty=10000
% \raggedbottom
% Downes, memman.pdf
\clubpenalty=9996
\widowpenalty=9999
\brokenpenalty=4991
\predisplaypenalty=10000
\postdisplaypenalty=1549
\displaywidowpenalty=1602
\raggedbottom

\paragraphfootnotes
% \threecolumnfootnotes
\setlength{\footmarkwidth}{0em}
%\setlength{\footmarksep}{0em}
\footmarkstyle{\textsuperscript{\color{myred}\bfseries#1}~}
%\footmarkstyle{\textsuperscript{[#1]}~}

\selectlanguage{british}\frenchspacing

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Paper's details
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{\propertitle}
\author{%
\hspace*{\stretch{1}}%
%% uncomment if additional authors present
% \parbox{0.5\linewidth}%\makebox[0pt][c]%
% {\protect\centering ***\\%
% \footnotesize\epost{\email{***}{***}}}%
% \hspace*{\stretch{1}}%
\parbox{0.75\linewidth}%\makebox[0pt][c]%
{\protect\centering P.G.L.  Porta Mana \href{https://orcid.org/0000-0002-6070-0784}{\protect\includegraphics[scale=0.16]{orcid_32x32.png}}\\%
\footnotesize Kavli Institute, Trondheim\quad\epost{\email{pgl}{portamana.org}}}%
\hspace*{\stretch{1}}%
%\quad\href{https://orcid.org/0000-0002-6070-0784}{\protect\includegraphics[scale=0.16]{orcid_32x32.png}\textsc{orcid}:0000-0002-6070-0784}%
}

%\date{Draft of \today\ (first drafted \firstdraft)}
\date{\firstpublished; updated \updated}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Macros @@@
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Common ones - uncomment as needed
%\providecommand{\nequiv}{\not\equiv}
%\providecommand{\coloneqq}{\mathrel{\mathop:}=}
%\providecommand{\eqqcolon}{=\mathrel{\mathop:}}
%\providecommand{\varprod}{\prod}
\newcommand*{\de}{\partialup}%partial diff
\newcommand*{\pu}{\piup}%constant pi
\newcommand*{\delt}{\deltaup}%Kronecker, Dirac
%\newcommand*{\eps}{\varepsilonup}%Levi-Civita, Heaviside
%\newcommand*{\riem}{\zetaup}%Riemann zeta
%\providecommand{\degree}{\textdegree}% degree
%\newcommand*{\celsius}{\textcelsius}% degree Celsius
%\newcommand*{\micro}{\textmu}% degree Celsius
\newcommand*{\I}{\mathrm{i}}%imaginary unit
\newcommand*{\e}{\mathrm{e}}%Neper
\newcommand*{\di}{\mathrm{d}}%differential
%\newcommand*{\Di}{\mathrm{D}}%capital differential
%\newcommand*{\planckc}{\hslash}
%\newcommand*{\avogn}{N_{\textrm{A}}}
%\newcommand*{\NN}{\bm{\mathrm{N}}}
%\newcommand*{\ZZ}{\bm{\mathrm{Z}}}
%\newcommand*{\QQ}{\bm{\mathrm{Q}}}
\newcommand*{\RR}{\bm{\mathrm{R}}}
%\newcommand*{\CC}{\bm{\mathrm{C}}}
%\newcommand*{\nabl}{\bm{\nabla}}%nabla
%\DeclareMathOperator{\lb}{lb}%base 2 log
%\DeclareMathOperator{\tr}{tr}%trace
%\DeclareMathOperator{\card}{card}%cardinality
%\DeclareMathOperator{\im}{Im}%im part
%\DeclareMathOperator{\re}{Re}%re part
%\DeclareMathOperator{\sgn}{sgn}%signum
%\DeclareMathOperator{\ent}{ent}%integer less or equal to
%\DeclareMathOperator{\Ord}{O}%same order as
%\DeclareMathOperator{\ord}{o}%lower order than
%\newcommand*{\incr}{\triangle}%finite increment
\newcommand*{\defd}{\coloneqq}
\newcommand*{\defs}{\eqqcolon}
\newcommand*{\Land}{\bigwedge}
\newcommand*{\Lor}{\bigvee}
%\newcommand*{\lland}{\DOTSB\;\land\;}
%\newcommand*{\llor}{\DOTSB\;\lor\;}
\newcommand*{\limplies}{\mathbin{\Rightarrow}}%implies
%\newcommand*{\suchthat}{\mid}%{\mathpunct{|}}%such that (eg in sets)
%\newcommand*{\with}{\colon}%with (list of indices)
%\newcommand*{\mul}{\times}%multiplication
%\newcommand*{\inn}{\cdot}%inner product
%\newcommand*{\dotv}{\mathord{\,\cdot\,}}%variable place
%\newcommand*{\comp}{\circ}%composition of functions
%\newcommand*{\con}{\mathbin{:}}%scal prod of tensors
%\newcommand*{\equi}{\sim}%equivalent to 
\renewcommand*{\asymp}{\simeq}%equivalent to 
%\newcommand*{\corr}{\mathrel{\hat{=}}}%corresponds to
%\providecommand{\varparallel}{\ensuremath{\mathbin{/\mkern-7mu/}}}%parallel (tentative symbol)
\renewcommand*{\le}{\leqslant}%less or equal
\renewcommand*{\ge}{\geqslant}%greater or equal
%\DeclarePairedDelimiter\clcl{[}{]}
%\DeclarePairedDelimiter\clop{[}{[}
%\DeclarePairedDelimiter\opcl{]}{]}
%\DeclarePairedDelimiter\opop{]}{[}
\DeclarePairedDelimiter\abs{\lvert}{\rvert}
%\DeclarePairedDelimiter\norm{\lVert}{\rVert}
\DeclarePairedDelimiter\set{\{}{\}}
%\DeclareMathOperator{\pr}{P}%probability
\newcommand*{\pf}{\mathrm{p}}%probability
\newcommand*{\p}{\mathrm{P}}%probability
\newcommand*{\E}{\mathrm{E}}
%\renewcommand*{\|}{\nonscript\,\vert\nonscript\;\mathopen{}}
\renewcommand*{\|}[1][]{\nonscript\,#1\vert\nonscript\;\mathopen{}}
%\DeclarePairedDelimiterX{\cond}[2]{(}{)}{#1\nonscript\,\delimsize\vert\nonscript\;\mathopen{}#2}
\DeclarePairedDelimiterX{\condt}[2]{[}{]}{#1\nonscript\,\delimsize\vert\nonscript\;\mathopen{}#2}
%\DeclarePairedDelimiterX{\conds}[2]{\{}{\}}{#1\nonscript\,\delimsize\vert\nonscript\;\mathopen{}#2}
%\newcommand*{\+}{\lor}
%\renewcommand{\*}{\land}
\newcommand*{\sect}{\S}% Sect.~
\newcommand*{\sects}{\S\S}% Sect.~
\newcommand*{\chap}{ch.}%
\newcommand*{\chaps}{chs}%
\newcommand*{\bref}{ref.}%
\newcommand*{\brefs}{refs}%
%\newcommand*{\fn}{fn}%
\newcommand*{\eqn}{eq.}%
\newcommand*{\eqns}{eqs}%
\newcommand*{\fig}{fig.}%
\newcommand*{\figs}{figs}%
\newcommand*{\vs}{{vs}}
\newcommand*{\etc}{{etc.}}
%\newcommand*{\ie}{{i.e.}}
%\newcommand*{\ca}{{c.}}
\newcommand*{\eg}{{e.g.}}
\newcommand*{\foll}{{ff.}}
%\newcommand*{\viz}{{viz}}
\newcommand*{\cf}{{cf.}}
%\newcommand*{\Cf}{{Cf.}}
%\newcommand*{\vd}{{v.}}
\newcommand*{\etal}{{et al.}}
%\newcommand*{\etsim}{{et sim.}}
%\newcommand*{\ibid}{{ibid.}}
%\newcommand*{\sic}{{sic}}
%\newcommand*{\id}{\mathte{I}}%id matrix
%\newcommand*{\nbd}{\nobreakdash}%
%\newcommand*{\bd}{\hspace{0pt}}%
%\def\hy{-\penalty0\hskip0pt\relax}
%\newcommand*{\labelbis}[1]{\tag*{(\ref{#1})$_\text{r}$}}
%\newcommand*{\mathbox}[2][.8]{\parbox[t]{#1\columnwidth}{#2}}
%\newcommand*{\zerob}[1]{\makebox[0pt][l]{#1}}
\newcommand*{\tprod}{\mathop{\textstyle\prod}\nolimits}
\newcommand*{\tsum}{\mathop{\textstyle\sum}\nolimits}
%\newcommand*{\tint}{\begingroup\textstyle\int\endgroup\nolimits}
%\newcommand*{\tland}{\mathop{\textstyle\bigwedge}\nolimits}
\newcommand*{\tlor}{\mathop{\textstyle\bigvee}\nolimits}
%\newcommand*{\sprod}{\mathop{\textstyle\prod}}
%\newcommand*{\ssum}{\mathop{\textstyle\sum}}
%\newcommand*{\sint}{\begingroup\textstyle\int\endgroup}
%\newcommand*{\sland}{\mathop{\textstyle\bigwedge}}
%\newcommand*{\slor}{\mathop{\textstyle\bigvee}}
%\newcommand*{\T}{^\intercal}%transpose
%%\newcommand*{\QEM}%{\textnormal{$\Box$}}%{\ding{167}}
%\newcommand*{\qem}{\leavevmode\unskip\penalty9999 \hbox{}\nobreak\hfill
%\quad\hbox{\QEM}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Custom macros for this file @@@
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \definecolor{notecolour}{RGB}{68,170,153}
\newcommand*{\puzzle}{{\fontencoding{U}\fontfamily{fontawesometwo}\selectfont\symbol{225}}}
%\newcommand*{\puzzle}{\maltese}
\newcommand{\mynote}[1]{ {\color{notecolour}\puzzle\ #1}}
\newcommand*{\widebar}[1]{{\mkern1.5mu\skew{2}\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu}}

% \newcommand{\explanation}[4][t]{%\setlength{\tabcolsep}{-1ex}
% %\smash{
% \begin{tabular}[#1]{c}#2\\[0.5\jot]\rule{1pt}{#3}\\#4\end{tabular}}%}
% \newcommand*{\ptext}[1]{\text{\small #1}}
%\DeclareMathOperator*{\argsup}{arg\,sup}
\newcommand*{\prop}[1]{\text{\textquoteleft}#1\text{\textquoteright}}
\newcommand*{\propf}[1]{\,#1\,}
\newcommand*{\dob}{degree of belief}
\newcommand*{\dobs}{degrees of belief}
\newcommand*{\yK}{I}
\newcommand*{\yO}{\mathrm{O}}
\newcommand*{\hH}{\Hat{H}}
\newcommand*{\sS}{\mathsf{S}}
\newcommand*{\sD}{\mathsf{D}}
\newcommand*{\sM}{\mathsf{M}}
\newcommand*{\sH}{\mathsf{H}}
\newcommand*{\sI}{\mathsf{I}}
\newcommand*{\eq}{\mathrel{\!=\!}}
\newcommand*{\liff}{\mathrel{\;\Leftrightarrow\;}}
% \newcommand*{\eq}{\DOTSR\!=\!}
%%% Custom macros end @@@

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Beginning of document
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\firmlists
\begin{document}
\captiondelim{\quad}\captionnamefont{\footnotesize}\captiontitlefont{\footnotesize}
\selectlanguage{british}\frenchspacing
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Abstract
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\abstractrunin
\abslabeldelim{}
\renewcommand*{\abstractname}{}
\setlength{\absleftindent}{0pt}
\setlength{\absrightindent}{0pt}
\setlength{\abstitleskip}{-\absparindent}
\begin{abstract}\labelsep 0pt%
  \noindent It is shown that the log-likelihood of a hypothesis or model
  given some data is equal to an average of all leave-one-out
  cross-validation log-scores that can be calculated from all subsets of
  the data. This relation can be generalized to any $k$-fold
  cross-validation log-scores.
% \\\noindent\emph{\footnotesize Note: Dear Reader
%     \amp\ Peer, this manuscript is being peer-reviewed by you. Thank you.}
% \par%\\[\jot]
% \noindent
% {\footnotesize PACS: ***}\qquad%
% {\footnotesize MSC: ***}%
%\qquad{\footnotesize Keywords: ***}
\end{abstract}
\selectlanguage{british}\frenchspacing

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Epigraph
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \asudedication{\small ***}
% \vspace{\bigskipamount}
% \setlength{\epigraphwidth}{.7\columnwidth}
% %\epigraphposition{flushright}
% \epigraphtextposition{flushright}
% %\epigraphsourceposition{flushright}
% \epigraphfontsize{\footnotesize}
% \setlength{\epigraphrule}{0pt}
% %\setlength{\beforeepigraphskip}{0pt}
% %\setlength{\afterepigraphskip}{0pt}
% \epigraph{\emph{text}}{source}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% BEGINNING OF MAIN TEXT
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Log-likelihoods and cross-validation log-scores}
\label{sec:loglhood}

What is the probability of hypothesis $H$, given data $D$ in some specific
context $\yK$? \; It is $\p(H \| D \land \yK)$. The probability-calculus
gives a straightforward relation between this probability and the
likelihood for the hypothesis given the data -- that is \citep[\sect~6.1
p.~62]{good1950}, the probability of the data given the hypothesis,
$\p(D \| H \land \yK)$:
\begin{equation}
  \label{eq:posterior_hypothesis_universal}
  \p(H \| D \land \yK) = \frac{\p(D \| H \land \yK)\;\p(H \| \yK)}{\p(D \| \yK)} \;.
    % \label{eq:posterior_hypothesis_sethypotheses}
    % &=\frac{\p(D \| H_{h} \, \yK)\;\p(H_{h} \| \yK)}{\sum_{h'} \p(D \| H_{h'} \, \yK)\; \p(H_{h'} \| \yK)}.
\end{equation}

If we have hypotheses $\set{H_{h}}$ that are mutually exclusive on context
$\yK$, and if their probabilities $\set{\p(H_{h} \| \yK)}$ are all equal,
then their likelihoods decide which among them is the most probable given
the data, owing to the proportionality in the equation above.

In the literature in probability and statistics we also find other
\enquote{scores} attached to a hypothesis. Here I consider one in
particular: the \emph{leave-one-out cross-validation
  log-score}\citep[\sects~3.4, 6.1.6]{bernardoetal1994_r2000}[see
also][]{stone1977,geisseretal1979,vehtarietal2012,vehtarietal2002,krnjajicetal2011,krnjajicetal2014,gelmanetal2014,gronauetal2019,chandramoulietal2019},
which I shall call just \enquote{log-score} for brevity. If the data are
the conjunction of data points $D_{1} \land \dotsb \land D_{N}$, the
log-score of hypothesis $H$ is defined as
\begin{equation}
  \label{eq:log-score}
  \frac{1}{N} \sum_{d=1}^{N} \log\p(D_{d} \| D_{-d} \land H \land \yK) \;,
\end{equation}
where $D_{-i}$ denotes the conjunction of the data with datum $D_{i}$
excluded.

The intuition behind the log-score can be colloquially expressed thus:
\enquote{let's see what my belief about observing one datum would be, on
  average, once I've observed the other data, if I consider $H$ as true}.
Bernardo \amp\ Smith \citep[\sect~6.1.6]{bernardoetal1994_r2000} give a
deeper though informal motivation for the log-score. From their reasoning,
based on the principles of probability theory and decision theory, the
log-score of $H$ comes forth as an approximation of an expected utility.
This expected utility concerns an act (in the sense of decision theory)
that can be interpreted \emph{as if} we fully believed in the hypothesis
$H$. The utility function and the act in Bernardo \amp\ Berger's reasoning
are quite specific (I personally find them too involved and doubt their
usefulness outside of their specific context). But they make clear that the
log-score and the likelihood have very different roles: the first pertain
to decision theory, the second to probability theory only.

My purpose here is to show that
\begin{enumerate}[label=(\alph*)]
\item an analysis of what a hypothesis or \enquote{model} is, and
  especially of the difference between \emph{hypotheses that allow
    learning} and \emph{hypotheses that do not allow learning}, leads
  to a slightly modified log-score involving data subsets of all sizes;
\item such modified log-score equals the log-likelihood.
\end{enumerate}


\section{Hypotheses and learning}
\label{sec:hyp_learn}

Hypotheses are important for the inferences and forecasts they allow us to
make.

Consider a set of mutually exclusive statements we are interested in;
typical in the sciences are statements about observations or
measurements, of the joint form
\begin{equation}
  \label{eq:statements_interested}
  \prop{X_{1}\eq x_{1}} \land
  \prop{X_{2}\eq x_{2}} \land \dotsb
  \prop{X_{N}\eq x_{N}}
\end{equation}
where each value $x_{d}$ belongs to a common finite set. We must consider
all relevant observations, with unknown as well as known outcomes.

A hypothesis $H$ is well-defined in a context $\yK$ only if it and the
context together fully determine the probability-values of all statements
above,
\begin{equation}
  \label{eq:prob_stat_from_hyp}
  \p( \propf{X_{1}\eq x_{1}} \land
  \propf{X_{2}\eq x_{2}} \land \dotsb \land
  \propf{X_{N}\eq x_{N}} \| H \land \yK) \;.
\end{equation}
This requirement agrees with the emphasis placed by many authors on 
predictive distributions rather than abstract parameters
\citep[\eg][]{definetti1930,definetti1937,roberts1965,lauritzen1974,diaconis1988,bernardoetal1994_r2000,fortinietal2000,fortinietal2012}.

When we consider several hypotheses we must require more. A context $\yK$
and hypotheses $\set{H_{h}}$, usually assumed mutually exclusive in that
context, are well-defined only if they determine the probabilities
$\p( H_{h} \| \yK)$ for all $h$.

I insist on speaking about a \enquote{context} because a hypothesis alone
rarely, if ever, determines probability-values; even if it may constrain
them \citep{hailperin2011}. It determines them only when conjoined with a
larger context, which can itself include additional hypotheses. The
hypothesis that there are 7 blue and 3 red balls in an urn, for example,
does not determine a unique \dob\ about the draws from the urn. This remark
is strikingly true for so-called \enquote{composite hypotheses}, having the
form $H_{h'} \lor \dotsb \lor H_{h''}$. Since, for any data $D$,
\begin{multline}
  \label{eq:comp_hyp}
  \p[ D \| (H_{h'} \lor \dotsb \lor H_{h''} ) \land \yK]
  ={}\\
 \p( D \| H_{h'} \land \yK) \times \p(H_{h'} \| \yK) +
  \dotsb +
 \p( D \| H_{h''} \land \yK) \times \p(H_{h''} \| \yK) \;,
\end{multline}
the probabilities of the data conditional on a composite hypothesis depend
fully on the context. These probabilities are not determined by those
conditional on the individual hypotheses. This is apparently a trivial
remark. And yet it seems to me that many studies that discuss composite
hypotheses end up comparing \emph{contexts} instead, without realizing the
shift in intent and getting confused by it.

The definitions just given are important for the present discussion. They
make clear the difference between two kinds of hypotheses: \emph{hypotheses
  that do not allow learning} versus \emph{hypotheses that allow learning}.

The first kind satisfies the property
\begin{multline}
  \label{eq:nonlearning_hyp}
\text{$H$ non-learning} \liff  \p( D \| D' \land H \land \yK) = 
\p( D \| H \land \yK)
\\\text{for all logically independent data $D, D'$}\;.
\end{multline}
The second kind does not satisfy that property:
\begin{equation}
  \label{eq:learning_hyp}
\text{$H$ learning} \liff  \p( D \| D' \land H \land \yK) \ne 
  \p( D \| H \land \yK) \;.
\end{equation}
Hypotheses about physical theories are often of the first kind, as are
many hypotheses about long-run statistics, such as frequency distributions, in
infinite-exchangeability contexts.





% \begin{multline}
%   \label{eq:comp_hyp}
%   \p[ \propf{X_{1}\eq x_{1}} \land \dotsb \land
%   \propf{X_{N}\eq x_{N}} \|
%   (H_{h'} \lor \dotsb \lor H_{h''} ) \land \yK]
%   ={}\\
%  \p( \propf{X_{1}\eq x_{1}} \land \dotsb \land
%   \propf{X_{N}\eq x_{N}} \|
%   H_{h'} \land \yK) \; \p(H_{h'} \| \yK) +
%   \dotsb \\+ 
%  \p( \propf{X_{1}\eq x_{1}} \land \dotsb \land
%   \propf{X_{N}\eq x_{N}} \|
%   H_{h''} \land \yK) \; \p(H_{h''} \| \yK) \;,
% \end{multline}

% \begin{equation}
%   \label{eq:conj_hyp}
%  \p( H_{h} \| \yK),
%  \quad
%  \p( H_{h'} \lor H_{h''} \| \yK),
%  \quad \dotsc, \quad
%   \p( H_{h'} \land H_{h''} \| \yK)
% \end{equation}
% must be determined for all $h,h',h''$.





% for all the logical
% combinations of statements we want to infer from it. Let us consider the
% case of statements of the form $D_{d} \defd \prop{X_{d}\eq x_{d}}$,
% concerning the outcomes of several measurements or observations
% $d \in \set{1,2,\dotsc}$. Thus every probability such as
% \begin{equation}
%   \label{eq:prob_from_hyp}
%   \p(D_{1} \land D_{2} \land D_{3} \land \dotsb \| H \land \yK) \;,
% \end{equation}
% must have a numerical value. These probabilities allow us to calculate
% others, such as
% \begin{equation}
%   \label{eq:cond_prob_from_hyp}
%   \p( D_{3} \land \dotsb \| D_{1} \land D_{2} \land  H \land \yK) \;,
% \end{equation}








\enquote{On average} means considering such belief for every single datum
in turn, and then taking the geometric mean of the resulting
beliefs. % , that is, the arithmetic mean on a log scale.
Other variants of this score use more general partitions of the data into
two disjoint subsets\addtocounter{footnote}{-1}\footnotemark{}.



find which hypothesis should have our highest belief
given the data.


Note that we are speaking about our \dobs\ in the hypotheses, not about the
\emph{choice} of one among them. Such a choice would require also a utility
function, and the chosen hypothesis would not need to be the most probable
one. But in fact we rarely have to \enquote{choose} among hypotheses. We
usually have to make a decision that can somewhat be interpreted \emph{as
  if} we fully believed in a hypothesis. A clinician, for example, may give
a patient some treatment for a disease and yet believe that the patient is
healthy. Simply because the treatment will not harm the patient if he is
healthy, and will cure him in the improbable case he is not. Likewise, an
astrophysicist may use Newton's equations to find the motion of a celestial
object and yet firmly believe in the correctness of Einstein's equations.
Simply because the approximate answer of the former is faster to compute
and enough precise for the problem considered. Beliefs and decisions are
different things.

I assume that the problem of \emph{model comparison} often discussed in the
literature ***

Despite the clear relation of
equation~\eqref{eq:posterior_hypothesis_universal}, the literature in
probability and statistics employs and debates other ad-hoc measures to
find which hypothesis should have our highest belief given the data.

Here I consider one measure in particular: the \emph{leave-one-out
  cross-validation log-score}\addtocounter{footnote}{-1}\footnotemark{},
which I'll just call \enquote{log-score} for brevity




quantify how the data relate to the hypotheses -- or even to select one
hypothesis for further use, discarding the others.
Here I consider one measure in particular: the \emph{leave-one-out
  cross-validation log-score}\addtocounter{footnote}{-1}\footnotemark{},
which I'll just call \enquote{log-score} for brevity:
\begin{equation}
  \label{eq:log-score}
  \frac{1}{d} \sum_{i=1}^{d} \log\p(D_{i} \| D_{-i} \, H_{h} \, \yK)
\end{equation}
where every $D_{i}$ is one datum in the data $D \equiv \Land_{i=1}^{d} D_{i}$,
and $D_{-i}$ denotes the data with datum $D_{i}$ excluded. The intuition
behind this score can be colloquially expressed thus: \enquote{let's see
  what my belief in one datum would be, on average, once I've observed the
  other data, if I consider $H_{h}$ as true}. \enquote{On average} means
considering such belief for every single datum in turn, and then taking the
geometric mean of the resulting
beliefs. % , that is, the arithmetic mean on a log scale.
Other variants of this score use more general partitions of the data into
two disjoint subsets\addtocounter{footnote}{-1}\footnotemark{}.




The probability calculus unequivocally tells us how our \dob\ in a
hypothesis $H_{h}$ given data $D$ and background information or assumptions
$\yK$, that is, $\p(H_{h} \| D \, \yK)$, is related to our \dob\ in
observing those data when we entertain that hypothesis as true, that is,
$\p(D \| H_{h} \, \yK)$:
\begin{subequations}
    \label{eq:posterior_hypothesis}
  \begin{align}
    \label{eq:posterior_hypothesis_universal}
    \p(H_{h} \| D \, \yK) &=
    \frac{\p(D \| H_{h} \, \yK)\;\p(H_{h} \| \yK)}{\p(D \| \yK)}\\
    \label{eq:posterior_hypothesis_sethypotheses}
    &=\frac{\p(D \| H_{h} \, \yK)\;\p(H_{h} \| \yK)}{\sum_{h'} \p(D \| H_{h'} \, \yK)\; \p(H_{h'} \| \yK)}.
  \end{align}
\end{subequations}
$D$, $H_{h}$, $\yK$ denote propositions, which are usually about
numeric quantities. I use the terms \enquote{\dob}, \enquote{belief}, and
\enquote{probability} as synonyms. By \enquote{hypothesis} I mean either a
scientific (physical, biological, \etc) hypothesis -- a state or
development of things capable of experimental verification, at least in a
thought experiment -- or more generally some proposition, often not
precisely specified, which leads to quantitatively specific distributions
of beliefs for any contemplated data set.
% $\p(\dotso \| H \, \yK)$.
In the latter case we often call $H_{h}$  a
\enquote{(probabilistic) model} rather than a \enquote{hypothesis}.
% \begin{innote}\label{note:syntax}
%   The probability calculus, just like the truth calculus, proceeds purely
%   syntactically rather than semantically. That is, if I tell you that $H$
%   and $H \limplies D$ are true, you can conclude that $D$ is true;
%   similarly, if I tell you that $\p(H \| \yK) = p$ and
%   $\p(H \limplies D \| \yK) = q$, you can conclude (try it as an exercise)
%   that $\p(H\;D \| \yK) = p + q -1$. And in either case you don't need to
%   know what $H$ and $D$ are about -- they could be about Donald Duck or
%   parallel universes. Don't we too often abuse of this syntactical
%   property? We often say \enquote{under model $H$ our belief about the
%     value of quantity $x$ is expressed by such and such distribution
%     $\pf(x \| H)=f(x)$}, without explaining what $\theta$ really is
%   and why it leads to $f$. Aren't terms such as \enquote{model} and
%   \enquote{hypothesis}, as often used in probability and statistics,
%   convenient and respectable-looking carpets under which we can sweep the
%   fact that we don't quite know what we're speaking about? The need to look
%   under the carpet arises, though, the moment we have to specify our
%   pre-data belief, the prior, about the mysterious $H$.
  
%   And yet again, semantics can very well be a by-product of syntax, or the
%   distinction between the two be a chimera
%   \citep{wittgenstein1945_t1999,girard2001,girard2003}. Such important
%   matters are unfortunately rarely discussed in probability and statistics.
% \end{innote}


Expression~\eqref{eq:posterior_hypothesis_sethypotheses} assumes that we
have a set $\set{H_{h}}$ of mutually exclusive and exhaustive hypotheses
under consideration, which is implicit in our knowledge $\yK$. In fact it's
only valid if
\begin{equation}
  \label{eq:implicit_knowledge}
  \p\bigl(\tlor_{h} H_{h} \| \yK\bigr) = 1,
  \qquad
  \p(H_{h} \land H_{h'} \| \yK) = 0 \quad \text{if $h \ne h'$}.
\end{equation}\pagebreak
Only  rarely does the set of hypotheses $\set{H_{h}}$
encompass and reflect the extremely complex and fuzzy hypotheses lying in
the backs of our minds. They're simplified pictures. That's also why
they're %$\yK$ or the hypotheses $\set{H_{h}}$
called \enquote{models}.
% The background knowledge $\yK$ and the hypotheses
% $\set{H_{h}}$ are therefore only simplified pictures of our actual
% knowledge and conjectures. That's why
% they're %$\yK$ or the hypotheses $\set{H_{h}}$
% often called \emph{models}.% \; \enquote{A theory cannot duplicate nature,
  % for if it did so in all respects, it would be isomorphic to nature itself
  % and hence useless, a mere repetition of all the complexity which nature
  % presents to us, that very complexity we frame theories to penetrate and
  % set aside. If a theory were not simpler than the phenomena it was
  % designed to model, it would serve no purpose. Like a portrait, it can
  % represent only a part of the subject it pictures. This part it
  % exaggerates, if only because it leaves out the rest. Its simplicity is
  % its virtue, provided the aspect it portrays be that which we wish to
  % study} (Truesdell \citep[Prologue p.~xvi]{truesdelletal1980}).

Expression~\eqref{eq:posterior_hypothesis_universal} is universally valid
instead, but it's rarely possible to quantify its denominator
$\p(D \| \yK)$ unless we simplify our inferential problem by introducing a
possibly unrealistic exhaustive set of hypotheses, thus falling back
to~\eqref{eq:posterior_hypothesis_sethypotheses}. We can bypass this
problem if we are content with comparing our beliefs about any two
hypotheses through their ratio, so that the term $\p(D \| \yK)$ cancels
out. See Jaynes's \citey[\sects~4.3--4.4]{jaynes1994_r2003} insightful
remarks about such binary comparisons, and also Good's
\citey[\sect~6.3--6.6]{good1950}.

\bigskip

The term $\p(D \| H_{h} \, \yK)$ in \eqn~\eqref{eq:posterior_hypothesis} is
called the \emph{likelihood} of the hypothesis given the data
\citep[\sect~6.1 p.~62]{good1950}. Its logarithm is surprisingly called
log-likelihood:
\begin{equation}
  \label{eq:log-likelihood}
  \log\p(D \| H_{h} \, \yK),
\end{equation}
where the logarithm can be taken in an arbitrary basis (Turing, Good
\citep[\eg][]{good1985,good1950,good1969}, Jaynes
\citep[\sect~4.2]{jaynes1994_r2003} recommend base
$\cramped10^{1/10}$, leading to a measurement in decibels; see the
cited works for the practical advantages of such choice).

The ratio of the likelihoods of two hypotheses, called \emph{relative Bayes
  factor}, or its logarithm, the \emph{relative weight of
  evidence},\footnote{\cites[\chap~6]{good1950}{good1975,good1981,good1985},
  and many other works in \cite{good1983};
  \cites[\sect~1.4]{osteyeeetal1974}{mackay1992,kassetal1995}; see also
  \cite[\chaps~V, VI, A]{jeffreys1939_r1983}.} are often used to quantify
how much the data favour our belief in one versus the other
hypothesis (that is, assuming at least momentarily that they be
exhaustive). \enquote{It is historically interesting that the expression
  ``weight of evidence'', in its technical sense, anticipated the term
  ``likelihood'' by over forty years} \citep[\sect~1.4.2
p.~12]{osteyeeetal1974}.
\begin{innote}
  Recent literature \citep[for example][]{kassetal1995} seems to
  exclusively deal with \emph{relative} Bayes factors. I'd like to recall,
  lest it fades from the memory, the definition of the non-relative Bayes
  factor for a hypothesis $H_{h}$ provided by data
  $D$:\citep[\sect~2]{good1981}
  \begin{equation}
    \label{eq:proper_Bayes_factor}
    \frac{\p(D \| H_{h} \; \yK)}{\p(D \| \lnot H_{h} \; \yK)} \equiv
    \frac{\yO(H_{h} \| D \; \yK)}{\yO(H_{h} \| \yK)} =
    \frac{\p(D \| H_{h}\; \yK)\; [1- \p(H_{h} \| \yK)]}{
\sum_{h'}^{h' \ne h} \p(D \| H_{h'} \; \yK) \; \p(H_{h'} \| \yK)
    },
  \end{equation}
  where the \emph{odds} $\yO$ is defined as $\yO \defd \p/(1-\p)$. Looking
  at the expression on the right, which can be derived from the probability
  rules, it's clear that the Bayes factor for a hypothesis involves the
  likelihoods of \emph{all} other hypotheses as well as their pre-data
  probabilities. This quantity and its logarithm, the (non-relative) weight
  of evidence, have important properties which relative Bayes factors and
  relative weights of evidence don't enjoy. For example, the expected
  weight of evidence for a correct hypothesis is always positive, and for a
  wrong hypotheses always negative\citep[\sect~6.7]{good1950}. See
  Jaynes\citey[\sects~4.3--4.4]{jaynes1994_r2003} for further discussion
  and a numeric example.
\end{innote}


\bigskip

The literature in probability and statistics has also employed and debated
other ad-hoc measures to quantify how the data relate to the hypotheses --
or even to select one hypothesis for further use, discarding the
others\citep[\sects~3.4, 6.1.6 gives the clearest motivation and
explanation]{bernardoetal1994_r2000}[see
also][]{stone1977,geisseretal1979,vehtarietal2012,vehtarietal2002,krnjajicetal2011,krnjajicetal2014,gelmanetal2014,gronauetal2019,chandramoulietal2019}.
Here I consider one measure in particular: the \emph{leave-one-out
  cross-validation log-score}\addtocounter{footnote}{-1}\footnotemark{},
which I'll just call \enquote{log-score} for brevity:
\begin{equation}
  \label{eq:log-score}
  \frac{1}{d} \sum_{i=1}^{d} \log\p(D_{i} \| D_{-i} \, H_{h} \, \yK)
\end{equation}
where every $D_{i}$ is one datum in the data $D \equiv \Land_{i=1}^{d} D_{i}$,
and $D_{-i}$ denotes the data with datum $D_{i}$ excluded. The intuition
behind this score can be colloquially expressed thus: \enquote{let's see
  what my belief in one datum would be, on average, once I've observed the
  other data, if I consider $H_{h}$ as true}. \enquote{On average} means
considering such belief for every single datum in turn, and then taking the
geometric mean of the resulting
beliefs. % , that is, the arithmetic mean on a log scale.
Other variants of this score use more general partitions of the data into
two disjoint subsets\addtocounter{footnote}{-1}\footnotemark{}.

\textcolor{white}{If you find this you can claim a postcard from me.}


My purpose is to show an exact relation between the
log-likelihood~\eqref{eq:log-likelihood} and the leave-one-out
cross-validation log-score~\eqref{eq:log-score}. This relation doesn't seem
to appear in the literature, and I find it very intriguing because it
portrays the log-likelihood as a sort of full-scale use of the log-score:
it says that \emph{the log-likelihood is the sum of all averaged log-scores
  that can be formed from all data subsets}. The relation can be extended
to more general cross-validation log-scores, and it can be of interest for
the debate about the soundness of log-scores in deciding among hypotheses.




% All terms in \eqn~\eqref{eq:posterior_hypothesis} are always important in
% an inference problem, both at decision stages (for example, evaluating
% which hypotheses to include in our simplified set, or which hypothesis to
% finally choose -- if we \emph{have to} -- discarding its competitors for
% future calculations; or if we must choose among possible medical
% treatment), and at exploratory stages (for example, examining whether a
% hypothesis leads to peculiar beliefs for peculiar kinds of data). But also
% other expressions are often used, either derived via the probability rules or
% constructed on more intuitive grounds.


% based on the observed data, the
% post-data belief~\eqref{eq:posterior_hypothesis} is necessary but not
% sufficient. We also need to specify a utility or cost function to calculate
% the expected gains of choosing one or another hypothesis or making one or
% another decision
% \citep{kadaneetal1980b,degroot1970_r2004}[\chap~2]{bernardoetal1994_r2000}.

% If our problem has an exploratory nature instead -- for example, evaluating
% which hypotheses to include in our simplified set, or examining whether a
% hypothesis leads to peculiar beliefs for peculiar kinds of data -- then all
% terms appearing in expression~\eqref{eq:posterior_hypothesis} are usually
% freely examined.



% Gelman \etal\ \citey{gelmanetal2014} show among other things that it's
% approximately equal to expected value of the post-data log-probability of a
% new datum:
% \begin{equation}
%   \label{eq:logscore_approx_logposterior}
%   \E\condt[\Big]{\log\p(D' \| D\; H_{h} \; \yK)}{ D\; H_{h} \; \yK},
% \end{equation}
% ***entropy, remark on invariance***
% where $D'$ represents the new datum. Krnjaji\'c \etal\
% \citey{krnjajicetal2011,krnjajicetal2014} numerically compare it with the
% deviance information criterion. Regarding its uses I recommend reading the
% recent debate among Gronau \etal\ \parentext{\cite*{gronauetal2019} \amp\
%   \cite{chandramoulietal2019}}, where all authors give very insightful
% remarks.

% (There's an ambiguity in this
% definition, because we can ask: what's a \enquote{datum} in the case of
% multi-dimensional observations? a single numerical value? or a
% multidimensional point? Different interpretations lead to different
% log-scores. We'll come back to this point later.)

% This is a reasonable intuition, and the log-score~\eqref{eq:log-score} and
% post-data probability~\eqref{eq:posterior_hypothesis} often lead to
% qualitatively similar results in comparing two hypotheses. There are
% exceptions, though.

\section{A relation between log-likelihood and log-score}
\label{sec:relation}

% I'd like to show an exact relation between the log-score~\eqref{eq:log-score} and
% the log-likelihood~\eqref{eq:log-likelihood} which doesn't seem to appear
% in the literature. I find this relation very intriguing because it portrays
% the log-likelihood as a sort of full-scale use of the log-score.% : it says that
% \emph{the log-likelihood is the sum of all averaged log-scores that can be formed
%   from all data subsets}.

% My point of view, which hinges on the logical foundations of the
% probability calculus
% \citep{polya1939_t1941,polya1949,polya1954b_r1968,cox1946,hailperin1996,jaynes1994_r2003,paris1994_r2006,snow1998,tereninetal2015_r2017},
% is that every intuitively built quantitative assessment of belief is either
% \begin{enumerate*}[label=(\arabic*)]
% \item an approximation of a formula that can be derived from the
%   probability calculus, or
% \item wrong.
% \end{enumerate*}

% I shall now show that the log-score above can be viewed as an approximation
% of the log-likelihood $\p(D \| H_{h} \, \yK)$ of the post-data
% probability~\eqref{eq:posterior_hypothesis}; or, if you like, that the
% post-data probability can be seen as a refined version of the log-score.

\bigskip

We can obviously write the likelihood as the $d$th root of its $d$th power:
\begin{equation}
  \label{eq:root_product}
  \p(D \| H \, \yK) \equiv  \bigl[\,
  \underbracket{\p(D \| H \, \yK) \times \dotsm \times
  \p(D \| H \, \yK)}_{\text{$d$ times}}
  \,\bigr]^{1/d}
\end{equation}
where we have dropped the subscript ${}_{h}$ for simplicity. By the rules
of probability we have
\begin{equation}
  \label{eq:product_rule}
  \p(D \| H \, \yK) =
  \p(D_{i} \| D_{-i} \, H_{h} \, \yK) \times \p(D_{-i} \|  H_{h} \, \yK)
\end{equation}
no matter which specific $i \in \set{1, \dotsc, d}$ we choose (temporal
ordering and similar matters are completely irrelevant in the formula
above: it's a logical relation between propositions). So let's expand each
of the $d$ factors in the identity~\eqref{eq:root_product} using the
product rule~\eqref{eq:product_rule}, using a different $i$ for each of
them. The result can be thus displayed:
\begin{equation}
  \label{eq:product_2}
  \begin{aligned}
    \p(D \| H \, \yK) \equiv{}
    \bigl[\,&\p(D_{1} \| D_{-1} \, H \, \yK) \times
            \p(D_{-1} \|  H \, \yK) \times{}\\
          &\p(D_{2} \| D_{-2} \, H \, \yK) \times
            \p(D_{-2} \| H \, \yK)\times{}\\
          &\hphantom{\p(D_{2} \| D_{-2} \, H \, \yK)}
            \mathrlap{\dotso}\hphantom{{}\times  \p(D_{-2} \| H \, \yK)}\times{}\\
          &\p(D_{d} \|\underbracket[0pt][1ex]{ D_{-d} }_{\mathclap{%
              \substack{\Big\uparrow\\\text{this column leads to the log-score}}%
}} H \, \yK) \times\p(D_{-d} \|  H \, \yK)
            \,\bigr]^{1/d}.
  \end{aligned}
\end{equation}
Upon taking the logarithm of this expression, the $d$ factors vertically
aligned on the left add up to the log-score~\eqref{eq:log-score}, as
indicated. But the mathematical reshaping we just did for
$\p(D \| H \, \yK)$ -- that is, the root-product
identity~\eqref{eq:root_product} and the expansion~\eqref{eq:product_2} --
can be done for each of the remaining factors $\p(D_{-i} \| H \, \yK)$
vertically aligned on the right in the expression above; and so on
recursively. Here is an explicit example for $d=3$:
\begin{multline}
  \label{eq:example_further_expansion}
  \p(D \| H \, \yK) \equiv{}\\
  \begin{alignedat}[b]{2}
    \Bigl\{\,&\p(D_{1} \| D_{2} \,D_{3} \, H \, \yK) \times
    \bigl[ &&\p(D_{2} \| D_{3} \, H \, \yK) \times \p( D_{3} \| H \, \yK) \times{}\\[-0.5\jot]
    &&&\p(D_{3} \| D_{2} \, H \, \yK) \times \p( D_{2} \| H \, \yK)
    \bigr]^{1/2}\times{}\\[\jot]
 %
    &\p(D_{2} \| D_{1} \,D_{3} \, H \, \yK) \times
    \bigl[&&\p(D_{1} \| D_{3} \, H \, \yK) \times \p( D_{3} \| H \, \yK) \times{}\\[-0.5\jot]
    &&&\p(D_{3} \| D_{1} \, H \, \yK) \times \p( D_{1} \| H \, \yK)
    \bigr]^{1/2}\times{}\\[\jot]
 %
    &\p(D_{3} \| D_{1} \,D_{2} \, H \, \yK) \times
    \bigl[&&\p(D_{1} \| D_{2} \, H \, \yK) \times \p( D_{2} \| H \, \yK) \times{}\\[-0.5\jot]
    &&&\p(D_{2} \| D_{1} \, H \, \yK) \times \p( D_{1} \| H \, \yK)
    \bigr]^{1/2}\Bigr\}^{1/3}.
  \end{alignedat}
\end{multline}
In this example the logarithm of the three vertically aligned factors in
the left column is, as already noted, the log-score~\eqref{eq:log-score}.
The logarithm of the six vertically aligned factors in the central column
is an average of the log-scores calculated for the three distinct subsets
of pairs of data $\set{D_{1}\,D_{2}}$, $\set{D_{1}\, D_{3}}$,
$\set{D_{2}\, D_{3}}$. Likewise, the logarithm of the six factors
vertically aligned on the right is the average of the log-scores for the
three subsets of data singletons $\set{D_{1}}$, $\set{D_{2}}$,
$\set{D_{3}}$.


In the general case with $d$ data there are $\binom{d}{k}$ subsets with $k$
data points. We therefore obtain
\begin{multline}
  \label{eq:general_equivalence}
  \log\p(D \| H \, \yK) \equiv
  \frac{1}{d} \sum_{i=1}^{d} \log\p(D_{i} \| D_{-i} \, H \, \yK)
+{}\\
\shoveright{\frac{1}{d} \sum_{i\in\set{1,\dotsc,d}} \frac{1}{d-1} \sum_{j\in\set{1,\dotsc,d}}^{j\ne i}  \!\!\! \log\p(D_{-i,j} \| D_{-i,-j} \, H \, \yK)
  +{}}\\[0.5\jot]
\shoveright{\binom{d}{d-2}^{-1} \!\!\! \sum_{i,j\in\set{1,\dotsc,d}}^{i<j} \frac{1}{d-2} \sum_{k\in\set{1,\dotsc,d}}^{k\ne i,j} \!\!\! \log\p(D_{-i,-j,k} \| D_{-i,-j,-k} \, H \, \yK)
      +{}}\\[\jot]
       {} \dotsb +{}\\
  % \binom{d}{k}^{-1}\frac{1}{k}\quad
  % \sum_{\mathclap{\text{$k$-tuples}}}   \log\p(D_{i_{1}} \| \underbracket{D_{i_{2}} \dotsm D_{i_{k}}}_{\mathclap{\text{order doesn't matter}}} \, H \, \yK)
  % +{}&\\[0.5\jot]
  % \dotsb +{}&\\[0.5\jot]
\shoveright{      \binom{d}{2}^{-1} \!\!\! \sum_{i,j\in\set{1,\dotsc,d}}^{i<j} \frac{1}{2}
%      \sum_{k\in\set{1,\dotsc,d}}^{k\ne i,j}  \!\!\! \log\p(D_{k} \| D_{i,j} \, H \, \yK)
      \bigl[ \log\p(D_{i} \| D_{j} \, H \, \yK) +
      \log\p(D_{j} \| D_{i} \, H \, \yK)\bigr]
  +{}}\\[0.5\jot]
  \frac{1}{d} \sum_{i=1}^{d} \log\p(D_{i} \| H \, \yK),
\end{multline}
which can be compactly written
\begin{empheq}[box=\fbox]{equation}
    \label{eq:general_equivalence_compact}
    \log\p(D \| H \, \yK) \equiv
    \sum_{k=1}^{d}
    \binom{d}{k}^{-1}
    \!\smashoperator{\sum_{\substack{\text{ordered}\\\text{$k$-tuples}}}} 
    \;\;\;\frac{1}{k}\;\;
    \smashoperator{\sum_{\substack{\text{cyclic}\\\text{permutations}}}} 
    \log\p(D_{i_{1}} \| D_{i_{2}} \dotsm D_{i_{k}} \, H \, \yK).
\end{empheq}
% The post-data log-probability for $H$ will be equal to this expression,
% plus the pre-data log-probability for $H$, plus a term which is the same for
% all hypotheses.
That is, \emph{the log-likelihood is the sum of all averaged log-scores
  that can be formed from all (non-empty) data subsets with $k$ elements},
the average for log-scores over $k$ data being taken over the
$\binom{d}{k}$ subsets having the same cardinality $k$.

There's also an equivalent form with a slightly different cross-validating
interpretation: We take each datum $D_{j}$ in turn and calculate our
log-belief in it conditional on all possible subsets of remaining data,
from the empty subset with no data (term $k=0$), to the only subset
$D_{-j}$ with all data except $D_{j}$ (term $k=d-1$). These log-beliefs are
averaged over the $\binom{d-1}{k}$ subsets having the same cardinality $k$.
The result can be expressed as
\begin{empheq}[box=\fbox]{equation}
    \label{eq:general_equivalence_alternative}
    \log\p(D \| H \, \yK) \equiv
    \frac{1}{d}\sum_{j=1}^{d}
    \sum_{k=0}^{d-1}
    \binom{d-1}{k}^{-1}
    \smashoperator{\sum_{\substack{\text{ordered}\\\text{$k$-tuples,}\\\text{$j$ excluded}}}}
    \log\p(D_{j} \| D_{i_{1}} \dotsm D_{i_{k}} \, H \, \yK).
\end{empheq}

\section{Brief discussion}
\label{sec:discussion}

% The relation~\eqref{eq:general_equivalence_compact} just proven between the
% log-likelihood and the log-score brings forth several thoughts.

It's remarkable that the individual log-scores in
expressions~\eqref{eq:general_equivalence_compact} and
\eqref{eq:general_equivalence_alternative} above are computationally
expensive, but their sum results in the
log-likelihood, which is less expensive.

The relation~\eqref{eq:general_equivalence_compact} invites us to see the
log-likelihood as a refinement and improvement of the log-score. The
log-likelihood takes into account not only the log-score for the whole
data, but also the log-scores for all possible subsets of data.
Figuratively speaking it examines the relationship between data and
hypothesis locally, globally, and on all intermediate scales. To me this
property makes the log-likelihood preferable to any single log-score
(besides the fact that the log-likelihood is directly obtained from the
principles of the probability calculus), because our interest is usually in how
the hypothesis $H$ relates to single data points as well as to any
collection of them. I hope to discuss this point, which also involves the
distinction between simple and composite
hypotheses\citep[\sect~6.1.4]{bernardoetal1994_r2000}, more in detail
elsewhere\citep{portamana9999b}.

By applying the identity~\eqref{eq:root_product} and generalizing the
expansion~\eqref{eq:product_rule} to different divisions of the data --
leave-two-out, leave-three-out, and so on -- we see that the
relation~\eqref{eq:general_equivalence_compact} can be generalized to any
$k$-fold cross-validation log-scores. Thus the log-likelihood is also
equivalent to an average of \emph{all conceivable} cross-validation
log-scores for all subsets of data, though I haven't calculated the weights
of such average.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Acknowledgements
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%\iffalse
\begin{acknowledgements}
  To
  Aki Vehtari for some references. To the staff of the NTNU library for
  their always prompt assistance. To Mari, Miri, Emma for continuous
  encouragement and affection, and to Buster Keaton and Saitama for filling
  life with awe and inspiration. To the developers and maintainers of Open
  Science Framework, \LaTeX, Emacs, AUC\TeX, Python, Inkscape, Sci-Hub for
  making a free and impartial scientific exchange possible.

    This work is financially supported by the Kavli Foundation and the Centre
  of Excellence scheme of the  Research Council of Norway (Roudi group).  
%\rotatebox{15}{P}\rotatebox{5}{I}\rotatebox{-10}{P}\rotatebox{10}{\reflectbox{P}}\rotatebox{-5}{O}.
%\\\mbox{}\hfill\autanet
\end{acknowledgements}
%\fi

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Appendices
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%\clearpage
% %\renewcommand*{\appendixpagename}{Appendix}
% %\renewcommand*{\appendixname}{Appendix}
% %\appendixpage
% \appendix

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Bibliography
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\renewcommand*{\finalnamedelim}{\addcomma\space}
\defbibnote{prenote}{{\footnotesize (\enquote{de $X$} is listed under D,
    \enquote{van $X$} under V, and so on, regardless of national
    conventions.)\par}}
% \defbibnote{postnote}{\par\medskip\noindent{\footnotesize% Note:
%     \arxivp \mparcp \philscip \biorxivp}}

\printbibliography[%prenote=prenote%,postnote=postnote
]

\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Cut text (won't be compiled)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 


%%% Local Variables: 
%%% mode: LaTeX
%%% TeX-PDF-mode: t
%%% TeX-master: t
%%% End: 
