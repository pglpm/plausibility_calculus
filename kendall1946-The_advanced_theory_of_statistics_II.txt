THE ADVANCED 
THEORY OF STATISTICS 
by 
MAURICE G. KENDALL, M.A. 
An Honorary Secretary of the Royal Statistical Society 
Statistician to the Chamber of Shipping of the United Kingdom 
Fellow of the Institute of Mathematical Statistics 
VOLUME II 
With 30 Illustrations and 52 Tables 
LONDON 
CHARLES GRIFFIN & COMPANY LIMITED 
42 DRURY LANE 
1946 
[All Rights Reserved] 
TO 
PETER and PAUL 
Printed in Great Britain 
by Butler <fb Tanner Limited, Frome 
PREFACE TO VOLUME II 
This volume falls into five sections. The first, comprising chapters 17 to 20, deals 
with Estimation. The second, comprising chapters 21, 23, 24 and 26 to 28, covers the 
Theory of Statistical Tests, including the Analysis of Variance and Multivariate Analysis. 
The third, consisting of chapter 22, deals with Regression Analysis and completes the 
account of statistical relationship begun in chapters 13 to 16 of Volume I. In the fourth, 
chapter 25, I have tried to give an introductory account of the reaction of theoretical 
considerations on the Design of Statistical Inquiries. Finally, the fifth, comprising chapters 
29 and 30, deals with the Analysis of Time-Series. 
The literature of statistical theory is now so vast that it seemed worth while devoting 
considerable space to a bibliography, which is given in Appendix B. Although it is far 
from complete, I hope that it will serve its purpose in guiding the student to the main 
sources. 
The chief problem in the writing of this volume arose in connection with the logic of 
statistical inference. Whenever possible I have kept the treatment objective. It is, 
I consider, unfair in a book of this kind not to present all sides of a case, particularly when 
there is so much disagreement among the authorities. Some day I hope to show that 
this disagreement is more apparent than real, and that all the existing theories of inference 
in probability differ essentially only in matters of taste in the choice of postulates. But 
this book is not the place for such work, and for the present I am content to state the 
position and to leave the reader to exercise his own choice. 
The difficulty became most acute in dealing with confidence intervals and fiducial 
inference, where two approaches which at first sight appear identical can lead to different 
results. Rather than try to reconcile them I have written a separate chapter on each. 
Professor E. S. Pearson was kind enough to read the manuscript of chapter 19 and Professor 
R. A. Fisher that of chapter 20, so that I think their respective views are, at any rate, not 
misrepresented. I am very grateful to them both for their help in this connection. 
My thanks are also due to Mr. P. A. Moran and Mr. A. J. H. Morrell, who cheerfully 
undertook to help with the proof reading and to whose painstaking scrutiny I owe the 
removal of a number of obscurities and errors. I shall be grateful to any reader who 
detects and notifies me of any further slips which have evaded us. Once again I have also 
to thank the publishers and the printers for the trouble they have taken in the production 
of the finished work. 
London, 
April, 1946. 
v 
TABLE OF CONTENTS 
CHAP. 
17. Estimation: Likelihood 
18. Estimation : Miscellaneous Methods 
19. Confidence Intervals 
20. Fiducial Inference 
21. Some Common Tests of Significance 
22. Regression ... 
23. , The Analysis of Variance—(1) ... 
24. The Analysis of Variance—(2) ... 
25. The Design of Sampling Inquiries 
26. General Theory of Significance-Tests- 
27. General Theory of Significance-Tests- 
28. Multivariate Analysis 
29. Time-Series—(1) 
30. Time-Series—(2) 
Appendix A : Addenda to Volume I 
Appendix B: Bibliography 
Index to Volume II 
-(1) 
-(2) 
**• • ■ ■ • • • »• 
• •• •* • • * • •• 
• •■ • • * * • • • ft 
• P • ••• •»• •• 
**• '*•• # • * •• 
• «• • • • a> • • • • 
■ *• ••■ * a • • • 
• • • ■•• • a • • • 
*•■ ••• » a » •* 
a ■ • # a » a » * •• 
■ •• »** ••■ ■• 
a • ■ •)■■ a a » a * 
• •• a » • • « • a 4 
• • a » a • •*■ a • 
PAGES 
1-49 
50-61 
62-84 
85-95 
96-140 
141-174 
175-217 
218-246 
247-268 
269-306 
307-327 
328-362 
363-395 
396-439 
440-441 
442-503 
504-521 
vii 
CHAPTER 17 
ESTIMATION: LIKELIHOOD 
The Problem 
17.1. On several occasions in previous chapters we have encountered the problem 
of estimating from a sample the values of the parameters of the parent population. We 
have hitherto dealt on somewhat intuitive Hues with such questions as arose—for example, 
in the theory of large samples we have taken the means and moments of the sample to be 
satisfactory estimates of the corresponding means and moments in the parent. 
We now proceed to study this branch of the subject in more detail. In the earlier 
part of the present chapter we shall examine the sort of criteria which are required of 
a et good " estimate and discuss the question whether there exist " best " estimates in 
any acceptable sense of the term. In the remainder of the chapter and in Chapter 18 
we shall consider various methods of obtaining estimates with the required properties. 
In Chapters 19 and 20 we shall look at the same problem from a rather different point of 
view and discuss the theories of confidence intervals and fiducial limits. 
17.2. It will be evident that if a sample is not random and nothing precise is known 
about the nature of the bias operating when it was chosen, very little can be inferred from 
it about the parent population. Certain conclusions of a trivial kind are sometimes 
possible—for instance, if we take ten turnips from a pile of 100 and find that they weigh ten 
pounds altogether, the mean weight of turnips in the pile must be greater than one-tenth of 
a pound ; but such information is rarely of value, and estimation based on biassed samples 
remains very much a matter of individual opinion and cannot be reduced to exact and 
objective terms. We shall therefore confine our attention to random samples only. Our 
general problem, in its simplest terms, is then to estimate the value of a parameter in the 
parent from the information given by the sample. In the first instance we consider 
the case when only one parameter is to be estimated. The case of several parameters 
will be discussed later. 
17.3. Let us in the first place consider what we mean by " estimation ". We know, 
or assume as a working hypothesis, that the parent population is distributed in a form 
which, would be completely determinate if we knew the value of some parameter 0. We 
are given a sample of values xx . . . xn. We require to determine, with the aid of the 
x's, a number which can be taken to be the value of (9, or a range of numbers which can 
be taken to include that value. 
Now a single sample, considered by itself, may be rather improbable, and any estimate 
based on it may therefore differ considerably from the true value of 6. It appears, 
therefore, that we cannot expect to find any method of estimation which can be 
guaranteed to give us a close estimate of 0 on every occasion and for every sample. We must 
content ourselves with formulating a rule which will give good results " in the long run " 
or " on the average ", or which has " a high probability of success "—phrases which 
express the fundamental fact that we have to regard our method of estimation as generating 
a population of estimates and to assess its merits according to the properties of this 
population. 
A.S.—II 1 B 
2 ESTIMATION: LIKELIHOOD 
17.4. It will clarify our ideas considerably if we draw a distinction between the 
method or rule of estimation, which, following Pitman, we shall call an Estimator, and the 
value to which it gives rise in particular cases, the Estimate. The distinction is the same 
as that between a function / (x), regarded as defined for a range of the variable x, and the 
particular value which the function assumes, say / (a), for a specified value of x equal to a. 
Our problem is not to find estimates, but to find Estimators. We do not reject a method 
because it gives a bad result in a particular case (in the sense that the estimate differs 
materially from the true value). We should only reject it if it gave bad results in the long 
run, that is to say, if the population of possible values of the estimator were seriously 
discrepant with the value of 6. The merit of the estimator is judged by the population 
of estimates to which it gives rise. It is itself a random variable and has a distribution 
to which we shall frequently have occasion to refer. 
17.5. In the theory of large samples we have often taken as an estimator of a 
parameter 6 a statistic t calculated from the sample in exactly the same way as 0 is calculated 
from the population, e.g. the sample-mean is taken as an estimate of the parent mean. 
Let us examine how this procedure can be justified. Consider the case when the parent 
population is 
1 
(IF — ——- exp { — | (x — 0)2} dx, -- oo <# ■::: co . . (17.1) 
\/(2uz) 
Requiring an estimator for the parent mean 0, we take 
t = - 
n 
The distribution of t is 
y x>;. . . . . (i. t .z) 
dF = t(£) exp {~ "1{t ~oy)Llt (17,3) 
that is to say, t is distributed normally about 0 with variance l/n. We notice two tilings 
about this distribution : (a) it has a mean (and median and mode) at the true value 0, 
and (b) as % increases, the scatter of possible values of t about 0 becomes smaller, so that 
the probability that a given t differs by more than a fixed amount from 0 decreases. We 
may say that the accuracy of the estimator increases as n increases, or simply with n. 
17.6. Generally, it will be clear that the phrase " accuracy increasing with n " has 
a definite meaning whenever the sampling distribution of t lias a variance which decreases 
with l/n and a central value which is either identical with 0 or differs from it by a quantity 
which also decreases with l/n. Many of the estimators with which we are commonly 
concerned are of this type, but there are exceptions. Consider, for example, the Cauchy 
population 
dF = - —— , , — oo <x < oo' . . . (17.4) 
TC 1 + (#- 0)* " 
The mean (assuming that we conventionally agree that it exists) is at x = 0. But if we 
try to estimate 6 by the mean-statistic t we have, for the distribution of t, 
<£F = ~ - - -T ~ -,---, — oo<£<oo . . . (17.5) 
% 1 + (t — 0)2 
(Cf. Example 10.1, vol. I, pp. 233-4.) In this case the distribution of t is the same 
as that of any single value of the sample, and does not increase in accuracy as n increases. 
CONSISTENCE 3 
Consistence 
17.7. The property of possessing increasing accuracy is evidently a very desirable 
one ; and indeed, if the variance of the sampling distribution decreases with increasing 
n it is necessary that its central value should tend to 0, for otherwise the estimator would 
have values differing systematically from the true value and would be useless, not to say 
dangerous. We therefore formulate our first criterion for a suitable estimator as follows :— 
An estimator tn, computed from a sample of n values, will be said to be a consistent 
estimator of d if, for any positive e and r], however small, there is some N such that the 
probability that 
I h - d \<£ (17-6) 
is greater than 1 — 77 for all n > N. In the notation of the theory of probability, 
p { K -" I < e) > l - V> n>N. . . . (17.7) 
The definition bears an obvious analogy to the definition of convergence in the 
mathematical sense. Given any fixed small quantity e we can find a large enough sample number 
such that for all samples over that size the probability that t differs from the true value 
by more than s is as near zero as we please. tn is said to converge in probability to 6. Thus 
t is a consistent estimate of 6 if it converges to 6 in probability. 
Example 17.1 
The sample mean is a consistent estimator of the parameter d in the population (17.1). 
This we have already established in general argument, but more formally the proof would 
proceed as follows :— 
Suppose we are given e. From (17.3) we see that (t — 6) a/u is distributed normally 
about zero with unit variance. Thus the probability that \ (t — d) <\/n \ < e a/u is the 
value of the normal integral between limits i &-\/n. Given any positive ?/, we can 
always take n large enough for this quantity to be greater than 1 — rj and it will continue 
to be so for any larger n. N may therefore be determined and the inequality (17.7) is 
satisfied. 
Example 17.2 
Suppose we have a statistic tn whose mean value differs from 6 by order n~l, whose 
variance vn is of order n'1 and which tends to normality as n increases. Clearly 
(tn — 0)/^/vn will then tend to zero in probability and tn will be consistent. This covers 
a great many statistics encountered in practice. 
Unbiassed Estimators 
17.8. The property of consistence is a limiting property, that is to say, it concerns 
the behaviour of an estimator as the sample number tends to infinity. It requires nothing 
of the behaviour for finite n, and if there exists one consistent estimator tn we may construct 
infinitely many others ; e.g. 
n — a 
n ~~ 0 
is also consistent. We have seen that in some circumstances a consistent estimator of the 
mean is the sample mean 
ok 
n 
4 ' ESTIMATION: LIKELIHOOD 
But so is 
1 
x1 = Zx,u . . . . . (17,9) 
n — 1 
Why do we prefer one to the other ? Intuitively it seems absurd to divide the sum of 
n quantities by anything other than their number n. We shall see in a moment, however, 
that intuition is not a very reliable guide on such matters. There are reasons for preferring 
n 
3 = 1 
n 
I 
to 
— 111 ^' ~"^2 (17.10) 
\X<*-" <»■»> 
n 
3 = 1 
as an estimator of the parent variance, notwithstanding that the latter is the sample 
variance. 
17.9. Consider the sampling distribution of an estimator t. If the estimator is 
consistent, its distribution must, for large samples, have a central value in the 
neighbourhood of 6. We may choose among the field of consistent estimators by requiring that 
Q shall be equated to this central value not merely for large, but for all samples. Whether 
we choose as the appropriate central value the mean, the median or the mode is to some 
extent a matter of taste. We shall consider below what follows if we select the mode 
(which gives us the maximum likelihood estimators). For the present we discuss the mean. 
If we require that for all n the mean value of t shall be d, we define what is known as 
an unbiassed estimator : 
E {t) =0 (17.12) 
This is an unfortunate word, like so many in statistics. There is nothing except 
convenience to exalt the arithmetic mean above other measures of location as a criterion of 
bias. We might equally well have chosen the mode as determining the " unbiassed " 
estimator, in which case the mean estimator would be u biassed " whenever it gave a 
different result. Since the use of " unbiassed " in connection with the mean is fairly wide- 
spread, however, we shall continue to use it.* 
Example 17.3 
Since E Ji E {x) t = - Z {E (x) } 
[n ) n 
™—-. i y § r w.—— / / 
n 
the mean-statistic is an unbiassed estimator of the parent mean whenever the latter exists. 
But the sample-variance is not an unbiassed estimator of the parent variance. We have 
W !z[x - 1 £ [x)W 
E {Z{x -£)*} = jE IL 
n 
Ji 
= jE7 /— Z (aa) - 1 E (xf xk)\, j^Jc 
I lb lb 
= (n - l)fi2 — (n — l)^i2 
= (n — I) fi%. 
* The word has already occurred in vol. I, p. 200, in this sense. It may be spelt with either one 
or two s's. My usage, I am afraid, is not consistent, but in this volume I use two. 
EFFICIENT ESTIMATORS 5 
I n i 
Thus - E (x —- x)2 has a mean value a*. On the other hand, an unbiassed estimator 
n n 
is given by 
1 
E (x — x)%, 
n — 1 
and for this reason it is sometimes preferred to the sample variance. There are other 
reasons which will appear when we come to study the analysis of variance. 
Efficient Estimators 
17.10. In general there will exist more than one consistent estimator of a parameter, 
even if we confine ourselves only to unbiassed estimators. Consider once again the 
estimation of the mean of a normal population with known variance. The sample mean is 
consistent and unbiassed. We will now prove that the same is true of the median. 
Consideration of symmetry is enough to show that the median is an unbiassed estimate 
of the parent mean, which is, of course, the same as the parent median. For large n the 
distribution of the median tends to the normal form (cf. Example 9.7, vol. I, p. 213), 
dF oc exp {- 2nj\ (x - 0)2} dx ... (17.13) 
where fx is the median ordinate of the parent, in our present case 1/V(2?r) = 0-3989. The 
variance tends to zero and the estimator is consistent. Its variance is 7t/2n. 
17.11. We are therefore at liberty to seek for further criteria to choose between 
estimators with the common property of consistence. Such a criterion arises naturally 
if we consider the sampling variances of the estimators. Generally speaking, the estimator 
with the smaller variance will be grouped more closely round the value 6 ; this will certainly 
be so for distributions of the normal type. An estimator with a smaller variance will 
therefore deviate less, on the average, from the true value than one with a larger variance. 
Hence we may reasonably regard it as better or more efficient. 
If, of two consistent estimators tx and t2, we have var tx < var t2 for all n, then tx is 
more efficient than t2 for all sample sizes. It is possible to have var tx < var £2 for some 
ranges of n and var tt > var t2 for others, in which case the estimators are more or less 
efficient in different ranges. 
In the case of mean and median we have, for any n, 
a2 
var (mean) = —, . . . . . (17.14) 
n 
and for large n 
<> 
7t(S" 
var (median) = —, ..... (17.15) 
2n 
where a2 is the parent variance. Since n/2 = 1-57 > 1 the mean is more efficient than 
the median for large % at least. For small n we have to work out the variance of the median. 
The following values may be obtained from those given in Table XXIII of Tables for 
Statisticians and Biometricians, Part II :— 
n 2 3 4 5 
var (median) 1-00 1-35 1-19 1-44 
It appears that the mean is always more efficient than the median in estimating the 
parameter 6 for the normal distribution (17.1). 
6 ESTIMATION: LIKELIHOOD 
Example 17 A 
For the Cauchy distribution 
1 (1X 
dF ==— .. , -.---, — oo <a; < oo 
7Z 1 + (X - 0)2 
we have already seen that the sample mean is not a consistent estimator. However, for 
the median in large samples we have, since the median ordinate is 1/n, 
var (median) = 
4c7l 
It is seen that the median is consistent, and although direct comparison with the mean 
is not possible because the latter does not possess a sampling variance, the median is 
evidently a better estimator for 6 than the mean. This provides an interesting contrast with 
the case of the normal parent, particularly in view of the similarity of the parent frequency- 
distributions. 
J. / • jLj£ • Xn some cases, as we shall see below, there exist consistent estimators whose 
Sampling variance for large samples is less than that of any other such estimator. We 
shall call such estimators most-efficient. When they exist they provide a standard of 
measurement of efficiency. In fact, if t2 has variance v2 and the most-efficient estimator 
tx has variance vu the efficiency E of t2 is defined as 
E =^ (17.16) 
It will be seen later that in normal samples the mean is a most-efficient estimator, so that 
the efficiency of the median for such samples is 
2w 1 
= — . --= 0-637. 
ti n 
17.13. If we have a sample of 100 members the variance of the median (assuming 
normality) will be about the same as that of the mean in only 64 members. Thus, if 
sampling variance be accepted as a criterion of accuracy of estimation, the use of the median 
instead of the mean sacrifices about 36 observations in 100. It is not possible to economise 
by using a different estimator than the mean. 
Other things being equal, the estimator with the greater efficiency is undoubtedly 
the one to use. But sometimes other things are not equal. It may, and does, happen 
that a most-efficient estimate derived from. tx is more troublesome to calculate than an 
alternative tz. The extra labour involved in calculation may be greater than the saving 
in dealing with a smaller sample number, particularly if there are plenty of further 
observations to hand. 
Example 17.5 
Consider the estimation of the standard deviation of a normal population with variance 
or2 and unknown mean. Two possible estimators are the standard deviation of the sample 
(or the square-root of E (x — x)2/(n — X) if it is desired to use an unbiassed estimator) 
and the mean deviation of the sample multiplied by y/(n/2) (cf. 5.20). The latter is 
easier to calculate, as a rule, and if we have plenty of observations (as, for example, if we 
are finding the standard deviation of a set of barometric records and the addition of further 
SUFFICIENT ESTIMATORS 
members to the sample is merely a matter of turning up more records) it may be worth 
while estimating from the mean-deviation rather than from the standard deviation. 
In normal samples the variance of the mean-deviation is (9.13)— 
2 n 
71 
a2 ( —h <\/{n(n — 2)} — n + arc sin 
r*«^ 
G" 
n n2 V2 ' v ^~N~ ,} " ' n — 1/ n \ n 
The variance of the estimator from the mean deviation is then approximately 
1 - -) . (17.17) 
G 
71 — 2 
n 
(17.18) 
Now the variance of the standard deviation is (9.22) a2/2n, and we shall see later that it 
is a most-efficient estimator. Thus the efficiency of the first estimator is 
E 
G' 
2% 
g* 
n 
'tx - 2s 
= 0-876. 
7t 
The accuracy of the estimate from the mean deviation of a sample of 1000 is then about 
the same as that from the standard deviation of a sample of 876. If it is easier to calculate 
them.d. of 1000 observations than the s.d. of 876 and there is no shortage of observations, 
it may be more convenient to use the former. 
It has to be remembered, nevertheless, that in adopting such a procedure we are 
deliberately wasting information. By taking greater pains we could improve the efficiency 
of our estimate from 0-876 to unity, or by about 14 per cent, of the former value. 
Sufficient Estimators 
17.14. The comparison of the efficiencies of two estimators, as measured by their 
variances, may be made for any n, but the absolute efficiency as defined in 17.12 by relation 
to a most-efficient estimator is in the main a limiting property. We shall see below (17.36) 
that the definition may be extended to small samples and to non-normal variation, but 
most-efficient estimators for finite n do not exist so frequently in statistical practice 
as in the limiting case of large samples. Sometimes, however, there are estimators which 
may be regarded as the " best " for samples of any size, and we proceed to consider 
them. 
Before doing so, we prove that, in the limit, all most-efficient estimators tend to 
equivalence. 
More precisely, if two most-efficient estimators tx and t2 tend in the limit to be 
distributed in the bivariate form 
dF oc exp T- 2-ri- - {(tx - 0)« - 2p (t, - 6) («, - 0) + (t, -6)*} 
then the correlation p = 1. Here v is the variance of each estimator. 
Consider the estimator 
ILi = -g- (Cj[ ~f- t2). 
Clearly ux is consistent since tx and t2 are both so. Putting 
Uz = J (tx — t2) 
we have, for the joint distribution of ux and u2, 
1 
dtxdU, . (17.19) 
dF oc exp 
2v (1 - p2) 
{2 (1 -p)(% -0)2 + 2 (1 +p)ul} 
dux du^ 
(17.20) 
8 ESTIMATION": LIKELIHOOD 
Thus u2 is distributed independently of u± and 0 and we have 
v(l — p2) 1 +p 
var ^ = - j-^^y = —~- v (17.21) 
Now tx is a most-efficient estimator and hence 
X "~j p 
2 
v = var Mi > var ^ = v 
giving _±£ > 1 (17.22) 
But /> cannot be greater than unity and hence p = 1, which proves the theorem. 
17.15. Consider once again the estimation of 6 in the normal population (17.1). 
The joint distribution of the sample is given by 
dF = exp j - \Y* (Xj - 6)A dxx . . . dxn . . (17.23) 
(2^)2 l / = 1 J 
We have the familiar result 
n 
JT (x, _ 0)2 = £ (& - £)2 + rc (x - BY, 
and hence 
dF = exp J — 7 (x —- 0)H exp {— \ S (x — x)2} dxx . . . dxn . (17.24) 
(2*r)5 ^ 2 J 
Thus the frequency function of the distribution of x's (which is equivalent to the likelihood 
function) can be factorised into two parts, one depending on x and 0, the other depending 
on the x's but not on 0. 
The quantity x is then said to be a sufficient estimator of 6 ; and generally, if the 
likelihood function is expressible in the form (as a product of two frequency functions)— 
L (xl9 . . . xn) Q) = Lx (t, 0) L2 (a?!, . . . xn)9 . . (17.25) 
where Lt does not contain the x's otherwise than in the form t and L2 is independent of d, 
t is said to be a sufficient estimator of 6. 
17.16. As so defined, a sufficient estimator, if it exists at all, is unique except that 
if t obeys the relation (17.25) any function of t will obviously also obey the same relation. 
From all such functions we must evidently choose one which gives a consistent estimator 
and can sometimes, as in the example of the previous section, find the estimator which is 
unbiassed. Apart from such ambiguities, which offer no difficulties in practice, the property 
of uniqueness holds. For if tx and t2 were two different sufficient statistics, not functionally 
related, we should have— 
and hence 
■^1 (&1> ") __ -"^2 c\n 9AY 
. — ——, , , . . . . (l/.ZO/ 
Mx (t2, 6) L% 
Since the expression on the right does not contain 6, Lx must be a factor of Mx and 
moreover the quotient must be a constant; for if it were a function of the x's that function 
would have been assimilated to L% or Ma. 
SUFFICIENT ESTIMATORS 
9 
Hence 
Lx (tl9 6)^hMx (t29 6)9 
and this cannot be so unless tx and t2 are functionally related. 
17.17. The fundamental property of sufficient estimators derives from the following 
theorem :— 
If tx is sufficient and t2 is any other estimator of d (not a function of tx) the joint 
distribution of tx and t2 may be put in the form 
dF = fx (tx, 6) f2 (t2) tx) dtx dt2, . . . ' . (17.27) 
where/2 does not contain 6. Conversely, if (17.27) holds for every t2 then tx is sufficient. 
Before proving this result let us notice its importance. From (17.27) it follows that 
for any given tx the distribution of t2 is equal to f2 (t2, tx) dt2, i.e. is independent of 0. 
Consequently, if we know tl9 the probability of any range of values of t2 is the same for all d. 
The distribution of t2 given tx, therefore, can throw no light whatever on 0. Thus, a 
knowledge of tx gives all the information that the sample can supply about 6 and no other 
estimator can add anything to it. We are clearly justified in such circumstances in 
describing a sufficient estimator as the "best". 
Now as to the theorem itself. The direct part is easily proved. In fact, we have from 
(17.25)— 
JLj \XX^ ... X,„9 t7) CLXX 
Make the transformation 
dx, 
n 
L1 (tl9 0) L2 (xl9 
71) ^™ 1 
* \AJ\AJm * 
y* 
y* 
The element of frequency becomes 
JL>,, 
n 
. (17.28) 
Lx(tx, 0)L2(xl9 . . . xn) J^'X*\dy1 . 
• dyn 
. (17.29) 
where the t's and x's are to be expressed in terms of the y'a. We have excluded the case 
when t% is functionally related to tl9 and hence the Jacobian d (xl9 x2) /d(tl9t2) does not 
vanish identically. The frequency element of yx and y2 is then obtained from (17.29) by 
integrating out the other variables. Since yx and y2 are equal respectively to tx and t2 
this process will leave unchanged the function Lx (tx, 6) and reduce the other part to a 
function of tx and t2, say f2 (tl9 t2). Writing fx for Lx we then have 
dF =fx(tx, 6)f2(tx, t2)dtxdt2, 
as stated in the theorem. 
The converse is a little more difficult. Let tx be sufficient and make the transformation 
yx = tx, y2 = x2, etc. The joint distribution of sample values becomes 
dtx 
jj [xl9 . . . xn) — h {tl9 y2, . . . yn) 
dxx 
. (17.30) 
Since tx is independent of 0, so is dtx/dxx. Hence, if the distribution of tx is / (tx) dtl9 L' may 
be written 
/ (tx) L" (tl9 y2, . . . yn)9 . . . . (17.31) 
and the converse will be established if we can show that L" does not contain 6. This we 
B 
10. ESTIMATION: LIKELIHOOD 
do by demonstrating that if there are values y'% . . . yn for which L" assumes different 
values for different values of 6 then the joint distribution of t1 and £2 cannot be independent 
of d, which contradicts our hypothesis. 
Suppose, then, that for two values of 0, say 6X and 02, 
L"(ti,y'2> * • • yn)ex = L" (tn y* • • • yn)&* + 2a> • • (17.32) 
where a is not zero. Consider a new statistic tz defined by 
il = £(yi-y'y (17-33) 
Assuming that L" is continuous in the y's, we may determine a value of t3} say t$9 such that 
L"(tl9y29 . . . yn)di>L" (tl9yi9 . . . yn)e§ + <* • • (17.34) 
everywhere inside the range of values bounded by 
Then for any fixed tt the total frequency inside this range is obtained by integrating L* 
•over the appropriate values, and we shall find, in virtue of (17.34), 
fex>fo^ ...... (17.35) 
the fs referring to total frequencies. 
But if the joint distribution of tx and tz is 
dF = h (tl9 t2)o dtx dt2 
we have for the frequencies /, 
Jo 
J 0 
and hence 
J( 
3 
{& (h, tt)d - h (tu tz)dl) dU > 0, 
0 
so that the joint distribution cannot be independent of 6. 
The above demonstration relates to the case when the frequency functions are 
continuous. In the discontinuous case the argument simplifies and we leave it to the reader 
to supply the proof. 
17.18. We now prove an important further result to the effect that a sufficient 
estimator is most-efficient, provided that a most-efficient estimator exists. We assume 
that the joint distribution of the sufficient estimator tx and any other estimator t2 tends 
to normality for large n, say in the form 
dF oc exp 
(h - 0)* 2P (tt - 6) (tt -6) (tt - 6) 
dtxdt, . (17.36) 
2(1 — P2) I Vi V(*>i*>a) v* 
where vt and v2 are the variances of tx and t% respectively. Since tt is sufficient, the 
distribution of t2 given tt does not contain 6. Now the distribution of t± is 
dF oc exp (-t-^1 ""9)2j dtx .... (17.37) 
SUFFICIENT ESTIMATORS 11 
and hence that of i2 given tx is 
dF cc exp 
i r (h - ey _ 2P (fi - e) (f, - e) («, - 6)n x ^ - g) 
2(i -p2) 1 »x V(«i».) ». J »x " 
2" 
^2 
<tta . . (17.38) 
which reduces to 
If this is not to involve 6 we must have 
p = / J. = y^ where E is the efficiency of t2. . (17.30) 
Since p < 1 it follows that vx < v2, i.e. t± has a smaller variance than any other estimator. 
Consequently, if there exists a most-efficient statistic, tx itself is most-efficient. 
17.19. The criterion of sufficiency is not a limiting property. A sufficient estimator 
is best for any sample size since it gives all the information about 6 that the sample can 
give ; and it is most-efficient for large samples. If we could always find a sufficient 
estimator our problem would be solved, but unfortunately sufficiency is the exception 
rather than the rule. 
Example 17 £ 
The frequency element of a sample of n from the population 
j r (f£ tyi\2 
dF *= a~V(2n) 6XP J" 4 h^- > dX 
can be put in the form 
n—l . 
ns1 
dF = _^_ exp J - n {* - m)2 1 " 2 . v e"^*»-8 ds «fe« 
(Cf. Example 10.5, vol. I, p. 238.) 
If we know a, then, as we have already seen, x is sufficient for m. But if we know 
m, $ is %o£ sufficient for g. In fact, the factorisation in the above equation requires the 
appearance of a in the element relating to x, and we cannot separate a factor containing 
^ and a alone or the remaining variables alone. 
• This is what we might expect. If we know the real mean m there is little point in 
preferring the sample variance 
to the second moment 
1 
s2 = - £ (x — x)2 
n 
s'2 = — E (x — m)2 
n 
-as an estimator of the parent variance. The distribution of s' is given by 
n 
n2 m'* 
dF = , : e 2<?« (s')n-2 ds'2 
n ,n 
(ww n § 
12 ESTIMATION: LIKELIHOOD 
and this embodies the whole of the frequency element of the sample, apart from differentials 
in the other variables. Thus s' is sufficient for a. 
17.20. This completes the first stage of our inquiry. The criteria of consistence, 
efficiency and sufficiency provide standards which we shall look for in " good " estimators. 
Of themselves, however, they do not provide any systematic way of deriving estimators 
which obey them. We shall now consider various methods which have been proposed for 
providing estimators and examine how far they conform to our criteria. The most 
important method is that of maximum likelihood, which will occupy the remainder of this 
chapter. In the next chapter we shall consider four others, the method of minimum 
variance, the method of minimum #2, the method of least squares, and the method of 
inverse probability. 
Maximtcm Likelihood 
17.21. If the frequency function of the parent population is f (x, 6), the likelihood 
function of a sample of n is, by definition, 
L =/(&!, 0)f(xz, 6) . . . f{xw d) . . . . (17.40) 
The Principle of Maximum (or Maximal) Likelihood then states that if there exists a statistic 
t = t (xt, . . . xn) which maximises L for variations of d, then t is to be taken as an 
estimator of 6. In short, t is the solution (if any) of 
_ = 0 -— < 0 (17 41) 
Since L is positive, the first equation is equivalent to 
a form which is frequently more convenient. 
There is one small point to notice here. In our usual convention, if a frequency 
function has a finite range, we regard it as defined from — oo to ~f oo but as zero outside 
that range. In this chapter we shall occasionally meet the reciprocal of/, which is undefined 
for zero /. Unless the contrary is specified we shall suppose that where / is zero 1 // is also 
to be regarded as zero. This will enable us to continue to regard the range as infinite, but 
some care is necessary where / is assumed everywhere continuous, for discontinuities may 
appear in / and l/f at the terminals of the finite range. The point becomes important 
when we try to make certain existence theorems rigorous. 
17.22. In sections 7.27 to 7.31 we touched on the principle of maximum likelihood 
from the point of view of statistical logic. We pointed out that its adoption required a 
new postulate in the theory of inference, but referred to the fact that the principle was 
recommended by the statistical properties of the estimators to which it leads. We now 
proceed to prove a series of theorems about these estimators, from which it will be seen 
that the posterior recommendation, so to speak, is very strong. In fact, maximum 
likelihood estimators are consistent, tend to normality for large n, have minimum variance 
in the limit at least, and provide sufficient statistics where such exist. 
17.23. The reader may feel convinced intuitively that maximum likelihood estimators 
MAXIMUM LIKELIHOOD 13 
are consistent, in which case he can pass to the next section. We shall now prove the 
result formally. 
(a) If the frequency function / (x, 6) is continuous in x throughout its range, and 
(b) if f (x, 6) is continuous and monotonic in 6 in some 0-interval containing the true 
value of d, say dQi and for all x in some ^-interval, 
then the maximum likelihood estimator of d} say t, is consistent. 
Our proof will also cover the case of discontinuous variates which can be reduced to 
the continuous case hy replacing each value by an interval in which the frequency is 
uniformly distributed. 
We first eliminate an inconvenience due to the infinitude of the range. In fact, if the 
range is infinite we make the variate transformation x = tan y. The conditions (a) and (6) 
remain true of y, and the maximum likelihood estimator in x transforms to that in y. We 
may therefore take the range as finite. 
The next step is to reduce the case to one of grouped frequencies by dividing the range 
into m intervals, the width of the jth interval being lp (We shall decide on the actual 
values of the Z's below.) Writing 
fj = ljf{x,d)dx, ..... (17.43) 
we have, in virtue of the continuity of/ in x, that/^/^ differs as little as we please from 
/ [xp 6). Then if L' is the likelihood of the grouped data, proportional to 
where ttj is the number of observations in the jth interval, we have, except for constants, 
and this will differ arbitrarily little from the logarithm of the true likelihood 
n 
logL = JTlog/(a:,, 0), (17.46)" 
provided that we take m large enough and the Vs in consequence small enough. 
Hence we see that if t is the estimator which maximises L and t* that which maximises 
L', in virtue of hypothesis (b) that L and U are continuous in 0, t and t' will differ as little 
as we please for any given values of the %'& and that uniformly. We may therefore prove 
our theorem for the finite number of variables n$ and infer its truth for the continuous 
case by proceeding to the limit. 
In different samples the n$ will vary, subject only to the condition that S (7ij) = n. 
Let us choose the ranges 1$ such that fj (d0) = l/m for all j, that is to say, such that the 
frequencies in all intervals are equal when 0 takes its true value dQ. Consider the likelihood 
function 
m 
K = y n$ log Zp ..... (17.47) 
where the s's are subject only to the condition 
-27(2) = 1 (17.48) 
14 ESTIMATION: LIKELIHOOD 
We consider three values of K defined by particular values of the jz's. 
(a) When Zj = n^/n, K is a maximum, say KR. For we have 
n1 
z. 
6K = 27 ^ &, 
o = 2; &,., 
w2 Z (n) 
~~ z9 ~~ " E (z) 
and hence 
n-i n» E (n) 
S{z) 
(b) When zi = fy (0O) = l/w&, K is, say, i£M. 
(c) When the estimator t' assumes the value, say, £0' corresponding to the n/s, and 
hence Zj = jfy (^), iT is a maximum, say KZi among the particular set of values of d for 
which Zj =/^ (0) ; for this is our definition of t', 
We have at once that 
Now, as the sample increases, the observed n^/n converge in probability to their 
theoretical values/;. (0O) = 1/m. Since K is continuous in the z's, KR — KM will converge 
to zero in probability and, from (17.49), so will KR — Kz. 
Now we show that this entails that each of 
n 
does so, it will be enough 
l/,(*o)-/y(0.)| 
converges to zero in probability. In fact, since fa (0O) 
to prove that the same holds for 
fj(Q --• • (17.50) 
Let Kx be the maximum of K for some fixed zt. Then KR > Kx and 
7? -**. T\,f i^" Jtx. j Jtx. n j. 
Hence Kt — KM converges to zero. The maximum Kx is readily seen to be given by 
z ^MIZL^, j = 2 w . . . (17.51) 
n — %! 
it, = nx log ^ + (n - %) {log (1 - zx) - log (w - nx)} + JT ^ log rij, (17.52) 
Now 2i is a double-valued function of Kx, continuous and having its two values equal 
for Kx = if^ ; for Kx is continuous in zx from 0 to 1 (not inclusive), and —-1 changes sign 
oz x 
only for zx = %/ft, where i£x = KR. It follows that when KR — A\ is small, so is 
«i — %i/%. If the other z's are not given by (17.51) KR — K is smaller still. 
A similar argument applies for any^, and hence Zj — ^ converges to zero in proba- 
To 
bility when jK^ — K does so. Taking zi = fj (t0) and remembering that in this case K 
becomes Kz, we reach (17.50). 
Finally, by hypotheses (a) and (b) at least some of the fa (Q) have continuous inverse 
functions expressing 6 in terms of the functions /, and hence by taking 
MAXIMUM LIKELIHOOD 15 
as small as we please, we may make t0 — 0O as small as we please. Consequently f 
converges to do in probability and is consistent. 
17.24. The reader may find the foregoing proof easier to follow if we express its 
main points in geometrical terminology. 
Consider the m proportions n^/n as the co-ordinates of a point in a space of m 
dimensions. The theoretical frequencies 
fj (d0) = 1/m define a point, say M, in 
this space, and the sample point B, 
corresponding to an observed set of n/s, may 
be regarded as varying round the " 
theoretical " point M. The quantities z are 
the co-ordinates of any point in the hyper- 
plane Z (z) = 1, which contains M and R. 
(See Fig. 17.1.) 
Now, for any sample point B the 
maximum likelihood estimator t' assumes 
a value £0' which in general differs from 
0O. This value defines m quantities fa (t'Q) 
which determine a point Z. This also ^ jrIG. n.i. 
lies in the hyperplane since the sum of 
the frequencies is unity. Thus the points B determine a set of points Z which all lie on 
the curve defined for variations in 0 by 
zj =fj{d) (17.53) 
Since 0 = 00 is a possible value of 0, the point M lies on this curve ; B in general does 
not. 
What we have shown in analytical form is that the function K, which is the logarithm 
of a likelihood function defined for any point on the hyperplane, has a maximum at R 
and a maximum on the curve itself at Z. As the sample size increases, B is as near as 
we like to M (in the sense of convergence in probability, that is to say, that as high a 
proportion of points B as we like are as near as we like to M). This involves that Z also is as 
near as we like to M. This in turn involves that the parameter-value t'0 corresponding to 
Z is as close as we like to 0O for as high a proportion of the possible points Z as we like, 
which is our theorem. 
17.25. We now prove a second fundamental property of maximum likelihood 
estimators, namely that they tend to normality for large n. More precisely, 
(a) If condition (a) at the beginning of 17.23 is satisfied ; and if (more stringently 
than condition {b) of that section) (c) in a 0-interval containing the true value 0o, 
-™ is continuous in 0 for every x, x2 -~ approaches a continuous function of 0 as # 
df 
tends to infinity, and ~ does not vanish in some interval, 
then the maximum likelihood estimator t tends to normality for large n. The condition 
as to x2 4k ensures that in the transformation to finite range ~ remains continuous in 8 
BO do 
throughout that range. 
16 ESTIMATION: LIKELIHOOD 
We recall that if 
£. r=r — , . . . . . ^1 /.04:J 
J Yb m 
that is, if the f's are the deviations of the actual proportional frequencies n^/n from the 
" expected " frequencies l/m, the distribution of the £'s in the limit will be normal and their 
distribution spherically symmetric. Consider again the orthogonal space of the previous 
section. The sample points are distributed about the point If in a symmetrical form which 
tends to normality. If we choose a set of orthogonal axes in the hyperplane, the projection 
of the sample points on any axis is in the limit distributed normally with variance l/mn. 
In the neighbourhood of M the curve (17.53)'approaches its tangent line as n becomes 
larger, and we therefore have, if s is the distance along the tangent from M, 
S3 = (0_0O)*2, j^/,-(0o) ", .... (17.55) 
as follows from (17.53). (The tangent exists in virtue of our hypothesis as to the differential 
coefficients of/in 6.) 
Now consider the point Z on the curve corresponding to the sample point R, We 
know that at Z the function 
K — En, log (Zj + — j, ..... (17.56) 
where we now measure z from M, is a maximum for variations in z such that Z lies on 
the curve. R is determined by finding the hypersurface (17.56) tangent to the 
hyperplane E {Zj) = 0, for at that point dK/dzj is zero. We know that the co-ordinates of 
this point are Zj = n,/n — 1/m and that R is the point of tangency. KR as defined in 
17.23 is the value of K at R, and Kz is that at Z. We then have, by Taylor's theorem, 
jC\ rr JlV 
JR 
*£(!&*+ *£(£*).** ■ •«"•"' 
to the second order of small quantities in dz. From (17.56) we see that 
dK 
dZj 
n . ... . . . (17.58) 
= 0, j ^ k 
dz, dzk 
'ft2, 
j = k 
Hence 
j 
(17.59) 
Kz = KR + nZ{dzA -Vle&^L . . . .(17.60) 
2 ^ 
Now E {dz/) = 0, for the variation takes place in the hyperplane. Hence, for given R, 
(dz-)2, 
Z is the point for which E—*— is a minimum. As n tends to infinity the n/s tend to 
nj 
equality, and hence Z is the point on the curve which is nearest to J?. Thus R is, in the 
limit, projected orthogonally on to the curve, that is to say, in the limit, on the tangent 
line. 
Now we know that these points are distributed normally with variance l/mn and 
MAXIMUM LIKELIHOOD 17 
this proves the theorem. We may also evaluate the variance of the maximum likelihood 
estimator; for 
^t var s 
varr = 
__ f. (A\ I 
dOJ* 
mn'E {irzfj (0) 
and since f approaches t for fine grouping we have also, remembering that l/m =/5- (0O), 
^Tt~n)_„ [do) f 
= n\[ (~W^YfiX9 ' (17-62) 
where 0 is to be put equal to 0O on the right. 
It may be remarked that condition (c) at the beginning of the section prevents the 
vanishing of ~ which might render the expression (17.61) nugatory. 
ou 
17.26. We have, then, under the afore-mentioned conditions, 
1 =nE^logf" 
var t \ B6 
If the range is independent of 6, or if/ and ~~~ vanish at any extremity of the range which 
depends on 0, we have the alternative form— 
jL- = -nE(?^£) (17.63) 
var£ V dd J 
rb 
In fact, since f dx = 1 where a, b are the limits of the range and may contain 0, we 
J a 
have * 
o = * 
36 
fdx - [bf^ldx + /(M)§ -/(M)|j 
a J a 
c", /a log a , 
Differentiating again, we have 
t3 O 
n fft/31og/Vrj , ("f^]ogf\,, t /.,d log f\db f.dlogf\da .._.., 
0 = U~ar) /d*-HJA~^ )7rfa + (y-^i Jig -{l-W-)m- ■ (17-64) 
Again, if the range is independent of 0 or if ( ~ ) vanishes at the extremity, the last two 
* The operation of differentiating under the integral sign requires certain conditions as to uniform 
convergence, even when the limits are independent of Q. To avoid prolixity we shall always assume 
that the conditions hold unless the contrary is stated. The point gives rise to no statistical difficulty 
but is troublesome when one is aiming at complete mathematical rigour. 
A.s.—II 0 
18 ESTIMATION: LIKELIHOOD 
terms on the right in (17.64) are zero, and we have (reverting to our usual convention as 
to limits) 
and the result follows from (17.62). 
17.27. We now prove a third fundamental property concerning the efficiency of 
maximum likelihood estimates. 
If t be any estimator of Q, the range of/ (x, 6) is independent of 6, and in large samples 
t is distributed normally about mean 60 (the true value of 6) with variance v ; then 
— cannot exceed (—™~- ) fdx, with 8 = 00; 
nv J„oo \ od J 
and hence, if a maximum likelihood estimator exists, it is most-efficient in the class of 
such estimators. 
By hypothesis, we have in the limit for the frequency function of t, 
0 = ^— exp I- (LziTl .... (17.65) 
V(2ot) l \ 2v ) v 
and hence 
a2log0 1 
00" V 
where, for convenience, we drop the suffix of 6 until the end of the proof. We then have 
Now consider 
1 = f _3'l06% 
dt 
M i /a$v 
<ft (17.67) 
<u= (logi) ...... (17.68) 
as a random variable over the possible values xx . . . #n conditioned by t = constant* 
Since the frequency of m is Z, we have 
m""lW {^W* .... (17.69) 
with summation (or integration) over the range of x's. Now 0 is the frequency of all 
samples having a constant t, and hence 
0 = E (L). 
Hence 
2(Lu*) {Z{Lu)}* 
var u = 
0 $2 
-Wx(s)"}-*K£» • ■ •<1"0) 
Now varw cannot be negative and 0 is not negative, and hence 
^)2BM^>°- • • •<"•»> 
MAXIMUM LIKELIHOOD 19 
But I(dJi\=±(EL) = d^, 
and hence, substituting in (17.71) and integrating over all t, we have 
1 /dLY 
Alio) J**)™0{T6) 
Now E is carried out over all x for constant t and the integration over all t, so that the two 
summations together are equivalent to summation over the x's without restriction. Hence 
&tzr^[~\\> *4(5Y = -- • ■ .(17.72) 
v 
QlOC/n 
< I ... L ( 1— ) dxx . . . dx, 
J-oo J-oo V d6 J 
r- /3iog/\2 
^ n 
•00 
dB 
fdx 
which establishes the result, since the expression on the right is the reciprocal of the variance 
of the maximum likelihood estimator, if it exists. 
17.28. The fourth fundamental theorem of maximum likelihood estimators is as 
follows :— 
If a sufficient estimator exists, it is a function of the maximum likelihood estimator. 
In fact, the likelihood can then be put in the form 
L = Lx (t, 6) L2 (#!... xn), 
where Lo does not contain 0. Hence 
\J 1 -J- t/ "I "W" 
-logi-^logZ, 
= rp (0, t), a function of 0 and t only. . (17.73) 
Hence, for fixed t, —r. log L is constant, and it follows from the previous section that the 
otl 
variance of t is equal to the variance of a most-efficient estimator (for var u is then zero 
for fixed t and the inequality (17.72) becomes an equality). Hence the sufficient estimator 
is most-efficient, confirming the result of 17.18. 
It follows from (17.73) that the maximum likelihood estimator is given by 
ip(0,t) = O, (17.74) 
which proves the theorem. 
Conversely, if t is such that (17.73) is true, it must be sufficient; for then we have 
log L = C + J y (0, t) dd, 
where C does not depend on 0 and the likelihood is of the requisite form. 
Example 17.7 
Consider the estimation of the parameter m in the population 
20 ESTIMATION: LIKELIHOOD 
where a is known. The frequency function is easily seen to obey the conditions relating 
to maximum likelihood estimators. We have 
log L = - n log g V(%n) - ^i ]£ (xi - m)2> 
and hence the maximum likelihood estimator is the root of 
—-logL = -■ £ (x ~ m) = 0, 
dm g* 
giving m = 1 27 (a;) = x. 
n 
It is frequently convenient to denote the estimator of a parameter by writing a 
circumflex accent over it in this way. 
In this case the sample mean is the maximum likelihood estimator. It is therefore 
most-efficient and no other estimator can have a smaller variance in the Umit. For the 
variance we have, from (17.63), 
var m 
J-» \ d02 Je-m 
«00 
n \ -^~ f dx 
J-oo or2 
n 
(7 
giving the familiar result- 
cr2 
var x = —. 
m 
71 
This, as it happens, is true for any n. The estimator is also sufficient, for 
log L = ~- (wa; — ww) 
dm a2 
= a function of m and £ only. 
The condition that a2 is known is to he noted. Complications arise when two parameters 
are estimated- simultaneously, as we shall see presently. 
Example 17.8 
Consider the estimation of 6 in the Type III distribution 
%p — l a — x/O 
dF = _——-_ dx, 0 < x < co 
r(p)0v 
where p is known. 
We have 
log/ = (p - 1) log x - I - log r(p) -p log 0 
and hence, dropping terms independent of 6, 
1 
log L = — ~ 27 (a?) — ftp log 0. 
0 
MAXIMUM LIKELIHOOD 21 
The equation of maximum likelihood is then 
giving 
The variance is given, by (17.63), 
1 _ 
var § 
— 
*"N 
var 6 = 
np 
as 
-u- 
\d2 
62 
,—? 
X 
^2x + p\fdx 
T2J ' 
np 
where d is the true value of the parameter. We could also have obtained this result directly 
(and again it happens to be true for all n). From Example 10.11 (vol. I, p. 244) we have 
for the distribution of x/p = d, 
dF = n"* (%) V d / dd, 
W r(np) 
from which the first two moments about the origin are 
' _ f) ' __ %P + l /)2 
//l — P, //2 W , 
np 
02 
giving var 0 — /u = —-. 
np 
We note that the likelihood function may be put in the form 
log L = (p — I) Z log x — n log F (p) — -±— — np log 0, 
from which it is evident that 6 is sufficient. 
Example 17.9 
Consider the estimation of the parameter 1 in the Poisson distribution whose general 
term is e S- 
XI 
In this case the likelihood function is discontinuous and we have 
«/ }[ • . a • ^fi, • 
Hence -^-log L = — n + —, 
giving I = *, the sample mean. 
22 ESTIMATION: LIKELIHOOD 
For the variance we have 
oo 
1 &/x x& 
var A ^o ^ ^" 
n 
J 
var A = -, a familiar result, 
n 
It is easy to see in this case also that X is sufficient. 
Example 17.10 
What is the most general form of distribution, differentiable in d, for which the sample- 
mean is the maximum likelihood estimator ? 
We are given that a solution of 
2 log £-27^ dA =0 
dd * \f dd) 
1 
is 6 == - £ (x) 
n 
or £ (x — 6) = 0. 
This is true for all # and 6, and hence 
/I-<*-*>*• 
where K is independent of x but may be dependent on 0, say equal to -^~. Then, 
integrating, 
log/ = J dfl (* - 0) O 
a2^ 
^x ~~ ^ ae + v + ^ ^ 
where 4" (#) is an arbitrary function of x. Hence 
/=yfcexp f(x-0)^ + y,(0) + £(*)}, 
which is the most general form of/. 
If f (6) = P2, C (x) = - Jx2 
the form becomes the normal distribution 
/ = *exp {-£(* -0)2}. 
Successive Approximations to Efficient Estimators 
17.29. In the examples we have just given, the solution of the maximum likeUhood 
equation was carried out without difficulty. It frequently happens, however, that the 
equation is by no means so easy to solve explicitly, though it can sometimes be solved 
SUCCESSIVE APPROXIMATIONS TO EFFICIENT ESTIMATORS 23 
for particular values of x by iterative methods. Another possibility is to compute an 
inefficient estimator and correct it by an extra term, which can be obtained as follows :— 
Let tr be an inefficient estimator and t a most-efficient estimator. Let 
6 = t' - t. 
Then var d = var t' + var t — 2 cov (t', t). . . . (17.75) 
Remembering that if E is the efficiency of tf, 
var t = E var f 
^l^jJl = ^E (see (17.39) ) ; 
(var t var J')* 
we have 
1 — E 
vard= —-—var 2. ..... (17.76) 
If then t' is " nearly " efficient, that is, if 1 — E is small, the average value of d = t' — t 
will be small. 
If the maximum likelihood equation is 
consider 
t" = t' + var t { '^f-^ ) .... (17.77) 
We have 
1f>) = ( =1 - I +(<'-<)( ^| ) + terms of higher order 
For large n, approximately 
and hence, approximately, 
Hence 
I ___ ZdHogL 
var t V dOz /^/ 
aiogLX __t -V 
30 Jth^r var t" 
r == £ -f var 11 —^~— 1 
\ 30 J0=i> 
= t' +t -t' 
and tf" is an efficient estimator to a better order of approximation. This process may be 
repeated and, rather like Newton's successive approximation to the roots of an equation, 
may be expected to improve the efficiency of an estimator. 
Example 17Jl 
Suppose we have to estimate d, the parameter in the Cauchy population 
24 ESTIMATION: LIKELIHOOD 
We have already seen that the sample-mean is not a satisfactory estimate and that for 
large samples the median is consistent and has variance n2/4n. 
The equation of maximum likelihood gives 
dlogL = y f 2 (g - fl) ] Q 
This is a (2rc-l)-ic in 0 and correspondingly difficult to solve. We may, however, 
find the variance of the solution 0 from (17.63). We have 
f00 a2 log/ . , _lf 2 (x - 0)2 - 2 
Hence 
2 
var 0 = -. 
The median, therefore, has an efficiency of — = 0-8, and we expect that 
fi/8 log L 
t = £' + var 0 ' ° 
d0 /0=r 
- r - - 2 
4 „ r x - // i 
rc t1 + (*- 02J' 
where £' denotes the median, will be an improved estimator. 
Most General Form of Distributions possessing Sufficient Estimators 
17.30. If t is sufficient for 0 we have 
^^=K(t,6), (17.79) 
ou 
where K is some function of t and 0. Regarding this as an equation in t we see that it 
remains true for any particular value of 0, say zero. It is then evident that t must he 
expressible in the form 
r n 
t^M<^k(xj)V (17.HO) 
v. j—1 J 
where M and k are arbitrary functions. If w = 2 k (x) then K is a function of 0 and 
w only, say N (t, w). We have then 
dddxt " a^ dxZ (I7*si) 
"3 v vu vw7 
Now the left-hand side is a function of 0 and x* only and w is a function of x4 only. Hence 
dN . 
— is a function of 0 and ^ only. But it must be symmetrical in the a?'s and hence is a 
function of 0 only. Hence, integrating with respect to w, we have 
N (t, w) = wp (0) + q (0), 
DISTRIBUTION OF SUFFICIENT ESTIMATORS 25 
where p and. q are arbitrary functions of 6. Thus 
1 (logL) = il {log/(a,, 6) } = p (6) Ek (x,) + q (6) . . (17.82) 
7i 1 
whence ^ log/ (x, 0) = p (0) h(x) + ~q (0), 
ov n, 
giving / (x, 6) = exp {p (6) Jc (x) + q (d) + r (x)}9 . . . (17.83) 
where we still write p and q for the integrated functions. 
The expression may also be written 
f(x, 0) = Q {6) E (x) exp {p (6) k {x)} . . . (17.84) 
or, if we simplify the specification of the distribution by writing 0 instead of p (d), 
f(x)=Q (6) R (x) exp {d h (x)}. ■ . . . . (17.85) 
It will be found that if (17.85) holds, the likelihood function is of the required form for 
the existence of a sufficient estimator, so that the equation is sufficient as well as necessary. 
Distribution of Sufficient Estimators 
17.31. It is remarkable that the distribution of a sufficient estimator can be obtained 
directly from the likelihood function. From (17.85) we have 
log L = n log Q +• E log R (x) + 6 E h (x) 
giving, for the maximum likelihood estimator, 
£ H + E h (x) = 0 (17.86) 
Now, for the characteristic function </> (a) of w (= Eh (x)) we have— 
</>(oc) = ... eiwaf(x1,6)dx1 . . . f{xni0)dxn 
J — 00 J —00 
— 00 
elkWctfdx 
C oo . ^ n 
Q (0) B (x) e^+0)kM dx 
- oo 
/ <W> . V* (17.87) 
Hence the frequency function of w, if existent, is 
J ' 27rJ_oc \Q(0 +*a) r 
Now from (17.86), 
= n S (t), say, 
and hence the frequency function of the estimator t is 
fm-*(Ti)Lr~"{w¥s>>*- ■ •(I,-8S> 
26 ESTIMATION: LIKELIHOOD 
Example 17.12 
The normal distribution with unit variance may be put in the form 
l 
Comparing this with (17.85), we see that if 
Q (d) = e~w 
K \XJ —— Jb 
the condition for a sufficient estimator is satisfied. That this is (as we already know) 
the mean x may be confirmed from (17.88). We have 
S(6)= -±logQ=d; 
and hence for the frequency function of the estimator x, 
n 
2jz 
e-imx J e ^ doL 
/•QO 
exp {— -|wa2 — ian(x — 6)} da. 
2n m 
— 00 
Example 17.13 
The Type III distribution considered in Example 17.8 may be put in the slightly 
different form 
dF = -£— tP'1 e~yx dx, 0 < x < oo. 
r(p) 
Regarding p as known and considering y as the parameter under estimate, we see that 
a sufficient estimator exists, because we may write 
R(x) =xp-1 
k (x) = x, 
which throws the distribution into the form (17.85). We have found the estimator and 
its distribution in Example 17.8. 
On the other hand, suppose that y is known and we wish to estimate p. Writing 
B (x) =e~'yx~logx 
k (x) = log x 
we see that a sufficient estimator for p also exists. It is the solution of 
log r (p) + log y + - S log x = 0, 
dp n 
SUFFICIENT ESTIMATORS WHEN RANGE DEPENDS ON PARAMETER 27 
which does not permit of expression of p as a simple function of the sb's. The sampling 
distribution is not expressible in a simple form. 
Example 17.14 
Consider again the Cauchy distribution 
dF = — --, — oo < # < oo. 
n I +(x - 0)2 
Evidently this cannot be thrown into the form (17.85) and hence no sufficient estimator 
exists. We have already found (Example 17.11) that there is an efficient estimator. For 
finite n no single estimator can contain all that the sample can tell us about 6. 
Sufficient Estimators when the Range depends on the Parameter 
17.32. One of the conditions of the theorem of 17.23 and that of 17.27 is that the 
range should be independent of d. In the contrary case our results, particularly for sufficient 
estimators, require reconsideration. 
Suppose the range of the frequency function is from 6 to b, where b is fixed. If there 
is a sufficient estimator for 6, say t, the distribution of t and any other estimator is 
independent of 0. Take xu the lowest value of the sample, as such other estimator. Then 
if t is fixed the distribution of xx is independent of 6, which is clearly impossible unless in 
fixing / we also fix xu that is to say, t is a function of xx. Thus if a sufficient estimator 
exists it must be a function of xx. 
Similarly if the range is from a to 6, a sufficient estimator for 6 must be a function 
of the largest sample member. 
xx or some function of it is sufficient for 0, the lower extremity of the range, 
and xx is fixed, the probability that any particular sample value x is greater than xx is 
proportional to / (x, 0). This must be independent of 0, since x± is sufficient, and hence 
so is f(x, 0)/f(x1, 0). Thus 
/(M)=r§£ ...... (17.89) 
h(6) 
and this is the most general form admitting a sufficient estimator. 
It remains true in such circumstances that the smallest member of the sample is 
a maximum likelihood estimator. For the likelihood is 
{h{Q)Y ' 
which is clearly a maximum when h (6) is a minimum. Now since the total frequency is 
unity we have, from (17.89), 
rb 
h (d) = g (x) dx (17.90) 
Jo 
6 cannot be greater than xl7 for then such a sample value could not appear. The value 
which minimises h (6) is seen from (17.90) to be that which minimises the range, i.e. xv 
17.34. When both extremes of the range, a and 6, depend on 6, some further 
modification is necessary. Suppose that a is equal to 0 and that 6 (d) is some strictly decreasing 
28 ESTIMATION: LIKELIHOOD 
function of 0. Let Xn be the value such that b (Xn) = xn, the greatest member of the 
sample, and let t be the smaller of xt and Xn. Then of the inequalities 
t <xl9 h {t)>xn (17.91) 
one at least is true. But the first equality implies that t > B and the second that 
h (t) < b (6), and either of these two implies the other. Hence both inequalities in (17.91) 
are true, and 
6 <t <xx <xn <h{t) <b{6) (17.92) 
Samples with fixed t then lie in a fixed range, and hence t is sufficient if the frequency 
function is of the form (17.89). It would seem that this remains the most general form of 
frequency function admitting a sufficient estimator when both extremes of the range 
depend on 0. 
Example 17.15 
Consider the rectangular distribution 
dF =% - 0 < x < 6. 
If we take the ordinary likelihood equation we get 
IlogL = -¥enlogm "~l 
For this to vanish 6 must tend to infinity, an obviously nugatory result. In accordance 
with the above discussion we should take as our estimate of 0 the smaller of xx and — xn, 
and this is obviously sufficient, for nothing in the sample can tell us more about the 
terminals of the range than its most extreme members. 
Intrinsic Accuracy 
17.35. If the sampling distribution of an estimator t is 
dF =&{t,6)dt (17.93) 
we define the accuracy of t as 
r _ r (d&y i 
~ J— \dOJ 0 
„dt 
It is evidently essentially a positive quantity. We assume, unless the contrary is stated, 
that the range is independent of 6. 
F is the quantity we have already encountered in (17.67) as the reciprocal of the 
variance of t when it tends to normality in large samples. As in 17.27, we have 
F <nC (dJ^\*fdx (17.95) 
< n I, say, where 
J —00 \ 
1 = i {^f~Yfdx (i7-96) 
Now / is independent of the estimator t and we may call it the intrinsic accuracy of 
the distribution / in regard to d. It is intrinsic because it depends only on /. It may 
INTRINSIC ACCURACY 29 
be termed accuracy because it provides, for large samples at least, a minimum to the 
variance of possible estimators of 0. We know from 17.25 that under certain conditions 
the maximum likelihood estimator attains this minimum for large samples. 
17.36. We may now extend the definition of efficiency of an estimator to the case 
of small samples. In fact, the efficiency is the ratio of the accuracy of an estimator to the 
intrinsic accuracy of the distribution for the parameter under estimate. This is easily 
seen to apply to the case of large samples for which efficiency was defined in 17.12, and 
may be applied to finite samples or non-normal sampling variation. For such cases, 
however, it is conceivable that the efficiency might exceed unity. A proof that this is not 
so when the range is independent of d is suggested in Exercise 17.12. 
17.37. If the range is independent of 6 we have 
31°g/\ _ f a/^~_ d 
J dd dd)J 
dd 
and hence the following three expressions for the intrinsic accuracy are equivalent: 
E 
dd 
E(dHogf\ [ (1797) 
dd2 ) 
This equivalence holds if/ is zero at the extremes of the range. For we then have 
f 
J a 
Max. 
But if/ is not zero at the extremes the equivalence may break down. (Cf. Exercises 17.9 
and 17.11.) 
Amount of Information 
17.38. The quantity nl has been called the amount of information about 6 in the 
sample of n, and / may be called the amount of information per member of the sample. 
The use of "information" in this specialised sense has not been universally accepted, 
but some of the properties of I are such as we should require of any measure of information. 
(a) If the parent does not contain 0, I = 0 so that no sample can tell us anything 
about 0, which must obviously be so. 
(b) Since sufficient estimators contain all the relevant information in the sample 
we expect their accuracy to be nl, and conversely. That this is so may be seen as 
in 17.27 and 17.28. In fact, if t is such that the equality in (17.72) holds, var u = 0 
and for fixed t, —^r— is constant, irrespective of the form of distribution of t. Log L 
is then of the type required for sufficiency. 
30 ESTIMATION: LIKELIHOOD 
(c) The sum of the amounts of information in two independent sample-members 
is the amount of information in the pair taken together. For if their joint distribution is 
dF = fx (x, 0) dxf2 (y, d) dy, 
we have for the intrinsic accuracy 
= - If ^^ dxdy - [f ^/i/adx dy 
= - | t^f.dx - Ji!M/v,^, .... (17.98) 
which is the property stated. 
Loss of Accuracy 
17.39. Where no sufficient estimator exists, it follows from (b) of the previous 
paragraph that no estimator for finite n can contain all the information in the sample, in 
so far as any particular estimator falls short of the ideal we may be said to lose information 
by using it. No estimator can avoid losing something, although of course some may 
lose less than others. 
Presumably the loss will be greater for large samples than for small ones, and will 
be least for maximum likelihood estimators. We may calculate the loss in this case. If 
t is the maximum likelihood estimator of 6, we have, to a first approximation, 
dJ%Sjf = {6-t)?^ (17.99) 
ou oul 
The variance of ——— in samples for which t is constant is thus the variance of—■•-., 
ov o() - 
within the set multiplied by (t — 0)2. Now the total loss of information, f 
-| J, and hence is equal to the variance of t multiplied 
d2 log L 
by the total variance of —-^~- within sets for which t is constant. This we now evaluate. 
ou1 
Suppose the distribution is grouped so that the " expected " frequency in the jth 
group is m$. The likelihood is then proportional to m/'1 m2n* . . . and apart from 
constants independent of 6 we have 
log L = Z % log mi (17.100) 
?" 
d lose L tTb dtn 
—-3— = Z — n, where m' = —- .... (17.101) 
36 m dO x } 
od2, V m m2 
We have at once 
(1.7.102) 
m" m'2\ 1 ^ „_, f „ m'2 
-EZX[—-'^L-\%\ = -EZ\m" 
vart l\m m2 / J [ m 
= 27^—J (17.103) 
LOSS OF ACCURACY 31 
We shall find it most convenient to regard the n's as distributed over the groups first of 
all without restriction and then subject to two linear constraints expressed by £ (n^) = n 
and —^1— = £ ( — n J = constant. Prom this viewpoint the n'& may be regarded as 
distributed in the Poisson form with mean and variance m (not the binomial because we 
are not introducing the restriction that the samples should be of fixed size, except as a 
constraint). 
Now if £ (kj rij) is a linear function of the n's subject to a linear constraint £ (a0- n^ = pf 
its variance is 
E{k*m) -^li*^, (17.104) 
£ (ma2) 
and a second constraint reduces the variance by a term similar to the second in this 
expression. The result may be seen from geometrical considerations. We may write 
£ (Jen) = £ I k\/m.—— ] and 
£ (an) = £ I on\/m. 
^/m 
71 
where the variables —— have unit variance and mean \/m. Consider the different values 
ym 
of the n9s, say s in number, as the co-ordinates in a Euclidean space. The density function 
of the variables is then symmetrical about a point (Vmu Vm^ • • • Vm8) ^° which we 
transfer the origin. The variance of the unconstrained variables is then equal to the 
reciprocal of the distance from the origin to the hyperplane £ (k^/mx) = 1, namely, to 
£ (k2 m). But when the constraint is imposed, the variance becomes proportional to the 
reciprocal of the distance from the origin to the hyperplane in the direction parallel to 
£ (a*\/mx) = 0 and is hence reduced by the amount 
cos2 $ £ {k2 m), 
where </> is the angle between the planes. This quantity is 
£(k2m)£\a2ni) ~ V h 
which gives us the second term in (17.104). 
Now for the first linear constraint £ (n) = constant = n we have a = 1, and the 
reducing term is (since £ (m) = n also) : 
1 
£2 (km). 
n 
7TL 
For the second constraint we have a = — and hence the term is 
m 
£* (km') 
Thus the variance of £ (kn) is 
£(Jc2m)--Z2 (km) - ^j^Q. ... (17.105) 
m 
32 ESTIMATION: LIKELIHOOD 
Now taking 
k 
and remembering that 
tf t 9 
m m2 
1 _ y /m'2 
var t \ m 
we see from (17.102) that the loss of information is, for large samples, 
\m\ m / J 1 yfm'2\ I m\ m 
2M A_^\ LJ..m . (17.106) 
m 2\ n \ m J yz (m 
m J \ m 
By considering the width of the groups as tending to zero we may apply this result 
also to continuous distributions. 
Example 17.16 
In the distribution 
dF = - 7—[-r-—_ - oo < x < oo 
71 1 + (X — 0)2 
there is no sufficient estimator, as we have seen. Let us consider the loss of information 
consequent upon using the maximum likelihood estimator. 
We may write for our " expected'' value m 
n dx 
m = — 
Til + (X — 0)2 
w y (m'2\ ___ n f00 4^2 d^p __ n 
Hence 2j i — J I _ . — ~ 
\ m J TzJ^K (1 + #2 8 2 
(1 +#2)8 2 
fj./ „ m'2y*\__™p° 4(p2 - l)a(ip _ 7n 
7C J 
^m\ m ) ] 7i J-oo (1 +.jP2)5 8 
2 «{ — m ] v = 0. 
[ m \ m J ] 
Hence, from (17.106), the loss of information is 
7 1 ** 
4 2^ 4 
The intrinsic accuracy of the original distribution is -|, so the loss of information is equivalent 
to 2J observations for large samples. 3?or small samples it will presumably be smaller, 
since it vanishes for samples of one. The loss by use of the maximum likelihood estimator 
is therefore very slight and becomes of diminishing importance as the size of the sample 
increases. 
Ancillary Estimators 
17.40. Where no sufficient estimator exists no single estimator can avoid the loss 
of information; but we may take an additional function of the variables which, together 
with the maximum likelihood estimator, will give an accuracy tending to unity in large 
samples. By taking a third function we can improve the accuracy still further, and so 
MULTIVARIATE DISTRIBUTIONS WITH ONE PARAMETER 33 
on. The process is analogous to approximating to the value of a function (the likelihood 
function) by ascertaining its differential coefficients at some particular point of the range. 
3 1 T 
In fact, suppose that, in addition to the estimator which gives —~— for some value 
of 6 such as t, we also find ^ J for that value. The variance of -_~f- over values 
ddz od 
in the neighbourhood of those for which these two are constant is then, to the first 
approximation, the variance of 
a dnogL 
\{t~6Y 
ae3 
which has ordinarily a mean value and variance of lower order in n. In particular, if t 
dlogL\ A Al . _/d2logL 
( 
is the maximum likelihood estimator, so that ——— ) = 0, the value of . _, 
may provide supplementary information which enables us to approximate more closely 
to the likelihood function and hence salvage some of the lost information. Such a quantity 
is accordingly called an ancillary estimator. Cf. 17.29 above. 
Multivariate Distributions with One Parameter 
17.41. We now proceed to consider the extension of some of the foregoing results 
in two directions : (a) where there is more than one variate but still only one parameter, 
and (b) where there is more "Cnan one parameter to be estimated. 
The former raises no new point of difficulty. To take the bivariate case as an example, 
if the frequency function is f(x,y, 0), the likelihood is 
L =f(xl9yu0) . . . f(xn,yn,Q) .... (17.107) 
and our maximum likelihood estimator is obtained by maximising L in the usual way. 
Example 17,17 
To estimate the parameter p in samples of n from 
(IF = f exp J - M (x* - 2Pxy + y2) i dxdy. 
2tz(1 - p-)4 I 2(1 - p~) J 
We find 
log L = constant - ^ log (1 - />2) - t) {E (x*) - 2P E (xy) + E (y*) }, 
Z Z {l —~ p") 
whence, for ~~— = 0 we have 
dp 
np - p {E (a») - 2P E (xy) + E (y*) } + ■ ~~ E (xy) = 0 ; 
I — p2 (1 — /=>")" L — p" 
reducing to the cubic in p, 
n + ,J^* E (xy) - —L-5 {E (*») + E (*/*) } = 0. 
p(l — p") 1 — p2 
It is interesting to note that this does not yield the product-moment of the sample. 
a.s.—n d 
34 ESTIMATION: LIKELIHOOD 
We haTe, after a little reduction, 
Since 2? (x2) = E (y*) = 1 and E (xy) = p, we have, for the estimator p, 
1 1 + p2 2 (1 + 3p2) , 4p2 
wvarp (l-/>2)2 (l~/>2)2 (1~P2)2' 
whence varp = v ■ "J 
r n{l +p2) 
This is less (and may be considerably less) than the variance of the sample product-moment 
in large samples, (1 — p2)2/n. The efficiency of the latter is 1/(1 + p2)- 
Simultaneous Estimation of Several Parameters 
17.42. We now turn to the case when the unknown parameters are more than one 
in number. To simplify the exposition we shall consider the case of two parameters dx 
and 02, but examples not infrequently arise where more than two have to be estimated— 
for instance, in the fitting of certain Pearson curves there are four. To fix the ideas, 
consider the normal distribution 
dF " 6, vW 6XP {" '261 {X ~ 6l)2} ^' " °° < * < °°' 
The likelihood function, except for constants, is given by 
logL == - nlogd2 -~Z(x ~Q1)\ . . . (17.108) 
201 
It is natural to generalise our principle of estimation by looking for estimators which shall 
maximise L for independent simultaneous variations of 0X and 025 i.e. to require that 
abgi = aio|L = 0 
In our case this leads to 
S (x - Bj) = 0 
whence for the estimators 6X and 6%, 
1 
6± = -Z (x) = x (17.110) 
n 
1 
0§ = -2 (x —%)* (17.111) 
n 
Thus the sample mean and variance are estimates of the population mean and variance. 
We note incidentally that the estimator §z is biassed. 
17.43. There is one possible source of confusion here which should be removed. 
If we know 0lt then §3 is given by 
02 = - I {x - 0O2, (17.112) 
n 
which is not the same as (17.111), the sample-mean x having been replaced by the known 
SIMULTANEOUS ESTIMATION OF SEVERAL PARAMETERS 35 
quantity Qx. Suppose then we estimate 8X by x, as we may do whether we know 02 or not, 
since (17.110) does not contain 02. We may then ask, what is the estimator of 62 which 
maximises the likelihood for all samples giving the ascertained value of 0l9 namely, x 1 
This is an entirely different question from the one which gave rise to (17.111) and we 
must not he surprised if it has a different answer. The variations of L from sample to 
sample are now considered in a certain sub-population for which x has a fixed value. 
In our particular case the problem can be solved explicitly. The likelihood function 
can be thrown into the form, with variables x and s— 
X —_—-_—_ —_[ — ] ~- exp — -— ] ax as, . . (17.113) 
where s2 is the sample variance. 
If we maximise the likelihood in this form for simultaneous variations of dx and 02 
we arrive back at (17.110) and (17.111), as of course we must. But if x has a fixed value, 
the distribution of s becomes of one lower degree of freedom. The likelihood is then 
proportional to the second factor in (17.113), viz. 
5ti-2 /ns2' 
lf^ exp I Wi 
'2 \ ™%t 
and for variations of 02 this is maximised by 
§? ^ _ZL_*2 = _L_27(<e-£)2. . . . (17.114) 
n — 1 n — 1 
This, it may be noticed, is an unbiassed estimator. 
17.44. The difference between (17.111) and (17.114) is apt to be confusing, for both 
are, in a sense, maximum likelihood estimators. The distinction arises from the fact that 
we are considering the variation of L in two different populations, the first over all samples 
of size n, the second over the more restricted samples subject to the further constraint 
E (x) = constant. The difference when n is large, of course, is quite unimportant, but 
as a theoretical matter the point has some interest. 
Which of the two is employed for practical estimation is a matter of choice. At first 
sight it may strike the reader as objectionable to use (17.114), because x is not known before 
the sample is drawn, and there are obvious dangers in basing an inference on properties 
of the sample which are determined a posteriori. This objection, however, does not lie 
in the present case. We make up our mind beforehand that, whatever x may turn out 
to be, we will make an inference in relation to the sub-population of samples determined 
by it. There is, in fact, no posterior determination of the rule of inference. 
17.45. Possibly without realising it, the reader is already accustomed to make an 
inference of this kind in relation to a sample number. We do not usually determine 
beforehand what size the sample must be ; our results (apart from the distinction between small 
and large samples, which is another matter) are true for any n, whatever n may turn out 
to be in practice. In the same way the estimator (17.114) is a maximum likelihood 
estimator, whatever x may turn out to be, x being a property of the sample, just as n is. 
The fact remains, of course, that (17.111) and (17.114) give different results. Which 
36 ESTIMATION: LIKELIHOOD 
is the better ? The answer depends on what we require of the estimator. If we wish 
to choose Qx and 02 so as to maximise their joint likelihood we choose (17.111). If we wish 
to select them so that the likelihood is maximised for Bx and then, for the observed x, is 
maximised for 62) we choose (17.114). 
17.46. It may be shown that, as for the case of one parameter, the likelihood 
estimators of several parameters are consistent under very general conditions and tend for 
large n to be distributed in the multivariate normal form. We omit the proof of these results, 
which the reader will probably be willing to accept, and proceed to a generalisation of 
the theorem of 17.26. Thus :— 
(a) If the frequency function / (x, dl9 0a, . . . 0P) is continuous in x, and 
df 
(6) if in a certain interval containing the true values 610, d20i . • • 0pi), -r^- is 
continuous in dj for every x, x2 ~j- approaches a continuous function of fy for large 
ucoy (8,, 6k) = 4», . . . . (17.115) 
where A is the (Hessian) determinant 
dlogA /dlogf\ , 7 /i«iii»\ 
bJ * ' &J * fax . . . (17.1.1b) 
d®o J°jo \ dOk Mo 
and Aft is the minor of the jth row and Hh column. When p = 1 this reduces to the 
case of a single parameter. 
As n tends to infinity the joint distribution of the maximum likelihood estimators 
tends to the form 
/ = * exp j- | Z qih (0, - BjQ)0h - fl*„)|. . . . (17.117) 
The theorem will be established if we show that 
r»oo 
9jk 
^0 (%J) ^ • ■ • (l7-lls) 
for then the values of the variances and covariances of the (9's are as stated in (17.116). 
(Compare 15.12.) 
Make the transformation 
qh^ZAhj{6i -0,o) (17.119) 
j 
and choose the A's so that the exponential of (17.117) becomes 
w y 2 
* 1 
Then gjJc = £ Ahj Ahk. . . . . . (17.120) 
h 
The g's are independent normal variates with variance l/n. Hence, from the theorem for 
the case of a single parameter, already proved, we have 
f 
'OO 
. fdx = 1 (17.121) 
SIMULTANEOUS ESTIMATION OF SEVERAL PARAMETERS 37 
Further, we have 
C (dJ?iLdJ§il)fdx = 0, h^l, . . . .(17.122) 
J_ooV dqh dqt } 
for if we put qh = — {uh - «*,) 
1 
and qt = — (uh + uj) 
the expression becomes one half of 
~ dx\(d\ogfY _(d\ogf 
J ; 
J —00 
which vanishes since the u's have the same variance as the g's. 
Now 
fyh 
Hence 
r /3iog/\ /aiog/\ fdx_r (EA A d\ogfd\ogj\ 
= y A A 
h 
in virtue of (17.121) and (17.122), 
= <Jih 
from (17.120). The theorem follows. 
Example 17.18 
Let us estimate the five parameters of the hivariate normal form 
1 
dF = -—-—- ■, exp 
2jZ(71<72 (1 -p2)* F 
1 f I x — oc\2 2p (a; — a) (y — /J) 
f 
da: tfo/, — oo < x, 2/ < oo. 
It will be found that the partial differential coefficients of log L yield, on solution, the 
estimators 
a = £, /j = y 
1 
^2 V / — \ o 
(J. rrr: -— ^ ^* —- X J 
n 
1 
p^i^a = -2 (x —x)(y~y) 
a\ = -Z{y -yy 
so that for simultaneous estimation the sample means, variances and covariances are 
estimates of the corresponding parameters. 
To evaluate the sampling variances and covariances we have to evaluate integrals 
of the type 
aiog/ a_bg/\ dF_ 
QO /"OO 
J —CO J 
-oo \ dOj d6k 
These are easily obtainable, being merely functions of moments of different orders. 
38 ESTIMATION: LIKELIHOOD 
Taking the parameters oc, ft <tu ff», p in that order, we find for the Hessian (17.116) 
a\ (1 - p2) o-i o-2 (1 - p") 
0 0.0 
p L_ o oo 
ff,ff,(l-V) of(l-P2) 
o o 
o o 
0 0 
2 — p2 p2 p 
<rf (1 — p2) <Tl(r2(l-p2) ^(1-p2) 
p2 2 —- p2 p 
a\a,ii"-v> "(TfTr^2) o-; (i -7») 
p p 1 + p: 
.2 
C)rx(l -p2) or2(l -p2) (1 -,,2)2 
This confirms, what we know already, that the distribution of means is independent of 
variances and covariances. We may consider the 2x2 block in the top left-hand corner 
and the 3 X 3 block in the bottom right-hand corner separately. If the determinants 
of these blocks are Ax and Zl2, we have 
A, = 
o\o\ (1 - p2) 
_ 4 
The minors will be found to be given by 
<r2<r!(l-p2)4 (l-p2)4afai 
4p 4 
a\a\(\ -7y (T=7F<^ 
0 0 
0 0 
0 0 
Hence we find 
0 0 0 
0 0 0 
2 2pa 2p 
of oj (1 - P2)4 of <r| (1 - p2)4 oi{' 4(1 - p2)3 
2p2 2 2p 
erf <r» (r^Sj* <y\ ai (T - p2)4 ^| (J~~ p2)a 
2p 2p 4 
0 O 
0*1 * <?S 
var 6c = -±, var p = -^, 
n*2 rr2 M __ n2\2 
var^ =/.1 var£2 = 2s, varS = l - p ] . 
2tc, 2ft r » 
These results are already familiar. We have further— 
cov fo, <J2) = *!£**■, cov (5c, /&) = ^^ 
covtf,^) - PJ!L(lj=£,)> cot fl, *,) - * ff» <* ~ '-'>■ 
Hence the correlation between ax and 6\ is p2, that between a and /? is p, and that between 
p and 6\ or <72 is ~^~. 
y2 
SUFFICIENT ESTIMATORS FOR SEVERAL PARAMETERS 
39 
Example 17.19 
Consider the Type III distribution 
dF = 1 (x-^p-1 
ar(p)\ a 
For the likelihood we have 
exp 
x — a 
a 
(XiX. 
a < x < oo. 
log L = — n log a — n log r (p) + (p — 1) E log 
The three partial differential coefficients give 
x — a^ y /a; 
(p - 1) E 
(x — a) a* 
+ - = 0 
+ — 27 (a; — a) 
0 
a 
n — log r(p) H-Tlog 
dp \ a 
0. 
For the Hessian, taking the parameters in the order a, a, p, we have 
a»(p - 2) 
1 
1 
<r(p-'l) 
1 
cr2 
cr2 
J. 
a 
2 f log -T (P) 
<r (p - 1) 
1 
cr 
d2iogr(p) 
dp* 
1 
(p - 2) <r4 [ dp2 p - 1 (p - 1) 
From this the sampling variances are found to be 
= A 
var a 
var a 
var p 
1 
dnogr(p) 
dp* 
1 
nAo2 \ 
1_ j^JL ri2logr(p) 
1 
w/lcr2 \p — z 
_ 2 
w/i (p — 2) ^ 
dp2 
(p -1)2. 
Sufficient Estimators for Several Parameters 
17.47. As a natural generalisation from the case of one parameter we shall say that 
tt . . . tp are jointly sufficient for Bx . . . 6p if, and only if, the likelihood function can 
be expressed as 
JL/ \X\ ... *^fi> l * * * V/ == 1 \ 1 * * * *>' 1 * * " v) 2 v^l • • • "^n) \ • *-^oj 
It evidently does not follow that if 02 . . . dp are known tx is sufficient for fl^. This will 
be true only if the function Lt may itself be factorised, e.g.— 
Jbl (t1 . . . tp, Ui . . . Op) = -L/j! (fcj, C/j . . . Up) JL712 (*2 • • • ^pj Cfa . . . Up). . (17.1^4:) 
If a case occurred in which 
Lx = Ln (tu dx) L1Z (t2t 02) * • • Llp(tpldp) . , . (17.125) 
40 ESTIMATION: LIKELIHOOD 
we might say that each t was sufficient for the corresponding 6 or that the set of tfs was 
completely sufficient for the 0's. Such cases, however, are very rare. 
Example 17.20 
From (17.113) it is evident that x and s are jointly sufficient for m and a. If a is known 
x is sufficient for m, but if m is known s is not sufficient for a. The two are not completely 
sufficient. 
17.48. The properties of sufficient estimators may be proved true, with certain 
modifications, for several parameters, but we shall not take the subject further except 
to quote one result. 
If f(x, dt . . . ftp) is continuous and not zero over some continuous range of the 0's, 
and ~- exists, then it is necessary and sufficient for the existence of a set of jointly sufficient 
ox 
estimators that 
/ == exp I 2j 4A + B + YV, . . . (17.126) 
where Ak and B are arbitrary functions of the 6's and Xk and Y oix. (See Koopman, 1936.) 
Example 17,21 
The Type III distribution of Example 17.19 gives us 
log/ = ~ P log o - log r (P) + (P - 1) log (x - a) 
If a is regarded as known, this may be put in the form 
x — a 
a 
a + (p - 1) log (a: - a) — /o log a - log F (p), 
a 
which is of type (17.126) with 
1 
a 
A2 = p ■— 1, X2 = log (x — on) 
B = — p log a — log F (p). 
Thus if a is known, there are sufficient estimators for a and p jointly. It will be clear on 
inspection that if oc is unknown there are no sufficient estimators, even if a and p are known. 
Parameters of Location and Scale 
17.49. Consider a frequency function expressed in the form 
"-'(VVO-r) • ■■ ' ■(1""1 
The parameter a. may be regarded as locating the distribution and /? as determining its 
scale. In particular the normal distribution may be put in this form. We may write 
iF = exp 4 {£) d$ = exp 4 (£)j, . . . (17.128) 
where f = 2LZ_? and <f> (£) = log g (|). 
r 
PARAMETERS OF LOCATION AND SCALE 41 
In samples of n we have 
log X = £ cj) — n log /S, 
giving for the maximum likelihood estimators 
£M^ = -\z4>' =0 (17.129) 
COL p 
dJ^ = - I(27^'f + n) = 0, . . . .(17.130) 
whence we may solve for a and /?. 
For the variances and covariance we find 
*(w-*(f.)—m* 
and the Hessian of (17.116) becomes 
<£" \ BT / <£" f 
2 
E \ l~\ — E 
(17.131) 
from which the variances and covariance of 6c and /§ may be determined in the usual way. 
/ <h" f \ 
In (17.131) it would be a great convenience if the quantity — E I —- j vanished, for 
then 6c and /5 would be independent. By a suitable choice of origin we can, in fact, ensure 
that this is so. Put 
■- = t.^(ff) (17.132) 
Then E (f f) = ^7 -| (0" C) 4- f ^(^ 
= E (£<//' + £</>"), 
so that 
E (</>" C) - 0. 
With this origin we have for the variances of the (uncorrelated) variables 6c and p, 
vara- --=£L- (17.133) 
'/?,i£ (<£ ) 
™? = ~nWGhrri} (17,m) 
The point of location so defined, namely, as that for which a and /? are uncorrelated, has 
been called by Fisher the centre of location. 
42 ESTIMATION: LIKELIHOOD 
Example 17.22 
For the normal distribution 
dF = _—_- exp < — hi —r-^ ) > dx 
we have <£ = — Jf2 
j0 (f) = - 1 and E (f f) = 0. 
Hence £ = £, and the origin chosen is itself the centre of location. From (17.133) and 
(17.134) we find the familiar results (for large samples) 
#2 
var 6c = var x = — 
n 
var S == var 5 = —, 
with a; and s uncorrelated. 
Example 17.23 
Consider again the Type III distribution 
**"m\-T) expi—r\d\i~)' a<*<00' p>2 
where we assume p known. The condition p > 1 is required to ensure the vanishing of 
the frequency function at the extremity x = a, and p > 2 to ensure the convergence of 
some of the mean values. 
Here 
£ = constant — £ + (p — 1) log f. 
Hence 
E (f) = E 
:2 , 
Thus 
E (t P) = E (-Z-jl^j = -1 
E (£2 f) = E (- p + 1) = - (p - 1). 
£ = f - (p - 2). 
The centre of location is distant (p — 2) to the right of the start of the distribution. In 
terms of £ we have 
fi = constant — £ — (p — 2) + (p — 1) log (£ +./> — 2) 
^ T £ + P - 2 ^ (f + p _ 2)» 
JB? (f) = - l/(p - 2) 
Jf (f C2 — 1) = — 2. 
Hence 
» £*(/» - 2) 
var a = - --it—■- 
var o == -£-. 
EFFICIENCY OF THE METHOD OF MOMENTS 43 
Efficiency of the Method, of Moments 
17.50. In previous chapters we have fitted distributions of the Pearson type to 
other distributions by identifying lower moments. We were there mainly concerned with 
the properties of populations only and no question of the reliability of estimates arose. 
If, however, we regard the data as a sample from a population, the question arises whether 
fitting by moments provides the most efficient estimators of the unknown parameters. 
As we shall see presently, in general it does not. 
Consider a parent form dependent on four parameters. If the maximum likelihood 
estimators of these parameters are to be obtained in terms of linear functions of the moments 
(as in the fitting of Pearson curves), we must have 
dl°J>L = a0 + a, E (a?) + a, E (a;2) + az E {xz) + a, E (a;4) . (17.135) 
da 
and consequently 
/ C», 9i, 02, 0„ 0«) = exp {b0 + &i x + b2 x* + 63 a3 + 64 x*}, . (17.136) 
where the 6's depend on the 0's. This is the most general form for which the method of 
moments gives maximum likelihood estimators. The 6's are, of course, conditioned by 
the fact that the total frequency shall be unity and the distribution function converge. 
Without loss of generality we may take b± = 0. If, then, the other 6's vanish except 
60 and 62 the distribution is normal and the method of moments is most-efficient. In 
other cases, (17.136) does not yield a Pearson distribution except as an approximation. 
For example, 
dJ^l = 262 x + 363 x2 + 464 x*. 
ax 
If b3 and 64 are small this is approximately 
dlogf 2b2x 
CtX , <^t*a *jOa n 
1 /y „_ <y £ 
X __ iHj _. ^ 
2fr„ b» 
(17.137) 
which is one form of the equation defining Pearson distributions (cf. 6.2). Only when 
63 and 64 are small compared with 6a can we expect the method of moments to give estimates 
of high efficiency. 
17.51. A detailed discussion of the efficiency of moments in determining the 
parameters of a Pearson distribution has been given by Fisher (1921a). We will here quote 
only one of the results by way of illustration. 
We found in Example 17.19 that the variance for large samples of the maximum 
likelihood estimator p is given by 
var p = 
or, if p = p — 1, by 
^ dp2 p - 1 T (P - l)2 
varp = —-= — -■ . . . (17.138) 
H ' d2iogr(i +p) 2 ■ •• v ; 
»^2=-^-v;-'-^-- + i} 
dp2 p p 
44 ESTIMATION: LIKELIHOOD 
Now for large p* 
l^log^d +p) = ~{\log2n + (p + i)logP-p+^-p __^- + _^_ 
We then find 
d^2 j?9 £>2 3 [p3 5_p5 7p7 
and hence, approximately, 
var 6 = ~(pz +}p) (17.139) 
If we estimate the parameters by equating sample-moments to the appropriate moments 
in terms of parameters, we find 
oc + ap = mx 
a2p = m2 
2pa3 = m3 
so that, whatever a and a may be, 
bx= 7A=-, (17.140) 
m| p 
where bx is the sample value of /?i. Now for estimation by the method of moments (cf. 
9.22), 
var bx = ^ (4/?4 - 240a + 36 + 9ft ^2 - 12)3, + 35)80, 
which for the present distribution is readily seen to reduce to 
8? 6 (p -f 1) (p + 5) ,,-nn 
var 6t = ^. vr — vr —. .... (17.141) 
w- p 
Hence, from (17.140) we have for p, estimated by the method of moments, 
var p = — var 6-, 
^ 16 
= Jp (p + 1) (p + 5). 
For large p the efficiency of this estimator is then, from (17.139) with p — 1 + p, 
E 
i>3 + }p 
(p + 1) (p + 2) (p + 6)' 
which is evidently short of unity in many cases. When p exceeds 38-1 (ft = 0-102) the 
efficiency is over 80 per cent. For p = 19 (ft = 0*20) it is 65 per cent. For p = 4 a more 
d2loe; r (1 4- p) 
exact calculation based on the tables of the trigamma function •-—^-~- -- shows 
dp 
that the efficiency is only 22 per cent. 
* The series for the log F function is given in most books on advanced calculus, e.g. J. Edwards, 
Integral Calculus, vol. 2, article 942. 
NOTES AND REFERENCES 45 
NOTES AND REFERENCES 
The greater part of this chapter is based on the researches of R. A. Fisher, the main 
papers being those of 1921a, 1925& and 1934a. The idea of maximising likelihood may 
be traced back to Gauss and was considered by Edgeworth, but may be regarded as 
beginning to exercise an influence on statistical theory only with the publication of Fisher's 
first paper in 1912. 
The theorem giving the limiting variances and covariances of maximum likelihood 
estimates was proved (incorrectly) by Karl Pearson and Filon in 1898 before it was realised 
that it applied only to maximum likelihood. The necessary correction was given by Edge- 
worth (1908) and Fisher (1921a), but rigorous proofs were not available until the work of 
Hotelling (1930) and Doob (1934a and b, 1935, 1936). In the text we have followed 
Hotelling's treatment. 
The inefficiency of moments in fitting distributions, pointed out by Fisher (1921a), 
has led to some controversy, for which see Koshal (1933, 1935), Myers (1934), Elderton 
and Hansmann (1934), K. Pearson (1936), and Fisher (1937a). The reader who pursues 
this subject so far as to read any one of these papers should read them all. 
For work on sufficient estimators see Koopman (1936) and Pitman (1936, 19376), who 
independently obtained the general form of distribution admitting such estimators. The 
theorem that sufficient estimators have the property 17.17 is due to Fisher, rigorous proofs 
being provided by Neyman (1935a) and Dugue (1936a). Reference should also be made 
to papers by Bartlett (1936a, 6, 1937c, 19386, 1939a, 1940) on the problem of several 
parameters and what he calls " conditional " statistics, i.e. those similar to s2 when x or some 
other function of the sample values is regarded as known. See also Neyman and Pearson 
(1936a). 
Among recent papers, that by Pitman (1939a) on parameters of scale and location, 
and that by Welch (1939c) on the distribution of maximum likelihood estimates, are 
noteworthy. 
Geary (1942) has recently proved a remarkable generalisation of the theorem that 
in large samples maximum-likelihood estimators have minimum variance in the case of 
one parameter. In fact, for several parameters the maximum likelihood estimators 
minimise the " generalised variance " as defined in Chapter 2S. 
lii X. E Rj C1 h) lii d 
17.1. If t is a most-efficient estimator and t' a less-efficient estimator with efficiency 
E, and if the correlation of t and t' is p, show by considering the estimator t" defined by 
(1 + E - 2p y/E) t" = (1 - p y/E) t + {E - p V$) ? 
that p = \/E (for in the contrary case var f > var t). 
(Fisher, 19256.) 
17.2. If in n trials of an event with probability p there are x successes, show that 
a maximum likelihood estimator of p is x/n. Find its sampling variance and show that 
it is sufficient. 
17.3. Show that the distribution 
dF = \ exp {— | x — 6 | } dx, — oo < x < oo 
46 ESTIMATION: LIKELIHOOD 
has a likelihood function for a sample of n which is a maximum at the median if n is odd 
and between the (w/2)th and (n/2 + l)th members if n is even. 
17.4. For the distribution of the previous exercise show that for a sample of (2m -f- 1) 
members the median has an accuracy 
(m + 1) (2m + 1) f _ (2m) ! 1 
(m — 1) { 22w~1 (ml)2 J 
Hence, as m tends to infinity, the loss of information tends to 4 <\/(m/n) — 4. Thus, 
although the median is most-efficient the loss of information in large samples does not 
tend to a constant. 
(Fisher, 19256.) 
17.5. Show that if a most-efficient estimator A and a less-efficient estimator B tend 
to joint normality for large samples, B — A tends to zero correlation with A. 
Show that the error in B may be regarded as composed (for large samples) of two 
parts which are independent, the error in A and the error in B —A. (The first may be 
regarded as sampling error, necessarily inherent in the problem of estimation, the second 
as error due to the inefficiency of the estimator.) 
(Fisher, 19256.) 
17.6. Show that the distribution of the median in a sample of (2m + 1) observations 
from the population 
1 dx 
71 1 + (X — 0)2 
is given by 
(m !)2 rc2M+1 \4 9 J I + (x - flja' 
where tan <£ = x — 0 and | <j> | < |tt. 
Show hence that the accuracy of the median is 
(2w + 1) ! [£ f0 , o , , (^ ,*\ • oj.!2/^2 ji»\m 2 ja 
a 
_ 3m (2m + 1) (m + i) ! /2 V»+t f 2m 
- i + 2 (m-3 1)^. + 2m - T (^ J |rT31 ^ (?t) ~ 2/'» + * I71) 
(m + 1) I / 1 \m+i | 2m _ 2m + 3 
where J„ (z) is the Bessel function of order n and in particular Jj (je) = J^ (2th) = O, 
J,, (n) = —, J, (2tt) = , and 
J n-Vl ~ J n J n-l 
Z 
(Fisher, 19256.) 
EXERCISES 47 
17.7. Show that the most general continuous distribution for which the maximum 
likelihood estimator of a parameter 6 is the geometric mean of the sample is 
/(M) = (|Y35exp {y>(6) + rQ (x) }, 
where ip is an arbitrary function of 6, and £ of x. Show further that the corresponding 
distribution giving the harmonic mean is 
f (x, 6) = exp 
;{•£-»}-3 + t» 
(Keynes, J.R.S.S. (1911), 74, 323.) 
17.8. Show that, if m is known, the estimator 
s = I-Z(x -~m)*y 
is sufficient for a ia samples of w from 
^=^)eXP{-2^("-m)2}&' 
and find its distribution by the method of 17.31. 
17.9. By considering the distribution 
dF = e~ix-6) dx, d < x < oo 
show that the three forms of (17.97) are not necessarily equivalent when the range contains 
the parameter to be estimated. 
(Pitman, 1936.) 
17.10. Show that if the frequency function is continuous and is zero at an extreme 
which is a function of 6, there still exists a maximum to the intrinsic accuracy, defined 
aiogjn2 
as E 
dd 
(Pitman, 1936.) 
17.11. By considering the distribution 
d F = ~~ , 6 < x < 0 + 1 
2i\j -j~ 1 
show that the intrinsic accuracy is 4^2/(20 + l)2. Show further that the largest member 
of the sample is sufficient for 0 and that its distribution is 
2nx (x2 Q2)n~l 
dF — ol (x) dx = -— -—•- dx. 
w (20 + If 
Hence show that 
4:71,0 2 
V d6 ) ~ (20 H- l)2 + (n - 
2) (26 + l)2' 
so that the mean value in this case is greater than the intrinsic accuracy. 
(Pitman, 1936.) 
48 ESTIMATION: LIKELIHOOD 
f 1 / d& \2 
17.12. If the frequency function of an estimator t is 0 its accuracy is E < -= I — J 
If every possible sample with frequency </» gave a different value of t the accuracy would 
be E < -j ( ~? ) > and would be independent of t. Show that the difference in accuracy" 
may be expressed as 
\9\<f>dd 0 ddt 
and hence is not negative. 
Hence show that the efficiency as defined in 17.36 cannot exceed unity, at least if the 
range is independent of 0. 
(Fisher, 19256.) 
17.13. Show that 
djr = f — oo < x < oo 
•does not admit of a sufficient estimator for either parameter if the other is known, or 
a pair of jointly sufficient estimators if both are unknown. 
(Koopman, 1936.) 
17.14. Show that if a distribution admits a sufficient estimator for either of two 
parameters when the other is known, it admits of a pair of jointly sxifficient estimators 
when both parameters are unknown. 
(Koopman, 1936.) 
17.15. Show that the centre of location of the Type IV distribution 
dF oc e~v tan"~l ^~a>^ jl + ( X-^- \ 21" "2 dx, - oo < x < oo 
vB 
where v and p are assumed known, is distant ---—- to the left of the mode of the distribution. 
p -f- 4 
(Fisher, 1921a.) 
17.16. For the distribution 
dF = d~, 0X -!a <x <6L + -a 
show that, in large samples, the mean tends to the form 
Show further that the distribution of the centre of the sample, say c (the mean of the two 
extreme values), tends to 
«-r>{-^i.i}*. 
TT. var c o 
Hence : = -, 
var x n 
so that the centre is a far better estimator of location than the mean for this distribution. 
(Fisher, 1921a.) 
EXERCISES 49 
17.17. Show that for the Type I distribution 
dF = n l v a?-1 (1 - a^-1 tfa;, 0 < x < 1 
the geometric mean of the sample values x and that of the values (1 — x) are jointly 
sufficient for the estimation of p and q. 
17.18. Show that all the Pearson distributions have sufficient estimators for some 
of the parameters if the others are assumed known, and ascertain which are the parameters 
concerned for each type. 
17.19. For the distribution of Exercise 17.15 show that the intrinsic accuracy for a is 
1 (/> + 1) (p + 2) (p + 4) 
and that the efficiency of the method of moments in locating the curve is 
P2(p - 1) {(p + 4)2 +^2} 
(p + l)(p + 2)(p + 4)(p»+i>»)* 
(Fisher, 1921a.) 
A. • Cj •—~~~ JL JL 
E 
CHAPTER 18 
ESTIMATION:/ MISCELLANEOUS METHODS 
Minimum Variance 
18.1. We have seen in the previous chapter that under certain general conditions 
the maximum livelihood estimator is most-efficient for large samples, and that for finite 
samples it leads to sufficient estimators where such exist. Sufficient estimators themselves 
contain all the information in the sample about the parameter under estimate. What 
we have not shown, however, is that maximum likelihood estimators have minimum variance 
in finite samples. 
We now consider the subject from a slightly different standpoint. Instead of 
beginning with the criteria of efficiency and sufficiency and showing that they lead to certain 
minimal properties, we shall examine the class of estimators which (a) are unbiassed and 
(b) have Tni-mmi-im variance. The minimal property is here taken as the starting-point. 
18.2. Consider, then, a frequency function f [x, 0), and as usual let us write 
L = / (xl9 0) . . . f {%n, d). Then, writing 
dx for the %-fold integral over the range 
of the x% we have to find * = t (xl9 . . . xn) such that 
o 
tLdx = 6 (18.1) 
00 
/•OO 
(t — 6)2Ldx = minimum. .... (18.2) 
The first equation may also be written 
(t - 6) L dx = 0 (18.3) 
/•CO 
— oo 
The problem of finding t is one of the familiar problems in the Calculus of Variations. The 
minimal value of (18.2) has to be found subject to the condition (18.1), which is equivalent to 
oo *> r 
t — dx=*l, (18.4) 
J 
— 00 
provided that the range of / is independent of 6 or that / vanishes at any extreme which 
depends on 6. 
If 2X is an unspecified parameter (which may depend on 6 but not on the re's) the 
problem is equivalent to finding an unconditioned minimum of 
The solution is * 
j Ut-dyL~2M~\dx (18.5) 
£{<*-»>,L-^^-° 
* See, for example, J. Edwards, Integral Calculus, vol. 2, article 1504, or A. K. Forsyth, Calculus 
of Variations, article 15. Since the expression to be minimised does not contain d~, the Euler equation 
ox 
for a stationary value to the integral f V dx reduces to ^ = 0. The derivation of (18.7) is not, 
50 
MINIMUM VARIANCE 51 
dT 
or (t - 6)L - X—- =* 0. . . . '. (18.6) 
ov 
We then have 
X BL 
t = 6 + 
Ldd 
,=(0 + A^§^> (18-7) 
where t is a function of the #'s but not of 6. Thus there exists a t satisfying our conditions 
if we can express —-§— in the form 
ov 
d log L _ t - 6 • 
~ae T ' (18,8) 
This is a necessary and sufficient condition, except that it gives only stationary values of 
(18.2) which might, for instance, be maxima instead of minima. This is not a point, 
however, which need detain us from the statistical viewpoint, troublesome as it is to the 
mathematician. 
Example 18.1 
To estimate d in the normal population 
dF = ——— exp \ -~ —- (x — OV \ dx, — oo < x < oo 
g\/(2ti) [ 2a2 J 
where a is assumed known. 
We have 
dfJ G" 
This can be put in the form (18.8) by taking 
a2 
x = t and A = —, 
n 
and hence x is the required estimator. We note that it has minimum variance for any 
n in the class of unbiassed estimators of 6. 
Example 18.2 
To estimate 0 in 
7t I + (x — 0) 
We have 
1 dx 
dF —~ —, — oo <£ < oo. 
5 log L ___ „ f x — 6 
BO {I + (x ~-6>)2 
This cannot be put in the form (18.8) and the method fails. There is no estimator which 
is unbiassed and has minimum variance. 
however, without its difficulties, and I think some conditions have been accidentally suppressed in 
the Aitken-Silverstone method. I understand that Dr. Leon Solomon, working with Dr. Aitken, has 
obtained a proof which depends on the fact that L shall be the product of n independent frequency- 
functions. But for the war the point would doubtless have been cleared up by now, but at present 
it remains open. 
52 ESTIMATION: MISCELLANEOUS METHODS 
18.3. Integrating (18.8) with respect to 6 we have 
log L = a (0).(* - 0) + 0 (0) + Zy (Xj)9 
i 
where a, ft, y are arbitrary functions (apart from the fact that the two former depend on 
A). Hence 
log/ (a, 0) = it (0) (t - 0) + B (0) + 0 (a?) 
= jp (0) « (a) + g (0) + r (a?), say. . . . (18.9) 
Comparing this with (17.83), we see that the method of minimum variance will give a 
solution only if there exists a sufficient estimator. This explains the success of the method 
in Example 18.1 (where x is sufficient) and its failure in Example 18.2 (where no sufficient 
estimator exists). 
18.4. In the method of maximum likelihood it makes no difference to the final 
result whether we estimate for a parameter 0 or for some other parameter % functionally 
related to 0. For 
d log L __ d log L d% 
and the two sides of the equation vanish together. In the method of minimum variance, 
however, there is an interesting difference. 
Suppose we wish to estimate 0 in 
We have 
dF = vmexp (" ^t) **• -«<*<». 
d log L ___ n I U (x2) 
and this may he put in. the form (18.8) with 
1 202 
t = S (x2) and 1 = —. 
n n 
If, however, we consider the parallel problem of estimating a in 
1 / 1 x2\ 
dF = _^^ exp — ~ — ] dx, — oo < x < co 
ay/{2n) \ 2 o2) 
we find 
d log L n £ (#2) 
da a a 
3 
which cannot be put in the form (18.8). We thus reach the peculiar result that the method 
will provide an estimator for o*2 but not for a. It follows that in general we may have 
to estimate, not 0 itself, but some function of 6, say x (0). 
18.5. If a minimum-variance estimator exists for some t (0) we must have 
d log L __t — r 
dx X (r)' 
which is equivalent to 
— (t — x) 
SlogX __ddK } 
MINIMUM VARIANCE 53 
We estimate t by putting it equal to t and thus we shall have, for the estimator, 
^^) ■ - 0 (18.11) 
This is equivalent to the equation of maximum likehhood. The two are not, however, 
identical. Maximum likehhood is not concerned with the existence of the function A. 
Minimum variance takes the function as fundamental, and when it exists the solution 
(which is the same as the maximum likelihood solution) has minimum variance for all n 
in the class of unbiassed estimators, not merely for large n. 
18.6. Let us suppose that 9 is the parameter (transformed if necessary) for which 
the estimating function is 6 itself. Then we have for the minimum-variance estimator t 
var t = (t — 6)2 L dx, 
J — 00 
which, on substitution from (18.8), yields 
r»oo 
var t = 
— 00 
l* (dlo£LYLdx . . . (18.12) 
= -A2j°° (?^~\Ldx, . . .(18.13) 
if the range is independent of 6 or / vanishes at any extreme dependent on 6. 
Now from (18.8) we find 
and hence, substituting in (18.13) and remembering that (t -— 8) L dx = 0, we find 
J —00 
var t = — A2 I ( — -- | L dx 
J-co\ A/ 
= I. (18.14) 
The variance of the minimum-variance estimator is thus simply the parameter 2.. It also 
follows from (18.13) that 
= -nll(d-^&\ (18.15) 
V do* y 
so that the result we reached in Chapter 17, as a limiting form for large n, is now seen to 
be exact for finite n under present conditions. 
Example 18.3 
To estimate 6 in the Type III form 
1 
dF — x9"1 e"x/0 dx, 0 < x < oo, p > 1, 
1 (p) up 
where p is assumed known. 
54 ESTIMATION :.... MISCELLANEOUS METHODS 
We, have; , •■ 
d log L ___ __ np nx 
dd ~~T T2 
which is of the form (18.8) if 
x ' 62 
\ ' ... ' ' t = - and X = —. •-,■:■ 
p w>p 
' 02 . ■■■' 
Thus t is the minimum-variance estimator and has variance — for finite n> even-though. 
Tip 
the distribution is not normal. (Compare Example 17.8.) 
18.7. We may readily determine what function r (d) should be taken as the estimating 
function. Taking the general form from (18.9), 
log/ (x, 6)=p (6) t(x)+q (6) + r (x), 
we have 
log L = p 21 (x) + nq + S r (x) 
or or or 
^nd^(~Z(t) + $-\ (18.16) 
Hence, if 
we have 
log/ = - \ log (2tt) - log a - £—, 
1 
whence p (a) = — —-, ■ t (x) = x2, q = — log o. 
2a2 % 
Thus the appropriate value of t, from (18.17), is 
dor/ 3c; 
(18.17) 
(18.18) 
5r \w dp 
?£ = - ?2/?? 
1 
3 log L __n 
dr t j ^V 
I/n — 
dr 
which is of the required form provided that 
*r* '■—• lb " '. • • • • . .(JLO.JLi/} 
a or 
Example 18.4 
Consider again the estimation of a in 
1 / 1 x2\ 
dF = -77^—9\ exP ( ~ ^ -5 ) dx> ™ °° <# < °°* 
V(2mr2) \ 2 c;2/ 
Here 
cr2, 
■. MINIMUM %* 55 
which is thus determined as our estimating function. For the variance of th# estimator 
of r we have 
the estimator itself being - S (x2). 
7v 
A = l/n ~ = — 
dr n 
4 
Minimum #2 
18.8, We now turn to consider another principle which has been suggested for 
providing estimators. If the data are grouped into cells with expected frequency typified 
by Xj and observed frequency by l^ then the function 
%* ^ Efy -- h)2 (18t20) 
where n = S (^) = E fy) (18.21) 
can, as we saw in Chapter 12, be used as a measure of closeness of fit. The method of 
minimum %2 adopts this standpoint (which is, of course, arbitrary in the logical sense) 
and attempts to determine the parameters K such that %% is a minimum. 
In practice the method is not very easy to apply because of the difficulty of expressing 
the A's in terms of the parameter under estimate, 0. For some illustrations reference 
may be made to Kirstine Smith (1916). We shall not consider the method at length 
here for two reasons :— 
(a) it may be shown that for large samples the minimum-%2 estimator tends to 
the maximum-likelihood estimator ; 
(6) there is a modification of the method, considered below, which is much easier 
to apply. 
18.9. For samples of fixed size n the distribution of the quantities l3- is multinomial, 
and we have for the likelihood function 
n(lj\) / \n) 
i 
nl n(lAli n(^)\ .... (18.22) 
11(1,1) \nj \l 
■i 
x. nus 
log L = constant + Z lj log (y )• • - ■ • (18.23) 
Now for large samples we may put 
where ty is finite and therefore small compared with \; | a^ n* | < l^; and S (fy) = 0. 
56 ESTIMATION: MISCELLANEOUS METHODS 
Hence, from (18.23), 
log L = ifc •+Z«, log ^ 1 +^ 
jfc _ | Z^L-^ + O (»"*). . . . (18.24) 
Now write 
h 
y'2 = J ^ "" ^^ 
= 27^ _ w. (18.25) 
Then we see that, to order %"**, L is maximised by minimising %*. This latter quantity 
is not the same as %2 because the denominator terms are Z's instead of A's. However, for 
large % the difference is of order n~*, for 
If 
= 0 (n-*). 
Hence, to order n~^ the estimates obtained by minimising either %2 or %~ will be equivalent 
to maximising L. 
18-10. The advantage of using %'z instead of %2 in practice resides in the fact that 
the denominators in the former are integral. However, if there are any empty cells (i.e. 
those for which l^ = 0) the formula (18.25) requires some modification. 
In the likelihood function, if ^ = 0, ( ~ 1 =1 for all A,-. The substitution 
X. = if 4. cif nl 
will give us, for the empty cells, a term in (18.24) equal to — Ua^n* = — 27 A;- = M, 
say. Hence we have 
%>* ^z^j ^ W + 2M, .... (18.26) 
where the summation takes place over occupied cells and M is the sum of the theoretical 
frequencies A in the empty cells. 
Example 18.5 
As an example (Jeffreys, 1941) we consider a case where the maximum likelihood 
estimator is known, so that a comparison may be made with the result given by 
minimum %'*. 
Col. (2) of th.6 following table shows the frequency of women in the first class of Part II 
MINIMUM x2 57 
of the Mathematical Tripos from 1910 to 1938 inclusive. Assuming that this distribution 
follows the Poisson distribution —n—, to estimate 0. 
(1) 
Number of 
XXXouo, J 
0 
1 
2 
3 
4 
5 
over 5 
Totals 
(2) 
Frequency 
h 
6 
8 
11 
3 
0 
1 
0 
29 
0 « 1 • 
10-7 
10-7 
5-3 
1-8 
0-5 
0-1 
0-0 
(3) 
e = i-5 
6-5 
9-7 
7-3 
3-6 
1-4 
0-4 
01 
0 = 2 
3-9 
7-9 
7-9 
5-2 
2-6 
1-0 
0-5 
0 = 1 
3-7 
0-9 
3-0 
0-5 
— 
0-8 
2M= 1-0 
9-9 
(4) 
6 = 1-5 
0-0 
0-4 
1-2 
01 
0-4 
2M = 30 
5-1 
0 « 2 
0-7 
0-0 
0-9 
1-6 
— 
0-0 
2M = 6-2 
9-4 
The sample mean (a sufficient estimator of 6) is in this case 44/29 = 1-52 with a standard 
error /- = 0-23. 
To apply minimum %'2 we have to express the theoretical frequencies in terms of 6. 
This results in an unmanageable equation if we then substitute in #'2. Instead we 
calculate the minimum by finding %'2 for some trial values of 0 (in this case 1,1-5 and 2) and 
then interpolating. 
The expectations 1 for the three selected values of d are shown in column (3) of the 
table and the corresponding %'2 in column (4). It is found that, writing 0 = 1-5 -+• <£, 
the values of %2 maY be represented by the quadratic 
x'* = 54 _ o-5(£ + 18-2<£2. 
The minimum of this is given by <j> = 0-01, and hence our estimate of 6 is 1-51, very close 
to the value of 1-52 given by the maximum likelihood estimator. 
18.11. On theoretical grounds there seems no reason to use minimum %2 instead of 
maximum likelihood. The method has some practical value, however, where the 
maximum likelihood equations are difficult to solve. We can usually follow the device of the 
example just given, find %2 or %'2 for some trial values of the parameter, and approximate 
to the value which minimises %2 or %'2. Whether this is easier than finding the maximum 
likelihood estimate in the same sort of way depends on the circumstances of the case, but 
it may well be so when the frequency function is a tabulated integral, so that expected 
frequencies for specified parameter-values can be readily obtained. 
18.12. In the manner of 17.39 we can estimate the loss of information occasioned 
by the use of minimum %2. We have, for the minimum of %2, 
1 r <z - A)2 = o 
S8 ESTIMATION: MISCELLANEOUS METHODS 
which reduces to ' 
SI ~ — = 0. . . . . . (18.27) 
p do 
gince ,. + . tends to the constant value 2 for large samples, this is equivalent to the 
A 
maximum likelihood equation 
2Jl~X~ = 0, (18.28) 
A dd 
1 ; > 
confirming that maximum likelihood and minimum %2 give the same results in the limit. 
Since 
Z2 --P = 22 (Z - X) + (I -~/l)2 
the deviation of —r|— from its mean is 
ou 
IZ !lzA2 dl - x E (Lzii* dA • • • • (18.29) 
t P 36 2 A2 dff 
the first term vanishing on summation. As in 17.39 we find the variance of this quantity 
B loff Jj 
within samples for which ,__ is constant. We have 
ou 
2 U2 (kl'2) 
YMEk{l-iy^2Z(k2X2)-~-E*{kX*)-2Z' [ } 
n M — 
A 
X 
and on substituting k = \ -rr we find 
, Xz 
IZ(* )-j VW, . . . .(18.30) 
giving the loss of .information. 
As the sample size increases, this quantity remains finite. It is interesting to observe, 
however, that as the number of classes increases it also increases without limit, indicating 
that minimum x2 breaks down for fine grouping. 
il Inverse " Probability 
18.13. According to Bayes' theorem (7.24), if h (0) dd is the prior probability of 0 y 
the posterior probability is given by 
P (6 | xu . . . xn) = L {xl9 . . . xn, 6) h (0) dd . . . (18.31 ) 
It is then easy to determine the " most probable " value of 6 by maximising Lh(d) if wo 
know h (0). The principles of inference with which we have been concerned up to th.o 
present do not require the notion of the probability of 6 and, even if they did, would noi> 
give any guide to the nature of the function h (8). In fact, to an adherent of the frequency- 
theory of probability, the prior probability of 6 requires the distribution of 6 in some form., 
and if 6 is merely an unknown constant it has no distribution (except the trivial one tha/t> 
/ = 1 when 0 takes its true value and/ = 0 elsewhere). The alternative school of thought 
assumes the existence of h (0) as denoting a prior measure of belief, but, in order to fiixcl 
LEAST SQUARES, 59 
the most probable value of 6, has to make some further assumption as to its values 
comparable to Bayes' postulate that for a finite range h is a constant. 
We have already noted that on this assumption the maximisation of L is equivalent 
to finding the value of 6 with the greatest posterior probability. It is also interesting to 
note that, whatever the form of h (0), maximum likelihood tends to give the same estimator 
as the method of maximising posterior probability for large n. In fact, for the maximisation 
of P in (18.31) we have 
aiogP _ alogL 3 log fr 
~W. dd~+-~W--°' ' '■ * .-(18.32), 
In ordinary cases the variance of —x§— is of order n9 whereas the second term is 
independent of n. In the limit, therefore, the second term is negligible and we are reduced to 
the likelihood equation 
d log L 
BO 
0. 
Least Squares 
18.14. The method of least squares bears an analogy to minimum %2. Suppose 
we have an expression depending on a number of unknown parameters 6X . . . dp and 
certain observed values x. This can be thrown into a form such as 
h (x, dx . . . 6p) = 0, . . . . (18.33) 
where k is a given function (not a frequency function). If we have n values of x and n > p 
it is not possible to solve the n resulting equations of type (18.33) for the 0's. We then 
consider the " residuals " h (xp dl . . . 0P), and the principle of least squares states that 
the values of Bx . . . 0p are to be chosen so that 
E {k ty, 0X . . . 0p) }2 — minimum, . . . (18.34) 
or, in other words, so as to satisfy the p equations 
a. 
7 ddj 
■^-—{P(^, 0! . . . 0P)} - 0, l - 1 . . . p. . . (18.35) 
18.15. Consider the case when the residuals are all distributed normally with variance 
a2. The logarithm of the likelihood is then (except for constants)— 
1 
logi = — wlogo- — -— 17 fe2 (xj} 6t . . . dp) . . . (18.36) 
2cr2 
and this is clearly maximised by minimising the sum (18.34). In this case, then, the method 
of least squares is equivalent to the method of maximum likelihood. In other cases it 
may give different results, and the justification for using it then becomes more or less 
empirical. 
18.16. The most important case occurring in statistical theory of the use of the 
method of least squares concerns regression equations. We have already seen that the 
coefficients of regression are, in effect, determined so as to minimise the sum of squares of 
residuals (cf. 15.2). We also know that, for the multiple normal distribution, residuals 
from the population regression lines are, in fact, normally distributed (15.13). For normal 
60 . ESTIMATION: MISCELLANEOUS METHODS 
variation, therefore, the method of least squares is equivalent to maximum likelihood so 
far as concerns the simultaneous estimation of regression coefficients. 
18.17* This is a convenient point to prove a theorem (due to Gauss) which in one 
form or another is constantly occurring in statistical theory, particularly in connection 
with the normal distribution. Suppose we have a population (not necessarily normal) 
in which the regression of one variate y on the others x0 ( = 1), x± . . . , xp is given by 
y = j80+j81aI + . . . +j8p<v . . • (18.37) 
The #3s may be correlated among themselves and, in the extreme case, functionally related, 
so that this case includes that of curvilinear regression for our present purposes. Suppose 
that we have a sample of n values, where n > p. Denoting by Z summation over these 
n values, we determine the estimates of the /5's by minimising the sum of squares, e.g. 
2, (y p0 px x± . . . pp xp) . 
Suppose that b0 . . . bp are the solutions of this process. Then our regression formula is 
y — b0 — b1x1 — . . . -— bp xp = 0. . . . (18.38) 
The observed residuals, obtained by substituting the observed values in this equation, 
are typified by 
e = y — b0 — b1 xx . . . — bp xp, . ' . . (18.39) 
whereas the cc real " residuals are typified by 
e — y — fi0 — /?! x1 . . . — /?p xp. . . . (18.40) 
We proceed to compare the sampling variances of e and s and to show that 
var e = — var e, . . . . (18.41) 
n — p — 1 
provided that the residuals are uncorrelated. 
Let us transform the observed values of the x9s to new values £0? £x . . . fp (n for 
each) such that 
S {Xj Sk) = 1 3 = ^ 
= 0 j^k\ (18.42) 
2 (h V) = h 
This involves, for each £, p + 1 equations in n unknowns and is therefore possible in general. 
We then have 
- 2 h (e ~ e) = Z £,, { (/?0 - 60) + (/?! ~b1)xl + . . . {pp - bP) xp } 
= pJc — bk. 
But Z £ke = Z (^ y) — Z £k {b0 + b1z1 + . . . bp xp] 
= h - h = °- 
Hence /?/c -— bk = —- Z %ke. . . . . (18.43) 
Now — Z e (e — e) = Z {y — b0 — . . . — bp xp} { (f}0 — b0) + - . . (/S^ — bp) xp} 
since the summations give terms the vanishing of which determines the 6's. Hence 
Z e2 — Z e2 = Z (e — e) e 
= S (bj — /??-) 27 xfJ e, 
NOTES AND REFERENCES—EXERCISES 61 
where S denotes summation over the (p + 1) values of j, 
= 8 S Sj b £ X] e 
= S {HgjXje*} + cross-product terms in e, 
cross-product terms. 
When we take expectations the cross-product terms vanish since the residuals are 
uncorrected. Hence 
E{S^) -E{8 e2) =zEZe\ 
or (n — p — 1) var e = n var e, . . . (18.44) 
from which (18.41) follows at once. 
For normal variation we shall consider this result from a slightly different viewpoint 
in Chapter 22. 
NOTES AND REFERENCES 
The approach to minimum-variance estimators through the Calculus of Variations is 
due to Aitken and Silverstone (1942). For minimum %2 see K. Smith (1916) and R. A. 
Fisher (1922a, 19256). For the modification x'2 see Jeffreys (19386, 19396, 1941). 
A method of estimation essentially depending on the median has been proposed for 
use in quality control, but its value is as yet problematical. For an account of the technique 
see Simon (1941). 
EXERCISES 
18.1. From the property that the variance of a minimum-variance estimator is 
equal to A show that the most general distribution for which the sample mean is a sufficient 
estimator is 
/ (x, 0) = c (x, ex) exp 1 - _ (x - 6>)2 L 
where c is an arbitrary function and a2 is the variance of /. 
Hence show that no Pearson carve other than the normal admits the sample-mean 
as a sufficient estimator, but that a Gram-Charlier series may do so. 
(Aitken and Silverstone, 1942.) 
18.2. If the function A exists and 
,m f dO 
*{e) = hWY 
show that the variance of the estimator t is 
_ 3 ^, 
n dot2' 
where q is the function of 18.7. (Aitken and Silverstone, 1942.) 
18.3. If a population (p -f- g)4 is regarded as distributed in 5 classes, show that the 
intrinsic accuracy is —. Show further that the loss of information through estimating 
p from minimum %2 is 
This is least when p — q and is then equivalent to the loss of 5 observations. 
(Fisher, 19256.) 
CHAPTER 19 
CONFIDENCE INTERVALS 
19.1. In the previous two chapters we have been concerned, with methods which 
will provide an estimate of the value of one or more unknown parameters ; and the methods 
gave functions of the sample values—the estimators—which, for any given sample, 
provided a unique estimate. It was of course fully recognised that the estimate might differ 
from the parameter in any particular case, and hence that there was a margin of 
uncertainty. The extent of this uncertainty was expressed in terms of the sampling variance 
of the estimator. With the somewhat intuitional approach which has served our purpose 
up to this point, we say that it is probable that 6 lies in the range t ± V var t, very probable 
that it lies in the range t ± 2 V var t, and so on. In short, what we have done is in effect 
to locate Q in a range and not at a particular point, although we have regarded one point 
in the range, viz. t itself, as having a claim to be considered as the " best " estimate of 0, 
19.2. In the present chapter we shall examine the logic of this procedure more 
closely and look at the problem of estimation from a different point of view. We now 
abandon attempts to estimate 6 by a function which, for a specified sample, gives a, unique 
number. Instead we shall consider merely the specification of a range in which 0 lies. 
We shall not attempt to specify whereabouts in the interval the value of 0 really is ; all 
values in the range have an equal claim to be taken as the " true " value. Nor shall we 
assess the probability that 6 lies in the interval in the sense that 0 is regarded as a random 
variable. In fact, in the frequency theory of probability 0 is not a random variable (except 
trivially in that the frequency of 9 is unity when it takes the true value and is zero 
elsewhere). Nevertheless, probability plays an essential part in the determination of the 
interval and in the degree of confidence we have that it cc covers " 0. 
Case of one Unknown Parameter 
19.3. Consider in the first place a population dependent on a single unknown 
parameter 6 and suppose that we are given a random sample of n values xx . . . :vn from the 
population. Let z be a statistic dependent on the x's and on 9, whose sampling distribution 
is independent of d. (The examples given below will show that in some eases at least such 
a statistic may be found.) Then, given any probability a, we can find a value z, such that 
dF (z) = a, 
J —CO 
and this is true whatever the value of 6. In the notation of the theory of probability we 
shall then have 
P(z <zx\ 0) = a (19.1) 
Now it may happen that the inequality z <zx can be transformed to the form 0 " tx or 
6 > tl9 where tx is some function depending on the value zx and the x'a but not on (9. For 
instance, if z = x — 6 we shall have 
x ~~ 6 <zx 
and hence Q > x — zx. 
If this transformation can be made we then have, from (19.1), 
P(fl<*1|fl)B=a (19.2) 
62 
CASE OP ONE UNKNOWN - PARAMETER 63 
More generally, suppose that we can find a function tl9 depending on oc and the x's 
but not on 6, such that (19.2) is true for all 6. Then we may use this equation in. probability 
to make certain statements about 6. 
19.4. Note, in the first place, that we cannot assert that the probability is a that 
6 does not exceed a constant tlt This statement (in the frequency theory of probability) 
can only relate to the variation of 6 in a population of 0's, and in general we do not know 
that 6 varies at all. If it is merely an unknown constant then the probability that 6 < ^ 
is either unity or zero. We do not know which of these values is correct, but we do know 
that one of them is correct. 
We therefore look at the matter in another way. Although 6 is not a random variable, 
#i is and will vary from sample to sample. Consequently, if we assert that d < tt in each 
case presented for decision, we shall be right in a proportion a of the cases in the long run. 
The statement that the probability of 6 is less than or equal to some assigned value 
has no meaning except in the trivial sense already mentioned; but the statement that 
a statistic tt is greater than or equal to 6 (whatever 6 happens to be) has a definite 
probability a of being correct. If therefore we make it a rule to assert the inequality 6 < tx 
for any sample values which arise, we have the assurance of being right in a proportion 
a of the cases'" on the average" or "in the long run." 
This idea is basic to the theory of confidence intervals which we proceed to develop, 
and the reader should satisfy himself that he has grasped it. 
19.5. To simplify the exposition we have considered only a single quantity tx and 
the statement that 6 < tx. In practice, however, we usually seek for two quantities t0 
and tl9 such that 
P{t<> <6 <tx.|0} = a, . . . . (19.3) 
and make the assertion that 0 lies in the range t0 to tx. These quantities are known as the 
Lower and Upper Confidence Limits respectively. They depend only on a and the sample 
values. For any fixed a the totality of values of t0 and tx for different samples determine 
a field within which 0 is asserted to lie. This field is called the Confidence Belt or Region 
of Acceptance. We shall give a graphical representation of the idea below. The number 
a is called the Confidence Coefficient. 
Example 19.1 
Suppose we have a sample of n from the normal population with unit variance 
1 
dF = ■ exp {— J (x — /i)2} dx, — oo < x < oo. 
V (2tt) 
The distribution of means x will be 
— - (x — fi)2 I dx, — oo < x < oo. 
From the tables of the normal integral we know that the probability of a positive deviation 
from the mean not greater than twice the. standard deviation is 0-97725. We have 
then— 
x - a < —:- \a \ 5= 0-97725, 
64 CONFIDENCE INTERVALS 
which is equivalent to 
£_ ^ < | 1 = o-97725. 
Thus, if we assert that fi is greater than or equal to x - 2/Vn we shall be right in about 
97-725 per cent, of the cases. 
Similarly we have 
9 
P^_/,>__|;M|=p|/< < x + ^ I p \ = 0-97725. 
Hence, combining the two results, 
p\x -4" <^<£ + 4-|/4 = 2(0-97725) - 1 =0-9545. 
Hence, if we assert that p lies in the range x ± 2/V^ we shall be right in about 95-45 per 
oent. of the cases in the long run. 
Conversely, given the confidence coefficient we can easily find from the tables of the 
r 7 7 "\ 
normal integral the deviation d such that P < x y- < u < x + —■ \ = a. For instance, 
if a = 0-8, e£ = 1-28, so that if we assert that fx lies in the range x ± 1-28/Vw the odds 
are 4 to 1 that we shall be right. 
The reader to whom this approach is new will probably ask : but is this not a 
roundabout way of using the standard error to set limits to an estimate of the mean ? In a 
way, it is. In effect, what we have done in this example is to show how the use of the 
standard error of the mean in normal samples may be justified on logical grounds without 
appeal to new principles of inference other than those incorporated in the theory of 
probability itself. In particular we make no use of Bayes' postulate. 
Another point of interest in this example is that the upper and lower confidence limits 
derived above are equidistant from the mean x. This is not by any means necessary, 
and it is easy to see that we can derive any number of alternative limits for the same 
confidence coefficient a. Suppose, for instance, we take a = 0-9545, and select two numbers 
a0 and a1? which obey the condition 
(ct0 + ax — 1) = 0-9545, 
say a0 = 0-9645 and o^ = 0-99. From the tables of the normal integral we have 
tJ - 2-326 ] 
P<x — fi < -y— \}A = 0-99 
and hence 
P^x - fi> — | n )• = 0-9645, 
_ 2-326 ^ .. 1-806 , I 
x j- <u < # H — //, } = 0-9545. 
yn r y/n ' f 
Thus, with the same confidence coefficient we can assert that// lies in the range x — 2/Vn, 
to x + 2/vX or in the range x - 2-326/Vn to x + l-806/V^. In either case we shall be 
right in about 95-45 per cent, of the cases. 
We note that in the first case the range is 4/V^ units and in the second case it is 
4-132/V/2* units. Other things being equal, we should choose the first set of limits since 
GRAPHICAL REPRESENTATION • 65 
they locate the parameter in a narrower range. We shall consider this point in more 
detail below. It does not always happen that there is an infinity of possible confidence 
limits or, if there is, that any simple rule of choice between them can be formulated. 
Graphical Representation 
19.6. In a number of simple cases, including that of the previous example, the 
confidence limits can be represented in a useful graphical form. We take two orthogonal 
axes, OX relating to the observed x and OY to p, (see Eig. 19.1). 
The two straight lines shown have as their equations 
/i = x + 2, /i = x —- 2. 
Consequently, for any point between the lines, 
x — 2 < [a < x + 2. 
Hence, if for any observed x we read off the two ordinates on the lines corresponding to 
that value we obtain the two confidence limits. The vertical interval between the limits 
is the confidence range (shown in the diagram for x = 1), and the total zone between the 
lines is the confidence belt. We may refer to the two lines as the Upper and Lower 
Confidence lines respectively. 
This example relates to the somewhat trivial case n = 1. For different values of n 
there will be different confidence lines, all parallel to \i = x. They may be shown on a 
single diagram for selected values of n, and a figure so constructed provides a useful method 
of reading off confidence limits in practical work. 
a.s.—vol. ii. f 
66 CONFIDENCE INTERVALS 
Central and Non-central Intervals 
19.7. In Example 19.1 the sampling distribution on which the confidence intervals 
were based was symmetrical, and hence, by taking equal deviations from the mean, we 
reached equal areas of the frequency function as oc0 and ax. In general we cannot achieve 
this result with equal deviations, and subject always to the condition a0 + <x.x — 1 = a 
the two quantities may be chosen arbitrarily. 
If a0 and ax are taken to be equal, we shall say that the intervals are central. In such 
a case we have 
P(t0 <0) =P(0 <*x) = X-~ (19.4) 
In the contrary case the intervals will be called non-central. 
19.8. In the absence of other considerations it is usually convenient to employ 
central intervals, but circumstances sometimes arise in which non-central intervals are 
more serviceable. Suppose, for instance, we are estimating the proportion of some drug 
in a medicinal preparation and the drug is toxic in large doses. We must then clearly 
err on the safe side, an excess of the true value over our estimate being more serious than 
a deficiency. In such a case we might prefer to take ax very near to unity or even equal 
to unity, so that 
P (0 < tt) - 1 
P (t0 < 0) = a, 
and we are certain that 6 is not greater than tt. 
Again, if we are estimating the proportion of viable seed, in a sample of material that 
is to be placed on the market, we are more concerned with the accuracy of the lower limit 
than that of the upper limit, for a deficiency of germination is more serious than an. excess 
from the grower's point of view. In such, circumstances we should probably take a„ as 
large as conveniently possible so as to be nearer to certainty about the minimum value 
of viability. This kind of situation often arises in the specification of the quality of a 
manufactured product, the seller wishing to guarantee a minimum standard but being 
much less concerned with whether his product exceeds expectation. 
19.9. On a somewhat similar point, it may be remarked that in certain, 
circumstances it is enough to know that P {t0 < 0 < tt \ 0 } exceeds some quantity a. We then 
know that in asserting 0 to lie in the range t0 to tx we shall be right in at least a proportion 
oc of the cases. Mathematical difficulties in ascertaining confidence limits exactly for 
given oc, or theoretical difficulties when the distribution is discontinuous may, for example, 
lead us to be content with the inequality rather than the equality of (19.3). 
Example 19.2 
To find confidence intervals for the parent proportion w of successes in sampling for 
attributes. 
In samples of n the distribution of successes is given by the binomial (% + cj)". We 
will determine the limits for the case n = 20 and confidence coefficient 0-95. 
We require in the first instance the distribution function of the binomial, which is 
obtainable from Table 5.2 (vol, I, p. 119). Summing the number of successes and dividing 
by 10,000, we find from that table the following :— 
CENTRAL AND NON-CENTRAL INTERVALS 
67 
Proportion of 
Successes 
p 
0-00 
0-05 
0-10 
0-15 
0-20 
0-25 
0-30 
0-35 
0-40 
0-45 
0-50 
0-55 
0-60 
0-65 
0-70 
0-75 
0-80 
0-85 
0-90 
0-95 
-™-v-»„ _- . - „,„, -, ,i„.rt „■ 
W = 0-1 
0-1216 
0-3918 
0-6770 
0-8671 
0-9569 
0-9888 
0-9977 
0-9997 
1-0001 
1-0002 
— 
— 
— 
— 
— 
-— 
—— 
—„ 
~"""'■ ■""'• 
I 
w = 0-2 
0-0115 
0-0691 
0-2060 
0-4114 
0-6296 
0-8042 
0-9133 
0-9678 
0-9900 
0-9974 
0-9994 
0-9999 
10000 
..„„„ 
""™-- 
m = 0-3 
0-0008 
0-0076 
00354 
0-1070 
0-2374 
0-4163 
0-6079 
0-7722 
0-8866 
0-9520 
0-9828 
0-9948 
0-9987 
0-9997 
0-9999 
— 
— 
W = 0-4 
0-0005 
0-0036 
0-0159 
0-0509 
0-1255 
0-2499 
0-4158 
0-5955 
0-7552 
0-8723 
0-9433 
0-9788 
0-9934 
0-9983 
0-9996 
0-9999 
— 
—. 
-~~~"","""n* 
w « 0-5 
0-0002 
0-0013 
0-0059 
0-0207 
0-0577 
0-1316 
0-2517 
0-4119 
0-5881 
0-7483 
0-8684 
0-9423 
0-9793 
0-9941 
0-9987 
0-9998 
1 -0000 
The final figures may be a unit or two in error owing to rounding up, but that need 
not bother us to the degree of approximation here considered. Values for w = 0-6 to 0-9 
may be obtained by symmetry. 
We note in the first place that the variate p is discontinuous. On the other hand 
we are prepared to consider any value of w in the range 0 to 1. For given m we cannot 
in general find limits to p for which a is exactly 0-95 ; but we will take p to be the nearest 
multiple of 0-05 which gives confidence coefficients at least equal to 0-95, so as to be on 
the safe side. We will consider only central intervals, so that for given w we have to find 
p0 and px such that 
P {w >Po} > 0-975 
P {w <px} > 0-975, 
the inequalities for P being as near to equality as we can make them. 
Consider the diagrammatic representation of the type shown in Fig. 19.1 and given 
for our present case in Fig. 19.2. 
From the table we can find, for any assigned w, the values w0 and wx such that 
P (]) > wQ) > 0-975 and P (p < wx) > 0-975. Note that in determining wx the distribution 
function gives the probability of obtaining a proportion p or less that the 
complement of the function gives the probability of a proportion 1 — p ~~ 0-05 or less 
(not 1 — p). Here, for example, on the horizontal through w = 0*1 we find vjq ™ 0 and 
mx = 0*30 from our table ; and for w ~ 0-4 we have w0 — 0-15 and wx = 0-65. The points 
so obtained lie on stepped curves which have been drawn in. The zone between them is 
the confidence belt. For any p the probability that we shall be wrong in locating w inside 
the belt is at the most 0-05. We determine p{) and px by drawing a vertical at the given 
value of p on the abscissa and reading off the values where it intersects the curves. That 
these are, in fact, the required limits will be shown in a moment. 
68 CONFIDENCE INTERVALS 
We could have found more-preciae confidence limits by interpolating in the labl, 
obtained above. For example, with p = 0-30 we see that 
for w = 0-1, P = 0-9977 
for w = 0-2, P = 0-9133. 
Hence, for P = 0-975 we have approximately 
w 
_ o-i + ??ZLriZ5? (o-i) = 0-127, 
"" ^ 9977 - 9133 
and closer approximations can 
be obtained if desired. The corresponding point, on tlw 
0-5 
Values of p 
Fig. 19.2. 
1-0 
lower confidence line to wx = 0-127 is p = 0-35. Calculations on these lines #ive us the 
values of zo such that 
P{^o <^ <^i} = oc exactly, 
whereas the former approach gave values such that 
P {Po <® 
Pi) 
- a approximately, 
^ a in any case. 
Discontinuous variates usually give rise to this sort of arithmetical nuisance, but the 
approximation in practice is sufficiently good, except for very small samples. The broken 
curves in Fig. 19.2 give the more precise limits. They lie, of coarse, inside the mure 
approximate step-curves. 
It is, perhaps, worth noticing that the points on the curves of Fig. 19.2 were const rueted 
by selecting an ordinate w and then finding the corresponding abscissae m0 and mt. The 
diagram is, so to speak, constructed horizontally. In applying it, however, wo read il 
vertically, that is to say, with observed abscissa p we read off two values p0 and pt and 
assert that p0 < w <px. It is instructive to observe how this change of viewpoint can 
be justified without reference to Bayes' postulate. 
CONFIDENCE INTERVALS FOR LARGE SAMPLES 
69 
Consider Fig. 19.3, which shows a pair of confidence lines for the binomial. Let mf 
be a given value of w and let the horizontal through w( meet the confidence lines in points 
with abscissae w0 and mx. Then we know that in repeated samples from a population 
with parameter m' a proportion <x will give observed values of p lying between w0 and mx; 
for the curves were constructed so that this should be so. 
Now since the horizontal at m' lies entirely within the confidence belt for w0 <p < wt 
(and does so for any m')9 it follows that the assertion that mr lies in the belt is correct if, 
Values 
of 
5J 
57 
0 / J- / 
Values of p 
Fig. 19.3. 
and only if, p lies between w0 and wl9 that is in a proportion a of the cases. This, being 
true for any m', is true for all m\ irrespective of the relative frequency of occurrence of the 
zd's under estimate. Consequently our assertion that w lies in the confidence belt is correct 
in a proportion a of the cases ; and, in particular, for any observed p we may assert that 
m lies within the ordinates determined on the two curves by the vertical through p. 
Confidence Intervals for Large Samples 
19.10. In our usual notation, the logarithm of the likelihood function gives 
n 
logL = Y^log f(xp 6), 
i-i 
and 
d log L ____ yd log/ 
__ _ —^-. 
(19 .5 
(19.6) 
We may regard —~— as a random variable, and in particular write- 
dd 
so that 
A ( d log L 
nA = var —-~— 
30 
3 log/ 
A ■=■ var 
36 
(19.7) 
70 CONFIDENCE INTERVALS 
dlogL 
Write w = ~-~~ (19.8) 
Then, for large samples, xp will be distributed normally in the limit with unit variance, in 
virtue of the Central Limit Theorem, under very general conditions. It will also have 
zero mean, since 
dd ) \fdd 
J-dx = —z\ fdx 
= ^.1-0 (19.9) 
Hence, from the distribution of xp we may easily determine confidence limits for 6 in large 
samples if xp is a monotonic function of 6, so that inequalities in one may be transformed to 
inequalities in the other. 
Bf 
It is sufficient (but not necessary) for the existence of the normal limit to xp that -r^ 
exists for all x, except perhaps at isolated points, that the range is independent of d and 
3 1 -P 
that the Central Limit Theorem applies (e.g. if the third moment of —^^- exists). We 
also assume, as usual, that differentiation under the integral sign, as in (19.9), is legitimate. 
Example 19.3 
Consider again the problem of Example 19.1. We have, with fi for (9, 
1 
f(x> /') = -j7zr\ exP {- 2 (a - /0s} 
d log/ 
dju 
/ O 10£ T \ 1 i'00 
var 
1 f°° 
— I (x-fi)zfdx 
= 1. 
Hence xp = ZI ~.—--J- J = (x — /u) \/n 
is normally distributed with unit variance for large n. (We know, of course, that this 
is true for small n as well in this particular case.) The confidence limits may then be set 
as in Example 19.1. 
Example 19.4 
Consider the Poisson distribution whose general term is 
f(z, X) = —^—. 
x ! 
SHORTEST SETS OF CONFIDENCE INTERVALS 71 
We have 
dlogf _x __ l 
dX X 
var 
di J ^\x ) xi 
_ 1 
i 
"~ Jm4 [X J 72/ I 
Hence W = —r. ;;-;— = » / t (^ ~~ ^)* 
v ~- v(^r ~vi 
For example, with a = 0-95, corresponding to a normal deviate ± 196, we have, for the 
central confidence limits, 
giving, on solution for 1, 
(x -A) . /-= ± 1-96, 
P - ( 2x + ***\ I + x2 = 0 
_ , 1-92 , //3-845 , 3-69 
n v \ n n2 
the ambiguity in the square root giving upper and lower limits respectively. 
To order rr* this is equivalent to 
X = x + 1-96 /-, 
from which the upper and lower limits are seen to be equidistant from the mean x, as we 
should expect. 
Shortest Sets of Confidence Intervals 
19.11. It has been seen in Example 19.1 that in some circumstances at least there 
exist more than one set of confidence intervals, and it is now necessary to consider whether 
any particular set can be regarded as better than the others in any useful sense. The 
problem is analogous to that of estimators, where we found that in general there are many 
different estimators for a parameter, but that we could sometimes find one (such as that 
with minimum variance) which was superior to the rest. 
In Example 19.1 the problem presented itself in rather a specialised form. We found 
that for the intervals based on the mean x there were infinitely many sets of intervals 
according to the way in which we selected oc0 and at (subject to the condition that 
oc0 + ax = 1 + a). Among these the central intervals are obviously the shortest, for a 
given range will include the greatest area of the normal curve if it is centred at the mean 
of the curve. We might reasonably say that the central intervals are the best among 
those determined by x. 
But it does not follow that they are the shortest of all possible intervals, or even that 
such a shortest set exists. It might also happen that for two sets of intervals cx and c2 
those of cx are shorter than those of c2 in part of the range of #'s and longer in other parts. 
72 
CONFIDENCE INTERVALS 
19.12. We will therefore consider sets of intervals which are shortest on the average. 
That is to say, if 
d = tfi — t0 
we require that 
i 
. (19.10) 
d dF = minimum, .... 
where the integral is taken over all #'s and is therefore equivalent to 
p Too 
1 ... 1 0 JLj OjX^ . . . (X/tCm* .... ^iy.JLJL^ 
J ~O0 J —QO 
We now prove a theorem which is very similar to the result that maximum-likelihood 
estimators in the limit have minimum variance, namely that in a certain class of intervals 
the method of 19.10 gives those which are shortest on the average. 
Let k (x, 6) be a function which has a zero mean value and is such that the sum of 
a number of similar functions obeys the Central Limit Theorem. Then 
n 
£ 
2^h [xp 6) 
-/=! 
<\/(n var h) 
. (19.12) 
is normally distributed in the limit with zero mean and unit variance. %p of equation 
(19.8) is a member of the class £. We prove that the average rate of change of y; with 
respect to 0, for each fixed 6, is greater than that of any f except in the trivial case 
3 log/ 
dd ' 
h = Jc 
d log f 
Writing g (x, 0) = —^-^, we have 
dd 
dip 
89 
1 
-izdl 
Hence 
<\/(n var g) [ dd 
i r dh __ 
\/{n var h) \ dd 
2 var g 
1 
yd var g 
i 
\dd J -\/(n var g) 
do 
^7 d var h 
var h dd 
1 v ™ / x 3 var Q 
HE (a) 'j 
2vary KJ} dO 
(19.13) 
(19.14) 
Now E (g) = 0 and 
E 
dd 
Thus 
E ( a2 lQg^ 
E 
9 log/ 
dd 
E 
dip 
dd 
nE {g2) 
^/(n var g) 
V (n var g) = Au say. 
Similarly, 
E 
at 
ae 
% 
var h 
E\dd) ==A^ say* 
(19.15) 
. (19.16) 
JOLtJ LlOtJ 
SHORTEST SETS OF CONFIDENCE INTERVALS 73 
Since E (h) = 0 we have 
= — cov (h, g). . . . . . . (19.17) 
A\ ~- A% = n var a cov2 (h, g) 
var h 
var A 
Thus, unless h is a multiple of g, we have 
ZJ i ^> ZJ o 
which was to be proved. 
Now if Wq is a value such that 
i rva 
(var h var g — cov2 (h, g) }. . . (19.18) 
1 ' fi-^X2 ft™ 1 „ 
V(2»)J. 2' 
the upper and lower confidence points for central intervals are i ipa and the values of 0 
are the solutions of 
S1^3 =±VaL, (19.19) 
y(n var g) 
say £0 and £x. Similarly those for any function h are given by 
V^-L = ± V» (19-20) 
V (?fc var /&) 
say u0 and ^x. The equations for confidence points are equivalent to 
W (t) = ± V* 
'Q (u) = :,|:i y<* 
or, effectively, in large samples, by 
V (0.) + (< - <M (||)() = ± Tec 
C(0.) + («-0o)(|^ = ± Wa, 
where 0O is a fixed value of 0. When t -- 0o and u = 0o we have ?/> (0o) = £ (0o)* Hence 
^^(^)», = (tt~0o)(il (19-21) 
Now we have just shown that, on the average, — > -^. Hence, on the average, 
t — 0O < u — 0O, 
and the confidence limits t are closer together than those of any member of the class u for 
any fixed value of 6. 
comparison of the result we have just proved and the properties of 
maximum likelihood estimators in the limit will show the close relation between confidence 
intervals and the theory of estimation developed in Chapter 17. In 17.27 we showed, 
74 CONFIDENCE INTERVALS 
hy cohering the qM»t% . - ££* that any „*-_ , which h in th, fe* 
distributed normally about the true value 0o cannot have a variance less than 
31og/\* 
1/nE 
30 
and that the latter quantity, in the limit, is the variance of the maximum likelihood 
estimator. It attains the minimal value when u is constant over samples for which t is constant. 
The theorem of 19.12 shows that on the average the intervals determined by the 
distribution of u are shorter than those based on any other function with a zero mean value 
(obeying the usual conditions as to continuity, etc.). Since the maximum likelihood 
estimator has minimum variance, we should expect that confidence intervals based on its 
distribution would be shorter than others ; and this we now see to be so. For if u is constant 
over samples of constant t, the distribution of u in all samples is equivalent to that of t. 
Confidence Intervals and Sufficient Estimators 
19.14. Pursuing this line of thought, we are led to inquire whether sufficient 
estimators provide confidence intervals for finite samples and whether they have any minimal 
properties of the kind we have just established for large samples. 
It is easy to see that sufficient estimators do in fact provide confidence intervals. 
If t is sufficient for d, the likelihood function may be put in the form 
L^f1(t,6)f2(xl . . . xn) .... (19.22) 
and the distribution of t and 6 is 
dF =/i(£, 6)dt (19.23) 
Given oc we can then find t0 and t± such that F (t0} 0) = 1 — a0 and F (tl9 0) = ax and solve 
for 6 in terms of t0 and a0 or tx and al9 as the case may be. This process will provide the 
inequalities of the type we require, a proposition which we shall prove formally below 
(19.25). 
Example 19.5 
In Example 17.8 we saw that 
is sufficient for 0 in the distribution 
e = ± 
p 
dF = „, ' dx, 0 < x < oo, p > 1, 
f(p)Op 
where p is regarded as known. The distribution of 0 is in fact 
S'^exp (-^ 
^"U/ ~~T"(Hp)~\ d°- 
The distribution function of m = -~— is the incomplete jT-function 
rm (np) T ( m 
r (np) V VfaP) 
CONFIDENCE INTERVALS AND SUFFICIENT ESTIMATORS 75 
We then find the values of m corresponding to a0 and ocj. from the tables, and have 
P (m < m0) = oc0 
P (m^ mx) = ocl3 
whence 
[m0 mx } 
•=■ a. 
19.15. The position in regard to minimal properties of confidence intervals based 
on sufficient estimators remains somewhat obscure, but one would expect some such proper- 
9 lofif L 
ties to hold even for finite n. Since u = —rf— is constant for constant t when t is sufficient, 
ov 
the variance of u will be a function of the variance of t. This, however, is not necessarily 
enough to establish the fact that the corresponding confidence intervals are shortest on the 
average. It is imaginable that the confidence intervals derived from its distribution might 
be longer on the average than those of some other system. This seems rather unlikely, 
at least for the ordinary distributions of statistical theory, but apparently no proof has 
been given. 
19.16. Neyman (19376) has proposed to apply the phrase "shortest confidence 
intervals " to sets of intervals defined in quite a different way. As it does not appear 
that such intervals are necessarily the shortest in the sense of possessing the least length, 
even on the average, we shall attempt to avoid confusion by calling them " most selective." 
Consider a set of intervals c0, typified by <5, obeying the condition that 
P {d0cd | 0} = oc, (19.24) 
where we write d0 c 6—that is, dQ cc contains " 6—for the more usual t0 < 0 < tx (tx — t0 = d0). 
Let cx be some other set typified by <3i such that 
P {d1c0\6} = a. (19.25) 
Either set is a permissible set of intervals, as the probability is oc in both cases that the 
range d contains 6. 
If now for every c± we have, for any value Q' other than the true value, 
P {S0c(r \0} <P {d1cOf |0}, . . . .(19.26) 
c0 is said to be most selective. 
19.17. The ideas underlying this definition will be clearer from a reading of Chapters 
26 and 27 dealing with the Neyman-Fearson theory of inference. We anticipate them here 
to the extent of remarking that the object of most selective intervals is to cover the true 
value with assigned probability a, but to cover other values as little as possible. We may 
say of both c0 and cx that the assertion 6 c 9 is true in proportion a of the cases. What 
marks out c0 for choice as the most selective set is that it covers false values less frequently 
than the remaining sets. 
The difference between this approach and the one leading to shortest intervals is that 
the latter is concerned only with the narrowness of the confidence interval, whereas the 
former gives weight to the frequency with which alternative values of 6 are covered. One 
76 CONFIDENCE INTERVALS 
concentrates on locating 6 with the smallest margin of error ; the other takes into account 
the desirability of excluding so far as possible false values of 6 from the interval, so that 
mistakes of taking the wrong value are minimised. 
19.18. Neyman himself has shown that most selective sets do not usually exist (for 
instance, if the distribution is continuous) and has proposed two alternative systems :— 
(a) most selective one-sided systems (Neyman's " shortest one-sided " sets) which 
obey (19.26) only for values of 6' —■ 6 which are always positive or always negative ; 
(b) selective unbiassed systems (Neyman's " short unbiassed " sets) which obey 
(19.25) but, in place of (19.26), the further relation 
p {dc6\d} = oc>P {dc6 \d'} (19.27) 
In essence these sets amount to a translation into terms of confidence intervals of 
certain ideas in the theory of tests of significance, and we may defer consideration of them 
until Chapters 26 and 27 are reached. 
Generalisation to the Case of Several Parameters 
19.19. We now proceed to generalise the foregoing theory to the case of several 
parameters. Although, to simplify the exposition, we shall deal in detail only with a single 
variate, the theory is quite general. We begin by extending our notation and introducing 
a geometrical terminology which may be regarded as an elaboration of the diagrams of 
Eigs. 19.1 and 19.2. 
Suppose we have a frequency function of known form depending on I unknown 
parameters, 61 . . . dh and denoted by / (x, dl . . . Bt). We may require to estimate either 
6± only or several of the 0's simultaneously. In the first place we consider only the 
estimation of a single parameter. To determine confidence limits we require to find two functions 
u0 and uu dependent on the sample values but not on the 0's, such that 
P {u0 < 6± < ux | Qx . . . 0t) = a, . . . . (19.28) 
where a is the confidence coefficient chosen in advance. 
With a sample of n values, x± . . . xn, we can associate a point in an ^-dimensional 
Euclidean space, and the frequency-distribution will determine a density function for 
each such point. The quantities u0 and ul9 being functions of the x's, are determined in 
this space, and for any given a will he on two hypersurfaces (the natural extension of the 
confidence lines of Fig. 19.1). Between them will lie a Confidence Zone or Region of 
Acceptance. 
In general we also have to consider a range of values of 6 which are a priori possible. 
There will thus be an Z-dimensional space of d's subjoined to the w-space, the total region 
of variation having (I -f- n) dimensions ; but if we are considering the estimation of 6U 
this reduces to an (n -f l)-space, the other (I — 1) parameters not appearing as variables. 
We shall call the sample-space W and denote a point whose co-ordinates ax e Ju j_ . . . x,yi 
by E. We may then write u0 (E), u± (E) to show that the confidence functions depend 
on E. The interval ux (E) — uQ (E) we denote by d (E) or d, and as above we write d c 6X 
to denote uQ <i61 <-w1. The region of acceptance or confidence zone we denote by A, 
and may write E e <3 or E e A to indicate that the sample-point lies in the interval <5 or 
the region A. 
GENERALISATION TO THE CASE OF SEVERAL PARAMETERS 77 
19.20, In Fig. 19.4 we have shown two axes xx and x2 and a third axis corresponding 
to the variation of 0X. The sample-space W is thus two-dimensional. For any given 
0l9 say 0i, the space W is a hyperplane (or part of it), one such being shown. 
Fig. 19.4. 
Take any given pair of values (xu x2) and draw through the point so defined a line 
parallel to the 0x-axis, such as PQ in the figure, cutting the hyperplane at B. The two 
values of u0 and u± will give two limits to 0X corresponding to two points on this line, say 
U, V. Consider now the lines PQ as xl9 x2 vary. In some cases U, V will lie on opposite 
sides of 11, and 01 lies inside the interval UV. In other cases (as for instance in TJ'V shown 
in the figure) the contrary is true. The totality of points in the former category 
determines the region of acceptance A, shaded in the figure. If for any point in A we assert 
S c 0i, we shall be right; if we assert it for points outside A we shall be wrong. 
19.21. Evidently, if the sample-point E falls in the region A, the corresponding 
0t lies in the confidence interval and conversely. It follows that the probability of any 
fixed 0[ lying in the confidence interval is the probability that E lies in A (Q\); or in 
symbols— 
P{d c61 \61 . . . 0,} =P {uQ <d[ <ux\ 01 . . . 6,} 
= P {E e A {61) \01 . . . 0J. . (19.29) 
From this it follows that if the confidence functions are determined so that 
P{u0 < 0X < ux | 0! . . . 0j} = a 
we shall have, for all 0l7 
P{E s A (0X) | 0X . . . 0,} = a (19.30) 
It follows also that for no Qt can the region A be empty, for if it were the probability in 
(19.30) would be zero. 
78 CONFIDENCE INTERVALS 
19.22. If the functions u0 and u± are single-valued and determined for all E, then 
any sample-point will fall into at least one region of acceptance. For on the line PQ 
corresponding to the given E we take an R between U and V, and this will define a value of 
0l3 say 0i, such that E eA (d[). 
More importantly, if a sample-point falls in the regions A (Q[) and A (0'[) 
corresponding to two values of 01? d[ and d[, it will fall .in the region A (0'i')> where 0'i is any value 
between Q[ and d"x. For we have 
and hence ^0 < d[ < 0j < ux 
if 0'i is the greater, and hence 
u0 < 61 <81 < 0X <ux 
or u0 <-0i" < %!• 
Further, if a sample-point falls in any of the regions A (0X) for the range of 0-values 
d'x < 0i < 0J, it must also fall within A (6[) and A (0'i). 
19.23. The conditions referred to in the two previous sections are necessary. We 
now prove that they are sufficient, that is to say : if for each value of 61 there is defined 
in the sample-space W a region A such that 
(1) P{E s A(0O|0 whatever the value of the 0's ; 
(2) For any E there is at least one 0l3 say 6'l9 such that E s A (0|) ; 
(3) If E s A (0j) and E s A (0j), then E s A (07) f°r any #i" between 0| and 0'j ; 
(4) If i£ 6 ^4 (0X) for any 0X satisfying 0j < 0X < 0j, E s A (#i) and E s A (()'[) ; 
then u0 and %, viz. confidence limits for 0, are given by taking the lower and upper bounds 
of values of 0X for which a fixed sample-point falls within A (0X). They are determinate 
and single-valued for all E, and P{ 0i < u>i | 0i} = a for all 0X. 
The lower and upper bounds exist in virtue of condition (2), and the lower is not greater 
than the upper. We have then merely to show that P {uQ < 0X < ut | 0X] = a, and for 
this it is sufficient, in virtue of condition (1), to show that 
P{u0 <91 <u1\d1] ==P{E sA (0X) 10!}. . . . (19.31) 
We already know that if E s A (#x) then u0 < ()l < ux ; and our result will be established 
if we demonstrate the converse. 
Suppose it is not true that when u0 < 0X < ul9 E s A (0J. Let E' be a point outside 
A (0i) for which u0 < dx < ux. Then must either u0 = 0X or u1 = 0X or both ; for 
otherwise u0 and ul being the bounds of the values of 6± for which E lies in A (0X), there would 
exist values 0i and 0'j, such that E s A (d\) and E e A (0'/) and 
^o < 0i < 0i < 0'i < ^i, 
so that, from condition (3), E s A (0X) which is contrary to assumption. 
Thus ^o = 0i or %! = 0X or both. If both, then E must fall in A (0|), for ^0 a^d % 
are the bounds of 0-values for which this is so, and if they coincide their common value 
must be so. Finally, if u0 = 0X < ux (and similarly if u0 < 0X = u±) we see that for 
^o < 0i < uu E must fall in A (0J from condition (3), and hence, from condition (4), E 
must fall in A (0j) and A (0'/) where 0j = w0 and Q'[ = ux. Hence it falls in A (0X). 
19.24. The foregoing theorem gives us a formal solution of the problem of finding 
confidence intervals in the general case, but it does not provide a method of finding the 
STUDENTISATION 79 
intervals in particular instances. In practice we have three lines of approach : (1) to use 
sufficient estimators, (2) to adopt the process known as " studentisation," and (3) to 
" guess " a set of intervals in the light of general knowledge and experience and to verify 
that they do or do not satisfy the required conditions. 
19.25. Consider the use of sufficient estimators in the general case. If tx is sufficient 
for dt we have 
L = ii (#!, Q±) L2 (#! . . . xn, d2 . . . 0Z). . . . (19.32) 
The locus t± = constant determines a series of hypersurfaces in the sample-space W. If 
we regard these hypersurfaces as determining regions in W7 then t± < Jc, say, determines 
a fixed region K. The probability that E falls in K is then clearly dependent only on 
tx and 6±. By appropriate choice of h we can determine K so that 
P{E s #10!} = a, 
and hence set up regions of acceptance based on values of tt. We can do so, moreover, 
in an infinity of ways, according to the values selected for a0 and ax. 
Studentisation 
19.26. In Example 19.1 we considered a simplified problem of estimating the mean 
in samples from a normal population with unit variance. Suppose now that we require 
to determine confidence limits for the mean /a in samples from 
dF = exp < I ——— > ax. 
The approach of Example 19.1 would lead us to the conclusion that, for confidence coefficient 
0-9545 and central intervals, 
P< x r- <, ii <-, x ~—y- \ a, a >== 0*9545. 
{ \/n ' <>/n J 
But we cannot now say that the confidence limits are x ± 2a/-\/n because a is unknown. 
Consider then the distribution of z = — :, where s2 is the sample variance. This 
is known to be the "' Student " form 
k dz 
dF - 
(i + z*y 
(Cf. Example 10.6, vol. I, p. 239.) Given a, we can now find z0 and zl9 such that 
" (IF = 
*/ ' ~jo */ c o 
9 j 
and hence 
P { — zx < z < s0} = a, 
which is equivalent to 
Hence we may say that //, lies in the range x — sz0 to x + 521 with confidence coefficient 
a, the range now being independent of either ju or a. In fact, owing to the symmetry of 
" Student's " distribution, z0 = zlt but this is an accidental circumstance peculiar to the 
present case. 
80 CONFIDENCE INTERVALS 
19.27. The possibility of finding confidence intervals in this case arose from our 
being able to find a statistic z, depending only on the parameter under estimate, whose 
distribution did not contain a. A scale parameter can often be eliminated in this way, 
although the resulting distributions are not always easy to handle. If, for instance, we 
have a statistic t which is of degree p in the variables, then t/sv is of degree zero, and its 
distribution must be independent of the scale parameter. When a statistic is reduced 
to independence of the scale in this way it is said to be " studentised," after " Student " 
(W. S. Gosset), who was the first to perceive the significance of the process. 
19.28. It is interesting to consider the relation between the studentised mean- 
statistic and confidence zones based on sufficient estimators in the normal case. The 
distribution of means and variances in normal samples is 
and x, s are jointly sufficient for p, o*. In the sample space W the regions of constant x 
are hyperplanes and those of constant s are hyperspheres. If we fix x and s the sample- 
point E lies on a hypersphere of (n — 2) dimensions. Choose an area on this hypersphere 
of content a. Then the acceptance region will be obtained by combining all such areas 
for all x and s. 
One such region is seen to be the sc slice " of the sample-space obtained by rotating 
the hyperplane passing through the origin and the point (1, 1 ... 1) through an angle 
jroc (not 2tzcx. because a half-turn of the plane covers the whole space). 
The situation is illustrated for n = 2 in Fig. 19.5. 
Fig. 19.5. 
For any given jir the axis of rotation meets the hyperplane /a. = /uf in the point 
xx = x2 = [i\ and the hypercones = constant in the W space become the plane 
STUDENTISATION 
81 
areas between two straight lines (shaded in the figure). These may be regarded as regions 
of acceptance, and one set is that obtained by rotating a plane about the line X^ X% ~~~" /^ 
7Z0L 
through an angle so as to cut off in any plane p, = \if an angle — on each side of 
The boundary planes are given by 
Jb o ~"~*"" Mf 
X 
X 
™ -ft 
4 2 
x — fji = (#2 — ju) tan f 
x - fi = fa — fi) tan (^ + | V 
where /? = n(l —■ a); or, after a little reduction. 
jil 
// 
•^l i^ "^a "^l — **■' 
2 
_ 
*—J >. 
2 
*Av i 
«/0 «> 
cot i- 
2 
P 
cot 
/* then lies in the region of acceptance if 
xx -j- x% 
iy /y 
^1 <*'2 
1 cot I <- a *- Xl + X* A- I Xl ~ 
- con ^ /* ^ 2 i- -~ 
X{ 
cot -. 
2 
These are in fact the limits given by iC Student's " distribution for n = 2, since the sample 
variance then becomes 
iy ___ /y 
2 
\ C 
7Z J » 1 
ofe 
** 2 
and 
Id-tan-i*.)-!-^ 
so that 
tan [ - 
2 
2 
2 
B 
cot --. 
2 
19.29. Tables or diagrams of the confidence intervals for selected values of a have 
been given for the following parameters :— 
(a) the proportion w in the binomial (Clopper and Pearson, 1934) ; 
(6) the parameter of the Poisson distribution (Garwood, 1936 ; Bicker, 1937) ; 
(c) the correlation coefficient in normal samples (David, 1938a) ; 
(d) the median in samples from any population (K. R. Nair, 19406). 
In addition, results for the mean of a normal population may be obtained from " Student's " 
integral as shown above. Those for the variance of a normal population may be obtained 
from the P-function or the equivalent #2-integral. For simultaneous estimation of mean 
and variance there are difficulties, as we proceed to show. 
19.30. It might have been expected that the foregoing theory could be generalised 
to give simultaneous pairs of confidence intervals for two unknown parameters when 
intervals for each separately cannot be found. Very little progress in this direction has, 
however, been made. The difficulty may be illustrated by reference to the joint distri- 
A.S.—VOL. II. 
G 
82 CONFIDENCE INTERVALS 
butioii of mean and variance (19.33). From the independent distributions of x — p, and 
&■ : ■' ' ' '■ 
- we can, given a, /?, find t0> tt and u0, u± such that 
a 
\ 
k < -—£ < tQ y - a 
P\u*<8-<ux\=p 
where the fs and u9& depend only on sample values and a, /? may be chosen at will. The 
inequalities are equivalent to 
x — at0 < fc < x + atx . . . . (19.34) 
and these give 
But can we then infer that 
■ ± <<r <_£. (19.35) 
x-hs<fil<x + hs (19.36) 
pJx-~^s<a<x+^s\=y, . . .(19.37) 
[ u0 ux J 
where y is a constant dependent on a and /? ? We cannot. This equation is, in fact,, 
not generally true. The fact can be verified by considering the distribution of the statistic 
x — ks and showing that its distribution function F (u) is not independent of /u and a. 
19.31. In the next chapter we shall see that a similar problem, giving rise to Behrens' 
test, provides a crucial point of difference between the theory of confidence intervals and 
that of fiducial intervals. All we need say here is that from the point of view of the former 
the problem of simultaneous confidence intervals for several parameters remains unsolved, 
except of course in the degenerate case when we can find independent intervals for each 
parameter separately. 
19.32. In conclusion we indicate without proof a few results which have recently 
been obtained. 
(1) Wilks and Daly (19396) have generalised the theorem of 19.12 to the case of several 
parameters. Under fairly general conditions the confidence regions which are shortest 
on the average are given by 
where (af,-) is the inverse matrix to that whose general element is 
w( 8 log/ d log/ 
B[-wt—w 
and xl is su°h that P (x2 < %l) = <*> ^he probability being calculated from the 
^-distribution with v = I. This is clearly related to the result of 17.46 giving the limiting forms 
of variances and covariances of niaximuni. likelihood estimators. 
(2) Wald (1942) has considered the problem of large samples from the point of view 
of most selective sets (" shortest " in Neyman's sense) and has proved results somewhat 
similar to those of Wilks and Daly. 
NOTES AND REFERENCES—EXERCISES 83 
(3) Wald and Wolfowitz (19396, 1941c) and Kolmogoroff (1941) have considered the 
problem of setting confidence limits to the terminals of an unknown frequency-distribution. 
NOTES AND REFERENCES 
When the theory of confidence intervals and that of fiducial intervals were first 
developed many statisticians regarded them as equivalent. In papers written between 1930 
and 1938 " confidence limits " and cc fiducial limits " are often used in the same sense ; 
and even where a distinction of approach was drawn the results given by the two methods 
appeared identical. The case of Behrens' test, however, provided an illustration where 
the methods lead to different results—see the following chapter. 
The fiducial approach is due to R. A. Fisher, references being given at the end of 
Chapter 20. The approach of the present chapter has been developed mainly by Neyman 
(see particularly 19376), E. S. Pearson, Wilks (19386, c, 1939a and—with Daly—19396), 
Wald (1939a, 1942), Welch (1939a), and Bartlett (1936a, 1939a). A number of the references 
to Chapters 26 and 27 are also relevant. 
Confidence intervals can be obtained for the median and other quantiles which are 
independent of the form of distribution. See Thompson (1936), Savur (1937a) and K. R. 
Nair (19406), and compare Exercise 19.5. 
EXERCISES 
19.1. Show that for the rectangular population 
dF = y, o < x < e 
and confidence coefficient oc, confidence limits for 0 are t and t/ip where t is the sample range 
and ip is given by 
yi-i | n _ [n — 1)^| = 1— a. 
(Wilks, 1938c.) 
19.2. Show that, for the distribution of the previous exercise, confidence limits 
for samples of two, xx and #2, are 
1 +"V(l'- oc)' 1 -V(l - «V 
(Neyman, 19376.) 
19.3. Show also, in the case of the previous exercises, that if L is the larger of a 
sample of two, confidence limits are 
j- JU 
(Neyman, 19376.) 
Show further that if M is the largest of samples of four, confidence limits are 
M _ zr 
(V-a)*" 
(For an experimental verification, see Frankel and Kullback, 1940.) 
84 • CONFIDENCE INTERVALS 
19.4. Show that, for the distribution 
dF = 0 e~xd dx, 0 < x < oo 
central confidence limits for large samples with oc = 0-95 are given by 
i . 1'96 
* ± 7— 
d = V^ 
(Wilks, 1938c.) 
19.5. If a frequency function is continuous, the probability that the kth of a sample 
of n (arranged in ascending order of magnitude) lies in the range dx is 
J?(^~Hi) 
where F is the distribution function. Deduce that 
P {afc < M <xn__k+1} = 1 — 2I0.5.(» -Hi, &), 
where ikf is the median, and hence show how to determine confidence intervals for M from 
the incomplete jB-function. 
Generalise the result for quantiles. Show that the results do not hold for 
discontinuous distributions. 
(Thompson, 1936.) 
CHAPTER 20 
FIDUCIAL INFERENCE 
20.1. We now proceed to examine a type of inference known as fiducial. As in 
other methods of estimation, given a distribution of known form depending on an unknown 
parameter 6, we shall attempt to find limits between which 6 lies in some sense associated 
with the theory of probability. To that extent our present approach is similar to the 
use of estimators with their associated sampling error and to the use of confidence intervals ; 
but it is distinct from the latter both in essential ideas and in some of the results to which 
it leads. 
20.2. Consider samples of n from a normal population of unknown mean ju and 
unit variance. The sample-mean x is sufficient for /u and its distribution is 
dF = / — exp < — - (x — /i)2 I dx. . . . (20.1) 
In speaking of a distribution in this sense we regard p as fixed and consider the totality 
of values of x derived by random sampling from the population with given /lc. The 
proportion of samples falling in a range dx is then given by (20.1), which holds for each 
value of fi. 
We now change our viewpoint and consider a different kind of distribution based on 
(20.1). If we are given a value of x from a sample, what are the values of fi which could 
have given rise to this value to any fixed level of probability ? If the deviation x — ji is 
written as h, we know that the probability of the inequality 
x — /i < fe . . . . . (20.2) 
being true is a, where oc depends on h and is in fact 
I 
h I n / nx2 
exp ( — —- ) dx. . . ... (20.3) 
Looking at this the other way round, we may say that given any a we can find h, a function 
of oc only, such that 
fi > x — h . . . . . (20.4) 
is true with probability a. For any fixed x this gives us a distribution of ^. Consider 
in fact the equation 
p, = x —- h. . . . . . . (20.5) 
If p has a distribution function F (/i), we have, since (20.4) is true with probability oc, 
whence 
f{ji)dn = - A/- exp f - — )dh. 
But in virtue of (20.5), dfi — — dh and h = p, — x. Thus 
f{f,)dF = J-^-ex^f-^—^y/x. . . . (20.6) 
This is called the fiducial distribution of /u. 
85 
86 FIDUCIAL INFERENCE 
20.3. It so happens that in this example the non-differential parts of (20.6) and 
(20.1) are the same. This is not essential although it is not infrequent. The crucial 
point of difference, however, lies in the appearance of the differential element dp, relating 
to the variation of /i, and the disappearance of dx relating to the variation of x. We have 
derived a distribution of the parameter ^ from that of the random variable x by 
transferring our attention in (20.4) from x to \i and regarding the inequality as still satisfied 
with probability a. 
20.4. We note in the first place that this distribution is not necessarily existent. 
When we come to make an inference in any particular case we do not assume that \x is 
itself distributed in the fiducial form in the sense that it has been chosen at random from 
an existent population of /-t's of that form. Such a prior distribution, which would be 
required for the application of Bayes' theorem, is not admissible from the point of view 
of the frequency theory of probability. The fiducial distribution is a hypothetical one of 
conceivable values of //. We attach probabilities to these values, or rather to values in the 
range dp, by identifying them with the probabilities (based on frequency) which are derived 
from the distribution of a sufficient estimator of fx. For this reason the fiducial distribution 
is not a frequency-distribution in the ordinary sense ; but it is a probability distribution 
in its own special sense. We use it to make statements of the kind : among the values 
of \i which are possible, only those in a certain range give rise to the observed x with 
probability a, and hence we will locate \i in that range. 
20.5. In our present example the argument would proceed as follows. From equation 
(20.6) and the use of the normal integral, the probability that /i — x does not exceed a 
certain h is ascertainable as a function of h ; for instance, 
P\ u — x < -7- >- = 0-9775. 
If we regard a probability as high as this as acceptable, we may say that fi < x + 2/V^- 
This result is equivalent to that given by the theory of confidence intervals, for if 
we assert /,/, < x + 2/s/n we shall be right in the long run in 97-75 per cent, of the cases. This 
identity of result is found in most elementary cases where a single parameter is concerned, 
but is to be regarded as accidental. In the theory of confidence intervals it is fundamental 
(a) that the assertion as to the parameter lying in a given range should be true in an assigned 
proportion a of the cases, and (b) that no assumption need be made as to the prior 
distribution of the parameter, either in the frequency sense or in the fiducial sense. In fiducial 
theory it is not necessary that (a) should be true, but the fiducial distribution is 
a fundamental part of the inference. 
20.6. There is a further distinction between the two theories. In that of confidence 
intervals it is possible to have two entirely different sets for the same parameter, and in 
fact part of that theory is devoted to finding " best " sets among the possible ones. In 
fiducial theory such a state of affairs must not be possible, for different limits would imply 
different fiducial distributions for the same parameter on the same evidence. This is avoided 
by confining fiducial distributions to those based on sufficient estimators, or more generally 
on a set of estimators which together avoid all loss of information. Since such estimators 
alone contain all the information relevant to the problem of estimation they alone can 
give the fiducial distributions accurately. It follows, of course, that where no sufficient 
FIDUCIAL DISTRIBUTIONS 8.7. 
estimator—or estimator with complete set of ancillary estimators—caji.be found, the 
fiducial method is inapplicable. - 
r' ■ ' , ' 
20.7. Generally, let F (6, t) be the distribution function of a sufficient estimator i 
for a parameter d. Then for the frequency distribution of t we have 
dF = a^A) dt' (20.7) 
F (t, 0) is the probability that a random value of the estimator does not exceed a given 
value t. In accordance with the fiducial principle, this may be equated to the probability 
that for fixed t the value of 6 will exceed t, so that for the fiducial distribution of 6 we have 
dF --Li1 -F&Q) )dQ 
= _ W) dd. ... . . (20.8) 
dd 
This shows the general relation between the frequency-distribution of the estimator and 
the fiducial distribution of the parameter. 
Example 20.1 
If p is known, the estimator 6 = - is sufficient for 6 in samples from 
dF = - -- — dx. 0 <# <'oo 
0'pT(p) 
the distribution of 6 being, in fact, 
. dF-\T) r(nP)e^{--T)df)- 
(Of. Example 17,8.) We may write this in the form 
/ npd\ 
dF^f^X^^^ . . • (20.9) 
It is then clear that, since 
_ dF ___ _ dF dt_ 
~~ JQ ~~~ ~dt "90' 
the corresponding fiducial distribution of d is 
/ npO\ 
^ = (-f) r'inp) W*V ■ ■ ■ (20-10) 
which may also be put in the form (20.9), provided that we interpret the differential element 
~ dO 
now as relating to 6 and not to B. It will be noticed that we have replaced dd by 0 -=-, 
not merely by dd. 
From the fiducial distribution (20.10) we can find the probability that 6 lies in a certain 
range dependent on the observed 6 and the chosen probability oc. This is in fact the same 
range that we should obtain by applying confidence intervals to (20,9). Once again the 
results of the two methods are the same. 
88 FIDUCIAL INFERENCE 
Fiducial Inference based on " Student's " Distribution 
20,8. Consider now the estimation of the mean \i in samples from a normal 
population with unknown variance a2. The treatment of 20.2 is no longer of use, for it would 
result in a fiducial distribution of \i containing the unknown a. We therefore " studentise " 
the problem by considering the distribution of 
. t = {£ ~~ ^ V% ...... (20.11) 
s' 
which is independent of a*, being in fact 
dF oc ^tttxiv (20-12) 
where v — n —- 1. Here s'2 is the unbiassed estimate of the sample variance 
1 
n ■ 
The distribution of t may be written 
S (x — x)%. 
d J ^^ ^n 
dF a 7 ~~(^T^jl^y* (20-13) 
1 —1~ 
s'2(^ - 1) 
The fiducial distribution is then 
dfi 
(fi — x)2 n 
dF oc r -z£——^r. .... (20.14) 
1 + 
s'2(n - 1) 
In the usual way we can find two constants, for any given oc, such that,.from (20.14), 
P {/u0 </^ </^1}= a, ..... (20.15) 
the probability being based on (20.14) and therefore to be understood in the fiducial sense. 
Had we worked with (20.12) or (20.13) we should have found tl9 t0 such that 
which is equivalent to 
p\z-^ <n <x+^\=ol. . . .(20.16) 
{ Vn Vn J 
This may be interpreted in the sense of confidence intervals, i.e. that in asserting the 
inequality in (20.16) we should be right in a proportion a of the cases in the long 
run. (20.15) does not rest on this statement as to frequency, though tha limits to which 
it leads are the same and the statement happens to be true. 
20.9. The case we have just discussed raises a new point. Is it still true that 
the fiducial distribution is unique, and is it consistent with the distributions of ft and o 
separately % The distribution is based only on the sufficient estimators x and sf (which 
are jointly but not separately sufficient for /u and a) and we should expect this to be so. 
But the matter requires investigation, for we are here using a fiducial distribution based on 
two estimators. 
FIDUCIAL INFERENCE BASED ON "STUDENT'S" DISTRIBUTION 89 
The simultaneous distribution of x and s' is 
dF 'i -»{- £«• - ">■} - (sr^ {- ^^i t- • (2o-n> 
If we were considering fiducial limits for p with known a we should use the distribution 
dF oc - exp -i — —- (x —- /^)2 > dS. 
If we were considering fiducial limits for <r with known p we should not use the other factor 
in (20.17), 
for in such circumstances $' is not sufficient for ex, the appropriate estimator being 
1 
- £ (x — //)2. The question is, what form of fiducial distribution must hold for a in order 
lb 
that the ct Student " form (20.14) should hold for p when a is unknown ? 
Suppose the fiducial distribution is / (s'9 a) do. We have then for the joint fiducial 
distribution of /i and o, 
1 f n 1 
djP1 oc —exp < — —- (# — /u)z > d/Ltf(s\ a) do. 
We have therefore to solve 
HoIexp {-£* -*Af(s'a)da\d" = 77, J^W* * • (20J9) 
«'*(tt- 1) 
where & is some constant. Putting (^ — x)2 = a, — — == /?, we have then to solve 
I 
DO 
n\dfi__ k 
e*P fl sf, — 
0 \ \l %P J P f 710C if 
(n - l)*7* j 
n 
J 
w 
is the frequency 
n 
not 12 
function whose characteristic function is l/<j 1 + —-— ^ y9 which gives 
sj~^) a/»ln"1«P 
from which we find 
xt t \ l f (^ — l)s' 
/ (5 , ct) oc ~- exp J 
(»-i>Cn> 
or, on evaluation of the constant, 
This, then, is the fiducial distribution which o must obey. We should have arrived at 
90 FIDUCIAL -. INFERENCE ■ 
the same result had we taken (20.18) and transformed it to the fiducial form, as if it related 
to s' and a only and the former were sufficient for the latter. 
It' appears, then, that in this case at least the fiducial method gives consistent results 
when two parameters are involved. The general problem of many parameters presents 
difficulties and has not been elucidated to any great extent. 
The Logic of Fiducial Inference 
20.10. The notion of fiducial probability was introduced by Fisher (1930) for the 
case of a single parameter. Regarding the estimate t as fixed, Fisher considers the 
distribution of values of 6 for which t can be regarded as a representative 
estimate—representative,, that is to say, in the sense that it could have arisen by random sampling from the 
population specified by 6. As pointed out above, this does not mean that we are regarding 
the true value of 6 as a member of an existing population. Rather, we are considering the 
possible values of 6 and attaching to each value a measure of our confidence in it, based 
on the probability that it could have given rise to the observed t. 
If I interpret him correctly, Fisher would regard a fiducial distribution as a frequency- 
distribution. This implies that 6 is regarded as a random variable. It appears to me, 
however, that it is not a random variable in the ordinary sense of the frequency theory 
of probability, in which values of 6 either are or can be generated by an actual sampling 
process. We can never test whether the fiducial distribution holds in the frequency sense 
by drawing a number of values and comparing observation with theory. Nor, in 
calculating fiducial limits of the type 6 = t + h (a), do we imply that the proportion of cases 
for which 6 < t + h is true will be a in the long run. 
20.11. The reader has a choice of several attitudes towards the foundations of the 
fiducial argument: (a) he can accept the argument as involving a new postulate of 
inference ; (b) he can regard it as sanctioned by the approach of the previous section ; or (c) he 
can, so far as estimates based on a single parameter are concerned, console himself with 
the thought that the results of the process are the same as those given by the theory of 
confidence intervals. 
20.12. Although Fisher is careful to emphasise the distinction between his own 
approach and that based on Rayes' postulate, it is interesting to note that the theory of 
inverse probability as modified by Jeffreys gives results which are in many cases identical 
with those of fiducial inference. 
In the example of 20.2, for instance, suppose that the prior distribution of ju is/ (/*) d/x. 
Then for any given x the posterior probability of fz is 
dF =f(p)dp J~exp j -|(£ -aO2}- • • • (20.21) 
If the total probability is unity we have 
fw v^6^!™!^"^2}^^1 (20-22) 
Clearly / (ju) == 1 is a solution, and we may use characteristic functions to show that it is 
the only solution. In fact we have from (20.22), writing it for nx— 
i 
i 
/ (?) exp ( - nJ^ ) e* dp 
.00 
BEHRENS' TEST . 91 
The expression on the right is the characteristic function of exp ( — J~~ V" and hence 
/(/*) exp C-^yJ =exp (-^p 
or/(/i) = l. 
We have, then, for the posterior probability distribution of p, 
dF = J^~ exp.'J — |(i" ~ %)2\d/ji9 . . . (20.23) 
which is the same as the fiducial distribution. The requirement that/ {pi) = 1 is equivalent 
to a prior distribution of pi, dF = dpi, which is the form given by Bayes' postulate for a 
parameter which can extend to infinity in either direction. 
Example 20.2 
In Example 20.1, a similar argument leads to a prior distribution of 0, 
dF oc —. 
0 
This is the form given by Jeffreys' modification of Bayes' postulate when a parameter 
can extend to infinity in only one direction. 
It does not appear, however, that fiducial and inverse probability always give the 
same results. Consider the distribution of the correlation coefficient in normal samples 
(14.14)— 
dF oc (1 - p2p~" (1 - y«)T- .-J-~. 0 i u"y---. { £.> L dr. . (20.24) 
v H } v ; d{rp)n~~2 \ V(l ~p2r2)J v ; 
The argument of the type we have just employed would require a prior distribution of p— 
dF oc dp -, 
(1 - p-)» 
and the resulting posterior distribution (which is equivalent to that obtained by 
interchanging r and p in (20.24)) is not the same as we should get by using equation (20.8). 
Behrens* Test 
20.13. Suppose we have two samples of nx and n« members from normal populations 
with possibly unequal variances. The fiducial distributions of ptr and pc2 are of the 
" Student " form (20.14). Writing 
fi± = xx + S[ Ux 
pi2 = #2 ~r $2 u2 
we have 
/*i — ^2 = %i — ^2 + «$i ^i — ^2 ^a- • • • • (20.25) 
If now 
s depends only on the known quantities x and s' and the difference of means pix ~- pc%. 
From the fiducial distributions of ptx and //2 we can find that of s, and hence make fiducial 
statements of the type 
#i ~ x* — Go V(*i2 + 522) < i^i — /*2 < #i — ^2 + *i V(^i2 + 42)- • (20.27) 
92 FIDUCIAL INFERENCE 
20.14. The distribution of s is not of a simple form. Putting tan ip = A we see that 
a = ?LZJ^ cos y - ^-"i*2 sin v, ... (20.28> 
$1 52 
so that e is distributed fiducially as the weighted difference of two variables, each of which 
is distributed as " Student's " t. We have then to find the distribution of 
s = tx cos xp — t2 sin ip 
where the joint distribution of tx and t2 is given by 
dF oc d\ s. 7 *\ x. • • • • (20.29> 
1+—L) fl+~'2 * 
The distribution has been studied by Sukhatme (19386) and in more detail by Fisher 
(1941a). Tables are given for various values of nXi n2 and the ratio s'^/s'^ (or the 
equivalent angle ip) showing the values of a corresponding to given probability levels. Some of 
the tables are included in the second (1943) edition of Fisher and Yates' Statistical Tables for 
Agricultural, Biological and Medical Research. 
20.15. The joint distribution of s'x* and s^ is 
f '2 ' 2 ^ 
dF oc s^-* 4*»-3 exp \ - i (nx - 1) % ■ - \ (n2 - 1) \ I d^2 c?42. 
L °T °2 J 
o 2 j Q o 2 
Putting ^ = J- and ^ = \ \ (nx — 1) -~ + (w2 — 1) 
we find, on a little reduction, 
V " I *1 *2 
dF oc -= 1 "^L.^^^ w*(ni+«,-4) e-« dw. 
jj (71! - 1) 7l2 — l)t(Wt + na-2) 
(20.30) 
9 i 9 
Thus u is distributed (independently of p) in the Type III form. Further,. 
(xx — fjbt) — (x2 — ^a) is distributed normally about zero mean with variance erf + ^l- 
Hence, if -| = d, we find that the quotient 
{{xx — fzx) — (xa — ^2)}2 (?&! + 7ia — 2) = £2 (1 + p) (nx + ^2 - 2) _ (20 31V 
(of + *f) | ^JM2 + ^"^2 j |(na _ i) + (Wl _ i)|| (i + fl) 
is distributed as £2 with nx ■+ n2 — 2 degrees of freedom. (Cf. Example 10.17, vol. I, 
p. 248, for the distribution of a normal variate divided by a Type III variate.) 
Now if we knew 6 we could find fiducial (or confidence) limits to e, and hence to fjtx — /u2r 
in the usual way, for the distribution of s would then be independent of unknown constants 
and ascertainable from " Student's " integral. Since, however, B is not known, we require 
in turn the fiducial distribution of this quantity. Since 
z = i i0ff (n*si* In* S22S 
is distributed in Fisher's form (cf. Example 10.18, vol. I, p. 249), the required fiducial 
BEHRENS' TEST 93 
form for 6 can be obtained from that of z, which incidentally is equivalent to that of p 
in (20.30). If we express (20.31) as the joint fiducial distribution of s and d and integrate 
out for 6, we shall be left with an equivalent form to that derived from (20.29). 
20.16. It also follows from the above that the inequality (20.27) is not satisfied in 
proportion a of the cases independently of d, so that the limits to pc1 — \i% are not confidence 
limits, although they are fiducial limits. It will, in fact, be evident enough from (20.31) 
that if we determine t0 and tt so that the integral of " Student's " form between those 
limits is a, then the corresponding limits for s, say e0 and sl9 are dependent on the variance 
ratio 6 = a\/a\. This is fairly evident on general grounds, and the point has been put 
beyond doubt by both Fisher (19376) and Neyman (1941a), who have worked out particular 
cases of difference. 
The fiducial distribution of e (which is an extension by Fisher of a result given by 
Behrens as early as 1929) thus provides a crucial point of difference between the theory of 
fiducial inference and that of confidence intervals. 
20.17. In conclusion, we will indicate the viewpoint of Jeffreys towards the type of 
problem dealt with by " Student's " distribution for limits to the mean and Behrens' 
distribution for limits to the difference of two means. 
If H denotes the general data, we have for the " Student " distribution— 
P{dt\fi,a,H} = -, *fV(,+ 1) • ... (20.32) 
JL |" — 
V 
The expression on the left states the probability that t will lie in a given range dt on the 
assumption that H is true, the parent mean being /x and the parent variance a2. Since 
ft and a do not appear on the right they are irrelevant and may be suppressed, and hence 
P{dt\H}=; --Stt-th (20-33) 
I j[ —|— — j 
\ v I 
Suppose now that we assume that 
P {dt | x, s, H} = / (t) dt .... (20.34) 
Then, as before, x and s may be suppressed and we have 
P{dt\H} =f(t)dt, (20.35) 
and hence, by comparison with (20.33), 
P{dt\x,s,H} = - y^r,- • • • (20.36) 
J_ —J— ..... 
V 
We can then proceed to find limits to t, given x and s, in the usual way. Jeffreys 
emphasises, however, that this depends on a new postulate expressed by (20.34) which, though 
natural, is not trivial. It amounts to an assumption that if we are comparing different 
distributions, samples from which give different x's and s's, the scale of the distribution 
of ji must be taken proportional to s and its mean displaced by the difference of sample 
means. 
i7~fc 
FIDUCIAL INFERENCE 
20.18. In a similar way it will be found that to arrive at the Behrens distribution 
it is necessary to postulate that 
P {dtu dt2 | xl3 x2, s'v «;, H} = f± (h)f2 (t2) dh dt2 . . . (20.37) 
Jeffreys5 derivation of the Behrens' form from Bayes' theorem would be as follows :— 
The prior probability of dfi-i dfi2 dax da2 \H is 
-r\ ( i 7 ^ T I nl CtLli UtU,2 (IG\ Qj02 
P {d(jt1dfi2da1da2\ H} oc — r 
The likehhood (denoting the data by D) is 
0*1 #2 
P{D | /tl9 /x2, au <r2, H) oc ——- exp 
or 
{(y"i - ^O2 + 4} 
n2 
2a\ 
{(/i2 ~ »a)2 + 4} 
Hence, by Bayes' theorem 
P { dfix dju2 dax da2 \ DH) = 
1 
0?i+1 <7?2+ 
ieXP 
n 
•«r* {(i"i -^)2 + 4} 
24 
{ {[H ~ ^2)2 + 4} 
d/iii rf//2 rfcrx rfo*2. 
Integrating out the values of c^ and a2, we find for the posterior distribution of//! and /*a 
a form which is easily reducible to (20.29). 
20.19. To sum up : so far as concerns problems of estimation the Behrens test is 
accurate both in fiducial theory and in the theory of probability propounded by Jeffreys. 
But the test does not hold in the theory of confidence intervals. In fact the latter fails 
to provide an exact solution to the problem, though we shall see below (21.28) that 
approximations are possible. Fisher has criticised confidence intervals on the ground that they 
do not give an answer to what is admittedly an important question ; but it appears possible 
to maintain consistently that some questions may not have an answer. 
NOTES AND REFERENCES 
For the general theory of fiducial inference see Fisher (1930a, 1933, 1935a, 6, 1936c, 
1941a). The difficulties of reconciling Behrens' test with confidence-interval theory were 
noticed by Bartlett (1936a) and led to some controversy, for which see Fisher (19376, 
1939a, 1940c), Bartlett (1939a), Yates (1939/), and Neyman (1941a). For Jeffreys' views 
see his papers of 19376, 1938c, 1939^ and 1940. 
For the practical application of Behrens5 distribution see Sukhatme (19386) and Fisher 
(1941a). Behrens himself stated his results explicitly only for the case of equality of sample 
number, nx = n%9 the extension being given by Fisher (19356). 
EXERCISES 
x is the mean of a sample of n values from 
1 f (x -— /t)2 
dF 
oV(2tt) 
exp 
2a 
dx, 
s'2 is equal to 27 (x — f)2, and x is a further independent sample value, show that 
lb JL 
EXERCISES 95 
is distributed in " Student's" form with v = n — 1. Hence show that fiducial limits 
for x are 
fn + 1 
x ± s% 
n 
where tx is chosen so that the integral of " Student's " form between — t± and tx is an 
assigned probability a. 
(Fisher, 19356. This gives an estimate of the next value when n values have 
already been chosen, and extends the idea of fiducial limits from parameters 
to variates dependent on them.) 
20.2. Show similarly that if a sample of nx values gives mean xx and estimated variance 
sx2, the fiducial distribution of mean xz and estimated variance s'%2 in a second sample of n2 is 
in sini~~ Szn*~~ dx2 ds2 
/ n, n* 1 *(wi+wa-i)* 
{7li _ i) 5;2 + (na _ !) 42 + {£i _ ^2 /M. I 
Hence, allowing n2 to tend to infinity, derive the simultaneous fiducial distribution of 
ju and a. 
(Fisher, 19356.) 
m 
CHAPTER 21 
SOME COMMON TESTS OF SIGNIFICANCE 
Tests of Significance 
21.1. We now pass from the problem of estimation to that of significance. The 
two are closely allied and in practical problems they both arise together as a rule ; but 
it is useful to preserve a distinction between them. In estimation we try to find, with 
greater or less accuracy, the value of some parameter in a population which is known to 
be (or assumed to be) dependent on that parameter. In tests of significance we are given 
some value of a parameter beforehand and wish to decide whether it is acceptable in the 
light of the evidence. This is the distinction in its simplest terms, but of course the 
-associated problems become increasingly complex when several parameters are concerned. 
21.2. From one point of view the problem of significance is logically anterior to that 
of estimation. Suppose we have records of the yields of two varieties of wheat grown 
under similar conditions, and are interested in a comparison of the average yields of the 
two. Our first question is whether the observed mean yields indicate any difference between 
the varieties—a matter of significance. Not until significant differences are established 
does our interest turn to the magnitude of the difference—a matter of estimation. Again, 
if we have a set of records of only one variety, our primary problem may be to decide 
whether they are consonant with the hypothesis of normality in the parent population, 
whatever its mean and variance ; and only when this point has been settled affirmatively 
do we proceed to estimate those parameters. 
Nevertheless, we have lost very little by taking the problem of estimation first. In 
some practical problems the question of significance is already decided, and in many others 
we use estimates of parameters to test the significance of the latter, in which case estimation 
and significance become different aspects of the same statistical fact. 
21.3. We shall consider the general theory of testing statistical hypotheses in Chapters 
26 and 27. That theory is, however, rather abstract, and we anticipate it to some extent 
in this chapter by giving an account of the principal tests in current use, without for the 
moment going too deeply into their rationale. It will be seen later that there are sometimes 
many significance tests which can be applied to the same problem, and that it is possible 
to lay down criteria for deciding which, if any, are the " best ". This aspect of the subject 
will not concern us for the present. We shall not discuss whether the tests we describe 
are the best possible (though some of them, in fact, are so) but shall merely present them 
.as useful and convenient, albeit perhaps not unique, solutions of our problems. 
21.4. Developments in statistical theory in the last two decades have resulted in 
-a great many tests of significance appropriate to special problems. It is not easy to classify 
them and quite impossible to deal extensively with them all. We shall consider them 
under the following heads :— 
(a) Tests of the significance of a specified parameter value.—The typical hypothesis 
here is that a parameter in a population of known form has a specified value (usually 
zero). We wish to know whether the evidence provided by the sample supports the 
hypothesis or not. 
96 
STANDARD ERRORS " 97 
(b) Tests of goodness of fit.—The hypothesis is that the population is of a certain 
kind which is either fully specified beforehand or can be " estimated " with the help 
of the data. We wish to know whether the sample values fit this population in the 
sense that they could have arisen from it by random sampling to any acceptable degree 
of probability. This hypothesis is more general than that of (a) since it concerns 
the whole distribution function and not merely one of its parameters. 
(c) Tests of homogeneity.—The hypothesis here concerns two or more populations, 
each providing a contribution to the sample. We wish to test whether the populations 
have certain parameters in common, or in the extreme case, whether they are identical. 
This case can be regarded as an elaboratibn of (a) where several parameters are 
simultaneously tested. In the particular case when only two populations are concerned 
we may sometimes reduce it directly to type (a) by considering differences ; e.g. if 
we are making a comparison of parent means the hypothesis might be that the single 
difference of means is zero. 
In addition we shall also consider two sets of tests of rather a different kind :— 
(d) Tests of order of occurrence.—The hypothesis here is that the sample members 
occurred in random order, and we wish to ascertain whether the observed order indicates 
any systematic effects, as, for instance, whether there are any cyclical effects in time- 
series. The test here is of the sampling process rather than of parameters of the 
parent population. 
(e) Conditional tests.—The hypothesis may be any one of the above types, but 
we restrict the inference to a sub-population for which certain qualities are 
determined by the observed sample values. For instance, we may use the distribution 
of the sample variance $2 for which the mean x is equal to the observed value. In 
short the variation of sample values is conditioned. Type (d) may from some points 
of view be regarded as a particular case of this type. 
It is not intended to convey that the above five categories are mutually exclusive. 
A test of type (a) may, for example, be conditional or non-conditional. The classification 
will, however, provide some sort of articulation for a rather long chapter and serve to 
explain our sequence of treatment. 
Standard Errors 
21.5. For large samples the test of significance of a parameter can usually be carried 
out by standard errors. We find an estimator t of the parameter 6 and consider whether 
the given value of 0 falls in the range tt i &V var t, where tx is the value of t for the observed 
sample and k is a constant chosen at will according to a probability oc. If so we may accept 
the value of 0, at least so far as this test is concerned ; if not, we reject it. 
If the variance of t does not depend on unknown quantities such as other parameters, 
this type of inference is justifiable as an application of the theory of confidence intervals. 
In accepting 0 when it falls in the range tx ± k\/YSbr t, we shall be right in proportion oc of 
the cases in the long run. As a refinement we may, of course, use non-central intervals 
and locate 0 in an asymmetrical range t} — k0 Vvar t to tx + kx Vvar t. The test of 
significance is equivalent to the estimation of the true value of 6 ; and it will clearly be better 
if the range of estimation is narrower, for then we reject more wrong values of 6. 
21.6. If the variance of the estimator t depends on unknown parameters d2 . . . dp 
we can usually substitute estimates of those parameters obtained from the sample itself, 
a.s.—vol. ii. h 
98 COMMON TESTS OF SIGNIFICANCE 
provided that the sample is large. For example, we have for normal samples 
2(7 
P (ft <x + 
0-97725. 
The sample standard deviation s will differ frora a by a quantity of order 1/V^3 so that 
to that order 
Piu < x + ~i = 0-97725. 
The approximation breaks down for small samples, and more accurate methods are required. 
21.7. The use of standard errors in testing significance has been illustrated in previous 
chapters, and we need not enlarge on the process further. We may, however, remark 
two things :— 
(a) That if the distribution of an estimator t tends to normality for large samples 
irrespective of the parent form (as, for instance, is the case with the mean and other moments 
under very general conditions), it is not necessary that the hypothesis should specify the 
parent form. In short, our test of significance is independent of the parent, a valuable 
generality which rarely obtains for small samples. 
(&) That we have justified the logic of reasoning involving the use of standard errors 
by the theory of confidence intervals (and a similar justification can be given in terms 
of fiducial intervals if we use an efficient estimator for which the loss of information tends 
to zero relative to the total information in large samples). This appears to be the most 
satisfactory basis for the use of standard errors. The usual intuitive basis advanced 
(necessarily) in introductory textbooks is not easy to defend. For instance, it is customary 
to reject a value of 6 if it gives to an observed tx or greater value a small probability ; and 
there is no obvious reason why we should base our inference on the improbability of greater 
values of tl9 namely on the improbability of something which has not occurred (see 21.55 
below). Our present approach shows that in fact the use of standard errors can be justified 
logically without invoking a new principle of inference. 
Significance of the Mean in Normal Samples 
21.8. Suppose we have a sample from a parent population which is known to be 
normal, but of whose mean and variance we are ignorant. We wish to test the significance 
of a given value fi0 of the mean, that is to say, we wish to consider whether the observations 
could, to any acceptable probability, have been derived from a population with mean //„, 
whatever the variance may be. 
We calculate the statistic 
t = *~Jl.0-<y/v, . . . . . . (21.1) 
s 
all the quantities in which are given. We know that the distribution of t is 
r 11± 1 \ 
-..__ \ jj I at 
dF = - —— ■■- — (*>] k>\ 
and hence can find the probability that our calculated value of t is attained or exceeded. 
If this is small we reject /li0; if not, we accept it. What values are regarded as " small " 
SIGNIFICANCE OF THE MEAN IN NORMAL SAMPLES 99 
for this purpose is a matter of convention, but the most frequently used values are 0*05, 
0-01 and 0-001. 
From the work of the previous two chapters it will be evident that this type of 
inference is the confidence- or fiducial-interval approach in a slightly different form. Given 
a we can find -- tx and t0 such that the integral of dF in (21.2) between those limits is oc. 
f Q f Q 
This gives us confidence or fiducial limits to a of the type x — -~ and x -\—x~- ; and if 
fi0 lies in this range we accept it. In particular cases we may have t0 = tly in which cases 
the intervals are central and our probability oc is the chance of t being attained or exceeded 
in absolute value ; or £0 = + °o> in. which case a is the chance that — tx will be attained 
or exceeded, and no lower limit to //0 is imposed. 
Example 21.1 
The weights of fifteen bags of sugar taken from a filling machine are found to be, in 
ounces, 161, 15-8, 15-8, 15-9, 16-1, 16-2, 16-0, 15-9, 16-0, 15-7, 15-7, 15-8, 16-0, 16-0, 15-8. 
Each bag should be 16 ounces, but some deviation is inevitable. One of the 
manufacturer's problems, of course, is to keep this deviation to a minimum, but that is not the 
point we now consider. Our question is : if the machine is supposed to be giving weights 
of 16 ounces on the average, does the sample suggest that it is failing in its purpose ? 
The hypothesis is that the parent mean is 16 ounces and the deviations from this 
mean are, in order of magnitude, —- 0-3 (twice), — 0-2 (four times), —- 0-1 (twice), 0-0 
(four times), 0-1 (twice), 0*2 (once). The sample mean is thus — 0-08 and to that extent 
the average of the sample is slightly underweight. Is this a significant effect ? 
It will be found that #2 = 0-0216 so that 
0-08 
t = - a/14 = - 2-04, v = 14. ■ 
V0-0216v 
From Appendix Table 3 (vol. I, p. 440) we find that for v — 14 the probability of a deviation 
greater in absolute magnitude than 2-04 is about 2 (1 — 0-969) = 0-062. This is small, 
but whether we regard it as significant or not depends on the probabilities we are prepared 
to consider as defining significance. The usual values are 0-05 and 0*01, and with such 
criteria we should not take the observed value as significant, though it arouses suspicions. 
We have here used central intervals, which are usual for the t-test of significance 
of the mean ; but it is easy to imagine circumstances in this particular case for which 
non-central intervals might be required. For instance, if the machine was at fault and 
had a true mean filling weight of more than 16 ounces the manufacturer would be giving 
sugar away for nothing. This might be serious, but probably not so serious as if the 
machine was erring in the other direction, which would render him liable to prosecution 
for selling short weight. Suppose he assessed the latter risk as nine times as serious as 
the former and was working to a probability level of 0-05. Then he would require 
the probability of a negative value of t greater than the significance value to be 
0-955 ( = 1 — 0-045) but could allow that of a positive value less than the significance value 
to be 0-995 ( = 1 — 0-005). From Appendix Table 3 we see that this corresponds to 
deviations of approximately — 1-8 and f 3-0. Our observed value is outside this range 
and is thus significant. Small as the average shortage is, it would be prudent to overhaul 
the machine and to make sure that it is giving fair weight on the average. 
We may note further that if the sample had occurred in the order 
15-7, 15-7, 15-8, 15-8, 15-8, 15-8, 15-9, 15-9, 16-0, 16-0, 16-0, 16-0, 16-1, 16-1, 16-2 
100 
COMMON TESTS OF SIGNIFICANCE 
we should almost certainly have concluded that there was something wrong with the 
machine, for the weights are steadily rising. The t-test would give the same result for 
this sample as for the first, since it does not depend on the order of occurrence of the 
members. Where, therefore, the appearance of individual sample members is ordered in time, 
the t-test alone may fail to reveal significant effects due to the changing of the population 
between drawings. Our data are still such as could have arisen at a single drawing of 
fifteen members from a population with mean equal to 16 ounces ; but the data throw 
doubt on the point whether we are really asking the right question in assuming that they 
all came from the same population. We consider the point again below (21.41). 
Before leaving this example, we may note another possible test, cruder than the £-test 
but sometimes useful. If the parent mean were really zero, positive and negative 
deviations should occur equally frequently in the long run. In our present case there are 8 
negative deviations, 3 positive ones and 4 zero. If we allot, conventionally, two of the 
last to each group we have 10 negative and 5 positive deviations. The expected number 
is 7^, so that the deviation is 2|~, with a standard error of <\/(l5 X | X -|) = 1-94. The 
observed deviation is very little in excess of this, so we conclude that the preponderance 
of negative signs in the sample is not significant of a negative mean in the population. 
More exactly, we find that the occurrence of 5 or fewer positive deviations is the sum of 
the first six terms in the binomial (J + -|)15, namely 0*151, leading to the same conclusion. 
The test is a very rough one since it pays no attention to the magnitude of the deviations ; 
but it has the advantage of applying to any symmetrical form of parent population for 
finite samples. 
Properties of the t-Distribution 
21.9. " Student's " distribution has numerous applications in the testing of 
significance apart from the one just considered, and we proceed to study its properties. 
The form (21.2) is a Pearson Type VII and may be transformed to the Beta-distribution 
(Type I) by the substitution f = 1/( 1 ~| J. The distribution function of t may thus 
be obtained direct from the B-iunction. For instance, we have 
dF -. 
ri 
F(t) = 
— mXi 
\ + 
Jo 
^/(v7c) r 
whence 
2F 
r 1 v 
whence 
F = 1 
(1 _f)-*df 
. (21.3) 
PROPERTIES OF THE ^-DISTRIBUTION 101 
The values of the argument for which I has the values 0-50, 0-25, 040, 0-05, 0-025, 0-01, 
0-005 and v = 1 (1) 30, 40, 60, 120, oo, have been tabled to five significant figures by 0. M. 
Thompson and others (1941a) and can hence be used to derive the values of t corresponding 
to those probability levels. 
21.10. Except for special purposes, however, the use of the JS-function is unnecessary, 
since the distribution function of t itself and tables based thereon are available. 
We have 
- log ( 1 +-) = --+—--. -. . + v . / + . . . 
\ v J v 2v2 jv7 
and hence 
2 toV v) f ^ ^ 2j(/+l)* 
Further, from the expansion for log F (1 + x) we find 
v -|- 1 
(21.4) 
log < 
1 . 
2 
2 J 12 
v\ K v 
111 
\ = 1_ . (21.5) 
f 4tv 24:v3 20^5 . 
j 
Now as v tends to infinity, t tends to the normal form with zero mean and unit variance. 
Writing 
1 
we find for the logarithm of the ordinate of (21.2), in descending powers of v, 
log y | *- (/,4 - 2t2 - 1) —^ (2P - M") I -I_ (M8 - 4tf6 + 1) 
fo J 4v v ; 12/2 v ; 24?;3 v ; 
- — (to10 - 5J8) + — (5^a _ 6*io - 3) - (21.6) 
Taking the exponential and integrating from t to oo, we find 
I -F =y\—t(t* + 1) + ~ (3«* - 7£4 - 5Z2 - 3) M ?~ (Z10 - ll*8 
J \4v v ; 96i'2 v ; 384v3v 
+ 14Z6 + 6*4 - 3f2 — 15) M- —--- (15£14 - 375J12 + 2225J10 - 2141Z8 
y 92160v4v 
- 939*fl - 213£4 ~ 915£2 + 945) t + . . . j . . . . (21.7) 
This is the expression, due to Fisher, which was used by " Student " himself in calculating 
the distribution function of t given in Appendix Table 3, Vol. 1. For values of v > 18 the 
first four terms of (21.7) give F to an accuracy of about 0-000,005. 
21.11. Tables are also available in the " inverse " form, that is to say, giving values 
of t corresponding to specified values of v and F. Such tables may be derived by 
interpolation from the cc Student " tables or by the normalisation method of 6.32. In work 
involving tests of significance this type of table is perhaps the most convenient, since it 
102 COMMON TESTS OF SIGNIFICANCE 
enables one to decide without calculation (other than interpolation for values of the 
argument not covered by the tables) whether particular values are significant for chosen 
probability «. The complement of the probability a is spoken of as a level of Kign.ficance 
and expressed either as a number between 0 and 1 or as a percentage. Similarly the 
corresponding values of t are called significance points, and we may speak, tor example, 
of the 5 per cent, value of t, meaning that value for which F is 0-95. 
Fisher and Yates (1938a) give the values oft for v = 1 (1) 30, 40, 60, 120 and oo and 
2 (1 - F) = 0-9 (0-1) 0-1, 0-05, 0-02, 0-01, 0-001. These tables, it should be remembered, 
give the significance points corresponding to twice 1 — F, that is to say the values of t 
such that the proportion of the distribution outside the range ± t is 1 h\ 
21.12. The number v is usually called the number of degrees of freedom of /. This 
is an expression which occurs in other connections, and a few words of explanation are 
desirable. 
It has been seen that the variance of a normal sample is distributed like the sum of 
(n — 1) squares of independent variates (compare Example 10.5, vol. I, p. 2:58) and 
generally, that if there are Jc linear relations connecting the original variates, the sum of squares 
of the originals is distributed as the sum of n — Jc independent normal variates of equal 
variance. Each linear relation reduces the freedom of the variation, as it were, by unity. 
It is thus natural to speak of the number of degrees of freedom, v, of a function sueh as 
X2, meaning thereby that it is distributed as the sum of squares of v independent 
normal variates with equal variance. The expression only has this natural meaning when 
normal variation is concerned. 
It so happens that the quantity t depends on a parameter v which is convenient for 
tabulating its distribution function and is also the number of degrees of freedom of the 
statistic s2 entering into the denominator of t. v may thus, by an extension of the term, 
be called the number of degrees of freedom of t, but this usage does not imply that / is 
distributed as the sum of squares of normal variates. 
Distribution of t in Non-normal Case 
21.13. Part of the price we have to pay for the precision of the tf-test in small samples 
is the assumption of normality in the parent. If the population is not normal we may still, 
of course, consider the distribution of " Student's " ratio, which will remain independent 
of the scale parameter; but complications appear because the parameters which express 
the deviation from normality will, in general, appear in the sampling distribution. Kurt her 
more, the distributions of x and s are no longer independent. 
Let us in the first instance prove the last assertion which is due to Geary (19.'W/j), 
in the form : If the mean and variance in samples from a population are independent 
and the population has finite cumulants, it must be normal. 
From 11.13 we have 
Kr(21r)=^±i, r>0. 
nr 
If mean and variance are independent, K (2F) = 0 and hence /cr+2 = 0 for r > 0. Thus 
the population must be normal. It is rather remarkable that we have not had to use 
relations of the type * (2s lr) = 0, s > 1 in arriving at this result and that we need only 
assume independence for one size of sample. 
DISTRIBUTION OF t IN NON-NORMAL CASE 
103 
21.14. In the notation of Chapter 11 we write 
t 
s 
\/k2 ( 1 -f~ 
k* 
K* 
K* 
fco 
and expand in terms of powers of — 
Ko 
The method follows that of 11.23 and we 
Kn 
find for the moments of t about the parent mean, assumed zero, to order v~2 
If 3 
J-A3' + —- (2A3 — 2A5 + 5A3A4 
Vv 
l$v 
/4 = 1 +?(1 + Af) + 4(3 -A, 
_ _ _i r_ 
/':! 
7A3 -r y* (210A3 
lbv 
— 3A3 A5 -j- 6Ag A4) 
66A5 + 105A3A* + 210A|)1 * 
th 
3 + ? (9 - A, + 14A5) + ~ (102 - 30A4 + 24AB 
+ 120A2 + 4^« - 132A3 A5 - 6Af + 168A1 A4 + 120A43) 
(21.8) 
where 
A, 
Kr 
/<>> 
ir' 
If the parent form is symmetrical, cumulants of odd order vanish and we have, to 
order v 2 and first order terms in the A's-— 
/'i = 
i 
//2 = 
//, = 
fh = 0 
I 1 _ 1 _ 
"J- r "T *~~ """ » • 
1 ■ »> >> 
,)f cy,~ y~ 
., , 18 102 2A4 
= •* "I" ■ "I- ■ 0 - 
V V" V 
V — 1 
*> 
v __ ,5 
0OA4 , 
- , -1- • • 
V" 
2 A 4 
••} 
1' 
1 
' = (v ■ 
i (v - l)2 
- %)(v ~"5) " 
2A4 
i> 
( 
oOA^ 
„ 0 
(21.9) 
Except for the term in A4 these are the values of the moments of t in " Student's " 
distribution, and it follows that for symmetrical parents which are not excessively lepto- 
or platykurtic we should not expect the £-test to be invalidated. If the parent is skew 
the situation may be different. 
21.15. The general, skew ease has been considered by E. S. Pearson and Adyanthaya 
(1928, 1929) from the experimental viewpoint and by Bartlett (1935a) and Geary (19366) 
from the theoretical viewpoint. Various writers have derived exact distributions of t 
in non-normal samples, but the sample numbers are, as a rule, trivially small and the 
results of little practical value. Geary considers the population expressed by the first 
two terms of the Gram-Charlier series— 
(IF 
1 
K. 
1 — ~.3 (3# — x2) \> e~*x* dx 
(21.10) 
and assumes that powers of k3 above the first may be neglected. He finds (cf. Exercise 
21.1) that the frequency function of t in this population is equal to the " Student " form 
plus a corrective factor 
1 - '" -A tdt . . (21.11) 
, {3v- t*(2v + I)} - - 
V{2n(v + I)}1 V Vi + ** 
V v 
2\ £(v+4) 
104 COMMON TESTS OF SIGNIFICANCE 
The integral of this factor from — co to — t is 
tr If 1 \ / *2\~i(H-2) / 2v 4- 1 
K* ll l ^ ' 1 + — ) (1+=—t~~V], • • (21.12) 
6 \l \2(v + l)nj \ v) \ v 
giving the correction to be applied. (Geary gives a table for some representative values.) 
This, of course, depends on kS9 but even where exact knowledge of the skewness is not 
available we may sometimes safeguard against error by considering the correction for 
plausible values of /c3. 
Other Uses of the t-distribution 
21.16. The usefulness of " Student's " t derives from the fact that it is independent 
of the scale parameter, and the simplicity of its distribution from the fact that it is the 
ratio of two independent variates, the numerator distributed normally and the denominator 
distributed in the Type III form. We shall see below (21.26) that these properties can 
be used to test the difference of two means in normal populations with equal variance, 
and in Chapter 22 we shall encounter a test of regression coefficients which is based on 
the same properties. 
We have also noted that " Student's " form can be used to test the significance of the 
product-moment correlation (14.15) and the Spearman rank correlation p (16.18). These 
facts are, however, in a sense accidental. They do not derive from the expression of the 
parameters concerned as the ratio of a normal to a Type III variate, but from the simpler 
fact that the distributions are of the Type II form (symmetrical with finite range) and 
hence can be transformed to the " Student " distribution, which is of Type VII. 
Symmetrical distributions of finite range can often be represented very approximately by a 
transformation to the " Student " form, especially if they tend to normality. 
Test of a Variance in Normal Samples 
21.17. The distribution of the sample variance s2 in normal samples is 
Thus, given for consideration a value of o2 and an observed s2, we can find the probability 
that s2/<72 is attained or exceeded and accept or reject a2 in the usual way. The 
distribution function of (21.13) may be expressed as an incomplete F-function, or more 
conveniently for statistical purposes in terms of y2 ( = ns2/a2) with v = n — 1. 
Example 21.2 
In Example 21.1 we found s2 = 0-0216, v = 14. Could the data have arisen by chance 
from a population in which the true variance is 0-01 ? 
MO & 
We have %2 = —~ = 32-4, v = 14. From the diagram on p. 446 of vol. I we see 
that the probability of such a value or greater is between 0-01 and 0-001, a very improbable 
result; and hence we reject a2 = 0-01 as a value of the parent variance. 
Once again this type of inference can be justified by the theory of confidence intervals 
since the probability 
pJ7^ > 32-41 < 0-01 
TEST OF A VARIANCE IN NORMAL SAMPLES 105 
is equivalent to 
p \c* <~\ < 0-01. 
\ 32-4 J 
In asserting that a2, was less than «2/32*4 (in our present case 0*01) we should be wrong 
more than 99 times in 100 on the average. 
There is a point of interest to note here. In Example 21.1 we considered a hypothesis 
as to the mean //, and in the present example a hypothesis as to the variance or2. Had we 
considered the two together, that is to say the compound hypothesis that \i = 16 and 
or2 = 0-01, we should have been in difficulties in justifying our procedure by reference to 
confidence or fiducial intervals, since we could no longer assert that our conclusions were 
right in an assigned proportion of cases. We have avoided this complication by 
considering separately the hypotheses (a) that /i = 16 whatever the variance, and (6) that 
a2 = 0-01 whatever the mean. This resource is not as a rule open to us where non-normal 
variation is concerned. 
Tests of Normality 
21.18. In large samples we can group the data into ranges and compare the actual 
frequencies with those to be expected on the hypothesis of parent normality. This 
comparison over the course of the frequency function is not satisfactory for small samples 
unless the grouping is so broad as to deprive the test of most of its efficacy. An 
alternative is to compute some statistic of the sample and to examine how far it departs from 
the mean value to be expected on the hypothesis of parent normality. 
Consider, for instance, the statistic 
A. o 
. (21.14) 
This is independent of the mean (because the k-statistics are so) and is also independent 
of the scale parameter because it is " studentised ". In normal samples, therefore, the 
distribution of t is independent of mean and variance and thus depends only on the sample 
number n. We have already given formulae for its mean and variance (Exercise 11.16, 
vol. I, p. 289). In fact, 
//.; (t) - fi, (t) - 0 
__ Gn(n - 1) > . . . . (21.15) 
^2^ ~~ (n 2) (n + l)(n + 3), 
Since the distribution of t is symmetrical we may, for moderate n, consider it as normally 
distributed with zero mean and variance given by (21.15), and this will provide a test— 
of a somewhat approximate kind—of normality in the parent from which the sample is 
derived. 
Example 21.3 
In the data of Examples 21.1 and 21.2 we have, for the sample moments about origin 
16, in units of 0-1 
mx = — 0-8 
m2 = 2*16 
m, = 0-496 
106 . COMMON TESTS OF SIGNIFICANCE 
71 
whence fc, = ma = 2-31429 
n — 1 
7) ^ 
3 (n - 1) (w - 2) 
and * =. -^ = 0-174. 
A/o" 
The variance of £, from (21.15), is 0-3188 and its standard error accordingly about 
0-57. The observed deviation from zero is considerably less than this, and we see no reason 
to doubt the hypothesis of normality so far as this test is concerned. 
21.19. Another test of normality has been proposed by Geary (1935a), namely 
the use of the ratio 
mean deviation /rt, ,~x 
w = - — - ——-—- . . . . .(21.1b) 
standard deviation 
If the parent mean is zero, the parent value of w is /- = 0-79788. The test has also 
been adapted to the case when the parent mean is not zero, and tables provided for the 
application of the test (Geary and Pearson, 1938). 
Geary's ratio is directed towards detecting deviations from mesokurtosis in the parent. 
The criterion based on k^/k\, which is a natural extension of that for skewness based on 
kz/k$, is not very suitable for the purpose, since it has a skew distribution for quite high 
values of n. The distribution of Geary's ratio tends to normality fairly rapidly 
{cf. Exercise 21.2). 
Tests of Goodness of Fit 
21.20. In Chapter 12 we considered in some detail the use of %% in testing 
correspondence between observation and hypothesis. If the hypothesis specifies the theoretical 
values completely no question of estimation arises, and each cell contributing to %2 could, 
if so desired, be tested separately. From this point of view %2 compounds into a single 
test a number of tests of the kind already considered. 
If the hypothesis does not specify the theoretical values completely, but leaves them 
to be estimated in part from the data, some modification in the #2-test is necessary. We 
can now establish a result which in 12.13 was announced without proof : if the estimators 
employed are maximum likelihood estimators, then for large samples the #2-test of 
significance retains its validity, provided that the number of degrees of freedom is reduced by 
unity for every parameter estimated. 
Suppose the hypothesis leaves unspecified a parameter 0, and let t be its maximum 
likelihood estimator. Then if the theoretical frequencies based on the true value of 0 
are X and those based on t are 1\ we may write 
X1 =£(~..-Ar ..... (21.17) 
2 . 
2 . 
~ " I 
ril-W 
-*- x, ■ 
%'* =Z}L—JLL- (21.18) 
■and 
TESTS OF GOODNESS OF FIT 107 
%* is distributed as the sum of squares of v normal variates with unit variance. The problem 
is to find the distribution of %'2. We have 
and for large samples the difference between 1 and 1' will be of order n~~-. We then have, 
expanding the difference in terms of 66, to order W"1, 
Now for large samples the maximisation of the likelihood is equivalent to minimising %2, 
&nd hence 
ti ^ ^ \ = q 
A'2 30 
= (W)*i7{i^yj (21.20) 
But the sum on the right is the reciprocal of the variance of the maximum likelihood 
estimator, and writing dt for 30, as is legitimate for large samples, we have 
xi~x"=^£t •(2L21) 
The quantity on the right is itself the square of a variate which (in the limit) is normal 
and has unit variance. Furthermore, its distribution is independent of that of %*. For 
consider the spherically symmetric density-distribution of the v normal variables whose 
sum of squares composes ^2. Let 0 be the origin and P any point ; then x1 = OP1. Now 
for large samples the variation takes place in the neighbourhood of 0. A surface of 
constant t through P is approximately plane in the effective range of variation. If OQ is the 
normal to this surface, 
OP* = OQ~ + PQ\ 
corresponding to 
___ (dt)2 ,2 
var t 
for t is chosen so as to minimise %* = PQ2. Thus if we take t as a new co-ordinate, together 
with (v — 1) others in the surface of constant t, the axis of t is orthogonal to the space of 
constant t, and t will be independent of %". 
It follows further that %2 is distributed as the sum of (v — 1) squares of normal 
variates. Thus the usual Type III distribution of %2 holds for v — 1 degrees of freedom ; 
and so for every constant fitted, with a reduction of unity in the number of degrees for 
each constant. We have already exemplified the use of the result in Example 12.4 (Vol. I, 
P. 301). 
The co ^-distribution 
21.21. For small samples the ^2-test is difficult to apply, since it depends for its 
validity on the fact that the binomial distribution in individual cells may be represented 
by the normal distribution, and hence requires that cell-frequencies shall not be small. 
* 
108 COMMON TESTS OF SIGNIFICANCE 
A test of a different kind has been proposed by Cramer (1928) and independently by von 
Mises (1931). 
Put 
CO 
00 
2 
y»OU 
{F (x) - F {x) }2 dx, . . . • (21.22) 
J —00 
where F (a?) is the observed distribution function and F (x) the hypothetical distribution 
function. The quantity co2 varies from sample to sample, its mean value being 
1-oo 1 
jB? (©*) = - F(z){l-F(x)}dz=±-Al9 . . .(21.23) 
where A1 is Gini's coefficient of mean difference (cf. 2.24). For 
E(a>*) = \ S{F-F}»dx. 
J —00 
For any given x the expectation of (F — F)2 is merely the variance of the proportion F 
F (I — F) 
and hence is equal to — . The result (21.23) follows at once. 
n 
The contest consists of comparing the observed with the mean value ; but it is not 
possible to express the comparison in terms of probability as the sampling distribution 
of co2 is not known. 
21.22. The numerical evaluation of the integral (21.22) is tedious in the case of a 
continuous distribution, and Wold (1938a) has suggested a modification. If the variate 
range is divided into intervals at — oo, xl9 x2 . . . x3- . . . oo, we define 
w2 
= Z{F (xj) - F {xj) }2 (21.24) 
i 
If the intervals are all of width A, 
JE? (u>«) = i- F (x){l -F (x)\dx + ~R, . . .(21.25) 
where R/n is a remainder term. If this maybe neglected, the w2-test is equivalent to the 
to2-test but easier to apply. If the data are ungrouped, the x/s may be taken at equidistant 
intervals. 
In the particular case when F is normal, we have 
%GO ftj> -i /»Q0 T| 
nE{toi) = e-**1 ——e-Wdudvdx. . . (21.26) 
V(2w) J* vW 
Putting u — a + x and v =/}-{> x, we find, after integration with respect to x, 
1 f° P^ 
exp { — |" (a — /3)2} da d/?. 
o 
■ 00 fc 
2V^J 
A further substitution of y = a — /? and 5 = a + /S gives 
or 
^V^J-y Jo 
1 f00 
2vXJo 
1 
DIFFERENCE OF TWO MEANS 109 
21.23. An interesting modification of the &>2-test has been given by Smirnoff (1936) 
who defines 
9 
/•CO 
{F-F)2dF (21.28) 
J —oo 
The difference lies in the differential element which has the effect of rendering 
the distribution of co\ independent of F. It is shown that as n tends to infinity the 
distribution function of odI tends to the form 
n 
GO 
, . 4/ "V .... (21.29) 
1 yi C™n 
71 k^i J<2*-i: 
but this does not look a very promising formula for application in particular cases. 
Cramer (1928) has extended formula (21.27) to the goodness of fit of Gram-Charlier 
series and gives some examples of fitting to observed distributions. 
Difference of Two Means 
21.24. A common case occurring in practice is that of two independent samples of 
% and n-2 members from two populations which may or may not be different. We wish 
to decide whether the evidence indicates a significant difference between the parent means. 
This situation forms a kind of border-line case between the testing of a prior value of a 
parameter and the homogeneity tests which we shall consider below. It is a test of 
homogeneity in the sense that we are to discuss the question whether two populations are equal 
in certain respects ; but we do not necessarily assume that they are identical, and in any 
case we can regard the problem as equivalent to the testing of a single parameter (the 
difference of the means) to see whether it is different from zero. 
21.25. For large samples we discussed the question in Example 9.10 (Vol. I, p. 226) 
and gave two tests. If the hypothesis is that the parent populations are identical (a true 
hypothesis of homogeneity) we may pool the samples to form a single sample and test 
whether either mean differs from the mean of the total. If, however, we wish to test the 
less general hypothesis that the parents have the same mean but not necessarily the same 
variance, we may test the difference of means by the ordinary equation expressing the 
variance of a difference in terms of the separate variances. This is not a homogeneity test 
in the strictest sense of the word, but tests of such a character may conveniently be 
discussed in conjunction with the other type, both for small and for large samples. 
21.26. We now consider the corresponding problem when the samples are small 
and the parent populations are assumed to be normal. In the first place we take the 
case when the two populations have the same variance c2. 
'> O 
(J" CX* 
The sample means xx and x2 are distributed normally with variances — and — and 
lb ]_ 'lb 2 
means //.x and //,2. Consequently * x—-——— — is distributed normally with variance 
— '+ —, and hence 
nx n2 
Xi — X% — \/^i — 1^2) I ^1 ^2 
fjhj-^ .... (21.30) 
V nt + n. 
110 COMMON TESTS OF SIGNIFICANCE 
is distributed normally with unit variance about zero mean. Further, if Sj and S* are- 
the sample sums of squares about the mean, the quantity 
^-=l(Sf+-Si) (21.31) 
is distributed as' x2 with nx + n2 — 2 degrees of freedom, independently of the expression 
(21.30). It follows that 
x1 — x2 — (fi! — ju2) I (n± n2 (%! -f n2 — 2) ] . 
S V 1 ^i + ^a J 
is distributed like " Student's " t with v = nx + n2 — 2 degrees of freedom. This 
expression does not contain the unknown a and hence may be used to test the difference /^ //.. 
This result is due to Fisher (1926a). 
Example 21.4 
In a class of 20 children, 10 chosen at random were given a ration of orange-jnice 
each day for a certain period and the other 10 a ration of milk. Their gains in weight 
during the period were, in pounds :— 
First group : 4, 2£, 3£, 4, 1|, 1, 3~|, 3, 2|, 3-| 
Second group : 1£, 3|, 2£, 3, 2-|, 2, 2, 2-|, T|, 3 
The mean increase in the first group is 2*9 pounds, and in the second 2-4 pounds. Putting 
aside other explanations, one possible factor accounting for this difference is the difference 
in treatments. But we wish to know in the first place whether this is significant. We 
assume, then, that treatment exerted no differential effect and that the samples came 
from normal populations with the same mean and variance. We find 
x1 = 2-9 x2 = 2-4 
Hence, from (21.32), with \xx — /u2 = 0, 
v = 10 + 10 - 2 = 18 
u = J>±_ V18 /I00 = !.3o. 
V13-3 v V 20 
From Appendix Table 3 (vol. I, p. 441) we see that such a value would be exceeded in 
absolute value with probability 0-21. The difference of a half-pound between the sample 
means is not significant. 
We note incidentally that the sample variances, 0-940 and 0-390, differ considerably, 
and shall see below how the significance of the difference may be tested. At the present 
stage our conclusion as to the non-significance of the difference of means is to be regarded 
with reserve, for the data themselves suggest that we have over-simplified the problem 
in assuming equal variance in the two populations. 
21.27. Apart from the question of unequal variances, the data of the previous 
example will serve to illustrate a further point of interest. Our hypothesis is that the 
children within each group may be regarded as a sample from a population with the same 
mean. Had we been dealing with a sample of, say, seedlings grown from the seed of a 
single plant, this hypothesis would not have been unreasonable ; but children differ very 
much among themselves in nutritional standard, and so forth. Our hypothesis is again 
liable to over-simplify the problem. 
DIFFERENCE OF MEANS WHEN VARIANCES ARE UNEQUAL 111 
When the statistician can direct the sampling himself, this kind of problem can be 
tackled with success by pairing. Suppose we select children in pairs of the same sex, 
each pair resembling each other as closely as possible in all the factors which might influence 
the experiment such as age, weight and nutritional standard. We allot at random one 
member to the first group and one to the second, and so for each pair. The differences 
in weights gained between members of a pair may then be regarded as samples from 
a population with zero mean, even if the pairs differ among themselves, and the set of 
differences tested in the usual way. 
Example 21.5 
Suppose that, in the previous example, the data had related to 10 pairs of children, 
thus 
No. of Pair. 
I 
2 
3 
4 
5 
6 
7 
8 
9 
10 
Totals 
First Group 
wt. in lbs. 
4 
2i 
3| 
4 
H 
1 
31 
3 
2J 
31 
29 
Second Group 
wt. in lbs. 
1* 
31 
2* 
3 
i.J'0 
2 
2 
21 
H 
3 
24 
Difference, 
First - Second. 
Jjit 
- 1 
1 
1 
- 1 
- 1 
11 
1 
1 
I 
■if 
5 
For the values in the last column we find 
x = 0-5 s2 = 1-25 
VI *25 v 
v 
1-34. 
9 
The probability of obtaining such a value or greater (absolutely) is about 0-22, and 
the observed differences are therefore not significant. This is the same conclusion that 
we reached in Example 21.3, but it would not have been surprising had the conclusions 
differed, for they relate to different questions. 
Difference of Means when Variances are, Unequal 
21.28. When population variances are not assumed equal the £-test of difference 
of means no longer applies. We can, if we choose, apply a test based on fiducial intervals, 
namely, the Behrens test, considered in the previous chapter. We put 
d 
. (21.33) 
Of* *—- Of* 
The fiducial limits of d for various significance levels have been tabulated by Sukhatme 
112 
COMMON TESTS OF SIGNIFICANCE 
(19386) and Fisher (1941a) for %^ and n2 greater than 5. If the observed d falls inside the 
range, we may accept the hypothesis that the population means are equal. 
21.29. As we have seen, an inference of this kind does not imply that we shall be 
correct in a certain proportion of the cases, and if we wish to find a test satisfying such 
a criterion a different approach is necessary. The following investigation is due to Welch 
(19386). 
Consider the distribution of u of equation (21.32) when the means are the same but 
the variances are different, i.e. 
*V —— /V 
u 
Sf + S 
1 1 
— ~j 
nx + n2 — 2 \nl %% 
V 
(21.34) 
Put 
0\ 
r2\* 
#1 — »2 = — + ~ ) % 
,2 „2 
9 9 
w 
*i %i +• °S JKi 
(»x + n, - 2) ( £ + ?* 
1 + 1 
7?a ^2 
. (21.3 5) 
. (21.36) 
where a\ %\ = Si and hence %\ is distributed as #2 with ?^ = nx — 1 degrees of freedom, 
and similarly for %\. % may be regarded as a single normal variate with zero mean and 
unit variance. We have then 
Now put 
where, from (21.36), 
a 
b 
11, — —-—. 
w = a%\ + b%l 
n1 + n2 — 2 
0*2 
% + n2 — 2 
Wi Wa 
^1 w2 
1 1 
% ^2 
2 9 
°A + ?? 
(21.37) 
. (21.38) 
. (21.39) 
w itself is not distributed in the Type III form unless a1 = a2, but we will find a distribution 
of that form which approximates to it by equating lower moments. The first two moments 
of w\ being the sum of the separate parts, are 
[i[ (w) = avx + bv2 1 
^2 (w) = 2 (a2 vt + 62 ^2)J ' 
The moments of 
. (21.40) 
rfj 
(2y)*T(^) 
Wh>-1 e-w/%0 dw 
are 
/*1 =?V 
^2 = 2#2v(" 
(21.41) 
DIFFERENCE OF MEANS WHEN VARIANCES ARE UNEQUAL 113 
Identifying (21.40) and (21.41) we find- 
9 
a2 Vx + b*v2*) 
av1 + bv% 
(av1 + bv2)2 
a2 vx + b2 v2. 
> 
. (21.42) 
With these values of g and v the distribution of w/g is approximately of the Type III form 
with v degrees of freedom and will be independent of %'. Hence, 
%' Vv 
w 
9 
= % 
gv_ 
w 
= uVigv) 
. (21.43) 
is distributed approximately as " Student's " t with v degrees of freedom. In particular, 
if o"i 
(J 2 ■> (m 
b and we reduce to the test of 21.26. 
21.30. In general, when ax ^ a^, the quantities g and v depend on the ratio 
0 = <j\/o\. We have 
(v.O+v,)2 
V 
Vi 02 + v2 
and may put u = ct where c = l/<\/vg, and hence 
(21.44) 
c 
< 
A f I) (Vl o + v%) 
> 
(21.45) 
Without a definite knowledge of 0 we cannot apply the it-test, but the advantage of putting 
the expressions in this form is that by considering particular values of 0 we are able to 
judge how far the test based on " Student's " distribution is likely to be affected. 
Example 21.0 (from Welch, 1.9386) 
Consider the case n1 = n2 = 10. From (21.45) we have c 
9 {() + l)2 
1 and from (21.44) 
6>2 + 1 
Suppose now we were to use the test of 21.26, based on the assumption that 0 = 1. We 
should find, to a probability level of 0-05, that | u | must exceed 2-101 to be significant. 
If we judge u significant for such values how far are we in error when 6 is not unity ? That 
is to say, what are the true probabilities that 
P { \u\ > 2-101} 
for varying values of 0, as compared with our value of 0-05 ? 
For a specified d the probabilities can easily be obtained from the approximate 
distribution u\/(gv) of equation (21.43). They are shown graphically in Fig. 21.1. The full 
line (a) shows P for various values of 6 and nx = n2 = 10. The full line (b) shows similarly 
the values for nx = 5, w2 = 15. (The dotted line (c) we refer to below.) 
A.S.—VOL. II. I 
\ 
114 
COMMON TESTS OF SIGNIFICANCE 
0-3 - 
0-2 - 
Values 
of P 
O-I - 
0-05 : 
0-0 
In case (a) the line 
does not deviate very 
much from the horizontal 
at P = 0-05, and we may 
conclude that the test 
based on the assumption 
of equal variance is not 
very much in error. In 
any case, if the curve 
falls below the line P = 
0-05 we are on the safe 
side, for our true 
probability is then less than 
0-05, and in rejecting the 
hypothesis at that level 
we are adopting more 
stringent standards than 
is apparent. 
In case (6), when the 
sample numbers are 
unequal we have a different 
state of affairs. For 0 < 1 the test is very conservative, but for 6 > 1 it may err very 
seriously in the wrong direction. 
21.31. Welch concludes that for samples of equal size there is not a serious 
likelihood of error in testing the difference of means as if the parent variances were equal. For 
samples of unequal size the error may invalidate the tf-test and an alternative criterion is 
proposed. Write 
0 01 
O-IO 1-0 10 
Values of 9 (logarithmic scale). 
100 
Fig. 21.1. 
Sf 
si 
%i (n>i — 1) ' n2 (n2 - 1) 
Here, it will be observed, the denominator is an estimate of 
(21.46) 
~ H—- ) , the standard 
deviation of the difference xl — x2. Precisely as for u we approximate to the distribution 
of this denominator by a Type III form. Corresponding to (21.39) we find 
a 
o; 
*■•> 
M^i 
'2 
n2 (n2 — I) i \nx nt 
Corresponding to (21.45) we find c = 1, and to (21.44) 
(21.47) 
v 
(21.48) 
n\ (nx —I) n\ {n2 - 1) 
v is then distributed approximately in " Student's " form with v degrees of freedom. The 
dotted line (c) in Fig. 21.1 shows the relationship between 6 and P { | v \ > 2-101} for 
% = 5, n2 — 15. Clearly the error is now much smaller than when we used u for the same 
sample numbers. 
DIFFERENCE OF TWO VARIANCES IN NORMAL SAMPLES 115 
Difference of Two Variances in Normal Samples 
21.32. If we have samples of nt and nz members from normal populations with 
s2 
variances o\ and o\, the ratio of sample variances p2 = ~J is distributed in the form (cf. 
Example 10.18, vol. I, p. 249)— 
dF oc __J^—^-—- _ (21.49) 
%1^1 L VH 
err o" 
9 
The related quantity 
is distributed in Fisher's form 
z = £log^? ^2 (21.50) 
CtJu OC ■—— r ———■—- . . , ( Jj 1,01) 
—— -| ~ \ 
&1 G% j 
where vx = nx — 1, ^2 = w2 — 1- The v's may, by a convenient extension of our previous 
terminology, be called the degrees of freedom associated with z. In practice, z is generally 
used in preference to p, but tables of both are available. 
These distributions provide a test of significance of the equality of the ratio a\/o\. 
On the hypothesis of equality they are independent of the ratio and the probability of 
an observed p or z can be obtained. As usual, if this is small we reject the hypothesis. 
We leave it to the reader to show that this type of inference can be based on the theory 
of confidence intervals or the theory of fiducial intervals in the usual way. 
Example 21.7 
In Example 21.4 we had two samples of children and found that the difference in 
means was not significant. This was on the hypothesis that the variances were identical, 
and since the two samples are equal in number the inference remains valid even if the 
variances are different, as illustrated in 21.31. We will now test directly whether the 
sample variances themselves indicate any significant difference in parent variances. 
We have 
L (xY - xx)* = 9-40 vx = 9 
S (x2 - x,)2 = 3-90 v% = 9. 
Hence 
9-40 / 3-90 
&6 9 / 9 
z = AW, - - / ^~ = 0-4398. 
From Appendix Tables 4 and 5 of Vol. I (pp. 442-3) we see that for v2 = 9 the 5-per-cent 
points of z are 
Vl = 8, 0-5862 
v1 = 12, 0-5613 
and the 1-per-cent. points are 
Vl = 8, 0-8494 
vx = 12, 0-8157. 
Thus, notwithstanding that one variance is about 2-| times the other, the probability that 
the observed z will be exceeded on random sampling from populations with the same 
variance is greater than 0-05, and the difference of sample variances is not significant. 
116 
COMMON TESTS OF SIGNIFICANCE 
There is a point here which is frequently overlooked. In carrying out the 2-test we 
always take the ratio of the larger variance to the smaller, so that our probability levels 
relate, not to the chance that a given pair of variances have a larger ratio than the observed 
one, but to the chance that the bigger of the two exceeds the smaller in a certain ratio. 
A probability of 0-05 thus relates to the chance that either s\/s\ exceeds a given amount 
k, or s\/sl falls short of a given amount 1/fc. If we are interested only in the former 
contingency our probabilities should be halved. 
Properties of Fisher's Distribution 
21.33. The 2-distribution plays a very important part in statistical inference based 
on small samples, and we digress at this point to give an account of its main features. 
The distribution function of z may be obtained from the incomplete J5-function, for 
z may be easily transformed into a Type I variate. There are, however, special tables 
for lower values of vx and v2 and satisfactory approximations of various kinds for higher 
values. 
The characteristic function of z is proportional to 
e(0+Vl)z fa 
/»O0 
• CO 
where 6 = it, and is thus 
{vx6lz + i;2)HM-'V) 
4(t) 
Vi 
10 
r\ 
v9. 
0 
r 
Vi 
r 
r 
Vi + 0 
it* 
( Jj 1 mt) Aj ) 
Thus, taking logarithms and using the expansion 
log F (1 + x) — \ log 2n + (x + -|) log x 
we find 
log <£(*) = - 
+ 
*Aj 1 
jl judo 
6» / 1 1\ fi2 /l 
2 \vY v2 J 4 \i/! )'a 
Thus, for large i\ and v2, z is distributed normally with mean 
1 1 \ , . . / 1 . 1 
(21.53) 
t 
and variance 
V* 
V* 
21.34. Various approximations have been given for the case when vx and v2 are 
not large enough to justify the assumption of normality. 
(a) (Cornish and Fisher, 1937). The method is that of 6.32 and depends on the 
expansion of the distribution in a Gram-Charlier series. From the successive derivatives 
of log r (1 + x) we can find those of <j> (t), and hence ascertain the cumulants of z. Writing 
1 ' =1. we find 
r x = — and r 
V* 
kx = ~ \ [rx - r2) - £ (r? - r\) 
*2 = i {rx + r2) + i {r\ + r\) + -J- (rj + rf) 
*3 = - i {r\ - r\) - [r\ - r%) 
k4 = r\ + r\ + 3 (rf + r\) 
K5 = - 3 (rj; - r|) 
k6 = 12 (rj + r%) 
r 
I >-J X . tJTc ) 
PROPERTIES OF FISHER'S DISTRIBUTION 117 
Hence, putting a = rx + r2 and 5 = rx — r2, we find for the J's of 6.32 (m = 0, 
variance = ia)— 
*« = i (* +^*)+ *(*■ +3 aa), 
and so on. i\.it}ei> some reduction we find, for the value of z corresponding to a probability 
oc (which in turn corresponds to a normal deviate f),— 
* h 
** (f' + 2> + VI {£ <*' + W + It T«* + m } 
- da (£4 + 9£2 • h 8) + -il~ (3£* + 7£2 + 16) + . /- i -^- (£5 + 20£3 + 15f) 
120V' ; ' 3240<rv ;nV2\l92(T ^ ^ w 
■+ A4 (£5 + 44^3 _.| 183c?) + ¥— (9£5 - 284|3 - 1513f) 1 . (21.55) 
2880 v 155520<r2 J v 
(b) (Fisher, extended by Cochran, 1940a). Writing n indifferently for vx and v^ we 
have, from (21.55), to order n 2 — 
°" - U (e h 2) + r- {— (£3 + 3f) +- -1-~~ (P + ii£) 
2 (> v" ' y ^ V 2 I 24 l ; 72 a ' 
Put h = 2/(7. Then 
~ "" VA B ls j j ] ~y/h\ 12A ' 144 
i<3 (P I- 2) ■]- -~ 1 s , :7; + s JA4 * Wa ^. . (21.56) 
Now ^ = * + J* + 0 (*-*). 
^(h — /) V™ 2h<\/h 
Hence, if we put 
£ 
the difference of this quantity from (21.56) is 
(P + llf)«5VA 
144 
fc2 _|_ 3 
£3 (£2 + 2), (21.57) 
1 l\a 
provided that we take K = s 
6 
The difference is small in virtue of the large denominator and the factor 52 = 
which is small if vx and y2 are not too different. Thus we may take z as approximately 
given by (21.57). The values of 1 for various values of the significance level are 
Level 40% 30% 20% 10% 5% 1% 0-1% 
I 0-51 0-55 0-62 0-77 0-95 1-40 2-09 
118 
COMMON TESTS OF SIGNIFICANCE 
For the commoner levels of significance the form taken by (21.57) is 
0-8416 
20 per cent, level: ——--.i - 0-4514(5 . . . (21.58) 
5 per cent, level: - i"^- - 0-7843S . , . (21.59) 
1 per cent, level: - ^i6l - 1-235(5. . . . (21.60) 
0-1 per cent, level: ^°1 - 1-925(5. . . . (21.61) 
y\rb — A) 
The accuracy of the approximation for v± = 24, v2 = 60 may be judged from the following 
comparison :— 
Value of z from 
(21.57). 
0-1337 
0-3748 
0-4966 
Exact Value. 
01338 
0-3746 
0-4955 
(c) (Paulson, 1942). The Wilson-Hilferty approximation to %% of 12.7 indicates that 
r2V 2 2 si 
— ] is distributed normally about mean 1 — — with variance —. The ratio -~ itself 
v / 9^ 9^ s% 
is the ratio of two independent quantities distributed as %l with vx and v2 degrees of 
freedom. Further, in virtue of Geary's theorem (Vol. I, p. 253) the ratio 
normally distributed in standard measure. 
We may thus regard 
m 
m 2 p 
(of + eip2)1 
is 
2 
% = 
9?'o 
52 
2 
9i^ 
2 T i 
(21.62) 
9vx 
as approximately normally distributed in standard measure. The approximation seems 
remarkably good. For instance, the following shows the exact and approximate values 
of p2 for vx = 6, vz = 12. 
ij 
per 
evol 
cent. 
20 
5 
1 
0-1 
ft1} = J)* ^ fr-OIIl 
(21.62). 
1-72 
3-00 
4-85 
8-58 
Exact Value, 
1-72 
3-00 
4-82 
8-38 
THE PROBLEM OF h SAMPLES 119 
The Problem of k Samples 
21.35. We now proceed to consider the case when we have samples from h different 
populations and wish to determine whether there is any evidence of significant differences 
between those populations. In some cases the appropriate test can be carried out by the 
^-distribution, particularly if the data are grouped. Eor the groups may then be regarded 
as determining the rows of a contingency table and the different samples the columns, and 
a homogeneity test applied to the table in the manner of Chapter 12. Again, we may 
compare the samples pair by pair by the foregoing methods ; but this, apart from being 
tedious, does not give us what we want, namely a test of homogeneity of the set of samples 
taken together. 
21.36. Consider in the first instance the sampling of attributes. Suppose we have 
samples from populations in which the true proportions of successes are w, the observed 
proportions being p1 . . . pk and the sample numbers nx . . . nk, totalling n. 
Ifp is the mean proportion of successes in all samples taken together, and our hypothesis 
is that the populations have a common value, p will be an estimate of w and we have for 
the variance of pj— 
my 
var #>,,■ = — 
= — approximately, .... (21.63) 
1 
where p = - E n$ p^ 
lb 
It follows that (pj — p) -3- will be distributed normally about zero mean with unit 
variance, and hence 
z* = ZA?hS2L=LV)3 (21.64) 
pq 
in the Type III form with k — 1 degrees of freedom (not k because we have lost a degree 
by estimating p). Hence the ratio 
Qz^Zni^i -.■?>* . . . . . (21.65) 
pq(k - 1) 
has expectation unity. The quantity Q is called the Lexis ratio, after the author who 
first discussed it in detail (Lexis, 1903).* 
* Lexis first developed the use of Q in a paper " tJber die Theorie der Stabilitat statistischer Reihen," 
1879, Conrad's Jahrbucher, 32, 60, reproduced in the reference given above. He dealt, however, only 
with the case when all the n's were equal and had no knowledge of the sampling distribution of Q. In 
practical applications he took as each n$ the average for the group. " Der dadurch begangenen Fehlei 
kann man beurteilen wennman n einmal mit der grossten und einmal mit der kleinsten GrundzaK 
berechnet." 
120 
COMMON TESTS OF SIGNIFICANCE 
Example 21.8 
From 1910 to 1919 the numbers of live male and female births in England and Wales 
were as follows :— 
Proportion 
Male/Total. 
0-5098 
0-5095 
0-5099 
0-5093 
0-5087 
0-5097 
0-5119 
0-5108 
0-5117 
0-5145 
0-51(4 
Year. 
1910 
1911 
1912 
1913 
1914 
1915 
1916 
1917 
1918 
1919 
Totals 
Male Births. 
457,266 
448,933 
445,004 
449,159 
447,184- 
415,205 
402,137 
341,361 
339,112 
356,241 
4,101,602 
Female Births. 
439,696 
432,205 
427,733 
432,731 
431,912 
399,409 
383,383 
326,985 
323,549 
336,197 
3,933,800 
Total Births. 
896,962 
881,138 
872,737 
881,890 
879,096 
814,614 
785,520 
668,346 
662,661 
692,438 
8,035,402 
The proportion of male births showed an increase during the war years 1916It)19. 
This is a well-known effect of war, but suppose we had noticed it here for the first time. 
The natural question is : can the effect be accidental '( There is no doubt about its realitt/, 
for the data cover the whole population ; but if we suppose that sex at birth is distributed 
according to the laws of chance, do the differences observed suggest that in the ten years 
concerned there was a significant change in the population (as regards proportion of male 
births) ? Let us consider the homogeneity test applied to the 10 proportions. 
We have p = 0-5104, n = 8,035,402, k - 1 =v = 9 and the sum 2"^ {pj ~ />)2 will 
be found to be 19-895,783. Hence 
19-895,783 
9 X 0-5104 x 0-4896 = 
Q 
2-974 
%2 = (£ _ !)Q2 _ 79.6I8. 
Q is sufficiently far from unity to reject decisively the hypothesis that the data are 
homogeneous. A 22-test will confirm the conclusion. We infer that, whatever the reason, 
the differences in proportions of male births, slight as they are, cannot be accounted for 
on the supposition that the distribution of sex is according to chance in samples from 
a constant population. We may observe that, had we obtained the same proportions 
for a sample one-tenth the size, x2 would have been 7-962 and we should not have inferred 
non-homogeneity. 
21.37. A similar test may be applied with k samples of variables. Let the samples bo 
lis x12? 
^21) ^223 
X 
hi 
with mean x1 
Xo 
Tin 
3 3 
3 3 
JL> i 
The variance of the jth sample is 
• 4 * 
•£kn/c 
33 
33 
X 
k 
THE PROBLEM OF h SAMPLES 
121 
and an estimate of the population variance may be obtained by taking the weighted mean 
of sample variances 
as = _i_ i7 27 (^ - ^)2 (21.66) 
71 — rC 'j J, 
Here we have reduced the divisor ton — k so as to correspond with the number of degrees 
of freedom. 
a 
Furthermore x$ will be distributed with variance — and hence (assuming without 
loss of generality that the parent mean is zero), 
k 
E Y{nj (xj ~~ x)2} = E{E (n; x*) - E (nx*) 
;-i 
v2\ X 
k 
= ICG" 
0" 
(k - 1) <72. 
Putting then 
1 
8. 
u 
. (21.67) 
we have another estimate of a2. Within sampling limits sv and su should be equal. If 
they are not, we suspect the homogeneity of the population. 
21.38. The above test is a simple form of the analysis of variance, which, we shall 
study extensively in Chapters 23 and 24 ; it is therefore unnecessary for us to develop it 
further at the present stage. Essentially the test is one of simultaneous significance of 
differences between means on the assumption that variances are constant. We shall also 
discuss in Chapter 26 a generalisation of the variance ratio for testing the homogeneity 
of a set of variances. 
Example. 21.9 
The following table (from the Registrar-General's Statistical Review of England arid 
Wales for 1933, Part II) shows the numbers of males married in England in that year 
classified according to age and district. (Certain small numbers of unspecified age and 
those under 21 have been omitted.) 
District. 
South-East . 
North. 
Midland . 
East .... 
South-West . 
Totals 
21- 
31,714 
31,507 
17,465 
4,016 
4,323 
89,025 
25- 
43,979 
39,849 
21,486 
5,297 
6,065 
116,676 
Age (Years). 
30- 
14,995 
13,620 
6,729 
1,820 
2,218 
39,382 
• 
35- 
7,985 
7,108 
3,340 
962 
1,177 
xIU,0 / a 
45- 
3,928 
3,362 
1,624 
457 
514 
9,885 
55- 
3,717 
2,916 
1,509 
386 
580 
9,108 
JL 01. AJLS. 
106,318 
98,362 
52,153 
12,938 
14,877 
284,648 
Note the changes in interval at 25- and 35- years. 
122 
COMMON TESTS OF SIGNIFICANCE 
The question we shall consider is whether age at marriage differs significantly between 
different districts. This might, for example, be an important point if we were about to 
sample the population for some quality related to age at marriage, such as the number 
of children per family. The data might be regarded as a contingency table and %2 used 
as a test of independence in the usual way. Here we adopt an alternative by considering 
the mean age at marriage in the five different districts. 
Taking the centres of the intervals to be 23, 27*5, 32-5, 40, 50 and 57-5 years (the latter 
being admittedly an approximation) and making no corrections for grouping, we find :— 
District. 
i South-East 
1 North. 
Midland 
East 
South.-West 
Whole population 
Number. 
106,318 
98,362 
52,153 
12,938 
14,877 
284,648 
Mean 
(years). 
29-681,799 
29-312,626 
29-007,344 
29-425,761 
29-873,731 
29-429,049 
Sum of Squares 
of Deviations 
from Mean. 
7,092,490 
6,092,375 
3,105,520 
807,911 
1,025,284 
18,143,921 
Variance. 
66-710 
61-938 
59-546 
62-445 
68-917 
63-741 
The total of the sum of squares about district means, Z (%# — x$)2, is the sum of the 
figures in the fourth column, namely 18,123,580. The sum of squares Z' n$ (xj — x)2, is 
found to be 20,341. We have the useful check that these two together are equal to the 
sum of squares of deviations from the population mean, 18,143,921 (a property which we 
shall often require in the analysis of variance). 
Thus 
18,123,580 
6V 
5'2 
u 
284,648 
20,341 
63-67 
5085-25. 
No test of significance is required to see that the difference in mean age at marriage between 
districts is not a chance effect. 
Tests of Random Order 
21.39. The tests described above are concerned with the values of a number of 
sample members but not with the order in which these values occur. Sometimes there 
may not be an order, as, for instance, if a number of plants are grown simultaneously or 
a number of names drawn from a hat in a single handful. More frequently there is a 
temporal order of appearance in the values, and it is clear that, on some occasions at least, 
the order may be material. To take an extreme case, suppose we are told that in a sample 
of 100 births 53 are male. We conclude that the sample is concordant with the hypothesis 
that male and female births occur at random with probability J. But if we knew in addition 
that the first 53 births were male and the next 47 female we should almost certainly reject 
the hypothesis. 
21.40. If sampling is conducted by taking members one at a time from a population 
and the process is random, then any order is as probable as any other order. The sample 
RANKING TESTS 123 
may be considered as a section of an infinite series generated by the sampling process, and 
this series ought to behave like von Mises' Irregular KoUektiv (7.15). It is a happy 
hunting-ground for the theorist, since there is no limit to the number of tests which can 
be invented to ascertain whether a given finite series conforms to the random scheme. We 
have considered a few such tests in connection with random sampling numbers (8.15) 
and shall discuss others in connection with time-series (Chapter 30). Here we discuss a 
few tests which are useful in detecting departures from randomness in the sampling. We 
are not now considering hypotheses as to the parent population, but since the randomness 
of the sampling is an essential element of inferences in probability it is convenient to 
consider the reliability of the sampling, together with inferences from the sample about 
the parent. 
Ranking Tests 
21.41. Suppose we have a sample of n members that order, and are 
doubtful about its randomness. Such doubts may arise owing either to defects in the 
sampling or to possible alterations in the population while the sampling is going on. In 
the first case the process itself is at fault; in the second, circumstances are at work to make 
the sample something other than it purports to be, a random sample from a single 
population. Either influence may relate the magnitude of the x's to the order in which they 
occur, and the values xx . . . xn are not then a random order in the sense that any other 
order was equally probable. 
Let us then consider all the possible orders, n ! in number, of the observed values 
A proportion of these, determined by a significance level of 5 per cent, or 
1 per cent., say, we will decide to reject as improbable ; and we will select as the " 
improbable " rankings those which exhibit the systematic appearance of which we are afraid, 
and particularly the regular rise or fall from x± to xn in magnitude. In short, we rank the 
sample in order of magnitude, say Xx . . . Xn, where the X's are a permutation of 
the first n integers, and compute a rank correlation coefficient between this order and the 
order 1 ... n. If the coefficient is large in absolute value (" large " being determined 
by the significance level) we suspect the sample of being subject to systematic influences. 
Exa7nple 21.10 
Thirty persons in the income group £1000-£1500 are asked to supply returns of their 
annual income for some purpose connected with taxation. It is intended to summarise 
their replies by a given date, but when that date arrives only 20 answers have been received. 
This is a frequent event in postal inquiries, even when the return is compulsory, and it 
has to be decided whether the 20 returns may be accepted as representative of the 30. 
There are prior reasons for suspecting that persons with bigger incomes may delay more 
than the others, partly because of difficulty in completing returns and partly because of 
a natural reluctance to part with information which may tell against them.*. We 
therefore wish to ascertain from the 20 returns whether there is any evidence that persons with 
smaller incomes tend to submit returns earlier than those with larger incomes. 
Suppose the 20 returns give incomes, in that order, of £ per annum : 1180, 1270, 1400, 
* This is an assumption for the purposes of the example and not intended as a statement about 
taxation returns in real life. 
124 COMMON TESTS OF SIGNIFICANCE 
1090, 1190, 1250, 1170, 1300, 1290, 1310, 1280, 1350, 1320, 1380, 1420, 1390, 1470, 1360, 
1220, 1460. The ranking order is— 
No. of sample . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 
Banl- 3 7 17 1 4 6 2 10 9 11 8 13 12 15 18 16 20 14 ;> 19 
Difference - 2 - 5 - 14 3 1 0 5 - 2 0 - 1 3-1 1-1-3 0-3 4 14 I 
The sum of squares of differences is 508 and thus the Spearman coefficient of rank 
correlation between observed and natural order 1 .... w is 
6 X 508 ft 
p = 1 — -- -—• = 0-618. 
H 7980 
The probability of obtaining such a value or greater (16.18) may be found from ""' Student's " 
distribution by putting 
'-'(r^r-)'-3"* 
v = 18, 
and is found from Appendix Table 3, vol. I, to be about 0-004. The test confirms our 
suspicion that size of income is correlated with order of appearance, and if we intend to 
use the mean income of the 20 returns as an estimate of the income in the full W we must 
recognise that it may very well be an under-estimate. 
21.42. It will be noted in this example that we have made no assumption about 
the distribution of incomes in the sample or the population (the latter of vvliicli would 
certainly not be normal) and have used the sample values themselves without any reference 
to the question whether they were representative. This does not invalidate our inference, 
which is made within the population of samples obtained by permuting the observed values. 
(Cf. 17.44 and 17.45.) 
21.43. A second test of use in random series, particularly when it is suspected that 
cyclical effects are present, may be obtained by counting the occurrences of iw peaks n or 
" troughs " in the series. A member is said to be a " peak " if it is greater than the* two 
neighbouring members, and a " trough " if it is less than those members. In either ease 
it is a " turning-point ". The interval between turning-points is called a tl phase 1\ 
Three consecutive observations are required to define a turning-point. If the series 
is random the probability that any given three provides a turning-point is ?,, for the values 
xu xit x3 may occur in six orders and in only four is the greatest or least value the middle 
one. In a series of N terms there are N — 2 sets of three, and hence the expected number 
of turning-points p is 
E(p) =-§.(^-2) (21.08) 
The variance and higher moments of p are not so easy to determine. Like the ranking 
problems considered in Chapter 16 (to which the present problem is analogous), the 
distributions resulting are rather complicated. We quote without proof the results 
, . I6N - 29 
p%(p) = ^_ (21.09) 
, x 16 (N + 1) 
MP = l^f— 21.70) 
945 
U8N2 - 1976^ + 2301 
Mp) = j^5 " * * - (21.71> 
RANKING TESTS 125 
As N tends to infinity the distribution tends to normality fairly rapidly, and p may, 
for finite N, be taken as normally distributed about mean | (N - 2) with variance 
UN - 29 
90 
21.44. A further test may be derived from the distribution of phase lengths. The 
2 
probability of a phase of length d in a series of d + 1 terms is clearly - — for only 
\(i ~f~ 1) I 
two of the possible permutations are favourable. In a series of length N there are 
jV — d — 2 possible phases of length d, for d + 3 points are required to determine the 
phase. The probability of a phase d in d + 3 terms is 
i i 1 r i i } d2 + u +1 
(d + 1) ! (d + 2) ! J 1 (d + 2) ! (d + 3) ! j (d + 3) ! 
and hence the number of phases of length d is 
(21.72) 
2 (N ~~~ d - 2) (d* + 3d + 1) 
^ liTsrr-—" (2L73) 
Now the number of possible phases is 
Nl{^f1 + m} (2L74) 
for there is one fewer phase than turning-points, | (JV -— 2) in number, and the whole 
series may be a phase, which accounts for the factor 2/JV ! In practice this is negligible, 
and for the probability of a phase d in a series of N we then have (21.73) divided by (21.74), 
namely 
6 (d2 f 3d + 1) (N - d - 2) 
(d + 3) ! (2N - 7) v ; 
The moments of this distribution are easily obtained to a very close approximation. 
For example, 
<> "\7 j (N - d - 2) (d2 + 3d + 1) 
//.', (d) =■■= - > d . 
1 l v ; 2N -- 7 ZJ 
22V 7 ^ ' (d + 3) ! 
jr [ (N -2) t (rf -i-3) (rf +2) (rf i- *) -3 (d +3) (fi +2) +5 (d H~3) "■3) 
2iV • 7 
i 
(d I 3) (d I 2) (d -I- 1) d -|- 3 (d f 3) (d + 2) (d + 1) - 8(d + 3) (d + 2) 
13 (d | 3) - 9]/(d + 3)! 
(> 
V 
■4—1 
2N - 7 
/\r .... >>\ j ] _ ;J .i„ ._ 5. ._ _J'L 
W "' [ d ! (d + 1) ! (d + 2) ! (d f- 3) ! 
1 3 8 13 9 
" (d - 1) ! + d ! "" (d + 1) 1 (d'+2) ! " (d + 3) ! J 
Remembering the rapid convergence of E T. to e, we may write this as 
o 
6 
iAj 
[ (N - 2) {e - 1 - 3 (e - 2) + 5 (e - |) - 3 (e - f) } 
_ e + 3 (e - 1) - 8 (e - 2) + 13 (e - f) - 9 (e - f) ]. 
126 
COMMON TESTS OF SIGNIFICANCE 
Similarly we find 
{{Se - 21) N2 + (4e - 17) tf - (48e2 - 140e + 14) } ^ 0-560. (21.77) 
21.45. In comparing observed distributions of phases with expected values the 
ordinary #2-test cannot be applied, because the probabilities of the events in a finite series 
are not independent. A test of significance has been derived by Wallis and Moore (1941), 
who consider a grouping into three categories, d = 1, d = 2 and d >'3. They conclude 
that x2, calculated from these three groups can be tested in the usual Type III form 
with v = 2\ if ^2 > 6-3. For lower values -f#2 can be tested in that form with v == 2. 
This test is independent of the law of distribution of the variables and is thus of general 
application. It has to be remembered, however, that generality in these matters may 
be offset by loss of sensitivity, and more searching tests may be required in certain cases. 
Example 21.11 
The following table shows the deviations from a moving nine-year average of potato 
yields in England and Wales for the years 1888-1935 (units are ^th ton) :— 
Year. 
1888 
89 
90 
91 
92 
93 
94 
95 
96 
97 
98 
99 
Jl XC'JLLl. 
- 6 
4- 2 P 
- 4 T 
o 
-1 
h 6 P 
— 2 T 
4- 7 P 
■f 3 
-67 
+ 2JP 
0 
Year. 
1900 
01 
02 
03 
04 
05 
06 
07 
08 
09 
10 
11 
Yield. 
- 7 T 
4 6 P 
- 3 
-IT 
4 •« ■*■' 
0 T 
4- 1 P 
- 7 T 
+ 8 P 
4_ 4 
4- 3 T 
4- 4 P 
Year. 
1912 
13 
14 
15 
16 
17 
18 
19 
20 
21 
22 
23 
Yield. 
Year. 
Yirld. 
15 T 
3 P 
2 
J 
2 T 
5 P 
Hr 
4 T 
3 P 
9 T 
11 P 
I 1 
1924 
or. 
26 
27 
28 
29 
30 
31 
32 
33 
34 
35 
1 
9 
3 
9 
r> 
I 
10 
i 
<•> 
4W 
5 
4 
7 
/' 
T 
P 
T 
r 
J 
We have marked with P and T the peaks and troughs of the series. The observed 
number of turning-points is 31 in a series of 48 terms. The expected number is, From 
(21.68), -| (48 — 2) — 30-67, almost exactly the number observed. No test of significance 
is required. 
The duration of phases is :— 
d = 1 
2 
3 and over 
Observed 
20 
6 
Predicted (21.75) 
18-75 
8-07 
:M8 
30 
30-00 
Here, again, a test is hardly necessary. We find, in fact, #2 = 0-826, 7 of which for 
v = 2 is not significant. 
We conclude that these tests provide no evidence against the randomness of the series 
and hence do not suggest any cyclical movement in the yields. 
CONDITIONAL TESTS 127 
21.46. In the foregoing example we have treated the two values in 1923 and 1924 
as a single value since they are equal. These so-called " ties " frequently occur in ranking 
work and are a great nuisance. In the present case there is only one, and any reasonable 
method of treating it will not affect the test. Where "ties" are numerous enough to 
make a serious difference some systematic method of treating them is desirable, particularly 
if more than two individuals are tied. They may be treated as a single observation, as 
in this case (although it would probably be better then to reduce N accordingly) ; or, 
preferably, they may be counted as a mean value, e.g. with a tied pair we should consider 
the first as greater than the second and then the second greater than the first, counting the 
number of turning-points or phases as one-half in each case and adding the two together. 
This, as in all similar ranking problems, makes the theoretical discussion of sampling very 
complicated, and if it is desired to make a precise use of significance tests a further 
possibility is to assume that the tied members are ranked in the order most unfavourable to 
the hypothesis under test, so as to be on the safe side. 
Conditional Tests 
21.47. When.several unknown parameters are concerned, it may be difficult to find 
a sampling distribution dependent only on one of them which will form a basis for estimation 
or a test of significance. Sometimes, however, we can get rid of undesirable parameters 
by restricting the distribution in some way, and particularly by considering a distribution 
of samples which have some specified quality in common with the observed sample. Such 
distributions we shall, in Bartlett's phrase, call conditional. Fisher expresses a similar 
idea by speaking of samples which have the same configuration. 
The most important application of this principle is in the testing of regression 
coefficients, which we shall consider in the next chapter. Here we give a simple illustration 
of the method for the Poisson distribution. 
Example 21.12 
Suppose we have two samples from populations which are known to give the Poisson 
type of distribution but may have different parameters. We wish to determine whether 
the populations could be identical. 
Suppose the frequencies of successes in the two samples are r1 and r2. If .A is the 
parameter of the parent (assumed the same for each), the probabilities of the samples are 
erA — and erA — 
a 
5 
and their joint probability is accordingly 
P{rurt\k}=\ ,*_■,- (21.78) 
1 
ri ! To. 1 
This depends on X and does not help us in answering the question. However, for the 
probability of a sample with rx + r2 successes we have (since the sum of two Poisson variates 
with parameters /ll5 A2 is distributed in the same form with parameter Xx + Aa) :— 
P {r1 + r% 1} = -—-l-J-—., 
0"i + ra) ! 
128 COMMON TESTS OP SIGNIFICANCE 
and hence 
P{rl9rt\X} = K + n) 1 _ _H__ (21 79) 
Pj^+r.ll} 2*+* rx ! r2 ! 2* rx I ra! 
where r = fj + r2. 
Now in accordance with Bayes' theorem we have 
P{rltrz |A} =P{r1,r%\rl + r2} P {rx +r2\X} 
and hence 
P{r»rt\r}=-- --/ -. (21.80) 
i. 2r r1\ r2 ! 
Consequently, if we confine our attention to samples < for which the total number of successes 
is r, the probability of the observed rx and r2 is independent of I and is, in fact, the 
corresponding term in the binomial [\ + \)T. The probability is clearly that of a partition of 
r into the observed rx and ra, and if it is small we suspect the hypothesis that the samples 
emanated from the same population. 
This kind of conditional inference raises the same sort of point as we noticed in 17.44. 
We decide beforehand that, whatever r turns out to be, we will make the inference in the 
population of samples which yield that value of r. 
Pitman's Tests 
21.48. In the extreme conditional case we may consider an inference in a population 
of samples the members of which are the same as those actually observed, the population 
being given by permutations or partitions of the observed values. The tests of ranking 
and periodicity given above are cases of this kind. A similar procedure has been advocated 
.by Fisher in the analysis of variance and the design of experiments, and will be considered 
in due course. We now proceed to examine tests of the same nature proposed by Pitman 
(1937a, 1938). 
Suppose we have two sets of values u± . . . um and vx . . . vn with means u and v 
-and the mean of the two together equal to z. Given m + n objects, there are ( 
ways, say N, of separating them into two sets of m and n objects, of which the given set 
is one. We call | u— v \ the spread of the separation. Since 
mu -\- nv = (m ~\- n) z, 
we have also for the spread 
(m -f n) | u — z | (m + n) \ E (u) — mz 
n mn 
(21.81) 
Take a probability 1 — a = M/N, where M is an integer. If R is a particular separation, 
and the number of separations with spread not less than that of R is not greater than M, 
we call R discordant. If there are M or more with a greater spread we call it concordant. 
A separation which is neither concordant nor discordant is called neutral If m = n the 
separations occur in pairs with equal spreads, and we then take M to be even. The 
discordant separations are most easily picked out as those with the largest values of 
\Eu — mz\. 
If the observed separation is arrived at by chance, the probability that it is discordant 
is M/N = 1 — a when there are no neutral separations. If such exist, the probability 
PITMAN'S TESTS 
129 
is less than 1 — a. Similarly the probability that a separation is concordant is 1 — a, 
or more, as the case may be. 
Two samples ux . . . um and vt . . . vn are said to be discordant, concordant or 
neutral according as the separations u and v are so. Having selected our significance 
points dependent on a, and hence having fixed M, we can find for what values of the spreads 
a pair of samples is discordant or otherwise, and hence whether our observed pair is so. 
If they are discordant we reject the hypothesis that they came from the same population. 
Example 21.13 (Pitman, 1937a) 
Two samples have the following values :— 
0, 11, 12, 20 
16, 19, 22, 24, 29. 
Are they significantly different ? 
There are 9 members altogether and hence 
9 
5 
126 separations into samples of 
five and four. We take a to be as near as possible to 0-95, corresponding to a 5-per-cent. 
level of significance, and hence M = 6. We then find the groups which have the largest 
values of the spread. We have z = 17, so that mz = 68, and using the form I Su — 68 | 
we find those groups of four from 
0, 11, 12, 16, 19, 20, 22, 24, 29, 
which give the maximum value to this quantity. They are— 
| 27 w - 68 | 
29 
26 
25 
27 
26 
The group 0, 11, 12, 20 gives the fifth largest spread, and so with M — 6 the observed 
separation is discordant. Our inference is that the samples come from different 
populations. Only in four other cases out of 126 should we get so large a spread in samples from 
the same population, 
21.49. The extended use of the above test is barred by practical inconvenience, 
but an aj)proximate form based on a different measure of discordance may be used. We 
now put 
m (u ~~ z)2 
(N — m) //,./ 
o, 
o5 
o, 
29, 
29, 
29, 
11, 
11, 
11; 
24. 
24 
24 
12, 
JL<£, 
12, 
, 22, 
, 22> 
, 20, 
16 
19 
20 
20 
19 
19 
w 
. (21.82) 
where //,2 is the variance of the samples taken together and is thus a constant. The function 
w is hence linear in (u — z)2, the device of squaring, as usual, getting rid of difficulties 
associated with the use of the modulus | u ~ z |. N here refers to the total sample 
m ~f n. 
Now, for the moments of u — z we may use the results of 11.26 (vol. I, p. 284), giving 
the moments of the mean in sampling from a finite population ; for z is the population 
a.s.—vol. n. k 
130 
COMMON TESTS OF SIGNIFICANCE 
mean. Replacing n in the formulae of that section by m and putting N = m + n, 
we have— 
E (u — z) = 0 
jE7 (& — 2) 
£\2 _ 
N — m 
f>2 
E (u — zY 
(N - 1) m 
(N - m) [{N* + Nj- 6m (N - m)\/*4 + ZN (N 
" m*~(N~- iy^"-^(]\r"-T)' 
and hence for the first two moments of w we find 
1 
m 
I) (m - 1)//|] 
E (iv) 
E {w2) 
N - 1 
3 
where 
6 
N* - 1 
N (N + 1) 
3 (N - 2) (N - 3) [m^-'m) 
(1 + 9), • 
iV + 1 
- 6 
y2 + 
6 
jV + 1 
(21.83) 
(21.84) 
(21.85) 
y2 referring to the measure of kurtosis ~ — 3. 
For fixed N the modulus of the second factor in (21.85) will be found to have a maximum 
2(N - 2) 
at 
N 
when m = %N9 and it takes this value again at 
N — 2m 
"" 'N 
N 
2N 
m 
sivmg -r-—— = I or 5 for N — 14 and wider limits for lamer N. It will also be found 
2V (jV H- 1) 
that for iV > 6 the factor , ,T - ■ ■ , — 6 is not greater in absolute value than 
m (N — m) 
2 (2V - 2) . 
JV 
if 
1 _, m 
5 " jV — ra 
< 5, 
i.e. unless one sample is more than four times as big as the other. Thus for such values 
and y2 not large, 0 is small, and approximately 
A1^2) - vo'*-v (21-S6) 
iv " — J. 
Similarly, using the fact that for large m and N 
E (u - zfr - 1.3.5 ... (2r - 1) 1 ~~ 
my //£ 
IV7 "m7' 
we find approximately 
E (W*) 
3.5 
(iV - 1) (N + 1) (JV + 3) 
The moments given by (21.83), (21.86) and (21.87) are those of the J5-distribution 
1 
. (21.87) 
V 4d J M f 
(21.88) 
PITMAN'S TESTS 131 
Ai^hT therefOTe+b\used t0 approximate to the distribution of w. In point of fact the 
distribution seems to he. rmna.rVakLr nW~ * 
seems to be remarkably close. 
to may also be written 
w~- HL±!t 
mn 
(u —- vy 
m + n 
(21.89) 
which shows that w < 1. 
We also have 
mn 
- (u —• v)2 
w __ rn + n 
'' T^T^ti)* + Z [v - v)* ' (2L90) 
w 
and it is instructive to observe that the function on the right is the same as that of 
^ of (21.32) with a few changes of notation. A transformation of (21.88) to 
Wi -r n2 
(bib 
(IF cc , —— . (91 cm 
iC Student's " form will in fact show that we can test / —— in the ^-distribution with 
\/ 1 — w 
3r r = 7n h n — 2 ; for (21.88) then becomes 
du 
JL —— 
m + n — 2 
whew u = /j^__m (21.92) 
21.50. A test of a similar kind may be evolved for the product-moment correlation. 
Suppose we have two samples xx . . . xtl and yx . . . yn and calculate 
cov xy 
V(var x var y) 
for every possible pairing of the #'s and j/'s, n ! in number. As before, if we choose an 
a and hence a number M such that 1 — oc = M/n ! we may determine those pairings for 
which r is greatest and reject the hypothesis that x and y are independent in such cases 
if they fall among the M greatest. Since the denominator of r is constant, this is equivalent 
to attributing significance to the values of \Exy — nxy | which exceed a given value 
determined by a. 
Taking x — // = 0, without loss of generality we find 
E (r) - 0 (21.93) 
E{r*) = 2 -E(Zxyy 
n2Y8iVXY8bry 
——, (21.94) 
n — 1 
132 
COMMON TESTS OF SIGNIFICANCE 
and similarly, if yl9 yz are the modified measures of skewness and kurtosis for x (expressed 
h k\ 
\ , y2 = — ) and y[ and y2 those for y, it will be found that 
in terms of ^-statistics, i.e. yx 
JcJ: 
k 
E (r3) = 
E (r?>) = 
n 
n (n — I)2 
3 
Yi7i 
. (n — 2) (n - 3) 
(n — 1) (w + 1) w (n + 1) (^ — 1) 
. (21.95) 
. (21.96) 
Thus to order n x we have 
E{r)^E (r3) = 0 
E (*■*) 
£ (r4) = 
w 
> . 
3 
(n — 1) (n + 1). 
These are the first four moments of the distribution 
. (21.97) 
dF = 
1 
(1 -a2)*n-2<te, 
X -^ X -^ X. 
B (h %n ~~~ 1) 
Thus r may be tested in this distribution or equivalently, putting 
*= /// ov V(W ~ 2), 
. (21.98) 
. (21.99) 
in " Student's " form with v = n — 2. 
In particular, if the numbers x and ?/ reduce to rankings, we have the test already 
introduced in 21.41. Compare also the result given for the distribution of Spearman's 
p in 16.18 (vol. I, p. 401). 
The Combination of Tests 
21.51. It sometimes happens that we have a number of tests of significance, all 
yielding various probabilities, which we wish to express as a single probability. Suppose, 
for instance, that we conduct an experiment five times and that some test, such as that 
of the mean, gives probabilities to the observed deviations of 0-2, 0-8, 0-01, 0-1, 0*03. In 
the ordinary way two of these values would be regarded as significant and the other three 
not. What conclusion are we to draw as to the five taken together ? 
Suppose we have k values of the probability, px . . . pk. The distribution of any 
particular p is rectangular, i.e. 
dF = dp 0 <p < 1. 
Hence, if x = — log p the distribution of x is 
dF = e~x dx, 0 < x < oo 
and its characteristic function is 
/•CO 
<£ (0 = 
Jo 
e 
w(iv """* iV 
'dx 
1 
it 
THE COMBINATION OF TESTS 
Hence if we write 
A: 
A 
^ ~ ]l log &> 
2 = 1 
the distribution of A has a characteristic function 
<f>(t) 
and is therefore given by 
dF 
1 
Putting 
r{k) 
(i - it)k' 
Ak-le~AdA. 
M* = 2A = - 2riog^ = - 2 logilp 
we see that the distribution of M2 is 
dF ex JbT3*-! exp (- pra) daf . 
or if2 is distributed as #2 with v = 2fc degrees of freedom. 
133 
. (21.100) 
(21.101) 
. (21.102) 
. (21.103) 
Example 21.14 (K. Pearson, 19336, quoting data from E. M. Elderton, 1933). 
Pairs of boys were selected in various age-groups and one member of each pair fed 
on raw, the other on pasteurised milk. The differences in gain in weight are shown in 
the following table, together with the standard errors of the differences based on large- 
sample theory. 
(1) 
(Central valur 
in years). 
(2) 
Number 
of Pairs. 
«3 
v a 
10? 
/,) 
7« 
71 
77 
00 
(3) 
Moan Difference 
in Weight 
(lamed, Raw less 
Pasteurised. 
- 0-066 
-I- 0-022 
- 0-003 
+ 0-011 
H~ 0-002 
Standard 
Error of 
Difference. 
0-054 
0-053 
0-052 
0-055 
0-057 
(5) 
Probability 
of Observed 
Difference or 
Greater, p^. 
0-8888 
• 0-3409 
0-5239 
0-4207 
0-4840 
(6) 
logio Pk- 
1-9488 
1-5326 
1-7193 
1*6240 
1-6849 
2-5096 
The va lues of $>;. in column (5) are obtained by expressing the observed deviations in column 
(?>) in terms of the standard error in cohimn (4) and hence determining the probability 
from the normal integral. We have 
jyra = ___ 2Tloge^ 
i71ogioff 
logio e 
V 
6-86 
10. 
'Hie probability of a value of %2 > 6-86 for v = 10 is about 0-74, and the test as a whole 
does riot support the hypothesis of a differential effect on feeding between the two lands 
oi muiv« 
134 COMMON TESTS OF SIGNIFICANCE 
Nuisance Parameters 
21.52. From the foregoing it will have been clear that in the theories of both 
estimation and significance one of the main problems is to find a distribution which is independent 
of certain unknown parameters in the parent population. Parameters of this kind, 
necessary as they are in the specification of the parent and the precise formulation of our problem, 
can be a nuisance when we are seeking to make exact statements about some other 
parameter on which interest is focussed. For this reason they have been named nuisance 
parameters. It may be useful if at this point we summarise the methods available for 
getting rid of them. 
(a) First of all there is the process of " Studentisation ", whereby we can remove 
scale parameters from the sampling distribution by a suitable choice of statistic. (Cf. 
19.26.) 
(b) Secondly, we may restrict the inference to a sub-population which is conditioned 
by having certain values in common with the observed sample. It sometimes happens 
that the distribution in this sub-population does not contain the nuisance parameters, 
whereas a distribution in the full population would do so (21.47). 
(c) In the comparison of two samples, or even the testing of a single sample involving 
an unknown mean, that parameter may be eliminated by differencing (21.27). As regards 
the case of the single sample, it is clear that if JU 1 • • • "A/1 (Mi. tJ 
independent and n is even, 
the values will also be independent and be distributed 
with zero mean (though of course there are only \n of them). 
(d) Transformations of the variate may sometimes either eliminate the nuisance 
parameter altogether or reduce its importance. The most noteworthy case is Fisher's 
transformation of the correlation coefficient (14.18, vol. I, p. 345). The transformed 
function z — £ is distributed nearly normally with variance l/(n — 3), so that the difference 
of two correlations when transformed does not involve the common value of C- 
(Cf. Example 14.8.) 
(e) We may find distributions which are independent of the unknown parameters, 
and even of the population, by using the methods of ranking or considering partitions 
(21.41, 21.48). 
(/) The fiducial argument, in at least one known case, gives a test independent of 
unknown parameters, namely the Behrens test (20.13). 
It must be realised, however, that all these types of inference do not stand on equal 
footings. In particular (e) requires further examination, as we proceed to show. 
21.53. We may now review the many different tests which have been described in 
this chapter and consider more closely the type of reasoning on which they are based. 
We may group our tests broadly into two classes, those which give a direct test of a given 
value of a parent parameter and those which do not. 
The first class rests on a type of inference which we have discussed fully in connection 
with the problem of estimation. There is, in fact, only a difference in viewpoint, and little 
or none in essential ideas, between estimating a parameter by assigning a range to 
acceptable values (whether by confidence intervals or fiducial intervals) and ascertaining whether 
some prior value lies in that range. The significance of parameters in large samples, the 
test of the mean in normal samples by " Student's " distribution, the test of a correlation 
coefficient in normal samples, and others of the same kind relating to a specified parameter 
have the same logical foundation as the theory of confidence intervals or the theory of 
NUISANCE PARAMETERS 135 
fiducial intervals, whichever is preferred. They all provide for the consideration of alternative 
values of the parameter. 
21.54. The second group of tests are not, on the face of it, concerned with the value 
of a parameter in a parent population, and some of them take no account of possible 
alternative hypotheses. Consider, for example, a test of normality or a test of randomness. 
I he hypothesis is that the population is normal or the sampling is random, as the case 
may be, but this does not specify a parameter. What alternatives to normality or to 
randomness are we considering, if any ? We must have the existence of such alternatives 
in mind, however vaguely, for otherwise we should not be testing these particular 
hypotheses. But can we say what they are ? And if not, do our inferences remain valid ? 
When working with a probability oc shall we still be right in a proportion oc of the cases in 
link ufe \<mr. Jk. JL, • 
21.55. The kind of argument we have used in all these cases is this : on the given 
hypothesis the observed sample and all samples providing a greater value of the statistic 
being used for the test have a small probability. Therefore we reject the hypothesis. 
We may note at once that in rejecting the hypothesis we do so in favour of another 
hypothesis for which the observations are more probable. We may not express this thought 
explicitly, but it is there. The various statistics we use for testing normality, for instance 
&1? can arise with greater probability from other populations which are skew or have a 
marked deviation from naesokurtosis ; the fact is assumed as self-evident (as indeed it 
is) and hence, if the statistic is improbable for the normal case there will be non-normal 
oases of greater probability. We remark, nevertheless, that the actual probability oc is 
calculated on the normal hypothesis and does not hold for the non-normal cases. Thus 
wo can no longer assert that we are right in proportion a of the cases. We are therefore 
relying on a less definite principle of inference to the effect that we reject a hypothesis 
which jjives an improbable value to observation, provided that there exists some other 
hypothesis which gives a more probable value. 
21.56. A similar argument applies to tests of randomness. It is obvious that many 
other methods of generating a series exist which give a greater probability to a systematic 
series than the random method, and in rejecting the latter we do so more or less consciously 
in favour of the former. Our intuitive feelings on the point lead us to apply one test when 
we have the possibility of systematic order in mind (the ranking test) and another when 
we are interested in oscillations (the phase test). What we are doing, in effect, is selecting 
the test, of randomness which we feel to discriminate best between the hypothesis of 
randomness and the alternative possibilities. 
21.57. Although, therefore, much remains to be done in putting tests of normality, 
randomness and goodness of fit on a formal logical basis, there do not appear to be any 
serious difficulties in doing so insofar as the specification of alternative hypotheses is 
concerned. But there remains the difficulty hinted at at the beginning of 21.55. In the 
majority of cases we have a probability 1 - oc that the observed statistic *0 will be exceeded, 
and if this is small reject the hypothesis. But why exceeded ? Why reject the hypothesis 
because of the improbability of a number of events which have not happened ? 
Here also it seems that a closer inquiry into the logic of the process would be worth 
while We have seen how it can be justified by confidence-interval or fiducial theory 
136 COMMON TESTS OF SIGNIFICANCE 
when a parameter is under consideration. When no parameter is specified, the process 
must, in the present state of our knowledge, rest on more intuitive ideas. My own view 
is that, in a vague kind of way, we are really considering the range of values of a parameter 
without realising it. In selecting a statistic to carry out the test, we usually relate it to 
the sort of effect we are expecting to divert the real state of affairs from those of 
our hypothesis. For instance, if we suspect cyclical effects in a random series we base 
a test on oscillations in that series. The further the series deviates from randomness the 
greater will be the value of our statistic ; and consequently, if we could measure deviation 
from randomness (in the direction of cyclicality), we should have a parameter which could 
be located in a range in the manner of confidence intervals. Such a range would exclude 
the larger values of our statistic if it can be regarded in any sense as estimating the 
parameter (or, more generally, as increasing with it) ; and hence the procedure of rejecting the 
hypothesis if the statistic is among these large values may be justified. 
21.58. It is for this reason that we began the chapter by defining tests of significance 
in relation to a parameter-value given a priori. It seems probable that in the ultimate 
analysis no other definition will be satisfactory. The fact that in this chapter we have 
given tests of hypotheses which do not appear to specify a parameter value is, I think, 
merely a reflection of the fact that the nature of those hypotheses and the inferences about 
them are not usually understood clearly but are based on more or less intuitive ideas. It 
is probable that many of these ideas are sound and can be given explicit logical foundation ; 
but the matter awaits investigation by the statistical logician. 
21.59. There remains for consideration the type of inference used in Pitman's tests 
(21.48 and 21.49). These are of the character of tests of randomness. Given a set of 
values, we consider all the arrangements in which they could have happened and reject 
the hypothesis if the observed arrangement is improbable. Here again, as it seems to me, 
there is a suppressed series of alternative hypotheses which would make the observed 
value more probable ; and in choosing the test, such as the " spread " or the high value 
of a correlation, we are intuitively relating the magnitude of a statistic to the deviation 
from randomness. Pitman himself has shown, however, that when the hypothesis is 
definite and specifies the difference of two means, the tests give confidence intervals in the 
ordinary way (cf. Exercise 21.15.) 
We shall resume the general theory of tests of significance in Chapter 26. 
NOTES AND REFERENCES 
For the use of, the ^-distribution in non-normal cases see Geary (19366) and Bartlett 
(1935a), the latter of whom shows that, for moderate samples, departures from meso- 
kurtosis are not very serious. For approximations to t in the normal case see 
(1936) and Hotelling and Frankel (1938). For approximations to the ^-distribution see 
Cochran (1940a), Cornish and Fisher (1937), and Paulson (1942). See also references to 
Chapter 23. 
For the further theory of the #2-test see Neyman and Pearson (1928, 1931a) and for 
another test of goodness of fit Neyman (1937a). The theory of 21.44 has been studied 
by a number of writers, notably by Andre (1884), Kermack and McKendrick (1936, 1937), 
and Wallis and Moore (1941). 
The amalgamation of tests given in 21.51 was apparently first given by Fisher in an 
EXERCISES 137 
early edition of Statistical Methods for Research Workers and was studied in detail by 
K. Pearson (19336) under the title of the Potest, and by E. S. Pearson (1938). 
For a test of significance of the difference of two variances hi samples from a bivariate 
normal population see Hirschfeld (1937), Finney (1938), Pitman (1939c), Morgan (1939), 
and De Lury (1938) ; and see Exercise 21.3. 
For the tests by Pitman, see his papers of 1937a, 1938. The similar problem in the 
testing of homogeneity in the analysis of variance has also been studied—see references 
to Chapters 23 and 24. 
For the test of difference of means when variances are unequal from the point of view 
of confidence intervals see Welch (19386) and the appendix to this paper by Miss Tanburn. 
EXERCISES 
21.1. For the population represented approximately by 
dF = _—-i 1 - ^ (3a - xz) t e~~~2 dx, 
V(2tt) { 6 J 
show that, if /eg is negligible, the joint probability of a sample differs from that 
if k3 is zero by a term 
K 
2. 6 , 
o / \™i) r t/Xp ( ~<.f jLu octj axx • • • ax^. 
By the transformation 
V 
-—-" • I iXj i JL » J 
V2 
1 
?/a = - , " (Xi + #2 - 2*3) 
1 
and the further transformation. 
yx = p sin <£w__;i sin </>„, . . . sin <f>x sin <£0 
?/2 = p sin </v-3 sin </>„_! . . • sin cf>± cos <j>0 
yz = p sin <£/?i_ 3 sin <£l/fc__, . . . cos cf>x 
show that the corrective term to the distribution of " Student's " t is 
and hence obtain equation (21.11). 
() ^^^+^P^-:^)exP|™^(l-f^jp^p 
(Geary, 19366.) 
21.2. By the polar transformation of the type of the previous exercise applied to 
all n variates show that if a random sample is drawn from a normal population with zero 
mean the frequency element may be written as 
1 
-pn"1 <r~&p2 dp d<f>0 sin 4>xd<f>x sin2 </>2 d<f>2 . • • sinri~2 </>n._2<^-2- 
(2nf 
138 COMMON TESTS OP SIGNIFICANCE 
Hence if w = —'—- 'where s2 is the sample variance, the distribution of w is independent 
ns 
of that of , Hence show that ta the distribution of », writing . _ A 
{T{\n + \)Y 2» ■ 
/£,- — - — - -———— -~ ———■— U, 
n2 
/^3 
^?{2nW + 3nW + a2 # } /?L±i 
w2 l * J n 
Hence show that for n = 50, Vi#i ^ — O'2^ a]a(l & = 3-10, indicating fairly rapid tendency 
to normality. 
(Geary, 1935a). 
21.3. Show that in samples from a normal bivariate population 
dF ex exp - ■—-— -- { ~ - ?^ + ^ ij <fo #, 
[ 2 (1 — p2) 101 OTjCTa O-oJJ 
the functions ^ = — + —, #/ = — — — 
(7i cr2 o*! cr2 
are distributed independently and that their correlation coefficient R may be written 
\/{[a + a)2 — 4aar2}' 
where a = ~|, a = ' 
and r is the correlation between the observed #'s and y's. Hence show that 
^ ^ jRVfa ~ 2) _ (a - a) V(^ ~ 2) 
~" V(l - ^R2) "" V{4 (1 - r2) aoc} 
is distributed as " Student's " t with n — 2 degrees of freedom. Show how to test the 
ratio a from this result. 
(Pitman, 1939c. The test lias the remarkable property of being independent of the 
parent correlation p.) 
21 A. If an even number n of members of a sample come from a population with 
mean ju, show how to find a sample of half the size distributed with twice the variance 
about zero mean. Hence show how to extend the result of Exercise 21.2 to the case where 
the population mean is not zero. 
21.5. If a parameter admits of a sufficient estimator, show that a test of its significance 
can be derived direct from the likelihood function. 
21.6. Derive equations (21.47) and (21.48). 
EXERCISES 139 
21.7. Let lll9 Zia . . . li)7l^i be (n — 1) linear functions of the observations which 
are orthogonal to one another and to xu and let them have zero mean and variance erf. 
Similarly define Zai . . . lXtn__2. 
Then, in two samples of n from normal populations with equal means and variances 
a\ and a\, the function 
Vn (#! — x2) 
will be distributed as " Student's " t with n — 1 degrees of freedom, 
(Bartlett, 1937c, and Welch, 19386. The test does not depend on the ratio a\/a\ and 
can be extended to the case of unequal sample numbers, but only at the expense of losing 
efficiency in the sense that the degrees of freedom number one less than the lower of the sample 
numbers.) 
21.8. Given two samples of n1} n2 members from normal populations with unequal 
variances, show that by picking n1 members at random from the n2 (where n% > nx) and 
pairing them at random with the members of the first sample, a test of significance of 
difference of means can be based on " Student's " distribution independently of the 
variance ratio in the populations. (This test, again, is exact, but sacrifices the information of 
n2 — nx members of the second sample.) 
21.9. If z is the ratio of the sample mean to sample standard deviation in normal 
samples, and n is large enough for the distribution of the variance to be regarded as normal, 
show that 
z 
V(z2 + 2) n v v ' V{ *2 + ^ (n -1) } 
is distributed approximately normally with zero mean and unit variance, where 
-jf"-* / f V 
,2 \2 I 3 
'" *' n r(n — 1\ 4:?i 32/?,2 
9 
(Hendricks, 1936.) 
21.10. If %, y have a continuous frequency function f (x, y), their charactferistic 
function is 
,00 _ 00 
<f> (u, v) = I I exp (iux 4~ ivy)f(x: y) dx dy. 
J -coJ -co 
Show that the distribution of x when y is given has a characteristic function 
<f> (u | y) 
«00 
J — 00 
/•OO 
00 
e-iw ^ (o, v) dv 
(Bartlett, 19386.) 
21.11. If a set of parameters Qx . . . 6p admit of a set of sufficient estimators, show 
that conditional inferences independent of 0t . . . 6P are possible, the conditions being 
142 REGRESSION 
22.3. We may also consider the more general curves typified by 
0 
— 00 
"Too" 
/ (X, y) dy 
J — 00 
. (22.4) 
the regression now being of the rth moment of y on x. If r = 1 we have the regression 
of the first moment, or simply the regression. If r = 2 and y is measured from the mean 
we have the so-called scedastic curve of y on x, 
-00 
y = p» -~' ~ 'j .... (22.5)' 
f(Xyy)dy 
J —00 
which shows how the variance of y varies with x. Other forms which have been studied 
are the clitic curve 
J (y -yx)3f(X,y)dy 
-* = -co • • • • (-2.6) 
f(X,y)dy 
J —oo 
and the kurtic curve 
(>/ -9x)*f(X,y)'dy 
• - "" (22.7) 
/ (X, y) dy 
I 
— 00 
These curves correspond to the moments of a univariate distribution, and the main 
characteristics of a bivariate form may be studied with their aid in much the same way 
as the lower moments can be used to summarise the properties of a univariate form. 
22.4. It is interesting to remark that, just as we can find the moments direct from 
the characteristic function, so also we may ascertain the regressions of moments from 
the bivariate characteristic function, even when the distribution function itself is not 
explicitly given. 
Let us write the frequency on in the form 
f{x,y)=g(z)gx(y), (22.8) 
where g (x) is the total frequency for any given x and gv (y) is the frequency of y for any 
given x. In the notation of the theory of probability we should write this 
f(*>y) =g(x)ff(y \%)- 
The characteristic function of x and y is then 
n'JQ v. 00 
<f> (h, t2) = exp {itx x + its y} g (x) gj: (y) dx dy 
J — 00 J — 00 
/•OO 
= e'Yl xg (x) fa (h)]dx (22.9) 
J —00 
/»CC 
where fa (tz) = &iUl'g* (y) dy .... (22.10) 
and is the e.f. of y for a given x. 
— 00 
THE ANALYTICAL THEORY OF REGRESSION 
143 
If the rth moment of y about the origin for a given x is /j,rx, we have 
% firx 
and hence, from (22.9), 
dti 
z. <i> (h, h) 
V 
<f>x (h) 
u=o 
u=o 
-co 
e%t*x g (x) firxdx 
J —00 
(22.11) 
Thus, by the Inversion Theorem, 
g (x) ftrx — 
(-jy 
2n 
/•CO 
J —00 
e" 
itj JC 
^ <£ <*l, «,) 
^1? 
/2 = 0 
(22.12) 
subject, of course, to conditions of existence. This gives us the required expression for 
ju'rx in terms of x, and the regression can be written down at once. 
22.5. Since 
<f> (h* h) = exp JT J Kjk ii 
we have 
j,fc = 0 
(ij£(jhP\ 
dt2 
% exp 
*a=o 
00 
Z1 ^° 
(ife)' 
L / = «) 
J! 
= ». ^ (^ 0) ^ icn M ■ 
Z K,1TT 
j 3=0 
J' 
00 
. (22.13) 
/ = o 
and cj) (tx, 0) may be written </> (tj), being the characteristic function of g (x). We also 
have, subject to existence conditions, 
Dig 
d> 
dxJ' 
• 9 (x) 
(-*)' 
m 
j /-CO 
- t{ e~i(^ <j> (h) dtv 
J — oo 
(22.14) 
Hence, from (22.12), (22.13) and (22.14) we find 
ATI J .„. oq 
dto 
<f> (h, h) 
dtx 
tt=a 
1 r 
s J.. --' *<'•>! {-" if 
dtx 
JT 4'-} (- £)' g (x) 
7=0 
(22.15) 
provided that the interchange of summation and integration in the last step is legitimate. 
Thus we have, for the regression of the mean, 
00 is ... 
y y ^yl. 
0 j ! 
. (22.16) 
£ = X 
(- 2>)' g (x) 
g{x) 
This notable result is due to Wicksell (19346). The expansion is valid if the cumulants 
exist and if g (x) and its derivatives are continuous in the range and zero at its extremes : 
for then the interchange of summation and integration in arriving at (22.15) is legitimate. 
In particular, if g (x) is normal and in standard measure we have 
y y Kjl 77 / y\ 
J ]- 
. (22.17) 
where Hj (x) is the Tchebycheff-Herrnite polynomial of order j (6.20, vol. I, p. 145). 
144 
REGRESSION 
Example 22.1 
For the bivariate normal distribution about the mean we have 
dF = & exp 1 — ■ --—r: ( -0- -— + -5 
dx dy, 
Hence 
<£ (*i, *a) = exp {- £ (orf if + 2poi <r2 ^ £a + c\ t\) }. 
and from (22.12) 
dt 
2 J 
ifa=»0 
g (x) !i\x = 
/•QO 
2tt% 
= - pa1a2t1 exp (— £<rf <f), 
/ocTi a% ^i exp { — Jof i\ — itx x) dtt 
~oo 
Hence 
iCs 
x e 2ff? 
and 
r 
0*1 
pG2 
or, 
-*C\> « 
the famihar relation of linearity for the regression of the mean of the normal distribution. 
Alternatively, direct from (22.17) we have, since kjx = 0, j> 1 
Y 
= Kqi + — Hx (X) 
Y = -- X, as before. 
Example 22.2 (Wicksell, 19346) 
Consider the frequency distribution of £ = -J--27 (a;2) and rj = |>T (t?/2) where a?, ?/ are 
samples of n from the bivariate normal population 
1 
dF oz exp — i? {a;2 — 2p#v/ + 2/2} d# dy. 
2(1— pl) 
The characteristic function is 
n 
I— /»CG 
(f> OC 
/■OO 
■00 J 
exptfe;2^ + y*d.i)dF 
■oo 
« 
(i -00(i -0.) -/>2M2 
l_sr 
where 0X = i^ and 02 = i£2. 
The distribution function cannot be expressed in a simple form, but we may determine 
the regressions without it. We have 
drJ> 
d6l 
02 = O 
2 
i*»l«m# '[ 
\lr}{! _(l_p2)fll}r 
(1 - Oj)in+r 
Thus, from (22.12) 
9 (f) M 
(Vl + r — i V e-°^ (1 - (1 - /o2) 0, V" 
2tc J_m (l-61)^+r 
FITTING OF CURVILINEAR REGRESSION LINES 
145 
The integrals may be evaluated by successive application of 
1 p er^dQ 
27rJ_oo(l -6)k r(k) 
and we find, for the regression of rj on |, 
n 2 / t n 
l— f^-1 e-* 
w 
Thus the regressions of both mean and variance of rj on £ are linear. 
Fitting of Curvilinear Regression Lines 
22.6. From thq practical point of view the case we have just considered, namely, 
the one where the distribution or characteristic function is given, is exceptional. The 
determination of regression curves has, in the majority of cases, to be carried out from 
numerically specified material, which we shall consider in the remainder of the chapter. 
We shall confine our attention to the regression of the mean. 
In general the means of arrays will not lie exactly on a smooth curve (unless of course 
we choose a curve of order equal to the number of points to be fitted, less one). Nor do 
we know a priori what is the appropriate degree of a polynomial which will 
approximately represent the regression line. Let us, however, assume that the regression can 
be represented by a polynomial of order p : 
J = a0 + ax X + aa X2 + . . . + av Xi}. . . (22.18) 
We will consider later how the appropriate value of p is to be determined in particular 
cases. Our problem is to determine the coefficients a from the data. As usual, we appeal 
to the principle of least squares, that is to say, we find the values of the a's which will 
minimise 
U — E (y — a0 — axx — . . . — aJ}xp)2, . . . (22.19) 
the summation extending over the sample values. 
Differentiating with respect to a>p we have 
and similar equations for j = 0, . . . p. Writing the moments without primes for 
simplicity and letting ^ represent the jth. moment of x, and f,in the bivariate moment 
2J (xj y), we have 
a{) /Wo + #i /Mi +- ... + ctJ} fj'p = fiox 
ao fti r Q>\ ftz ~t~ • * • ~r apft'p+{ === f'U 
do fA{) -f- Cti /ip-f-l ~f" • • ■ + Cip /,/<2 
p rzp 
ftpl. 
(22.20) 
Writing now 
A(p) ^ 
fto 
ftl 
ftp 
ftl 
/^2 
ftp + l 
• ■ ftp 
' ' ftp+i 
• • fttp 
A.S.—VOL. II. 
146 
EEGRESSION 
and A? for the determinant obtained by substituting the product-moments fk 
for the (j + l)th column, we have, as the solution of (22.20), 
u 
Af 
3 A{p) 
ft i*i 
22,22) 
22.7. » might appear that W. solution j*-, *™jf £- 0. «. » 
thing is not possible, however, except in the most trivial case, in law, n mil ami 
function of the a's is G (a;), we have for A{p) 
Aiv) 
JJ...J 
0C>c 
9 
it 
r2 
^1 
1/1/ 
,p 
0 
X 
,p-M 
# 
2p 
p 
W/LTO rfuTj . • • (&Uj 
or, if 
D 
1 #0 
1 Xi 
0 
P 
1 a; 
'p 
z 
,.p 
p 
j(p) 
1 I /y«0 ^y.1 rtn2 /v,P 
I 1 *t/Q X-j^ Xi2 . • • *Vjj 
* D dOa dGi 
• (i \ W , j • 
If we now permute the suffixes of them's in all possible ways and sum the (/> | I)! resultant* 
we obtain, in virtue of the definition of a determinant, 
(p + 1) ! zlM = f f f D2 d«0 dOx . . . rfG„, 
and hence A{p) is essentially positive. 
22.8. From (22.18) and (22.22) we see that the regression line may be written 
JL J- J.1. * • • .4. JL 
0 
1 ^ rfW * aft* * / 
/^pl /^/j flp+l • ' ' fhp 
This is a formal solution of our problem. The moments // can be obtained from observation* 
and equation (22.24) then gives the regression line. 
It will be observed that in order to preserve the symmetry we have written //„ for 
the total frequency unity. 
22.9. A somewhat different approach leads to the same solution. If we assume 
that the regression line is a parabolic curve of order p, we may find the coefficients by the 
principle of moments. This would lead us to identify the lower momenta 
E (xj y) = Exj (a0 + at x + . . . + ap xl>) 
as far as was necessary to determine the a's. This clearly leads back to equation (22.20). 
Orthogonal Polynomials 
22.10. The use of equation (22.24) in practice is subject to one serious drawback. 
If we have a set of data and no guide, apart from inspection, to the appropriate value of 
ORTHOGONAL POLYNOMIALS 147 
p, the only course is to fit curves of order 1, 2, 3, . . . and so forth, until we reach the point 
when further terms do not improve the fit. Every time we add a new term the determin- 
antal arithmetic has to be done afresh. To obviate this nuisance we shall consider the 
regression line in the form 
Y = b0 P0 + bx P± + . . .+bp Pp9 .... (22.25) 
where the P's are polynomials in X, Pj being of degree j. We shall determine the P's 
so that 
S (Pj Pk) = 0, j ** k . . . . (22.26) 
the summation extending over the observed values. 
In minimising 
Z(y-b0P0-~b1Pl . . . -bpPp)\ 
we shall have equations such as 
E (yPj) - 60 Z (Po P^ - • . • -bpZ(PpPj)=0, 
and in virtue of the orthogonal relations (22.26), this reduces to 
E (yPj) - bj E (P)) = 0. .... (22.27) 
Thus bj is determined simply by Pj ; and if, having fitted a curve of order p, we wish to 
go a step farther and add a term bp+t Pp+1, the coefficients &<,•■• f>p found from (22.27) 
remain unaltered. 
22.11. Furthermore, the use of these orthogonal polynomials will give us a very 
convenient method of determining step by step the goodness of fit of the regression line. 
We have 
V = E(y~baPn-. . .-bpPvY 
= E (t,*) - 26. E (2/P„) - ... - 2b,, E (yPp) + b* E {P\) + . . .+b*Z (P»). 
But from (22.27) we may express E {yPj) in terms of E (P'j), and we thus find 
U = E (ya) - bl E (PI) - ... - bl.Z (Pi). . . . (22.28) 
Thus the effect of any term btj Pj is to reduce U by l>] E (Pjf) and we may examine the effect 
of this term on U separately. If we find that the addition of any term bp Pp does not 
reduce U significantly, we may conclude that it is redundant (so far as concerns the 
representation of a regression line by a polynomial). 
22.12. We proceed then to derive expressions for the orthogonal polynomials in the 
general case. Later we shall examine the important special case when the values of x 
are equidistant (as, for instance, with grouped data and most time-series). 
Put 
p 
P.p = y cpj XJ. ..... (22.29) 
In this expression there are (p + 1) unknown constants c, and hence in all the polynomials 
up to and including those of the pth order there are J (p + 1) (p + 2) constants. The 
orthogonal relations up to and including order p will then provide \p (p + 1) conditions 
148 
REGRESSION 
on the c's, so that p + 1 constants are assignable at will. We will take one for each P and 
assign it so that the coefficient of X* in Pj is unity : 
0,j™ ' JL • • . • • • 
Iii particular c00 = P() = 1. The orthogonal relations are then just sufficient to determine 
the other c's. Eor instance, for the set cpp j = 0 . . . p — I, they are 
2j Jl ,vj x i === U 
and so on. This system is clearly equivalent to the p equations 
v 
01 
0 
Sxv"lPn = 0 
p 
On substituting for the P's from (22.29) we get 
cp, p-l M'p — l ~r /^/> 
= 0 
- 0 
(22.31) 
cpo H'p-x + Si y-p + • • • + cp,p~i fop-* + y-tp-i — o- 
The solution may be expressed as a determinant in the usual way. Writing A{p~~l) in 
accordance with (22.21) and A$ for the minor of the term in the last row and (j + l)th column 
in (22.21), we find 
cpj 
Mp) 
_ Pi 
. (22.32) 
This expresses the c's in terms of the ascertainable constants /,i. It follows that 
1 
p ^j(p-i) 
[12 
ftp-I /'p 
1 -A. 
M'P+i 
ll^p~{ 
X" 
We notice in particular that, in virtue of the diagonal symmetry of A{"\ we have 
ejk 
<■;»■ 
I *j —i.»j ~c) 
22.13, In virtue of (22.31) we have 
S (P*) = E (x" Pp) 
and thus, from (22.33) on multiplying the last row and summing, 
V /p2 
Pi) 
nA{p) 
2'(FT)' 
Similarly 
nA{p) 
(Z^j.t) O) 
(22.36) 
ORTHOGONAL POLYNOMIALS 
149 
Finally, from (22.27) 
A(P) 
b = p 
. (22.37) 
Our problem is now solved. We have expressed all the unknowns in terms of 
calculable determinants. 
We may note in passing that since the regression equation must remain covariant 
under a change of origin, all the coefficients b except 60 are semmvariant, and the origin 
can thus be chosen at will. 60 itself is the mean of the ^-values. 
22.14. Explicitly for the polynomials we have (taking fjLx = 0, ju2 == 1)— 
P0 = 1 (22.38) 
Pi 
1 0 
= Z 
(22.39) 
. 
L 0 1 
0 1 fiz 
1 Z Z2 
"T i o 
0 1 
X — jLt^X — 1 
(22.40) 
1 
r^4 r^3 
P. 
1 
0 
1 
1 
0 
1 
1 flz 
jt*3 H\ /is 
X Z2 Za 
i o i 
0 1 //3 
1 //3 /i4 
, \ (/*4 A*3 1) A (//-5 /i4 /i3 ^3) A" 
+ (/i3 /i5 fl^ + //4 ^3) X + (/^5 — 2^4 ^3 + ^3) I 
and so on. In particular, if the population is normal, 
. (22.41) 
px = 
JT 0 = 
^3 = 
= z 
= Z2 - 
= Z3 - 
- 1 
- 3Z, etc. 
the polynomials in this case reducing to the Tohebycheff-Hermite functions (6.20) which 
we know to form an orthogonal system in the normal case. 
Example 22.3. U?igrouped Data 
Table 22.1 shows the relationship between the percentage loss in weight (T) and the 
temperature (Z) in a number of samples of soil. We require to find the regression of Y on Z. 
150 
REGRESSION 
TABLE 22.1 
Fitting of Curvilinear Regression for Ungrouped Data 
(Data from J. R. H. Coutts, J. Agr. Sci., 20, 541.) 
Percentage Loss 
in Weight. 
Y 
3-71 
3-81 
3-86 
3-93 
3-96 
4-20 
4fc"OTC 
4-51 
4-73 
5-35 
5-74 
614 
6-51 
6-98 
7-44 
7-76 
Temperature 
(degrees F.). 
X 
100 
105 
110 
115 
121 
132 
! 144 
! 153 
! 163 
! 179 
I 191 
j 203 
1 212 
j 226 
237 
For the sums required we find— 
n = 16, Z (y) = 82-97, Z (y2) = 459-4363 ; 
Z(x) = 2642, Z{z*) = 474,050, Z (x3) = 91,244,582 ; 
Z(x*) =-- 18,553,164,842, Z (x5) = 3,930,294,225,302; 
Z (V) = 858,077,668,755,250 ; Z (yx) = 14,736-19 ; 
Z(yx*) = 2,819,909-45, Z (yx*) = 571,902,362-11. 
These can be rim off fairly quickly on a machine. We have not bothered to take a diiferent 
mean from those given, but in general a certain amount of arithmetic can be saved by 
so doing. 
Considering first of all the straightforward approach of (22.24), we have for the straight 
line of closest fit, 
7 1 X 
82-97 16 2642 
14,736-19 2642 474,050 
-0, 
reducing to 
Y = 0-660 + 2-741 
100 
. (22.42) 
We have put n^ instead of /^ in the second and third rows of the determinant, as we are 
clearly entitled to do. 
Similarly we find for the second- and third-order parabolas— 
J = 3-551 
0-929 l'™ 1 + 1-070 / X 
Y = 7-783 - 8-940 
\100 
.A 
loo 
/ ~ 5' 
875 
100 
100 
0-9189 
( 
—Y 
100/ 
. (22.43) 
. (22.44) 
ORTHOGONAL POLYNOMIALS 
151 
Fig. 22.1 shows the straight line and cubic fitted to the data by these means. An 
examination of the coefficients in the equations illustrates the point made above, that as successive 
terms are added to the polynomials the coefficients of all terms may alter very considerably. 
200 220 240 260 230 ZOO 220 240 260 
Temperature (degrees) 
Fig. 22.1.—Straight Line and Cubic Parabola of Closest Fit to the Data of Table 22.1. 
Consider now the alternative approach by the use of orthogonal polynomials. By 
the use of equations (22.33) we have 
Pi 
=: JL 
16 2642 
- 165-125. 
16 
P. 
16 
2642 
1 
2642 
474,050 
474,050 
91,244,582 
= X2 - 343-137X + 27,032-435. 
16 
2642 
2642 
474,050 
X n 
16 
2642 
474,050 
1 
2642 
474,050 
91,244,582 
X 
474,050 
91,244,582 
18,553,164,842 
X2 
91,244,582 
18,553,164,842 
3,930,294,225,302 
X3 
16 2642 474,050 
2642 474,050 91,244,582 
474,050 91,244,582 18,553,164,842 
= X3 - 522-940X2 + 87,182-434X - 4,605,047. 
The 6-coefficients are given by (22.37), the determinants in the numerator having been 
already tabulated in finding the P's. We have 
0-91889 
i. rior^ i 2-7409 , 1-0695 
6 = 5-1856, 6, = , 6, = , 
o > i 100 ' " 1002 ' 
6, - 
100; 
152 
EEGRESSION 
these being the values already found in arriving at (22.42) to (22.44). Thus 
7 = 
54856 + 2'?*?9 {X - 165-125) + ^£r (X2 - 343-137X + 27,032-4) 
100 
100' 
0-91889 
1003" 
(X» - 522-940Z2 + 87,182-4X - 4,605,047). 
(22.45) 
Tf we stop at the second term we have 
2-7409 w 
Y = 5-1856+ ±£z-(X 
100 
0-660 + 2-741 
165-125) 
100 
which is the same as (22.42), as of course it must be. Similarly, if we stop at the third or 
fourth terms we find equations (22.43) or (22.44). 
Now consider the fit of the regression line. We have from (22.35), 
b% Z (P%) = n b\ -£~ =bp S (YPP). 
The determinants in this expression have already been evaluated in finding the regression 
line. Kemembering that 2 (y2) = 459-436 we obtain the following :— 
0 
1 
2 
3 
5-1856 
2-7409 x 10-a 
1-0695 x 10-« 
- 0-91889 x 10-° 
Ad) 
430-247 
28-390 
0-069 
0-080 
U (equation (22.28) ) 
29189 
0-799 
0-130 
0050 
In calculations of this kind it is as well to take b$ to an extra place of decimals, as the value 
of U is rather sensitive to small errors of rounding up. Even so, the last figure in U is 
unreliable. 
From the values of U it is clear that the fit is greatly improved by taking a quadratic 
term, and still further improved by adding the cubic term. How far a quartic term would 
improve matters cannot be decided without ascertaining the term. We have, however, 
not proceeded beyond the third degree because to do so would require moments of the 
eighth order. For a small population such as this, which in practical applications would 
be considered as a sample only, the errors in higher moments would probably be considerable. 
The reader who works through the arithmetic of this example will find that there is 
about the same labour involved in either method. It is in the fitting of higher order terms 
that the method of orthogonal polynomials shows its superiority. In practical cases it 
is preferable to avoid the large numbers arising from the evaluation of determinants by 
a modification of the procedure given in 22.27 below. 
Example 22.4. Grouped Data 
In Example 14.1 (vol. I, p. 331) we considered the correlation between age and highest 
audible pitch in 3379 subjects and found the linear regressions. Let us take the work 
a stage further. 
STANDARD ERRORS OF REGRESSION COEFFICIENTS 153 
For the data of the table (Z = age, Y *= pitch) we find— 
708 ; Z (y2) = 8894 ; S (yx) = - 12,535 ; 
-T (a) = 2604 ; r (x2) = 47,392 ; S (x*) = 387,498 ; 
2 (x4) = 4,842,172 ; 2* (x5) = 62,401,794 ; E {x6) = 883,576,012. 
As a variation on the procedure of the previous example, we will convert these figures 
to moments about the mean (with Sheppard's corrections) and put them in standard measure. 
p01 = - 0-209,529 ; ju02 = 2-504,904 ; 
(ix = 0-770,642 ; ju, == 13-348,229. 
In standard measure the other moments are 
fiz = 1-705,375 ; /^ = 6-295,759 ; 
ju5 = 20-729,861 ; //6 = 78-409,775. 
We may now use equations (22.38), etc., direct, and find 
1\ = 1, Pt = Z, P2 = X2 - 1-705Z - 1, P3 = X3 - 3-471Z2 - 0-376Z + 3-560. 
We now require the moments ptn and /uzl. We find 
2(yx2) = - 112,495 
S(yxz) = - 1,399,639, 
and hence, with Sheppard's corrections and in standard measure, 
tuzi = - 1-177,920 //,31 = — 4-215,958. 
We now find, from (22.37), 
hx = - 0-613,626 
b2 = - 0-055,064 
63 = 0-010,205. 
The regression line of the third degree is then 
Y - - 0-6136X - 0-0551 (Z2 - 1-705Z - 1) + 0-0102 (Z3 - 3-471Z2 - 0-376Z + 3-560), 
where the origin is at the mean and the units are in standard measure. 
Standard Errors of Regression Coefficients 
22.15. The standard errors of unknowns derived from least squares can be found 
by the use of a result clue originally to Gauss. Suppose ocj is the true value of a^ and the 
residuals y Eai,fcj are distributed normally with variance v. Writing das = ocj — ap 
we have for the frequency function of the residuals— 
oc exp - i-p (y-2 <h d\2 + Z 2 (daj xr 
154 REGRESSION 
(E denoting summation over the sample and E over the values a0 to ap, and the cross- 
\ s j 
term vanishing because the a's are minimal values) ; 
1 
cc constant X exp — ~■ E E (da3- x'J)' 
1 
oc exp E E (dctj dak xi+k) 
2>V s j,k 
oc exp —— E {da,j ddjc ftj+k)- ..... (22.46) 
In the limit, then, the deviations are distributed in the bivariate normal form, and from 
the results of 15.12 (vol. I, p. 376) it follows that 
A(P) v 
vara, = tL*L i!, (22.47) 
3 A^n 
for the determinant whose terms are jnj+k is in fact the determinant we have already defined 
as A{p\ and A$ is the minor of the item in the jth row and column. 
Now v is the variance of deviations from the theoretical regression line, and in terms 
of variations about the observed line we have, remembering the result of 18.17— 
vara. = ^t . var e (22.48) 
jft» n-p-1 
Since the correlation ratio of y on x is given by 
var e = var y (1 —- ??2), 
we have also * 
A{}-] (i — n2) var?y too aq\ 
var a; = 11 • .... (22.49) 
For large samples the replacement of n by n — p — 1 in the denominator is an unnecessary 
refinement. 
22.16. For the case of orthogonal polynomials the results apply with a slight but 
important simplification. The coefficient bj is the same as a-} if polynomials up to order j 
only are fitted, and hence, since A$ = Aij~~l) we have 
Au"l) (1 — f]2) var y 
"375T n — j __ i 
The same result follows by modifying (22.46), which for orthogonal polynomials becomes 
/ oc exp ~~ 1^ (27PJ (dbj)*}, .... (22.51) 
2'y j \ 8 j 
showing that the 6's are independently and normally distributed with variance 
var 6,=^^ -'//*"/ (22>50) 
/ — v 
var Oj — yrnv 
reducing to (22.50) in virtue of (22.35). 
22.17. If the parent population is normal, r\ = p, and the determinants A{j) can be 
evaluated explicitly in terms of the variance of x. In fact, 
A® j ! (varx)*5 
. (22.52) 
STANDARD ERRORS OF REGRESSION COEFFICIENTS 
155 
and hence 
var b, 
(1 — p2) var y 
n 
or, in standard measure, 
var bj = 
j — 1 j ! (var x)j 
1 1 — p2 
% 
(22.53) 
(22.54) 
Equation (22.52) can be found by evaluating the determinants in the ordinary way, but 
A{j) 1 
it follows more simply from the consideration that Aij^l) is equal to - S P?, which, in the 
normal case, is for large samples equal to E (P|) = j! (var x)j (6.22. vol. I, p. 147, with 
a change of scale). 
22.18. The advantages of using orthogonal polynomials instead of powers of X 
are apparent in the forms taken by the standard errors of the coefficients a and 6. The 
latter are independent of the order of the polynomial fitted and can be tested once and for 
all. The former do not possess this advantage. It seems preferable, therefore, as a matter 
of technique, to work with orthogonal polynomials throughout, whenever regressions of 
order higher than the first are likely to require investigation. 
Example 22.5 
Consider again the data of Example 22.4 (regression of highest audible pitch on age). 
We have there expressed the regression line in standard measure and in the orthogonal 
form, and may therefore use equation (22.50) in the form 
i - yi*am 
v wu. \j i - 
var b2 = 
var b3 = 
n A{1) 
1 - rj*AM 
n AM 
1 - rj*AM 
n A^' 
(The sample number n is so large that we can ignore the element — (j + 1) in the divisor.) 
The determinants required are already known, having been ascertained in the course of 
the work. We have 
AM 
2J7T) 
= 1 
AM 
AM 
0-4189. 
AM 
AM 
0-0985. 
We also require 77, which was found in Example 14.11 (vol. I, p. 352) to be rjyx = 0-6231. 
Thus 1 — ?72 : 
0-6117. We find 
, 1-8104 
var 6, — 
var bo = 
0-7584 
var bo. — 
10* ' " 10* 
The values of the 6's and their standard errors are then 
0-1783 
104 
b. 
0-6136 
0-0551 
0-0102 
Standard Error. 
0-013 
0-0087 
0-0042 
156 REGRESSION 
In all cases we should judge the coefficients significant, as being more than twice the standard 
error. Although, therefore, the second- and third-order terms are small and the regression 
is approximately linear, the deviation from linearity is not merely a chance effect. 
Exact Significance Tests in the Normal Case 
22.19. When the parent population is normal, more exact tests than those derived 
from the use of standard errors may be obtained. We have already seen (14,21, vol. I, 
p. 348) that a function dependent only on sample values and the first regression coefficient 
bx was distributed in " Student's " form. We proceed to generalise this result, 
Consider in the first place the linear regression equation 
Y = y + bx (X - x), (22.55) 
and let ft be the population value of b± and a\ the variance of y in the population. Since 
the parent is normal, the variance of y for any fixed value of x is o?>. 
Our estimate of bt is 
v = ^lJ^LZ^)3 (22.56) 
1 Z{x-xY' 
where summation takes place over the sample values. Thm for fixed values qfx we have 
S (x — x)2 var y 
var &! = 
{Z(x -£)2}2 
-2 
2 /*>0 r*'", \ 
—: ..... (^.Oi/ 
2 (x — xy 
Thus, since the mean of the distribution of bx is /?1? we see that, for samples having the 
same #'s as those observed, bx is normally distributed about mean ftx with varianee given 
by (22.57)—normally because it is a linear function of the y's which are themselves normal. 
Consequently, 
(&i -/?i) <s/Z{x-xy /oo«uv 
. _.—„— m ^ ^ __ . ^ *j *«(,«; o ^ 
is distributed normally about zero mean with unit variance. 
If orj were known this would provide a test of significance of bx in the ordinary way ; 
but in fact o2 is not known and the substitution of an estimate distributed in the Type ill 
form brings in the ^-distribution in the usual way. We take as our estimator of at the 
function s, where 
*2 = ^~~2 S {V ~~ T)2) .... (22.59) 
amd 7' represents the values " predicted " by the regression line, that is, the values 
Y' = y - bx {x - x) (22.60) 
Thus s2 is based on the sum of squares of residuals. We shall show presently that *2 is 
distributed in the Type III form with n - 2 degrees of freedom independent!v' of h, - /J,. 
It follows that 
t = (&i - Pi) V^ (x - xY V(n~- 2) 
Wi^Yr 
is distributed as " Student's " t with v = n — 2, 
A given value ft may be tested accordingly. But we notice that the inference is a 
conditional one, that is to say, we are considering the distribution of * in a sub-population 
tor which the x s are the same as those actually observed. (Cf. 21.47.) 
EXACT SIGNIFICANCE TESTS IN NORMAL CASE 157 
22.20. To establish the foregoing result we have to show that 2 (y — Y')2, the sum 
of squares of residuals about the observed regression line, is distributed in the Type III 
form with n — 2 degrees of freedom. This is a particular case of a general theorem we 
ahall prove at the beginning of the next chapter, but we will sketch an ad hoc proof here 
for the sake of completeness. 
Since the population is normal, the deviations of y from the true regression line for 
fixed a?'s, Y = /}0 + fi± (X — x), where p0 is the parent mean of y, is normal with variance 
ci. Now 
(n - 2)sl2=JLz(y - Y'Y =±X'{y - 6. - 6X (x-S) }» 
Ml 
The coefficients b0 and 6X were chosen so as to minimise this sum, and hence 
{n __ 2) l! = \z{y - /?. - & (a - x) } 2 - 4(60 - ^0)^ - (6l~/l)2^(a; - x)\ (22.61) 
The first term is the sum of squares of n normal variates with zero mean and unit variance ; 
the second is also such a variate, for it is the square of the deviation of the mean of y about 
its true value divided by the variance a\/n; and the third term is also such a variate, as 
-shown above. 
(n 2) «s2 
It does not follow immediately that ——-0-;—• is distributed as the sum of squares of 
al 
n — 2 normal variates in standard measure, for the constituent items might be correlated. 
Let us then find an orthogonal transformation to new variates £x . . . £n linearly related 
to the n normal variates y — /S0 — /?,. (x — x). These also will be normally and 
independently distributed. In particular (remembering that our summations refer to the 
v/s and #\s, but the latter are constant for our distributions), take 
£x = —L- 2{y -fa- Pi 0* - x)} 
= ^ (60 ™ /3o) 
fa 
<T2 
Is 
VJL (a; — x)" 
1 
= -.:.- (b1 — jSx) V-^ (# — #)2- 
£t and £a are then normal variates in standard measure. Moreover they are orthogonal since 
yt a — l y _ 
<?2 V^ - V^ (# ~" X) 
= /£ £j \X X) 
= 0. 
n 
Consequently our transformation exhibits the first term on the right in (22.61) as £ % ami 
n 
the second and third as ff and £\. Thus the total is distributed as JT f *>which is the 
result required. 
158 BEGRESSION 
We may compare the result of 18.17—in which we saw that the mean value of £2 
was n, whereas that of e2 was n — p — 1, one degree of freedom having been lost in the 
sum of squares of residuals for every constant estimated—and the approximate result of 
21,20 in which %2 had to lose a degree for each constant fitted by maximum likelihood. 
Fundamentally all these results are different aspects of the same thing and rest on the fact 
that the variation of the sum of squares of normal variates in standard measure is spherically 
symmetric, so that a hyperplane in the sample space " cuts " the distribution in a 
spherically symmetric form of one lower degree of freedom. 
Extension to Curvilinear Regression 
22.21. The foregoing result can be extended without difficulty to the case when 
the regression is curvilinear. If 
Y = b0 •* o + &i Pi + . - . + bv Pp, 
where the P's are orthogonal, then 
h- = ^3 i • 
i y pa > 
and we have also, for the variance of bj when the x's are fixed, 
ft 
var bj = jrTps!? 
so that 
is distributed normally with zero mean and unit variance. Taking as our estimate of a\ 
*2 = —\—-rfe- Y')\ 
n -j - 1 
(6, - fa) V (n -j - 1) VZPf m fi2v 
is distributed as " Student's " t with v — n — j — 1 degrees of freedom. 
It will be observed that in this and the previous section we have not assumed anything 
about the distribution in a;-arrays. We have merely supposed that for any given x, y is 
normally distributed with constant variance. 
Example 22.6 
Consider again the soil data of Example 22.3. We found, for the cubic term in the 
parabola, a coefficient of — 0-9189 x 10~"6. Is this significant ? 
Here fy - fa = - 0-9189 x 10~6 for j = 3 ; 
Vfa —j - 1) = Vi1® - 4) = 3-464. 
We have already found Z (y —• Y')2 = U, namely 
U = 0-050. 
We further require X P? which has been obtained incidentally in the working of Example 
22.3 and is equal to 9-31525 X 1010. Hence 
- t = °'9189 x 1Q~6 (3'464) 3-052 X 105 
" 0-2236 
= 4-3. 
This is highly significant. 
we see, as before, that 
CASE OF VABIATE WITH EQUIDISTANT VALUES 159 
Case when the Independent Variate proceeds by Equal Steps 
22.22. An important special case arises when the independent variate has values 
which are equidistant, as, for instance, in most time-series and in grouped data. If we take 
the interval between successive values of x as our unit, the variate-values may, by a 
suitable choice of origin, be taken as 0, 1, 2, ... n — 1. The various moment-functions 
fXj entering into the expressions for polynomials, etc., may be written down once for 
all. Furthermore, this case lends itself to simpler summatory methods of forming the 
actual polynomial values and the residuals. 
22.23. For a set of values 0, 1, 2, ... n — 1, we have 
6 
2 (*■) = ^Zl!)2, etc. 
v ; 4 
Thus— ptx = \ (n — 1), fj,2 = —-r—, ^3 = 0, etc. 
iz 
From (22.38) and similar equations we then find 
n — 1 
jL J = JL 
2 
> . (22.63) 
J 
p -A- /^2 ^'3 /f2 P2 ^ 
and so on. The polynomials may be obtained more systematically as follows :— 
We show first of all that 
" ' n - 1 \ A* 
J" ( U ■ ) A—-Pp = °> q = l, 2, . . . p, . . (22.64) 
fri V J ) <i+3 
where ZP is the ^"th terminal difference of Pp and the x9s range from 0 to n — 1. In fact, 
from Newton's interpolation formula, 
Pv = > ~ A*PP; (22.65) 
and since the P's are orthogonal, 
£{x + g - 1)[«-1] Pp = 0, g <p. . . . (22.66) 
X 
Substituting from (22.65), we find for the term in Ai Pp— 
3 '• * (i + ?) J 
(;' +1) j i 
Thus for all 5 from 1 to p we have 
= (n + j _ i)fe+fl ^iL p 
j' 
# 
_(» + ?-l)! „/»-l\ ^ p 
whence follows (22.64). We now find functions obeying these conditions. 
Consider 
y = G (x + p)[p] (22.67) 
160 
REGRESSION 
This is a polynomial of degree p, and if for # = 0, 1, . . . p it assumes the values y0, . . . yv 
we have— 
for this also is of degree p and has the right values at x = 0, . . . p. Taking now 
. (22.68) 
(w-j~l).! 
(22.69) 
we find that for x = — q 
y 
(-«)=C(P + q)^H-iy]r.lV^i 
Vi 
. (22.70) 
Now from the definition of y this clearly vanishes for — x — q = 1, . . . p, and thus 
(22.70) is zero. Comparing it with (22.64) we see that the conditions are satisfied if we 
give to y$ the value of ZP of (22.69), i.e. 
(n~j-1)1 
p (ft-i)i(p-i)! 
(-ip-'yj 
= c^n ~~i ~ L)! (^ +J)! 
(-i)p-i. 
. (22.71) 
(n - 1)! (p -j)! j! 
The constant O is evaluated by the fact that the coefficient of Xp in Pp is unity, giving 
Av Pv = p ! This gives 
(7 
(p !)2 (n - 1) ! 
(2jp) ! {n - p - 1) ! 
Finally, substituting in (22.65), we find 
(22.72) 
v,-j; (-!>*-' 
(jpl)MP+i)I (w-j-l)! 
Ar(Z - 1) . . . {X ~j + 1), (22.73) 
i«o 
(2p)I(j!)*CP-i)l (n-p-1)! 
where by convention the term Zcn is unity for j = 0. The first six polynomials are 
?fc — 1 
P1==X 
P — P2 
P3 = P| - 
P p4 
-r 4 — J- i ~ 
p p5 
■* 5 — ■* i — 
p* = p? - 
jt ^ 
2 
n2 — 1 
12"" 
3ti2 - 7 
3ti2 -^13 
5 (ti2 - 7) 
P{ + 
3 (n1 - 1) {n* - 9) 
_ _ __ 
15™4 - 230n2 +407 
1008~"^ ~ 
P. 
5 (3^2 - 31) 5ti4 - llO^2 + 329 m 
44 
176 
5 (?_! ~ 1) (^2 ~ 9) (wl_-_25) 
14784 
(22.74) 
CASE OF VARIATE WITH EQUIDISTANT VALUES 161 
Four more values are given by Allan (1930), to whom the above derivation of (22.73) 
is due. 
Values of the polynomials up to and including the fifth are given in Fisher and Yates' 
.Statistical Tables up to n = 52. 
22.24. We can now find an explicit expression for £ P^. Since the polynomials 
are orthogonal we have 
which, by the argument resulting in (22.64), leads to 
rp2== y< (n + V) 1 A* p 
* feJHn-j -1)1 V+J + l *' 
Putting q = p + 1 in (22.67) and (22.70), we have 
y(-ff) -0(- !)<« = (- D»(2p+ l)*-"^" 7 1)F-j^n>,; 
whence, after a little rearrangement, 
£ (^ + j>) ! ^fg ^ (P I)2 fo +j>) f- £ . 
^ ! (^ — j — 1) ! p + i + 1 (2p ■+- 1) - {n — 1) - 
and thus, substituting for O from (22.72), we find 
X 2 PI = ■ v t(^1)4 % (wa - 1) ... {n* ~~ ©*). . . (22.75) 
p {2p) 1 (2p + 1) ! v ' v ^ ; v ; 
22.25. It is also possible to express the orthogonal polynomials in terms of central 
differences. We quote without proof the results (for details of which see Allan, 1930) :— 
The series is summed from j = 0 until 2j > p, when the denominator vanishes and (p — |) ! 
is written for T (# + |) to preserve the factorial notation. In practice the polynomials 
for particular examples are not determined from (22.73) or (22.76) but by the use of tables, 
or by summation from differences in the manner of Example 22.9 below. 
Example 22.7 
For the fitting of a regression line in the case of equidistant intervals various methods 
are in use. A choice between them depends on the length of the series, the order of 
regression to which it is desired to go, and the computing resources at the investigator's disposal. 
We will illustrate two methods in this and the next example. 
a.s.—vol. ii. m 
162 
RE GRE S S ION 
TABLE 22.2 
Fitting of Regression Line by Orthogonal Polynomials—Equidistant x-intervals. 
(1) 
x ©£ir. 
1811 . . 
1821 . . 
1831 . . 
1841 . . 
1851 . . 
1861 . . 
1871 . . 
1881 . . 
1891 . . 
1901 •. . 
1911 . . 
1921 . . 
1931 . . 
(2) 
Variate. 
Pi 
- 6 
- 5 
- 4 
-3 
- 2 
- 1 
0 
1 
2 
3 
4 
5 
6 
(3) 
Population 
(million). 
Y 
10-16 
12-00 
13-90 
15-91 
17-93 
20-07 
22-71 
25-97 
29-00 
32-53 
3607 
37-89 
39-95 
(4) 
J°2 
■ 22 
11 
2 
- 5 
- 10 
- 13 
- 14 
- 13 
- 10 
- 5 
2 
11 
22 
(5) 
IPs 
- 11 
0 
6 
8 
7 
4 
0 
- 4 
- 7 
- 8 
- 6 
0 
11 
(6) 
7 p 
1 iix 4 
99 
- 66 
- 96 
- 54 
11 
64 
84 
64 
11 
- 54 
- 96 
- Q6 
99 
In Table 22.2, column 3 shows the population of England and Wales (in millions) 
for the years shown in column 1. These are at ten-yearly intervals, and the variate-values 
in units of 10 with origin at the mid-point of the range are given in column (2). These 
are the values of Px. 
The corresponding values of P2, P3 and P4 are given in the last three columns. They 
may be calculated direct from (22.74), but are most conveniently taken direct from the 
Fisher-Yates tables. 
We find, for n — 13, 
X YPt = 474-77 
X 7P2 = 12319 
X YPZ = - 39-38 x. 6 = - 236-28 
X 7P4 = - 374-30 x -V2- = - 641-657,143, 
and, direct from the tables, 
XP\ = 182, XPl = 2002, ZP\ = 572 x 36, 
i 
XP\ = 68,068 x (\2-)2. 
Z YP- 
Hence, from equations of the type ft. = ^ ^J, we find 
ZP] 
bx = 2-608,626, 62 = 0-061,533,467, b3 — - 0-011,474,359, bt = - 0-003,207,699 
and the quartic curve is 
Y - 24-1608 = 2-6086X + 0-061,53 (X2 - 14) - 0-011,47 (Xs - 25X) 
We can now find the residuals for each term in this equation. We find 
Z 72 = 8839-9389 
E Y = 314-09. 
. (22.78) 
CASE OF VARIATE WITH EQUIDISTANT VALUES 
163 
Hence the sum of squares of Y about the mean of Y, 
Z(Y — T)2 = 1251-283. 
Thus we have :— 
Residual Sum of Squares. 
Original variation. 
Contribution of first term = bx £ (YPX). 
Contrilxition of second term = b2 £ (YP2) . 
Contribution of third term = 63 £ (YP3)~ . 
Contribution of fourth term = 64 £ (YPA) . 
1251-283 
1238-497 
7-580 
2-711 
2-058 
• * • 
12-786 
5-206 
2-495 
0437 
For the variance of the residual elements we divide by the number of degrees of freedom 
(n — j — 1) and obtain 
Residual Sum of Squares. 
12-786 
5-206 
2-495 
0-437 
Residual Variance. 
1-162 
0-521 
0-277 
0-055 
Fig. 22.2 shows the data graphically with the cubic and quartic of closest fit. 
1821 1841 1861 1881 1901 1921 
Years 
Fig. "22.2.—Cubic (full line) and Quartic (broken line) Parabolas fitted to the Data of Table 22.2. 
The fit is evidently a good one, as is borne out by the smallness of the residual variance, 
but we must sound a warning as to the use of this polynomial. For interpolation in the 
variate range it would probably suit very well; but for extrapolation outside the range 
it is dangerous unless there is good reason to suppose that the polynomial has some theoretical 
basis (which is not so). It would, for instance, be most unsafe to try and estimate the 
population in 1960 by inserting X = 9 in equation (22.78). 
164 
REGRESSION 
Example 22.8 
In Chapter 3 it was seen that factorial moments can be derived by summatory 
processes. A somewhat similar method can be used to fit orthogonal polynomials. We will 
illustrate it on the data of the previous example. 
TABLE 22.3 
Fitting of Orthogonal Polynomials by Factorial Sums. 
S0 
10-16 
12-00 
13-90 
15-91 
17-93 
20-07 
22-71 
25-97 
29-00 
32-53 
36-07 
37*89 
39-95 
314-09 
Si 
10-16 
22-16 
36-06 
51-97 
69-90 
89-97 
112-68 
138-65 
167-65 
200-18 
236-25 
274-14 
314-09 
1723-86 
S* 
10-16 
32-32 
68-38 
120-35 
190-25 
280-22 
392-90 
531-55 
699-20 
899-38 
1135-63 
1409-77 
1723-86 
7493*97 
St 
10-16 
42-48 
110-86 
231-21 
421-46 
701-68 
1094-58 
1626-13 
2325-33 
3224-71 
4360-34 
5770-11 
7493-97 
In Table 22.3 the column headed S0 gives the value of Y. The next column, headed 
$1? gives the sums of the values in the first column proceeding from the top ; and so for 
the columns headed S2 and S3. 
Now construct the quantities 
a0 = ^S0 = ^^ = 24-160,769 
a, = 
2! a 2(1723*86) -.^ ^^ ,-ir» 
8X = __L___../ = 18-943,516 
n (n + 1) 
182 
3! 
a2 = _ __ $2 = ^ili9l,97) = 16-470,264 
n(n+l){n + 2) 2730 
the general formula being 
a^ 
(j + 1) ! S. 
Then obtain the quantities 
a{ 
a( 
a. 
n (n + 1) . . . (n + j)' 
24-160,769 
ax = 5-217,253 
. (22.79) 
a2 = a0 — 3ax + 2aa = 0-270,749. 
the general formula being 
a, 
p 
a. 
P (P + 1) „ , (P-1)(P)(P + l)(p + 2) 
(1 !)22 
#i -f~ 
(2!)2 3 
a. 
. (22.80) 
CASE OF VARIATE WITH EQUIDISTANT VALUES 
165 
Finally put 
60 = 
61 = 
6. = 
a 
0 
24-160,769 
6 
626 
n 
_ai = 6J^^) = 2-608, 
1 * 12 
= SOJO.270/749) = 
132 
a, 
p 
. (22.81) 
(w - l)(n~ 2) 2 
the general formula being 
b ^(2p + 1)! 
p [p\)2 (n - 1) . . . (n -p) 
Then the 6's are the coefficients of the orthogonal polynomials in the regression equation. 
The values we have found check with those of the previous example and the reader may 
care to work out b3 and 64 by the same method. 
This process is due to R. A. Fisher and avoids the direct calculation of the values of 
the orthogonal polynomials. Its validity may be established by using equations (22.75) 
and (22.73), which give 
1 __ZyPp _ (2p\){2p + 1) ! 
(p !)* n (n2 — 1) . . . (n2 
fp 
y p2 
^ x P 
P2) 
2 (y Pp) 
{2p + l) ! 
Z 
(_l)p-> {p+j) ! (n-j-l) ! (j+1) Eyx . . . (x-j+1) 
(p !)2 (n-1) . . . (n-p) j (j !)2 (p-j) ! (j + 1) (n-p-l)l n . . . (n+p) 
The first part of the expression explains the coefficients in (22.81), the second part those 
in (22.80). The third part gives rise to (22.79) when it is remembered that the sums S 
are expressible as sums of factorials (cf. 3.10, vol. I, p. 58), but the summation takes place 
from the top of the column. 
Example 22.9 
As a rule it is unnecessary to evaluate the polynomial at all the points for which data 
are given ; but if the values are desired for comparison with observation they may be 
obtained by summatory processes from the differences. 
The terminal differences themselves are obtainable simply from the quantities ap of 
the previous example. For a polynomial of the first degree we have 
6 
AY = - 
For that of the second degree, 
A2 Y 
n — 1 
Y = a'(] + 3a i- 
60 
> 
(22.82) 
(n — 1) (n 
___ 6 
a2 
n 
- 2) 
For the third degree, 
Y = a'{) + Sal + ^a2- 
- 840 
(22.83) 
—■-. CV'i 
A2 Y = 
AY = 
(n - 1) (n - 2) (n - 3) 
60 
(n 
\){n 
6 
2) 
\CL% -J~ iCt-^j 
n — 1 
(a'l + 5a2 + 14a3) 
Y — aQ + 3a{ + 5a'2 + 7a3. 
(22.84) 
166 
REGRESSION 
The formulae for higher degrees are constructed on analogous lines, the multiplying 
factors for successive differences being given by 
(• 
and the coefficients of the 
r 
AY 
A*Y 
A*Y 
A*Y 
A5 Y 
jv^Gp + i)Cp + 
(n — 1) (n — 
a's by 
1 3 
1 
5 
5 
1 
2) • 
- 2) . 
7 
14 
7 
1 
. . (2p + 1) 
9 11 
30 55 
• 27 77 
i/ j! JC 
1 11 
1 
etc. 
We leave the proof of these results to the reader. 
For instance, for the data considered in the two previous examples we found, for the 
parabola of the second degree, 
Y = 24-160,8 + 2-608,6X + 0-061,533 (X2 - 14) 
a 
o 
24-160,769 ; a[ = 5-217,253 ; a2 = 0-270,749. 
Hence, from (22.83), 
A*Y = 
AY 
60 
(n 
l)(n 
6 
-2) 
(aj + 5a2) 
a2 = 0-123,068 
3-285,499 
% -~ 1 
7 = a0 + 3a[ + 5a2 = 41*166,273. 
We then build up the polynomial values as shown in Table 22.4. The second difference 
0-123,068 is shown at the foot of column (2). Being a constant, it could have been written 
TABLE 22.4 
Calculation of Polynomial Values from Differences. 
(1) 
Number of 
Term. 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
(2) 
Second 
Difference. 
0123,068 
(3) 
First 
Difference. 
- 1-808,68 
- 1-931,75 
- 2-054,82 
- 2-177,88 
- 2-300,95 
- 2-424,02 
- 2-547,09 
- 2-670,16 
- 2-793,23 
- 2-916,29 
- 3-039,36 
- 3-162,43 
- 3-285,499 
(4) 
Polynomial 
Value. 
9-863 
11-795 
13-849 
16-027 
18-328 
20-752 
23*299 
25-969 
28-763 
31-679 
34-718 
37-881 
41-166,27 
(5) 
Observed 
Value. 
10-16 
12-00 
13-90 
15-91 
17-93 
20-07 
22-71 
25-97 
29-00 
32-53 
36-07 
37-89 
39-95 
(6) 
Difference 
(5)-(4) 
0-297 
0-205 
0-051 
- 0-117 
- 0-398 
- 0-682 
- 0-589 
0-001 
0-237 
0-851 
1-352 
0009 
- 1-216 
all the way up, but to do so is a waste of time (and in practice, of course, we should not 
devote a separate column to it). The first difference is shown at the foot of column (3), 
MULTIPLE CURVILINEAE REGRESSION 167 
and the figures above it constructed by adding the second difference at each stage. The 
polynomial values themselves are compiled by adding the first differences to the value 
at the foot of the column, 41-166,27. 
We have also shown the observed values and the difference between polynomial and 
observed values. The sum of squares of the latter is 5-204, agreeing within the margin 
of rouncling-up error with the value for the sum of squares of residuals* found in 
Example 22.7. 
As an exercise the reader should work out the polynomial values for the third- and 
fourth-order polynomials and compare the sum of squares of residuals with the values of 
Example 22.7. 
Multiple Curvilinear Regression 
22.26. We considered the linear regression of one variate on a number of others 
in Chapters 14 and 15. There now remains the extension of our results to the 
curvilinear case. 
The extension is very easy to carry out when we remember that in multiple linear 
regression there is no restriction on the degree of dependence among the " independent" 
variates. In particular, some of them may be functionally related, and more particularly 
still, one variate may be a power of another. It is thus clear that the process of fitting 
curved regression lines can be regarded as formally equivalent to that of fitting linear 
regressions. For instance, the fitting of 
Y = $o ~\~ &i Xx H- <x2 a2 -j- &3 JC3 -f* ^4 -^-4 ~t~ ®5 As 
is equivalent to 
the latter being a particular case of the former where X2 is the square of Xx (and their 
covariation accordingly complete) and similar relations exist between X3, X4 and Z5. 
The case of curvilinear regression for a single variate, which has occupied the 
foregoing part of the chapter, could then have been treated by the methods of Chapter 15. 
We have discussed it afresh only because it is more easily dealt with by direct methods. 
22.27. In multiple regression analysis it sometimes happens that, having worked out 
a regression equation, we wish either to take account of a new factor or to remove one 
which appears redundant. To avoid the necessity of solving a new set of determinantal 
equations the following device is useful:— 
Consider the case of three independent variates measured from their mean 
Y = b1X1 + biX,+bzX5 (22.85) 
In accordance with our general method the constants b are given by 
6X E (x[) + b% E (xx x%) + b^E {xx x3) = E (xt y) 1 
lh E (x, x,) + 62 E (xl) + bzE (x2 x5) = E (x2 y) \ . . (22.86) 
lh E (xt x,) + b,E (x, x3) +b,E (xl) = E (x3 y) J 
Suppose now we replace the functions E (xy) on the right by 1, 0, 0 and obtain the solutions 
il'=c11, 6a = c12, 6a^c13; and similarly for replacement by 0,1,0 and 0,0,1, 
the solutions being written 
b± = cxl, c12, c13 
b2 = cia, c22, c23> (22.87) 
03 = C13, C23, C33^ 
168 
REGRESSION 
. (22.88) 
Then the solution of (21.86) is 
bx = cxl E [x± y) + cla E (x2 y) + c18 E (re8 2/)' 
b2 == c12 Z (a?! ?/) + c22 17 (x2 y) + c23 X (x, y) \ . 
63 = c13 ^ (»x ?/) + c23 r (<ea y) + c33 ^ (a8 2/) . 
as is immediately evident on substitution. The values of the c's are those we have denoted 
earlier in the chapter by determinantal forms, e.g. cjk = A{$!/A{p). 
22.28. Now suppose that we wish to discard the variate zz. Prom (22.86), with 
1, 0, 0 written on the right, we find 
c12 — 
1 
A 
(11) 
(12) 
(13) 
(13) 
(23) 
(33) 
where (jk) stands for E (Xa xk), and 
A - 
(11) 
(12) 
(13) 
(12) 
(22) 
(23) 
1 
0 
0 
(13) 
(23) 
(33) 
. (22.89) 
. (22.90) 
There are similar expressions for the other c's. If the values of the constants when x3 
is removed are cu, c'12, c'22 we shall have 
where 
Now we have 
1 
2P 
(12) 
(22) 
1 
0 
A' = 
°Vl-Ar 
(11) 
(12) 
1 
0 
'13 ^23 
'33 
(11) 
(12) 
(12) 
(22) 
etc. 
(11) 
(12) 
(13) 
A 
(12) 
(22) 
(23) 
(11) 
(12) 
(13) 
1 
0 
0 
(11) 
(12) 
(13) 
(12) 0 
(22) 0 
(23) 
1 
(12) 
(22) 
(23) 
0 
1 
0 
1 
(22.91) 
(22.92) 
Thus 
c12 
^13 ^23 
C 
33 
(12) 
(13) 
Cio Coo Cia C 
'12 ^33 
13 ^23 
y33 
(22) 
(23) 
(11) 
(13) 
A A' 
(12) 
(23) 
(12) 
(13) 
(23) 
(33) 
(11) 
(12) 
(12) 
(22) 
(12) 
(13) 
(22) 
(23) 
(11) 
(13) 
(12) 
(23) 
(12) A 
AA' 
c12- 
A A' 
. (22.93) 
Gc>o Coo 
r2 
C33 
r2 
°2'S 
MULTIPLE CURVILINEAR REGRESSION 169 
Similarly 
• • • . . . yJjZ.vQcj 
(22.95) 
This gives us the new c's in terms of the old. Denoting similarly the new &'s by primes, 
we have 
6a _ b[ = (cn - cj £ (xx y) + (c19 - 4) Z (<e2 2/) + c13 r (<e3 2/) 
= — { cjf3 Z (»! y) + c13 c23 r (x2 y) + c13 c33 £ (x3 y) } 
C33 
Ci36 
13 ^3 
33 
Hence we have 
b\ — &x 
&a = ^2 
C13 03 
^23 ^3 
C33 ' 
(22.96) 
expressing the new constants in terms of the old and the known constants c. 
Finally, the contribution to the sum of squares due to the variate x3 is 
&x E (xx y) + b2E {xz y) + 68 E (xz y) —b\E (xx y) — b\ E (x2 y) 
C:13 63 E (x, y) + C23 b3 E (x2 y) + b, E (x3 y) 
^33 £33 
C>: 
(22.97) 
33 
22.29. Generally, if there are p independent variates the equations for the 6's are 
• • • * 
&x r («! ^) + &2 ^ (a;a ^,) + ♦ . . + bp E (x*) = E (y xp). 
If xv is omitted the equations become (p — 1) in number in variables b'x . . . 6p_x. 
Subtracting from these the first (p — 1) of the above equations we find (p — 1) equations, 
typified by 
(6;-6J)Z(^o;i) + (6;-62)2:(x2^.) + • • . + {bp^i—bp^) E (x.p_lxj)-bpE (x^Xp) = 0 
(22.98) 
But these equations are the same as those for the coefficients clp . . . cpp with (b[ — b±) 
in place of cipi etc., and — bp in place of cpp. Hence 
or 
K 
&i 
-&, 
0^ 
-61 
Crp 
5 
Cp;p 
h 
^rp &p 
Cpp 
. (22.99) 
170 
Similarly it will be found that 
REGRESSION 
2 
Cu - cn == ~~~ 7" 
c13 - c12 = 
. (Za.ivV/ 
with similar equations for the other c's. 
22.30. Somewhat similar results apply when a variate isjadded. H primes again 
refer to new coefficients when £9 is added, we have, as above 
Ciq Vq 
cn ~~ cn 
C12 C12 
'2 
* 
CQQ 
ciq cn 
. («M*J. I \J it jf 
cm 
In order to use these equations to adjust the constants we require ciq . . . cqq and 6 r 
By writing down the equations satisfied by cu . . . clp and subtracting the 
corresponding equations in cn . . . c1(P we get p equations such as 
(cu - cu) T fo *,) + ...+ (c^, - clp) £ (x,j xp) = - cifl 2 (xj xQ). 
These are the same as the equations in b± . . . bq with — clqE {xj xq) instead of 2 (xg y) 
on the right, and hence 
v 
cip ~ cip = ~ ci<z Jit Cpj ^ ^Xj X^' 
Thus, using (22.101), 
CPQ 
j-1 
^= -JTc^i;^^ (22.102) 
Cdl ^Ml 
The last of the equations satisfied by cqq is 
c^rO^aO + . . . + cpqZ{xqxp) +cqqI(xl) = 1. 
Substituting for c[a, etc., in terms of cqq, we get 
p 
This gives cgg, and clq . . . c^q are derivable from (22.102). The other constants then 
result from (22.101). 
Cochran (1938a), to whom this proof is due, says that the elimination of two variates 
is best carried out in two stages of one each ; that where one variate is eliminated the 
method is quicker than re-solving the regression equations, except where there are only- 
two independent variates in the first instance ; and that if two variates are being eliminated 
the method is quicker if the original number of independent variates is six or more. For 
the addition of variates the method is in all cases more expeditious than re-solving the 
regression equations. 
MULTIPLE CURVILINEAR REGRESSION 171 
Example 22.10 (Cochran, 1938a) 
In a study of the effect of weather factors on the number of noctuid moths per night 
caught in a light-trap, regressions were worked out on Xt (minimum night temperature), 
Z2 (the maximum temperature of the previous day), Z3 (the average speed of the wind 
during the night), and Z4 (the amount of rain during the night). The dependent variate 
was log (1 + n)9 where n was the number of moths. 
It was subsequently decided to investigate the effect of cloudiness, measured on a 
conventional scale as the percentage of starlight obscured by clouds in a night sky camera. 
This is the new variate Z5. 
The quantities Cjk for the first four variates were :— 
V V V ■ V 
-A-i -A-2 -^3 *£•& 
Zx + 0-105,423,56 - 0-041,946,20 — 0-096,067,09 - 0-018,490,96 
Z2 ... + 0-086,038,69 + 0-033,172,71 + 0-012,903,58 
, . X. ... ... +0-572,652,01 +0-008,116,62 
Z4 ... ... ... + 0-062,275,32 
and the sums E (a?. x5) were 
S (xt xi) = - 4-867, * E (x2 x5) = + 0-206, E {xz x5) = - 0-5446, 
E (a?4 x6) = - 5-42, E (xl) = 7-87. 
We then find from (22.103) 
and from (22.102) 
c' = + 0-210,133,14, 
%!l = + 0-369,198,24 % = - 0-133,872,86 ^ = - 0-118,533,74 
cs?l c55 c55 
oo 
^ = + 0-249,298,91 
Or ! 
oo 
so that the new c's are given by (22.101) as 
V V V V V 
Xx 0-134,066,25 - 0-052,332,16 - 0105,263,03 + 0-000,849,84 + 0-077,580,79 
Z2 ... + 0-089,804,68 + 0-036,507,20 + 0-005,890,52 — 0-028,131,12 
Z3 ... ... + 0-575,604,43 + 0-001,907,12 - 0-024,907,87 
Z4 ... ... ... + 0-075,335,08 + 0-052,385,96 
Z5 ... ... ... ... +0-210,133,14 
The original regression coefficients were 
hx = + 0-198,140,7 b2 = + 0-038,528,4 bz = - 0-508,649,2, 
64 = + 0-031,848,2. 
5 
We now find b'5 = y. {c'j5 ^ (xj V) } 
= — 0-227,149,6, 
and from (22.101) we then have 
b[ = + 0-114,277,5 b2 = + 0-068,937,6 63 = - 0-481,724,3, 
64 = _ 0-024,779,9. 
As usual we have retained more figures than are necessary, in order to avoid cumulating 
errors and to facilitate the detection of computational slips. 
172 REGRESSION 
22.31. The constants c found in the foregoing method have a further use : they 
give the standard errors of the regression coeflficients and provide some of the functions 
required in more exact tests based on the ^distribution. If, measuring y about the mean, 
we have 
Y = bx -X.! + 62 -X-2 •+*.•• ~\- bp 2£p, 
then there are p equations of the kind : 
E (xx y) = b1Ex\ + b2E (xx x2) + . . . + bp E (xt xp), 
and thus, recalling the definition of the c's, we have 
bx = olx E (xt y) + c12 E (x2 y) + . . . + clp E (xp y). 
Thus, for fixed values of the x's, 
var b± = var y / J>j c1§ clk xj xk 
\ 3, k j 
= cnvar y, ...... '(22.104) 
and so for the other 6Js. 
For large samples var y may be taken to be the estimated variance 
1 
If the sample is small and it is desired to make a more accurate test, then we have, 
by an extension of 22.21, that 
t = ^-lM^~l-^ .... (22.105) 
is distributed in cc Student's " form with v = n — p — 1 degrees of freedom. 
22.32. As a final comment we may emphasise that regression equations are only 
polynomials fitted to the means of arrays, and consequently that if the scatter about 
those means is substantial they are not very reliable as estimators (though they may be 
better than other methods). The comment would hardly be necessary were it not for a 
tendency to use the equations somewhat uncritically for purposes of prediction. The 
point assumes even greater importance when attempts are made to estimate the dependent 
variate for values of the independent variates outside the range on which the regressions 
are based ; or again, if the observations are distributed over time so that the population 
may be changing while the sample is being drawn. The technique of regression analysis 
is undoubtedly useful in many fields, but—as with many other statistical techniques— 
the careful investigator will apply it with a certain amount of self-discipline. 
NOTES AND REFERENCES 
The theory of curvilinear regression was studied by Karl Pearson (1905). Orthogonal 
polynomials had been considered, and the essential problems solved, by Tchebycheff as 
far back as 1857, but their use in statistics was not fully appreciated until about sixty years 
later. Pearson gave in 1921 the general formulae for fitting curved regression lines up to 
the fourth order. Neyman (1926) pointed out the elegance of the determinantal approach. 
From aboi^t 1920 onwards there may be discerned two main lines of development. 
The Scandinavian school, led by Wicksell, has developed the analytical theory of regression 
—see Wicksell (19176, 1933, 19346) and a useful memoir by W. Andersson (1932). The 
EXERCISES 173 
second line, followed by Fisher, Aitken and others, has been concerned with the fitting of 
regression curves to arithmetical data and exact significance tests—see Fisher's papers of 
19216, 19226, 19246, 1926a, a paper by Allan (1930), and three papers by Aitken (1933a, 
b, c). The literature on orthogonal polynomials is now very large. 
For some illustrative material, see K. Pearson (1905), Andersson (1932), and Pretorius 
(1930). See also references to Chapters 14 and 15. 
EXERCISES 
22.1. Show that the regression of y on the variance of x (the scedastic curve) is 
given by 
y = V(- iySL±A *im _ \y till f(i W K,_s l^W^£(?)" 
Zj y ' i\ a(X) Zj i ! Z-t \ s 7 A ] s'* g X) a (X) 
J 1 9 (*) 
L 0 j\ -&W J ' Jffl 9(X) 
where S I -?-r- )=(-£' .. 
V 3 ! / V 3 I 
(Wicksell, 19346.) 
22.2. Show that if the regression of y on the mean of x is linear, then from (22.11) 
«n v 
oo 
0 J ' 
d 
is a linear function of </> (tt) and j~- <f> (tx). Hence that 
(Wicksell, 19346.) 
22.3. Show that if the marginal distribution of a bivariate distribution is of the 
Gram-Charlier Type A : 
/ = a (x) { 1 + a3 #3 + a4 Z/4 + . . . } 
the regression of y on # is 
00 00 
y j ~0 A'==0 ,? • 
00 
1 + S^ajHj (X) 
j=z 
(Wicksell, 19176.) 
22.4. Transforming the orthogonal polynomials of (22.74) to a new variate 
I = X —, note that Pp — fP.p-1 is a numerical multiple of Pv-%, say 1PV^^ Show- 
that 
I 
and deduce the recurrence relation, 
(Allan, 1930. The relation is due to Tchebycheff.) 
174 REGRESSION 
22.5. A regression line 
T = a0 + ax X + a2 X2 + az X3 + a4 X4 
is fitted to normal data and the number of observations N is large. If r is the correlation 
'2 
between the variates and c = ^ (the moments referring to the #-variate), show that 
var a0 = ^^ (45 + 30c2 - 8c3 + c4) (1 - r2) 
var ax = IEJ!! (15 + 30c - 15c2 + 4c3) (1 - r2) 
vara2 = ^(4-3c + 3c2) (1 - r2) 
2N/4 
var a3 = —^ (1 + 4c) (1 — r2) 
var 2/ ,, 9v 
TOr a> = 2i4 (1 ~ f'>• 
(Andersson, 11)32.) 
22.6. In the notation of 22.31 show that 
cov (bx b2) = c12 var y 
and hence show how to test the difference of two coefficients in a regression equation. 
22.7. Show how to derive a test of the significance of the difference of corresponding 
regression coefficients in two equations derived from independent samples, based on the 
result of 21.26. 
CHAPTER 23 
THE ANALYSIS OF VARIANCE—(I) 
23.1. At various points in this book we have encountered in different guises the 
result that the sum of squares of a set of observations about their mean can be represented 
as the sum of two independent sums of squares, each of which provides an estimate of 
the parent variance ; and that their ratio provides a test of homogeneity, at least when 
the parent is normal. We now proceed to study in more detail a method of statistical 
analysis with considerable generality which springs from this result. In view of the 
complexity of the general case we shall begin by considering simpler cases under somewhat 
restrictive conditions and shall extend our results stage by stage. 
One-way Classification 
23.2. Suppose we have a set of variate-values divided into $> families : 
/y* ry ry 
•^11 «^21 W-il 
ryt ry* ry* 
■^12 ^22 ^ntf 
ry ry* ry 
Denoting by x the mean of the whole set and by Xj the mean of the values in the jth family, 
we have the identity 
V"1 - V1 
/ Y \xij ~~ x) — / j \xij xi r Xj — x) 
i, j h j 
= y (x,tj — Xj)2 + 2_j (xj — ^)2? • • - (23.1) 
since the cross-product term %y (#y — Xj) (xj — x) vanishes. We may also write this as 
2^ {'xij - x)2 = JT (x^ — x,j)2 + JT* ty (xj — x)2, . . (23,2) 
h j i, j j 
where n,j is the number of members in the ^'th family. 
It will also be convenient, from the point of view of a later generalisation, to write 
the mean of the^th family as xmj and that of the whole as xt, the periods in the subscripts 
showing which factor is being averaged. We have then the alternative form 
X (** ~ x-.)2 - X &V - X^ + H n* (XJ ~ X-)2 ' ' (23'3) 
*. 'J i, j i 
23.3. The problem we shall discuss in connection with families of values of this type 
takes some such form as the following : the members of each family are randomly chosen 
from some parent population corresponding to that family. The populations themselves 
are, as a rule, defined by some prior system of classification given among the data of the , 
problem, e.g. they might be different varieties of wheat, the x's being the yields of the 
varieties grown under similar conditions, or they might be defined by income levels and 
the sc's the expenditure on food of a sample chosen from the different income groups. We 
now ask : is there any evidence that the factor measured by x varies significantly from 
175 
176 THE ANALYSIS OF VARIANCE 
family to family ? Alternatively, can the data be regarded as homogeneous, i.e. as 
emanating from populations which are identical so far as concerns the factor measured by x ? 
Further, when the question of significance is decided, how can we estimate the variation 
of x in families or groups of families, and how can we estimate the magnitude of any 
differences which exist ? 
23.4. We will assume, until further notice, that within each family the variation 
is normal with variance v, and that v is the same for each family. In later sections we 
shall endeavour to remove these rather restrictive conditions. On our present hypothesis 
the populations corresponding to the different families can differ, if at all, only in their 
means, and our first question is whether the sample values afford any evidence of such 
-differences. 
Let us take as our hypothesis that the parent populations have a common mean m. 
Then we recall the following facts :— 
1 
(1) The sum —■ JLi \X<i<i ~~~ x )2 is distributed in the Type III form of x2 with 
Jtf — 1 = S (rij) — 1 degrees of freedom, that is to say as the sum of squares of N — 1 
independent normal variates with zero mean and unit variance. 
(2) In any given family xm$ / ~ is distributed normally with unit variance about 
1 
mean m, and is independent of the sum - S (x^ — x A2, which is itself distributed as #2 
v i 
with rij — 1 degrees of freedom. 
Since on our hypothesis the observations may be regarded as a single sample from 
the same population, it follows that 
1 v^ ^ 
- > (x^ — a;..)2 is distributed as %2 with N — 1 d.f. 
i, 3' 
\X &a - *.;)2 . » ^ K - i) = n - p d.f. > (23-4) 
%3 
-Zn^xj -x..)2 „ „ p - 1 d.f. 
The only statement requiring any proof is the last. It may be proved directly (see Exercise 
23.1), but we shall deduce it as the corollary of a general theorem due to R. A. Fisher which 
will often be required in this chapter. 
23.5. Suppose we have q variates which are independently and normally 
-distributed with unit variance about the same mean, which we may assume to be 
zero. Put 
a 
Cr = 2^ Ksxs> r = 1 • • • 2 (23.5) 
.9=1 
If we choose the coefficients X so that 
Z Ars hts = 1 r = t\ 
s =0 r y^tt ' " ' ' ( ) 
then each £ is distributed normally with unit variance independently of the others. There 
ONE-WAY CLASSIFICATION 177 ■ 
are g2 coefficients A, and the equations (23.6) impose \q (q + 1) conditions on them, so that 
the /Ts can always be found in a multiplicity of ways. In effect they correspond to the 
rotation of orthogonal co-ordinate axes in a g-dimensional space. 
Now suppose that we have h linear functions of the #'s, £i . . . rQh (h < q) whose 
coefficients obey the orthogonality relations (23.6). These h variates are then distributed 
independently, normally and with unit variance. 
It is now possible to find q — h further variates fA+1 . . . £ff which are orthogonal 
among themselves and to f x . . . £h. Geometrically this is evident from the possibilities 
of rotations in the g-way space. Algebraically it follows from the consideration that if 
qh of the A's in (23.6) are known, q (q — h) are unknown, and the number of conditions 
they must obey is 
\q (q + 1) — \% (h + 1) = | (q —- h) (q + h +• 1), 
so that values of the unknowns can be found in at least one way if 
|- (q + h + 1) < # 
or h + 1 < q. 
Now suppose we express a sum of squares of q normal variates with unit variance, 
say A, as the sum of two quantities B and G ; and suppose that B is distributed as the 
sum of squares of h independent normal variates with unit variance which are linear 
functions of the variates entering into A. Then we can find q — h such variates 
independent of the first h, and G must be their sum of squares. Further, the distributions 
of B and G are independent. By an extension of the same argument, if 
A = At + A* + . . . + Ak, . . . . (23.7) 
A is distributed as %2 with v degrees of freedom, Ax with vu . . , Ak^} with vk^x ; and 
if the variates entering into Ax . . . Ak_x are mutually independent and are linear functions 
of those entering into A, then Ak is distributed as %2 with vk degrees of freedom, where 
v = Vl + v2 + . . . + vk .... (23.8) 
and Ak is independent of Al9 . . . ^-1.- 
23.6. As an extension and kind of converse of this theorem we have the result, due 
to Cochran, that if Al . . . Ak are distributed as x" with i\ . . . vk degrees of freedom, 
and their sum A is distributed as %l with v — 2 (vf) degrees, then Ax . . . Ak are 
independent. We will prove this for the case k ~ 2, the more general result following in a 
similar way. 
If the characteristic function of Ax and A.x is <f> (tu t^)y we have, by hypothesis, 
<j> {tu 0) = 
<f> (°> *0 „ «.-, u„ 
(1 - 2^)K 
1 
(1 - 2it^ 
1 
and <k> (t, t) = . .. , .. 
Hence 0 (Z, *) = <£ (*, 0) <£ (0, t) = (1^j^-y 
and thus <j> (t, 0) and <f> (0, £) are both divisible by a factor in (1 — 2it)~l and no other 
a.s.—vol. 11. k 
178 
THE ANALYSIS OF VARIANCE 
factor in t because of the symmetry of cj> (tl9 tz). These factors are identified by <j> (tl9 0) 
and cj> (0, t2) as (1 - 2it)~^ and (1 — 2it)~^, and hence 
<f> (tu t%) = <f> (*!, 0) $ (0, t2), 
or Ax and A2 are independent. 
1 
23.7. Let us now return to the statements in (23.4). The sum - £ (xi;j — xmm)2 is 
1 
distributed as x2 wi^ v = N — 1. The sum - £ {xitj — xmj)2 is so distributed with 
Vl = N — p. Further, the quantities xLj — xtj may be transformed to N — p independent 
normal variates which are linear functions of the variates entering into the first sum. It 
1 
follows from 23.5 that because of the identity (23.3) the third sum - Zn^ (xtj — x_)% is 
distributed as %2 with v2 = {N — 1) — (N — p) = p — 1 degrees of freedom, and that 
independently of the second sum. 
Thus we may exhibit our break-up of the total sum in the following form :— 
TABLE 23.1 
Form of Analysis of Variance for One-way Classification. 
Sum of Squares. 
Of family means about the mean of the^ 
whole j 
Of individuals in families about the^ 
respective family mean . J 
Of individuals about the mean of the" 
whole 
2^ (xv 
x..y 
X.j)2 
i, j 
z< 
Xij 
x..) 
i, i 
Quotient. 
71 j [Xmj 
P - 1 j 
N — p 
Xjj 
I,'J 
N 
~i Z <*« 
hO 
We note that the sums of squares and the degrees of freedom in the first two rows sum to 
those in the third row (though the quantities in the quotient column are not additive). 
This is the origin of the expression " analysis of variance/' though, to be accurate, it is the 
sum of squares of the total which is analysed. 
To avoid cumbrous phrases we refer to the sum of squares of family means about 
the mean of the whole as the sum of squares " between families," and to that of individuals 
about the respective family-means (for the time being) as " residual." We shall also speak 
of total sum of squares and total mean with the obvious significance, and denote degrees 
of freedom by the initial letters " d.f." * 
23.8. Since the mean value of #2 with v degrees of freedom is v, the quotients in 
* The need has been felt for a word to denote Msum of squares about the mean". Professor 
Pitman has suggested the word " squariance ", though he seems to feel that this leaves something to 
be desired. In my own notes I use the word " deviance " but have not ventured to introduce it into 
the text. 
ONE-WAY CLASSIFICATION 
179 
(23.1) are all unbiassed estimators of v, the parent variance. Only the first two, however, 
are independent. We recall that the ratio 
z 
ilog 
N — p 2%j {xmj — x.y 
p - 1 2T(% - xj* 
(23.9) 
is distributed in Fisher's form, which is independent of the variance v. This distribution 
accordingly provides a convenient test of significance in the normal case. 
Example 23.1 
Let us consider the application of the foregoing theory to a simple example which 
has been chosen to reduce the arithmetic to a small amount. The following shows the 
lives in hours of four batches of electric lamps :— 
Batch 1 
Batch 2 
Batch 3 
Batch 4 
1600, 1610, 1650, 1680, 1700, 1720, 1800. 
1580, 1640, 1640, 1700, 1750. 
1460, 1550, 1600, 1620, 1640, 1660, 1740, 1820. 
1510, 1520, 1530, 1570, 1600, 1680. 
We know that the batches were made from four different specimens of wire, but were 
otherwise made under identical conditions. (This, of course, over-simpliiies the problem as it 
is encountered in practice, but will serve for purposes of illustration.) The question is, 
do the batches differ among themselves in length of life % If so, we suspect that the quality 
of wire is varying materially, and if the lamps are to be standardised as far as possible the 
quality of wire must be made more uniform from batch to batch before manufacture is 
undertaken. The numbers in this example are small, but not much smaller than would 
be desirable in practice, owing to the expense and time involved in testing a lamp by running 
it until it burns out. 
The sums of x and x2 for the four batches will be found to be— 
Batch 1 
„ 2 
„ 3 
„ 4 
X k) -1 Axjo 
Number hi Sample. 
5 
8 
6 
26 
Xml I JL 1 
11,760 
8,310 
13,090 
9,410 
42,570 
Xrf yju J 
19,785,400 
13,828,100 
21,503,700 
14,778,700 
69,895,900 
Thus for the mean life of lamp in the four batches we have 11,760/7 = 1680; 
8310/5 = 1662 ; 13,090/8 = 1636-25 ; 9410/6 = 1568-33. These certainly differ, but is 
the variation such as cannot have arisen by mere sampling fluctuations ? 
We find 
xmm = 42,570/26 = 1637-3077. 
X fiUS 
Zfry-zJ* = Ex% -Nxl 
= 69,895,900 - 69,700,189 
= 195,711. 
180 
THE ANALYSIS OP VARIANCE 
We also have 
EnAx* - £j2 
44,360. 
Nx 
2 
The analysis then takes the form- 
Sum of Squares. 
Between batches 
Residual 
Totals 
44,360 
151,351 
195,711 
d.f. 
3 
22 
25 
Quotient. 
14,787 
6,880 
7,828 
We have 
itoffi 
14,787 
6880 
0-383 
v. 
3, va = 22. 
The 5-per-cent. point for these degrees of freedom is seen from the tables to be 0-5574. 
The observed value is therefore not significant, and we conclude that, so far as this test is 
concerned, there is nothing to throw doubt on the homogeneity of the group. 
Having decided, provisionally at least, to accept the hypothesis that the data are 
homogeneous, we may ask, what is the best estimate of the parent variance ? Our analysis 
has given three different estimates, viz. 14,787, 6880 and 7838. It seems natural to use 
the last, which depends on the greatest number of degrees of freedom. 
With this value we find for the variance of the mean of samples of n, 
'7828 88-48 
n 
\/n 
The greatest difference of means observed is that between the first and fourth batch, 
1680 — 1568-33 = 111-67. The standard error of this difference is 
88-48 V (+ + i) = ^9-2. 
The observed difference is rather more than twice the standard error, but we cannot 
conclude that it is significant on that account. In fact, we have picked out the greatest 
difference for examination from the six possible comparisons of pairs, and the distribution of 
the greatest difference must have a larger standard error than that of a difference chosen 
at random, which is what we have found. Nevertheless the fact that even the greatest 
difference is only slightly in excess of twice the standard error affords some general evidence 
in support of the hypothesis of homogeneity. 
We may also note that if a more accurate test of the difference of two means is required 
the Z-test may be invoked ; but here also we must remember that we are testing the greatest 
of a set of differences. Where there are only two families concerned, the analysis of variance 
reduces to the Z-test for the difference of sample means when variances of the parents are 
assumed equal. 
23.9. Suppose now that in the case of one classification we have applied a test by 
means of the analysis of variance and have found that the hypothesis of homogeneity is 
TWO-WAY CLASSIFICATION 181 
unacceptable, or, in plain English, that the parents do differ. Let us then consider the 
alternative that the populations are still normal and that they differ in their means but 
not in their variances. 
At first sight this may seem a highly artificial assumption to make, for if the 
populations differ in their means it is not unlikely that they may differ in other respects. This 
is undoubtedly so, but if there is serious possibility of difference in variances their 
homogeneity may be discussed separately by means of tests we shall consider in Chapter 26. 
Apart from this, there often arise in practice situations in which approximate equality of 
variance is plausible on prior grounds. For instance, we may be testing the effect of 
manuring on cereal yields, and it is reasonable to suppose that if the manure exerts any 
effect at all it will increase all plants of the same variety to about the same extent—that 
it will, in fact, displace the location of the distribution of yields without affecting 
its dispersion. 
23.10* The question we have now to consider is whether we can ma/ke an estimate 
of the common variance of the populations. A little thought will show that we can. The 
reasoning which led to the conclusion that the residual sum of squares is distributed as 
v%% with N — p degrees of freedom remains unchanged, so that the residual quotient in 
Table 23.1 continues to provide an estimator of v. The other two no longer do so. 
Consider, in fact, the sum of squares between families, and let the mean of the ^th family be 
mmj. Then we have 
E E rij (x,j — xmtY = E Enj{xj — mmj — (x„ — mmm) + maj — mtm} 2 
j J 
— E ZUj{xtj — mtj — {xmm — m_)} 2 + E n^ (mtj — m##)2. (23.10) 
1 
Here ///. is the mean -^ E n* m * and hence x ,- —- m , has the mean x — m . Thus 
Enj{xmJ ■- mmj — (xmm — ra#.)} 2 is distributed as v%2 with p — 1 degrees of freedom and 
EZnj {xmj - x.y = (p - 1) v + Infj (m.j. - mj2. . . (23.11) 
j 
Not unless m • = ?nmm — that is, all populations have the same mean—does the expression 
on the right reduce to (p — l)v, and hence the quotient between families give an unbiassed 
estimator of v. In other cases it is greater. 
Similarly, 
E JT1 (x,fj - :c..)a - E £ {xtj - m4 - (a. - mj }*+E £ (mmj - m .)* 
= (j/'-i l)v + Enj{mJj -m.)8 . . ' . . .(23.12) 
j 
The expectation of the difference of the two terms considered in (23.11) and (23.12) 
confirms that, the residual sum of squares provides an estimator of (iV —p)v. 
23.1.1. A comparison of the formulae we have already reached and those of section 
14.31 will show that the study of intra-class correlation is very closely related to the analysis 
of variance. It is an interesting exercise to derive the s-test directly from the sampling 
distribution of intra-class r given in equation (14.110) (vol. I, p. 362) and vice-versa. 
Two-way Classification 
23 12. We proceed to the case when the variate-values belong not to one ol a single 
act of families but to two, say A and B. In the first instance we shall consider the situation 
182 
THE ANALYSIS OF VARIANCE 
when there is only a single value in the jth class of A and the &th class of B. 
may then be set out in the tabular form : 
Class B 
Our sample 
Class A 
! 
A-i 
A2 
A% 
m 
m 
Ap 
Totals 
B-± 
xn 
X2l 
xpl 
PXA 
B2 
x12 
X22 
X32 
* 
• 
* 
%>2 
px.2 
Bs 
X12 
*^23 
^33 
• 
XP% 
px.s 
• • • 
■ ' * • 
• • • 
• • • 
• * • 
• • -a 
• • • 
a m • 
• • ■ 
B(l 
X\q 
x2q 
xZq 
m 
* 
Wpq 
px.q 
Totals 
q%2. 
qxs. 
• 
m 
qxp. 
pqxmm 
(23,13) 
This is not a contingency table. The numbers xjk are variate-values, not frequencies. 
As usual, Xjm signifies the mean of values in the class Aj and xJc the mean of values in the 
class Bk, xmm being the mean of the whole. 
We have the algebraic identity 
j, k 
j, k 
2^ {xjk - xt. - x.k + a;..)2 + JT (as,. - a;..)2 + £ {x.k - xj* 
j, k j, k j, k 
= Y.i*ik ~xi. ~x.k +*..)2 + ?£(*,. -a;..)2 +pZ{x.k -*..)* (23.14) 
j, k J k 
the cross-product terms vanishing on summation in the usual way. 
23.13. We are interested in the variation of the a's according to class membership. 
Let us take as our hypothesis that the pq values are homogeneous, that is to say that they 
all emanate from (normal) populations with the same mean m and variance v. In such 
a case class-membership exerts no influence on variate-values, and the observed differences 
are pure sampling effects. 
The expression on the left in (23.14) is then distributed as v%1 with pq — I degrees 
of freedom. The mean xjt is distributed normally with variance v/q and thus E q (xjm — xtm)" 
j 
is distributed as vx2 with p - 1 d.f. Similarly, Ep {xmk — x )2 is so distributed with 
k 
q — 1 d.f. Finally the remaining term on the right is distributed as vx2 with (p — 1) (q — 1) 
d.f.; for each term is normal with variance — ' v, since 
Jb 
'jk 
Xjm xtk -f- x.m — Xjk 
? p pq 
pq 
Ex, 
1 
1 
'3l 
E xmk ) + — / j Xjm> 
m. \p pq) pqTi 
q pq 
— 2-j xlmy I ^h m ^ k 
TWO-WAY CLASSIFICATION 
so that the sum of squares of coefficients on the right is 
-'^Jr^y + c - »(P~Y + <p -» M' + ^~l) {q ~ " 
M J \ P4 J \ pq J {pq)* 
= (y ~ 1) (g ~ 1) 
- . ,m • • . . 
pq 
Thus, since there are $> + j — 1 linear relations connecting the pq quantities 
183 
. (23.15) 
x 
jk 
Xa 
X j. ~j~" X 
their sum of squares is distributed as vx2 with pq — (p + q — 1) = (p — 1) (q — l) degrees 
of freedom, which checks against the mean value of the individual square given by (23.15). 
We may thus analyse the variance in the following way :— 
TABLE 23.2 
*orm of Analysis of Variance for Two-way Classification with One Member in each Subclass 
Sums of Squares. 
Between A -classes 
Between /^-classes 
Residual 
q S (ay. 
o 
P E {x.k 
k 
• \E{x- 
rjfc - xj. - x.k + z..)2 
j\ k 
r,„ 
fOTALS 
j,k 
d.f. 
p _ i 
ff- 1 
(p - l)(g - 1) 
Quotient. 
j»2 - 1 
2 
P 
\p^3 • 
~ 2j (x.k 
q - 1 k 
(p - 1) (q - 1) 
X.k + ^..)2 
The sums of squares and degrees of freedom (but not the quotients) are additive as 
before. It follows from the theorem of 23.6 that the three constituent sums are 
independent. Each quotient provides an unbiassed estimator of v. 
23.14. Our use of these results proceeds by an easy generalisation of the method 
exemplified, in. Example 23.1. We take as our hypothesis the supposition that all samples 
are from normal populations with identical mean and variance. Comparison of the 
estimates in the quotient column then provides a test of significance. If the hypothesis is 
rejected we may examine the alternative that means are different but variances identical 
throughout, in which case we shall find that the residual still provides an estimate of the 
variance, provided that an important additional assumption is made. 
Jhmnpk 23.2 
The following data (Daniels, Supp. J.R.8.S., 1938, 5, 89) show the weight in grams 
of 95-yard lengths of wool thread from 100 " ends " being spun on four bobbins, 25 ends 
184 
THE ANALYSIS OF VARIANCE 
to the bobbin. We are interested in two factors, the variation between bobbins and the 
variation in the 25 ends on the same bobbin, according to their position. 
TABLE 23.3 
Weight in Grams of 100 95-yard Lengths of Wool Thread spun m Four Bobbins. 
End Number. 
1 
2 
3 
4 
5 
' 6 
7 
8 
9 
10 
11 
12 
* 
13 
14 
I 15 
| 16 
| 17 
! 18 
1 19 
20 
21 
.i 22 
23 
24 
1 25 
! 
i 
Totals 
1 
1 
7-50 
7-52 
7-70 
7-93 
7-78 
7-73 
8-07 
8-01 
8-22 
8-24 
817 
8-09 
8-11 
7-96 
8-09 
8-04 
7-78 
811 
8-17 
8-12 
8-13 
8-01 
8-17 
8-05 
7-91 
199-61 
Bobbin Number. 
2 
7-23 
7-81 
7-94 
7-94 
7-89 
8-23 
8*27 
8-54 
8-24 
8-35 
8-29 
8-54 
8-45 
8-43 
8-47 
8-33 
8-47 
8*63 
8-31 
8-31 
8-10 
8-01 
7-92 
8-27 
7-92 
204-89 
3 
7-50 
7-77 
7-83 
7-96 
8-02 
7-99 
8-25 
8-24 
8-37 
8-43 
8-46 
8-33 
8-27 
8-24 
8-12 
814 
8-19 
8-36 
8-31 
8-47 
8-19 
8-37 
8-27 
8-07 
8-28 
204-43 
_— . 
4 
7-53 
8-05 
8-16 
7-76 
7-85 
8-14 
8-26 
8-54 
8-10 
8-15 
8-38 
8-47 
8-38 
8-60 
8-45 
8-43 
8-57 
8-38 
8-16 
8-41 
8-27 
7-96 
8-08 
8-16 
8-52 
205-76 
Totals. ; 
i 
29-76 
31*15 
o X * u o 
31-59 
31-54 
32 09 
32-85 
33-33 
32-93 
3317 
33-30 
33*43 
33-21 
33-23 
33-13 
32-94 
33-01 
33-48 
32-95 
33-31 
32-(>9 
32-35 
32-44 
32-55 
32-63 
814-09 
It simplifies the arithmetic if we take a working mean at 8-00. 
squares about this mean is then found to be 
£{xjk\2 = 9*3829, 
and we have also 
Hence 
S{x,jk) = 14-69. 
Z(xjk - X..V = 9-3829 - (0-1469) (14*69) 
= 7-224,939. 
The means of the four bobbins are 
7-9844, 8*1956, 8-1772, 8-2304. 
With the same working mean we find for the sum of squares 
<£>.&)2 = 0-122,986,72; 
The total sum of 
TWO-WAY CLASSIFICATION 
185 
and hence 
■pE{xmk - xmy = 25 (0-122,986,72) - (0-1469) (14-69) 
= 0-916,707. 
The means of the four ends of corresponding position on the four bobbins can, of 
course, be found from the totals in the last column of the table, but it is simpler to find 
£(q%j, — #^..)2 and then divide by g2. We find 
Z{xjm - a..)2 = ii?Z^??l) - (0-1469) (14-69) 
= 4-637,814. 
The continual appearance of the factor (0-1469) (14-69) = Ncc*t is to be noted. The 
quantity is best computed once for all at the outset. 
The residual sum of squares is then obtainable by subtraction, and we have the 
following analysis :— 
TABLE 23.4 
Analysis of Variance for the Data of Table 23.3. 
Sums of Squares. 
Between bobbins 
Between ends 
Residual 
JL QTAJuS 
0-916,707 
4-637,814 
1-670,418 
7-224,939 
d.f. 
3 
24 
72 
99 
Quotient. 
0-3056 
0-1932 
0-0232 
0-0730 
The variation between bobbins and that between ends are both significant—the ratio 
of the corresponding quotients to the residual quotient is so big in each case as hardly to 
require the 3-test. We are led to suspect that the variation between bobbins, small as it 
is, cannot be a chance effect, and it looks as if bobbin number 1 is not getting its fair share 
of thread. Similarly, the weight of thread seems to be dependent on whereabouts the 
thread is spun on the bobbins, and an inspection of the original data suggests a systematic 
variation as we proceed along the bobbin from end number 1 to end number 25, with a 
possible maximum, in the middle. If the manufacturing process is to be standardised as 
much as possible, we should have to examine the reasons for the shortage of weight on 
the first bobbin and for this systematic effect of position on the bobbin. 
23.15. Suppose now that, as in the example just given, the hypothesis of 
homogeneity is rejected. What interpretation can we put on the residual quotient ? Let us 
assume that each observation conies from a normal population with variance v, but that 
the parent mean of the subclass A$ Bk is mjk, these quantities varying from one subclass 
to another. Is the residual quotient an unbiassed estimator of v ? In general the answer 
is " no ", but there is an important class of case in which it is affirmative. 
Let mjt be the mean of the q values of m$k in the class Ap mtk that of the p values 
in Bk, and mtm the mean of the whole set of m's. Then we may write 
H~* Sjic ... ... (23.16) 
Xjk 
m. 
'jk 
XJ. =m3. +■$.> etc- 
186 THE ANALYSIS OF VARIANCE 
JL lie 11 
=fiI(mj^m^m.fc+mJ^^I(^^,--^+|J25 • (23.18) 
the product term vanishing as usual. The second term on the right is equal to 
(P — 1) (? -~ 1) v> f°r "the f's are distributed with variance v about sero mean, so that the 
term in question is the residual sum of squares in a p x q two-way classification of a 
homogeneous sample and hence has the stated expectation. Thus we have 
EZ(xjk —xim -xtk + xmy = Z(mjk -mu —mmk + mm y + (p - l)(q ~ I) v. (23.19) 
The residual quotient will then provide an unbiassed estimator of v if and only if 
mjk ~" mj. ~~ m.k + mtt = 0. . . . . (23.20) 
23.16. Now suppose that xjk is made up of three parts which are additive, viz. 
(1) the effect of the class Ap say a^; 
(2) the effect of the class Bk, say bk ; and 
(3) a residual £jk which is normal and has zero mean. 
This kind of hypothesis will recur frequently. It amounts to an assumption that there 
is in Xjk an element a^ which affects alike all members of the class A$ but varies from one 
J.-class to another ; an element bk which similarly affects alike all members of Bk but varies 
from iJ-class to .B-class ; and a third component representing random variation which, 
apart from the sampling factor, is the same for all subclasses Aj Bk. We then have 
Xjk = ttj + bk ~\~ (sjk ..... (23.21) 
and 
mjk = fy + bk 
mj. — aj + &. 
771 k == Of ~\- uk 
m = a + b 
where, as usual, the subscript periods in the a's and 6?s denote averaging. Thus 
mjk ~ mj. - m.k + m.. = aj + h ~ (aj + b.) — K + bk) + a. + bt 
= 0, 
so that (23.20) is satisfied and the residual quotient is an unbiassed estimator of the 
variance v. 
Under the same conditions it will be found that 
qE Z{xjm -x..)2 = (p - l)v +q£(mJm - mj2 
. (23.22) 
pE S (xmk 
k 
xLt JLi \Xjk ' 
= (p - l)v +qZ(aj ~ 
-^..)2 = (? - 1) v + pZ(h - 
k 
-^..)2 = (m - i)v + ]T (% 
j, k 
= (pq - 1) v + q S (^ 
3 
- &y 
-by 
- a. + bk - by 
-ay + pU{bk - 
k 
• 
■ 
■by 
i & i j, JZi «j j 
. (23.24) 
. (23.25) 
23.17. We have supposed that the component £ had a zero mean, but of course if 
all these components had the same mean, the constant common to them could be absorbed 
THREE-WAY CLASSIFICATION 187' 
into the functions a-} and bk. Our hypothesis is thus a little more general than it appears. 
In certain practical cases it is a plausible hypothesis to make. For instance, in Example 
23.2 it is reasonable to suppose that the effect of a particular bobbin is the same for all 
ends, and the effect of situation the same for all bobbins. If there is any serious doubt 
on the point we have to collect further data and consider interactions in the manner 
described later (see 23.22). 
It may, however, be noted that if the variation of the m^'s is comparatively small 
the appearance of the term containing them in (23.19) does not materially vitiate an estimate 
of v from the residual quotient. In any case that estimate will be greater than the unbiassed 
estimate, so that our inferences about significant differences of mean values will, properly 
interpreted, be on the safe side. 
23.18. Before going farther we may remark that the quantity we have called the 
residual sum of squares and the associated quotient are often referred to as " error " or 
" interaction " terms. The former is likely to cause misunderstanding and is better avoided 
altogether, for, as we have seen, it provides a measure of sampling variance, and 
therefore of experimental error, only in particular cases. The word u interaction " we shall 
define below ; it has been used in different senses by different writers, and when consulting 
original memoirs the reader should endeavour to ascertain the precise meaning which 
is being attached to it—if he can. In considering a given analysis it is as well to reflect 
on the precise nature of the items covered by such expressions as " residual", " remainder ", 
4C error " and so forth. 
Three-way Classification 
23.19. Consider now the case when there are three classifications into A-, B- and 
•O-classes. As before, we shall consider in the first place one member in each subclass 
Aj Bk Ch typified by XjM. We now have 
3, k, I, 
+ z&jk. -xj.. - x.k. + x..y + % (xj.i - xj.. - x.j + ^...)2 
-f S (xmkl — xmkm — xmml + #...)2 
+ % (xm - Xjk. - xj.i ~ xm + xj.. + x.k. + x..i - x...)*> • (23.26) 
the summations extending over all members of the sample, pqr in number, so that we may 
replace expressions such as y. (xj.. — x...)2 ^Y Qr £ (xj.. ~~~ x...Y2> e^c- 
'}, k, /, J 
On the usual hypothesis of normality and homogeneity we find that the first three 
terms on the right of (23.26) are distributed as v%1 with p — 1, q — 1 and r — 1 degrees 
of freedom. The second group is so distributed with (p — 1) (q — 1), (p — 1) (r — 1) and 
(q — 1) (r — 1) degrees of freedom. The last is distributed with (p — 1) (q — 1) (r — 1) 
degrees of freedom. All but the last of these results follow from the two-way case, and 
the last may be established as in 23.13 or by the consideration that for any fixed I the 
term has (p — 1) (q — 1) degrees of freedom and that there are (r — 1) independent Vs. 
We may then write the analysis in the form shown in Table 23.5. (For the present 
the expression " interaction AB " is to be regarded merely as a name given to a particular 
sum of squares. As before, the sums of squares and degrees of freedom are additive, 
188 
THE ANALYSIS OF VARIANCE 
and the seven items into which the total sum of squares is analysed are distributed 
independently.) 
TABLE 23.5 
Form of Analysis of Variance for Three-way Classification with One Member in each Subclass. 
Sum of Squares. 
Between ^.-classes . 
Between 23-classes . 
Between C-classes . 
Interaction AB . 
Interaction BG . 
Interaction GA . 
[Residual .... 
Totals . 
^ \xj.. x*..) 
£(x.k. - ^...)2 
£(x..i - a...)2 
Z{xju. ~xj., - x.k. + x...)2 
2{x.u - x.jc, - x..i + x...)2 
£(Xjkl — Xj.m - X,Jc. —X.J + Xjk. 
4- x.m -f xj.i - x...)2 
£(Xj7cl ™ X...)2 
d.f. 
p — 1 
q - 1 
r - 1 
(p- i)fo-l) 
(q - l)(r - 1) 
(r - 1) (p _ 1) 
(p _ i)(fif _ i)(r _ 1) 
3>gr - 1 
Quotient. 
The quotient of 
the sum of 
squares by the 
corresponding 
d.f. 
23.20. If the hypothesis of homogeneity is rejected we may consider the alternative 
represented by 
xm = aj + bk + ci + tjku .... (23.27) 
where £, as usual, is normal with zero mean. As in 23.16 it will be found that the residual 
term in Table 23.5 has expectation (p — 1) (q —- 1) (r — 1) v, and hence continues to provide 
an unbiassed estimator of v. The quotients between classes are affected like those in 
equations (23.23) to (23.25) ; but the interaction terms also provide estimators of v with 
the appropriate degrees of freedom. For instance, 
(*jk. - xi.. - x.k. + «...) = aj + h + c. + Cjk. - K- + bm + cm + 'Qj,.) 
- (a. + bk + cm + C./J+ (a + bm + c. -f C...) 
= £#. ~ C/\. — C.fc. + £... . ... (23.28) 
so that the expectation of the sum of squares of the :r-terms is that of the £-terms, which 
we know to be (p — 1) (q — 1) v. 
23.21. This brings up a new point arising for the first time in the three-way 
classification. If (23.27) is true, the analysis of variance will provide four different estimators 
of the variance v, namely the interactions AB, BC and GA and the residual. These are 
independent (for they depend only on the £'s, and the theory appropriate to the case of 
homogeneity continues to apply) and their ratios may be tested in the ^-distribution. If 
these ratios are such as can have arisen from random sampling we may accept the hypothesis 
represented by (23.27) ; if not we must reject it. In short, the interaction quotients 
provide a test of the hypothesis (23.27). In the two-way classification no such test is available. 
Interactions 
23.22. On the hypothesis (23.27) the interaction quotients of type AB give unbiassed 
estimators of the variance v. If in any particular case these quotients differ significantly 
among themselves or from any other independent estimator of v, we have to reject the 
hypothesis. Apart from the normality of the variation of f, which is not for the moment 
in question, this means that we cannot represent the data as the sum of separate effects 
due to A-, B- and (7-classes, together with a residual f which is the same in form for all 
^-WAY CLASSIFICATIONS 
189 
subclasses. The effects of the classes are entangled—or, as we may say, they interact 
This is the origin of the fern " interaction ". 
Suppose, for instance, our data are crop-yields, and membership of the three classes 
corresponds to applications of three manures, nitrogen (A), potash (JB) and phosphate (C). 
The hypothesis represented by (23.27) would then be equivalent to supposing that all three 
manures exerted an effect on yields, but that they did so independently. A given dressing 
of nitrogen would increase the yield by dp whatever dressings of the other fertilisers were 
applied. But it might happen that the response in yield to a0- varied according to how 
much of the others were present—potash might either stimulate the effect of nitrogen or 
inhibit it. If this were so, the fertilisers would interact and the hypothesis (23.27) would 
break down. Significant departures from homogeneity in the interaction terms usually 
lead us to search for possible entanglements of this kind. 
23.23. It must not be overlooked, however, that significant interactions do not 
necessarily imply interaction in any real sense. They may arise from heterogeneity in 
the data. To return to our example of crop-yields, suppose the yields were taken from 
a series of plots which differed materially in natural fertility. It might very well be found 
that the hypothesis (23.27) could not be justified even if the differences in yields due to 
the natural effect were partially absorbed into the coefficients a, b and c. If by chance 
the heavier dressings of fertilisers were applied to plots of greater fertility, the hypothesis 
might be shown as failing and " significant " interactions appear. Such points as this 
require careful consideration in the interpretation of significance, and we shall illustrate 
them in some examples below. 
23.24. Interactions of type AB, involving two classes, are said to be of the first 
order. When considering the general n-w&y classification we shall see that there can 
appear interactions of second, third, fourth . . . order. In fact, the residual in Table 23.5 
is formally equivalent to an interaction of the second order, of type ABC, just as the first- 
order interaction is equivalent to the residual in the two-way analysis of Table 23.2. 
To complete the definitions, we may define the sum of squares between .4-classes as 
an interaction of order zero. The seven constituent items in Table 23.5 would then 
correspond to the following :— 
Order zero 
ler 1 
Order 2 
Interaction. 
.4 
B 
a 
AB 
BC 
(.j A 
A BC 
! ip 
1 
(P 
(<? 
(r 
- 1 
d.f. 
V - 1 
q — 1 
r — 1 
- d(« -1) 
- I)(r - 1) 
- l)(p - 1) 
) ('1 - 1) (r - 
1) 
This illustrates the general symmetry of the analysis and suggests obvious 
generalisations. 
n-way Classifications 
23.25. For instance, with five classes A, B, (7, D and E we may analyse the total 
sums of squares into 25 — 1 = 31 components. There will be 
5 
I 
5 interactions of 
190 
THE ANALYSIS OF VARIANCE 
order zero ; ( ) = 10 interactions of first order, type AB ; f J = 10 interactions of 
second order, type ABC; 
5 interactions of third order, type ABCD ; and one 
residual or interaction of fourth order, type ABODE. The interactions of zero, first and 
second order are of a type already familiar :— 
2(xj.... ~~x )2 
jLs \0Catm — OCa ■— X x- "T" 0C 
E{x. 
■jkl- 
Ju, 
QIC* * • 
X 
fCl . • 
Jilt 
'j.L. 
+ %... +*.k...+x..i. ~x )2 • (23.29) 
The third-order interactions are typified by 
£ (xjkhn. ~ xjkL. ~~ x.klm. ~~ xj.lm. ™ xjk.m. + xjk... + xj.U. + xj..?n. 
+ x.kL. + x.k.m. + x.J?n. - xj.... - x.k... - »..*.. ~ x...m. + x ) 2 - (23.30) 
and the reader will be able to write down the residual for himself. 
As usual, the 31 terms all furnish independent estimators of the variance on the 
hypothesis of homogeneity, and if this is rejected we may consider the alternative 
represented by 
xjklmn ~ aj + "k + Cl + ^m + en + Cjklmn • • • . (23.31) 
The complete analysis in such cases may become very complex., but frequently it is sufficient 
to consider only sums of squares suggested for investigation by prior expectations. 
Example 23.3 
The following data show the percentage water-content in a number of samples of 
a commercial product. Six samples were chosen ; each sample was tested by four different 
operators ; and each operator carried out the determination by three different methods. 
We have thus a 6 x 4 x 3 classification. 
TABLE 23.6 
Percentage Water-Content of Six Samples determined by Four Operators using Three 
Methods. 
Samples. 
1 
2 
3 
4 
5 
1 6 
———- 
1 
59 
57 
55 
60 
61 
1 63 
1 
Tests. 
2 
61 
58 
57 
57 
61 
59 
3 
61 
60 
59 
58 
60 
60 
- - 
1 
57 
57 
55 
56 
59 
62 
2 
Tests. 
2 
60 
58 
66 
57 
58 
63 
Operators. 
3 
58 
58 
56 
57 
59 
61 
1 
55 
61 
54 
54 
61 
64 
3 
Tests. 
2 
58 
60 
52 
58 
57 
62 
_... 
3 
62 
57 
58 
55 
60 
59 
— ■- 
1 
54 
60 
53 
61 
62 
59 
4 
Tests. 
2 
56 
56 
55 
59 
60 
60 
3 
59 
58 
55 
58 
60 
61 
n-WAY CLASSIFICATIONS 
191 
We will first of all analyse the variance systematically with rather more arithmetical 
detail than is usually required, in order to illustrate the process. 
A great deal of work is saved if we take a mean at 60. The table then becomes— 
TABLE 23.7 
Samples. 
1 
2 
3 
4 
5 
6 
Totals 
1 
-1 
-3 
-5 
0 
1 
3 
-5 
1 
Tests. 
2 
1 
-3 
-3 
1 
-1 
7 
3 
1 
0 
-1 
-2 
0 
0 
Totals 
1 
-5 
-9 
-5 
2 
2 
-14 
——— 
2 
Operators 
Tests. 
1 
-3 
-3 
-5 
-4 
-1 
-14 
2 
0 
-2 
-5 
-3 
-2 
3 
_9 
3 
„ 0 
_2 
_4 
-3 
-1 
1 
-11 
Totals 
-5 
_7 
-14 
-10 
-4 
0 
-34 
1 
-5 
1 
-6 
-6 
1 
4 
-11 
!. 
3 
Tests. 
2 
-2 
0 
_8 
-2 
-> 
-13 
3 
2 
-3 
-2 
-5 
0 
-1 
-9 
Totals 
-5 
-16 
-13 
-2 
5 
-33 
1 
-6 
0 
_7 
1 
2 
-1 
-11 
4 
Tests. 
2 
-4 
-4 
-5 
-1 
0 
0 
-14 
3 
-1 
-2 
-5 
-2 
0 
1 
_9 
Totals 
-11 
-6 
-17 
-2 
2 
0 
-34 
nrnnfAT.c: 
-20 
-20 
-56 
-30 
-2 
13 
-115 
We have shown the totals of the tests for each operator, of the tests for all operators, and 
of samples for each test. 
We now form three two-way tables from this by adding the values of one of the 
variates, e.g.-— 
m 
1AJBJLlii 23.8 
Operators. 
Samples. 
1 
2 
3 
*db 
5 
6 
Totals 
1 
1 
- 5 
- 9 
_ 5 
2 
2 
- 14 
2 
- 5 
_ 7 
- 10 
- 4 
6 
- 34 
3 
- 5 
- 2 
- 16 
- 13 
- 2 
5 
- 33 
4 
- 11 
- 6 
- 17 
-2 
2 
0 
- 34 
X KJ .1. A Ijo. 
- 20 
- 20 
- 56 
- 30 
- 2 
13 
- 115 
192 
THE ANALYSIS OF VARIANCE 
TABLE 23.9 
Tests. 
Samples. 
1 
2 
3 
4 
5 
6 
Totals 
1 
- 15 
— 0 
- 23 
- 9 
3 
8 
- 41 
2 
- 5 
- 8 
- 21 
- 9 
_ 4 
4 
■- 43 
3 
0 
- 7 
- 12 
- 12 
- 1 
1 
- 31 
Totals. 
- 20 
- 20 
- 56 
- 30 
- 2 
13 
- 115 
TABLE 23.10 
Operators. 
Tests. 
1 
2 
3 
Totals 
1 
- 5 
- 7 
- 2 
- 14 
2 
- 14 
- 9 
- 11 
- 34 
3 
- 11 
- 13 
- 9 
- 33 
. 
- 11 
- 14 
- 9 
- 34 
Totals, j 
\ 
- 41 
- 43 | 
- 31 
110 1 
1 
As we have inserted the totals of various kinds in Table 23.7 these subsidiary tables 
oan be picked out at once ; but in general, totals are not available in the original (and for 
four-way classifications it is difficult to find a form of tabular presentation which will permit 
of their insertion) so that the tables have to be separately compiled. In practice I find it 
convenient to do so in any case to avoid picking out the wrong figures in the original table. 
Pursuing the condensation process, we should now derive three one-way tables from 
Tables 23.8 to 23.10, but in fact the row and column totals already give us what is required 
(and incidentally provide a check on the arithmetic). 
Now we proceed to find the various sums of squares. For the total of all observations 
we find — 115, and for the sum of squares of observations 653. Thus 
- 115 
x 
Nxl = 
115s 
1-597,222 
Z (x, 
jkl 
= 183-680,556 
= 653 - 183-680,556 
= 469-319,444 
with 6x4x3 — 1=71 degrees of freedom. 
(23.32) 
w-WAY CLASSIFICATIONS 193 
For the interactions of order zero we require the sums of type 
Z(x* - x Y = E{x, )2 - Nx2 , 
where summation takes place over the N values. It is, however, unnecessary to work out 
the means xjmt. Consider, for example, the sum of squares between samples. From the 
totals of Table 22.8 or Table 22.9 we find (j denoting samples)— 
E(12xjmm)* = (- 20)2 + (- 20)a + . . . + 132 
= 5009, 
where the summation is over six values only. Thus, for summation over the 72 values— 
E(x.} y = ii- 5009 = 417-416,667. 
v j..t 122 
Hence 
Zty.. ~^...)2 = 417-416,667 - 183-680,556 
= 233-736,111 (23.33) 
with 6 — 1=5 d.f. 
Similarly (k denoting operators) we find— 
3597 
E(x k -x )a = ~— - 183-680,556 
= 16-152,778 (23.34) 
with 3 d.f. ; and (I denoting tests)— 
Six j - x )2 = - - - 183-680,556 
= 3-444,444 (23.35) 
with two degrees of freedom. 
Now we require first-order interactions. We have (summation being over the N 
values)— 
%(xjk. ~~~ xj.. ""■ x.k. + x..yz — % (xj/c. — x..y2 + ^ (xj.. ~~~x...)^ 
+ £ (xmkm — &'...)2 ~~ 22" {xjkm — xmm) (xjm — xmmm) 
- %E{x.jkm - s...) (xukm -«...) 
= Z(xikm -xmJ*-Z(zimm - xmmm)* - 2 (zmkm -a;...)2 (23.36) 
and thus the first-order interaction term is ascertainable from E (%:,)2 and quantities which 
have already been computed. 
From the body of Table 23.8 (remembering that summation relates to 72 values and 
hence that each value in the table is counted 3 times) we find 
^ 1 4.QQ 
:£{18 + (- *)*I"' • ■}= 1 
= 499-666,667. 
The interaction term is then 
499-666,667 - 183-680,556 - 233-736,111 - 16-152,778 = 66-097,222 . (23.37) 
with (6 — 1) (4 ~- 1) = 15 d.f. 
Similarly in the body of Table 23.9 we find for the sura of squares 1915. Hence the 
interaction of samples and tests is 
1Q1 % 
— - 183-680,556 - 233-736,111 - 3-444,444 = 57-888,889. . (23.38) 
4 
£(%•.)'=-.,s {i« + (-r,)* + . . .} 
A.S.—VOL. II. 
O 
194 
THE ANALYSIS OF VARIANCE 
In the body of Table 23.10 the sum of squares is 1245. Hence the interaction of tests 
and operators is 
1245 
6 
183-680,556 - 16152,778 - 3-444,444: = 4-222,222. 
. (23.39) 
Finally, the residual is given by the difference of the total sum of squares and the 
interactions already found, namely by 
469-319,444 - 233-736,111 - 16-152,778 - 3-444,444 - 66-097,222 - 57-888,889 
— 4-222,222 = 87-777,778 . . . (23.40) 
with (6 — 1) (4 — 1) (3 — 1) = 30 degrees of freedom. 
We can now make up the table of variance analysis as follows :— 
TABLE 23.11 
Analysis of Variance of Data of Table 23.7. 
Sum of Squares. 
Between samples (S) 
„ operators (O) 
tests (T) 
Interaction SO 
OT . 
ST . 
Residual 
Totals 
233-736 
16-153 
3-444 
66-097 
4-222 
57-889 
87-778 
469-319 
5 
3 
2 
15 
6 
10 
30 
71 
Quotient. 
46-747 
5-384 
1-722 
4-406 
0-704 
5-789 
2-926 
We proceed to discuss the data in the light of this analysis. 
The most striking feature of the table is the size of the quotient between samples. 
46-747 
The variance ratio here is 
2*926 
15-976, with a corresponding value of z equal to 1-38. 
For v± = 5, v2 = 30 the 0-1-per-cent. point is 0-8554, and the ratio is highly significant. 
We remark in passing on a point which will be taken up later. The ordinary 2-test 
gives the probabilities that the ratio of two variances chosen at random does not exceed 
a given value. But in this case we have deliberately picked out the largest quotient for 
one of our estimates. If z had fallen at the 5-per-cent. level we could not have argued that 
the odds were 19 to 1 against the event. They are very much less, since we have 
deliberately chosen the largest value for comparison with the residual. However, in the present 
case our probability is so small that we can confidently assume the significance of z (see 
23.27 below). 
Our first inference, then, is that the whole sample is not homogeneous. There appear 
to be variations from sample to sample which are not assignable to differences between 
tests or operators, and if we wished to standardise our product with greater accuracy we 
should be led to examine the manufacturing process. This conclusion is, however, subject 
to a point which we discuss in the next example. 
Having rejected the hypothesis of homogeneity we are now faced with the question 
whether the other quotients in Table 23.11 can be compared so as to assess the relative 
^-WAY CLASSIFICATIONS 
195 
variability of the other factors. We must then take a new hypothesis, and we will suppose 
that the variable may be written 
xjkt = aj + %jkl> .... . (23.41) 
where a$ is an unknown quantity expressing the accepted variation between samples. 
Unless there is something very peculiar about the tests or operators it is reasonable to 
suppose that the variation between samples can be isolated in this way. We will now 
suppose that the £'s, not the #'s, are distributed normally with common mean and variance v. 
If the values given by (23.41) are substituted in the various constituent items of Table 
23.5, it will be found that except for the variation between samples all the other sums of 
squares assume the same form with £ written instead of x. This, of course, follows from 
23.20 of which our present hypothesis is a particular case. On the hypothesis of (23.41) 
we are thus enabled to compare the quotients in the table in the usual way. The element 
of variation between samples has, so to speak, been abstracted from the discussion. 
We then turn to the sum of squares between operators in Table 23.11. The variance 
5-384 
= 1*84. For vx = 3, v2 = 30 this is not significant. Similarly, for the sum 
ratio is 
2-926 
1-722 
of squares between tests we find a ratio of ttt^^j again not significant. Provisionally we 
2-926 
conclude that there is no-evidence of variation between operators and tests, apart from 
pure sampling effects. 
Now we have to consider the interactions. For that of SO we have the variance ratio 
4*406 
——- = 1 -51, which is not significant. We find the same for the interaction ST. For 
OT we have (taking the larger variance as the numerator) 
2-926 
i log< 
0-703 
0-713, vx = 30, v< 
6. 
This value is just beyond the 5 per cent, point and, judged by itself, might have been regarded 
as significant ; but taken in conjunction with the others it may, perhaps, be accepted as 
a permissible sampling fluctuation. 
To sum up, therefore, the only evidence of deviation from homogeneity appears in the 
sample-differences, and we see no reason to reject the hypothesis represented by (23.41). 
Since all the other items in the analysis, apart from that between samples, are 
homogeneous, we could condense the table into the form- 
Sum of Squares. 
Between samples 
Remainder 
Totaxs 
233-736 
235-583 
469-319 
d.f. 
5 
66 
71 
Quotient. 
46-747 
3-569 
The reader may wonder why, in carrying out the tests of significance, we have 
throughout used the residual quotient as the denominator of the variance ratio, and not, for instance, 
one of the interactions. There are two reasons. First, the residual has more degrees of 
freedom, so that it is preferable notwithstanding that the 2-test is valid for any number 
196 
THE ANALYSIS OF VARIANCE 
of degrees of freedom. Second, the residual is not so likely to be affected by interactions 
which, though not emerging into significance, might nevertheless exist. But once we have 
established that an interaction is not significant, there is no reason why it should not be 
amalgamated with the residual, as in the table on page 195. 
Example 23A 
There is a point of great importance concerning the inference from analyses of variance, 
which we will illustrate by an imaginary example based on the data we have just 
considered. Suppose our analysis of variance were of the following form :— 
Sum. of Squares. 
Between samples .... 
Between operators .... 
Totals 
125 
60 
150 
48 
383 
d.f. 
5 
3 
15 
48 
71. 
Quotient. 
25 
20 
10 
1 
We will suppose that the sums of squares between tests and the other first-order 
interactions are not significant, so that they can be amalgamated with the residual to give a 
remainder with 48 degrees of freedom as shown. 
On this evidence the sums of squares between samples and between tests are both 
significant, as also is the interaction SO. What inference can be drawn about the 
variability of the product from one sample to another ? We know that the readings differ 
significantly; but may not this difference itself be due to the demonstrated variation 
between operators, or does it really exist ? Is there in fact any variability in the water- 
content of the product, apart from the sampling effect in homogeneous variation V 
The significance of the SO interaction means that we cannot now regard the effects 
of operator and sample as independent. We must consider the possibility of entanglement. 
This is not the only explanation—there may be some other specific cause of variation 
present which we have not thought of, and on which our present data throw no light. But 
in this case there is sonie prior possibility that samples and operators are " entangled M or 
interacting in the ordinary sense. An operator may be getting better results from his 
material when it has high water-content than in the reverse case ; or, knowing that the 
mean content is near 60 per cent, he may unconsciously (or even consciously) bring his 
determinations nearer to that figure and hence reduce their spread. 
In a case of this kind, and indeed in all statistical inquiries, it is important to have 
a clear idea of the question which is being asked and of the population to which it relates. 
We have had a number of samples and have tested them, by four operators each using 
three tests. So far as we can see, the tests are equivalent but the operators are not. All 
the same, we are not very interested in the variation among operators (unless this is 
an experiment in psychology and not in chemistry). What we want to know is whether 
the water-content varies in reality, that is to say as the average of a large number of 
determinations by different operators. Our particular four are themselves samples of 
a population of operators. 
^-WAY CLASSIFICATIONS 
197 
If we confine our attention to the four operators and suppose that each has a specific 
reaction to particular samples YYhj^ SO that 
xjk = mjk + Sjk (23.42) 
where f is a normal random residual with variance v for all j, k, then in the usual 
way we find 
E E (xjk - xjm - xJc + xty = (p - 1) (q - 1) v + E (mjk - mjm ~~ mmk + m J2 . (23.43) 
But suppose we consider the matter from a different viewpoint. Regard m^k as itself 
chosen at random from a normal population of operators with variance v'. Then, taking 
expectations of this population in addition, we find from (23.43) 
E E (xjk ~~ xJm - xmk + a;..)* = (p - 1) (g - 1) (v + vf). . . (23.44) 
Thus the interaction term provides an unbiassed estimator of the variance v -+- v' of xjk. 
By " unbiassed " in this connection we mean that the average over all determinations and 
all operators will give the variance of xjk in the population of all determinations and all 
operators. 
Similarly we shall have, on the same interpretation, 
E E (xu - x.y = (p - 1) (v + v')\ 
E E (x,k - xj* « (q - 1) (t; + v')] ' K"6M) 
and hence the ratio of either interaction of zero order to the first-order interaction may be 
tested for homogeneity. Our analysis then becomes— 
Sum of Squares. 
Between samples .... 
Between operators .... 
Residual (SO) 
JLOrAJuS ..... 
125 
60 
150 
335 
vJL • X • 
5 
3 
15 
23 
Quotient. 
25 
20 
10 
Neither ratio is now significant. For the sum of squares between samples we have 
a ratio of 2-5, vx 
!">, V» 
15, which is below the 5 per cent, point. 
Thus we should conclude that, regarding the data as a member of possible samples from 
all possible operators, there is little or no evidence of real variation from sample to sample. 
This is quite consistent with the inference we drew at the beginning of the example as to 
the " significance '' of the terms concerned, though at first sight it appears directly 
contradictory. In the first case we inferred that for these four operators there were 
significant differences in their determinations for the samples, so that sample-differences are 
" real " in the sense that they cannot be attributed solely to random variation in 
homogeneous material. In the second case we enlarge the domain by considering operators as 
subject to " error " in the sense that one human being differs from another, and find that 
sample-differences can now be ascribed to variation in the population of operators. 
No further emphasis is needed on the care necessary for the proper interpretation of 
the results of an analysis of variance. The nature of the population which, is being 
considered should be brought explicitly to mind in every case ; and the reader should form 
198 THE ANALYSIS OF VARIANCE 
the habit of asking himself, whenever a result is found to be " significant " : significant 
of what ? 
Arithmetic of Variance Analysis 
23.26. Before considering further examples we will dispose of a few points arising 
from the calculation of the constituent sums of squares and the application of the 2-test 
in determining the significance of variance-ratios. 
The calculation of sums of squares for an %-way classification can very conveniently 
be carried out by the use of a punched-card system when the data are numerous, and some 
remarkable computing feats have been performed by this technique. For ordinary 
laboratory work with a machine, the process of Example 23.3 is possibly the best, though some 
modifications may be made to suit individual taste. 
The main work lies in computing the total sum of squares. This is done by finding 
the sum of squares of observations from the original data (with a convenient working 
mean) and the sum of observations obtained at the same time. The formula 
^ \xju X..J — ^ xm iy/x... 
~ ^xm ~~~ x... £ xjki - (23.46) 
then gives the total sum required. The quantity Nx2mum is constantly needed and should 
be recorded. It is useful to preserve a few more decimal places than will ultimately be 
used in the final presentation of the analysis. 
The original data are then condensed into n (n — l)-way tables by summing over 
each class in turn. In Example 23.3 this was done so as to give three tables : Operators- 
Samples, Tests-Samples and Operators-Tests. The main body of these tables gives means 
of the type xjkm multiplied by a constant factor. A further condensation will give f 
sets of means of type x3-mm ; and so on, as far as is required. 
From the condensed tables we can then determine the sums of squares of means of 
various orders, and hence the interactions. The main pitfall lies in the way of the 
application of the correct multipliers and divisors—it has to be borne in mind that the summation 
takes place over all values of the sample. 
Suppose, for example, we have a four-way classification into classes with p, q, r and s 
numbers of members. The first condensation gives us four tables of which a typical one 
is p x q x r, based on the sum of s members. The next condensation gives us six two-way 
tables typified by p x q, based on the sum of rs members. The third gives us four 
oneway tables such as p, based on qrs members. Consider the variance between /^-classes :— 
Ufa -x f=2xl - Nx* .... (23.47) 
In the condensed one-way table of p classes each term is to be counted qrs times, and 
thus, if S is the sum of squares in this table as it stands, 
v 
S =£(qrsxj„y. 
Thus, summing over all members, we find 
Sxi = qrs-—~- ■ 
3 ' (qrs)2 
= ...... [JiO.^xCi) 
qrs 
whence (23.47) gives the zero-order interaction for p-classes. Similarly for q, r and s. 
USE OF THE 2-TEST FOR SEVERAL VARIANCE-RATIOS 199 
For the first-order interaction we have 
Zfrjk.. -xj... -«.&.. + ^....)2 
= Z(zjkm. -x....)2 -27 (a,... -*....)*-£(*.*.. ™-x....)2 . . (23.49) 
The last two terms on the right have already been found. We require 
£(*&.. -*...)* =£*&.. -#??... • • • .(23.50) 
If S' is the sum of squares of elements in the body of the two-way table found by adding 
r- and s-items, we find 
^ xjk.. === ? ..... (23.51) 
rs 
and so on. The general process will now be clear. 
Unfortunately there is no convenient independent check on the calculations. The 
various condensed tables are self-checking since their totals are the sum of all observations, 
but the sums of squares do not check with anything. It is, of course, possible to evaluate 
each individual term in the residual and to check by summing squares, but this is too 
laborious for use except in the simplest cases. 
Use of the z-test for Several Variance-ratios 
23.27. In the complete analysis of n classes there are 2n — 1 elements, and the 
number of variance ratios arising for test may be considerable. The z-test gives the 
probability that a particular value chosen at random will be exceeded. If therefore we pick 
out the largest ratios for test, the chance that one of them is " significant " in the sense 
of exceeding the lOOP-per-cent. point is a good deal greater than P, and we run into the 
danger of attributing significance to what may be a pure sampling effect. 
Suppose we make r different and independent tests of r values of z. The chance that 
each does not exceed a fixed value (depending on the number of degrees of freedom) is 
1 — P, where P is some assigned level of significance. Hence the chance that none of 
them exceeds its appropriate value is 
(1 — P)r = 1 — rP, approximately, . . . (23.52) 
provided that P and rP are small. For instance, if P = 0-01 and r = 7 the probability 
that no z exceeds its appropriate significance value is 0-93, and thus there is a probability 
of 0-07 that at least one of them will do so. 
In practice the problem of numerous comparisons is more complicated because they 
are not independent. In such circumstances our judgment of significance has to 
incorporate an element of the intuitive. However, if all the comparisons are based on the 
common residual quotient it is possible to find the probabilities that the largest of r values 
exceeds assigned values. The resulting expressions are complicated, even when all the 
sums of squares have the same degrees of freedom, but reference may be made to Hartley 
(1938) for approximations and to Cochran (1941) and Finney (1941a) for exact expressions. 
The conclusion reached by Finney is that if the degrees of freedom in the residual are 
sufficiently numerous the ratios may be treated as completely independent. 
23.28. There is a particular case of the n-w&y classification which is worth special 
mention, namely, that for which each classification is a simple dichotomy, so that there 
are 2n subgroups. This case arises frequently when so-called " factorial" experiments 
are being conducted to determine the effect of a treatment which is either applied or with- 
200 THE ANALYSIS OF VARIANCE 
held. The analysis of variance remains the same in principle, but of course the arithmetic 
becomes a good deal simpler. 
Example 23.5 (F. Yates, Supp. J.R.S.S., 1935, 2, 181) 
An area of ground was sown with peas and divided into 24 plots in the manner shown 
in Table 23.12. The plots received, or did not receive, dressings of nitrogen (N), phosphate 
(P) and potash (K) in the manner shown, the yields in pounds being given in the table. 
TABLE 23.12 
Yields of Peas and Manurial Treatments on 24: Plots 
PK 
49-5 
NP 
62-8 
N 
59-8 
NPK 
58-5 
P 
62-8 
NPK 
55-8 
46-8 
■ NK 
57-0 
A' 
55-5 
P 
56-0 
N 
69-5 
K 
55-0 
N 
62-0 
NPK 
48-8 
NP 
52-0 
51-5 
NK 
57-2 
NP 
59-0 
K 
45-5 
P 
44-2 
NK 
49-8 
PK 
48-8 
53-2 
56-0 
There is some purpose here in the alternation of treatments, but that need not concern us 
for the present. We have 24 observations in four classes, viz. blocks (3), nitrogen (2), 
phosphate (2) and potash (2), giving 3x2x2x2 = 24 records. 
Condensing f the table by adding blocks we get the following :— 
No treatment N P K NP NK PK NPK Total 
154-3 191-3 163-0 156-0 173-8 164-0 151-5 163-1 1317-0 
Condensing according to the three treatments we have— 
p 
not-P 
Totals 
N 
336-9 
355-3 
692-2 
not-N 
314-5 
310-3 
624-8 
Totals 
651-4 
665-6 
1317-0 
p 
not-P 
Totals 
K 
314-6 
320-0 
634-6 
noWv 
336-8 
345-6 
682-4 
Totals 
651-4 
665-6 
1317-0 
USE OF THE s-TEST FOR SEVERAL VARIANCE-EATIOS 
201 
K 
not-AT 
Totals 
N 
3271 
365 1 
692-2 
not-N 
307*5 
317-3 
624-8 
Totals 
634-6 
682-4 
1317-0 
We omit the remaining calculations. The analysis in its final form is given i 
Oil *li«l • 1 t r a 
IB 
TABLE 23.13 
inalysu of Varia/nee, of the Data of Table 23.12 
Hums of Squares. 
Between blocks ( 
Y 
J) 
Interaction UN 
*» 4 } i 
<* t * ' * \ 
,, A' i\ 
„ HXK 
HPK 
Residual (HXPK 
Totals 
li) . 
# * 
) 
177-803 
189-282 
8-402 
95-202 
94-255 
2-260 
23-685 
21-281 
33 134 
0-481 
25-302 
36-004 
3-782 
37-003 
128-489 
876-365 
d.f. 
2 
1 
1 
1 
2 
2 
2 
1 
1 
1 
2 
2 
2 
1 
2 
Quotient. 
88-90 
189-28 
8-40 
95-20 
4713 
113 
11-84 
21-28 
3313 
0-48 
12-65 
18-00 
1-89 
37-00 
64-24 
\\V have carried out the analysis in full so as to illustrate the arithmetical process 
' a four way classification, hut we may note at once that it is unduly elaborate. There 
> only 24 observations in the data and we cannot expect them to provide all the answers 
the questions which we could frame as to the significance of the various constituent 
ms in the analysis. 'Phis is borne out by the s-test. The residual variance is 64-24 
th two decrees of freedom. For r, = 1, vz = 2 the variance ratio at the i-per-cent. 
int is 9S-49 and that for r, 2, r, ~ 2 at the same point is 99-00. Only values greater 
an about 100 times 04-24 or less than 1/100th of that value would thus be significant, 
ily the interaction PK falls outside this range, and even this, among so many, can hardly 
regarded as significant. 
The inquiry is not, however, completely frustrated. Since the second-order inter- 
turns are not significant, we amalgamate them with the residual to give a remainder 
m of squares of 230-5K0 with nine d.f. and a quotient of 25-62. It will now be found 
202 
THE ANALYSIS OF VARIANCE 
that among the first-order interactions only two are significant, PK and BP being too 
small. Had they been too large we might have attributed some genuine significance to 
this result, but it is not very plausible to suppose that there is a'" real " interaction between 
blocks and phosphate, or that phosphate and potash inhibit each other's action. The 
differences from expectation are more probably due to individual soil variation from plot 
to plot. 
If we accept the first-order interactions as not significant, we may amalgamate them 
with the remainder to give the following :--~ 
Sum of Squares. 
Blocks . 
N . . . . 
P . . . . 
Remainder 
Totals 
177-803 
189-282 
8-402 
95-202 
405-676 
876-365 
d.f. 
2 
1 
1 
1 
18 
23 
Quotient. 
88-90 
189-28 
8-40 
95-20 
22-54 
Here the P-quotient is not significant, but the variance ratio for blocks, 3-99, is near the 
5-per-cent. point. The jV-quotient will be found to be significant at the 1-per-cent. point, 
the if-quotient near to the 5-per-cent. point. Our conclusion is that there is strong 
indication that nitrogen influenced the yield, some indication that potash did so, and little 
indication that phosphates did so ; and that there is ground for suspecting heterogeneity in the 
soil partly because of the difference between blocks and partly from some of the first-order 
interactions. 
In this case, of course, we knew already more or less what was to be expected of these 
data and are the readier to accept the conclusions on that account. Had we known nothing 
of the effect of fertilisers on leguminous crops our conclusions on such slender evidence 
must have been very tentative indeed, particularly if we wished to extend them to peas 
grown on other soils under different climatic conditions with different amounts of fertiliser. 
Example 23.6 (C. E. Gould and W. M. Hampton, Supp. J.E.S.S., 1936, 3, 137) 
In the manufacture of optical glass there appear small bubbles known as "seed", 
which constitute a defect. The glass is made in " pots " which take about a year to 
prepare, and are run continuously over long periods when once started. There are two pots 
to a furnace and materials are introduced into a pot from time to time which, after fusion, 
provide a " run " of glass. Each run provides several days' work, one day's work being 
known as a " journey ". At each journey quantities of glass are drawn from the pot and 
blown into " cylinders ", there being about 18 or 20 to the journey. For the purposes of 
the experiment three cylinders were chosen, the third, tenth and sixteenth, and pieces of 
regular size cut from them for examination as to frequency of seed. The first five journeys 
of each of five runs were sampled. 
We have here a four-way classification 2 (pots) x 5 (runs per pot) x 5 (journeys per 
run per pot) x 3 (cylinders per journey per run per pot). The actual dates of the runs 
were February 16th, May 23rd, June 12th, September 1st and December 6th, so that the 
manufacturing period covered about ten months. We shall assume that the glass was 
USE OF THE z-TEST FOR SEVERAL VARIANCE -RATIOS 
203 
of the same type throughout, although in actual fact it was different in one or two cases 
—but not sufficiently different to affect the analysis. 
The topic of main interest here is whether the frequency of seed varies significantly 
according to the four factors concerned. If so, the alteration of manufacturing conditions 
may improve the wastage due to seed ; but if not—and the variation is the kind of thing 
which can be accounted for as chance fluctuation in sampling from a homogeneous 
population—there is little hope of improvement except perhaps by a radical alteration in the 
process affecting all pots, runs and journeys alike. 
TABLE 23.14 
Frequency of " Seed " in Samples of Glass 
tJ X 
Run 1\ 
2< 
3 
4 
5 
i/ i. 
<> 
3 
(J 1 
Run 3< 
Run 4< 
3 
5 
J I 
«■■> 
aj 
3 
4 
5 
J I 
Run 5< 3 
4 
5 
Cyl. 1. 
47 
55 
35 
78 
33 
52 
21 
31 
43 
37 
50 
33 
24 
18 
28 
24 
24 
76 
31 
34 
120 
109 
69 
Pot 1. 
Cyl. 2. 
56 
89 
57 
67 
40 
66 
61 
39 
72 
51 
61 
27 
39 
18 
42 
49 
21 
69 
48 
54 
24 
122 
119 
Cyl. 3. 
100 
93 
56 
113 
128 
36 
25 
52 
67 
60 
49 
24 
43 
28 
43 
42 
51 
48 
42 
40 
46 
120 
120 
60 
Before plunging into the analysis of variance it is as well to look over the data to see 
whether they themselves suggest any lines of inquiry. We observe considerable 
variability from journey to journey within the same run, J3 and J4 of run 5 being conspicuous 
in pot 1 ; and in run 1 the numbers of seed appear to increase from cylinder 1 to cylinder 3 
in a rather exceptional way. The runs themselves seem to differ materially. Prior con- 
204 
THE ANALYSIS OF VARIANCE 
siderations also suggested an examination of the way in which frequency of seed varied 
between pots, since they were chosen so as to differ substantially in constitution. 
A complete analysis of variance of the data is as follows :— 
TABLE 23.15 
Analysis of Variance of the Data of Table 23.14. 
Sums of Squares. 
Between pots (P) .... 
„ runs (R) 
journeys (J) . 
„ cylinders (<7) . 
Interaction PR . 
PJ • 
PC . 
RJ 
RO 
JO 
PRJ 
RJG 
JCP 
CPR 
Residual (PRJC) 
Totals . 
898 
14,059 
4,355 
10,631 
16,133 
4,081 
587 
45,934 
11,626 
2,540 
9,711 
12,472 
1,656 
1,862 
8,110 
144,655 
d.f. 
1 
4 
4^ 
2 
4 
4 
2 
16 
8 
8 
16 
32 
8 
8 
32 
149 
Quotient. 
898 
3,515 
1,089 
5,315 
4,033 
1,020 
293 
2,871 
1,453 
317 
607 
390 
207 
233 
253 
The second-order interactions will be found non-significant, so we amalgamate with 
the residual, giving a sum of squares 33,811, d.f. 96, quotient 352. 
It then appears that of the first-order interactions PR, RJ and RO are significant and 
PJ may be so. There is beginning to appear evidence of heterogeneity, and that of a rather 
complicated kind. It seems that pots are interacting with runs, runs with, journeys and 
runs with cylinders. 
Taking 352 as the quotient, we find that except for P the zero-order interactions are 
significant. The five jR-means are 68-50, 62-67, 42-23, 47-77 and 59-27, so that the variation 
of runs is not a simple rise or fall, which could have been explained as a time-effect. The 
five J-means are 58-93, 55-37, 49-97, 64-83 and 51-33, again not a regular' effect. The 
O-means are 44-46, 59-68 and 64-12, which are significantly different. Inspection of the 
table suggests that the first run is the source of the trouble. 
With data as heterogeneous as these it is rather difficult to set up a plausible hypothesis 
to test. The interactions of first order suggest that no simple additive effects of the four 
factors will explain observation, and if these terms are used as denominators in tests of 
variance ratios the variation between classes appears on the whole non-significant on the 
usual hypotheses. The analysis, then, suggests several subjects for inquiry as concerns 
the homogeneity of the data, but does not suggest any simple explanation of the observed 
figures. The reader may care to refer to the original paper for a more complete discussion 
of the subject. 
NON-NORMAL DATA 205 
23.29. Perhaps we may pause at this point to review progress. We have seen 
that for an w-way classification of the special type wherein each subclass contains a single 
member, the sum of squares of all observations about their mean can be exhibited as the 
sum of a number of such sums. On the hypothesis of normality and homogeneity each 
constituent sum of squares, on division by its appropriate number of degrees of freedom, 
gives an estimator of the parent variance, and each is distributed as x2 independently of 
the others. The hypothesis of homogeneity can then be tested in Fisher's ^-distribution, 
subject to the adoption of a conservative attitude where many tests are made on the same 
data. If the hypothesis is rejected we may replace it by a simple form in which the effects 
of the different classes are additive, provided that the interactions are not significant. 
The particular ratio chosen for a test depends on the hypothesis concerned, and it is 
important to have a clear idea of the exact question to which an answer is sought. 
23.30. In the next chapter we shall consider the case when the numbers in different 
subclasses are not equal, discuss the additive hypothesis in more detail, examine the 
relationship of variance- and regression-analysis, and extend our results to the analysis of covariance. 
We conclude this chapter by an examination of the important question : what can be 
done with the analysis of variance when the variation is not normal ? 
Non-normal Data 
23.31. The analysis of a sum of squares into its constituent sums can, of course, be 
undertaken in all circumstances, but the various quotients may not continue to provide 
unbiassed estimators of the parent variance if the population is not-normal. What is 
equally serious, the constituent sums of squares may not be distributed independently. 
Thus, when parent normality cannot be assumed, the quotients in the analysis table are 
no longer equal within sampling limits and their ratio is distributed in unknown form ; and 
even if the form were known it would probably depend on parent parameters and hence 
fail to provide an exact test of significance. 
The problem has been considered in four ways :— 
(a) Sampling experiments have been undertaken to see how far moderate deviation 
from normality affects the ^-distribution ; 
(/>) Attempts have been made to find transformations of the variate to throw the 
parent distributions into forms with equal variances, at least approximately, 
before the analysis is applied ; 
{(') By introducing a randomising process into the data before they are collected, 
attempts have been made to preserve the ^-distribution as a close approximation 
—this amounts to a change in the nature of the inference, as we shall see below ; 
(d) Tests have been found which can be applied to ranked data irrespective of the 
parent form—this approach is a particular case of (c), but seems to merit special 
mention. 
We proceed to consider these four possibilities. 
23.32. The arithmetic entailed by a single analysis of variance, even in simple cases, 
implies that an extensive sampling inquiry into the distribution of z in non-normal 
populations would be a very formidable undertaking. E. S. Pearson (19316) has studied in some 
detail the case of a one-way classification with unequal numbers, when the distribution 
206 THE ANALYSIS OF VARIANCE 
of z becomes equivalent to that of the correlation ratio r\2. Six populations were chosen, 
characterised by the following values :— 
iff1 = 0, /?2 = 2-50 (symmetrical platykurtic) ; 
f}± = 0, /?2 = 4-1 (symmetrical leptokurtic) ; 
P± = 0, /32 = 7-05 (symmetrical leptokurtic) ; 
/3X == 0-2, 0a = 3-3 (skew, Type III); 
pt = 0-49, 02 = 3-72 (skew, Type III) ; 
/3X = 0-99, /?2 = 3-83 (very skew, Type I, with abrupt start). 
The results suggested that for this range of ^ and /?2 the distribution of z is adequately 
represented by Fisher's distribution, and that therefore the homogeneity test may be 
applied. The case when the variation changed from group to group was not considered. 
It was also concluded that " it seems probable that the more elaborate forms of analysis 
of variance are also of fairly wide application ". 
Some work by Eden and Yates (1933) is often referred to as experimental confirmation 
of the same kind, but in fact it was carried out with rather a different object, that of 
confirming the 3-test for data under randomisation (see below, 23.36). 
Variate Transformations 
23.33. Suppose f is a new variate f (%). Then approximately we shall have 
dp2 
dx 
var f = ( ~ ) var x (23.53) 
If now the parent variance of the ^-distribution is related in some known manner to the 
mean, say / (m) — v, we have 
/d£\2 
varl=U)/(m)- 
As a further approximation, if x varies about m by small quantities we have 
varf-^V/^) (23.54) 
Now we wish f to have a constant variance, say A, and if this is so, 
dl I 1 
or 
dx \J f (x) 
„, dx. . . . . . (23.55) 
Although this expression is arrived at by approximation we are entitled to hope that 
the variate £ will have almost constant variance, and at any rate a more stable variance 
fanan x* i 
For instance, if the original variation is thought to be of the Poisson type we have 
f(x) = x, and from (23.55) are led to consider the transformation 
f = I X_ dx % 
J V# 
= y/x, (23.56) 
VARIATE TRANSFORMATIONS 
207 
If we choose X to be J, 
p (1 —• p) we have 
Similarly, if the variation is of the binomial type with variance 
V* 
Vip (i - p)} 
dp 
sin 
~i 
Vx, 
. (23.57) 
on suitable choice of X. 
23.34. These transformations are designed to (i stabilise " the variance. They do 
not necessarily bring the variate closer to normality, though in some cases they will do so 
—we have, for instance, seen that Vz2 tends to normality quicker than %% (12.7). The 
following values (Bartlett 1936d) illustrate the way in which the square-root transformation 
stabilises the variance of a Poisson distribution :— 
Mean m. 
0-0 
0-5 
1-0 
2-0 
3-0 
4-0 
6-0 
9-0 
120 
15-0 
Variance of Poisson 
Variate y/x. 
0-000 
0-310 
0-402 
0-390 
0-340 
0-306 
0-276 
0-263 
0-259 
0-256 
Variance of Poisson 
Variate <\/(x -f- J). 
0-000 
0102 
0-160 
0-214 
0-232 
0-240 
0-245 
0-247 
0-248 
0-248 
The term 1 in the third column was added by Bartlett on the analogy of a continuity 
correction. For m > 3 the variance is evidently quite stable. 
now, having stabilised the variance, we carry out an analysis in the ordinary 
way, our residual sums of squares divided by the appropriate degrees of freedom will 
continue to be unbiassed estimates of the common variance v, even if there are differences 
between the means of the classes. Instead of assuming as part of the hypothesis that the 
different classes are distributed with the same variance, we have transformed the variate 
so that this shall be so, at least to a close approximation. Relying further on the result 
that the transformed variates approximate to normality, or that if they do not the 
difference will not seriously vitiate the 2-test, we may apply that test to the transformed data 
in the usual way. 
Example 23.7 (Bartlett, 1936d) 
Table 23.16 shows the number of wheat seeds out of 50 which failed to germinate in 
four repetitions of an experiment with different treatments. 
208 
THE ANALYSIS OF VARIANCE 
TABLE 23.16 
Germination of Wheat Seeds 
Number of 
Experiment. 
1 
■ 2 ' 
3 
Totals 
! 
1 
10 
8 
5 
1 
24 
2 
11 
10 
11 
6 
38 
Number of Treatment. 
3 
8 
3 
2 
4 
17 
4 
9 
7 
8 
13 
37 
5 
7 
9 
10 
7 
33 
6 
6 
3 
7 
10 
26 
7 
9 
11 
11 
10 
41 
Totals. 
60 
51 
54 
t j JL 
216 
In point of fact, treatment 7 was a repetition of treatment 6, the others being different. 
The point of interest is whether the treatments exert any effect on germination. We shall 
not inquire into any differences between experiments (which appear to be negligible from 
the row totals) and shall accordingly consider this as a one-way classification into seven 
•classes, four numbers to the class. 
The presumption is that in any given class the variation is of the binomial type. We 
might apply the sin"™3-^^ transformation, but will adopt instead an ad hoc square-root 
transformation obtained as follows :— 
We have 
v = np(l -p). 
Suppose now that p = p0 + d where 6 is small. Then 
v 
n {(1 ~~2p0) (p -pQ) + p() - p*\ 
up (1 — 2p0) + npy. 
If we now put 
—|— and x is the observed frequency, then f will tend to have constant 
zp0 
where k = 
variance. 
In our example the total frequency is 216 out of 1400 seeds, so that we may take as 
an estimate of pQ the ratio 216/1400 = 0-15. The transformed variate then becomes 
f 
/( . x . 50 (-0225)1 
\/\np+*+ o-70 } 
\/[np + 2), approximately. 
RANDOMISATION 
209 
On this basis the transformed variate-values are- 
TABLE 23.17 
Transformed Variates of Table 23.16 
Number of 
Experiment. 
1 
2 
■3 
4 
Totals 
1 
3-464 
3162 
2-646 
1-732 
11-004 
2 
3-606 
3-464 
3-606 
2-828 
13-504 
Number of Treatment. 
3 
3162 
2-236 
2-000 
2-449 
9-847 
4 
3-317 
3-000 
3162 
3-873 
13-352 
5 
3-000 
3-317 
3-464 
3-000 
12-781 
6 
2-828 
2-236 
3-000 
3-464 
11-528 
7 
3-317 
3-606 
3-606 
3-464 
13-993 
Totals. 
22-694 
21-021 
21-484 
20-810 
86-009 
Slip 
The analysis of variance is- 
'" ~"" 
Sums of Squares 
Between treatments .... 
• 
3-486 
4-316 
7-802 
d.f. 
6 
21 
27 
Quotient 
0-581 
0-206 
The sum of squares is particularly easy to obtain, being the sum of the original variates 
plus twice the number of variate-values. 
The variance ratio, 2-8, is barely significant, being just beyond the 5-per-cent. point. 
There is little evidence that treatments are exerting any effect on germination, since a 
comparison of treatments 6 and 7 (which are the same) indicates that such " significance " 
as exists may be due to heterogeneity in the seed. 
Randomisation 
23.36. Consider a two-way classification of pq members, the observed value of the 
jth. 4-member of the kth. 5-class being xjk. Following the line already considered in 21.48, 
we will consider the ^-distribution in the population of values obtained by permuting the 
members in any J.-class in all possible ways. There will thus be (q \)p possible values of 
z, all based on the observed values. We have already considered a case of this kind in 
dealing with the problem of m rankings (16.29) and we shall follow the same procedure 
in solving the more general problem. 
a.s.—vol. ii. p 
210 THE ANALYSIS OF VARIANCE 
Let the values be arrayed as 
#11 
XZ1 
xpl 
#12 
#22 
Xp2 
Xlq 
X 
& y « * • . (2o.58) 
Xpq J 
If SR is the sum of squares between rows, Sc that between columns and $ the total, we 
know that in the ordinary case considered earlier in the chapter, 8C is distributed as v%2 
with q - 1 d.f, and S - SR - Sc as vX2 with (p - 1) (? - 1) d-f- B follows that 
%- = W, say, (23,5 
S — SR 
is distributed in the Type I form 
dF ac W*{q-l)~l {1 - W)M-n ^1]~l dW. • . . (23.60) 
It is easier to work with W than with z, but there is of course no difficulty in passing from 
one to the other. 
We proceed to find the first four moments of W in the population of (</!)" values obtained 
by permuting the rows of (23.58) in all possible ways. 
23.37. If in (23.58) we increase the members of any row by a constant a, it in easily 
seen that Sc and S ■— SR remain unaffected, and hence so does W. ThuK we may take 
the mean of each row to be zero and then 8R = 0. With this origin we- have 
Z (Z X^y 
W =^= f^—L (23.61) 
If nOW i,3 
Q 
■8<fc = £ (xa %/) (23.62) 
and the ^-statistics of the q values xip j = 1 ... j, are written kiU fr/2, etc., and 
u = £%<&> (23.63) 
i, k 
we find 
Tf = i+ 2U 
p p(q~l)Zh 
■<2 
i 
(mo. v4r) 
■® ^ _ ° (23.65) 
^ (-Btt) = (3 - 1) io *fc (23.66) 
p/j?3 \ _(2 - l)(g -2) 
A (i%) ^ 4fc8 . (236?) 
* ^ = ^vxr-3 * «» + ^il^JiMjr 3> A, i,, m68> 
q + 1 <7 (<7 + 1) 4 M* * v^**"**/ 
RANDOMISATION 211 
Then, for the moments of U, 
E(U) =0 (23.69) 
E (U*) = (q - 1) £'k^ kk2 (23.70) 
t, K 
E 
(Z7») - 6 (g - 1) £' ^ hz fe + ^=^M^1 JT' *b *ta • • , (23.71) 
w (Tjt) - 3 (? - I)3 V' JP P 4- (g ~~ l)(g — g)-(g - 3) yr/ 
q + l jLj '" K" l a (a + 1) 
U- li/k4: 
+ 1l^.r.ili«J^2J X' ^ &fc3 fo + 72 (<? - 1) 2" ki% k!c2 kl2 km2 . (23.72) 
where S' denotes summation over values for which the subscripts are unequal and 
permutations are not allowed. 
Finally, for the moments of W we have 
E{W) = ~ (23.73) 
1 ; i>2¥=i) (2^2)T 
(23.74) 
llOZ, ^ /vi2 kjc2 h>i2 A'm'2 
16 fe - 2) (g - 3) T' ^ kki 192 (? - 2) Z' ka kk:i ku 
P* (7 + 1) (?) (<Z - I)3 (Z h*)* P*(q- I)3<? (^ ^2)4 ' " * ' ^ 
These formulae can be derived in the manner of 16.33, but reference may be made to 
Pitman (1938) for further details. 
23.38. We now consider how far the first four moments of W9 as found above, agree 
with the first four moments of the distribution (23.60). The mean and variance of the 
latter are 
and -■--— — -.—— . . . , \£jo.i1) 
p p~(pq-p + 2) 
The means agree exactly. For the variances to agree we must have, from (23.74) and 
(23.77), 
4 rknkk2 2(p — 1) 
pa(?-l) &W P*(pq~P + 2) 
(23.78) 
Writing Z-2^^2, (23.79) 
212 THE ANALYSIS OF VARIANCE 
we find that (23.78) is equivalent to 
K = (iiz: 1) (^IlJJ (23.80) 
pq-p + 2 
The ratio K may have any value from 0 to , the lower Umit being approached when 
one of the second ^-statistics ki2 is much larger than the others, the upper Umit when they 
are all equal. Hence all that can be said about the variance of W is that it is not greater 
2 (fp x) 
than -— ~ and that it takes this value when the variance of each -p-class is the same. 
Pz (g - 1) 
Turning to the third and fourth moments, we note that in many cases where the 
variation is not too skew the quantities kiz and ki4i will be negligible. A number of terms in 
(23.75) and (23.76) may thus be neglected, but even those that remain are fairly 
complicated, and it is difficult to say how far the distribution of W will approach the Type I 
distribution (23.60). In practice the values may be worked out and compared. If there 
is reasonable agreement, the ^-distribution of the variance ratio will hold in the particular 
population which we are considering. 
23.39. A better approach is to find the Type I distribution which has the same first 
two moments as W and to modify the 2-test where necessary. It may be shown that when 
K is not too small the third and fourth moments of W and the fitted Type I distribution 
are in fairly good agreement, so that we may expect a good fit. 
The Type I distribution with mean - and variance —r-.— ---, has the mean and variance 
of W by definition. Its third moment is easily seen to be 
82?2 p — 2 
v^W^)^,7"2* (23'81> 
q — 1 
« 
We have to see how far this differs from the actual third moment of W given by (23.75). 
Now 
3 E ki2 kk2 kl2 = E ki2 E kk2 kl2 ~ E' &f2 kk2 
~Ski2 E' kk2 kl2 - {E ki2 E /4 - E k?2) 
= E ki2 [%Ef ki2 kk2 ~~Ek*2) +E k%, 
and hence 
o 2j JGjo iCjfO, fCio ^ rr « _ E k)>> 
(Ska)* ~3A " + "(Tfe^3- • , • • (2'{'82) 
Since all the fc's here concerned are positive, 
and hence 
21<k ^ f Z%2 l2 = fl 
> .„," =(1 -K)\ . . . (23.83) 
Hence, from (23.82) and (23.83), 
*?-^*J^>M-2+2\l-K)*=K*(l-L^Z). .: (23.84) 
RANDOMISED BLOCKS 213 
Similarly, since 
S *" < (■/f>t*gv« T = (1 - K)t < (1 - K) (1 - *A' - |Z2) 
(Zki2)s {(Zk^)* 
it appears that 
(Th% <K —7— (23.85) 
On comparing (23.75) and (23.81), and assuming that the second term in the former may 
be neglected, we see that they differ by the factor whose limits we have found in (23.84) 
and (23.85), namely 
1 — K , 3 + K 
x. ——— and -. 
If K is not too small the limits are not very different from unity, and the third moments 
are accordingly in fairly good agreement. 
In the same way but with rather more complicated algebra it may be shown that the 
fourth moments are m fair agreement. 
When all the rows are rankings, the case reduces to that considered in 16.33 et seq., 
and we have already seen that the distribution of W is closely approximated by the Type I 
distribution in that case. 
23.40. Suppose, now, that we have p classes of objects, one of each class belonging 
to a second series of classes, q in number. As our hypothesis we will suppose that 
membership of the (/-classes is independent of the variate-values, so that we may suppose it to be 
a matter of chance how the values in any $)-class are distributed among the g-classes. On 
this hypothesis the variance ratio will, follow the s-form approximately (subject to the 
conditions we have discussed above) in the population consisting of the (q \)p permutations 
of observed values ; and this will be so whether the parent is normal or not. 
By shaping the inference in this way, and making it conditional, we are thus able to 
apply the s-test even in cases of non-normality. The test of homogeneity still applies, but 
of course the inference is rather different from the usual type. This point has not, perhaps, 
been adequately emphasised in the past and there still seems to be confusion on the subject. 
Randomised Blocks 
23.41. The principle of testing in a conditional population has received its chief 
applications in a certain type of agricultural experiment (and analogous cases in other 
fields), known as a randomised block experiment. We are given p blocks of land and wish 
to test the existence of differential effects among q treatments, e.g. manurial treatments, 
of a crop to be grown on it. We divide each block into q plots and grow the crops on each 
of the pq plots. In any one block we apply a different treatment to each of the q plots ; 
and we allocate the treatments among the plots at random. 
This randomisatio n. is an essential part of the process. If the treatments exert no 
effect the observed yields might have occurred in any order, and by making the inference 
in the proper way we are able to test in the ^-distribution without assuming parent 
normality or the non-existence of fertility differences between plots of the same block. If, 
of course, the parent is near to normality the test is strengthened. Had we not allocated 
the treatments at random the use of the ^-distribution would not have been valid in the 
absence of normality (at least approximate) on the part of the parent. 
214 THE ANALYSIS OF VARIANCE 
23.42. It is of some importance to make clear the exact hypothesis which is being 
tested in this approach, since misunderstandings on the point have led to some rather 
heated controversy. If the treatments are numbered 1 to q, we consider the possible yield 
on the plot j, h if it received the Zth treatment, say xjk (Z). In actual fact only one of these 
treatments was carried out; the other values of xjk {l) are hypothetical and are based on 
our conception of what would happen if the treatments were differently distributed. The 
totality of values xjk{l) form our hypothetical population. We are supposing that the 
observed yields can be expressed as 
xjk[l) = aj + %jk(l)> 
where a^ is an effect differing from block to block but constant within blocks, and gjk {l) is the 
" individual " plot effect which has a zero mean. The hypothesis we have considered in 
arriving at the validity of the jz-test in conditional inferences is that every treatment affects 
every plot to the same extent, apart from the block effect ety. In short, we suppose that 
£jk {l) is the same for all I. This is the hypothesis usually tested in data from randomised 
blocks. 
Neyman (1935a) proposed an alternative hypothesis, viz. that the mean effects of 
treatments over all blocks were the same, on the ground that we are interested in average 
treatment effects when testing fertilisers, not the effect on particular plots. The hypothesis 
here is that x_ {l) = xuti which is not the same as before ; and it appears from Neyman's 
analysis that the ^-distribution under randomisation may not hold to such a satisfactory 
approximation as in the former case. Once again we have to stress the importance of 
gaining a clear idea of the hypothesis under test. 
Example 23.8 (Eden and Yates, 1933 ; Pitman, 1938) 
Eden and Yates considered some data, based on actual experience of heights of wheat 
shoots, comprising eight classes of four, equivalent to the following measurements :— 
Class 
1 
433 
429 
383 
437 
2 
455 
419J 
479" 
504J- 
3 
487J 
389 
463* 
4691- 
4 
407| 
574J 
4774 
452* 
5 
452J 
436* 
415 
418 
6 
257A- 
2631 
392 
426 
7 
434* 
526.1 
470 
532 
8 
475-i- 
473i 
423£ 
48 a 
The variances of the eight classes, in units of TVbh, are then found to be 
7628; 15,702; 22,669 ; 59,732 ; 3,666; 90,593; 26,297; 8672. 
The quantity K of equation (23.79) is then found to be 0-7577. The quantity 
tp ___ i) (n _ i) 
. 0 is 0-8077. Thus (23.80) is approximately satisfied and we expect that the 
jP<2 ■— jp ~\- z 
^-distribution will be approximately reproduced by the data under random permutations. 
This was confirmed by Eden and Yates in a sampling experiment on the data. 1000 
sets of permutations were taken and z calculated for each. Agreement with expectation 
was good. 
Example 23.9 (Friedman, 1937) 
A good example of data from populations which are probably far from normal is given 
in Table 23.18, showing the standard deviations of expenditures on various items for six 
RANDOMISED BLOCKS 
215 
income-groups. The figures relate to families of wage-earners and lower salaried workers 
in Minneapolis and St. Paul, U.S.A. in 1935-6. 
TABLE 23.18 
Standard Deviation* of Expenditure on Certain Items of Families in Specified Income Groups. 
(Figures in brackets are ranks.) 
Category of Expenditure. 
; 750- 
i 
100-3 (5) 
42-2 (1) 
71-3(1) 
37-6(1) 
! 58-3 (2) 
46-3 (1) 
10-0(1) 
8-3 (I) 
! 20-1 (1) 
1 3-2(1) 
i 4-1 (1) 
7-7 (1) 
1 5-3 (1) 
6-0 (5) 
Annual Family Income (dollars) 
1000- 
68-4 (1) 
44-3 (3) 
81-9 (2) 
60-0 (3) 
52-7 (1) 
82-2 (2) 
23-1 (2) 
8-4 (2) 
33-5 (2) 
4-1 (2) 
18-9 (5) 
11-2 (5) 
10-9 (2) 
5-6 (4) 
1250- 
89-5 (3) 
60-9 (4) 
100-7 (7) 
57-0 (2) 
96-0 (6) 
129-8 (3) 
38-7 (3) 
9-2 (3) 
60-1 (4) 
12-7 (4) 
8-5 (2) 
10-4 (2) 
11-2 (3) 
22-2 (7) 
1500- 
77-9 (2) 
73-9 (6) 
86-5 (3) 
60-8 (4) 
60-4 (3) 
181-0 (6) 
45-8 (4) 
14-3 (6) 
69-3 (5) 
18-9 (5) 
12-9 (3) 
10-9 (4) 
25-3 (4) 
2-5 (2) 
1750- 
100-0 (4) 
43-9 (2) 
100-3 (5) 
71-8 (5) 
104-3 (7) 
172-3 (5) 
59-0 (7) 
10-6 (4) 
114-3 (7) 
8-9 (3) 
25-3 (7) 
10-5 (3) 
42-3 (5) 
6-2 (6) 
• 
i 
2000- 
108-2 (6) 
61-7 (5) 
90-7 (4) 
83-0 (6) 
89-8 (5) 
164-8 (4) 
50-7 (5) 
15-8(7) 
45-3 (3) 
41-5(6) 
19-9 (6) 
14-.0 (6) 
48-8 (6) 
1-0(1) 
2250-2500 
184-9 (7) 
102-3 (7) 
100-6 (6) 
117-1 (7) 
85-8 (4) 
246-8 (7) 
55-2 (6) 
12-5 (5) 
101-6 (6) 
66-3 (7) 
16-8 (4) 
14-4(7) 
69-4 (7) 
4-0 (3) 
x, i (M*oii*m » » . . 
II (nMi *1 mid <>p<ration 
Food 
Clothing . 
Furnishings,, etc. 
Transportation 
Recreation 
Personal care 
Medical cart) . 
Kducati<m 
Community welfare , 
Vocation . 
Gifts 
Other 
In brackets we show the ranks of the figure for different income-groups for each 
category of expenditure. We wish to know whether the standard deviations for each 
category differ significantly for the different income levels. On the hypothesis that they 
do not it is a matter of chance how the ranks fall. 
The sums of ranks in each column are :— 
23, 36, 53, 57, 70, 70, 83. 
The coefficient, of concordance (vol. I, p. 411) is then W = ., 3 ,, where m = 14, 
71 7 and ,S' is the sum of squares of deviations of sums of ranks from the mean 
■///. (n I 1) 
*> 
f><i ; we find that S 
(vol. I, p. 419) by writing 
2620 and W = 0-4774. We may test the significance 
z 
- ° 1 — pp 
Vl = (w - 1) 
m 
°7 
r.> 
= (m — 1) vi = 76^-. 
The value of z is highly significant, and we conclude that standard deviation is related to 
size of income- the more money there is to spend, the more variable is the expenditure 
on particular items. 
216 
THE ANALYSIS OF VARIANCE 
NOTES AND REFERENCES 
The idea of comparing variance between classes with the variance within classes in 
order to test homogeneity is found as early as Lexis (see footnote on page 1 M>). Modern 
developments, and particularly the exact test of significance for normal parents, are dn^ 
mainly to R. A. Fisher. Apart from papers by Irwin (1931 and 1934), connected accounts 
of the theory of variance analysis are hard to find, many points of theoretical interest being 
scattered among papers which are primarily practical. 
For the general theory and applications reference may be made to Fisher's Statistical 
Methods (1925a, 1944) and Design of Experiments (1935c, 1942), to a useful introductory 
account by Goulden (1939), and to the writings of Yates, particularly bin V'Mv/r/w and A nalyttw 
of Factorial Experiments (19376). 
On the question of randomisation in preserving the ^-distribution wee Eden and Yates 
(1933), Welch (1937, 1938a), and Pitman (1938). References to work on ranking art* given 
at the end of Chapter 16. 
For work on the distribution of the greatest of a set of variances see Fisher (1929r/, 
1940a), Cochran (1941), Stevens (1939a), Hartley (1938), and Finney (1941u). Fur further 
work on the square-root and sin"1 transformations see Cochran (1940/>), 'HealI (MM2) and 
Curtiss (1943). 
The literature of this subject is now very large. Some further references are given 
at the end of the next chapter. 
EXERCISES 
23.1. If xj (j = 1 ... n) are a set of normal independent variates with variances 
1/w .consider the transformation 
n 
1lh = 
= 2j ^ X* ^Wi> 
where the V s are denned by 
kk = V(u>k/Zw) 
3 = 1 
I. 
'jk 
Wj Wk 
(I 
?-0 (£-)] 
Ic ™ 1 . . 
n 
.... <•> *> 
.1 
k — 1 '» 
, "^ » 
Show that the Z's are orthogonal and hence that 
q - O *i 
k ~- j i 1 
n 
J 
n 
n 
n n 
jr n\ = £ wk 4 
k = l Jc=\ 
is distributed as z« with n degrees of freedom. Noting that w, . £ ,;. ,.. x 2,,. is ,„„. 
tribnted normally with unit variance independently of „8 . . . „/Jmw „,„, 
n 
k=\ 
is distributed as x* with n - 1 degrees of freedom. 
EXERCISES 217 
Hence derive the 2-test for the analysis of variance with unequal members in a one-way 
classification. 
(Irwin, 1942.) 
23.2. Verify the arithmetic in the analysis of variance of Example 23.5. 
23.3. Verify the arithmetic in the analysis of variance of Example 23.6. 
23.4. In a bivariate table with k rows (different rows corresponding to different 
values of the ^-variate) write 
h =—2nx(yx - y)* 
1 
q = —2 (nx 4), 
where a2 is the variance of the y variate, si the variance, and nx tlie frequency in the row 
with variate-value x. Thus 
_?&_ = ~ 
1 - ifvx q 
and the ratio on the right is the variance-ratio in a one-way classification with unequal 
numbers. 
Show that, for any form of population, 
E (ft) = k - 1 E (q) = N - k 
var h = 2 (k - 1) + (/?a - 3) {r — + ^=^1 
L x nx N J 
var q - 2 (N - k) + (ft - 3) Is — + N - 2k 
I x nx 
f. k 1 
cov (A, q) = (/5\ — 3) ^ A — 1 + — — 27 — 
[ Jy x nx^ 
Hence, approximately, that 
F (h\ ^^Wfi i vaI^» cov (ft, <f) 
' W * (?) \ ^ (?) ^ (ft) E (q) 
r f^\2 ~~ ^2 W Ii var ^ __ ^ cov (^' #) j_ **var ? 
4 y - ^-y | + Fa-^j - E{h)E {q) + -ETffi 
In the case when all rows contain the same frequency 
and then , 7 x , , r « ^ 
A\ 2 (/<: - 1) (iV - 1) 
var I - = v- r-ki ra ■ 
Hence show that the mean and variance of the variance-ratio are, to this order, independent 
of the distribution of y, indicating that the 2-test is not very sensitive to deviations from 
normality. 
(E. S. Pearson, I93I6. It is rather remarkable that the correlation of h and q, far from 
disturbing the ^-distribution, contributes to its stability.) 
CHAPTER 24 
THE ANALYSIS OF VARIANCE—(2) 
Estimation of Class-differences 
24.1. In the previous chapter we considered the analysis of variance mainly as the 
provider of tests of homogeneity. We have now to examine in more detail the problem of 
estimating class-effects, assuming that the homogeneity tests have shown them to exist. 
We discuss in the first instance the case in which there is only one member in each 
subclass, and for the sake of simplicity confine ourselves to a two-way classification, though 
the theory is quite general. 
The fundamental hypothesis to be examined is that the data may be expressed in 
the form 
xjk = aj + h + £jk> (24*1) 
where a^ and bk represent class-effects and ; is a random normal variate with zero mean. 
Our analysis of variance will have shown whether this is an acceptable hypothesis, and 
our present problem is to estimate the unknown values of a's and 6's from the observed as's. 
24.2. The joint probability of the £'s is 
dF oc —■ exp i - — E (xjk .- a5 - bk)2 I d£n. . . . dCm, . . (24.2) 
where v is the variance of £, and in conformity with the notation used in the previous chapter 
we have p JL-classes and q JS-classes. The maximum likelihood estimates of the a's and 
6Js are then those which minimise the sum in curly brackets in (24.2), that is to say, the 
least-squares solution of the equations (24.1). In the usual way we find 
[Xjjc — (ij ok) = U, j = I, . . . p 
(3# ~ % - bk) = 0, k = 1, . . . q 
which reduce to 
x 
k = \ 
z 
?=1 
(24.3) 
XU ~" ai "" b- ~" ° \ (2± l\ 
xnk-am -6fc = 0J [ } 
Summing the first equation over j, dividing by p, and subtracting from the first, we obtain 
jOa x — a a a 7 ——- a , . . . x) . . . . (^4.*.) j 
and similarly 
x.k — %.. ^ bk — bm k = 1, . . . q. . . . (24.6) 
In (24.5) there are p equations, but if we sum them all we reach the identity 0 = 0, so that 
only p — 1 are independent. There is thus an element of indeterminacy which we may 
remove by supposing that a% = 0. Similarly we may take b^ = 0, and then we have 
aj = xj. - x.. 3 = 1» • • • P • • • • (24-7)* 
bk = x k — x k = 1, . . . q. . . . (24.8) 
218 
ESTIMATION OF CLASS-DIFFERENCES 219 
Our estimate of any class-effect i<* pmial +^ +1^ a^; +• r ^ . ,i , i r- 
Al , 4 « ' 1S e<lual t0 tne deviation of the mean in that class from, 
the total mean. 
24.3. Kyidently similar equations arise in the general rc-way classification. We shall 
see below that they break down for unequal numbers in subclasses, except in a special 
case when the numbers are proportionate. 
The assumption that a, and bk have zero means is not, in effect, a restriction on 
generally but on y a convention. If we prefer it, we may consider the slightly more general 
hypothesis that . has a mean m, in which case we have to minimise 
2 {xjk - dj - bk - m)2 (24.9) 
Tliis will be found to lead back to equations (24.7) and (24.8), with the additional equation 
for est in sating m 
m == x%% (24.10) 
Or apiin, if we prefer to absorb m into the a-effects we have 
d,i = Xa "1 
iJ 3m > (24.11) 
the mean of //; in this ease not vanishing. Which form we use is a matter of convenience. 
24.4. It is important to notice that the equations of estimation which we have just 
reached give each <if and bk independently of values in other classes. We obtain the same 
equal ion for <tj whether we happen to be estimating other a's and 6's or not. This property, 
as we shall see shortly, fails to hold if the numbers in subclasses are disproportionate. 
The situation is similar to that in which we can determine the constants in a regression 
line independently of the others if orthogonal polynomials are used, in that each constant 
is given by a separate equation not containing any of the others. Data of this kind are 
called orthotjotml. 
The direct comparison of class-means which is possible with orthogonal data can be 
seen, from general considerations, to be legitimate. In comparing xim — xmm with xjt — as-#, 
the estimates of t he effects in the ith and jth ^.-classes, we are in each case averaging over 
// /^classes with one member in each. The l?-classes, therefore, affect each mean to the 
same extent and do not affect their difference. If there are more members in some 
subclasses than in others, the means are unequally weighted with different 5-effects and 
the comparison is invalidated. 
24.5. Regarding xi% xmm as the estimate of ^ and xmk — xmm as the estimate of bk9 
we see that tin* familiar equation 
±'Uit. ■ .r f 2(r, ~x )*+2{xmk -x.y+Ztyk-Xj. -*.ft + Oa (24-12) 
J h mm' * ■/ * * * 
can be regarded a.s an analysis of the sum of squares on the left, which has pq - 1 degrees 
of freedom, into terms in which there is one degree of freedom for every fitted constant and 
a residual with (j I) (q - 1) degrees of freedom. Every constant fitted reduces the 
number of dcKrees of freedom in the residual by unity. 
220 THE ANALYSIS OP VARIANCE 
Unequal Numbers in Subclasses 
24.6. For a one-way classification we have already considered (23.7 and 23.8) the 
case where the numbers in subclasses are unequal. It was seen that the total sum of squares 
could be expressed as a sum between classes and a residual which were independently 
distributed and whose ratio therefore provided a homogeneity test in the usual way. 
When we try to extend this result to two-way or generally to w-way classifications, 
we begin to run into diflSiculties. A^e can still find, as shown below, an estimator of v based 
on p — 1 degrees of freedom and differences between ^-classes, and one with q — 1 d.f. 
based on differences between ^-classes ; but these are no longer independent, and 
consequently we cannot subtract their sum from the total sum of squares in order to obtain 
a residual or an interaction term which also provides an unbiassed estimator. 
On the other hand, there is now available an independent estimator of v which did 
not appear in the orthogonal case where only one member was included in each subclass. 
In fact, since there are several members in any given subclass, we can find an estimator 
of v based on those members alone ; and we may pool all such to form an estimator with 
N — pq degrees of freedom, where there are pq subclasses. This estimator will be 
independent of subclass means and any estimators based on them, and hence provides 
a "residual" such as we require to carry out homogeneity tests. 
24.7. Suppose we have a two-way classification into p ^[-classes and q JB-classes, and 
let the number of members in the subclass Aj Bk be w,7c. Let xjk be the mean of these 
members. We may array the means as 
x±1 
X21 
Xpl 
^12 
#22 
• 
Jsp2 
X2q 
xmJ 
. (24.13) 
Now we may, in the first instance, test for homogeneity by ignoring the differences 
between A- and ^-classification and merely regarding the data as a one-way classification 
with pq classes. The usual test for homogeneity is then applicable. The sum of squares 
between means of classes will have pq — I degrees of freedom, the total JV — 1 d.f., and. 
the residual N — 1 — (pq —■ 1) ~N — pq d.f. This residual, in fact, is the one 
mentioned in the previous section, and is based on the pooled sums of squares within the pq 
classes. The other term based on pq — 1 degrees of freedom is the sum 
^ njk (xjk — ##J2 
and is derivable from the array (24.13). 
24.8. To test the effect of A -classification separately we proceed as follows :— 
Any xjk is the mean of njk values and, on the usual hypothesis as to normality, will 
have variance —. If x is the mean of all N values we have 
.■=^^%fc% (24.14) 
x 
N, 
3, k 
UNEQUAL NUMBERS IN SUBCLASSES 221 
Let the marginal unweighted means in (24.13) be x, ,|foSO that 
1 
Xj = ~ E #., 
3»i 
> (24.15) 
On the hypothesis of homogeneity the variance of % is given by 
v ( l j. * j. * \ w 
when 
1 ^ 
tf.^fl — ) (^-17) 
Now let us regard the means xju as the means in p classes whose numbers are Np as 
is legitimate from (24.16). Then writing 
C = —yTT^—" ...... (24.18) 
we have for an unbiassed estimator of v 
1 
J> 
- f-ty (%. - c)a = —^ {^(JVy^.) - cai7JvA. . . (24.19) 
l j* p 1 [ j i J 
This estimator has p 1 degrees of freedom and is distributed as %2. (This follows from 
the one- way ease except that Nj may not be integral; and its general truth may be 
established as in Kxord.se 23.1.) It is independent of the residual with N — pq d.f., and hence 
the -4-effects may be tested separately. 
Similarly, if 
y.-?f(s) (24'20) 
an unbiassed estimator of v is given by 
_L { E (Mk &k) - d* E Mk\, .... (24.21) 
q — 1 { k k J 
where 
EMkxmk 
d = k , (24.22) 
EMk ' ; 
k 
and this also may be compared with the independent estimator based on N - pq d.f. 
Example 24.1 (data from Brandt (1933) considered by Yates (1934a)) 
Table 24 1 shows, for a number of breeds of pig, the numbers of each breed, 
divided into male and female, and the total logarithm of the percentage bacon yielded by 
the slaughtered carcases. The logarithm has been taken so as to normalise the variate. 
222 
THE ANALYSIS OF VARIANCE 
TABLE 24.1 
Numbers and Logarithm of Percentage Bacon in Breeds of Pigs. 
Hampshire . 
Duroc Jersey 
Tamworth 
Yorkshire 
Berkshire 
Poland China 
Chester White 
Others 
Totals . 
Female. 
Number. 
33 
51 
13 
4 
8 
15 
35 
12 
171 
Log. Percent 
Bacon. 
66-55 
98-69 
25-90 
7-62 
14-64 
2811 
66-90 
23-32 
331-73 
89 
141 
17 
9 
4 
32 
47 
362 
Male. 
Number. 
Log- Percent, j 
Bacon. 
LSI-04 
281-43 
34-20 
17*58 
8-20 
54-42 
00-52 
46-70 
724-09 
The total sum of squares, which is not obtainable from this table as it stands, we quote 
as 13-0142. 
The class-means and reciprocals of class-frequencies are given in Table 24.2. 
TABLE 24.2 
Glass-Means and Reciprocals of Class-Frequencies for the Data of Ttthh 21.1 
Breed. 
Hampshire . 
Duroc Jersey . 
Tamworth . 
Yorkshire 
Berkshire 
Poland China 
Chester White 
Others . 
Unweighted M 
ean 
0 
f 
Fen 
Mean. 
2-016,667 
1-935,099 
1-992,307 
1-905,000 
1-830,000 
1-874,000 
1-911,429 
1-943,333 
1-925,979 
l/njic 
0-030,30 
0-019,61 
0-076,92 
0-250,00 
0-125,00 
0-066,67 
0-028,57 
0-083,33 
(Total) 
0-680,40 
Male. 
Mean. 
2-034,158 
1-995,958 
2-011,765 
1-953,333 
2-050,000 
2-013,125 
1-925,958 
2-030,434 
2-001,841 
1 Mjk 
0-011,24 
0-007,09 
0-05H,K2 
0-111,11 
0-250,00 
0-031,25 
0-021,28 
0-043,48 
(Total) 
0-534,27 
I'liu-ri^hted 
Moan of 
M«MWIN. 
HM5.52K 
2-002,030 
I-020,167 
H>40,000 
1 * ,1 * «> ,()()** 
1-0 IS, 094 
1-0KH,KK4 
1*0034*10 
UNEQUAL NUMBERS IN SUBCLASSES 
223 
Taking first the classification into male and female (q = 8), we find, from the relations 
1 ^l^j, 1 
Nj q2 k njk 
64 
#i = 
N, = 
0-680,40 
64 
0^534^27 
= 94-0623 
= 119-7896. 
Then, from (24.18) 
s* ___ J J* 
INj 
(94-0612|_3_XJ^2^979) + (119-7896 x 2-001,841) 
94-0623 + 119-7896 
= 1-968,474. 
Thus our estimate of v, with one degree of freedom 
= 0-3032. 
Similarly for the eight breed-classes we find an estimate of v with seven degrees of 
0-6056 
freedom to be ——- = 0-0865. 
Considering the 16 subclasses as a one-way classification, we find the following 
preliminary analysis (the arithmetical details of which we omit) :— 
TABLE 24.3 
Analysis of Variance of Data in Table 24.1. 
Sum of Squares. 
Between classes 
Residual 
Totals 
J.'Zt i JLO 
11-7427 
13-0142 
d.f. 
15 
517 
532 
Quotient. 
0-0848 
0-0227 
The variance ratio here gives a value of z equal to 0-659, which is significant. Thus the 
data are not homogeneous. 
We now require to decide whether the departure from homogeneity is due to either 
breed or sex or to a combination of the two. For sex-differences we have found an estimate 
of v equal to 0-3032 with one d.f. Comparing this with the independent residual from 
Table 24.3 of 0-0227 with 517 d.f., we find that the effect of sex is significant. Similarly, 
for breed, the estimate of v is 0-0865 for 7 d.f., which again is significant. We conclude 
that both breed and sex influence the departure from homogeneity. 
224 
THE ANALYSIS OF VARIANCE 
It is particularly important to note that since the estimates between breeds and between 
sex are dependent, we cannot analyse the variance as follows :— 
TABLE 24.4 
Incorrect Form of Analysis of Variance of Data of Table 24.1. 
Sum of Squares. 
Totals 
0-3032 
0-6056 
0-3627 
11-7427 
130142 
d.f. 
1 
7 
517 
532 
Quotient. 
0-3032 
0-0865 
0-0518 
0-0227 
In fact the term shown as " interaction ", calculated so as to make the sums of squares 
&,nd degrees of freedom additive in the usual way, is not an unbiassed estimate of v. This 
is a critical point of difference between the orthogonal and the non-orthogonal case. 
24.9. Suppose that the homogeneity test has shown the existence of significant 
class-effects. As before, we turn to consider the hypothesis that the data can be expressed 
■as the sum of A- and B-effects separately with a random normal residual. Let x,jM be 
the typical member of the (j, &)th subclass, 7, varying from 1 to njk. Our hypothesis is then 
* 7 <~ 
xiki = &j + bk + <>w> (24.23) 
where f is normal with variance v. Por convenience we will regard the mean of C as absorbed 
in the coefficients a, so that we may take £ to have zero mean. 
The usual process of estimation of the a's and 6's leads to the minimisation of the 
sum over all N values of 
E (XjId — a.j — bk)2. 
Differentiating with respect to a^ and bk, we find the series of equations 
E E (Xjki — aj — bk) = 0, 
k 
E « (Xjki — a^ — bk) — 0, 
3 
le 
1 
1 
- V 
• q 
■\ 
> 
. (24.24) 
where E' denotes summation over the njk values in a subclass. These equations reduce, to 
— ~\ 
E njk aj + E njk bk = z, ?ijk xjh 
k k k 
E njk aj + E njk b}, = E njk xjk 
(24.25) 
Writing Njm for E njk and Nmk for E njk, we have 
k ' i 
Njm as + Enjkbk = Znjkxjk j = 1, 
k k 
Snjkaj + Nmkbk = Enjkxjk ft = 1, 
o j 
To which we may add 
V 
q. 
E b 
k 
k 
0. 
(24.26) 
(24.27) 
(24.28) 
UNEQUAL NUMBERS IN SUBCLASSES 225 
Had we chosen to absorb the mean of £ into the 6's, this last equation would be replaced 
by S dj = 0. 
o 
When all the n's are equal these equations reduce to the orthogonal case, and each 
a- or 6-coefficient can be independently estimated. In the contrary case the equations 
have to be solved as they stand. 
Example 24.2 
Returning to the data of Table 24.1, we find for equations (24.26) and (24.27) the 
following, the values of the constants required being obtainable from the body or marginal 
sums of the table itself:— 
171ft! + 336x + 5162 + 1363 + 46, + 865 + 156, + 3567 + 1268 = 331-73 
362aa + 89&! + 14162 + 1763 + 964 + 465 + 3266 + 47&7 + 2368 = 724-09 
33a, + 89a2 + 1226,. = 247-59 
51a, 1 141a2 + 19262 == 380-12 
13a, + 17a, + 3()63 " = 60-10 
4a, f 9a2 + 136, = 25-20 
8a, + 4a, + 1265 = 22-84 
15a, + 32a2 + 4760 = 92-53 
35a, -h 47a2 + 8267 = 157-42 
12a, -l- 23a2 + 3568 = 70-02 
To which, we may add a, + a2 = 0. 
The solutions are 
— a, = a% = 0-026,507 ; 
6, = 2-017,259 ; 62 = 1-967,367 ; 63 = 1-999,799 ; 64 = 1-928,267 ; 
6r> = 1-91.2,169 ; 6(J = 1-959,136 ; 67 = 1-915,877 ; 6S = 1-992,241. 
rrhese give us the " best " estimates of the mean effects of sex and breed on the 
hypothesis expressed by (24.23). 
The mean of the 6's is 1-961,514 which may be taken as an estimate of the mean of £, 
the 6-efteets then being the differences of the above 6-values from this mean. 
24.10. Let us now consider the analysis of variance in the non-orthogonal case, 
when constants have been fitted by least squares in the above-mentioned way. 
To make the discussion clearer we will regard the estimation as relating to p constants 
a.j, related by U (a.j) = 0, q constants 6A,, related by Z (6A:) -- 0, and the mean m. There 
are thus p \ q 1 independent constants which, in effect, provide estimates of the means 
of subclasses. Whatever these means really are, the residual quotient based on N — pq 
degrees of freedom gives an unbiassed estimator of v, the common variance. We have 
now to analyse the remaining sum of squares based on pq — 1 d.f. 
If the true (population) values of the constants are denoted by a.,-, /?A. and //, the sum 
• Z(xjkI "" ai " Pk—/1)2 
is distributed as v%% with N degrees of freedom. Developing yet another variation on 
a familiar theme, we show that the corresponding quantity 
E (xj/d - ctj - bk - m)* = S {xjkl - aj - pk - /02 - E (aj - oc,)2 
- E (bk - ftj2 - E(m - /02 ■ (24:29) 
is distributed as v%1 with N — (p + q — 1) d.f. 
a.s.—vol. n. Q 
226 THE ANALYSIS OF VARIANCE 
In fact, equations (24.26) and (24.27) show that the estimators a, b (and in our present 
case m also) are linear in the variables x. We can then find p + q — 1 orthogonal normal 
variables in terms of which they can be expressed. Their sum of squares will be distributed 
as v%2 with p + q — 1 degrees of freedom (not some multiple of x2 because the mean value 
must be p + q — 1 in virtue of 18.17). Thus the remaining term S (xm — efy — bk — m)2 
is distributed as ^2withiV" — (p + q — 1) degrees of freedom, independently of the portion 
due to the constants a, b and m. 
Furthermore, the actual reduction in sums of squares, equivalent to the sum of the 
last three terms in (24.29), may be easily determined. Precisely as in the similar problem 
of evaluating residuals in a regression equation, we have 
Z {Xjki -a, -bk-my =2 a?|w - S % Z xm - S bk 2 xjkl - mS xjM . (24.30) 
j k,l k j,l M 
where, of course, summation takes place over all values. 
24.11. The total sum of squares is already calculated about the estimated mean 
m, so that the reduction for the term Im2 =N x2mm has already been taken into account. 
The total sum is then distributed as v%2 with N — 1 d.f., as we already know. We know 
further that we can split off the independent residual sum based on N — pq degrees of 
freedom. This leaves us with a sum based on pq — 1 d.f. From the previous section it 
follows that we can analyse this sum into two parts : (a) the sum of squares due to fitting 
the constants a^ and bk, accounting for p -f- q — % d.f., and (b) the remainder based on 
pq — 1 — (p + q — 2) == (p — 1) (q — 1) d.f. This remainder is independent of the sum 
of squares due to fitting constants and provides an unbiassed estimator of v. If the ratio, 
as compared with the residual based on N — pq d.f., is significant, the hypothesis of additive 
effects breaks down. In short, we may regard this quantity as an interaction term. 
24.12. One important point to notice in this connection is that the interaction term 
depends on whether p + q —- 2 or fewer constants are fitted. In the orthogonal case we 
can determine an interaction term once and for all, however things stand in regard to the 
estimation of inter-class effects ; but for non-orthogonal data the number of class-effects 
estimated affects the interaction term, and if necessary a new significance test has to be 
applied if further estimates are calculated. The situation is similar to the testing of 
regression coefficients when orthogonal polynomials are not employed. 
Example 24.3 
Returning again to the data discussed in Examples 24.1 and 24.2, let us regard the 
means in all 16 subclasses as simultaneously under estimate. For the reduct i on m sum 
of squares due to the constants we find, using the values of a and 6 found in Example 24.2,— 
0-026,507 (- 331-73 + 724-09) + (2-017,259 x 247-59) + (1-967,367 x 380-12) . . . 
- (1055"82)2 = 1-04146. 
533 
Here, for instance, the sum Z a\ is given by multiplying ax by the term Exlk already 
k 
found. The last term removes the effect of including the mean among the 6's. 
UNEQUAL NUMBERS IN SUBCLASSES 
227 
The sum of squares between classes was found in Example 24.1 to be 1-2715, based 
on 15 d.f. We then have 
Sex and breed (estimation of constants) 
Interaction 
Between classes 
10415 
0-2300 
1-2715 
8 
7 
15 
Quotient. 
Comparing the interaction term 0-0329 (7 d.f.) with the residual 0-0229 (517 d.f.) we see 
that it is not significant. 
If we neglect sex and consider breed alone, we have only to estimate eight constants 
bj . . . b8 subject to E (b) = 0. The sum of squares for breed alone is given by 
-i- (247-59)2 + JL (380-12)2 + . 
122 v ' 192 v ; 
533 
(1055-82)2 = 0-7253. 
Similarly the sum of squares for sex alone will be found to be 0-4224. We have the 
following analysis :— 
TABLE 24.5 
Further Analysis of Variance of Data of Table 24.1. 
Sum of 
Test for Sex 
Between breed (estimation 
Sex 
Sex and breed .... 
Test for Breed 
Between sox (estimation of 
Brood 
Sex and breed .... 
Interaction 
Between classes 
Squares, 
of constants) 
— 
constants) . 
m • • • 
* ■ • ■ 
0-7253 
0-3162 
1-0415 
0-4224 
0-6191 
1-0415 
0-2300 
1-2715 
v J,«JL* 
7 
1 
8 
1 
7 
8 
7 
15 
Quotient. * 
0-3162 
0-0884 
0-0329 
Here, for instance, if we test for sex there are seven independent constants for breed 
and one for sex, the latter being the only one that interests us ; and similarly for breed. 
On comparison with the residual 0*0227 both sex and breed are found to be significant. 
24.13. The reader may perhaps find the various tests of Examples 24.1 and 24.3 
confusing, and we accordingly summarise our results for the case of unequal numbers in 
subclasses. 
In every case, except where each subclass contains not more than one member, an 
estimate of the common variance v may be obtained, with N — pq d.f., by pooling the 
sums of squares within the pq subclasses. Call this v1. 
228 THE ANALYSIS OF VARIANCE 
Homogeneity may then be tested (a) by considering the pq classes as a single one-way 
classification and comparing the quotient between means with vl9 or (b) by calculating 
for either classification separately the estimates based on (24.19) and comparing them with vt. 
If homogeneity is rejected in favour of the additive effect of classes expressed by the 
usual hypothesis, the sum of squares between all classes based on pq — 1 d.f. may be split 
into independent sums related to the fitting of the constants and to an interaction term. 
The latter can be compared with vx to test for interaction. If this is not significant, 
alternative tests for effects between A- and between ^-classes may be derived by testing the 
sum of squares attributable to the fitting of the respective constants against vx. These 
tests are, in effect, tests of one class neglecting the effect of the other, and may not be 
accurate if the latter effect is not negligible. It is probably better to fit constants to both 
classes simultaneously in the first instance. 
Proportionate Frequencies 
24.14. We have previously spoken of non-orthogonal data as meaning any 
classification with unequal frequencies in the subclasses, but there is one other case of unequal 
frequencies for which orthogonality exists, namely the one in which frequencies are 
proportionate, i.e. there are marginal frequencies lp mk9 such that 
n*k == va nik. . » . . . y&j-c.olj 
Here the means of A -classes are estimates of the individual corresponding as (though it 
must not be overlooked that they are based on different numbers of members in margins), 
and the sum of squares between ^.-means may be computed in the usual manner 
appropriate to a one-way classification with unequal numbers. Similarly for B. The interactions 
may be estimated by subtracting the A- and J3-sums from the sum of squares between 
classes. We leave it to the reader to verify these statements. 
Special case of 2 x 2' . . . Classification 
24.15. The foregoing analysis can be extended to the w-way classification, but in 
the general case the solution of the equations becomes rather complex and the arithmetic 
a considerable nuisance. Where, however, the classifications are simple dichotomies the 
problem simplifies to a great extent. For instance, in equations (24.27), if there are only 
two values of a*, which we may take to be + a and -— a, we have 
■N jc bk — S njk XaJc — nlk a 4- n%k a . 
We have selected the a's so that E (a) = 0, which implies that the mean mis amalgamated 
with the 65s. Substituting for the 6's in (24.26), we find 
a\Nim -Enlk^^n^\=Snjkxjk^2Jp^njxjk 
I k Jy.k J k k Jy .k 
which reduces to 
^11^12 i ^21^22 , V ^11^12 /- - \ i ^21^22 
+ " '--+ . . . a=—^l («„-£!,)+- "," (s„-sM)+ .... (24.32) 
Thus a is the weighted mean of the differences of corresponding i?-class means and may 
be determined direct. So generally for a 2 x 2 x 2 . . . classification. The differences 
may be tested for homogeneity by the z-test, which in this case reduces to the £-test. 
24.16. In view of the relative complexity of the non-orthogonal case, it is natural 
to wonder whether any serious error would be committed if we regarded the p x q table 
of array means as an ordinary two-way table with one member in each class and analysed 
THE MISSING PLOT TECHNIQUE 
229 
the variance accordingly. Evidently such a procedure sacrifices a lot of information about 
variation in subclasses, but that is not the point. Is the analysis valid ? 
The hypothesis on which the analysis is based is equality of variance in subclasses. 
If the numbers in subclasses are very unequal the means based on them will have very 
unequal variances, and we expect that the analysis may be misleading. If, however, the 
numbers are close to equality the analysis will probably be approximately correct. 
Example 24.4 
Reverting once again to the data considered in earlier examples, we have the following 
x ujuh5 v aa jlgwlujc 
Between sex . 
Between breed 
ToTAIiS 
UJ. UilC £i A O U 
etuie ui u±<x»s-i.ijlc 
Sum of Squares. 
* • • • • 
• • ■ • • 
0-3032 
0-2635 
0-2387 
0-8054 
;d»ii» . 
1 
7 
7 
15 
Quotient. 
0-3032 
0-0376 
0-0341 
• 
The sum of squares between sex is the same as before, as it must be for a dichotomy, 
but the effect of breed is seriously underestimated and would not be judged significant by 
comparison with the interaction term, which is our residual. The numbers in the breed- 
classes are, in fact, too different to justify the approximation. 
The Missing Plot Technique " 
24.17. The simplicity of the analysis of variance in the orthogonal case and the 
economy imported by keeping the number of values as low as possible often leads to the 
carrying out of experiments with only one member in each subclass. But this has a certain 
practical danger in that the value in a subclass may be lost through circumstances beyond 
the experimenter's control. For instance, an animal may die in the course of an experiment, 
or a crop on a particular plot may be ruined by pest; or sometimes a record may actually 
be lost after measurements have been carried out. In such cases we may estimate the 
missing values and perform a variance-analysis in the following way. 
24.18. Consider in the first place a p x q classification with certain missing values, 
r in number. We assume as usual that the variate-values are expressible in the form 
xjk = ^ -f bk + rCjk + m, (24.33) 
and we know that the " best " estimators of the constants are 
m = x 
an 
fk 
Xj 
X 
k 
x 
. (24.34) 
• • j 
The quantities on the right are, however, unknown to us because of the missing values. 
Suppose that we estimate the constants by minimising 
r (xjk - ^ - bk - m)2 (24.35) 
where the summation £' takes place over known values. Our estimators are then 
determinate and may be written a'j, bk and m'. 
230 
THE ANALYSIS OF VARIANCE 
We will now estimate the missing value on the plot (j, k) by the equation 
X'jk = a, +b'k+m' (24.30) 
We have 
li (Xai 
w 
a. 
bk - m)2 = Sf (xjk - a* - bk - m)2 + S (Xjk - a, - bk - m)K (24.37) 
Let us now consider this as a function to be minimised, involving the unknowns a, b, m 
and r further unknowns Xjk. The equations giving the latter will be obtained by 
differentiating (24.37) with respect to each Xjk, and in fact are typified by 
%'jk = a'j + b'k + m'> 
that is to say, by (24.36). The other constants are given by such equations as 
r (x, 
jk 
a* 
bk-m')+Z(Xjt 
jk 
a, 
b' - m') = 0. 
. (24.38) 
The second term vanishes, and hence we obtain the same minimal values for a'j9 bk and 
m' as by minimising (24.35) by itself. Furthermore, the equations of estimation (24.38) 
may be written 
Z (xjk - a'j - ^ _ m') = 0, . . . . (24.39) 
where the summation takes place over all values, those of the observed #'s where known 
and over the estimated X's where values are missing. 
It follows that if we write Xjk for the r missing values, ascertain the residual sum of 
squares, which will be a function of observations and these r unknowns, and minimise 
it for variation in these unknowns, we shall obtain equations providing estimates of the 
unknowns equivalent to (24.36). The following example illustrates the method. 
Example 24.5 (Yates, 19336) 
The following table shows the measurements of intensity of infection of certain potato 
tubers under eight manurial treatments in ten blocks. 
Treatments. 
1 
2 
3 
4 
5 
6 
7 
8 
Totals 
3-55 
2-30 
3-96 
2-99 
a 
2-36 
216 
3-16 
20-48 
2-29 
4-03 
3-62 
3-99 
3-07 
3-47 
2-34 
2-52 
25-33 
TABLE 24.6 
Intensity of Infection of Potato Tubers. 
Blocks 
5 
3-34 
3-29 
2-94 
*x "™e %j 
3-99 
3-26 
3-77 
c 
6 
3-83 
2-93 
3-70 
4-70 
3-48 
3-28 
d 
e 
25-08 
-+- c 
21-92 
H-d + e 
7 
3-86 
/ 
3-82 
3-86 
3-80 
9 
3-20 
3-85 
22-39 
+/+<7 
8 
3-50 
% 2-55 
2-54 
h 
3-68 
i 
3-47 
3-36 
19-10 
-\~h+i 
9 
2-23 
2-20 
3 18 
3-50 
3-24 
3-07 
2-67 
2-50 
22-59 
0 
r*i 
Totals. 
2-91 
2-30 
3-69 
3-59 
2-70 
312 
3-33 
4-13 
5-77 
27-51 + b 
24-96 + f 
33-41 
33-99 4- h 
28-52 -|- a 
24-37 + g + 
25-50 -f d 
25-59 4- c -1- 
223-85 -h a 
-ffr-fc-fri-f e 
~\~J ~f" Q -\~ h ~\~ 1 
e 
THE MISSING PLOT TECHNIQUE 
231 
There are nine missing values in this table, indicated by the letters a . . . i. Omitting 
purely numerical terms, which are irrelevant for the purposes of minimisation, we have 
for the total sum of squares, 
a2 + 62 + c2 + . . . + i* — -gL (223-85 + a + b + c + . . . + *)2 ; 
for the sum of squares between blocks, 
x { (20-48 + a)2 + (19-38 + 6)2 + . . . + (19-10 + h + i)2} 
- 3-L (223-85 + a + b + c + . . 
and for that between treatments, 
TV { (27-51 + &)2 + (24-96 + /)2 + 
+ i)2; 
+ i)2. 
. . + (25-59 + c + e)2 } 
— ¥V (223-85 + a + 6 + c + 
The residual sum of squares is the difference of the first and the sum of the second and 
third of these expressions. For minimisation we differentiate with respect to a, b, . . . i 
in turn. On some arithmetic simplification we find 
63a -f- b -\- c-f- d + e -f- / -)- gr~|~ 
a + 636 + c + d + e -f- / + </ +• 
a + 
a + 
a -f- 
a +- 
a -f- 
a + 
a + 
6 + c + 63cZ — 9e + 
- 7c - 9d! + 63e + 
b - 
6 + 
0 ™j~" 
6 + 
6 + 
c -|- 
fj —4 
c + 
/ + 
d + e + 63/ - 
d + e + 
(y —J— /^/ —J— 
g _|_ fo _J- 
Q -J— ^/ —j— 
9gr + h + 
9/ + 63# + ^ — 
/ + # + 63A — 
i = 209-11 
i = 190-03 
i = 231-67 
i = 199-35 
i = 200-07 
i = 199-73 
7i = 195-01 
9i = 239-07 
c? + e + f - 7# - 9h + 63i = 162-11 
This set of linear equations can, of course, be solved by routine methods, but also by iterative 
processes as follows :— 
The mean of existent values is 3-15. Assume this to be approximately the values of 
b, c . . . i. Then for a we have, from the first of the above equations— 
a = -& {209-11 - (8 X 3-15) } = 2-92. 
Taking this value of a and 3-15 for c, d . . . i, we find for b from the second equation, 
b = fe {190-03 — (7 X 3-15) - 2-92} = 2-62. 
Similarly, from the third equation, 
c == ik {231-67 + (2 x 3-15) - 2-92 - 2-62} = 3-69, 
and so on. On reaching i we recalculate a from the first equation, using the approximations 
to the values of the other constants already obtained ; and so on until our values do not 
alter. In this case only a second approximation is necessary, the values being— 
First Approx. 
Second Approx. 
a 
2*92 
2-88 
b 
2-62 
2-58 
c 
3-69 
3-73 
d ; 
3-27 
3-33 
e 
! 
3-76 
3-76 
/ 
3-26 
3-32 
9 
3-60 
3-61 
h 
3-88 
3-89 
i 
• 
3-22 
3-22 
These are our estimates of missing yields. The treatment means are found to be :• 
12 3 4 5 6 7 8 
3-009 2-828 3-341 3-788 3-140 3-120 2-883 3-308 
232 
THE ANALYSIS OF VAEIANCE 
24.19. The question now arises how we may analyse the variance of data for which 
missing values have been estimated in this way. 
The original data provided a classification with unequal numbers in subclasses and 
can be analysed by the methods given earlier in the chapter ; except that, since no 
subclass contains more than one member, we cannot find a residual sum of squares within 
subclasses based on N — pq d.f. (N — pq, in fact, is a negative number.) For instance, 
regarding the data as a one-way classification with pq — r classes, we shall have an analysis 
of this type :— 
Sums of squares 
d.f. 
Between classes * 
Residual . 
p +q -2 
(p - 1) (q 
-1) 
Total 
pq 
1 
r 
. (24.40) 
The effect of the two classifications separately can be dealt with in the manner of 
Example 24.1. 
4 
24.20. Two simplifications are possible. In the first place, since the minimisation 
of the residual is the same for the original data as for the data completed by estimates of 
missing values, we can use the latter to compute the residual precisely as for an orthogonal 
case, which simplifies the arithmetic. 
Secondly, it appears that to an adequate approximation we may substitute the 
estimated values for missing values and analyse the resulting material in the ordinary way 
as if it were orthogonal. If the proportion of missing values is high this approximation 
may perhaps break down, and in practice we should probably regard the experiment as 
ruined. More usually only a few records are missing, and the effect of replacing them by 
estimates is hardly likely to affect judgments of significance seriously. 
Example 24.6 
Continuing the analysis of the data of the previous example, we find, for the total sum 
of squares, 32-1012 with 70 d.f. The analysis of the completed data, that is to say the original 
data plus the estimates of missing values, is as follows :— 
Sum 
Between treatments . 
Residual .... 
Totals 
of 
. 
Squares 
• • 
* 
9-7176 
6-5812 
17-6902 
33-9890 
d.f. 
9 
7 
54 
70 
Quotient. 
1-0797 
0-9402 
0-3276 
* It is assumed that no row or column in the two-way classification is entirely empty. If it were, 
we should have to ignore it and confine attention to the remaining arrays. 
RELATIONSHIP WITH REGRESSION ANALYSIS 
Treating the original data as a case of unequal class numbers we find :- 
233 
Sum of Squares. 
Between blocks and treatments 
Residual .... 
Totals 
14-4110 
17-6902 
32-1012 
d.f. 
16 
54 
70 
Quotient. 
0-9007 
0-3276 
For blocks only :- 
Sum of Squares. 
Between blocks 
Remainder. 
.Blocks and treatments 
8-5690 
5-8420 
14-4110 
d.f. 
9 
7 
16 
Quotient. 
0-9521 
0-8346 
For treatments only : 
Sim i 
Between treatments . 
Remainder. 
of Squares 
■ * • 
Blocks and treai 
.»ment8 
• 
6-2648 
81462 
14-4110 
d.f. 
7 
9 
16 
Quotient. 
.... 
0-8950 
0-9051 
Whether wo use the analysis of completed data or the more exact form, we see that 
differences between blocks and between treatments are significant as judged by the residual 
variance. The two analyses are, in fact, not very different, and even with as many as nine 
missing values out of SO we should not err by substituting estimated values and treating 
the data as orthogonal. 
Relationship with Rvyrvssion Analysis 
24.21. The general //-way classifications to which variance-analysis may be applied 
are not necessarily determined by a measurable variate. As for contingency tables, rows 
or columns can be interchanged without affecting the analysis. We can, however, regard 
a multivariate frequency table as an w-way classification and apply variance-analysis to 
it; arid just as regression and correlation analysis provide a refinement on contingency 
analysis because of the arrangement of the classes in order by reference to a variate, so we 
may to some extent refine the analysis of variance in such a case. 
24.22. Consider in the first instance ajx? table of frequencies in the form of a 
correlation table. We will suppose the ^-classification to be according to the variate x 
234 
THE ANALYSIS OF VARIANCE 
and the ^-classification according to y. Let us now consider the hypothesis that the data 
emanate from a normal bivariate population with zero correlation (or, somewhat more 
generally, that for any given y the x's are distributed normally with the same mean and 
variance). We can then regard the data as a one-way classification according to y with 
unequal frequencies and analyse the variance in the usual form :— 
Sum of Squares. 
Between classes . 
Residual 
Totals 
q 
3 = 1 
S(xi:j - %)2 
N var x 
d.f. 
ff-1 
N -q 
N - 1 
Quotient. 
N rj2 var x 
N (1 — rj2) vara; 
—^jr— 
Here Xj is the mean of n$ ^-values in the jth ^/-class, x is the mean of all N values, x.tj is the 
variate-value in the ith x-class and jth. y-class, and there are q ^/-classes. The quotients 
are expressible in terms of the correlation ratio of x on y, viz. rjxy (cf. 14.23. vol. I, p. 351). 
Now, on our hypothesis, the sums of squares between classes and the residual are 
independently distributed in the Type III form, and hence the variance ratio 
rj 
N ~q 
3-11 
7]' 
(24.41) 
can be tested in Eisher's distribution with v± = q — 1, v% = N — q. This is the test we 
gave in 14.25 (vol. I, p. 353) and it is reached by an argument of essentially the same 
kind. 
24.23. Now suppose that our p x q table is normal but correlated ; or, somewhat 
more generally, that the values in arrays of constant y are normally distributed with the 
same variance but with means which vary linearly with y, say 
mr 
m + 6 j/.. 
(24.42) 
Then our data can be represented by the form 
x.tj = m + byj + dp (24.43) 
where the £'s are distributed normally with zero mean and the same variance v. Apart 
from the constant m, the only unknown here is the constant b. Our least-squares estimates 
(measuring from the means of x and y) now lead to the familiar form for the regression 
coefficient 
h = 
(24.44) 
where summation takes place over all values observed. This is, of course, equivalent to 
h = CQV fa V) 
var y 
(24.45) 
RELATIONSHIP WITH REGRESSION ANALYSIS 
235 
Further, the reduction in sum of squares attributable to fitting the constant b is 
„xV cov^ (x yj\ 
Nb cov (x, y) = \-iJLl = J\r r2 var x, 
var y 
where r is the correlation coefficient of the sample. 
Our analysis of variance may then be written— 
(24.46) 
TABLE 24.7 
Analysis of Variance of a Correlation Table 
Sum of Squares. 
Regression constant 6 
Between classes (after regression is eliminated) 
Residual 
Totals 
Nr2 var x 
N (f]2 — r2) var x 
N (1 — ?72) var x 
N var x 
d.f. 
1 
ff -2 
N ~q 
N 
Quotient. 
Nr2 var x 
N r- var x 
2-2 
1 - ri2 
jV -=7= var a? 
This analysis gives us a test of the significance of the correlation coefficient in samples 
from an uncorrelated population and also of linearity of regression. 
In fact, if the parent correlation is zero, the parent value of b is zero and the quotient 
due to b is independent of the sum of the other items in the analysis. Thus the ratio 
Nr2 var x 
N (1 — r2) var x 1 — r 
. (24.47) 
is distributed in Fisher's form with vt = 1, v% = N — 2. This is equivalent to saying that 
'rZ{^P (24.48) 
is distributed in " Student's " form with N — 2 d.f., which brings us back by a different 
route to the test given in 14.15 (vol. I, p. 342). 
24.24. Secondly, if we assume that the parent correlation is not zero but the 
regression is linear, the sum of squares between classes after regression is eliminated is independent 
of the residual in Table 24.7, and hence the ratio 
N var x - 
q — 2 __ ?72 —- r2 N — q 
N var x 
Tj 
q-2 1 
rj 
. (24.49) 
#-9 
is distributed in. Fisher's form with v± = g — 2, v2 = N — q. This test (due to Fisher 
himself) gives a test of linearity of regression in the normal case. 
It should be noticed that this test is only approximate if the classification is one of 
a normal population with broad groupings. If correlation exists, the distribution of a 
bivariate normal sample in an array of finite width is not exactly normal, being the sum 
236 
THE ANALYSIS OF VARIANCE 
of a number of normal distributions with slightly different means. Unless the grouping 
is very coarse, this is not likely to invalidate tests of significance in practice. 
24.25. Consider now the general regression formula for p variates, 
Xi ===1 0% X% \ ^3 "^3 ~T* * • * ~T* ^7} Xp. 
. (24.50) 
v 
If we assume that the residuals x± — /.fyXj (say x) are distributed normally with 
constant variance, our least-squares estimates of the regression coefficients are those given 
by the usual theory, and the fitting of (p — 1) constants reduces the sum of squares by 
Nv&T x jR2, where R is the multiple correlation coefficient (cf. 15.16, vol. I, p. 380). We 
then have the analysis— 
Sum of Squares. 
Between, classes (regression constants) 
Residual 
Totals 
N var x R'- 
Nvaxx(l - JR*) 
N var x 
Quotient. 
R2 
p - 1 
1 -IT2 
'N ~p 
N var x 
N var x 
If the regression is in fact linear of type (24.50), the residual quotient is independent of 
that due to fitting regression constants, and the hypothesis may be tested by means of 
the ratio 
R2 N -p 
p - 1 1 - R2 
which is distributed in Fisher's form with vx = p - 
the distribution of R2 given in 15.20. 
1, v2 - N 
. (24.51) 
£>. This brings us to 
24.26. It is to be observed that in (24.50) we may choose the variates d\> . . . xp 
as we please. In particular, we can take them to be polynomials of a single variate. From 
this point of view the analysis of variance links up with the theory of regression analysis, 
given in Chapter 22. If the polynomials are orthogonal we can fit the constants b one 
at a time, the fitting of any constant leaving unchanged the previous determination of those 
of lower orders. The reduction in sum of squares for each constant can be separately 
ascertained and corresponds to the loss of a further degree of freedom ; and at any stage 
we may test the residual variance to see whether any particular term is worth while in the 
sense that it makes a significant contribution to the total variance. The exact test, of 
course, depends on the usual assumptions of normality. 
24.27. The reader is now in a position to see a number of statistical topics which 
on the surface appear to be distinct as parts of a single theory. Eegression analysis, with 
its subsidiary of correlation analysis, proceeds by the successive fitting of constants by 
least-squares. For the normal case this is equivalent to estimation by maximum likelihood. 
Partial and multiple regression, together with curvilinear regression, can all be subsumed 
THE ANALYSIS OF COVARIANCE 237 
under this central idea. The fitting of each constant splits off a separate contribution to 
the total variance which, under certain hypotheses, is independent of the others. Variance- 
analysis proceeds in much the same way, but is more general in the sense that it can deal 
with the classification of values, however determined. Our various exact tests of 
significance of homogeneity in variance, of linearity of regression, of significance of correlations 
in uncorrelated material, of the difference of two means where variances are equal, of the 
correlation ratios, of the multiple correlation coefficient—all derive ultimately from Fisher's 
distribution of the variance-ratio in the normal case. 
The Analysis of Covariance 
24.28. Suppose that we have a one-way classification, possibly with unequal numbers, 
and that in each class the members present values not of a single variate, such as we have 
considered up to now, but pairs of variate-values typified by xij} yij: j referring as usual 
to class and i to the number within the class. By the ordinary methods of variance-analysis 
we can discuss the effect of classification either on the sc-variate or on the ^-variate ; but 
there also arises for consideration the effect of class-membership on the covariation of 
x and y. This leads us to an extension of the analysis of variance to that of covariance. 
24.29. By an easy extension of the results for a single variate" we have, analogously to 
>"i 'J i, 2 j 
the equation in product terms 
£(*u •>'..) (?//i - v..) = £(xij - xj) toy - yj + J£nj (x.i - x..) (y.i - 2/..) (24-52) 
U'j i,j j 
If we consider the whole sample as homogeneous the correlation between x and y is given by 
vWW^.^^y^y.yy ' " ' [ 
We have also the correlation between means of classes 
„_ ^^.i-^..)(y,j~y..) mm 
V {2 (xmj - x'..)" 2 {y,} - ?/.> } 
and may calculate4 a. correlation of residuals within classes 
,. __ % (xfj — xmj) (y>j — ymj) ,~> rr, 
v v-' wj xj) ^ vJtj y.j) j 
24.30. If there is heterogeneity present we should expect these correlations to differ ; 
and similarly for the three kinds of regression of y on x, such as 
b=£^ij T. ?■-•).fyA~JL») (24.56) 
E (x{j - a..)2 
The three correlations of (24.53)-(24.55) are, however, not additive, like sums of squares ; 
nor are the regressions corresponding. The covariances expressed by (24.52) are additive, 
but there is no simple test, such as exists for variance-ratios, to determine the significance 
of differences or ratios of covariances. Covariance analysis, however, is not primarily 
designed to test independence, but to examine whether there is any variation according 
238 
THE ANALYSIS OF VARIANCE 
to class between the regressions of y on x within and between classes. Let us suppose 
that there is some linear relation of the form 
Y — fly = (} (X — fig.). 
Following the notation of E. S. Pearson, we write 
@llj = % (Xij ~ X.j)2 
^22j 
^127 
£ {ViS 
i 
£ (xtj 
y.})2 
x.i) (Nig 
y.i) 
3 
- E C22j 
3 
- *- ^12j 
G 
C 
22a 
12a 
@llm — ^nj (x.j 
3 
022m = % nj (V.j 
3 
@12m — £ nj (x.j 
> 
X )2 
y..V 
x..){y.j -v..) 
> 
(24.57) 
(24.58) 
(24.59) 
(24.60) 
and Cue ^220? G120 for the corresponding total sums of squares and products. We may 
then exhibit the composition of the total sums of squares and products in the form of Table 
24.8. The arithmetic of the analysis follows that of ordinary variance-analysis. We 
shall give an example presently. 
TABLE 24.8 
Analysis of Variance and Covariance for One-Way Classification—Sums of Squaws and 
Products and Regression Coefficients. 
Variation. 
Within jth group 
Within groups . 
Between groups 
JL \J JL jfa.XJO • 
nj - 1 
N -p 
p - 1 
N - 1 
Sum of Squares 
oj-variate. 
#iii 
#lla 
#llm 
#110 
Sum of Squares.! Sum of Products. Regression 
2/-variate. ! xy. Coefficients. 
#22j 
#2 2a 
#2 2m 
#220 
0 
12 j 
0\2j 
c 
12a 
#12m 
#120 
°j 
K 
bm 
h 
#lli 
_ #_12a 
#11 a 
#12/ 
#11?] 
#120 
#110 
We now suppose that, apart from the regression effects represented by (24.57), the 
variation of x is normal with constant variance v. We can then compile various estimates 
of v from the residual variation after the effect of fitting regression constants has been 
THE ANALYSIS OE COVARIANCE 
239 
removed. For instance, within classes we have for the estimator of v, with N — 2p degrees 
of freedom, 
L i, j 
N —'2p 
1 
»j)} 
£ (C22j — fy ^12j) 
N — 2p j 
= •== — Sl9 say. 
N — 2p x J 
The number of degrees of freedom follows from the fact that we have fitted a mean and 
a regression coefficient to each of p classes, making a reduction of 2p in all. We then obtain 
Table 24.9 :— 
TABLE 24.9 
Analysis of Covariance for One-Way Classification with Linear Regressions. 
Variation clue to 
Deviations from linear regressions 
within classes 
Differences among regressions . 
d.f. 
N - 2p 
Sum of Squares. 
p - 1 
Deviations within classes from 
linear regression ba 
JV — p — l 
/J {y%j - y.j - h (x%j - ®.3)V 
i,j 
j 
JT (bj - bay (Xij - xmj)* 
• * 
1.3 
/ J {y%j - y.j - ba [shj - v.j)r 
i>j 
Deviations between classes from 
linear regression bm . 
Differences between ba and bm 
p - 2 
= (722a — ba G\2a 
— Sx + S% 
y nj {ymj - y.m - bm (xj - x„)}< 
i 
= (722m — bm Ci2m 
/ { (ba "™" bm) (Xij — Xmj) 
= £3 
U 
Total deviation from linear 
regression b0 
N - 2 
4- (&«» - &o) fay - <0>a 
lm 
(6fl - bmy Cn{) 
sA 
y] {ytj - 2/.. -b0 (xij - x..)y 
— ^220 ~~" ^0 C 
120 
= >S1 -|-/S'2 +$3 -f $4 
240 THE ANALYSIS OF VARIANCE 
The reader will probably find it useful to check the expressions in the third column of 
Table 24.9 and to examine how the sum of squares of deviations from the regression line 
of the whole is analysed into the constituent items. 
24.31. Suppose now that we wish to test whether the relationship between x and y 
can be represented by the formula (24.57), and that there is no material class-effect present. 
Then $x of Table 24.9 should be an unbiassed estimator of (N — 2p) v and should be 
independent of the residual estimator $2 + $3 + $4, which has 2p — 2 d.f. We may therefore 
test the hypothesis by the ratio 
If this variance ratio is insignificant we consider next whether the regressions differ in 
the p classes. For this purpose we compare the estimator derived from S2 with that based 
on $!; i.e. the ratio 
J1! . *L^?P9 v^p-h v2 = N - 2p . . (24.62) 
p — 1 bx 
will be significant if differences are to be regarded as real. 
If this ratio is not significant, Sx and S2 may be pooled. Comparison of their sum 
with $3 will afford a test whether the relation between group means is linear. The ratio 
for this purpose is 
^ ± S.a " . ?LrJ3 Vl = N - p - 1, v2 - p ~~ 2 . . (24.63) 
N — p — I Oa 
Finally, even if this ratio is not significant, it does not follow that the common regression 
within groups is the same as the regression of the means of groups. To test this point 
we consider the ratio 
Si ~f- $2 1 
N —p — 1, v2 = 1. . . (24.64) 
Example 24.7 
A number of recruits are given a preliminary test to ascertain their suitability for a 
certain course of training. At the end of the training course they undergo a proficiency 
test. The marks for three groups of recruits from three different towns are— 
p fPreliminary : 45, 50, 56, 58, 59, 60, 62, 64, 65, 75 
UP [Proficiency: 46, 60, 52, 46, 48, 50, 55, 63, 58, 64 
p fPreliminary : 44, 49, 52, 52, 58, 59, 60, 62, 63, 63, 66, 69, 70; 72, 73 
UP [Proficiency: 48, 55, 45, 60, 65, 64, 69, 71, 77, 70, 75, 80, 72, 75, 81 
fPreliminary : 47, 52, 59, 60, 63, 66, 68, 69, 74, 76 
UP [Proficiency: 43, 56, 51, 72, 60, 61, 55, 74, 72, 80. 
We are interested here in the efficiency of the preliminary test as a predictor of the 
proficiency test. We therefore consider the regression of the marks obtained in the latter 
(y) on those obtained in the former (x). We are, however, also very much interested in 
the question whether the regressions are the same, apart from purely sampling effects, 
in the three groups. Such a matter would naturally arise, for instance, if we were thinking 
THE ANALYSIS OF COVARIANCE 
241 
of applying the same rejection standards in preliminary tests to all recruits, irrespective of 
their town of origin. 
Our scores are given to the nearest unit, and hence the variates are discontinuous. 
We will neglect this effect and assume that the scores are distributed approximately 
normally. 
About origin x = y = 50 the sums of squares and cross-products are :— 
Group 1 
Group 2 
Group 3 
n. 
10 
15 
10 
S (x). 
94 
162 
134 
S(y). 
42 
257 
124 
£(x*). 
1496 
2802 
2556 
We can then calculate the quantities G. For instance 
94 
0 
in 
1496 - 94 
10 
94 
612-4 
G 
ii a 
694 - 42 — = 299-2 
10 
Gm + G1V1 + 6113, etc. 
£(y2)- 
594 
6101 
2776 
2 (any). 
694 
3989 
2422 
We find the following table in the form of Table 24.8 : 
lAJBJuJii 24.10 
Analysis of Variance and Govariancefor Data of Example 24.7—Sums of Squares and Products 
and Regressions 
Variation. 
d.f. 
Within first group 
second group 
third group 
j> 
Within, groups . 
Bobwno.ii groups 
Totals 
0 
14 
0 
09 
9 
34 
Sum of Squares. 
•V • 
C^ in ■--— 012 "4 
Gn, = 1082-4 
G'm = 760-4 
Cu„ = 2425-2 
Cilm = 83-09 
6'1)0 --= 2508-29 
Sum of Squares. 
y\ 
C\>n = 417-6 
OQ = 1697-73 
Caaa -13353-73 
C22wi= 1005-01 
CV>0 - 4358-74 
Sum of Products. 
GVrt - 299-2 
(7r>o - 1213-4 
<7l88 - 760-4 
Oi2a. = 2273-0 
C12l) « 2391-57 
Regressions. 
bt = 0-4886 
62 - 1-1530 
63 = 1-0000 
ba = 0-9372 
6m- 1-4270 
60 - 0-9535 
A comparison of the three regressions within groups indicates some heterogeneity. 
It looks as if the preliminary test is not such a good predictor for the first group as for 
the others. We may proceed to test the reality of this effect by constructing Table 24.11 
on the lines of 'fable 24.9. For instance, 
Sx = £{C.m - Cujbj) = (417-6 - 299-2 x 0-4886) + (two similar terms) 
j 
= 1048-1. 
Xju * k(J • V v«/ Jl-J * JL X1 • 
R 
242 
THE ANALYSIS OF VARIANCE 
We find— TABLE 24.11 
Analysis of Covariance of Data of Example 24.7—Linear Regressions. 
Variation. 
Deviations from regressions bj 
Differences bj . 
Deviations from ba • - - 
Deviations of groups from bm 
Difference between ba and bm 
Totals .... 
d.f. 
29 
2 
31 
1 
1 
Sums S. 
o 
o2 
33 
$4 
#! -f- tfa + £3 + £4 - 2078-4 
Quotient. 
1048-1 
175-4 
1223-5 
835-6 
19-3 
361 
87-7 
39-5 
835-6 
19-3 
A comparison of the quotient 36-1 (29 d.f.) with the quotient of the remaining items, 
257-6 (4 d.f.) indicates that there are real differences between classes. A single regression 
equation will not represent all three class-relations. A comparison of the deviations from 
regressions, 36-1 (29 d.f.), with the differences of regressions among themselves, 87-7 
(2 d.f.), does not reject the hypothesis of equality of regressions within groups. We 
therefore compare the deviations from ba, 39-5 (31 d.f.), with the deviations of groups from bm, 
835*6 (1 d.f.). This is significant, suggesting that the hypothesis of linearity of regression 
of group-means should be rejected. 
The general result is to confirm our suspicion of heterogeneity, 
coefficients between x and y are— 
0-592 
The correlation 
I T 1. UJ.llJ.1 J.J.L KJ V C~ JL VUIJ • • • 
„ second group . 
,, third group 
Within groups .... 
Between groups .... 
Total ..... 
. . \J U«./ <M 
. 0-908 
. 0-784 
. 0-797 
. 0-410 
. 0-722 
Again the deviations between groups stand out as indicating heterogeneity. 
24.32. The analysis of covariance may be extended to the case where there is more 
than one independent variate. The regression coefficients are found in the usual way, 
and the sums of squares after regressions have been removed can be found and compared 
on the usual hypotheses. Suppose, for instance, there are two independent variates and 
a classification giving an analysis between classes and residual. We may represent the 
analysis thus :— 
Between classes 
Totals 
d.f. 
n 
n" 
Sum of Squares. 
A' 
jri. 
o 
B 
B' 
B" 
c 
Gf 
C" 
Sum of Products. 
P 
P' 
P" 
Q 
Q' 
y*>. 
R 
R' 
I?" 
THE ANALYSIS OF COVARIANCE 
243 
Our regressions are then™ 
Between classes 
Residual 
Totals 
&1 
BQ - PR 
AB - P2 
B'Q' - P'R' 
A'B' - P't 
B"Q" - P"R* 
ATB" - P//2 
&2 
JLJci' —Jrty 
AB - P2 
-4'P' - P'Q' 
A'B' - P'2 
.4"P" - P"Q» 
A"B" - P'f* 
The sums of squares C can then be reduced by eliminating regressions, i.e. by subtracting 
Qbx + Rb,, giving 
C 
BQ*- 
AB 
ABC 
PQR 
AR* 
PQR 
rpi- 
AR* - BQ* - OP2 ±2PQR 
^^____ . . _ 
. (24.65) 
This and the analogous quantities with primes give independent estimators of the 
variance of the residual element, and a comparison to test homogeneity may be made in 
the usual way. 
24.33. In a case such as that of Example 24.7 it is evident that a comparison of 
?/-means between groups is affected by what we know about the #-values. If we know nothing 
about the latter, comparison of the ?/s is a univariate problem and can be treated by the 
methods already discussed, the difference of means, for example, being tested by the use 
of standard errors or the #-test. But suppose that our x's themselves are found to be 
different between groups and that there is significant correlation between x and y. Then 
it is possible that the relation, if any, between ?/s in different groups is not, so to speak, 
an inherent quality of the variation of ?/, but is merely a reflection of their dependence on 
the #\s, which happen to exhibit significant differences. In Example 24.7, differences in 
proficiency between groups may be due simply to differences of ability which were present 
before the training began and, if so, should be shown by differences between groups in the 
preliminary scores. We should not then be able to conclude from proficiency scores alone 
that training in one group had a more marked effect than in another. The differences 
were there before the training was applied. 
24.34. If, then, we require to consider the effects of training alone on the groups, 
we may " correct " the y-values by deducting the estimates 
y«=0.. +bo(xij-x..) (24.66) 
ij 
or other more general regression equations. This, so to speak, allows for differences due 
to variations of the x-variate. 
244 THE ANALYSIS OF VARIANCE 
Assuming that one linear regression equation adequately describes the relationship 
between y and x, so that the corrected values are 
Vii - Yio = Vio ~ 2/-. ~ 6° (% ~ a'-.)' ' (24.67) 
we see that the difference of the corrected means of two classes ymj and ymk is 
V.j - V.k - ^o {xmi - xmk). .... (24.68) 
This may be regarded as the sum of two parts which are independent. The estimated 
2s2 
variance of the first part, y j — y k, is —, where s2 is the mean-square of the residual after 
q 
correcting for regression and the means of y • and ymk are both based on q members. Shni- 
s2 
larly the variance of b is -j, where A is the sum of squares of the x-variate entering into 
the residual row of the analysis. Regarding the #'s as fixed from sample to sample, so 
that our inference is conditional, we see that the variance of the difference (24.68) is given by 
S212 + (^LZL^Il .... (24.69) 
U A J 
The ratio of the difference to the square root of this expression is distributed as " Student's " 
t, with degrees of freedom one fewer in number than those of the original residua]. 
24.35. Similarly, if we have two independent variables x± and x2i the corrected 
difference of ^/-means is 
y.j ~ V.k ™ {bi (xij - xik) + b., (x2j - xu) } . . . (24.70) 
where temporarily we write x^ for the mean of the variate xx in the jth class, and so on. 
The variance of the part in curly brackets may be derived by considering the variance of 
the general expression Xbl + (j,b*,. From the equations for bx and b. we have 
h = B £ (yxJ ~ P E (VX^ 1 
h = ~~ P 2 (yXl^ + A^ ^Xi^ 
2 " AB- P2 
. ( •ii'db. i 3L I 
where, as in 24.32, A and B are the sums of squares for xu x», and P is the cross-product. 
Thus the coefficient of any y in Xb1 + fib2 is 
(IB — /iP) xx + {^A — XP) x2 
Since the y's are independent the estimated variance of Xbl -j- {ubz is 
s2. 
(AB - P2)2 
{ A (IB - [aP)2 + 2P [XB - (iP) (fiA -~ XP) + B (fiA - XPy } 
_X2B^2XfzP + ^A 
~~ AB-P2 ("4-7^ 
Thus for the estimated variance of the corrected difference (24.70) we have 
i2 i 2 X2B -2XpP + ^A 
~q + ~ AB- P~" " / " • ' * (24*73) 
where X = xH — xlk and p = x2j — x2k. As usual, the difference divided by the square 
root of this quantity may be tested in the ^-distribution. 
THE ANALYSIS OF COVARIANOE 245 
24.36. Our account of the analysis of variance and covariance has not attempted 
to cover all the applications of the method in particular directions. We have concentrated 
so far as possible on the fundamental ideas and the broad lines of analysis to which they 
lead. Some further developments will be given in later chapters, but we must refer the 
reader who requires a complete acquaintance with the subject to the references given at 
the end of this chapter and the preceding. We will conclude our exposition with three 
final comments. 
(a) Part of our hypothesis throughout has been that the residual element f has constant 
variance from one subclass to another. In Chapter 26 we shall discuss methods of testing 
homogeneity in residual variance. For completeness we might perhaps have anticipated 
some of these tests in the present chapter, at least to the extent of exemplifying their use. 
We have not done so mainly for reasons of economy in space ; but the omission of mention 
of the point in foregoing examples should not lead the reader to overlook (as many writers 
do overlook) the necessity for testing variance-homogeneity where possible, if it is required 
as part of the hypothesis. 
(b) In the majority of our examples we have proceeded at once to analyses of variance 
or covariance without dwelling on points which would require attention in any practical 
inquiry. For instance, since the primary function of many variance-analyses is to test 
the homogeneity of a set of class-means, the first stage would be to compute those means 
and examine whether they suggest any lack of homogeneity on intuitive grounds. Again, 
if heterogeneity is established, consideration of the means themselves, or of the primary 
data, will sometimes show how it arises. The student must never lose sight of his primary 
material. 
(c) Elaborating this point to some extent, we would emphasise that the analysis of 
variance, like other statistical techniques, is not a mill which will grind out results 
automatically without care or forethought on the part of the operator. It is a rather delicate 
instrument which can be called into play when precision is needed, but requires skill as 
well as enthusiasm to apply to the best advantage. The reader who roves among the 
literature of the subject will sometimes find elaborate analyses applied to data in order to 
prove something which was almost obvious from careful inspection right from the start; 
or he will find results stated without qualification as ''significant" without any attempt 
at critical appreciation. This is not the occasion to deliver a homily on the necessity for 
self-discipline in the use of advanced theoretical techniques, but the analysis of variance 
would provide quite a good text for a discourse on that interesting subject. 
NOTES AND REFERENCES 
For the analysis of variance where subclass frequencies are unequal, see Brandt (1933) 
and an important paper by Yates (1034a). Wilks (1938e) has considered the subject from 
the theoretical viewpoint and exhibited the main results cleterminantally. For the missing 
plot technique see Allan and Wishart (1930) and Yates (19336). For the analysis of 
covariance see Fisher's Statistical Methods, Bartlett (1934a), an appendix by E. S. Pearson 
to a paper by Wilsdon (1934), Brady (1935), Wishart (1936), and Day and Fisher (1937). 
The last-mentioned paper works through a practical example in some detail and will 
repay study. 
See also references to the previous chapter. 
246 THE ANALYSIS OF VARIANCE 
EXERCISES 
24.1. For a two-way classification with one member in each subclass show that, 
for normal variation, 
and hence that the sums E (xj — xtt)2 and Z (xtk —• x_)2 are independent. Examine 
j " ft 
how this breaks down for the non-orthogonal case. 
24.2. Verify the arithmetic of Example 24.6. 
24.3. Generalise formula (24.73) in the following way. If there are m independent 
variates, the variance of corrected differences is 
m 
I? r,s = l J 
where Ar = xrj — #r/c, and crs = —■ where Ars is the cofactor of ars in the determinant 
I &„«> I, and summed over the sample. 
(Wishart, 1936.) 
24.4. Derive by the analysis of variance the test of a regression coefficient given 
in 22.19. 
CHAPTER 25 
THE DESIGN OF SAMPLING INQUIRIES 
Influence of Theory on Sampling Design 
25.1. The reader who is accustomed to handling the results of a sampling investigation 
as they appear in everyday statistical work may have wondered more than once in previous 
chapters whether theory was not reaching out too far in advance of practice. It is true 
that for certain types of experimental inquiry, notably in agricultural and biological research, 
the precision of exact statistical tests does not seem out of place ; but in economic or social 
statistics, for example, there is often so much error and imperfection in the raw data that 
the application of refined methods of analysis would be a waste of time. It is clearly 
useless, and may even be dangerous, to exercise an elaborate mathematical technique on 
data which are suspect from the very start of the inquiry. If our theory is to be really 
serviceable to the statistician and not merely an enticing mental exercise it must be capable 
of solving practical problems. 
25.2. Now it has to be admitted that much of the material with which statisticians 
have to work at the present day cannot be treated by the methods expounded in the 
foregoing pages when sampling questions are concerned. The commonest reason, but by no 
means the only one, is that the sampling process by which the data were obtained was 
biassed. In such cases the statistician has to lay aside the refined implements of his craft 
and do the best he can with his refractory material in the light of his own judgment and 
eommonsense. A good deal of current statistical work is of this kind, and there is even 
a section of thought which is inclined to depreciate the advanced theory of the subject as 
4* academic " in the sense that it is too remote from practical affairs to be worth studying. 
The misunderstanding is not likely to be removed by the counter-accusation sometimes 
launched by theoreticians that the theory is quite capable of being applied by anyone who 
has the ability to comprehend it. 
25.3. .Fortunately there is a growing realisation that the two points of view can 
often be reconciled by collecting the data in such a form that the theory can be applied to 
it. If only enough care is taken at the initial stages of an inquiry there is no need for the 
appearance of imperfect data which defy exact analysis. Knowing beforehand what 
theoretical instruments are at our disposal, and armed with a clear understanding of what 
questions we are trying to answer, we can frequently frame the investigation so as to 
maximise the information acquired with the minimum of effort. In short, the scope and nature 
of our theory itself dictates, to some extent, the form which the sampling inquiry should 
assume. In former times the statistician was usually asked to extract information from 
data, which were collected by inexpert agents, frequently for quite different purposes. 
Nowadays he is still in the same position in some respects, but sometimes he is called in to 
advise on the design, of the inquiry and can, within limits, determine the form in which the 
data are collected". He can make his theory applicable by selecting his sample in the 
proper way. 
25.4. The general theory of the design of sampling inquiries has not progressed far 
enough for us to be able to give a systematic account of it in this chapter. In some fields, 
247 
♦ 
248 DESIGN OF SAMPLING INQUIRIES 
particularly that of agricultural experimentation, it has reached quite an advanced degree 
of perfection ; in others there remain many problems unsolved and possibly many more 
which have not yet even been formulated. At the risk of some discontinuity of treatment, 
therefore, we shall only give in this chapter a number of instances in which theoretical 
considerations exert a considerable effect on the scope of a sampling inquiry, in order to 
illustrate the field to be covered. There are, of course, manj| factors which ultimately 
determine the form of an investigation, such as cost and expenditure of time, but they will 
not concern us here. For the present we shall be concerned solely with the extent to which 
theoretical considerations contribute to all the factors that have to be taken into account 
when an inquiry is designed. 
Some Preliminary Points 
25.5. There are certain preliminary points which, though obvious enough when stated 
explicitly, are often overlooked and cause a good deal of bad design. 
(a) The fundamental object of sampling is to obtain information about a population, 
and it is of the first importance to begin with a clear idea of what that population 
is. Imagine, for instance, that we are asked to ascertain whether pasteurised milk has 
a different feeding value from raw milk. In what population is this inquiry to be made : 
among children ? among the inhabitants of the British Isles ? among those who habitually 
drink milk or those who do not ? among townspeople or among country folk ? and so 
on. Again, suppose that we are given a new variety of barley and wish to know whether 
it has a heavier yield than a previously known type. Do we mean heavier in the usual 
barley-growing areas ? in every kind of climate or on the average over a series of different 
climatic conditions ? when subject to the same manurial treatments as those in current 
use ? and so on. 
(b) In a similar way, it is necessary to have an equally clear idea of what we are trying 
to find out about the population. In our example of raw and pasteurised milk, are we 
content to know that there is (or is not) a differential effect for children as a whole ? or do 
we wish to ascertain whether any such effect varies at different ages, between sexes, or 
according to nutritional standards ? What exactly should we like to know ? It is no use 
returning the facile reply " all about it " to this query, for our information must be limited 
in virtue of the finite size of our sample. We must make up our minds what information 
we require and which questions have priority if it becomes necessary to sacrifice some of 
them for practical reasons. 
(c) Thirdly, we should consider what we know already about our population. This 
point becomes of particular importance when our prior knowledge indicates heterogeneity, 
for then we may, in effect, have to divide the population into sub-groups and sample 
separately from each. In our milk example, it is to be expected that children of different ages 
may react differently, or that children from lower-class schools may respond differently 
from those in middle-class schools. Or again, in our barley example, the two varieties 
may compare quite differently on Hertfordshire loam and on Lincolnshire chalk. It would 
be misleading to lump all the comparisons together when we have strong reason to suspect 
heterogeneity beforehand. In effect, prior knowledge of this kind frequently dictates the 
types of question we ask under (6), and the two are often different facets of the same problem. 
(d) As an extension of the same point, we may notice that prior knowledge about the 
population sometimes indicates what sort of averages to use and what sort of tests of 
significance it is proper to apply. Crop-yields, for instance, are known to be distributed 
in a form approaching the normal, so that arithmetic means are good estimates of parent 
STRATIFIED SAMPLING 249 
means and the tests based on normal theory may be applied. Accident statistics, on the 
other hand, are often distributed in a modified Poisson form ; income statistics in a J-shaped 
form, and so forth. 
(e) A specification of the population and a decision as to the precise object of the 
inquiry will usually determine certain parameters which it is required to estimate or certain 
hypotheses for test. In general the problem is one of estimation, but not necessarily so. 
In our case of pasteurised and raw milk, for instance, we should probably wish to know 
the exact amount of the difference between the effects of the two (a matter of estimation), 
not merely whether a difference existed (a matter of significance). We then wish to know, 
before the inquiry begins, whether the estimates we shall have are going to be accurate 
enough for our purpose ; or alternatively, if the sample is of a given size, how accurate they 
will be. It may not always be possible to answer such a question completely beforehand, 
since the sampling variances will in general depend on quantities which have to be estimated 
when the data are available, but it is always useful to consider in a general way what sort 
of magnitudes would be shown as significant and what values would leave us still in 
reasonable doubt. As a rule, matters such as this are closely related to sample size. 
(/) Finally, our estimates will be subject to experimental error and, in development 
of the last point, we have to try to find the form of experimental design which, while 
answering our questions, does so with the minimum error. From a slightly different standpoint, 
if we can determine the amount of error which is admissible, the problem is to find the 
design which achieves no more than that error with the minimum expenditure of effort. 
Furthermore, we require to be able to estimate the extent of probable errors. In short, we 
require an efficient design, just as the engineer requires an efficient engine or the aircraft 
designer an efficient form of airscrew, and for exactly the same reasons. 
25.6. To sum up, our primary task in embarking on a sampling inquiry is to ascertain 
as accurately as possible what is the population under examination, and what is the 
information about it which we require. If, as usually is the case, that information concerns 
statistical characteristics such as means and variances, or more generally frequency-distributions, 
our second task is to design an inquiry which will provide estimates of these unknown 
quantities and will, at the same time, provide estimates of their sampling error. It is not 
always possible, as we shall see later, to obtain full satisfaction in the reduction of error 
and the estimation of error simultaneously. Increased accuracy of estimation may mean 
loss of precision in our estimate of sampling error, so that although we are nearer the truth 
we do not know how near. There does not appear to be any single rule which will cover 
all the cases that can arise. We shall refer to a particular case of some interest in 25.39. 
Stratified Samp ling 
25.7. We consider at the outset a case of fairly frequent occurrence in the sampling 
of existent populations. Suppose we are interested in the mean value of a variate x in 
some population II; and that we know, or suspect, that the population is heterogeneous 
in the sense that we can delimit sub-populations Hu /72, . . . IIk in which the distributions 
according to x may differ. This type of case might, for example, arise if we were sampling 
the population of a town for income, there being districts, wards or even streets which are 
known to be inhabited by classes living at different income-levels. 
Practical considerations alone may require that we draw a prescribed portion of the 
sample from each sub-population. For instance, with a town of 500,000 inhabitants it 
250 DESIGN OF SAMPLING INQUIRIES 
would be most tedious to sample by using random numbers applied to the whole town. 
We should probably divide the work among districts and blocks and select random samples 
within the blocks. This, however, is not to be confused with the division of the town into 
relatively homogeneous districts because of its heterogeneity. Either process is called 
stratification. The problem we shall discuss is this : If we have decided to draw a total 
sample of n members, and can assign at will the number ^ drawn from the ith stratum 
JJi3 subject to the condition E (%) = n, how should we choose the numbers %, or need we 
choose them at all ? • Will our estimate of the mean value of x be better if we merely choose 
n members at random from II, or can we improve it by controlling the numbers nt and not 
merely leaving them to chance ? 
25.8. Let Xy be the ^"th member of the sample from the ith sub-population, and let 
the latter contain a number Ni of members with mean ^ and variance of. If fi is the 
mean of II we shall have 
fi = -ZrJjjNiPi' (25*1) 
i = l 
We shall now seek for parameters Atj such that our estimator of fjt, say t, is given by 
k ni 
t — / j y (A^ x^), ..... (25.2) 
that is to say, is a linear estimator in the observed variate-values. We shall seek for that 
estimator which is unbiassed and has minimum variance, i.e. for which 
JLU \t) == jA> ..... . \j&0.O) 
E {t -E (t) }2 = minimum. .... (25.4) 
Substituting from (25.2) and (25.1) in (25.3), we find 
and since E (x^) = fi,t this gives 
E fiil E Ay — -yTT- i=0.. . . . . (25.5) 
For this to be generally true we must have 
Ehi 
N_ 
N 
I 
(25.6) 
a first condition on the A's. If hL is the mean of Ay in the ith. set we have 
A; =: •=-=— ....... (ZO.i ) 
.Nnt v ' 
Now consider (25.4). The variance of t is the sum of h variances, for the samples from 
sub-populations are independent. Consider then the variance of EXyXy, remembering 
STRATIFIED SAMPLING 251 
that the population of Ni members is finite. We have 
variance = E S {Xtj (xy — ^) }2 
s^% ai + Zj iE kj kk ixa — to) (xik - N) }> j ^k 
j j, k 
2 
iVT-T "TV/-T 
% 
•2 
0~i 
N.-J i** ^ - ■**) ^ + N* E fa - X^}- ■ ■ ■ (25-8) 
This is clearly minimised only if 
Xi5 - lL = 0, (25.9) 
that is, if all the A's for any sub-population are equal. This is what we should expect on 
intuitive grounds, for there is no reason for weighting the sample members differently in 
the same sub-sample. 
Our minimal variance, say v, is then given from (25.8), by summing over i, as ' 
-o? IN* — nA 
v=?- TV - 1 ■"*%• 
2 
1 y (A {Ni "~ Mi) N'l 
N* 7 N< - 1 n 
3 
1 o? iV? 
= JT» f (tf< 1- 1) % + C°nStant (26J0) 
This is a minimum for variations in n,t subject to 27% = n if 
b 
where p is an undetermined constant. This yields almost at once 
n\ acjp^j. (25.11) 
25.9. If we know the population variances g\ and the numbers N\ this equation 
determines the numbers n{; but in practice it is rather unlikely that we should know the 
variances without knowing the means, in which case we should not have to sample to find 
the mean of the whole population. Our result is not, however, useless. In the first place 
we find for the estimator t 
i i ij N n* 
2J —jrjr%i. ..... (2o.l2) 
so that the estimate is a weighted average of the sample means, the weights being 
proportional to the population numbers Nti not to the numbers nt. Secondly, without knowing 
the variances a\ exactly, we may sometimes reach approximations from prior knowledge 
of the populations. Such, values, without giving absolute accuracy, will at least represent 
improvements on selecting the w's by chance. 
252 DESIGN OF SAMPLING INQUIRIES 
25.10. If the numbers Ni are effectively infinite the formulae simplify, and, for 
instance, instead of (25.11) we have 
niCcaiNi, . . . ■* . . . (25.13) 
the sample number varying with the standard deviation in the stratum concerned, as well 
as its number of members. 
25.11. If there is no information available at all about the variances aj the most 
reasonable course in applying (25.11) appears to be to suppose them all equal. In such 
a case, for large Ni we have 
7^ oc Ni9 ...... (25.14) 
or the sampling numbers are proportional to the population numbers. This is what we 
might expect on intuitive grounds. If the populations are infinite the w/s are equal, which 
again is in accordance with intuitive ideas. 
25.12. The above will serve as an illustration of the way in which theoretical 
requirements can influence the scope of an inquiry conducted among an existent population. By 
seeking for an estimator with minimum variance we have been led to expressions 
determining the allocation of sample numbers among the different strata—and incidentally, of 
course, we have derived expressions for the minimum variance, so that the maximum 
possible precision can be ascertained. The fact that some of our results depend on unknown 
constants suggests that in some circumstances it may be worth while conducting a 
preliminary or cc pilot " inquiry in order to estimate the unknowns and hence to improve the 
precision of the main inquiry which is to follow. The possibilities of such pilot surveys 
have yet to be explored, but the technique appears to merit serious investigation. 
25.13. In passing, we may mention one other topic of great practical importance on 
which theory can throw a good deal of light, that of optimum size of a sampling unit. In 
sampling a human population of a town, for instance, need we take individuals as our 
units ? It would be easier to sample households, or streets, or even whole districts ; but 
do we lose anything by this method, and if so, how much ? Furthermore, the grouping of 
individuals into units of larger size sometimes has a peculiar effect on correlations which 
may lead to erroneous conclusions, and a theoretical investigation may be required to 
safeguard against error. We shall not pursue the subject further here—the sampling problem 
would require a book in itself—but the reader who is interested may like to consult some 
of the papers referred to at the end of the chapter. 
The Design of Experiments 
25.14. For an existent population the flexibility of sampling technique is somewhat 
limited. We are given an aggregate of values, some of which are to be extracted for scrutiny, 
and no manipulation of the sampling can tell us more than exists, so to speak, already 
inscribed upon the population itself. Consequently the main line of endeavour in such 
cases lies in estimating with the greatest accuracy (which is largely a matter of choosing 
the right statistics and minimising sampling variability), or in ensuring that sufficient 
material is available to enable the requisite comparisons to be made with significance 
(which is largely a matter of sample size and selecting the most suitable tests of significance). 
Nothing can alter the population, and theory will, as a rule, only react upon the sampling 
process by some such method as has already been exemplified, e.g. in dictating that the 
THE DESIGN OF EXPERIMENTS 253 
sampling must be random, in stratifying the population before the sampling is carried out, 
and in deciding how limited resources can be expended to the best advantage. 
25.15. For hypothetical populations there are often wider possibilities, for the nature 
of the inquiry may itself determine which populations are to be studied, and the populations 
may, to a certain extent, be set up at will. For instance, if we are interested in an inquiry 
into the relationship between income and size of family in the United Kingdom, the 
population already exists and we cannot go outside it ; whereas if we wish to discuss the effect 
of a poison on bacterial growth or of a fertiliser on the yield of barley we can not only 
reproduce experimental data ad libitum but can arrange the inquiry so as to confine it to 
certain populations (e.g., by considering only a given type of bacterium in fixed nutritional 
circumstances or at fixed temperatures), or we may extend the domain of consideration as 
far as purely practical limitations will allow (e.g., by growing barley in new surroundings 
or in new climates). This is rather a pretentious way of saying that we may experiment 
in a domain which, within limits, can be assigned at will. The statistician has a much 
greater scope for ingenuity in the design of experiments than in the design of sampling 
inquiries on existent populations because of the greater degree of control over the population 
under examination. 
25.16. In the classical ideal experiment, only the factors under consideration were 
allowed to vary, other conditions being kept as constant as laboratory practice would allow 
—in investigations concerning the relation between resistance and current in an electric 
circuit, for instance, attempts would be made to keep factors such as temperature and 
external magnetic effects strictly constant. It would be recognized that there would be 
residual errors which would affect the exactitude of the results, but these would be 
measurable on certain assumptions. 
25.17. Statistical theory can, of course, deal with such cases, but it can also go farther 
and often wishes to do so. In the first place, it frankly admits the existence not only of 
experimental error (in the sense of aberration from a " true " value) but of the much wider 
type of variation which gives rise to frequency-distributions in practice. Instead of isolating 
particular factors for study, it may wish to give full play to the disturbances which arise 
in practice in order to investigate what happens in iC natural " conditions. For this reason, 
statistical experiments are often complex in the sense that a number of factors are allowed 
to vary simultaneously. 
Secondly, the admission of outside influences which together make up what is generally 
called experimental error implies that it should be possible to estimate the extent of such 
error from the data themselves. We wish to obtain, not the functional relations between 
variables which may only exist under artificial conditions, but the stochastic relations 
observed in practice. 
25.18. The effect of this on experimental design is that the hypothetical population 
we consider is often a rather general one. Taking the case of trials of a new variety of 
barley as an example, we should wish to compare its yields with those of other varieties 
in different soil conditions, with different manurial treatments, in different years (so as to 
get variations in climate), and so on. Furthermore, to obtain estimates of the error due 
to other factors we usually have to replicate the experiment. A great number of inter- 
comparisons fall to be made, and the process of design is essentially that of finding a form 
254 DESIGN OF SAMPLING INQUIRIES 
of experiment which will permit all these comparisons and yet save as much unnecessary 
labour as possible. 
Orthogonality 
25.19. To reduce the discussion to more concrete terms we will consider the testing 
of a new variety of barley. In order to study its behaviour under different soil conditions 
we will select a number of areas in which barley is grown and choose a block of ground in 
each. This will give us inter-soil comparisons. We will also arrange to carry the 
experiment on for a period of years, so that climatic variations may also be compared. The 
other factor in which we are interested is the response to certain manures, which we will 
take to be dung (D), potash (K)9 nitrogen (N), and phosphates (P). 
Consider any block at any one place in any year. We will decide on certain standard 
quantities of the four manures and assume that for any manure either a dressing of this 
standard amount is to be given, or it is to be withheld. This simplifies the experiment, 
for then every manure either is or is not applied, and our results can be classified by simple 
dichotomies. Of course more complicated experiments can be devised to allow for different 
quantities of fertiliser, but the simpler case will be sufficient for our purposes. 
We have then set up a population which can be classified according to six qualities, 
place, time, and the application of four manures. Our results are intended to show whether 
there is any variation in yield between these conditions and various combinations of them. 
Of course, it does not follow in deductive logic that if there is significant variation from year 
to year in the particular years chosen there will always be temporal or climatic variation ; 
and similarly, if there is significant variation from place to place it does not follow that 
other soil conditions which have not been tested will show a significant variation. To 
arrive at such conclusions we have to perform an ordinary generalisation by induction. 
What we shall say, if significant results appear, is that in the regions tested, or for the years 
tested, there were significant variations, and that it therefore appears likely that soil and 
climate exert a material effect on yield—and we shall maintain this with more or less 
confidence according as our experience is wider or narrower. This is the familiar inductive 
inference which forms the basis of all scientific inquiry. 
25.20. Within any one block we shall wish to study the effect of manorial treatments 
not only separately but in combination. We therefore divide the block into sixteen 
compartments and treat them, respectively, with no manure, I), /v, N, P, DK, DN, DP, KN, 
KP, NP, KNP, DNP, DKP, DKN and DKNP. Here every possible combination appears 
once and only once. To compare, for instance, the mean yields in the presence or absence 
of dung we add all the eight yields for plots on which no dung was spread and compare it 
with the sum of the other eight. All the necessary comparisons can be made. 
Data of this kind are said to be orthogonal. Each possibility arises an equal number of 
times. The reason for the use of the word is that such material is orthogonal in the sense 
we have considered in the analysis of variance. We saw in Chapters 23 and 24 that where 
cell-frequencies were equal the analysis was greatly simplified, and that under the 
customary hypotheses the estimates of means were independent. It is not, of course, absolutely 
necessary to have orthogonal data—in fact, we have shown in Chapter 24 how to deal with 
the non-orthogonal case ; but it is evidently a great convenience to be able to arrange 
for orthogonality, and no efficiency is lost by doing so. 
RANDOMISATION ' 255 
Replication 
25.21. If, as suggested above, we divide each block into 16 plots and treat each 
differently, the analysis of variance of any block will have 15 degrees of freedom; and if we 
cannot ignore any of the interactions there will be no residual variance due to " error ", 
that is to say we cannot estimate the reliability of our comparisons. All the 15 possible 
independent comparisons may be made, but we cannot decide whether differences are 
significant in the sense that they may be due to the other factors which we have agreed 
to allow to bear on the experiment, such as individual soil differences from plot to plot. 
If we are to estimate such " error " we must give the factors which produce it an 
opportunity of varying. This may be done by replicating the experiment, that is to say, by 
repeating it in the same form. For instance, suppose that we set up four blocks and divide 
each into 16 plots, applying our manurial treatments to each block. Then, assuming that 
there are no significant interactions between blocks and treatments (a matter which we 
can test by examining the interaction terms in the variance-analysis), we shall have 63 
degrees of freedom, of which. 15 are assignable to treatments and their interactions and the 
remaining 48 to a u residual " term, the latter providing an estimate of experimental error. 
We have exemplified this process in Chapter 23. 
Randomisatio?b 
25.22. Up to this point we have said nothing about the arrangement of our 16 plots 
within the block. Suppose we divide our block into plots of equal size. Is there any 
advantage in allocating the treatments systematically, or is it preferable to assign them 
at random 1 
We shall consider the relative merits of random and systematic arrangements in more 
detail below, but we can announce the general rule now : unless there is some good reason 
to the contrary, it is better to allot the treatments at random. Where possible, chance 
should be given full play. 
25.23. The justification for this rule in our present instance can be seen by reference 
to the section on randomised blocks in 23.41. We saw there that by randomising the 
allocation of plots we were able to preserve the ^-distribution and hence to validate our 
tests of significance, even where normality in the parent form was not assilmed. The 
process is essentially one of extending our hypothetical population. Instead of considering 
the observed yields as specimens of what might happen in repeated trials of the same variety 
of barley if the same manurial, treatments were applied to the same plots, we consider the 
possible yields in repeated trials if the manurial treatments were applied in all possible 
ways to different plots. Our experiment is systematic in the sense that we prescribe a 
different treatment for each plot; it is random to the extent that we allot the treatments 
to plots by chance. 
25.24. There is one source of possible confusion here which it is desirable to remove. 
In our agricultural example complications arise because of the physical contiguity of the 
plots, and we shall see below that it is often desirable to eliminate by special designs 
systematic fertility gradients in the soil. In other classes of experiment where we desire 
orthogonality, the members need not be subject to this kind of effect, and often are not. Reverting 
to the example of raw versus pasteurised milk which has already been mentioned, suppose 
we take a simplified case and wish to measure whether the two different milks have different 
5? 
256 DESIGN OF SAMPLING INQUIRIES 
effects on boys and girls. With a class of 40 children, 20 boys and 20 girls, we can proceed 
in several ways. It is obviously useless to give raw milk to all the boys and pasteurised 
milk to all the girls, for then we have no measure of the differential effect, if any, for either 
sex alone. We might toss up in each case and allot raw or pasteurised milk to each child 
by chance ; but this would probably make the data non-orthogonal. To attain 
orthogonality, we should allot 10 children to each of the four sub-groups BP, GP, BR, GR (where 
B = boy, G = girl, P = pasteurised, R = raw). We then have an analysis of variance— 
Degrees of freedom 
Between sexes . •• • . . ...J. 
Between milks ......... 1 
Residual (including interactions) ...... 37 
Total ........ 39 
This is analogous to a test of a cereal with two fertilisers and 10 replications. 
The question is, how should we allot the children to the four groups ? Their sex, of 
•course, is determined, but the nature of the milk they receive is at choice. It is here that 
the randomisation will help. The ten children of a specified sex who receive raw milk 
should be chosen at random from the 20 available. In this instance it might be thought 
that any method would do ; but it is best to avoid the risk of bias. If the children were 
chosen by the teacher he might tend to select the 10 bigger boys or the 10 brighter boys. 
If they were chosen alphabetically, we might get brothers and sisters automatically 
receiving the same treatment ; and so on. The randomisation process avoids all systematic 
effects of this kind and brings us a stage nearer to obtaining an unbiassed answer to our 
questions. 
Sensitivity of a Test 
25.25. In some cases, where the variate is discontinuous, the nature of the test of 
significance which we propose to apply may make a difference to the form of the experiment. 
If we are testing a certain hypothesis which can produce a specified number m of 
experimental results which are acceptable as conforming to the hypothesis, whereas other 
hypotheses produce a number n of other results, we clearly want to keep m as small as 
possible compared with n. The ideal case, of course, is that of the " crucial " experiment 
in which the hypothesis can only give one result and other hypotheses give a different 
result. The result then proves or disproves the truth of the hypothesis, and no test of 
significance arises. In statistical practice we do not as a general rule perform crucial 
experiments, but we can sometimes design an experiment so that it is more crucial, if the 
expression be allowed, than alternative methods. 
25.26. Consider, for instance, the case of a cashier who claims to be able to detect 
good money from false at a glance. To test this ability we spread ten coins before him, 
tell him that p are good, and ask him to point them out. What number of good coins p 
should we include among the ten ? 
If the cashier had no power of discrimination and there are p good coins, the 
probability that he would guess right by chance is 
LATIN SQUARES 257 
for the total number of ways of selecting p from 10 is the denominator of this fraction and 
only one of them is right. Now we want to choose p so as to minimise the probability of 
such an event, i.e. so as to maximise f ). This is clearly done when p = 5, so that we 
ought to have five good and five bad coins in the set. Any other number would increase 
the probability that he might be right by chance and hence decrease the sensitivity of the 
experiment. 
Latin Squares 
25.27. We now proceed to consider a different type of design, which has been freely 
applied in agriculture but may also be applied to other forms of inquiry. Suppose we 
have a variety of barley to test and five different treatments to apply. We will assume 
that replication has been considered necessary and will replicate five times, the same number 
as the treatments. We will then divide our block into 25 plots like a chessboard (though 
the plots may be rectangular and need not be exact squares, provided they are all the same 
size). Each row may be considered a replication of the five treatments, and this itself 
involves the appearance of each treatment once and only once in each row. Can we extend 
the arrangement and ensure that in addition the treatments will occur just once in each 
The answer is affirmative, as the following example shows :— 
ABODE 
B G A E D 
C E D A B (25.15) 
D A E B C 
E D B C A 
An arrangement of this kind is called a Ci Latin square ". It was studied extensively by 
Euler in the eighteenth century, though not of course from the statistical viewpoint. 
25.28. The advantage of this arrangement lies in the fact that it eliminates possible 
correlational effects due to fertility gradients in the soil or accidental circumstances which 
may exercise a " patchy " influence on the whole block. If we could be sure that there 
were no such influences at work, and that the soil was entirely homogeneous in the block, 
it would not matter where the treatments were placed ; but by imposing the restriction 
that no treatment appears more than once in the same row or column we remove at least 
horizontal and vertical gradients from our comparisons. Suppose in fact that there were 
gradients running across the block and down it. When we work out the mean yield of the 
treatment A we shall add together five values, one of each in the various rows and columns. 
Similarly for B, so that a comparison of A and B is not affected by the systematic influences, 
which work equally on both. 
It is not, of course, true that the Latin square arrangement eliminates every effect due 
to soil heterogeneity. There might be systematic effects running diagonally which might 
still remain. It is, however, clear that in removing the effects in two perpendicular 
directions we have substantially improved the comparison of mean yields as compared with 
a systematic arrangement. 
A..S. VOL. -LJL. 
S 
258 
DESIGN OF SAMPLING INQUIRIES 
25.29. The analysis of variance of a p X p Latin square may be carried out in the 
following'form :— 
Sum of squares d.f. 
Between rows . . . . p — 1 
Between columns . . . . p — 1 
Between treatments . . . p — 1 
Residual . . . . . (p — 1) (p — 2) 
Total . . . . p2 - 1 . . (25.16) 
and the four constituent sums are, on the hypothesis of homogeneity, distributed as v%2 
independently. Before proving this result we will consider an example. 
Example 25.1 (from Thomson, Brit. J. Educ. Psych., 1941, 11, 135 ; data by S. D, Nisbet). 
A set of children were divided into four equal groups and each group was given four 
lists of words to test spelling ability. Each list formed one of four different types of test 
which we denote by A, B, C, D. The arrangement of the experiment is shown in the 
following table, together with the total scores of the corresponding groups :— 
Groups of children 
Lists of 
words 
! 
; 
i i 
2 
3 
4 
Totals 
1 
jfx 
81 
D 
38 
G 
31 
B 
51 
207 
2 
B 
41 
A 
97 
D 
43 
C 
33 
214 
3 
C 
44 
B 
42 
A 
67 
D 
43 
196 
* 
D 
53 
ft 
49 
B 
36 
.4 
81 
219 
JL \) X x\ ,1jo 
219 
226 
"■ fi ft 
214 
836 
For instance, the first group of children had the first list of test A, the second of test 
D, and so on. No group had the same lists as another group, and each list was used exactly 
once. The scores (corresponding to yields in the agricultural case) were in fact the number 
of words spelled wrongly in a prior test but correctly in this test. 
The above table, of course, does not represent anything corresponding to the physical 
layout of an agricultural experiment, but it shows how a similar object can be secured to 
the avoidance of contiguous effects. Since it is possible that some relationship may exist 
between the lists of words and the tests (e.g. by accident one list might be particularly 
unsuitable for a test), we wish to ensure that not only will each group of children have 
each of the four tests, but that no list shall be given more than once and every list at least 
once. This is precisely what the Latin square accomplishes. The fact that the diagonal 
arrangement of the letters is systematic does not affect the present inquiry, though in an 
LATIN SQUARES 
259 
agricultural experiment a systematic diagonal fertility gradient might affect comparisons 
between treatments. 
An analysis of variance on the usual lines gives the following results :— 
Sum of Squares. 
d.f. 
Lists (rows) 
Groups (columns) 
Tests (treatments) 
Residual . 
rp 
Totals 
359-5 
74-5 
4626-5 
606-5 
3 
3 
3 
6 
119-83 
24-83 
1542-17 
101-08 
The differences between lists are evidently not-significant, from which we should conclude 
that they appear to be on a par so far as these tests are concerned. The quotient due to 
groups indicates that the children are more alike than chance would lead us to expect, but 
not significantly so, for the variance ratio 101-08/24-83 = 4-1, vx = 6, v& = 3, is not 
significant. On the other hand, the quotient due to tests is very significant, the ratio 
.1542-17/101*08 = 15*3, vx = 3, r2 = 6 being beyond the 1-per-cent. point. We conclude 
that there do exist differences between the tests. 
Oonstrudion of Latin Squares 
25.30. The numbers of possible Latin squares of order p is very large for high values 
of. p. There are, for example, 576 squares of order 4 ; 161,280 squares of order 5 ; 373,248,000 
of order (> and 61,428,210,278,400 of order 7. Up to this order they have been enumerated. 
Although many examples of squares of higher orders are known, the problem of enumeration 
for p > 8 awaits solution. Details and examples will be found in Fisher and Yates5 
Statistical Tables. 
By interchanging rows and columns the square can always be brought to a form in 
which the top row and left-hand column are in the order ABC, etc. It is then said to be 
a "" standard square ". For instance, there are four standard squares of the fourth order :— 
A B G D 
B A D G 
0 D B A 
I) G A B 
A B C D 
B C D A 
C D A B 
D A B C 
A B C D 
B D A C 
O Jx JJ JtS 
D G B A 
A B G D 
B A D C 
C D A B 
D C B A 
(25.17) 
From each of these, 144 (= 4 ! 3 !) squares may be derived by permuting all columns and 
all rows except the first. (There is no point in permuting the first row, because the result 
would be a repetition of squares already obtained with an interchange of the letters 
A . . /), not an. essentially different layout.) The total number of squares, as stated 
above, is therefore 4 x 144 = 576. 
ft is only necessary to specify the standard squares. To select a Latin square at 
random we choose a standard form at random and then permute rows and columns at 
random, the randomising process being most conveniently carried out by Sampling 
Numbers. For squares of order 8 or more, where the standard types have not been 
enumerated, we can only choose one of those which has, and hence select one at random from a 
restricted set of all possible squares. 
260 DESIGN OF SAMPLING INQUIRIES 
Analysis of Variance for Latin Squares 
25.31. We must now justify our assertion that the Latin square may be analysed 
in the form (25.16), and that the z-test applies to the variance ratios which arise in the 
analysis. 
For an ordinary two-way classification we have 
Z (xjk - x..)2 = 2 (xj. ~ O* + E (x-k - 02 + s Otyt - xj. - x.k + »..)*• 
JLI1ULS, II tJGy, IS the mean of rows and xc that of columns in the Latin square, we have, writing 
x for 
Z {xrc - x)2 = 27 (xr -x)2 + 2(xc- x)2 + 27 (xrc - xr - xc + x)2 . (25.18) 
and the three parts on the right are distributed independently as v%2 with # — 1,^ — 1 and 
(p — 1) (p —- 1) degrees of freedom respectively. 
Now 
27 (xrc — xr — xc + x)2 = 27 (xt — x)2 + 2 (xrc — xr — xc — xt + 2£)2 
+ 227 (^ — x) (xrc — xr — xc — xt + 2x) . . (25.19) 
where xt is the mean of treatments. 
Consider the cross-product term in (25.19). The summation takes place over all p2 
values in the Latin square. Let us confine our attention to the summation for some 
particular treatment. For this summation the factor Xt X lib constant. Summation for 
the other factor gives 
£ (xrc "~ xr ~~ xc ~~ xt + %x) = pxt — Exr — 27 xc — pxt + 2px • (25.20) 
and since one treatment occurs in each row and column, 
2xr =px\ (25>21) 
2, xc = px,j 
and hence the sum (25.20) vanishes. 
Thus the cross-product in (25.19) vanishes also and we have * 
27 (xrc — x)2 — 27 (xr — x)2 + 27 (xc — x)2 + 2 (xt — x)2 
+ 27 (xrc — xr — xc — xt + 2x)2. . . (25.22) 
This gives us the analysis of the sums of squares, and it only remains to show that the third 
term on the right in (25.22) is independent of the fourth. It will then follow that the four 
terms are distributed independently with p — 1, p — 1, p — 1 and (p ~~ 1) (p — 2) degrees 
of freedom. 
The required property of independence can be established directly, but it also follows 
from considerations of symmetry in the Latin square which have an interest of their own. 
We have regarded the square as composed of rows and columns, with treatments allotted 
in a certain way ; but by rearrangement we can equally well regard it as composed of rows 
and treatments with columns allocated in a certain way. For instance, if we take the 
first standard square in (25.17) we may write it:- 
F VVC llLOiJ 
Rows : 1 
2 
3 
4 
W JL HjC i-V . 
Treatment: 
ABC 
n ri /i 
Ul ^2 ^3 
6 2 Ct C4 
64 C3 C1 
^3 ^4 ^2 
D 
c\ 
where, for instance, treatment A occurs in row 1, column 1 (d), row 2, column 2 (C2), and 
ANALYSIS OF VARIANCE FOR LATIN SQUARES 261 
so on. This, of course, is not a physical layout, but that is immaterial for present purposes. 
It follows that since the sum of squares between columns is independent of the residual in 
(25.22), so also is that between treatments. 
The variance analysis then takes the form 
Sum of Squares. d.f. 
Rows . 
Columns . 
Treatments 
Residual . 
Totals 
25.32. The above form provides a homogeneity test of the usual kind. If the test 
proves significant of heterogeneity we may, in the usual way, consider the hypothesis that 
xrc = ar + bc + ct + Crc (25.24) 
where £rc is normally distributed about zero mean. We leave it to the reader to show, as 
in Chapter 23, that in such an event the residual mean square is an unbiassed estimate of 
the variance of £ with (p — 1) (p —• 2) degrees of freedom. 
25.33. As in the case of randomised blocks, it appears that under certain general 
conditions the ^-distribution is reproduced approximately for fixed values which are 
permuted in all the permissible ways consistent with the Latin square design. We omit an 
investigation into this result (for which see Welch, 1937) as the algebra is considerably 
more complicated than for randomised blocks. The result has been confirmed by a limited 
number of experiments. 
Graeco-Latin and Orthogonal Squares. 
25.34. If the two squares 
Jx Jlj O JtJ 
B A D 0 
C D A B 
D C B A 
are superposed we have the arrangement— 
AA BB 
BO AD 
OD DC 
DB CA 
in which every possible pair of letters (XY being regarded as different from YX) appears 
just once. Such a pair of squares is said to be orthogonal. The form (25.26) is sometimes 
written with Greek letters instead of the second Roman set ; hence the name of Graeco- 
Latin square. It is also possible to superpose a third factor which we will denote by the 
2j [Xrc 
JLi \pCip '— X) 
jLi \Xq Xi" 
S{xt - x)2 
xr — xc — %t + 2^)2 
jLj yXfC **■* / 
(p 
p 
p 
p 
- 1 
T2 
- 1 
- 1 
- I 
) (P - 
- 1 
2) 
. (25.23) 
A 
G 
D 
B 
B 
D 
C 
A 
0 
A 
B 
D 
D 
B 
A 
C 
. (25.25) 
CO 
DA 
AB 
BD 
DD 
CB 
JLJJtI. 
AC 
(25.26) 
262 
DESIGN OF SAMPLING INQUIRIES 
A oil 
B y 4 
0 62 
D/?3 
P/?2 
Ad 3 
Dy 1 
0 oc 4 
C y 3 
£>oc2 
Afi4: 
Bdl 
D<54 
0 j8l 
J3a3 
J. y 2 
numerals 1-4 in such a way that each combination of any pair of types occurs just 
once, e.g. 
(25.27) 
Complete sets of orthogonal squares (i.e. those in which there are p — 1 factors for a p x p 
square) are known for all prime p and for p = 4, 8 and 9. Curiously, there is no set for 
p = 6. Up to and including p = 7 they have been enumerated. 
We shall not enter here into the use of these squares in experimental design. They 
are generalisations of the Latin square in which, by suitable arrangements, several factors 
can be tried out simultaneously, so that all possible combinations of pairs occur an equal 
number of times. 
Confounding 
25.35. It will be evident that if we wish to consider in full a classification according 
to several variates, particularly with replications, the number of individual members in 
the sample may be very large. For instance, if we. wish to test a variety of barley with 
three different applications of four types of fertiliser, there must be 81 yields even without 
replication, if we want to make all the comparisons possible. Physical considerations may 
make a layout of an experiment on such a scale impossible. The difficulty is possibly more 
serious in experiments on expensive animals such as cows. 
Where economy in the size of sample is a very material factor we may be able to reduce 
the sample at the expense of sacrificing some of the less important comparisons. For 
example, to consider once again the case of barley and the effect of fertilisers : we shall 
undoubtedly wish to compare yields of D and not-D, K and not-K, P and not-P, N and 
not-iV". We may also wish to compare first-order interactions of the type DK and not-D, K. 
But it is quite possible that interactions of higher order, such as the effect of dung in the 
presence of two other fertilisers, are negligible. Where we are prepared to assume that this 
is so, on the basis of prior evidence or otherwise, we can dispense with certain information 
and still make the comparisons we wish while retaining properties of orthogonality. 
25.36. Consider, as an illustration, an experiment with three fertilisers, each of which 
is applied or not applied, say N, P and if, and four replications. In the ordinary way 
there would be 32 plots and we should have an analysis of variance as follows, assuming 
that block-treatment interactions may be regarded as part of the residual :— 
Sum of squares. d.f. 
Blocks ...... 3 
AT 
P 
K 
NP 
NK 
PK 
NPK 
Residual 
1 
1 
1 
1 
1 
1 
1 
21 
Total 31 
CONFOUNDING 
263 
Now suppose that we divide our main blocks into two sub-blocks, the first containing 
the treatments 
0 (None), NP, NK, PK, (25.28) 
and the second the treatments 
N, P, K, NPK (25.29) 
We may then analyse the variance as follows, regarding the sub-blocks as blocks of four 
plots each :— 
Sum of squares d.f. 
Blocks ...... 7 
P 
A 
NP . 
NK . 
PK . 
Residual 
Total 
1 
1 
1 
1 
1 
1 
18 
31 
In fact, if we wish to compare the yields with N and those without N, i.e. 
with 
N + NPK + NP + NK 
0 +PK + P + K, 
it will be seen that we add two members from (25.28) and two from (25.29), so the difference 
is not affected by block differences ; and similarly for the other comparisons. Such a 
design is said to be balanced, and the interaction NKP is confounded with block-differences, 
since in the eight blocks it cannot now be isolated from block effects. The advantage of 
the second design over the first is that, without losing anything appreciable in comparisons 
between treatments, we have gained a good deal in the assessment of block effects ; for the 
residual has only declined from 21 to 18 d.f. whereas the sum of squares between blocks 
has increased from 3 to 7 d.f. 
25.37. The ideas of orthogonality, randomisation, balance and confounding have 
been developed to an advanced degree and with great ingenuity, particularly by Fisher 
and Yates. The slight sketch we have given of the methods in this chapter is intended to 
be no more than illustrative of the way in which the theory of experimental design is capable 
of development, at least in certain fields, and the manner in which efficiency may be imported 
into a practical inquiry by a due regard to theoretical requirements of the design. For a 
comprehensive account of this branch of the subject the reader should consult Fisher's 
Statistical Methods and Design of Experiments, Yates (19376), and a useful introductory 
account by Goulden (1939). At this point we leave these particular topics and return to 
certain general matters. 
Design and Randomisation 
25.38. Whenever an inference is to be made, and particularly where hypothetical 
populations are concerned, the reader will find it useful to ask himself what precisely is the 
population under consideration. We can illustrate the point very usefully by discussing 
264 DESIGN OF SAMPLING INQUIRIES 
a subject on which there has recently been difference of authoritative opinion—that of 
occasional conflict between the requirements of balancing and randomisation. 
25.39. Consider in the first place the testing of a cereal under two treatments, denoted 
by A and B ; and to simplify matters as much as possible, suppose we are to sow eight 
plots in a straight line. In what order shall we allot the treatments ? 
If the plots are not too large so that the row covers a big area, it is quite possible that 
there may be a trend of fertility in the soil itself which will affect yields differentially and 
hence interfere with comparisons which we might make. Suppose that we do wish to 
guard against a fertility gradient so far as possible. We might then decide on one of the 
" balanced " arrangements : 
AABBBBAA (25.30) 
ABBAABBA (25.31) 
ABABBABA (25.32) 
As will be easily seen, if there is a linear gradient in fertility along the row the means of 
A and B treatments respectively will be affected to the same extent and hence their 
difference unaffected. For instance, consider (25.30) and suppose the linear gradient is 
represented by an additive factor q + kp, k = 1 . . . 8. On the hypothesis that the 
remaining effect consists of a constant a for J.-treatments with a normal residual £, and similarly 
for jB, the yields are 
A -treatments : q + p + a + £l5 q + 2p + a + £23 <1 + 7? + a + £?> q + &p + « + £« 
JS-treatments : q + Zp 4- b + f s, q + 4p + b + £4, q + 5p + b + £5, q + ®p + b -f f c 
with means 
I (4tq + 18p) + a + I (fx + £2 + £7 + f8) 
I (4q + ISp) + b + I (f8 + £4 + £5 + £e) 
respectively. The differences of these two are independent of q and p. 
25.40. The alternative procedure in allotting treatments would be to distribute 
them at random. Such balanced arrangements as (25.30)-(25.32) might then arise by 
chance. But we might also get such an arrangement as 
AAAABBBB (25.33) 
What are we to do in such circumstances ? If we reject this arrangement we are rejecting 
the random allocation of treatments in favour of systematisation. If we accept it we 
know quite well that a fertility gradient, if it exists, will invalidate the inquiry. 
The reader will no doubt agree that, if other things are equal, the balanced 
arrangement is better than the arrangement (25.33). What we have to examine is whether other 
things are equal ; in short, whether in rejecting randomisation we have lost anything 
useful in the testing of significance. 
25.41. Consider a rather more general case in which an experimental area is laid 
out in p blocks of q treatments each. If the subscript j refers to blocks and k to 
treatments, we have the usual analysis with sum of squares between blocks (p — 1 d.f.), between 
treatments (q — 1 d.f.), and residual ( (p — 1) (q — 1) d.f.). 
Now we have seen that if the individual plot-yield can be regarded as a block effect 
plus a treatment effect plus a normal residual with constant variance from plot to plot, 
DESIGN AND RANDOMISATION 265 
the significance of treatment effects can be judged from the s-test in the usual way by 
comparing sum of squares between treatments with the residual sum of squares. This 
is true whether treatments are allocated at random or not. 
But suppose we wish to adopt the alternative viewpoint of 23.41 and make the 
inference in the set of values obtained by permuting the observed values. These permutations 
will not affect the block means or the total mean, and hence the sum of squares between 
blocks remains constant. The remaining part of the analysis may be written— 
Treatment 
Residual . 
Totals 
Rather remarkably, the s-test holds for the ratio 
fli (P - 1) (g - 1) 
q — I JS2 
provided that treatments are allocated at random, independently of the distribution of 
residual effects in individual plots. 
25.42. Consider, then, the population of values, (q !) p~~1 in number, obtained by 
permuting the observed values. The total sum of squares Ss in (25.34) is the same for all 
members. Consequently if Sx is too great, S2 must be too small and vice-versa ; and in 
s general, if we confine ourselves to certain layouts and reject others, all the possible values 
of St cannot appear. It is this fact which has been seized on by advocates of 
randomisation. They point out that for balanced layouts JSX tends to be smaller than for random 
layouts (a conclusion supported by experiment) ; consequently that the test of significance 
is invalidated and the estimate of error S2 too big. The difference between the two modes 
of thought may be expressed briefly in this way : with balanced layouts the real error is 
reduced but the estimate of error is too large, so that the significance of the result is more 
in doubt; whereas with random layouts the estimate of error is exact but the error itself 
may be larger. The question is whether one prefers to be nearer the truth without knowing 
how near, or farther from the truth with a knowledge of the limits of error. 
25.43. For details of the controversy on this topic the reader may consult the papers 
referred to at the end of the chapter. It brings into prominence an important question 
of inference which can only be decided by the experimenter himself. If he chooses to 
regard any act of experimentation as one of a large population of such acts, to be carried 
out by himself or other workers, he may prefer randomisation in all circumstances, 
notwithstanding that every now and again he will hit by chance on a design which he knows 
is likely to give misleading results. But if he cannot take this very detached attitude (and 
most experimenters, being human, would think it poor compensation that their own errors 
are balanced by the better luck of other people) then he will prefer to design a balanced 
layout, even if the exactitude of his tests of significance is impaired. 
Sum of Squares. 
Si 
S9 
x..)z 
Xj. ~~ x.k + ^..)2 
£3 = 2 (%jk 
ax.)* 
(p 
d.f. 
q - 1 
-1) to - 
p to - 1) 
i) 
. (25.34) 
266 . DESIGN OF SAMPLING INQUIRIES 
25,44, We must, however, not leave the reader with the impression that the 
desiderata of both schools of thought are totally incompatible. It frequently happens that 
one can select a design which is both balanced and random. The Latin square is a good 
example. By imposing the restriction that a treatment must not appear more than once 
in a row or column we remove to some extent the interference of fertility gradients ; by 
requiring that it shall appear just once we balance the design ; and by leaving the rest 
of the layout to be determined by a random selection from all possible Latin squares of 
that order we randomise so as to reproduce the distribution of the variance ratio in the 
required form, thus, as " Student" remarked, " conforming to all the principles of allowed 
witchcraft ". 
REFERENCES 
A classical case of how an inquiry can be spoilt by poor design is the Lanarkshire Milk 
Investigation, for which see " Student " (1931c) and E. M. Elderton (1933). This case 
will repay study. On some theoretical problems arising from the sampling of existent 
populations see Bowley (1925), Jensen (1925), Sukhatme (1935), Neyman (19336, 1934, 
1938a, 1939a, 19416), Olds (1939, 1940), and Frankel and Stock (1939). The war has 
accentuated many of the points remaining unsolved, and there is much of general interest 
in recent issues of the Journal of the American Statistical Association and the Annals of 
Mathematical Statistics. For some work on the " pilot " sampling technique see Sukhatme 
(1935) and C. Bose (1943). 
Reference has been made in the text to Fisher's Desig?i of Experiments, Yates5 
Principles of Orthogonality and Confounding, and Goulden's Methods of Statistical Analysis. 
For the problem of size of sampling units see the papers by Neyman referred to above, 
particularly 1934, and for its effect on correlation analysis see an interesting appendix >> 
in Wold's Analysis of Stationary Time Series. 
For the controversy on balance versus randomisation see " Student " (1938), Barbacki 
and Fisher (1936), E. S. Pearson (19376, 1938), and Jeffreys (1939e). 
25.1. A population is given by specifying the frequencies in comparatively narrow 
ranges of one variate, the frequency in the ith range being Ni and ranges being of equal 
width. Show that if the population frequencies are large, the best estimator of the mean 
of a second variate which is linearly related to the first (in the sense of the unbiassed estimator 
of minimum variance) in a sample obtained by taking n,t members from the ith range is 
given when ni is proportional to N{. 
25.2. Extend the result of the previous exercise to the case where ranges are of 
unequal width. 
If the number of farms in England and Wales is known in the acreage ranges 0-49, 
50-99, 100-199, 200-499, 500 and over, what sampling proportions would you take in the 
various ranges to estimate the total acreage under wheat ? 
EXERCISES 267 
25.3. If a variate $ can be regarded as the sum of a systematic component f (x) and 
an uncorrelated random component sx and rj similarly as tj (x) + e2, and if the random 
components are uncorrelated with each other, show that 
r (£ v) _ cov{§(x), rj(x)} ^^ 
{ (var | (x) + var e1) (var rj (x) + var e2j} *" 
Hence, if a population is divided into strata the correlation between | and r\ for these strata 
will, in general, be less than that obtained by combining strata to obtain larger units; 
and as the strata are further subdivided the correlation between £ and r\ tends to zero. 
(Spearman, 1907, Am. J. Psych., 18; Wold, 1938a.) 
25.4. Illustrate the effect of the foregoing exercise by calculating the correlation 
coefficients for the data of Table 14.4 (vol. I, p. 333), (a) by adding the variates in pairs 
and so obtaining 24 values ; (b) by repeating the operation and obtaining 12 values ; 
and (c) by repeating the operation and so obtaining 6 values. 
25.5. (MarkofFs theorem.) Consider a sample of n independent values X ^ • • . X a) , 
xi being drawn from a population IJi with mean /jLt and variance of. Suppose we have 
a function 0 defined by 
s 
where the f/s are known and the parameters p3- depend on the /t's according to the equation 
8 
the rc's also being known. Then an unbiassed estimator of 0, say t, with minimum variance 
mav be written— 
n 
£ 7 /I. 'J[,-. 
Show that the function t is given by substituting for the p's in the expression for 6 the 
functions q given by minimising 
-t } 
2^ (a« ?*) 
with regard to the q's considered as independent variables. 
Show further that if this minimum value is $0 the estimated variance of t is 
S° 2 [X\ a?). 
n —- s 
25.6. In a feeding experiment there are given five different foods, each of which is 
available in four grades. It is desired to feed each animal with one grade of each food, 
but only one, so that a comparison may be made of the effect of the different grades of any 
particular food. Use the Graeco-Latin square to show how the feeding can be carried 
out. 
268 DESIGN OF SAMPLING INQUIRIES 
25.7. A water diviner is to be taken to ten spots and asked to say whether water 
is present below the surface. It is decided to choose five spots where water is known for 
certain to exist and five where it is known not to exist. The order in which the spots are 
to be presented is determined by spinning a coin, heads denoting water and tails not-water. 
The spinning of the coin results in the first five trials giving heads. Would you 
accept this result or spin again ? 
25.8. Show that a Latin square may be regarded as a uliree~ way classification in 
which p2 members are not zero, but pz — p2 members vanish. Derive the analysis of 
variance for the Latin square from this approach and generalise it to the Graeco-Latin 
square. 
CHAPTER 26 
GENERAL THEORY OF SIGNIFICANCE-TESTS—(1) 
Hypotheses to be Considered 
26.1. The kind of hypothesis which we test in statistics is more restricted than the 
general scientific hypothesis. It is a scientific hypothesis that every particle of matter 
in the universe attracts every other particle, or that Homer was blind; but these are not 
hypotheses such as arise for testing from the statistical viewpoint. A review of the various 
tests which have been introduced earlier in this book indicates that the great majority 
specify something about a population. Some merely assert a general fact such as " the 
population is continuous " or " the population is rectangular ". Others are more definite, 
as for instance " the population is normal and has a mean /u0 " ; and again others are less 
definite in one direction and more definite in another, e.g. " the population has unit 
variance ". It is also usually a part of the hypothesis that the sample from which the inference 
is being made was obtained by a random process. 
26.2. Suppose we have a set of random variables xx . . . xn. In the sample space 
W of n dimensions the sample-point whose co-ordinates are ffi\ ... x^ determines a point 
jE7, say, with a distribution function which we may write as P (W). If w is any region in 
W, we may derive the probability that E falls in w, say P (E & w). Then we shall say that 
any hypothesis concerning the law P (E ew) is a statistical hypothesis. If it determines 
the law completely we shall call it simple. In the contrary case it is said to be composite. 
For instance, in testing the significance of the mean of a sample of n, it is a statistical 
hypothesis that the parent is normal. This is composite, as also is the hypothesis that 
the parent is normal with mean p or the hypothesis that the parent is normal with variance 
<t2. The hypothesis that the parent is normal with mean ju and variance c2 is simple because 
then the parent is fully determined. 
Example 26.1 
In sampling from a population dichotomised into classes possessing the attributes 
A or not-A, say in proportion m and % (== 1 — w), the sampling distribution is the binomial 
(X + w)n- ^his 'm completely determined by the value of w, and hence a hypothesis as 
to the value of m is simple. Such, for instance, would be the hypothesis that male and 
female births occur in equal proportions. Similarly, in a multiple classification with 
proportions wlf w2, . . . ws, a simple hypothesis would specify values for all the ro's ; if only 
one were specified and $ were greater than two the hypothesis would be composite. 
In sampling from a bivariate normal population characterised by two means, two 
variances and a correlation, a hypothesis about any one parameter would be composite, 
and similarly for a hypothesis concerning two, three or four parameters. Only if all five 
were specified in addition to the normality of the parent would the hypothesis be simple ; 
and this notwithstanding the fact that the sampling distribution of the means is 
independent of the other three parameters, and that of the correlation coefficient independent 
of the other four. 
269 
270 GENERAL THEOEY OF SIGNIFICANCE-TESTS 
26.3. A hypothesis which determines the law P(Esw) completely except for v 
parameters is sometimes said to have v degrees of freedom. Such a hypothesis may be 
regarded as an aggregate of simple hypotheses. For instance, the hypothesis that a 
population is normal with mean // is the aggregate, for all c2, of hypotheses that it is normal with 
mean ju and variance <j2. 
26.4. The kind of argument we have used in testing hypotheses, for both large and 
small samples, is of this character: assuming that the hypothesis.is true, wre can, with 
any assigned probability a, find a region w0 in the sample space W such that the probability 
of E falling in W-w0 is a. We call W-w0 the region of acceptance and the complementary 
domain wQ the critical region. (This is the nomenclature of Chapter 19.) If our observed 
E falls in w0 we reject the hypothesis ; if not we accept it. As a rule, in practical cases, 
our regions w0 are determined by the values of some statistic such as x in testing the mean. 
Errors of Edrst and Second Kind 
26.5. In general, as we saw in Chapter 19, there are many possible regions of 
acceptance for any given hypothesis and any given probability level a. For all of them we shall 
err in proportion 1 — a of the cases in the long run by rejecting the hypothesis if E falls 
in the critical region—provided that the hypothesis is true. But what about the case when 
it is not true ? We cannot ignore this case, for its possible existence is the very reason for 
carrying out the test. It is of no use whatever to know merely what the test will do when 
the hypothesis is true without regard to its behaviour in the contrary case ; for if we are 
to consider only the events which happen when the hypothesis is true we have no right to 
use a test based on that assumption to reject it. 
By having regard to the behaviour of the test when the hypothesis is not true we are 
able to lay down criteria for choosing among the various tests obeying the rule 
P {E £ w0 | H0} = 1 — oc, (26.1) 
where H0 is the hypothesis. In fact we shall seek for the test which, while obeying (26.1), 
minimises the risk of accepting H0 when an alternative hypothesis Hx is true and HG 
accordingly is false. That is to say, we shall endeavour to find w0 such that, in addition, to (26.1), 
we also have 
1 — P {E e w0 I Hx] = minimum. .... (26.2) 
26.6. From a slightly different viewpoint we may say that there are two possible 
errors in judging a statistical hypothesis : 
(a) We may reject it when we ought to accept it, that is, when it is true. 
(b) We may accept it when we ought to reject it, that is, when it is false. 
These are known as errors of the first and second kind respectively. The error of the 
first kind we can control exactly by setting up the proper region of acceptance determined 
by oc. Errors of the second kind cannot be controlled in this way, but we can sometimes 
calculate their probabilities, and in any case can try to reduce them to a minimum, .jluis 
is the fundamental idea, first given explicit expression by Neyman and E. S. Pearson, 
which determines most of the work in the present and succeeding chapters. 
26.7. The possibility of finding regions of acceptance obeying (26.2) clearly depends 
on a precise specification of what alternative hypotheses are under consideration. We 
had better emphasise the importance of this point. It is customary to speak, and even, 
ERRORS OF FIRST AND SECOND KIND 
271 
in a loose kind of way, to think of testing a hypothesis without reference to alternatives. 
To take the case of testing for normality, we often say that the hypothesis under test is 
that the population is normal without specifying what other form it might have. The 
reader may say that the alternative he has in mind is merely the negation of the hypothesis, 
namely that the population is not normal. But if so he will find it very difficult—in my 
own view impossible—to justify any of his tests on a logical basis. He will calculate certain 
statistics and accept the hypothesis if their values are consonant with the normal values ; 
but it will always be possible to find other populations for which the observed values are 
even closer to expectation. If agreement between theoretical and observed values is the 
criterion he should reject normality in favour of these alternative hypotheses. It is not 
until he specifies his alternatives and considers errors of the second kind that some firm 
foundation for intuitive processes begins to appear. 
26.8. Perhaps it may help to clarify the fundamental concepts of the present approach 
x*k 
vor 
• ••■"•-:-£ 
Vm. 2(5.1 (see text), 
if we consider a simple illustration where the hypothesis under test HQ is simple and there 
is only one alternative IIl which is also simple. In Fig. 26.1 we show diagrammatically the 
scatter of sample-points which would arise in samples of two, xt and #2, the cluster on the 
right being that due to IIQ and the one on the left to IIx. In practice, of course, the sampling 
distributions are more usually continuous, but the dots will indicate roughly the condensation 
of sample density round central values. 
In determining the critical region we have to find an area in the (xlt x2) plane such that 
its " content " is 1 — a. Two possible areas are shown, w0 being the area to the left of 
the line PQ, and w'{) the area between the lines AB and BO. In either case the proportion 
in the critical regions of the frequency on hypothesis II0 is 1 —■ a, and if we reject H0 
whenever the sample-point falls in w0 (and similarly for w'{)) we shall commit an error of the first 
kind in proportion 1 — oc of the cases in the long run. 
Consider errors of the second kind. By using the region w0 we should reject H0—and 
;272 GENERAL THEORY OF SIGNIFICANCE-TESTS 
therefore accept Hx—every time the sample-point arose from Hti that is to say in practically 
all the cases where Hx was true, since nearly all the sample-points arising from Hx lie in 
w0. Errors of the second kind are therefore very rare. On the other hand, if we were to 
use w'0 we should accept H0 every time a sample-point arose from Hx but did not fall between 
the lines AB and BC, that is to say fairly frequently. Clearly w0 is the better critical 
region and has a much smaller error of the second kind than w0. 
26.9. It is to be noted that the argument does not depend on the relative frequencies 
of occurrence of the hypotheses HQ and Hx. This is generally true. There is no concealed 
form of Bayes' postulate in this approach. 
26.10. When there are n variates and p unknown parameters the geometrical 
representation can be extended by imagining a sample-space W of n dimensions adjoined to 
a parameter space of p dimensions. We cannot draw a picture of such a case on a two- 
dimensional sheet of paper, but the geometrical imagery and terminology of the method 
are frequently useful. A graphical illustration of a two-dimensional sample-space and 
a one-dimensional parameter space has already been given in Fig. 19.3. 
The Power Function 
26.11. If for a simple hypothesis H0, (26.1) is true we define 
P{Eew0\H1} ^PiH^Wo) . . . . (26.3) 
.as the power of the critical region w0 with respect to Hx. Clearly the power is greatest 
when the probability of an error of the second kind is least. 
In the expression on the left of (26.3) we regard the probability that E falls in w0 as 
dependent on Hl9 the hypothesis alternative to HQ. In the expression on the right we have 
regard to the power of the test for Hx as dependent on wQ. 
If there exists a particular region wQ with greater power than any other region obeying 
(26.1) we shall say that it.is the best critical region, and the test based on it will be called 
the most powerful test. 
26.12. We proceed to consider in turn the following cases :— 
(a) H0 simple ; one alternative H1 which is simple. 
(b) H0 simple ; an alternative H1 which is composite but can be regarded as an aggregate 
of simple alternatives. 
(c) H0 and H1 composite but expressible as aggregates of simple hypotheses. 
Simple Hypotheses: One Simple Alternative 
26.13. Suppose the parent population is continuous, so that the simultaneous 
distribution of the n sample values xx . . . xn is continuous ; and let the frequency functions 
of the sample values on hypotheses H0 and Hx be p0 (xx . . . xn)B,ndjjl(xl . . . xn) 
respectively. 'Write dx for the element dxt . . . dxn. Then we have 
\ p0 dx == 1 — oc . . . . . (26.4) 
.and wish to maximise, for variations in the domain w0, the integral 
\ Pi dx (26.5) 
SIMPLE HYPOTHESES: 'ONE SIMPLE ALTERNATIVE 273 
This is a problem in the Calculus of Variations and is equivalent to maximising 
unconditionally the integral 
Pi ~ tPq ) dx, . • • . . (26.6) 
I 
or, what is the same thing, to minimising 
(Po — kpi) dx, ..... (26.7) 
Wc 
where h is a constant to be determined by (26.4). 
It is known that the condition for a stationary value of (26.7) is that, on the 
boundary of w0, 
p0 — kpx = 0. . . . . . (26.8) 
If the solution is a minimum we have, inside w0, 
Po < kpx . ...... (26.9) 
and outside iv0, 
Po > %?i- ...... (26.10) 
This solution to the problem is fairly obvious on general grounds. If U is a function which 
is sometimes positive and sometimes negative, with a line of demarcation where it is zero 
(as must exist in virtue of continuity), we clearly minimise 1 U dx by taking into the region 
w0 all the points for which U is negative and no more. This gives us (26.9) and (26.10), 
and the boundary of w0 is the locus for which U vanishes. By convention we regard the 
boundary as included in w0, which accounts for the equality in (26.9) and its absence in 
(26.10). * 
26.14. The conditions expressed by (26.8), (26.9) and (26.10) are sufficient as well 
as necessary. For let wx be any other region for which 
j 
p0 dx — 1 — a. 
If wQ and wx have a common part denote it by w0l. Then 
p0 dx = 1 — a — I p0dx 
Wo —Wax J Wot 
— Po dx 
J Wx — Wqi 
and hence, from (26.9) 
k pi dx > I p0 dx = I p0dx 
JWa — Wot Jwn—W0i JWi — Wol 
> k 
px dx. 
Wi~Wox 
Adding to both sides k px dx, we have 
Pi dx > k I ptdx, ..... (26.11) 
and hence, for positive k, the power of wx is less than that of w0 and the latter is the best 
critical region. 
a.s.—vol. n. x 
274 
GENERAL THEORY OE -SIGNIFICANCE-TESTS 
Both in this section and implicitly in the last we have required h to be positive. That 
it must be so if w0 is to exist emerges from (26.8), for p0 and px are essentially not negative, 
and if k were negative no solution for real variate-values would exist. 
Example 26.2 
Consider the normal population 
1 
dF = 
exp { — \ (x -— ju)2} dx, 
00 < X < 00. 
V(2w) 
Let the hypothesis H0 be that /a = a0, and the alternative that /u — ax. We have- 
1 
n 
P« = n eXP 
(2tt)2 
l]P(xj -«o)2 y 
i=l 
We can conveniently express this in terms of the sample mean x and the sample variance 
s2, obtaining for the density function 
1 
Po = —- exp 
(2n)2 
n 
- { (x - a0)2 + s2} 
A similar expression is found for px and thus, for the boundaries of the best critical region, 
we have 
1 =Pi 
k p0 
tJ-X. U 
exp 
n 
- { (x - ax)2 — (£ — a0)2} 
n 
— - (a0 — ax)(2f — a0 — ax) 
This yields for the critical region 
or 
(a0 — ax) (2a$ — a0 — ax) < -log /c, 
n 
(a0 — ax)x <\ (ag - a{) + - log £ = (a0 — ax) £0, say. 
?z 
If a! < a0 the region is then defined by 
x < x 
Oj 
but if ax > a0 it is defined by 
The reader should compare the two cases on a diagram similar to that of Fig. 26.1. 
Example 26.3 
Consider again the normal population when the mean is known, say zero, but the 
variance unknown, e.g.— 
SIMPLE HYPOTHESES: FAMILIES OF SIMPLE ALTERNATIVES 275 
We now find, for hypotheses a = cr0 and a — ax 
h=* = (*j\" exp{~- % (S.+..)^-i2 
P! \a0J r\ 2 V^o of 
which yields, for the best critical region, 
(<g2 + 52)(<72 ___ a2) < ^M log f /, / *o V 
< v (o"o — crf), say. 
Thus our critical regions are defined by 
m2 = x2 + s2 <v if a1! < cr0 
m2 = x2 -f #2 > v if a"! > cr0 
The best critical regions in the space W are thus bounded by hyperspheres centred at the 
origin. Whether we take the space inside or the space outside a particular hypersphere 
as the critical region depends on the alternative hypothesis. The probabilities concerned 
can be evaluated directly without evaluating the constants k and v. In fact, the proba- 
bility of exceeding a given value of ~- = ——Tr—— = yg is obtainable from the #2-dis- 
o*0 era 
tribution with n degrees of freedom, and hence the relation between v and oc can be 
ascertained from the #2-integral. 
In this particular case we may find without difficulty the power of an alternative test 
(n — 1) v' 
which would suggest itself on intuitive grounds. Suppose we find -2 = %[ from 
aQ 
the ^-distribution corresponding to n — 1 degrees of freedom and probability level a, 
and use, instead of the hyperspheres centred at the origin, those centred at the sample mean 
S" < V , 6'2 > V . 
Suppose that the alternative Hx is that a] = 1-1 of>. In testing H0 for the alternative 
Ox > a0 we should, for the test based on v, find xf) and accept aQ if 
For instance, with n = 5, 1 — a -- 0-01 we find xi ™ 15-086. The probability of an error 
of the second kind is 
•x.71-1 
r p,.vii 
Pi dx = dF (x2), 
J w;0 J 0 
i.e. is obtained from the //-integral with argument ■—■- = 13-71, giving /S (Hx \ wQ) = 0-018. 
On the other hand, had we used %\ instead of %\ we should have entered the table with 
four degrees of freedom, giving 13-277. Divided by 1-1 this gives 12-07, resulting in a 
probability of rather less than 0-017. This is the power of the second test and is lower 
than that of the first test, as of course it must be since the latter has maximum power. 
Simple Hypotheses : Families of Simple Alternatives 
26.15. Consider now the case where HQ is simple but Hx is composite and consists 
of a family of simple alternatives. The most frequently occurring case is the one in which 
we have a class of simple hypotheses Q of which H0 is one and H1 comprises the remainder; 
for example, the hypothesis H0 may be that a mean has some value /i0 and the hypothesis 
Hx that it has some other value unspecified. 
i 
276 GENERAL THEORY OF SIGNIFICANCE-TESTS 
For each of these other values we may apply the foregoing results and find for each oc 
corresponding to any particular member of Hl3 say Ht, a best critical region wt. But this 
region in general will vary from one Ht to another. We obviously cannot determine a 
different region for all the unspecified possibilities and are therefore led to inquire whether 
there exists, among the family of best critical regions wt, one which is the best for all of 
them. Such a region is called the Uniformly Most Powerful and the test based on it the 
Uniformly Most Powerful test, conveniently shortened to U.M.P. test. 
26.16. Unfortunately, as we shall find below, the U.M.P. test does not usually 
exist unless we restrict our family Q in certain ways. Consider, for instance, the case 
dealt with in Example 26.2. We found there that for ax < aQ the best critical region for 
a simple alternative was defined by 
Now the boundaries of the regions determined by x = constant do not depend on ax and 
can "be found directly from the sampling distribution of x when the probability level 1 — a 
is given. Consequently the regions defined by x < x0 are the same for all ax < a0 and hence 
the test is U.M.P. for the class of hypotheses that ax < aQ. It is difficult to see how a better 
test could be devised, for, whatever ax subject to ax < a0, the test controls errors of the first 
kind and minimises those of the second. 
However, if ax > a0 the best critical regions are defined by x > x0. Here again, if 
our class Q is confined to the values of ax greater than a0 the test is U.M.P. But if ax can 
be either greater or less than a0 no U.M.P. test is possible. The reader will easily verify 
for himself that the same is true for the test considered in Example 26.3. 
26.17. We now show formally that for a simple hypothesis depending on QQ—the 
value taken by the parameter 0 defining a family of alternatives—no U.M.P. test exists 
for both positive and negative values of 6 — 60 if the frequency function p (E | 6) is 
continuous, has everywhere a continuous derivative with respect to 0 which does not vanish 
identically, and admits of differentiation under the sign of integration over W. 
Suppose that such a test does exist. Then for any 0 we have, inside w0 
p0 < 7cp, 
which we may write 
p(E\0) >h(d)p0(E\d0) (26.12) 
Likewise, for any point E on the boundary of w0 we have 
p(E\d) ^h(6)pQ(E\e0) (26.13) 
By hypothesis p is differentiate in 0 and hence so is h. Moreover, as 0 ~> 0o, h (6) -> 1. 
Hence if 
a == e - 90 
and primes denote differentiation with respect to Q, we have 
h (d) = 1 + A [h\+qA 0 < q < 1 
a p(E\6) 
= 1 + A 
ddp0(E\e0)J 
d<>+qA 
X+ ,i,fl,li>'(llfl)W •... (26.14) 
p0 (Mi | UQ) 
BEST CRITICAL REGIONS AND LIKELIHOOD 277 
Further we have 
P (E | 6) = p0 (E | do) + A [pf (E | d)\+rA 0 < r < I. . . (26.15) 
Substituting in (26.12) from (26.14) and (26.15), we find 
A { [p' (j»| d))6o+rA _ ?Lgl^[p' (J | d) ],i+aA > 0 . . (26.16) 
This is true for any E and E and for all A, whatever its sign, and hence the expression in 
curly brackets vanishes. Thus we have 
W W I 0) ]6o - ^S|^ [?' (^ I 9) la. =0. . \ . (26.17) 
Po {E | 0O) 
Similarly this equation may be shown to hold outside w0, and hence it is true throughout W. 
Now we have 
I, 
49 (25 I 6) dx = 1, 
and hence, differentiating with respect to 0 and putting (9 = 0O, 
f |>'(25 | 0) ]So cfe = 0. 
Substituting from (26.17), we have 
Po (E I fl0) 
^7^0 (J I 0O) 
/, 
b' (S I 6) V & = 0, 
and hence 
Thus, from (26.17) 
I^U 6)A = 0 (26.18) 
Po (E \0o) 
\y (E I 6)]0 = 0 (26.19) 
But this implies that the derivative of p with respect to 0 is identically zero at 0O, which 
is contrary to hypothesis. The theorem follows. 
It may be noted that in deriving (26.17) from (26.16) we used the property that A 
may have either sign. If it can have only one sign, that is, if our class of admissible 
alternatives is confined to the case when either 0 < 0o or d > d0, a U.M.P. test may exist; and 
so we found in Examples 26.2 and 26.3. 
Best Critical Regions and Likelihood 
26.18. Since on the boundary of a best critical region we have pQ — kpx = 0, that 
boundary is determined by the condition that on it the ratio of the likelihoods of two 
functions corresponding to H0 and II\ is constant. 
Consider now the case where Hx comprises a set of alternatives varying according to 
the parameter 6, H0 being one of them. In accordance with the principle of maximum 
likelihood we should obtain, as the most likely value of (9, the solution of 
-l\ =Q, (26.20) 
dd 
278 GENERAL THEORY OF SIGNIFICANCE-TESTS 
where 6 is then expressed as a function of the variables. If this value is substituted in 
p9 we obtain the distribution with greatest likelihood which may be written p (Q max.). 
The surfaces of constant likelihood are defined for this distribution by 
p0 — Xp (Q max.) ==0. .... (26.21) 
Now these surfaces are, in fact, the envelopes of the family, varying with 6, 
p0 — kp6 = 0, ..... (26.22) 
dp 
for to obtain the envelope we differentiate with respect to 0, giving ^ = 0 and eliminate 8, 
leading back to (26.21). Thus, if there exists a best critical region (and hence a U.M.P. 
test) for all permissible alternatives Hd, such a region will be the envelope with respect to 
such alternatives and will therefore be identical with a region defined by (26.21) ; and 
hence a test based on the principle of likelihood leads to best critical regions, if they exist. 
If, as is more usual, there is no common best critical region, the ratio of the likelihood 
of H0 to that of any particular Hd is fc. The surface (26.21) remains the envelope of the 
family of surfaces (26.22) for which k = X. 
Example 26.4 
Consider once again the normal form, where both mean // and variance a2 are specified 
and the admissible alternatives are that they can have any values, subject of course to the 
variance being positive. For any given jli_ and g_ the best critical, region will be given by— 
*• = ( * X exp 
^ i y I x ju,q \ _____ y ( ^ ™ /'i 
<k 
or 
l(Lii!'-r^- > 2 ^ J t IZ- 
i- 
This may be written in the form 
where 
Thus, if ax > a0 we have 
and if ax < g0 we have 
9 9 
ft* ft-' 
n _-JL~~~J> { (x — p)2 + s2} > constant 
aiao 
<n - <*5 
(x — p)2 + s2 > v2, say ; 
(x - p)2 -f s2 < v2. 
For any specified px and ax the best critical regions are bounded by hyperspheres with radius 
vVn and centre ata^ = __a = . . . = xn = p. Owing to the fact that p varies with f*_ and 
au there will not in general be a best common critical region and a U.M.P. test; and this 
remains true even if we limit our alternatives to ax <oQ and //x </i0 or by similar 
inequalities. 
We may regard x and s as independent variables and represent the data on a two- 
way plane (x, s). The best critical regions are then seen to be bounded by circles with 
BEST CRITICAL REGIONS AND LIKELIHOOD 
279 
centre (p, 0) and radius v. Pig. 26.2 (adapted from Neyman and Pearson, 1933c) illustrates 
some of the contours for particular cases. A single curve, corresponding to a single 
probability level, is shown in each case. 
Cases (I) and (2) : ax = a0 and p = ± co. The best critical region lies on the right 
of the line (1) if fa > fa and on the left of (2) if fa < fa. This is the case discussed in 
iliXample 26.2. 
Case (3) : ax < o0, say <jt = lov Then p = ju0 + f (fa — fi0) and the region lies 
inside the semicircle marked (3). 
Case (4) : <jx < a0 and fa = fa. The region is inside the semicircle (4). 
Case (5) : ox > o0 and fa = fa. The region is outside the semicircle (5). 
There is evidently no common best critical region for these cases. The regions of 
(MOr0) 
Fio. 2<>.2.—Contours of Constant Likelihood, in a Two-dimensional Case. (See text.) 
acceptance, however, may have a common part, centred round the value (fa, o-0), and we 
should expect them to do so. Let us find the envelope of the best critical regions, which 
is, of course, the same as that of the regions of acceptance. The likelihood ratio is 
h = 
(T; 
n 
exp 
ns 
1 
1 
a 
.2 
a 
n 
2 
G0 
JC> /^"L 
a 
The partial differentials with respect to fa and at equated to zero give 
n 
0*1 
ns* 
a\ 
n 1 x 
fa 
G 
n ,_ v 
-5 (x ~ W 
GX 
0 
0. 
whence we find fa = £ and ax = * and the envelope is 
2 
n 
logfc 
# — fa 
log 
a) 
2 S2 
+ -*• 
280 GENERAL THEORY OF SIGNIFICANCE-TESTS 
The dotted curve in Fig. 26.2 shows one such envelope. It touches the boundaries of all 
the critical regions which have the same likelihood-ratio h. The space inside may be 
regarded as a a good " region of acceptance and the space outside accordingly as a good 
critical region. 
There is no best region for all alternatives, but the regions determined by envelopes 
of likelihood-ratio regions effect a sort of compromise by picking out and amalgamating 
parts of critical regions which are best for individual alternatives. 
Example 26.5 
In the previous example we have supposed that the sample space W was the same for 
all admissible alternatives. This is quite legitimate, for we can always regard the domain 
of variation as infinite by supposing that p = 0 outside the range of the 
frequency-distribution of the variates. In the normal case, of course, p does not vanish anywhere, so that 
we are compelled to consider W as infinite. 
When, however, the sample-space for non-vanishing p is bounded, special 
circumstances may arise, and it is occasionally necessary to consider separately the different 
discriminating regions. For instance, if the sample-spaces corresponding to H0 and Hx 
are W0 and Wx, it may happen that W0 and Wx have no common part when both p0 and 
px are greater than zero. If so, we can distinguish between H0 and FIt with certainty. 
If there is a common region W01 then W± — W01 should be included in the best critical 
region, for to do so reduces the probability of errors of the first kind. But it does not follow 
that this should constitute the whole of the critical region, for we might then commit too 
many errors of the second kind, i.e. accept HQ too often when H± is true. We may then 
wish to add to Wx — WQ1 a region wQ0, making w0 altogether, such that w0Q lies inside W{)1 
and pQ (E e w00) ^ p0(E s wQ) = 1 — a. This controls the first kind of error to level a 
and reduces the second kind of error. 
Consider the population 
P (x) = t> a~-^b<x<sa-]-^b 
= 0, elsewhere. 
Suppose a sample of n to have been drawn from a population of this kind where b is known. 
We wish to test whether a has some value a0 as against the alternative ax. 
The sample-spaces W0 and Wt are hypercubes centred at a0 and ax. If they have 
a common part W01 the probabilities p0 and px in that part are both proportional to the 
volume and p0/Pi = 1 everywhere in the region, we take any region w00 of 
content 1 — ot in W0l and add it to Wx — W01 we get a best critical region, and there are clearly 
infinitely many such. 
For the admissible alternatives ax the hypercube Wj will move along the long diagonal 
xx = xz = . . . = xn as ax varies, and we cannot always find a common region of size 1 — a 
to form Woo- By taking such a region as a hypercube of side 6(1 — a)n, however, fitted 
into one of the corners of W0 lying on the long diagonal, we " nearly " obtain such an object 
since this region provides what is required so long as W0 and Wt have a common part of 
content 1 — a. Which corner we choose depends on whether the hypothesis is at > aQ 
or a0 > ax. 
RELATION BETWEEN U.M.P. TESTS AND SUFFICIENT ESTIMATORS 281 
Relation between U.M.P. Tests and Sufficient Estimators 
26.19. It was thought at one time that the existence of a set of U.M.P. tests for 
a continuous range of admissible alternatives involved the existence of a sufficient estimator 
for the parameter concerned. This does not appear to be true in full generality, but is 
so in nearly all the cases occurring in statistical practice. We will prove a theorem on the 
subject :— 
If a system of U.M.P. tests exists and if any point in the sample-space lies on the 
boundary of a best critical region, then a sufficient estimator exists for the parameter whose 
variation provides the admissible alternatives.* 
It is enough to show that for an arbitrary point we have 
pt (JE?) = h {t, d)p0(E) (26.23) 
for then t is sufficient for 6 by definition. Now we know that on the boundary of a critical 
region we have 
Pi {E) _ 1 __ , 
—„ — _. — ti, say, 
P* \BS) k 
where h varies with the x's and with 6. We show that h has the form h (t, d) by defining 
a function t and showing that if t has the same value at any two points Ex and E*,, then 
ffi (#i) __ Pi (JBJ2) 
p0 (E,) p0 (E2) . 
for all 6. 
26.20. For this purpose we require a lemma to the following effect: if a set of U.M.P. 
tests exists, it will be said to be ordered if the condition ax > a2 implies that the critical 
region w (ocj.) is included in the region w (a2) ; and if a set of U.M.P. tests exists but is not 
ordered we can always find another set which is. 
w (ax) and w (oc2) may include parts of W where p vanishes. Let the remaining parts 
be v (<xx) and v (oc2) and, if v0 is the common part of these regions, write 
V (a,) = t>0 + »'\ (26i24) 
v (a,) = v0 + v" J 
where «0) v' and v" have no common points. Now for any value, of 6 and for any E in w (kx) 
—and therefore in v'—there is an hx such that 
p1(E) >hlp0 {E) in v' 
< hl p0 (E) outside, and therefore in v". 
Similarly, within w (oc2) and hence within v" we have an h2 such that 
px (E) >h2p0 (E) in v" 
< &2 jPo {$) in v'- 
It follows that, from the inequalities deriving from v", hx > h2, and similarly, from v', 
h2 > hx. Hence hx = h2 == h, say, and 
Pl(E) ^hp0(E) (26.25) 
within v' and v" for any 6. 
* The theorem remains true if there is a set of points of measure zero for which the condition as to 
boundaries is not fulfilled. It is also true for several parameters, as may be seen by an easy 
generalisation of the argument. See Neyman and Pearson (1936a). 
282 GENERAL THEORY OF SIGNIFICANCE-TESTS 
Now take 
«(ax) =v0 +v'" . . . . . . (26.26) 
such that 
I 
p0 dx = 1 — ax. . . . ' . - (26.27) 
**(ax) 
This is always possible, for the integral of p0 over v0 + v" is 1 — a2, which is greater than 
1 — ax. It follows from (26.27) and the first equation of (26.24) that 
p0 dx = I p0 dx. ..... (26.28) 
V'" J V' 
Now put 
w' («i) = W0 + u (ai) = Wo + ^o + v'", 
where W0 is the part of W for which p0 = 0. Then from (26.27) 
i 
$>0 da; = 1 — ot] 
io' (ai) 
Further, w' (ax) is a best critical region with respect to admissible alternatives, for (26.25) 
and (26.28) imply that 
I px dx = I px dx, 
J V'" J 17' 
and hence 
pxdx = \ pt 
J W (at) J v (ai) 
Finally, w' (ax) is wholly included in w (a2). 
We have therefore replaced the region w (ax) by another region w' (ax) with the same 
properties except that it is included in w (a2). The lemma follows. 
26.21. To return now to the main proposition, let E be any point of W. If it belongs 
to only one boundary of a best pritical region with content 1 — a we put t(E) = 1 — oc. 
If it belongs to more than one, we put t(E) equal to the mean between the upper and lower 
bounds of values of 1 — a for which the boundaries include E. In virtue of the lemma, 
this implies that whatever the value of 1 — a between these bounds, the corresponding 
boundary must contain E. 
Thus t is defined everywhere. Further, if it has the same value at two points Ex and 
23 2 these points must lie on the same boundary. It follows that on this boundary 
Pl (JQ „ Pl (E%) 
p0 (Ex) p0 (E2) 
and hence the theorem is proved. 
The converse is not generally true, but one has to exercise some ingenuity and import 
some artificiality to construct examples where it fails. Cf. Exercises 26.3 and 26.4. 
26.22. We shall consider a class Q of admissible hypotheses depending onr+s 
parameters dx . . . dr . . . dr+s and shall regard the hypothesis H0 under test as one of 
this class. A composite hypothesis of r degrees of freedom is one for which s of the 
parameters, say 0r+1 . . . 0r+s, are specified, the hypotheses determining the distribution 
apart from the unspecified parameters. For example, the hypothesis that a population 
COMPOSITE HYPOTHESES 283 
is normal with specified mean, nothing being supposed about the variance, is a composite 
hypothesis of one degree of freedom. It will be assumed that any admissible simple 
alternative is given by specifying the other r parameters dt . . . 6r and that there is a common 
sample-space W for all such alternatives. 
Regions Similar to the Sample Space 
26.23. In order to test the composite hypothesis H0 we need in the first place to 
control errors of the first kind by determining a critical region w, such that 
j 
J w 
pQ dx = 1 — a. . . . . . (26.29) 
This, however, differs from the simple case in that pQ can vary according to the unknown 
parameters, and to be certain of controlling the error we must be able to find w such that 
(26.29) is true whatever 6X . . . 6r. If this can be done we shall call the region w similar 
to the sample-space W and shall speak of 1 — a as its size. 
The problem of testing composite hypotheses then becomes one of (a) rinding the 
similar regions, and (b) selecting from among those regions the one which minimises the 
second kind of error for a simple admissible alternative Ht. If this is the same for all 
Ht we shall have a common best critical region. 
26.24. We consider in the first place the composite hypothesis with one degree of 
freedom. The general problem of finding similar regions in such a base has not been solved, 
but a solution is possible in one important class of case, namely, that for which 
(a) p() is indefinitely differentiate with respect to 0t for almost all values of 01? 
(b) the function ^0 obeys the relation 
f = A + B</>, (26.30) 
where 
</> = gg ■■ lOg^O, f = gp .... (26.31) 
and A and B depend on 0l but not on the #'s. In particular the normal distribution 
is of this tvpe. 
Under conditions (a) and (b) it follows that for w to be similar to W it is necessary and 
sufficient that 
-Jrdx =0, fc- 1, 2, (26.32) 
I 
Let w be a region for which (26.32) is true. Then for k = 1 and 2 we have 
p0 cf> dx = 0 
I 
p0 (<£2 4 <-/>') dx = 0. 
In virtue of (26.30), this last may be written 
p0 ((f>2 f A + B<f>) dx = 0, 
[ 
w 
whence 
I 
p0 <l>2 dx = — A po dx = - A (1 — a). . . • (26.33) 
W J W 
I 
Jw 
284 GENERAL THEORY OF SIGNIFICANCE-TESTS 
Differentiating (26.33) with respect to 6t and using previous results, we find 
p0 <f>*dx = {2AJB - A') (1 - a), . . . . (26.34) 
Jw 
and generally 
f p0 <f>kdx = (1 - a) y)k (00, (56.35) 
J W 
where %pk (6J is a function of dx only, and is therefore independent of w. Now (26.32) is 
true for W = w, and we find 
f Po<f>*dx = ^(0!), (26.36) 
JW 
so that 
—— f p0<f>kdx = [ p0<j>kdx. . . . . (26.37) 
1 ~~ & Jw JW 
Now consider the random variable <f>. Since p0 integrated through w is equal to 1 — a, 
we may regard - Q as a frequency function defined in w. It follows from (26.37) that 
1 — ex 
the moments of <j> in this domain are the same as those of <j> in W. Consequently, if the 
moments determine the distribution uniquely, the distributions of <f> are identical. 
Hence we may use the hypersurfaces <f> = constant to set up similar regions. The 
space W may be imagined as composed of shells of infinite thinness bounded by these 
hypersurfaces. If we determine an " area " on one of these shells equal to 1 — a times 
its area in W, the totality of such areas will constitute a region w of size 1 — a ; and since 
this will be so irrespective of 6t the region w is similar to W. 
26.25. When similar regions are determined by the above method we have to find 
the best critical region from among them. Let Ht be a simple admissible alternative. 
We require to find from the regions w a region w0 such that 
<ptdx = maximum. ..... (26.38) 
Co 
We now show that this is equivalent to maximising 
J ptdw {(/>), (26.39) 
Jw (A) 
subject to 
f p0dw(<l>) = (1 -a) [ p0dW {</>). . . . (26.40) 
Jw{4>) • JW(<f>) 
Here w (<f>) means the element of w for constant <f>—the " shell " of the previous section. 
The object of this is to reduce our present case to that of simple hypotheses. We take 
^asa new variable and consider together the remaining variables (which amounts to 
determining similarity of w and W in each separate shell between <f> and <f> +- d</>, as in the previous 
section), and are thus left with regions dependent on <f>. Equation (26.39) then requires 
that the probability of the second kind of error in each shell must be a minimum, subject 
to the control of the first kind asserted by (26.40). 
COMPOSITE HYPOTHESES 285 
Suppose that (26.39) were not maximised. There would then exist a set of values of 
<j> for each of which we could determine a region v (cj>) such that 
f Podv(c/>) = (I - a) [ p0dW(cf>) . . . (26.41) 
and 
pt dv (^) > I pt dw0 (<£). .... (26.42) 
v (4) J «>o {4) 
Let E be this set of values of <f> and CE the remaining set. We prove our result by 
obtaining a contradiction, namely by defining a region v which is similar to W, and such that 
I ptdx> I ptdx, ..... (26.43) 
J V J Wo 
which contradicts (26.38). 
Take as v the shells of hypersurfaces (1) in CE which are identical with w0((j>) and 
(2) in E which satisfy (26.42). Now 
ptdx = d$\ pt dv {<j>) 
Jv JB+CE Jv{<f>) 
and 1 ptdx = I d<f> 1 ptdw0 (</>). 
Hence 
#, da: - #, da; = I d<£ J I p, dv (</>) - ft dw0 (<£) 
J v J tt'„ J JS7+ C/i' IJ v (4) J 'M?0 (<£) 
= f d<f> { f p, dv ((/>) - f ft dw0 (<f>)\ > 0, . . (26.44) 
which is the contradiction required. 
26.26. Thus our problem is reduced to that of finding, in the shells </> = constant, 
portions w0 (<f>) which maximise the integral of pt. We have, so to speak, brought the 
problem down one dimension by locating it in shells instead of dealing with it throughout 
the spaces w and W. It now becomes that of a simple hypothesis in (n — 1) dimensions, 
and the best critical region is the one for which 
Pt>jc'Po, (26.45) 
where k is a function of <f>. The sum of these regions for the various values of cf> gives us 
the complete solution to the problem, and if this sum has boundaries which are independent 
of Ht we have a common best critical region and a XJ.M.P. test. 
Example 26.6 : " Student's " Hypothesis 
A single sample is taken from a normal population 
dF = —L^ exp { - * ^^\ dx, 
with unspecified a. We have then one degree of freedom, d^ = a, and the hypothesis H0 
is that fi = fa, say. 
286 GENERAL THEORY OF SIGNIFICANCE-TESTS 
We find 
n . E (x — fjiQ) 
2 
d<f> 
da 
da 
n 
; 
n 
<72 
^6 
— 
2n 
a* 
JJQ 
_ Zi (x 
cr4 
3<£ 
a 
cr 
a 
/*o)2 
- /^o)2 
s 
+ 
cr3 
• 
s2}. 
Condition (26.30) is satisfied, and </> is constant over the hypersurfaces 
E (x — fi0)2 == n {(x — fi0)2 + s2} = constant. 
The hypersurfaces are hyperspheres in W. To construct a similar region we have merely 
to pick out a region of size 1 — a on each shell and to amalgamate them. In our present 
case this is particularly easy because pQ is constant over the shells and we need only pick 
out areas on each shell bearing to the area of the hypersphere the ratio 1 — a. 
These areas need not be of the same shape or similarly situated. By selecting them 
in different ways an infinite variety of regions may be constructed. We have to find the 
best for an alternative simple hypothesis a = a1} pu = /ux. 
The condition (26.45) becomes 
1 
exp 
o\ 
~£f{(f ~^l)2+"2} 
1 
> -— exp 
U {{x-^Y+s*} 
2a 
As we are dealing with regions which are similar with regard to cr, we may put a = ax 
and find 
1 
x (/i± — /uQ) >% {fi\ — fi\) - - <s\ log k = (^i — fi0) kl9 say, 
lb 
where kt = kx (<f>). Thus we find, for the boundary of w0 (<£), 
if [ix > //0J x > kx (<£) 
if /lc1 < /u0, x < kx (</>), 
where kx has to be chosen so as to satisfy 
p0 dw ((f)) = (1 — a) 1 p0 dW ((/>). 
Jw(6) J WU) 
w (<£) J W(<f>) 
Thus on any particular shell the " cap " cut off by the hyperplane x = constant must have 
area 1 — a and hence must subtend the same solid angle at the origin. Consequently the 
boundaries lie on a right hypercircular cone through the point whose co-ordinates are all 
equal to ^o an(i whose axis is perpendicular to x — 0, namely the line 
For each a there will be a different cone. If fix > /lc0 the cones will be in the 
positive quadrant and in the contrary case in the negative quadrant. 
Furthermore, these regions are independent of /^x. Thus for the class of hypothesis 
jtz1 > jLc0 or ju± < ju0 (but not both together) the common best critical regions and U.M.P. 
tests exist. 
Finally we have to evaluate a in terms of the sample values determining the critical 
COMPOSITE HYPOTHESES: SEVERAL DEGREES OF FREEDOM 287 
/V» II 
cones. We have already seen in Example 10.6 (vol. I, p. 239) that if z = — the 
s 
frequency inside the cone is 
1 Cz dz 
= a. 
n 
0 
js'!L2^'iru{i+a!*)2 
Thus " Student's " test, which we have previously considered on more or less intuitive 
grounds, is now seen to be the best in the sense of the theory herein developed, for the 
admissible class ^ > /u0 or for that jut < jli0. 
Example 26.7 
Consider a sample from the normal population with unspecified mean, the hypothesis 
being that a = gq. We now find 
4> = 
d<f> 
d/u 
= — log p0 = 
OjU 
n 
°V 
n (x — [a,) 
°0 
at > a0 
at < or0 
we have 
we have 
s2 > k± (cj>) 
s2 < k± (</>). 
so that (26.30) is satisfied. 
The hypersurfaces <f> — constant are the hyperplanes x = constant, and any regions 
1 
of size 1 — a on these hyperplanes will provide similar regions w. The condition pt > -=■ pQ, 
will be found to reduce to 
s2 (<r§ - of) < - {x - //t)3 {al ~~ of) + 2gI of hog ~° + -log k\ = (eg - erf) kl9 say. 
If 
and if 
Since s2 is independent of x, kt will be a function of a and n only. The best critical 
regions are those given by s2 > $1 and s2 < .sjj as the case may be, and the appropriate 
values of s0 corresponding to a may be found from the known distribution of s2. The 
critical regions are hypercylinders, and again there are two sets of best common critical 
regions, according as at > cr0 or Gt < o*0- 
Composite Hypotheses : Several Degrees of Freedom 
26.27. As a preliminary to extending the theory for one degree of freedom to the 
case of several degrees, we note that if a region w is similar to W with regard to 0X ... 6r 
jointly, then it is so for each of them separately ; and conversely. The direct result is 
obvious and the converse follows in this way : (we need prove it only for r = 2 because 
the rest follows step by step). If then 
p dx — 1 — a 
is true for 025 0S . . . 6r independently of 6l9 and for 6l9 03 . . . 6r independently of (92, 
then it is true for any values of 6t and 02 and any other fixed values of 03 . . . 6r; and 
hence it is true independently of 0X and 0% together. 
288 GENERAL THEORY OF SIGNIFICANCE-TESTS 
26.28. An additional preliminary requirement is the concept of independence of 
a family of surfaces of a parameter. Suppose 
fj(xl...xnid)=GJ j = 15 2 . . . k < n . . . (26.46) 
represents a family of surfaces, where 6 and the O's are variable parameters. Let 
S (9, Gx . . . Gk) be the intersection of these surfaces, or, if h = 1, the surfaces themselves. 
Consider the family obtained by fixing (9 and allowing the O's to vary. Then if any surface 
of this family for Qx can also be obtained from a second family for 02 we shall say that the 
family is independent of 6. We get the same aggregate of intersections however 6 is chosen. 
For example, if 
and /a = #! + #2 + #3 = <72> 
the family $ consists of circles in planes at right angles to the Hne x1 = x% = #3 and having 
their centres on that line. This is true however 6 is chosen, and S is therefore 
independent of 6. 
26.29. Under certain restrictive conditions similar to those of 26.24 it is now possible 
to find solutions to the problem of determining best critical regions. We assume 
(1) that -^~ exists almost everywhere for all k and j = 1 . . . r ; 
(2) that if fy = — logpQ and </>■ = ^p. 
then <f>'j = Aj + Bj </>,j ; (26.47) 
(3) that the family of surfaces given by the intersections of <f>j = Gj is independent of 
Q. for j = 1 . . . r. 
Subject to these conditions (which are sufficient but not necessary) similar regions exist. 
Consider any two surfaces <f>x and cf>2. Since w is similar with respect to 6X alone, we may 
find surfaces <f)± = constant and 
| pdw((f>1)=\ pdWifa). . . . (26.48) 
In accordance with assumption (3), the family of surfaces <j>x = Cx is independent of 62. 
Thus if 6 2 varies, W (</>x) and w {^>x) will not vary, though perhaps they may correspond to 
other values of Gx. Furthermore, (26.48) is true regardless of 02- Hence within the shell 
</>! = constant we can repeat the analysis used for one degree of freedom. We find that 
the necessary and sufficient condition for w to be similar to W with regard to both dx and 62 is 
[ p0 dw\</>l9 <£2) = (1 - a) f p0 dW (<f>l7 fa), . . (26.49) 
where W is the intersection of ^t = Gu </>2 = 02 for any values of Gx and C%; and similarly 
for w. 
As before, the most general region w is obtained by amalgamating the portions of size 
(I — a) on the intersections of <f>x and </>2. The generalisation to r degrees of freedom is 
COMPOSITE HYPOTHESES: SEVERAL DEGREES OF FREEDOM 289 
immediate. It also follows in the usual way that the best critical region is the one for 
which 
1 pt dx > \ pt dx, 
J W0 J V 
v being any other region of size 1 — a ; and w0 is defined by 
pt>h(01 . . . dr)p0. . . . . . (26.50) 
The following examples will illustrate the theory. 
Example. 26.8. Ratio of Two Variances 
Suppose we have two samples of nl9 n2 members from independent normal populations 
whose means and variances are unknown. The joint distribution may be expressed as 
1 
f oc exp 
nL { {Sl _ ^y + si) _ g* {(gf - M%)* + 5|} 
.2 
'9! 
Consider the composite hypothesis a1 = cr2 — a, say. This has three degrees of freedom, 
for /i1? //2 and a are unspecified. As the alternative Ht we will take 
61=/Ll1, 02 = /Ja —■/*! =&i, 03 = ^ls 04=—, 
and for j?0 itself 
0! = jLl, 02 =6, 03 = <7, 04 = 1. 
We have first to consider whether the conditions of 26.29 are satisfied. 
(1) Evidently p0 is difFerentiable for all parameters any number of times. 
(2) We find— 
7) 1 
4>i = -a- log JPo = ,{^1 (£i - /') + ^a {$* - /* — &)} 
</>:{ = „. log p0 = — ^l -N + —{rii (f:, - /O2 + n« (x. - ft - 6)2 + nx s\ + n2 si} 
oa ' " a a%i 
and (26.47) is seen to be satisfied. 
(3) The hypersurfaces </>x ™ Cx are evidently equivalent to 
n1x1 + 7&ao?a = Cj, 
where (7j is an arbitrary parameter. The hypersurfaces <f>2 = C2 give similarly 
Both these are independent of 0a and their intersections, namely i^ = constant, x2 = 
constant, are independent of 03. Thus the third condition is fulfilled and we may apply the 
foregoing theory. 
The equations <f>x = constant, <f>2 = constant, <£3 = constant are equivalent to 
xx = constant 
x« = constant 
nxs\ + na52 — constant = (nx + n2) s% say. 
A.S.—VOL. II. U 
290 
GENERAL THEORY OF SIGNIFICANCE-TESTS 
The element w0 is part of W (fa, fa, fa) within which 
Pt > Po/h (*i> %*> sa) 
and this condition, by reference to the frequency function, becomes 
< 
— exp 
1 1 
2a2 
exp 
2a' 
2o\ 
{nx (xt — ^x)2 + nxsl + nSt1 {x2 — /ux + ,a2)2 + n2d±2 si} 
Since the region w is independent of fi, b and o*, we may put them respectively equal to 
/^i, &i and ax and hence find for the condition 
n2 (1 - ej) {(£2 - /*! - 6X) + s\ } < 2crf flf (log A - wa log 04). 
Since this inequality holds good on x2 = constant it contains only one variable s| an(i we 
accordingly find two cases :— 
If 04 = -i > 1 the best region is defined by si > A-i (#i, x2, si); 
If 04 = ~1 < 1 the best region is defined by s% < h'2 (xl9 x2, s2). 
0i 
We have now to determine h2 so as to satisfy 
p0 dx = (1 — a) I $>0 dx. 
J W?0 {^i, <£a, <£a) J ^Fo (<£x» &> <W 
Now TF (^x, ^2? ^s) is the locus for which xl3 xz and ^ are constant, and thus the integral 
on the right is the product of 1 — a and the frequency function p0 (xl3 x%, s%). Similarly 
that on the left is the integral of this function over the region for which si "$ hf. Thus 
I p0dx = \ p0 (#1, x2, «5«3 si) dsl in the first case, 
J w0 J V 
with a similar expression but different limits in the second. Now we have for the joint 
frequency function of Xi} X%) s? and si 
f oc 
Q-rfci+na 
sj1"3 s™2""3 exp 
- Tnfyi (£1 — /^i)2 + ^2 (aa - /^2)2 + (%i + %) ^} 
2crl 
Transforming from s\ to s\ as variable, we find for the condition, after a little reduction- 
{ {nx + n2) sl-n2sl} 2 sf1*-* Asf = (1 - a) {(nx + n2) s2a~~n2sl} 2 5s«»-3 cfe2, 
J /1' J 0 
where V = — s\. On substituting n2 s\ = (wx -f ^2) ^ ^ we find— 
(1 - ^) 
Jo 
nx — 3 ?i3 —3 
2^2 dlfc 
J Wo 
Wi — 3 n2 —3 
^) 2 ^2 d^ = (1 — a) £ 
'%! — In. 
2 ' 2 
It follows that uu3 u0 depend only on a, n1 and n2. Thus, whatever the values of xl3 x2 
and ^5 the best critical region is defined by 
02 -^ v _ (ni ~^~ n*> sa 
^r 
if 0*2 > 0*1 
n. 
^ < ^ = ^L±Jh)3 u' if G, < Cl. 
7bo 
COMPOSITE HYPOTHESES: SEVERAL DEGREES OF FREEDOM 291 
These are equivalent to 
nx si + n2 si 
u 
u, 
If we put 
if <t2 > &i 
if Gfi: 
(7o. 
2J 
i loef 
ttl (Wj 
IK 
the ^-distribution of u reduces to Fisher's form. The result we have reached is therefore 
equivalent to showing that the 3-test is the best for the ratio of two variances in normal 
samples. As usual, there is no U.M.P. test for the whole range of the ratio from 0 to oo, 
but two U.M.P. tests for the ranges 0 to 1 and 1 to oo respectively. 
Example 26.9. Difference of Two Means 
Consider again the previous example, where now the variances are unspecified but 
equal and the means /ux and + b may have any values. The hypothesis H0 is that 
6 = 0 and has two degrees of freedom corresponding to /u and 0. 
Let the alternative Ht specify the parameters 
u1 = fj,t, c/2 = &p $3 = fy. 
In addition to the quantities required in the previous Example we now use also x0 and 
s()9 the mean and variance of the pooled samples. 
We find that the three conditions of 26.29 are satisfied, and 
nx + n2 
0i 
(X0 — IH) 
<f>2 
a- 
7b 1 4~ 1^2 , nx -\- 712 ( /- 
0 06 
Equivalent to this family are the surfaces 
_ n 
- \j <>. 
Hi)* + 4}- 
r 
S: 
The condition <pt > h (cf>u <f>2) p0 reduces to 
and as usual we find two cases according as //,2 > /j.x or vice-versa. We consider only the 
first, the second being analogous. 
Writing v = xx — x2 > k% we have to determine h' by 
r/ii" __ rhiv 
p0 (x , si v) dv = (1 — a) p0 {x0, si v) dv, 
J h'" J h"' 
where ti" and Kie are the lower and upper limits of the variation of v for fixed values of x0 
and sf}. 
The frequency function of x0, sf)7 v and s( is easily found to be 
/ cc sxn^'U {nx + n2) s20 —n1s\ — —x—~ v 
whence that of x0, si and v is found to be 
2 I -~rr~ 
exp 
n1 -{- n2 r , Xo 2>v 
2(T 
/ °c 
4 
nx n2 
(Wi + ^2)2 
t?2 
Ux + Tta— 4 
exp 
■n1 -f- 9?/2 c 
21 
2cr 
{(«o-A*l)B + ^} 
292 GENERAL THEORY OF SIGNIFICANCE-TESTS 
Since x0 and s20 are constant over the domains under consideration we have to satisfy 
LA (wx + ^)a / Jo V K + ^)- 
,,/, {n1+n2)s0,iv __ (nx+n2) s0 
where h = 77~~~"\ > ^ ~~ 7/IT ™ \ * 
If we put 
fai + n%) sQ 
Vi^i^f (l~+~s2)*' 
this reduces to 
1 [*•' 
A ^1 + ^2 — 2\ J_ fl 
rfz 
and 
B I i ^.LXJ^LTLf: W - (1 + ^f1^ 
iCj —' 00% / *"1 ^2 
a 
VK 5f + ^2 5|) V ni + ^2 
We have thus arrived at the J-test for the diflference of two means in normal variation when 
variances are equal. Once again the test we introduced on more or less intuitive grounds 
has heen shown to be justified in the light of the theory developed in this chapter. 
Linear Hypotheses in Normal Variation 
26.30. Several of the hypotheses dealt with in foregoing examples are particular 
cases of a general class known as linear hypotheses, which accounts for the fact that we 
keep arriving at the same sort of conclusions respecting them. 
Suppose we have n independent variates typified by x$ distributed in the normal form 
with common variance a2 but different means. Suppose the means are connected with 
r and s unknown parameters 6X . . . 6r . . . 6r+s by linear equations of the type 
f^k == ^ fyk Vj' • - (26.51) 
i 
Suppose further that the hypothesis H0 specifies r parameters 
and hence is composite with s degrees of freedom. Then H0 will be called a " linear 
hypothesis ". The reader can verify for himself that " Student's " hypothesis, and the 
hypothesis as to the difference of two means when variances are equal, are of this type. 
The homogeneity test in variance-analysis and the test of regression coefficients are also 
reducible to the same form. If, of course, H0 specifies r linear relations among the (9\s 
instead of the (9's themselves, it can be reduced to a hypothesis which specifies the Q'& 
directly, except perhaps in degenerate cases which need not detain us. 
26.31. The theory developed in the earlier part of the chapter for, composite 
hypotheses may be applied to linear hypotheses as we have defined them, and the argument 
LINEAR HYPOTHESES IN NORMAL VARIATION 
293 
follows exactly that of Examples 26.8 and 26.9. It is readily verified that the three 
conditions of 26.29 are satisfied. We have— 
*, = s cjk ?* - w 
k O 
cf>j = constant 
> 
. (26.52) 
<f>'o 
71 i 1 vt \2 
or or5" & 
2n 
a 
4>c 
. (26.53) 
We can therefore find similar regions w (^ . . . <£r, <f>a) and select from them the best 
critical regions in the usual manner. We will omit the rather cumbrous algebra and quote 
the following result (Kolodzieczyk, 1935). 
Transform to new variates Ex . . . Er+8i yr+s+x . . . yn by the equation 
*k = /** + 2^1 Cjk Ei + JC Cik yi> ' (26.54) 
where the c's are those given in (26.51) for j, h < r +• s and the other c's are orthogonal, i.e. 
k 
= 1, Ic = j, 
j > r + s 
j > T _|_ ^ 
(26.55) 
.l.*r V/Xlllv/ 
W 
nfi 
a 
Z y< 
and 
?uSYjj 
k~\ \j=1 
A further transformation of Er+1 . . . Er+S is now made to variables yr+i 
that (26.57) becomes 
r r'+s 
V* 
j, &~1 
7c=r+l 
</£ 
(26.56) 
(26.57) 
^r+S S0 
(26.58) 
(26.59) 
fc=r+l 
The coefficients jR can, of course, be obtained from the c's by ordinary determinantal 
algebra. 
Writing now e; = 63 - 0", i.e. the difference between fy on the alternative hypothesis 
and its value if jH^is true, we find that the best critical region is given by 
1 
r 
y Rjk £j ^k 
V 
j,k=l 
VfaS* + nSl) 
- > Vo> • 
(26.60) 
S T 
sj [ 2L/Rjk £j &k 
1,k~l 
294 GENERAL THEORY OF SIGNIFICANCE-TESTS 
where v is distributed in the form 
dF or (i ^.v2)—T-~dv - 1 <v < 1 . . • (20.01) 
and v0 is given by 
oc = I 
J v 
dF (2<»-<>2) 
26.32. There is one interesting conclusion to be drawn from (26.60). If a U.M.P. 
test exists, v should be independent of Q5 and hence of sj. This appears to be possible 
only if the denominator in the second part of (26.60) is rational. But this denominator 
is seen from (26.59) to have the coefficients of a positive definite form and hence is only 
rational if r = 1. We conclude that if r > 2 no XJ.M.P. test is possible for linear hypotheses 
in normal variation. 
We have already seen that under general conditions no U.M.P. test exists for r = 1. 
A similar conclusion follows from (26.60) if r = I, for it then becomes 
which, as usual, leads to two cases according as et < 0. 
26.33. We will pause at this point to review our results. We began by'defining two 
kinds of error and showing that a test could be defined as " best " for a single alternativo 
hypothesis if it controlled the first kind and reduced the second to a minimum. When 
there is a class of admissible alternatives we may sometimes arrive at a U.M.P. test which 
will minimise errors of the second kind for any member of the class, and such a test may 
be regarded as the best attainable. Though the XJ.M.P. test does not exist in the great 
majority of cases, we may find tests which are U.M.P. for either dx > 0O or 6t < 0O. Such 
tests have been reached for " Student's " hypothesis and several others in common use, 
and are found to give the same tests as those introduced on rather intuitive grounds in 
Chapter 21. 
26.34. The absence of a U.M.P. test implies that in the majority of cases we have 
to look for other criteria to provide " best " tests. In the remainder of this chapter and 
in the next we shall consider several lines of approach which have been developed :— 
(a) Relying on 26.18 we may evolve tests based on the likelihood ratio. These will 
give U.M.P. tests if such exist, and in the contrary case will do their best, so to speak, by 
finding the greatest common denominator among the best critical regions. 
(h) We may consider the properties of tests when the sample number n tends to infinity, 
and so obtain tests which are U.M.P. in the limit. Such tests, like maximum likelihood 
estimators, may be employed on the grounds that they are " best " for large n and 
presumably good for small n. 
(c) We may derive a new criterion from the concept of bias in statistical tests, which 
will be explained in the next chapter. 
(d) Recognizing that there is no test which is U.M.P. everywhere, we may seek for 
one which is U.M.P. in the neighbourhood of the true value. The idea behind this approach 
is that it will be more important to detect errors in the neighbourhood of the true value, 
TESTS BASED ON LIKELIHOOD 295 
and that large errors may be left to look after themselves, either because they are infrequent 
or because almost any " reasonable " test will reveal them.* 
(e) When a number of independent parameters are involved, we may abandon the 
attempt to test for each separately and confine our attention to the class of hypotheses for 
which they are functionally related, e.g. by ip =f(0i . ■ - 6r). This reduces our problem 
to the case of a single parameter ip, and we may be able to show that a particular ip0 is the 
best in the sense that it is U.M.P. with respect to all other ^?s, that is, to all other tests 
depending on the single function of the unknown parameters. 
We proceed to consider these approaches. 
Tests Based on Likelihood 
26.35. Suppose that for a given member of a composite hypothesis H0 the joint 
sampling distribution of the variables xt . . . xn has a frequency function pQ (which is, 
of course, the likehhood). Considering the #Js as fixed, we may examine the variation of 
p0 according to variation in the unspecified parameters dx . . . dr which form a set, say 
co. Let pQ (to max.) be the maximum value of p0 for such variation. Similarly, if Q is 
the class of admissible alternatives Hly let px (Q max.) be the maximum of the likelihood 
for variations of all the parameters Qx . . . 6r+s. Write 
X = P.jgJ^) (26.64) 
px (U max.) 
Then a possible criterion for accepting H0 is to take as critical regions those points for which 
X < constant = C, say, . . . . . (26.65) 
where C is determined by relation to a probability level a from the sampling distribution 
of X, which of course is independent of the unknown parameters. In defining X we have 
assumed that the maxima on the right of (26.64) exist, but we can give the equation greater 
generality by taking p0 (o) max.) as the upper bound of values of p0 in the set co where no 
maximum exists ; and so for Q. 
In this form the criterion states that we are to accept H0 if the maximum likelihood 
in the set of permissible jHVs is greater than a specified proportion of that in the set of 
alternatives Hx. In doing so we control the first kind of error in the ordinary way. So 
far as concerns the second kind of error we saw in 26.18 that for H0 simple the criterion 
provided a sort of highest common factor among available tests ; and presumably qualities 
of this kind will be equally useful when H0 is composite. 
The Problem of k Samples 
26.36. We will illustrate the theory of the likelihood tests by discussing a problem 
of considerable practical importance. Suppose we have a sample from each of k normal 
populations, x{j being the ^"th member of the ^th sample. Let 
% be the number in the ith sample ; 
JSf = E (n.t) be the total number of observations ; 
xt be the mean of the ith sample ; 
sf be the variance of the ith. sample. 
* An alternative line would be to concentrate on errors of the second kind for larger deviations, 
on the ground that large errors are more important than small ones. I understand from Dr. B. L. Welch 
that he considered this approach shortly before the war ; the results did not differ very materially from 
those given by requiring optimum properties near the true value in the case he examined, and the 
results were not published. 
296 GENEEAL THEORY OF SIGNIFICANCE-TESTS 
We will consider three different hypotheses H0:— 
(1) H, that all populations are the same and hence have the same unspecified mean and 
unspecified variance. 
(2) El9 that they have the same variance but different unspecified means //,x . . . fik. 
(3) H2i when it is known that they have the same variance, that they have the same means. 
We have for the joint likelihood— 
v^e*p{-jy**-2^+i° 
p ■$ -j- exp <; — > n* 
(2*) T n o?' [ H 
Consider first of all H. We find, for p (Q max.), 
$i = (?£, . . . . . (26.67) 
and for p (ao max.), putting all the /*'s and cr's equal and equating the first partials of log p0 
to zero, 
1 * 
^t =z x0 = — y n.i Xi . . . . . . (26.68) 
i = l 
1 k 
df = ag = _ JT ni i & - f o)2 + «f} .... (26.69) 
Inserting these values in p we find, after a little reduction, 
k / s% \ m 
Similarly it may be shown that 
XB == n I -| )2 (26.70) 
Ahi = P (?)2' (26'71) 
and also that 
s. 
2\,V 
*ffa == H )2 (26.73) 
so 
It will be noticed that XH = XH XH . 
26.37. The function XHz may be related to the correlation ratio ^2. We have 
sl = $ +-]y21ni @i ~ ^>)2> .... (26.74) 
and hence 
N 
= (1 - ??2)2 (26.75) 
The distribution of lK% is thus obtainable directly from the known form for rj2 in samples 
from an uncorrected population. 
THE PROBLEM OF 1c SAMPLES 297 
We also find 
(Ah,)£ = -*- {n (a?P }h (26.76) 
W = 4 {n (4)"' F (26.77) 
o 
The distribution of (A^jx is that of 1 — ?72, where the distribution of rj* is 
A—3 N-k-2 
dF oc (rj2) 2 (1 ~ ^2) 2 drj2 (26.78) 
It can accordingly be tested in this distribution or the related z-form. This is, in fact, 
the criterion used in the analysis of variance for homogeneity tests, and it is interesting to 
remark that the g-test here arises in considering the hypothesis that the various distributions 
parent to the sample values, being already known to have the same variance, have the 
same mean. The other form of hypothesis, H, is that the samples come from the same 
population, and the equality of variance is not part of the data but part of the hypothesis. 
We are not then surprised, or should not be so, to find that the XH criterion leads to a 
different test. 
26.38. The moments of the distribution of XH may be obtained as follows. The 
joint distribution of x<t and S,; IS 
dF oc II (s^ exp 
ndxtndsl . (26.79) 
The distribution of means is independent of that of variances and can be ignored. 
Further, if 
1 
X2 = ~ Sni (xt - £0)2 
or* 
then x2 is also independent of the variances, and we have 
dF oc II fa)"' ~ 3 exp ( -2 ^ \ x/c'2 exp (- $x*) H d*i d%- • • (26-80) 
\ 2<T" I 
Put now 
y = *lhli, (26.81) 
N st 
and note that 
a*z* =#*?,--27 *,*? 
= 2V^(1 --£•%)- .... (26.82) 
Transforming to variables ip and s0i we find 
dF oc IJ y)fT~ (1 — -Sty*) *~ Hd^ sf~* exp f — -^-\ \ ds% 
whence, for the distribution of the ?//s, 
nj-'A 7c-3 
dF oc J7%"^(1 -2ty<) 2 #<*%■ • • • (26-83) 
Now xH^n(^\T (26.84) 
and hence we may find the moments of XH by integrating its powers over the distribution 
298 
GENERAL THEORY OF SIGNIFICANCE-TESTS 
(26.83). Integrals of this kind, known as Dirichlet's, are expressible in terms of gamma 
functions and we find, for the pth moment of 1H about zero, 
NTT1 
ftp (^h) = 
2 
r [(p + 1)N -1) ! 
* 1 2 
77- L 
?5* /ws- 1 
n,2 ri ~^~z— 
When all the n's are equal this reduces to 
ftp (4zr) 
pN 
Jc2 
< 
\" 2 
^* p(N - 1 
r 
n -— 1 
. (26.85) 
. (26.86) 
26.39. For the criterion XBi we start from the distribution 
dF oc 77s/V-3exp 
2(T2 
Z (utsX) \ IIds\ 
and on putting 
5- % 5t 
4 
#*; 
« 
»*4 = ^(i-2^^) 
i = 1, 2 . . . h — 1 
ft-i 
we find, in much the same way as before, 
fc-i 
Further, 
ft— 1 uj — 3 / -^—j 
<zf (ti • . . rQk-i) oc n 'Qpr (i - 2j d 
nk-Z 
lHt 
N 
ft-1 
4m^m 
nk k-l / jy ^ \?i£ 
77 
b£ 
whence we find 
vN (N~k\ r{b + i)%- i 
ivir r 
ftp fej 
\ 2 
* 1 ' 2 
*>"* ( 7L- - 1 
r|tojij^z:*|i %-rp2 
(26.87) 
(26.88) 
(26.89) 
(26.90) 
(26.91) 
26.40. 
function, 
For large % we find, in virtue of the Stirling approximation to the gamma 
(1) for XH 
(2) for XHx 
ftp 
ftp 
1 
(p + 1) 2 
(3) for AHl 
/ip 
(# + 1)~ 
THE PROBLEM OF ft SAMPLES 299 
These limiting forms are the moments of the distributions— 
(1) 
r(fc-i) 
fc-3 
( — log x) 2 
(2) and (3) ^—jf } 
Hence, by the transformation x — e~~**a we see that approximately XH is distributed as 
X2 with v = 2ft—2, and lBx and XB% as %2 with v = ft —1. 
26.41. For small samples Neyman and Pearson have suggested approximating to 
o 
M 
the distributions of XH^J and Xhn by identifying their lower moments with those of the 
form 
dF oc x7^-1 (1 ~~ x)m*-1. 
This possibility has been examined in detail by Nayer (1936) for the hypothesis H± when 
all the n's are equal. The distribution of XR has also been studied by Wilks and Thompson 
(1937a). 
26.42. Modified forms of the above tests have been considered by various authors. 
We may write 
log AHi =4 27 w€ logi?, (26.92) 
s, 
'a 
where, of course. 
1 
o 
In short, si is a weighted mean of the sf and (hHl)M is a weighted geometric mean. Bartlett 
(1937c) has proposed using the degrees of freedom vi (= ^ — 1) instead of nt in these 
equations, that is to say, defines a criterion 
a / O* \ 'l 
fir « 77 ( ^A^ )r (26.93) 
\SviSiJ 
This test is, in the sense defined in the next chapter, unbiassed, whereas that based on 
2 loff u 
XHi is not. Bartlett also suggested as an approximation that — could be regarded 
as distributed as #2 with ft — 1 degrees of freedom, c being given by 
"-i+»(»-1-ijM.iH} (26-M) 
This has recently been reconsidered by Hartley (1940), who showed that it is not very exact 
for large ft and gave a better approximation which can be reduced to tabular form. C£ 
Exercise 27.2. 
• 
300 GENERAL THEORY OF SIGNIFICANCE-TESTS 
Likelihood Criteria for the Linear Hypothesis 
26 A3. We now proceed to consider the application of the likelihood criterion to the 
class of linear hypothesis as defined in 26.30. We have, for the likelihood function, 
p° - (*~vk) Jexp {- £. * <*> - ^ }■ • ■ ■ (26-95) 
Writing S2 = Z (xj — ^)2 we have, for the stationary values of p0 with respect to a and 
the parameters 8 (related to the p's by (26.51)), 
9 , n , S2 
_log^0 = ^-+- 
gg; loS Po = —t JT* (xk - pk) cjk = 0. ... (26.97) 
0 Jz — l 
This last equation is clearly the one we should get if we were seeking to minimise S2 itself 
for variations in the 0?s. Let nS\ be this minimum value. We shall then have, from 
(26.96), 
a2 = Si (26.98) 
The maximum of p in the class Q of admissible hypotheses is then 
piQjnaxl-fg-—) f*. . . . . (26.99) 
logp0 = -2+ ^.= 0 (26.96) 
imilarly the maximum of p in the class co for which 6t . . . dr are fixed and the other 
s 0's vary, is found to be 
/ 1 \n Jit 
p{m™x-) = {v(sTTWv&))e'i> ■ ■ • (26-100) 
where n (Si + S%) is the minimum of S2 under the conditions that 6X . . . 0r are fixed. 
Thus we find for the likelihood ratio X 
*. 1 
Xn = — —T, (26.101) 
I) 
1 + 
S 
or, if more convenient, we may use the function 
to provide a criterion. 
Now we make the transformation (26.54) and show that the values Sa and Sb as we 
have defined them here have, in fact, the values given by (26.56) and (26.59). We have, 
from (26.54), 
n f r+s n 
S2 = Z(xj-~H)2=£l2^cjkEj+ JT Cjkyj 
n n 
\. 
j = l j—r+s-\-l 
= £ (Z <* E,)* + £ (Z c,k yt) 
&=1 ft = l 
n n 
£(£cskEi)*+ £ " 
LIKELIHOOD CRITERIA FOR LINEAR HYPOTHESIS 301 
Since n Si is the minimum of S2, for all variations of the 0's and E and y are independent 
of the 0's, we must have 
Also, since nS\ is the minimum of S2 when the values dx ... 6r are fixed, it is seen to have 
the value given in (26.59). 
We have also 
S* =:nSl + nSl (26.102) 
where «fi> = JT ( £ cjk B, 
and the frequency function of jE's and 2/'s is given by 
(26.103) 
f(E1... ErJrS, yr+s+1 . . . yn) oc exp J - ^ (S* + SI) 
Now nSl is the sum of squares of n — r — s normal variates, and hence 
/ (Sa) oc Si—'-' exp ( - ^). . . . (26.104) 
Hence, since the E's are independent of the s/'s, and since 8% depends only on the y% 
f{Sa, Ex . . . Er+S) oc 8%-*-*-i exp | - 1L (SI + S$) j. . (26.105) 
We have seen, in effect, that n S2b is the minimum value of 8%. It depends on Ex . . . Er 
and hence is independent of S% and is distributed as 
/ <n Si? 
Thus we have 
/ (8a, Sb) oc Sjj-r-,-1 Sr--v exp f _ JL (flf» + 5?) 1. . . (26.106) 
Putting now Z = Sb/8a, we find 
/(Z) oc Zr+"-[ (1 +JZfa)"'Ua " .... (26.107) 
which may be reduced to Fisher's form by putting 
z = J log 8^n-^J ~~ S) - log Z + i log n ^ r "1 . . (26.108) 
raj " - - f 
We have thus reduced the test of the linear hypothesis to the z-test and it is seen that 
several of the tests introduced in Chapter 21 can be justified on the likelihood criterion. 
These include the " Student " test for one mean, the extended form for the difference of 
two means, and the test for the ratio of variances. Certain other tests in which the 
^-distribution (which, of course, reduces to the ^-distribution for vx = 1) appears—such as 
that of the correlation ratio, the multiple correlation coefficient and regression coefficients 
—also depend on the linear hypotheses, and in the light of the theory here presented are 
seen to be different aspects of the same thing, at least so far as the testing of hypotheses 
is concerned. 
26.44. We will indicate briefly, without going into the complicated mathematics 
involved, some interesting results obtained by P. C. Tang (1938) and P. L. Hsu (19416) 
concerning the power of the z-test as applied to linear hypotheses. 
302 GENERAL THEORY OF SIGNIFICANCE-TESTS 
The functions 8% and 81, as we have seen, are distributed independently in the 
^2-form5 and their ratio accordingly in Fisher's form. From this viewpoint the test of 
the linear hypothesis is a generalisation of the test of homogeneity in the analysis of 
variance. Tang considers the distribution of 
E* = ~^ = 1 - An . . . . (26.109) 
and the variation for errors of the second kind, namely, when the values 6X . . . Br are 
different from the specified values. He shows that the power of the test depends, not on 
individual alternative values, but on a single function of the #'s. He also obtains the 
power function and tabulates it. 
Hsu then considers other possible tests which are based on this single function and 
shows that in this class of test the z-test or the equivalent i?2-test is the uniformly most 
powerful. 
26.45. For large samples, when maximum likelihood estimators of the parameters 
exist, the distribution of -— 2 log X is that of x2 wftfa s degrees of freedom. For the 
distribution may then be written (see 17.46)— 
dF = A exp 1 - - E gjk (dj - 0,) (6k - Bk) \dd1 . . . ddr+s 
so that p (Q max.) = A. . . . . (26.110) 
If Qx . . . 6r are fixed the likelihood becomes 
p = A exp i - | Egjk z) zk - \%\\, 
r 
where %l = ]£ 9jk (fy - fy) ih ~ 6k) .... (26.111) 
j,k = l 
and z'j is given by Sj — &0- — L^ where Lj is a Unear function of the r specified parameters. 
Thus— 
p (co max.) = A0 e~^«5 ..... (26.112) 
where A0 is the value of A when dj takes its true value dj0. Thus, when H0 is true, 
X = e~**8 (26.113) 
But the characteristic function of %f} ( = — 2 log X) is 
p0 eitrz d6x . . . d6r+s 
i 
= A \ exp J - | E gjk zj z'k + xl (it - J) I d61 . . . d0r+s 
oc .—1 s (26.114) 
(1 — 2it)z 
This is the characteristic function of a quantity distributed as x2 with s degrees of 
freedom, and hence the result follows. 
26.46. In concluding this chapter we may mention briefly a question which 
frequently presents itself when statistical hypotheses are being tested in practice. Our tests 
are based on the observed values obtained in the sampling process, and in order to apply 
NOTES AND REFERENCES 303 
them we require no prior knowledge of the parameters to which they relate. They can 
be used in a state of complete ignorance about the parameters. But suppose some 
information is already available ; or suppose that we attach varying degrees of importance to the 
avoidance of particular types of error. How far are the tests developed in this chapter to 
be modified ? 
26.47. Consider, for example, the situation which has already been mentioned in 
connection with the theory of estimation, of the chemist who is assaying the strength of 
a particular drug. If the drug has harmful effects in large quantities it may be much more 
important for him to detect cases in which the true strength exceeds his hypothetical value 
than when the true strength is deficient. Again, the manufacturer of a " guaranteed " 
product is usually much more concerned with ensuring that it does not fall below the 
guaranteed standard than that it exceeds such standard. In such circumstances we may 
be particularly interested in " one-sided " tests of the type £ < f0, and as we have seen, 
there more often occur U.M.P. tests for this class of alternative than in the case when f 
can have any value. We might, therefore, be quite ready to accept such a test, knowing 
quite well that it may be insensitive in part of the range of the unknown parameter, merely 
because errors in that range are relatively unimportant. 
Similarly we might be willing to accept a test which had a poor discriminatory power 
in part of the range but compensating advantages elsewhere, simply because we know 
beforehand that values of the parameter rarely or never fall into that particular part of 
the range. This is equivalent to prior knowledge of the distribution of the values 
determining the alternative hypotheses. 
26.48. It is difficult to reduce rather vague prior knowledge of a parameter to 
numerical form, and hence to extend our theory with great precision to cover these cases ; but in 
practice it is desirable to consider, before adopting a test, whether any prior knowledge is 
available, or whether our interests centre on particular parts of the range. If they do, we 
may consider the behaviour of power functions of the possible tests at our disposal and 
examine which is the more powerful test in the particular part of the range which interests 
us most. The mere fact that the theory developed in this and the succeeding chapter 
makes no assumptions about the prior probabilities of admissible alternatives does not 
mean that we should be acting sensibly in ignoring any prior information which may be 
at hand when applying the theory, or that we need feel compelled to apply tests with 
optimum properties in regions where we know the unknown parameter-values will not fall. 
NOTES AND REFERENCES 
The theory of this chapter is very largely due to Neyman and E. S. Pearson, whose 
treatment has been closely followed. In their first contribution to the subject (1928) the 
likelihood criterion was developed, the theory of first and second kind of errors and power 
of tests being given in 1933. For the theory of unbiassed tests, see the papers of 1936 and 
1938. In the last few years the literature has grown considerably. 
Feller (1938) has shown that similar regions only exist in rather exceptional 
circumstances and that the theory of composite hypotheses is incomplete. Tables of certain 
power functions and distributions associated with likelihood tests are given by Mahalanobis 
(1933), Neyman and Tokarska (19366), Wilks and Thompson (1937a), P. C. Tang (1938), 
304 GENERAL THEORY OP SIGNIFICANCE-TESTS 
David (1939), Nayer (1936), and in Tables for Statisticians, Part II (Tables 35-37). See 
also Mahalanobis (1933). 
For tests based on the likelihood ratio, seeNeyman and Pearson (1928, 1931a, 19315), 
Pearson and Wilks (19336), Wilks (1935a), Nayer (1936), Welch (1936a), R. W. Jackson 
(1936), Sukhatme (19366), Bartlett (1937c), Wilks and Thompson (1937a), Wilks (1938a), 
Bishop (1939), G. W. Brown (1939), Mood (1939), Hartley (1940), Wald and Brookner 
(19416). 
For the general theory, see also Welch (1935), Kolodzieczyk (1935), Neyinan (19356, 
19376, 19386), Daly (1940), Pitman (19396), Wald (1939a, 1941a), Wolfowitz (1942), E. S. 
Pearson (1941, 1942a), Dantzig (1940), P. L. Hsu (19416), Simaika (1941), MacStewart 
(1941), Scheffe (1942a, 1943). 
EXERCISES 
26.1. Examine the following argument: To accept H when it is false is equivalent 
to rejecting not-fl" when not-J? is true. Hence, if K = not-ff, to commit an error of the 
second kind for H is to commit an error of the first kind for K ; and thus there is 
no distinction between the first and second kinds of error. 
26.2. For the distribution 
dF = p e^{x~y) dx, x > y 
= 0 x < y 
show that for a hypothesis H0 that ft = /?0, y — y0 and an alternative H1 that j3 ^ /?x, 
y = 7i, the best critical region is the region W0 where p0 = 0, together with the region 
TTT 1 £ 11 
W + defined by 
* < -R »-\ y£i - y<A - -log k + log^1 L 
provided that the admissible hypothesis is restricted by the conditions yx <y(), /5»l ;>. (i0. 
Hence show that a U.M.P. test exists in such circumstances. 
(Neyman and Pearson, 1936a. This shows that a U.M.P. test can exist for more than one unknown 
parameter.) 
26.3. If the distribution function of xx . . . xn is given by 
dF = — - - ' ' ^ 
n 
exp 1 _ w (Z xi ~nyY ~*2>; dx^ ■ ■ ■ dx»' 
y, a > 0, — oo <;t-1 . . . xn -:: oo 
show that the frequency function may be put in the form 
, / n2 (x - v)2 \ / n 
f cc exp ^ - ^A_JIL j exp ^ - £2^ 
and hence that x is a " shared " estimator sufficient for y and a. Show further that the 
best critical regions for y0, <r0 differ according as o*2 > of„ a* < of, or a = a0, and that 
their boundaries depend on y. Hence no U.M.P. test exists for admissible alternatives 
<r> 0. 
(Neyman and Pearson, 1936a.) 
EXERCISES 
305 
26.4. In the previous exercise put a = y and consider the class of hypothesis y > 0. 
Show that there are different best critical regions according as y > y0, y < y0 and that 
their boundaries depend on y. Hence there is no tl.M.P. test, but x is sufficient for y. 
(Neyman and Pearson, 1936a.) 
26.5. In samples from a normal population, show that the probability of accepting 
the hypothesis that the mean p < ^0 when, in fact, it is false and /u = /u1> /i0—that is, 
the probability of an error of the second kind—is 
vn~l exp ( — —- )—_— e~*u dudv 
n 
tj 2*tn-vr(&)% 
where 
a 
O" 11 
and t is the value of - -™ corresponding to the significance level 1 — a for the control 
of errors of the first kind. 
(Neyman and Tokarska, 19366.) 
26.6. In six samples of six members each the following values were obtained- 
Sample. 
1 
2 
3 
5 
Mean. 
8433 
8200 
7933 
8120 
7971 
8263 
24,722 
94,133 
149,733 
45,037 
88,480 
49,921 
with si = 104,588, tfj* - 75,338, 
42 
Show that XXip ~- 0-8508 and XHu ■■= 0-6219. The 5-per-cent. levels are respectively 
0-67 and 0-54, so that there is no evidence of heterogeneity. 
(Pearson, appendix to papers by Wilsdon, 1934). 
26.7. Verify that the likelihood ratio leads to " Student's " test for an unknown 
mean in normal samples, to the use of Fisher's z in testing the equality of two variances, 
and to the i-teat for the diiference of two means in normal populations with the same 
variance. 
26.8. If samples nl . . . nk are drawn from the populations 
dF 
dx, 
1 ... k 
°i \ ai 
use the likelihood ratio to test the hypothesis H0 that the populations are identical, 
showing that 
k 
TN _ 3 — H $i ~" Xil)'H 77 in. nv 
lJ/0 — r = l 
(X0 Xti) 
N 
l0 
A.S.—VOL. II. 
306 
GENERAL THEORY OF SIGNIFICANCE-TESTS 
where x, is the mean of the ith sample, x,tl is the smallest member of that sample, x0 is the 
mean of all samples together and xtl is the smallest value in all samples together. 
Show that the distribution of % and \ is 
f = JLV— ^ 
^ "" o*n* \ (% - 2)! 
and hence the moments of L0 are 
l?>-s exp J - n^ + *«> 
a 
r 
ftp 
I *v*i J. "f" 
% n r (^ — 1) 
If Hx is the hypothesis that the populations have the same a but any possible different 
/}'s, show that 
1 "#! ^ J 
where Z is the weighted mean of the Z's, and that 
/■ 
r N*>r(N-lc) 
r u 
1 + l!!!*Yl 
pn* 
If i?2 is the hypothesis that the populations, being known to have identical <x's, have 
the same /?, show that the distribution of 
1 T 
(Sukhatme, 193 66). 
26.9. In the notation of 26.36 show that, if H is true, the criteria XHx and AJ/a are 
distributed independently. 
(Neyman and Pearson, 19316). 
CHAPTER 27 
GENERAL THEORY OF SIGNIFICANCE-TESTS—(2) 
.Bias in Statistical Tests 
27.1. In considering the problem of estimation by confidence intervals in Chapter 19 
wo had occasion to remark on the rather arbitrary nature of determining the interval so 
that both inequalities 0X < 0 and 0 < 02 had an equal chance \a of fulfilment. A point 
of a similar nature arises in the testing of hypotheses, particularly when an asymmetrical 
sampling distribution for the criterion is concerned. Consider, for instance, the testing 
of the hypothesis that in a normal sample of n members the standard deviation a has an 
assigned value a0 irrespective of the mean p. As we have seen in Example 26.3, there is 
no U.M.P. test for all <r > 0, though there is one for a > a0 and another for a < <r0. In 
choosing a test to cover the whole range a > 0 we have, therefore, a certain freedom of 
choice, since there exists no " best " test as we have previously defined the term. A 
common test in practical use is to take the sample variance s2 and accept the hypothesis 
a -■■■■■■ <r{) if and only if 
<9l "^ S ^5. So 9 . . . . . [A! I . 1J 
where sf and si are determined from the distribution of s2, namely 
dF oc *»-» exp ( - J^) d(s*), .... (27,2) 
such that 
dF =£ (1 - a). . .'" . . (27.3) 
. 
«s 
In short, sf and si are chosen so as to cut off equal " tail " areas of the distribution. This 
procedure will, of course, control errors of the first kind ; but so equally well would the 
selection of sj and si so that 
Jo 
dF = | - ax (27.4) 
o 
/»0t) 
f = i - a„ (27.5) 
8 
it 
provided that ocx | oca ~ oc. Thus we have an infinite number of regions which will control 
errors of the first kind. It is natural to seek for some criterion which will distinguish one 
as better than the others, recognizing that no U.M.P. test exists. 
27.2. Such a criterion arises naturally from the following consideration. In the 
example given, with. ax = a3 = fa, let us calculate the power of the test for different values 
of a. This can readily be done from the distributions of type (27.2) by means of the 
incomplete /'-function or the equivalent x2 integral. For any given a we have to find 
P Cs'i, 4 | cr) = I + 
J o 
r> 00 
dF, .... (27.6) 
307 
308 
GENERAL THEORY OF SIGNIFICANCE-TESTS 
where 
dF = 
In—I 
r 
n — 1 
o2\?i-3 ns- /o2 
. (27.7) 
Fig. 27.1, adapted from Neyman and Pearson (1936), shows the relation between 
the power function j8 and a2 for <xx = oca = 0-49, n = 3, the rejection level being 0-02. 
10 r 
a 
0 
20 
Fig. 27.1. 
, 0-5 1'0 1-5 
Oz in Sampled Population (in units ofol). 
-Power Curve in Samples of 3 for a2 from a Normal Population (see text). 
We see that for a > 1 = a0 the power increases, and so also for a < \ = la{). But 
between |(r0 and <70 the power is less than 0-02, i.e. less than 1 — a. Hence for such values 
the chance of an error of the second kind, namely, the acceptance of a false hypothesis, 
would be greater than the chance of an error of the first kind, namely, the rejection of 
a true hypothesis. 
27.3. Whether this is felt to be anomalous depends on the relative importance of 
the two kinds of error in particular cases ; but, other things being equal, it may be felt 
more important to avoid the second kind than the first, and not to have a greater probability 
of accepting the hypothesis when it is false than of rejecting it when it is true. This, at an y 
rate, is the basis of the criterion which we proceed to discuss, namely, that the critical region 
w should be chosen so that P (E e w) is a minimum when the hypothesis tested is true. 
Consider then the case when H0 ascribes to a parameter 0 the value 0O, and the 
admissible alternatives ascribe other values to 0 but do not differ from H0 in other respects. We 
shall say that w is an unbiassed critical region if, and only if, 
p0dx = P (E sw 0O) = 1 
a. 
and for any other 0, say 6' 
w 
(27.8) 
p (6') dx = P(Eew\B')>\ 
W 
a. 
(27.9) 
UNBIASSED REGIONS OF TYPE A 309 
Equation (27.8) expresses the usual control of errors of the first kind and (27.9) the 
minimising property of w. If a region is not unbiassed it will be said to be biassed. 
27.4. In certain cases there will exist among the unbiassed regions a w0 such that 
p(d')dx>'\ p(df)dx (27.10) 
wD 
w 
for all admissible 0'. Such a region may be called the best unbiassed critical region and 
the test based on it the uniformly most powerful unbiassed test, or briefly the U.M.P.U. 
test. It minimises the risk of errors of the second kind among the class of unbiassed tests. 
As we shall see presently, U.M.P.U. tests do in fact exist in certain cases. 
The use of the word " unbiassed " in this connection is rather arbitrary and is not to 
be interpreted as meaning that biassed tests will give systematically wrong results, or that 
unbiassed tests are based on unbiassed estimators. Fortunately the different uses of 
the term " bias " usually occur in different contexts and confusion is infrequent. 
Unbiassed Regions of Type A 
27.5. Following Neyman and Pearson, we now define an unbiassed critical region 
of Type A as one for which 
p0dx 
a, 
w 
and 
30 
a2 
p dx 
w 
Je=0„ 
bo 
J W 
0. 
is a maximum. 
. (27.11) 
. (27.12) 
. (27.13) 
0=0O 
We shall, as usual, assume that the differential coefficients exist and shall also assume that 
differentiation may be carried out under the integral sign, so that we have for all w, 
--- p dx = I -J? dx = p' dx, say, . . . (27.14) 
30 J w J w ™ J w 
and similarly for the second differential coefficient which we denote by p". 
The first condition (27.11) controls errors of the first kind; the second makes the 
region ir locally unbiassed ; the third, (27.115), implies that as 6 departs from 0O the power 
function increases more rapidly than for any other unbiassed critical region of the same 
size. Tims in the neighJjourhood of 0o the test may be said to be better than others of the 
unbiassed type. It may not be better for larger values of | 0 — 0O |, but the Type A tests 
are based on the supposition that it is more important to detect small errors of the second 
kind than to minimise the risk of large errors, which will probably be detected in any case. 
27.6. The regions of Type A may be found by the use of the following theorem : 
the region w0 is an unbiassed critical region of Type A if, within iv0, 
V" (Go) > hp' (0o) + k*P (So), .... (27.15) 
and outside w0) 
p" (0O) < klP' (0O) +hp (0O), .... (27.16) 
BO 
where 
p' (0.) = 
etc., 
6 = 0, 
and kx, k2 are chosen so as to satisfy (27.12) and (27.13). 
310 
GENERAL THEORY OF SIGNIFICANCE-TESTS 
Suppose that F0 . . . Fm are functions of xx . . . xn and that 
F-dx = cl9 a constant. 
(27.17) 
w 
Let w0 be a region such that inside it 
F0 > £ fy J, . 
and outside it 
5=1 
F0 <EfaFd, . 
. (27.18) 
. (27.19) 
where the it's are constants chosen so as to satisfy (27.17). Then for any w for which 
(27.17) is valid 
F0dx <\ F0dx (27.20) 
V J Wo 
In fact, let ww0 be the common part, if any, of w and w0. As both to and w0 satisfy (27.17), 
we have 
j. 
Now 
W— WWa 
F0 dx — 
J Wn J I 
F3.dx 
F0 dx 
J Wo—WWa 
CLX. 
(27.21) 
F 0 ra — i j o 
J. 
.Fn ^^ 
>0 
2; (L j?,) do? 
Wfi—WWo 
E (kj Fj) dx 
W—WWq 
in virtue of (27.21). 
In our present case take F0 as p" (0O) and Fu F2 as p' (0O), p (0O) respectively. Then 
(27.20) is true, and hence (27.13) is satisfied if (27.18) and (27.19) are true ; and these will 
be found to reduce to conditions (27.15) and (27.16). The theorem follows. 
27.7. If (27.14) holds, and if there exists a sufficient estimator t for 0, then the 
Type A region is bounded by surfaces of constant t. For then we have 
p(d) = p1 (t, 0) p% (x) (27.22) 
and hence, from (27.15), on substitution, 
p[ (t, 0O) > lxpx {t, 0O) + hp, (t, 0O) 
within w0, and conversely outside it. The equality must hold on the boundary, which 
is equivalent to the theorem. 
27.8 • Writing 
4 = 
V 
we have 
dd 
i! 
logp 
J 0=6, 
logp 
J6=e( 
(27.23) 
(27.24) 
p' (Oo)^cf>p(d0) 
#"(0o) = (f + <f>2)p(d0) 
UNBIASSED REGIONS OF TYPE A i 311 
and hence the inequality (27.15) reduces to 
<f>' + <£2 > Id <f> + k2 . . . . . (27.25) 
within w0i wherever p (0O) does not vanish ; and conversely outside w0. 
We may distinguish three special cases :— 
(a) If <f>' is a function of </>, say F (</>), we have— 
F (cj>) +- <p > kx <f> + k2, (27.26) 
and the Type A region is bounded by the surfaces 
<f). = c$ and j = 1 . . . m, . . . . (27.27) 
where m is the number of roots of (27.26). In this case, as we saw in 17.30, there exists 
a sufficient estimator. It follows that w0 is defined by inequalities of the type 
and we may, as in 26.24, use the <£'s as new co-ordinates and calculate the size of a region 
from their distribution functions. 
(b) As a simple case of (a), if 
f = A + B<f> . . . . . . . (27.28) 
we find, for (27.26), 
c/>2 - jfc8 <f> - h = 0, (27.29) 
and the limits of <f> are given by the two roots of this quadratic. 
(c) If j>' cannot be expressed as a function of cj> which does not involve the x's explicitly, 
we shall have 
</>' > &a + hl(j> - </>2 (27.30) 
In this case, considering <j> and </>' as two co-ordinates of a point in a plane, we see that 
the region for which (27.30) is true is the one " above " the parabola <£' = &2 + fcx <\> — <j)2, 
and that /c1; k2 are determined by 
d<j> p (cf>, <j>')d4' = 1 - a . . . . (27.31) 
J —oq J <£' 
.00 
/"00 /»«) 
cj> d<j> p (c/>, </>') dfi - 0 (27.32) 
In this instance we can reduce the problem to two dimensions by using two new co-ordinates 
</>, <f>'. 
Example 27.1 
Consider the normal distribution 
dF = ~~77S~^ exp {— | (x - /*)a } dx. 
y\mt) 
To apply the foregoing theory with complete rigour we have to show that (27.14) is true. 
We shall assume that this is so, referring the reader for a formal proof to Neyman and 
Pearson (1936). 
We have, then, with 0 = /u, 
log p (p) = - I n log (2tz) - -I E {x - //,)2 
</> = E (x — /i0), </>' = — n, 
and hence this case reduces to that of (27.28). We write 
<j> -= n (x — ^0), 
312 
GENERAL THEORY OF SIGNIFICANCE-TESTS 
and can clearly use x instead of <f> as a co-ordinate, which confirms the result of 27.7 since 
x is sufficient for /u. 
It follows that the unbiassed region of Type A is given by 
X < Xl3 X > X* 
where p (x) dx = oc 
J #1 
and 
p (x) (x — pt) dx = 0. 
«i 
Now if H0 is true, that is if ^ = fi0, x is distributed in the form 
Hence and the Type A region is defined as being outside the range 
fi0 - 
where X is given by 
< X < /^o + 
v^ 
V^ 
Ja 
1 
e~^2 da; = J (1 — oc). 
In this case the Type A test leads to the usual test based on equal tail areas, 
same test follows from the likelihood ratio, as the reader can verify for himself. 
Example 27.2 
If the distribution is normal with zero mean and variance a2, and H0 is that o 
we find 
JL X.L\> 
Co* 
* 
— Z(x2) — al\ = — (v — ri), say. 
cr0 
(r0 I n 
This also satisfies (27.28), and the Type A region will be defined by 
1 
v2 < a = _ E x2, or v < v1: 
ao 
where 
and 
I p (v) dv = a 
J vx 
p(v)(v 
Jt'l 
n) dv = 0. 
Here p («), the frequency function of the second moment, is 
1 
P W 2** r (in) 
and we find, for the second equation, 
vUn~2) e-fr, dv 
"Ua 
vin e $v dv — 71 
Vl 
-u2 
V 
*n-le-ivdv = 0. 
V, 
Integrating the first member by parts, v being one part, we are left with 
Avx 
or 
vf e~^ = v\n e-^ 
UNBIASSED REGIONS OF TYPE A 
313 
This has to be solved in conjunction with 
1 
vUn~2) e-iv fa = a< 
h 2*w r m 
The numerical solution can be carried out by successive approximation or graphically. 
In this connection Fig. 27.2 is of interest. It shows, for samples of two and a = 0-98, 
the graphs of the power function for the ordinary test with equal tail areas, in addition to 
the power functions for the Type A test, the U.M.P. test with a > a0 and the U.M.P. test 
with a < (T0. 
Evidently, for o* > a0 the best critical region (2) has the greatest power (as it must 
have), and for a < a0 the best region (1) has the greatest power. The test based on equal 
12 
10 
08 
K 
ex. 
B.C.R.W / 
! /EQUAL 
Oh 
02 
0 
typeA 
B.CR.C2) 
.ttmmZi^Z ""i i— 
0 
20 
06 1-0 1-5 
Oz (in units ofo%). 
Fig. 27.2.—Power Curves of Four Different Tests of the Variance in Normal Samples of 2 (see text). 
tail areas has a greater power than the Type A test for a > a0 but a lower power for a < aQ7 
besides being biassed, as we have seen. 
As n becomes larger the same effects persist, but the Type A and the " equal tails " 
tests become closer together in power. "For samples of 20 or more there seems to be no 
serious loss in using the latter since the range of bias and its magnitude are then very small. 
If, of course, we knew in practice that a > a0 we should use the U.M.P. test, and cases may 
arise, even when such knowledge is lacking, where cc one-sided " hypotheses of this kind 
are all that concern us. 
Invariance Theorem for Type A Regions 
27.9. It is important to show that the regions selected on the basis of Type A criteria 
conform to corresponding criteria if some other function £ (0) is used instead of 6 itself. 
In Example 27.2, for instance, where we took 0 to be the standard deviation a, should we 
314 GENERAL THEORY OF SIGNIFICANCE-TESTS 
have obtained the same regions if we had taken d to be the variance c;2 ? The answer is 
affirmative under certain general conditions, as we should expect from the relationship 
with sufficient estimators. 
Suppose we have a new parameter £, given by 
0 = 6)0+/(C) = wa (27*33) 
where /(0) = 0. Then if p (ip) satisfies (27.14) and the similar equation in second 
differentials, if ip is monotonically increasing and 
> 0, then the region based on f is an 
o 
dip 
unbiassed critical region if that based on 0 is so. It is sufficient to show that (27.15) 
and (27.16) are satisfied for £. Now 
0 - ip (C), V (0) = 0O: 
Thus 
dip 
= ip' ?£ 0. 
Jo 
d2^ 
3? 
= V>" (sa7)- (27.34) 
o 
Pc (jB I e0) = p; (Jff I v (o)) 
= ft (J£ | fl0) Y>'> 
and ft (J5 | V (0) ) = ft (JE7 | 0O) y/» + ft (E | 0O) <//'. 
Solving these for ft and ft and substituting in (27.15) and (27.16), we find 
ft (S | ip (0) ) > W ft (JE7 | v (0) ) + fc,' ft (E | ? (0) ) . . . (27.35) 
within w and the contrary outside, where 
Jc\ = kl$Ll±y"t ho = Ic f'2 (27.36) 
The result follows. 
Regions of Type Ax 
27.10. The regions of Type A are determined so that tests based on them are 
U.M.P.U. in the neighbourhood of 0O. We now consider a region, said to be of Type Al9 
which is U.M.P.U. everywhere, i.e. which obeys (27.11) and (27.12) but has, in place of 
(27.13), 
\ pdx> 1 pdx . . . . . (27.37) 
J w0 J w 
for every admissible 8 and every w satisfying the other two conditions. 
It is conceivable that (27.37) does not entail the existence of a U.M.P.U. test, for there 
might be an unbiassed region of size 1 - a for which the derivative of [pdx did not exist 
at 0 = 0O but which nevertheless gave a more powerful test. This refinement, however, 
need not detain us. 
27.11. If W+ represents the sample-space where the density is not zero, if 
<f>' = A + J3<£, 
and if cj> (0O) does not vanish identically in W + then the unbiassed critical region of Type A 
is necessarily of Type Ax. 
Let'w0 be the Type A region, which is determined ex hypothesi by two numbers c% 
and c25 such that— 
<a < <f>Q < c2 outside w0. 
REGIONS OF TYPE A, 315 
We have to show that 
I, 
p dx 
w 
p dx > 
for all admissible 0 and any w for which 
p dx = 1 - a, . . . . . (27.38) 
with the consequence that 
dx = 0 (27.39) 
i 
2>' 
J 10 
Since <// = J. + i?^ we have, solving this equation as a linear differential equation 
of the first degree, 
<f>^\\Aex^(-[Bdejde + T\exp['Bde. . . (27.40) 
The reader may verify that this is a solution, and since it contains the arbitrary constant 
T it is the most general solution. It follows that we may write 
logp = P (fl) + TQ (6) +f(x), say, . . . . (27.41) 
where P and Q do not depend upon x. We then have—primes denoting differentiation with 
respect to 0 and the suffix 0 relating to d0— 
<£„ = P'0+TQ'Q (27.42) 
We note that $,', cannot be zero, for if it were we should have 
0 = i </>0.:Po dx = P0 I pQ dx = P0, 
which would imply that <j>0 was identically zero. 
In virtue of the lemma of 27.6, the proposition will be proved if we can show that 
for fixed 0 and 0Q there are two numhers a and b, depending on 6 and 60 but not on the 
i**X such that 
V > Po Wo + b) inside w0 (27.43) 
and the contrary outside iv0. Putting the values of p and cf>0 in this expression, we have 
to show that a and /; can be found such that, inside w0i 
exp{ P (0) + TQ (0) +f(x) } > exp{P (0O) + TQ (fl0) + f (x)} {aP'Q + aTQ'Q + '&} 
or, writing r - P (0) - P (0O), q = Q @) - Q (0o), such that 
exp (r + qT) > aQ0T + aP'0 + b 
> ax T + &i> saJ- • * • •' (27.44) 
Here q cannot be zero, for if it were Q (d) would be equal to Q (0O) and, integrating the 
frequency functions over W, we should find r = 0. The alternative hypothesis would 
not then differ essentially from H0. 
Consider at the outset the case when cx and c2 are different. From (27.42) we see 
that </►« depends only on T so far as variation in x is concerned, and that 
if <£0 = Cl T = ^-=^ = JPx (say) . . . (27.45) 
if fr = c2 r = ^-=r^ = Tt (say). . . . (27.46) 
^5o 
316 GENERAL THEORY OP SIGNIFICANCE-TESTS 
T1 and T2 are different. Choose ax and bx so as to satisfy 
a^+b^^t\ • • • • ' <27"47> 
ax T%+bx = <?+*T* J 
Then (27.44) is satisfied at the boundary points and we have merely to prove that 
<h. < h < c2 implies er+«T < ax T' + bx \ , . . (27.48) 
<£0 < d and <£„ > c2 imply er+(2i > ax 1 + ox 
This follows from the fact that 
y = er+zT _axT__bi 
has only one. minimum, between Tx and Tz, as may be seen by differentiating it twice, for 
the second derivative is positive and hence the first is a monotonically increasing function. 
But y vanishes at 2\ and T2 and hence is negative between those values and positive 
outside them. 
Finally, if cx and c2 are equal, say to c, we choose ax and bx so as to satisfy 
p; + q-0 t0 = c^i 
jef+a*'. __ai ^qL (27.49) 
<f+«r. _ai>r0 -6, =oJ 
It will be found that y has a minimum at T = T0 and vanishes there. It follows that in 
the region wQ complementary to wQ, where 8Q — c, we have 
e+& ^a,T + bl9 
and thus in w0 where <f>0 < c or c < </>0 the left-hand side must be less than the right- 
hand side. The demonstration is complete. 
Example 27.3 
Consider again the data of Example 27.2. We have already seen that for this 
distribution <£' = A<f> + B, so that the regions of Type A are also of Type Ax. Among 
unbiassed tests of the hypothesis this is the uniformly most powerful test. 
Composite Hypotheses : Regions of Type B 
27.12. We now consider the extension of the foregoing results to the case when 
HQ is composite. For simplicity we will suppose that there are two parameters 6l and 0a,. 
-H0 specifying 6t as say 01O and leaving 82 undetermined. Then a region ?/»0 will be said 
to be of Type B if 
(a) 
w. 
P (0io, 0%) dx = 1 — a for all admissible 0a ; (27.5(>> 
(6) p (6l9 0a) dx may be differentiated twice with respect to 01 under the integral 
J W0 
(c) 
sign; 
a 
1 J Wo _ 
90 
(d) For any other region w satisfying (27.50) 
= 0. (27.51)- 
r a2 r 7 i r d2 c i 
— # dx > ~\ p dx . . . (27.52) 
REGIONS OF TYPE C 
317 
These conditions are obvious generalisations of those defining Type A. Putting now 
A d 
?i = — 
<f> 
*jk 
39, 
do* 
logp 
? 
9 
<t> 
kji 
h = 1, 2. 
. (27.53) 
. (27.54) 
we state that the Type B region will exist and may be found if cf>1 and <f>2 are algebraically 
independent, if 
</>i2 = -B0 + J3X </>! + J5a </>2 V .... (27.55) 
<£22 = C0 + 02 </>2^ 
and if the law of distribution of cj>2 is uniquely determined by its moments. We omit the 
proof of this theorem, for which see Neyman (19356). 
Simple .Hypotheses with Two Parameters : Regions of Type C 
27.13. The extension of the foregoing theory to the case of a simple hypothesis 
specifying several parameters presents some new features. Again to simplify the discussion 
we shall consider two parameters, 0X and 02- 
Consider the power function in the neighbourhood of Qx = 02 = 0 which we will suppose 
to be the values specified by H0. Writing for the function 
P (0l3 0a | w) = p (6U 03) dx 
J w 
"dp 
30, 
at^o2^o 
a2 p 
-h 
§jk-> 
/ = 1, 2 
y, /c = i5 2 
(27.56) 
(27.57) 
(27.58) 
30y SflfcJo^ o,-=o 
we have, assuming an expansion by Taylor's theorem, 
fi (0l9 0Z | w) - p (0, 0 | m;) + 0l /?! (w) + 0« & (w) 
+ I {0? /»ii M + 20x 0, /iia (w) f 0§ /?aa (w) } + ... . 
To extend the idea of unbiassed tests to such a case we require in the first place 
p, (w) =-■ 0 1 
/».('«) -0/ 
Secondly, there will be a minimum at 0t = 0a = 0 if 
A = j8fa - 0n /3aa < 0 .... 
and plx, Pan >• 0. . 
If these conditions are satisfied the power function for small values of 6X and 02 is effectively 
P (0l5 0a | w) = 1 - a + 1 {0* plt + 20x 0. pl% + e\ /SM} . . (27.63) 
We may represent this diagrammatically as in Fig. 27.3, which shows one of the ellipses 
for which the power function is constant. 
Since the hypothesis H0 is that 0L = 0a = 0, we may speak of the value 0X as the " error 
in 0t ", and similarly for 02 ; and if, as in the case depicted, the co-ordinate axes are not 
the same as the principal axes of the ellipse it is clear that for values of 6X which are not 
zero, errors of positive and negative sign in 0a are not equal. From this viewpoint it may 
(27.59) 
(27.60) 
(27.61) 
(27.62) 
318 
GENERAL THEORY OF SIGNIFICANCE TESTS 
be said that the minimisation of the power function does not control positive or negative 
errors to the same extent; for the points A and B in Fig. 27.3 lie on the ellipse of constant 
Fig. 27.3.—Ellipse of Constant Power for Simple Hypothesis with Two Parameters (see text). 
0, so that the probability of detecting them is the same, though A represents a positive 
" error " in 02 greater than the negative " error " given by B. 
27.14. Whether this is a desirable property of the test depends to some extent on 
what the test is intended to do. To avoid the anomaly we must require that 
/512 = 0 (27.64) 
Furthermore, even if this condition is satisfied and the principal axes of the ellipse coincide 
with the co-ordinate axes, there may still appear anomahes if the length of one axis is greater 
than that of the other ; for then errors in one parameter are not detected as frequently 
as errors of the same size in the other. Here again it is a matter of particular circumstance 
whether such an effect is regarded as objectionable. (We disregard the fact that it can 
be removed by appropriate scaling of the parameters, which may or may not be artificial.) 
To remove it we must require that 
0n = 022, (27.65) 
so that the ellipses reduce to circles. 
We may refer to the ellipses as " curves of equidetectability." 
27.15. With the foregoing explanation in mind we define w0 as a regular unbiassed 
critical region of Type C if it obeys the conditions 
A (Wo) = 02 {W0) = 0 
012 (tt>o) = 0 
011 K) = 022 K) 
. (27.66) 
. (27.67) 
. (27.68) 
REGIONS OP TYPE C 319 
and if, for any other region obeying these three conditions and for which 
0 (0, 0 | w0) = 0 (0, 0 | w) = 1 - oc, . . . . (27.69) 
we have 
Pu(wo) >Pu(u>) (27.70) 
Secondly, if a region wx possesses the property that 
0i {wl) - P% {wx) = 0 . . . . (27.71) 
012 K) - 0n K) 02. (w?i) < 0 . . . . (27.72) 
and for any other region obeying the conditions 
£ (0, 0 | wO = 0 (0, 0 | w) = 1 — oc . . . (27.73) 
^11 (Wl) ^ 012 (Wl) ^ 022 (WQ , . 
0ii (w) 0i2 (w) p„(w) ' - * ' l * j 
we have 
0u («i>i) > 0ii (w) ■ (27.75) 
we snail say that w1 is a non-regular unbiassed critical region of Type C. 
These equations are analytical ways of saying that the regular region of Type C is 
the one, among all regions having circular curves of equidetectability, which has the smallest 
radius for any given value of the power function ; whereas the non-regular region of Type C 
is the one, among all regions having similar ellipses of equidetectability, which has the 
smallest axes. 
27.16. We now state without proof theorems similar to those demonstrated above 
for the case of a single parameter. 
Write 
d2p 
Pjk 
et)c. 
Then w0 is a regular unbiassed critical region of Type C if 
(a) inside w0 
Pn > hi (?ii - P^z) + h Pri + h Pi + K P* + h P, • • (27.76) 
and outside w0 the inequality is reversed— 
(b) 
\ 'j)j dx = I p12 dx 
J W0 J ICo 
(Pu -pM)dx = 0, j = l, 2, (27.77) 
W, 
Secondly, if wx satisfies the conditions— 
(a) that inside wx 
Pn > K (yia Pn — yn Pvi) + h (yaa Pu — Yu p**) + Jc* px + K p2 + fap (27.78) 
and outside wx the inequality is reversed, the Ics as usual being constants and the y's obeying 
the conditions 
Yu > 0, yjf2 — yxl yaa < 0 ; 
(b) pj dx = (yia #n — yn pia) cfe = (y22 £>1X - yu ^a2) da; = 0, (27.79) 
then Wi is a non-regular unbiassed critical region of Type C, having ellipses of 
equidetectability determined by 
Yu 0? + 2yia 0! 0a + y|2 61 = constant. . . . (27.80) 
320 
GENERAL THEORY OF SIGNIFICANCE-TESTS 
27.17. The theorem of invariance of 27.9 no longer holds in general for the present 
case. If we transform to new parameters £i and f2, the equations of transformation 
^=I: *>*+S m» 
etc. will not transform an ellipse co-axial with the co-ordinate axes 6lt d2 into one co-axial 
with £i> £a. Thus, in general, the effect of a transformation is to make a regular Type C 
region into a non-regular Type C region. 
27.18. As usual, the conditions for the Type C region may be simply written in terms 
of the derivatives of logp. Write 
d i 
logp 
h 
<l>Jk 
L903. 
d2 logp 
ddj dO* 
k J 
fli-Oa-0 
0^03=0 
(27.81) 
(27.82) 
Then if 
we shall have 
<f>jk — ^-jfc + Bjk <£i + @jk 4>i 
(27.83) 
(27.84) 
Pjk = (<f>j 4k + Ajk + Bjk <f>i + G}k 42)p 
-and the inequality (27.76) becomes 
(1 - kx) <f>\ - h ^i ^ + kt <f4 - A.; fa - k\ fa - At; > 0 . . (27.85) 
where the ¥ are new constants easily expressible in terms of the old. They must be 
determined so as to satisfy (27.77), which reduce to 
cf>j p dx = (<f>t j>2 + Al2)p dx = { $\ — <f>* + (Axl — A22)]p dx = 0. (27.86) 
Example 27A 
Suppose we have a sample of ni from a normal population with mean fix and unit 
variance and a second sample of n2 from, a normal population with mean //.2 and also unit 
variance. The simple hypothesis to be tested o, where //0 is some specified 
valu6. We consider two cases :— 
(i) in which errors of the same size in \xx and //,.> are equally important; 
(ii) in which, for some reason, there is a stronger desire to avoid errors in /i2 than 
in (jlx and that therefore a greater number n2 of members has been taken in the second 
sample. We also assume that the sizes of errors judged of equal importance are 
inversely proportional to <\/n, so that we are led to consider new parameters— 
Vi — (^i ~ fa) Vnu V* = (fa — fa) Vn* • - . (27.87) 
Case 1.—The frequency function is 
p oc exp 
It will be found that 
■/ii H-n3 
tm-i 
* 
ii 
<f>t = nx (xx — p0) ; </>2 = n2 (x% — fa) ; 
^i = A1X, cj>12 = 0 = A12; cf>22 = — n< 
JjL <>o. 
REGIONS OF TYPE C 321 
From (27.85) we then find 
(1 - kx) n\ (xx — fi0)2 — &a nx n2 (xx — fi0) (x2 — p0) + &i n\ (x2 — ptQ)2 
— fc's ^i (^i — ^0) — k\ n% (x2 — fa) — Jc's > 0. . (27.88) 
The law of distribution of xx and x% may be written 
p oc exp [— £ {% (»! — /^o)2 + wa (£2 - /i0)2} ]- . . (27.89) 
Put u = V^i (xx — //0) and r = V^2 (x, — /«0). 
Then the region w0 is determined by 
(1 — k^ nt u2 — &2 w V(^i ^2) + ^ nt v2 — &'3 wV^i ~~ &'* *> Vwa — k'0 > 0 (27.90) 
where p (u, v) du dv 
a 
% # ('M, v) rfw. <fo = v p (u, v) du dv = I iw p (u, v) du dv = 0 . (27.91) 
J ir0 J Wo J Wo 
1 
and 
(nx u2 — ?z2 v2) p (u, v) du dv = (1 —a) (%x — w2) . . (27.92) 
1 
p (w, v) = — exp {— i (u2 + v2)} 
It is evident from (27.90) that in the (u, v) plane the boundary of w0 is a conic. From 
(27.91) we see that it must be coaxial with the co-ordinate axes and have its centre at the 
origin. Hence A'2 = k$ = k[ = 0. Finally from (27.92) we find that the boundary is 
of the form 
where 
1 
0 
0 f> 
a2 b~ 
nx (1 - feO 
At, 
= 1 
1 
6» " 
* 
& 
. (27.93) 
(27.94) 
The Type C regions are then defined by (27.93), but we have to express a and b in terms 
of known constants, including the probability level 1 — a. We have to satisfv (27.92), 
and will show that a solution always exists. 
Put 
F (a, b) .--■= c-~- (nl u2 — n2 v2) exp { — | (a2 ~f v2) } du dv — (wL — w,a) (1 — a). (27.95) 
if the boundary of w0 is a circle, its radius is easily found to be 
a =6 == V {— 2 log (1 — a) J-. 
The integral F (a, b) outside this circle, by the substitution u — r cos ?/», v — r sin ?/>, is 
found to be 
F (a, a) = (% — wa) -; - u2 exp { — A (w- -f v;2) j- rfw. dv — (wx — n,) (1 — oc) 
= (1 — a) (tti - n*) ha2. 
Now taking w0 as the space outside the parallel lines 
v = ± h 
2 f"0 
which is given by a infinite, so that ~^-\ I e~^ dx = 1 ■— a, 
V(^)Ja 
a.s.—vol. II. 
Y 
322 GENERAL THEORY OF SIGNIFICANCE-TESTS 
n, 
/• 
F (oo, A) = - (% - wa) (1 - a) + ~ u2 exp {- £ (™2 + v2) } d% ^ 
^7T 
Wo 
7i r 
~ v2 exp {— \ {u2 + v2) } du dv 
n< 
Similarly, 
?- X e~^ < o. 
71 
F(X, oo) -% /he-*^ 0. 
7T 
Thus, since jF (a, b) is continuous it must vanish somewhere in the range X < a < co, 
A <6 <oo. The values for which it does so define the Type C region. 
Case 2.—In this case, using the parameters tjx and r)2 of (27.87), we find 
cf)1 = W, (jf>2 = V 
<f>U = — 1, cf)1Z = 0, cf>22 — — 1. 
The inequahty becomes 
(1 — kx) u2 — kz uv -f kx v2 — k'zu — fc± v — k'5 > 0, 
where 
(u2 — v2) p (u, v) du dv = 0. 
■w 
In a similar way it follows that the Type C region is the one lying outside the circle 
uz _j_ v2 __ __ 2 log (1 ~~ a). 
We leave the verification of this result to the reader. 
Certain Limiting Properties 
27.19. From the foregoing examples it will be seen that in certain cases the optimum 
critical regions are by no means easy to determine numerically ; and it is not always clear 
that the labour involved is repaid by the results. Some consideration has been given by 
various writers to tests which have optimum properties for large n, the presumption being 
that the same tests will be good, if not the best, for small values. As usual when several 
limiting processes are involved simultaneously, the rigorous enunciation and proof of 
theorems in this field is a matter of some complexity, and we shall here merely indicate 
some of the results in very general terms without including proofs. 
It has been shown by Neyman (19386) that there do exist tests which are unbiassed 
in the limit, and rules have been given for finding them. It has also been showm by Wald 
(1941$) that there exist tests which are most powerful in the limit, and that such as are 
based on maximum likelihood estimators are of this class. The tests are uniformly most 
powerful for the single parameter 6 > 6Q and for 6 < 60, but not both ; and for any range 
they are the most powerful unbiassed tests in the limit. Furthermore, the Type A test 
tends to the most powerful unbiassed form. 
The general conclusion seems to be that, even where the variation is not normal, most 
of the tests in current use which are based on likelihood estimators have optimum properties 
in the limit, and may therefore be used confidently for moderate or large samples. For 
small samples the position is not so clear, particularly for non-normal variation. Tests 
based on inefficient estimators are presumably less satisfactory ; and for the 
non-parametric case there is as yet no complete theory. On this latter question reference may be 
made to a useful review by Scheffe (1943). 
PITMAN'S METHOD FOR LOCATION" AND SCALE PARAMETERS 323 
The Unbiassed Character of Likelihood-ratio Tests 
27.20. It is of some interest to consider how far the tests based on likelihood (26.35) 
are unbiassed. 
It has been shown (Pitman, 19396 ; Brown, 1939) that the Neyman-Pearson test in 
the problem of k samples based on AHt is biassed unless all the samples are of the same size ; 
but that Bartlett's modification (26.42) is unbiassed. We prove this in 27.25 below. 
On the other hand, Daly (1940) has shown that in certain multivariate tests such as those 
of regressions, multiple correlations, Hotelling's T (which we introduce in the next chapter), 
and the ordinary analysis of variance and covariance for orthogonal or non-orthogonal 
data, the likelihood-ratio tests are unbiassed, at least in the Type A sense (i.e. locally) 
and in some cases completely so. 
Pitman's Method for Location and Scale Parameters 
27.21. In the special but not uncommon case where the hypotheses under test 
concern parameters of scale or location, a simplified approach is possible. Suppose the joint 
distribution of k sample-values is 
dF = / (#! — Qu #2 — 02» ■ • ■ xt ~~ ®ic) d%t • • • d%fc. . . (27.96) 
We seek for a statistic J, independent of the 0's, to test the hypothesis ; and clearly, if the 
test is to be satisfactory, J must be independent of the origin, i.e. must be seminvariant. 
The test that the (9's are all equal is then equivalent to testing the hypothesis 
01 = fl8 = . . . = ()k = 0. .... (27.97) 
Without loss of generality we may suppose the hypothesis rejected if J is small and less 
than some quantity depending on the acceptance value a, and we may also suppose J 
positive ; for if either condition is not satisfied we can transfer to some other function of 
/ for which it is. 
In the sample space W, J must be constant along the line xx = x2 ~ . . . = xk = 
constant, and therefore the critical region w0 will be the one lying outside a hypercylinder 
whose axis is parallel to this line. When H() is true, the probability of rejection is then 
i 
dF (xx . . . xk) = 1 — a, . . . . (27.98) 
and when it is not true the probability is 
dh (xl \)u . . . xk — 0k) 
J u\ 
\ 
J w 
dF (xx, . . . xk)9 . . . . . (27.99) 
where w is merely derived from w0 by a translation in W without rotation. If L is any line 
parallel to xx = . . . = xk = 0, we write 
P (L) = dF (xx . . . xk) 
-- I /' (xl . . . xk) drj .... (27.100) 
J L 
1 
where rj = —■■ 2J (x) ; ...... (27.101) 
yk 
and rj is thus the distance of the point (xx . . . xk) from the plane E (x) = 0. 
324 GENERAL THEORY OF SIGNIFICANCE-TESTS 
Now if w0 is defined as the locus of all lines for which P (L) > h, a constant, P (L) will 
be less than h on any L which is in w but not in w0. Hence 
dF>\ dF, (27.102) 
and so the resulting test is unbiassed. Thus an unbiassed test is given by choosing J so 
that at any point of a line L it is equal to P {L) at that point. Now we may write for the 
variable co-ordinate on a particular L, say £r, 
CJ"- === Xy v 
where t = T £ (x) 
k v ' V*' 
Hence 
-00 
P (L) = \/h\ f (xl — t, x2 — t, . . . xk — t) dt. 
J —QO 
(27.103) 
Taking J = ■-- P (L) 
we find 
v& 
,00 
J = \ f (xi — t, x% — t, . . . xk — t) dt, . . . (27.104) 
J ~O0 
which gives us an unbiassed test. 
Example 27.5 
Consider the case where the variables are distributed normal! v wifon unit} variance. 
/ = -^exp{-i2;(s,-0,)M. 
(2ti)2 
Then we have, from (27.104), 
1 f 
J = ^ I exp {— l Z (x,j — t)2 } d£ 
e-*# 
V& (Step-1* 
where S = E (x — x)2. 
In practice we should take S as our criterion, not J, and reject the hypothesis that 
the means were unequal if S exceeded some fixed value determined by a. We observe 
that in fact S is distributed as x2 with k — 1 degrees of freedom when //„ is true, so that 
this value is easily ascertained. 
27.22. Consider now the case where the frequency function is 
1 p, { Xi Xf. \ 
MT.T. e/U ' " ' h) (27.105) 
If the x's are positive in range we put 
%=log^, fy =log0JS .... (27.106) 
and for the frequency function of the y's we find 
«p(27y -Z$)f(e^-+\ e^'\ . . . ev*-*'). . . (27.107) 
PITMAN'S METHOD FOR LOCATION AND SCALE PARAMETERS 325 
This reduces to our first case, and we have an unbiassed criterion that 
4>i = j>2 = . . . = <f>k 
by putting 
/» CO 
J = exp (Zy - H)f{ev^\ &*~\ . . . e^~l)dt 
J —CO 
k \ r=° 
n *,) / 
j-1 /JO 
X\ *^2 fc 1 
T' T' * * * t ) tk+1' ' ' ' (27-108) 
When the #'s are not necessarily positive the expression remains the same, except that in 
(27.108) 77 (x) becomes 77 (| x |). Small values of J are significant. 
27.23. Suppose now that our hypothesis asserts the equality of 0's or </>'s and 
states that they have a common value d0 or cf>0, as the case may be. Then if we take 
Jf =( n\xj\)f(x1 . . . xk), .... (27.109) 
the test will be unbiassed. Moreover, if we regard small values of J' as significant and the 
#'s are independent, and if each frequency function is unimodal, then when 
0X = ea = . . . = e* = 0O 
is not true the probability that J' exceeds the specified limit based on 1 — oc increases as 
any 0 tends to 0O- J' therefore provides an unbiassed test. 
dF = , 7"/:r-v exp ( - 7 ] f 7 ) fy. . . . (27.110) 
27.24. Finally, consider the case of k variates each distributed in the form typified by 
I / aA fx*\mi~^ 
exp ( —■ —- 1 ( — 
Their joint distribution is 
n(lYl exp (~s l)ndx 
(IF = \-TJ- ^ f , \ TJ ..... . . . (27.111) 
77 { <f> F (m)} } 
Hence, to test the hypothesis that the samples have the same <f> we have 
II [T (m)} J0 tM+v 
where M = Z (m), * 
__ r (M) n (xm) 
~ 77 {7" (m)} " (£ ap*" 
It is sometimes convenient to deal with 
II (a^) 
which differs from J only by a constant factor 
The maximum value of K is 
77 (mm) 
and we put 
(27.112) 
Ar = r^ (27-11;^) 
L = - _ l0gK -JT - M log ( — "\ - <£ ( m log ~ ) . . (27.114) 
log max. A & \ M / \ & m ' 
L is essentially not negative, and large values are significant. 
326 GENERAL THEORY OF SIGNIFICANCE-TESTS 
For testing the hypothesis that a set of variances have some specified equal value, we 
find similarly from (27.109) 
L' =27(a?) - M -27^mlog-V . . .(27.115) 
\ m) 
27.25. The foregoing result has an immediate application to the case of k normal 
samples, for the variances are then distributed in the Type III form of equation (27.110). 
The criterion L becomes 
L ^ N \og (?^-\ ~ Z U\ogSi\ . . .(27.116) 
where v as usual represents the number of degrees of freedom and N = E (v). This, as 
will be seen by comparison with (26.93), is equivalent to Bartlett's test, and shows that 
it is unbiassed. 
NOTES AND REFERENCES 
For the theory of unbiassed tests see particularly Neyman and Pearson (1936 ; 1938) 
and Neyman (19356). Regions of Type B have also been considered by Scheffe (1942a), 
who discusses a Type Bx standing in relation to B as Type At to Type A. 
For limiting properties see Neyman (1938&) and Wald (1941a). 
See also references to the previous chapter. 
EXERCISES 
27.1. Show that the test of Example 27.1 provides regions which are of Type A! 
as well as of Type A, and that the test is a U.M.P.U. one. 
27.2. Show that the cumulants of the distribution of L of (27.114) are 
Kl = M {Gl (M) - log M} - E \m {Gx (m) - log m} ] 
Kr = (- if {E?nr Or (m) - Mr Gr (M) }, r > 1 
dr 
where Gr = --- - log F (m). 
1 dmr 
L k -— I 
Hence show that the cumulants of are approximately Kr — —-— F (r), where 
2L 
and thus that ——r is distributed approximately as %2 with k — 1 degrees of freedom. 
1 ~T" P 
(Bartlett, 1937c; Pitman, 19396.) 
EXERCISES 
327 
27.3. Show that in samples of 3 from a normal population the distribution of the 
range r is given by—- 
dF 
6 
e 4<x» 1 —rr-r—r e~"iy dy dr. 
laa 
Jo 
&V71 Jo V(^) 
Hence that an unbiassed critical region of Type A is given by 
r e 
•lra 
\V° e-*v' dy 
Jo 
r2 
rx 
0 
rx e 
-Hi 
V6 e-w dy = r% e~W ve 
0 J 0 
the region lying outside rx < r < r2. 
c~^ dy, 
(Neyman and Pearson, 1936.) 
CHAPTER 28 
MULTIVARIATE ANALYSIS 
28.1. We have already considered some aspects of the case in which each member 
of a population is characterised by several variates x1 . . . xp. For instance, we have 
examined the measurement of correlation between the variates and the regression of one 
variate on some or all of the others. In this chapter we shall extend our inquiries into 
the multivariate case a good deal further, mainly by taking into account the possibility 
that different sample-members may have emanated from different populations. This 
will lead to some generalisations of the methods already discussed for the univariate case, 
such as tests of homogeneity and tests of differences between two samples. Some of our 
known results generalise with nothing more than additional mathematical complexity; 
but in others certain new features appear, and the theory of multivariate analysis is not 
entirely a matter of generalising univariate results to p dimensions. 
28.2. One or two examples will illustrate the kind of problem with which we are 
concerned. A number of skulls are discovered in a burial-ground. They are found- to 
vary among themselves in the manner usual in biological material. Is the observed 
variation consistent with the hypothesis that all the skulls were derived from members of the 
same race or does it suggest a mixture of racial types ? If heterogeneity is indicated, do 
the skulls fall into two well-defined categories, such as we might expect if the burial-ground 
were the site of a battle between two races such as Saxon and Celt; or are there several 
types such as we should expect in the normal burial-ground of a town where races were 
living together and interbreeding ? Or again, if the skulls are compared with another set 
known to have been buried at a much earlier time from the same race, is tnere any evidence 
of a significant change in skulls from one period to the other ? 
There is no single measurement on a skull which is marked out from the infinite number 
of possible measurements for deciding questions of this kind. It is quite common for 
thirty or forty measurements to be taken by craniometricians on a single skull. Even if 
we reject many of these for practical reasons, leaving out the jawbone, for instance, because 
it is often separated from the skull and cannot be identified, we shall still be left with a 
number p which require consideration. For n skulls we shall then have n sets of p values 
corresponding to variates xt . . . xp which are, in general, correlated among themselves 
and may be highly so. Our problem is to test the homogeneity of these values, or to 
estimate differences between parent populations from which they were derived. We may, 
of course, apply methods which are already familiar by picking out one variate and testing 
for homogeneity. But we might pick out quite an unsuitable one and sacrifice most of the 
information. Even if time permits we cannot take each variate in turn and test it because 
the variates are correlated and our p tests are not independent. 
28.3. Again, suppose we have two different breeds of laying hen and are given a 
batch of eggs from the hen-run without knowing which hen laid which egg. We require 
to allocate the eggs to the two breeds. Assuming that there is no decisive criterion such 
as colour of shell, we may measure various properties of the eggs such as length, breadth, 
328 
THE SUMMATION CONVENTION 329 
weight, volume, specific gravity and so on. Some of these measurements will be highly 
correlated or, in the extreme case, perfectly correlated, as with weight, volume and specific 
gravity. In such circumstances we may reject some variates as redundant; but in general 
we shall be left with several sets of measurements. Our problem is to find some method 
based on the retained variates for allocating the eggs to the correct parent breed. In 
particular we might search for the best linear function of the variates to discriminate between 
breeds and to enable us to assign the eggs with the maximum probability of correctness. 
28.4. Throughout the whole chapter we shall, except when the contrary is stated, 
assume that the variation is normal. In addition, to render our formulae a little less 
cumbrous we shall borrow a summation convention from the tensor calculus. If the 
affixes i, j range from 1 to p we shall write 
p p 
^ "a = X X ^ a'»> .(28.1) 
i-i i-i 
the affixes to A being regarded as ordinary superscripts, not as powers. Similarly we 
shall have 
v 
Aijalk^ ^A^aik (28'2) 
Whenever an affix occurs as a superscript and a subscript, summation is to be understood. 
Clearly the actual letter used is a dummy and we have, for instance, 
Aij afj = Akj akj = Au aId. . . . . . (28.3) 
We shall write the array of values Aij (a square matrix) as (Aij) and its determinant 
as | Av | or simply as | A . 
To every matrix (a,-.) with a non-vanishing determinant there corresponds a reciprocal 
or inverse matrix which we may write (a>j). Since 
(a.j) (dij) = 1, 
we have, on carrying out the multiplication, 
= 0, j 5* k, 
which we may express as 
a%iaik =aiiak! - <$£ (28.4) 
where dk, one form of the Kronecker delta, is zero ifj ^ k and unity otherwise. The 
quantity aij is the minor of al} in \ A \ divided by | A | itself. 
28.5. It will further simplify our formulae and will give rise to no loss of generality 
if we suppose our variates to be in standard measure, that is to say, to have zero mean 
and unit variance. If we require results for the more general case we can easily obtain 
them from transformations of the type 
%i = ai £* + m% (28*5) 
With this convention the equation of the multivariate normal distribution (cf. 15.12, 
vol. I, p. 376) may be written 
dF = ^LJ exp (— hAV xt xA dxx . . . da;-,, . . . (28.6) 
(2jt)"j j 
330 MULTIVARIATE ANALYSIS 
where the A's are related to the correlation determinant 
A = | p*ij |. . . • • • (28.7) 
In fact (Aij) is reciprocal to (p^), as we saw in 15.12. 
28.6. We shall also frequently refer to the matrix of sample variances and covariances 
which we shall call the dispersion matrix and write as (a^), where 
n 
%' 
— / ixi — %t) ixj — %j)- .... (28.8) 
This, it is to be remembered, is in standard measure for the population, that is to say the 
observed variates are taken from the parent means and divided by the parent standard 
deviations. 
Wisharfs Distribution 
28.7. We now proceed to generalise to p variates the joint distribution of dispersions 
arrived at in 14.12 (vol. I, p. 339) for the bivariate case ; and we shall also show that 
the distribution is independent of that of means. The result and method of proof are 
due to Wishart (1928). 
First of all let us write the result for the bivariate case in our new notation. For 
the distribution of means we have 
n | A I* . / n 
2to~ 
and for that of dispersions 
dF = ( - ) \A\«»-« % . ---. exp ( -% A* a.io-) daxl iau da22. (28.10) 
dF = —L—-!■ exp ( — - Ali xt Xj ) dxx dx2, i, j — 1, 2 . . (28.9) 
2/ * r (!Lz±y (JLzlj 'V 2 
For instance, we have 
2 2 
1 
P 
{AV) 
so that (28.10) is equivalent to 
1 -p2 1 - 
-P 1 
n—i 
dF 
nn-l ^ _ r2)— sn-2 sn-2 
»-W*r(n-^) r(n^l) (W)*M 
X exp j - _-^-_ (s~ — 2Prst s2 + 4) \ dsi ds* dr- 
This, with the substitution 
\ 2 / \ 2 ) 2n~* 
is the form found in equation (14.44), vol. I, p. 342, when it is remembered that we are 
working in standard measure. 
WISHART'S DISTRIBUTION 331 
28.8. Now consider the general case. With a sample of n values of p variates we 
consider p rectangular spaces of n dimensions each as the domain of variation. If a point 
in one of these spaces be fixed, the variation in the other spaces is constrained for fixed 
values of the sample dispersions. The following argument is a generalisation of that given 
in 14.12 leading to the bivariate result, and the reader may like to refresh his memory 
by re-reading that section. 
Writing x0l . . . xin for the n values of the jth variate, we have for the density function 
of the whole sample, from (28.6), 
; = (toffi* 6XP 1 ~ * Z {Ali Xik ***> 
and /» = .J/a:ih\.iw, exP( - a^ia<j b ■ • -(28.13) 
= fesw exp C ™ i r iAvJ (*ik ~ *i) fait - fy) } ] X exp f - - A* xt xs J . (28.11) 
We may thus factorise the density function into two parts, 
f^-^^^{-2AJx^) * * * ■ (28J2) 
' A \U»>-» ( n 
where we have chosen the constant factor of fx so that the distribution shall have the total 
frequency unity. 
n 
Consider now the volume element FI dxUc dx2k . . . dxp1r In any particular ?i-space 
the density is constant over hyperspheres centred at the mean. The volume element may 
then be represented as the product of elements dxj and of independent elements depending 
on dispersions. In the total space of pn dimensions the volume element may thus be 
represented -as the product of p elements d/Xj and an independent element depending on 
dispersions. Thus the volume clement also factories, and we have immediately for the 
distribution of means 
n*p I A I* / n _ \ p 
dF = ' ' ,:exp( — A'J xt Xj ) II dxp . . . (28.14) 
(2nyv \ 2 / j = 1 
showing that the means are distributed in the multivariate normal form independently 
of dispersions. 
If we define a matrix (B) with elements \n times those of (A), we may write the 
distribution of means in the simple form 
I jB I* 
dF = -'—— exp (— B'J x- xA IT dx. . . . . (28.15) 
We note that this checks with the known results for p = I and p = 2. It is also seen 
almost at once that the variance of #■ is Oj/n, as we expect. 
28.9. We have now to consider the more complicated expression for the volume 
element of dispersions. Let us in the first instance transfer our origins to the sample means, 
remembering that in doing so we have lost one dimension (or degree of freedom) in the 
variation of our sample-points. Let P1 . . . Pp be the sample-points whose co-ordinates 
are the n values of xt . . . xp, one point P lying in each w-space. We shall consider in 
turn the variation of Px, then that of P2 for fixed P1? then that of P3 for fixed Px and P2, 
332 
MULTIVARIATE ANALYSIS 
and so on. The total variation will be given by multiplying the various expressions so 
obtained ; and* it will be sufficient if we consider the typical case of the variation of Pm 
for m — 1 fixed points Px . . . Pm_r 
For a fixed length 0Pm and fixed angles with OP± . . . 0Pm_1, Pm can vary on a 
hypersphere of n — m dimensions ; for, if we fix any particular angle, Pm is constrained 
to lie on a hypercone which cuts its hypersphere of variation in a hypersphere of one fewer 
dimensions, and the fixation of the origin at the sample mean imposes a further constraint. 
Further, if we regard the p spaces as superposed, as we may, the centre of this (n — 
Tridimensional hypersphere is the foot of the perpendicular from Pm on to the space containing 
the points, 0, Px . . . Pm_i- Call the length of this perpendicular for the time being rm. 
The volume of a ^-dimensional hypersphere of radius r is 
c/ JL JL 
r 
k + 2 
and its surface area, obtained by differentiating with respect to r, is 
nnm^ 
The surface area of the hypersphere of variation of Pm is thus 
(28.16) 
r 
n — m 
(28.17) 
To find the element of volume due to the variation of Pm and the angles which OPm 
makes with 0PX . . . OPm_1 we have to multiply (28.17) by an element of variation 
normal to the hypersphere of n — m dimensions. This variation lies in the hyperplane 
determined by the origin and Px . . . Pm which is, in fact, normal to the hypersphere. 
To evaluate it, consider the transformation 
m 
z 
&=1 
^mk ^jk-) 
J 
m. 
(2 8. J. o) 
where, of course, the x's are measured from the sample means in virtue of our choice of 
origin. We have for the Jacobian— 
J 
d\x. 
ml 
nf 
ml 
/y 
"^ 112 
■^2 2 
2x 
lm 
2x 
2m 
2v 
X 
lm 
2m 
2r 
"°ram 
(28.19) 
where vd is the volume (or " content ") of the hyperparallelopiped having one corner at 
the origin and edges running to the points Px . . . Pm. Furthermore, 
\ t I — I T nr 
>mj 
Jmk %jk 
•^mk 
= V. 
m* 
. (28.20) 
WISHART'S DISTRIBUTION 333 
The required element is thus 
2v. 
1 m 
m 7c=1 
and the total element of variation of P , on multiplication by (28.17), is 
jj. I {n—m) y.n — m.—1 m 
n d£mk (28.21) 
Now rm is the length of the perpendicular from Pm on to the space OPx . . . Pm^1 
and is therefore equal to vm/vm_1. Hence, for the variation of Pm we have the element 
11 &±mk* .... (28.22) 
r U ~mK.n-m-l *-l 
We now derive the total element for variation of Px . . . Pm by multiplying expressions 
of type (28.22) for m = 1, 2, . . . p. The terms in y cancel except vp and v0, the latter 
being unity, and we find 
TriPCin—p—l) m p 
- ■ ■. ,-, »s-p~8 ^ # ^*- ■ • • (28-23) 
h r ^k) 
Now from (28.18) we have 
£jk = 7i aj7. ...... (28.24) 
and from (28.20) vjj = n/> \ a | (28.25) 
Making tlie necessary substitutions in (28.23) and adjoining the frequency element given 
by (28.13) we find, after a little reduction, 
■M \ lPiU-\) 
*> ) 
iF ^ X p / .„ l \ cxp ( — - A'J an ) II da, . (28.26) 
k~\ \ 2 / 
This is Wishart's generalisation of the distribution of dispersions in a multivariate 
normal system. The reader who feels that the foregoing proof demands too much of his 
powers of geometrical insight may refer to alternative derivations by Wishart and Bartlett 
(1933c) or I\ L. Hsu (1939a). The domain of variation of the a's is 0 to oo for % and 
corresponding values for ar, i .- y, such that correlations do not exceed unity in absolute 
value. 
28.10. It must be remembered that we are regarding ai;i as the same as a,jt and that 
the product of differential elements in (28.20) contains hp (p + 1) items, not p2; for there 
are p elements of the form dau and hp (p — 1) of the form da.fj, i ^j. The expanded form 
of AiJ arr however, takes place over ■£, j from 1 to p, so that any particular term such as 
A'u aM occurs twice, once as A™ a34 and once as A'lli a,u ; except that when i = j the term 
occurs once. For instance, with p = 2 we have 
AiJ ai3. = A11 alx + 2A** a13' -f A^ aaa. . . . (28.27) 
We can now derive the characteristic function of the Wishart distribution. Ignoring 
334 
MULTIVARIATE ANALYSIS 
constant factors and writing a single integral sign for summation over all a^, we have, 
from (28.26)- 
* a | Kn-g.,) exp ( - | A* a^j nda = , A f_n . . (28.28) 
i 
J. *(»-!) 
where i£ is some constant. In this form let us replace Alj by Alj &ij when i 9^j and 
% 
by -4'^' — -0iJ" when i == j. Then the resulting integral is the characteristic function of 
n 
the a's, 0'^ being the parameter itij corresponding to a{j. We thus have 
A I *(n~1) 
_ .„.. 
<t> (6V) 
A11 --011 A12 -- 012 . . . 41* 
:qip 
n 
A12 - i 012 ^[22 _ ? 022 , ^ ^ A2p _ l92p 
w r& n 
A*p - I 0i* J> - I 02* . . . A*>* - - 0**> 
n n n 
. (28.29) 
the constant being evaluated by the consideration that <f> (0) = 1. 
Example 28.1 
Let us apply these results to an examination of the moments of the distribution of 
covariance in the bivariate case. We have 
1 
A 11 A 
22 
10 ' 
— p" 
We then find for the c. f. of all5 a12, a22— 
1 2d11 
<f> oc 
1 — p2 n 
- p 012 
1 — p 
1 
J 12 __ P 
. _012 
2022 
■i(n— 1) 
1 — p2 n 1 — p2 7i 
We are interested only in the parameter 012 which we will write as 6, putting the others 
equal to zero. We then find— 
c/> oc 
1 
01 2' 
-Un-l) 
__ (1 — p2)3 [ 1 — p- n 
l ___ 2p0 __ (1 _p2) 0a^| -l(n-l) 
n n* 
Taking logarithms and evaluating coefficients of powers of 6, we find for the cumulants 
n — 1 
K, = 
n — 1 
/<:<> — 
K\ 
7i 
2 
(1+P2) 
2 (n - 1) „ 
%• 
/c 
'6.(w, - 1) 
n' 
P(3+P2) 
(1 + 6p2 + /o4). 
HOTELLING'S DISTRIBUTION 
335 
In standard measure the distribution tends to normality as n tends to infinity. But for 
finite n we have 
6 = 4 P2(3 +P2)2 
Pl n - 1 ~(1 +p2)¥ 
?i — 1 (1 + p2)2 
Thus, even when p — 0 our distribution, though symmetrical, is not normal. 
Wishart (1928) has given formulae as far as those of the fourth order for eight or 
fewer variates. 
Hotelling's Distribution 
28.11. In the univariate case we can test the significance of a mean by comparing 
it with the estimated standard deviation, the ratio being distributed in ce Student's " form 
(or some simple transformation of it if we compare the mean with the actual sample variance 
and not the unbiassed estimator). We proceed to generalise this result. 
We require a single quantity which will serve as a measure of departure of all the means 
Xj from the population values which, as usual, we take to be zero. In place of the matrix 
of dispersions, we shall consider the matrix of sums of squares and products (by) where 
n 
. (28.30) 
As usual we take (blJ) to be the matrix inverse to (by). Let us now write 
T2 = n (n - 1) bij x^j (28.31) 
This is Hotelling's generalisation of the " Student " ratio t. 
In the simplest case when p = 1 we have 
1 
,n 
and hence 
wi 
1 
n 
ns - 
- I 
4:"' *^L' ■ 
S1 
. (28.32) 
so that T becomes equal to the ratio £ as required. 
7"b 0 X,j Xj. 
. (28.33) 
28.12. We have 
n — I 
Let us now denote by m^ the sum of squares or products about the origin, so that 
mfj — bti + nXjXy . . . .• . (28.34) 
The determinant of m.y may be written 
1 xx*\/n x2Vn - - • %pVn 
0 btl + nx\ bvl + nXiXz 
0 613 + nx%xx b22 + nxl 
\) ®lfn ~T™ flXflXi )rn \~ tvXftXs 
Z'D I™" 'bXi^X'i) 
0.tyn ~j~ wx 
-2 
'pp 
p 
* 
336 
MULTIVARIATE ANALYSIS 
On subtracting xx \/n times the first row from the second, and so on, we find- 
mu I = 
1 
xx*\/n . . . xp\/n 
x^n b^ 
X.jj *\f 7b ^l'D 
. . . b 
ip 
... b 
pp 
and on expanding according to the border row and column, 
It follows that 
T2 
. (28.35) 
or 
6.. 
u%3 
1 4- 
i 
n — 1 
1 
272 
1 mil 
\ bij 
\™<ij 
— h.. 
u%3 
a 
. (28.36) 
n — 1 
This is a fundamental equation in the sampling theory of T and we proceed to interpret 
it geometrically. 
28.13. In the case p = 1 we have a single sample space of n dimensions. The 
numerator and denominator of (28.36) then reduce to 6n and mn—that is to say, the squares of 
distances from the sample-point Px to its projection on the unit vector whose direction 
cosines are all equal, and from Px to the origin, respectively. The ratio of (28.36) has 
zero dimensions and is in fact the square of the sine of the angle between OPx and the unit 
vector. This is the geometrical approach which gave us " Student's " distribution in 
Example 10.6 (vol. I, p. 239). 
In the general case let us regard the p ^-spaces as superposed in one n-space. The 
points Pt . . . Pv will lie in a space of p — 1 dimensions, a hyperplane in the ?z-space. 
Now we may rotate the axis without altering the functions m^ | or j b,^ I which are easily 
seen to be invariant under orthogonal variate-transformations. If we perform such a 
rotation so as#to bring the (p — l)-space of sample-points into correspondence with p —■ 1 
co-ordinate dimensions, we see from (28.20) that | mtj \ is the square of the content of a 
hyperparallelopiped with one corner at the origin and sides parallel to OPx . . . OPp. 
Now consider a hyperplane perpendicular to the unit vector meeting it, say, in 0', 
and let P[ . . . Pp be the projections of the points P on to this hyperplane. Then b{j 
is the covariance of the co-ordinates P- and P- referred to 0\ and hence | b{j \ is the square 
of the content of the hyperparallelopiped in the hyperplane. Furthermore, the content 
of this figure bears to that given by | m(j | a ratio equal to the cosine of the angle between 
the unit vector and the hyperplane. Representing this angle by 0, we have 
^2 -cos26 (28.37) 
1 ■+- 
n 
28.14. Now if the sample-points P are distributed in the w-space with random 
orientation, the hyperplane which they determine will be distributed randomly in regard 
to the angle which it makes with a fixed vector, and in particular with the unit vector. 
The sampling distribution of 6 is then that of an angle between a fixed vector and a random 
plane. But this, from a slightly different viewpoint, is precisely the problem of distribution 
which we solved in connection with the multiple correlation coefficient R, for we saw (15.18, 
HOTELLING'S DISTRIBUTION 
337 
vol. I, p. 381) that R is the sine of the angle between a residual vector represented by a 
variate xla2mm,p and the space containing other variates x2 . . . xp ; and in the case when 
the former is independent of the latter we can regard it as fixed. Thus, from (28.37) we 
may write—- 
(28.38) 
1 + 
J72 
i?2. 
n 
The distribution of R2, in the case when the variate concerned is independent of the 
others is 
dF 
B 
n 
p p - 1 
(1 - R*)K*-P-*) (£2)^-3) dR^ 
. (28.39) 
2 
where we must remember that p is the total number of variates and the variates are measured 
from their means in forming the regression equation. Before substituting (28.38) in this 
expression we must increase p by unity, since in effect we are considering p + 1 variates 
—the unit vector determining an additional one ; and we must also increase n by unity 
because oiir variation is not restricted to that about the mean, as for multiple correlation. 
With these alterations in (28.39), we have, on substituting for R from (28.38) and a little 
reduction, 
1 {T*/{n - l)}«*-2> / T2 
dF 
B 
n ~~~ P P 
1 
rp% 
hi 
n 
. (28.40) 
2 2J\ n — I 
This is the distribution of Hotelling's generalisation of t£ Student's " ratio. 
28.15. At the end of the chapter we shall see that this is a particular case of a more 
general distribution (28.31). A third and instructive derivation, due to Wilks, is as 
follows :— 
From the manner of derivation of Wishart\s distribution it will be clear that if we 
substitute the moments about the origin a'i} for- those about the mean ai}, the distribution 
is the same, except that there is an extra degree of freedom. The distribution is then 
dF 
7lp | A 
j n, 
a 
fr(/», V l) 
nh>iv-\) u r 
n 
1 
Jfc 
exp 
n 
LyAija'i:j 1 17 da'. 
9. 
n 
Putting Bij = - Aij, we find, on integration, 
\ 
71, 
a' 
k(H--l)-l) 
exp ( — Bi} a)]) IT da' 
i, nr(n + l- 
2 
Jc 
B 
III 
(28.41) 
Now replace n by n + 2r in this expression and divide by the term on the right in (28.41). 
The result is to give us the rth moment of | a' 
Mr ( I a> I ) 
1 
B 
as 
.-. / % + 1 — k , 
p ( —^—__ + r 
v \ 2 
n 
r k^i pfn 4- 1 -- & 
. (28.42) 
2 
A.S.—VOL. II. 
338 
MULTIVARIATE ANALYSIS 
We may also write the distribution of a'fj in the form given by our original derivation of 
Wishart's distribution :— 
dF 
JJ 1(»-1) I a I* (n-u-2) 
niP(p~D nr 
n — k 
2 
exp (— JBtf aw) II da x —~-- exp (— Blj xt xj) IT dx. 
7t 
kP 
Multiply this by | a' \r, integrate, and use (28.42), transferring constant terms to the right 
as in (28.41) ; then replace n by n +• 2s and divide by the constant terms as they were 
before substitution. We find— 
[j, 
T, S 
( I a' I? | a | ) 
B 
r+s 
ri +r + 8\rl—— + 8 
fc=i 
n71±iz:* + /\r/n-* 
2 
2 
Now put r = — s and note that 
a 
a' 
b 
m 
We find 
fit 
n! WifJ!+. 
m 
rf | + *)r 
n — p 
Bl?L^P+8,P 
2 
2 
S 
n -~ p p 
. (28.44) 
Now the function on the right is the 5th moment of 
1 
dF 
n — p p 
ajHw-p-2) (! _ ^ito-2) ^ # 
. (28.45) 
which is uniquely determined by its moments. This, then, is the distribution of the ratio 
9 and on substitution in terms of T from (28.36) brings us back to the distribution of 
m 
(28.40). Incidentally this method gives us one more derivation of the distribution of 
multiple correlations and correlation ratios when the respective variates are independent. 
Significance of a Set of Means 
28.16. Suppose that we have a set of k samples with numbers nx . . . nk, each 
from a 2>-variate population. Let us also suppose that the populations have the same 
dispersion matrix but different means, that of the jth variate in the Zth sample being //. (z). 
We proceed to derive a criterion for testing the means simultaneously. Our result is a 
generalisation of the testing of k means in normal samples, and we shall obtain it by applying 
the same method, namely by using the likelihood criterion 
; __ Po (co max.) 
px (Q max.) 
as given in equation (26.64). Here co is the domain for which all the means of the jth 
SIGNIFICANCE OF A SET OF MEANS 
339 
variate have a common value /u, and Q that for which they have the more general values 
Let bij{l) be the function b,id for the Ith sample (Z = 1, 2, ... k) and xi(l) the mean 
of the ^th variate in that sample. Put 
k 
where, of course. 
bij (I) = /, ( 
X, 
it (I) 
Xi (l)) (Xit (l) ~" xjQ)) 
'3 (?) ■ 
Put, for the functions of the pooled samples, 
(28.46) 
(28.47) 
%x^ 
- % xu (i) 
n t,i 
®ij — ^ ^ \Xit (I) ~~ xi) \X 
t I 
"jt (I) 
Xj). 
(28.48) 
(28.49) 
(28.50) 
(28.51) 
If then 
mij (l) " ^ (Xit {l) ~ M'i (/)) \Xjt (l) ~~ t*j (/)) 
t 
the likelihood of all samples together is 
c | A \ln exp { — \ I {nt Aij m,tj (/)) }, . 
where c is a constant. 
Taking logarithms and differentiating, we have for the maximum value equations 
typified by 
~ ^ ni Au \ (Xu (i) ~~ Mi (/)) +" (xjt (i) ~~ flj (i)) ) — ^ 
/ t 
which reduce to 
xi (I) ~' Hi (/)■ 
The maximum likelihood values of the m\s are then given by 
m.(j {l) ~ bjj {l). 
. (28.52) 
Furthermore, the values of Atj are then given by the inverse, of the matrix ( - E€j ), and the 
n 
exponent of (28.51) becomes 
}n 2 (A« by m) 
\nl( 
. (28.53) 
We then find 
px (D max.) 
c e~ 
■Ink 
8, 
n 
a 
\n 
(28.54) 
In a similar way it will be found that 
p0 (a> max.) 
c e" 
■Ink 
n 
h 
in 
(28.55) 
340 
MULTIVARIATE -ANALYSIS 
Hence 
and we may write 
and take L as our criterion. 
% = 
i-6-. 
n 
1 z. 
-hi 
n J 
\n 
\n 
L = fa = 
7i 
i 
_ 5«l 
1 &<* 
. (28.56) 
28.17. The distribution of L for general k is not easily expressible, but we may 
1 
determine its moments by the method employed in 28.15. The functions - 6.y are dis- 
n 
tributed in Wishart's form and their moments accordingly given by equations of the type 
(28.42) with n replaced by n — 1, namely, 
m n — m 
r [ —-r— + r 
Mr ( I bij I ) = -^r-r- n 
B 
. (28.57) 
m=l 
r 
n — m 
Now each 6^- (Z) is distributed in Wishart's form, and therefore their sum is so distributed 
(cf. Exercise 28.3). In the manner of 28.15—we omit the details—it is found that 
Pi 
'ij 
hi 
r . n — m\ r / n — m + 1 
p \ 2 / \ 2 
-77 
Jfc 
+ r 
Wl = l 
rfl^ + ^r'" 
m + 1 — & 
, (28.58) 
2 
where we now use m as an index of summation, reserving & for the number of samples. 
This gives us the moments of L. 
In the case k = 2 we have 
- p - 1 
2 
r 
71 
1 \ r ( n 
r 
Mr 
Fl !L i +r )F 
n 
p — 1 
9 
(28.59) 
and hence the distribution of L is in the form 
1 
dF 
B n-p-1p 
2 ' 2 
L*(n-p-3) (1 __ £,)*(p-2) ^Z/< 
. (28.60) 
In the case k = 3 we find 
DISCRIMINATORY ANALYSIS 341 
which, in virtue of the relation 
becomes 
r(n-2)r(n-p -2+2r) 
lr ~T{n-2 '+ 2r) r(n-'p- 2) { ' 
These are the moments of the distribution 
dl = ^g^-1—-} WLY-^ (1 - VLY-* dL, . . (28.62) 
a rather unusual form. The results are due to Wilks. 
28.18. The line of generalisation of univariate analysis will now probably be clear. 
Corresponding to most of our results for a single variate there will be a generalised result 
for p variates ; and, in fact, if we like to regard the ^p-variate as a vector we can often draw 
direct analogies between results for vectors and those for the (univariate) scalar. It is 
of special interest to observe that the role played by the variance in univariate theory is 
taken over by the determinant of the dispersion matrix in multivariate theory. 
Up to this point we have generalised the distribution of variance (the ^-distribution) 
into Wishart's form, and the ^-distribution into Hotelling's form. 
Other results which suggest themselves for generalisation are regression and variance 
analysis. But in a sense our treatment of regressions is already general, for we have 
discussed the regression of one variate on p — 1 others. Below we shall go further and 
examine the relations between p dependent and q independent variates. In vector 
language, we consider the regression of a #-way vector y on a #-way vector x. We have also 
considered the analysis of variance for the bivariate and trivariate case in Chapter 24 
under the title of analysis of co variance, and since the interest lies mainly in the direction 
of regressions we shall not take the subject further here, though it is capable of 
development and even, perhaps, of application if data become available in sufficient abundance. 
In the remainder of the chapter we shall, in the first instance, deal with an offshoot of 
regression theory which has some interesting taxonomic applications, namely 
discriminatory analysis ; and we shall then proceed to the general problem of the relationship between, 
two sets of variates. 
Discriminatory Analysis 
28.19. Suppose we have p observations for each of 2n sample members, and that 
each member can have emanated from one of two populations, n to each population. We 
require to find some measurement depending on the p observations which will enable us 
to assign subsequently drawn members correctly to their parent populations with the 
greatest assurance of success. For this purpose we shall find p quantities X1 . . . P and 
a discriminant function X related linearly to the variates by 
X = ti Xj (28.63) 
The criterion on which we shall rely is that the A's must be chosen to maximise the ratio 
of the difference between sample means to the standard deviation within the two classes. 
Any linear function of type (28.63) has variance S, given by 
£ = A* 71 a{j , (28.64) 
342 ' MULTIVARIATE ANALYSIS" 
where, as usual, afj is the covariance of xi and xj which we assume to be the same for both 
populations. Further, if the difference of the two means of as, is dj9 the difference of the 
function X for the two samples is 
D = #d< (28.65) 
We have then to maximise for variation in X the function 
D2 M)2 
8 tfPai0>. 
This gives for each X 
leading to equations typified by 
IdS ^SldD 
2dX ~~ D dV 
P atj — ~ <!$. 
Multiplying by aik and summing over i, we have 
tf a., aik = -- d* aik 
or, replacing k by j7 
= # «$* = Xk 
s 
M = -=r d- a)K 
D % 
S 
. (28.66) 
. (28.67) 
. (28.68) 
This determines the x's, except for the constant ^ which can be chosen at will so far as the 
discriminant function is concerned. If c is some constant, we have 
/? = c di ai7'. . . . . . (28.69) 
The result also holds if there are n1 members in the first sample and n2 in the second. 
Equation (28.65) remains true, and the rest of the analysis is the same as for equal class- 
numbers. 
Example 28.2 (from R. A. Fisher, 1936a). 
Measurements were made on fifty specimens of flowers from each of two species of 
iris, setosa and versicolor, found growing in the same colony. Four measurements were 
taken, viz. sepal length, sepal width, petal length, and petal width. We denote them by 
xu x2, x5 and xA respectively. 
The means of the specimens were (in centimetres) :— 
Difference 
(V~S). 
0-930 
- 0-658 
2-798 
1-080 
Variate. 
Xi 
iii) 
iT3 
X± 
Versicolor. 
5-936 
2-770 
4-260 
1-326 
Setosa. 
5-006 
3-428 
1-462 
0-246 
DISCRIMINATORY ANALYSIS 
343 
The sums of squares and products about the means were (in cm.2) 
/y-t 
tCi 
191434 
90356 
9-7634 
3-2394 
fct/4* 
9-0356 
11-8658 
4-6232 
2-4746 
9-7634 
4-6232 
12-2978 
3-8794 
The inverse matrix is, in cm."2 :— 
Jbtj 
3-2394 
2-4746 
3-8794 
2-4604 
X, 
0-118,7161 
0-066,8666 
0-081,6158 
0-039,6350 
3C< 
0-066,8666 
0-145,2736 
0033,4101 
0-110,7529 
0-081,6158 
0-033,4101 
0-219,3614 
0-272,0206 
X.t 
0-039,6350 
0-110,7529 
0-272,0206 
0-894,5506 
We need not bother to divide these quantities by n because there is an arbitrary 
constant in our discriminant function which absorbs it. The matrices are diagonally 
symmetric, and it is not always necessary to write out the values below the diagonal as we 
have done here. 
From (28.69), with c = 1, we then find— 
A1 = - 0-031,1511 X- = — 0-183,9075 
A3 = 0-222,1044 A4 = 0-314,7370. 
If we choose the coefficient of xx to be unity the discriminant function is then 
X = xx + 5-9037#a — 7-1299:rs - 10-1036^. . . . (28.70) 
The mean of X for versicolor, obtained by substituting the means of the afs for that species, 
is found to be — 21-4815, and that for setosa is 12-3345. The difference is thus 33-816 cm. 
Let us compare this with its standard error to see whether it is significant of real differences 
in the values of X for the two species. 
From the matrix of sums of squares and products we find 
= 1085-5522, 
where the A's are, of course, the coefficients in (28.70). N here is the number of degrees 
of freedom of the estimate of the variance. There are 100 members altogether, with 99 
degrees of freedom, but we have eliminated four corresponding to the means of the four 
variates. We therefore take N to be 99 — 4 = 95, and find 
varl = 11-4269. 
This is the variance of a single value. That of the difference of the two means of 50 values 
is obtained by division by 25 and is thus 0-4571, the corresponding standard error being 
0-676. 
The observed difference of means, viz. 33-816, is about 50 times this amount, and 
there is thus a real difference in the values of X for the two species. In other words the 
discriminant function is a good one. It is best among the linear functions of the x's because 
N var X = X1 Xj ai} 
O Jt JL 
MULTIVARIATE ANALYSIS 
we have chosen it so that the difference of two values, divided by their estimated standard 
error, shall be the greatest possible. To use the function we should, given a flower of 
doubtful species, calculate X for it and assign it to one species or the other according as 
X were nearer to the mean value of X for one species or the other. If, of course, 
the observed value differed from the mean values by more than twice the standard error 
of each, we should begin to doubt whether it belonged to either. 
The analysis may be put in rather a different way. Suppose we analyse the variation 
of X between and within species. The sum of squares between species in the 50 X 2 
classification is 
where Xl5 Z2 are the respective means and X the mean of the whole. This reduces to 25.Z)2. 
The sum of squares within classes is 1085-55 with 95 d.f., as found above, and we have— 
Sum of Squares* 
Totals .... 
28,588-05 
1,085-55 
29,673-60 
d.f 
95 
99 
Our method of selecting the discriminant function has been such as to minimise the sum 
of squares within species and, for constant total, to maximise the sum between species, 
and hence to minimise the ratio of the latter to the former. For the moment we cannot 
assume that this ratio may be tested in the ^-distribution in the usual way, though we shall 
see presently that this is so. 
28.20. The relationship of discriminatory analysis for two classes and the theory of 
regression may be brought out by introducing a formal variate y for the classes. If there 
are % members in one class and n2 in the other we shall assign the values 
Tic 
Wi 
nx + n2 
nx -f n2 
to the ^/-variate for the two classes respectively. The mean of y for the whole sample is 
then zero and the sum of squares is 
nx n2 
nx + n2 
£, say. 
Considering now 
Y = ti Xj 
3.71) 
. (28.72) 
as a regression equation, we find for the coefficients X 
EiYxj) - tt 2 (xiXj) = 0, 
2(YXj) -?l 
or 
Now 
a.tj = 0. 
. (28.73) 
ZiYxA 
- 2 y ( \ n 
1/ — ; Z/j \Xa) — 
Wx + n2 v nx + n 
—— L2 (Xa), 
DISCRIMINATORY ANALYSIS 345 
where the suffixes of the 2"s relate to the first and second classes, 
rij -+- n2 
= Zty. 
Thus 'Qd5 -=Xla^ (28.74) 
which is another way of writing (28.69) with a particular value for the constant c. 
28.21. Pursuing the analogy with regression analysis further, we see that since 
and E (Yx.) = C^ 
we may analyse the sums of squares as— 
Sums of squares. cl.f. 
C Jf d, p 
£ (1 - tt d4) nx + n% — p — 1 
rQ nx + n2 - 1 ... (28.75) 
a 
as for a regression line. If R is the multiple regression of Y on the x-variates, 
R* = X1 d,t (28.76) 
In ordinary regression analysis we may test the ratio U2/(l — J?2), multiplied by 
suitable constants, in the ^-distribution ; but this depends on the assumption that the 
dependent variate ?/ is normal for any fixed #'s. Here we have the case when the dependent 
variate is fixed but the cc's are normal. The test still holds in such a case, the reason being 
the kind of duality we noted in 28.14 in arriving at Hotelling's distribution. The 
distribution of angles between a fixed plane and a random vector is the same as that between 
a fixed vector and a random plane. Consequently the table of (28.75) can be regarded 
as an analysis of variance and the 3-test applied. 
28.22. We may extend the discriminant function to the case when the property to 
be discriminated is not, as above, a matter of allocation to one of two classes, but to several 
which may in particular be determined by certain values of a continuous variate. If we 
have various measurements of p ^-variates corresponding to values of a y-variate, we may 
form the regression of y on the x's and use the resulting function as a discriminator. As 
in the case of dichotomy, the regression will maximise the difference between classes as 
compared with intra-class variation ; and its significance may be tested in much the 
same way. 
Example 28.3 (from M. M. Barnard, 1935). 
An investigation was undertaken into the changes taking place over time of the 
characteristics of certain Egyptian skulls. There were four sets of skulls, known to be from 
Late Predynastic, Sixth to Twelfth, Twelfth to Thirteenth and Ptolemaic dynasties respect- 
346 
MULTIVARIATE ANALYSIS 
ively, and the relative time-intervals were taken to be in the proportions 2:1:2, so that 
the values of t for the four periods may be taken to be respectively — 5, — 1, + I, + 5. 
For the skulls four measurements were selected : 
x1} basi-alveolar length ; 
x2, nasal height; 
x3, maximum breadth ; 
#4, basi-bregmatic height. 
It is required to find a function 
X = A1 xx + A2 x2 + A3 xz + A4 #4 
which will best discriminate between skulls belonging to different periods. 
The means of the series were as follows, the sample numbers also being shown :— 
Variate. 
Series I 
K = 91). 
133-582,418 
98-307,692 
50-835,165 
133-000,000 
Series II 
(n2 = 162), 
134-265,432 
96-462,963 
51-148,148 
134-882,716 
Series III 
(n8 - 70). 
134-371,429 
95-857,143 
50-100,000 
133-642,857 
Series IV 
(n4 = 75). 
135-306,667 
95-040,000 
52-093,333 
131-466,667 
The sums of squares and products about the means are- 
JUs 
JU< 
445-573,301 
9073-115,027 
1130-623,900 
1239-221,990 
3938-320,351 
2148-584,219 
2255-812,722 
1271-054,662 
8741-508,829 
The mean value of t, I, for the 398 observations is — 0-432,161, and the values of t 
for the four series are accordingly 
— 4-567,839; -0-567,839; 1-432,161; 5-432,161. 
The sums E Xj (t — I) are respectively 
tAj-t 
tAj 2 
x% 
x& 
718-762,86 
- 1407-260,75 
410-101,94 
- 733-668,32 
and finally, E (t — i)2 = 4307-668,32. 
We could obtain the coefficients A from the reciprocal of the matrix above on the lines 
of the previous example. It is also instructive to observe, from the analogy with 
regressions, that instead of that matrix we may use the matrix (depending on one extra degree 
of freedom, 395 in all) obtained by adding to the sums of squares the regressions on time. 
DISCRIMINATORY ANALYSIS 
347 
For instance, instead of 9661-997,470 we have 9661-997,470 + (718-762,86)2/4307-668,32. 
The resulting matrix is 
£E, 
9781-927,828 
The reciprocal of this is (units = 10~6) 
#, 
x2 
•t!4 
110-368,975 
lift; 
6-938,481 
115-693,529 
210-762,489 
9532-849,476 
* * • 
mm* 
X3 
1199-052,135 
1105-246,827 
3977-363,203 
• • • 
xA 
2026-206,952 
2405-414,318 
1201-230,304 
8866-382,928 
xa 
- 28-145,236 
- 24-948,984 
273-988,409 
Xa 
- 23-361,935 
- 30-767,069 
- 23-666,591 
129-990,069 
The resulting values of X are 
X1 = 0-075,156,739, 
W = 0-144,6* 
A2 = 
X" = 
0-145,490,050, 
0-078,538,419 
and these, or constant multiples of them, give us the constants in the discriminant function 
which will best enable us to assign a skull to the correct period by measurements of the 
four specified variates. 
In this analysis we have 398 members, but of the 397 d.f. we have discarded two with 
the general mean. The d.f. of the sum 4307-6683 ™ U (t ~ i)2 are 395, of which four are 
attributable to regressions on the other variates. For the contribution of these four we 
have 
X1 X 718-762,86 ■■{- etc. = 375-6657. 
The analysis of variance is thus — 
Sum of Squares. 
Regression 
Remainder 
Totals 
375-6657 
3932-0026 
4307-6683 
d.f. 
4 
391 
395 
Quotient. 
10-0563 
The analogy of the discriminant function with regressions noted above may be used 
to provide standard errors of the coefficients X. In our present case the variance of A1 
is obtained by multiplying the remainder quotient, viz. 10-0563, by the term corresponding 
348 MULTIVARIATE ANALYSIS 
to x\ in the reciprocal matrix of sums of squares of the se's, namely 110-368,975 x 10~6. 
This gives a standard error of 0-0333. We obtain finally 
A1 = 0-0752 ± 0-0333 
42 = __ 0-1455 ± 0-0341 
A3 = 0-1446 ± 0-0525 
A* = - 0-0785 ± 0-0362. 
All coefficients exceed twice their standard error, and hence all the variates are useful in 
discriminating between skulls of different periods. 
I am indebted to Dr. M. S. Bartlett for the calculations of this example. His results 
differ from those reached by Miss Barnard in her original investigation since she took an 
unweighted regression of the variates with time, whereas he has weighted the values 
according to sample numbers. He also notes that the significance of the results has been 
tested above on the basis of variability within classes, but that a fuller analysis of the means, 
bringing back the two degrees of freedom discarded, reveals further differences between the 
series. Thus, though the discriminant function will efficiently sort the series examined in 
relation to their periods, we must be cautious about associating the observed differences 
with the time-changes. 
Canonical Correlations 
28.23. We now turn to consider the general theory of the relations between two 
sets of variates xx . . . xp and xp+l . . . %p+q9 where we suppose that p < q. Following 
Hotelling (19366), we shall show that in general there can be found linear transformations 
to variates fx . . . t=p> |p+1 . . . £p+q such that 
(a) all the |'s have unit variance and zero mean ; 
(b) any £ in the ^p-group is independent of the other £'s in that group ; 
(c) any | in the g-group is independent of the other f's in that group ; 
(d) the correlation between any f in the ^9-group and any £ in the g-group is zero except 
for p correlations px . . . pp, which may be taken to be the correlations between 
c,l and £p+i' ?2 and fp+2> • • • £p and c^p- 
The variates £ are then said to be canonical variates and the p's canonical correlations. 
This part of our work is, fundamentally, the reduction of two quadratic forms and an 
associated bilinear form to canonical types and does not depend on the distribution laws 
of the variates. Furthermore, the reduction can be carried out either on the population 
or on the sample. In the latter case it will yield sample canonical correlations which may 
be written rx . . . rp and regarded as sample-values of the parent p's. 
We will suppose that our variates x have zero means and dispersions denoted by ai}, 
where, for the time being, we use a to denote a variance or covariance instead of the more 
usual a2. Those dispersions in the £>-group we denote by Greek affixes : a^, and those 
in the g-group by Roman affixes : atj. For a covariance of a p-variate with a ry-variate 
we write one Greek and one Roman affix : aai. 
Consider now a particular pair of variates given by 
f ~ faX«' a " !' " ' " P\ (28.77) 
da a;,,, a = 1, ... Q\ v 
ii — a Xf. j a — \-, . . . q i 
If their variances are unity we have 
d« db aZ = ' f" (28-78) 
CANONICAL CORRELATIONS 
349 
We will also impose the condition that their correlation R is stationary for variations in 
the coefficients c and d, i.e. that 
B = c* da aaa = stationary. .... (28.79) 
hquatiuns (28.78) and (28.79) then require an unconditioned stationary value of 
c* da Gaa - P ca c^ <ra/3 - ^ d« d* aab ... (28.80) 
where X and // are undetermined multipliers. This leads to 
ca a, 
rxa 
- fi db aab = 0 
. (28.81) 
Multiplying: the first equation by da and summing and the second by ca and summing, 
we have, in virtue of (28.78) and (28.79), 
XX, — /, = n. 
. (28.82) 
Equations (28,81) will then be soluble for the p + q unknowns c and d if the determinant 
of their array vanishes, that is if, writing X for the constants // and X, 
/M 
11 
~~ ^a\p 
Ail 
t>i 
la 
a 
i" i. i 
a 
pp 
P+1,P 
al,PJrl 
m 
— ™p+l,p+l 
fT 
/i'm/, I 
a 
P + Q, P 
-la. 
p + q,p-hl 
ahP + Q 
ap, p+q: 
Xa, 
p + q,P+Q 
0 
am equation determining X. 
a somewhat different form 
• (28.83) 
Before studying it further we will throw the equation into 
28.24. We may write (28.83) as 
- Xa, 
aft 
a 
it* 
a 
a) 
Xau 
0. 
. (28.84) 
Multiplying the first p rows by - X and dividing the last q columns by - X we find the 
equivalent form 
(-D 
a—p 
X*a. 
oc/3 
a. 
ip 
a, 
a; 
au 
0. 
. (28.85) 
Writing, in conformity with our usual notation, {&**) for the matrix inverse to {au) and 
remembering that 
aa aik = <%, 
let us multiply (28.85) on the left by 
<5« 
0 
Vik Vyk 
a 
li 
. (28.86) 
35o MULTIVARIATE ANALYSIS 
The product of determinants is then 
W <ffiy ~ Oifi °ik Oyk '; dy a«j - aik °yk °ij 
^ ffpy - Oif, oik aYk 
«" <tv 
0 
4 
which gives 
(- W* I P c^ - a,? &<* ayk | = 0, . . • (28.87) 
a determinant with p rows and columns multiplied by a power of A. 
28.25. Returning now to our original problem, we see that if a simple root of (28.83) 
is substituted in (28.81) the c's and d's are determinate, except of course that they may be 
replaced by — c and — d. For a root of multiplicity m they are determinate except for 
m — 1 assignable constants, a result we take without proof from the theory of algebraic 
forms (reference may be made to Hotelling's paper for details). 
From (28.87) we see that the equation in X has p + q roots. It cannot have fewer, 
for the coefficient of the highest power of X in (28.83) is the product of two principal minors 
which do not vanish unless the variates are linearly dependent, a case which we exclude 
from the discussion. Of these p + q roots q — p are zero. The remaining 2p can be 
grouped in pairs, each of which is the negative of the other. There are thus roots which 
we may write ± pl9 . . . i p.p. We choose as the roots those which are not negative and 
proceed to prove that they are the canonical correlations as we have defined them. That 
they are, in fact, correlations follows from (28.82). 
Suppose we have a root py and determine the corresponding constants c, and dy and 
hence a pair of variates £y and r\y. Then we have, from (28.81), 
Cy a(xa = Py ^y aab\ /t>n ftQx 
ay °a.a ~ Py °y axPJ 
Similar equations obtain for a second pair, say £, and rjd. Between these four variates 
there are six correlations, two of which are py and pd. We wish to show that the other 
four vanish. They are 
E (SY £6) = oy 4 a^ E (tJy %) = d- «# aah 
E (Sy Vd) = c* dbd aah E (f, rly) = eg d« a(0. . . . (28.89) 
Multiply the first of (28.88) by d« and sum. Using (28.89), we have 
E (fy Vd) = Py E (Vy Vd) (28.90) 
Similarly from the second of (28.88) multiplied by cj, 
E (£> rh) = Py E (£y ft) (28.91) 
Interchanging y and d we find from (28.90) and (28.91) 
PyE (VyVd) = P6® (£y£d)' .... (28.92) 
Equally, again interchanging y and d in (28.92) we have 
P6 E (rJy %) = Py E (iy ft). .... (28.93) 
CANONICAL CORRELATIONS 
351 
Thus, unless p* — pf, 
E (L Sd) = E (rjy m) = 0. 
(28.94) 
It follows from (28.90) and (28.91) that the other correlations also vanish. 
We have only to round off the proof by showing that if p is a root of multiplicity m 
the property still holds. This follows from the consideration that we may then choose 
our c'& and d's to obey certain orthogonal conditions ensuring that 
E (SY £d) + E (rh Vd) = 0. . 
It will then follow from (28.92) that each expectation vanishes unless py 
(28.95) 
= pd = 0 ; and 
even in this case, (28.91) and (28.92) show that two expectations vanish, and we may then 
choose our assignable constants so that the others vanish. 
28.26. When the variates are put into canonical form the dispersion matrix reduces to 
1 
0 
0 
pi 
0 
0 
0 
0 . 
1 
0 . 
0 
p2 ■ 
0 
0 
. 0 
. 0 
. 1 
. 0 
. 0 
■ ■ pp 
. . 0 
pi 
0 
0 
1 
0 
0 
0 
0 . 
Pi • 
0 . 
0 
1 
0 
0 
. 0 
. 0 
Pp 
. 0 
. . 0 
. . 1 
. . 0 
. 0 
. 0 
. 0 
. 0 
. 0 
. 0 
. 1 
. (28.96) 
with a determinant equal to 
(1 -p?) (1 -pi) ... (1 -pp. 
Example 28.4 (from Hotelling, 19,'Hi/;, dealing with data of T. L. Kelley). 
140 seventh-grade school children were given four tests in (a) reading speed, (6) reading 
power, (c) arithmetic speed, and (d) arithmetic power. It is required to find canonical 
variates for the two reading tests and the two arithmetic tests. 
The correlations between the variates were — 
.*\ 
1 -0000 
0-6328 
0-2412 
0-058(1 
The determinant (28.83) becomes 
t ( i) 
0'(k 
10000 
00553 
0-0655 
j\ 
0-2412 
0-0553 
1-0000 
0-4248 
rv* 
0-0586 
00655 
0-4248 
1-0000 
X 
0-6328A 
0-2412 
0-0586 
— 0-6328A 
_ X 
- 0-0553 
0-0655 
0-2412 
- 0-0553 
-A 
— 0-42482 
0-0586 
0-0655 
— 0-4248A 
-A 
0 
352 MULTIVAEIATE ANALYSIS 
or 
0-491,370 24 - 0-078,803,4 A2 + 0-000,362,490 = 0, 
giving P = 0-155,635 or 0-004,740 
with X == 0-3945 or 0-0688. 
To find the transformed variates themselves we use (28.81). For instance, with the 
root 0-3945 for /li, we have 
c1 + 0-6328 c2 - 0-6114 d1 - 0-1485 d2 = 0 
0-6328 c1 + c2 + 0-1402 d1 - 0-1660 d2 = 0 
- 0-6114 c1 + 0-1402 c2 + d1 + 0-4248 d2 = 0 
- 0-1485 c1 - 0-1660 c2 + 0-4248 d!1 + d2 = 0 
The last equation is linearly dependent on the other three, so adds nothing. In the other 
three we solve for the ratios of c's and d's, finding 
c1:^: d1 : d2 =■ - 2-7772 : 2-2655 : - 2-4404 : 1. 
Thus the transformed variates are 
/Clfi = - 2-7772 xt + 2-2655 x, 
Jc2 rj1 = — 2-4404 xz + x^ 
where kt and lc% may be chosen so that the variances of ^ and r/1 are unity, if desired. Similar 
•equations with the root 0-0688 will give us a further pair of canonical co-ordinates. Those 
we have worked out have the maximum correlation, the other pair having the minimum 
and therefore being of less interest. 
28.27. In practical cases it is of some importance to know whether an observed 
canonical correlation r1? say, is significant of real correlation. The problem has been solved 
for large samples but not completely for small samples. We shall conclude this chapter 
with a short account of the main results which have been reached. 
For large samples we shall show that, for the standard error of a canonical correlation, 
1 
varr = -(1 - r2)2 (28.97) 
n 
•a remarkable result showing that the variance is the same as for a product-moment 
coefficient. 
Denoting as usual the sample covariance by a^ we have to the first order 
E {au) = Oij (28.98) 
To the same order, 
Z K- akl) = — E { E (xia xja) E {xkp xw) 1. 
If a -^ j8 the sums on the right are independent, and there are n (n — 1) such cases. When 
a = p we have n terms such as 
E (^a Xja Xka. Xla.) = <^ij % + <*il °jk + <*ik °jl> • ■ *' • (28.99) 
as follows from the consideration that the characteristic function of the multivariate normal 
form is 
exp(—£o^$*#) 
(of. 15.12, vol. I, p. 376). 
CANONICAL CORRELATIONS 353 
Hence we have 
■a i .71(71 — 1) , n . , , . 
1 
= <*ij aki + —K/ <*jk + <*ik <ty)- • ■ • • • • (28.100) 
$ (daa rfaw) = E (al} ahl) — o^ ald 
1 
= - (<*m ajk + aik <Jji) (28.101) 
7b 
Now for any canonical correlation r we have 
Thus 
r==c-d*aflt£ J (^8.102) 
If now we define for the sampling deviations in c's and ri's corresponding to deviations 
in the a's, 
Zlca = 2; _- Aatw . . . . . (28.103) 
we find 
2 aa/? ca Ac? + ca c^ Zlaa/? = 0 
2 aa6 da Adb + da db Aaab = 0 I. . (28.104) 
Zl^i = aa&-ca Zl# -+- aa6 <#> Zlca + ca d6 Zlaa^ 
Without loss of generality we may now suppose the variates canonical and hence put 
ci = l, ca = c3 = . . . = c*> - 05 d1 = 1, ri2 - . . . = (¥ = 0. We then find— 
2/lc1 + zlan = 0, 2Adl + Jai)+liP+1 = 0 \ 
Arx = r,. Ad1 + rx Ac1 + Aah p+l J 
Substituting from the first two in the third of these equations we have 
Art = Jalf /m — I'/'i (Zlalx + ^m> ,p+l). . . . (28.106) 
Similar equations apply for any other simple root, e.g. 
Aft == ^J^2, i> + 2 ^a (^&aa ~\~ A(2p + 2, p + 2)' 
Squaring these equations and substituting from (28.101) we find 
nE {Ar^ = (I - rj)3 
jS? (/b'l5 Jra) = 0. 
It follows that 
var rx = - (1 — pf)2 
cov (rl5 ra) = 0 
to our order of approximation. 
(28.107) 
28.28. Equation (28.107) applies to a simple non-vanishing correlation. If a 
canonical correlation vanishes and p = q, the result holds, with the qualification that sample 
values of r near the zero root must be allowed to have positive or negative values, or 
alternatively that the distribution of r is that of absolute values of a normal variate (cf. Exercise 
28.7). If p = 2, q > 2 a zero root is of multiplicity q at least. In this case, if it has exactly 
A.s.—vol. ii. a a 
354 MULTIVARIATE ANALYSIS 
multiplicity qy nr2 is distributed as %% with q — 1 degrees of freedom. For the proof of 
this result see Hotelling (19366). 
There is another rather curious difficulty in testing the significance of roots of the 
equation giving the canonical correlations, namely, that if several roots exist it is not 
possible to relate them with certainty to specified parent correlations—any one might have 
arisen from any one of the parent values. This is not serious for large samples when the 
roots are distinct, since the sample values cluster closely round the parent values ; hut 
for small samples or canonical correlations in the parent which are close together it presents 
a theoretical problem of a novel kind. See Hotelling (19366) and Bartlett (1941) on 
this point. 
28.29. We proceed to find the sampling distribution of canonical correlations in the 
case when the parent values are all zero and the ^-variates and ^-variates accordingly 
independent. 
Reverting to equation (28.87) in the form appropriate to samples/we have 
| A2 a,pY — aifi aik ayk | = 0. . . . . (28.108) 
We write 
and a,fr = Zpy + tpy, (28.110) 
so that (28.108) becomes 
\^(^y +hy) -fo,| =0 (28.111) 
The significance of this device is that z and t are distributed independently in Wishart's 
form, as we now proceed to show. 
One instructive way of looking at the problem is to consider the regression of the 
p-way vector y on a g-way vector x. Corresponding to the univariate equation 
y = bx + e, . . . . . (28.112) 
where e is a residual, we have 
Vol ~ b<x %i \ #£a> ..... (28.113) 
where the 6's are given by minimising the sum of n values 
namely, by 
or, in our notation for canonical variates, 
which yields 
bl = ««*<&**. (28.114) 
We may analyse the variance of y in the form— 
2 (yl) = 2 (6* a* + *.a)2 
= Kbkaaik + 2{xia)\ .... (28.115) 
corresponding to the univariate case 
2 {y2) = b*2 (za) + 2 (e2), 
and the two constituents on the right in (28.115) are independent, just as in the univariate 
case. Tbis may be shown by a direct extension of 22.19. 
CANONICAL CORRELATIONS 355 
Furthermore, if we wish to find the linear function of the y's, say Aaya, which has 
maximum correlation with the x's, we have to maximise the ratio 
^ (*«%«,)' = ^ v K ty <% = r* (28116) 
This is equivalent to maximising unconditionally 
A- V (6* b\ au - r* aafi) = 0, 
giving, for r2, the equation— 
\*>m*ij -r***p\ = 0 (28.117) 
Now in virtue of (28.1.14) this reduces to 
r aoip aij amn a app a 
0 
or 
| rzaaP — a^aPjaPv | = 0, . . . . (28.118) 
which is equivalent to (28.108) with a slight change of notation. This must be so, for 
we arrived at both equations on essentially the same assumptions. Now we see that the 
term on the right in the determinant of (28.118) is the first item on the right of the variance 
analysis given by (28.115), and the other term in the determinant is the sum E (y%) of the 
analysis. It follows that z and t of (28.111) are independent, for they are the constituent 
items of the analysis. Furthermore, the z's will be distributed as sums of squares or 
products about the means with n -™ q degrees of freedom, that is in Wishart's form; and 
similarly the £'s are distributed as q sums of squares or products about the origin, i.e. in 
Wishart's form with n = q + 1. 
28.30. Without loss of generality we may take the parent variances to be unity; 
the covariances are zero by hypothesis. The joint distribution of z and t is then, from 
(28.26), 
f p 1 
| l | ft (ff-fl-1) | g |* (n-«z-*~2) exp _|V (£.. _|_ zu) llJdtdz 
(IF = p f , ^ ■ I.1 . = J_ ,x s (28.119) 
%\p(n+D ^ip(p-i) jj J w ? + I " * j r(n q2 % 
In the determinant 
| A2 (z + t) - t | = 0 
put u = A2 and let the roots in u be arranged in descending order of magnitude. Consider 
the distribution for a given value of tfj and z^ which in particular we take to be dtj. Let us 
choose new variates from a set £j7c obeying the orthogonality conditions— 
v 
k=l 
= 0 if i ^ j 
= 1 if i = j (28.120) 
Make the transformation ty = E {£ik £jk uk) . . . . . (28.121) 
k 
*„+*<,=£ (f* £,*)= 3* (28.122) 
k 
356 MULTIVABIATE ANALYSIS 
Instead of the lp(p + 1) values of tit we will take the p values of u and \p{p — 1) of the 
|'s as our new variates. We have 
t\ = if«fi*«*l = h % <28-123) 
2 I = I f« fi* (1 - *« «*) I = # (1 - «fc) • • • (28-124) 
7c=l 
and have only to consider the Jacobian. This is clearly of degree \p {p — 1) in u, for the 
Jacobian of t and z + t is the same as that of t and z and only * contributes factors in u 
in the former. Furthermore, every term {w,L — Uj), i < j is a factor of J. For consider 
wx — u2 and let us take as our f-variates those for which j > i. Then to satisfy the 
conditions on the others, derivable from (28.120), 
■57- % (lift £jk) = ^5 
0$ij k 
we must have 
3|l2 111' 9ll2 111 
|N = 0, j > 2, 
»f 12 
whence -r-^ = -^r- 2 (£ik 1^ uk) 
= - ZjlJbl (Ul --Us) (28.125) 
In 
Thus every term (u.t — Uj) occurs in J, and there can be no further factors in u because 
the power in u is \p (p — 1). 
Substituting in (28.119) we have, integrating out the I -variates, 
dF = c h {u\^~v-l) (1 - u^ («-fl-»»3) } jrj (%. _ M ) /7rte m (28.126) 
where 
c = _. . (28 V>1\ 
The constant k arises from terms involving n and p in the original density and from the 
Jacobian. It therefore does not involve q and may be written k (n, p). Evaluation of 
k by direct integration is a matter of some difficulty, but we may find it indirectly 
as follows :— 
In (28.126), if we increase q and n by 2s9 the corresponding value of c is 
*<» + *■*> . . . (28.128) 
n \r(q+ 1+2a -*) r(n-q-i 
The only other term in (28.126) which is affected is that in II (u) and, with the original 
CANONICAL COREELATIONS 357 
c of (28.127), the integral of the distribution so modified would give us the moment of 
order * of 77 (u)9 namely of | t |. This may be found in the manner of 28.15 to be 
r/q + l+2s-i\r/n + l-i 
TYSJIT1^ r(n + 2a + l-i\ ' * * (28.129) 
2 
(see Exercise 28.11). It follows that 
p, n + 2s — i 
kll±*LP) = n V 2 ) (28130) 
whence 
k(n,p) ( n - i 
k(n,p)^nr(/)^.)f(p) (28.131) 
It remains to evaluate / (p). To do so we make the substitution in (28.126) 
2vt 
n 
letting n tend to infinity. Our distribution becomes 
dF = J ^p1^' exp (- Z v<) 77 (vt - vt) 77 dv. . . (28.132) 
/ ^ __ 
This may be reduced by successive substitutions of the type 
Vx = Wl5 Vj = Wj + «!, j> 1, 
and choosing <y at each stage so that the term in 77 (v) vanishes (as we may, since the result 
is independent of </). On integration for vu then repeating the process, and so on, we find 
Using the relation 
r (x) l1 (x + J) - 2~2^x V^ -^ (2»), 
wo have 
Thus our distribution is finally 
dJP - c 77 {v> to"*"1 -(1 - n)* <»-*-«~a> } 77 (u, - u5) 77cZu, . (28.134) 
where 
r, n — i 
p 
r = jrt'P ft —* ; r~^ '—=~\ 7 Z T\ , • (28.135) 
a remarkable form obtained in the general case by Fisher (19396), P. L. Hsu (19396), and 
Boy (19396). 
358 MULTIVARIATE ANALYSIS 
We have supposed throughout that q > p. In the contrary case we reverse the roles 
of q and p and hence merely have to interchange p and q in (28.134) and (28.135). 
28.31. Let us consider some special cases. When q = 1 the distribution becomes 
n — 1 
r 
2 
dF = \ ^ u\ (i>-2)(i _ U)k (n-p-8) ^ . (28.136) 
r(»^f^')r(|) 
confirming the distribution of equation (28.40) leading to Hotelling's distribution ; for 
the canonical correlation is then the multiple correlation between the g-variate and the 
23-variates ; and as the former is measured from its mean there is one fewer degree of 
freedom, i.e. n is replaced by n — 1. 
When q = 2 we have 
n — l \ „/n—2 
n\ r [ 'i—i \ r 
gF^ _^^_^ /_\ / ^__^ (w u^ (p-3) r (1 _tt ) (1 _w ) j 
"U (w—p—4) 
2 7 V 2 / V 2 
X («! — u2) duxdu2. . (28.137) 
Writing 
(1 — ux) (1 — u%) = v, 
% + ^2 = ^3 
we find 
dF = -~- -^ ^—- -- (v - 1 + w)*Cp-3) vKn-p-4) ^ fc . (28.138) 
4^(71 -p - 2) r(p ~ 1) 
For given v the limits of w are 1 — v and 2 (1 — y^), and integrating for w we find 
dF = 17^ ^i!L^ . _2 (1 _ Vv)p^ Wv)«-p-tdv 
4:1 (n — p — 2) r (p — 1) _p — 1 
or, for V^? 
dF = „^Ji (l _ y^"1 (V^~P~3^V^ • ■ - (28.139) 
B{n-p -2,p) 
a result due to Wilks—cf. equation (28.62). 
28.32. The distribution of the ^'s does not immediately provide a test of significance 
of the canonical correlations, except when there is only one of them. The criterion 
v =17(1 - u) (28.140) 
is sometimes useful in the general case for testing simultaneously the departure of the 
u's from zero. Cf. Exercises 28.11 and 28.12. 
NOTES AND REFERENCES 
Among earlier papers in which various aspects of the multivariate problem began to 
be studied, reference may be made to Karl Pearson (19266) on the " coefficient of racial 
likeness " and Ragnar Frisch (1929), who independently arrived at the dispersion matrix 
and proposed to call its determinant in standard measure the " scatterance ". Reference 
NOTES AND REFERENCES 359 
to the papers by Wishart (1928), Wishart and Bartlett (1933c) and Hotelling (1931) on the 
generalised product-moment distribution and the generalised " Student " ratio has been 
made in the text. 
In more recent literature three lines of development are discernible :— 
(a) American writers have developed the theory of canonical correlation and multiple 
analysis mainly on algebraic and analytical lines. See Hotelling (1933, 19366), Wilks 
(I932e, 1934, 10356, 1935c, 1936, 1943), Girshik (1939), and Madow (1938). 
(b) English schools have investigated the theory of discriminant functions and 
developed the sampling theory of canonical roots. See R. A. Fisher (1936a, 6, 1938c, 19396, 
l!)40rZ), P. L. Hsu (1938c, 19396, 1941a, c, d), and for illustrative material Martin (1936), 
Barnard (1935), Fairfield Smith (1936) and Wallace and Travers (1938). See also Bartlett 
(LSW46, 1938c, 19396, c, 1941), E. S. Pearson and Wilks (19336), Welch (19396), Lawley 
(1938) and Bishop (1939). Simaika (1941) has proved that tests based on Hotelling's T 
and the multiple correlation coefficient are uniformly most powerful in the class depending 
on a single parameter. 
(c) The Indian school, whose contribution has not been referred to in this chapter, 
has developed some interesting work based on what is known as the D2-statistic. See 
Mahalanobis (1930, 1936a), Mahalanobis, Bose and Roy (19366), R. C. Bose (1936a), R. C. 
Bose and Roy (1938c), and later papers in Sankhyd. If, with two samples from ^p-variate 
populations, d.t is the difference of sample means for the ith variate, the studentised 
/> ^statistic is 
1 
D2 = - avj cL dh 
P 
where ar} refers to the reciprocal of the sample dispersion matrix. Bose and Roy have 
shown that in normal samples this has the same distribution as one of Fisher's forms for 
the multiple correlation coefficient. The corresponding parameter for the population 
A2 - - a" di Sj 
P 
is known as Mahalanobis's generalised distance. 
EXERCISES 
28.1. In a four-variate normal distribution show that the correlation between the 
■novariances av> and aM is 
(Wishart, 1928.) 
28.2. For a pair of normal variates with correlation p, show that, denning v by 
axa2 (1 — p2)' 
we have for the frequency function of v 
f(v) = JJ_._£_>l^ {v^k^ (v)} 
360 MULTIVARIATE ANALYSIS 
for v > 0 and a similar expression with — v for v inside curly brackets if v < 0. Here 
K is the Bessel function of second kind with imaginary argument. 
(Wishart and Bartlett, 1933c. See also K. Pearson and others, 1929.) 
28.3. Show that if h sets of variates a{$, h = 1 . . . lc\ i9 j — 1 . . . p are each 
distributed in Wishart's form, with sample numbers nx . . . nk, then the variates 
k 
E 
k 
are also distributed in Wishart's form with n = ^ (nh). (This follows readily from the 
characteristic function. It is a generalisation of the additive properties of %*.) 
28.4. If a sample of n is chosen from a ^-variate normal population, the variates 
being grouped into k classes xl} x2 . . . xPi; xPi+l ... xPi+p%] . . .; xPl+..Ph_l+i 
. . . xp, consider the function— 
I r{0) 
where ru = 1 and rf) is zero if the variates belong to different classes and equals the 
correlation r^ if they belong to the same class. 
By considering the function 
show that 
C -r, / n 
k Pt 
^(W) = n n \ 
r 
rirL^l + r 
rl lb v 
+ r 
p \ 2 
y n -A. 
i = l p f U — % 
(Wilks, 19356. The distribution provides a test of the independence of k sets of normal variates.) 
28.5. As a particular case of the last exercise, show that if a single variate xx is 
independent of a second set then— 
and hence find the distribution of the multiple correlation coefficient when the parent 
coefficient is zero. 
(Wilks, 19356.) 
28.6. Show algebraically that Hotelling's T is invariant under linear transformations 
of the p variates. 
28.7. If the determinantal equation (28.83) with p = q has a double root equal to 
zero, show that for large samples the value of r corresponding to the canonical correlation 
EXERCISES 
361 
is given by omitting all terms in the determinant when expanded, except those in A2 and 
A0. Noting that the latter is a perfect square, show that r is the ratio of a polynomial 
in the sample dispersions to a non-vanishing function regular in the neighbourhood of 
zero. Hence that (28.107) holds when p = 0. 
(Hotelling, 19366.) 
28.8. In the notation of 28.23, if 
0 
0 
- - - ■■ - - - 
aia 
A 
- 1. . _ 
<*0Li 
_ _ _ 
°ii 
°« 
- - . 
P 
J 
B = 
D = 
a*p 
G 
ca 
vx 
tj 
show that the vector correlation coefficient K defined by 
AB 
find tine square of the vector alienation coefficient Z defined by 
D 
Z 
AB 
are invariant under linear transformations of the variate. Also that 
K- — i p\ p% • • 
Z = (1 - pi) (1 
where the p's are canonical correlations. 
■Pt) 
(I -4) 
(Hotelling, 19366.) 
28.9. In the notation of the previous exercise, k and z being the sample values of 
Ar and Z, show that if the population canonical correlations are all distinct, 
var k 
n <£-j 
2\2 
(i - A) 
var z 
Z 
n 
v 
z* 
p 
cov (1c; z) 
±KZY{\ -pi). 
n « 
In particular, when p = 2, 
varfc= * {(1 -Xa)a~Z(l + Z2) } 
n 
var 2: = (1 — Z + K2) 
n 
cov (k,z) = -*KZ(1+Z -iP). 
n 
(Hotelling, 19366.) 
362 
MULTIVARIATE ANALYSIS 
28.10. In the previous exercise, with p = q = 2, show that, in standard measure, 
and hence derive a test of significance of the " tetrad difference " r13 r24 — r14 r23. 
(Hotelling, 19366.) 
28.11. In the notation of Exercise 28.9, show that 
E (jfc« zf>) 
p 
n 
i = l 
r ( % + a + l 
2 
l\ jp7ft - g + 2ft -» \ ji/ 
n — * 
/»[ g + * ~~M r^ *~g 
A p/n + a + 2/S — i 
2"^ 
(Girshik, 1939.) 
28.12. Find the characteristic function of —logs, where % is defined as in the 
previous exercise, and hence show that — n log z or, to a better approximation, 
— {n — 1 — i (p + q + l)}logz tends to be distributed as %2 with pq degrees of freedom 
when n is large. 
(Bartlett, 1938c.) 
CHAPTER 29 
TIME-SERIES—(1) 
29.1. A time-series, as its name indicates, is a series of values assumed by a variable 
at different points of time. We shall consider only cases where the variable is univariate 
and shall denote its value at time t by ut. The study of such series forms an important 
branch of statistics because the majority of types of time-variation encountered in practice 
are not of the regular functional type in which ut can be represented exactly by a 
mathematical function of t, but present in some degree those irregularities of a random character 
which can only be discussed in terms of probability. One of our main problems, in fact, 
will be to isolate systematic from casual effects in the series so as to be able to study 
them separately. 
29.2. In general it is possible to observe a time-variable at any instant, and thus 
the temporal intervals between successive members of the series need not be the same. 
Practice and theory alike, however, usually require the observations to occur at regular 
intervals, and in the sequel we shall assume, unless the contrary is specifically stated, that 
the interval from each observation to the next is the same throughout the series. As 
a matter of convenience we may take this interval as our time-unit and write the series as 
w^, W"2) '^33 • * ' t) ••• • • • • . (ii<7.X,) 
where t must be an integer. Where a series extends backwards and forwards from some 
given point which we wish to regard as origin we may write it as 
In this chapter and the next we shall study the way in which ut varies with t, such variation 
being in general of the stochastic type, that is to say, involving random variables. 
Some Examples of Time-series 
29.3. Tables 29.1 to 29.5 provide 3 of the kind of variation encountered 
in practice. Table 29.1 (illustrated in Kg. 29.1) gives the annual yields per acre of barley 
in England and Wales from 1884 to 1939. Table 29.2 (Fig. 29.2) shows the human 
population of England, and Wales at ten-yearly intervals from 1811 to 1931. Table 29.3 (Fig. 29.3) 
gives the sheep population of England and Wales for each year from 1867 to 1939. 
Table 29.4 (Fig. 29.4) gives the annual rainfall in London for each year from 1813 to 1912. 
Table 29.5 (Fig. 29.5) gives the average egg-production per laying hen in the U.S.A. for 
each month of the years 1938 to 1940. 
363 
364 
TIME-SERIES 
TABLE 29.1 
Annual Yields per Acre of Barley in England and Wales from 1884 to 1939. 
(Data from the Agricultural Statistics.) 
Year. 
1884 
85 
86 
87 
88 
89 
90 
91 
92 
93 
94 
95 
96 
97 
Yield per 
acre (cwts.). 
15-2 
,16-9 
15-3 
14-9 
15-7 
151 
16-7 
16-3 
16-5 
13-3 
16-5 
15-0 
15-9 
15-5 
Year. 
1898 
99 
1900 
01 
02 
03 
04 
05 
06 
07 
08 
09 
10 
11 
Yield per 
acre (cwts.). 
16-9 
16-4 
14-9 
14-5 
16-6 
15-1 
14-6 
16-0 
16-8 
16-8 
15-5 
17-3 
15-5 
15-5 
Year. 
1912 
13 
14 
* 15 
16 
17 
18 
19 
20 
21 
22 
23 
24 
25 
Yield per 
acre (cwts.). 
14-2 
15-8 
15-7 
14-1 
14-8 
14-4 
15-6 
13-9 
14-7 
14-3 
14-0 
14-5 
15-4 
15-3 
Year. 
1926 
27 
28 
29 
30 
31 
32 
33 
34 
35 
36 
37 
38 
1 » 
Yield per 
acre (cwts.) 
16-0 
16-4 
17-2 
17-8 
14-4 
150 
16-0 
16-8 
16-9 
16-6 
16-2 
14-0 
18-1 
17-5 
1880 
1890 
WO 
1920 
1930 
1910 
Years. * 
Fig. 29.1.— Graph of the Data of Table 29.1 (Barley Yields per Acre). 
mo 
SOME EXAMPLES OF TIME-SERIES 
365 
TABLE 29.2 
Population of England and Wales at Ten-Yearly Intervals from 1811 to 1931, 
(Data from the Registrar-General's Statistical Review, 1933, Part II.) 
Year. 
1811 
21 
31 
41 
51 
61 
71 
81 
91 
1901 
11 
21 
31 
Population 
(millions). 
10-16 
12-00 
13-90 
15-91 
17-93 
2007 
22-71 
25-97 
29-00 
32-53 
36-07 
37-89 
39-95 
40 
^ 30 
si 
o 
«-0 
o 
20 
■■-- ■ ■ ■ ■ ■ BHiMMIW^_^^MWMH MMI^^MHH^HIIIi 
1811 
1831 
1851 
1891 
1911 
1931 
Fig. 29.2. 
757/ 
Years. 
-Graph of the Data of Table 29.2 (Population of England and Wales). 
366 
TIME-SERIES 
TABLE 29.3 
Sheep Population of England and Wales for each Year from 1867 to 1939. 
(Data from the Agricultural Statistics.) 
Year. 
1867 
68 
69 
70 
71 
72 
73 
74 
75 
76 
77 
78 
79 
80 
81 
82 
83 
84 
85 
Population 
(10,000). 
2203 
2360 
2254 
2165 
2024 
2078 
2214 
2292 
2207 
2119 
2119 
2137 
2132 
1955 
1785 
1747 
1818 
1909 
1958 
Yfiai* 
1886 
87 
88 
89 
90 
91 
92 
93 
94 
95 
96 
97 
98 
99 
1900 
01 
02 
03 
04 
Population 
(10,000). 
1892 
1919 
1853 
1868 
1991 
2111 
2119 
1991 
1859 
1856 
1924 
1892 
1916 
1968 
1928 
1898 
1850 
1841 
1824 
Year. 
1905 
06 
07 
08 
09 
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
22 
23 
Population 
(10,000). 
1823 
1843 
1880 
1968 
2029 
1996 
1933 
1805 
1713 
1726 
1752 
1795 
1717 
1648 
1512 
1338 
1383 
1344 
1384 
Year. 
1924 
25 
26 
27 
28 
29 
30 
31 
32 
33 
34 
35 
36 
37 
38 
39 
Population 
(10,000). 
1484 
1597 
1686 
1707 
1640 
1611 
1632 
1775 
1850 
1809 
1653 
1648 
1665 
1627 
1791 
1797 
24 
2Z 
S 20 
o 
•■o 
s 18 
16 
14 
72 
• 
*s 
1865 
Fig. 29.3. 
1885 
1925 
1905 
Years. 
-Graph of the Data of Table 29.3 (Sheep Population). 
1945 
SOME EXAMPLES OF TIME-SERIES 
367 
TABLE 29.4 
Total Annual Rainfall at London in Inches, for each Year from 1813 to 1912. 
(Data from D. Brunt, Phil. Trans. A, 225, 247, 1925.) 
1813 
14 
15 
16 
17 
18 
19 
20 
21 
22 
23 
24 
25 
26 
27 
28 
29 
30 
31 
32 
33 
34 
35 
36 
37 
Rainfall 
(inches). 
23-56 
26-07 
21-86 
31-24 
23-65 
23-88 
26-41 
22-67 
31-69 
23-86 
24-11 
32-43 
23-26 
22-57 
23-00 
27-88 
25-32 
25-08 
27-76 
19-82 
24-78 
2012 
24-34 
27-42 
19-44- 
Year. 
1838 
39 
40 
41 
42 
43 
44 
45 
46 
47 
48 
49 
50 
51 
52 
53 
54 
55 
56 
57 
58 
59 
60 
61 
62 
Rainfall 
(inches). 
21-63 
27-49 
19-43 
31-13 
2309 
25-85 
22-65 
22-75 
26-36 
17-70 
29-81 
22-93 
19-22 
20-63 
25-89 
18-65 
23-06 
22-21 
22-18 
18-77 
28-21 
32-24 
22-27 
27-57 
Year. 
1863 
64 
65 
66 
67 
68 
69 
70 
71 
72 
73 
74 
75 
76 
77 
78 
79 
80 
81 
82 
83 
84 
85 
86 
87 
Rainfall 
(inches). 
21-59 
16-93 
29-48 
31-60 
26-25 
23-40 
25-42 
21-32 
25-02 
33-86 
22-67 
18-82 
28-44 
26-16 
28-17 
34-08 
33-82 
30-28 
27-92 
27-14 
24-40 
20-35 
26-64 
27-01 
19-21 
Year. 
Rainfall 
(inches). 
1888 
89 
90 
91 
92 
93 
94 
95 
96 
97 . 
98 
99 
1900 
01 
02 
03 
04 
05 
06 
07 
08 
09 
10 
11 
12 
27-74 
23-85 
21-23 
28-15 
22-61 
19-80 
27-94 
21-47 
23-52 
22-86 
17-69 
22-54 
23-28 
22-17 
20-84 
38-10 
20-65 
22-97 
24-26 
23-01 
23-67 
26-75 
25-36 
24-79 
27-88 
1860 
1870 
1900 
1910 
1880 1890 
Years. 
Fig. 29.4.—Graph of the Last 50 Terms of the Data of Table 29.4 (Rainfall). 
368 
TIME-SERIES 
TABLE 29.5 
Average Number of Eggs per Laying Hen in the U.S.A. for each Month of the Years 1938-1940. 
(Data from Report of the Bureau of Agricultural Economics, U.S. Dept. of Agriculture, on the 
Poultry and Egg Situation, March, 1941.) 
Year. 
1938 
1939 
1940 
Jan. 
7-9 
8-0 
7-2 
Feb. 
9-9 
9-7 
9-0 
Mar. 
15-4 
14-9 
1,4-4 
Apr. 
17-5 
170 
16-5 
May. 
17-3 
17-0 
17-0 
June. 
14-9 
14-6 
14-8 
July. 
13-6 
13-2 
13-4 
Aug. 
11-8 
11-7 
•11-8 
Sept. 
9-4 
9-3 
9-7 
Oct. 
7-5 
7-4 
7-9 
Nov. 
5-9 
6-0 
6-2 
Dec. 
6-4 
6-8 
6-8 
20 
o 
75 
10 
&5 
- " -■- 
• 
—— t 1. ._! 
i 
L- ■— 
_ 1 ■ ■ 
/""A 
i 
..,. ..i... 
»■ — 
Mar. June Sept. Dec. Mar. June Sept. Dec. Mar. June Sept Dec 
1938 1939 1940 
Date. 
Fig. 29.5.—Graph of the Data of Table 29.5 (Egg Production). 
These series are fairly typical of the kind of material with which our theory has to 
deal. The data of Table 29.1 (barley yields) present a very irregular fluctuation, and so 
far as the eye can see (which is not a decisive test) there is no systematic oscillation and no 
regular movement in mean yields over the period. By contrast, Table 29.2 (human 
population) shows a relatively smooth movement without apparent oscillation. Table 29.3 (sheep 
population) combines a general decline in numbers with marked oscillatory effects which, 
though not perfectly regular, appear to be systematic to some extent. Tables 29.4 and 
29.5 exhibit an oscillatory effect which is definitely seasonal for the latter and much less 
regular for the former, neither indicating a variation, in the periods covered, of the average 
values about which the series oscillate. 
29.4. It must not be overlooked that our method of determining the values of the 
series at fixed equal intervals of time may suppress evidence of oscillatory movements 
which have a period equal to those intervals or to some sub-multiple of them. Suppose, 
for instance, that there was a systematic oscillation in the English population expressible 
ANALYSIS OF TIME-SERIES 369 
by a harmonic component with period of exactly 10 years, or exactly 5 years, or exactly 
3-J- years. Clearly, by observing the series at 10-yearly intervals we should never find any 
evidence of this effect, for it would contribute exactly the same amount to each observation, 
without oscillation. In the population case, of course, we have collateral evidence to 
indicate that no such oscillation exists, but where nothing is known of the series otherwise 
we can never exclude the possibility of a period exactly equivalent to our time-interval. 
Sometimes, in fact, we know that it is there, and choose our interval so as to exclude the 
oscillation from consideration. For instance, in our sheep population we know that there 
is a seasonal effect within the year, which is not brought out in Table 29.2 because the 
sheep census is taken on June 4th each year ; and again, in the rainfall data of Table 29.4 
we have taken as representing the year the whole rainfall within the year, knowing quite 
well that rainfall is seasonal to some extent, even in London. 
29.5. A general survey of these and similar series suggests that the typical time- 
series may be regarded as composed of three parts :— 
(a) a trend, or long-term movement; 
(b) an oscillation about the trend of greater or less regularity ; 
(c) a " random ", " irregular " or " unsystematic " component. 
It is customary to regard the series as composed of these elements superposed one on 
another ; that is to say, we consider the movement of the series as the sum of three 
different components which may be generated by different causal systems. Particular series, 
of course, need not exhibit them all. That of Table 29.2 (human population) seems 
to be almost entirely trend, with perhaps a small unsystematic residual, whereas that of 
Table 29.5 (egg production) appears to be entirely oscillatory, and very regularly so. 
But some series at least exhibit all three. 
29.6. The primary problem of time-series analysis from the statistical viewpoint is 
to isolate the three factors for individual study, and in this chapter and the next we shall 
be mainly concerned with various methods of carrying out the necessary analysis. Before 
proceeding, however, we must look a little more closely into the reality of the effects which 
we are investigating and the basis on which we assume that the analysis is legitimate. 
29.7. Perhaps the easiest component to understand and to remove from the series 
is the seasonal effect. This is a fluctuation imposed on the series by a cyclic phenomenon 
external to the main body of causal influences at work upon it. The oscillation in egg- 
production in Table 29.5, for instance, reflects the rhythm in the reproductive process 
which is found among birds in virtue, ultimately, of the fact that the earth goes round 
the sun once a year. Strictly speaking, we ought to confine the word " seasonal " to those 
effects which are annual in period ; but where no confusion is likely to arise we can apply 
the same word and the same ideas to any phenomenon, generated by strictly periodic natural 
processes, such as " spring " and Li neap " variation in tides or daily variation in 
temperature. We must, however, be careful about extending the notion of seasonality to phenomena 
which are not demonstrated beyond reasonable doubt to depend on strictly periodic stimuli. 
For instance, it would be going too far, in the present state of our knowledge, to speak of 
sunspot variation as seasonal in this sense, and much too far to speak of seasonality in 
crop-yields as determined by sunspots, even if the relation between the two were 
established. We shall return to this point below when defining what we mean by a cycle " 
as distinct from an u oscillation ". 
a.s.—vol. ii. bb 
370 TIME-SERIES 
29.8. As we noted in 29.4, the seasonal effect may already be removed from the 
series by the way in which the data are specified. Where we ourselves have any choice 
in the determination of the data, we may eliminate seasonality in the same way, namely, 
by selecting for measurement of the series a point of time which is fixed in relation to the 
year, such as June 4th for the agricultural returns of England and Wales, or by averaging 
over the year, or (what is much the same thing) by cumulating the series over the year, 
as for instance with rainfall data. 
29.9. The concept of trend is more difficult to define. Generally, one thinks of it 
as a smooth broad motion of the system over a long term of years, but (i long " in this 
connection is a relative term, and what is long for one purpose may be short for another. For 
example, if we were examining rainfall records over a hundred years a slow rise from the 
beginning of the period to the end would be regarded as a trend ; but if we possessed records 
for two thousand years (and the rings in some of the giant redwood trees give an index of 
climatic conditions for periods of this order) the rise over a particular century might appear 
as part of a slow oscillatory movement, so that any inference from the " trend " ina 
particular century to the effect that the weather was likely to continue becoming wetter and 
wetter might be quite false. What inference we should make in practice would depend 
on what we were trying to do. If we were engineers designing a water-supply system and 
wished to provide against droughts of reasonable extent, we might perhaps assume that the 
trend would last as long as our works and proceed accordingly ; but if we were attempting 
to study climatic changes over the face of the earth for geological periods of time we should 
accept the continuance of the trend with the greatest reserve or, more probably, should 
reject it on collateral grounds. 
29.10. However long a series may be, we can never be certain, and often not even 
reasonably sure, that a trend in it is not part of a slow oscillation, except of course when 
the series has terminated (as might, for instance, be the case if we were considering the 
lengths of reigns of the Roman Emperors). In speaking of a trend, therefore, we must 
bear in mind the length of the series to which our statement refers. Perhaps it would be 
more accurate to speak of slow or quick movements rather than of trend and oscillation, 
but even so the distinction between the two would remain a matter of subjective judgment 
to some extent. 
29.11. When seasonal variation and trend have been removed from the data we 
are left with a series which will present, in general, fluctuations of a more or less regular 
kind. Fig. 29.1 represents the kind of series we obtain, since it has no components of 
trend or seasonality. The question then arises, is this residual series systematic in the 
sense that its values can be represented as a function of the time ? Or, on the other hand, 
are the values random in the sense that they could occur, in the observed order, by random 
sampling from a homogeneous population ? Or again, is there some possibility intermediate 
between complete functional variation and complete randomness ? The search for 
systematic effects in residual fluctuation gives rise to several techniques of analysis, the object, 
of which is to detect whether any part of the series is subject to law, and therefore 
predictable, and whether any part is purely haphazard. Jhe former part we shall call systematic,. 
and it will be referred to as an " oscillation " (not a " cycle ", which is a very special case 
of an oscillation, as we shall see later). The remainder of the series we shall call the 
unsystematic component, and refer to its movements as " random ". When a series is a mixture 
DETERMINATION OF TREND 371 
of oscillation and random movement it will not cause any inconvenience to refer to the 
up-and-down movement generally as fluctuation before we have analysed it into its 
constituents ; that is to say, we may speak of fluctuation without prejudice to the possibility 
of detecting oscillatory movements in it. 
In this chapter we study trend and random residuals. In the next chapter we shall 
deal with oscillatory and cyclical components. 
29.12. The logician or the economist who wants to be difficult can always maintain 
that, although any series can be separated into our three specified components as a matter 
of mathematical or statistical analysis, the results throw little or no light on the causal 
influences at work to produce the series. To such a critic we have to concede, I think, 
that in carrying out the analysis we have at the back of our minds the strong possibility 
that the three elements are due to independent causal systems. If he refuses to accept 
this view—and some economists do—we can only invite him to produce a better statistical 
method. 
Possibly the reader will feel, on reaching the end of Chapter 30, that we have not been 
wasting our time, and that our methods do throw light on the way in which time-series 
behave. If not, he shoxild consult some of the references and see whether he finds them 
statistically more satisfying. 
Determination of Trend 
29.13. It is an essential part of the concept of trend that the movement over fairly 
long periods is smooth. This means that we can represent the trend component, at least 
locally, by a polynomial in the time element t. Thus, given the series ut, we may, in the 
first instance, seek for some polynomial 
ut -- a0 + ax t f a2 t2 + . . . + ap tp . . . .- (29.3) 
which will give an account of the trend movement. By taking p great enough we can, of 
course, obtain as close a representation as we like to a finite series ; and how large we 
take p is a matter for decision in particular cases. 
If the polynomial is fitted to the whole series by least squares, it evidently gives the 
curvilinear regression line of ut on the variable /,. This method would then lead to the 
fitting of regressions in the manner of Chapter 22, and we need not repeat here what has 
been said on the subject in that chapter. In Example 22.7 we did, in fact, fit a quartic 
to the population data of Table 29.2 and found a good fit. 
is, however, clear that to obtain a satisfactory trend-curve for data such 
as that of Table 29.3 (sheep population), we should have to take a polynomial of rather 
high order. This may appear somewhat artificial and in any case the coefficients of such 
a polynomial, being based on high-order moments, would be very unstable from the sampling 
viewpoint. A more practical objection, though by no means an unimportant one, is that 
if we add another term to the series, as for example if we are keeping an annual series up 
to date from year to year, the work of fitting has to be done afresh each time. Moreover, 
the trend-line may be affected throughout its length. When, therefore, the series has no 
very obvious trend Such as that of Table 29.2 it is more convenient to use the simpler 
methods described below. 
/ 
372 TIME-SERIES 
Moving Averages 
29.15. An alternative to finding a polynomial which will represent the whole series 
is to determine a polynomial which will represent a part of it, and to use different 
polynomials for different parts. The simplest method, and one which forms the basis of the 
majority of methods of trend fitting, is to take the first m terms (m being chosen at will), 
fit a polynomial of order p, not greater than m — 1, to them, and use that polynomial to 
determine the value in the middle of its range ; then to repeat the operation with the m 
terms from the second to the (m + l)th, and so on, moving on one term at each stage. 
Unless other considerations require it, we take m to be odd, so that the middle point of 
the range corresponds to a value which is actually observed. Otherwise the middle point 
falls half-way between two observed values, or we have to use some value of the fitted 
polynomial other than the middle point, which results in a loss of useful symmetry. 
29.16. Suppose, then, that the number of terms is chosen to be odd and is denoted, 
with a slight change of notation, by 2m + 1. Without loss of generality we may denote 
the terms by u_mi u_^m_1)J . . . u0, . . . um_x, um. If we choose to fit to them a 
polynomial of the pth. order (29.3) we may, in the usual way, determine the coefficients by 
least squares, i.e. solve the equations 
m 
^7 ^ (^ - <^o - • • • - S iP )2 = 0, j = 0 . . . p . . (29.4) 
which will give us equations typified by 
2 (P ut) ~a02 {P) -axZ {P+1) - . . . - ap X (P+*>) = 0. . . (29.5) 
Now the sums 2 (P) are functions of m only. Thus, if we solve (29.5) for a0 we shall find 
an equation of the form 
&o == co + ci u~m ~t~ c2 %_(m_i) + • * • + C2m-H wm3 * • (29.6) 
where the c's depend on m and p, but not on the u's. 
Now u0 assumes the value a0 at t = 0 and hence this value, as given by (29.6), is the 
value we require for the polynomial. As we see, this is equivalent to a weighted average 
of the observed values, the weights being independent of which part of the series is taken. 
Thus our process of fitting a trend-line consists of determining the constants c (which 
depend on m and p and therefore give us a twofold element of choice) and then calculating, 
for each consecutive set of (2m + 1) terms in the series, a value given by (29.6). If the 
terms are ux . . . u2m+x, the calculated value will correspond to t = m + x. There will 
be no values corresponding to the m terms at the beginning and the m terms at the end. 
Example 29.1 
Suppose we have a series and wish to fit a curve which best approximates to sets of 
seven points ; and suppose we regard a cubic as providing a satisfactory approximation. 
What are the weights of the moving average ? 
We have m = 3 and p = 3, and our polynomial is 
111 = (3/q -j- di t -j- &2 t ~\- (t>z t . 
MOVING AVERAGES 
373 
Taking our origin at t = 0, we find, for equations (29.5), in virtue of the fact that E (tk) 
for odd fc, 
E (u) = 7<x0 + 28a2 
E{tu) = 2Sax + 196a3 
E {t2u) = 28<x0 + 196a2 
2" (i3^) = 196a! +■ 1588a3 
= 0 
giving, for aQ, 
a0 = —{lE(u)-E(t*u) } 
21 
{— 2w_3 + 3^_2 + 6^] + 7^o + 6^! + 3^a — 2^3}. 
We may write this conveniently as 
1 
21 
[- 2, 3, 6, 7, 6, 3, - 2] 
or, when symmetrical formulae are used, as in the present case, by 
[-2, 3, 6, 7 . . . ], 
denoting the middle term by heavy type. 
To take a simple illustration. Suppose the series is given by the following values :- 
t : 1 
ut : 0 
8 
27 
5 
04 
6 
125 
216 
8 
343 
9 
512 
10 
729 
We have, for the trend value at t 
a 
0= i-{(--2x0) + (JJxl)+(6x8) + (7x27) + (6x64) + (3xl25)-(2x216) }- --{567} 
27, 
Similarly, at t = 6 we find 
a« = 
— {(- 2x8) h (» X 27) + . . . -(2 X 512) } 
= 1.25. 
In both cases the trend-value is equal to the actual value of the series, and this obviously 
must be so when we note that we are fitting a cubic to the series 
'Wt 
It will be observed that in this example we should have obtained the same value for 
a0 if we fitted quadratics instead of cubics ; and generally the case p odd includes the 
case of the next lowest (even) value of p, so that we need not give separate formulae for 
even p. 
29.17. Writing a0 [fc] for the value of a0 calculated in the above manner for an average 
of k successive terms, we find the following formulae up to p = 5. The reader may care 
to verify them for himself as an exercise. 
374 
TIME-SERIES 
Quadratic and Cubic 
a0 [5] 
[7] 
[9] 
[11] 
[13] 
[15] 
[17] 
[19] 
[21] 
[7] 
[9] 
[11] 
[13] 
[15] 
[17] 
[19] 
[21] 
35 
J^ 
21 
1 
231 
1 
429 
1 
I Oj JL Z, X / , . . .J 
[-2, 3, 6, 7, . . .] 
143 
1 
1105 
1 
323 
1 
2261 
1 
3059 
- 21, 14, 39, 54, 59, ... ] 
36, 9, 44, 69, 84, 89, ... ] 
11, 0, 9, 16, 21, 24, 25, ... ] 
78. 
13, 42, 87, 122, 147, 162, 167, . . . ] 
J&KJ JL 
1 
429 
1 
[5, 
[15, - 55, 30, 135, 179, . . . ] 
429 
;[18s 
45, 
10, 60,-120, 143, . . . ] 
1 
46,189 
1 
4199 
1 
— [110, - 198, - 135, 110, 390, 600, 677, 
[195, - 195, — 260, - 117, 135, 415, 660, 825, 883, . 
7429 
1 
> (29.7) 
-21, - 6, 7, 18, 27, 34, 39, 42, 43, ... ] 
- 136, - 51, 24, 89, 144, 189, 224, 249, 264, 269, . . . ] 
— 171, - 76, 9, 84, 149, 204, 249, 284, 309, 324, 329, . . .] 
Quartic and Quintic 
30, 75, 131, . . . ] 
1 
[2145, - 2860, - 2937, - 165, 3755, 7500, 10,125, 11,063, . . .] 
.] 
260,015 
[340, - 255, - 420, - 290, 18,405, 790, 1110, 1320, 1393, . . .] 
[11,628, - 6460, - 13,005, - 11,220, - 3940, 6378, 17,655, 
28,190, 36,660, 42,120, 44,003, . . . ] 
(29.8) 
29.18. Several methods have been proposed to simplify the arithmetic of fitting 
a trend-line by moving averages, the large numbers in some of the expressions in (29.7) 
and (29.8) involving considerable labour in straightforward application. The simplest, 
perhaps, is that of iterated averages. 
Suppose we take an average of sets of four with equal weights—a very simple process 
MOVING AVERAGES 375 
—and then another average of the same kind of that average. If the primary series is 
utJ the result of the first operation will be to give a series 
v1 = - (ux + u2 + u3 + w4) 
4 
v2 = - (u2 + u3 + ^4 + u5), etc., 
and that of the second operation to give 
Wl = - (fli + W2 + ^3 + Vi) 
It 
I 
= [^ _(» 2%2 + 3u3 + te4 + 3^5 + 2uq + uA. . . (29.9) 
16 
We may write this symbolically as 
~[1, 1, 1, I]}*—^[1, 2,3,4...], . . . (29.10) 
1 
or, reserving the symbol ~ [jfc] for a simple arithmetic mean of k terms, as 
Ic 
^M2=^[l, 2, 3, 4 . . .] (29.11) 
Now compare the weights of the average derived in Example 29.1 for fitting a cubic 
to seven points. Reduced to unit divisors we have for the weights of the latter 
— 0-0952, 0-1429, 0-2857, 0*3333 . . . 
and for the weights of (29.9) 
0-0625, 0-1250, 01875, 0*2500 . . . 
The two are not identical, but they follow the same sort of course and it might be possible 
to regard the latter as an approximation to the former. (We shall derive better 
approximations presently, but this will serve for purposes of Illustration.) Now the iterated 
summation resulting in (29.9) is much easier to carry out than the single weighted averaging 
process of Example 29.1. Generally, if we can find averages with simple integral weights, 
preferably unity, which will, in conjunction, give approximations to the more complicated 
weights of a single average, it is usually easier to use the iteration process. 
29.19. hi the notation of finite differences, write 
Aut =•= uf.., „i — ut . . . . . .(29.12) 
Eut = ut.vi = (1 }- A) nt .... (29.13) 
Sut = %H — Ut_v ..... (29.14) 
We have, for the second " central " difference d2ut, 
= (E: - 2 +H-l)u, (29.15) 
Writing 
we find, symbolically, 
E = exp (2icf>) (29.16) 
08 = e - 2 + E~l 
— exp {%$) + exp (— 2i<f>) — 2 
= -4sin2f (29.17) 
376 
TIME-SERIES 
m 
m 
Then 
— ^ ^ y 
t=—m 
t~—m 
< 
m 1 
1 + 2 JT* (cos 2?<£) I <a0: 
i-i 
since the terms in sin 2j<f> vanish, 
sin (2m + 1)<£ 
sin <f> 
tin. 
. (29.18) 
Thus 
1 „, __ 1 sin k<f> 
A; & sin <£ 
If, h (fc2 - 1) - o , , fe (&2 - I2) (&2 ™ 32) . . , 
J i L_— sm2 <£ -f- -i ^ sm </> 
. > w0 
223 ! 
245! 
(29.19) 
This interesting formula gives the arithmetic average in terms of the middle term u{) and 
its central differences. 
If now our series is approximately represented by a cubic, so that fourth differences 
vanish, we have 
1 /c2 — 1 
[k] u0 = u0 + ——— 62u0 
k 24 
. (29.20) 
and this equation will in any case be true up to third differences. Similarly, for two iterated 
averages we have, to the same order, 
1 1 
j~j ^ ^ u° ^ u° + 24 ^ + ^ ~~ 2) <52^o 
(29.21) 
'1 ^2 
and so on. We will use these results to derive two formulae in very general use by actuaries 
for " graduating " a series, a process which is very similar to that of fitting a trend-line. 
Example 29.2. Spencer's 15-point Formula 
Consider three successive averages with equal weights 
80 
M[4][B]*. 
= uo + ii {42 - 1 + 42 - 1 + 52 ™- 1} d*u9 
24 J 
9 
= u0 + ~ d2u0. 
4 
We then have, to third differences 
Wo = ^ [4? [5] ( 1 ™~^2)w0. 
Substituting for <52 the formula [1, — 2, 1], as given by (29.15), we find 
1 
u, 
320 
[4]2[5][-9, 22, -9]. 
Now without affecting the order of the approximation we may add factors in <34 or higher 
central differences, and can simplify the numerical coefficients to some extent. Let us 
MOVING AVERAGES 377 
add to the factor [- 9, 22, - 9] a term - 3<34 = [ — 3, 12, - 18, 12, - 3]. The result 
is [—3, 3, 4, 3, — 3], giving 
^0-3^ [4? [5] [-3, 3, 4, . . .]. 
This is Spencer's 15-point formula. It covers sets of 15 consecutive terms, the weights, 
in full being 
~ [- 3, - 6, - 5, 3, 21, 46, 67, 74, ... ] 
Example 29.3. Spencer's 21-point 'Formula 
In a similar way we find 
1 [5? m = i + «*, 
175 
giving, to third differences, 
uD«^[5]*[7](l-4a»)*D 
= ^[5? [7] [-4, 9, -4] nQ. 
We now add to the factor [—4, 95 — 4] the expression 
- 3<34 - U« = [- 3, 12, - 18, 12, _ 3] + [- i 3, - 7-|, 10, - 7-|, 3, - J] 
giving 
t6. = rL[5]»[7][-J, 0, £, 1, |, 0, -J] 
= ~7;[5]H7][- 1, 0, 1, 2, . . .]. 
This is Spencer's 21-point formula. 
29.20. A few practical points arising in the application of the foregoing formulae 
are worth mentioning. 
(a) The order in which the iterations are carried out is of course immaterial, as the 
reader can easily verify. It is therefore more convenient, as a rule, to carry out the more 
complicated operations first, while the numbers being handled remain small. For instance, 
in applying the Spencer 15-point formula we should carry out the moving average 
[— 3, 3, 4, 3, — 3] first, then apply the simple average -|- [5], and then the two averages 
of four. This does not apply if the series is short, inasmuch as there are fewer of the final 
than of the initial operations. 
(b) The use of a moving average of extent 21c + 1 involves the absence of k terms at 
the end and h terms at the beginning of the trend-series. If the original series is short the 
loss may be serious, and this effect sometimes restricts considerably the extent of the 
average which we are able to apply. 
(c) It is possible to remedy the deficiency at the ends of the series by special formulae, 
but the values so derived have less reliability than those of the main trend-line, and on 
the whole it seems better to accept the loss of 21c terms unless trend-values for the beginning 
and end of the series are really essential. 
378 TIME-SERIES 
(d) As yet we have given no guide as to the choice of most suitable values of m and p. 
In practice we do not usually require to fit curves of degree higher than five, and often 
a cubic is sufficient, as is assumed in the Spencer formulae. There is greater elasticity in 
the choice of m, but the point mentioned in (b) above requires m to be as small as possible, 
consistent with other requirements. We shall see later in the chapter that the variate- 
difference method gives some further guide as to p, and that certain effects of 
trend-elimination on random elements bear on the extent determined by m. 
(e) There is a voluminous literature on trend-fitting which appears to me out of 
proportion to the importance of the subject. It is not difficult to pursue inquiries on the 
above lines to the point of extreme apparent precision and great mathematical complexity, 
and perhaps such work is valuable where the series is fairly smooth and not disturbed 
seriously by sampling variation or superposed random fluctuation. But many of the 
series encountered in statistical practice will not bear the weight of great refinement in 
trend-fitting. The student will probably find that a knowledge of fitting by moving 
averages will be sufficient for all ordinary and many extra-ordinary purposes. 
The Effect of Trend-elimination on Other Components 
29.21. In Table 29.6 we have applied the Spencer 21-point formula to an artificial 
series obtained by adding a random element to a cubic. Specifically, 
ut = (t- 26) + 1 (* - 26)2 + jJq (* - 26)3 + £t. . . (29.22) 
The component et was taken from tables of random numbers and consists of samples from 
a population in which all integral values from 0 to 99 are equally frequent. The various 
columns of the table illustrate the process of fitting, and we may note in passing that for 
a series as short as this it is convenient to leave the more difficult summations to the last 
as there are substantially fewer of them. 
Now we know that the Spencer formula will fit a cubic exactly, so that when we 
subtract the trend from the original series we ought to eliminate the systematic constituent 
entirely and be left with our random component, except in so far as we have rounded off the 
systematic element to the nearest unit. A comparison of columns (2) and (9) in Table 29.6, 
remembering that the latter includes an element 49-5 equal to the mean of the random 
component, shows that we do not do so. The reason is not far to seek. The moving 
average has acted on the random element itself and determined a trend-line in it. 
The results of applying the Spencer 21-point formula to the random element et are 
shown in column (11). We should expect that if the method were perfect the values in 
this column would be 49-5, the mean of et, apart from irregular sampling effects ; but 
not only do the observed values deviate from this mean, they do so systematically, the 
values having a small oscillatory movement which is shown as part of the trend. 
29.22. This effect can assume considerable importance, particularly if we are 
eliminating trend so as to concentrate attention on oscillations. We proceed to examine it more 
closely. 
Suppose that we have a series composed of the sum of three parts, a trend <f>x (t), an 
oscillatory term ^>2 (t)9 and a random element cj>z (t), so that 
ut = ^ + cf>2 + <f>z (29.23) 
EFFECT OF TREND-ELIMINATION 
379 
TABLE 29.6 
Series given by Equation (29.22) with Trend-Line determined by a Spencer 21-point Formula. 
:u 
t 
1 
2 
3 
4 
0 
6 
7 
8 
9 
10 
11 
12 
13 
14 
15 
10 
17 
18 
19 
20 
21 
22 
23 
24 
25 
20 
27 
28 
29 
30 
31 
32 
33 
34 
35 
30 
37 
38 
39 
40 
41 
42 
43 
44 
45 
40 
47 
48 
49 
50 
51 
(2) 
Cubic 
Term. 
-119 
-105 
- 92 
- 80 
- 70 
- 00 
- 51 
- 44 
- 37 
~ 31 
- 26 
- 22 
- 18 
- 15 
- 12 
- - 10 
- 8 
i 
- 6 
5 
- 4 
i - 2 
i _ 2 
; - i 
0 
1 1 
1 4 
IS 
u 
15 
20 
! 24 
30 
36 
44 
52 
! 01 
1 r-t | 
83 
; 95 
109 
! 124 
140 
158 
177 
198 
220 
244 
(3) 
St 
23 
15 
75 
48 
59 
1 
83 
72 
59 
93 
76 
24 
97 
8 
80 
95 
uO 
o 
67 
44 
5 
54 
55 
50 
43 
10 
74 
1 « 
: 90 
01 
! 18 
OCT 
1 t> i 
44 
10 
90 
2>Jt 
: 13 
43 
14 
87 
i lo 
3 
50 
32 
40 
43 
62 
23 
50 
5 
(4) 
Ut 
-96 
-90 
-17 
-32 
— 11 
-59 
32 
28 
22 
62 
50 
2 
79 
- 7 
74 
85 
15 
— 4 
61 
39 
1 
51 
53 
48 
42 
10 
75 
37 
1 L) 
9(3 
70 
30 
52 
64 
34 
120 
58 
57 
95 
75 
158 
99 
98 
159 
156 
180 
201 
239 
221 
270 
249 
(5) 
[5] Ut 
• * a 
■> • • 
-246 
-209 
- 87 
- 42 
12 
S5 
194 
164 
215 
186 
198 
233 
246 
163 
231 
196 
112 
148 
205 
192 
195 
204 
228 
212 
176 
230 
290 
245 
260 
O I*i 
250 
306 
334 
339 
370 
411 
*i:4:ti> 
484 
525 
589 
670 
092 
794 
935 
997 
1,111 
1,180 
• * » 
• • • 
(6) 
[5] (5). 
• « • 
• • • 
• a • 
• a • 
-572 
-241 
162 
413 
670 
844 
957 
996 
1,078 
1,026 
1,071 
1,069 
948 
850 
892 
853 
852 
944 
1,024 
1,031 
1,015 
1,050 
1,136 
1,1.53 
1,201 
1,337 
1,357 
1,373 
1,462 
1,541 
„JL a<)«/t/ 
1,760 
1,897 
2,047 
Uf&iOO 
2,452 
2,711 
2,960 
3,270 
3,680 
4,088 
4,529 
5,017 
• a * 
• a • 
• • • 
* a * 
(7) 
[7] (6). : 
« a • 
• a * 
* * * 
• * a 
* " m 
* * • 
2,233 
3,801 
5,120 
5,984 
6,642 
7,041 
7,145 
7,038 
6,934 
6,709 
6,535 
6,408 
(5,363 
6,446 
6,011 
6,769 
7,052 
/ ,*>«.)«) 
7,610 
/ * -. 7 *J « ) 
O J Jmt *X «/ 
J 8,007 
| 9,019 
1 9,424 
9,870 
1.0,429 
10,989 
11,079 
12,539 
13,529 
14,(599 
16,060 
17,570 
i.«?, a %) o 
21,394 
23,090 
26,255 
• a * 
a * * 
• ■ * 
1 ■ • • 
« a • 
* » » 
a a a 
1 
(8) (9 
[- 1, 0, 1, 
2 - . ] (7). ijf-^ 1 
• w m • a> 
• * * « * 
a a * • * 
« a • * • 
m • t • a 
* * a • • 
• a * • • 
* • • * a 
• * « ■ a 
» • » <■ * 
14,352 41 
) 
18). 
1 
15,470 44 
15,815 U 
* 
15,676 45 
14,978 4c 
14,166 4( 
X%f}ij i .J OQ 
12,703 3( 
lijj 10<J Ot 
&.<£>) JL\)Zi Ok 
12,279 3L 
12,676 3( 
13,228 3* 
13,857 4( 
14,508 4] 
15,120 4: 
15,634 4J 
16,251 4( 
17,002 4! 
17,717 5 
1.8,499 5: 
19,307 5. 
20,159 5 
21,133 6( 
22,417 6- 
23,797 6 
25,737 7- 
27,955 8 
30,456 8 
33,334 9 
36,716 10 
• a * • • 
• a ■ • a 
* 
. 
* 
w 
m 
. 
* a a 
• a a 
a) • ■ 
a a • 
a a a 
• * • 
• • • * • 
* 
m ■ • 
\ 
) 
J 
1 
V0 
5 
■>* 
■ 
3 
) 
L 
$ 
■m- 
3 
3 
I 
3 
r> 
8 
0 
4 
8 
4 
0 
5 
5 
(10) (11) 
Deviation Graduation 
ut — (9). of st alone. 
9 67 
-42 66 
34 63 
-52 60 
31 55 
45 51 
-23 47 
-40 43 
26 40 
4 39 
-34 39 
15 39 
15 40 
8 41 
1 42 
-33 4 
30 4 
- 9 4 
-37 4 
45 4 
17 4 
-25 4 
- 6 4 
4 4 
-30 3 
58 3 
-16 3 
-23 3 
8 3 
-20 3 
53 3 
. . 
3 
4 
4 
5 
4 
4 
3 
2 
1 
9 
8 
7 
6 
5 
4 
4 
380 TIME-SERIES 
If we determine the trend by a moving average, denoted by an operation T, then clearly 
Tut = T<f>x + Tfa + T<f>9 (29.24) 
Let us now suppose that our method of determining trend is perfect in the sense that 
T<f>i = (/>i. Then, on subtracting (29.24) from (29.23) to eliminate trend, we find 
ut-Tut = (<£2 -2V«) + (^3 -Tfa). . . .(29.25) 
The point of present interest is that the terms Tcj>z and Tfa in (29.25) may distort 
the genuinely oscillatory parts of the residual series and induce spurious oscillatory 
movements. 
29.23. Consider the simple case when <f>% is a sine term, sin (a + M), t being integral. 
Since 
k 
jL~J 
sin (a + to) = !Ei** sin {a + J (jfc + 1) 2}, . . . (29.26) 
sin \X 
a simple moving average of h consecutive terms will result in a sine series of the same 
period and phase as the original, but with the amplitude reduced by the factor 
\*^W (29.27) 
k sm \a 
Iteration q times will reduce the amplitude by the qth power of this factor. 
Thus the term T<f>2 will be small if 1c is large, q is large, or if \kX is a multiple of n, 
that is, if the extent of the moving average is a period of the oscillation. But if X is small 
and kX is small the amplitude is reduced very little and <f>2 — Tcj>2 will largely disappear, 
i.e. the moving average will partially obliterate the term in </>2- In. this case, kX being 
small, the extent of the moving average is small compared with the period of the harmonic 
term, that is to say the oscillation is a slow one. This result is what we should expect. 
A slow oscillation is treated as a trend by the moving average and eliminated accordingly. 
Generally, the moving average will emphasise the shorter oscillations at the expense of the 
longer ones. Furthermore, if the extent of the average is slightly greater than the period, 
the term (29.27) may have a negative sign, and consequently the difference from the trend 
may somewhat exaggerate the true oscillations. 
It is not so easy to exhibit the precise effect of the moving average when the weights 
are unequal and the terms are not harmonic, but evidently the same kind of situation is 
apt to arise. 
29.24. Now consider the effect of a simple moving average (that is, one with equal 
weights) on the residual element <£3 which we will suppose to be a random element et with 
variance v. For the term T(f>z we have 
T<f>3 = ~ £ et+j (29.28) 
-Ufc] 
where [|fc] is the greatest integer which does not exceed \k. Consecutive values of et are 
independent, but consecutive values of Tcf>z are not; for T<j>z (a) and Tfa (b) have 
Jc — {a - b) values of s in common and are correlated if a - b < k. Thus the series Tfa 
will be much smoother than <£3, and if we proceed to further averagings will become smoother 
still. We have had an example of this effect in Table 29.6, and shall meet further 
examples below. 
EFFECT OF TREND-ELIMINATION 381 
29.25. The effect of taking a moving average of a random series will then be to 
generate an oscillatory series, provided that the weights are such as to give a positive 
correlation between successive members of the generated series, a condition which is always 
realised in moving averages employed for trend-fitting. We shall call this the Slutzky- 
Yule effect, after the two statisticians who (independently) studied it in detail. 
The generated series is not regular in the cyclical sense, that is to say its peaks and 
troughs do not recur at equal intervals of time, and the amplitudes of the oscillations vary 
considerably. Nevertheless such oscillations present a striking resemblance to the kind 
of movement which is found in practice, particularly in economic time-series, and we shall 
consider them in more detail in Chapter 30. For our present purposes we require to 
consider how far the process of trend-elimination itself may generate such effects in order 
to be sure that oscillatory movements in a trend-free series have not been put there, so 
to speak, by our own arithmetical processes. 
29.26. For this purpose we shall consider the period and variance of a series 
generated by the Slutzky-Yule effect. 
Since the peaks and troughs do not recur at equal intervals there is no quantity which 
we can conveniently call the length of the oscillation. There will, in fact, be a distribution 
of lengths. We may define as the mean length either the mean period from peak to peak, 
or that from trough to trough ; but this raises some difficulties as to whether we are 
prepared to admit as periods small ripples on the main undulation. 
Recognising its somewhat arbitrary character, we shall take as our measure of 
oscillatory length the mean distance between " upcrosses ", that is to say the mean distance 
between points where the series changes sign from negative to positive or " crosses the 
a'-axis \ Suppose the series is generated by a moving average with weights ax . . . ak 
of a random variable which is normally distributed with variance v. Then the probability 
that 
A- 
= yV^/ < 0 (29.29) 
% 
j \ 
k 
and %_H — y aiFj\\ :> ^ ..... (29.30) 
./ -i 
i.e. that the generated series changes sign from negative to positive, is the proportional 
frequency of 
1 ynr 2 
2v -<w J 
I 
Cf/Ju :r=™ • ■ -. ,., , '.i 6X1) 
(2jr)*<*-»-i) * 
j = i 
k k 
de% . . . dek+i . . (29.31) 
between the hyperplanes / ctj e} = 0 and Y* a; *,_,., = 0. This is equal to the angle 
between these two planes, which is given by 
u-1 
/ j aj aJ 
ft 
cos (9 = *~\ (29.32) 
2/a] 
Hence the mean distance between upcrosses is 2jr/0, where 0 is given by (29.32). 
382 TIME-SERIES 
29.27. In a similar way, the probability that 
%+i - uk < 0 (29.33) 
uk — uk_x > 0, (29.34) 
that is that uk is a peak of the series, is the angle between the two hyperplanes 
k k 
^aj£j+i ~ y. aj£j == ® (29.35) 
k k 
y ctj £j — y aj sj~i = 0 . . . • ■ (29.36) 
and is given by 
(aa — &0 ax +- (a8 — &2) (^2 — ax) + . . . 
cos 0 = — + (% — flfc-i) fa-i — gfe-2) ~ ak iak — <%-i) (29 37J 
1 {a\ + {a2 — ax)2 + . . . + a%) 
Thus the mean distance between peaks is 2n/Qx. The same formula obviously applies to 
mean distance between troughs. 
29.28. If we wish to exclude " ripples" of a certain length d from consideration 
we may inquire for the probability that (29.35) and (29.36) are satisfied in conjunction with 
ll 7. *^> 11 J> iff • • • • • . \j-id.tJOJ 
This is evidently the area cut off on the unit sphere by the three planes (29.35), (29.36) and 
k k 
/ Obi Sj y CCj £7-J-/7 ==: ^- • ■ • • \*-i\.r ,0*J J 
If the angles between the planes are A, B and G this area is A + B + C — 2rr = 0a, say. 
The mean length between peaks, ripples excepted, is then 4tz/62. 
Example 29.4 
In Table 29.7 we show 480 terms of a series of random numbers which can take integral 
values from 0 to 19, together with a moving sum of fives of a moving sum of threes. 
Kg. 29.6 shows a portion of the derived series graphically. There are 474 terms of the 
smoothed series. 
The mean value of our series is 15 x 9-5 = 142-5. The number of upcrosses will be 
found from the table to be 23, the first between the 19th and 20th term of the smoothed 
series, the last between the 459th and the 460th. The mean distance between upcrosses 
is then 440/22 = 20 units. How does this compare with the mean-distance given by 
normal " theory ? 
The weights of the graduation are [1, 2, 3, 3, 3, 2, 1] and from (29.32) we have 
a (1 X 2) + (2 x 3) + . . . + (2 x 1) 
cos Q = —— — - - -—* 1 
l2 + 22 + . . . + l2 
= 3i = 0-9189 
37 
a 
0 = 23° 14'. 
Hence the mean distance = ^ — 15:5 units. 
23-233 
~!^^^^^^^^W«05MMWMWwCvMb5tCtOtOt5bBbOMWMMMMHi-i|-i|-i|-iI-i 
■ lv r- i— r— p— r— p— !— !—' !— »-* 
i — *— r— 
: C tO O VI Ci "*J #^ 
M M 
-— i— ;— :— i—' S—11—'!—i J—' MMH S—i S—i uuuu j_i j_i!_i 
CO 
! ft 
»uuHH>-iMf^W)->i->wuH)->WMMH"MS—L !—11—1 !—IS—iw ;—IMMMMMMMMMMMMM 
yT J », ^ C ^ -; CH Q M M a C ^OT^CS^OOiCnCiCiCCO^OClO^OOMCCt^OOQOMOCnCrxCTiCO^Irfi. 
UMH.-WH"H'-tvtOKWM{iHHHHWMHHHHMt5MMMMMMHMWl>5MMMHi-lM 
ti^CncnjiCi^vICtt^tiHCOCOD^rCIWww^i^^iaiOCOaOOaCOCOl-iHSCJi^l-iOCOOOOvI© 
^iTUS'XX-SiOOCO^lTr.OSDDCiOOCTOCiCHvnOlifi.MOOvIOOCiH^HMOOJI-iOMWl-'^l-'M 
£<. c^ to m o w x -a' 
4» t5 O Oi Ci 
n!^KS»-OC 
—' M MM !—i!—i S—i !—i UUMMMM !—i $—i !—i uuu 
^M~.^MMOCBK(CiMOQ0Oi^tv(BM«OtC»05M««^!-iWC5)ti-MMWl-i00 
i_it-it«MMMMMM 
to mo o o o o o — 
i-Jj-iMMMMWMMMMMMMMMMMMMMMtOtOMMMMMMMMMMMMMMM 
lJMti3caiTtrjy£WOtTi:(35CnM<aHMtfi03i!i-(»iti.CiOb3»JWOtOOOHOMWC»K) 
t3 m 
■-"—"lys™, ;—;—~ 
:odoc:x:xocx:x3dogod^^:i^^i^^^^^^c>®ooc»o©c^ 
~ "i^MMHOCXMC!W^WKHOO03vlO5ill»-M»i-'OSl!»^5!Ci^Wt5l-'OO00-a010i 
CC-X-l! 
i_i M wbi >-t i—i ■,—i )_i s-u M M )—i )-i !_l t_l l—I i_l i_L j—i j—l !—I H-J j—I !—I $—I 
tTMC^tw^^W*^OC;N3C>OMMHO!tOMO.<IHOOt«OOHMH^ffiMClOlvIOOK©HCaC»HlfiOC5COC» 
t—■ i—';—' 
MH'!-iHHi-'H'HHh'WI-iWI-lHMI-l|-l|-lMM 
HMI-iHHHHHI-<MHMI-'l-ihi|-iHHH 
i_i j_ii_i)_i^j i—i I—t (—i 1—« !—I !—I S—IS—I M I—»> !—l 
to to to to to to to to to to to to toto to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to 
j i I i uu i—i !_i!_lj_i w s-l t-» s-l !_l !—I f—I f—1 j_l j-l M M j-l j-l M M M 
CCvJC.0iX^'<IO£XXtCX^^<tHHCMMO©^Q0CnMW©-|^M&MH©lt»-BOi»JO,sI05OWOSI-1 
K«MMMMi:titiMKltC'tCtitCteM!-iHI-iMMI-iMf-'MI-iOOOOOOOOOO<0C0ffl©CO©©O©t0 00 
CCi^WtC'HCCaWO;T^MtOMOOOO^Ot7ii^WMMOffiOO^OWi^tO!H3MOCOOOSC>STifi.Wl5l-iOO 
to m ©i ac ; 
j ) WJ ^ J^1 J_U ^*l )—I )—1 J—I J—I J—I M M M M I—I I—I M I—* M 1—* M M M M 
^KSWX'SO!»CWPSOC5t*OOQ^I^t0O^HOI«0DOt0it»-it»-O^t<!M©0iCSOiWb3Ot5CS 
iJuwHHI-lHMHI-ltSr'MUI-lWMMtStabOKiHI-ll-lMH j_l|-i)-lS-lMMMMMMMMMMM 
a^^IK'OC"OOMW?^OOOOOOOiaMOMMHOOONlCiWO©tDOl-'bBtO»WO:Wif>.i^*'lt>.Oi(»00 
KCiMPMS>JW02KMHOi^3C;CiOt0HWK)HOMO<ICCOHO©«iO0l00OCi©»Jlf>-W£fiClM00O00 
Oq 
_ ■— — — — — —■ j. jj j- j. j. <j^ _»w -ju ji_ _*- —j -m —.j ^j ^j —-j^j^j-vj^j «s ^„ s_s> t^j i^s cs ^ cs o; ^ v vi wi t/i w lii \j\ tn v« wi >»*■ ' 
aP^Wtv"CCa^«i^^WtiHCCX^lC5^4'Cit<(MOO0E^C5Cnl^K^5MO©05^Iffi^apf'«tv)i-lO5C 
i—i .—i M — j~i f—' .i—i s—i ?—' i^-t i-1!—'•-'!—i MS—l !—1 J—I S—I !—I !—I !—1 !—1 i_l )—l l—I uulul >_* j_i 
cs. 
ft 
Gq 
Jfc 
pj 
MCnCCCCCX^S«<IC:<ie0it0Ci!Ci<rONrNlMMC!W(»ifi»Jl5Ol-'MOt!i0iM^00SM^OHl-'O<a ; 
ft 
u-uuww^uuhwHH!-' HHHHMMMMS-JHM !—15—l!—'5—1!—IS—II—1!—'5—1 ; 
CMWUwUC;M^^^MHCCO:*WvrODCOHHMMts!»Mb5MOOOffi©©©SOb5Ml)3Klb3M^^ I &2 
atCa:tiC-Ki^'J-CO2CM©MCTOt*MMHO«aC5Otai»0lM«J>JC0l-iCnC3©0l!-iOOta0Dl!0l|i.«J i 
ft 
&3 
ft 
02 
OOC30CXCC'00-^~a*a~a~a~a~J*J~J~JCSCSC»C^C3C»0»OC^CSCnCJtOxCnCnC^CnC^C3lCylt4i>^>^ 
i^«tvHO©X^C!Ci^Mtv)HO©03^O<3^M[aHO©l»MO01rtiWt)0MC©C0rIO0ii|i.Ci5MMO©0!l^ 
! s_is_i*_is_is_i s-i !_l !—11—i I—I S-l}-* *-» j-l !_is_ls_ll_ls_ls_ls_lS-l 
s^-atoM-aooc7ioo>-j^j^c^oc^ip't>5C>tooc™toi-ttotoe>c™ip'«ocicsooc»^c»c^co 
l-i !_ii_ij_ii_i!_ij_iwi_i!-i!-1S-i i_«MMMMMMMMMMMMMMMMMMMMMMMMMMMMM 
HCOODOMCn^COODOO>^MOO©OOQDOOMMtOHW«CTWC4©C»WCnCl-<I^MM<I©ClOlSSR,ISI^Q 
O©00©SIH^SO^O00O©^©00l^OCTt)0ffl^SO»OWW00©<J"J©Wlfk^CJ>lit5«c'WOI-lHi|v-a 
WWWMKtCWMMMMtCKlHPS'HHHHMHHOOOOOOOOOOfflffiffiCpOOgggOOOOOOCOCD 
MMO©(»<IKCTiFkMMMO©(»MC5W^«toHO©!»SIO0T^Wt5HO©00^OClil(i.WNI-lO©00»10>CT 
S-iM MM MM M MMM MMMM MMM MM M M M M M MM M 
«CT^O©WtOtOM©0©COtO«t<)l^H»l^O(»S©^W^^(X)»*J03WI-lC3©fflOOCOHCnili--atOMOlOO 
!_i!_iMMMMMMMMMMM MMMMtOtOtOMMMMtOMMMMMMMMWMMMMMMMM 
si^w»MOOw^oi^»o<icJ*w£(»o^^ooooofflo©o©a)5^^^o©o©©«^']^oif; 
O^C!©©OCS(^!fiWaOCTQ0l«OffiC.C5^OlliMt0^W^Hb0 00CnMW03 00WCDC0C0H^Ot^C0©Offl 
Cc<i^^^MMMS^^c«©o©o©C5C5QOOiCTcnonCTCnoncncncn^^il^(^^i^^^^^wcococ*coww 
0©03S©tn^MtOHO©{B^CiW^WMMO©03Sr©Cni^Wb5MO©COSI©CT^WbaHO©QOM05 0i!^W 
M !-i M M MHi-i MM M MM M , MM 
CnCiMMCnvJCTO©^C5 00«n(^CT©<lW^MW03CnM*-b300WI>0b3«nl0^^b5OW00CttWRC0MW(X)©l^ 
t^ w^ w^ w^ u^ u^ w^ w^ w^ u^ i 1 u^ w^ u-i u^ w^ w^ u^ 1^^ ^^ ^^ i^^ H^ H^ H^ ^^ ^^ ^^ U^ 
©©OOt5tOWWWW^OS-<IOit!S©©i-i|-iK)b3l-iO©©OI-'K)K)OOOklM©a©^OiOi-CIOWO\»J 
W©WOCnQC0C5MO©©QOOM(»00O©HOHON-JC},sIlf'lfi-©0»0l00i-'C5Ht5tflWWN[WOTt0 
1 » 
I 
! 
ft 
&2 
c* 
(0 
&2 
5Q 
Ob 
CO 
Oo 
feci 
o 
to S 
S to 
&, '^ 
*^ 
<s>. 
Co 
Co 
5^ 
O 
Cxi 
03 
o 
o 
Co 
•384 TIME-SERIES 
The observed mean distance is 20*0 units, but this is based on rectangular variation, and 
we are, perhaps, entitled to expect some difference from normal theory. For rectangular 
random variables, values distant from the mean occur more frequently, and it is not 
surprising to find oscillations in the series which do not result in upcrosses. 
The number of peaks in the series will be found to be 62, the first at the seventh term, 
459 
the last at the 466th. Hence the mean distance between peaks is — = 7-5 units. From 
formula (29.37) we find 
cos!?! =-> Bx = 48° 11'. 
6 
Thus the theoretical mean distance is - = 7-5 units, in good agreement with 
experiment. It will be observed that several of the distances between peaks are due to very 
:small ripples. 
From a number of experiments Dodd (1939a) concluded that series generated from 
xectangular material conformed fairly well to normal theory. 
29.29. Let us now examine how the variance of the induced oscillation compares 
with the variance of the original random series. 
The sum of k random elements with variance v has variance lev and its mean has 
-variance v/k. It does not follow that a simple moving average has a variance l/k times 
that of the random element, because of correlations between successive members in the 
derived series. If the original series was ex . . . sn the derived series is, with weights 
ax e± + a2 s2 + . . . + ah elc = rj^ say" 
' U1 '■ >• . . (29.40) 
ai En-h+l + a2 sn-k+2 + • • • + ak en ~ Vn-k+h 
The expected value of the sum of these values is zero since the expected value of f may be 
Ttaken to be so. Since there are n — k + 1 terms we have for the variance 
y 
n — k + 1 
V2 (29.41) 
The expected value of this, since the e's are independent, is 
—~^j E {Z(^) } = E fo») = (a* + 4 + . . . a%) v. . . (29.42) 
In particular, if the a's are all equal to l/k, the expected value of the variance is v/k. This 
gives us the average reduction in the variance. 
If a simple average of extent k is iterated q times the weights are the successive 
coefficients in 
'The sum of squares of these coefficients is the coefficient of xq{k 
i) • 
i d + * + *> + ■ ■ • + a*-*)- = ^(f ^ • ■ (29.43) 
EFFECT OF TREND-ELIMINATION 385 
and this gives the average reduced variance for a simple average of k iterated q times. 
The following are the values of the reducing factor for some of the values of k and q :— 
9. 
3 
4 
5 
(5 
7 
1 
033 
0-25 
0-20 
017 
014 
2 
0-23 
0-17 
0-14 
0-11 
0-10 
3 
0-19 
0-14 
0-11 
0-09 
0-08 
4 
0-17 
0-12 
0-10 
0-08 
0-07 
5 
0-15 
0-11 
0-09 
0-07 
0-06 
Evidently the result of the first moving average is to generate a series with a much 
lower variance than that of the original random element, but the second and succeeding 
iterations do not reduce the variance further to the same extent. In the case h = 7 the 
first averaging reduces the variance to one-seventh, but the next three reduce it only by 
a further half. 
29.30. To apply such results in practice we require an estimate of the variance of 
the random element in the original series. If this is available we can estimate the variance 
of the generated series and also, from 29.26, the mean distance between npcrosses or 
between peaks. If then our residual series, after the elimination of trend, showed an 
oscillatory movement with this variance and these mean-distances, within sampling limits, we 
could not conclude that the oscillatory effect was real. It could have been induced by 
our method of eliminating trend. 
In the present state of knowledge it is not possible to assign permissible limits of 
sampling variation by relation to standard errors in the usual way. Whether any particular 
effect is significantly different from the values of the series generated from the random 
element remains, therefore, a matter of subjective judgment to some extent. The sampling 
problems involved are formidable, but there does not seem any reason why they should 
not be capable of explicit solution. This field of study awaits the attention of the theorist. 
Example 29.5 
For the data of Table 29.3 (sheep population of England and Wales) trend was 
eliminated by a simple average of nines, the resulting residuals being shown in Table 29.8. 
A glance at the series suggests some sort of oscillatory effect, since the signs of terms cluster 
together. By the methods of the next chapter the effect may be brought into greater 
prominence. The data themselves, however, indicate a mean-distance between upcrosses 
of about 8 or 9 years, and actual calculation gives a variance of 8474. Can this be due 
to the operation of our trend-elimination on a random element in the original series ? 
For the mean distance between upcrosses due to a simple nine-point average we have 
cos 0 - -8, 0 = 27° 16', 
9 
360 
and the mean distance is ~_- = 13-2 approximately. This is considerably in excess of 
our observed value, but not sufficiently so to reject outright the possibility we are examining. 
Since, however, the variance of residuals is 8474 this must, to have been generated 
from a random series by a simple average of nines, derive from a random element with 
A.S.—-VOL. II. C C 
,386 TIME-SERIES 
TABLE 29.8 
Besidual Values of the Sheep Series of Table 29.3 after Elimination of Trend by a Simple 
Nine-Point Moving Average. 
• 
Year. 
1871 
72 
73 
74 
75 
76 
77 
78 
79 
80 
81 
82 
83 
84 
85 
86 
87 
88 
89 
90 
91 
92 
Residual 
(10,000). 
- 176 
- 112 
+ 50 
+ 141 
+ 60 
- 20 
+ 12 ' 
+ 82 
' + 130 
- 14 
- 166 
- 179 
- 84 
+ 38 
4- 97 
+ 8 
- 5 
- 105 
- 99 
+ 35 
+ 159 
+ 167 
Year. 
1893 
94 
95 
96 
97 
98 
99 
1900 
01 
02 
03 
04 
05 
06 
07 
08 
09 
10 
11 
12 
13 
14 
Residual 
(10,000). 
+ 34 
- 103 
- 104 
- 15 
- 23 
+ 17 
+ 71 
+ 35 
+ 16 
- 27 
- 32 
- 49 
- 61 
- 52 
- 24 
+ 68 
+ 141 
+ 119 
+ 66 
- 52 
- 117 
- 61 
Year. 
1915 
16 
17 
18 
19 
20 
21 
22 
23 
24 , 
25 
26 
27 
28 
29 
30 
31 
32 
33 
34 
35 
Residual 
(10,000). 
+ 19 
+ 128 
+ 97 
+ 69 
- 29 
- 174 
- 107 
- 142 
- 109 
- 23 
4- 60 
+ 121 
+ 94 
- 25 
- 90 
- 75 
+ 72 
+ 152 
+ 112 
- 64 
- 87 
variance 76,266. An estimate of the variance of the random element in the original series, 
obtained by the variate-difference method which we describe below, was only 350 
approximately. Making every allowance for sampling effects, we cannot do otherwise than reject 
decisively the possibility that the residual oscillation is spurious in the sense of having 
been induced into the data by the effect of the elimination of trend on a random element. 
29.31. We may summarise the foregoing discussion of trend-eHmination as follows :— 
(a) The conception of a trend as a " smooth " or " regular " movement is equivalent 
to the supposition that trend can be represented, at least locally, by a smooth mathematical 
function and in particular by a polynomial in the time-variable. 
(6) Certain series can be treated on lines formally equivalent to regression analysis ; 
but a more generally applicable procedure is to represent the trend by a moving 
parabolic arc. 
(c) The moving arc of best fit in the least-squares sense gives values which are 
derivable from a moving average of the data. The weights of this average are to some extent 
at choice, according to the extent of the average and the closeness of fit required in the 
moving arc. 
(d) A moving average of extent k sacrifices (k — 1) terms, in the sense that the derived 
series is (lc — 1) terms shorter than the original series. If the series is short it is usually 
desirable to keep this loss to a minimum, that is, to keep the extent of the average as 
short as possible. 
THE VARIATE-DIFFERENCE METHOD 387 
(e) A moving average may distort genuine oscillatory effects, in general exaggerating 
the shorter variations at the expense of the longer ones, and may induce spurious oscillatory 
phenomena by its action on random residuals. For harmonic components the effect is 
minimised by taking the average as simple, with extent equal to the period of the 
component. For random components the effect is minimised by making the sum of squares 
of weights in the average a minimum, i.e. by using a simple average. 
29.32. In the theory of time-series there are very few rules which can be laid down 
without a good deal of proviso and caveat. It will be evident from the foregoing that there 
is no golden rule in trend-fitting which can be applied irrespective of individual 
circumstances. If we desire to get a close fit to the data we must use a parabola of fairly high 
order, which involves a moving average with weights which are far from equal. This, 
however, increases the danger of obscuring the true oscillations in the residuals. In 
most practical cases it is necessary to strike a balance between conflicting requirements 
by intuitive judgment as to the appropriate moving average to use. 
The Variate-dijference Method 
29.33. We now proceed to consider the random constituent of a time-series. From 
the very nature of random variation we cannot expect to derive any formula, however 
approximate, which will measure the random component directly at any given point of 
the series. The best we can hope to do is to determine the non-random components and 
to obtain a random residual which is left unaccounted for by those components ; and even 
this, as we shall see in the next chapter, is not a very strong hope when oscillations appear 
in the series. 
On certain assumptions, however, we may determine the variance of the random 
component and hence obtain a general idea of its magnitude and importance. Suppose 
that the systematic part of the series can be represented, at least locally, by a polynomial. 
Then successive differencing of the series will gradually eliminate the polynomial element 
but will not reduce the random element correspondingly. As we proceed with the 
differencing, the random element becomes more and more predominant until finally the 
systematic component is negligible. Hence we can determine effectively the variance of the 
random component in the differenced series, and by a simple calculation derive an estimate 
of that in the original series. 
29.34. Consider the differencing of a random series et. We have 
/.A Si ^^ S/ i | " St . . . • « . . \m\?.rkrlcj 
Ar et - e(+r - ( M et+r-i + ( % ) e<+»-2 +••■+(— !)r «<• • (29.45) 
Without loss of generality we may suppose that the mean value of et is zero, and thus 
E {Ar £t) = 0 (29.46) 
Hence 
var (dr et) = E (Ar et)* 
E «j Si+r """(-.J e* + r-l +•••+(— IT 8t 
9 
r x 2 
E 1 £t+r + ( -j ) e?+r-l +•-■+£* 
T \ 
:==: v \ x ~~r~ [ ] -j- . . . ~7~ i- 
388 TIME-SERIES 
The sum in curly brackets is easily evaluated from the consideration that it is the coefficient 
var (Ar et) = v (*\ . - . - • • (29.47) 
We may then derive an estimate of v by writing 
v __y'AAret) .... . (29.48) 
It is to be noticed that we use the second moment about zero, not the observed variance 
of Ar st, since the mean is known to be zero. This shortens the arithmetic to some extent. 
The factor ( T ) for r = 1 to .10 has the following values :■ 
(2;) •/(*) 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
2 
6 
20 
70 
252 
924 
3,432 
12,870 
48,620 
184,756 
0-5 
0-166,667 
0-05 
0-014,285,7 
0-023,968,25 
0-02l,082,25 
0-03,291,375 
0-0477,700,1 
0-0420,567,7 
0-055,412,54 
29.35. Basing itself on equation (29.48) the method of variate-differences proceeds 
as follows : We difference the series once, find the second moment about zero of the 
resultant and divide by 2 ; we then difference again and find the second moment about zero, 
dividing in this case by 6 ; and so on. If the successive estimates of v decrease, we 
continue with the differencing. There will, in general, come a point when they cease decreasing 
and remain constant within sampling limits (which may be rather wide). At this stage 
we may suppose that we have ehminated the systematic element in the original series. 
The final estimate gives us an estimate of the variance of the random element in the original 
series, and the order of the difference to which we have had to go will give an indication 
of the degree of the polynomial representing the systematic component. 
Example 29.6 
Let us apply the variate-difference technique to the series of Table 29.6. We know 
from the method of constructing the series that the systematic part ought to be completely 
eliminated after the third differencing, and also that the random part consists of an element 
with variance 833 approximately. In fact, the random numbers from 1 to N have a 
variance (N2 - 1)/12 and N in this case is 100. The actual variance of the random element 
in Table 29.6 is 843. 
THE VARIATE -DIFFERENCE METHOD 
389 
TABLE 29.9 
Differences of the Series ut of Table 29.6. 
2 
5 
0 
7 
! s 
9 
i .... 
10 
11 
12 
i IO 
14 
15 
j 
10 
1 17 
18 
i 1$> 
20 
i 21 
k>2 
24 
«J W 
! 27 
28 
j 29 
30 
31 
1 'i1-} 
j 33 
34 
! 35 
i 30 
! 37 
! 38 
39 
i 40 
' 41 
42 
4-3 
i 44 
45 
4t> 
4:7 
48 
! 49 
50 
51 
i 
-96 
-90 
-17 
-32 
-11 
-59 
32 
28 
22 
62 
50 
2 
79 
- 7 
74 
85 
15 
— 4 
* 
01 
39 
1 
51 
53 
48 
42 
19 
75 
37 
1 
90 
70 
30 
04 
34 
120 
58 
i 57 
95 
i 75 
! 158 
f 99 
98 
159 
150 
180 
201 
239 
i 221 
270 
249 
A1. 
- 6 
-73 
AO 
-21 
48 
"""""" *7 JL 
4 
6 
-40 
12 
48 
-77 
80 
-81 
-11 
70 
19 
-65 
22 
38 
-50 
- 2 
5 
0 
32 
05 
38 
25 
84 
20 
40 
„_t)9 
— 12 
30 
- 92 
08 
1 
38 
20 
- 83 
59 
! i 
01 
3 
24 
™" A A 
- 38 
18 
--49 
21 
■ • « 
A*. 
67 
- 88 
36 
- 69 
139 
- 95 
- 2 
46 
- 52 
- 36 
125 
-163 
167 
- 70 
- 81 
51 
84 
- 87 
- 16 
88 
- 48 
_ 7 
- 1 
_ 20 
97 
-103 
13 
109 
-110 
14 
02 
- 10 
- 42 
122 
-160 
07 
39 
- 58 
103 
-142 
58 
62 
- 64 
27 
- 3 
17 
- 56 
67 
- 70 
■ • • 
* • - * 
A\ 
155 
-124 
105 
-208 
234 
- 93 
- 48 
98 
- 16 
-161 
288 
-330 
237 
11 
-132 
- 33 
171 
- 71 
-104 
136 
- 41 
- 6 
25 
-123 
200 
— 116 
- 96 
219 
- 96 
- 76 
72 
32 
-164 
282 
-227 
28 
97 
—161 
245 
-200 
- 4 
126 
- 91 
30 
- 20 
73 
-123 
137 
» * * 
0mm 
• » * 
A*. 
279 
-229 
313 
-442 
327 
- 45 
-146 
114 
145 
-449 
618 
-567 
226 
143 
- 99 
-204 
242 
33 
-240 
177 
- 35 
- 31 
148 
-323 
316 
- 20 
-315 
315 
- 20 
-148 
40 
196 
-446 
509 
-255 
- 69 
258 
-406 
445 
-196 
-130 
217 
-121 
50 
- 93 
196 
-260 
* a • 
* • • 
• • • 
• • • 
A5. 
508 
- 542 
755 
- 769 
372 
101 
- 260 
- 31 
594 
-1067 
1185 
- 793 
83 
242 
105 
- 446 
209 
273 
- 417 
212 
- 4 
- 179 
471 
- 639 
336 
295 
- 630 
335 
128 
- 188 
- 156 
642 
- 955 
764 
- 186 
- 327 
664 
- 851 
641 
- 66 
- 347 
338 
- 171 
143 
- 289 
456 
• • • 
• • • 
... 
• • • 
« * • 
A\ ■ 
1050 
-1297 
1524 
-1141 
271 
361 
- 229 
- 625 
1661 
-2252 
1978 
- 876 
- 159 
137 
551 
- 655 
- 64 
690 
- 629 
216 
175 
- 650 
1110 
- 975 
41 
925 
- 965 
207 
316 
- 32 
- 798 
1597 
-1719 
950 
141 
- 991 
1515 
-1492 
707 
281 
- 685 
509 
- 314 
432 
- 745 
• • • 
390 TIME-SERIES 
Table 29.9 shows the series and the differences up to A\ For the sums of squares 
in the various columns Sj corresponding to A\ we find— 
Sx = 107,541 
/Sa = 318,115 
/S8 = 1,033,513 
#4 = 3,445,308 
85 = 11,720,069 
SB = 40,548,844 
To obtain second moments we divide by 51 — j and then, to obtain the estimate of v, 
by f . ). We find the following :— 
j Estimate. 
1 1075-41 
2 1082-02 
3 1076-58 
4 1047-21 
5 1011-05 
6 975-20 
Curiously enough, the estimate for j = 2 is higher than that for j = 1 and there is 
little difference between the various estimates. In the ordinary way we should have 
concluded that the systematic component was adequately represented by a polynomial 
of order 1, that is to say a straight line, and that the residual random element had a variance 
of about 1000. 
The reader must not be surprised to find discrepancies of this kind between theory 
and experiment in short series ; and the discrepancy is not, in fact, as big as it seems. 
The variance of the original series is 6272-61. The mean square of the first difference, 
divided by 2, is 1075-41, so that about five-sixths of the variance has been eliminated by 
•the first differencing, and the method indicates, quite correctly, that the greater part of 
the systematic element is linear. The random element is rather large compared with the 
non-linear systematic terms, and the latter have got caught up in it—the series is too short 
for the variate-difference method to disentangle them. Consider, for instance, the cubic 
term T^ (t — 26)3. In the original series this varies in value from — 156-25 to + 156-25. 
3 
First differences reduce it to —- (t — 26)2, varying from 18-75 through zero to 18-75, 
whereas the random element is increased in range from 0 to 198. Already the systematic 
term is being swamped by the random element, and a slight degree of accidental correlation 
between the two can easily account for the increase in the mean-square of second differences. 
The matter may be put in a slightly different way. Suppose that, relying on the 
variate-difference method, we regarded the data as represented by a linear equation plus 
a random residual. If we fitted a straight line by least squares and examined the residuals, 
we should probably find very little evidence of departure from randomness. This 
representation would differ from the mode of construction of the series, but it would be a possible 
method of construction. Only the failure of the representation to conform to further 
terms of the series would reveal its weakness. 
THE VARIATE-DIFFERENCE METHOD 391 
29.36. The variate-difference method thus provides a kind of lower limit to the 
degree of the polynomial which will represent a series locally or generally. There remains 
for consideration the question as to what sort of differences between successive estimates 
of v can be regarded as chance effects, in order to decide when the value has reached a 
stationary level. The sum of squares S^ is a constant factor times the second moment, 
but as its members are correlated among themselves we cannot use the variance of the 
second moment to test its significance. Further, Sj and Sj+1 are correlated. We proceed 
to derive the sampling variance of their difference, the somewhat complicated formulae 
being due to Anderson (1914). 
29.37. Write 
bj = ( r\ . (29.49) 
.1 . 
Then we have, as in (29.42), 
"E{Aru)^^l^jlf=^. . .(29.50) 
r J 
where /za is the variance of u. Further 
E (Ar u)4 ~- E [ {b0 ur+l — bx ur + 62 ur-i — .■■+( — l)r br u±}2 
+ {b0ur+)i ~~b1ur+1 +b2ur — . . . + ( — l)rbru2}2 
+ . . . 
+ {b0un - b1nn^i + &2%_2 --..+(— l)r K ^-r}2]2- . (29.51) 
(Consider first of all the terms in this which result in fourth powers of %. They will 
derive from 
R {fcjj #;,., -|- b\ n* h . • • f- b*u\ + b\ v£+2 + b\ u?^ + . . . + b* u\ + . . . 
»•* ^ i /.2 y i i / '2 '2 V7 
i" <K) 'lln r °l un~~l r • • • H™ urU7i-rj" 
- k % (■«* -i- A) i- 0>l + b\) M-t + rt) + {b'i + b\ + 61) «_a + «!) + . . . 
■i- (ftg f ft? I- . . . ftjL.) (««_r+l + «?) + (6Ji +6f + • • • + &?) 
(ul..r + «*_,._, + • ■ . I- «;+1)}2 (29-52) 
Writing now 
Bl = (6*)* + $i + &?)* + • • • + (bl + &!+••.+ #-i)2 • (29-53) 
4g---= (&o + &i + • • • +KY^(2r\ (29.54) 
we see Unit the term in E ('«*) is 
{At (n - 2r) + 2Bl} H (W) (29.55) 
The only other term appearing from (29.51) will be of type E («? «&), Z ^ m. If the reader 
will write out the expansion of (29.51) he will find that the coefficients are expressible in 
terms of 
A) = (6. bt + &x bJ+1 + • • • + &_, *V)2 = (r 1, J • • (29-56) 
3 = (b,bi)* + {b»bj+blbJ+1)i+ ■ ■ ■ +(b0b.j+b1bJ+1+ . . . + &,_,_! K^)*. . (29.57) 
392 
TIME-SERIES 
The expression for E (Ar uY reduces to— 
(n - 2r) Al E (u4) + 4 {(n - 2r + 1) A\ + (n - 2r + 2) A\ + . . • 
+ A* (n - 2r + r) } E (uf u2m) + 2B\ E (u*) 
+ 8 {B\ + B\ + . . . + -Br2_! + B\}E (uf <). . . . (29.58) 
Substituting pi^iovE (u*) and p\ for E {ui u^), dividing by (n — r) 
/*§, we find the sampling variance of the estimate of v. The expression can, however, be 
simplified to some extent. Putting 
2r\2 
2' ) and subtracting 
r—l 
T. 
>-£{'< 
r 
r-2 
2=0 
i + 
T \ 2 
r 
j +2 
7=0 X 
r\* 
3 
iJ \?+z 
+ 
we find, after lengthy algebraic rearrangement, 
2T„ 
var 
S. 
/^4 «ty*' 
2 
1 
(% — r) 
2r 
n 
(n — r) 
2r\2 
T \ # y * 
0 
r 
(29.59) 
_[_ 
2/z2 
4r 
2r 
7i 
2r 
r 
2 (n — r) 
f, r < \n. 
If terms of order (n — r) 2 can be neglected, this reduces to 
/4r\ 
pit — 3^1 . V 2r / 2/4 
n — r 
2r\2?z 
r / 
or, using the Stirling approximation to factorials, 
1 
n — r 
{ji* - 3/4 + /4 V(2m) }, 
. (29.60) 
. (29.61) 
. (29.62) 
which is a fair approximation to (29.61), being within 3 per cent, for r as low as 6. 
When the population of values of u is normal, ^4 — 3/4 vanishes and the formula 
simplifies accordingly. 
29.38. In a similar way it may be shown that 
cov 
/SL 
j (n — r) 
2r 
r 
$H-1 
(n 
1) 
2r + 2 
r + 1 
> 
/*4 ""~" OU/e 
% — r 
1 
2M2 
n — 
< 
2t: 
2A/2. + 2 
r J \ r + 1 
4r + 1\ 
2r ) 2n - 2r — 1 
2r 
2r + 2 
r + 1 
(% — r — 1) 
> 
r + 1 
n — r —- 1 
2 (n — r — I) 
. (29.63) 
THE VARIATE-DIFFERENCE METHOD 
393 
where 
r-~l 
rtr 
£ 
r + 1 
j •+ 2 
r-2 
+ 2Z 
r 
r -f- 1 
+ . . . + r 
r 
r ~j~ x 
Prom (29.60) and (29.63) we can determine the variance of the difference of 
8„ ., S, 
r —|— x 
{n •— r) 
2r 
and 
'r-j-1 
(n 
r 
i) 
2r + 2 
r j ' \ r + 1 
The general formula is complicated, but for normal variation, large n and r > 6 we have, 
analogously to (29.62), 
:r 
var < 
A3 
(n — r) 
'H-l 
2r 
(71 
1) 
/ 2r + 2 \ V 
\r + l ) , 
(3r + l)V(27rr)_ 
2~(2r+ 1)3(n ~~~ r — 1)1 (ft — r) 
2r 
r 
. (29.64) 
The arithmetic application of the formulae has been facilitated by the preparation of tables 
of the constants involved. Reference may be made to Tintner (1940) who gives tables 
prepared by himself, Anderson and ZaycofF. 
Example 29.7 
For the data of Table 29.3 (sheep population) an application of the variate-difference 
method up to the tenth difference gave the following results :— 
r 
1 
2 
3 
4 
5 
(> 
7 
8 
9 
10 
^/(2;)<» 
r) 
3468 
1442 
854 
629 
518 
:01 
371 
357 
347 
The values here are falling steadily from r = 1 to r = 10, but very slightly towards 
the end. From (29.64) for r = 6 we have for the variance of the difference, 80*7 
approximately and for r = 10, 25-8 approximately. It appears that the reduction in variance 
at r — 10 is losing significance, and that a moving arc of degree 10 would be sufficient to 
eliminate the systematic component. It does not, of course, follow that the trend-line 
must be of this degree, for we may not want to eliminate the oscillatory movements in 
the trend-line. 
29.39. The variate-difference method will clearly not eliminate systematic effects 
such as periodic terms with very short period. Consider, for instance, the series 1, — 1, 
1, — 1, etc. The first differences give us a series 2, — 2, 2, — 2, etc., second differences 
394. TIME-SERIES 
4j _ 4, 4, - 4, etc., and so on. The variance of the series of rth differences is, neglecting 
effects due to the shortness of the series, 22r times that of the original, and the quotient 
( 2r\ 
when this is divided by I J tends to 
22r(H)2 /— 
V L. —> V %T 
{2r!) 
and so increases without limit. In such a case we cannot obtain an estimate of the variance 
of any random element which may be present. 
NOTES AND REFERENCES 
References to the fitting of polynomials are given at the end of Chapter 22. For the 
moving average see Whittaker and Robinson's Calculus of Observations and the books by 
Macaulay (1931) and Sasuly (1934). 
Attempts have been made to use trend-lines for purposes of forecasting, and even to 
measure the standard error of a forecast—see Schultz (1930) and a discussion in Davis 
(1941). The methods proposed appear to me theoretically unsound and in practice they 
lead as a rule to such wide limits of error as to be of doubtful value ; but this is a personal 
opinion and the less sceptical reader may care to consult Davis's book and to follow up 
the references given therein. 
For the effect of moving averages on random variables see Yule (1921) and Slutzky 
(19376), the latter being an English version of a paper published in Russian many years 
earlier. See also Dodd (1939a, 1941a). Slutzky proves an interesting theorem the 
theorem of the sinusoidal limit—to the effect that repeated moving averages of certain 
kinds applied to random series generate a sine-curve. 
For the variate-difference method see the book by Tintner (11)40), a very thorough 
practical account with useful tables. The more important earlier memoirs are those by 
Anderson (1914, 1923, 1926), "Student" (1914), Morant (1921), and K. Pearson and 
Cave (1914). 
EXERCISES 
29.1. Show that in the formulae of equation (29.7) and similar formulae of higher 
orders the sum of the weights is unity. 
29.2. By evaluating the solutions of (29.5) determinantally show that a parabolic 
curve of second or third order giving a graduation 
at u_t + a(^1} u{t_x) . . . + a0 u0 + . . . + atut 
has 
_ o 3^2 + (3w ~ 1) - 5f 
ao 
(2n - 1) {2n + 1) (2ra+'~3~j" 
29.3. Show that the weights in the Spencer 21-point formula are 
_[- 1, _ 3, - 5, - 5, - 2, 6, 18, 33, 47, 57, 60, ... ] 
and that if it is applied to a random series the variance of the resultant is about one-seventh 
EXERCISES 395 
of the original series-—about the same reduction as would be given by a simple moving 
average of sevens. 
29.4. Show that Macaulay's 43-point formula, 
1 [12] [8] [5] 
7 - 1, 0, 0, 0, 0, 0, 0, 1, . . . 
10 
960 
has weights 
Jkk F7> 18> 30> 40> 45> 28> - 8, - 60, - 122, - 178, - 205, - 190, - 127, 
9600 
- 6, 163, 360, 562, 760, 928, 1050, 1127, 1156, . . .] 
and that it reduces the variance of a random series about as much as a simple average 
of nines. 
29.5. Take a random series of, say, 200 terms and determine " trends " by moving 
11 1 
averages -[9], qT[9]2 and ~ [9]s. Compare the mean distances between peaks and 
9 ol 729 
upcrosses with the theoretical values based on normal theory. 
29.6. If st is a random series, show that the correlation between successive members 
k 
of Ak et for long series is — « and hence tends to -1 as k increases. Hence show 
that the signs of successive terms in Ak ut tend to alternate, where ut is the sum of a random 
element and a systematic element representable by a polynomial ; and verify by reference 
to Table 29.9. 
29.7. By eliminating <32 from (29.19) show that, for a cubic curve, an accurate trend- 
line is given bv 
X lb * - X ^, -, /u ' X r 7 -| 
and generalise this result. 
(Cf. J. A. Higliam, J. Inst. Act. (1882-5), 23, 335; 25, 15, 245.) 
CHAPTER 30 
TIME-SERIES—(2) 
30.1. The present chapter is devoted to a discussion of oscillatory effects in time- 
series. We shall suppose that our series is stationary, i.e. has no trend, either because the 
original data contained none or because trend has been removed by one of the methods 
described in the last chapter. Our typical series will then fluctuate round some constant 
value which we may usually, without loss of generality, take to be zero. We shall assume 
that there is a prior possibility that part of the variation at least is random. This, indeed, 
TABLE 30.1 
Trend-free Wheat-Price Index {European Prices) compiled by Sir William Beveridge for 
the Tears 1500-1869. 
(From Beveridge, 1921.) 
i 
tS 
1500 
01 
02 
03 
04 
05 
06 
07 
08 
09 
10 
11 
12 
13 
14 
15 
16 
17 
106 
118 
124 
94 
82 
88 
87 
88 
88 
68 
98 
115 
135 
104 
96 
110 
107 
97 
18 75 
19 
20 
21 
22 
I 23 
25 
26 
27 
28 
I 29 
30 
31 
32 
33 
j 34 
! 35 
! 36 
i 
86 
111 
125 
78 
86 
102 
71 
81 
129 
130 
129 
125 
139 
97 
90 
76 
102 
100 
i 
1537 
38 
39 
40 
41 
42 
43 
44 
45 
46 
47 
4^ 
d 
50 
51 
52 
53 
54 
55 
56 
57 
58 
59 
60 
61 
62 
63 
64 
65 
66 
67 
68 
69 
70 
71 
72 
73 
M 
73 
86 
74 
74 
76 
80 
96 
112 
144 
80 
54 
69 
100 
103 
129 
100 
90 
100 
123 
156 
71 
71 
81 
84 
97 
105 
90 
78 
112 
100 
86 
77 
80 
93 
112 
131 
158 
i 
© 
1574 
75 
76 
77 
78 
79 
80 
81 
82 
83 
84 
85 
86 
87 
88 
89 
90 
91 
i 1 
1 
M 
113 
89 
87 
87 
79 
90 
90 
87 
83 
85 
76 
110 
161 
97 
84 
106 
111 
97 
92 108 
93 
94 
95 
96 
97 
98 
99 
1600 
01 
02 
03 
04 
05 
06 
07 
08 
09 
10 
100 
119 
131 
143 
138 
112 
99 
97 
80 
90 
90 
80 
77 
81 
98 
115 
94 
93 
• 
© 
I* 
1611 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
22 
23 
24 
25 
! 26 
27 
28 
29 
30 
31 
32 
33 
34 
35 
36 
37 
38 
39 
40 
41 
42 
43 
44 
45 
46 
47 
o Index. 
o 
99 
100 
94 
88 
92 
100 
82 
73 
81 
99 
124 
106 
106 
121 
105 
84 
97 
109 
148 
114 
108 
97 
92 
97 
98 
105 
97 
93 
99 
99 
107 
106 
96 
82 
88 
116 
© 
1648 
49 
50 
« 51 
52 
53 
54 
55 
56 
57 
58 
59 
60 
61 
62 
63 
64 
65 
66 
67 
68 
69 
70 
71 
72 
73 
74 
75 
76 
77 
78 
79 
80 
81 
82 
83 
84 
© 
1 
M 
122 
134 
119 
136 
102 
72 
63 
76 
75 
77 
103 
104 
120 
167 
126 
108 
91 
85 
73 
74 
80 
74 
78 
83 
84 
106 
134 
122 
102 
107 
115 
113 
104 
92 
84 
86 
101 
a 
© 
1685 
86 
87 
88 
89 
90 
91 
92 
93 
94 
95 
96 
97 
98 
99 
1700 
01 
02 
03 
04 
05 
06 
07 
08 
09 
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
© 
s 
74 
75 
66 
62 
76 
79 
97 
134 
169 
111 
109 
111 
128 
163 
137 
99 
85 
72 
88 
77 
66 
64 
69 
125 
175 
108 
103 
115 
134 
108 
90 
89 
89 
94 
107 
89 
79 
a 
© 
1722 
23 
24 
25 
26 
27 
28 
29 
30 
31 
32 
33 
34 
35 
36 
37 
38 
39 
40 
'41 
42 
43 
44 
45 
46 
47 
48 
49 
50 
51 
52 
53 
54 
55 
56 
57 
58 
i 
91 
94 
110 
111 
103 
94 
101 
90 
96 
80 
76 
84 
91 
94 
101 
93 
91 
122 
159 
110 
90 
81 
84 
102 
102 
100 
109 
104 
90 
99 
95 
90 
80 
85 
117 
112 
95 
£ 
1759 
60 
61 
62 
63 
64 
65 
66 
67 
68 
69 
70 
71 
72 
73 
74 
75 
76 
rjn 
78 
79 
80 
81 
82 
83 
84 
85 
86 
87 
88 
89 
90 
91 
92 
93 
94 
95 
© 
91 
88 
100 
97 
88 
95 
101 
106 
113 
108 
108 
131 
136 
119 
106 
105 
88 
84 
94 
§7 
79 
87 
88 
94 
94 
92 
85 
84 
93 
108 
108 
86 
78 
87 
85 
103 
130 
i 
© 
1796 
97 
98 
99 
1800 
01 
02 
03 
04 
M 
M 
95 
84 
87 
120 
139 
117 
105 
94 
125 
05 114 
06 
07 
08 
09 
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
22 
23 
24 
25 
26 
27 
28 
29 
30 
31 
32 
98 
93 
94 
94 
104 
140 
121 
96 
96 
130 
178 
126 
94 
86 
84 
76 
f-ff-r 
71 
71 
69 
82 
93 
114 
103 
110 
105 
82 
» 
© 
1833 
34 
35 
36 
37 
38 
39 
40 
41 
42 
43 
44 
45 
46 
47 
48 
49 
50 
51 
52 
53 
54 
55 
56 
57 
58 
59 
60 
61 
62 
63 
64 
65 
66 
67 
68 
69 
CD 
S3 
80 
78 
82 
88 
102 
117 
107 
95 
101 
92 
88 
92 
Ho 
139 
90 
80 
74 
78 
86 
105 
138 
141 
138 
107 
82 
81 
97 
116 
107 
92 
79 
81 
94 
119 
118 
93 
102 
396 
OSCILLATION AND CYCLE 397 
is necessary if our results are to have any practical application, for most of the series 
encountered in practice have some element of irregularity, however small. 
30.2. Four examples of the type of series under consideration have already occurred. 
The table of Example 21.11 (page 126) gives the deviations from a simple nine-year moving 
average of the yields of potatoes in tenths of tons per acre in England and Wales for the 
years 1888-1935. Table 29.1 (Pig. 29.1) gives the annual yields of barley in cwts. per 
acre in England and Wales for 1884-1939, no nine-year elimination of trend having been 
carried out in this case. Table 29.4 (Fig. 29.4) gives rainfall data at London over the 
century 1813-1912. Table 29.5 (Fig. 29.5) gives egg-production per laying hen in the 
XAjdJuH* 30.2 
Marriage Bate in England and Wales : Deviation from a Simple 11-Year Moving Average 
for the Years 1843-1896. 
Units 1 in 10,000. 
Year. 
1843 
44 
45 
46 
47 
48 
49 
50 
51 
52 
53 
54 
r>r> 
50 
57 
58 
59 
GO 
"Fables 30.1 and 30.2 give two further examples. The first is a famous series of trend- 
free wheat-price indices compiled by Sir William Beveridge and extending over 370 years, 
a phenomenal length of time for economic series. The second is the deviation from a 
simple 11-year moving average of marriage rates for the years 1843-1896. 
Oscillation and Cycle 
30.3. We will now attempt to define more closely the sense in which we use the 
words " oscillation " and " cycle ". It is particularly important to exercise great care in 
the use of an accurate nomenclature because a great deal of the literature on this subject 
suffers from confusion due to loose wording. 
Marriage 
Rate. 
- 6 
1 
12 
10 
- 6 
- 8 
- 6 
3 
11 
3 
- 8 
- 2 
3 
- 7 
3 
Year. 
1861 
(52 
65 
66 
67 
68 
70 
71 
72 
73 
74 
75 
76 
77 
78 
Marriage 
Kate. 
5 
7 
1 
6 
8 
9 
2 
8 
10 
7 
0 
8 
i 
5 
4 
3 
(J 
Year. 
1879 
80 
81 
82 
83 
84 
85 
86 
87 
88 
89 
90 
91 
92 
93 
94 
95 
Marriage 
Kate. 
12 
5 
0 
5 
7 
3 
4 
8 
6 
5 
1 
0 
(J 
2 
() 
5 
6 
1 
■398 TIME-SERIES 
By a cyclical component of a time-series we shall mean one which is a strictly periodic 
function of the time, that is to say, for which there exists a period co such that 
ut = %+a, = Wf+2a> = . . . = ^t^rk(x> — • • • • • * (30.1; 
whatever the value of t. The periodic functions which we shall consider in particular are 
the sine and cosine functions. If the series can be represented as the sum of a cyclical 
component and a random constituent, or by a cyclical component alone, we may speak 
of it as a cyclical series. 
30.4. If the series is not random it must move with more or less regularity about 
the mean value, and we shall then speak of it as oscillatory. The oscillatory movement 
may be in part due to random elements but must not be entirely so. A cyclical series is 
oscillatory, but an oscillatory series is not necessarily cyclical. 
An oscillatory movement may be the sum of two or more cyclical components. 
Consider, for instance, the sum of two periodic terms 
ut = sin h sm —. 
If co1 and co2 are commensurable there will be numbers, and in particular a smallest number 
a), which is an exact multiple of both of them. This is clearly a period of the series. 
But if cdx and o>2 are not commensurable there will be no period of this kind and the sum 
will be oscillatory but not cyclical. 
30.5. It may be felt by the reader that we could reasonably extend the use of the 
word " cyclical " to cover series which are the sum of cyclical terms ; but the danger of 
doing so is that within certain limits any series can be represented as a sum of harmonic 
terms, even if it is not itself oscillatory, in virtue of Fourier's theorem. Admittedly such 
a representation, to be exact, must in general consist of an infinite series of terms and is 
valid only in a certain range, but in practice a comparatively small, number of terms often 
gives quite a good approximation. We do not call a function a polynomial because it 
can be expanded in powers of the variable by Taylor's theorem ; and correspondingly 
we shall not call it cyclical because it can be expanded as a sum of harmonic terms by 
Fourier's theorem. On the whole it seems safer to avoid the word " cyclical " for series 
which consist of a finite number of cyclical terms. 
30.6. For our present purposes the main significance of the distinction we are 
attempting to make is that in a cyclical series the maxima and minima, apart from disturbances 
due to the superposition of a random element, occur at equal intervals of time and are 
therefore predictable for, a long way into the future—for so long, in fact, as the constitution 
of the system remains unchanged. In oscillatory series, on the other hand, the distances 
from peak to peak, trough to trough or upcross to upcross, are not equal, but vary very 
considerably. Similarly, in the oscillatory series the amplitudes of the movements may 
vary very substantially, whereas in a cyclical series they should be constant (again, except 
in so far as superposed random elements disturb them). 
30.7. Now the time-series observed in practice are very rarely cyclical as we have 
defined the term. The only case among those cited at the beginning of the chapter in which 
there appears to be any cyclical movement is that of egg-production per hen in Table 29.5. 
The far more usual case is that of varying amplitude and period from peak to peak or upcross 
TESTS FOB RANDOMNESS ■ 399 
to upcross. We shall therefore begin our study of oscillatory movements by considering 
the kinds of scheme which can give rise to the observed phenomena ; and then we shall 
examine methods of deciding which of the possible schemes should be chosen as the 
hypothetical representation in particular cases. 
Tests for Randomness 
30.8. The first stage, when confronted with a fluctuating stationary series, is to 
examine whether the fluctuations are purely random. Tests of randomness are easy to 
find, and in fact the random series is the happy hunting-ground of the worker whose interests 
lie mainly in the mathematics of the direct theory of probability. We have considered 
Home tests which are appropriate to the study of oscillatory movement in 21.43 to 21.46. 
Others which have gained popularity are based on the distribution of " runs " and on the 
correlation between successive members of the series. The reader will have no difficulty 
in composing others. All these tests are based on the non-parametric case, so that the 
alternative hypotheses are not usually brought specifically into view. We cannot 
therefore apply the general theory of Chapters 26 and 27 to determine " best " tests, and in the 
present state of knowledge are forced to be content with less definite ideas. So far as 
ease of application goes, the tests of 21.43 and 21.44 seem to have decided advantages, 
though they may be somewhat insensitive. The method of serial correlation, to which we 
refer below, gives a useful alternative in doubtful cases. In the sequel we shall suppose 
that before proceeding to search for systematic movements we have satisfied ourselves by 
one or more of these tests that such movements exist. 
30.9. We shall consider three schemes which can account for the typical oscillatory 
movement usually observed. 
(a) Moving Averages.—We have already seen in Chapter 29 that a moving average 
of a, purely random element can generate an oscillatory series with all the required properties 
of varying amplitude and mean distances—the Slutzky-Yule effect (29.25). Fig. 29.6 
illustrates the kind of oscillation which may arise. It is at least possible that some of the 
observed oscillations in time-series may be generated in this way; and in fact Slutzky 
(I93<>) has given an interesting example in which a part of his series generated by the 
moving average happens to agree very closely with an observed series. 
(b) tiums of Cyclical Components.—We may attempt, by Fourier analysis or the more 
general harmonic analysis, to represent the oscillations as the sum of a number of cyclical 
components. This is the classical approach. 
(r) Autotrgression Equations.—If a series is constructed by the recurrence formula 
ut+i =*f{ut, %-i,'. • • Ut~k) + **+i> • • • (30-2) 
where/ is a mathematical function and s a " disturbance " function which may be a random 
variable, then under certain conditions the generated series is of the required type. We 
shall consider in particular the series 
^/+2 = — aut+i — but + stJr2, .... (30.3) 
where a and b are constants and e is random. 
Table 30.3 (Fig. 30.1) shows a series of type (6) in the simplest case where only one 
cyclical component is involved, together with a random residual. Table 30.4 (Fig. 30.2) 
shows an autoregressive series constructed from random numbers by the formula 
uM = 1-1 ut+l - 0-5 ut + ei+2 (30.4) 
400 
TIME-SERIES 
TABLE 30.3 
Values of the Series ut = 10 sin ^ + st where st is a Rectangular Random Variable with 
Range — 5 to + 5, rounded off to Nearest Unit. 
Number of 
Term. 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
Series. 
3 
8 
6 
2 
- 4 
- 7 
- 9 
- 9 
- 10 
- 1 
8 
7 
6 
4 
- 3 
- 10 
- 11 
- 15 
- 4 
4 
Number of 
Term. 
21 
22 
23 
24 
25 
26 
27 
28 
29 
30 
31 
32 
33 
34 
35 
36 
37 
38 
39 
40 
Series. 
11 
13 
10 
6 
- 5 
- 8 
- 12 
- 10 
- 7 
0 
1 
8 
13 
7 
4 
- 9 
- 9 
- 6 
- 4 
- 2 
Number of 
Term. 
41 
42 
43 
44 
45 
46 
47 
48 
49 
50 
51 
52 
53 
54 
55 
56 
57 
58 
59 
60 
i 
Series. 
5 
12 
7 
5 
3 
- 2 
- 12 
- 12 
- 8 
- 1 
11 
13 
12 
7 
5 
- 1 
- 6 
- 14 
- 8 
CO 
CO 
o 
*2 
0 I L 
of Term. 
er 
Fig. 30.1.—Graph of the Values of Table 30.3. 
TESTS FOR RANDOMNESS 
401 
't~{~l 
Values of Series ut+2 = l'l u 
Variable with Range 
TABLE 30.4 
- 0*5 ut + s^^ where st+2 ^s a Rectangular Random 
9-5 to 9-5, rounded off to Nearest Unit. 
Number 
of Term. 
Value of 
Series. 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
22 
r~t 
6 
- 6 
- 4 
3 
- 4 
- 5 
- 1 
10 
10 
6 
- 4 
— 4 
- 7 
- 2 
6 
17 
24 
17 
4 
1 
- 5 
Number 
of Term. 
24 
25 
26 
27 
28 
29 
30 
31 
32 
33 
34 
35 
36 
37 
38 
39 
40 
43 
44 
Value of 
Series. 
4 
5 
9 
4 
4 
3 
y 
4 
8 
6 
3 
2 
0 
1 
3 
3 
1 
8 
3 
8 
10 
16 
Number 
of Term. 
45 
46 
47 
48 
49 
50 
51 
52 
53 
54 
55 
56 
57 
58 
59 
60 
61 
62 
63 
64 
65 
CO 
•—» 
O 
<3 
-70 
4* 
0 
30 
hlwnfc.elr of Ter 
-20 
Fig. 30.2.—Graph of the Values of Table 30.4. 
A.S.--VOL. II. 
D D 
402 TIME-SERIES 
30.10. It is quite possible that theoretical reasons may suggest other schemes for 
study as the subject progresses. For instance, we might wish to consider series defined 
by differential equations, on the analogy of the similar equations determining oscillations 
in physical phenomena such as vibrating strings or electrical discharges. Something has, 
in fact, already been done in this direction. We shall, however, confine our attention 
to the three schemes indicated above, and particularly the second and third. 
30.11. On the face of it, an observed series exhibiting the typical movements in 
amplitude and period might be due to any one of the three schemes or even to a combination 
of them. We require, in the first instance, some objective criterion for deciding which of 
them is appHcable in particular cases. Inspection of the primary data, though useful, is 
quite an unreliable guide in making a decision on this point, particularly if the series 
is short. Experience seems to indicate that few things are more likely to mislead in the 
theory of oscillatory series than attempts .to determine the nature of the oscillatory 
movement by mere contemplation of the series itself; and yet this is the method, if one can 
dignify it by such a term, which has perhaps been most widely used in the past. 
Serial Correlation 
30.12. Suppose our series of values IS t*»J ... W,vj • Let us form the product-moment 
correlation coefficient between successive terms, i.e. 
^ cov (%, u^) 
(var Uj var uj+1)* 
There will be (n — 1) pairs entering into the correlation, and the variances of Uj and uj+x 
differ only in the fact that the first relates to the terms uu u2, . . . un^x and the second 
to the terms u2i u3, . . . un. The coefficient rx is called the serial correlation coefficient 
of the first order, or more briefly the first serial correlation.* 
More generally, let us define a coefficient of order h : 
covK, uj+k) 
(var Uj var uj+kf- 
]-l V ' \ 3 = 1 
S 
n—k 
By convention we define 
1 V1 2 1 
< —^ /j u)— ___01 ? u* i v < —- y u 
2i /"ii n "—F. /. "j+* 
(30.7) 
(30.8) 
30.13. In practice we often require to calculate serial correlations up to r30 and for 
long series as many as 60. The arithmetic is tedious but may be systematised so as to 
reduce labour, which arises chiefly in the determination of cross-products forming the 
covariances. 
The series of n terms is written down vertically on each of two slips of paper, the spacing 
being equal on the two slips. This can very conveniently be done on a Burroughs tabulator 
with a split keyboard, the series being recorded in duplicate and the resulting strip cut up 
* It is sometimes convenient to confine this expression to values calculated from samples, the 
corresponding values for the infinite series being termed " autocorrelations " and denoted by a Greek p. 
SERIAL CORRELATION 
403 
the middle. To calculate the first product-sum we pin the slips so that the first term 
on the right-hand slip is opposite the second term on the left-hand slip, and hence so that 
the jth term on the right is opposite to the (j + l)th on the left all the way down. For 
most series the differences of two terms which are opposite can be obtained mentally by 
subtraction, squared, and set up on an adding-machine. The sum of squares of differences 
is thus determined, and the cross-product found from the simple identity 
2 E (XY) = E (X2) + E (72) - 27 (X - Y)\ 
We then move the right-hand slip down one space so that the jth term is opposite the 
(j + 2)th term on the left and repeat the process ; and so on to as many terms as may 
be required. 
In this process E (X2) and E (Y2) are required at each stage, and it is as well to 
determine them by cumulative summation from the two ends of the series. E (X) and E (Y) 
are also required. It is also convenient on occasion to reduce the series to zero mean 
approximately before beginning the analysis. 
Example 30.1 
To illustrate the arithmetic we will take a very trivial example which the reader should 
check for himself. Take the series 
- 5, - 6, -2, 4, 7, 3, 1, - 5, - 1, 2. 
We set up the following scheme of tabulation for calculating serial correlations up to the 
fifth order :— 
n — h. 
10 
9 
8 
7 
6 
5 
h. 
0 
1 
2 
3 
4 
5 
E(X) 
(from beginning 
of series). 
2 
4 
3 
2 
1 
9. 
£{Y) 
(from end 
3 
9 
11 
7 
0 
(from 
beginning). 
170 
166 
165 
140 
139 
130 
£(Y*) 
(from end). 
170 
145 
109 
105 
89 
40 
2 (X - Y)2. 
0 
143 
344 
445 
380 
172 
E{XY). 
170 
84 
- 35 
- 100 
— 76 
- 1 
The number n — k is the number of pains entering into the fcth correlation. E (X) is the 
sum of n — h terms beginning at the first term, E (Y) the corresponding sum of the last 
n — k terms, and similarly for E (X2) and E (Y2). These are the quantities required to 
calculate the variances entering into the denominator of the Jdh. serial correlation. The 
quantities E (X — Y)2 are calculated by the moving-slip method described above. 
We now calculate the correlation coefficients in the usual way, e.g. for r1 
var X 
var Y 
= 18-247 
16-000 
cov (X, Y) 
V(18-247 x 16) 
9-4815 
+ 1 1. ri r\ • 
V fjfj y 
404 TIME-SERIES 
and for r6 
var X = ~2 - ( - | V = 25-840 
40 / 0 \ 2 
var Y = — - [ - ) = 8-000 
cov(Zjr) = -I-(-|)(2) = -0-200 
r5 = - 0-01. 
When w is large and the origin is chosen so that the mean of the whole series is approxi- 
y (XY) 
mately zero, a sufficiently good value of r is given by f y ,Yz\ rcy2\'\V ^e corrections 
required to adjust the sums of squares and products to values about the mean being small; 
but this approximation must be used with some care and in any case the first two or three 
serial coefficients should be worked out exactly. 
The Correlogram 
30.14. The diagram obtained by graphing rk as ordinate against k as abscissa and 
joining the points each to the next is called a correlogram. We shall give a number of 
examples below and shall see that the form of the correlogram provides a method of 
discriminating between the various types of oscillatory series. 
30.15. Suppose, for example, that the series is generated by a moving average of 
random elements with weights al9 a2i . . . am. The typical term of the series is then 
Uj = ax Sj + a2 Sj+1 + . . . +ct>ms.j+m_l . . . (30.9) 
Without loss of generality we may take JE7 (e) = 0 and hence E (uj) = 0. Then 
E (uj uj+k) = E {«! Sj + a2 ej+1 + . . . + am ej+m^.1} 
|ai sj+k + a* Gj+k+l + • • • + ame}+k+m-lf- 
Since 
E (sj sj+k) = 0, k ^ 0 
= v, say, if k = 0 
we have 
E (uj ujJrk) = (ax ak+1 + a2 ak+2 + . . . + am_k am) v, . . (30.10) 
provided that ra > k> But if k > m then 
E(ujUj+k) =0. . . . . . (30.11) 
Thus for an infinite series generated by the moving average the serial correlations vanish 
for k> m, and the correlogram from that point onwards coincides with the x-axis. In 
particular, if the a's are all equal to l/m, we have 
v 
E ty uj+k) = (m — fe) —, 
and hence 
k 
rk = 1. — —, (30.12) 
m 
so that the correlogram consists of a straight line joining the point (0, 1) to (k, 0), together 
with the #-axis from the latter point onwards. 
THE CORRELOGRAM 
405 
Example 30.2 
The weights of the Spencer 21-point formula are 
350 
| j^ j^ 5^ 
2, 6, 18, 33, 47, 57, 60, . . .}. 
Apart from the divisor 350, which may be disregarded for present purposes, the sum of 
squares of weights is 17,542. The products (30.10) and the corresponding serial correlations 
are as follows :— 
Km 
0 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
U a,j ay-ffc. 
17,542 
16,786 
14,667 
11,584 
8,085 
4,726 
1,951 
n> 
1-000 
0-957 
0-836 
0-660 
0-461 
0-269 
0-111 
k. 
6 
1,074 
1,430 
1,298 
0-000 
0-061 
0-082 
0-074 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
S Cbj CLj+h' 
- 930 
- 528 
- 214 
- 27 
50 
59 
40 
19 
6 
1 
0 
n. 
0-053 
0-030 
0-012 
0-002 
0-003 
0-003 
0-002 
0-001 
0-000 
0-000 
0-000 
7*0 
0-8 
0-6 
f? 0-2 
o 
CD 
$-0-2 
0-4 
■0-6 
•0-8 
~V0 
X 
/ 
\ Xs^_7 
Vol 
' 
5 2 
ue$ of k. 
0 2 
5 3 
0 
Fig. 30.3.—Correlogram of Series generated by the Spencer 21-point Formula (Example 30.2). 
The correlogram is shown in Fig. 30.3. From 7c = 13 onwards the correlations are very 
small, and from h = 21 onwards they vanish completely. 
406 TIME-SERIES 
30.16. Suppose now that the series consists of a sine term A sin Qt plus et, a random 
residual. As before, we may suppose E (ut) = 0, and hence 
E (uj uj+k) = E {A sin dj + £j} {A sin 6 (j + h) + sj+k} 
= A2 E {sin dj sin 6 (j + k) } 
= £_ y {sin dj sin 6 (j + k) } (30.13) 
)l JmmJ 
A2 
= — E {cos Ok — cos 6 (2j + k) } 
A2 fl7 A* Gosd (h+n + l)smnd 
= -__cos 0A: — — -_—3—L.—J—£—__., 
2 2n sin 0 
(30.14) 
Thus for large n we have effectively, unless 6 is small 
A2 
2 
Similarly we find 
j? (^ Uj+k) = -vr cos 0fc = B cos 0fc, say. . . . (30.15) 
Hence 
E (u*) — jB + var s — O, say. .... (30.16) 
fA=~cos0&, fe>0. . . . . (30.17) 
In short, for an infinite cyclical series the correlogram itself is a harmonic with period 
equal to that of the original harmonic component. 
30.17. When the original series is the sum of several harmonic terms the formula 
for rk will, in general, be the sum of harmonics, not necessarily with the same periods. 
Thus the correlogram will present a sinusoidal form which will not degenerate to the #-axis 
after some fixed point and will not, in fact, be damped. 
30.18. Consider now the series defined by (30.3), namely 
ut+2 = — au/+l — but + et+2- 
This is a difference equation which is easily solved by the usual methods.* The general 
solution of 
ut+2 + aui+l + but = 0 . . . . . (30.18) 
is 
ut = p* {A cos 6t + B sin 6t) .... (30.19) 
where p = ^b "\ 
a a \ (30.20) 
cos 0 = — N ' 
2^b J 
Here <\/b is to be taken with positive sign, and it is assumed that 46 > a2. We also assume 
that «sjb is not greater than unity. The contrary case is mathematically permissible, but 
it implies that ut increases without limit, which is outside the domain of our consideration. 
* See, for instance, *Mi]ne-Thomson, Calculus of Finite Differences, chapter 13. 
THE CORRELOGRAM 407 
Consider now the series 
/ J £j et-j+l, (30.21) 
where tjt is a particular solution of (30.19) such that £0 = 0 and fx = 1, i.e. such that 
t 
V(46 - a2) 
ptaixiQt. .... (30.22) 
On substituting (30.21) in the original equation it will be found to provide a particular 
solution. The general solution is then 
oo 
ut = p* (A cos Ot + B sin 6t) + V ^ s*_m. . . . (30.23) 
As p is not greater than unity we shall, in general, find that the first term in this expression 
is damped out of existence. If we may regard our series as having been " started up " 
some time prior to the point t = 0, the solution is effectively 
op 
30.19. In this form the autoregressive scheme is seen to be a moving average of 
a component e with infinite extent and damped harmonic weights. Consider now its 
correlogram. We have 
GO 
X S* l'+k = 4T=T»S {pW+'c sin dj sin e {j + k) } 
Now 
..2®!.._. z [p*»{ cos Ok - cos 0 (2j + ft) } ] 
4:6 CI" 
2pIc f cos flfc cos OA; — p2 cos ^ (^ ~" 2) 
W~—~a*\T~'p* ~~ " ' 1 -~'2p2 00^20""+^" 
E (u,j uj+k) = E {27 (^ e,_i+1) 27 (f, e,+Jfe-:J+i) } 
(30.25) 
OQ 
var e JT (f, f j+fc) (30.26) 
Thus 
i-o 
CO 
var e JT* (£, |j+fc) 
7=s(.) 
var e 
oo 
408 TIME-SERIES 
which, on substitution from (30.25), reduces to 
rh = ^ t ^ _.. {sin (k + 1) 6 - p* sin (k - 1) 8}. . . (30.27) 
(1 + p2) sin 6 
Writing 
tkn y = i-±^ taa 0, (30.28) 
1 —- p2 
we find 
g*anjfcfl + y) k>Q (30 29) 
rk = 
sin ^ 
Prom this we see that the correlogram will oscillate with period 2jt/0, but that, owing to 
the factor pk, it will be damped. If k is negative the formula applies, except that | k 
must be used instead of k on the right-hand side of (30.29). 
30.20. We thus reach the interesting conclusion that the three types of series 
considered in 30.9, however similar to the eye, will have distinct types of correlogram, 
provided that the series are long enough for the observed correlations to approach the expected 
values for an infinite series. The correlogram of a series generated by moving averages, 
though it may oscillate as in Example 30.2, will vanish after a certain point; that of a 
series of harmonic terms will oscillate, but will not vanish or be damped ; that of the auto- 
regressive scheme will oscillate and will not vanish, but it will be damped. The correlogram 
therefore offers a theoretical basis for discriminating between the three types of oscillatory 
series. 
30.21. Unfortunately the series with which we have to work are very frequently 
too short to enable a decisive distinction to be made. We shall see below that divergence 
between theory and observation can be very considerable, and that sampling theory has 
not yet advanced far enough to enable us to make objective judgments in probability 
about its significance. We shall have to rely on limited experimental evidence and to 
some extent on intuitive judgment in reaching conclusions. If, therefore, the remainder 
of this chapter contains gaps in the treatment and leaves certain points undecided the 
reader will understand that the reason is ignorance rather than indifference. 
Examples of Correlograms from Observed Series 
30.22. We will in the first place give the correlograms of a few of the series given 
earlier in this and the preceding chapter. 
Example 30.3 
In Table 30.2 we gave the deviations from the trend of marriage rates for the years 
1843-1896. The first 20 serial correlations of this series are shown in Table 30.5 and the 
correlogram in Pig. 30.4. 
THE CORRELOGRAM 
409 
TABLE 30.5 
Serial Correlations of the Marriage Data of Table 30.2. 
Order 
of 
Correlation 
k. 
1 
2 
3 
4 
5 
N 6 
7 
8 
9 
10 
VO 
' 0-8 
0-6 
^ 0-2 
o 
kf3 
-OS 
-0-8 
0-563 
- 0-089 
- 0-498 
- 0-631 
- 0-467 
- 0-025 
0-353 
0-396 
0-254 
0104 
Order of 
Correlation 
ifc. 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
\ 5 [ 1 
/ Values 
° V s^- 
ofk. 
'?%• 
- 0-080 
- 0-136 
- 0132 
- 0-058 
- 0-095 
- 0-126 
- 0-036 
0-131 
0-209 
0-205 
5 / 2 
0 
-1-0 
Fig. 30.4.—-Corrologram of Marriage Data of Table 30.2 (Table 30.5.). 
The corrclogram is smooth and suggests the operation of an autoregressive scheme. 
There is little indication that a moving average, at least of extent less than 20, would account 
for the series, but on the other hand some damping appears to he present. 
Example 30.4 
Table 30.6 shows the first 60 serial correlations of the Beveridge series of Table 30.1, 
the correlogram being given in Fig. 30.5. 
410 
TIME-SERIES 
TABLE 30.6 
Serial Correlations of the Beveridge Wheat-Price Index of Table 30.1 
Order of 
Correlation 
k. 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
14 
15 
rh. 
0-562 
0-103 
- 0-075 
- 0-092 
- 0-082 
- 0-136 
- 0-211 
- 0-261 
- 0-192 
- 0-070 
- 0-003 
- 0-015 
- 0-012 
0-047 
0-101 
tCt 
16 
17 
18 
19 
20 
21 
22 
23 
24 
25 
26 
27 
28 
29 
30 
n* 
0158 
0-109 
0-002 
- 0-075 
- 0-062 
- 0-021 
- 0-062 
- 0*088 
- 0-084 
- 0-076 
- 0-091 
- 0-052 
- 0-032 
- 0-012 
0-059 
k. 
31 
32 
33 
34 
35 
36 
37 
38 
39 
40 
41 
42 
43 
44 
45 
r» 
0-060 
0-008 
0-039 
0-007 
0-056 
0-010 
0-004 
0-015 
0-047 
0-047 
0-008 
0-034 
0-065 
0-099 
0-009 
46 
47 
48 
49 
50 
51 
52 
53 
54 
55 
m 
57 
58 
59 
60 
rk> 
0-036 
0-013 
0-042 
0-062 
0-065 
0-050 
0-009 
0-027 
0-053 
0073 
0*106 
0-084 
0019 
0-003 
0-010 
Fig. 30.5.—Correlogram of the Beveridge Series of Table 30.1 (Tublo 30.6). 
# 
The correlogram here is almost certainly damped. The oscillations persist in a most 
remarkable way, notwithstanding the diminishing amplitude, and the presumption is 
a strong one that the series is of the damped type. 
THE CORRELOGRAM 
411 
Example 30.5 
In Table 29.8 (page 386) we gave the residuals of a sheep-population series for the 
years 1871 to 1935. Table 30.7 shows the first 30 serial correlations of this series and 
*xg. 30.6 the correlogram. Again the correlogram is oscillatory, but the damping is not 
so clear. . ° 
TABLE 30.7 
Serial Correlations of the Sheep Data of Table 29.8. 
Order of 
Correlation 
k. 
1 
3 
4 
5 
6 
7 
o 
9 
10 
rk. 
0-595 
0151 
0-601 
0-537 
0138 
0144 
0-203 
0118 
0-006 
0-078 
k. 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
»V 
- 0-142 
- 0-172 
- 0-186 
- 0-128 
0-052 
0-276 
0-439 
0-293 
- 0-074 
- 0-359 
k. 
21 
22 
23 
24 
25 
26 
27 
28 
29 
30 
rk. 
- 0-381 
- 0-118 
0173 
0-343 
0-352 
0-154 
- 0-203 
- 0-456 
- 0-415 
- 0-184 
Fig. 30.6.~-Correlogram of the Sheep Population Data of Table 29.8 (Table 30.7.) 
412" TIME-SERIES 
Significance of a Correlogram 
30.23. The foregoing examples illustrate one of the main difficulties we have to face 
in correlogram analysis. On intuitive grounds we seem to be justified in rejecting the 
scheme of moving averages as a possible scheme for the series of these examples, since the 
oscillations in the correlograms persist; but we can no doubt find moving averages which 
will produce such correlograms, though their extents would have to be long (over 60 in 
the case of the Beveridge series) and their weights artificial. The only final test seems to 
be to ascertain such a moving average and then to examine whether it will predict further 
terms in the series if such can be observed. 
30.24. Distinction between the scheme of harmonic components and the auto- 
regressive scheme is even more difficult for short series, since the correlograms for the 
latter do not damp out according to expectation. Consider in fact an autoregressive 
scheme of the simple linear type (30.3). There will be the usual variation in length from 
peak to peak and in amplitude ; but if the section of the series is a comparatively short 
one, covering, say, four or five oscillations, the oscillations will not have time to get very 
much out of step and the serial correlations will be systematically larger than one would 
expect for an infinite series. This effect is exhibited in Table 30.8 and Pig. 30.7, which 
give the serial correlations and the correlogram for the series of Table 30.4, given by the 
formula 
ut+2 = 1*1 ut+l - 0-5 ut + e,+2. 
Here the damping factor p = \/b = 0-7071, and by the thirtieth correlation rk should be 
very small, less than 0-002 in absolute magnitude. Actually it is 100 times as large. The 
mere fact that an observed correlogram for a short series fails to damp very rapidly is 
not, therefore, a very definite indication that the series is not ruled by the autoregressive 
scheme. On the contrary, failure to damp may be expected. 
30.25. We are on firmer ground when considering the significance of a correlogram 
in the sense of judging whether it can be derived from a random series. 
1 
(a) The variance of rk in a random series of n terms is approximately j, provided 
lb A/ 
that n is large. For 
m 
j=i 
(n — 1c) 
1 
1 
E S (x'j xj+Jc) 
n — h 
var2 x. 
Hence, for large samples, 
1 var2 x 1 
var r = — = (30.30) 
n — k var2 x n — h 
R. L. Anderson (1942) has recently given exact results for the significance of a serial 
correlation. 
(6) For our purposes, however, the important point is not whether a particular serial 
coefficient is significant, but whether the oscillatory character of the correlogram as a whole 
SIGNIFICANCE OF A CORRELOGRAM 
413 
TABLE 30.8 
Serial Correlations of the Artificial Series of Table 30.4. 
Order of 
Correlation 
k. 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
013 
0-07 
012 
0-05 
17 
18 
19 
20 
1"k' 
0-70 
0-29 
0-01 
- 017 
- 0-27 
- 0-25 
k. 
11 
12 
13 
14 
15 
16 
*•*. 
0-05 
0-17 
0-27 
0-31 
0-30 
0-18 
0-12 
0-29 
0-33 
0-22 
k. 
21 
22 
23 
24 
25 
26 
27 
28 
29 
30 
*V 
0-05 
- 042 
- 0-28 
- 0-43 
- 0-57 
- 0-56 
- 0-26 
0-02 
047 
0-27 
1-01- 
Fig. 30.7.—Correlogram of the Artificial Series of Table 30.4 (Table 30.8.). 
is so. Here we have to form an intuitive judgment, but it can hardly be doubted that 
the undulations in Figs. 30.4 to 30.6 are not accidental. Something exists to be explained 
as a systematic effect, though what that effect is may be more difficult to decide. 
30.26. We shall proceed to study the autoregressive scheme and the scheme of 
cyclical components in more detail, without prejudice for the time being to the question 
as to which is the better representation in particular cases. This latter is not, in fact, 
entirely a statistical matter, and we shall return to it in 30.39. 
414 
TIME-SERIES 
The Autoregressive Scheme 
30.27. We consider in the first instance the simplified scheme of equation (30.3). 
The theoretical correlogram for a series generated by this equation is of the damped type 
given by (30.29), 
*k 
pk sin (kd + y>) 
sin ip 
where 2n/d is the autoregressive period of the regression equation and is given by 
cos 6 
a 
2^b 
The typical series of this kind has no " period " in the strict sense. The lengths from 
peak to peak or from upcross to upcross vary in the characteristic way. It appears from 
experiment (but has not, I think, been shown theoretically) that the distribution of 
distances from peak to peak is of the unimodal type with a central value somewhere near 
the mean distance between peaks ; and similarly for troughs and upcrosses. In speaking 
of the " period " of an autoregressive series we mean the central value of one of these 
distributions. The question we have now to consider is whether this period is the same 
as the autoregressive period 2n/6 of the regression equation. 
30.28. We have seen in 29.26 that the mean distance between upcrosses of the 
series generated by the moving average whose weights are f x . . . |m is given by 2n/<f>, 
say, where 
m—1 
cos 6 = i=!_—_. 
Substituting for | from (30.22) and using (30.25), we find 
2p J cos d _ . cosd(l~-p2) 
cos 
a = 46 — a% { 1 — p2 1 — 2p2 cos 28. + pA 
~~ 2 ( I ~ T - p* oos26T 
46 - a2 \ 1 - p* 1 - 2p2 oosW+'p1 
_ 2p cos 6 
~ "l~+~pT 
a 
1+6 
. (30.31) 
Thus the mean period as defined by upcrosses is 
2n/8bro cos 
a 
1+6 
(30.32) 
whereas that for the autoregressive period of the equation is 
27r/arc cos 
a 
2V&/ 
(30.33) 
THE AUTOREGRESSIVE SCHEME 415 
30.29. The mean period between upcrosses is thus not the same as the autoregressive 
period. The two are very close for many of the values of a and b arising in practice. For 
instance, when 6 = 1 they are identical; when a = 1, 6 = 0-5 their ratio is 1-07. One 
might infer that an estimate of the period of an autoregressive scheme can be obtained 
from the correlogram, but this generalisation requires some important qualifications. 
(a) Firstly, the ratio of (30.33) to (30.32) is not necessarily close to unity for values 
of 6 in the neighbourhood of a2/4, i.e. when d is small and the autoregressive period is long. 
Consider, for instance, the series generated by 
ut+2 = !*2%f i — 0'4ut + st+2. 
We have 
cos d = - -?— = s-^J-j = 0-9499 
2*s/b 2V0-4 
6 = 18-2°, period = 19-7 units. 
However, for ^>, 
cos <f> = — = 0-8571 
^ 1-4 
<f> — 31°, period = 11-6 units. 
The mean distance between upcrosses, and a fortiori that between peaks, is very much 
shorter than the autoregressive period. 
(b) The mean distance between upcrosses may miss certain oscillations above or 
below the #-axis, so that it overestimates the period between peaks or troughs. On the 
other hand, the latter may include ripples on the main wave which we wish to ignore. 
The reader can verify for himself, by constructing an autoregressive series by some such 
formula as the above, how difficult it is to draw the line in particular cases. The difficulty, 
however, must be faced, for it is precisely the kind which we meet in dealing with observed 
series. 
(c) Owing to the appearance of the phase angle yj in equation (30.29) the starting- 
point of the correlogram (k = 0) is not to be regarded as a maximum. The period of the 
correlogram is therefore to be calculated either by ignoring this point or by reference to 
distances between troughs and upcrosses in the correlogram. 
30.30. The equation 
ut+% + aui+\ t- but ^ £h-2 
may be regarded as expressing the regression of ul+2 on ut+\ an(^ ut> ^le "term ei+2 being 
a residual error. We may therefore estimate the constants a and 6 from the regression 
equation of the observed series in the usual, way. If we assume that the series is long enough 
for end effects to be negligible in determining the variances of the finite series, then 
var ^+2 = var%fi = var ut> ancl from the usual formulae for regressions we find 
a = - fl(1 ^va) (30.34) 
1 - rj 
b = - ^—^ = - 1 + t r4 (30.35) 
1 — rj 1 — r j 
This gives us the constants of the autoregressive scheme from the serial correlations. 
It should, however, be realised that these estimates are rather sensitive to superposed 
error of the type we refer to below (30.32), and it is therefore unsafe to estimate the 
416 TIME-SERIES 
autoregressive period from them. The correlogram itself appears to be a safer guide on 
this matter. 
Example 30.6 
Consider again the sheep data of Table 30.7 and Fig. 30.6. Suppose we have decided, 
from the appearance of the correlogram, to attempt to represent the series by an 
autoregressive scheme. 
In the first place, we have to inquire whether a scheme of the simple linear form (30.3) 
is likely to be adequate. Would it, for example, be better to consider the more general form 
or need we take into account curvilinear regressions such as 
The first point can be elucidated by the use of partial and multiple correlations. The 
following are the partial coefficients and the function of the multiple correlation 1 — JR2 
as determined by the continued product of (1 — r2) (cf. vol. I, equation 15.45, 
p. 380) :— 
Order of Partial 
Correlation. 
12 
13.2 
14.23 
15.234 
16.2345 
17.23456 
Value of Partial 
Correlation. 
0-595 
- 0-782 
0-097 
- 0-183 
0-031 
0-014 
TT /I -.9!\ 
II (1 — r4). 
0-6460 
0-2509 
0-2485 
0-2402 
0-2400 
0-2400 
Evidently no appreciable gain in representation is to be obtained by taking the regression 
on more than the two preceding terms. 
The possibility as to better representation by taking curvilinear regressions may be 
considered by drawing the scatter diagrams of ut on utJrl and ut on ut+2. These are 
shown in Fig. 30.8. It seems clear that there is an essential scatter in the data which no 
ordinary polynomial can represent, and that curvilinear terms are unlikely to add anything 
material to the linear regressions. 
We conclude that if the data are of the autoregressive type it is unnecessary to 
consider any more elaborate scheme than the simple type 
ut+2 + aui+l + but = st+2. 
For this series we have 
Tl = 0-595, ra = — 0-151. 
Hence 
— f\ 
-200 
200 
Values 
t+1 
100' 
X 
X 
X 
X 
-too 
X 
X X 
X 
X X 
X 
X 
-100 
X 
X 
X X 
X 
X * 
Values of ir ioo 
x "C 
-20 0J 
■m n 
200 
x 
X 
r— 
•200 
200 
Values of 
u 
t+2 
x 
X 
100 
X 
X 
X 
X 
-100 
X 
X 
-100 * 
X 
Value of ut x X10° 
200 
x * 
X X 
Fig. 30.8. 
VOL. II. 
-200, 
-Scatter Diagrams of ut on ut+i (top figure), and ut on ut+2 (bottom figure). 
417 E 
418 TIME-SERIES 
The autoregression equation is 
ut+2 = 1-060 ut+1 - 0-782 ut + £/+2. 
For the autoregressive period we have 
cos 6 = r-^kv = °-600> ° = 53*2° 
2V(0'782) 
and hence the period is —— = 6-8 years. 
Now in the correlogram (Fig. 30.6) there are peaks at h = 7, 17 and 25, giving a period 
of about 9 years ; and there are troughs at k = 3, 13, 21 and 28, giving a mean period 
of 8-3 years. The autoregressive period as estimated from the correlogram is then between 
8 and 9 years, whereas that given by the autoregression equation is 6-8 years, considerably 
shorter. 
Using the values of a and b found above, we have for the mean distance between 
upcrosses, 
1-060 
cos 6 = i^l = 0-5948, 6 = 53-5°, 
r 1-782 ' Y 
giving a mean distance practically equal to the autoregressive period as shown by the 
regression equation. 
Finally, looking to the original series, we see that there are nine major peaks, the 
58 
first in 1874 and the last in 1932, so that the mean distance between peaks is — = 7-25 
o 
years ; and nine upcrosses, the first between 1872 and 1873 and the last between 1930 and 
58 
1931, so that the mean distance between upcrosses is — = 7-25 years, the same as for peaks. 
The upcross at 1876-7, however, is due to a temporary fall below the zero line, and had it 
not occurred we should have found a mean distance of 8-3 years. 
We have therefore reached this position : the mean period in the series itself appears 
to be about 7-25 years ; that given by the regression constants is 6-8 years ; and that given 
by the correlogram is about 8-5 years. These figures are scarcely close enough for comfort, 
and further data would be required to arrive at a more accurate estimate of the mean 
period. Nevertheless, they illustrate very well the kind of divergence which appears to 
be more the rule than the exception in dealing with short series. We should expect the 
correlogram to give a higher value than the series itself, for there may appear peaks or 
upcrosses in the latter which are purely temporary fluctuations due to the casual element. 
On the other hand, the regression constants appear to give consistently lowrer values for 
the autoregressive period than the correlogram, an effect found by Yule (1927a) for sunspots, 
Wold (1938a) for cost-of-living indices, and Kendall (1944a) in series of agricultural prices, 
acreage and livestock populations. 
30.31. Let us examine more closely the effect referred to at the end of the previous 
example. Our autoregressive system is based on a random element st which is added to 
the term ut+2- We can therefore regard the value at time t + 2 as composed of two parts, 
a systematic element expressed by aut+1 + but, giving the effect of the past history of the 
system at times t + 1 and t, together with a new random element peculiar to the moment. 
This latter is random in the sense that it is casual and unpredictable ; but once it has 
occurred it is incorporated into the motion of the system and exerts an influence on future 
THE AUTOREGRESSIVE SCHEME 419 
history. It is therefore quite unlike an error of observation or a sampling error which 
distorts the value of a particular member but does not affect the others. 
Now suppose that such an error of observation is present, and let us represent it by 
7], For long series this element will increase the variance of the observed values by var r], 
but if it is independent of the remaining constituents of the series it will not affect the 
covariances. Hence the serial correlations will all be reduced in a constant proportion c, 
except of course r0; and this, as we proceed to show, will affect the autoregressive period 
as derived from the regression constants, in general shortening the period quite considerably. 
30.32. If rx is reduced to crx and r2 to cr2, the constants of the regression equations 
are, from (30.34) and (30.35), 
_ a' = ?liLzi^) (30.36) 
1 — c2 rf 
-V = C^ £-£ (30.37) 
1 — c2 r{ 
The estimated autoregressive period is then d\ given by 
cos d = - 
2<sJV 
or1{l — cr2) 
2 V(l — c2 rf) (c2 r\ — cr2) 
Differentiating the logarithm of this expression and putting c = 15 we find 
-2tan0'f = 1 __*•_+_*L, + ^ 
do 1—^*2 1 — rj 
r2 - rj 
which reduces to 
tan 6- *- = %+^^+L=^). . . . (30.38) 
dc 2b {(1 + 6)2 -a2} v } 
Now tan 0 = ^ / ( ^2 — 1 ) and the period P = 2tz/6. We then find 
a- 
(30.39) 
dP\ _ P*a (1 + 6) (36a + * - aa) 
dc /c-i "~ 4rc/> { (1 + fc)a - «2}V(46 - a2)' 
This equation gives us an approximate idea of the change in the period P for small 
changes in c near c = 1. For instance, with a = — 1*5, b = 0-9 we find P = 9-7 units, 
and from (30.39), 
(**\ ^-16.5. 
Thus, if c = 0-9, i.e. the variance of r\ is about 10 per cent, of the total, the period will be 
reduced by about 1-65 years, a substantial amount. 
30.33. It is thus possible that the observed discrepancies between the autoregressive 
periods as given by the regression constants and the correlogram may be due to superposed 
random fluctuation which is not incorporated into the autoregressive scheme. This is 
not the only possible explanation ; for instance, in particular cases the disturbance function 
s may not be random. The hypotheses to be considered in such a case, however, are so 
complex that it is difficult to pursue a quantitative investigation without a wealth of 
material; and this, unfortunately, is usually denied to us, at least in economic work. 
420 TIME-SERIES 
Meteorological data are more numerous, and we may hope that further light will be thrown 
on the autoregressive scheme by a re-examination of the material available in this field. 
30.34. Consider now the more extended autoregression equation 
%hm + ai Ut+m-l + a* ut+m-2 + • • • + am ut = £*4-m* • • (30.40) 
The explicit solution cannot be given in the simple form available when m == 2. It has, 
in general, the solution 
ut = Ax *[ + A\ <4 + • . . + Am o4 + B, . . • (30.41) 
where 0C1 ... oc^« are the roots of 
am + ai am-l + a% am-2 + . . . + aw = 0, . . , . (30.42) 
3,nd B is a particular integral involving the e's. Eor the series to be oscillatory without 
increasing indefinitely no term such as x\ where x is real and greater than unity, can appear. 
Assuming this to be so, and assuming further that the series was " started up " some time 
before t = 0, we reduce the solution to the particular integral B. 
m 
Choose a particular value ijt of y ^j aj> S1IC'1 ^at 
lo = 0 * 
SI T" Q>1 SO ~ *■ 
h + a>i li + ^2 f o = 0 >. . . . (30.43) 
# » mm 
£m-l + ai ?m-2 + • • • + am-l$o = 0. , 
This is always possible in general, for it imposes m conditions on the m constants A. Then 
it will be found on substitution that a particular integral B is given by 
oo 
ut=2mJ^jet-j+^ (30.44) 
i=o 
a generalisation of (30.24). Our series may then be regarded as generated by a moving 
average of infinite extent, the weights being combinations of damped harmonic and 
exponential terms. 
30.35. The correlogram of such a series may be determined by the following method, 
due to Walker (1931). Multiply (30.40) by ut_k and sum. We find 
rk+m + ^l^+m-l + «2^+m-2 + • • • + *mTk = -^t^T*). . (30.45) 
var u 
Now ut_k depends only on et_k and terms with lower subscripts and hence is uncorrected 
with st+m for k > — m. Thus we have 
+ • • • + am rk = °> fc > — m. . . (30.46) 
If we multiply (30.40) by ut+k+m we find similarly 
'* + «i *Vn + • • • + am rk+m = ^±^!±M^l) . . (30.47) 
var u 
but the expression on the right no longer vanishes. In fact ut+k+m contains the term 
Sk+i si+m, and hence 
rk + a,i rk+1 + . . . + am rk+m = gk+1I?^Le, h > - m. . (30.48) 
var u x ' 
THE AUTOCORRELATION FUNCTION 
421 
From (30.46) it follows that the serial correlation rk will be given by 
Tfc ==: £-i \jOLa Oij )} 
. (30.49) 
where the oc's are the roots of (30.42) and the A's are constants to be determined from initial 
conditions. Thus the correlogram will be the sum of terms which either decay exponentially 
to zero (a real) or oscillate with a similar decay to zero {ol complex). Walker (1931) has 
used this result in an inquiry into a series of atmospheric pressures. 
The Autocorrelation Function 
30.36. If we have a series u (t) defined at every point of time in some range — h 
to + h, we may define its variance as 
1 Ch 
u2 (t) dt . 
. (30.50) 
on the assumption that the mean value is zero, which does not limit our generality. 
Suppose the series is reduced to standard measure by dividing throughout by the square root 
of this variance. Then an evident generalisation of the serial correlation is given by 
1 r u (t) u (t + Jc) dt (30.51) 
r (Jc) 
2h 
■h 
We shall call this the autocorrelation function. We can likewise regard it as defined when 
h tends to infinity, provided that the limit on the right in (30.51) exists. It is to be noted 
that r (Jc) is m that case an even function of Jc. 
30.37. We shall also consider the function 
-co 
E (Jc) = I u (t) u (t + ifc) dt, 
when it exists. We have 
y»CO >»00 pOO 
E (Jc) dik» dk = elkP u (t) u (t + Jc) dt dk 
J -~OQ J —QO J —00 
/»oo /»oo 
= e."'" {tHc) u (t \- Jc) (f1*1 a (t) dt die. 
J — OO J — 00 
The simple substitution t + Jc = q reduces this to 
. (30.52) 
/•CO p-JO 
e'w® u (q) dq e"^ u (t) dt. 
J —oo J —oo 
Thus, if we write 
oc (p) + ifi (p) 
00 
ewa u (q) dq. 
. (30.53) 
•00 
we have 
/•OO 
— CO 
B (Jc) eikp dJc = a2 (p) + P (p). 
. (30.54) 
It follows, as is otherwise evident from the fact that B (Jc) is an even function, that the 
imaginary part on the left of (30.54) vanishes, and we have 
i 
oo 
B (Jc) cos hp dJc = a2 (p) + /?2 (p). 
. (30.55) 
— 00 
422 TIME-SERIES 
If, following the notation of characteristic functions, we write <j>B{p) for the integral on 
the left in (30.54) and <j>u (p) for that on the right in (30.53), we have 
<Ms>) = I <M?) I2 (30-56) 
We may then put <f>u(p) = Vfae*1. (30-57) 
where /z is an arbitrary real function. We shall then have 
If00 
u 
1 f°° 
^7T J -.qo 
If - 
= y~\ ^<t>^ exP ^ ~~ ity) d$' ... (30.58) 
Since u (t) must be real, the imaginary part vanishes and this is equivalent to 
u (t) = — V^b cos (/* ~ ty) dP> • • * • (30.59) 
^ J -oo T 
and fji must be an odd function of p. The result is due to Wiener (1930). It shows that 
the autocorrelation function R does not uniquely determine u (t) because of the arbitrary 
function pt. 
30.38. Consider now the autocorrelation function r (k) as denned in (30.51). Let 
us regard the series as defined but equal to zero outside the range — h to + h. 
Then we have 
2h 
r h r°° 
r (k) = u (t) u{t + k)dt=\ u (t) u (t + k) dt = B (k), . (30.60) 
J —h J —oo 
where R and r are zero outside the range — 2h to + 2h. The foregoing results then 
continue to hold with some modifications concerning factors in 2. If we write— 
&(#)=£ r (k) eUc*>dk = -~\ R (k) eik*>dk . . (30.61) 
GO 
00 
ip . i r 
and 4>u (p) = - m (*) e^ dt = — u («) e^ <ft, (30.62) 
•/ ~~ fir %J """~ 0O 
then corresponding to (30.56) we have 
2&(#) = |&(p)|2. ...'.. (30.63) 
We may now let A tend to infinity and observe that the results continue to hold under 
certain general conditions, provided that the limits exist. 
Example 30.7 
Consider the series 
u (t) = Ax sin (2X t + ai) + A% sin (A21 + a2) + . . . + Am sin (Am Z + am). 
For the variance we have 
lim i f u* (t) dt = lim ~ f J^ {4? sin2 (^ * + a,-) } dfe, 
since the cross-product terms will contribute only a finite amount to the integral and hence 
vanish in the limit, 
dt 
PERIODOGRAM ANALYSIS 423 
= Km i- [ J 27 [A) {1 - cos 2 (^ t + a,) } ] dt 
J ~~ lb 
= iZ(A% 
Similarly for u (t) u (t + k) we have 
Km^L [S ^sin^' + ^ }] [r Hfsin(V + V< + S') }] 
= lim — f J 27 £4J [cos A, k - cos {^ (2* + &) + 2a,.} ] } * 
— \ S A) cos A; &. 
Thus r (*) = ^M°2!(i^i. 
The correlogram is the sum of a series of harmonics, like the original series, but the 
coefficients are different and the harmonics are all in phase. 
30.39. The idea underlying the autoregressive scheme of representing time-series 
may perhaps be best illustrated by an analogy. Imagine a motor-car proceeding along 
a horizontal road with an irregular surface. The car is fitted with springs which permit 
it to oscillate to some extent but are designed to damp out the oscillations as soon as the 
comfort of the passengers will permit. If the car strikes a bump or a pothole in the road 
the body will oscillate up and down for a time but will soon come to rest so far as vertical 
motion is concerned. If, however, it proceeds over a continual succession of bumps there 
will be continual oscillation of varying amplitude and distance between peaks. The 
oscillations are continually renewed by disturbances, though the distribution of the latter along 
the road may be quite random. The regularity of the motion is determined by the internal 
structure of the car ; but the existence of the motion is determined by external impulses. 
30.40. It appears to me very plausible to suppose that oscillations in time-series 
are generated in this way. One does not have to postulate some external rhythmic influence 
which keeps the oscillation going, or to suppose that the system will oscillate without 
damping once it has been set in motion. Nor is it necessary to assume that the majority 
of the deviations between theory and observation are due to " errors " which exert no 
effect on the subsequent movement of the system. The reader, however, will have to 
form his owrn opinion on this matter.* We now proceed to examine an alternative scheme 
of representation in which the series is represented as a sum of (undamped) cyclic terms. 
Periodogram A nalysis 
30.41. It is well known that under certain general conditions a function f (t) can be 
expanded in the Fourier series, valid in a certain range, 
j (t) = aQ + ax cos -—\~ a2 cos — -f- a3 cos \~ . . . 
^1 %1 Ai 
+ f>0 + &i sin —-' -h b2 sin — -f- bz sin — + . . . . . (30.64) 
Ai Xi Xi 
* The scheme considered in this chapter may over-simplify natural conditions in that it assumes 
finite random disturbances at equidistant time-intervals. If the intervals are not equal, or if the 
disturbances are small and continually occurring, the autoregressive scheme is only an approximation. 
Much remains to be done on this subject. 
424 
TIME-SERIES 
Functions which are not periodic can be expanded in this way; for instance, in the 
range 0 < x < *, 
- = sin x — - sin 2x -f- - sin 3x — - sin 4x + . 
2 2 3 4 - 
The function of course, repeats itself in the range tc < x < 2tc9 and so on. 
As a representation of observed series the Fourier series is rather restricted in scope, 
since the period of every term is a multiple of the fundamental period 2XX. A more general 
scheme is provided by the series 
/ (*) = ^o + a,i cos —- + a2 cos - h 
/Li /L2 
+ b0 + bx sin - h 62 sm —- + . . . 
(30.65) 
or the alternative form 
f(t) = AQ + Aloosf-^- + olj j + A2co&( ~ 
"T~ ^2 j "I 
. (30.66) 
Here the A's are not necessarily commensurable. The object of our analysis is first of all 
to find out what are the best values of the A's to select, and secondly to evaluate the other 
constants a and b, or A and oc. 
30.42. Suppose we wish to test whether a time-series contains a harmonic term with 
period /lc. Consider the series 
n 
-O. 
2 ri 2m 
- > Ua COS —z- 
/u 
(30.67)* 
2 ^ 
. 2tij 
/u 
and write 
B = - y u* sin 
S2 = A* + B* 
2 „ f (2mi 
-L <Ua exp —z- 
. (30.68) 
. |l)v.Ui 
Suppose that the series is in fact given by 
Ua = a sm 
5 A 
+ b4, 
(30.70) 
where fy is a component which we will assume to contain no cyclical element, so that its 
correlation with the other component is zero, at least for long series. Then we have 
n 
n 
A=^2(s^cosM) + 2-.'r(,.„njM\ 
n 
3 = 1 
p i ^ vj > / 
* Some writers define these sums with j from Oton-1. The signs of A and B may then differ 
from those given by (30.67) and (30.68), but the intensity and phase are unaffected. 
PERIODOGRAM ANALYSIS 425 
and the second term may be neglected. Thus, writing 
oc = —, /3 = 
we have 
A = — E (sin olj cos fij) 
lb 
jE {sin (oc - 0) j + sin (oc + 0)j} 
JLC 
= ?. I sin Koc-^^sinKa^) (tt+1) , sm $ (n+P) nwLJ (*+$) (n + 1) . (3Q 
w, 
sini(a—j8) sin J (a+j8) 
For large n this remains small unless oc approaches /? (or — /?, which is essentially the same 
situation), and in that case we have 
A ~ a sin | (a — /?) (n + 1). 
Similarly, B ~ a cos ~| (a - /?) (n + 1), 
ho that S2 =,4a + i?2 ==a? (30.72) 
Thus fl remains small unless the u trial " period a approaches the real period A, and in that 
ease equals the amplitude a. 
30.43. Similarly we may expect that if the series consists of a sum of harmonics, 
with periods Xu A2, . . . Am, H will be small, unless fi is equal to one of these periods, in 
which case it is finite and equal to the amplitude of the term concerned. 
This result forms the basis of what is known as periodogram analysis. We select 
a number of trial periods for different values of \i and calculate S2 for each of them. S2, 
which is called the intensity, is then exhibited as a function of /i, and graphed as ordinate 
against //. as abscissa. The diagram obtained by joining the points, each to the next, is 
called the periodogram. If this figure has peaks at certain values 1Y . . . Xm and we are 
prepared to assume that these are not sampling accidents, the values are the appropriate 
periods of harmonic terms and the intensity $3 provides the corresponding amplitudes. 
The quantities A and B of (80.67) and (30.68) are obtained incidentally and provide the 
phase angles a of (30.66). We shall illustrate the arithmetic processes below. 
30.44. Fig. 30.9 shows the periodogram of the wheat-price index data of Table 30.1. 
J n order not to confuse the diagram for lower values of the trial period we have shown 
only the major fluctuations. The length of the series was about 300 years from 1545 to 
1844, earlier and later figures shown in Table 30.1 not having been taken into account. 
The primary data have been taken from Sir William Beveridge's classical paper (1922) and 
are shown in Table 30.9. For practical reasons which will emerge presently, certain trial 
periods are taken not over exactly 300 years but over the number N of years shown in 
the table. To reduce the figures to comparability, Beveridge therefore multiplied the 
N 
sum A- + B2 by r^. 
50 
Period (years). 
Fig. 30.9.—Periodogram of the Beveridge Wheat-Price Index (Table 30.9). 
i 
GO 
fed 
GO 
PERIODOGRAM ANALYSIS 
427 
TABLE 30.9 
Periodogram Analysis of the Beveridge Wheat-Price Index Data of Table 30.1. 
(From JM.S.S., 1922, 85, 412.) 
The first observation relates to 1545, except where A and B are given in heavy type. 
Period 
(Years). 
2-000 
2-049 
2054 
2-061 
2-069 
2074 
2-080 
2-087 
2-095 
2-105 
2112 
2-133 
2-154 
2-182 
2-200 
2-222 
2-286 
2-333 
2.364 
2-375 
2-381 
*j - < y ■ * 1. 
2-400 
2-412 
2-417 
—j roil 
2-452 
2-462 
2-470 
2-483 
2-500 
o - •*; i •> 
-J « J I. tU 
<u " * ) I *) 
2-545 
2-555 
2-571 
2-588 
2-000 
2-615 
Number 
of Years 
"V 
300 
336 
304 
340 
3(30 
336 
312 
288 
320 
-oh 
320 
!Z88 
308 
320 
312 
320 
308 
308 
320 
312 
320 
304 
300 
310 
330 
309 
312 
328 
348 
33 6 
304 
320 
288 
320 
301 
336 
*i <■> <■> 
306 
308 
*> I *w 
306 
294 
296 
X 
4- 0*11 
- 0 40 
+ 0-48 
4- 0-38 
+ 0-25 
- 0 61 
+ 0-92 
- 0-52 
- 0-91 
+ 0-90 
+ 0-90 
+ 0-89 
+ 0-48 
4 
1-32 
013 
0-32 
0-50 
0-38 
1-39 
0-10 
0-90 
0 12 
0-05 
0-29 
019 
1-00 
1-30 
0-72 
0-34 
0-08 
0-63 
0-44 
1-40 
0-25 
0-38 
0-07 
0-24 
0-80 
0-45 
019 
0-38 
1-25 
0-30 
1-02 
0-75 
0-45 
0-95 
B. 
- 009 
- 0-72 
- 0-57 
+ 0-63 
-|- 0*51 
- 0-50 
- 0-11 
+ 0-90 
+ 0-07 
+ 0-80 
4- 0-15 
+ 0-23 
- 0-59 
- 0-60 
- 0-02 
- 0-22 
- 0-85 
- 105 
- 0-25 
4- 0-07 
0-03 
__ o-28 
0-43 
0-89 
0-54 
| 0-60 
f 0-08 
0-05 
I- 0-57 
I 0-0.1 
0-51 
I 1-49 
■-1- 0-35 
•1- 0-74 
I MO 
■f- 0-39 
-1- 0-24 
0-31 
0-81 
0-50 
0-55 
0-43 
0-39 
0-24 
1-3(5 
0*62 
4 
4 
I 
Intensity 
" 300 ' 
0-01 
0-19 
0-77 
0-54 
0-46 
0-71 
1-14 
0-27 
1-69 
0-86 
1-38 
0-84 
0-29 
1-99 
0-39 
0-52 
0-31 
0-93 
3-11 
0-08 
0-80 
0-43 
0-08 
0*27 
1-53 
I -80 
2-18 
0-90 
0-60 
0-47 
0-69 
0-22 
2-23 
2-44 
0-27 
0-53 
1 -56 
0-97 
0-26 
013 
2-89 
0-42 
1. -9 1 
0-28 
I -25 
0-03 
2-0 L 
1-27 
Period 
(Years). 
2-667 
2-687 
2-692 
2-706 
2-714 
2-727 
2-733 
2-735 
2-737 
2-741 
2-750 
2-762 
2-769 
2-778 
2-800 
2-818 
2-833 
2-84(5 
2-857 
2-875 
2-888 
2-895 
2-909 
2 - 9 3 3 
2-947 
2-900 
3-000 
3-040 
3-077 
3111 
31 43 
3-167 
3-200 
3-217 
3-250 
3-273 
3-286 
3-304 
3-364 
3-385 
3-400 
3-407 
3-412 
3-417 
3-429 
3*444 
Number 
of Years 
N. 
312 
301 
315 
322 
304 
300 
287 
279 
312 
296 
308 
348 
325 
330 
310 
0 9'i 
296 
320 
32^ 
312 
330 
320 
308 
330 
296 
300 
304 
320 
330 
308 
304 
320 
290 
312 
324 
322 
304 
320 
29 0 
324 
308 
323 
276 
348 
£x, • 
4 
288 
310 
-0-92 
4- 1*23 
- 0-04 
- 0-27 
"*i"~ U"oo 
4- 0-86 
+ 2-05 
4- 2-44 
4- 2-23 
4- 2-43 
4- 0-90 
- 0-57 
4- 1-49 
4- 1-20 
- 1-01 
4 0-55 
0-78 
f 0-41 
1- 0-90 
f 0-35 
■|» 1-51 
- 0-69 
f 0-70 
- 0-04 
0-93 
- 0-00 
0-29 
I 0-09 
I 0-05 
-I 0-91 
2-01 
0-4(5 
0-43 
1-25 
1-22 
0-55 
0-11 
013 
0-90 
1-76 
0-55 
0-35 
1-12 
2-98 
1 27 
3-08 
3-11 
0-09 
4 
B. 
Intensity 
N{A2 4 52) 
4 
f 
4 
4 
4- 1-20 
-0-02 
4- 0-23 
4» 1*33 
4» 1*17 
+ 1*46 
4- 1-19 
4- 1-23 
+ 1-00 
4- 0-25 
- 0-84 
- 004 
0-23 
0-92 
0 19 
107 
- 0-10 
4- 0-42 
4- 0-21 
4- 0-14 
+ 0-26 
-- 1-57 
- MI 
+ 0-39 
- 1-19 
- 1-1.5 
- 0-39 
4- 0-75 
4- 1-18 
0-44 
-h 0-23 
- 1-05 
4 0-95 
4- 0-00 
~ 047 
4- 1-18 
■4 0-99 
4> 0-75 
4 1-58 
+ 0-98 
4- 0-92 
4- 1-03 
-h 2-37 
4 2-81 
3-98 
- 2-24 
- 1-40 
- 0-99 
300 
2-38 
1-52 
0-06 
1-97 
2-10 
2-87 
6-16 
7-82 
6-22 
5-86 
1-55 
0-37 
2-28 
2-48 
1-18 
1-49 
0-67 
0-34 
1-03 
0-15 
2-43 
3-21 
1-84 
0-1(5 
2-57 
1-30 
0-23 
0-58 
1-50 
1-15 
4-20 
1-33 
1-1(5 
1-80 
1*82 
1*07 
0-59 
3-54 
4-00 
1-24 
7-41 
14-90 
15-53 
15-84 
11-16 
1-03 
428 
TIME-SERIES 
TABLE 30.9—continued. 
Period 
(Years), 
3-455 
3-462 
3-500 
3-524 
3-538 
3-556 
3-571 
3*600 
3-619 
3-636 
3-643 
3-667 
3-679 
3-692 
3-700 
3-714 
3-727 
3-750 
3-778 
3-800 
3-833 
3-857 
3-888 
3-895 
3-923 
3-962 
4-000 
4-077 
4-111 
4-143 
4-167 
t*jl / o 
4-200 
4-250 
4*286 
4-333 
4-353 
4-364 
4-375 
4-385 
4-400 
4-412 
4-417 
4-429 
4-444 
4-471 
4-500 
4-571 
4-600 
4-667 
4-750 
4-800 
4-857 
4-888 
Number 
>f Years 
N. 
304 
315 
308 
296 
322 
320 
325 
324 
304 
320 
306 
308 
309 
288 
296 
312 
287 
315 
306 
304 
322 
324 
280 
296 
306 
309 
300 
318 
296 
290 
325 
322 
294 
323 
300 
312 
296 
288 
315 
342 
308 
300 
318 
310 
320 
304 
306 
320 
322 
336 
304 
288 
306 
312 
XX . 
+ 0-55 
+ 1-57 
H- 1-20 
H- 1-41 
+ 0*50 
+ 0-02 
"J™ [J 'Q\J 
- 1-03 
4- 1*18 
+ 1-14 
- 0-16 
- 2-14 
+ 0-34 
+ 1-28 
+ 0-90 
+ 1-15 
- 0-45 
+ 0*64 
- 117 
+ 1-60 
- 1-12 
4 1-63 
- 0-15 
- 0-66 
-f 0-64 
- 0-67 
+ 1-47 
4- 0-57 
4- 113 
- 0-50 
4- 1-21 
4- 0-66 
- 0-99 
+ 0-50 
- 0-65 
- 1-50 
- 2-85 
- 2-98 
- 2-47 
- 0-50 
- 1-38 
+ 0-08 
+ 0-87 
+ 1-80 
+ 2-15 
+ 0-91 
+ 1-87 
- 0-21 
- 0-08 
+ 019 
-0*12 
+ 2-44 
- 1-06 
- 1-80 
B. 
i 
4-0-29 
+ 102 
-0-94 
- 118 
- 1-45 
-0-43 
-0-69 
+ 0-82 
+ 1-23 
4~ 0-13 
4- 0-27 
- 1-07 
- 1-90 
- 0-22 
- 0-59 
+ 1-78 
- 1-65 
- 0-06 
- 0-68 
+ 0-80 
- 1-63 
+ 0-45 
4- 0-66 
+ 1-00 
— 1*61 
+ 1-74 
i'lo 
- 0-26 
- 1-70 
4-0-23 
4-0-32 
- 1-46 
- 0-41 
-2-73 
4- 0-79 
- 1-30 
- 0-24 
4 0-75 
4-0-87 
4- 2-55 
-1-3-27 
+ 3-62 
-1-3-85 
-1-2-41 
-1-0-83 
-1-0-79 
+ 0-72 
+ 0-04 
-f" 1*24 
+ 0-93 
4~ Ji'Jio 
+ 1-08 
- 1-30 
+ 2-11 
Intensity 
N(A* +B2) 
300 
0-39 
4-87 
2-38 
3-31 
2-53 
0-20 
1-21 
1*88 
2-94 
1-39 
0-10 
5-87 
3-83 
1-63 
1*18 
4-65 
2-72 
0-44 
1-86 
3-24 
417 
3 08 
0-43 
1*42 
3-06 
3-59 
3-64 
0-41 
413 
0-30 
1-70 
2-77 
102 
8-32 
1-04 
4-10 
8-05 
9-07 
719 
7-72 
12-89 
1311 
16-48 
9-32 
5-66 
1-48 
4-09 
0-22 
1-65 
1-00 
5-28 
6-84 
2-89 
8-00 
Period 
(Years). 
4-933 
5-000 
5-067 
5-091 
5-100 
5-111 
5125 
5143 
5-200 
5-250 
5-333 
5-400 
5-415 
5-429 
5-455 
5-500 
5-555 
5-600 
5-667 
5-692 
5-714 
5-750 
5-800 
5-846 
5-933 
6-000 
6-111 
6143 
6-167 
6-200 
6-250 
6-286 
6-333 
6-400 
6-500 
6-571 
6-667 
6-727 
6-750 
6-800 
6-909 
6-933 
7-000 
7-143 
7-200 
7-333 
7-400 
7-417 
7-429 
7-500 
7-600 
7-667 
7-750- 
7-857 
Number 
of Years 
N. 
296 j 
300 i 
304 
336 
306 
322 
328 
324 
312 
294 
320 
O j-J4r 
325 
304 
300 
308 
300 
336 
306 
296 
320 
322 
290 
304 
356 
300 
330 
301 
296 
310 
325 
308 
304 
320 
312 
322 
320 
296 
324 
306 
304 
312 
308 
300 
324 
308 
296 
, 356 
! 312 
! 315 
! 304 
322 
310 
330 
A. 
4 1-57 
4 1-85 
- 0-05 
- 0-73 
4 5-71 
4 5-70 
4 3-97 
4- 2-46 
4- 0-02 
4 1-74 
+ 0-71 
4- 1-04 
4 4-27 
4- 4-72 
+ 1-37 
- 104 
4- 2-40 
4- 0-46 
4 «3*ol 
4- 2-05 
4- 0-35 
4 1*39 
4 3-55 
4- 0-00 
4 4-37 
- 3-50 
- 0-79 
4 0-74 
- 0-22 
- 2-02 
- 3-23 
- I -72 
- 1-52 
4 0-80 
4 0-09 
4 1*49 
4 0-25 
4 0-08 
- 0-20 
4 0-23 
4 0-58 
4 1-68 
4 3-10 
4 1-S3 
4 0-54 
4 1 -52 
- 2-33 
! 4 1-50 
' - 3-80 
; 4 0-17 
j - 2-33 
i - 1-46 
! 4 1-38 
| - 0-50 
4 1-58 
4 1- 
4 o' 
4 5 
~\~ JL' 
+ 0- 
+ 2 
4- 2- 
4- 0- 
4- 1- 
"**** "3T * 
4- 3- 
-f 1- 
- 0- 
- 3- 
4 1- 
_ ()• 
-f 1 
- 1- 
- - 3- 
~ 9. 
—i 
<>■ 
- 2- 
- 2- 
■1- 0 
()• 
~ 1 
2- 
- 
- 3- 
()■ 
0- 
4- I- 
1 " 
()• 
- ()• 
4. ()• 
... ()• 
1- 
0- 
4 2- 
1 2- 
I- 
3- 
4- 
-4 
4 
1- 
4- I- 
- 1- 
_ 2- 
_ (>. 
H- 0 
00 
98 
•55 
98 
29 
90 
46 
30 
92 
46 
71 
90 
io 
73 
49 
68 
21 
97 
91 
13 
33 
75 
29 
91 
12 
■90 
90 
94 
38 
I I 
59 
*ml *.* 
74 
73 
r-t *■? 
/ i 
•> J 
13 
(Mi 
65 
56 
01 
1 *-» 
1 i 
86 
93 
81 
72 
01 
49 
r>o 
37 
61 
39 
•28 
Intensity 
N (A2 + B*) 
"" 300" -■ 
_ 
4-91 
4-30 
16-09 
3505 
*TT & * O *X 
34-91 
26-38 
13*09 
010 
6-56 
21-72 
16-06 
23-66 
22-01 
15-76 
3-39 
0-23 
1-88 
32-72 
1918 
4-97 
2-18 
19-47 
5-35 
23 03 
12-29 
4-(>(> 
9-32 
8-56 
HS-02 
1 1*30 
3-4! 
4-02 
8-71 
0-94 
3-02 
0*11 
0-02 
3-01 
0-48 
7-00 
7-15 
14-74 
0*79 
16-96 
10-46 
12-65 
21-72 
17-28 
2-40 
7*43 
9-57 
2-13 
0-36 
PERIODOGRAM ANALYSIS 
429 
TABLE 30.9—continued. 
Period 
(Years). 
8-000 
8-091 
8-200 
8-222 
8-333 
8-500 
8-667 
8-800 
9-000 
9-200 
9-333 
9-500 
9-667 
9-750 
9-818 
10-000 
10-200 
10-250 
10-400 
10-500 
10-750 
10-800 
11 -000 
11-200 
11-500 
11-667 
12-000 
12143 
12-333 
12-500 
12-667 
12-800 
12-875 
13-000 
J, O * O Q it 
13-500 
13-667 
14-000 
14-500 
14-667 
1 5-000 
15-200 
15-250 
15-280 
15-500 
16-000 
16-667 
17-000 
17-333 
Number 
of Years 
N. 
312 
356 
287 
296 
325 
323 
308 
306 
322 
336 
304 
290 
312 
324 
320 
306 
328 
312 
294 
301 
324 
308 
336 
322 
280 
312 
340 
296 
325 
ufx 
309 
320 
324 
328 
308 
290 
308 
300 
304 
305 
321 
310 
320 
300 
306 
312 
B. 
+ 
+ 
-f 
+ 
- 3-96 
+ 4-32 
+ 1-62 
+ 0-19 
+ 0-21 
+ 0-17 
+ 2-51 
+ 2-97 
- 1-51 
- 0*16 
- 0-74 
1-08 
5-03 
4-46 
1-21 
- 1-19 
-f 0-86 
- 0 69 
-p 1 -88 
-f- 2*46 
-p X '4: / 
1-00 
3-85 
2-48 
1-32 
0-46 
- 2-47 
0 22 
- 2*44 
- 1-22 
h 2-28 
I- 5-70 
f 6-46 
f 4-26 
f 0-40 
f 2-56 
f 3-49 
-h 
115 
3-78 
1-50 
6-32 
119 
0-28 
2-35 
3-89 
0-92 
1-46 
5-21 
2-56 
3 04 
_[_ 
+ 1-34 
- 0-98 
- 0-64 
- 0-56 
+ 0-91 
-f 319 
- 1-01 
+ 0-83 
- 0-57 
- 1-56 
+ 0*64 
-f 1-07 
"J" \)'O I 
- 3-56 
- 4-94 
- 0-83 
- 0-22 
1 10 
1-65 
1-82 
313 
4-75 
4-26 
0-55 
0-66 
1-42 
4-04 
4-37 
2-74 
2-63 
5-19 
! 3-26 
h 0-77 
- 4-32 
+ 0-37 
■- 2-09 
1 34 
- 1-00 
- 0-18 
-f 4-23 
_ 2-66 
- 8-52 
- 8-65 
- 7-15 
- 6-55 
2-02 
-f 4-52 
- 0-39 
- 6-35 
- V)-m 
+ 
+ 
Intensity 
N (A* + B*) 
300 ™ 
18-67 
23-23 
2-90 
0-34 
0-95 
10-41 
7-59 
9-77 
2-65 
2-65 
2-26 
24-55 
33-89 
27-90 
2-25 
0-80 
1-84 
6-52 
9-19 
11-98 
25-48 
33-84 
7-24 
2-34 
2-07 
23-30 
21-66 
11 -43 
913 
32-58 
40-01 
43-58 
38-23 
0-32 
11-79 
15-28 
2-38 
13-82 
20-69 
46-83 
75-04 
76-17 
60-62 
02-29 
59 • 1 1 
24-02 
27-33 
47-84 
54-55 
Period 
(Years). 
Number 
of Years 
N. 
17-500 
18-000 
18-500 
19-000 
19-750 
20-000 
21-000 
22-000 
23-000 
24-000 
24-667 
25-000 
26-000 
27-000 
28-000 
29-000 
30-000 
31-000 
32-000 
33-000 
34-000 
35-000 
36-000 
37-000 
38-000 
40-000 
41-000 
42-000 
44-000 
45-000 
46-000 
48-000 
50-000 
52-000 
53-000 
54-000 
55-000 
50-000 
58-000 
60-000 
62-000 
64-000 
66-000 
68-000 
70-000 
74-000 
76-000 
78-000 
80-000 
84-000 
280 
306 
296 
304 
316 
320 
294 
308 
322 
288 
296 
325 
312 
324 
308 
290 
300 
310 
320 
330 
306 
280 
288 
296 
304 
320 
328 
294 
308 
315 
322 
288 
300 
312 
318 
324 
330 
336 
290 
300 
310 
320 
330 
340 
280 
296 
304 
312 
320 
336 
430 
TIME-SERIES 
An examination of the periodogram suggests the possibility of 20 periods, as follows :— 
"PfiTpinH 
(Years). 
2*735 
3-417 
4-417 
5100 
5-415 
5-667 
5-933 
7-417 
8-091 
9-750 
Corrected Intensity 
N (A* -f £2)- 
300 
7-82 
15-84 
16-48 
42-34 
23-66 
32-72 
23-63 
21-72 
23-23 
33-89 
"Paim of? 
(Years). 
11-000 
12-000 
12-800 
15-250 
17-333 
20-000 
24-000 
35-000 
54-000 
68-000 
Corrected In 
N (A2 + 
300 
33-84 
23-30 
46-01 
76-17 
54-55 
37-88 
26-10 
23-29 
26-09 
13-58 
This is evidently rather an embarrassing profusion of possibilities, and we cannot 
immediately accept all these periods as significant. Sir William discussed them in detail 
in the original paper and was inclined to attribute reality to 18 or 19 of them, partly on 
grounds which do not concern us here, such as the existence of weather oscillations with 
these " periods ". In particular, where a period had a high intensity he analysed the 
two halves of the series separately to see whether the periods persisted, finding that most 
of them did. 
30.45. An inspection of the correlogram of the series in Fig. 30.5 reveals a striking 
difference between the two methods of analysis. From the correlogram we should be 
inclined to suspect a mean period of about 15 years, corresponding to the peak of greatest 
intensity in the periodogram, with a subsidiary ripple of about 5 to 6 years' period, 
corresponding to one or more of the peaks in the periodogram ; but of the other IS periods there 
is no sign. The conclusion is inevitable that either the correlogram is insensitive or the 
periodogram is misleading. Having raised this highly important question we shall, 
unfortunately, have to leave it unsettled in part ; but we shall show that at least three-quarters 
of the periods thrown up for consideration by the periodogram are not significant. 
30.46. The calculation of the intensity S2 depends on that of the quantities A and B 
of equations (30.67) and (30.68). Suppose in the first place that our trial period //, is an 
integer. We then write down the series in rows of fi9 thus :— 
u 
fi+i 
u 
u. 
H-: 
u 
'/H-3 
u 
(p-1) aH-1 u{p-l) ft + 2 u(p~l) H-3 
u 
ft 
p/l 
Totals mx 
m< 
rn, 
(30.73) 
mttJ 
We continue writing down the rows until there are fewer than //, terms remaining, the 
extra terms being left out of account. The number Pfi is then as near in multiples of p 
as we can get to the number in the series n, and may be denoted by N. This array is 
sometimes known as the Buys-Ballot table. 
® 
PERIODOGRAM ANALYSIS 431 
We then form the sum- 
—{ mx cos — + m2 cos — + . . . + m cos -J— \ . . (30.74) 
and this is clearly the quantity A of (30.67) for the series of N terms. Similarly we have 
B = — V( raisin ^- J. . . . . (30.75) 
If the trial period a is a rational fraction - we write the series down in rows of v and 
a 
proceed in the same way ; and if it is irrational or is a number which gives a large value 
of v when expressed as a fraction, we take two convenient neighbouring values of p and 
interpolate in the periodogram. 
30.47. In actual practice we do not write down the array (30.73). The sums m 
may be formed on an adding machine by starting with ux and then adding every fith. 
member to give m1 ; then starting with u% and adding every /^th member to give m2, and so on. 
Or alternatively, the values may be written on cards, one for each member of the series, 
and the pack dealt into /u heaps. The total of the m's, together with any members left 
over, equals the sum of the series and provides a check on the work. 
Example 30.8 
Consider the Beveridge series of Table 30.1. For the trial period 2 we may take 300 
terms of the series, and m\ (about ze.ro mean) will be the sum of the values ul9 uz . . . um 
and m% will be the sura of the values with even subscripts. Tliese sums are for the years 
1545 to 1844 inclusive, 
m\ — 14,909 
ml> = 14,893. 
The mean is 14,901, so that about the mean of the series 
ml = + 8 
ma == — 8. 
Now, for a trial period 2, sin — vanishes and hence B ~ 0. For A we have (in our 
notation, which gives different signs from Beveridge's to A and B)— 
300 
A = —"-■< w&i cos -~~ + m9 cos — V 
2 
300 l J 
300 
011. 
Thus S2 (corrected) = —- A2 = 0-01, 
v ' 300 
as shown in Table 30.9. 
13 
For a trial period 2*600, we could take //, = '-— and arrange the series in rows of 13, 
o 
requiring 23 rows accounting for 299 values of the series. We may, however, save 
ourselves some arithmetic by taking 24 rows, a multiple of 4, occupying 312 observations. 
432 TIME-SERIES 
Or rather, we take 6 rows of 52, giving us the values for a trial period 52 ; then add mx 
to m27, m2 to ma8 and so on, giving the result we would have got by taking 12 rows of 26 
and hence providing the values for a trial period of 26 ; then we add again in the same way, 
and so on, obtaining successively the values of m required for trial periods of 13, 6-5, and 
3-25. Similarly, by multiplying the original 52 values of m by the respective values of 
VOrrn 9 Otto PC O 
<?os -—■ and sin ~—~ we get the values of A and B required for a trial period of —. It is 
52 52 ° 10 
thus evident that we can use the single set of 52 values of m to provide the required constants 
52 52 52 
for trial periods —, —, —, and so forth. This is the main reason why, in Table 30.9, 312 
12 3 
observations are shown as N for the trial periods 2-080, 2-261, 2-364, 2-476, 2-600, 2-737, 
2-888, 3-250, 3-714, 4-333, 5-200, 6-500, 7-429, 8-667, 10-400, 13-000, 17-333, 26-000 and 
52-000. The arithmetic, though difficult enough, is not as laborious as appears at first sight. 
30.48. There is an interesting relation between the periodogram and the correlogram 
by which the latter, in theory, determines the former. We consider, as in 30.38, a function 
•u (t) defined at every point of time in some range — h to h. Then 
ot (p) + %} (p) = l[ eW u (t) dt 
hJ-h 
1 Ch i [h 
= T oos pt u (t) dt + 7 sin pt u (t) dt . . (30.76) 
* J -* h J ~h 
•corresponds to the sums of (30.67) and (30.68) and may be written A + iB, where 
p = — (30.77) 
It follows that the intensity S2 is related to the Fourier transform of r (k) by the relation, 
derived from (30.63), 
S* = 2$r (p) 
2 r2h 
= h 
r(k)eikPdk, . . . (30.78) 
~2h 
which is true also in the limit, subject to conditions of existence. Thus the intensity is, 
if r (k) exists over an infinite range, the quantity— 
2 f2^ 
lim - r (k) cos kp dk, 
h J _2& 
and if B (k) exists the parallel quantity— 
J —00 
R (k) cos kp dk. 
The periodogram is thus derivable from the autocorrelation function. Since the latter 
does not uniquely determine the series the periodogram will not do so either. 
Example 30.9 
Consider the autocorrelation function, which in present notation may be written 
R (k) = pk Sin (A6> + yl 
sin y> 
SIGNIFICANCE OF A PERIODOGRAM 433 
This, as we have seen, represents the correlogram of an autoregressive series of the simple 
linear kind involving ^+2, ut+x and ut. We may write this as 
jB(i) = rfl*ain(fefl + v), q>o 
sin \p 
since p is less than unity. It is to be remembered that since R (— k) = R (k), the modulus 
of lc is to be used when k is negative. 
We have 
p e- 1^1 gin gcd + j 
o2 = I — —— -- — —- -- cos kp dk 
J _«, sin v-> 
== I c" ' qk' cos &0 cos kp dk 
J —00 
2 , q 
?2 + (6> + 2>)2 ?a + (0-p)a 
This is the intensity in the periodogram of the series, p being the quantity — and not to 
/a/ 
be confused with our original damping factor p. 
2(7 
It is remarkable that, as /./, becomes large, S2 tends to the constant value ——-^, 
that is to say, the periodogram tends to a fixed level, without peaks. From the analogy 
with the analysis of light-rays into colours (each colour corresponding to a particular 
harmonic), we may say that the periodogram develops a " continuous spectrum ". In a 
very interesting chapter on periodogram analysis Davis (1941) has given a number of 
examples exhibiting this kind of effect. 
Significance' of a Periodogram 
30.49. Suppose that the values ?/., . . . nn are random elements from a normal 
population with variance <r'2. Then the function 
/ i 
is normally distributed with variance 
4 2 vm zm 
A = > uj cos 
■a 
j i 
2cra 
n 
and similarly 
9(7 2 
4-0-- vnr „ ZTTj 
var -4. = - > cos- 
(30.79) 
vari? = -"-. (30.80) 
n 
We also see that cov (A, B) = 0 so that A and Zi are independent. Hence the joint 
distribution of A and B is 
dF = — exp | - -~- (.42 + £2) 1 d4 AB. . . . (30.81) 
A.S.—VOL. II. P ff 
434 TIME-SERIES 
Thus the distribution of S2 —A2 + 2J2 is 
dF = —exp(-~sAdS> (30.82) 
The probability that S2 exceeds in value is immediately obtainable as e K. 
X »/ .Art 
30.50. This result is due to Schuster (1898), but it gives only the probability that 
a value of S2 chosen at random will exceed a given value ; whereas in the periodogram 
we deliberately pick out the biggest values for inspection. Walker (1914) pointed out that 
if e~K is small the probability that all of m independent values of S2 should not exceed 
is (1 — e"K)TO, so the probability that at least one should exceed that amount is 
1 - (1 - e~T- • • • • • (30-83) 
Davis (1941) gives tables of this function. 
30.51. Both the Schuster and the Walker tests depend on a knowledge of <r2. Since 
4(T2 
the mean value of S2 in (30.82) is , the usual procedure is to consider the test as a com- 
n 
parison of S2 with E (S2) ; but cr2 itself has to be estimated from the original data. 
30.52. Fisher (1929a) has given a test which avoids the inexactitude due to the 
estimation of or2. If v is the estimate and £2 is the largest intensity, then the probability that 
#2 
g = 7T- (30.84) 
will exceed a given value is 
v (1 - gy~i - (V\ (1 - 2g)»-i +...+(- l)~-i f^\ (1 - mgy-\ (30.85) 
where v — \ [n — 1), n being the (odd) number of observations, and m is the greatest- 
integer less than l/g. The result was extended by Stevens (1939a)—see also Fisher (1940r/) 
and Finney (1941a). Davis (1941) also gives tables of this function. 
30.53. All the tests we have described are based on random normal variation in the 
original series ; but in practice nobody would embark on the labour of a periodogram 
analysis unless he had satisfied himself that the data were not random. It seems to me, 
therefore, that these tests are really off the main point, being tests based on a hypothesis 
which we have already rejected. They are not without their usefulness, however. We 
may assume with some confidence that if a particular intensity in the series is not shown 
as significant on the hypothesis of random variation, it is not significant when the series 
is systematic. What does not follow is that if one intensity is significant then others must 
be so, even if they exceed the significance values; for they are not independent of the 
significant value, at least for short series. What we ought to do, perhaps, is to extract 
the component which is considered significant from the series and then analyse the 
remainder; and so on as long as significant terms appear. But this is hardly a practical 
computational possibility. Tests of significance in the periodogram, as in the correlogram, 
remain undiscovered. 
LAG CORRELATION 
435 
Example 30.10 
Let ns examine the significance of the 20 periods of the Beveridge periodogram given 
in 30.44. 
4(7'^ 
Sir William gave the value of — in his original paper as 5-898. Expressing the 
lb 
intensities as a multiple k of this amount, we find :— 
Period. 
2-735 
4-417 
5-100 
5-415 
5-667 
5-933 
7-417 
8-091 
9-750 
K. 
1-33 
2-69 
2-79 
7-18 
4-01 
5-55 
4-01 
3-68 
3-94 
5-75 
Period. 
k. 
11-000 
12-000 
12-800 
15-250 
17-333 
20-000 
24-000 
35-000 
54-000 
68-000 
5-74 
3-95 
7-80 
12-91 
9-25 
6-42 
4-43 
3-95 
4-42 
2-30 
There are 305 trial periods in Table 30.9. Let us consider the probability that at least 
one of 305 independent values of k will exceed given values, that is to say, the probabilities 
given by (30.83). We find— 
k Probability. 
2 1-000 
4 0-990 
0 ...... 0-531 
8 0-097 
10 0-014 
On this basis we should be inclined to attribute significance to the period 15-25, for which 
k 12-91. We have no right to be surprised that at least one value exceeds /< = 6. If 
we take this value as the critical one, only the periods 5-100, 12-800, 15-250, 17-333 and 
20-000 would be significant, that is to say, live out of 20. 
Again, since c n ~ 0-007, we should expect to find in 305 independent members two 
in excess of 5. Actually there are eight. But they are not independent and we cannot 
rely on this comparison to say that six are significant. On the whole, however, it looks 
as if at least three-quarters of the periods aro not significant, and possibly more. The 
example will illustrate the difficulty of testing the significance of the periodogram as a whole. 
Jj(Uj Correlation 
30.54. The idea of serial correlation can be extended to the joint variation of two 
series. If we have two series u (t), v (t) in standard measure, we may define the lag 
correlation of order k as 
r (k) - j u (t) v (t | k)dt, 
. (30.86) 
where the integral includes summation in the case when the series are specified at 
equidistant points of time. We note that in this case r (k) is not equal to r (— k) and r (0) 
is not unity. 
436 
TIME-SERIES 
Table 30.10 shows the lag correlations between two series of English wheat prices and 
horse populations (for the original series see Kendall, 1944a). The data are shown as a lag 
correlogram in Fig. 30.10. 
TABLE 30.10 
Lag Correlations for Two Series of English Wheat Prices and Horse Populations (Deviations 
from a Simple Nine-Year Average). 
(The order of the correlation is the number of years by which horse population lags behind wheat price, 
e.g. r10 is the correlation of wheat price with the horse population of ten years earlier.) 
Order of 
Correlation 
k. 
- 10 
- 9 
- 8 
- 7 
- 6 
- 5 
- 4 
- 3 
- 2 
- 1 
0 
rjc- 
- 0-22 
- 0-19 
- 0-24 
-0-16 
- 0-09 
0-07 
0-27 
0-31 
0-41 
0-25 
- 0-12 
Order of 
Correlation 
k. 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
• 
He- 
- 0-24 
- 0-36 
- 0-12 
0-16 
0-17 
0-39 
0-36 
0-15 
- 0-16 
- 0-44 
Fig. 30.10.—Lag Correlation of Wheat Prices and Horse Populations (Table 30.10), 
NOTES AND REFERENCES 437 
The systematic appearance is unmistakable and we notice in particular that the maximum 
correlation occurs between the wheat price and the horse population of two years later. 
This bears the obvious explanation that when a farmer earns more he buys or breeds more 
horses ; but it does not follow logically that this must be so or that there need be any 
causal nexus between the two series. If two autoregressive series are oscillating with 
mean periods which are close together and only a short span of experience is available for 
scrutiny, then lag correlations of the damped sinusoidal type may appear, as it were, by 
accident. 
30.55. We have now reached the end of our account of the statistical analysis of 
time-series and the end of this book ; and the final words we have to say of the one will 
apply generally to the other. Much has been left unsaid, partly from lack of space, partly 
from deficiencies in the present state of knowledge, and partly from a desire not to 
overburden the reader. We have not avoided mathematical analysis where it was necessary 
to advance the argument; but we have insisted on the expression of results in numerical 
form and the necessity of experimental confirmation whenever it could be obtained. That 
there are gaps in the treatment we have given and unexplored branches of the subject 
to which we have barely referred are not entirely matters of regret; for the over-early 
and peremptory reduction of knowledge into arts and methods is one of the errors which 
Bacon cautioned us against more than 300 years ago. Much remains to be clone ; and this 
book will have served its purpose if the reader is left with the desire to do some of it himself. 
NOTES AND REFERENCES 
The theoretical aspects of the autoregressive series and of moving averages are 
discussed in Wold's book on The Analysis of Stationary Time-Series (1938a). The basic 
memoir is that by Yule (1927r/,) on sunspots. For applications to meteorology see Walker 
(1931) and to economies Kendall (1944a). Davis's book on The Analysis of Economic Time 
Series (1941) contains a great deal of interesting material but should not be read uncritically. 
Two earlier papers by Yule (1921 and 1920) a,re also ol' interest. See also my paper on 
<u The Analysis of Oscillatory Time-Scries ^ in the Journal of the Royal Statistical Society 
for 1945, a paper by Yule in the same journal, my brochure (in press) on u Researches in 
Oscillatory Time-Series ", and a symposium introduced by Bartlett in the Supplement to 
the Journal for 1940. 
The classical work on periodogram analysis is that of Schuster (1898). The books 
by Brunt (1931) on The Oermbimution of Observations and by Whittaker and Robinson 
(1940) on The Calculus of Observations contain useful introductory accounts ; and Davis's 
hook referred to above lias an excellent chapter illustrated with an unusual number of 
examples. Papers by Crum (1923) and Oreenstein (1935) are of interest. The papers by 
Sir William Beveridge (1921, 1922) on wheat prices and rainfall have been justly described 
by Davis as a heroic piece of periodogram analysis. "Fables facilitating the calculation, 
of intensities were published by Turner (1913), and more complete tables will be given in 
my brochure referred to above. See also the book by Stumplf (1937). 
Various short-cut methods of periodogram analysis have been proposed by several 
authors, e.g. Oppenheim (1909), Brims (1921) and Alter (1933, 1937); but their value is 
problematical. There is a useful memoir by Bartels (1935) which is worth studying. 
438 TIME-SERIES 
EXERCISES 
30.1. For the autoregressive series 
ui+2 + aut+1 + but = at+2 
show that if £ is a random variable and the series is long, 
var u ___ 1+6 
vare ~ (1 - b) {(1 + 6)2 -a2}5 
and hence that the variance of the generated series may be much greater than that of 
e itself. 
30.2. For the autoregressive series of the previous exercise use the relation 
fy+2 + flWfc+i +brk = 0, k > — 1 
to derive the relation 
pk sin (kO + ip) 
rjc = 
sin y> 
30.3. If the estimated coefficients a' and b' in the autoregressive scheme are reduced 
in the manner of 30.32 by a superposed error, show that 
b' b 
a a 
(Yule, 1927a.) 
30.4. Show that if, in the autoregressive scheme of Exercise 30.1, 4 = 1, the series 
becomes undamped and the correlogram reduces to a simple harmonic. Examine the 
effect on the solution (30.23). 
any series has fitted to it a series generated by the scheme of Exercise 30.1, 
a and b being any constants, show that for the serial correlations of the residuals, say ak, 
we have 
a = (1 + ^2 + 62) pk + a (1 + b) (Pk+l + Pk^) + b (Pk+2 + Pk_,), 
k 1 + a2 + b2 + 2a (1 + b) p1 + 26pa 
30.6. Show that the series with an autocorrelation function 
, 7 > sin Xk 
r (fc) = — 
V ' Xk 
_ or . 77" 7T 
has a periodogram which is zero for periods less than - and has ordinate T for periods greater 
/ A 
than -, i.e. has a continuous spectrum. 
30.7. In equation (30.71), noting that the dominant term vanishes for a — B = "—, 
n 
where m is an integer, show that for such a " vanishing " trial period /* 
/ 771 \ 
/u — X f1 +—ju), approximately. 
EXERCISES 439 
2/L2 
Hence the width of a peak in the periodogram is approximately —, and the main peak 
lb 
will be flanked by smaller peaks of the same width. (This lc side-band " effect is another 
complication in the interpretation of the periodogram, but not apparently a very serious 
one.) 
30.8. If a series of values ux . . . nn is supplemented by a number of zeros as 
"^Oj ^—1' ^—2 * * * w+ls ^?i4-2> 
etc., as far as is necessary, and the resulting series differenced, 
show that 
^"p'(j)~2PiG-i)+ap,G-2)" • ■ ■ +2(-i),jP* 
where Xj is the sum of squares of jth differences and Pt} = y xk xk+j. Hence show that 
the arithmetic of serial correlation may be related to that of the variate-difference method, 
and vice-versa. 
30.9. Show that the serial correlations of a long series obtained by differencing a 
random series m times are given by 
r (]A = /_ i)km (m - l) - ' * (m - k + l) 
yv) y } (m + 1) . . . (m + k) 
and hence that the correlograni of such a series oscillates. 
30.10. The Whittaker periodogram. Writing 
„ , v var m 
(At 
where var u is the variance of the Heri.es and var m is the variance of the sums m of (30.73), 
show that if 
*■> 
-it.j a sin |- bj, 
A 
where bj is uncorrelated with periodic terms, then 
ar /i" sm- -- 
A n 
-i. Jz var b 
2N2 mil2—'- 
■ha" 4- var 6 
Hence show that, in the neighbourhood of A, the graph of tj as ordinate with ft as abscissa 
(Whittaker's periodogram) has a peak of breadth ----_- flanked by smaller peaks. 
(Whittaker, Month. Notes E. Astr. Soc, 1911, 71 ; of. Whittaker and Robinson, Calculus of 
Observations.) 
APPENDIX A 
ADDENDA TO VOLUME I 
(1) Frequency and Distribution Functions 
An interesting paper by Burr (1942) considers the possibility of fitting elementary 
mathematical functions, not to the frequency function as has been the almost universal 
practice hitherto, but direct to the distribution function. This approach seems to merit 
further attention. In general, the distribution function has fewer analytical peculiarities 
than the frequency function—for instance, it cannot be infinite—and in applications to 
sampling it is the former which is nearly always required. The frequency function can, 
of course, be derived from the distribution function to a close approximation by 
differencing, or differentiation, processes which are usually easier to carry out than the inverse 
processes of integration. 
(2) Extension of the Carleman Criterion (4.22) 
Cramer and Wold (1936) have extended Carleman's criterion for uniqueness in the 
problem of moments in the following form :— 
If 
*=^o...+^d... + /*<»... +■ ■ • • 
the distribution is completely determined by its moments if 
diverges. It is rather interesting that the criterion is independent of the product-moments. 
(3) Convergence of Series Leading to Standard Errors 
The usual type of expansion in differentials, exemplified in 9.6, raises a point of 
mathematical difficulty in that the differentials themselves and the remainder terms, though 
usually small, may sometimes be large for sampling reasons, however large the sample. 
The necessary rigorisation of the process has been given by Derkson (1939) in terms of the 
notion of stochastic convergence, that is to say, a sort of statistical convergence in which 
the series converges nearly always in a precisely defined sense. 
(4) Moments of Moments for Finite Populations 
The formulae for moments of the mean and variance in samples from a finite population 
were stated without proof in 11.26. It is obvious that if in these results we let N, the 
population number, tend to infinity, we obtain the formulae for sampling from an infinite 
population. Irwin and I (1944) have recently shown that the process may be reversed 
and the formulae for the finite case derived from those for the infinite case. This offers 
the simplest and most direct method of deriving the formulae known to me. Reference 
may also be made to Sukhatme, " On Bipartitional Functions " (Phil. Trans., 1938, A, 
237, 375) and " Moments and Product-Moments of Moment-statistics for Samples of the 
Finite and Infinite Populations " (Sankhyd, 1944, 6, 363). 
440 
APPENDIX A 441 
(5) Tied Banks 
In the treatment of rank correlation in Chapter 16 it was assumed that ranking was 
always possible ; but in practice cases occur when two or more individuals " tie " and the 
ranks have to be equalised in some way. This possibility introduces the most intractable 
complications into theoretical work, but sometimes ties occur so frequently that a 
systematic method of dealing with them is necessary. The subject has been reviewed and 
reconsidered by Woodbury (1940) and more recently by myself (Biom., 1945, 33, part 3). 
(6) Coefficients of Rank Correlation 
Daniels (1944) has recently unified the theory of rank correlation by showing that 
Spearman's p, my r and the product-moment coefficient are particular cases of a general 
coefficient. In particular he has demonstrated the formula for the covariance of p and t 
given in 16.24 as very probably true. 
APPENDIX B 
BIBLIOGRAPHY 
The following Bibliography has no pretensions to completeness in spite of its length. 
It contains about half the titles recorded in my own notes, which themselves are doubtless 
far from comprehensive. Nevertheless, I hope it will be useful to those readers who want 
to take their studies of particular subjects somewhat further. By consulting the references 
given here and following up the references which they themselves provide, it should be 
possible for the reader to acquaint himself with most of what is known, or at least with 
what is worth knowing, about a particular topic. 
The names of authors are not included in the Index (pages 504 ff.) unless they occur 
in the text, since the Bibliography itself is arranged alphabetically under authors' names. 
The subjects, however, are indexed, and anyone wishing to consult references on a 
particular topic should refer in the first place to the Index, which in turn will refer to the 
authors who have dealt with the matter in question. 
In general the Bibliography contains only references to theoretical papers ; 
applications and illustrative material are included only when some theoretical point is involved. 
Papers which have been superseded by later work are omitted, except where they have 
a, historical interest. 
In compiling this material I have been particularly indebted to the valuable periodical 
reviews of Recent Advances in Mathematical Statistics by Irwin, Hartley and others in 
the Journal of the Royal Statistical Society : 1932, 95, 498; 1934, 97, 114 ; 1935, 98, 
88; 1936, 99, 714; 1938, 101, 394; 1939, 102, 406; and 1940, 103, 534. 
Many papers written since 1939 are included, but some journals are not available in 
war-time so that foreign work published after the entry of various countries into the war 
may be incompletely represented. Where possible, the references have been checked 
against the original publications, but here also I have had to rely on second-hand references 
in cases where the original papers were inaccessible. 
Note.—Names beginning with de, del, le, St., van, von, etc., are entered under those 
titles, i.e. the order is strictly alphabetical. 
Abernethy, J. R. (1933). On the elimination of systematic errors duo to grouping. Ami. 
Math. Stats., 4, 263. 
Aokebmann, W. G. (1939). Eine Erweiterung des Poissonschen Grenzwertsatzes unci ihre 
Anwendung auf die Risikoprobleme in der Sachversichorung. Schrijt. math. InM. Berlin, 
Adoock, R. J. (1878). A problem in least squares. Analyst, 5, 53. 
Aitken, A. C., and Oppenheim, A. (1931). On Charlier's new form of the frequency function, 
Proc. Boy. Soc. Edin., 51, 35. 
Aitken, A. C. (1931). Some applications of generating functions to normal frequency. Quart. J. 
Maths., 2, 130. 
Aitken, A. C. (1932). On the orthogonal polynomials in frequencies of Type B. Proc. Roy. 
Soc. Edin., 52, 174. 
Aitken, A. C. (1933a). On the graduation of data by the orthogonal polynomials of least squares. 
Proc. Roy. Soc. Edin., 53, 54. 
Aitken, A. C. (19336, c). On fitting polynomials to weighted data by least squares. Proc. Roy. 
Soc. Edin., 54, 1 ; and : On fitting polynomials to data with weighted and correlated 
errors. Ibid., 54, 12. 
442 
BIBLIOGRAPHY 443 
Aitken, A. C. (1935a). On least squares and linear combination of observations. Proc. Boy. 
Soc. Edin., 55, 42. 
Aitken, A. C, and Gontn, H. T. (19356). On fourfold sampling with and without replacement. 
Proc. Boy. Soc. Edin., 55, 114. 
Aitken, A. C. (1937a, b, 1938). Studies in practical mathematics : I. The evaluation with 
applications of a certain triple product matrix. Proc. Boy. Soc. Edin., 57, 172 ; II. The 
evaluation of the latent roots and latent vectors of a matrix. Ibid., 57, 269 ; III. The 
application of quadratic extrapolation to the evaluation of derivatives and to inverse 
interpolation. Ibid., 58, 161. 
Aitken, A. C, and Silverstone, H. (1942). On the estimation of statistical parameters. Proc. 
Boy. Soc. Edin., 61, 186. 
Allan, F. E. (1930). The general form of the orthogonal polynomials for simple series with 
proofs of their simple properties. Proc. Boy. Soc. Edin., 50, 310. 
Allan, F. E.,#and Wishart, J. (1930). A method of estimating the yield of a missing plot in 
held experimentation work. J*. Agr. Sci., 20, 399. 
Allen, H. V. (1938). A theorem concerning the linearity of regression. Stat. Bes. Mem., 2, 60. 
Allen, R. G. D. (1939). The assumptions of linear regression. Economica, 6, 191. 
Alt, F. L. (1942). Distributed lags. Econometrika, 10, 113. 
Alter, I). (1924). Application of Schuster's periodogram to long rainfall records, beginning 
1748. Monthly Weather Bev., 52, 479. 
Alter, I). (1925). Equations extending Schuster's periodogram. Astr. J., 36, No. 850. 
Alter, I). (1926a). An examination by means of Schuster's periodogram of rainfall data from 
long records in typical sections of the world. Monthly Weather Bev., 54, 44. 
Alter, I). (19266). The criteria of reality in the periodogram. Monthly Weather Bev., 54, 57. 
Alter, I). (1933). An extremely simple form of periodogram analysis. Proc. Nat. Acad. Sci., 
IV, ooo. 
Alter, I). (1937). A simple form of periodogram. Ann. Math. Stats., 8, 121. 
Alter, 1). (1939). Correction of sample moment bias due to lack of high contact and to histogram 
grouping. Ann. Math. Statu., 10, 192. 
'Alumnus ' (1932). A comparison of the effect of rainfall on spring and autumn-dressed wheat 
at Kothamsted Experimental Station, Harpcndeii. */. Agr. Sci., 22, 101, 
Ambarzhmian, G. (1937). Vertcilungskurven der Wahrscheinlichkeiten, welche im Limit die 
Vcrteihuigskurven von Pearson crgeben. (J.R. Acad. Sci. U.S.S.B., 16, 251. 
Anderson, O. (19.14). Noelimals iiber k The elimination of spurious correlation due to position 
in time and space.1 Biom., 10, 2C>9. 
Anderson, O. (1923). tlber ein nenes Vorfahren bei Anwondung der L Variate-difference ' Methode. 
Biom., 15, 134. Corrigenda, 15, 423. 
Anderson, O. (1926). Ober die Anwondung dor Difforeiizenmethode (Variate-difference method) 
bei Reilie nausgleic hungen, Stab i 1 itiits untersuclmngen, und Korrelationsmessungen. 
Biom., 18, 293. 
Anderson, O. (1927). On the logic of the decomposition of statistical series into separate 
components. J.R.S.S., 90, 548. 
Anderson, O. (1929). Die Korrelationsrexliriwng in der Kon/junkiurforschung. Schroeder, 
Bonn. 
Anderson, O. (1935). Einjiihrung in dip. nuithernatische Statislik, Springer, Wien. 
Anderson, P. H. (1942). Distributions in stratified sampling. Ann. Math. Stats., 13, 42. 
Anderson, R. L. (1942). Distribution of the serial correlation coefficient. Ann. Math. Stats., 
13, 1. 
Anderson, T. I\ (1935). Some further notes upon experiments with actuarial functions and 
Fourier's series. J. Inst. Act., 67, 31. 
Andersson, W. (1932). Researches into the theory of regression. Medd. Lunds Astr. Obs., Series 2, 
No. 64. 
444 BIBLIOGRAPHY 
Andersson, W. (1934). On a new method of computing non-linear regression curves. Ann. 
Math. Stats., 5, 81. 
Andre, D. (1884). Etude sur les maxima, minima et sequences des permutations. Ann. Ec. 
Norm. Sup., (3), 1, 121. 
Aroian, L. A. (1937). The Type B Gram-Charlier Series. Ann. Math. Stats., 8, 183. 
Aroian, L. A. (1941). A study of R. A. Fisher's z-distribution and the related JF-distribution. 
Ann. Math. Stats., 12, 429. 
Aroian, L. A. (1943). A new approximation to the levels of significance of the chi-square 
distribution. Ann. Math. Stats., 14, 93. 
Atjmann, G. (1934-1935). Aufbau von Mittelwerten mehrere Argumente. Math. Ann., 109,. 
235, and 111, 713. 
Ayyangar, A. A. K. (1934). Note on the recurrence formulae for the moments of the point 
binomial. Biom., 26, 262; and: Note on the incomplete moments of the hyper- 
geometrical series. Ibid., 26, 264. 
Ayyangar, A. A. K. (1938). On the semi-invariants of two variates and their additive property. 
Sankhya, 4, 85, and J. Indian Math. Soc, 3, 1. 
Bacon, H. M. (1938). Note on a formula for the multiple correlation coefficient. Ann. Math. 
Stats., 9, 227. 
Bailey, A. L. (1931). The analysis of covariance. J. Am. Stat. Ass., 26, 424. 
Baker, G. A. (1930a). Transformations of bimodal distributions. Ann. Math. Stats., 1, 334. 
Baker, G. A. (19306). The significance of the product-moment coefficient, with special reference 
to the marginal distributions. J. Am. Stat. Ass., 25, 387. 
Baker, G. A. (1930c). Random samples from non-homogeneous populations. Metron, 8, 
No. 3, 67. 
Baker, G. A. (1930a1). Distribution of the means of samples of n drawn at random from a 
population represented by the Gram-Charlier Series. Ann. Math. Stats., 1, 199. 
Baker, G. A. (1931). The relation between the means and variances, means squared and variances 
in samples from combinations of normal populations. Ann. Math. Stats., 2, 333. 
Baker, G. A. (1932). Distribution of the means divided by the standard deviations of samples 
from non-homogeneous populations. Ann. Math. Stats., 3, 1. 
Baker, G. A. (1934). Transformation of non-normal frequency-distributions into normal 
distributions. Ann. Math. Stats., 5, 113. 
Baker, G. A. (1935). Note on the distribution of the standard deviation and second moments 
from a Gram-Charlier distribution. Ann. Math. Stats., 6, 127. 
Baker, G. A. (1936). The probability that the mean of a second sample will differ from the 
mean of a first sample by less than a certain multiple of the standard deviation of the 
first sample. Ann. Math. Stats., 7, 197. 
Baker, G. A. (1937). Correlation surfaces of two or more indices when the components of the 
indices are normally distributed. Ann. Math. Stats., 8, 179. 
Baker, G. A. (1938). The probability that the standard deviation of a second sample will differ 
from the standard deviation of a first sample by a certain multiple of the first sample. 
Metron, 13, No. 3, 49. 
Baker, G. A. (1940). A comparison of Pearsonian approximations with exact sampling 
distributions of means and variances. Ann. Math. Stats., 11, 219. 
Baker, G. A. (1941). Tests of homogeneity for normal populations. Ann. Math. Stats., 12, 233. 
Barbacki, S., and Fisher, R. A. (1936). A test of the supposed precision of systematic 
arrangements. Ann. Eug. Lond., 7, 189. 
Barnard, M. M. (1935). The secular variations of skull characters in four series of Egyptian 
skulls. Ann. Eug. Lond., 6, 352. 
Barnard, M. M. (1936). An enumeration of the confounded arrangements in the 2x2x2 
factorial designs. Supp. J.B.S.S., 3, 195. 
BIBLIOGRAPHY 445 
B artels, J. (1935). Zur Morphologie geophysikalischer Zeitfunktionen. Sitz. Berl. Akad.Wiss., 
139. 
» 
Bartky, W. (1943). Multiple sampling with constant probability. Ann. Math. Stats., 14, 363. 
Bartlett, M. S. (1933a). On the theory of statistical regression. Proc Roy. Soc Edin., 53, 260. 
Bartlett, M. S. (19336). Probability and chance in the theory of statistics. Proc. Roy. Soc, 
A, 141, 518. 
Bartlett, M. S. (1934a). The problem in statistics of testing several variances. Proc. Camb. 
Phil. Soc, 30, 164. 
Bartlett, M. S. (19346). The vector representation of a sample. Proc Camb. Phil. Soc, 30, 
327. 
Bartlett, M. S. (1935a). The effect of non-normality on the ^-distribution. Proc Camb. Phil. 
Soc, 31, 223. 
Bartlett, M. S. (19356). Contingency table interactions. Supp. J.R.S.S., 2, 248. 
Bartlett, M. S. (1935c). Some aspects of the time-correlation problem in regard to tests of 
significance. J.R.S.S., 98, 536. 
Bartlett, M. S. (19350*). An examination of the value of covariance in dairy-cow nutrition 
experiments. /. Agr. Sci., 25, 238. 
Bartlett, M. S. (1936a). The information available in small samples. Proc. Camb. Phil. Soc, 
32, 560. 
Bartlett, M. S. (19366). Statistical information and properties of sufficiency. Proc Roy. Soc, 
A, 154, 124. 
Bartlett, M. S. (1936c). A note on the analysis of covariance. /. Agr. Sci., 26, 488. 
Bartlett, M. S. (1936d). Square-root transformations in the analysis of variance. Supp. 
J.R.S.S., 3, 68. 
Bartlett, M. S. (1936c), Some notes on insecticide tests in the laboratory and in the field. 
Supp. J.R.S.S., 3, 185. 
Bartlett, M. S. (1937a). Sub-sampling for attributes. Supp. J.R.S.S., 4, 131. 
Bartlett, M. S. (19376). Note on the derivation of fluctuation formulae for statistical assemblies. 
Proc Camb. Phil. Soc, 33, 390. 
Bartlett, M. S. (1937c). Properties of sufficiency and statistical tests. Proc Roy. Soc, A, 
160, 268. 
Bartlett, M. S. (1937d). Some examples of statistical methods of research in agriculture and 
applied biology. Supp. J.R.S.S., 4, 137. 
Bartlett, JM. S. (1937a). The statistical conception of mental factors. Brit. J. Psych., 28, 97. 
Bartlett, M. S. (1938a). The approximate recovery of information from replicated experiments 
with large blocks. ♦/. Agr. Sci., 28, 418. 
Bartlett, M. 8. (1.9386). The characteristic function of a conditional statistic. J. Lond. Math. 
ioOC , J. O, Ui-J. 
Bartlett, M. S. (1938c). Further aspects of the theory of multiple regression. Proc Camb. 
Phil. Soc, 34, 33. 
Bartlett, M. S. (1939a). Complete simultaneous fiducial distributions. Ann. Math. Stats., 
10, 129. 
Bartlett, M. S. (19396). A note on tests of significance in multivariate analysis. Proc Camb. 
Phil. Soc, 35, 180. 
Bartlett, M. S. (1939c). The standard errors of discriminant function coefficients. Supp. 
J.R.S.S., 6, 169. 
Bartlett, M. S. (1940). A note on the interpretation of quasi-sufficiency. Biom., 31, 391. 
Bartlett, M. S. (1941). The statistical significance of canonical correlations. Biom., 32, 29. 
Baten, W. I). (1931). Corrections for the moments of a frequency distribution in two variables. 
Ann. Math. Stats., 2, 309. 
Baten, W. 1). (1933a). Frequency laws for the sum of n variables which are subject to given 
frequency laws. Metron, 10, No. 3, 75. 
446 BIBLIOGRAPHY 
Eaten, W. D. (19336). Sampling from many parent populations. Tokohu Math. Journ., 36, 206. 
Eaten, W. D. (1934). The probability law for the sum of n independent variables, each subject 
to the law (1/2A) sech (nx/2h). Bull. Am. Math. Soc, 40, 284. 
Battin, I. L. (1942). On the problem of multiple matching. Ann. Math. Stats., 13, 294. 
Eayes, T. (1763). An essay towards solving a problem in the doctrine of chances. Phil. Trans., 
53, 370. 
Beale, F. S. (1937). On the polynomials related to the differential equation 
1 dy a0 -\- ax x __ N 
y dx b0 + bx x + b2 x2 D' 
Ann. Math. Stats., 8, 206. 
Beall, G. (1939). Methods of estimating the population of insects in a field. Biorn., 30, 422. 
Beall, G. (1942). The transformation of data from entomological field experiments so that the 
analysis of variance becomes applicable. Biorn., 32, 243. 
Beck, E. (1936). Existenzbeweise zur Wahrscheinlichkeitstheorie. Math. Zeit.,Al, 222. 
Becker, E., Plant, H., and Uttnge, I. (1930). Anwendung der mathematischen Statistik auf 
Probleme der Massenfabrikation. Springer, Berlin. 
Behrens, W. V. (1929). Ein Beitrag zur Fehlerberechnung bei wenigen Beobachtungen. Landiv. 
Jb., 68, 807. 
Belardinelli, G. (1934). Su una teoria astratta del calcolo della probability. Giorn. 1st. ItaL 
Att., 5, 418. 
Benini, R. (1926). Principi di statistica metodologica. Unione Tipografica Editrice Torinese, 
Turin. 
Bennett, T. L. (1920). The theory of measurement of changes in the cost of living. J.R.S.S., 
83, 455. 
Berge, P. O. (1938). A note on a form of Tchebycheff's theorem for two variables. Blow., 
29, 405. 
Bergstrom, S. (1918). Sur les moments de la fonction de correlation normale de n variables. 
Biorn., 12, 177. 
Berkson, J. (1930). Bayes' theorem. Ann. Math. Stats., 1, 42. 
Berkson, J. (1938). Some difficulties of interpretation encountered in the application of the 
chi-square test. J. Am. Stat. Ass., 33, 526. 
Bernoxjilli, J. (1713). Ars coniectandi. (A German translation in Ostwald's Klassiker der 
Exakten Wissenschaften, Nos. 107 and 108.) 
Bernstein, F. (1932). Die mittleren Fehlerquadrate und Korrelation der Potenzmomente und. 
ihre Anwendung auf Funktionen der Potenzmomente. Metron, 10, No. 3, 3. 
Bernstein, F. (1937). Regression and correlation evaluated by a method of partial sums. Ann. 
Math. Stats., 8, 77. 
Bernstein, S. (1927). Sur l'extension du theoreme limite du calcul des probabilities avix sonnnes 
de quantites dependantes. Math. Ann., 97, 1. 
Bernstein, S. (1936). Determination d'une limite inferieure de la dispersion des somines do 
grandeurs liees en chaine singuliere. Rec. Math. Moscou, 1, 29. 
Bernstein, S. (1937). Sur quelques modifications de l'inegalite de Tchebychcff. C.R. Acad. 
Sci. U.S.S.R., 17, 279. 
Berteand, J. L. F. (1889). Calcul des probabilites. Gauthier-Villars, Paris. 
Besicovitch, A. S. (1932). Almost Periodic Functions. Cambridge University Press. 
Besson, L. (trans, and abridged by E. W. Woolard) (1920). On the comparison of meteorological 
data with chance results. Monthly Weather Rev., 48, 89. 
Beveridge, Sir W. H. (1921). Weather and harvest cycles. Econ. J., 31, 429. 
Beveridge, Sir W. H. (1922). Wheat prices and rainfall in Western Europe. J.R.S.S., 85, 412. 
Bhattacharya, D. P., and Narayan, R. B. (1942). Moments of D2-statistic for population* 
with unequal dispersions. Sankhya, 5, 401. 
Bhattachabya, K. N. (1943). A note on twofold triple systems. Sankhya, 6, 313. 
BIBLIOGRAPHY 447 
Bilham, E. G. (1926). Correlation coefficients. Q. J. Boy. Met. Soc, 52, 172. 
Bingham, M. D. (1941). A new method for obtaining the inverse matrix. J. Am. Stat. Ass., 
36, 530. 
Bishop, D. J. (1939). On a comprehensive test for the homogeneity of variances and covariancea 
in multivariate problems. Biom., 31, 31. 
Bishop, D. J., and Nair, U. S. (1939). A note on certain methods of testing for the homogeneity 
of a set of estimated variances. Supp. J.B.S.S., 6, 89. 
Bispham, J. W. (1922). Note on a heterotypic frequency function. J.B.S.S., 85, 488. 
Bispham, J. W. (1920, 1923). An experimental determination of the distribution of the partial 
correlation coefficient in samples of thirty. Proc. Boy. 80c, A, 97, 1920, and Metron, 
2, 684, 1923. 
Blakeman, J. (1905). On tests for linearity of regression in frequency-distributions. Biom., 
Blakeman, J., and Pearson, K. (1906). On the probable error of the coefficient of mean square 
contingency. Biom., 5, 191. 
Bliss, C. I. (1935). The calculation of the dosage-mortality curve. Ann. App. Biol., 22, 134 ; 
and : The comparison of dosage-mortality data. Ibid., 22, 307. 
Bliss, C. I. (1937). The calculation of the time-mortality curve. Ann. App. Biol., 24, 816. 
Bliss, 0. I. (1938). The transformation of percentages for use in the analysis of variance. Ohio 
J. Sci., 38, 9. 
Blumel, H. (1939). Bemerkungen uber die Sheppardsche Korrektur. Arch. math. Wirtsch.- u. 
8ozialforschimgi 5, 39. 
Boas, R. P., and Smithies, F. (1937). On the characterisation of a distribution function by its 
Fourier transform. Am. J. Maths., 60, 513. 
Booh nek, 8., and Jessen, B. (1934). Distribution functions and positive definite functions. 
Ann. Math*., 35, 252. 
B00HNER, S. (1936). A converse of Poisson's theorem in the theory of probability. Ann. Maths., 
37, 8IB. 
Boohnkr, N. (1937). Stable laws of probability and completely monotone functions. Duke 
Math, J., 3, 726. 
Bodkwadt, U. T. (1936). Zum Momentproblem fur das interval! [0, 1]. Math. Zeit., 40, 426. 
Bohr, H. (1925). Zur theorie fast periodische Funktionen. Acta Math., 45, 29. 
BoNFERRONi, V>. (1933). Nulla probability ina.ss.iina ncllo Schema di Poisson. Giorn. 1st. Ital. 
AtL, 4, 109. 
Bonferroni, (1. (1939). I)i una, cstensione del eoeiTiciente di eorrelazione. Giorn, (legli Economists, 
Nov.-Dec, p. 7. 
Borkl, 10. (editor) (.1925 and subsequently). Traiiv. du calcul dcs probabilites el de ses applications. 
(jlauthier-Villars, Paris. 
BoRKii, E. (1933). Nur un probleme elementaire de probabilities ot la quasi-periodicite de certains 
phenomenes arithmetiques. Gomptes rendus, 196, 881. 
Borki'j, 10. (1937). Sur limitation du haward. Camples rendus, 204, 203. 
Borel, K. (1939). Sur une interpretation des probabilites virtuellcs. Gomptes rendus, 208, 
1309 ; and : Sur certains problemes de repartition et lew probabilites virtuelles. Ibid., 
208, 1177. 
Bosk, A.*N. (1941). Nome problems of field operations in labour inquiries. Sankhyd, 5, 229. 
Bosk, 0. (1943). Note on the sampling error in the method of double sampling. Sankhyd, 
6, 329. 
Bose, R. 0. (1934). On the application of hyperspaee geometry to the theory of multiple 
correlation. Sankhyd, 1, 338. 
Bosk, R. C. (1936a). On the exact distribution and moment-coefficients of the D2-statistic. 
Sankhyd, 2, 143. 
Bose, K. C. (19306). A note on the distribution of differences in mean values of two samples 
448 BIBLIOGRAPHY 
drawn from two multivariate normally-distributed populations, and the definition of 
the Z)2-statistic. Sankhyd, 2, 379. 
Bose, R. C. (1938a). On the distribution of the means of samples drawn from a Bessel function 
population. Sankhyd, 3, 262. 
Bose, R. C. (19386). On the application of Galois fields to the problem of construction of Hyper- 
Graeco-Latin squares. Sankhyd, 3, 323. 
Bose, R. C, and Roy, S. N. (1938c). The distribution of the studentised Z)2-statistic. Sankhyd, 
Jt , .Li/. 
Bose, R. C. (1939). On the construction of balanced incomplete block designs. Ann. Eug. 
Lond., 9, 353. 
Bose, R. C, and Naik, K. R. (1939). Partially balanced incomplete block designs. Sankhyd, 
4, 337. 
Bose, R. C, and Roy, S. N. (1940). The use and distribution of the studentised D2-statistic 
when the variances and covariances are based on k samples. Sankhyd, 4, 535. 
Bose, R. C, and Kishen, K. K. (1941). On the problem of confounding in general symmetrical 
factorial designs. Sankhyd, 5, 21. 
Bose, R. C. (1942a). A note on the resolvability of balanced incomplete block designs. Sankhyd, 
6, 105. 
Bose, R. C, and Nair, K. R. (19426). On complete sets of Latin squares. Sankhyd, 5, 361. 
Bose, S. N. (1935). On the complete moment coefficients of the D2-statistic. Sankhyd, 
2, 385. 
Bose, S. N. (1937). On the moment coefficients of the D2-statistic and certain integral and 
differential equations connected with the multivariate normal population. Sankhyd, 
3, 105. 
Bose, S. S. (1934a). Tables for testing the significance of linear regression in the case of time- 
series and other single-valued samples. Sankhyd, 1, 277. 
Bose, S. S. (19346). A note on the mathematical expectation of the value of the regression 
coefficient. Sankhyd, 1, 432. 
Bose, S. S. (1935). On the distribution of the ratio of variances of two samples drawn from a 
given normal bivariate correlated population. Sankhyd, 2, 65. 
Bose, S. S. (1938a). On a Bessel function population. Sankhyd, 3, 253. 
Bose, S. S. (19386). Relative efficiency of regression coefficients estimated by the method of 
finite differences. Sankhyd, 3, 339. 
Bose, S. S., and Mahalanobis, P. C. (1938a). On the exact test of association between the 
occurrence of thunderstorm and abnormal ionisation. Sankhyd, 3, 249. 
Bose, S. S., and Mahal anobis, P. C. (19386). On estimating individual yields in the ease of 
mixed-up yields of two or more plots in field experiments. Sankhyd, 4, 103. 
Bowley, A. L. (1912). The measurement of the accuracy of an average. J.R.S.S., 75, 77. 
Bowley, A. L. (1919). The measurement of changes in the cost of living. J.R.S.S., 82, 343. 
Bowley, A. L. (1920). Prices and Wages in the United Kingdom, Clarendon Press, Oxford. 
Bowley, A. L., and Smith, K. C. (1924). Seasonal variations in Finance, Prices and Industry. 
Lond. and Camb. Ec. Service, Special Memo. No. 7. 
Bowley, A. L. (1925). Measurement of the precision attained in sampling. Bull. Int. Inst. 
Stat., 22, ler livre. 
Bowley, A. L. (1926). The influence on the precision of index-numbers of the correlation between 
the prices of commodities. J.E.S.S., 89, 300. 
Bowley, A. L. (1928). F. Y. Edgeworth's Contributions to Mathematical Statistics. Royal 
Statistical Society, London. 
Bowley, A. L. (1933). The action of economic forces in producing frequency-distributions of 
income, prices and other phenomena. Econometrika, 1, 358. 
Bowley, A. L, (1938). Note on Professor Frisch's ' The Problem of Index Numbers '. 
Econometrika, 6, 83. 
BIBLIOGRAPHY 449 
Bradley, P. D., and Crum, W. L. (1939). Periodicity as an explanation of variation in hog 
production. Econometrika, 7, 221. 
Brady, J. (1935). A biological application of the analysis of covariance. Supp. J.R.S.S., 2, 99. 
Brander, F. A. (1933). A test of the significance of the difference of the correlation coefficients 
in normal samples. Biom., 25, 102. 
Brandt, A. E. (1933). The analysis of variance in a 2 x 6" table with disproportionate frequencies. 
J. Am. Stat. Ass., 28, 164. 
Brelot, M. (1936, 1937). Sur l'influence des erreurs de mesure en statistique. J. Math. Pur. 
App., 15, 113, and 16, 285 ; also Congres Int. de Math., Oslo (1936). 
Brelot, M. (1937). Quelques difficultes dans Fapplication pratique de la theorie. des erreurs. 
Mathematica, 13, 243. 
Broderick, P. S. (1937). On some symbolic formulae in probability theory. Proc. Roy. Irish 
Acad., A, 44, 19. 
Broggi, U. (1934). Su di uno speciale problema clei momenti. Ann. di Mat., (4), 12, 63. 
Brown, G. M. (1933). On sampling from compound populations. Ann. Math. Stats., 4, 288. 
Brown, G. W. (1939). On the power of the Lx test for equality of several variances. Ann. Math. 
Stats., 10, 119. 
Brown, G. W. (1940). Keduction of a certain class of statistical hypotheses. Ann. Math. Stats., 
Brown, J. W., Greenwood, M., and Wood, Frances (1914). A study of index correlations. 
J.R.S.S., 77, 317. 
Brown, W. (1909). Some experimental results in correlation. Proceedings Sixth Int. Congress 
Psychology, Geneva. 
Brown, W., and Thomson, G. H. (1925). The essential* of mental measurement. Cambridge 
Uni versity Press. 
Brown, W. (1935). A note on the theory of two factors versus the sampling theory of mental 
ability. Brit. J. Psych., 25, 395. 
Brownlee, J. (1905). Statistical studies in immunity : small-pox and vaccination. Biom., 
Brownlee, J. (1910). The significance of the correlation coefficient when applied to Menclelian 
distributions. Proc. Roy. Soc. Edin., 30, 473. 
Brownlee, J. (1911). The mathematical theory of random migration and epidemic distribution. 
Proc. Roy. Soc. Edin., 31, 262. 
Brownlee, J., and Morikon, R. ML (H)ll). Notes on the calculation of the probabilities of life 
at high ages. J.R.S.S., 74, 201. 
Brownlee, J. (1918). Certain aspects of the theory of epidemiology in special reference to plague. 
Proc. Roy. Soc. Med., Sect. Epidem. and State Medicine, 10D, 85. 
Brownlee, J. (1924a). Experiments to test the theory of goodness of lit. JJi.S.S., 87, 76. 
Brownlee, J. (19246). Test of periodogram analysis. J.R.S.S., 87, 83. 
Brownlee, J. (1925). Error in the correlation due to random sampling when proportionate 
mortalities are used. J.R.S.S., 88, 105. 
Brit en, 0. (1938). Methods for the combination of observations, etc. Metron, 13, No. 2, 61. 
Brunk, H. '(1906). Wahrscheinlichkeit^rachnnmg und Kollehiivrtiasslehn. Teubner, Leipzig. 
Brunk, H, (1921). Ober die Analyse periodischer Vorgange. A sir. Nach., 188. 
Brunt, I). (1925). Periodicities in European weather. Phil. Trans., A, 225, 247. 
Brunt, I). (1928). Harmonic analysis and the interpretation of the results of periodogram 
investigations. Mem. R. Met. Soc, 2, No. 15, 47. 
Brunt, I). (1931). The Combination of (Jhservations. Cambridge University Press. 
Bunak, V. V. (1936). Changes in the mean values of characters in mixed populations. Ann. 
Eiig. Lond., 7, 195. 
BuRKHARDT, F., and Stackelberg, H. V. (1939). Zur Ableitung der Sheppardschen Korrektur. 
Arch. Math. Wirtsch.- u. Socialforsc/uing, 5, 127. 
A.S.—VOL. II. GG 
450 BIBLIOGRAPHY 
Burks, B. S. (1933). A statistical method for estimating the distribution of sizes of completed 
fraternities in a population represented by a random sampling of individuals. J. Am. 
Stat. Ass., 28, 388. 
Burnside, W. (1924). On Bayes' formula. Biom., 16, 189. 
Burnside, W. (1928). Theory of Probability. Cambridge University Press. 
Burr, I. W. (1942). Cumulative frequency functions. Ann. Math. Stats., 13, 215. 
Burrau, C. (1934). Contribution to the problem of dissection of a given frequency curve. Nordic 
Stat. J., 5, 43. 
Burt, C. (1927). Mental and Scholastic Tests. P. S. King, London. 
Burt, C. (1936). Marks of Examiners. Macmillan, London. 
Btjrt, C. (1937a). Correlations between persons. Brit. J. Psych., 28, 59. 
Burt, C. (19376). Methods of factor analysis with and without successive approximations. Brit. 
J. Educ. Psych., 7, 172. 
Burt, C. (1938a). The unit hierarchy and its properties. Psychometrika, 3, 151. 
Burt, C. (19386). Factor analysis by sub-matrices. J. Psych., 6, 339. 
Buys-Ballot, C. H. D. (1847). Les changements periodiques de temperature. Utrecht. 
-Cacoioppolli, R. (1932). SulF approssimazione per polinomi delle funzioni denniti in campi 
ilhmitati. Giorn. Ital. 1st. Att., 3, 364. 
Camp, B. H. (1922). A new generalisation of TchebychefPs statistical inequality. Bull. Am. Math. 
Soc, 28, 427. 
Camp, B. H. (1924). Probability integrals for the point binomial. Biom., 16, 163. 
Camp, B. H. (1925a). Probability integrals for the hypergeometric series. Biom,., 17, 61. 
Camp, B. H. (19256). Mutually consistent multiple regression surfaces. Biom,., 17, 443. 
Camp, B. H. (1932). The converse of Spearman's two-factor theorem. Biom., 24, 418. 
Camp, B. H. (1934). Spearman's general factor again. Biom., 26, 260. 
Camp, B. H. (1937). Methods of obtaining probability distributions. Ann. Math. Stats., 8, 90. 
Camp, B. H. (1938a). Notes on the distribution of the geometric mean. Ann. Math. Stats., 
9, 221. 
Camp, B. H. (19386). Further interpretations of the chi-square test. J. Am. Stat. Ass., 33, 537. 
Campbell, N. (1935). The statistical theory of errors. Proc. Phys. Soc, 47, 800. 
Campbell, N. (1939). Frequency interpretations in probability. Nature, 143, 601. 
Cannon, E. W., and Wintner, A. (1935). An asymptotic formula for a class of distribution 
functions. Proc. Edin. Math. Soc, 4, 138. 
Cantelli, F. P. (1913). Sulla differenza media con ripetizione. Giorn. Econ. e Riv. di Stat., 
February. 
Cantelli, F. P. (1916). La tendenza ad un limite nel senso del calcolo della probabilita. Rend. 
Circ. Mat. di Palermo, 16, 191. 
Cantelli, F. P. (1917). Sulla probabilita come limite della frequenza. Rend. R. Ace. Line. 
(5), 26, 39. 
Cantelli, F. P. (1923). Sulla oscillazione delle frequenze intorno alia probabilita. Metron, 3, 
No. 2, 167. 
Cantelli, F. P. (1929). Sulla legge di distribuzione dei redditi. Giorn. Econ. e Riv. di Stat. 
Cantelli, F. P. (1932). Una teoria astratta del calcolo della probabilita. Giorn. 1st. Ital. Att., 
3, 257. 
Cantelli, F. P. (1933a). Considerazione sulla legge uniforme dei grandi numeri e sulla general - 
izzazione di un fondamentale teorema del Sig. Paul Levy. Giorn. 1st. Ital. Att., 
Cantelli, F. P. (19336). Sulla determinazione empirica delle legge di probabilita. Giorn. 1st. 
Ital. Att, 4, 421. 
Cantelli, F. P. (1935). Considerations sur la convergence dans le calcul des probabilites. Ann. 
Inst. H. Poincare, 5, 1. 
BIBLIOGRAPHY 451 
Cantelli, F. P. (1936). Considerazione su alcuni concette esposti nella introduzione della nota 
di R. de Mises. Oiorn. 1st Ital. Att, 7, 256. 
Carleman, T. (1925). Les fonctions quasi-analytiques. Gauthier-Villars, Paris. 
Carlson, J. L. (1932). A study of the distribution of means estimated from small samples by the 
method of maximum likelihood for Pearson's Type II curve. Ann, Math. Stats., 3, 86. 
Carmichael, F. L. (1931). Methods of computing seasonal indices. J. Am.. Slat Ass., 26, 135. 
Carslaw, H. S. (1930). Introduction to the Theory of Fourier's Series and Integrals. Macmillan, 
London. 
Carver, H. C. (1932). Trapezoidal rule for computing seasonal indices. Ann. Math. Stats., 
Oj fjOJ. • 
Carver, H. C. (1933). Note on the computation and modification of moments. Ann. Math. 
Stats., 4, 229. 
Carver, H. C (1936). The fundamental nature and proof of Sheppard's adjustments. Ann. 
Math. Stats., 7, 154. 
Castellano, V. (1933a). Sulle relazioni tra curve di frequenza e curve di concentrazione e sui 
rapporti cli concentrazione corrispondenti a determinate distribuzioni. Metron, 10, 
No.'4, 3. 
Castellano, V. (19336). Sulla interpretazione dinamica del rapporto cli concentrazione. (Horn. 
1st Ital. Att, 4, 268. 
Castellano, V. (1934). Sulla scarto quadratico medio della probabilita di transvariazione. 
Metron, 11, No. 4, 19. 
Castellano, V. (1935). Recente lottoratura sugli indici di variabilita. Metron, 12, No. 3, 101. 
Castellano, V. (1937). Sugli indici relativi di variabilita e sulla ooneontraziono dei earattori 
con segno. Metron, 13, No. I, 31. 
Castelnuovo, G. (1.926-8). Calcolo delta probabilita. Bologna. 
Castelnuovo, G. (1932). Sur quelqucs problemes sc rattaehant an calcul des probabilitos. Ann. 
Inst II. Poincare, 3, 465. 
Cave, B. M., and Pearson, K. (1914). Numerical illustrations of tho variakwlifforeneo 
correlation method. IHom., 10, 340. 
Cave-Browne-Cavk, F. E. (1904). On tho influence of tho time factor on the*, correlation between 
the barometric heights at stations more than 1000 miles apart. Proc. Roy. Soc, A, 
74, 403. 
Chandra Sekar, C, and Francis, M. G. (1941). A method to got tho significance limit of a 
type of test criteria. Sankhya, 5, 165. 
Chapelin, J. (1932). On a method of proceeding from partial cell-frequencies to ordinates and 
to total eel I-frequencies in the case of a bivariafe fro(|uoncy surface*. IHom,, 24, 495. 
Chapman, I). W. (1935). The generalised problem of correct matehings. Ann. Math. Stats., 
6, 85. 
Chapman, R. A. (1938). Applicability of the 2-test to a Poisson distribution. Biom., 30, 188. 
Charliur, C. V. L. (1906). Researches into tho theory of probability. Mcdd. Lands Aslr. Obs. 
Ciiarlilr, C. V. L. (1912). Contributions to the mathematical theory of statistics. Mcdd. 
Lunds Aslr. Obs. 
Charlier, 0. V. L. (192S). A new form of the frequency function. Mvdd. Lunds Aslr. Obs., 
Series 2, No. 51. 
Charlier, C. V. L. (1931). Applications \d<> la theorie des probabililvs] a Vastronomie. ('Part of 
the Trait e edited by Borel.) Gauthier-Villars, Paris. 
Cheshire, L., Oldis, F., and Pearson, K. ft. (1932). Further experiments on the sampling 
distribution of the correlation coefficient. J. Am. IS tat. Ass., 27, 121. 
Chlodovsky, L. (1938). Le probleme des moments et les polynomes do S. Bernstein. Comptes 
re.ndus Acad. Sci. U.S.S.R., 19, 059. 
Christjdis, B. G. (1931). The importance of the shape of plot in field experimentation. J. Agr. 
Sci., 21, 14. 
452 BIBLIOGRAPHY * 
Church, A. E. R. (1925). On the moments of the distributions of squared standard deviations 
for samples of N drawn from an indefinitely large population. Biom., 17, 79. 
Church, A. E. R. (1926). On the means and squared standard deviations of small samples from 
any population. Biom., 18, 321. 
Cisbani, R. (1938). Contributi alia teoria delle medie. Metron, 13, No. 2, 23, and No. 3, 3. 
Clapham, A. R. (1931). Studies in sampling technique : cereal experiments. J. Agr. Sci., 
21, 366 and 376. 
Clapham, A. R. (1936). Over-dispersion in grassland communities and the use of statistical 
methods in plant ecology. J. Ecology, 24, 232. 
Claremont, C. A. (1916). On the correlation between the ' corrected ' cancer and diabetes 
death-rates. Biom., 11, 191. 
Clark, A., and Leonard, W. H. (1939). The analysis of variance with special reference to data 
expressed as percentages. /. Am. Soc. Agron., 31, 55. 
Clopper, C. J., and Pearson, E. S. (1934). The use of confidence or fiducial limits illustrated in 
the case of the binomial. Biom., 26, 404. 
Cobb, C. W. (1939). Note on Frisch's diagonal regression. Econometriha, 7, 77. 
Cochran, W. G. (1934). The distribution of quadratic forms in a normal system, with applications 
to the analysis of covariance. Proc. Camb. Phil. Soc, 30, 178. 
Cochran, W. G. (1935). A note on the influence of rainfall on the yield of cereals in relation to 
manurial treatment. J*. Agr. Sci., 25, 510. 
Cochran, W. G. (1936a). The ^2-distribution for the binomial and Poisson series with small 
expectations. Ann. Bug. Lond., 7, 207. 
Cochran, W. G. (19366). Statistical analysis of field counts of diseased plants. Supp. J.R.S.S., 
Cochran, W. G. (1937a). The efficiencies of the binomial series tests of significance of a mean 
and correlation coefficient. J.R.S.S., 100, 69. 
Cochran, W. G. (19376). Problems arising in the analysis of a series of similar experiments. 
Supp. J.R.S.S., 4, 102. 
Cochran, W. G. (1938a). The omission or addition of an independent variate in multiple linear 
regression. Supp. J.R.S.S., 5, 171. 
Cochran, W. G. (19386). Some difficulties in the statistical analysis of replicated experiments. 
Emp. J. Exp. Agr., 6, 157. 
Cochran, W. G.. (1939a). Long-term agricultural experiments. Supp. J.R.S.S., 6, 1.04. 
Coohran, W. G. (19396). The use of the analysis of variance in enumeration by sampling. 
J. Am. Stat. Ass., 34, 492. 
Cochran, W. G. (1940a). Note on an approximative formula for significance levels of z. Ann. 
Math. Stats., 11, 93. 
Coohran, W. G. (19406). The analysis of variance when experimental errors follow the Poisson 
or binomial laws. Ann. Math. Stats., 11, 335. 
Cochran, W. G. (1941). The distribution of the largest of a set of variances as a fraction of their 
total. Ann. Eug. Lond., 11, 47. 
Cochran, W. G. (1942a). The %2 correction for continuity. Iowa State College J. Sci., 61, 421. 
Cochran, W. G. (19426). Sampling theory when the sampling units are of unequal sizes. J. Am. 
Stat. Ass., 37, 199. 
Cochran, W. G. (1943). The comparison of different scales of measurement for experimental 
results. Ann. Math. Stats., 14, 205. 
Coleman, J. B. (1932). A coefficient of linear correlation based on the method of least squares 
and the line of best fit. Ann. Math. Stats., 3, 79. 
Comrie, L. J. (1936). Inverse interpolation and scientific applications of the National accounting 
machine. Supp. J.R.S.S., 3, 87. 
Comrie, L. J., Hey, G. B., and Hudson, H. G. (1937). Application of Hollerith equipment to 
an agricultural investigation. Supp. J.R.S.S., 4, 210. 
BIBLIOGRAPHY 453 
Comrie, L. J. (1939). Tables of tan''1 x and log (1 + x2). Tracts for Computers, No. 23. 
Cambridge University Press. 
Comrie, L. J., and Hartley, H. O. (1941). Tables of Lagrangian coefficients for harmonic 
interpolation in certain tables of percentage points. Biom., 32, 183. 
Co-operative Study, see Soper, H. E. and others, 1917. 
Copeland, A. H. (1928). Admissible numbers in the theory of probability. Am. J. Maths,, 
50, 535. 
Copeland, A. H. (1929). Independent event histories. Am. J. Maths., 51, 612. 
Copeland, A. H. (1932). The theory of probability from the point of view of admissible numbers. 
Ann. Math. Stats., 3, 143. 
Copeland, A. H. (1936). Point set theory applied to the random selection of the digits of an 
admissible number. Am,. J. Maths., 58, 181. 
Copeland, A. H., and Regan, P. (1936). A postulationai treatment of the Poisson law. Ann. 
Math., 37, 357. 
Copeland, A. H. (1937). Consistency of conditions determining collectives. Trans. Am. Math. 
Soc, 43, 333. 
Cornish, E. A. (1936). Non-replicated factorial experiments. J. Aus. Inst. Agr. ScL, 2, 79. 
Cornish, E. A., and Fisher, R. A. (1937). Moments and cumulants in the specification of 
distributions. Rev. Inst. Int. Stat., 5, 307. 
Cornish, E. A. (1940a, b, c). The estimation of missing values in incomplete randomised block 
experiments. Ann. Eug. Land., 10, 112 ; The estimation of missing values in quasi- 
factorial designs. Ibid., 10, 137 ; The analysis of eovariance in quasi-factorial designs. 
Ibid., 10, 269. 
Cowles, A. (1933). Can stock-market forecasters forecast ? Econometrika, 1, 309. 
Cowles, A., and Chapman, E. N. (1935). A statistical study of climate in relation to pulmonary 
tuberculosis. J. Am. Stat. Ass., 30, 517. 
Cowles, A., and Jones, H. E. (1937). Some a posteriori probabilities in stock-market action. 
Econonietrika, 5, 2(SO. 
Cox, (I. M., and Snedhcjor, (■}. W. (1930). Covariance used to analyse the relation between corn- 
yield and average. J. Farm,. Ecoh., 18, 597. 
Cox, C M. (1940). Enumeration and construction of balanced incomplete block configurations. 
Ann. Math. Slats., 11, 72. 
Craig, A. T. (1932). The simultaneous distribution of mean and standard deviation in small 
samples. Ann. Math. Stats., 3, 120. 
Craio, A. T. (1933a). On the correlation between certain averages for small samples. Ann. 
Math. Slats., 4, 127. 
Craw, A. T. (1933/;). Variables correlated in sequence. Hull. Am. Math. Soc, 39, 129. 
Craig, A. T. (1930a). Note on a certain bilinear form that occurs in statistics. Am. J. Maths., 
58, 804. 
Craio, A. T. (1930/j). A certain mean-value problem in statistics. Hull. Am. Math. Soc, 42, 070. 
Craio, A. T. (1938). On the independence of certain estimates of variance. Ann. Math. Stats., 
9, 48. 
Craig, A. T. (1939). On the mathematics of the representative method of sampling. Ann- 
Math. Slats., 10, 20. 
Craio, A. T. (1943). Note on the independence of certain quadratic forms. Ann. Math. Stats., 
14, 195. 
Craio, C. C. (1928). An application ot'Thiele\s seminvariants to the sampling problem. Mctron, 
7, No. 4, 3. 
Craio, C. C. (1929a). Sampling when the parent population is of Pearson's Type III. Biom., 
Mix., xjO / . 
Craig, C. C. (19296). The frequency function of y/x. Ann. Math., 30, 471. 
Craig, C. C. (1931a). Sampling in the case of correlated observations. Ann. Math. Stats., 2, 324. 
454 BIBLIOGRAPHY 
Ceaig, C. C. (19316). Note on the distribution of samples of N drawn from a Type A population. 
Ann. Math. Stats., 2, 99. 
Craig, C. C. (1931c). On a property of the seminvariants of Thiele. Ann. Math. Stats., 2, 154. 
Craig, C. C. (1932). On the composition of dependent elementary errors. Ann. Math., 33, 184. 
Ceaig, C. C. (1933). On the Tchebycheff inequality of Bernstein. Ann. Math. Stats., 4, 94. 
Ceaig, C. C. (1936a). On the frequency function of xy. Ann. Math. Stats., 7, 1. 
Ceaig, C. C. (19366). A new exposition and chart for the Pearson system of frequency curves. 
• Ann. Math. Stats., 7, 16. 
Ceaig, C C (1936c). Sheppard's corrections for a discrete variable. Ann. Math. Stats., 7, 55. 
Ceaig, C C (1940). The product seminvariants of the mean and a central moment in samples. 
Ann. Math. Stats., 11, 177. 
Ceaig, C C (1941a). Note on the distribution of non-central t with an application. Ann. Math. 
Stats., 12, 224. 
Ceaig, C C (19416). A note on Sheppard's corrections. Ann. Math. Stats., 12, 339. 
Ceaig, J. I. (1916). A new method of discovering periodicities. Month. Not. R. Astr. Soc, 76, 493. 
Crameb, H. (1923). Das Gesetz von Gauss und die Theorie des Risikos. SJcand. Aht., 6, 209. 
Cramer, H. (1926). On some classes of series used in mathematical statistics. Skandinaviske 
Matematikercongres, Copenhagen. 
Cramer, H. (1928). On the composition of elementary errors. Skand. AM., 11, 13 and 141. 
Cramer, H. (1934). Su un teorema relativo alia legge uniforme dei grandi numeri. Giorn. 1st. 
itat. jzxtt., o, jl. 
Cramer, H. (1935a). Sur les proprietes asymptotiques d'une classe de variables aleatoires. 
Comptes rendus, 201, 441. 
Cramee, H. (19356). Sugli sviluppi asintotici di funzioni di repartizione in serie di polinomi di 
Hermite. Giorn. 1st. Ital. Att., 6, 141. 
Cramer, H. (1936). tJber eine Eigenschaft der normalen Verteihmgsfunktion. Math. Zeit., 41,405. 
Cramer, H.3 and Wold, H. (1936). Some theorems on distribution functions. J. Lond. Math. 
Soc, 11, 290. 
Cramer, H. (1937). Random variables and probability distributions. Cambridge University Press. 
Cramer, H. (1938-9). Entwicklungslinien der Wahrscheinlichkeitsrechnung. 9e Gongres des 
Math. Scand., 67. 
Cramer, EL, Levy, P., and von Mises, R. (1938). Les sommes et les fonctions de variables 
aleatoires. Gonf. Int. de Sci. Math., 3. 
Crowther, G. (1934). The 'Economist' index of business activity. J.R.S.S., 97, 241. 
Crum, W. L. (1923). Cycles of rates on commercial paper. Rev. Econ. Stats., 5, 17. 
Crum, W. L. (1925). Progressive variation in seasonality. /. Am. Stat. Ass., 20, 48. 
Crum, W. L. (1933). An analytical interpretation of straw vote samples. J. Am. Stat. Ass., 
28, 152. 
Cttreton, E. E., and Dunlap, J. W. (1938). Developments in statistical methods related to test 
construction. Rev. Educ. Res., 8, 307. 
Curtiss, J. H. (1941). On the distribution of the quotient of two chance variables. Ann. Math. 
Stats., 12, 409. 
Curtiss, J. H. (1943). On transformations used in the analysis of variance. Ann. Math. Stats., 
14, 107. 
Czuber, E. (1921). Die statistische Forschungsmethode. Seidel, Wien. 
Cztjber, E. (1921, 1923). Wahrscheinlichkeitsrechnung und ihre Anwendung auf Fehlerausgleichimg, 
Statistik und Lebensversicherung. Teubner, Leipzig. 
Daly, J. F. (1940). On the unbiassed character of likelihood ratio tests for independence in normal 
systems. Ann. Math. Stats., 11, 1. 
Daniels, H. E. (1938a). The effect of departures from ideal conditions other than non-normality 
on the t- and z-tests of significance. Proc. Gamb. Phil. Soc, 34, 321. 
BIBLIOGRAPHY 455 
Daniels, H. E. (19386). Some problems of statistical interest in wool research. Supp. J.R.S.S., 
5, 89. 
Daniels, H. E. (1941). A property of the distribution of extremes. Biom., 32, 194. 
Daniels, H. E. (1944). The relation between measures of correlation in the universe of sample 
permutations. Biom., 33, 129. 
Dantzig, G. B. (1939). On a class of distributions that approach the normal distribution function. 
Ann. Math. Stats., 10, 247. 
Dantzig, G. B. (1940). On the non-existence of tests of e Student's ' hypothesis having power 
functions independent of a. Ann. Math. Stats., 11, 186. 
Darmois, G. (1928). Statistique mathematique. Octave Doin, Paris. 
Darmois, G. (1929). Analyse et comparaison des series statistiques qui se developpent dans 
le temps. Metron, 8, Nos. 1-2, 211. 
Darmois, G. (1933). Distributions statistiques rattachees a la loi de Gauss et la repartition des 
revenus. Econornetrika, 1, 159. 
Darmois, G. (1934). Sur la theorie des deux facteurs de Spearman. Comptes rendus, 199, 1176 
and 1358. 
Darmois, G. (1935). Sur les lois de probability a estimation exhaustive. Comptes rendus, 200, 
Darmois, G. (1936). Uemploi des observations statistiques. Methodes d'estimation. Actualites 
scientifiques et industrielles, No. 356. Paris. Hermann et Cie. 
Davib, P. N. (1934). On the P. test for randomness ; remarks, further illustration and table 
AH 
for Jyhlm Biom,., 26, 1. 
David, P. N. (1937). A note on unbiassed limits for the correlation coefficient. Biom., 29, 157. 
David, P. N. (1938a). Tables of the Correlation Coefficient. Cambridge University Press. 
David, P. N. (19386). Limiting distributions connected with certain methods of sampling human 
populations. Stat. lies. Mew.., 2, 09. 
David, P. N., and Neyman", J. (1938c). Extension of the Markoff theorem on least squares. 
Stat. Res. Mem,, 2, 105. 
David, P. N. (1939). On Ncyman's ' smooth ' tent for goodness of lit. I. Distribution of the 
criterion ?/r when the hypothesis tested is true. Biom,., 31, 191. 
Daviwh, G. 11. (1930). First moment correlation. J. Am. Stat. Ass., 25, 413. 
Davies, O. L. (1932). On the betas of quadrilateral distributions. Biom., 24, 498. 
Davids, O. L. (1933, 1934). On asymptotic formulae for the hypergcometric series. I. Hyper- 
geometric.', series in which the fourth element is unity. Biom., 25, 295 ; II. Ibid., 
26, 59. 
Davius, O. L., and Pearson, E. S. (1934). Methods of estimating from samples the population 
standard deviation. Supp. J.R.S.S., 1, 70. 
Davis, H. T. (editor) (1933, 1934). Tables of the Higher Mathematical Functions. Parts I and II. 
B1 < >on i i 11 g ton Press, Indian a. 
Davis, .11. T. (1933). 'Polynomial approximation by the method of least squares. Ann. Math. 
Stats., 4, 155. 
Davis, if. T. (1941). The Analysis of Economic Time Series. Bloomington Press, Indiana. 
Day, B., and Fisher, R. A. (1937). The comparison of variability in populations having unequal 
means. An example of the analysis of covarianec with multiple dependent and 
independent variat.es. Ann. Bug. Loud., 7, 333. 
I)io Finktti, B. (1929). Sidle funzioni a increments aleatorio. .Rend. R. Ace. Line, (6) 10, 163. 
de Finetti:, B. (1930a). Le funzioni carattoristiche di legge istantanea. Rend. R. Ace. Line, 
(6) 12, 278. 
de Finigttt, B., and Paciello, IT. (19306). Oalcolo dolla differenza media. Metron, 8, No. 3, 89. 
de Finetti, B. (1931). Sui metodi proposti per il calcolo dell a differenza media. Metron, 9, 
No. 1, 3. 
de Finetti, B. (1932). Sulla legge di probability degli estremi. Metron, 9, Nos. 3-4, 127. 
456 BIBLIOGRAPHY 
de Einetti, B. (1933a). Classi di numeri aleatori equivalents Bend. B. Ace. Line, (6) 18, 107 ; 
La legge dei grandi numeri nel caso dei numeri aleatori equivalents Ibid., 18, 203 ; and 
Sulla legge di distribuzione dei valori in una successione di numeri aleatori equivalenti. 
Ibid., 18, 279. 
de Finetti, B. (1933&). SulF approssimazione empirica di una legge di probabilita. Giorn. 1st. 
Ital. Abb., 4, 415. 
de Finetti, B. (1937). La prevision: ses logiques, ses sources subjectives. Ann. Inst. II. 
Poincare, 7, 1. 
de Finetti, B. (1939a). Resoconto critico del colloquio di Ginevra intorno alia teoria delle 
probabilita. Giorn. 1st. Ital. Att., 9, 1. 
de Finetti, B. (19396). La teoria del rischio e il problema della rovina dei giocatori. Giorn. 
1st. Ital. Att, 10, 41. 
del Chiaro, A. (1936). Sui momenti delle leggi di distribuzione del Polya a piu variabili. Giorn. 
1st. Ital. Att, 7, 151. 
dell' Agnola, C. A. (1937). Sulla tendenza ad una variabile casuale limite di una successione 
di variabili casuali punteggiate discontinue. Att. 1st. Veneto Sci., 96, 365. 
De Lury, D. (1938). Note on correlations. Ann. Math. Stats., 9, 149. 
del Vecchio, E. (1933). Sulla dipendenza statistica. Giorn. 1st. Ital. Att., 4, 235. 
Deming, W. E. (1931, 1934, 1935). On the application of least squares. I. Phil. Mag., (7), 
11, 146; II. Ibid., 17, 804; III. Ibid., 19, 389. 
Deming, W. E. (1934, 1938). The chi-test and curve fitting. J. Am. Stat. Ass., 29, 372 ; and : 
Some thoughts on curve fitting and the chi-square test. Ibid., 33, 543. 
Deming, W. E., and Birge, R. T. (1934). On the statistical theory of errors. Rev. Mod. Phys., 
6, No. 3, 122. 
Deming, W. E. (1937). On the significant figures of least squares and correlations. Science, 
85, 451. 
Demotvre, A. (1718). The Doctrine of Chances. (3rd edition, 1756.) 
Denk, F. (1936). tjber den Aufbau der Permutation geordnete Elemente. J. fur Math., 
176, 18. 
Derkson, J. B. D. (1939). On some infinite series introduced by Tschuprow. Ann. Math. Stats., 
10, 380. 
Detroit Edison Co. Statistical Department (1930). A mathematical theory of seasonal 
indices. Ann. Math. Stats., 1, 57. 
de Vergottini, M. (1936). Relazioni fra gli indici di variabilitd dei fenomeni collettivi composti 
e quelli dei fenomeni collettivi semplici. Eailli, Borne. 
Dieuleeait, C. E. (1934a). Contribution a l'etude de la theorie de la correlation. Biom., 26, 371). 
Dieuleeait, C. E. (1934&). Sur les developpements des fonctions des frequences en series de 
fonctions orthogonales. Metron, 11, No. 4, 77. 
Dieulefait, C. E. (1935a). Sur la correlation au sens des modes. Gomptes rendus, 200, 1511. 
Dieuleeait, C. E. (19356). Generalisation des courbes de K. Pearson. Metron, 12, No. 2, 95. 
Dixon, W. J. (1940). A criterion for testing the hypothesis that two samples are from the same 
population. Ann. Math. Stats., 11, 199. 
Dixon, W. J. (1944). Further contributions to the problem of serial correlation. Ann. Math. 
Stats., 15, 119. 
Dodd, E. L. (1923). The greatest and the least variate under general laws of error. Trans. 
Am. Math. Soc, 25, 525. 
Dodd, E. L. (1926). The convergence of a general mean of measurements to the true value. 
Bull Am. Math. Soc, 32, 282. 
Dodd, E. L. (1927). The convergence of general means and the invariance of form of certain 
frequency functions. Am. J. Maths., 49, 215. 
Dodd, E. L. (1930). The use of linear functions to detect hidden periods in data separated into 
small sets. Ann. Math. Stats., 1, 205. 
BIBLIOGRAPHY 457 
Dodd, E. L. (1931). Classification of sizes and measures by frequency functions. J. Am. Stat. 
Ass., 26, 277. 
Dodd, E. L. (1934). The complete independence of certain properties of means. Ann. Math.r 
35, 740. 
Dodd, E. L. (1937a). Internal and external means arising from the scaling of frequency functions. 
Ann. Math. Stats., 8, 12. 
Dodd, E. L. (19376). Regression coefficients as means of certain ratios. Am. Math. Monthly,. 
44, 306. 
Dodd, E. L. (1937c). Index numbers and regression coefficients as means, internal and external. 
Rep. Third Ann. Res. Conf. Boon. Stat., Colorado Springs, 13. 
Dodd, E. L. (1.938). Interior and exterior means obtained by the method of moments. Ann. 
jlvl azih. ozazs., s, x t)o. 
Dodd, E. L. (1939a). The length of the cycles which result from the graduation of chance elements. 
Ann. Math. Stats., 10, 254. 
Dodd, E. L. (19396). Periodogram analysis with the phase a chance variable. Econometrika, 
7, 57. 
Dodd, E. L. (1941a). The problem of assigning a length to the cycle to be found in a simple 
moving average and in a double moving average of chance data. Econometrika, 
9, 25. 
Dodd, E. L. (19416). The cyclic effects of linear graduations persisting in the differences of the 
graduated values. Ann. Math. Stats., 12, 127. 
Dodd, E. L. (1942). Certain tests for randomness applied to data grouped into small sets. Econo- 
metrika, 10, 249. 
Dodd, S. 0. (1927). On criteria for iactorising correlated variables. Biom., 19, 45. 
DoEBLiN, W. (1936, 1937). Sur les chainos discretes de Markoff. Com/ptes rendus, 203, 24 and 
1210 ; and : Elements d'une theoric generale des chaines eonstantes simples de Markoff. 
Ibid, 205, 7. 
Doeblin, W. (1937). 8ur le can eontinu des probabilites en chaine. Rend. R. Ace. Line, 25, 
170; Le cas discontinu des probabilites en chaine. Pub. Fac. Sci. Univ. Masaryk, 
No. 236, 3 ; and (with R. Fortct) : Sur des chaines a liaisons completes. Bull. Soc. 
Math. Franco, 65, 132. 
Doeblin, W. (1938). Premiers elements (Tune etude systematique de l'cnsemble de puissances 
(Fune loi de probabilite. CompUs rrndus, 206, 300 ; and : Etude de l'cnsemble de 
puissances (rune loi de probabilite. Ibid., 206, 718. 
Dokbmn, W. (1938, 1939). Sur les sommes (Tun grand nombrc de vecteurs aleatoires. Comptes 
rendus, 207, 511 ; Sur certains mouvements aleatoires. .Ibid., 208, 249 ; Sur les sommes 
d'un grand nombre de variables aleatoires indopendantes. Bull. Sci. Math., (2), 63, 
23 and 35. 
l)oi<yrs(ui, G. (1934). Die in dor Statistik seltener Wroignisso auftretenden Oharlierschen Polynome 
und eine damit zusammonhaiigen.de 1 )ifferen/JaldifIerenzglcichung. Math. Ann., 109, 257. 
Donnish, O. (1928). Die Saisonschuxmkungen als Problem der Konjimkturforschung. Vierteljahr- 
h often zur Koiijiinkturlbrsehung, Sonderheft 6. Hobbing, Berlin. 
Doob, J. L. (1934^). Stochastic processes and statistics. Proc Nat. Acad. Sci., 20, 376. 
Doob, J. L. (19346). .Probability and statistics. Trans. Am. Math. Soc, 36, 759. 
Doob, J. L. (1935). The limiting distributions of certain statistics. Ann. Math. Stats., 6, 160. 
Doob, J. L. (1930). Statistical estimation. Trans. Am. Math. Soc, 39, 410. 
Doob, J. L. (1937). Stochastic processes depending on a continuous parameter. Trans. Am. 
Math, Soc, 42, 107. 
Doob, J. L. (1938). Stochastic processes with an integral-valued parameter. Trans. Am. Math. 
Soc, 44, 87. 
Doob, J. L. (1941). Probability as measure. Ann. Math. Stats., 12, 206 (followed by discussion, 
Doob and von Mises, 12, 215). 
458 BIBLIOGRAPHY 
Doodson, A. T. (1917). Relation of the Mode, Median and Mean in frequency curves. Biam., 
11, 429 
Kobge, K. (1934). Eine Axiomatisierung der von Misesschen Wahrsehoinliehkei1,sfh< ■<>.•!<■. Jher. 
dtsch. Mat. Ver., 43, 39. ...... , 
Dobge, K. (1936). Zu der von R. v. Mises gegebenen Begrimdung dor Wakrschcmhehk.n srech- 
nung. Zweite Mitteilung. AUgemeine Wahrscheinlichkeitstheone. Mnlh. Ant., 40, 
161 
Dressel, P. L. (1940). Statistical seminvariants and their estimates, with particular emphasis 
on their relation to algebraic invariants. Ann. Math. State., 11, «>•>• 
Dressel, P. L. (1941). A symmetric method for obtaining unbiased estimates and expeeted 
values. Ann. Math. Stats., 12, 84. 
Dublin, L. I., Lotka, A. J., and Spiegelman, M. (1935). The construction of life tables by 
correlation. Metron, 12, No. 2, 121. 
Dubois, P. (1939). Formulas and tables for rank correlation. Psych. AV<*., 3, 40. 
Dubobdieu, J. (1939). Theorie de Vassurance-maladie. Paris. 
Dugue, D. (1936a). Sur le maximum de precision des estimations gaussiennes a ia limit e. ("am pies 
rendus, 202, 193; and: Sur le maximum de precision des lois limits dVstimation. 
Ibid., 202, 452. 
Dugue, D. (19366). Sur certaines modes de convergence de lois d'estimation. (\mpfrs rnuim, 
202, 1732. 
Dugue, D. (1937a). Sur une extension de la loi des grands nombres. (Jomptrs rnuius, 204, 317. 
Dugue, D. (19376). Application des proprietes de la limite au sens du ealeui des probability 
a l'etude des diverses questions d'estimation. J. Scole Poly., 3, No. 4, 305. 
Dugue, D. (1939). Sur quelques proprietes analytiques des fonetions caraeOVistiquos. Camptvx 
rendus, 208, 1778. 
Duistlap, H. F. (1931). An empirical determination of the distribution of moans, standard 
deviations and correlation coefficients drawn from rectangular populations. Ann. Math, 
Stats., 2, 66. 
Dwyer, P. S. (1937a). Moments of any rational integral isobarie sample moment ftmelion. 
Ann. Math. Stats., 8, 21. 
Dwyer, P. S. (19376). The simultaneous computation of groups of regression equations and 
associated multiple regression coefficients. Ann. Math. Slats., 8, 22-J. 
Dwyer, P. S. (1938). Combined expansions of products of symmetrica power sums and of sums 
of symmetric power products with application to sampling Ann. Math. Statu., 
9, 1 and 97. 
Dwyer, P. S. (1940). Combinatorial formulas for the rth standard moment of the .sample sum, 
of the sample mean and of the normal curve. Ann. Math. Stats-., U, 3f>3. 
Dwyer, P. S. (1941a). The solution of simultaneous equations. Pw/rhnmrlrifoi, 6, 101. 
Dwyer, P. S. (19416). The Doolittle technique. Ann. Math, Stats'., 12, 41!). 
Dwyer, P. S. (1941c). The skewness of the residuals in linear recession theory. Ann Math. 
Stats., 12, 104. 
Dwyer, P. S. (1942). Recent developments in correlation technique. ./. Am. Stat, Ass., 37, III. 
Ede*, T., and Yates, F. (1933). On the validity of Fisher's ,-test when appli,d to an aetuai 
example of non-normal data. J. Agr. Sci., 23, (>. 
Ebgett, G L. (1931) Frequency distributions with given statistics which are not all nmmmtH. 
Metron, 9, No. 2, 25. 
Edgeworth, F. Y., generally ; see Bowley (1928) 
Edgeworth, F. Y. (1905) The Law of Error. Trans. Garni, Phil. Sac, 20, 3f» and 113 (with 
an Appendix not printed m the T.G.P.S. but issued with reprints), 
Ebgeworoj, F.^Y. (1906). The generalised law of error, or law of great numbers. ./.«.*.«., 
BIBLIOGRAPHY 459 
►geworth, F. Y. (1908, 1909). On the probable errors of frequency constants. J.E.S.S., 
71, 381, 499, 651, and 72, 81. 
►oewokth, F. Y. (1925a). Article ' Index numbers ' in Palgrave's Dictionary of Political 
Economy, vol. 2, Macmillan. 
KiEWoRTH, F. Y. (19256). The plurality of index-numbers. Econ. J., 35, 379. 
kjeworth, F. Y. (1925c). The element of probability in index numbers. J.R.S.S., 88, 557. 
ills, W. C. (1929). Formulas for probable errors of coefficients of correlation. J. Am. Stat. 
.is-.y., 24, 170. 
oenberoer, F. (1924). Die Wahrscheinlichkeitsansteckung. Mitt. Verein. Schweiz. Versich. 
Math., Heft 19, 31. 
sbnhart, C. (1938). The power function of the #2~test. Bull. Am. Math. Soc, 44, 32. 
3ENHART, C. (1939). The interpretation of certain regression methods and their use in biological 
and industrial research. Ann. Math. Stats., 10, 162. 
derton, E. M. (1933). The Lanarkshire Milk Experiment. Ann. Fug. Lond., 5, 326. 
derton, W. P. (1933). Adjustments for the moments of J-shaped curves. Biom., 25, 179. 
DERTON, W. P., and Hansmann, G. H. (1934). Improvement of curves fitted by the method 
of moments. J.M.S.S., 97, 330. 
derton, Sir W. P. (1938a). Frequency Curves and Correlation, 3rd edn. Cambridge 
University Press. 
■DiiiRToN, Sir W. P. (19386). Correzioni doi momenti quando la curva e simmetrica. Giom. 
1st. Hal Att,, 16, 145. 
evin(j, (.{. (1937, 1938). Zur Theorie dor Markoffschen Ketten. Acta. Soc. Sd. Fennicae, 
2, 1 ; and : Tiber die Interpretation von Markoffschen Ketten. Soc. Sci: Fennicae 
Comment, phys.-nat., 10, No. 3, 1. 
. Sua n any any, M. R. (1936). An illustration of the accuracy of the ^-approximation. Biom., 
28, 179. 
imett, \V. (I. (1936). Sampling error and the two-factor theory. Brit. J. Psych., 26, 362. 
[qeliiart, M. I). (1936). The technique of path coefficients. Psychometrika, 1, 287. 
;i>i';lyi, A. (1937). Sullc connessioni fra duo problem! di ealcolo deile probabilita. Giom. 
1st. Hal. At I., 8, 32S. 
:di';lyi, A. (I93S). Cher cine orzeugende Funktion von Produkten Hermitescher Polynome. 
Math. Zeit., 44, 201. 
:nos, 1'., and Turan, P. (1937, I93S). On Interpolation. I. Quadrature and 
mean-convergence in the Lagrange interpolation. Ann. Math., 38, 142; and II. On the distribution 
of fundamental points of Lagrange and llermife interpolation. Ibid., 39, 703. 
:nos, I*., and Ka<^ M. (1939). On the Oaussian law of errors in (lie theory of additive functions. 
/W. Nat. Acad. Sci., 25, 200. 
:i)(")S, l\ (1939). On the smoothness of the asymptotic distribution of additive arithmetical 
functions. Am. J. Math., 61, 722. 
tnos, 1\, and Wintn er, A. (1939). Additive arithmetical functions and statistical independence. 
.1///. ./. Math-*., 61, 713. 
soinok, l<\ (1932). On the probability function in the collective theory of risk. Skand. AM., 
15, 17f). 
iuw, L. (17S2). Reeherches sur une nouvello espcec do quarres magiques. Verh. v. h. Zeeuwsch 
Gcnoolsch. der IVefensch. ll,issingen, Sf>. 
rRAnn, IL (1938a). Sur quelques lois d'erreurs a deux dimensions. Convptes rendus, 206, 402. 
'RAim, II. (1938/;). Sur certaines decompositions en aleatoires imaginaires. Comptes rendus, 
206, 723. 
:kknck, if. J. (1939). The validity of judgments as a function of the number of judges. 
./. Exp. Psych., 25, 650. 
hkiel, M. (1930a). Methods of Correlation Analysis. John Wiley and Sons, New York. 
(Chapman and Hall, London.) 
460 BIBLIOGRAPHY 
Ezekiel, M. (19306). The sampling variability of linear and curvilinear regression. Ann. Math. 
Stats., 1, 275. 
Falkner, H. B. (1924). On the measurement of seasonal variations. J. Am. Stat. Ass., 19, 167. 
Fare, W. (1919, 1920). Farr's law of density in relation to death rates. J.R.S.S., 82, 45, and 
83, 280. 
Fechner, G. T. (1897). KolleUivmasslehre. Engelmann, Leipzig. 
Feld, W. (1924). Internationale Bibliographie der Statistik der Kindersterblichkeit. Metron, 
3, Nos. 3-4, 604. 
Feldheim, E. (1936a). Sur rorthogonalite des fonctions fondamentales de ^interpolation de 
Lagrange. Comptes rendus, 203, 650. 
Feldheim, E. (19366). Sur les probabilites en chaine. Math. Ann., 112, 775. 
Feldheim, E. (1937a). Sulle legge di probabilita stabili a due variabili. Giorn. 1st. Ital. Att., 
8, 146. 
Feldheim, E. (19376). Applicazioni dei polinomidi Hermite a qualche problema di calcolo delle 
probabilita. Oiorn. 1st. Ital. AM., 8, 303. 
Feldman, H. M. (1935). Mathematical expectation of product-moments of samples drawn from 
a set of infinite populations. Ann. Math. Stats., 6, 30. 
Feller, W. (1936a). Zur Theorie der stochastischer Prozesse. Math. Ann., 113, 113. 
Feller, W. (19366, 1937). Uber den zentralen Grenzwertsatz der Wahr hkeitsrechiumg. 
Math. Zeit., 40, 521, and 42, 301. 
Feller, W. (1937). tJber das Gesetz der grossen Zahlen. Acta Lilt. Sci. Szeged, 8, 191. 
Feller, W. (1938). Note on regions similar to the sample space. Stat. lies. Mmti., 2, 1.17. 
Feller, W. (1943). On a general class of ' contagious ' distributions. Ann. Math. Stats., 14, 389. 
Fertig, J. W. (1936). On a method of testing the hypothesis that an observed sample of 
n variables and of size N has been drawn from a specified population of the same 
number of variables. Ann. Math. Stats., 7, 113. 
Fertig, J. W., and Proehl, E. A. (1937). A test of a sample variance based on both tail ends 
of the distribution. Ann. Math. Stats., 8, 193. 
Fieller, E. C. (1931a). The duration of play. Biom., 22, 377. 
Fieller, E. C. (19316). A problem in probability. Biom,, 22, 425. 
Fieller, E. C. (1931c). The game of heads and tails. Biom., 23, 419. 
Fieller, E. C. (1932a). Numerical test of the adequacy of A. T. McKay's approximation, 
J.R.S.S., 95, 699. 
Fieller, E. C. (19326). The distribution of an index in a normal bivariate population. Biom., 
24, 428. 
Fieller, E. C. (1940). The biological standardisation of insulin, tiupp. J.R.S.S., 7, I. 
Finney, D. J. (1938). The distribution of the ratio of estimates of the two variances in a sample 
from a normal bivariate population. Biom., 30, 190. 
Finney, D. J. (1940, 1941, 1942). The detection of linkage. Ann. Bug. Loud., 10, 171 ; 11, 10 ; 
11, 115; 12, 31. 
Finney, D. J. (1941a). The joint distribution of variance ratios based on a common-error mean 
square. Ann. Eug. Bond., 11, 136. 
Finney, D. J. (19416). On the distribution of a variate whose logarithm is normally distributed. 
Supp. J.R.S.S., 7, 155. 
Fischer, C. H. (1933a). On correlation surfaces of sums with a certain number of random elements 
in common. Ann. Math. Stats., 4, 103. 
Fischer, C. H. (19336). On multiple and partial correlation coefficients of a certain sequence of 
sums. Ann. Math. Stats., 4, 278. 
Fisher, A. (1922). The Mathematical Theory of Probabilities and its application to Frequency- 
curves and Statistical Methods. 2nd edn. Macmillan, New York. 
Fisher, Irving (1922). The Making of Index Numbers. Houghton Mifflin, Boston and New York. 
BIBLIOGRAPHY 461 
Fisher, R. A. (1912). On an absolute criterion for fitting frequency curves. Mess. Maths., 
Fisher, R. A. (1915). Frequency-distribution of the values of the correlation coefficient in samples 
from an indefinitely large population. Biom., 10, 507. 
Fisher, R. A. (1918). The correlation between relatives on the supposition of Mendelian 
inheritance. Trans. Roy. Soc. Edin., 52, 399. 
Fisher, R. A. (1920). A mathematical examination of the methods of determining the accuracy 
of an observation by the mean error and by the mean square error. Month, Not. R. 
Astr. Soc, 80, 758/ 
Fisher, R. A. (1921a). On the mathematical foundations of theoretical statistics. Phil. Trans. 
Roy. Soc, A, 222, 309. 
Fisher, R. A. (19216). Studies in crop-variation. I. An examination of the yield of dressed 
grain from Broadbalk. J. Agr. Sci., 11, 107. 
Fisher, R. A. (1921c). On the probable error of a coefficient of correlation deduced from a small 
sample. Metron, 1, No. 4, 1. 
Fisher, R. A. (1922a). On the interpretation of %- from contingency tables and the calculation 
of P. J.R.S.S., 85, 87. 
Fisher, R. A. (19226). The goodness of fit of regression formulae and the distribution of regression 
coefficients. J.R.S.S., 85, 597. 
Fisher, R. A., Thornton, H. G., and Mackenzie, W. A. (1922c). The accuracy of the plating 
method of estimating the density of bacterial populations. Ann. App. Biol., 9, 325. 
Fisher, R. A. (1923). Statistical tests of agreement between observation and hypothesis. 
Economica, 3, 139. 
Fisher, R. A. (1924a). The distribution of the partial correlation coefficient. Metron, 3, 329. 
Fisher, R. A. (19246). The influence of rainfall on the yield of wheat at Rothamstecl. Phil. 
Trans. Roy. Soc, B, 213, 89. 
Fisher, R. A. (1924c). On a distribution yielding the error functions of several well-known 
statistics. Proc hit. Math. Congress, Toronto, p. 805. 
Fisher, R. A. (1924a'). The conditions under which %* measures the discrepancy between 
observation and hypothesis. J.R.S.S., 87, 442. 
Fisher, R. A. (1925a, 1944). Statistical Methods for Research Workers. (1st edn. 1925. 9th 
edn. 1944). Oliver and Boyd, Edinburgh. 
Fisher, R. A. (19256). Theory of statistical estimation. Proc. (Jamb. Phil. Soc, 22, 700. 
Fishicr, R. A. (1920a). Applications of ' Student's ' distribution. Metron, 5, No. 3, 90 ; and : 
Expansion of ' Student's ' integral in powers of n L . Metron, 5, No. 3, 109. 
Fish MR, R. A. (19266). On the random sequence. Q.J. Rot/. Met. Soc, 52, 250. 
Fisher, R. A. (1920c). Bayes' theorem and the fourfold table. Eugenics Review, 18, 32. 
Fisher, R. A., and Wishart, J. (1927). On the*, distribution of the error of an interpolated value 
and on the construction of tables. Proc. (Uimb. Phil. Soc, 23, 912. 
Fisher, R. A., and Tifpett, L. H. 0. (1928a). Limiting forms of the frequency-distribution of 
the largest or smallest member of a sample. Proc (Jamh. Phil. Soc, 24, 180. 
Fisher, R. A. (19286). The general sampling distribution of the multiple correlation coefficient. 
Proc. Roy. Soc, A, 121, 054. 
Fisher, R. A. (1928c). On a property connecting the %2 measure of discrepancy with the method 
of maximum likelihood. Atti di (Jongresso Int. del Matematici, Bologna, 6, 94. 
Fisher, R. A. (1929a). Tests of significance in harmonic analysis. Proc Roy. Soc, A, 
Fisher, R. A. (19296). Moments and product-moments of sampling distributions. Proc Land. 
Math. Soc, (2), 30, 199. 
Fisher, R. A. (1930a). Inverse Probability. Proc Oamh. Phil. Soc, 26, 528. 
Fisher, R. A. (19306). The moments of the distribution for normal samples of measures of 
departure from normality. Proc Roy. Soc, A, 130, 10. 
462 BIBLIOGRAPHY 
Fisher, R. A., and Wishart, J. (1931). The derivation of the pattern formulae of two-way 
partitions from those of simpler patterns. Proc. Lond. Math. Soc, 33, 195. 
Fisher, R. A. (1932). Inverse probability and the use of likelihood. Proc. Oamb. Phil Soc, 
28, 257. 
Fisher, R. A. (1933). The concepts of inverse probability and of fiducial probability referring 
to unknown parameters. Proc. Roy. Soc, A, 139, 343. 
Fisher, R. A. (1934a). Two new properties of mathematical likelihood. Proc. Roy. Soc, A, 
' 144, 285.. 
Fisher, R. A. (19346). Probability, likelihood and quantity of information in the logic ol uncertain 
inference. Proc. Roy. Soc, A, 146, 1. 
Fisher, R. A., and Yates, F. (1934c). The 6 X 6 Latin square. Proc Oamb. Phil. Soc, 30, 492. 
Fisher, R. A. (1934d)'. The effect of methods of ascertainment upon the estimation of frequencies. 
Ann. Eug. Lond., 6, 13. 
Fisher, R. A. (1935a). The logic of inductive inference. J.R.S.S., 98, 39. 
Fisher, R. A. (19356). The fiducial argument in statistical inference. Ann. Eug. Lond., 6, 391. 
Fisher, R. A. (1935c, 1942). The Design of Experiments (1st edn. 1935, 3rd edn. 1942). Oliver 
and Boyd, Edinburgh. 
Fisher, R. A. (1936a). The use of multiple measurements in taxonomic problems. Ann. Eug. 
Lond., 7, 179. 
Fisher, R. A. (19366). The coefficient of racial likeness. /. Roy. Anthrop. Soc, 66, 57. 
Fisher, R. A. (1936c). Uncertain inference. Proc Roy. Soc, B, 122, 1. 
Fisher, R. A. (1937a). Professor Karl Pearson and the method of moments. Ami. Eug. Loud., 
7, 303. 
Fisher, R. A. (19376). On a point raised by M. S. Bartlett on fiducial probability. Ann. Eug. 
Lond., 7, 370. 
Fisher, R. A., and Yates, F. (1938a, 1942). Statistical Tables for use. in Biological, Agricultural 
and Medical Research. 2nd edn. 1942. Oliver and Boyd, Edinburgh. 
Fisher, R. A. (19386). Quelques remarques sur l'estimation statistique. Biotypologic, 6, ITul. 
Fisher, R. A. (1938c). The statistical utilisation of multiple measurements. Ann. Eug. Lond., 
8, 376. 
Fisher, R. A. (1938c?). Statistical Theory of Estimation. Calcutta Readership Lectures. 
Published by the University of Calcutta. 
Fisher, R. A. (1939a). The comparison of samples with possibly unequal variances. Ann. 
Eug. Lond., 9, 174. 
Fisher, R. A. (19396). The sampling distribution of some statistics obtained from non-linear 
equations. Ann. Eug. Lond., 9, 238. 
Fisher, R. A. (1940a). On the similarity of the distributions found for the test of significance 
in harmonic analysis and in Stevens's problem in geometrical probability. Ann. Eug. 
Lond., 10, 14. 
Fisher, R. A. (19406). An examination of the different possible solutions of a problem in 
incomplete blocks. Ann. Eug. Lond., 10, 52. 
Fisher, R. A. (1940c). A note on fiducial inference. Ann. Math. Stats., 10, 383. 
Fisher, R. A. (1940<#). The precision of discriminant functions. Ann. Eug. Lond., 10, 422. 
Fisher, R. A. (1941a). The asymptotic approach to Behrens' integral with further tables for 
the rf-test of significance. Ann. Eug. Lond., 11, 141. 
Fisher, R. A. (19416). The negative binomial distribution. Ann. Eug. Lond., 11, 182. 
Fisher, R. A. (1942a). New cyclic solutions to problems in incomplete blocks. Ann. Euq Lond 
11, 290. 
Fisher, R. A. (19426). The likelihood solution of a problem in compounded probabilities Ann 
Eug. Lond., 11, 306. 
Fisher, R. A. (1942c). The theory of confounding in factorial experiments in relation to the 
theory of groups. Ann. Eug. Lond., 11, 341. 
BIBLIOGRAPHY 463 
Fisher, R. A. (1942d). Some combinatorial theorems and enumerations connected with the 
numbers of diagonal types in a Latin square. Ann. Eug. Lond., 11, 395. 
iiSHBR, R. A. (1942e). Completely orthogonal 9x9 squares : a correction. Ann. Eug. Lond., 
11, 402. "■ 
Flux, A. W. (1921, 1933). The measurement of price changes. J. Roy. Stat. Soc. 84, 167, 
and 96, 606. 
Fortbt, R. (1935-8). Sur les probabilites en chaine. Comptes rendus, 201, 184, 202, 1362, and 
204, 315 ; and : Sur Fiteration des substitutions algebriques lineaires a une infinite de 
variables et ses applications a la theorie des probabilites en chaine. Rev. Ci., Lima, 
40, 185, 337, 481. 
Fkankkl, A., and Kullback, S. (1940). A simple sampling experiment on confidence intervals. 
Ann. Math.- Stats., 11, 209. 
Frankki,, L. R., and Hotelling, H. (1938). The transformation of statistics to simplify their 
distribution. Ann. Math. Stats., 9, 87. 
Frankrl, L. R., and Stock, J. S. (1939). The allocation of samplings among several strata. 
Ann. Math. Stats., 10, 288. 
FKfiCHKT, M. (1930). Sur la convergence en probabilite. Metron, 8, No. 4, 3. 
Fatalnt, M., and Shohat, J. (1931). A proof of the generalised second-limit theorem. Trans. 
Am. Math. Soc, 33, 533. 
FRfkiHET, M. (1933). Sur le coefficient, dit de correlation, et sur la correlation en general. Rev. 
Inst. Int. Stat., 4, 1. 
FrIwhkt, M. (1935). Sur Tequation fonctionnelle de Chapman et sur le probleme des probabilites 
en chaine. Proc. Lond. Math. Soc, 39, 515. 
FrIcchmt, M. (1930a.). Bull' espressione esatta di uno scarto medio. Giorn. 1st. Ital. Att., 
6, 164. 
FufcriiKT, M. (19306). Sul caso positivamente regolare nel problema delle probability concatenate. 
(Horn. I.st. Ital. Ait., 7, 28. 
FrIwhkt, M. (1937a). Sulla mescolanza delle palline e sulle leggi-limite delle probabilita. Qiorn. 
1st. Ital. Att,, 8, 14. 
FrAchkt, M. (19376). Recherches theoriqucs modernes. (Part of the Traite edited, by Borel.) 
< Janlhier-Villars, Paris. 
Frh'Kkv, K. (1037). The theory of index-number bias. Rev. Econ. Stat., 19, 161. 
Frikdman, M. (1937). The use of ranks to avoid the assumption of normality implicit in the 
analysis of variance. ./. Am. Stat. Ass., 32, 675. 
Frikdman, M. (1940). A comparison of alternative tests of significance for the problem of 
m, rankings. Ann. Math. Stats., 11, 80. 
Frisch, R. (1926). Sur les semi-invariants et moments employes dans 1'etude des distributions 
statistiques. Oslo, Skrifler af del Norsk?, Videnskaps Academic, II, Hist.-Filos. Klasse, 
No. 3. 
Frinch, II. (192K). ('hanging harmonics and other general types of components in empirical 
series. Skand. A/ct., 11, 220. 
Frikcu, R. (1929). Correlation and scatter in statistical variables. Nordish. Stat. J., 1, 36. 
Frisch, li. (1930). Necessary and sufficient conditions regarding the form of an index-number 
which shall meet certain of Fisher's tests. J. Am. Stat. Ass., 25, 397. 
Frisoh, R. (1931). A method of decomposing an empirical series into its cyclical and progressive 
('omj)onents. J. Aw,. Stat. Ass., 26, Supp. p. 73. 
Fiuscn, 11., and Mudubtt, B. D. (1931). Statistical correlation and the theory of cluster types. 
,/. Aw,. Stat, Ass., 26, 375. 
Fkik<w, R. (1932). On the use of difference equations in the study of frequency-distributions. 
Matron, 10, No. 3, 35. 
Fhikc'H, R. (1933). Propagation problems and impulse problems in dynamic economics. Economic 
Essays in honour of Gustav Cassel. London. 
464 BIBLIOGRAPHY 
Frisch, R. (1934a). Robert Schmidt's definition of skewness and kurtosis. Econometrika, 
2, 221. 
Frisch, R. (19346). Statistical confluence analysis by means of complete regression equations. 
Publication No. 5, Universitets Okonomiske Institut, Oslo. 
Frisch, R. (1936). Annual survey of general economic theory. The problem of index numbers. 
Econometrika, 4, 1. 
Frisch, R. (1938). On the inversion of moving averages. Skand. AM., 21, 218. 
Fry, T. C. (1928). Probability and its Engineering Uses, van Nostrand, New York. 
Fry, T. C. (1938). The #2-test of significance. J. Am. Stat. Ass., 33, 513. 
Galton, Sir. Francis (1886). Regression towards mediocrity in hereditary stature. J. Anthrop. 
Inst., 15, 246 ; and : Family likeness in stature. Proc. Boy. Soc, A, 40, 42. 
Galton, Sir Francis (1902). The most suitable proportion between the values of first and second 
prizes. Biom., 1, 385. 
Galvani, L. (1931). Contributi alia determinazione degli indici di variabilita per alcuni tipi di 
distribuzione. Metron, 9, No. 1, 3. 
•Galvani, L. (1932). Sulle curve di concentrazione relative a caratteri limitate e non limitate. 
Metron, 10, No. 3, 61. 
Garner, R. (1932). Concerning the limits of a measure of skewness. Ann. Math. Stats., 3, 358. 
Garwood, F. (1933). The probability integral of the correlation coefficient in samples from a 
normal bivariate population. Biom., 25, 71. 
Garwood, F. (1936). Fiducial limits for the Poisson distribution. Biom., 28, 437. 
Garwood, F. (1940). An application of the theory of probability to the operation of vehicular- 
controlled traffic signals. Supp. J.R.S.S., 7, 65. 
Garwood, F. (1941). The application of maximum likelihood to dosage-mortality curves. Biom., 
32, 46. 
Geary, R. C. (1927). Some properties of correlation and regression in a limited universe. Metron, 
7, No. 1, 83. 
Geary, R. C. (1930). The frequency distribution of the quotient of two normal variables, 
J.R.S.S., 93, 442. 
Geary, R. C. (1933). A general expression for the moments of certain symmetrical functions of 
normal samples. Biom., 25, 184. 
Geary, R. C. (1935a). The ratio of the mean deviation to the standard deviation as a test of 
normality. Biom., 27, 310. 
Geary, R. C. (19356). Note on the correlation between /32 and w'. Biom., 27, 353. 
Geary, R. C. (1936a). Moments of the ratio of the mean deviation to the standard deviation for 
normal samples. Biom., 28, 295. 
Geary, R. C. (19366). The distribution of ' Student's ' ratio for non-normal samples. Supp. 
J.R.S.S., 3, 178. 
Geary, R. C, and Pearson, E. S. (1938). Tests of Normality. Biometrika Office, London. 
Geary, R. C. (1942). The estimation of many parameters. J.R.S.S., 105, 213. 
Geary, R. C. (1943). Minimum range for quasi-normal distributions. Biom., 33, 100. 
Geary, R. C. (1944). Comparison of the concepts of efficiency and closeness for consistent 
estimates of a parameter. Biom., 33, 123, 
Gehlke, C. E., and Biehl, K. (1934). Certain effects of grouping upon the size of the correlation 
coefficient in census tract material. /. Am. Stat. Ass., 29, Supp., 169. 
Geiringer, H. (1933). Korrelationsmessung auf Grund der Summenfunktion. Zeit. ang. Math. 
und Mech., 13, 121. 
Geiringer, H. (1934). Une methode generale de statistique theorique. Comptes rendus, 198, 
420; and: Applications. Ibid., 198, 696. 
Getringer, H. (1938). On the probability theory of arbitrarily linked events. Ann. Math. 
Slats., 9, 260 (and Errata, 10, 202). 
BIBLIOGRAPHY 465 
Ueiringer, H. (1942). A new explanation of non-normal dispersion in the Lexis theory. 
Econometrika, 10, 53. 
Gini, 0. (1912). Variability e Mutability, contributo alio studio delle distribuzioni e relazioni 
statistiehe. Studi Economico-Giuridici delle It. University di Cagliari. 
Uini, (1 (1.9 10). fndicidi concordanza. Atti R. 1st. Veneto di Sci. Lett, ed Arte. 
(.3ini, 0. (1921). Still' interpolazione di una retta quando i valori della variabile indipendente 
sono affotti da errori accidental!. Metron, 1, No. 3, 63. 
Gini, (J., and (Ialvani, L. (1929). Di talnne estensioni del concetto di media ai caratteri quali- 
tativL Metron, 8, Nos. 1-2, 3. 
Gini, C. (1930). Sul massimo degli indici di variability assoluta, etc. Metron, 8, No. 3, 3. 
Gini, C. (1932). Intorno alle curve di concentrazione. Metron, 9, Nos. 3-4, 3. 
Gm, C. (1938). Di una formola comprensiva delle medie. Metron, 13, No. 2, 3. 
Gini, G, and Zappa, G. (1938). Sulleproprieta delle medie potenziate e combinatorie. Metron, 
Gini, C. (1939). Sulla deterrainazione dell' indice di cograduazione. Metron, 13, No. 4, 41. 
Girshik, M. A. (1936). Principal components. J. Am. Stat. Ass., 31, 519. 
Girshik, M. A. (1939). On the sampling theory of the roots of determinantal equations. Ann. 
Math. Stats., 10, 203. 
Gikshik, M. A. (1942). Note on the distribution of roots of a polynomial with random complex 
coeilieients. Ann. Math. Stats., 13, 235 ; Correction, ibid., 13, 447. 
Glivknko, V. (1933). Sulla determinazione empirica delle leggi di probability. Giorn. 1st. 
ItaL Att., 4, 92. 
Glivknko, V. (1930). Sul teorema limite della teoria delle funzioni caratteristiche. Giorn. 
1st. ItaL Att., 7, 1.00. 
Gnkuknko, B. (1938). Ob or die Konvergonz der Verteilungsgesetze von Summen voneinander 
unnbhangiger Summanden. C.R. Acad. Sci. U.S.S.E., 18, 231. 
GoNiN, II. T. (1930). The use of factorial moments in the treatment of the hypergeometric 
distribution and in tests for regression. Phil. Mag., (7), 21, 215. 
Gordon, It. A. (1937). A selected bibliography of the literature of economic fluctuations. Rev. 
Kcon. Stat., 19, 37. 
Gordon, It. !>. (1939). Estimating bacterial populations by the dilution method. Biom., 31, 
l(w. 
Gordon, It. I>. (1941). The estimation of a quotient when the denominator is normally 
distributed. Attn. Math. Stats., 12, 115. 
Got \ as, 1\ (1930). Eormules dc recurrence pour les semi-invariants a quelques lois de 
distribution a plusieurs variables. Cotn.ptes rvndus, 202, 019. 
Goi'ldkn, C, II. (1937). Efficiency in field trials of pseudo-factorial and incomplete randomised 
block methods. Canadian, J. Res., 15, 231. 
i ioruu-iN, < \ II. (193S). Modern methods for testing a large number of varieties. Dom. Canada 
Pep. Ayr. Tech. Bull, 9. 
GoruuoN, C. IL (1939). Methods of Statistical Analysis. John Wiley and Sons, New York. 
(Chapman and Hall, London.) 
Gram, J, I\ (1S79). ()m Rackkendviklinqer beslemte, ved Iljaelp af de mindste Kvadraters Methode, 
Copenhagen. Reprinted as Ober die Eritwicklung realer Eunktionen in Reihen mittelst 
dcr Metiiodc der kloinsten Quadraten. J. fur Math., 94, 41, 1894. 
Gkkhn'MOaf, II. K. II. (1932). Curve approximation by means of functions analogous to the 
Hcrmite polynomials. Ann. Math-. Stats., 3, 204. 
Grkhnstkin, B. (1935). Periodograrn analysis with special application to business failures in 
the United States. Econometrika, 3, 170. 
GitBKNWooD, J. A., and Stuart, C. B. (1937). Mathematical techniques used in extra-sensory 
perception research. ♦/. Parapsychology, 1, 200. 
Gbkbn'Wood, ,1. A. (1938). Variance of a general matching problem. Ann. Math. Stats., 9, 56. 
466 BIBLIOGRAPHY 
Greenwood, J. A., and Gbeville, T. N. E. (1939). On the probability of attaining a given 
standard deviation ratio in an infinite series of trials. Ann. Math. Stats., 10, 297. 
Greenwood, J. A. (1940). The first four moments of a general matching problem. Ann. Hug. 
Bond., 10, 290. 
Greenwood, M., and Yule, G. U. (1915). The statistics of anti-typhoid and anti-cholera 
inoculations, and the interpretation of such statistics in general. Proc. Boy. Soc. Medicine, 
8, 113. 
Greenwood, M., and Yttle, G. U. (1917). On the statistical interpretation of some bacteriological 
methods employed in water analysis. J. Hygiene, 21, 36. 
Greenwood, M., and Yule, G. U. (1920). An inquiry into the nature of frequency-distributions 
of multiple happenings, etc,. J.B.S.S., 83, 255. 
Greenwood, M. (1922). The value of life tables in statistical research. J.B.S.S., 85, 537. 
Gressens, O. (1925). On the measurement of seasonal variations. /. Am. Stat. Ass., 20, 203. 
Greville, T. N. E. (1938). Exact probabilities for the matching hypothesis. J. Parapsychology, 
2, 55. 
Greville, T. N. E. (1939). Invariance of the admissibility of numbers under certain general 
types of transformations. Trans. Am. Math. Soc, 46, 410. 
Greville, T. N. E. (1941). The frequency-distribution of a general matching problem. A?m. 
Math. Stats., 12, 350. 
Grunebero, H., and Haldante, J. B. S. (1937). Tests of goodness of fit applied to records of 
Mendelian segregation in mice. Biom., 29, 144. 
Gtjldberg, A. (1922). Sur un theorerne de M. Markoff. Comptes rendus, 175, 679. 
Gtjldberg, A. (1934). On discontinuous frequency functions of two variables. Skand. Afct., 
17, 89. 
Gtjldberg, A. (1935). Sur les lois de probabilites et la correlation. Ann. Inst. II. Poincar<\ 
5, 159. 
Gtjldberg, S. (1935). Sui momenti dell a legge di distribuzioni del Polya. Giorn. 1st. Hal. Att.t 
Gxjlotta, B. (1938). Sulla legge di probability della differenza tra la media empirica <v il valoro 
medio teorico dei quadrati d' una variabile casuale che segue la legge normals. (Horn. 
1st! Ital. Att., 9, 245. 
Gtjmbel, E. J. (1924). Eine Darstellung der Sterbetafel. Biom., 16, 283 (and Correction" ihid.t 
411). 
Gtjmbel, E. J. (1925). Lebenserwartung und mittleres Alter der Lebenden. Biom., 17, 113. 
Gtjmbel, E. J. (1932). La distribuzione dei decessi secondo la legge di Gauss. Giorn. 1st. Ital. 
Jxtt., %5, oil. 
Gtjmbel, E. J. (1934). Les valeurs extremes des distributions statiatiques. Ann. Inst. II. Poincarv, 
5, 115. 
Gtjmbel, E. J. (1935a). Les m-iemes valeurs extremes et le logarithme du nombre d'observations. 
Comptes rendus, 200, 509. 
Gtjmbel, E. J. (19356). Le plus grand age, distribution et serie. Comptes rendus, 201, 318. 
Gtjmbel, E. J. (1937). La duree extreme de la vie humaine. Actualites ScAentifiques ft Indus- 
trielles, No. 520. Paris. Hermann et Cie. 
Gtjmbel, E. J. (1938a). La prevision des inondations. Comptes rendus, 206, 558 ; and : La 
distribution des inondations, AM. Vedy Boc, 7, 85. 
Gtjmbel, E. J. (19386). Gli eventi compatibili. Giorn. 1st. Ital Att., 9, 3 and 58. 
Gtjmbel, E. J. (1939). Les valeurs de position d'une variable aleatoire. Comptes mndus 208, 
149. 
Gtjmbel, E. J. (1941). The return period of flood flows. Ann. Math. Stats., 12, 163. 
Gtjmbel, E. J. (1942). Simple tests for given hypotheses. Biom., 32, 317. 
Gtjmbel, E. J. (1943a). On serial numbers. Ann. Math. Stats,, 14, 163. 
Gtjmbel, E. J. (19436). On the reliability of the classical ;f-test. Ann. Math. Stats., 14, 253. 
BIBLIOGRAPHY 467 
Haavelmo, T. (1941). A note on the variate-difference method. Econometriha, 9, 74. 
Habeeler, G. (1927). Der Sinn der Indexzahlen. Mohr, Tubingen. 
Hadamabd, J., and Erechet, M. (1933). Sur ies probabilities discontinues des evenements en 
chaine. Zeit. ang. Math, und Mech., 13, 92. 
Haldane, J. B. S. (1937). The exact value of the moments of the distribution of %2, used as a 
test of goodness of fit, when expectations are small. Biom., 29, 133. 
Haldane, J. B. S. (1938, 1939, 1940). The first six moments of %2 for an w-foid table with 
n degrees of freedom when some expectations are small. Biom., 29, 389 ; The mean and. 
variance of ^2 when used as a test of homogeneity when samples are small. Biom., 31, 
346 ; The cumulants and moments of the binomial distribution and the cumulants of 
X2 for an n x 2-fold table. Biom., 31, 392 ; Corrections to formulae in papers on the 
moments of %2. Biom., 31, 220. 
Haldane, J. B. S. (1938). The approximate normalisation of a class of frequency-distributions. 
Biom., 29, 392. 
Haldane, J. B. S. (1941). The cumulants of the distribution of the square of a variate. Biom., 
32, 199. 
Haldane, J. B. S. (1942a). Moments of the distributions of powers and products of normal 
variates. Biom,., 32, 226. 
Haldane, J. B. S. (19426). The mode and median of a nearly normal distribution with given 
cumulants. Biom., 32, 294. 
Hall, P. (1927a). Multiple and partial correlation coefficients in the case of an. n-fold variate 
system. Biom., 19, 100. 
Hall, P. (19276). The distribution of means for samples of size N drawn from a population in 
which the variate takes values between 0 and 1, all such values being equally probable. 
Biom., 19, 24.0. 
Ha livh en, E. (1939). Sur la convergence des estimations. Gornptes rendus, 208, 708. 
Hamburger, H. (1920, 1921). tJber cine Erweiterung des Stieltjesschen Momentproblems. Math. 
Ann., 81, 235; 82, 120 and 108. 
Hansen, M. H., and Huewitz, W. N. (1943). On the theory of sampling from finite populations. 
Ann. Math. Stain., 14, 333. 
Hansmann, G. H. (1934). On certain non-normal symmetrical frequency -distributions. Biom., 
Harris, J. A. (1914). On the calculation of intra-elass and inter-class coefficients of correlation 
from class-moments when the number of possible combinations is large. Biom., 
9, 440. 
Harris, J. A., and Gunstad, B. (1931). Extension of Pearson's correlation method to intra- 
class and inter-class relationships. J. Ayr. Sri., 42, 279. 
Harris, J. A., and Treloar, A. E. (1.927). On a limitation in the applicability of the contingency 
coefficient. J. Am. Stat. As\s\, 22, 400 ; and : Harris and Chi Tu. A second category 
of limitations in the applicability of the contingency coefficient. Ibid., 24, 367. (Reply 
by K. Pearson, J. Am. Stat. Ass., 25, 320.) 
Hart, B. 1. (1942). Significance levels for the ratio of the mean square successive difference to 
the variance. Ann. Math. Statft., 13, 445. 
Hartley, H. 0. (1938). Studentisation and large sample theory. Supp. J..R.S.S., 5, 80. 
Hartley, H. 0. (1940). Testing the homogeneity of a set of variances. Biom., 31, 249. 
Hartley, H. O. (1942). The range in normal samples. Biom., 32, 334. 
Hartley, H. O. (1944). Studcntization, or the elimination of the standard deviation of the 
parent population from the random sample-distribution of statistics. Biom., 33, 173. 
Hartman, P., van Kampmn, E. R.? and. Wintner, A. (1937). Mean motions and distribution 
functions. Am. J. Maths., 59, 201. 
Hartman, P., van Kampen, E. R., and Wjntner, A. (1938). On the distribution functions of 
almost periodic functions. Am. J. Maths., 60, 491. 
468 BIBLIOGRAPHY 
Hartman, P., van Kampen, E. E., and Wintner, A. (1939). Asymptotic distributions and 
statistical independence. Am. J. Maths., 61, 477. 
Harzer, P. (1933). Tabellen fur alle Statistischen Zwecke. Abhandlungen des Bayerischen Ak. 
der Wiss., Math-natiirwiss. Abteilung, Neue Folge, Heft 21. 
Haussdorf, F. (1923). Momentprobleme fiir ein endliches Intervall. Math. Zeit., 16, 220. 
Haviland, E. K. (1934a). On the theory of absolutely additive distribution functions. Am. J. 
Maths., 56, 625. 
Haviland, E. K. (19346). On distribution functions and their Laplace-Fourier transform. Proc. 
Nat. Acad. Sci., 20, 50 ; and (with A. Wintner) : On the Fourier-Stieltjes transform. 
Am. J. Maths., 56, 1. 
Haviland, E. K. (1935). On the inversion formula for Fourier-Stieltjes transforms in more than 
one dimension. Am. J. Maths., 57, 94, and 57, 382. Also: Note, 57, 569. 
Haviland, E. K. (1935, 1936). On the moment problem for distribution functions in more 
than one dimension. Am. J. Maths., 57, 562, and 58, 164. 
Haviland, E. K. (1939). Asymptotic probability distributions and harmonic curves. Am. J. 
Maths., 61, 947. 
Helguero, F. (1906). Per la risoluzione delle curve dimorflche. Rend. B. Acad. Line, 6. 
Helmert, F. E. (1875). Uber die Berechnung des wahrscheinlichen Fehlers aus einer endlichen 
Anzahl wahrer Beobachtungsfehler. Zeit. fiir Math, und Phys., 20, 300. 
Helmert, F. E. (1876a). Tiber die Wahrscheinlichkeit der Potenzsummen und iiber einige damit 
in Zusammenhange stehende Fragen. Zeit. fiir Math, und Phys., 21, 192. 
Helmert, F. R. (18766). Die Genauigkeit der Formel von Peters zur Berechnung des 
wahrscheinlichen Beobachtungsfehlers direkter Beobacht ungen gleicher Genauigkeit. 
Astronomische Nachrichten, 88, N"o. 2096. 
Henderson, J. (1922). On expansions in tetrachoric functions. Biom., 14, 157. 
Henderson, E. (1907). Frequency curves and moments. J. Inst. Act., 41, 429. 
Hendricks, W. A. (1931). The use of the relative residual in the application of the method of 
least squares. Ann. Math. Stats., 2, 458. 
Hendricks, W. A. (1934). The standard error of any analytic function of a set of parameters 
evaluated by the method of least squares., Ann. Math. Stats., 5, 107. 
Hendricks, W. A. (1935). The analysis of variance considered as an application of simple error 
theory. Ann. Math. Stats., 6, 117. 
Hendricks, W. A. (1936). An approximation to ' Student's ' distribution. Ami. Math. Stats., 
7, 210. 
Hendricks, W. A., and Eobey, K. W. (1936). The sampling distribution of the coefficient, of 
variation. Ann. Math. Stats., 7, 129. 
Hersch, L. (1934). Essai sur les variations periodiques et leur mensuration. Metron, 12, No. I, 3. 
Hey, G. B. (1938). A new method of experimental sampling illustrated on certain non-normal. 
populations. Biom., 30, 68. 
Hildebrandt, E. H. (1931). Systems of polynomials connected with the Charlier expansions 
and the Pearson differential equations. Ann. Math. Stats., 2, 379. 
Hilton, J. (1924, 1928). Enquiry by sample; an experiment and its results, J.R.S.S., 87, 
544; and: Some further enquiries by sample. Ibid., 91, 519. 
Hirschfeld, H. 0. (1935). A connection between correlation and contingency. Proc. (Jamb. 
Phil. Soc, 31, 520. 
Hirscheeld, H. O. (1937). The distribution of the ratio of covariance estimates in two samples 
drawn from normal bivariate populations. Biom., 29, 65. 
Hoel, P. G. (1937). A significance test for component analysis. Ann. Math. Stats., 8, 149. 
Hoel, P. G. (1938). On the chi-square distribution for small samples. Ann. Math. Stats., 9, 158. 
Hoel, P. G. (1939). A significance test for minimum rank in factor analysis. Psychometrika, 
4, 245. 
Hoel, P. G. (1941). On methods of solving normal equations. Ann. Math. Stats., 12, 354. 
BIBLIOGRAPHY 469 
Hojo, T. (1931, 1933). Distribution of the median, quartiles and interquartile distance in samples 
from a normal population. Biom., 23, 315 ; and : A further note on the relation between 
the median and the quartiles in small samples from a normal population. Biom., 25, 79. 
Holzingeb, K. J., and Church, A. E. R. (1929). On the means of samples from a C7-shaped 
population. Biom,., 20A, 361. 
Horst, P. (1935). A method for determining the coefficients of a characteristic equation. Ann. 
Math. Stats., 6, 83. 
Hortjnskv, B. (1937). Sur les probability relatives aux variables aleatoires liees entre dies. 
Applications diverses. Ann. Inst. H. Poincare, 7, 69. 
Hotkixtncj, H. (1.925). The distribution of correlation ratios calculated from random data. 
Proc. Nat. Acad. Sci., 11, 657. 
Hotellinu, H. (1927). An application of analysis situs to statistics. Bull. Am. Math. Soc, 
July-Aug., 467. 
Hotmlltnq, H. (1930). The consistency and. ultimate distribution of optimum statistics. Trans. 
Am. Math. Soc, 32, 847. 
Hotellwu, H. (1931). The generalisation of ' Student's ' ratio. Ann. Math. Stats., 2, 360. 
HoTELLiNa, H. (1933). Analysis of a complex of statistical variables into principal components. 
.Reprinted from J. Educ. Psych. (24, 417), Sept.-Oct. 1933, Warwick and York, Inc., 
.Baltimore. 
MoTiaxiNU, H. (1936a). Simplified calculation of principal components. Psychometrika, 
1 c>7 
Hotiollinu, H. (19366). Relations between two sets of variates. Biom,., 28, 321. 
Hotkujnu, H., and Pabst, M. R. (1936c). Rank correlation and tests of significance involving 
no assumptions of normality. Ann. Math. Stats., 7, 29. 
HoTMuviNtJ, PI., and Frankel, L. R. (1938). The transformation of statistics to simplify their 
distribution. Ann. Math. Slats., 9, 87. 
Hotellinu, H. (1939). Tubes and spheres in n-spaces and a class of statistical problems. Am. J. 
Math*., 61, 440. 
Hotellinu, 1L (1940). The selection of variates for use in prediction with some comments on the 
problem of nuisance- parameters. Ann. Math. Stats., 11, 271. 
Hotruijnu, II. (1941). Experimental determination of the maximum of a function. Ann. 
Math. Stats., 12, 20. 
HoTioiiUNu, SL (1943). Some new methods in matrix calculation. Ann. Math. Stats., 14, 1 
and 440. 
.Hsu, <\ T., and Lawltcy, I). N. (1939). The derivation of the fifth and sixth moments of b2 in 
samples from a normal population. Biom., 31, 238. 
Hsu, C. T. (1940, 1941). On samples from a normal bivariate population. Ann. Math. Stats., 
11, 410; and: Samples from two bivariate normal populations. Ibid., 12, 279. 
Hsu, P. I,. (1938a). Contribution to the theory of ' Student's ' it-test as applied to the problem 
of two samples. Stat. Res. Mem., 2, 1. 
Hsu, P. L. (1938/;). On the best unbiassed quadratic estimate of the variance. Stat. Res. Mem., 
Ml , . r 1 . 
Hsu, P. L. (1938c). Notes on Hoteiling's generalised T. Ann. Math. Stats., 9, 231. 
Hsu, P. L. (1939a). A new proof of the joint product-moment distribution. Proc. Gamb. Phil. 
Sac, 35, 330. 
Hsu, P. L. (19396). On the distribution of roots of certain determinants! equations. Ann. 
tiny. Lond., 9, 250. 
Hsu, P. L. (1940). On generalised analysis of variance. Biom., 31, 221. 
Hsu', P. L. (1.941a). On the limiting distribution of the canonical correlations. Biom., 32, 38. 
Hsu; P. L. (19416). Analysis of variance from the power function standpoint. Biom., 32, 62. 
Hatr[ P. L. (1.941c). On the problem of rank and the limiting distribution of Fisher's test function. 
Ann. Evtg. Lond., 11, 39. 
470 BIBLIOGRAPHY 
Hsu, P. L. (1941^). Canonical reduction of the general regression problem. Ann. Bug. Lond., 
11, 42. 
Hsu, P. L. (1943). Some simple facts about the separation of degrees of freedom in factorial 
experiments. Sanhhyd, 6, 253. 
Immrr, F. R. (1937). Correlation between means and standard deviations in field experiments. 
J. Am. Stat. Ass., 32, 525. 
Ingham, A. E. (1933). An integral which occurs in statistics. Proc Gamb. Phil. Soc, 29, 270. 
Iewin, J. O. (1925a). The further theory of Francis Galton's individual difference problem. 
Biom., 17, 100. 
Irwin, J. O. (19256). On a criterion for the rejection of outlying observations. Biom., Y7, 238. 
Irwin, J. O. (1927, 1929). On the frequency-distribution of the means of samples from a 
population having any law of frequency with finite moments, etc. Biom., 19, 225, and 21, 431. 
Irwin, J. O. (1929a). On the frequency-distribution of any number of deviates from the mean 
of a sample from a normal population and the partial correlations between them. 
J.R.8.S., 92, 580. 
Irwin, J. O. (19296). Note on the #2-test for goodness of fit. J.R.S.S., 92, 274. 
Irwin, J. O. (1930). On the frequency-distribution of the means of samples from populations 
of certain of Pearson's types. Matron, 8, No. 4, 51. 
Irwin, J. O. (1931). Mathematical theorems involved in the analysis of variance. <J .jcx.aj ,io ., 
94, 284. 
Irwin, J. O. (1933). A critical discussion of the single-factor theory. Brit. J. Psych., 23, 371. 
Irwin, J. O. (1934). On the independence of the constituent items in the analysis of variance. 
Supp. J.R.S.S., 1, 236. 
Irwin, J. O. (1935). Tests of significance for differences between percentages based on small 
numbers. Metron, 12, No. 2, 83. 
Irwin, J. O. (1937a). The frequency-distribution of the difference between two independent 
variates following the same Poisson distribution. J.B.S.S., 100, 415. 
Irwin, J. O. (19376). Statistical method applied to biological assays. Supp. J.R.S.S., 4, 1. 
Irwin, J. O., and Cheeseman, E. A. (1939). On the maximum-likelihood method of 
determining dosage response curves. Supp. J.B.S.S., 6, 174. 
Irwin, J. O. (1942). On the distribution of a weighted estimate of variance and on analysis of 
variance in certain cases of unequal weighting. J.R.S.S., 105, 115. 
Irwin, J. O., and Kendall, M. G. (1944). Sampling moments of moments for a finite population. 
Ann. Bug. Lond., 12, 138. 
L. (1914, 1916). On the partial correlation ratio. Part I. Theoretical. Biom., 
10, 391 ; and Part II. Numerical. Ibid., 11, 50. 
Isserlis, L. (1915). On the conditions under which the probable errors of 
frequency-distributions have a real significance. Proc. Boy. Soc, A, 92, 23. (Correction, Biom., 12, 261.) 
Isserlis, L. (1916). On certain probable errors and correlation coefficients of multiple frequency- 
distributions with skew regression. Biom., 11, 185. 
Isserlis, L. (1917). On the representation of statistical data. Biom., 11, 418. 
Isserlis, L. (1918a). On the value of a mean as calculated from a sample. J.B.S.S., 81, 75. 
Isserlis, L. (19186). On a formula for the product-moment coefficient of any order of a normal 
frequency-distribution in any number of variables. Biom., 12, 134. (Correction, 
ibid., 12, 266.) 
Isserlis, L. (1918c). Formulae for determining the mean values of products of deviations of 
mixed moment coefficients in two to eight variables in samples taken from a limited 
population. Biom., 12, 183. 
Isserlis, L. (1931). On the moment distributions of moments in the case of samples drawn from 
a limited universe. Proc. Roy. Soc, A, 132, 586. 
Isserlis, L. (1936). Inverse probability. J.R.S.S., 99, 130. 
BIBLIOGRAPHY 471 
Jackson, D. (1921). Note on the median of a set of numbers. Bull. Am. Math. Soc, 27, 
160. 
Jackson, D. (1934). Series of orthogonal polynomials. Ann. Math., 34, 527 ; and : The 
summation of series of orthogonal polynomials. Bull. Am. Math. Soc, 40, 743. 
Jackson, J). (1937). Orthogonal, polynomials on a plane curve. Duke Math. J., 3, 228. 
Jackson, D. (1938). Orthogonal polynomials in three variables. Duke Math. J., 4, 441. 
JxiCKSON, R. W. (1936). Tests of statistical hypotheses in the ease when the set of alternatives 
is discontinuous, illustrated on some genetical problems. Stat. Res. Mem., 1, 138. 
Jacob, M. (1933), Sullo sviluppo di una curva di frequenza in serie di Cliarlier Type B. Oiorn. 
1st. Ital. Att., 4, 221. 
Jacob, M. (1935, 1937). Sul fenomeno di Gibbs nello sviluppo in serie di polinomi di Hermite. 
Giorn. 1st. Ital. Att., 6, 1, and 8, 297. 
Jeffreys, H. (1933). On Gauss's proof of the law of errors. Proc. Camb. Phil. Soc, 29, 
231. 
Jeffreys, H. (1937a). On statistically steady distributions in Astronomy. Monthly Not. R. 
Astr. Soc, 97, 59. 
Jeffreys, H. (19376). On the relation between direct and inverse methods in statistics. Proc 
Roy. Soc, A, 160, 325. 
Jeffreys, H. (1937c). The law of errors and the combination of observations. Phil. Trans., 
Jeffreys, H. (1938a). Significance tests for continuous departures from suggested distributions 
of chance. Proc Roy. Soc, A, 164, 307. 
Jeffreys, H. (19386). The use of minimum %2 as an approximation to the method of maximum 
likelihood. Proc Camb. Phil. Soc, 34, 156. 
Jeffreys, H. (1938c). Maximum likelihood, inverse probability and the method of moments. 
Ann. Eug. Lond., 8, 146. 
Jkffrkys, H. (1938a7). The correction of frequencies for a known standard error of observations. 
Month, Not. R. Astr. Soc, 98, 190. 
Jeffreys, H. (1939a). The Theory of Probability. Cambridge University Press. 
Jeffreys, H. (19396). The minimum #a approximation. Proc Camb, Phil. Soc, 35, 520. 
Jeffreys, 11. (1939c). The posterior probability distributions of the ordinary and intra-class 
correlation coefficients, Proc Hoy. Soc, A, 167, 464. 
Jeffreys, H. (1939ri). The comparison of series of measures on different hypotheses concerning 
the standard errors. Proc Roy. Soc, A, 167, 367. 
Jeffreys, H. (1939e). Random and systematic arrangements. Biom,, 31, 1. 
Jeffreys, H. (1940). Note on the Bchrens-Fisher formula. Ann. Eug. Lo?id., 10, 48. 
Jeffreys, H. (1941). Some applications of the method of minimum %*. Ann. Bug. Lond., 
11, 108. 
Jenkins, T. N. (1932). A short method and tables for the calculation of the average and standard 
deviation of logarithmic distributions. Ann. Math. Stats., 3, 45. 
Jennett, W. J., and Welch, B. L. (1.939). The control of proportion defective as judged by a 
single finality characteristic varying on a continuous scale. Supp. J.R.S.S., 6, 80. 
Jensen, A. (1.925). Report on the representative method in statistics. Bull. Int. Stat. Inst,, 
22, l(r livre. 
Jeshen, B., and Wintner, A. (1935). Distribution functions and the Riemann zcta-function. 
Trans. Am. Math. Soc, 38, 48. 
Johnson, E. (1,940). Estimates of parameters by means of least squares. Ann. Math. Stats., 
JL JL, tu(). 
Johnson, N. L., and Welch, B. L. (1939). On the calculation of the cumulants of the 
^-distribution. Biom., 31, 216. 
Johnson, N. L., and Welch, B. L. (1940a). Applications of the non-central ^-distribution. Biom., 
31, 362. 
472 BIBLIOGRAPHY 
Johnson, 1ST. L. (19406). Parabolic test for linkage. Ann. Math. Stats., 11, 227. 
Johnson, P. 0., and Neyman, J. (1936). Tests of certain linear hypotheses and their application 
to some educational problems. Stat. Res. Mem., 1, 57. 
Jokes, H..E. (1937a). Some geometrical considerations in the general theory of fitting lines and 
planes. Metron, 13, No. 1, 21. 
Jokes, H. E. (19376). The nature of regression functions in the correlation analysis of time- 
series. Econometrika, 5, 305. 
Jokes, H. E. (1937c). The theory of runs as applied to time series. Report, Third Annual 
Research Conf. on Economics and Statistics, p. 33. (Cowles Commission.) 
Jordan, C. (1927). Statistique Mathematique. Gauthier-Villars, Paris. 
Jordan, C. (1932). Approximation and graduation according to the principle of least squares 
by orthogonal polynomials. Ann. Math. Stats., 3, 257. 
Jordan, 0. (1933). Inversione della formola di Bernouilli relativa al problema delle prove ripetute 
a phi variabili. Giorn. 1st. Ital. Att., 4, 505. 
Jordan, C. (1934). Teoria della perequazione e delP approssimazione. Giorn. 1st. Ital. Att., 
5, 81. 
J5rgensen, N. R. (1916). Undersogelser over Frequensflader og Korrelation. Busck, Copenhagen. 
Kac, M. (1939). On a characterisation of the normal distribution. Am. J. Maths., 61, 726. 
Kac, M., and van Kampen, E. R. (1939). Circular equidistributions and statistical independence. 
Am. J. Maths., 61, 677. 
Kalecki, M. (1935). A macrodynamic theory of business cycles. EconomMrika, 3, 327. 
Kamke, E. (1932). Einfuhrung in die WahrscheinlichJceitstheorie. Hirzel, Leipzig. 
Kaplansky, J. (1939). On a generalisation of the ' probleme de rencontres '. Am. Math. Monthly, 
46, 159. 
Kapteyn, J. C. (1903). Skew Frequency-Curves in Biology and Statistics. Noordhoff, Groningen 
and Wm. Dawson, London. 
Katjcky, J. (1936). Le probleme des iterations dans un cas de probability clependantes. Comptes 
rendus, 202, 722. 
Kelley, T. L. (1923). Statistical Method. Macmillan, New York. 
Kelley, T. L. (1928). Cross-roads in the Mind of Man. Stanford University Press, California. 
Kelley, T. L., and McNemar, Q. (1929). Doolittle versus Kelley-Salisbury iteration method for 
computing multiple regression coefficients. J. Am. Stat. Ass., 24, 164. 
Kelley, T.L. (1935) . An unbiassed correlation ratio measure. Proc. Nat. Acad. Sci., 21, 554. 
Kelley, T. L. (1938). The Kelley Statistical Tables. Macmillan, New York. 
Kendall, M. G. (1938a). The conditions under which Sheppard's corrections are valid. J.R.S.S., 
101, 592. 
Kendall, M. G. (19386). A new measure of rank correlation. Biom., 30, 81. 
Kendall, M. G., Kendall, S. F. H., and Babington Smith, B. (1939). The distribution of 
Spearman's coefficient of rank correlation, etc. Biom., 30, 251. 
Kendall, M. G., and Babington Smith, B. (1939a). Tables of Random Sampling Numbers. 
Tracts for Computers, No. 24, Cambridge University Press. 
Kendall, M. G.3 and Babington Smith, B. (19396). The problem of m rankings. Ann. Math. 
Stats., 10, 275. 
Kendall, M. G., and Babington Smith, B. (1940). On the method of paired comparisons. 
Biom., 31, 324. 
Kendall, M. G. (1940). Some properties of ^-statistics. Ann. Eug. Bond., 10, 106 ; Proof of 
Fisher's rules for ascertaining the sampling semi-invariants of ^-statistics. Ibid., 10, 
215 ; The derivation of multivariate sampling formulae from univariate formulae by 
symbolic operation. Ibid., 10, 392. 
Kendall, M. G. (1941). A theory of randomness. Biom., 32, 1. 
Kendall, M. G. (1942a). Partial rank correlation. Biom., 32, 277. 
BIBLIOGRAPHY 473 
Kendall, M. G. (19425). On seminvariant statistics. Ann. Eug. Land., 11, 300. 
Kendall, M. G. (1944a). Oscillatory movements in English agriculture. J.B.S.S., 106, 91. 
Kendall, M. G. (19445). On autoregressive time-series. Biom., 33, 105. 
Kerchner, R., and Wintner, A. (1936). On the asymptotic distribution of almost periodic 
functions with linearly independent frequencies. Am. J. Maths., 58, 91. 
Kermack, W. 0., and McKendrick, A. G. (1936). Tests for randomness in a series of numerical 
observations. Proc. Boy. Soc. Edin., 57, 228. 
Kermack, W. 0., and McKendrick, A. G. (1937). Some distributions associated with a randomly 
arranged set of numbers. Proc. Roy. Soc. Edin., 57, 332. 
Kerrich, J. E. (1935). Systems of osculating arcs. J. Inst. Act., 66, 88. 
Kerrich, J. E. (1937). Least squares and a generalisation of the ' Student '-Fisher theorem. 
Stand. Akt, 20, 244. 
Keyfitz, N. (1938). Graduation by a truncated normal. Ann. Math. Stats., 9, 66. 
Keynes, J. M. (1911). Principal averages and the laws of error which lead to them. J.B.S.S., 
74, 322. 
Keynes, J. M. (1921). A Treatise on Probability. Macmillan, London. 
Khintchine, A. (1928). Begrimdung der Normalkorrelation nach der Lindebergschen Methode. 
Nachr. ForscJiungsinst. Moskau, 1. 
Khintchine, A. (1932-1933). Sulle successione stazionarie di eventi. Oiorn. Ital. 1st. Att., 3, 
267 ; and "Uber stationare Reihen zufalliger Variablen. Bee. Mathematiques, Moscou, 
40 
Khintchine, A. (1933). Asymptotische Gesetze der Wahrscheinlichkeitsrechnung. Springer, 
Berlin. 
Khintchine, A. (1934). Korrelationstheorie der stationare stochastischer Prozesse. Math. 
Ann., 109, 604. 
Khintchine, A. (1935). Sul dominio di attrazione della legge di Gauss. Giorn. 1st. Ital. Att., 
6, 378. 
Khintchine, A., and Levy, P. (1936). Sur les lois stables. Comptes rendus, 202, 374. 
Khintchine, A. (1937a). Zur Theorie der unbeschrankt teilbaren Vorteilungsgesetze. Bee. 
Math. Moscou, 2, 79. 
Khintchine, A. (19376). Series of papers on probability laws in Bull. Univ. Mat Moscou, Sir. 
Int. Sect. A, 1, Ease. 1, 1, 6 ; Ease. 5, 1, 4, 6. 
Khintchine, A. (1938). Zwei Satzc fiber stochastisehe Prozesse mit stabilen Verteilungen. 
Bee. Math. Moscou, 3, 577. 
Kibble, W. E. (1941). A two-variate gamma type distribution. SamJchyd, 5, 137. 
Kiser, C. V. (1934). Pitfalls in sampling for population study. */. Am. Slat. Ass., 29, 250. 
Kishen, K. (1940). On a sirapliiied method of expressing the components of the second-order 
interaction in a 33 factorial design. San/chyd, 4, 577. 
Kishen, K. (1942). Symmetrical unequal block arrangements. Sankhyd, 5, 329. 
KiTAOAWA, T. (1941). The limit theorems of the stochastic contagious processes. Mem. Fac. 
Sci., Kyusyu Imperial University, A, 1, 167. 
Kolmogoroff, A. (1929). Bemerkungen zu meinor Arbeit ' Uber die Summen zufalliger Grossen.' 
Math. Ann., 102, 484. 
Kolmogoroff, A. (1931). Uber die analytiselie Methode in der Wahrscheinlichkeitsrechnung. 
Math. Ann., 104, 415. 
Kolmogoroff, A. (1933a). Grundbegriffe der Wahrscheinlichkeitsrechnung, Berlin. 
Kolmogoroff, A. (19336). Sulla determiriazione empirica delle leggi di probabilita. Giorn. 
1st. Ital Alt., 4, 83. 
Kolmogoroff, A. (1937<z). Zur Umkehrbarkeit der statistischen Naturgesetz. Math. Ann., 
113, 766. 
Kolmogoroff, A. (19376). Chaines de Markoff avec une infinite denombrable des 6tats possibles. 
Bull. Univ. Mat Moscou, Ser. Int. Sect. A, 1, Ease. 3, 1. 
474 BIBLIOGRAPHY 
Kolmogoroef, A. (1941). Confidence limits for an unknown distribution function. Ann. Math. 
Stats., 12, 461. 
Kolodzieczyk, St. (1933). Sur 1'erreur de la seconde categorie dans le probleme de M. Student. 
Comptes rendus, 197, 814. 
Kolodzieczyk, St. (1935). On an important class of statistical hypothesis. Biom., 27, 161. 
Kondo, T. (1929). On the standard error of the mean square contingency. Biom., 21, 376. 
Kondo, T. (1930). A theory of the sampling distribution of standard deviations. Biom.} 22, 36. 
Konos, A. A. (1939). The problem of the true index number of the cost of living. Econometrika, 
7, 10. 
Koopman, B. 0. (1936). On distributions admitting a sufficient statistic. Trans. Am. Math. 
Soc., 39, 399. 
Koopmans, T. (1937). Linear regression analysis of economic time series. Neth. Boon. Inst., 
No. 20. Haarlem. 
Koopmans, T. (1940). The degree of damping in business cycles. EconometriJca, 8, 79. 
Koopmans, T. (1941). Distributed lags in dynamic economics. Econometrika, 9, 128. 
Koopmans, T. (1942). Serial correlation and quadratic forms in normal variables. Ann. Math. 
Stats., 13, 14. 
Koshal, R. S. (1933). Application of the method of maximum likelihood in the improvement 
of curves fitted by the method of moments. J.R.S.S., 96, 303. 
Koshal, R. S. (1935). Application of the method of maximum likelihood to the derivation of 
efficient statistics for fitting frequency curves. J.R.S.S., 98, 128. 
Koshal, R. S. (1939). Maximal likelihood and minimal %2 in relation to frequency curves. Ann. 
Bug. Lond., 9, 209. 
Kozakiewicz, M. W. (1937, 1938). Sur les conditions necessaires et suffisantes pour la 
convergence stochastique. Comptes rendus, 205, 1028 and Fund. Math., 31, 160. 
Kxtllback, S. (1934). An application of characteristic functions to the distribution problem of 
statistics. Ann. Math. Stats., 5, 264. 
Kxtllback, S. (1935a). On samples from a multivariate normal population. Ann. Math. Stats., 
6, 202. 
Kxtllback, S. (19356). On the Bernouilli distribution. Bull. Am. Math. Soc, 41, 857. 
Kxtllback, S. (1935c). A note on the distribution of a certain partial belonging coefficient. 
Metron, 12, No. 3, 65. 
Kxtllback, S. (1936a). The distribution laws of the difference and quotient of variables 
independently distributed in Pearson Type III laws. Ann. Math. Stats., 7, 51. 
Kxtllback, S. (19366). On certain distribution theorems of statistics. Bull. Am. Math. Soc, 
42, 407. 
Kxtllback, S. (1936c). A note on the multiple correlation coefficient. Metron, 12, No. 4, 67. 
Kxtllback, S. (1937). On certain distributions derived from the multinomial distribution. Ann. 
Math. Stats., 8, 127. 
Kxtketz, G. (1936). Sur quelques proprietes cles fonctions caracteristiques. Comptes rendus, 
202, 1829. 
Kctzmin, R. 0. (1939). Sur la loi de distribution du coefficient de correlation dans les tirages 
d'un ensemble normal. C.R. Acad. Sci. U.S.S.R., 22, 298. 
Kxtznets, S. (1929). Random events and cyclical fluctuations. J. Am. Stat. Ass., 24, 258. 
Kctzkets, S. (1933). Seasonal Patterns in Industry and Trade. New York. 
Laderman, J. (1939). The distribution of ' Student's ' ratio for sampling of two items drawn 
from non-normal universes. Ann. Math. Stats., 10, 376. 
Laderman, J., and Lowan, A. N. (1939). On the distribution of nth tabular differences. Ann. 
Math. Stats., 10, 360. 
Landahl, H. T>. (1938). Centroid orthogonal transformations. Psychometrika, 3, 219. 
Laplace, P. S., Marqxtis de (1818). Theorie analytique des probability. 
BIBLIOGRAPHY 475 
Larmok, Sir J., and Yamaga, N. (1917). On permanent periodicity in sunspots. Proc. Roy. 
Soc, A, 93, 493. 
Laska, V. (1935). Contribution a la standardisation des definitions des prineipales notions 
statistiques. Rev. Stat. Tchechoslovaque, 16, 3. 
Lawley, I). N. (1938). A generalisation of Fisher's s-test. Biom., 30,180 ; and Correction, ibid., 
30, 467. 
Le Cokbeiller, P. (1933). Les syste tries auto-entretenus et les oscillations de relaxation, Econo- 
metrika, 1, 328. 
Ledebmann, W. (1938). The orthogonal transformation of a factorial matrix into itself. Psycho- 
metrika, 3, 181. 
Ledekmann, W. (1939). Sampling distribution and selection in a normal population. Biom., 
30, 295. 
Lehmank, A. (1939). Uber die Inversion des Gausschen Wahrsclieinlichkeits-Integrals. Mitt. 
Verein. Schweiz. Versicherungs-Math., 38, 15. 
Lengyel B. A. (1939). On testing the hypothesis that two samples have been drawn from a 
common normal population. Ann. Math. Stats., 10, 365. 
Le Rotjx, J. M. (1931). A study of the distribution of variance in small samples. Biom. 
23, 134. 
Leseb, 0. E. V. (1942). Inequalities for multivariate frequency-distributions. Biom. 32, 284. 
Levy, P. (1925). Calcul des Probability. Gauthier-Viliars, Paris. 
Levy, P. (193b/). Quclques theorerncs sur les probabilities denombrables. Comptes rendus, 192, 
658. 
Levy, P. (1931 ft). Sulla legge forte dei grandi numeri. (Horn. 1st. Ital. Alt., 4, 1. 
Levy, P. (!931r). Sur tin theorenie de M. Khintchine. Bull. Sci. Math., (2), 55, 145. 
Levy, P. (1934). Sur les integrates dont les elements sont des variables aleatoires independantes. 
Annali It Sci. Norm. Sup. Pisa, (2), 3, 337. 
Levy, P. (1935a). Suit' appiicazione della geometria dello spazio di Hilbcrt alio studio delle 
successioni di variabili casuali. (Horn. Inst. Ital. Alt., 6, 13. 
Levy, P. (1935/>). Propriet.es asymptotiques des sommes de variables aleatoires independantes 
ou enchainees. J. Math. Pur. App., (7), 14, 347. 
Levy, P. (193(w). Sur quclques points de la theorie des probabilites denombrables. Ann. Inst. 
H. Poincarf, 6, 153. 
Levy, P. (19306). Determination generate des lois Hmites. Comptes rendus, 203, 098. 
Levy, P. (1930r). La loi forte des grands nombres pour les variables aleatoires enchainees. 
J. Math. Pur. App., 15, 11. 
Levy, P. (WYMa). LWithmctique des lois de probability. Compter rendu*, 204, 80 and 944 ; Sur 
les exponeutielles des polynomes et sur Farithmetique de-s produits do lois de Poisson. 
Ann. VEcole Norm. Sup., 54, 231 ; and : Nouvelle contribution a Farithmetique des 
produits de lois de Poisson. Comptes rendus, 205, 535. 
Levy, P. (I93S<7,). IVarithmetique des lois de probabilite. J. Math. Pur. App., 27, 17. 
Levy, P. (1938/>, 1939). Sur la definition des lois de probabilites par leurs projections. Com,ptes 
rendus, 206, 1240, and: Rectification. Ibid., 206, 1099. Also: Sur ies projections 
(Func loi de probabilite a n variables. Bull. Sci. Math., (2), 63, 148. 
Levy, P. (! 939*7.). I/addition des variables aleatoires definies sur une circonference. Bull. Soc. 
Math. France, 67, 1. 
Levy, P. (19396). Extensions stochastiques des notions de aerie, (Fint6grale et d'aire. Comptes 
rendus, 209, 591. 
Levy, P. (1939c)- Sur la division d'un segment par les points ehoisis au hasard. Comptes rendus, 
208, 147. 
Lewis, W. rF. (1935). A reconsideration of Sheppard's corrections. Ann. Math. Stats., 6, 11. 
Lexis, W. (1903). Abhandlungen zur Theorie der Bevoikerungs- und Moralstatistik. Fischer, 
Jena. 
476 BIBLIOGRAPHY 
Liafottnoff, A. (1900). Sur une proposition de la theorie des probabilites. Bull. Acad. Sci, 
St Pet, (5), 13, 359. 
Liapoxjhoff, A. (1901). Nouvelle forme du theoreme sur la limite de probability Mem. Acad. 
Sci. St. Pet., (8), 12, No. 5. 
Lidstone, G. T. (1933). Notes on orthogonal polynomials, etc. J. Inst. Act., 64, 128. 
Lidstone, G. T. (1937). Notes on interpolation, etc. J. Inst Act., 68, 267. 
Lindblad, T. (1937). Zur Theorie der Korrelation bei mehr-dimensionalen zufalligeii Variablen.. 
Acta Soc. Sci. Fennicae, (2), 10, 1. 
Lindeberg, J. W. (1922). Eine neue Herleitung des Exponentialgesetzes in der Wahrschemlich- 
keitsrechnung. Math. Zeit, 15, 211. 
Lomnioki, A. (1923). Nouveaux fondements du calcul des probabilites. Fund. Math., 4, 34. 
Lokseth, A. T. (1942). Systems of linear equations with coefficients subject to error. Ann. 
Math. Stats., 13, 332. 
Lorenz, P. (1931). Die Trend. Vierteljahrshefte zur Konjunkturforschung. 2e Auflage, Sonderhaft,, 
Lorenz, P. (1935). Annual survey of statistical technique : Trends and seasonal variations. 
Econometrika, 3, 456. 
Lotka, A. J. (1938). Some recent results in population analysis. J. Am,. Stat. Ass., 33, 164. 
Lotka, A. J. (1939). On an integral equation in population analysis. Ann. Math. Stats.,. 
10, 144. 
LiJDERS, R. (1934). DieStatistik der seltenen Ereignisse. Biom. 26, 108. 
Lijkomski, J. (1939). On some properties of multidimensional distributions. Ann. Math. Stats., 
10^ 236. 
Lurqttik, C. (1937). Sur la loi de Bernouilli a deux variables. Bull. Classe Sci. Acad. R. Belgiquer 
(5), 23, 857. 
Maoaulay, F. R. (1931). Smoothing of Time-Series. National Bureau of Economic Research,, 
New York. 
MacMahoh, P. A. (1915, 1917). Combinatory Analysis. Cambridge University Press. 
MacStewart, W. (1941). A note on the power of the sign test. Ann. Math. Stats., 12, 236. 
Madow, W. G. (1937). Contributions to the theory of comparative statistical analysis. 
I. Fundamental theorems of comparative analysis. Ann. Math. Stats., 8, 159. 
Madow, W. G. (1938). Contributions to the theory of multivariate statistical analysis. Trans. 
Am. Math. Soc, 44, 454. 
Madow, W. G. (1939). Generalisation of the Laplace-Liapounoff theorem. Ann. Math. Stats.,, 
10, 84. 
Madow, W. G. (1940). Limiting distributions of quadratic and bilinear forms. Ann. Math. 
Stats., 11, 125. 
Mahalanobis, P. C. (1922). On errors of observation and upper air relationships. Mem. Ind. 
Met. Dep., 24. 
Mahalanobis, P. C. (1930). On tests and measures of group divergence. I. J. Asiat. Soc. Beng., 
26, 541. 
Makalanobis, P. C. (1933). Tables of L-tests. Sanhhyd, 1, 109. 
Mahalakobis, P. C, Bose, S. S., Roy, P. R., and Banerji, S. K. (1934). Tables of random 
samples from a normal population. Sanlchyd, 1, 289. 
Mahal akobis, P. C. (1936a). On the generalised distance in statistics. Proc. Nat. hist. Sci. Ind.> 
12, 49. 
Maealakobis, P. C, Bose, R. C, and Roy, S. N. (19366). Normalisation of statistical variates 
and the use of rectangular coordinates in the theory of sampling distributions. Sankhya, 
3, 1. 
Mahalakobis, P. C. (1943). An inquiry into the prevalence of drinking tea among middle-class 
Indian families in Calcutta. Sankhya, 6, 283. 
BIBLIOGRAPHY 477 
M&hlmann, H. (1935). Ein Beitrag zu Untersuchungen iiber zweidimensionale Verteilungen von 
Massenpunkten bei zufallsartig bedingten Bewegungen. Biom., 27, 191. 
Matlock, R. R. M. (1933). An electrical calculating machine. Proc. Boy. Soc, A, 140, 457. 
Mann, H. B., and Wald, A. (1942). On the choice of the number of intervals in the application 
of the #2-test. Ann. Math. Stats., 13, 306. 
Mann, H. B. (1943). On the construction of sets of orthogonal Latin squares. Ann. Math. Stats., 
14, 401. 
Mabbe, K. (1934). Grundfragen der angcwandten Wakrscheirilickkeit&reclmung und theoretischer 
Statistik. Munchen and Berlin. 
March, L. (1926). L'analyse de la variabilite, Metron, 6, No. 2, 3. 
Mabohand, E. (1937). Probability experimentales, probabilites eorrigees et probabilites in- 
dependantes. Mitt. Verein. Schweiz. Versich-Math., 33, 49. 
Marcinkiewicz, J., and Zycmund, A. (1937). Sur les fonctions independantes. Fund. Math., 
29, 60. 
Marcokiewicz, J. (1939). Sur le probleme des moments. Comptes rendus, 208, 405. 
Markofe, A. A. (1912). Wahrscheinlichkeitsrechnung. Teubner, Leipzig. 
Marples, P. M. (1932). Linear difference equations. J. Inst. Act., 63, 404. 
Marsegtjerra, V. (1936). Considerazioni sulla cosidetta legge sinusoidale nel calcolo del la pro- 
babilita. Giorn. Inst. Ital. AM., 7, 206. » 
Martin, E. S. (1934). On the correction for the moment coefficients of frequency-distributions 
when the start of the frequency is one of the characteristics to be determined. Biom., 
Martin, E. S. (1936). A study of an Egyptian series of mandibles with special reference to 
mathematical methods of sexing. Biom. 28, 149. 
Mather, K. (1935). The combination of data. Ami. Bug. Lond., 6, 399. 
Mathisen, H. C. (1943). A method of testing the hypothesis that two samples are from the 
same population. Ann. Math. Stats., 14, 188. 
Matuszewski, T., Neyman J., and Supinska, J. (1935). Statistical studies in questions of 
bacteriology. Part I. The accuracy of the dilution method. Su/pp. J.B.S.S., 2, 63. 
Mazzoni, P. (1934). Su un' origine goomctrica di tipi di clistribuzioni di frequenze. Giorn. 1st. 
Ital. Ait., 5, 219. 
McCarthy, M. I). (1939). On the application of the z-test to randomised blocks. Ann. Math. 
Slats., 10, 337. 
McCrea, W. H. (1936). A problem on random paths. Math. Gaz., 20, 311. 
McKay, A. T. (1931). Distribution of the (estimated coefficient of variation. J.R.S.S., 94, 564. 
McKay, A. T. (1932). A Bossel. function distribution. Biom., 24, 39. 
McKay, A. T., Eieller, E. C, and Pearson, E. S. (1932). Distribution of the coefficient of 
variation and extended ^-distribution. J.R.S.S., 95, 695. 
McKay, A. T. (1933). The distribution of y'j8l in samples of four from a normal universe. Biom,., 
25, 204 ; arid : The distribution of/?2 in samples of four from a normal universe. Biom,., 
25, 411. 
McKay, A. T., and Pearson, E. S. (1933). A note on the distribution of range in samples of n. 
Biom., 25, 415. 
McKay, A. T. (1.934). Sampling from batches. Supp. J.B.S.S., 1, 207. 
McKay, A. T. (1935). The distribution of the difference between the extreme observation and 
the sample mean in samples of n from a normal universe. Biom., 27, 466. 
McKinsey, J. G. C. (1939). A. note on Reichenbach's axioms for probability implication. Bull. 
Am. Math. Soc, 45, 799. 
McMullen, L. (1936). The standard deviation of a difference. Arm. Bug. Lond., 7, 105. 
Meisbner, J. (1938). Erzeugende Funktionen der Charlierschen Polynome. Math. Zeit., 44, 531. 
Mendershausen, H. (1937a). An example of meaningful curvilinear regression in economic time- 
series. Econometrika, 5, 329. 
I 
478 BIBLIOGRAPHY 
Mendershattsen, H. (19376). Annual survey of statistical technique : methods of conipiiting 
and eliminating changing seasonal fluctuations. Econometrika, 5, 234. 
Mendershatjsen, H. (1939). Clearing variates in confluence analysis. J. Am. Stat. Ass., 
34, 93. 
Menge, W. 0. (1937). A statistical treatment of actuarial functions. The Record, 26, 65. 
Merril, W. W. (1937). Sampling theory in item analyses. Psychometrika, 2, 215. 
Merrington, M. (1941). Numerical approximations to the percentage points of the 
^-distribution. Biom., 32, 200. 
Merrington, M. (1942). Tables of the percentage points of the tf-distribution. Biom., 32, 300. 
Merrington, M., and Thompson, CL M. (1943). Tables of the percentage points of the inverted 
beta (F) distribution. Biom., 33, 73. 
Merzrath, E. (1933). Anpassung von Flachen an zwei-dimensionale Kollektivgegenstande unci 
ihr Auswerkung fur die Korrelationstheorie. Metron, 11, No. 2, 103. 
Messina, L. (1933). Un teorema sulla legge uniforme dei grandi numeri. Giom. I fit. ltd. Aft., 
4, 116. 
Mihoc, G. (1934). Sur les chaines multiples discontinues. Comptes rend/us, 198, 2135. 
Mihoc, G. (1935). Sur la determination de l'intervalle de contraction de la formula do la inoyenne. 
Comptes rendus, 200, 1654. 
Miller, J. C. P. (1934). On a special case in the determination of probable errors. Month. Not. 
R. Astr.Soc, 94, 860. 
Mitchell, W. C. (1913). Business Cycles. Univ. of California Press, Berkeley. 
Mitchell, W. C, and Burns, A. F. (1935). The National Bureau's Measures of Cyclical Behaviour. 
Bull. 57, National Bureau of Economic Research. 
Moisseiev, N. (1937). Uber Stabilitatswahrscheinlichkeitsrechnung. Math. Zcit., 42, f>13. 
Molina, E. C. (1931). Bayes' theorem. Ann. Math. Stats., 2, 23. 
Molina, E. C. (1942). Tables of Poisson's Exponential Limit. Van Nostrand Co., Inc., New York. 
Mood, A. M. (1939). On the Lx test for many samples. Ann. Math. Stats., 10, 187. 
Mood, A. M. (1940). The distribution theory of runs. Ann. Math. Stats., 11, 3(>7. 
Mood, A. M. (1943). On the dependence of sampling inspection plans upon population 
distributions. Ann. Math. Stats., 14, 415. 
Moore, H. L. (1914). Economic Cycles: their law and cause. Macmillan, New York. 
Moore, H. L. (1923). Generating Economic Cycles. Macmillan, New York. 
Moore, T. V. (1937). Reduction of data showing non-linear regression for correlation by the 
ordinary product-moment formula, and the measurement of error duo to linear regression. 
/. Educ. Psych., 28, 205. 
Morant, G. M. (1921). On random occurrences in space and time when followed by a Hosed 
interval. Biom. 13, 309. 
Morant, G. M. (1939). The use of statistical methods in the investigation of problems of 
classification in anthropology. I. The general nature of the material and the form of intra-racial 
distributions of metrical characters. Biom. 31, 72. 
Morgan, W. A. (1939). A test for the significance of the difference between the two variances 
in a sample from a normal bivariate population. Biom., 31, 13. 
Mortara, G. (1934). Sulle disuguaglianze statistiche. 22'' Se ssiofid dclP 1st. Int. Stat. London. 
Mosak, J. L. (1939). The least-square standard error of the coefficient of elasticity of demand. 
J". Am. Stat Ass., 34, 353. 
Moulton, E. J. (1938). The periodic function obtained by repeated accumulation of a, statist ieal 
series. Am. Math. Monthly, 45, 583. 
Mouzon, E. D. (1930). Equimodal frequency distributions. Ann. Math. Stats., 1, 137. 
Mtjenoh, H. (1936). The probability distribution of protection test results. J. Am. Mat. Ass., 
31, 677. 
Mtjenoh, H. (1938). Discrete frequency-distributions arising from mixtures of several single 
probability values. /. Am. Stat. Ass., 33, 390. 
BIBLIOGRAPHY 479 
MItllek, J. H. (1931). On the application of continued fractions to the evaluation of certain 
integrals, with special reference to the incomplete Beta-function. Biom., 22, 284. 
MnssBLMAN, J. R. (1926). On the linear correlation ratio in the case of certain symmetrical 
freqiiency-distribxitions. Biom., 18, 228. 
Myers, R. J. (1934). Note on KoshaPs method of improving the parameters of curves by the use 
of maximum likelihood. Ann. Math. Stats., 5, 320. 
Naokl, K. (1936). The meaning of probability. J. Am. Stat. Ass., 31, 10. 
.Naik, A. N. K. (1942). On the distribution of Student's t and the correlation coefficient in samples 
from non-normal population. Sankhya, 5, 393. 
Nate, K. R. (1936). A note on the extension of lagging correlations between two random series. 
J.R.S.S., 99, 559. 
Naik, K. R. (193$a). On Tippett's random sampling numbers. Sankhya, 4, 65. 
Naik, K. R. (19386). On a method of getting confounded arrangements in the general symmetrical 
^ typo of experiment. Sankhya, 4, 121. 
Naik, K. R. (1940a). The application of covariance technique to field experiments with missing 
or mixed~np yields. Sankhya, 4, 581. 
Naik, K. R. (19406). Table of confidence intervals for the median in samples from any continuous 
population. Sankhya, 4, 551. 
Naik, K. R. (1941). Balanced confounded arrangements for the 5n type of experiment. Sankhya, 
5[» rr 
> <) / . 
Naik, K. R. (1942). A note on the method of fitting constants for analysis of non-orthogonal 
data, arranged in double classification. Sankhya, 5, 317. 
Naik, K. R., and Rao, C. R. (1942). A note on partially balanced incomplete block designs. 
Science and Culture, 7, 568. 
Naik, K. R., and Siirivastava, M. P. (1942). On a simple method of curve-fitting, Sankhya, 
6, 121. 
Naik, K. R. (1943). Certain inequality relationships among the combinatorial parameters of 
incomplete block designs. Sankhya, 6, 255. 
Naik, K. R., and Banior.jke, K. tt. (1943). A note on fitting straight lines if both variables are 
subject to error. Sankhya, 6, 331. 
Naik, U. X. (193f>). The standard error of (fin'fs mean difference. Bicmi., 28, 428. 
Naik, V. X. (1939). The application of the moment function in the study of distribution laws in 
statistics. /Horn., 30, 274. 
Naik, V. X. (1941a). Probability statements regarding the ratio of standard deviations and 
correlation coefficient in a bivariatc normal population. Sankhya, 5, 151. 
Naik, U. X. (1941/>). A comparison of tests for the significance of the difference between two 
variances. Sankhya, 5, 157. 
N aiu'Mi, X. (1923a). On the general forms of bivariatc frequency-distributions which are 
mathematically possible, when regression and variation are subjected to limiting conditions. 
Part I, Biom., 15, 77, and Part II, Biom., 15, 209. 
Nakumi, X. (19236). On further inequalities with possible application to problems in the theory 
of probability. Biom., 15, 245. 
Naykk, P. P. N. (.1930). An investigation into the application of Neyman and Pearson's Lx test, 
with tables of percentage limits. Stat. lies. Mem., 1, 38. 
New bold, E. M. (1925). Notes on an experimental test of errors in partial correlation coefficients 
derived from fourfold and biserial total coefficients. Biom., 17, 251. 
Newbold, E. M. (.1927). Practical application of the statistics of repeated events, particularly 
to industrial accidents. J.R.S.S., 90, 487. 
Newland, W. R, and Neal, E. E. (1939). Statistical control of the quality of telephone service. 
Supp. J.R.S.S., 6, 25. 
480 BIBLIOGRAPHY 
Newman, D. (1939). The distribution of range in samples from a normal population, expressed 
in terms of an independent estimate of standard deviation. Biom., 31, 20. 
Neyman,. J. (1925). Contributions to the theory of small samples drawn from a finite population, 
Biom,., 17, 472. 
Neyman, J. (1926). Further notes on non-linear regression. Biom., 18, 257. 
Neyman, J., and Peaeson, E. S. (1928). On the use and interpretation of certain test criteria for 
purposes of statistical inference. Biom., 20A, 175 and 263. 
Neyman, J., and Pearson, E. S. (1931a). Further notes on the ^-distribution. Biom., 22, 298. 
Neyman, J., and Pearson, E. S. (19316). On the problem of h samples. Bull Acad. Polonaise 
Sci. Lett. Series A, 460. <» 
Neyman, J., and Pearson, E. S. (1933a). On the testing of statistical hypotheses in relation to 
probability a priori. Proc. Camb. Phil. Soc, 29, 492. 
Neyman, J. (19336). An outline of the theory and practice of representative method applied in 
social research. Polish Inst. Social Problems. Actuarial Series, No. 1. Warsaw. 
Neyman, J., and Pearson, E. S. (1933c). On the problem of the most efficient tests of statistical 
hypotheses. Phil. Trans., A, 231, 289. 
Neyman, J. (1934). On two different aspects of the representative method, etc. J.R.S.S., 
97, 558. 
Neyman, J. (1935a). Su un teorema concernente le cosidette statistiche sufficienti. Giorn. 1st. 
Ital. Att., 6, 320. 
Neyman, J. (19356). Sur la v6rification des hypotheses statistiques composees. Bull. Soc. Math. 
France, 63, 1. 
Neyman, J., Iwaskiewicz, K.5 and Kolodzieczyk, St., (1935c). Statistical problems in 
agricultural experimentation. Supp. J.R.S.S., 2, 107. 
Neyman, J.5 and Pearson, E. S. (1936a). Sufficient statistics and uniformly most powerful tests 
of statistical hypotheses. Stat. Res, Mem., 1, 113. 
Neyman, J., and Tokarska, B. (19366). Errors of the second kind in testing c Student's ' 
hypothesis. J. Am. Stat. Ass, 31, 318. 
Neyman, J., and Pearson, E. S. (1936, 1938). Contributions to the theory of testing statistical 
hypotheses: I. Unbiassed critical regions of Type A and Type A:I. Stat. Res. Mem,, 
1,1; II. Certain theorems on unbiassed critical regions of Type A ; III. Unbiassed tests 
of simple statistical hypotheses specifying the values of more than one unknown parameter'. 
Ibid., 2, 25. 
Neyman, J. (1937a). ' Smooth test' for goodness of fit. Shand. AM., 20, 149. 
Neyman, J. (19376-). Outline of a theory of statistical estimation based on the cl assicai bxic >orv 
of probability. Phil. Trans., A, 236, 333. 
Neyman, J. (1938a). Contribution to the theory of sampling human populations. J. Am. Stat. 
j?xSS., *>o, 1U1. 
Neyman, J. (19386), Tests of statistical hypotheses which are unbiassed in the limit. Ann. Math, 
Stats., 9, 69. 
Neyman, J. (1938c). On statistics the distribution of which is independent of the parameters 
involved in the original probability law of the observed variables. Stat. Res. Mem., 
2, 58. 
Neyman, J., and Pearson, E. S. (1938d). Note on some points in ' Student's ' paper on ' 
Comparison between balanced and random arrangements of field plots.' Biom., 29, 380. 
Neyman, J. (1939a). On a new class of ' contagious ' distributions applicable in entomology and 
bacteriology. Ann. Math. Stats., 10, 35. 
Neyman, J. (19396). On the hypotheses underlying the applications of statistical methods to 
routine laboratory analysis. Ann. Math. Stats., 10, 87. 
Neyman, J. (1941a). Fiducial argument and the theory of confidence intervals. Biom., 32, 128. 
Neyman, J. (19416). On a statistical problem arising in routine analysis and in sampling 
inspections of mass production. Ann. Math. Stats., 12, 46. 
BIBLIOGRAPHY 481 
Neyman, J. (1942). Basic ideas and some recent results of the theory of testing statistical 
hypotheses. J.R.S.S., 105, 292. 
Nicholson, C. (1941). A geometrical analysis of the frequency-distribution of the ratio between 
two variables. Biom., 32, 16. 
Nicholson, C. (1943). The probability integral for two variables. Biom., 33, 59. 
Norms, N. (1935). Inequalities among averages. Ann. Math. Skits., 6, 27. 
Norbis, N. (1937). Convexity properties of generalised mean value functions. Ann. Math. 
Stats., 8, 118. 
Norms, N. (1938). Some efficient measures of relative dispersion. Ann. Math. Stats., 9, 214. 
Norms, N. (1939). The standard errors of the geometric and harmonic means. Ann. Math. 
Stats., 10, 84. 
Norris, N. (1940). The standard errors of the geometric means. Ann. Math. Stats., 11, 445. 
Norton, H. W. (1937). Use of series in an exact test of significance in a discontinuous distribution. 
Ann. Eug. Lond., 7, 349. 
Norton, H. W. (1939). The 7x7 squares. Ann. Bug. Lond., 9, 269. 
Norton, K. A. (1938). Limits to the accuracy of estimated moment coefficients. Sankhya, 
Nydell, S. (1919). The mean errors of the characteristics in logarithmic normal distributions. 
Skand. AM., 2, 134. 
Olds, E. G. (1935). Distribution of greatest variates, least variates and intervals of variation 
in samples from a rectangular universe. Bull. Am. Math. Soc, 41, 297. 
Olds, E. G. (1937). On the remainder in the approximate evaluation of the probability in the 
symmetrical case of James Bernoulli's theorem. Bull. Am. Math. Soc, 43, 806. 
Olds, E. G. (1938a). A moment-generating function which is useful in solving certain matching 
problems. Bull. Am. Math. Soc, 44, 407. 
Olds, E. G. (19386). Distribution of sums of squares of rank differences for small numbers of 
individuals. Ami. Math. Stats., 9, 133. 
Olds, E. G. (1939). Remarks on two methods of sampling inspection. Ann. Math. Stats., 10, 
87. 
Olds, E. G. (1940). On a method of sampling. Ann. Math. Stats., 11, 355. 
Olshen, 0. A. (1938). Transformations of the Pearson Type. Ill distribution. Ann. Math. Stats., 
9, 176. 
Onicesou, 0., and Mnroc, G. L. (1935-1939). Sur les eh allies do variables statistiques. Com,ptes 
rmdus, 200, 511 ; 202, 2031 ; L'alluro asymptotiquo de la sonnne des variables d'une 
chaine de Markoff discontinue. Ibid., 205, 481 ; Sur les sonunes de variables enchaines. 
Bull. Math. Soc Mourn. Sri., 41, 99. 
Oppenheim, S. (1909). Ober die Bestimmung der Peri ode einei" periodischer Erscheinung nebst 
Anwendung auf der Theorie dew Enlmagnetismus. Wicn. Silzber., 2a, 118. 
OToolig, A. L. (1931, 1932). On symmetric functions and symmetric functions of symmetric 
functions. Ann. Math. Statu., 2, 101 and : (Multivariate case), ibid., 3, 56. 
OToolu, A. L. (1933). On the system of curves for which the method of moments is the best 
method of fitting. Ann. Math. Slats., 4, 1. 
O'Toole, A. L. (1934). On the best values of r in samples of R from a finite population of N. 
Arm. Math. Stats., 5, 140. 
Ottkstad, P. (1937). On some discontinuous frequency functions and frequency distributions. 
Skand. Aid., 20, 75. 
Ottestad, P. (1939). On the use of the factorial moments in the study of discontinuous frequency 
distributions. Skand. Aid., 22, 22. 
Pae-Tsi-Yuan, (1933). On the logarithmic frequency-distribution and the semidogarithmic 
correlation surface. Ann. Math. Slats., 4, 30. 
A.S.—VOL. II. I I 
482 BIBLIOGRAPHY 
Pairman, E., and Pearson, K. (1919). On the corrections for moment coefficients of limited- 
range frequency-distributions when there are finite or infinite ordinates and any slopes 
at the terminals of the range. Biom., 12, 231. 
Palm, C. (1937). Inhomogeneous telephone traffic in full-availability groups. Ericsson Technics, 
No. 1, Stockholm. 
Panse, V. G. (1939). Preliminary studies on sampling in field experiments. Sankhya, 4, 139. 
Paulson, E. (1941). On certain likelihood ratio tests associated with the exponential distribution. 
Ann. Math. Stats., 12, 301. 
Paulson, E. (1942). An approximate normalisation of the analysis of variance distribution. 
Ann. Math. Stats., 13, 233. 
Pearl, R., and Reed, L. J. (1923). On the mathematical theory of population growth. Metron, 
3, No. 1, 6. 
Pearl, R. (1930). Introduction to Medical Biometry and Statistics. Saunders and Co., Philadelphia 
and London. 
Pearl, R., and Miner, J. R. (1935). On the comparison of groups in respect of a number of 
measured characters. Human Biology, 7, 95. 
Pearl, R. (1937). On the moment product-sums of frequency-distributions. Human Biology, 
9, 410. 
Pearse, G. E. (1928). On corrections for the moment coefficients of frequency-distributions 
when there are infinite ordinates at one or both terminals of the range. Biom,., 
20A, 314. 
Pearson, E. S. (1923). The probable error of a class-index correlation. Biom., 14, 261. 
Pearson, E. S. (1924). Note on the approximations to the probable error of a coefficient of 
correlation. Biom., 16, 196. 
Pearson, E. S. (1925). Bayes' theorem, examined in the light of experimental sampling. Biom,., 
17, 388. 
Pearson, E. S. (1926). A further note on the distribution of range in samples taken from a normal 
population. Biom., 18, 173. 
Pearson, E. S. (1927). Further note on the linear correlation ratio. Biom., 19, 223. 
Pearson, E. S., and Adyanthaya, N. K. (1928, 1929). The distribution of frequency constants 
in small samples from non-normal symmetrical and skew populations. Biom., 20A,356, 
and 21, 259. 
Pearson, E. S. (1929). Some notes on sampling tests with two variables. Biom., 21, 337. 
Pearson, E. S. (1930). A further development of tests for normality. Biom., 22, 239. 
Pearson, E. S., and Neyman, J. (1930). On the problem of two samples. Bull. Acad. Polonaise 
Sci. Lett. Series A, 73. 
Pearson, E. S. (1931a, 1932). The test of significance for the correlation coefficient. J. Am. Stat. 
Ass., 26, 128, and 27, 424. 
Pearson, E. S. (19316). The analysis of variance in cases of non-normal variation. Biom., 
M& j JL .I.tc. 
Pearson, E. S. (1932). The percentage limits for the distribution of range in samples from a 
normal population. Biom., 24, 404. 
Pearson, E. S. (1933a). Statistical method in the control and standardisation of the quality of 
manufactured products. J.B.S.S., 96, 21. 
Pearson, E. S., and Wilks, S. S. (19336). Methods of statistical analysis appropriate for Jc samples 
of two variables. Biom., 25, 353. 
Pearson, E. S. (1934). Sampling problems in industry. Supp. J.M.S.S., 1, 107. 
Pearson, E. S., and Haines, J. (1935a). The use of range in place of standard deviation in small 
samples. Supp. J.R.S.S., 2, 83. 
Pearson, E. S., and Sukbeatme, A. V. (19356). An illustration of the use of fiducial limits in 
determining the characteristics of a sampled batch. Sanhhyd, 2, 13. 
Pearson, E. S. (1935c). A comparison of /Ja and Mr. Geary's wn criterion. Biom., 27, 333. 
BIBLIOGRAPHY 483 
Pearson, E. 8. and Chandra Sekar, C. (1936). The efficiency of statistical tools and a criterion for 
the rejection of outlying observations. Biom., 28, 308. 
Pearson, E. S. (1937a). Maximum likelihood and methods of estimation. Biom., 29, 155. 
Pearson, E. S. (19376, 1938). Some aspects of the problem of randomisation. Biom., 29, 53. 
II. An illustration of ' Student's ' inquiry into the effect of balancing in agricultural 
experiments. Biom., 30, 159. 
Pearson, E. S. (1938). The probability integral transformation for testing goodness of fit and 
combining independent tests of significance. Biom., 30, 134. 
Pearson, E. S. (1939). Note on the inverse and direct methods of estimation in R. D. Gordon's 
problem. Biom., 31, 181. 
Pearson, E. S. (1941). A note on further properties of statistical tests. Biom., 32, 59. 
Pearson, E. S. (1942a). Notes on testing statistical hypotheses. Biom., 32, 311. 
Pearson, E. S., and Hartley, H. O. (1942/;). The probability integral of the range in samples 
of n observations from a normal population. Biom., 32, 301. 
Pearson, E. S., and. Hartley, H. 0. (1943). Tables of the probability integral of the ' studentised ' 
range. Biom., 33, 89. 
Pearson, K. (1894). Contributions to the mathematical theory of evolution. Phil. Trans., A, 
185, 71. 
Pearson, K. (1895). Contributions to the mathematical theory of evolution. II. Skew variation 
in homogeneous material. Phil. Trans., A, 186, 343. 
Pearson, K. (189G). Mathematical contributions to the theory of evolution. III. Regression, 
heredity and panmixia. Phil. Trans., A, 187, 253. 
Pearson, K., and Lee, A. (1897a). On the distribution of frequency (variation, and correlation) 
of the barometric heights at diverse stations. Phil. Trans., A, 190, 423. 
Pearson, K. (1897/?). Mathematical contributions to the theory of evolution. On a form of 
spurious correlation which may arises when indices arc used in the measurement of organs. 
Proc. Roy. floe, 60, 489. 
Pearson, K., and Filon, L. N. G. (1898). Mathematical contributions to the theory of evolution. 
IV. On the probable errors of frequency constants and on the influence of random selection 
on variation and correlation. Phil. Trans., A, 191, 229. 
Pearson, K., Lkk, A., and Brawley-Moora, L. (1899^/). Mathematical contributions to the 
theory of evolution. VI. Genetic (reproduction) selection. Inheritance of fertility in 
man and of fecundity in thoroughbred racehorses. Phil. Trans., A, 192, 257. 
Pearson, K. (1899/;). On certain properties of." the hypergeometrioal scries arid on the fitting of 
such series to observation polygons in the theory of chance. Phil. Mag., (5), 47, 236. 
Pearson, K. (1900a). Mathematical contributions to the theory of evolution. VII. On the 
correlation of characters not quantitatively measurable. Phil. Trans., A, 195, 1. 
Pearson, K. (19006). Mathematical contributions to the theory of evolution. VIII. On the 
inheritance of characters not capable of exact quantitative measurement. Phil. Trans., 
A, 195, 79. 
Pearson, K. (1900c). On a criterion that a given system of deviations from the probable in the 
case of a correlated system ofvariabl.es is such that it can be reasonably supposed to have 
arisen in random sampling. Phil. Mag., (5), 50, 157. 
Pearson, K., and others (1901a). Mathematical contributions to the theory of evolution. 
IX. On the principle of homotyposis, etc.. Phil. Trans., A, 197, 285. 
Pearson, K. (19016). Mathematical contributions to the theory of evolution. X. Supplement 
to a memoir on skew variation. Phil. Trans., A, 197, 443. 
Pearson, K. (1901c). On lines and planes of closest lit to systems of points in space. Phil. Mag., 
(6), 2, 559. 
Pearson, K. (1902a). Mathematical contributions to the theory of evolution. XI. On the 
influence of natural selection on the variability and correlation of organs. Phil. Trans.> 
A, 200, 1. 
484 BIBLIOGRAPHY 
Pearson, K. (19026). On the modal value of an organ or character. Biom., 1, 256. 
Pearson, K. (1902c). Note on Francis Galton's problem, Biom., 1, 390. 
Pearson, K. (1903, 1913, 1920). On the probable errors of frequency constants. Biom., 2, 
273; 9, 1 and 13, 113. 
Pearson, K. (1904). Mathematical contributions to the theory of evolution. XIII. On the 
theory of contingency and its relation to association and normal correlation. Drapers1 
Co. Res. Mem. Biometric Series I. Cambridge University Press (formerly Dulau and 
Co.). 
Pearson, K. (1905). Mathematical contributions to the theory of evolution. XIV, On the 
general theory of skew correlation and non-linear regression. Drapers' Co. Res. Mem. 
Biometric Series II. Cambridge University Press. 
Pearson, K., and Blakeman, J. (1906). On the probable error of mean-square contingency. 
Biom., 5, 191 ; (Pearson alone, 1915). Biom., 10, 570 ; (with A. W. Young, 1916) On 
the probable error of a coefficient of contingency without approximation. Biom., 11, 215. 
Pearson, K. (1907a). Mathematical contributions to the theory of evolution. XVI. On further 
methods of determining correlation. Drapers' Co. Res. Mem. Biometric Series IV. 
Cambridge University Press. 
Pearson, K. (19076). On the influence of past experience on future expectation. Phil. Mag., 
(6), 13, 365. 
Pearson, K. and Lee, A. (1908). On the generalised probable error in multiple normal correlation. 
Biom., 6, 59. 
Pearson, K. (1909). On a new method of determining correlation between a measured character 
A and a character B, etc. Biom., 6, 96. 
Pearson, K. (1910). On a new method of determining correlation when one variable is given by 
alternative and the other by multiple categories. Biom., 7, 248. 
Pearson, K. (1911a). On the probability that two independent distributions of frequency are 
really samples from the same population. Biom., 8, 250. 
Pearson, K. (19116). On a correction to be made to the correlation ratio r\. Biom., 8, 254. 
Pearson, K. (1912a). Mathematical contributions to the theory of evolution. XVIII. On a 
novel method of regarding the association of two variates classed solely in alternate 
categories. Drapers' Co. Res. Mem. Biometric Series VII. Cambridge University 
xlebo. 
Pearson, K. (19126, 1913). On the appearance of multiple cases of disease in the same house. 
Biom., 8, 404, and 9, 28. 
Pearson, K. (1913a). On the probable error of a coefficient of correlation as found from a fourfold 
table. Biom., .9, 22. 
Pearson, K. (19136). On the measurement of the influence of c broad categories ' on correlation. 
Biom., 9, 116. 
Pearson, K. and Heron, D. (1913c). On theories of association. Biom., 9, 159. 
Pearson, K. (1913a*). Note on the surface of constant association. Biom., 9, 534. 
Pearson, K., editor, (1914, 1931). Tables for Statisticians and Biometricians, Part I (1914, 3rd edn. 
1930) and Part II (1931). Cambridge University Press. 
Pearson, K., and Cave, B. M. (1914). Numerical illustrations of the variate-difference correlation 
method. Biom., 10, 340. 
Pearson, K. (1914, 1921). On an extension of the method of correlation by grades or ranks. 
Biom., 10, 416 ; and Second note, Biom. 13, 302. 
Pearson, K. (1915a). On the partial correlation ratio. Proc. Roy. Soc, A, 91, 492. 
Pearson, K. (19156). On certain types of compound frequency-distributions in which the 
components can be individually described by binomial series. Biom., 11, 139. 
Pearson, K. (1916a). Mathematical contributions to the theory of evolution. XIX. Second 
supplement to a memoir on skew variation. Phil. Trans., A, 216, 429. (Correction, Biom., 
12, 259.) 
BIBLIOGRAPHY 485 
Pearson, K. (19166). On the general theory of multiple contingency with special reference to 
partial contingency. Biom., 11, 145. 
Pearson, K., and Tocher, J. (1916c). On criteria for the existence of differential death-rates. 
Biom., 11, 145. 
Pearson, K. (1916d). On some novel properties of partial and multiple correlation coefficients in 
a universe of manifold characteristics. Biom., 11, 231. 
Pearson, K. (191.6c). On the application of "' goodness of fit ' tables to test regression curves 
(and theoretical curves) used to describe observational or experimental data. Biom., 11, 
239. (Correction, Biom., 12, 259.) 
Pearson, K. (1916/). On a brief proof of the fundamental formula for testing the goodness of 
lit of frequency-distributions and on the probable error of P. Phil. Mag., (6), 31, 369. 
Pearson, K. (1917). On the probable error of biserial rj. Biom., 11, 292. 
Pearson, K. and Young, A. W. (1918). On the product-moments of various orders of the normal 
correlation surface of two variates. Biom., 12, 86. 
Pearson, K., and P airman, E. (1919). On corrections for the moment-coefficients of limited 
range frequency-distributions when there are finite or infinite ordinates and any slopes 
at the terminals of the range. Biom., 12, 231. 
Pearson, K. (1919). On generalised TchebycherY theorems in the mathematical theory of 
statistics. Biom., 12, 284. 
Pearson, K. (1920a). The fundamental problem of practical statistics. Biom., 13, 1. 
Pearson, K. (19206). Notes on the history of correlation. Biom., 13, 25. 
Pearson, K. (1920c). On the Construction of Tables and on Interpolation. Part I. Univariate 
Tables. Part II. Bivariate Tables. Tracts for Computers, Nos. 2 and 3. Cambridge 
University Press. 
Pearson, K. (1921). On a general method of determining the successive terms in a skew regression 
line. Biom., 13, 296. 
Pearson, K. (1922a, 1923). On the %* test of goodness of fit. Biom., 14, 186; and: Further 
note, Biom., 14, 418. 
Pearson, K., and Pearson, E. S. (19226). On polychoric coefficients of correlation. Biom.t 
14, 127. 
Pearson, K., and Elderton, E. M. (1923a). On the variate-difference method. Biom., 
14, 281. 
Pearson, K. (.19236). On the correction necessary for the correlation ratio >/. Biom., 14, 412. 
Pearson, K. (1923c). Notes on skew frequency surfaces. Biom., 15, 222 ; and : On non-skew 
frequency surfaces. Mom., 15, 231. 
Pearson, K. (1924a). Note on Professor Romano vsky\s generalisation of my frequency curves. 
Biom., 16, 116. 
Pearson, K. (19246). On the moments of the hypergeornotrical series. Biom., 16, 157. 
Pearson, K. (1924c) On a certain double hypergeornotrical series and its representation by 
continuous frequency surfaces. Biom., 16, 172. 
Pearson, Iv. (1924cZ). On the mean error of frequency-distributions. Biom., 16, 198. 
Pearson, K. (1924e). Historical note on the origin of the normal curve. Biom., 16, 402. 
Pearson, K. (1925a). The fifteen-constant bivariate frequency surface. Biom., 17, 268. 
Pearson, K. (19256). On first-power methods of finding correlation. Biom,., 17, 459. 
Pearson, K. (1926a). Researches on the mode of distribution of the constants of samples taken 
at random from a bivariate normal population. Proc. Hoy. tioc, A, 112, 1. 
Pearson, K. (19266). On the coefficient of racial likeness. Biom., 18, 105. 
Pearson, K., Jefeery,G. B., and Elderton, E. M. (1929). On the distribution of the first 
product-moment coefficient in samples drawn from an indefinitely large normal population. 
Biom., 21, 164. 
Pearson, K. (1931a). On the nature of the relationship between two of 'Student's' variates 
(Sj_ and 2a) when samples are taken from a bivariate normal population. Biom., 22, 405 ; 
486 BIBLIOGRAPHY 
Some properties of 'Student's' z. Biom., 23, 1; and: Further remarks on the s-test. 
Biom., 23, 408. 
Pearson, K. (19316). Appendix to a paper by Professor Tokishige Hojo. On the standard error 
of the median to a third approximation, etc. Biom., 23, 361. 
Pearson, K., and Pearson, M. V. (1931c, 1932). On the mean character and variance of a ranked 
individual, and on the mean and variance of the intervals between ranked individuals. 
Part I. Symmetrical Distributions (Normal and Rectangular). Biom., 23, 364. Part II. 
Case of certain skew curves. Biom., 24, 203. 
Pearson, K. (1931a*). Historical note on the distribution of the standard deviation of samples 
of any size drawn from an indefinitely large normal parent population. Biom., 23, 416. 
Pearson, K., Stoxjffer, S. A., and David, F. N. (1932a). Further applications in statistics of the 
Tm(x) Bessel function. Biom., 24, 293. 
Pearson, K. (19326). Experimental discussion of the (#2, P) test for goodness of fit. Biom., 24,351. 
Pearson, K. (1933a). On the applications of the double Bessel function Krxr%{x) to statistical 
problems. Part I. Theoretical. Biom., 25, 158. 
Pearson, K. (19336). On a method of determining whether a sample of given size n supposed to 
have been drawn from a parent population having a known probability integral has 
probably been drawn at random. Biom., 25, 379. 
Pearson, K. (1934). On a new method of determining ' goodness of fit'. Biom., 26, 425. 
Pearson, K. (1935). On the corrections for broad categories, being a note on Mr. Wisniewski's 
memoir. Biom., 27, 364. 
Pearson, K. (1936). Method of moments and method of maximum likelihood. Biom., 28, 34. 
Peek, R. L. (1937). Test of an observed difference in the frequency of two results. J. Am. Stat. 
Peierls, R. S. (1935). Statistical error in counting experiments. Proc. Roy. Soc, A, 149, 4()7. 
Peiser, M. A. (1943). Asymptotic formulae for significance levels of certain distributions. Ann. 
Math. Stats., 14, 56. 
Pepper, J. (1929). Studies in the theory of sampling. Biom., 21, 231. 
Pepper, J. (1932). The sampling distribution of the third moment coefficient—an experiment. 
Biom., 24, 55. 
Perlo, V. (1933). On the distribution of ' Student's ' ratio for samples of three drawn from a 
rectangular population. Biom. 25, 203. 
Persons, W. M. (1928). The construction of index numbers. Houghton Mifflin, Cambridge, Mass. 
Pietra, G. (1925). The theory of statistical relations, with special reference to cyclical series. 
Metron, 4, No. 3-4, 383. 
Pietra, G. (1932a). Nuovi contributi alia metodologia degli inclici di variabilita e di concentrazione. 
Att. R. 1st. Veneto di Sci., 989. 
Pietra, G. (19326). Dell' interpolazione parabolica nel caso in cui entrainbi i valori delle variabili 
sono affetti da errori accidentali. Metron, 9, Nos. 3-4, 77. 
Pietra, G. (1934). Statistica. (2 vols.) GiufTre, Milan. 
Pitman, E. J. G. (1936). Sufficient statistics and intrinsic accuracy. Proc. Gamb. Phil. Soc, 
32, 567. 
Pitman, E. J. G. (1937a, 1938). Significance tests which may be applied to samples from any 
population. Supp, J.R.S.S., 4, 119 ; II. The correlation coefficient test. Supp.J.R.S.S. 
4, 225; III. The analysis of variance test. Biom., 29, 322. 
Pitman, E. J. G. (19376). The < closest ' estimates of statistical parameters. Proc. Gamb, Phil. 
Soc, 33, 212. 
Pitman, E. J. G. (1939a). The estimation of location and scale parameters of a continuous 
population of any given form. Biom., 30, 391. 
Pitman, E. J. G. (19396). Tests of hypotheses concerning location and scale parameters. Biom., 
31, 200. 
Pitman, E. J. G. (1939c). A note on normal correlation. Biom., 31, 9. 
BIBLIOGRAPHY 487 
Pizzetti, E. (1939). Osservazioni sulle medie esponenziali e baso-esponenziali. Metron, 13, 
No. 4, 3. 
Poisson, S. D. (1837). Becker ches sur la probability des jugements, etc. Paris. 
Pollak, L. W. (1926). Rechentafeln zur harmonischen Analysis. Barth, Leipzig. 
Pollak, L. W. (1927). Periodogramme hochfrequenten Schwankungen meteorologischer 
Elemente. Met. Zeit., 4, 121. 
Pollak, L. W., and Kaisek, P. (1935). Methode numerique de J. Fuhrich pour le calcul des 
periodicites, sa mise a l 'epreuve et son application aux mouvements polaires. Rev. Stat. 
Tchechoslovaque, 16, 13. 
Pollard, H. S. (1934). On the relative stability of the median and arithmetic mean, with 
particular reference to certain frequency-distributions which can be dissected into normal 
distributions. Ann. Math. Stats., 5, 227. 
P6lya, G. (1920). t)ber den zentralen Grenzwertsatz der Wahrscheinlichkeitsrechnung und das 
Momentproblem. Math. Zeit., 8, 173. 
P6lya, G. (1923). Herleitung des Gauss'schen Gesetzes aus einer Punktionalgleichung. Math. 
Zeit, 18, 96. 
P6lya, G. (1931). Sur quelques points de la theorie des probabilites. Ann. Inst. H. Poincare, 
P6lya, G. (1937). Zur Kinematik der Geschiebebewegung. Mitt. Versuchs. Wasserbau an der 
Eid. Tech. Hochschule. Zurich. 
P6lya, G. (1938a). Sur redetermination d'un probleme voisin du probleme des moments. Gomptes 
rendus, 207, 708. 
P6lya, G. (19386). Sur la promenade au hasard dans un reseau de rues. Actualites Scientifiques 
et Industrielles, No. 734, Paris. Hermann et Cie. 
Powell, R. W. (1930). Successive integration as a method for finding long-period cycles. Ann. 
Math. Stats., 1, 123. 
Peetorius, S. J. (1930). Skew bivariate frequency surfaces examined in the light of numerical 
illustrations. Biom., 22, 109. 
Prokofovic, S. N. (1935). La correlation des series quantitatives. Rev. Stat. Tchechoslovaque, 
16, 04. 
Przyborowski, J. and WiLfosKi, BL (1935a). Sur les erreurs de la premiere et de la seconde 
categoric- dans la verification des hypotheses concernant la loi de Poisson. Gomptes 
rendus, 200, 1460. 
Przyborowski, J., and W.il(jjnskt, H. (1935/;). Statistical principles of routine work in testing 
clover seed for dodder. Biom., 27, 273. 
Przyborowski, J., and WiLfesKi, H. (1930). 'Mote on the application of a theorem of Frau 
Pollaczek-Geiringer. Biom., 28, 187. 
Przyborowski, J., and WilHinsiu, H. (1940). Homogeneity of results in testing samples from 
Poisson series, etc. Biom., 31, 313. 
Quensel, G. E. (1930). A method of determining the regression curve when the marginal 
distribution is of the normal logarithmic type. Ann. Math. Stats., 7, 196. 
Quensel, G. E. (1938). The distributions of the second moment and of the correlation coefficient 
in samples from populations of Type A. Lwnds Univ. Arsshr., N.P. 34, 4, 1. 
Raikov, I>. (1938). On the decomposition of Gauss and Poisson laws. Bull. Acad. Sci. U.S.S.R. 
Sir. Math., 1, 91. 
Reed, L. J. (1.922). Fitting straight lines. Metron, 1, No. 3, 54. 
Regan, P. (1936,1938). The application of the theory of admissible numbers to time-series with 
constant probability. Trans. Am. Math. Soc, 36, 511 ; and: The application of the 
theory of admissible numbers to time-series with variable probability. Am. J. Maths., 
58, 867. 
488 BIBLIOGRAPHY 
Reichenbach, H. (1935). Wahrscheinlichkeitslehre, Leiden. 
Reichenbach, H. (1937). Les fondements logiques du calcul des probabilites. Ann. Inst. H* 
Poincare, 7, 267. 
Reiersol, 0. (1940). A method for recurrent computation of all the principal minors of a deter- 
* minant and its application in confluence analysis. Ann. Math. Stats., 11, 193. 
Reiersol, 0. (1941). Confluence analysis by means of lag moments and other methods of 
confluence analysis. Econometriha, 9, 1. 
Rhodes, E. C. (1921). Smoothing. Tracts for Computers, No. 6. Cambridge University Press. 
Rhodes, E. C. (1923, 1925). On a certain skew correlation surface. Biom., 14, 355, and 17* 
Rhodes, E. C. (1924, 1925). On the problem whether two given samples can be supposed to have 
been drawn from the same population. Biom., 16, 239 and Metron, 5, 3. 
Rhodes, E. C. (1925). On sampling. Metron, 5, Nos. 2-3, 3. 
Rhodes, E. C. (1927). The precision of means and standard deviations when the individual errors 
are correlated. J.B.S.S., 90, 135. 
Rhodes, E. C. (1928). On the normal correlation function as an approximation to the distribution 
of paired drawings. J.B.S.S., 91, 548. 
Rhodes, E. C. (1930). On the fitting of parabolic curves to statistical data. J.B.S.S., 93, 569. 
Rhodes, E. C. (1936). The precision of index numbers. J.B.S.S., 99, 142. 
Rice, S. 0. (1938). Van Uven's theorem in probability theory and a self-reciprocal Hankel 
transform. Quart. J. Maths, 9, 1. 
Rice, S. 0. (1939). The distribution of the maxima of a random curve. Am. J. Maths., 61, 409. 
Richards, H. I. (1931). Analysis of the spurious effect of high intercorrelation of independent 
variables on regression and correlation coefficients. J. Am. Stat. Ass., 26, 21. 
Ricker, W. E. (1937). The concept of confidence or fiducial limits applied to the Poisson frequency. 
J. Am. Stat. Ass., 32, 349. 
Rider, P. R. (1929). On the distribution of the ratio of mean to standard deviation in small 
samples from non-normal populations. Biom., 21, 124. 
Rider, P. R. (1931a). On small samples from certain non-normal universes. Ann. Math. Stats., 
Rider, P. R. (19316). A note on small sample theory. J. Am. Stat. Ass., 26, 172. 
Rider, P. R. (1932). On the distribution of the correlation coefficient in small samples. Biom., 
24, 382. 
Rider, P. R. (1933). Criteria for rejection of observations. Washington University Studies, 
New Series, Science and Technology, No. 8. 
Rider, P. R. (1934). The third and fourth moments of the generalised Lexis theory. Metron, 
12, No. 1, 185. 
Rider, P. R. (1936). Annual survey of statistical technique : Developments in the analysis of 
multivariate data. Econometriha, 4, 264. 
Rietz, H. L., editor (1924), Handbook of Mathematical Statistics. Houghton Mifflin, Boston. 
Rietz, H. L. (1931a). Note on the distribution of the standard deviation of sets of three variates 
drawn at random from a rectangular distribution. Biom., 23, 424. 
Rietz, H. L. (19315). On certain properties of frequency-distributions obtained by a linear 
fractional transformation of the variates of a given distribution. Ann. Math. Stats., 
2, 38. 
Rietz, H. L. (1932). A simple non-normal correlation surface. Biom., 24, 288. 
Rietz, H. L. (1937). Some topics in sampling theory. Bull. Am. Math. Soc, 43, 209. 
Rietz, H. L. (1938). On a recent advance in statistical inference. Am. Math. Monthly, 45, 149. 
Rietz, H. L. (1939). On the distribution of the ' Student ' ratio for small samples from certain 
non-normal populations. Ann. Math. Stats., 10, 265. 
Risser, R. (1935-7). Expose des principes de la statistique mathematique. J. Soc. Stat. Paris, 
76, 281 ; 77, 337 ; 78, 40. 
BIBLIOGRAPHY 489 
Ritchie-Scott, A. (1918). The correlation coefficient of a polychoric table. Biom., 12, 93. 
Ritchie-Scott, A. (1921). The incomplete moments of a normal solid. Biom., 13, 401. 
Robb, R. A. (1929). The variate-difference method of seasonal variation. /. Am. Stat. Ass., 
24, 250. 
Robb, R. A. (1930). Modifications of the link relative and interpolation methods of determining 
seasonal variation. Ann. Math. Stats., 1, 352. 
Robinson, S. (1933). An experiment regarding the #2-test. Ann. Math. Stats., 4, 285. 
Roff, M. (1937). Relation between results obtainable with raw and corrected correlation 
coefficients in multiple factor analysis. Psychornetriha, 2, 35. 
Romanovsky, V. (1923). Note on the moments of a binomial about its mean. Biom., 15, 410. 
Romanovsky, V. (1924). Generalisation of some types of the frequency-curves of Professor 
Pearson. Biom., 16, 106. 
Romanovsky, V. (1925a). On the moments of standard deviation and of correlation coefficient- 
in samples from normal. Metron, 5, No. 4, 3. 
Roman ovsky, V. (19256). On the moments of the hypergeometrical series. Biom., 17, 57. 
Romanovsky, V. (1926). On the distribution of the regression coefficient in samples from normal 
population. Bull. Acad. Sci. U.S.S.B., (6), 10, 643. 
Romanovsky, V. (1927). Note on orthogonalising series of functions and interpolation. Biom., 
19, 93. 
Romanovsky, V. (1928). On the criteria that two given samples belong to the same normal 
population. Metron, 7, No. 3, 3. 
Romanovsky, V. (1929). On the moments of means of functions of one and more random variables. 
Metron, 8, Nos. 1-2, 251. 
Romanovsky, V. (1931a). Sulla probability, a posteriori. (Horn. 1st. Ital. Att., 2. 
Romanovsky, V. (19316). Sulle regressione multiple. Giorn. 1st. Ital. Ait., 2. 
Romanovsky, V. (1931c, 1932a, 1933ft). Generalisations d'im theoreme de M. Slutzky. Comptes 
rendus, 192, 718 ; Sur la loi sinusoidale limite. Rend. Girc. Mat. Palermo, 56, 1 ; Sur 
une generalisation de la loi sinusoidale limite. Ibid., 57. 
Romanovsky, V. (19326). Due nuovi criteri di controllo sulF andamento casuale di una successione 
di valori. Giorn. 1st. Ital. Att., 3, 203. 
Romanovsky, V. (19336). On a property of the mean ranges in samples from a normal population 
and on some integrals of Professor Hojo. Biom., 25, 195. 
Romanovsky, V. (1934). Su due problemi di distribuzione casuale. Giorn. 1st. Ital. Att., 5,196. 
Romanovsky, V. (1936a). Recherches sur les chaines de Markoff. Acta Math., 66, 147. 
Romanovsky, V. (19366). Note on the method of moments. Biom., 28, 188. 
Romanovsky, V. (1938). Analytical inequalities and statistical tests. (Russian, with English 
summary). Bull. Acad. Sci. U.S.S.Jt. Ser. Math., 4, 457. 
Roos, C. F. (1934). Dynamic Economics. Bloomington, Indiana. 
Roos, C. F. (193()). Annual survey of statistical technique. The correlation and analysis of time- 
series. Econometriha, 4, 308. 
Roos, C. F. (1937). A general invariant criterion of fit for lines and planes where all variates are 
subject to error. Metron, 13, No. .1, 3. 
Roy, S. N. (1938). A geometrical note on the use of rectangular co-ordinates in the theory of 
sampling distributions connected with a multivariate normal population. Sanhhyd, 
Roy, 8. N. (1939a) A note on the distribution of the Studentised D2-statistic. Sanhhyd, 4, 373. 
Roy, S. N. (19396). ^-statistics, or some generalisations on analysis of variance appropriate to 
multivariate problems. Sanhhyd, 4, 381. 
Roy, S. N. (1942a). The sampling distribution of ^-statistics and certain allied statistics on the 
non-null hypothesis. Sanhhyd, 6, 15. 
Roy, S. N. (19426). Analysis of variance for multivariate normal populations, etc. Sanhhyd, 
6, 35. 
490 BIBLIOGRAPHY 
Salvemini, T. (1934). Ricerche sperimentali sull5 interpolazione grafica di istogrammi. Metron, 
11, No. 4, 83. 
Salvemini, T. (1939). L' indice di cograduazione del Gini nel caso di serie statistiche con ripeti- 
zioni. Metron, 13, No. 4, 27. 
Salvosa, L. R. (1930). Tables of Pearson's Type III function. Ann. Math. Stats., 1, 191. 
Samuelson, P. A. (1942). A method of determining explicitly the coefficients of a characteristic 
equation. Ann. Math. Stats., 13, 424. 
Samuelson, P. A. (1943). Pitting general Gram-Charlier series. Ann. Math. Stats., 14, 179. 
Sandon, F. (1924). Note on the simplification of the calculation of abruptness coefficients to 
correct crude moments. Biom., 16, 193. 
Sansone, G. (1933). La chiusura dei sistemi ortogonali di Legendre, di Laguerre e di Hermite 
rispetto alle funzioni di quadrati sommabili. Giom. 1st. Ital. Att.} 4, 71. 
Sasuly, M. (1934). Trend Analysis of Statistics. Brookings Institution, Washington, D.C. 
Satterthwaite, F. E. (1943). Generalised Poisson distribution. Ann. Math. Stats., 13, 410. 
Savur, S. R. (1937a). The use of the median in tests of significance. Proc. Indian Acad. Sci., 
A, 5, 564. 
Savur, S. R. (19376). A new solution of a problem in inverse probability. Proc. Indian Acad. 
Sci., A, 5, 222. 
Savuk, S. R. (1939). A note on the arrangement of incomplete blocks when k = 3 and X = 1. 
Ann. Bug. Lond., 9, 45. 
Savur, S. R. (1941). A test of significance in approximate periodogram analysis. Sankhyd, 6, 
77. 
Scheffe, H. (1942a). On the theory of testing composite hypotheses with one constraint. Ann. 
Math. Stats., 13, 280. 
Scheffe, H. (19426). On the ratio of the variances of two normal samples. Ann. Math. Stats., 
13, 371. 
Scheffe, H. (1943). Statistical inference in the non-parametric case. Ann. Math. Stats., 14, 
305. 
Schmidt, E. (1934). t)ber die Charlier-Jordansche Entwicklung einer willkurlicher Funktion naoh 
der Poissonsche Funktion und ihrer Ableitungen. Zeit. ang. Math, und Mech., 13, 139. 
Schmidt, R. (1934). Statistical analysis of one-dimensional distributions. Ann. Math. Stats., 
5, 30. 
Schultz, H. (1930). The standard error of a forecast from a curve. J. Am. Stat. Ass., 25, 139. 
Schultz, H. (1933). The standard error of the coefficient of elasticity of demand. J. Am,. Stat. 
Ass., 28, 64. 
Schultz, H. (1939). A misunderstanding in index number theory : the true Konos condition on 
cost-of-living index numbers and limitations. Econometrika, 7, 1. 
Schultz, T. W., and Snedecor, E. (1933). Analysis of variance as an effective method of handling 
the time element in certain economic statistics. J. Am,. Stat. Ass., 28, 14. 
Schumann, T. E. W. (1938). A general graduation formula for the smoothing of time series. 
Phil. Mag., 26, 970. 
Schumann, T. E. W., and Hofmeyer, W. L. (1942). The problem of auto-correlation of 
meteorological time-series, etc. Q.J. Met. Soc, 68, 177. 
Schuster, Sir Arthur (1898). On the investigation of hidden periodicities with application to 
a supposed 26-day period of meteorological phenomena. Terr. Mag., 3, 13. 
Schuster, Sir Arthur (1899). The periodogram of the magnetic declination as obtained from 
the records of the Greenwich Observatory during the years 1871-1895. Trans. Camb. 
Phil Soc, 18, 107. 
Schuster, Sir Arthur (1906). On the periodicities of sunspots. Phil. Trans., A, 206, 69. 
Scukarev, A. N. (1932). Uber die Mechanik der Massenprozesse (Kollektivgegenstandlehre). 
Metron, 9, Nos. 3-4, 139. 
Seal, H. L. (1940). Tests of a mortality table graduation. J. Inst. Act, 71, No. 330. 
BIBLIOGRAPHY 491 
Segal, J. E. (1938). Fiducial distribution of several parameters with application to a normal 
system. Proc. Camb. Phil. Soc, 34, 41. 
Sheffer, H. M. (1935). Concerning some methods of best approximation and a theorem of 
Birkhoff. Am. J. Maths., 57, 587. 
Sheppard, W. F. For bibliography see Ann. Bug. Lond., 1937, 8, 13. 
Sheppard, W. F. (1898a). On the application of the theory of error to cases of normal distributions 
and normal correlations. Phil. Trans., A, 192, 101, and Proc. Boy. Soc, 62, 170. 
Sheppard, W. F. (18986). On the calculation of the most probable values of frequency constants 
for data arranged, according to equidistant divisions of a scale. Proc. Lond. Math. Soc.% 
29, 353. 
Sheppard, W. F. (1914). Fitting of polynomials by the method of least squares. Proc. Lond. 
Math. Soc, (2), 13, 97. 
Sheppard, W. F. (1929). The fit of a formula for discrepant observations. Phil. Trans., A, 
228, 199. 
Sheppard, W. F. (1939, posthumous). The Probability Integral. British Ass. Math. Tables, vol. 7. 
Cambridge University Press. 
Shewhart, W. A., and Winters, F. W. (1928). Small samples—new experimental results. 
./. Aw,. Stat. Ass., 23, 144. 
Shewhart, W. A. (.1931). The Economic Control of Quality of a Manufactured Product, van 
Nostrand, New York. 
Shohat, J. (1929). Inequalities for moments of frequency functions and for various statistical 
constants. Biom., 21, 361. 
Shohat, J. (1930). Stieltjes integrals in mathematical statistics. Ann. Math. Stats., 1, 73. 
Shohat, J. (1935). On the development of functions in series of orthogonal polynomials. Bull. 
Am. Math. Soc, 41, 49. 
Simaika, J. B. (1941). On an optimum property of two important statistical tests. Biom., 
32, 70. 
Simaika, J. B. (1942). Interpolation for fresh probability levels between the standard table levels 
of a function. Biom., 32, 263. 
Simon, 11. A. (1943). Symmetric tests of the hypothesis that the mean of one normal population 
exceeds that of another. Ann. Math. Stats., 14, 149. 
Simon, L. E. (1941). The Engineer's Manual of Statistical Methods. John Wiley, New York. 
Simonsun, W. (1937). On the distributions of certain functions of samples from a multivariate 
infinite population. Stand. Aid,, 20, 200. 
Sipos, A. (1930). Practical application of Jordan's method for trend measurement. Hornyansky, 
.Budapest. 
Slijtzky , E. (1914). On the criterion of goodness of lit of regression lines and on the best method of 
fitting them to data. J.R.S.S., 77, 78. 
Slijtzky, E. (1925). Ober stochastische Asymptoten und. Gronzwerte. Metron, 5, No. 3, 3. 
Slutzkv, K. (1.934). Alcune applicazioni dei coefficionti di Fourier all' analisi dellefunzioni aloatorie 
stazionarie. Giorn. 1st. Jtal. Att., 5, 435. 
Slutzkv, K. (1937a). Qualcune proposizione relativa alia teoria delle funzioni aloatorie. Giorn. 
1st. Ital Alt., 8, 183. 
Slutzkv, E. (19376). The summation of random causes as the source of cyclic processes. Econo- 
metrika, 5, 105. 
Smirnoff, N. (1935). Uber die Vertoilung des allgemeinen Gliodes in dor Variationsreihe. Metron, 
12, No. 2, 59. 
Smirnoff, N. (1936). Sur la distribution do ro2. Gomples rendas, 202, 449. 
Smith, 0. 1). (1930). On generalised Tchebycheff inequalities in mathematical statistics. Am. 
J. Maths., 52, 109. 
Smith, C. D. (1939). On Tchebycheff approximations for decreasing functions. Ann. Math. Stats. 
10, 190. 
492 BIBLIOGRAPHY 
Smith, H. Fairfield (1936). A discriminant function for plant selection. Ann. Bug. Lond., 
7, 240. 
Smith, K. (1916). On the e best' values of the constants in frequency-distributions. Biom., 
11, 262. 
Smith, K. (1918). On the standard deviation of the adjusted and interpolated values of an observed 
polynomial function and its constants etc. Biom., 12, 1. 
Smith, K. (1922). The standard deviations of fraternal and parental correlation coefficients. 
Biom., 14, 1. 
Snedecor, G. W., and Irwin, M. R. (1933). On the chi-square test for homogeneity. Iowa State 
College J. Sci., 8, 75. 
Snedecor, G. W., and Cox, G. M. (1934a). Disproportionate sub-class numbers in tables of 
multiple classification. Iowa Agr. Exp. Station Res. Bull. No. 180. 
Snedecor, G. W. (19346). Calculation and Interpretation of Analysis of Variance and Covariance. 
Collegiate Press, Ames, Iowa. 
Snedecor, G. W. (1935). Analysis of covariance of statistically controlled grades. J. Am. Stat.. 
Ass., 30, Supp., 263. 
Snow, E. C. (1911). On restricted lines and planes of closest fit to systems of points in any number 
of dimensions. Phil. Mag. (6), 21, 367. 
Solomon, R. S. (1939). An index of conformity based on the /-curve hypotheses. Sociomeiry, 
2, 63. 
Soper, H. E. (1913). On the probable error of the correlation coefficient to a second 
approximation. Biom., 9, 91. 
Soper, H. E. (1914). On the probable error of the bi-serial expression for the correlation coefficient. 
Biom., 10, 384. 
Soper, H. E. and others (1917). On the distribution of the correlation coefficient in small, samples. 
Biom., 11, 328. 
Soper, H. E. (1922). Frequency Arrays. Cambridge University Press. 
Soper, H. E. (1929a). The general sampling distribution of the multiple correlation coefficient. 
J.R.S.S., 92, 445. 
Soper, H. E. (1929&). The interpretation of periodicity in disease prevalence. J.R.8.8., 92, 34. 
' Sophister ' (1928). Discussion of small samples from an infinite skew universe. Blow., 20A, 
389. 
Spearman, C. (1906). A footrule for measuring correlation. Brit. J. Psych., 2, 89. 
Spearman, C. (1907). Demonstration of formulas for true measurement of correlation. Am. J. 
Psych., 18, 161. 
Spearman, C. (1910). Correlation calculated from faulty data. Brit. J. Psych, 3, 27J. 
Starkey, D. M. (1938). A test of significance of the difference between means of samples from two 
normal populations without assuming equal variances. Ann. Math. Stats., 9, 201. 
Starkey, D. M. (1939). The distribution of the multiple correlation coefficient in pCM'iodogram 
analysis. Ann. Math. Stats., 10, 327. 
Steffensen, J. F. (1923). Matematisk Iagtagelseslaehre. Copenhagen. 
Steffensen, J. F. (1930). Some Recent Researches in the Theory of Statistics and Actuarial Science. 
Cambridge University Press. 
Steffensen, J. F. (1934). On certain measures of dependence between statistical variables. 
Biom., 26, 251. 
Steffensen, J. F. (1936). Free functions and the Student-Fisher theorem. Shard. Akt., 19, 108. 
Steffensen, J. F. (1937). On the semi-normal distribution. Shand. Akt., 20, 60. 
Steinhaxts, H. (1923). Les probability denombrables et leur rapport a la theorie de la mesure. 
Fund. Math., 4, 286. 
Stekloff, W. (1914). Quelques applications nouvelles de la theorie de fermeture au probleme 
de representation approchee de fonctions et au probleme des moments. Mem. Acad, 
Imp. Sci. St. Pet, 32, No. 4. 
BIBLIOGRAPHY 493 
Stebne, T. E. (1934). The accuracy of least-square solutions. Proc. Nat. Acad. Sci., 20, 565 
and 601. 
Stevens, W. L. (1936). The analysis of interference. J. Genetics, 32, 5L 
Stevens, W. L. (1937a). The truncated normal distribution. Ann. Appl. Biol., 24, 847. 
Stevens, W. L. (19376). Significance, of grouping. Ann. Eug. Lond., 8, 57. 
Stevens, W. L. (1938a). The distribution of entries in a contingency table with fixed marginal 
totals. Ami. Eug. Lond., 8, 238. 
Stevens, W. L. (19386). The completely orthogonalised Latin square. Ann. Eug. Lond., 9, 82. 
Stevens, W. L. (1939a). Solution to a geometrical problem in probability. Ann. Eug. Lond., 
9, 315. 
Stevens, W. L. (19396). Tests of significance for extra-sensory perception data. Psych. Rev., 
JL xj* * ,„L jtateurf • 
St. Geoeoescu, N. (1932). Further contributions to the sampling problem. Biom., 24, 65. 
Stieltjes, J. (1918). Recherches sur les fractions continues. (Euvres, Groningen. 
Stock, J. S., and Feankel, L. R. (1939). The allocation of samplings among several strata. Ann. 
Math. Stats., 10, 288. 
•Stouefer, S. A., and Tibbits, C. (1.933). Tests of significance in applying Westergaard's method 
of expected cases to sociological data. J. Am. Stat. Ass., 28, 293. 
•Stouffer, S. A. (1934). A coefficient of combined partial correlation with an example from 
sociological data. J. Am. Stat. Ass., 29, 70. 
Stouffer, S. A. (193n7if.). Evaluating the effect of inadequately measured variables in partial 
correlation analysis. J. Am. Stat Ass., 31, 348. 
Stouffer, ft. A. (19366). Reliability coefficients in a correlation matrix. Psychometrika, 1, 17. 
"'Student' (W. ft. Gosset) (1907). On the error of counting with a hemacytometer. Biom., 
\J , rtt) I . 
' Student ? (1908a). On the probable error of a mean. Biom., 6, 1. 
''Student1 (I908/>). On. the probable error of a correlation coefficient. Biom., 6, 302. 
'Student' (1909). On the distribution of means of samples which are not drawn at random. 
Biom-., 7, 210. 
' Student ' (1913). The correction to be made to the correlation ratio for grouping. Biom., 
9, 310. 
4 Student ' (1914). The elimination of spurious correlation due to position in time or space. 
Biom., 10, 179. 
' Student ' (1919). An explanation of deviations from PoissoiTs law in practice. Biom., 12, 211. 
''Stu.df/nt ' (1921). An experimental determination of the probable error of Dr. Spearman's 
correlation coeflicient. Biom., 13, 203. 
'Student1 (1927). Errors of routine analysis. Biom., 19, 151. 
'Student' (1931u). On the rj-test. Biom., 23, 407. 
c Student ' (19316). Yield trials. Article in Bailliere's Encyclopedia, of Scientific Agriculture. 
' Student ' (193b*). The Lanarkshire milk experiment. Biom., 23, 398. 
'Student' (193S). Comparison between balanced and random arrangements of field plots. 
Biom., 29, 301. 
Stumpff, K. (1920)). F<».hlerthcoretischc Imtersuchungon zur Periodogrammanalyse. Astir. Nacli., 
226, 378. 
Stumpff, K. (1937). Grundlagcn und Mcthodcn dcr Pvriodv.nforxchimg. Berlin. 
Subramanian, S. (1935). On a property of partial correlation. J.R.S.S., 98, 129. 
Sukhatme, P. V. (1935). Contribution to the theory of the representative method. Supp. 
J.R.S.S., 2, 253. 
Sukhatme, P. V. (1936a). A contribution to the problem of two samples. Proc. Indian Acad. 
Sci., 2, A, 584. 
.Sukhatme, P. V. (193b7>). On the analysis of k samples from exponential populations with especial 
reference to the problem of random intervals. Stat. Res. Mem., 1, 94. 
494 BIBLIOGRAPHY 
Stohatme, P. V. (1937a). Tests of significance for samples of the %2 population with two degrees 
of freedom. Ann. Eug. Lond., 8, 52. 
Sukhatme, P. V. (19376). The problem of k samples for Poisson population. Proc. Nat. Inst. 
Sci. India, 3, 297. 
Sukhatme, P. V. (1938a). On the distribution of %2 in samples from a Poisson series. Supp. 
J.R.S.S., 5, 75. 
Sukhatme, P. V. (19386). On Pisher and Behrens' test of significance for the difference in means 
of two normal samples. Sankhya, 4, 39. 
Sukhatme, P. V. (1938c). On bipartitional functions. Phil. Trans. Roy. Soc, A, 237, 375. 
Sukhatme, P. V. (1944). Moments and product-moments of moment-statistics for samples of the 
finite and infinite populations. Sankhya, 6, 363. 
Swaroop, S. (1938). Tables of the exact values of probabilities for testing the significance of 
differences between proportions based on pairs of small samples. Sankhya, 4, 73. 
Swed, F. S., and Eisenhart, C. (1943). Tables for testing randomness in a sequence of 
alternatives. Ann. Math. Stats., 14, 66. 
Tang, P. C. (1938). The power function of the analysis of variance tests with tables and 
illustrations of their use. Stat. Res. Mem., 2, 126. 
Tang, Y. (1938). Certain statistical problems arising in plant breeding. Biom., 30, 29. 
Tappan, M. (1927). On partial multiple correlation coefficients in a universe of manifold 
characteristics. Biom., 19, 39. 
Tartler, A. (1935). On a certain class of orthogonal polynomials. Am. J. Maths., 57, 627. 
Tauoer, R. (1936). I fenomeni di selezione a la teoria dei gruppi. Giorn. 1st. Ital. Alt., 7, 16. 
Tohebychefp, P. L. (1907) (Euvres. 2 vols. St. Petersbourg: including : Sur une formule 
d'analyse, 1, 701 (1854) ; Sur les fractions continues, 1, 203 (1856) ; Sur une nouvelle 
serie, 1, 381 (1858) ; Sur l'interpolation par la methode des moindres carres, 1, 478 
(1859) ; Sur l'interpolation des valeurs equidistantes, 2, 2.19 (1875). 
Tedin, 0. (1931). The influence of systematic plot arrangements upon the estimate of error in 
field experiments. J. Agr. Sci., 21, 19.1. 
Thiele, T. N. (1931). Theory of Observations. Reprint in Ann. Math. Stats., 2, 165, of the English 
version published in 1903. 
Thompson, C. M., Pearson, E. S., Comrie, L. J., and Hartley, H. O. (1941a). Tables of 
percentage points of the incomplete beta-function. Biom., 32, 151. 
Thompson, C. M. (19416). Tables of percentage points of the ^-distribution. Biom,., 32, 187. 
Thompson, W. R. (1933). On the likelihood that one unknown probability exceeds another in 
view of the evidence of two samples. Biom., 25, 286. 
Thompson, W. R. (1935). On a criterion for the rejection of observations and the distribution of 
the ratio of deviation to sampling standard deviation. Ann. Math. Stats., 6, 21.4. 
Thompson, W. R. (1936). On confidence ranges for the median and other expectation 
distributions for populations of unknown distribution form. Ann. Math. Skits., 7, 122. 
Thompson, W. R. (1938). Biological applications of normal range and associated significance) 
tests in ignorance of original distribution forms. Ann. Math. Stats., 9, 281. 
Thomson, G. H. (1916). A hierarchy without a general factor. Brit. J. Psych, 8, 271. 
Thomson, G. H. (1919a). The criterion of goodness of fit of psychophysical curves. Biom., 12, 
216. 
Thomson, G. H. (19195). On the degree of perfection of hierarchical, order among correlation 
coefficients. Biom., 12, 355, and (correction), 15, 150. 
Thomson, G. H. (1935). On complete families of correlation coefficients, etc. Brit. J. Psych., 
26, 63. 
Thomson, G. H. (1939). The factorial analysis of ability. Brit. J. Psych., 30, 71 and 105. 
Thorndike, E. L. (1937). On correlations between measurements which are not normally 
distributed. /. Educ. Psych., 28, 367. 
BIBLIOGRAPHY 495 
Thotiless, R. H. (1939). The effect of errors of measurement on correlation coefficients. Brit. 
J. Psych, 29, 383. 
Thixrstone, L. L. (1935). Vectors of Mind. Chicago. 
Thttbstone, L. L. (1938). A new rotational method in factor analysis. Psychometrika, 43, 199. 
Tinbergen, J. (1937). An Econometric Approach to Business Cycle Problems. Paris. 
Ttkbergen, J. (1938). On the theory of business cycle control. Econometrika, 6, 22. 
Tintfner, G. (1935). Prices and the Trade Cycle. Vienna. 
Tiotner, G. (1940). The Variate-Dijference Method. Bloomington Press, Indiana. 
TnsronsrER, G. (1941). The variate-difTerence method : a reply. Econometrika, 9, 163. 
Tippett, L. H. C. (1925). On the extreme individuals and the range of samples taken from a normal 
population. Biom., Y7, 364. 
Tippett, L. H. 0. (1931). The Methods of Statistics. Williams and Norgate, London. 2nd edn. 
1937. 
Tippett, L. H. G. (1932). A modified method of counting particles. Proc. Roy. Soc, A, 
137, 434. 
Tippett, L. H. C. (1935). Some applications of statistical methods to the study of variation of 
quality in the production of cotton yarn. Supp. J.R.S.jS., 2, 27. 
Todhunter, I. (1865). ^4 History of the Mathematical Theory of Probability from the time of Pascal 
to that of Laplace. Macmillan, London. 
Torni.ee, E. (1929). Wahrscheinlichkeitsrechnung unci Zahlentheorie. J. rein, und ang. Math., 
160, 177. 
Tornibr, E. (1930). Die Axiome der Wahrscheinlichkeitsrechnung. J. rem. und ang. Math., 
163, 45. 
Tornier, E. (1933). Grimdlagen der Wahrscheinlichkeitsrechnung. Acta Math,., 60, 239. 
Tornier, E. (1936). Wahrscheinlichkeitsrechnung und allgerneine Integnttionstheorie. Teubner, 
Leipzig. 
Tornijbr, E. (1937). Verallgenieinerung des Riicksohluss-Satzes der Wahrscheinlichkeitsrechnung. 
Deutsche Math., 2, 469. 
Traohtenburu, H. L. (1921). Analysis of the poriodogram. J.U.S.8., 84, 578. 
Travers, R. JYI. W. (1939). The use of a discriminant function in the treatment of psychological 
group-differences. Psychometrika, 4, 25. 
Treloar, A. F., and Wilder, M. A. (1934). The adequacy of "Student's' criterion of deviations 
in small sample means. Ann. Math. Stats., 5, 324. 
Tricomi, F. (193f>, 1936a). Su la rappresentazione di una legge di probability mediant© espone'nziali 
di Gauss e la transformazione di Laplace. (Horn. 1st. Ital. Ait., 6, 135 and 7, 42. 
Tricomi, F. (19366). Sulla media- dei valori assoiuti di errori soguonte la legge di Gauss. Giorn. 
1st. Ital AtL, 7, 280. 
Tricomi, F. (1937). Sul rapporto fra la media dei quadrati di piu errori e il quadrato della media 
dei loro valori assoiuti. Giorn. 1st. Ital. Alt., 8, 68 and 127. 
Tricomi, F. (193S). Les transformations do Fourier, Laplace, Gauss et leurs applications au calcul 
des probabilities ot a la statistique. Ann. Inst. II. Poincare, 8, 111. 
Tritksa, L. (1940). The simultaneous distribution in samples of mean and standard deviation and 
of mean and variance. Biom., 31, 256. 
Tschuprow, A. A. (1918, 1919a). Zur Theorie der Stabilitat statistischer Reihen. Skand. Akt., 
1, 199 (1918), and 2, 80 (1919). 
Tschuprow, A. A. (19186, 1921, 1923). On the mathematical expectation of the moments of 
frequency-distributions. Biom., 12, 140 and 185 ; Biom., 13, 283 ; and Matron, 2, 
No. 3, 461 and No. 4, 646. 
Tschuprow, A. A. (1925). Grundbegriffe und Grundprobleme der Korrelationstheorie. Teubner, 
Leipzig. (English translation as The Mathematical Theory of Correlation, William Hodge, 
1939.)' 
Tschuprow, A. A., trans, by L. Isserlis (1928). The mathematical theory of the statistical 
496 BIBLIOGRAPHY 
methods employed in the study of correlation in the case of three variables. Tmns. 
Camh Phil Soc. 23 337. 
Tschuprow, A. A. (1934)." The mathematical foundations of the methods to be used in sliiiistiml 
investigation of the dependence between two chance variables. Norilsk Statist it:/ ithlmft, 
5, 34. 
Turner, H. H. (1913). Tables for facilitating the, use of harmonic analysis. Oxford University 
Press. 
Urban, F. M. (1918). Tiber den Begriff der mathematischen Wahrseheinlichkeit, Yinlvtjahr- 
schrift fur Wiss. Phil, and Soz., 10. 
Uspensky, J. V. (1937). Introduction to Mathematical Probability, McGraw-Hill, New York and 
London. 
Vajda, S. (1939). Die Wahrseheinlichkeit einer bestimmten Auszahhmgssumme. Stand. Akf.f 
22, 10. 
van der Pol, B. (1930). Oscillations sinusoidales et de relaxation. (Uonda vlhiriqw\ juin-juillef, 
Chiron, Paris. 
van Kampen, E. R. (1937a). On the addition of convex curves and the densities "of certain infinite 
convolutions. Am. J. Maths. 59, 679. 
van Kampen, E. R. and Wintner, A. (19376, 1937c). Convolutions of distributions on convex 
curves and the Riemann zeta-function. Am. J. Maths., 59, 175 ; and : On divergent 
infinite convolutions. Ibid. 59, 635. 
van Kampen, E. R. (1939<z). On the asymptotic distribution of a uniformly almost periodic 
function. Am. J. Maths. 61, 729. 
van Kampen, E. R. and Wintner, A. (19396). A limit theorem for probability distributions on 
lattices. Am. J. Maths. 61, 965. 
van Uven, M. J. (1932). Compensazione degli errori di un rapporto. Mvtmn, 10, No. 3, 
185. 
van Uven, M. J. (1939). Adjustment of a ratio. Ann. Eug. Land., 9, ISI. 
Venn, J. A. (1888). The Logic of Chance. 3rd edn., Macmillan, London. (Out of print.) 
Vernon, P. E. (1936). A note on the standard error in the eontingeney-inatrhinu: technique. 
J. Educ. Psych., 27, 704. 
Villars, D. S., and Anderson, T. W. (1943). Some significance tests for normal bivariate 
distributions. Ann. Math. Stats., 14, 141. 
Ville, J. A. (1936a, b). Sur les suites indifferentes. Comptes rendus, 202, 1393 ; and : Sur la 
notion de collectif. Ibid., 203, 26. 
Ville, J. A. (1936c). Sur la convergence de la mediane ties n premiers result ats d'une suite infiuie 
d'epreuves ind6pendantes. Comptes rendus, 203, 1309. 
Ville, J. A. (1939). Etude critique de la notion de collectif. Paris : These. 
VrNCi, F. (1920). Sui coefficienti di variabilita, Metron, 1, No. 1, n'2. 
Vinci, F. (1934). Significant developments in business cycle theory. Econonalrika, 2, 12o. 
Volterra, V. (1936). Les equations des fluctuations biologiques et le calcnl des variations. 
Comptes rendus, 202, 1935; Les equations canoniques. J hid,, 202, 2023; and: Sur 
l'integration des equations. Ibid., 202, 2113. 
von Bortkiewicz, L., (1898). Das Gesetz der Jcleinen ZaMen, Teubncr, Leipzig 
von Bortkiewicz, L. (1910). Zur Verteidigung des Gesetzes der kleinen Zahien. Jahrh. Sat. 
Ok, und Stat., (3), 39, 218. 
von Bortkiewicz, L. (1915a). Uber die Zeitfolge zufalligerEreignisse. Ball Inst. Int. th> Stat. 
20, 2e livre. 
von Bortkiewicz, L (19156). Realismus und Formalismus in der mathematischen StaUsiik. 
Allg. Stat. Arkiv., 9, 225. 
von Bortkiewicz, L. (1917). Die Iterationen. Berlin. 
BIBLIOGRAPHY 497 
von Boetkiewicz, L. (1922). Das Helmertsche Verteilunggesetz fur die Quadratsumme zufalliger 
Beobachtungsfehler. Zeit. ang. Math, und Mech. 2, Heft 5. 
von Boetkiewicz, L. (1931). The relation between stability and homogeneity. Ann. Math, 
Stats. 2, 1. 
von Mises, R. (1919a, 6). Fundamentalsatze der Wahrscheiniichkeitsrechmmg. Math. Zeit., 
4, 1 and : Grundlagen, ibid., 5, 52. 
von Mises, R. (1921). Das Problem der Iterationen. Zeit. ang. Math, und Mech., 1, 298. 
yon Mises, R. (1928). Wahrscheinlichkeit, Statistik und Wahrheit. Springer, Berlin; 3rd rev. 
edn., 1936 ; trans, as Probability, Statistics and Truth, 1939. W. Hodge, London. 
yon Mises, R. (1931). Wahrscheiniichkeitsrechmmg. Deuticke, Wien. 
yon Mises, R. (1933). Uber Zahlenfolge die em Kollektivahnliches Verhalten zeigen. Math. 
Ann., 108, 757. 
yon Mises, R. (1936a). Sul concetto di probability fondato sul limite di frequenze relative. 
Giorn. 1st. Ital. Att., 7, 235. 
von Mises, R. (19366). Les lois de probability pour les fonctions statistiques. Ann. Inst. H. Poincare, 
6, 185. 
yon Mises, R. (1937). Bestimmung einer Verteilung durch ihre ersten Momente. Skand. Akt, 
20, 220. 
yon Mises, R. (1938). A modification of Bayes' problem. Ann. Math. Stats,, 9, 256. 
yon Mises, R. (1939a). The limits of a distribution function if two expected values are given. 
Ann. Math. Stats., 10, 99. 
yon Mises, R. (19396). An inequality for the moments of a discontinuous distribution. Skand. 
Akt., 22, 32. 
von Mises, R. (1939c). Uber Aufteilungs- und Besitzungs-Wahrscheinlichkeiten. Rev. Fac. Set. 
Univ. Istamhoul, (4), Fasc. 1-2, 145. 
von Misios, R. (1941). On the foundations of probability and statistics. Ann. Math. Stats, 
12, 191. 
von Nkumann J., and others (1941a, 6). The mean-square successive difference. Ann. Math. 
Stats., 12, 153 ; (von Neumann alone) : Distribution of the ratio of the mean-square 
successive difference to the variance. Ibid., 12, 367 ; von Neumann and Habt, B. I. : 
Tabulation of the probabilities for the ratio of the mean-square successive difference to 
the variance. Ibid., 13, 207. 
von Sohiollino, H. (1934). Die Konzentration einer Verteilung und ihre Abhangigkeit von den 
(irenzen des Variationsbereiches. Mctron, 11, No. 4, 3. 
von Szelisiu, V. 8. (1929). Experiments in the correlation of time-series. ,/. Am. Stat. Ass,, 
24, Supp., 241. 
Wald, A. (1930r/.). Berechnmig und Ausschaltwng von Salsonschwanktmgeri. Beitnlge z/ur Kon- 
junkturforschung. Oester. Inst. Konj., 9, 35. 
"Wald, A. (19306). Sur la notion de collectif dans lo calcul des probability. (Joniptes rendus, 
202, 180. 
Wald, A, (1937). Die Widerspruchsfreihcit des Kollektivbegriffs der WahrscIu^inlichkeitFireehnung. 
Krgeb. math. Kolloqu., Hamburg, No. 8, 38. 
Wald, A. (1938). A generalisation of Markoff's inequality. Ann. Math. Stats., 9, 244. 
Wald, A. (1939a). Contributions to the theory of statistical, estimation and testing hypotheses. 
Ann. Math. Stats., 10, 299. 
Wald, A., and Wolfowitz, J. (19396). Confidence limits of continuous distribution functions. 
Ann. Math. Stats., 10, 105. 
Wald, A. (1940a). The fitting of straight lines if both variables are subject to error. Ann. Math. 
Stats., 11, 284. 
"Wald, A. (19406). A note on the analysis of variance with unequal class-frequencies. Ann. 
Math. Stats., 11, 96. 
A.S.—VOL. II. K K 
498 BIBLIOGRAPHY 
Wald, A. and Wolfowitz, J. (1940c). On a test whether two samples are from the same 
population. Ann. Math. Stats., 11, 147. 
Wald, A. (1941a). Asymptotically most powerful tests of statistical hypotheses. Ann. Math. 
Stats., 12, 1 and 396. 
Wald, A., and Bbookmk, R. J. (19416). On the distribution of Wilks' statistic, etc. Ann. 
Math. Stats., 12, 137. 
Wald, A., and Wolfowitz, J. (1941c). Note on confidence limits for continuous distribution 
functions. Ann. Math. Stats., 12, 118. 
Wald, A. (1941a"). On the analysis of variance in case of multiple classifications with unequal 
class frequencies. Ann. Math. Stats., 12, 346. 
Wald, A. (1942a). Asymptotically shortest confidence intervals. Ann. Math. Stats., 13, 127. 
Wald, A. (1943). On the efficient design of statistical investigations. Ann. Math. Stats., 14, 134. 
Walker, Sir Gilbert (1914). On the criterion for the reality of relationships or periodicities. 
Calcutta Ind. Met. Mems., 21, part 9. 
Walker, Sir Gilbert (1925). On periodicity. Q. J. Boy. Met. Soc, 51, 387. 
Walker, Sir Gilbert (1927). On periodicity and its existence in European weather. Mem. 
Boy. Met. Soc, 1, No. 9. 
Walker, Sir Gilbert (1931). On periodicity in series of related terms. Proc. Boy. Soc, A, 
131, 518. 
Walker, H. M. (1929). Studies in the History of Statistical Method. Williams and Wilkins, 
Baltimore. 
Walker, H. M., and Sanford, V. (1934). The accuracy of computation with approximate 
numbers. Ann. Math. Stats., 5, 1. 
Wallace, N., and Travers, R. M. W. (1938). A psychometric sociological study of a group of 
speciality salesmen. Ann. Eug. Lond., 8, 266. 
Wallis, W. A. (1939). The correlation ratio for ranked data. </. Am. Stat. Ass., 34, 533. 
Wallis, W. A., and Moore, G. H. (1941). A significance test for time-series. Technical Paper 
No. 1. Nat. Bur. Ec. Research. 
Wallis, W. A. (1942). Compounding probabilities from independent significance tests. Econo- 
metrika, 10, 229. 
Watkins, G. P. (1933). An ordinal index of correlation. J. Am. Stat. JLss., 28, 139. 
Waugh, F. V. (1942). Regressions between sets of variables. Econometriha, 10, 290. 
Webster, M. S. (1938). Orthogonal polynomials with orthogonal derivations. Bull. Am. Math. 
Soc, 44, 880. 
Weida, F. M. (1934). On measures of contingency. Ann. Math. Stats., 5, 308. 
Weida, F. M. (1935). On certain distribution functions when the law of the universe is Poisson's 
first law of error. Ann. Math. Stats., 6, 102. 
Weiss, M. G., and Cox, G. M. (1939). Balanced incomplete block and lattice-square designs for 
testing yield differences among large numbers of soya bean varieties. Iowa Agr. Exp. 
Stat. Bes. Bull, 257, 289. 
Welch, B. L. (1935). Some problems in the analysis of regression among k samples of two variables. 
Biom., 27, 145. 
Welch, B. L. (1936a). Note on an extension of the Lx test. Stat. Bes. Mem., 1, 52. 
Welch, B. L. (19366). Specification of rules for rejecting too variable a product, etc. Supp. 
J.B.S.S., 3, 29. 
Welch, B. L. (1937). On the 2-test in randomised blocks and Latin squares. Biom,,, 29, 21. 
Welch, B. L. (1938a). On tests for homogeneity. Biom., 30, 149. 
Welch, B. L. (19386). The significance of the difference between two means when the population 
variances are unequal. Biom., 29, 350. 
Welch, B. L. (1939a). On confidence limits and sufficiency with particular reference to parameters 
of location. Ann. Math. Stats., 10, 58. 
Welch, B. L. (19396). Note on discriminant functions. Biom., 31, 218. 
BIBLIOGRAPHY 499 
Welch, B. L. (1939c). On the distribution of maximum likelihood estimates. Biom., 31, 187. 
Wertheimer, A. (1932). A generalised error function. Ann. Math. Stats., 3, 64. 
Wertheimer, A. (1937). Note on Zocb's paper on the postulate of the arithmetic mean. Ann. 
Math. Stats., 8, 112. 
Wherry, R. J. (1935). The shrinkage of the Brown-Spearman prophecy formula. Ann. Math. 
Stats., 6, 183. 
Whitaker, L. (1914). On Poisson's law of small numbers. Biom., 10, 36. 
Whittaker, E. T., and Robinson, G. (1940). The Calculus of Observations. 3rd edn. Blackie 
& Sons. 
Whit worth, W. A. (1901). Choice and Chance. 5th edn. Deighton Bell and Co. Cambridge. 
Wicksell, 8. D. (1917a). On logarithmic correlation with an application to the distribution of 
ages at first marriage. Medd. Lunds Astr. Obs., No. 84. 
Wicksell, 8. D. (19176). The correlation function of Type A. Kungl. Svenska Vetenskapsakad. 
Handl. Bd. 58 ; Medd. Lunds Astr. Obs. Series 2, No. 17. 
Wicksell, 8. D. (1921). An exact formula for spurious correlation. Metron, 1, No. 4, 33. 
Wicksell, 8. I). (1933). On correlation functions of Type III. Biom., 25, 121. 
Wicksell, S. I). (1934a). Expansions of frequency functions for integer variates in series. SJcand. 
Matematikercongressen i Stockholm, p. 306. 
Wicksell, 8. I). (19346). Analytical theory of regression. Medd. Lunds Astr. Obs. Series 2, 
No. 09. 
Widder, I). V. (1934). The inversion of the Laplace integral and the related moment problem. 
Trans. Am. Math. Soc, 36, 107. 
Wiener, N. (1930). Generalised harmonic analysis. Acta Math., 55, 117. 
Wiener, N. (1938). The homogeneous chaos. Am. J. Math., 60, 897. 
Wilks, 8. S. (1932a). Moments and distributions of estimates of population parameters from 
fragmentary samples. Ann. Math. Stats., 3, 163. 
Wilks, 8. 8. (19326). On the sampling distribution of the multiple correlation coefficient. Ann. 
Math. Stats., 3, 196. 
Wilks, 8. 8. (1932c). On the distribution of statistics in samples from a normal population of two 
variables with matched sampling for one variable. Metron, 9, Nos. 3-4, 87. 
Wilks, 8. 8. (1932$). The standard error of a tetrad in samples from a normal population of 
independent variables. Proc. Nat. Acad. Sci., 18, 502. 
Wilks, 8. S. (1932e). Certain generalisations in the analysis of variance. Biom., 24, 471. 
Wilks, 8. 8. (1934). Moment-generating operators for determinants of product-moments in 
samples from a normal system. Ann. Math., 35, 312. 
Wilks, 8. 8. (1935a). The likelihood test of independence in contingency tables. Ann. Math. 
Stats., 6, 190. 
Wilks, 8. 8. (1935/;) On the independence of k sets of normally distributed statistical variables. 
Econometrika, 3, 309. 
Wilks, 8. 8. (1935c;). Test criteria for statistical hypotheses involving several variables. J. Am. 
Stat. Ass., 30, 549. 
Wilks, 8. 8. (193(5). The sampling theory of systems of variances, covariances and intra-clasa 
covariances. Ann. J. Math., 58, 426. 
Wilks, 8. 8., and Thompson, 0. M. (1937a). The sampling distribution of the criterion Au-x when 
the hypothesis tested is not true. Biom,., 29, 124. 
Wilks, 8. 8. (19376). The analysis of variance for two or more variables. Third Ann. Conf. 
Econ. Stat. Colorado Springs, p. 82. 
Wilks, 8. S. (1938a). The large-sample distribution of the likelihood ratio for testing composite 
hypotheses. Ann. Math. Stats., 9, 60. 
Wilks, 8. 8. (19386). Shortest average confidence intervals from large samples. Ann. Math, 
Stats., 9, 166. 
Wilks, S. 8. (1938c). Fiducial distributions in fiducial inference. Ann. Math. Stats., 9, 272. 
500 BIBLIOGRAPHY 
Wilks, S. S. (1938d5). Weighting systems for linear functions of correlated variables when there 
is no dependent variable. Psychometrika, 3, 23. 
Wilks, S. S. (1938e). The analysis of variance and covariance in non-orthogonal data. Metron, 
13, No. 2, 141. 
Wilks, S. S. (1939a). Optimum fiducial regions for simultaneous estimation of several population 
parameters for large samples. (Abstract). Ann. Math. Stats., 10, 85. 
Wilks, S. 8., and Daly, J. F. (19396). An optimum property of confidence regions associated 
with the likelihood function. Ann. Math. Stats., 10, 225. 
Wilks, S. S. (1941). On the determination of sample sizes for setting tolerance limits. Ann. 
Math. Stats., 12, 91. 
Wilks, S. S. (1943). Mathematical Statistics. Princeton University Press. 
Williams, C. B. (1937). The use of logarithms in the interpretation of certain entomological 
problems. Ann. App. Biol., 24, 404. 
Williams, J. D. (1941). Moments of the ratio of the mean-square successive difference to the mean- 
square difference in samples from a normal population. Ann. Math. Stats., 12, 239. 
Wilsdon, B. H. (1934). Discrimination by specification statistically considered and illustrated 
by the standard specification for Portland cement. Supp. J.B.S.S., 1, 152. 
Wilson, E. B. (1928). On hierarchical correlation systems. Proc. Nat. Acad. Sci., 14, 283. 
Wilson, E. B., and Hilfekty, M. M. (1931a). The distribution of chi-square. Proc. Nat. Acad. 
Sci., 17, 694. 
Wilson, E. B.} Hilferty, M. M., and Maher, H. C. (19316). Goodness of fit. J. Am,. Stat. Ass., 
26, 443. 
Wilson, E. B. (1938). The standard deviation of sampling for life expectancy. J. Am. Stat. Ass., 
33, 705. 
Wintner, A. (1934a). On the addition of independent distributions. Am,. J. Maths., 56, 8. 
Wintner, A. (19346). On the asymptotic differential distribution of almost periodic and related 
functions. Am. J. Maths., 56, 401. 
Wintner, A. (1935). Papers on convergent convolutions. Am. J. Maths., 57, 363, 821, 827, 
839; and Bull. Am. Math. Soc, 41, 137. 
Wintner, A. (1936). On a class of Fourier transforms. Am. J. Maths., 58, 45. 
Wishart, J. (1926). On Romanovsky's generalised frequency curves. Biom., 18, 221. 
Wishart, J. (1927). On the approximate quadrature of certain skew curves with an account of 
the researches of Thomas Bayes. Biom., 19, I. 
Wishart, J. (1928). The generalised product-moment distribution in samples from a normal 
multivariate population. Biom., 20A, 32. 
Wishart, J. (1929<x). The correlation between product-moments of any order in samples from 
a. normal population. Proc. Boy. Soc. Edin., 49, 1. 
Wishart, J. (19296). A problem of combinatorial analysis giving the distribution of certain 
moment-statistics. Proc. Land. Math. Soc, 29, 309. 
Wishart, J. (1930). The derivation of certain high-order sampling product-moments from a 
normal population. Biom., 22, 224. 
Wishart, J. (1931a). Notes on frequency constants. »/. Inst. Act., 62, 174. 
Wishart, J. (19317;). The mean and second-moment coefficient of the multiple correlation 
coefficient in samples from a normal population. Biom., 22, 353. 
Wishart, J. (1932a). A note on the distribution of the correlation ratio. Biom., 24, 441. 
Wishart, J., and Bartlett, M. $. (19326). The distribution of second-order moment coefficients 
in small samples. Proc. Gamb. Phil. Soc, 28, 455. 
Wishart, J. (1933a). The theory of orthogonal polynomial fitting. J.R.S.S., 96, 487. 
Wishart, J. (19336). A comparison of the semi-invariants of the distributions of moment and 
semi-invariant estimates in samples from an infinite population. Biom., 25, 52. 
Wishart, J., and Bartlett, M. S. (1933c). The generalised product-moment distribution in a 
normal system. Proc. Camb. Phil. Soc, 29, 260. 
BIBLIOGRAPHY 501 
Wishart, J. (1934a). Statistics in agricultural research. Supp. J.B.S.S., 1, 26. 
Wishart, J. (19346). Bibliography of agricultural statistics. Supp. J.B.S.S., 1, 95. 
Wishart, J., and Sanders, H. G. (1935). Principles and Practice of Field Experimentation: 
Empire Cotton-growing Corporation, London. 
Wishart, J. (1936). Tests of significance in analysis of covariance. Supp. J.B.S.S., 3, 79. 
Wishart, J. (1938). Field experiments of factorial design. J. Agr. Sci., 28, 299. 
Wishart, J. (1939). Statistical treatment of animal experiments. Supp. J.R.S.S., 6, 1. 
Wisniewski, J. (1934). Interdependence of cyclical and seasonal fluctuation. Econometrika, 
2, 176. 
WiSNiEWSKi, J. (1935, 1936). On the validity of a certain Pearson's formula. Biom., 27, 356; 
and.: Rejoinder. Biom., 28, 190. 
Wisniewsici, J. (1937a). A problem in least squares. Ann. Math. Stats., 8, 145. 
Wisniewski, J. (19376). A note on inverse probability. J.R.S.S., 100, 417. 
Wold, H. (1934a). Sulle correzione di Sheppard. Oiom. 1st. Ital. AM., 4, 304. 
Wold, H. (19345). Sheppard's correction formulae in several variables. Skand. AJct,, 
Wold, H. (1935). A study on the mean difference, concentration curves and concentration ratio. 
Metron, 12, No. 2, 39. 
Wold, H. (1936). On quantitative statistical analysis. Skand. A Jet., 19, 281. 
Wold, H. (1938a). A Study in the Analysis of Stationary Tims-Series. Almquist and Wiksells, 
Uppsala. 
Wold, H. (19386). On the inversion of moving averages. Skand. AM., 21, 208. 
Wold, H. (1939). Ober stochastische Prozesse, inbesondere solohe stationarer Natur. 9 Gong, 
des Math. Scand. Helsingfors, p. 207. 
Woleowitz, J. (1942). Additive partition functions and a class of statistical hypotheses. Ann. 
Math. Stats., 13, 247. 
Wolkowitz, J. (1943). On the theory of runs with some applications to quality control. Ann. 
Math. Stats., 14, 280. 
Wong, Y. K. (1935). An application of the orthogonalisation process to tho theory of least squares. 
Ann. Math. Stats., 6, 53. 
Wono, Y. K. (1937). On the elimination of variables in multiple correlation. J. Am. Stat. Ass., 
%JM , «>f) / . 
Woodbury, M. A. (1940). Rank correlation when there are equal, variates. Ann, Math. Stats., 
A A , »)t )o. 
Working, H. and Hotelunc, H. (1929). Applications of the theory of error to the interpretation 
of trends. J. Am. Stat. Axs., 24, Supp., 73. 
Wright, »S. (1934). The method of path coefficients. Ann. Math. Stats., 5, 161. 
Yasukawa, K. (1925). On the means, standard deviations, correlations and 
frequency-distributions of functions of variates. Biom., 17, 21L 
Yasukawa, K. (1926). On tho probables error of the mode of frequency-distributions. Biom., 
18, 203. 
Yas ukawa, K. (1934). On the deviation from normality of the frequency-distributions of functions 
of normally distributed variates. Tokohti Math. J., 38, 465. 
Yates, F. (1933a). Tho principles of orthogonality and confounding in replicated experiments. 
,/. Agr. Sci., 23, 108. 
Yates, F. (1933/>). The analysis of replicated experiments when the field results are incomplete. 
Emp. J. Exp. Agr., 1, 129. 
Yates, F. (1933c). The formation of Latin squares for use in field experiments. Emp. J. Exp. 
Agr., 1, 235. 
"Yates, F. (1934a). The analysis of multiple classifications with unequal numbers in the different 
classes. J. Am. Stat. Ass., 29, 51. 
502 BIBLIOGRAPHY 
x 
Yates, F. (19345). Contingency tables involving small numbers and the #2-test. Supp. J.R.S.S., 
Yates, F. (1935a). Some examples of biassed sampling. Ann. Eug. Lond., 6, 202. 
Yates, F. (19356). Complex experiments. Supp. J.R.S.S., 2, 181. 
Yates, F., and Zacopanay, I. (1935c). The estimation of the efficiency of sampling, with special 
reference to sampling for yield in cereal experiments. J. Agr. Sci., 25, 545. 
Yates, F. (1936a). Incomplete Latin squares. J. Agr. Sci., 26, 301. 
Yates, F. (19366). Incomplete randomised blocks. Ann. Eug. Lond., 7, 121. 
Yates, F. (1936c). Applications of the sampling technique to crop estimation and forecasting. 
Trans. Manchester Stat. Soc, 103. 
Yates, F. (1936c?). A new method of arranging variety trials involving a large number of varieties. 
J. Agr. Sci., 26, 424. 
Yates, F. (1937a). A further note on the arrangement of variety trials. Quasi-Latin squares. 
Ann. Eug. Lond., 7, 319. 
Yates, F. (19376). The design and analysis of factorial experiments. Imp. Bur. Soil Sci. Tech. 
Comm., No. 35. 
Yates, F. (1938a). The gain in efficiency resulting from the use of balanced designs. Supp* 
J.R.S.S., 5, 70. 
Yates, F., and Cochran, W. G. (19386). The analysis of groups of experiments. J. Agr. Sci., 
28, 556. 
Yates, F. (1938c). Orthogonal functions and tests of significance in the analysis of variance. 
Supp. J.R.S.S., 5, 177. 
Yates, F. (1939a). The recovery of inter-block information in variety trials arranged in three- 
dimensional lattices. Ann. Eug. Lond., 9, 136. 
Yates, F., and Hale, R. W. (19396). The analysis of Latin squares when two or more rows, 
columns or treatments are missing. Supp. J.R.S.S., 6, 67. 
Yates, F. (1939c). The adjustment of the weights of compound index numbers based on inaccurate 
data. J.B.S.S., 102, 285. 
Yates, F. (1939c?). Tests of significance of the differences between regression coefficients derived 
from two sets of correlated variates. Proc. Roy. Soc. Edin., 59, 184. 
Yates, F. (1939e). The comparative advantages of systematic and randomised arrangements in 
the design of agricultural and biological experiments, Biom., 30, 440. 
Yates, F. (1939/). An apparent inconsistency arising from tests of significance based on fiducial 
distributions of unknown parameters. Proc. Camh. Phil. Soc, 35, 579. 
Yates, F. (1940). The recovery of inter-block information in balanced incomplete block designs. 
Ann. Eug. Lond., 10, 317. 
Young, A. W., and Pearson, K. (1916). On the probable error of a coefficient of contingency 
without approximation. Biom., 11, 215. (Correction, Biom., 12, 259.) 
Young, L. C. (1941). On randomness in ordered sequences. Ann. Math. Stats., 12, 293. 
Yule, G. U. (1897a). On the significance of Bravais' formula? for regression, etc., in the case of 
skew correlation. Proc. Roy. Soc, A, 60, 477. 
Yule, G. U. (18976). On the theory of correlation. J.R.S.S., 60, 812. 
Yule, G. U. (1900). On the association of attributes etc. Phil. Trans., A, 194, 257. 
Yule, G. U. (1906). On a property which holds good for all groupings of a normal distribution, 
etc. Proc. Roy. Soc, A, 77, 324. 
Yule, G. U. (1907). On the theory of correlation for any number of variables treated by a new 
system of notation. Proc. Roy.^Soc, A, 79, 182. 
Yule, G. U. (1910). On the interpretation of correlations between indices or ratios. J.R.S.S., 
73, 644. 
Yule, G. U. (1912). On the methods of measuring the association between two attributes. 
J.R.S.S., 75, 579. 
Yule, G. U. (1921). On the time-correlation problem. J.R.S.S., 84, 497. 
BIBLIOGRAPHY 503 
Yule, G. U. (1922). On the application of the %2 method to association and contingency tables, 
with experimental illustrations. J.R.S.S., 85, 95. 
Yule, G. U. (1926). Why do we sometimes get nonsense correlations between time-series, etc. ? 
J.R.S.S., 89, 1. 
Yule, G. U. (1927a). On a method of investigating periodicities in disturbed Series, with special 
reference to Wolfer's sunspot numbers. Phil. Trans., A, 226, 267. 
Yule, G. U. (19276). On reading a scale. J.R.S.S., 90, 570. 
Yule, G. U. (1936). On a parallelism between differential coefficients and regression coefficients. 
J.R.S.S., 99, 770. 
Yule, G. U. (1938a). A test of Tippett's random sampling numbers. J.R.S.S., 101, 167. 
Yule, G. U. (19386). On some properties of normal distributions, univariate and bivariate, based 
on sums of squares of frequencies. Biom., 30, 1. 
Zaycoff, R. (1936). Uber die Zerlegung statistischer Zeitreihen in drei Komponenten. Stat. Inst. 
Econ. Res. Univ. Sofia, No. 4. 
Zaycoff, R. (1937). Uberdie Ausschaltung der zufalligen Komponente nach der Variate-difference- 
Methode. Stat. Inst. Econ. Res. Univ. Sofia, No. 1. 
Ziaud-Din, M. (1938). On differential operators developed by O'Toole. Ann. Math. Stats>3 
9, 63. 
Zoch, R. T. (1934). Invariants and covariants of certain frequency curves. Ann. Math. Stats., 
5, 124. 
Zoch, R. T. (1935, 1937). On the postulate of the arithmetic mean. Ann. Math. Stats., 6, 171; 
and : Reply to Mr. Wertheimer's paper. Ibid., 8, 117. 
Zrzavy, F. J. (1933). Ausschaltung von Saisonschwankungen mittels Lag-correlation. Monatsber. 
der Oest. Inst, fur Konjunldurforschung. Wien. 
INDEX 
(References are to pages. The abbreviations " JV.i?." and " BibL" refer to the Notes 
and References and to the Bibliography respectively. Greek letters are indexed under 
their Roman equivalents, e.g. %2 tinder Chi-squared and co under Omega.) 
Acceptance, region of, 63, 76. 
Accidents, see Industrial Accidents. 
Accuracy, of an estimator, 28-9 ; loss of, 30-2. 
of calculation, Bibl., Walker and Sanford 
(1934) 498. 
Addition of variate, in regression analysis, 167-70. 
Additive functions, Bibl. : Erdos and Kac (1939), 
Erdos (1939), Erdos and Wintner (1939) 
459. 
Admissible functions, see Random Sequence. 
Adyanthaya, A. K., distribution of t in non- 
normal case, 103. 
Age and audible pitch, (Example 22.4) 152-3, 
(Example 22.5) 155-6. 
Agricultural statistics, bibliography of, Bibl., 
Wishart (1934a, b) 501. 
Aitken, A. C, rninirnum variance, 51, (Exercises 
18.1 and 18.2) 61 ; N.R., 61, 173. 
Allan, F. E., orthogonal polynomials, 161, 
(Exercise 22.4) 173; N.R., 173, 245. 
Almost periodic functions, Bibl. : Besicovitch 
(1932) 446, Bohr (1925) 447, Hartman 
and others (1938) 467, Kerchner and 
Wintner (1936) 473, van Kampen (1939a) 
496, Wintner (19346) 500. 
Alter, D., N.R., 437. 
Amount of information, in estimation, 29-30. 
Analysis of variance, generally, 175-246 ; 
oneway classifications, 175-6 ; two-way 
classifications, 181—7 ; three-way classifications, 
187-8 ; interactions, 188-9 ; n-way 
classifications, 189-98 ; arithmetic of, 198-9 ; 
z-test in, 199 ; factorial experiments, 199— 
202 ; in non-normal data, 205-16 ; variate 
transformations, 206-9 ; randomisation, 
209-13 ; randomised blocks, 213-14 ; 
ranking tests, 214-15 ; estimation of class- 
differences, 218-19 ; different numbers in 
sub-classes, 220-8 ; factorial classifications, 
228-9 ; missing plot technique, 229-33; 
relation with regression analysis, 233-7 ; 
covariance analysis, 237-45. 
Bibl: Bartlett (1936d, e) 445; Beall 
(1942) 446 ; Bliss (1938) 447 ; Brandt 
(1933) 449 ; Clark and Leonard (1939) 452 ; 
Cochran (1935, 19376, 19396, 19406) 452; 
Comrie and others (1937) 452 ; Curtiss 
(1943) 454; Daniels (19386) 455; Fieller 
(1940) 460 ; Hendricks (1935) 468 ; P. L. 
Hsu (1940, 19416) 469 ; Irwin (1931, 1934, 
1942) 470; E. S. Pearson (19316) 482 ; 
Roy (19396, 1942a, 6) 489; Schultz and 
Snedecor (1933) 490; Snedecor and Cox 
(1934a) 492; Snedecor (19346) 492; P. C. 
Tang (1938) 494; Wald (19406) 497, 
(1941<Z) 498; Wilks (1932e, 19376) 499, 
(1938e) 500 ; Yates (1938c) 502. 
See also Fisher's Distribution, 
Replication, Blocks, Design, etc. 
Analysis situs, Bibl., Hotelling (1927) 469. 
Ancillary estimators, 32-3. 
Anderson, O., variate-difference method, 391, 393. 
N.R., 394. 
Andersson, W., N.R., 172 : (Exercise 22.5) 174. 
Andre\ D., N.R., 136. 
Animal experiments, Bibl., Wishart (1939) 501. 
Association, Bibl. : S. S. Bose and Mahalanobis 
(1938a) 448, M. Greenwood and Yule (1915) 
466, K. Pearson and Heron (1913c) 484, 
K. Pearson (1913d) 484, Yule (1900, 1912) 
502. 
Asymmetrical frequency-distributions, Bibl,, Haus- 
mann (1934) 467. See also Gram-Char Hot* 
Series, Pearson Distributions. 
Asymptotic distributions, Bibl., Hartman and 
others (1939) 468, Haviland (1939) 468. 
See also Convergence in Probability. 
Attributes, significance in k samples, 119-20.- 
——, sub-sampling for, Bibl., Bartlett (1937a) 445. 
Autocorrelation, see Serial Correlation. Oorrolo- 
gram. 
function, 421-3. 
Autoregression equations, 309 ; (Table 30.4) 401 ; 
406-8 ; period of, 414-21. See also Serial 
Correlation, Correlogram. 
Average, accuracy of, Bibl.: Bow ley (1912) 448, 
Keynes (1911) 473. Sac also Moan, Median, 
Mode. 
Balance, in design, 263-5. Bibl. : R. (.'. Boso 
(1939) 448, R. C. Bose and Nair (1939) 448, 
R. C. Bose (1942a) 448, Cox (1940) 453, 
K. R. Nair and Rao (1942) 479, Neyman and 
Pearson (1938rf) 480, E. S. Pearson (19376, 
1938) 483, "Student" (1938) 493, Weiss 
and Cox (1939) 498, Yates (1.938a, 1940) 
502. 
Barbacki, S., N.R., 266. 
Barley yields, (Table 29.1, Figure 29.1) 364. 
Barnard, M. M., (Example 28.3) 345-8 ; N.R., 359. 
504 
INDEX 
505 
Bartels, J., N.B., 437. 
Bartlett, M. S., distribution of t, 103 ; conditional 
tests, 127 ; k samples, 299, 323 ; stabilising 
variance, 207-8 ; Wishart's distribution, 
333. Exercises from : (21.7) 139, (21.10) 
139, (21.11, 21.13, 21.14) 140, (27.2) 326, 
(28.2) 360, (28.12) 362. N.B., 45, 83, 94, 
136, 245, 304, 359, 437f 
Bayes1 theorem and postulate, in estimation, 58-9 ; 
in relation to fiducial inference, 90™1, 93-4. 
Bibl. : Bayes (1763) 446, Berkson (1930) 
446, Burnside (1924) 450, Molina (1931) 
478, E. S. Pearson (1925) 482, K. Pearson 
(1920a) 485, von Mises (1938) 497, Wishart 
(1927) 500. 
Beall, G., N.B., 216. 
Behrens' test, 82, 91-4, 111-12. See Two Samples. 
Belonging coefficient, Bibl., Kullback (1935c) 474. 
Bessel function distribution, (Exercise 28.2) 359- 
60. Bibl. : R. C. Bose (1938a) 448, S. S. 
Bose (1938a) 448, Fioller (1932a) 460, 
McKay (1932) 477, K. Pearson (1933a) 486, 
K. Pearson and others (1932a) 486. 
Best critical regions, 272, 275-8. 
Beta (measure of skewness and kurtosis), Bibl., 
McKay (1933) 477. 
Beta-function, Bibl., Miillor (1931) 479, Thompson 
and others (1941a) 494. 
Beveridge, Sir William, (Table 30.1) 396, N.B., 
437. Sec Wheat-price Index. 
Bias?, in estimation, 3™4; in statistical tests, 
307-27. Bibl. : Daly (1940) 454, Neyman 
and Pearson (1936, 1938) 480, Neyman 
(19356) 480, Yates (1935a) 502. 
Bimodal distributions, transformations of, Blbl.9 
Baker (1930a) 444. 
Binomial, confidence intervals for, (Example 19.2) 
66-9 ; tables of, 81. 
—•—, generally, Bibl. : Ayyangar (1934) 444, 
Cam}) (1924) 450, Clopper and Pearson 
(1934) 452, Cochran (1936a, 1937a, 19406) 
452, Fisher (19416) 462, Greenwood and 
Yule (1920) 466, Kullback (19356) 474, 
Lurquin (1937) 476, K. Pearson (19156) 
484, Romanovsky (1923) 489. 
Biological assays, Bibl., Irwin (19376) 470. 
Births, proportion of males in, (Example 21.8) 120. 
Biserial coefficients, Bibl. : Newbold (1925) 479, 
K. Pearson (1909, 1910) 484, (1917) 485, 
Sopor (1914) 492. 
Bishop), D. J., N.B., 304, 359. 
Bivariate surfaces, Bibl. : Narumi (1923a) 479, 
Nicholson (1943) 481, Pretorius (1930) 487, 
Rhodes (1923, 1925) 488, Ritchie-Scott 
(1921) 489, Villars and Anderson (1943) 
496. 
Blocks, randomised, 213-14. Bibl. ; R. C. Bose 
(1939) 448, R. C. Bose and Nair (1939) 448, 
R. C. Bose (1942a) 448, Cornish (1940a, 6, c) 
453, Cox (1940) 453, Fisher (19406, 1942a) 
462, Goulden (1937) 465, Kishen (1942) 
473, Nair and Rao (1942) 479, Nair (1943) 
479, Savur (1939) 490, Yates (19366, 1939a, 
1940) 502. 
Bose, C, N.B., 266. 
Bose, R. C, N.B., 359. 
Bowley, A. L., N.B., 266. 
Brady, J., N.B., 245. 
Brandt, A. E., (Example 24.1) 221-5, N.B., 245. 
Breeds of pig, (Example 24.1) 221-5, (Example 
24.2) 225, (Example 24.3) 226-7, (Example 
24.4) 229. 
Brookner, R. J., N.B., 304. 
Brown, G. W., bias in tests, 323, N.B., 304. 
Brown-Spearman formula, Bibl., Wherry (1935) 
499. 
Bruns, H., N.B., 437. 
Brunt, D., rainfall data, (Table 29.4) 367, N.B,9 
437. 
Burr, I. W., distribution functions, 440. 
Buys-Ballot table, 430. 
Calculating machines, Bibl. : Comrie (1936) 452, 
Hey (1938) 468, Mallock (1933) 477. 
Canonical correlations, 348-58. Bibl. : Bartlett 
(1941) 445, Hotelling (19366) 469, P. L. 
Hsu (1941a) 469. See Multivariate 
Analysis. 
Carloman criterion, 440. 
Cauchy population, estimation of location, 2, 
(Example 18.2) 51 ; median in, (Example 
17.4) 6 ; approximation to estimator for, 
(Example 17.11) 23-4 ; loss of information, 
(Example 17.16) 32. 
Cave, B. M., N.B., 394. 
Cement, specification of, Bibl., Wilsdon (1934) 
500. 
Central confidence intervals, 66. 
limit theorem, Bibl. : Bernstein (1927, 
1936) 446, Boclmor (1936) 447, Feller 
(19366, 1937) 460, Gnedonko (1938) 465, 
Liapounoff (1900, 1901) 476, Lindeberg 
(1922) 476, Madow (1939) 476, P61ya (1920) 
487. See Convergence in Probability. 
Centre of location, 41. 
Chains, in probability, see Markoif Process. 
Characteristic equation, Bibl., Horst (1935) 469, 
Samuelson (1942) 490. 
functions, Bibl. : Boas and Smithies 
(1937) 447, Dugue (1939) 458, Glivenko 
(1936) 465, Haviland (19346, 1935) 468, 
Kullback (1934, 19366) 474, Kunetz (1936) 
474, Wintner (1936) 500. 
Charlier's series, see Gram-Charlier Series. 
Chi-sqxiared (%2), minimum, 55-8; in testing 
goodness of fit, 106-7 ; in testing hypo- 
506 
INDEX 
theses, 299, 302 ; generalisation in 
multivariate analysis, see Wishart's Distribution. 
Chi-squared, generally, Bibl. : Aroian (1943) 444, 
Berkson (1938) 446, Brownlee (1924a) 449, 
Camp (19386) 450, Cochran (1936a, 1942a) 
452, Deming (1934, 1938) 456, Eisenhart 
(1938) 459, ElShanawany (1936) 459, Fisher 
(1922a, 1928c, 1924d) 461, Fry (1938) 464, 
Griineberg and Haldane (1937) 466, Gumbel 
(19436) 466, Haldane (1937, 1938, 1939, 
1940) 467, Hoel (1938) 468, Irwin (19296) 
470, Jeffreys (19386, 19396) 471, Johnson 
and Welch (1939) 471, Koshal (1939) 474, 
Mann and Wald (1942) 477, Merrington 
(1941) 478, ISTeyman and Pearson (1931a) 
480, K. Pearson (1900c) 483, (1916e,/, 
; 1922a, 1923) 485, (19326) 486, Robinson 
(1933) 489, Seal (1940) 490, K. Smith (1916) 
492, Snedecor and Irwin (1933) 492, Su- 
khatme (1937a, 1938a) 494, C. M. Thompson 
(19416) 494, Wilson and Hilferty (1931a) 
500, Wilson and others (19316) 500, Yates 
(19346) 502, Yule (1922) 503. 
Clitic curve, 142. 
Clopper, C. J., confidence limits for a binomial, 81. 
Closeness, in estimation, Bibl., Geary (1944) 464. 
Closure, Bibl, Stekloff (1914) 492. 
Cochran, W. G., on Fisher's distribution, 117, 199 ; 
elimination of variates, 170, (Example 
22.10) 171 ; theorem on sum of squares, 
177-8; 2VJ?., 136, 216. 
Cograduation, Bibl., Gini (1939) 465, Salvemini 
(1939) 490. 
Combination of tests, 132-3. Bibl.: David (1934) 
455, E. S. Pearson (1938) 483, K. Pearson 
(19336) 486, Wallis (1942) 498. 
of observations, Bibl. : Bruen (1938) 449, 
Brunt (1931) 449, Mather (1935) 477. See 
Errors, general theory of. 
Compatible events, Bibl., Gumbel (19386) 466. 
Complete sufficiency, in estimation, 40. 
Complex experiments, Bibl., Yates (19356) 502. 
See Design. 
Composite hypothesis, 269, 282-3, 287-92, 316-17. 
Compound frequency-distributions, Bibl., Hel- 
guero (1906) 468, K. Pearson (19156) 484. 
See Bimodal. 
Concentration, Bibl. : Castellano (1933a, 6, 1937) 
451, Galvani (1932) 464, Gini (1932) 465, 
Pietra (1932a) 486, von Schelling (1934) 
497, Wold (1935) 501. 
Concordance, Bibl., Gini (1916) 465. 
Concordant samples, 128. 
Conditional statistics, (Exercise 21.10) 139 ; N.R., 
45. Bibl, Bartlett (19386) 445. 
tests, 127-8, 134. 
Confidence, belt, 63; coefficient, 63; intervals, 
62-84 ; for one parameter, 62-5 ; central 
and non-central, 66-9 ; for large samples, 
69-71 ; shortest sets, 71-4 ; sufficient 
estimators, 74-5 ; for several parameters, 
76-9, 81-2 ,* studentisation in determining, 
79-81 ; tables of, 81 ; limits, 63. 
Bibl : Clopper and Pearson (1934) 452, 
David (1937,1938a) 455,Frankel and Kull- 
back (1940) 463, Kolmogoroff (1941) 474, 
K. R. Nair (19406) 479, Neyman (19376, 
1941a) 480, E. S. Pearson (1932) 482, 
Pearson and Sukhatme (19356) 482, Ricker 
(1937) 488, W. R. Thompson (1936) 494, 
Wald and Wolfowitz (19396) 497, (1941c) 
498, Wald (1942a) 498, Welch (1939a) 498, 
Wilks (19386, c) 499, (1939a) 500, Wilks 
and Daly (19396) 500. 
Configuration of sample, 127. 
Confluence analysis, Bibl. : Cobb (1939) 452, 
Frisch (19346) 464, Mendershausen (1939) 
478, Reiersol (1940, 1941) 488. 
Conformity, index of, Bibl, Solomon (1939) 492. 
Confounding, 262-3. Bibl : Barnard (1936) 444, 
R. C. Bose and Kishen (1941) 448, Fisher 
(1942c) 462, K. R. Nair (19386, 1941) 479, 
Yates (1933a) 501. See Design. 
Consistence, of estimators, 3, 12-15. 
Contagious distributions, Bibl, Feller (1943) 460, 
Neyman (1939a) 480. 
Contingency, Bibl : Bartlett (19356) 445, Blake- 
man and Pearson (1906) 447, Harris and 
Treloar (1927) 467, Hirschfeld (1935) 468, 
Kondo (1929) 474, K. Pearson and Blake- 
man (1906) 484, K. Pearson (1900a, 6) 483, 
(1904) 484, (19166) 485, Stevens (1938a) 
493, Weida (1934) 498, Wilks (1935a) 499, 
Yates (19346) 502, Young and Pearson 
(1916) 502. 
Continuous spectrum, in periodogram, 433. 
Convergence in probability, Bibl : Cantolli (1916, 
1917, 1923, 1933a, 1935) 450, Cramer (1934) 
454, Dodd (1926, 1927) 456, Doeblin (1938, 
1939) 457, Dugue (1937a) 458, Feller (1937) 
460, Frechet (1930) 463, Jordan (1933) 472, 
Kolmogoroff (1937a) 473, Kozakicwicz 
(1937, 1938) 474, Levy (19356, 1936c, 1939a) 
475, Messina (1933) 478, Romano vsky 
(19326) 489, Slutzky (1925, 1937a) 491. 
See also Central Limit Theorem. 
Convolutions, Bibl, van Kampen (1937a) 496, van 
Kampen and Wintner (19376, c) 496. 
Cornish, E. A., on Fisher's distribution, 116, 
N.R., 136. 
Corrections, for grouping see Grouping 
Corrections ; to correlations, I?i6Z.,Roff (1937) 489. 
Correlated observations, sampling from, Bibl : 
A. T. Craig (19336) 453, C. C. Craig (1931a) 
453, (1932) 454, Rhodes (1927) 488. See 
also Time-series. 
INDEX 
507 
Correlation, confidence intervals for coefficient, 
81 ; Pitman's tost, for, 131-2 ; significance 
Of, iw «$«.)• 
Bihl, : Baker (19306) 444, Bilham (1926) 
447, Bispham (1920, 1923) 447, Bonferroni 
(1939) 447, Brander (1933) 449, W. Brown 
(1909) 449, Brownlro (1910, 1925) 449, 
Cheshire and others (1932) 451, Cochran 
(1937c/) 452, Coleman (1932) 452, Cowles 
and Chapman (1935) 453, Day and Fisher 
(1937) 455, David (1937, 1938) 455, G. R. 
Da vies (1930) 455, do Lury (1938) 456, 
Doming (1937) 456, Dieulefait (1934a, 
1935a) 456, S. C. Dodd (1937) 457, Dunlap 
(1931) 458, Fells (1929) 459, Fzekiel (1930a) 
459, Fischer (1933a, 6) 460, Fisher (1915, 
1918, 1921c, 1924a) 461, Frechet (1933) 
463, Frisch (1929) 463, Frisch and Mudgett 
(1931) 463, Garwood (1933) 464, Geary 
(1927) 464, Gohlke and Biehl (1934) 464, 
Gniringor (1933) 464, Jeffreys (1939c) 471, 
Khintehino (1928) 473, Kuzmin (1939) 474, 
Lindhlad (1937) 476, Morzrath (1933) 478, 
A. N. K. Nair (1942) 479, Nowbold (1925) 
479, E. W. Pearson (1923, 1924, 1931a, 1932) 
482, K. IVarson (18976, 1900a, 6, 1902a) 
483, (1904, 1905, 1907a, 1909, 1910, 1913a, 6, 
1914, 1921) 484, (19206, 19256) 485, Pitman 
(1939r) 486, Prokopovic (1935) 487, Quenscl 
(1938) 487, Rider (1932) 488, Romanovsky 
(1925a) 489, Sopor (1913, 1914, 1917) 492, 
Stofl'ensrn (1.934) 492, Stouffor (1934, 
L936a, 6) 493, "Student'1 (19086) 493, 
Thorndike (1.937) 494, Thouloss (1939) 495, 
Tsolmprow (1925, 1928) 495, (1934) 496, 
WieksoU (1917a, 6, 1921, 1933) 499, Yasu- 
kawa (1925) 501, Yule (1897a, 6, 1906, 
1907, 1.910) 502. 
Svc also Multiple Correlation, Regression. 
ratio, Bihl.: Hotolling (1.925) 469, Issorlis 
(1.914, 1916) 470, Kelley (1.935) 472, Mussel- 
man (1920) 479, E. S." Pearson (1927) 482, 
K. Pearson (1905, 1910, 1911a, 6, 1915a) 
484, (1.917, 19236) 485, "Student" (1913) 
493, WhIIih (1939) 498, Wishart (1932a) 500. 
Correiogram, 404-12 ; significance of, 412-13 ; of 
general linear series, 420 1 ; relation with 
periodogram, 432 -3. 
Cost of living, Bibl. : Bennett (1920) 446, Bowley 
(1919) 448, Konos (1939) 474. 
Cotton yarn, Bibl., Tippett (1935) 495. 
Counting experiments, Bibl., Poierls (1935) 486, 
Tippett (1932) 495. 
Coutts, J. R. H., data from, (Table 22.1) 150. 
Covariance, analysis of, 237-45. Bibl. : Bailey 
(1931) 444, Bartlott (1935a\ 1936c) 445, 
Brady (1935) 449, Cochran (1934) 452, 
Cornish (1940c) 453, Cox and Snedecor 
(1936) 453, Hirschfeld (1937) 468, K. K. 
Nair (1940a) 479, Snedecor (1935) 492, Wilks 
(1936) 499, (1938c) 500, Wishart (1936) 501. 
Covariance, distribution of, (Example 28.1) 334—5. 
Cramer, H., contest, 108-9 ; Carleman criterion,. 
Critical region, 270, (Example 27.2) 312-13. 
Crop estimation, Bibl., Yates (1936c) 502. 
Cram, W. L., N.B., 437. 
Cumulants, Bibl. : Ayyangar (1938) 444, Cornish 
and Fisher (1937) 453, C. C. Craig (1931c) 
454, Dressel (1940, 1941) 458, Frisch (1926) 
463, Gotaas (1936) 465, Thlele. (1931) 494. 
See also ^-statistics, Moments. 
Curtiss, «J. H., N.B., 216. 
Curve fitting, Bibl. : Elderton and Hansmanxx 
(1934) 459, Fisher (1912) 461, Jones (1937a) 
472, Kerrich (1935) 473, Koshal (1933, 
1935, 1939) 474, Myers (1934) 479, Nair 
and Shrivastava (1942) 479, ISFair and 
Banerjce (1943) 479, K. Pearson (1901c) 483, 
Rhodes (1930) 488, Roos (1937) 489, 
K. Smith (1916) 492, Snow (1911) 492, 
Wald (1940a) 497. See also Least Squares, 
Regression, Trend. 
Curvilinear regression, 145-74. Bibl., Menders- 
hausen (1937a) 477, T. V. Moore (1937) 
478 ; and see Regression. 
Cycle, 397-8. See Periodicity. 
Cyclical effects, tests for, 124-7, 370. See 
Periodicity. 
D2-statistic, N.R., 359. Bibl. : Rhattacharya and 
Narayan (1942) 446, R. C. Bose (1936a, 6) 
447, R. C. Boso and Roy (1938c, 1940) 
448, S. N. Bose (1935, 1937) 448, Roy 
(1939a) 489. See also Discriminatory 
Analysis, Multivariate Analysis. 
Daly, J. F., on shortest confidence intervals, 82 ; 
on bias in tests, 323 ; N.M., 304. 
Daniels, H. E-, (Example 23.2) 183™5 ; rank 
correlations, 441. 
Dantzig, G. B., N.R., 304. 
David, F. N., confidence intervals for correlations, 
81 ; N.R., 304. 
Davis, H. T., time-series, 433, 434 ; N.R., 394, 
437. 
Day, E. E., N.R., 245. 
Death rates, Bibl., Farr (1919, 1920) 460, Pearson 
and Tocher (1916c) 485. 
Decomposition of series, Bibl., Anderson (1927) 
443, Smirnoff (1935) 491. See also Time- 
series. 
Decreasing functions, Bibl,, C. D. Smith (1939) 
Degrees of freedom, of " Student's " t, 102 ; of 
hypotheses, 270. 
De Lury, D., N.R., 137. 
508 
INDEX 
Denumerable probabilities, Bibl., Steinhaus (1923) 
492. 
Dependence, see Independence, Correlation. 
Derkson, J. B. D., on stochastic convergence, 440. 
Design, of sampling inquiries, 247-68; 
preliminary points, 248-9 ; stratified sampling, 
249-52 ; design of experiments, 252-4 ; 
orthogonality, 254; replication, 255; 
randomisation, 255-6 ; sensitivity of a 
test, 256-7 ; Latin squares, 257-62 ; 
confounding, 262 ; design and randomisation, 
263-6. 
Bibl. ; Bhattacharya (1943) 446, Chris- 
tidis (1931) 451, Fisher (1935c) 462, Jeffreys 
(1939e) 471, "Student" (1938) 493, Wold 
(1943) 498, Yates (1939e) 502. See also 
Blocks, Factorial Experiments, Latin 
Squares, etc. 
Determinantal equations, Bibl., Girshik (1939) 
465. See also Matrix. 
Deviance, footnote, 178. 
Difference, of two means, test of (equal variances) 
109-11 ; (unequal variances) 111-14. See 
also Behrens' Test, Two Samples. 
, of two variances, 115-16. 
, equations, Bibl., Frisch (1932) 463, 
Marples (1932) 477. See also Auto- 
regression Equations. 
Differences of variates, Bibl., Irwin (1937a) 470. 
Dilution method, Bibl., R. D. Gordon (1939) 465, 
Matuzewski and others (1935) 477. 
Dirichlet integrals, 298. 
Discontinuous variates, Bibl. : dell' Agnola (1937) 
456; Guldberg (1934) 466, Muench (1938) 
478, H. W. Norton (1937) 481, Ottestad 
(1937, 1938) 481. 
Discordant samples, 128. 
Discriminatory analysis, discriminant function, 
341-8. Bibl.: Barnard (1935) 444, Bartlett 
(1939c) 445, Dwyer (1942) 458, Fisher 
(1936a, 1938c, 19396, 1940d) 462, P. L. Hsu 
(19396, 1941a, 1941c) 469, H. F. Smith 
(1936) 492, Travers (1939) 495, Wallace 
and Travers (1938) 498, Welch (19396) 498, 
Wilks (1938e£) 500. See also Multivariate 
Analysis. 
Dispersion, Bibl., No iris (1938) 481. See Variance, 
etc. 
matrix, 330, 341, N.B., 358. 
Dissection of frequency-distributions, Bibl., Burrau 
(1934) 450. 
Distributed lags, see Lags. 
Distributions, generally, Bibl. : Ambarzumian 
(1937) 443, Baten (1933a) 445, (1934) 446, 
Bispham (1922) 447, Bochner and Jessen 
(1934) 447, Bochner (1937) 447, Bowley 
(1933) 448, Burr (1942) 450, Camp (1937) 
450, Cannon and Wintner (1935) 450, 
Chapelin (1932) 451, Cramer and Wold 
(1936) 454, Edgett (1931) 458, Eyraud 
(1938a) 459, Glivenko (1933) 465, Guldberg 
(1935) 466, Hansmann( 1934) 467, Hartman 
and others (1937) 467, (1939) 468, Haviland 
(1934a, 6, 1935, 1939) 468, R. Henderson 
(1907) 468, Jessen and Wintner (1935) 
471, Khintchine (1937a) 473, Kullback 
(19366) 474, Mazzoni (1934) 477, K. Pearson 
(1923c, 1924a) 485, R Schmidt (1934) 490, 
von Mises (1939a) 497. 
Dodd, E. L., period generated by moving average, 
384, N.R., 394. 
Doob, J., N.R., 45. 
Dosage-mortality, Bibl., Garwood (1941) 464. 
-response, Bibl., Irwin and Cheeseman (1939) 
470. 
Dugue\ D., NM., 45. 
Duration of play, Bibl., de Finetti (19396) 456* 
Fieller (1931a) 460. 
Eden, T., on Fisher's distribution, 206, (Example 
23.8) 214, N.R., 216. 
Edgeworth, F. Y., N.M., 45. 
Edwards, J., Integral Calculus, footnotes, 44 and 
50. 
Efficiency, of estimators, 5-7 ; of maximum 
likelihood estimators, 18-19 ; of moments 
in fitting Pearson curves, 43-4 ; of sampling, 
Bibl., Yates and Zacopanay (1935c) 502. 
Egg-production, in laying liens, (Table 29.5, 
Figure 29.5) 368." 
Egyptian skulls, (Example 28.3) 345-8. 
Elasticity of demand, Bibl., Mosak (1939) 478, 
Schultz (1933) 490. 
Elderton, E. M., (Example 21.14) 133, N.B., 2&>. 
Elderton, Sir William P., N..R., 45. 
Electric lamps, testing of, (Example 23.1) 179-80. 
Elimination of variates, in regression analysis, 
167-70. 
Enumeration in sampling, Bibl., Cochran (19396) 
452. 
Equidetectability, curves of, 318. 
LCquimodal distributions, Bibl., Mouzon (1930) 478. 
Error, in variance-analysis, 187. 
Errors, of first and second kind, 270, (Exercise 
26.5) 305. ' 
, general theory of, Bibl. : Brelot (1936, 
1937) 449, Campbell (1935) 450, Cramer 
(1928) 454, Doming and Birgo (1934) 456, 
Edgeworth (1905, 1906) 458, Jeffreys (1933, 
1937c, 1938a1, 1939^) 471, Mahalanobis 
(1922) 476, Wertheimor (1932) 499. See 
also Least Squares. 
Estimation, generally, 1-49, 50-62 ; in analysis 
of variance, 181, 218-19. 
Estimator, definition, 2 ; consistence of, 3 ; bias 
of, 3-4 ; efficiency of, 5-10 ; sufficiency of, 
INDEX 
509 
7-12 ; approximation to, 22-4 ; most 
general sufficient form, 24-5 ; accuracy of, 
28-9 ; ancillary, 32-3; in multivariate 
case, 33-42; location and scale, 40-2; 
by minimum variance, 50-5 ; by minimum 
X2, 55-8; by inverse probability, 58-9; 
by least squares, 59-60. See also 
Maximum Likelihood, Minimum Variance. 
Bibl. ; Aitkon and Silvorstone (1942) 
443, BeiilI (1939) 446, S. S. Bose and 
Mahalanobis (19386) 448, Darmois (1935, 
1936) 455, O. L. Davies and Pearson (1934) 
455, Doob (1936) 457, Dugue (1936a, 6, 
19376) 458, Fisher (19256) 461, (1934a\ 
19386, d) 462, Geary (1942, 1944) 464, 
Halphen (1939) 467, Neyman (19376) 480, 
E. 8. Pearson (1937a, 1.939) 483, Pitman 
(19376, 1939a) 486, Walcl (1939a) 497. 
Expectation of life, see Life. 
Expected values, see Moan Values. 
case, in sociological data, Bibl., Stoufter and 
Tibbits (1933) 493. 
Expenditure of families, (Example 23.9) 214-15. 
Exponential distribution, (Exercise 26.8) 305-6. 
Bibl., Paulson (1941) 482, Sukhatme (19366) 
493. 
Extra-sensory perception, Bibl., Greenwood and 
Stuart (1937) 465, Stevens (19396) 493. 
Extremes, distribution of, Bibl. : Daniels (1941) 
455, do Finotti (1932) 455, Dodd (1923) 
456, Fisher and Tippett (1928a) 461, 
Gumbel (1934, 1935a) 466, McKay (1935) 
477, Olds (1935) 481, Tippett (1925) 495. 
See also mth Values. 
F-dLstribution (variance ratio), Bibl., Merrington 
and Thompson (1943) 478. See Fisher's 
Distribution. 
Factor analysis (psychology), Bibl. : Bartlett 
(1937<») 445, W. Brown (1935) 449, Burt 
(1937a, 6, 1938a, 6) 450, Camp (1932, 1934) 
450, Darmois (1934) 455, Eraraett (1936) 
459, Hool (1937, 1939) 468, Irwin (1933) 
470, Lodormann (1938) 475, Roff (1937) 
489, Thomson (1916, 19196, 1939) 494, 
Thurstone (1935, 1938) 495. 
Factorial experiments, 199-202. Bibl. : Barnard 
(1936) 444, R. 0. Bose and Kishon (1941) 
448, Cornish (1936, 19406, c) 453, Goulden 
(1937, 1938) 465, P. L. Hsu (1943) 470, 
Kishen (1940) 473, Wishart (1938) 501, 
Yates (19376) 502. 
moments, Bibl., Gonin (1936) 465, Ottestad 
(1939) 481. 
— sums, in fitting regressions, (Example 22.8) 
164-5. 
Factorisation of variables, Bibl., S. C. Dodd (1927) 
457. 
Families of alternatives, 275-6. 
Feller, W., N.R., 303. 
Fiducial inference, 85-95. Bibl : Bartlett (1939a) 
445, Fisher (1933, 1935a, 19356, 1936c, 
19376, 1939a, 1940c, 1941a) 462 ; Garwood 
(1936) 464, Ricker (1937) 488, Segal (1938) 
491, Wilks (19386, c) 499, (1939a, 6) 500. 
See Confidence intervals. 
Field experiments, Bibl., Wishart and Saunders 
(1935) 501. See Design. 
Fifteen-constant surface, Bibl., K. Pearson (1925a) 
485. 
Filon, L. N. G., N.R., 45. 
Finite popiilations, sampling from, Bibl. :• Church 
(1926) 452, Hansen and Hurwitz (1940) 
467, Irwin and Kendall (1944) 470, Isserlis 
(1918c, 1931) 470, Neyman (1925) 480, 
O'Toole (1934) 481, Sukhatme (1944) 494, 
Tschuprow (19186, 1921, 1923) 495. 
Finney, D. J., z-test, 199 ; test of significance in 
periodogram analysis, 434 ; N.R., 137, 216. 
Fisher, R. A., fitting by moments, 43 ; fiducial 
probability, 90 ; tables for Behrens' test, 
92, 93, 111; expansion of "Student's" 
integral, 101 ; tables of t, 102; difference 
of two means, 110; ^-distribution, 116, 
117 ; configuration of a sample, 127; 
fitting regressions, 165 ; theorem on sum 
of squares, 176-7 ; design of experiments, 
263 ; discriminatory analysis (Example 
28.2) 342-4; distribution of canonical 
correlations, 357 ; significance of a 
periodogram, 434 ; N.R., 45, 61, 83, 94, 136, 173, 
216, 245, 266, 359. 
Exercises from: (Exercise 17.1) 45, 
(Exercises 17.4, 17.5, 17.6) 46, (Exercise 
17.12, 17.15, 17.16) 48, (Exercise 
(Exorcise 18.3) 61, (Exorcises 20.1, 20.2) 
94 5. 
Fisher's distribution (z-distribution), properties of, 
116-18; in variance analysis, 179, 199; 
hi non-normal case, 205-6, 234-6, (Example 
26.8) 289-91 ,* in linear hypothesis, 301 ; 
in discriminatory analysis, 345. 
Bibl. : Aroian (1941) 444, R. A. 
Chapman (1938) 451, Cochran (1940a) 452, 
Daniels (1938a) 454, Eden and Yates (1933) 
458, Fisher (1924c) 461, P. L. Hsu (1941c) 
469, Lawloy (1938) 475, McCarthy (1939) 
477, Paulson (1942) 482, Welch (1937) 498. 
Fitting, see Curve Fitting, Least Squares. 
Flood flows, Bibl., Gumbel (1938a, 1941) 466. 
Fluctuations in time-series, Bibl., R. A. Gordon 
(1937) 465. See Time-series. 
Forecasting, Bibl.; Cowles (1933) 453, Cowles and 
Jones (1937) 453, de Finetti (1937) 456, 
Schultz (1930) 490, Yates (1936c) 502. 
Forsyth, A. R., Calculus of Variations, footnote, 50. 
510 
INDEX 
Fourier analysis, see Harmonic Analysis, 
Periodicity. 
Fragmentary samples, Bibl., Wilks (1932a) 499. 
Frankel, L. R., N.R., 136, 266. 
Freedom, degrees of, see Degrees of Freedom. 
Frequency-distributions, see Distributions. 
Frequency theory of probability, Bibl.: Campbell 
(1939) 450, Cantelli (1923, 1932, 19336) 450, 
(1936) 451, Dorge (1934, 1936) 458, von 
Mises (1931) 497. See Probability, Random 
Sequence. 
Friedman, M., (Example 23.9) 214—15. 
Frisch, R., N.R., 358. 
Galton's problem, Bibl. : Galton (1902) 464, Irwin 
(1925a) 470, K. Pearson (1902c) 484. See 
Rank Correlation. 
Gamma distribution, Bibl, Kibble (1941) 473. 
See Type III. 
Garwood, F., confidence intervals for Poisson 
distribution, 81. 
Gauss, K. F., variance of residuals,, 60-1; 
standard errors, 153; N.R., 45. 
Gaussian distribution, see Normal Population. 
Geary, R. C, distribution of t, 102-4 ; test of 
normality, 106 ; theorem on independence, 
118 ; (Exercises 21.1, 21.2) 137-8 ; N.R., 
45, 136. 
Geary's ratio, Bibl., Geary (1935a, b, 1936a) 464, 
Tricomi (1937) 495. 
General factor (intelligence), see Factor Analysis. 
Generalised distance, of Mahalanobis, N.B., 359. 
Generating functions, Bibl., Aitken (1931) 442. 
See Characteristic Functions. 
Geometric Mean, Bibl., Camp (1938a) 450, Norris 
(1938, 1940) 481. 
Germination of wheat-seeds, (Example 23.7) 207-9. 
Gini's mean difference, 108. 
Girshik, M. R., (Exercise 28.11) 362, N.B., 359. 
Glass, seed in, (Example 23.6) 202-4. 
Goodness of fit, tests of, 106-9. Bibl. : David 
(1939) 455, JSTeyman (1937a) 480, K. 
Pearson (1934) 486, Thomson (1919a) 494. See 
Cbi-squared. 
Gosset, W. S. (" Student "), 80, 266, N.R., 394. 
Gould, C. E., (Example 23.6) 202-4. 
Goulden, C. H., N.R., 216, 266. 
Grades, see Rank Correlation, Galton's Problem. 
Graduation, Bibl., Aitken (1933a, b, c) 442, Key- 
fitz (1938) 473. See Interpolation, Least 
Squares, Orthogonal Polynomials, Trend. 
Graeco-Latin square, 261-2. Bibl., R. C. Bose 
(19386) 448. 
Gram-Charlier series, estimation in (Exercise 18.1) 
61 ; for non-normal t, 103 ; goodness of 
fit in, 109 ; in ^distribution, 116. Bibl. : 
Aitken and Oppenheim (1931) 442, Aitken 
(1932) 442, Aroian. (1937) 444, Baker 
(1930d, 1935) 444, Charlier (1906, 1912, 
1928, 1931) 451, Cornish and Fisher (1937) 
453, C. C. Craig (19316) 454, Cramer (1926, 
19356) 454, Doetsch (1934) 457, Edgeworth 
(1905) 458, Gram (1879) 465, Hildebrandt 
(1931) 468, Jacob (1933, 1935, 1937) 471, 
Meisener (1938) 477, Quensel (1938) 487, 
Samuelson (1943) 490, Schmidt (1934) 490, 
Steffensen (1930) 492, Wicksell (19176, 
1934a) 499. 
Greenstein, B., N.B., 437. 
Grouping corrections, Bibl. : Abernethy (1933) 
442, Alter (1939) 443, Baton (1931) 445, 
Bltimel (1939) 447, Burkhardt and Stackel- 
berg (1939) 449, Carver (1933, 1936) 451, 
C. C. Craig (1936c, 19416) 454, Elderton 
(1933, 19386) 459, Kendall (1938a) 472, 
Lewis (1935) 475, Sandon (1924) 490. 
, effect on correlations, Bibl., Gehlke and 
Biehi (1934) 464. 
, significance of, Bibl., Stevens (19376) 493. 
Groups of experiments, Bibl., Yates and Cochran 
(19386) 502. 
Hampton, W. M., (Example 23.6) 202-4. 
Hansmann, G. H., N.B., 45. 
Harmonic analysis, Bibl. : T. F. Anderson (1935) 
443, Brunt (1928) 449, Carslaw (1930) 451, 
Fisher (1929a) 461, (1940a) 462, Frisch 
(1928, 1931, 1933) 463, Pollak (1926) 487, 
Turner (1913) 496, Wiener (1930) 499. 
See Periodicity. 
■ mean, Bibl., Norris (1939) 481. See Mean 
Values. 
Hartley, H. O., on z-test, 199 j k samples, 299 ; 
N.B., 137, 216, 304. 
Heads and tails, Bibl., Fieller (1931c) 460. See 
Duration of Play. 
Hendricks, W. A., (Exercise 21.9) 139 ; N.B., 136. 
Hermite polynomials, see Tchabycheff-Hermit e 
Polynomials. 
Heterogeneous populations, Bibl,, Baker (1930c, 
1932) 444. See also Lexis Theory, 
Stratified Sampling. 
Hierarchies in correlation, Bibl., Thomson (1916, 
19196, 1935) 494, Wilson (1928) 500. See 
Factor Analysis. 
Higham, J. A., (Exercise 29.7) 395. 
Highest audible pitch, (Example 22.4) 152-3, 
(Example 22.5) 155-6. 
Hirschfeld, H. O., see Hartley, H. O. 
Homogeneity, Bibl. : Baker (1941) 444, Hartley 
(1940) 467, Welch (1938a) 498. See k 
samples. 
Horse population and wheat prices, 436. 
Hotelling, H., canonical correlations, 348-58; 
(Exercises 28.7-28.10) 360-2; N.R., 45, 
136, 359. 
* 
INDEX 
511 
Hotelling's T, 323, 335-8; N.B., 359. Bibl, 
Hotolling (1931) 469, P. L. Hsu (1938c) 469. 
Hsu, P. L., linear hypothesis, 301 ; Wishart's 
distribution, 333 ; canonical correlations, 
357; N.B., 304, 359. 
Hypergeometric series, Bibl. : Ayyangar (1934) 
444, Camp (1925a) 450, O. L. Davies (1933, 
1934) 455, Gonin (1936) 465, K. Pearson 
(18996) 483, (19246, c) 485, Romanovsky 
(19256) 489. 
Hypotheses, testing of, see Statistical Hypotheses. 
Imaginary random variables, Bibl, Eyraud (19386) 
459. 
Immunity, Bibl., Brownlee (1905) 449. 
Incomes, distribution of, Bibl., Cantelli (1929) 
450, Darmois (1933) 455. 
Incomplete blocks, see Blocks. 
Independence, of quadratic forms, Bibl. : Cochran 
(1934) 452, A. T. Craig (1936a, 1943) 453, 
Madow (1940) 476. 
« , statistical, Bibl. : del Vecchio (1933) 456, 
Kac and van Kampen (1939) 472, Marcin- 
kiewicz and Zygmimd (1937) 477, Tschu- 
prow (1934) 496. See also Correlation, 
Contingency, etc. 
Index, distribution of, see Ratio. 
—— numbers, Bibl. : Bowley (1926) 448, Clare- 
mont (1916) 452, Crowthor (1934) 454, 
Dodd (1937c) 457, Edgeworth (1925a, 6, c) 
459, I. Fisher (1922) 460, Flux (1921, 1933) 
463, Frickoy (1937) 463, Frisch (1930) 463, 
Haberlor (1927) 467, Konos (1939) 474, 
Persons (1928) 486, Rhodes (1936) 488, 
Sohultz (1939) 490, Yates (1939c) 502. 
Indices, correlation of, Bibl. : Baker (1937) 444, 
J. W. Brown and others (1914) 449, Clare- 
mont (1916) 452. 
Industrial accidents, Bibl., Newbold (1927) 479. 
— processes, see Quality Control. 
Inequalities, Bibl. : Mortara (1934) 478, Narumi 
(19236) 479, Norris (1935, 1937) 481, 
Romanovsky (1938) 489, Shohat (1929) 
491, C. D. Smith (1930) 491, von Mises 
(19396) 497, Wald (1938) 497. 
Infantile mortality, Bibl., Fold (1924) 460. 
Infection in potatoes, (Example 24.5) 230-2, 
(Example 24.6) 232-3. 
Inference, see Statistical Hypotheses. 
Information, amount of, 29-30 ; loss of, 30-2 ; 
in minimum %2, 57-8. Bibl. : Bartlett 
(1936a, 6) 445, Fisher (19346, 1935a) 462. 
Intensity, of a periodogram, 425. 
Interaction, in variance-analysis, 187, 188-9. 
Interference, analysis of, Bibl., Stevens (1936) 493. 
Interpolation, Bibl. ; Comrie (1936) 452, Erdos 
and Turan (1937, 1938) 459, Feldheim 
(1936a) 460, Fisher and Wishart (1927) 
461, Gini (1921) 465, Lidstone (1937) 476, 
Pietra (19326) 486, Salvemini (1934) 490, 
Simaika (1942) 491, Tchebycheff (1907) 
494. See also Graduation, Least Squares, 
Orthogonal Polynomials. 
Intra-class correlation, 181, BibL Harris (1914) 
467, Harris and Gunstad (1931) 467. 
Intrinsic accuracy, in estimation, 28-9. 
Invariants of frequency curves, Bibl., Zoch 
(1934) 503. 
Inverse probability, in estimation, 58-9 ; 
relationship with fiducial inference, 90-1, 93-4. 
Bibl. : Bayes (1763) 446, Fisher (1926c, 
1930a) 461, (1932, 1935a) 462, Isserlis (1936) 
471, Jeffreys (19376) 471, Tornier (1937) 
495, Wisniewski (19376) 501. 
Iris (flower), (Example 28.2) 342-4. 
Irregular Kollektiv, 123. See Random Sequence. 
Irwin, J. O., (Exercise 23.1) 216-17 ; sampling 
moments, 440 ; N.M., 216. 
Item analysis, Bibl., Merril (1937) 478. 
Iterations, see Runs. 
J-shaped distributions, Bibl., Elderton (1933) 
459, Solomon (1939) 492. 
Jackson, W. R., N.B., 304. 
Jeffreys, H., (Example 18.5) 56-7; fiducial 
inference, 90-1, 93-4 ; N.B., 61, 94, 266. 
Jensen, A., N.R., 266. 
Joint sufficiency, 39. 
Judgments, validity of, Bibl., Eysenek (1939) 459. 
h samples, problem of, 119-22, 295-9 ; bias in, 
323, (Exercise 27.2) 326. Bibl. : Bartlett 
(1934a) 445, Bishop (1939) 447, Bishop and 
Nair (1939) 447, R. C. Bose and Roy (1940) 
448, G. W. Brown (1939) 449, Neyman 
and Pearson (19316) 480, Pearson and 
Wilks (19336) 482, Sukhatme (19366) 493, 
(.19376) 494, Welch (1935) 498, Wilks 
(19356) 499. See jD-tests. 
/c-statistics, Bibl. : Fisher (19296) 461, Fisher and 
Wishart (1931) 462, C. T. Hsu and Lawley 
(1939) 469, Kendall (1940) 472, (19426) 473, 
Wishart (1929a, 6, 1930, 19336) 500. See 
also Moments, sampling. 
Kelloy, T. L., (Example 28.4) 351-2. 
Kermack, W. O., N.B., 136. 
Keynes, Lord, (Exercise 17.7) 47. 
Kolmogoroff, A., confidence intervals for 
terminals, 83. 
Kolodzieczyk, St., linear hypothesis, 293 ; N.B., 
304. 
Koopman, B. O., (Exercises 17.13, 17.14) 48, 
N.R., 45. 
Koshal, R., N.R., 45. 
Kronecker delta, 329. 
512 
INDEX 
Kurtic curve, 142. 
Kurtosis, BibL, Frisch (1934a) 464. 
L-tests, BibL : Mahalanobis (1933) 476, Mood 
(1939) 478, Nayer (1936) 479, Paulson 
(1941) 482, Welch (1936a) 498, Wilks and 
Thompson (1937a) 499. See h samples. 
Lag correlation, 435-6. 
Lags, distributed, BibL : Alt (1942) 443, Koop- 
mans (1941) 474, K. R. Nair (1936) 479, 
Zrzavy (1933) 503. 
Lanarkshire milk investigation, N.R., 266. 
Large numbers, law of, see Convergence in 
Probability. 
Largest member of a sample, see Extremes. 
■ of a set of variances, see Variance ratio. 
Latent roots of a matrix, see Matrix. 
Latin squares, 257-62, 266. BibL : R. C. Bose 
(19386) 448, R. C. Bose and Nair (19426) 
448, Euler (1782) 459, Fisher and Yates 
(1934c) 462, Fisher (1942d, e) 462, Mann 
(1943) 477, H. Norton (1939) 481, Stevens 
(19386) 493, Welch (1937) 498, Yates (1933c) 
501, (1936a) 502. 
Lattices, distributions on, van Kampen and 
Wintner (19396) 496. 
Lawley, D. N., N.B., 359. 
Least squares, in estimation, 59 ; in regression 
analysis, 145 ; in time-series, 371. BibL : 
Adcock (1878) 442, Aitken (1933a, 6, c, 
1935a) 442-3, Davis (1933) 455, David and 
Neyman (1938c) 455, Deming (1931, 1934, 
1935, 1937) 456, Hendricks (1931, 1934) 
468, E. Johnson (1940) 471, Jones (1937a) 
472, Jordan (1932, 1934) 472, Kerrich (1937) 
473, Sheffer (1935) 491, Sheppard (1914, 
1929) 491, Sterne (1934) 493, Wisniewski 
(1937a) 501, Wong (1935) 501. 
Lexis, W., ratio, 119 ; N.R., 216. 
theory, BibL : Geiringer (1942) 465, Rider 
(1934) 488, Tschuprow (1918, 1919a) 495, 
von Bortkiewicz (1931) 497. 
Life, expectation of, etc., BibL : Brownleo and 
Morison (1911) 449, Dublin and others 
(1935) 458, Greenwood (1922) 466, Gumbel 
(1924, 1925, 1932) 466, Seal (1940) 490, 
Wilson (1938) 500. 
Likelihood, in estimation, see Maximum 
Likelihood ; in testing hypotheses, 277-80, 295- 
302, 323-6. BibL, Fisher (1932, 1934a, 6) 
462, Wilks (1935a) 499. 
Likelihood-ratio tests, BibL : Daly (1940) 454, 
Neyman and Pearson (1933c) 480, Wilks 
(1938a) 499, Wilks and Thompson (1937a) 
499, See .L-tests. 
Limiting form of significance tests, 322. BibL, 
Peiser (1943) 486. 
Linear equations subject to error, BibL, Lonseth 
(1942) 476. 
hypotheses, 292-5, 300-2. BibL, Johnson 
and Neyman (1936) 472, Kolodzieczyk 
(1935) 474. 
Linearity of regression, see Regression. 
Linkage, BibL, Finney (1940, 1941, 1942) 460, 
N. L. Johnson (19406) 472. 
Link-relatives, BibL, Robb (1930) 489. /See Index 
Numbers. 
Live births, proportion of males among, (Example 
21.8) 120. 
Location, estimation of parameters of, 40-2 ,* 
centre of, 41 ; Pitman's tests of, 323-6. 
BibL, Pitman (1939a, 6) 486. 
Logarithmic variate, BibL : Finney (19416) 460, 
Jenkins (1932) 471, Nydell (1919) 481, 
Pae-Tsi-Yuan (1933) 481, Quensel (1936) 
487, Wicksell (1917a) 499, Williams (1937) 
500. 
Loss of information, in estimation, 30-2. 
weight in soil, (Example 22.3) 149-52, 
(Example 22.6) 158. 
m rankings, problem of, (Example 23.9) 214-15. 
BibL, Friedman (1937, 1940) 463, Kendall 
and Babington Smith (19396) 472. 
Macaulay, F. R., (Exercise 29.4) 395 ; N.R., 394. 
MacStewart, W., N.B., 304. 
Madow, W. G., N.R., 359. 
Magnetic declination, BibL, Schuster (1899) 490. 
Magnitude, random division of, BibL, Fisher 
(1940a) 462, Stevens (1939a) 493. 
Mahalanobis, P. C, N.E., 303, 304, 359. 
Males, proportion in births, (Example 21.8) 120 ; 
marriages of, (Example 21.9) 121- 2. 
Markoff, A. A., theorem on least squares, (Exercise 
25.5) 267. 
:- process (Markoff chains), BibL : Dooblin 
(1936, 1937) 457, Elfving (1937, 1938) 459, 
Feldheim (19366) 460, Fortot (1935-8) 463, 
Frechet (1935, 19366, 1,937a) 463, Ueiringor 
(1938) 464, Hadamard and Frechet (1933) 
467, Hostinsky (1937) 469, Kolrnogoroff 
(19376) 473, "Levy (19356, 1936c) 475, 
Markoff (1912) 477, Mihoo (1934) 478, 
Onicescu and Mihoc (1935-9) 481, Roman- 
ovsky (1936a) 489, Seukarev (1932) 490. 
Marriage, males according to age at, (Example 
21.9) 121-2. 
rate in England and Wales, (Table 30.2) 397, 
(Example 30.3, Table 30.5, Figure 30.4) 
408-9. 
Martin, E. S., N.B., 359. 
Mass production, see Quality Control. 
Matching problems, BibL : Battin (1942) 446, 
D. W. Chapman (1935) 451, J. A. 
Greenwood (1938). 465, (1940) 466, Greville (1938, 
INDEX . 513 
1941) 466, Olds (1938a) 481, Vernon (1936) 
496, Wiiks (1932c) 499. 
Mathematical Tripos, distribution of women 
obtaining firsts in, (Example 18.5) 56-7. 
Matri x, an thmetic of, Aitken (1937a, 6, 1938) 443, 
Bingham (1941) 447, Dwyer (1941a, b) 458, 
Hotelling (1943) 469. 
Maximum likelihood estimators, 12-49 ; 
consistence, 13-15 ; normality, 15-17 ; variance 
of, 17-18 ; efficiency of, 18-19 ; sufficiency, 
19-20; for several parameters, 34-49 ; 
variance and covariance of, 36-7 ; relation 
with minimum variance, 53, and with 
confidence intervals, 73-4. 
Bibl : Carlson (1932) 451, Fisher (1912, 
1921a, 19256, 1928c) 461, (1932, 1934a) 
462, Hotelling (1930) 469, Jeffreys (19386, 
1938c) 471, Koshal (1933, 1935, 1939) 474, 
Myers (1934) 479, E. S. Pearson (1937a) 
483, K. Pearson (1936) 486, Welch (1939c) 
499. 
McKendrick, A. G., N.R., 136. 
Mean, arithmetic, estimation of, 2 ; (Example 
17.6) sufficient estimator for, 11 ; (Example 
17.7) 19-20 ; most general distribution for 
which it is estimator (Example 17.10) 22 ; 
significance of, 98-100, (Examples 27.1, 
27.2) 311-12. 
deviation, in testing normality (Geary's 
ratio), 106; distribution of m.d., Bibl. : 
Fisher (1920) 461, Frechet (1936a) 463, 
Tricomi (19366, 1937) 495. 
—-— difference, 108. Bibl. : Cantelli (1913) 450, 
do Finetti and Paciello (19306) 455, de 
Finetti (1931) 455, U. S. Nair (1936) 479, 
Wold (1935) 501. 
— values, 5/6/. ; Aumann (1934-5) 444, Bunak 
(1936) 449, A. T. Craig (19366) 453, Dodd 
(1934, 1937a, 6, c, 1938) 457, Doodson (1.917) 
458, Dressel (1941) 458, Norris (1935, 1937) 
481, Wertheimer (1937) 499, Yasukawa 
(1925) 501, Zooh (1935, 1937) 503. 
Means, distribution of, Bibl. : Baker (19306?, 1931, 
1932, 1936, 1940) 444, Behrens (1929) 446, 
R. 0. Bose (1938a) 448, Carlson (1932) 451, 
Cochran (1937a) 452, A. T. Craig (1932) 
453, Dodd (1926-7) 456, Diuilap (1931) 458, 
Hall (19276) 467, Holzinger and Church 
(1929) 469, Irwin (1927, 1929, a, 1930) 470, 
Immer (1937) 470, Isserlis (1918a) 470, 
Jeffreys (1940) 471, Kolmogoroff (1929) 473, 
Pizzetti (1939) 487, Pollard (1934) 487, 
Rhodes (1927) 488, Romanovsky (1929) 
489, Simon (1943) 491, Truksa (1940) 495. 
See also Central Limit Theorem, Mean 
Values. 
—, test of difference, see Difference ; in 
multivariate analysis, 338-41. 
A.S.—VOL. II. 
Mean-square contingency, see Contingency. 
successive difference, Bibl.: Hart (1942 > 
467, von Neumann and others (1941a, 6} 
497, J. D. Williams (1941) 500. 
Median, as estimator, 5 ; confidence intervals for, 
(Exercise 19.5) 84. Bibl : Cisbani (1938) 
452, Doodson (1917) 458, Gini and Galvani 
(1929) 465, Gini (1938) 465, Gini and 
Zappa (1938) 465, Guiotta (1938) 466, 
Haldane (19426) 467, Hojo (1931, 1933) 
469, Jackson (1921) 471, K. R. Nair (19406) 
479, K. Pearson (19316) 486, Pollard (1934> 
487, Savur (1937a) 490, W. R. Thompson 
(1936) 494, Ville (1936c) 496. 
Migration, see Random Migration. 
Minimum variance, of maximum likelihood 
estimators, 18-19 ; in estimation, 50-5. 
%2, in estimation, 55-8. 
Missing plot technique, 229-33. Bibl : Allan 
and Wishart (1930) 443, Cornish (1940a, 6} 
453, K. R. Nair (1940a) 479, Yates (19336) 
501, Yates and Hale (19396) 502. 
Mode, Bibl : Doodson (1917) 458, Haldane 
(19426) 467, K. Pearson (19026) 484, 
Yasukawa (1926) 501. 
Moment-function, Bibl., IT. S. Nair (1939) 479. 
See Characteristic Functions, Generating 
Functions. 
Moments, efficiency of, 43-4. 
of distributions (specification), Bibl : 
Cornish and Fisher (1937) 453, Fisher (1937a) 
462, R. Henderson (1907) 468, O'TooIe- 
(1933) 481, Pearl (1937) 482, K. Pearson 
(1936) 486, Romanovsky (19366) 489, von. 
Mises (1937) 497. See Curve Fitting. 
——, problem of, Bibl : Bodewadt (1936) 447, 
Broggi (1934) 449, Chlodovsky (1938) 451, 
Hamburger (1920, 1921) 467, Haussdorf 
(1923) 468, Haviland (1935, 1936) 468, 
Marcinkiewicz (1939) 477, Polya (1920, 
1938a) 487, Stekloff (1914) 492, Stieltjes 
(1918) 493, Widder (1934) 499. 
, sampling, Bibl : Bernstein (1932) 446, 
C. C. Craig (1928) 453, (1940) 454, Dwyer 
(1937a, 1938, 1940) 458, Fisher (19296) 
461, Fisher and Wishart (1931) 462, Geary 
(1933) 464, Irwin and Kendall (1944) 470, 
Isserlis (19.186, c, 1931) 470, St. Georgescu 
(1932) 493, Sukhatme (1938c, 1944)494, 
Tschuprow (19186, 1921, 1923) 495, Wilka 
(1934, 1936) 499, Wishart (1929a, 6, 1930, 
1931a, 6, 19336) 500, Wishart and Bartlett 
(19326) 500, Ziaud-din (1938) 503. See 
also ^-statistics. 
Monotonic functions, in distribution theory, Bibl, 
Bochner (1937) 447. 
Mood, A. M., N.R., 304. 
Moore, G., phases in time-series, 126 ; . JV.jR*, 136. 
514 
INDEX 
Morant, G., N.R., 394. 
Morgan, W. A., N.R., 137. 
Mortality, see Life. 
Most-efficient estimator, 6, 10, 18-19. 
Most-selective confidence intervals, 75, 82. 
Moths, effect of weather on, (Example 22.10) 
171-2. 
Moving averages, 372-87, 399. BibL : ' Dodd 
(1939a, 1941a, b) 457, Frisch (1938) 464, 
Wold (19386) 501. 
wth values, BibL, Gumbel (1934, 1935a, 1939) 
466. 
Multinomial distribution, BibL, Kullback (1937) 
474, Lurquin (1937) 476. 
Multiple correlation, BibL: Bacon (1938) 444, 
R. C.' Bose (1934) 447, Fisher (19286) 461, 
Hall (1927a) 407, Kelley and McNemar 
(1929) 472, Kullback (1936c) 474, K. 
Pearson and Lee (1908) 484, K. Pearson (1916<2) 
485, K. Pearson and Young (1918) 485, 
Soper (1929a) 492, Starkey* (1939) 492, 
Tappan (1927) 494, Wilks (19326) 499, 
Wishart (19316) 500, Wong (1937) 501. 
—— curvilinear regression, 167, 236. See 
Regression. 
happenings, BibL, Greenwood and Yule 
(1920) 466, K. Pearson (19126, 1913) 484. 
See Poisson Distribution, Polya 
Distribution. 
Multivariate analysis, 328-62 ; Wishart's 
distribution, 330-4; Hotelling's distribution, 
335-8 ; significance of set of means, 338- 
41 ; discriminatory analysis, 341-8; 
canonical correlations, 348-58. 
BibL : Bartlett (19396, 1941) 445, Bishop 
(1939) 447, Fisher (1936a, 6, 1938c, 19396, 
1940d) 462, Hotelling (1933, 1936a, b) 469, 
P. L. Hsu (19396, 1941a, c, d) 469, Madow 
(1937, 1938) 476, Mahalanobis (1930, 1936a) 
476, Mahalanobis and others (19366) 476, 
Martin (1936) 477, Rider (1936) 488, Roy 
(1938, 1939a, 6, 1942a, 6) 489, Simonaen 
(1937) 491, Wald and Brookner (19416) 
498. 
distributions, estimation in, 33-7 ; normal, 
see Normal. BibL : Leser (1942) 475, 
Lukomski (1939) 476, Mahlmann (1935) 
477. See also Multiple Correlation. 
Myers, R. J., N.R., 45. 
Nair, K. R., c6nfidenca intervals for median, 81, 
N.R., 83. 
Nayer, P. N., testing hypotheses, 299 ; N.R., 304. 
Negative binomial, BibL, Fisher (19416) 462, 
Greenwood and Yule (1920) 466. See Polya 
Distribution. 
Neyrnan, J., confidence intervals, 75-6 ; Behrens' 
,' . ' test, 93 ; randomised blocks, 214; theory 
of tests, 270, 299, 308, 311, 323 ; Exercises 
from : (Exercises 19.2, 19.3) '83, (Exercise 
21.12) 140, (Exercises 26.2, 26.3) 304, 
(Exercises 26.4, 26.5) 305, (Exercise 27.3) 
327. N.R., 45, 83, 94, 136, 172, 266, 303, 
304, 326. 
Nisbet, S. D., (Example 25.1) 258-9. 
Non-central confidence intervals, 66. 
£, BibL, N. L. Johnson and Welch 
(1940a) 471. 
Non-normal data, in variance-analysis, 205-15. 
—— populations, BibL : Baker (1934) 444, 
Bartlett (1935a) 445, C. C. Craig (1941a) 
454, Geary (19366) 464, Laderman (1939) 
474, A. N. K. Nair (1942) 479, Pearson and 
Adyanthaya (1928, 1929) 482, E. S. Pearson 
(19316) 482, Rider (1931a) 487, Rietz (1932, 
1939) 488, Thorndike (1937) 494. 
Non-orthogonal data, BibL : K. R. Nair (1942) 
479, Wilks (1938e) 500, Yates (1934a) 501. 
Non-parametric tests, 322. BibL, ScheffS (1943) 
490. 
Non-random samples, BibL, " Student,5 (1909) 
493. 
Nonsense correlations, BibL, Yule (1926) 503. 
Normal equations, solution of, BibL, Hoel (1941) 
468. 
population, estimation of mean, 2, (Example 
17.6) 11, (Example 17.7) 19-20, (Example 
18.1) 51 ; estimation of variance, (Example 
17.6) 11, (Example 18.4) 54-5 ; centre of 
location of, (Example 17.22) 42 ; confidence 
intervals for mean, (Example 19.1) 63-4, 
(Example 19.3) 70; fiducial distribution, 
85 ; bivariate, (Example 17.17) 33-4, 
(Example 17.18) 37-8 ; regressions of, 
(Example 22.1) 144. 
BibL : Baker (1931) 444, Borgstrom 
(1918) 446, Cramer (1923, 1936) 454, Erdos 
and Kac (1939) 459, Haldane (1942a, 6) 
467, C. T. Hsu (1940, 1941) 469, Isserlis 
(19186) 470, Kac (1939) 472, Khintchine 
(1935) 473, Kullback (1935a) 474, Ledor- 
mann (1939) 475, Lehmann (1939) 475, 
Lengyel (1939) 475, K. Pearson (1924c) 485, 
Polya (1923) 487, Raikov (1938) 487, 
Rhodes (1928) 488, Tricomi (1935, 1936a, 
19366) 495, Yule (19386) 503. 
Normalisation of frequency functions, BibL : 
Cornish and Fisher (1937) 453, Haldane 
(1938) 467, Mahalanobis and others (19366) 
476, Paulson (1942) 482. 
Normality, tests of, 105-6. BibL : Fisher (19306) 
461, Geary (1935a, 6, 1936a) 464, Geary 
and Pearson (1938) 464, E. S. Pearson 
(1930, 1935c) 482, Yasukawa (1934) 501. 
Nuisance parameters, 134. BibL, Hotelling (1940) 
469. 
INDEX 
515 
Olds, E. G., N.R., 266. 
Omega, for testing goodness of fit, 107-9. Bibl., 
Smirnoff (1936) 491. 
One-sided confidence intervals, 76. 
Oppenheim, S., N.R., 437. 
Order, in random series, 122-4, and see Random 
Order. 
Orthogonal data, in variance-analysis, 219, 254. 
polynomials, 146-54,159-67. Bibl. ; Aitken 
(1932, 1933a, b, c) 442, Allan (1930) 443, 
Dieulefait (19346) 456, Fisher (19216, 19246) 
461, Greenleaf (1932) 465, Jackson (1934, 
1937, 1938) 471, Jordan (1932) 472, Lidstone 
(1933) 476, Romanovsky (1927) 489, San- 
sone (1933) 490, Shohat (1935) 491, C. ■ D. 
Smith (1939) 491, Tartler (1935) 494, 
Tchebycheff (1907) 494, Webster (1938) 
498, Wishart (1933a) 500, Wong (1935) 501. 
—— transformations, Bibl., Landahl (1938) 474, 
Ledermann (1938) 475. 
Oscillations, in time-series, 369, 370, 380, 397-8. 
See Periodicity. 
p-statistics, Bibl., Roy (19396, 1942a) 489. See 
Multivariate Analysis. 
Pxn test, see Combination of Tests. 
Paired comparisons, Bibl., Kendall and Babington 
Smith (1940) 472. 
Parameters, estimation of, see Estimation. 
—— of location and scale, 40-2. 
Partial correlations, Bibl. : Isserlis (1914, 1916) 
470, Stouffer (1934) 493, Subramanian 
(1935) 493. 
Pasteurised milk, in feeding, (Example 21.1.4) 133. 
Path coefficients, Bibl., Engelhart (1936) 459, 
Wright (1934) 501, 
Paulson, E. A., ^-distribution, 118 and N.R., 136. 
Peaks, in time-series, 124. 
Pearson distributions, moments in fitting, 43 4 ; 
sufficient estimators in (Exercise 17.18) 49. 
Bibl. : Ambarzumian (1937) 443, Baker 
(1940) 444, Beale (1937) 446, C. C. Craig 
(19366) 454, Dieulefait (19356) 456, Fisher 
(1921a) 461, Hildebrandt (1931) 468, Irwin 
(1930) 470, K. Pearson (1894, 1895, 19016) 
483, (1916a) 484, (1924a) 485, Romanovsky 
(1924) 489, Wishart (1926) 500. See also 
Type I, etc. 
Pearson, E. S., confidence intervals for binomial, 
81 ; t in non-normal case, 103 ; test of 
normality, 106; z in non-normal case, 
205 j (Exercise 23.4) 216-17 ; analysis of 
covariance, 238 ; (Exercises 26.2, 26.3, 26.4, 
26.6) 304-5 ; N.R., 45, 83, 136, 137, 245, 
266, 303, 304, 359. 
, K., (Example 21.14) 133 ; N.R., 45, 137, 
172, 173, 394. 
Peas, yields of, (Example 23.5) 200-2. 
Periodicity and periodogram analysis, 423-5, 
432-3,433-5. Bibl: Alter (1924, 1925, 
1926a, b, 1933, 1937) 443, Beveridge (1921, 
1922) 446, Bradley and Crura, (1939) 449, 
Brownlee (19246) 449, Bruns (1921) 449, 
Brunt (1925, 1928) 449, Buys-Ballot (1847) 
450, J. I. Craig (1916) 454, Crum (1923, 
1925) 454, Dodd (1930) 456, (1939a, 6, 
1941a, 6) 457, Prisch (1928, 1931, 1933) 
463, Greenstein (1935) 465, Hersch (1934) 
468, Kalecki (1935) 472, Koopmans (1940) 
474, Kuznets (1929, 1933) 474, Larmor and 
Yamaga (1917) 475, Mitchell (1913) 478, 
Mitchell and Burns (1935) 478, Moore (1914, 
1923) 478, Mouiton (1938) 478, Oppenheim 
(1909) 481, Pietra (1925) 486, Pollak (1927) 
487, Pollak and Kaiser (1935) 487, Powell 
(1930) 487, Savur (1941) 490, Schuster 
(1898, 1899, 1906) 490, Soper (19296) 492, 
Starkey (1939) 492, Stumpff (1926, 1937) 
493, Tinbergen (1937, 1938) 495, Tintner 
(1935) 495, Trachtenbarg (1921) 495, Vinci 
(1934) 496, Walker (1914, 1925, 1927, 1931) 
498, Wallis and Moore (1941) 498, Yule 
(1927a) 503. See also Harmonic Analysis, 
Time-series. 
Phases, in. time-series, 124, 125-6. 
Pilot sampling, 252, N.R., 266. 
Pitman, E. J. G.s tests of significance, 128-32, 
136; 2-test, 211; tests of hypotheses, 
323-6 ; Exercises from, (Exercises 17.9, 
17.10, 17.11) 47, (Exercise 21.3) 138, 
(Exercise 21.15) 140, (Exercise 27.2) 326. 
N.R., 45, 137, 216. 
Plant breeding, Bibl., Y. Tang (1938) 494. 
Plot arrangements, Bibl., Tedin (1931) 494. See 
Poisson distribution, (Example 17.9) 21—2 ; 
confidence intervals for, (Example 19.4) 70-1, 
81 ; conditional test for, (Example 21.12) 
127 ; in variance-analysis, 206-7. 
Bibl. : Ackermann. (1939) 442, R. A. 
Chapman (1938) 451, Cochran (1936a, 
19406) 452, Oopoland and Regan (1936) 453, 
Doetsch (1934) 457, Fisher and others 
(1922c) 461, Garwood (1936) 464, Irwin 
(1935, 1937a) 470, L6vy (1937a) 475, Liiders 
(1934) 476, Molina (1942) 478, Poisson (1837) 
487, Przyborowski and Wilenski (1940) 487, 
Raikov '(1936) 487, Bicker (1937) 488, 
Satterthwaite (1943) 490, " Student" (1907, 
1919) 493, Sukhatme (19376, 1938a) 494, 
von Bortkiewicz (1898, 1910) 496, Weida 
(1935) 498, Whitaker (1914) 499. 
Poisson's theorem in probability, Bibl., Bochner 
(1936) 447, Bonferroni (1933) 447. See 
Central Limit Theorem. 
1" T V 
<* 
516 
INDEX 
P61ya distribution, Bibl, del Chiaro (1936) 456, 
S. Guldberg (1935) 466. See Negative 
Binomial. 
Folychoric correlations, Bibl, Pearson and Pearson 
(19226) 485, Ritchie-Scott (1918) 489. 
Polynomials, expansions in, Bibl, Cacciopolli 
(1932) 450, Davis (1933) 455. See 
Orthogonal Polynomials, Curve Pitting. 
Population of England and Wales, (Example 
22.7) 161-3, (Examples 22.8. 22.9) 164-7, 
(Table 29.2, Figure 29.2) 365. 
analysis, Bibl : Lotka (1938, 1939) 476, 
Pearl and Reed (1923) 482, Volterra (1936) 
496. 
Potato yields, (Example 21.11) 126. 
Power of a test, 272, 307-8. Bibl ; G. W. Brown 
(1939) 449, Dantzig (1940) 455, Eisenhart 
(1938) 459, MacStewart (1941) 476, Simaika 
(1941) 491, P. L. Hsu (19416) 469, P. C. 
Tang (1938) 494. See also Statistical 
Hypotheses. 
Powers of normal variates, Bibl, Haldane (1942a) 
467. 
Prediction, see Forecasting. 
Pretorius, S. J., N.B., 173. 
Principal components, Bibl : Girshik (1936) 465, 
Hotelling (1933, 1936a) 469, Landahl (1938) 
474, Ledermann (1938) 475, Thurstone 
(1935) 495. 
Probability, Bibl : Bartlett (19336) 445, Beck 
(1936) 446, Belardinelli (1934) 446, Borel 
(1939) 447, Broderick (1937) 449, Cantelli 
(1932, 19336) 450, Castelnuovo (1932) 451, 
Cramer (1937, 1938, 1939) 454, de Finetti 
(1933a, 6, 1939a) 456, Doeblin (1938) 457, 
Doob (19346, 1941) 457, Eggcnberger (1924) 
459, Erdelyi (1937) 459, Khintchine (19376) 
473, Kolmogoroff (1931, 1933a) 473, Levy 
(1931a, 1931c, 1936a, 1937a, 1938a) 475, 
Lomnicki (1923) 476, Marchand (1937) 477, 
McKinsey (1939) 477, Moisseiev (1937) 478, 
Nagel (1936) 479, Reichenbach (1937) 488, 
Rice (1938) 488, Romanovsky (1931a) 489, 
Tornier (1929, 1930, 1936, 1937) 495, von 
Mises (1919a, 6, 1928, 1931, 1936a, 6, 1939c, 
1941) 497, Urban (1918) 496, Uspensky 
(1937) 496. 
Probits, Bibl, Bliss (1935, 1937) 447. 
Product, distribution of, Bibl, C. C. Craig (1936a) 
454. 
Product-moment correlation, see Correlation. 
Proficiency test of recruits, (Example 24.7) 240-2. 
Proportionate frequencies, in variate-analysis, 228. 
Proportions, tests of, Bibl, Swaroop (1938) 494. 
Quadratic forms, see Independence of Quadratic 
Forms. 
Quality control, Bibl. : Becker and others (1930) 
446, Jennett and Welch (1939) 471, E. S. 
Pearson (1933a, 1934) 482, Shewhart (1931) 
491, Simon (1941) 491, Welch (19366) 498, 
Wilks (1941) 500, Wolfowitz (1943) 501. 
Quartiles, Bibl, Hojo (1931, 1933) 469. 
Quasi-Latin squares, Bibl, Yates (1937a) 502. 
Quasi-sufficiency, Bibl, Bartlett (1940) 445. See 
Conditional Statistics. 
Racial likeness, N.B., 358. Bibl, Morant (1939) 
478, K. Pearson (19266) 485. See 
Multivariate Analysis. 
Rainfall in London, (Table 29.4, Figure 29.4) 367. 
Random component in time-series, 369 ; effect of 
trend-elimination on, 378-87 ; tests for, 
399. 
migration, Bibl, Brownleo (1911) 449. 
occurrences, Bibl, Morant (1921) 478. 
order, tests of, 122-7. Bibl ; (runs, etc.) 
Andre (1884) 444, Besson (1920) 446, Borel 
(1933) 447, Denk (1936) 456, Fisher (19266) 
461, Gumbel (1943a) 466, Jones (1937c) 
472, Kaucky (1936) 472, Mood (1.940) 478, 
von Bortkiewicz (1915a, 1917) 496, von 
Mises (1921) 497, Wolfowitz (1943) 501. 
—- paths, Bibl, McCrea (1936) 477, Polya 
(19386) 487. 
—— samples, tables of, Bibl., Mahalanobis arid 
others (1934) 476. 
—— sampling numbers, Bibl. : Kendall and 
Babington Smith (1939a) 472, K. R. Nair 
(1938a) 479, Yule (1938a) 503. 
—— sequence, Bibl : Copeland (1928, 1929, 
1932, 1936, 1937) 453, Dorgo (1934, 1936) 
458, Greville (1939) 466, Regan (1936, 
1938) 487, Rice (1939) 488, fcSwod and 
Eisenhart (1943) 494, Ville (1936a, 6) 496, 
von Mises (1931, 1933) 497, Wald (19366, 
1937) 497, Young (1941) 502. 
_ variables, Bibl. : Cramer (1935a) 454, Cramer 
and others (1938) 454, do Finetti (1929) 
455, Eyraud (19386) 459, Levy (1934, 
1935a, 6, 1936c, 1939a, 6) 4.75. Sec 
Probability. 
Randomisation, and 2-tost, 209 13, 255 0 ; in 
design, 263-6. Bibl, E. W. Pearson (19376, 
1938) 483 ; and see Design. 
Randomised blocks, 213-14. Bibl. : Cornish 
(1940a) 453, McCarthy (1939) 477, Welch 
(1937) 498. See Blocks. 
Randomness, Bibl : Borol (1937) 447, Dodd 
(1942) 457, Kendall (1941) 472, Kermack 
and MoKendrick (1936,, 1937) 473, Wiener 
(1938) 499. 
Range, test of, (Exercise 27.3) 327. Bibl. : Ceary 
(1943) 464, Hartley (1942) 467, McKay and 
Pearson (1933) 477, Newman (1939) 480, 
Olds (1935) 481, E. S. Pearson (1926, 1932) 
INDEX 
517 
482, Pearson and Haines (1935a) 482, 
Pearson and Hartley (1942, 1943) 483, 
Romanovsky (19336) 489, W. R. Thompson 
(1938) 494, Tippett (1925) 495. 
Rank correlation, 123, 441. Bibl. : Daniels (1944) 
455,Dantzig (1939) 455, Dubois (1939) 458, 
Hotelling and Pabst (1936c) 469, Kendall 
(19386, 1942a) 472, Kendall and others 
(1939, 19396) 472, Olds (19386) 481, K. 
Pearson (1914, 1921) 484, Pearson and 
Pearson (1931c, 1932) 486, "Student" 
(1921) 493, Wallis (1939) 498, Watkins 
(1933) 498, Woodbury (1940) 501. 
Ratio, distribution of, Bibl. : C. C. Craig (19296) 
453, Curtiss (1941) 454, Pieller (19326) 460, 
Geary (1930) 464, Gordon (1941) 465, 
Hirschfeld (1937) 468, Kullback (1936a) 
474, Nicholson (1941) 481, van Uven (1932, 
1939) 496. 
Rectangular distribution, estimation of extremes, 
(Example 17.15) 28 ; intrinsic accuracy, 
(Example 17.11) 47 ; estimation by sample- 
centre, (Exercise 17.16) 48 ; confidence 
intervals for range, (Exercise ly.i) oo. 
Bibl. : O. L. Davies (1932) 455, Dunlap 
(1931) 458, Hall (19276) 467, Olds (1935) 
481, Rietz (1931a) 488. 
Region of acceptance, 63, 76, 270. 
Regression, Gauss' theorem on residuals, 60-1 ; 
generally, 141-74 ; analytical theory, 
141-5 ; fitting of curvilinear regressions, 
145-53 ; standard errors and tests of 
significance, 153-8 ; equal steps of variate, 
159-67 ; multiple curvilinear, 167 ; 
addition of new variates, 167-72 ; in analysis 
of variance, 233-6 ; relation with Hotelling's 
1\ 336-7 ; in discriminatory analysis, 344-5. 
Bibl : R. G. D. Allen (1939) 443, PL V. 
Allen (1938) 443, Andersson (1932) 443, 
(1934) 444, Bartlett (1933a, 1938c) 445, F. 
Bernstein (1937) 446, Blakeman (1905) 447, 
S. 8. Boko (1934a, 6, .19386) 448, Camp 
(19256) 450, Cochran (1938a) 452, Dodd 
(19376, c) 457, Dwyer (19376, 1941c) 458, 
Eisenhart (1939) 459, Ezekiel (19306) 460, 
Fisher (19226) 461, Galton (1886) 464, 
Jones (19376) 472, Koopmans (1937) 474, 
Mendershausen (1937a) 477, T. V. Moore 
(1937) 478, Neyman (1926) 480, K. Pearson 
(1896) 483, (1921, 1926a) 485, Qnensel 
(1936) 487, Richards (1931) 488, 
Romanovsky (1926, 19316) 489, Slutzky (1914) 
491, K. Smith (1918) 492, Wangh (1942) 
498, Welch (1935) 498, Wicksell (19346) 
499, Yates (1939d) 502, Yule (1936) 503. 
coefficients, standard error of, 153-6 ; exact 
tests of, 156-8. 
Regular unbiassed critical regions, 318-19. 
Rejection of observations, Bibl. : Irwin (19256) 
470, Pearson and Chandra Sekhar (1930) 
483, Rider (1933) 488, W. R. Thompson 
(1935) 494. 
Relaxed oscillations, Bibl., Le Corbeiller (1933) 
475, van der Pol (1930) 496. 
Reliability coefficients, Bibl, Stouffer (19366) 493. 
Replication, 255. Bibl: Bartlett (1938a) 445, 
Cochran (19376, 19386, 1939a) 452, Yates 
(1933a, 6) 500, (1936a7) 501. See Design. 
Representative method of sampling, Bibl : A. T. 
Craig (1939) 453, Jensen (1925) 471, Key- 
man (19336, 1934) 480, Sukhatme (1935) 
493. 
Residual, in variance-analysis, 178, 185-7. 
Ricker, W. E., confidence intervals for Poisson 
distribution, 81. 
Riemann zeta-function, Bibl, Jessen and Wintner 
(1935) 471. 
Risk, theory of, Bibl., Cramer (1923) 454, Esscher 
(1932) 459. 
Robinson, G., N.R., 394, 437. 
Roots of equations, distribution of, Bibl, Girshik 
(1939, 1942) 465. 
Routine analysis, Bibl : Neyman (19396, 19416) 
480, Przyborowski and Wilenski (19356) 
487, "Student" (1927) 493. 
Roy, S. N., distribution of canonical correlations, 
357 and N.R., 359. 
Runs, in time-series, see Random Order. 
Sampling distributions, moments of, see ^-statistics, 
Moments. 
inquiries, see Design. 
—-—, miscellaneous, Bibl : Bartky (1943) 445, 
Bartlett (19376) 445, Baton (19336) 446, 
Bowley (1925) 448, Burks (1933) 450, Clap- 
ham (1931, 1936) 452, Cochran (19366, 
19396, 19426) 452, A. T. Craig (1933a, 6) 
453, C. C. Craig (1931a) 453, Crum (1933) 
454, David (19386) 455, Hey (1938) 468, 
Hilton (1924, 1928) 468, Kiser (1934) 473, 
McKay (1934) 477, Neyman (1933a, 1934, 
1938a) 480, Olds (1939, 1940) 481, Panse 
(1939) 482, E. S. Pearson (1933a, 1934) 
482, Pepper (1929) 486, Rhodes (1925) 488, 
Rider (19316) 488, Rietz (1937) 488, Shew- 
hart and Winters (1928) 491, " Sophister " 
(1928) 492. 
surveys, Bibl, A. N. Bose (1941) 447, C. 
Bose (1943) 447; and see Sampling, 
miscellaneous. 
Sasuly, M., N.B., 394. 
Savur, S. R., N.B., 83. 
Scale, estimation of parameters of, 40-2 ; 
elimination of parameters of, 79-80 ; Pitman's 
tests of, 323-6. Bibl, Pitman (1939a, 6) 
486. 
518 INDEX 
Scale, reading, Bibl, Yule (19276) 503. 
Scales of measurement, Bibl., Cochran (1943) 452. 
Scatterance, N.R., 358. 
Scedastic curve, 142. 
Scheffe, H., non-parametric tests, 322 ; N.R., 
304, 326. 
Schoolchildren, tests of, (Example 25.1) 258-9, 
(Example 28.4) 351-2. 
Schultz, H., N.B., 394. 
Schuster, Sir Arthur, significance of periodogram, 
434 ; N.B., 437. 
Seasonal effect, in time-series, 369. Bibl. : Bow- 
ley and Smith (1924) 448, Carmichael (1931) 
451, Carver (1932) 451, Crum (1925) 454, 
Detroit Edison Co. (1930) 456, Donner 
(1928) 457, Falkner (1924) 460, Gressens 
(1925) 466, Mendershausen (19376) 478, 
Robb (1929, 1930) 489, Wald (1936a) 497, 
Wisniewski (1934) 501, Zrzavy (1933) 503. 
Second Limit Theorem, Bibl., Frechet and Shohat 
(1931) 463. 
moment, see Variance. 
Seed in optical glass, (Example 23.6) 202-5. 
Seeds of wheat, germination of, (Example 23.7) 
207-9. 
Selective confidence intervals, 75-6. 
Semi-normal distribution, Bibl., Steffensen (1937) 
492. 
Seminvariants, see Cumulants, /c-statistics. 
Sensitivity, of tests of significance, 256. 
Serial correlation, 402-4. See Correlograrn. Bibl.: 
R L. Anderson (1942) 443, Bartlett (1935c) 
445, Dixon (1944) 456, Kendall (1944a, 6) 
473, Koopmans (1942) 474, Marples (1932) 
477, Schumann and Hofmeyer (1942) 490, 
Yule (1921) 502, (1926, 1927a) 503. 
Sheep population of England and Wales, (Table 
29.3, Figure 29.3) 366, (Example 29.5) 
385-6, (Example 30.5) 411, (Example 30.8) 
416-18. 
Sheppard's corrections, see Grouping Corrections. 
Shortest confidence intervals, 71-5, 75-6. 
Significance tests, 96-140, 269-327. See Statistical 
Hypotheses. Bibl., Jeffreys (1938«) 471, 
Peiser (1943) 486. 
Silverstone, H., minimum variance, 6.1 ; 
(Exorcises 18.1, 18.2) 61. 
Simaika, J., N.R., 304, 359. 
Similar regions, 283. Bibl., Feller (1938) 460. 
Simon, L. E., N.R., 61. 
Simple hypotheses, 269, 272-82, 317-26. 
Simultaneous estimation, of several parameters, 
34-44. 
fiducial distributions, Bibl., Bartlett (1939a) 
Sinusoidal limit, N.R., 394. Bibl. : Marsueguerra 
(1936) 477, Romanovsky (1931c, 1932a, 
1933a) 489, Slutzky (19376) 491. 
Skewness, Bibl, Frisch (1934a) 464, Garner (1932) 
464. 
Skulls (Egyptian), (Example 28.3) 345-8. 
Slutzky, E., N.R., 394, 399. 
Slutzky-Yule effect, 378-87, 399. Bibl, Slutzky 
(19376) 491, Yule (1921) 502. 
Small numbers, law of, see Poisson Distribution. 
Smirnoff, N., to2-test, 109. 
Smith, H. Fairfield, N.R., 359. 
, K., minimum-^2, 55 and N.R., 61. 
Smoothing, see Moving Averages, Trend. 
Soil, loss of weight in, (Example 22.3) 149-52, 
(Example 22.6) 158. 
Solomon, L., footnote, 51. 
Spearman, C, (Exercise 25.3) 267. 
Spearman's factor theory, see Factor Analysis. 
/>, test of, 132. 
Speed tests in children, (Example 28.4) 351-2. 
Spelling ability in children (Example 25.1) 258-9. 
Spencer's formula in curve fitting, (Examples 29.2, 
29.3) 376-7, 378-80, (Exercise 29.3) 394-5, 
(Example 30.2) 405. 
Spurious correlation, Bibl. : K. Pearson (18976) 
483, Spearman (1907, 1910) 492, Wicksell 
(1921) 499. 
Square of a variate, Bibl., Haldane (1941) 467. 
Squariance, footnote 178. 
Stabilising of variance, 207. 
Stability of series, .see Lexis Theory. 
Stable laws of probability, Bibl. : Bochner (1937) 
447, Feldheim (1937a) 460, Khintchino and 
Levy (1936) 473, Khintchino (1938) 473. 
Standard deviation, estimation of, (Example 17.5) 
6-7, (Example 17.6) 11, 52. See Variance. 
errors, in testing significance, 97-8 ; of 
regression coefficients, 153-6. Bihl. : Dork- 
son (1939) 456, Edgeworth (1908, 1909) 
459, Eels (1929) 459, Hendricks (1934) 468, 
Isserlis (1915, 1916) 470, Miller (1934) 478, 
K. Pearson (1903, 1913, 1920) 484, (\924d) 
485, K. Pearson and Loo (1908) 484, K. 
Pearson and Filon (1898) 483. 
Latin squares, 259. 
Stationary time-series, 396. Bibl. : Khintchino 
(1932, 1933, 1934) 473, Slutzky (1934) 491, 
Wold (1938c/, 1939) 501. See Time-series, 
Correlograrn. 
Statistical hypotheses, definition, 269 ; orrors of 
first and second kind, 270-2 ; power 
function, 272 ; simple hypotheses, 272 5 ; 
best critical regions, 277-80 ; relation with 
sufficient estimators, 281-2 ; composite 
hypotheses, 282-3 ; similar regions, 283-7 ; 
of several, degrees of freedom, 287 ; linear 
hypotheses, 292-5; likelihood criteria, 
295 ; k samples, 295-302 ; bias, 307-26 ; 
regions of Type A, 309-14, of Type Ax, 
314-16, of Type B, 316-17, of Type 0, 
INDEX 
519 
317-22; limiting properties, 322 ; Pitman's 
tests, 323-6. 
BibL : G. W. Brown (1940) 449, Chandra 
Sekhar and Francis (1941) 451, Daly (1940) 
454, Dantzig (1940) 455, Gumbel (1942) 
466, R. W. Jackson (1936) 471, 
Kolodzieczyk (1933, 1935) 474, Neyman (19356, 
19386) 480, (1942) 481, Neyman and Pear- 
son (1928, 1931a, 1933a, c, 1936a, 1938) 
480, E. S. Pearson (1941, 1942a) 483, 
Pitman (19396) 486, Rietz (1938) 488, 
Scheffe (1942a, 1943) 490, Wald (1939a) 
497, (1941a) 498, Wilks (1935c, 1938a) 499, 
Wolfowitz (1942) 501. 
Statdstical Review of England and Wales, data from, 
(Example 21.8) 120, (Example 21.9) 121. 
Steveils, W. L., test of significance in periodograrn, 
434 ; N.R., 216. 
Stieltjos integrals, BibL, Shohat (1930) 491. 
Stochastic convergence, 440. See Convergence in 
Probability. 
■ dependence, see Independence. 
processes, BibL, Doob (1934a, 1937, 1938) 
457, Feller (1936a) 460. See Probability. 
Stock forecasting, BibL, Cowles (1933) 453, Cowles 
and Jones (1937) 453. 
Stock, J. S., N.R., 266. 
Stratified sampling, 249-52. BibL : P. H. 
Anderson (1942) 443, Baker (1930c) 444, G. M. 
Brown (1933) 449, Frankol and Stock (1939) 
463, McKay (1934) 477, Mood (1943) 478. 
See also Sampling, miscellaneous, 
Representative Method. 
" Student " (W. 8. Gosset), see Gosset. 
Studentisation, 79-81, 134. BibL, Hartley (1938, 
1944) 467, Newman (1939) 480. 
" Student's " distribution, confidence intervals 
based on, 79-80 ; fiducial inference based 
on, 88 ; properties of, 100-2 ; in tenting 
mean, 98-100 ; in non-normal case, 102-4 ; 
other uses, 104 ; in testing two means, 
109-10, 113-14; in testing Spearman's p, 
124 ; in Pitman's tests, 131, 132 ; in testing 
regressions, 156, 158, 172 ; in analysis of 
covariance, 244 ; (Example 26.9) 291. 
BibL : Bartlett (1935a) 445, C. C. Craig 
(1941a) 454, Daniels (1938a) 454, Fisher 
(1926a) 461, Geary (19366) 464, Hendricks 
(1936) 468, P. L.*Hsu (1938a) 469, N. L. 
Johnson and Welch (1940a) 471, Kerrich 
(1937) 473, Kolodzieczyk (1933) 474, Lader- 
mann (1939) 474, McKay and others (1932) 
477, Merrington (1942) 478, A. N. K. Nair 
(1942) 479, Perlo (1933) 486, Rider (1929) 
488, Rietz (1939) 488, Steffensen (1936) 492, 
" Student " (1908a, 1931a) 493,Treloar and 
Wilder (1934) 495. 
hypothesis, 285-7. BibL, Neyman and 
Tokarska (19366) 480, Przyborowski and 
Witenski (1935a) 487. 
Stumpff, K., N.R., 437. 
Sufficient estimators, 7—12 ; given by maximum 
likelihood, 19 ; general form possessing, 
24-5 ; distribution of, 25; when range 
depends on parameter, 27-8 ; for several 
parameters, 39-40 ; giving minimum- 
variance estimators, 52 ; relation with 
confidence intervals, 74-5, 79; relation 
with U.M.P. tests, 281-2, with U.M.P.U. 
tests, 310. 
BibL : Bartlett (19366, 1937c, 1940) 445, 
Darmois (1935) 455, Koopman (1936) 474, 
Neyman (1935a) 480, Neyman and Pearson 
(1936a) 480,, Pitman (1936) 486, Welch 
(1939a) 498. 
Sukhatme, P. V., tables for Behrens' test, 92, 111 ; 
(Exercise 26.8) 305-6 ; sampling moments, 
4.40. N.R., 94, 266, 304. 
Sum, distribution of, see Means. 
Summation convention, 329. 
Sunspots, BibL, Schuster (1906) 490, Yule (1927a) 
503. 
Symmetric functions, BibL, O'Toole (1931, 1932) 
481. See Moments, ^-statistics. 
T-distribution, see Hotelling's T. 
Tabular differences, BibL, Ladormann and Lowan 
(1939) 474. 
Tanbura, E., N.R., 137. 
Tang, P. C, linear hypotheses, 301; N.R., 303, 
Tchebycheff, P. L., (Exex*cise 22.4) 173 ; NJL, 172. 
Tchebycheff-Hermite polynomials, BibL : Doetsch 
(1934) 457, Erdelyi (1938) 459, Feldheim 
(19376) 460. See Gram-Charlier Series, 
Orthogonal Polynomials. 
Tchebycheff's inequality, BibL : Berge (1938) 
*446, Bernstein (1937) 446, Camp (1922) 450, 
O. G. Craig (1933) 454, K. Pearson (1919) 
485, C. D. Smith (1930) 491. 
Tea-drinking, BibL, Mahalanobis (1943) 476. 
Telephone service, BibL, Newland and. Neal (1939) 
479, Palm (1937) 482. 
Terminals of frequency-distribution, confidence 
intervals for, 83. 
Test construction, BibL, Cureton and Dunlap 
(1938) 454. 
Tests of significance, see Significance, Statistical 
Hypotheses. 
Tetrachorie functions, BibL : J. Henderson (1922) 
468, K. Pearson (1912a, 1913a, b) 484, K. 
Pearson and Heron (1913c) 484, New bold 
(1925) 479, Pearson and Pearson (19226) 
485. 
Tetrad difference, (Exercise 28.10) 362. BibL, 
Hotelling (19366) 469, Wilks (1932d) 499. 
See Factor Analysis. 
520 
INDEX 
Third moment, distribution of, Bibl., Pepper 
(1932) 486. 
Thompson, C, on /l-tests, 299; N.B., 303. 
Thompson, W. R., (Exercise 19.5) 84; N.B., 83. 
Thomson, G., (Example 25.1) 258-9. 
Ties in ranking, 127, 441. 
Time-series, 363-439 ; examples of, 363-9 ; trend, 
371-8 ; effect of trend elimination, 378-87 ; 
variate difference method, 387-94 ; 
oscillations, 397-9 ; tests for randomness, 399 ; 
types of oscillatory series, 395-402 ; serial 
correlations, 402-4 ; correlogram, 404-13 ; 
antoregressive schemes, 414-21 ; 
autocorrelation function, 421-3 ; periodogram 
analysis, 423-33 ; significance of a 
periodogram, 433-5 ; lag correlation, 435-7. 
Bibl. ; Bartels (1935) 445, Darmois (1929) 
455, Davis (1941) 455, Jones (19376, c) 472, 
Kendall (1944a, b) 473, Koopmans (1937, 
1940, 1941) 474, Macaulay (1931) 476, 
Roos (1934, 1936) 489, von Szeliski (1929) 
497, Wallis and Moore (1941) 498, Wold 
(1938a) 501, Zaycoff (1936, 1937) 503. 
See also Correlogram, Harmonic Analysis, 
Periodicity. 
Tintner, G., variate-difference method, 393. N.R., 
394. 
Tokarska, B., N.B., 303. 
Tolerance limits, see Quality Control. 
Trade cycles, see Periodicity. 
Traffic signals, Bibl., Garwood (1940) 464. 
Transformation of distributions, Bibl. : Baker 
(1930a, 1934) 444, Beall (1942) 446, Bliss 
(1938) 447, Curtiss (1943) 454, Frankel and 
Hotelling (1938) 463, Landahl (1938) 474, 
Rietz (19316) 488, Tricomi (1938) 495, 
Yasukawa (1925) 501, Zoch (1934) 503. 
Trans variation, Bibl., Castellano (1934, 1937) 451. 
Travers, R. M. W., N.R., 359. 
Trend, 369-70, 371-87. Bibl: Lorenz (1931, 
1935) 476, Macaulay (1931) 476, Rhodes 
(1921) 488, Sasuly (1934) 490, Schumann 
(1938) 490, Sipos'(1930) 491, Working and 
Hotelling (1929) 501. 
Trough, in time-series, 124. 
Truncated normal distribution, Bibl., Keyfitz 
(1938) 473, Stevens (1937a) 493. 
Turner, H. H., N.R., 437. 
Turning-point, in time-series, 124. 
Two samples, Bibl. : Behrens (1929) 446, Dixon 
(1940) 456, P. L. Hsu (1938a) 469, Lengyel 
(1939) 475, Mathisen (1943) 477, E. S. 
Pearson (1929) 482, Pearson and Neyrnan 
(1930) 482, K. Pearson (1911a) 484, (1931a) 
485, Peek (1937) 486, Rhodes (1924, 1925) 
488, Romanovsky (1928) 489, Starkey 
(1938) 492, Sukhatme (1935, 19366) 493, 
Swaroop (1938) 494, W. R. Thompson 
(1933) 494, Wald and Wolfowitz (1940c) 
498, Welch (1938a) 498, Yates (1939/) 
501. 
Type A, B, C, in statistical tests, 309-27. 
Type I distribution, (Exercise 17.17) 49. 
II distribution, Bibl., Carlson (1932) 451. 
Ill distribution, estimation of parameters 
in, (Example 17.8) 20-1, (Example 17.13) 
26, (Example 17.19) 39, (Example 18.3) 
53-4; sufficiency, (Example 17.21) 40 ; 
centre of location of, (Example 17.23) 42 ; 
confidence intervals for parameter (Example 
19.5) 74-5 ; fiducial distribution of 
parameter, 87. Bibl. : C. C. Craig (1929a) 453, 
Kullback (1936a) 474, Olshen (1938) 481, 
Salvosa (1930) 490, Wicksell (1933) 499. 
IV distribution, centre of location of, 
(Exercise 17.15) 48; intrinsic accuracy of, 
(Exercise 17.19) 49. 
Unbiassed estimators, 3-4 ; confidence intervals, 
76 ; tests, 309-27. 
Unequal subclasses, in variance-analysis, 220-4. 
Bibl. : Brandt (1933) 449, Wald (19406) 
497, (1941d) 498, Wilks (1938e) 500, Yates 
(1934a) 501. 
Uniformly most powerful tests, 276 ; unbiassed 
tests, 309, N.B., 359. 
t/-shaped distribution, Bibl., Holzinger and Church 
(1929) 469. 
Variability, measures of, Bibl. : Castellano (1935) 
451, de Vergottini (1936) 456, Gal van i 
(1931) 464, Gini (1912, 1930) 465, March 
(1926) 477, Pietra (1932a) 486, Vinci (1920) 
496. 
Variance, analysis of, see Analysis of Variance. 
——, distribution and tests of, Bibl. : Bakor 
(1931, 1932, 1935, 1940) 444, Church 
(1925, 1926) 452, A. T. Craig (1932, 1938) 
453, Dunlap (1931) 458, Fertig and Proehl 
(1937) 460, Greenwood and Groville (1939) 
466, Kondo (1930) 474, Lo Roux (1931) 
475, K. Pearson (1931a") 486, Quensel 
(1938) 487, Rhodes (1927) 488, Rietz (1931a) 
488, Romanovsky (1925a) 489, Truksa 
(1940) 495, von Bortkiewicz (1922) 497, 
Yasukawa (1925) 501. See also Fisher's 
.Distribution, k samples. 
, estimation of, Bibl., O. L. Davies and 
Pearson (1934) 455, P. L. Hsu (19386) 469. 
ratio, Bibl. : S. S. Bose (1935) 448, Cochran 
(1941) 452, Finney (1938, 1941a) 460, 
Morgan (1939) 478, U. S. Nair (1941a, 6) 
479, Scheffe (19426) 490. See also Fisher's 
Distribution. 
, test of, in normal samples, 104 ; difference 
of two variances, 115, (Example 26.8) 289. 
INDEX 
521 
Variate-difference method, 387-94. BibL : 
Anderson (191.4, 1923, 1926, 1929) 443, Cave- 
Browne-Cave (1904) 451, Cave and Pearson 
(1914) 451, Haavelmo (1941) 467, K. 
Pearson and Elderton (1923a) 485, Robb 
(1929) 489, " Student " (1914) 493, Tintner 
(1935, 1940, 1941) 495, Zaycoff (1936, 1937) 
503. 
Variate transformations, in analysis of variance, 
206-9. See Transformation. 
Variation, coefficient of, BibL : Hendricks and 
Robey (1936) 468, McKay (1931) 477, 
McKay and others (1932) 477. 
Variety trials, BibL, Yates (1936d, 1937a) 502. 
Vector correlation, alienation coefficients, 
(Exercises 28.8, 28.9, 28.10) 361-2. 
—— representation of a sample, BibL, Bartlett 
(19346) 445. 
von Mises, R., contest, 108 ; Irregular Kollektiv, 
123. 
Wald, A., most-selective confidence intervals. 
82-3; limiting properties of tests, 322, 
N.R., 83, 304, 326. 
Walker, Sir Gilbert, time-series, 420 ; significance 
of a periodogram, 434. 
Wallace, N., N.R., 359. 
Wallis, W. A., phases in time-series, 126, N.R., 136. 
Water-content in samples, (Example 23.3) 190—4, 
(Example 23.4) 196-8. 
Weather, effect on moths, (Example 22.10) 171-2. 
Welch, B. L., difference of two means, 112, 
(Example 21.6) 113; (Exercise 21.7) 139; 
Latin squares, 261 ; footnote 295. N.R., 
45, 83, 216, 304, 359. 
Wheat-price index (of Sir William Beveridge), 
(Table 30.1) 396, (Example 30.4, Table 30.6, 
Figure 30.5) 409-10; (Table 30.9 and 
Figure 30.9) 425-30; (Example 30.5) 
431-2 ; (Example 30.10) 435. 
Wheat prices, and horse population, (Table 30.10) 
436. 
Whittaker, Sir Edmund, periodogram (Exercise 
30.10) 439, Calculus of Observations, N.R., 
394, 437. 
Wicksell, S. D., theorem on regressions, 143; 
(Example 22.2) 144; (Exercises 22,1, 22.2, 
22.3) 173. N.R., 172, 173. 
Wiener, jNT., autocorrelation function, 422. 
Wilks, S. S., shortest confidence intervals, 82 ; 
A-tests, 299; Hotelling's T, 337-8; 
distribution of means, 341, 358 ; (Exercise 
19.1) 83, (Exercise ,19.4) 84, (Exercises 
28.4, 28.5) 360. N.R., 83, 245, 303, 304, 
359. 
Wilsdon, B. H., N.R., 245. 
Wilson-Hilferty transformation of %2, 118. 
Wishart, J., (Exercise 24.3) 246, (Exercises 28.1, 
28.2) 359-60. N.R., 245, 359. 
Wishart's distribution, 330-5, 337-8, (Exercise 
28.3) 360. BibL : P. L. Hsu (1939a) 469, 
Ingham (1933) 470, Wishart (1928) 500, 
Wishart and Bartlett (1933c) 500. 
Wold, H., contest, 108; (Exercise 25.3) 267; 
time-series, 418 ; Carleman criterion, 440. 
N.R., 266, 437. 
Wolfowitz, J., confidence intervals for terminals 
of a distribution, 83. N.R., 304. 
Woodbury, M., tied ranks, 441. 
Wool thread, weights of, (Example 23.2) 183-5. 
Yates, F., tables of *, 102 ; (Example 23.5) 200-2 ; 
^-distribution, 206; (Example 23.8) 214 ; 
(Example 24.1) 221-5; (Example 24.5) 
230-3 ; design of experiments, 263. N.R., 
94, 216, 245, 266. 
Yule, G. U., autoregressive series, 418 ; (Exercises 
30.3 and 30.9) 439. N.R., 394, 437. 
Zaycoff, R., variate-difference method, 393. N.R., 
394. 
^-distribution, see Fisher's Distribution. 
