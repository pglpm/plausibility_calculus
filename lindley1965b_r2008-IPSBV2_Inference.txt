INTRODUCTION TO 
PROBABILITY AND 
STATISTICS 
FROM A BAYESIAN VIEWPOINT 
PART 2 
INFERENCE 
BY 
D. V. LINDLEY 
Head of the Department of Statistics 
University College London 
CAMBRIDGE 
AT THE UNIVERSITY PRESS 
1970 
CAMBRIDGE UNIVERSITY PRESS 
Cambridge, New York, Melbourne, Madrid, Cape Town, Singapore, Sao Paulo, Delhi 
Cambridge University Press 
The Edinburgh Building, Cambridge CB2 8RU, UK 
Published in the United States of America by Cambridge University Press, New York 
ww w. Cambridge. org 
Information on this title: www.cambridge.org/9780521055635 
© Cambridge University Press 1965 
This publication is in copyright. Subject to statutory exception 
and to the provisions of relevant collective licensing agreements, 
no reproduction of any part may take place without the written 
permission of Cambridge University Press. 
First published 1965 
Reprinted 1970 
Re-issued in this digitally printed version 2008 
A catalogue record for this publication is available from the British Library 
ISBN 978-0-521-05563-5 hardback 
ISBN 978-0-521-29866-7 paperback 
INTRODUCTION 'TO 
PROBABILITY AND 
STATISTICS 
To 
M. P. MESHENBERG 
in gratitude 
Vll 
CONTENTS 
Preface page ix 
5 Inferences for normal distributions 
5.1 Bayes's theorem and the normal distribution 1 
5.2 Vague prior knowledge and interval estimates 
for the normal mean 13 
5.3 Interval estimates for the normal variance 26 
5.4 Interval estimates for the normal mean and 
variance 36 
5.5 Sufficiency 46 
5.6 Significance tests and the likelihood principle 58 
Exercises 71 
6 Inferences for several normal distributions 
6.1 Comparison of two means 76 
6.2 Comparison of two variances 86 
6.3 General comparison of two means 91 
6.4 Comparison of several means 95 
6.5 Analysis of variance: between and within 
samples 104 
6.6 Combination of observations 112 
Exercises 122 
7 Approximate methods 
7.1 The method of maximum likelihood 128 
7.2 Random sequences of trials 141 
7.3 The Poisson distribution 153 
7.4 Goodness-of-fit tests 157 
Viii CONTENTS 
7.5 Goodness-of-fit tests (continued) page 168 
7.6 Contingency tables 176 
Exercises 185 
S Least squares 
8.1 Linear homoscedastic normal regression 203 
8.2 Correlation coefficient 214 
8.3 Linear hypothesis 221 
8.4 Computational methods 236 
8.5 Two-way classification 246 
8.6 Further applications of linear hypothesis theory 257 
Exercises 270 
Appendix. Two-sided tests for the ^-distribution 282 
Bibliography 285 
Subject Index 287 
Index of Notations 292 
ix 
PREFACE 
The content of the two parts of this book is the minimum that, 
in my view, any mathematician ought to know about random 
phenomena—probability and statistics. The first part deals with 
probability, the deductive aspect of randomness. The second 
part is devoted to statistics, the inferential side of our subject. 
The book is intended for students of mathematics at a  
university. The mathematical prerequisite is a sound knowledge of 
calculus, plus familiarity with the algebra of vectors and 
matrices. The temptation to assume a knowledge of measure 
theory and general integration has been resisted and, for 
example, the concept of a Borel field is not used. The treatment 
would have been better had these ideas been used, but against 
this, the number of students able to study random phenomena 
by means of the book would have been substantially reduced. 
In any case the intent is only to provide an introduction to the 
subject, and at that level the measure theory concepts do not 
appreciably assist the understanding. A statistical specialist 
should, of course, continue his study further; but only, in my 
view, at a postgraduate level with the prerequisite of an honours 
degree in pure mathematics, when he will necessarily know the 
appropriate measure theory. 
A similar approach has been adopted in the level of the 
proofs offered. Where a rigorous proof is available at this level, 
I have tried to give it. Otherwise the proof has been omitted 
(for example, the convergence theorem for characteristic  
functions) or a proof that omits certain points of refinement has been 
given, with a clear indication of the presence of gaps (for 
example, the limiting properties of maximum likelihood). 
Probability and statistics are branches of applied mathematics— 
in the proper sense of that term, and not in the narrow meaning 
that is common, where it means only applications to physics. 
This being so, some slight indulgence in the nature of the rigour 
is perhaps permissible. The applied nature of the subject means 
that the student using this book needs to supplement it with 
X 
PREFACE 
some experience of practical data handling. No attempt has 
been made to provide such experience in the present book, 
because it would have made the book too large, and in any case 
other books that do provide it are readily available. The student 
should be trained in the use of various computers and be given 
exercises in the handling of data. In this way he will obtain the 
necessary understanding of the practical stimuli that have led to 
the mathematics, and the use of the mathematical results in 
understanding the numerical data. These two aspects of the 
subject, the mathematical and the practical, are complementary, 
and both are necessary for a full understanding of our subject. 
The fact that only one aspect is fully discussed here ought not to 
lead to neglect of the other. 
The book is divided into eight chapters, and each chapter into 
six sections. Equations and theorems are numbered in the 
decimal notation: thus equation 3.5.1 refers to equation 1 of 
section 5 of chapter 3. Within §3.5 it would be referred to simply 
as equation (1). Each section begins with a formal list of 
definitions, with statements and proofs of theorems. This is 
followed by discussion of these, examples and other illustrative 
material. In the discussion an attempt has been made to go 
beyond the usual limits of a formal treatise and to place the ideas 
in their proper contexts; and to emphasize ideas that are of wide 
use as distinct from those of only immediate value. At the end 
of each chapter there is a large set of exercises, some of which 
are easy, but many of which are difficult. Most of these have 
been taken from examination papers, and I am grateful for 
permission from the Universities of London, Cambridge,  
Aberdeen, Wales, Manchester and Leicester to use the questions in 
this way. (In order to fit into the Bayesian framework some 
minor alterations of language have had to be made in these 
questions. But otherwise they have been left as originally set.) 
The second part of the book, the last four chapters, 5 to 8, is 
devoted to statistics or inference. The first three chapters of the 
first part are a necessary prerequisite. Much of this part has 
been written in draft twice: once in an orthodox way with the 
use only of frequency probabilities; once in terms of probability 
as a degree of belief. The former treatment seemed to have so 
PREFACE 
xi 
many unsatisfactory features, and to be so difficult to present to 
students because of the mental juggling that is necessary in order 
to understand the concepts, that it was abandoned. This is not 
the place to criticize in detail the defects of the purely fre- 
quentist approach. Some comments have been offered in the 
text (§5.6, for example). Here we merely cite as an example the 
concept of a confidence interval in the usual sense. Technically 
the confidence level is the long-run coverage of the true value by 
the interval. In practice this is rarely understood, and is 
typically regarded as a degree of belief. In the approach adopted 
here it is so regarded, both within the formal mathematics, and 
practically. We use the adjective Bayesian to describe an  
approach which is based on repeated uses of Bayes's theorem. 
In chapter 5 inference problems for the normal distribution 
are discussed. The use of Bayes's theorem to modify prior 
beliefs into posterior beliefs by means of the data is explained, 
and the important idea of vague prior knowledge discussed. 
These ideas are extended in chapter 6 to several normal  
distributions leading as far as elementary analysis of variance. In 
chapter 7 inferences for other distributions besides the normal 
are discussed: in particular goodness-of-fit tests and maximum 
likelihood ideas are introduced. Chapter 8 deals with least 
squares, particularly with tests and estimation for linear  
hypotheses. The intention has been to provide a sound basis  
consisting of the most important inferential concepts. On this basis 
a student should be able to apply these ideas to more specialised 
topics in statistics: for example, analysis of more complicated 
experimental designs and sampling schemes. 
The main difficulty in adopting, in a text-book, a new  
approach to a subject (as the Bayesian is currently new to statistics) 
lies in adapting the new ideas to current practice. For example, 
hypothesis testing looms large in standard statistical practice, 
yet scarcely appears as such in the Bayesian literature. An 
unbiased estimate is hardly needed in connexion with degrees of 
belief. A second difficulty lies in the fact that there is no 
accepted Bayesian school. The approach is too recent for the 
mould to have set. (This has the advantage that the student can 
be free to think for himself.) What I have done in this book is to 
xii PREFACE 
develop a method which uses degrees of belief and Bayes's 
theorem, but which includes most of the important orthodox 
statistical ideas within it. My Bayesian friends contend that I 
have gone too far in this: they are probably right. But, to give 
an example, I have included an account of significance testing 
within the Bayesian framework that agrees excellently, in 
practice, with the orthodox formulation. Most of modern 
statistics is perfectly sound in practice; it is done for the wrong 
reason. Intuition has saved the statistician from error. My con* 
tention is that the Bayesian method justifies what he has been 
doing and develops new methods that the orthodox approach 
lacks. The current shift in emphasis from significance testing to 
interval estimation within orthodox statistics makes sense to a 
Bayesian because the interval provides a better description of 
the posterior distribution. 
In interpreting classical ideas in the Bayesian framework I 
have used the classical terminology. Thus I have used the 
phrase confidence interval for an interval of the posterior  
distribution. The first time it is introduced it is called a Bayesian 
confidence interval, but later the first adjective is dropped. I 
hope this will not cause trouble. I could have used another 
term, such as posterior interval, but the original term is  
apposite and, in almost all applications, the two intervals, Bayesian 
and orthodox, agree, either exactly or to a good approximation. 
It therefore seemed foolish to introduce a second term for 
something which, in practice, is scarcely distinguishable from the 
original. 
There is nothing on decision theory, apart from a brief 
explanation of what it is in §5.6. My task has been merely to 
discuss the way in which data influence beliefs, in the form of the 
posterior distribution, and not to explain how the beliefs can be 
used in decision making. One has to stop somewhere. But it is 
undoubtedly true that the main flowering of the Bayesian 
method over the next few years will be in decision theory. The 
ideas in this book should be useful in this development, and, in 
any case, the same experimental results are typically used in 
many different decision-making situations so that the posterior 
distribution is a common element to them all. 
PREFACE 
• • • 
Xlll 
I am extremely grateful to J. W. Pratt, H. V. Roberts, M. 
Stone, D. J. Bartholomew; and particularly to D. R. Cox and 
A.M. Walker who made valuable comments on an early version 
of the manuscript and to D. A. East who gave substantially 
of his time at various stages and generously helped with the 
proof-reading. Mrs M. V. Bloor and Miss C. A. Davies made 
life easier by their efficient and accurate typing. I am most 
grateful to the University Press for the excellence of their 
printing. 
D.V.L. 
Aberystwyth 
April 1964 
5 
INFERENCES FOR NORMAL 
DISTRIBUTIONS 
In this chapter we begin the discussion of the topic that will 
occupy the rest of the book: the problem of inference, or how 
degrees of belief are altered by data. We start with the situation 
where the random variables that form the data have normal 
distributions. The reader may like to re-read §1.6, excluding the 
part that deals with the justification of the axioms, before 
starting the present chapter. 
5.1. Bayes's theorem and the normal distribution 
A random sample of size n from a distribution is defined as 
a set of n independent random variables each of which has this 
distribution (cf. §§ 1.3, 3.3). If for each real number, 69 belonging 
to a set (say, the set of positive numbers or the set of all real 
numbers), f(x\ 6) is the density of a random variable, then 6 is 
called a parameter of the family of distributions defined by the 
densities {f(x \ 6} (cf. the parameter, p9 of the binomial  
distribution, §2.1). We consider taking a random sample from a 
distribution with density f(x \ 6) where 6 is fixed but unknown 
and the function / is known. Let H denote our state of  
knowledge before the sample is taken. Then 6 will have a distribution 
dependent on H; this will be a distribution of probability in the 
sense of degree of belief, and we denote its density by tt{0\H). 
As far as possible tt will be used for a density of beliefs, p will be 
used for a density in the frequency sense, the sense that has been 
used in applications in chapters 2-4. If the random sample is 
x = (xl9 x29 ..., xn) then the density of it will be, because the 
xt are independent, 
n 
Ilf(Xi\6)=p(x\d9H)9 say. (1) 
(The symbol H should strictly also appear after 6 on the left- 
hand side.) The density of beliefs about 6 will be changed by 
I LSII 
2 INFERENCES FOR NORMAL DISTRIBUTIONS [5.1 
the sample according to Bayes's theorem (theorem 1.4.6 and its 
generalization, equation 3.2.9) into 7r(6\x9 H) given by 
n(d\x9H)oc p(x109 H)7t{6\H) (2) 
according to the density form of the theorem (equation 3.2.9). 
The constant of proportionality omitted from (2) is 
{Jrtx16, H) n(61H)dd^1 = {tt(x|H)J"1 (3) 
say, and does not involve 6. H will often be omitted from these 
and similar equations in agreement with the convention that an 
event which is always part of the conditioning event is omitted 
(§ 1.2). It will accord with the nomenclature of § 1.6 if n{6 \ H) is 
called the prior density of 6; p(x \ 09 H)9 as a function of d9 is 
called the likelihood; and tt{6 | x, H) is called thQ posterior density 
of 6. We first consider the case of a single observation where 
x = x and f(x | 6) is the normal density. 
Theorem 1. Let x be N(69 cr2), where or2 is known, and the prior 
density of 6 be N(/i09 erg). Then the posterior density of 6 is 
N(jil9 erf), where 
/*i- 1/0-*+i/o-r ** -a +cr° • w 
(Effectively this is a result for a random sample of size one 
from N(0, or2).) The likelihood is (omitting H) 
p{x | d) = (2tt<t2)-1 exp [ - (jc - d)2\2c2\ (5) 
and the prior density is 
?T(d) = (2*o©-*exp [ - {6 -fiQfl2c2} (6) 
so that, omitting any multipliers which do not involve 6 and 
may therefore be absorbed into the constant of proportionality, 
the posterior density becomes 
7r(0|x)ccexp(-^/-^^ 
ex QXp{-id2(lI<r2 + llo-D + dix^+fiJo-2)} 
= exp{-i02M + %/crf} 
ocexp{-K0-/O2M}, (?) 
5.1] BAYES'S THEOREM 3 
where, in the first and third stages of the argument, terms not 
involving d have been respectively omitted and introduced. The 
missing constant of proportionality can easily be found from 
the requirement that 7t(0\x) must be a density and therefore 
integrate to one. It is obviously (27rcrf)-i and so the theorem is 
proved. (Notice that it is really not necessary to consider the 
constant at all: it must be such that the integral of tt{6 | x) = 1, 
and a constant times (7) is a normal distribution.) 
Corollary. Let x = (xl9 ) be a random sample of 
size n from N(d, <r2), where or2 is known and the prior density 
of 6 is N(/i09 erg). Then the posterior density of 6 is N(ju>n, a*2), 
where _. 2 . 2 
nx o-2+/iJorl _2 9 2 /o\ 
*» = nl^ + lU' °" = ^-2 + ^2, (8) 
ft 
and x = n_1 S **• 
i=l 
The likelihood is (equation (1)) 
/7(x|^) = (27rcr2)-^2expr- S (^- ^)2/2cr2l 
oc exp [ - \d\n\cr2) + dx(nl<r2)] 
ccexp[-i(x-d)2(nlor2)]9 (9) 
where again terms not involving 6 have been omitted and then 
introduced. Equation (9) is the same as (5) with x for x and 
njcr2 for cr2, apart from a constant. Hence the corollary follows 
since (8) is the same as (4), again with x for x and n\v2 for a2. 
Random sampling 
We have mentioned random samples before (§§1.3, 3.3). 
They usually arise in one of two situations: either samples are 
being taken from a large (or infinite) population or repetitions 
are being made of a measurement of an unknown quantity. In 
the former situation, if the members of the sample are all drawn 
according to the rule that each member of the population has 
the same chance of being in the sample as any other, and the 
presence of one member in the sample does not affect the chance 
of any other member being in the sample, then the random 
variables, xi9 corresponding to each sample member will have 
1-2 
4 INFERENCES FOR NORMAL DISTRIBUTIONS [5.1 
a common distribution and be independent, the two conditions 
for a random sample, t In the second situation the repetitions 
are made under similar circumstances and one measurement 
does not influence any other, again ensuring that the two  
conditions for a random sample are satisfied. The purpose of the 
repetition in the two cases is the same: to increase one's  
knowledge, in the first case of the population and in the second case 
of the unknown quantity—the latter knowledge usually being 
expressed by saying that the random error of the determination 
is reduced. In this section we want to see in more detail than 
previously how the extent of this increase in knowledge can be 
expressed quantitatively in a special case. To do so it is  
necessary to express one's knowledge quantitatively; this can be done 
using probability as a degree of belief (§1.6). Thus our task is to 
investigate, in a special case, the changes in degrees of belief, 
due to random sampling. Of course, methods other than 
random sampling are often used in practice (see, for example, 
Cochran (1953)) but even with other methods the results for 
random sampling can be applied with modifications and  
therefore are basic to any sampling study. Only random sampling 
will be discussed in this book. 
Likelihood and parameters 
The changes in knowledge take place according to Bayes's 
theorem, which, in words, says that the posterior probability is 
proportional to the product of the likelihood and the prior 
probability. Before considering the theorem and its  
consequences let us take the three components of the theorem in 
turn, beginning with the likelihood. The likelihood is equiva- 
lently the probability density of the random variables forming 
the sample and will have the form (1): the product arising from 
the independence and the multiplication law (equation 3.2.10) 
and each term involving the same density because of the  
common distribution. Hence, consideration of the likelihood 
reduces to consideration of the density of a single member of 
t Some writers use the term 'random sample from a population' to mean one 
taken without replacement (§1.3). In which case our results only apply  
approximately, though the approximation will be good if the sample is small relative to 
the population. 
5.1] BAYES'S THEOREM 5 
the sample. This density is purely a frequency idea, empirically 
it could be obtained through a histogram (§2.4), but is typically 
unknown to us. Indeed if it were known then there would be 
little point in the random sampling: for example, if the  
measurements were made without bias then the mean value of the 
distribution would be the quantity being measured, so  
knowledge of the density implies knowledge of the quantity. But 
when we say 'unknown', all that is meant is 'not completely 
known', we almost always know something about it; for 
example that the density increases steadily with the  
measurement up to a maximum and then decreases steadily—it is 
unimodal—or that the density is small outside a limited range— 
it being very unlikely that the random variable is outside this 
range. Such knowledge, all part of the 'unknown', consists of 
degrees of belief about the structure of the density and will be 
expressed through the prior distribution. It would be of great 
help if these beliefs could be expressed as a density of a finite 
number of real variables when the tools developed in the earlier 
chapters could be used. Otherwise it would be necessary to talk 
about densities, representing degrees of belief, of functions, 
namely frequency densities, for which adequate tools are not 
available. It is therefore usual to suppose that the density of x 
may be written in the formf(x\dl9 #2, ..., 6S) depending on a 
number, s, of real values dt called parameters; where the function 
/is known but the parameters are unknown and therefore have 
to be described by means of a prior distribution. Since we know 
how to discuss distributions of s real numbers this can be done; 
for example, by means of their joint density. It is clear that a 
very wide class of densities can be obtained with a fixed  
functional form and varying parameters; such a class is called a 
family and later we shall meet a particularly useful class called 
the exponential family (§5.5). In this section we consider only 
the case of a single parameter, which is restrictive but still 
important. 
Sometimes / is determined by the structure of the problem: 
for example, suppose that for each member of a random 
sample from a population we only observe whether an event A 
has, or has not, happened, and count the number of times it 
6 INFERENCES FOR NORMAL DISTRIBUTIONS [5.1 
happens, x, say. Then x has a binomial distribution (§2.1) and 
the only parameter is 6 = p9 the probability of A on a single 
trial. Hence the density is known, as binomial, apart from the 
value of an unknown parameter: the knowledge of the  
parameter will have to be expressed through a prior distribution. In 
other situations such reasons do not exist and we have to appeal 
to other considerations. In the present section the function/is 
supposed to be the density of a normal distribution with known 
variance, cr2, say, and unknown mean. These are the two  
parameters of the normal distribution (§2.5). The mean has  
previously been denoted by ju, but we shall now use 6 to indicate that 
it is unknown and reserve /i to denote the true, but unknown, 
value of the mean. Notice that this true value stays constant 
throughout the random sampling. The assumption of normality 
might be reasonable in those cases where past, similar experience 
has shown that the normal distribution occurs (§3.6). For 
example, suppose that repeated measurements of a quantity are 
being made with an instrument of a type which has been in use 
for many years. Experience with the type might be such that it 
was known to yield normal distributions and therefore that the 
same might be true of this particular instrument. If, in addition, 
the particular instrument had been extensively used in the past, 
it may have been found to yield results of known, constant 
accuracy (expressed through the variance or standard  
deviation). In these circumstances every set of measurements of a 
single quantity with the instrument could be supposed to have 
a normal distribution of known variance, only the mean 
changing with the quantity being measured: if the instrument 
was free from bias, the mean would be the required value of the 
quantity. Statistically we say that the scientist is estimating the 
mean of a normal distribution.! This situation could easily 
occur in routine measurements carried out in the course of 
inspecting the quality of articles coming off a production line. 
Often the normal distribution with known variance is assumed 
with little or no grounds for the normality assumption, simply 
because it is very easy to handle. That is why it is used here for 
the first example of quantitative inference. 
t Estimation is discussed in §5.2. 
5.1] BAYES'S THEOREM 7 
Prior distribution 
The form of the prior distribution will be discussed in more 
detail in the next section. Here we consider only the meaning of 
a prior density of 0. We saw, in §1.6, what a prior probability 
meant: to say that a hypothesis H has prior probability p means 
that it is considered that a fair bet of H against not-i/ would be 
at odds of p to (1 —p). We also saw that a density is a function 
which, when integrated (or summed), gives a probability (§2.2). 
Hence a prior density means a function which, when integrated, 
gives the odds at which a fair bet should be made. If n{d) is a 
'00 
prior density then n(d)dd is the prior probability that 6 is 
o 
positive, and a fair bet that 6 was positive would be at odds of 
/•oo /»0 
n(d)dd to 7r(6)d6 on. In particular, to suppose, as has 
JO J -co 
been done in the statement of the theorem, that 6 has prior 
density N(jid9 erg) means, among other things, that 
(i) 6 is believed to be almost certainly within the interval 
(X)-30-o, /^0 + 3cr0) and most likely within (jiQ - 2cr0, /i0 + 2cr0) 
(compare the discussion of the normal distribution in § 2.5. We are 
arbitrarily and conventionally interpreting' most likely' to mean 
that the odds against lying outside the interval are 19 to 1). 
(ii) 6 is just as likely to be near /i0 + Act as it is to be near 
fiQ-Xcr, for any A, and in particular is equally likely to be 
greater than ju,0 as less than ju,0. 
(iii) Within any interval (ji0 — Acr0, /i0 + Acr0) the central values 
are most probable and the further 6 is from the mean, the less 
likely are values near 6. 
Posterior distribution and precision 
Often these three reasons are held to be sufficient for assuming 
a normal prior density. But an additional reason is the theorem, 
which shows that, with a normal likelihood, the posterior  
distribution is also normal. The extreme simplicity of the result 
makes it useful in practice, though it should not be used as an 
excuse for assuming a normal prior distribution when that 
assumption conflicts with the actual beliefs. 
8 INFERENCES FOR NORMAL DISTRIBUTIONS [5.1 
The posterior distribution is, like the prior distribution, one 
of probability as a degree of belief and because of the normality 
enables statements like (i)-(iii) above to be made in the light of 
the datum, the single value of x9 but with different values of the 
mean and variance. Let us first consider how these are related 
to the corresponding values of the prior density and the  
likelihood; taking the variance first because it is the simpler. We 
shall call the inverse of the variance, the precision. The  
nomenclature is not standard but is useful and is partly justified by the 
fact that the larger the variance the greater the spread of the 
distribution and the larger the intervals in (i) above and therefore 
the smaller the precision. The second equation in (4) therefore 
reads: 
posterior precision equals the datum precision 
plus the prior precision (10) 
(this, of course, for normal distributions of datum and prior 
knowledge and a sample of size 1). The datum precision is the 
inverse of the random error in the terminology of §3.3. It 
follows therefore that the posterior precision is necessarily 
greater than the prior precision and that it can be increased 
either by an increase in the datum precision (that is by a decrease 
in the variance of the measurement, or the random error) or by 
an increase in the prior precision. These statements are all 
quantitative expressions of rather vaguer ideas that we all 
possess: their great merit is the numerical form that they assume 
in the statistician's language. It is part of the statistician's task 
to measure precision. Notice again that it is the inverse of the 
variance that occurs naturally here, and not the standard  
deviation which is used in statements (i)-(iii) above. This agrees with 
earlier remarks (§§2.4, 3.3) that the variance is easier to work 
with than the more meaningful standard deviation which can 
always be obtained by a final square root operation. 
The first equation in (4) can also be conveniently written in 
words provided the idea of a weighted mean is used. A weighted 
mean of two values a± and a2 with weights w1 and w2 is defined as 
(w1a1 + w2a2)l(w1 + w2). With equal weights, wx = w2, this is the 
ordinary arithmetic mean. As w1 increases relative to w2 the 
5.1] BAYES'S THEOREM 9 
weighted mean moves towards av Only the ratio of weights is 
relevant and the definition obviously extends to any number of 
values. In this terminology 
the posterior mean equals the weighted mean of the datum value 
and the prior mean, weighted with their precisions. (11) 
Information about 6 comes from two sources, the datum and 
the prior knowledge. Equation (11) says how these should be 
combined. The more precise the datum the greater is the weight 
attached to it; the more precise the prior knowledge the greater 
is the weight attached to it. Again this is a quantitative  
expression of common ideas. 
Small prior precision 
With equations (10) and (11), and the knowledge that the 
posterior density is normal, revised statements like (i)-(iii) can 
be made with fix and cr1 replacing fi0 and <r0. The most important 
effect of the datum is that the intervals in these statements will 
necessarily be narrower, since crx < cr0; or, expressed differently, 
the precision will be greater. A most important special case is 
where the prior precision is very low, or <r0 is very large. In the 
limit as cr0 -> oo (10) and (11) reduce to saying that the posterior 
precision and mean are equal to the datum precision and value. 
Furthermore, both posterior and datum distributions are 
normal. Consequently there are two results which are quite 
distinct but which are often confused: 
» 
(a) the datum, x9 is normally distributed about a mean /i 
with variance cr2; 
(b) the parameter, d, is normally distributed about a mean x 
with variance cr2. 
The first is a statement of frequency probability, the second 
a statement of (posterior) beliefs. The first is a distribution of x9 
the second a distribution of d. So they are truly different. But 
it is very easy to slip from the statement that x lies within three 
standard deviations of ju, (from (a)) to the statement that 6 lies 
within three standard deviations of x (from (b)—cf. (i) above). 
Scientists (and statisticians) quite often do this and we see that 
10 INFERENCES FOR NORMAL DISTRIBUTIONS [5.1 
it is quite all right for them to do so provided the prior precision 
is low in comparison with the datum precision and they are 
dealing with normal distributions. 
Precision of random samples 
The corollary establishes similar results for a normal random 
sample of size n instead of for a single value. It can also usefully 
be expressed in words by saying: 
a random sample of size n from a normal distribution is 
equivalent to a single value, equal to the mean of the 
sample, with n times the precision of a single value. (12) 
(An important proviso is that normal distributions are assumed 
throughout.) The result follows since, as explained in the proof 
of the corollary, (8) is the same as (4) with x for x and n/or2 
for a-2. The result is related to theorem 3.3.3 that, under the 
same circumstances, Q)2{x) = <r2jn9 but it goes beyond it because 
it says that the mean, x, is equivalent to the whole of the 
sample. The earlier result merely made a statement about x, 
for example that it was a more precise determination than a 
single observation; the present result says that, with normal 
distributions it is the most precise determination. This  
equivalence between x and the sample may perhaps be most clearly 
expressed by considering two scientists both with a random 
sample of n measurements. Scientist 1 uses the procedure of the 
corollary. Scientist 2 is careless and only retains the number 
and mean of his measurements: he then has a single value x, 
with mean 6 and variance cr2\n (§3.3), and a normal distribution 
(§3.5), and can use the theorem. The two scientists end up with 
the same posterior distribution, provided they had the same 
prior distribution, so that scientist 2's discarding of the results, 
except for their number and their mean, has lost him nothing 
under the assumptions stated. One of a statistician's main tasks 
used to be called the reduction of data, replacing a lot of numbers 
by a few without losing information, and we see now how this 
can be done in the special case of a normal distribution of known 
variance: n values can be replaced by two, n and x. But  
remember that this does assume normality, a very important proviso. 
5.1] BAYES'S THEOREM 11 
Notice that the proof of the corollary does not use any of the 
distributional theory of §§3.3 and 3.5. It follows a direct and 
simple calculation in which everything about the sample, except 
x and n9 is absorbed into the irrelevant constant of  
proportionality. 
Beliefs about the sample 
The constant is not always irrelevant. The general expression 
is given in equation (3). n(x\H)9 which will now be written 
7t(x)9 can be thought of as the distribution of x obtained as a 
marginal distribution from the joint distribution of x and 6; 
the joint distribution being defined by means of the conditional 
distribution of x for fixed 6 and the prior distribution of 6. 
n(x) can be obtained by evaluating (3), but is most easily 
obtained by using the results on the bivariate normal  
distribution in §3.2. In the notation of that section: if y9 for fixed x9 is 
N(oc + fix, cr2) and x is N(jil9 erf) then y is N(ji29 cr|) with 
/i2 = <x + fi/il9 erf = <r2 + j32<rl. Here, in the notation of this 
section, x9 for fixed d9 is jV(#, cr2) and 6 is N(jlc09 erg); consequently 
x is N(ji09 cr2 + erg). The meaning to be attached to this  
distribution requires some care. Suppose that, before making the 
observation x9 and therefore when one's knowledge was 
described by H9 one had been asked one's beliefs about what 
value the observation would have. There are two sources of 
doubt present about x; first, the doubt that arises from x having 
a distribution even if 0 is known, and secondly the doubt about 
the value of 6 itself. The former is described by p(x \ 6) and the 
latter by 7t(6). They may be compounded in the usual way, as 
above, and yield n(x) = jp(x\ 6)7r(6)d6. To illustrate the  
meaning of n(x) we may say that if x0 is the median of n(x) then, 
before experimentation, a bet at evens would be offered that 
x would be below x0. It is a degree of belief in the outcome of 
the experiment. Notice that p(x 16) can also be thought of as 
a degree of belief in the outcome of the experiment, but when 
the parameter is known to have the value 69 as distinct from 
n(x) which expresses the belief given H. That p(x\6) is both 
a frequency and belief probability causes no confusion since, as 
explained in § 1.6, when both types of statement are possible the 
12 INFERENCES FOR NORMAL DISTRIBUTIONS [5.1 
two probabilities concerned are equal. For example, if the 
parameter were known to have the value 6 and x1 were the 
median of p(x\6)9 then a bet at evens would be offered that 
x would be below x1 because, on a frequency basis, x would be 
below x1 one half of the time. 
Example 
In the preparation of an insulating material, measurements 
are made of the conductivity using an instrument of known 
standard deviation which we can suppose, by suitable choice of 
units, to be one. Prior knowledge of the production process 
suggests that most likely the conductivity will lie between 15 
and 17 (cf. (i) above) and therefore it seems reasonable to  
suppose a prior distribution of conductivity that, in these units t 
is jV(16, J); that is, jlc0 = 16, <r0 = \. Ten readings are made 
giving values 16-11, 17-37, 16-35, 15-16, 18-82, 18-12, 15-82, 
16-34,16-64,15-01, with a mean of 16-57. Hence, in the notation 
of the corollary, w = 10, <r=l,je = 16-57 and from (8) 
10x16-57 + 4x16 ir ., 
^ = 10^4 = 16'41 
and crf02 = 10 + 4 = 14, cr10 = 1/^14 = 0-27. 
Hence the posterior distribution is JV(16-41, (0-27)2). On the 
basis of this it can be said that the mean conductivity of the 
material most likely lies between 15-87 and 16-95, the most 
probable value being 16-41. We shall see in the next  
section the formal language that the statistician uses. Notice 
that the prior mean is 16, the sample mean is 16-57, and the 
posterior mean at 16-41 occupies a position between these two 
but nearer the latter than the former because the sample mean 
has precision (it/or2) of 10 and the prior precision (o-q2) is only 4. 
The posterior precision, at 14, is of course the sum of the two. 
If the prior knowledge is very imprecise we could allow <r0 to 
tend to infinity and attach no weight to it. The posterior mean is 
then the sample mean, 16-57, but its precisionhas decreased to 10. 
It is instructive to consider what would have been the result 
t Notice that in the notation N(ji, <r2), the second argument is the variance 
(here 1/4) and not the standard deviation (1/2). 
5.1] BAYES'S THEOREM 13 
had the prior distribution been N(10, £), with a much lower 
mean. The corollary can still be used but the sample and the 
prior knowledge are incompatible: before sampling the mean 
was almost certainly less than 11-5 (X) + 3o"0) yet the sample 
values are all above 15. It is therefore absurd to take a weighted 
mean. The experimenter is in the position of obtaining readings 
around 16 when he had expected readings in the interval (6-64, 
13-36) (that is /i0 ± 3{cr2 + erg}-*, from tt(x)). Clearly he would 
say that somewhere a mistake has been made either in the prior 
assessment, or in obtaining the data, or even in the arithmetic. 
All probabilities are conditional and these are conditional on 
a mistake not having been made; this is part of H. One should 
bear such points in mind in making any statistical analysis and 
not proceed only by the text-book rules. 
Robustness 
A general remark, that will apply to all the methods to be 
developed in the remainder of this book, is that any inference, 
any statement of beliefs, is conditional not only on the data but 
also on the assumptions made about the likelihood. Thus here, 
the posterior normal distribution depends on the normality 
assumptions made about the data. It might, or might not, be 
affected if the data had not a normal distribution. We say 
that an inference is robust if it is not seriously affected by small 
changes in the assumptions on which it is based. The question 
of robustness will not be investigated in this book but it is 
believed that most, if not all, of the inference methods given are 
reasonably robust. 
5.2. Vague prior knowledge and interval estimates for the 
normal mean 
Theorem L A random sample, x = {xl9 x29 ..., xn), of size n is 
taken from N{d, <r2), where or2 is known. Suppose there exist 
positive constants; a, e, M and c {small values of a and e are of 
interest^), such that in the interval Ia defined by 
x-X^crj^n < 6 < x + Aacr/A/«, (1) 
t Strictly the constants depend on x but the dependence will not be indicated 
in the notation. 
14 INFERENCES FOR NORMAL DISTRIBUTIONS [5.2 
where 20( —Aa) = a, the prior density of d lies between c{\ — e) 
and c{\ + e); and outside Ia it is bounded by Mc. Then the posterior 
density n(61 x) satisfies the inequalities 
(1-e) / n \* / n{x-df\ ... 
(l + e)(l-a) + Mal2^j eXp|—^~) < ^x) 
^ (1 + e) / n \* / «(*-#)2) ™ 
* (l-e)(l-«) (WJ 6XP (- "255- (2) 
z/uftfe A, awrf 
a> 
outside Ia. 
The likelihood of the sample is given by equation 5.1.9 
which, on inserting a convenient constant, is 
Hence by Bayes's theorem (equation 5.1.2), within Ia 
^ 
and outside 7a 
0 < n(6\x) < AMc(^fr«p{-*|^}, (5) 
where A is a constant equal to 7r(x)-1, equation 5.1.3. The 
right-hand inequality in (4) gives 
J^lx)* < Ac(l+e)ji^c^-n^}d6 
rAa 1 
= Ac(l+e) .,. .e-&2dt, where t = Jn(x-d)Ior9 
= ^c(l+6) [0(Aa)-0(-Aa)] = Ac(l+e)(l-*) 
since 0(-Aa) = 1 — ®(Aa). Similarly, the same integral exceeds 
Ac(l — e) (1 -a) and, if Ja is the outside of 7a, 
0 < 
7T(6\x)d6 < AM col. 
Joe 
5.2] VAGUE PRIOR KNOWLEDGE 15 
Combining these results we have, since n(d\x)dd = 1, 
J la.-]-Jo. 
^c(l-e)(l-a) < 1 < Ac[(l+e)(l-a) + Mot], 
and hence 
1 < Ac ^ „ * ,. (6) 
I. 
(l+e)(l-a) + Ma" (l-e)(l-a)" 
Inserting (6) in (4) immediately gives (2); a similar insertion in 
(5) gives (3) on remarking that the maximum value of the 
exponential in Ja occurs at the end-points 0 = x ± Aa cr^n where 
it has the value e~^«. 
If e and a are small, so that e~ix« is also small, the results say 
that the posterior distribution of 6 is approximately N(x, (x2jn). 
Definition. If 7t(6\x) is any posterior distribution of 6 after 
observing x and I^(x) is any interval of 6, depending on x and yff, 
0 < p < 1, such that 
n(0\x)dB = fi9 (7) 
then I^x) is called a 100/? % (Bayesian) confidence interval for 6 
(given x). The words in brackets are usually omitted. I^(x) is 
often called a (Bayesian) interval estimate of 0. fi is called the 
(Bayesian) confidence coefficient, or (Bayesian) confidence level. 
The definition is not restricted to the case of any particular 
prior distribution. 
Discussion of the theorem 
The importance of the theorem lies in the fact that it enables 
a good approximation to be made to the posterior distribution 
when sampling from a normal distribution of known variance, 
without being too precise about the prior distribution. The idea 
behind the theorem and its proof can be applied to distributions 
other than the normal, and is an important statistical tool. 
With the normal sample the likelihood function is given, apart 
from a constant of proportionality, by equation 5.1.9. If a  
constant (nllnor2)^ is inserted it is proportional to 
/ n \* / n{x-6f\ 
\2^J CXPr 255T • (8) 
16 INFERENCES FOR NORMAL DISTRIBUTIONS [5.2 
which, as a function of 0, is a normal density function of mean 
x and standard deviation cr\^n. We know from the properties 
of the normal density that (8) decreases rapidly as 6 departs 
from x and so does the indefinite integral of (8) (the normal 
distribution function) iff 6 < x. If 6 had the density given 
by (8) we could say that 6 almost certainly lay within three 
standard deviations (3<rl*Jri) of the mean x (cf. equation 2.5.13 
and 5.1(i)): generally we could say that the probability that 6 
lay within Aa standard deviations of x (that is, within IJ is 
1 — 20( — Aa) = 1 — a, say. But, in fact, 6 has not the posterior 
density given by (8), its density is obtained by multiplying (8) by 
the prior density and dividing by an appropriate constant. 
Nevertheless, in 7a, which is the only part of the range of 0 where 
the likelihood is not very small, the prior density from the  
theorem is itself almost constant. Consequently the true posterior 
density of 6 is, in 7a, almost a constant times (8): this is  
equation (4). Now what happens outside 7a, in Ja ? There the  
likelihood contribution, (8), is very small. Hence unless the prior 
density is very large in Ja their product, the posterior density, 
must be very small, apart again from this multiplying constant. 
So, with the boundedness condition on n(6) in /a, we obtain (5). 
It only remains to determine the multiplying constant to make 
the integral one. This is done by evaluating separately the  
integrals over Ia and /a. The result is (6). If e, a and Ma are small 
Ac is almost equal to 1 and the limits in (2) differ but little from 
the density (8), that is N(x9 cr2jri). The upper bound given in (3) 
is also small provided a is small, because then e~^« will be small. 
Hence the posterior distribution is approximately N(x, or2 /«). 
Example 
Consider a numerical example. Suppose Aa = 1 -96, or about 
2, so that a = 0-05. The interval Ia then extends two standard 
deviations (Icrj^ri) either side of x. Consider the values of 6 
within this interval: it may be judged that prior to taking 
the sample no one value of 6 in Ia was more probable than 
any other so that 7t{6) is constant within Ia and we can put 
e = 0. Consider the values of 6 outside Ia: it may be judged 
t If 0 > 3c, the indefinite integral rapidly approaches one. 
5.2] VAGUE PRIOR KNOWLEDGE 17 
that prior to taking the sample no value of 6 there is more than 
twice as probable as any value of 6 in Ia; that is M - 2. Then 
(2) says that the true density certainly lies between multiples 
(l-ot + Mot)-1 and (1-a)-1 of the normal density within 7a; 
that is within multiples (1-05)-1 and (0-95)-1, or within about 
5 % of the normal density. Thus the posterior probability that 
0 lies within Ia is within 5 % of 0-95, the value for the normal 
density: this posterior probability is at least 0-90. If Aa is 
increased to 3-29 so that a = 0-001 then, again taking M = 2, 
the true density lies within 0-1 % of the normal density, and the 
posterior probability that 6 lies within two standard deviations 
of x differs by only 0-1 % from 0-95. These statements, with a 
quite accurate probability interpretation, can be made without 
too precise assumptions about the prior density. 
Interpretation of the prior distribution 
The restrictions on the prior density correspond to a certain 
amount of vagueness about the value of 6 before sampling. 
Within the effective range of the likelihood no value of 6 is 
thought to be substantially more probable than any other and 
values outside this range are not much more probable. This is 
certainly not the attitude of someone who has strong prior ideas 
about the value of 6, as in the example of § 5.1 where the prior  
distribution was N(16, i) and cr/^n was l/>/10. In the modification 
of this example in which cr0 was allowed to tend to infinity, the 
prior distribution does satisfy, for large cr0, the conditions of 
the theorem, and the posterior distribution is N(x, cr2/«). The 
scientists' practice of passing from statement (a) to (b) in §5.1 
is allowable provided the prior distribution has the degree of 
vagueness prescribed by the theorem. 
Large samples 
The theorem also has a useful interpretation as a limiting 
result. It will surely be true for a wide class of prior  
distributions that the conditions of the theorem will be satisfied for 
sufficiently large n. As n increases, the width of the interval 7a, 
namely 2Aacr/A/«, tends to zero, and therefore the condition that 
7t(6) be almost constant in Ia becomes less of a restriction. This 
is a particular case of a general result that as the sample size 
2 
LSII 
18 INFERENCES FOR NORMAL DISTRIBUTIONS [5.2 
increases the sample values, the data, influence the posterior 
distribution more than the prior distribution. We have already 
had one example of this in equation 5.1.10: the datum  
precision is njor29 increasing with n9 whilst the prior precision 
remains constant at <Tq2. Indeed we know from the laws of 
large numbers (§3.6) that x converges (weakly and strongly) 
to ji, the true value of d, as n -> oo so that prior information 
necessarily becomes irrelevant: at least with one proviso. If 
7t{6) = 0 for any 6 then no amount of experimentation will 
make 7t(6\x) other than 0 (a direct consequence of Bayes's 
theorem). Hence tt{6) = 0 represents an extreme pig-headed 
view that will not be influenced by any evidence. The proviso is 
therefore that tt{6) =(= 0, for any possible parameter value. (If 
6 is, for example, a binomial parameter then only values 
0 < 6 < 1 are possible.) Notice that, as n -> oo, the posterior 
distribution tends to be concentrated around one particular 
value, x, and the variance about this value tends to zero. 
A distribution which has this form means that one is almost 
certain that 6 is very near to x and, in the limit n -> oo, that one 
knows the value of 6. This acquisition of complete knowledge 
in the limit as n -> oo is a general feature of Bayesian inferences. 
Uniform prior distribution 
In subsequent sections we shall often find it convenient to use 
a particular prior distribution: namely, one with constant 
density for all 69 the uniform distribution (cf. §3.5). The reason 
for this is that it is a reasonable approximation to a distribution 
satisfying the conditions of the theorem, and is particularly easy 
to handle. It should not be treated too literally as a distribution 
which says that any value of 6 is as likely as any other, but rather 
as an approximation to one which satisfies the conditions of the 
theorem; namely, that over the effective range of the likelihood 
any value of 6 is about as likely as any other, and outside the 
range no value has much higher probability. If 6 has infinite 
range (as with a normal mean) then the uniform distribution 
cannot be defined in the usual way; there is no n(d) = c such 
that foo 
cdd = 1. 
J —00 
5.2] VAGUE PRIOR KNOWLEDGE 19 
Instead it must be defined as a conditional density: if F is any 
set of 0 of finite length, then the distribution, conditional on 6 
belonging to F9 has density n(d \ F) = m(F)~19 where m(F) is the 
length of F, so that 
7T(6\F)dd = miF)-1 
F 
d0=\. 
F 
In this way we can talk about the uniform distribution on the 
real line. As an illustration of the simplicity obtained using the 
uniform distribution consider the case of normal sampling (§5.1) 
with7r(#) constant. The likelihood is given by equation 5.1.9 and 
the posterior density must be the same, apart from the constant of 
proportionality, which can include the (constant) prior density. 
The constant of proportionality is obviously (njlnor2^ and the 
posterior distribution is N(x, cr2lri). 
Sample information 
The uniform distribution, representing vagueness, can often 
be used even when one's prior distribution is quite precise, for 
a different reason. Much effort has been devoted by scientists 
and statisticians to the task of deriving statements that can be 
made on the basis of the sample alone without prior knowledge 
(see §5.6 for more details). For example, they have tried to 
answer the question, what does a sample have to say about 6 ? 
Our approach does not attempt to do this: all we claim to do 
is to show how a sample can change beliefs about 6. What 
change it effects will depend not only on the sample but also on 
the prior beliefs. Modern work suggests that the question just 
posed has no answer. The approach adopted in this book would 
suggest that what the questioner means is, what does a sample 
say about 6 when the prior knowledge of 6 is slight; or when 
the scientist is not influenced by strong prior opinions about 6. 
What, in other words, does the sample have to say about 6 
when the sample provides the bulk of the information about 6 ? 
This can be answered using the theorem and the uniform prior 
distribution, so that even when one has some appreciable prior 
knowledge of d one may like to express the posterior beliefs 
about 6 without reference to them. This has the additional 
2-2 
20 INFERENCES FOR NORMAL DISTRIBUTIONS [5.2 
advantage of making the results meaningful to a wider range of 
people, namely those with vague prior beliefs, and the results 
have a superficial appearance of more objectivity than if a 
subjective prior distribution had been used. 
Problems with substantial prior information 
Although the uniform prior distribution will be used in most 
of the book, for the reason that most interest centres on the 
situation where we wish to express the contribution of the 
sample to our knowledge, there are problems wherein the  
contributions from prior knowledge and the likelihood are  
comparable. They may sometimes be treated by the methods used 
for small prior knowledge, in the following way. If, in respect 
of observations x now being considered, the prior knowledge 
has been obtained from past observations y, known to the 
experimenter, the relevant statement of prior knowledge when 
discussing x will be the posterior distribution with respect to the 
past observations y. Three distributions of degrees of belief are 
therefore available: (1) before the observations y; (2) after y, 
but before x; (3) after x and y. Although the original problem 
may have involved the passage from (2) to (3), and hence a 
statement of appreciable prior knowledge due to y, it is possible 
to consider instead the passage from (1) to (3), incorporating 
both the observations x and y, and for this the prior knowledge 
is weak and may therefore be treated by the methods of this 
book. An example is given in the alternative proof of theorem 
6.6.1. The method is always applicable if x and y come from the 
same exponential family (§5.5) and the distributions (1) to (3) 
are all members of the conjugate family. 
There remain problems in which there is an appreciable 
amount of prior knowledge but it is not possible to be precise 
about the observations from which it has been obtained. These 
cannot be treated directly by the methods of this book, though 
minor modifications to the methods are usually available. To 
see the form of this modification we must await the development 
of further results. For the moment we merely recall the fact that, 
accepting the arguments summarized in § 1.6, any state of  
knowledge or uncertainty can be expressed numerically and therefore 
5.2] VAGUE PRIOR KNOWLEDGE 21 
in the form of a probability distribution. The form of this  
distribution can usually be ascertained by asking and answering 
questions like, 'Do you think the parameter lies below such and 
such a value?', in the way discussed in §5.1. If the likelihood 
belongs to the exponential family and the prior distribution so 
obtained can be adequately t described by a member of the 
family conjugate to this family (§5.5) then again the methods 
appropriate to vague knowledge may be used. For we may 
suppose that the prior distribution of the conjugate family has 
been obtained by, possibly imaginary, observations y from the 
same exponential family starting from weak prior knowledge. 
An example is given in example 2 of §6.6 and another is given 
in §7.2. 
Non-normal distributions 
It is possible to generalize Theorem 1 to distributions other 
than the normal. The basic idea of the theorem is that if the 
prior density is sensibly constant over that range of 6 for which 
the likelihood function is appreciable and not too large over that 
range of 6 for which the likelihood function is small, then the 
posterior density is approximately equal to the likelihood  
function (apart from a possible constant). The result thus extends to 
distributions other than the normal. In extensions, the uniform 
distribution will often be used to simplify the analysis. The 
principle of using it has been called by Savage, the principle of 
precise measurement. (Cf. the discussion as n-+co above.) 
The theorem is often held to be important for another reason 
but the argument is not satisfactory. In §3.6 we discussed the 
Central Limit Theorem 3.6.1 and saw that, provided @\x?) 
exists, x will have, as n increases, an approximately normal 
distribution, N(d, cr2jn)\ or, more exactly, n%(x-d)l<r will 
have a distribution which tends to N(09 1). Consequently if 
x = (xl9 x2, ..., xn) is a random sample from any distribution 
with finite variance, the density J of x is approximately given by 
t More research needs to be carried out on what is meant by 'adequately' 
here. It is usually difficult to describe how the final inference is affected by 
changes in the prior distribution. 
t The central limit theorem concerns the distribution function, not the density 
function: but a density is a function which, when integrated, gives the distribu- 
22 INFERENCES FOR NORMAL DISTRIBUTIONS [5.2 
(5.1.9) again and hence the posterior density, for any sufficiently 
large random sample, is N(x9 cr2ln). But the unsatisfactory 
feature of this reasoning is that the equivalent, for the non- 
normal sample, of the three lines before (5.1.9) has been 
omitted. In other words (5.1.9) is not the likelihood of x, but 
of x, and hence what is true is that for a sufficiently large 
random sample from any distribution of finite variance tt{6 \ x) 
(but not necessarily 7r(#|x)) is approximately N(x9or2lri). It 
might happen that inferences based on x are substantially 
different from inferences based on the whole of the sample 
available. We saw in the last section that with normal samples 
x and n are equivalent to x, but this is not true for samples from 
other distributions. Indeed, in §3.6, it was shown that if the 
sample is from a Cauchy distribution then x has the same  
distribution as xl9 and is therefore only as good as a single  
observation. Obviously x contains much more information than just 
xl9 say; though the above reasoning would not be used since the 
Cauchy distribution does not satisfy the conditions for the 
Central Limit Theorem. The situation with the Cauchy  
distribution is extreme. It is probably true that in many situations 
n(d | x) will not differ substantially from tt{6 | x) and it may not 
be worth the extra labour of finding the latter. 
Confidence intervals 
The definition of confidence interval given above is not made 
for mathematical use but for convenience in practical  
applications. Undoubtedly the proper way to describe an inference is 
by the relevant distribution of degrees of belief, usually the 
posterior distribution. But, partly for historical reasons, partly 
because people find the idea of a distribution a little elaborate 
and hard to understand (compare the use of features of  
distributions in §2.4), inferences have not, in practice, been described 
this way. What is usually required is an answer to a question such 
as 'in what interval does 6 most likely lie?'. To answer this the 
concept of a confidence interval has been introduced. A value 
tion function and in this sense the integration of the normal density gives the 
correct approximation. The central limit theorem does not say that the density 
of x tends to the normal density, though usually this is true and conditions for 
it are known. 
5.2] VAGUE PRIOR KNOWLEDGE 23 
of fl commonly used is 0-95 and then an interval is quoted such 
that the posterior probability of 6 lying in the interval is 0-95. 
For example, in the situation of the theorem 
(x- l'96<rljn9 x+ h96crl^n) 
is an approximate 95 % confidence interval for 6: one would be 
prepared to bet 19 to 1 against 6 lying outside the interval. 
A higher value of /3 gives a wider interval and a statement of 
higher probability: fi = 0-99 is often used, giving (x — 2-58o-l<Jn9 
x + 2-5$orl*Jri) in the normal case. Values of /? near one 
are those most commonly used but there is sometimes an 
advantage in using /3 = 0-50 so that 6 is as likely to lie inside the 
interval as outside it: indeed this used to be common practice. 
In the normal case the result is {x — Q'61crj^n, x + 0-67o-l*Jri) and 
0-67 cr J'^Jn is called the probable error of x. Modern practice uses 
orj*Jn and calls it the standard error. Thus the sample mean, plus 
or minus two standard errors, gives a 95 % confidence interval 
for the normal mean. 
An important defect of a confidence interval is that it does 
not say whether any values of 6 within the interval are more 
probable than any others. In the normal case, for example, 
values at the centre, 6 = x, are about seven times more probable 
than values at the ends, 6 = x ± l-96cr/A/«, in the case of a 95 % 
interval (^(0) = 0-3989, 0(2) = 0-0540 and their ratio is 7-39: 
for the notation see §2.5). The difficulty is often avoided in the 
case of the normal mean by quoting the interval in the form 
x ± l-96orl^n; thus, in the numerical example of the last section 
with cr0->oo, the mean conductivity would be described as 
16-57 ±0-63. This indicates that the most probable value is 
16-57, but that values up to a distance 0-63 away are not  
improbable. Sometimes the most probable value alone is quoted 
but this is bad practice as no idea of the precision (in the sense 
of the last section) is provided. Such a single value is an 
example of a point estimate (as distinct from an interval  
estimate). Point estimates have their place, either in conjunction 
with a standard error, or in decision theory, but will not be 
discussed in detail in this book. Their place in most problems 
will be taken by sufficient statistics (§5.5). 
24 INFERENCES FOR NORMAL DISTRIBUTIONS [5.2 
Estimation 
Something, however, must be said about estimates and 
estimation because the terms are so commonly used in statistical 
writing. Current statistical thinking divides problems of  
inference rather loosely into problems of estimation and problems 
of tests of hypotheses. The latter will be discussed in §5.6. It is 
difficult to draw a hard and fast distinction between the two 
types of problem, but a broad dividing line is obtained by 
saying that hypothesis testing is concerned with inferences about 
a fixed value, or set of values, of 6 (for example, is 6 = 7 a 
reasonable value, or is it reasonable that 6 < 6 < 8) whereas 
estimation problems have no such fixed value in mind and, for 
example, may conclude with a statement that 6 lies between 
6 and 8 (as with a confidence interval); the values 6 and 8 
having no prior significance. The distinction may be illustrated 
by inferences appropriate to the two situations: 
(a) Is the resistance of a new alloy less than that of aluminium ? 
(b) What is the resistance of this new alloy ? 
The former demands a statement relative to the resistance of 
aluminium: the latter requires no such consideration. We shall 
define significance tests of hypotheses in §5.6. We shall  
occasionally use the term,' an estimate of a parameter', when we refer 
to a value which seems a fairly likely (often the most likely if it 
is the maximum likelihood estimate, §7.1) value for the  
parameter. As just mentioned such an estimate should have  
associated with it some idea of its precision. A rigorous definition of 
a least-squares estimate is given in §8.3. 
Choice of a confidence interval 
Confidence intervals are not unique: there are several 
intervals containing an assigned amount of the posterior 
probability. Thus, in the case of the normal mean the infinite 
interval 6 > x—\'6Acrj^n is a 95 % confidence interval since 
®(-1-64) = 0-05. In this book a confidence interval will 
usually be chosen such that the density is larger at any point in 
the interval than it is at any point outside the interval; points 
inside are more probable than points outside. This rules out the 
5.2] VAGUE PRIOR KNOWLEDGE 25 
infinite interval just quoted since x—l-lOcrl^n has higher 
density, for example, than x+l-SOo-j+Jn. The reason for the 
choice is that the interval should contain the more probable 
values and exclude the improbable ones. It is easy to see that 
this rule gives a unique interval (apart from arbitrariness if there 
are values of equal probability). It can also be shown that the 
length of the interval so chosen is typically as small as possible 
amongst all confidence intervals of prescribed confidence  
coefficient. This is intuitively obvious: thinking of a probability 
density in terms of the mass density of a rod (§2.2) the part of 
the rod having given mass (confidence coefficient) in the least 
length is obtained by using the heavier parts (maximum density). 
A rigorous proof can be provided by the reader. Notice that 
the rule of including the more probable values is not invariant 
if one changes from 6 to some function of d, <f>(6), because in so 
doing the density changes by a factor \d(j)jdd\ (theorem 3.5.1) so 
that the relative values of the densities at different values of 6 
(and hence <p) change and a high density in terms of 6 may have 
a low one in terms of $. Usually, however, there is some reason 
for using 6 instead of 0. For example, here it would be  
unnatural to use anything other than 6, the mean. 
Several parameters 
The idea of a confidence interval can be generalized to a 
confidence set. If S/x) is any set of values of 6 (not necessarily 
an interval) with r 
7T(6\x)d6 = /?, (9) 
then S^x) is a confidence set, with confidence coefficient ft. 
The definition of confidence sets enables one to make  
confidence statements about several parameters, though this is rarely 
done. It is only necessary to consider the joint posterior  
distribution of several parameters and to replace (9) by a multiple 
integral. 
The definition of confidence interval given here is not the usual 
one and hence the qualification, Bayesian. The usual one will be 
given later (§5.6) together with our reasons for altering the 
definition. In most problems the intervals produced according 
26 INFERENCES FOR NORMAL DISTRIBUTIONS [5.2 
to our definition will be identical with those produced by the 
usual definition, and from a statistician's practice one would 
not be able to tell which definition was being used. 
5.3. Interval estimates for the normal variance 
In this section, as in the last, the data are a random sample 
from a normal distribution but instead of the mean being 
unknown it is the variance whose prior and posterior  
distributions interest us, the mean having a known value. If a random 
variable x, in this context often denoted by %2, has a density 
exp {- i*} xm-x\2m(rn -1)! 
= exp{-M(%2)^1/2K^-l)! 
(1) 
for x > 0, and zero for x < 0, it is said to have a ^-distribution 
with v degrees of freedom, where v = 2m > 0. 
Theorem L Let x = (xl9 x2, ... xn) be a random sample of size n 
from N(/i, 6), where /i is known, and the prior density ofv^crlJO be 
X2 with v0 degrees of freedom; then the posterior density of 
(Vq&I + S2)/d is x2 with vQ + n = vl9 say, degrees of freedom, where 
n 
S2 = 2(x,-/02. 
i=l 
If the random variable x = v0 <r\\d has prior density given by 
(1) it follows from theorem 3.5.1 that 6 = v0<rllx has prior 
density , ...v , 
-) 1?/2*HK-1)! 
exp - 
26 
6 
since 
oc exp 
dx = -v0<rlddld*. 
vn<r, 
26 } 
(2) 
The likelihood of the sample is 
n 
p(x\6) - (27r0)-i»exp {- ^(Xi-/iyi26 
i=l 
oc e-s2/200-l«. (3) 
Hence using Bayes's theorem with (2) and (3) as the values 
of prior density and likelihood, the posterior density is  
proportional to e^*HS«w»0-K'**»>-i. (4) 
5.3] NORMAL VARIANCE 27 
It only remains to note that (2) and (4) are the same expressions 
with v0or% and vQ of the former replaced by vQcrl + S2 and 
v0 + n = v1 in the latter. Since (2) is obtained from %2, so is (4) 
and the theorem follows. 
We record, for future reference, the following result. 
Theorem 2. 
00e-A\od-mdQ = (m-2)\\Am^ (A > 0,m > 1). 
o 
The substitution x = A/0, with dx = -Add/62 gives (§2.3) 
e-xxm-2dxlAm-1 = (m-iy.lA™-1. 
/*oo 
Jo 
Example 
The situation envisaged in this section where a random 
sample is available from a normal distribution of known mean 
but unknown variance rarely occurs in practice: but the main 
result (theorem 1) is similar in form to, but simpler than, 
a more important practical result (theorem 5.4.2) and it may 
help the understanding to take the simpler result first. It can 
occur when a new piece of testing apparatus is being used for 
the first time: for example, suppose someone comes along with 
a new instrument for measuring the conductivity, in the example 
of §5.1, which he claims has smaller standard deviation than the 
instrument currently in use (of standard deviation unity). 
A natural thing to do is to measure several times, n say, the 
conductivity of a piece of material of known conductivity, /i. 
If the instrument is known to be free from bias and is assumed 
to yield a normal distribution, each xt is N(ji, 6) with unknown 
precision 0_1: the problem is to make inferences about 6 (or 
equivalently #_1). The snag here is the phrase 'known to be free 
from bias'. This is rather an unusual situation; normally ju, is 
also unknown, methods for dealing with that problem will be 
discussed in §5.4. 
The ^-distribution 
The ^-distribution was known to Helmert in 1876, but its 
importance in statistics dates from its introduction in 1900 by 
28 INFERENCES FOR NORMAL DISTRIBUTIONS [5.3 
Karl Pearson in a problem to be considered later (§7.4). It is 
not a complete stranger to us, for suppose y has a r(n9 A)-distri- 
bution (§2.3) and let x = 2Xy; then since dx = 2Xdy it follows 
from theorem 3.5.1 that x has the density (1). Hence if y is 
T(n, A), 2Xy is x2 with 2n degrees of freedom: conversely, if x is 
X2 with v degrees of freedom then x\2X is T(^, A). The reasons 
for using the ^-distribution will appear when we examine  
statement (b) below. Essentially they are that it is a convenient 
representation of many states of belief about 6 and that it leads 
to analytically simple results. There is no obligation to use x2: 
it is merely convenient to do so. The reason for the name 
'degrees of freedom' for v will appear later (§6.1). 
For v > 2 the density, (1), of the ^-distribution increases 
from zero at %2 = 0 to a maximum at x2 = v~^ and then 
diminishes, tending to zero as x2 -> °°- F°r v = 2 the density 
diminishes from a finite, non-zero value at %2 = 0 to zero as 
X2 -> oo. For 0 < v < 2 the density tends to infinity as x2 -> 0, 
to zero as %2 -> oo and decreases steadily with x2- The mean of 
the distribution is v and the variance 2v. All these results 
follow from similar properties of the T-distribution (§2.3, and 
equation 2.4.9). For large v the distribution of x2 is  
approximately normal. A proof of this is provided by relating x2 to T 
and using the facts that the sum of independent T-distributions 
with the same parameter has also a T-distribution with index 
equal to the sum of the indices (§3.5), and the Central Limit 
Theorem (3.6.1) (see §3.6). 
Prior distribution 
Let us now consider what it means to say that the prior 
density of v0 o%\d is x2 with v0 degrees of freedom, so that the 
density of 6 (the unknown variance of the normal distribution) 
is given by (2). In this discussion we omit the suffix 0 for 
simplicity. For all v > 0 the latter density increases from 
0 at 6 = 0 to a maximum at 0 = a2vl(v + 2) and then tends to 
zero as 0 -> oo. The density has therefore the same general shape 
for all degrees of freedom as the ^-distribution itself has for 
degrees of freedom in excess of two. To take it as prior density 
for 0 is equivalent to saying that values that are very large 
5.3] NORMAL VARIANCE 29 
or very small are improbable, the most probable value is 
or2vl(v + 2) and, because the decrease from the maximum as 6 
increases is less than the corresponding decrease as d diminishes, 
the values above the maximum are more probable than those 
a similar distance below the maximum. The expectation and 
variance of 6 are most easily found by remarking that since 
vcr2jd is x2 with v degrees of freedom x = v<r2\2d is TQv, 1) with 
density e-xx^v-x\{^y-1)!: hence 
/» 
€{6) = E(v<r*l2x) = fra2 e-*xfr-adxl(bv-l)l 
= <*,/(?-2) (5) 
and S (02) = \ vV* f e~xxi>-s dx\(\v -1)! 
= o*v»l(v-2)(y-4), 
S0 that 2\d) = S{d2)-i\d) = 2<r*v2l(v-2)2 (v-4). (6) 
These results are only valid if v > 4, otherwise the variance is 
undefined (or infinite). If v is large the values are  
approximately cr2 and 2<j*\v. Hence the two numbers at our disposal, 
or2 and v9 enable us to alter the mean and variance of the prior 
distribution: or2 is approximately the mean (and also the most 
probable value) and ^{2jv) is approximately the coefficient of 
variation. Large values of v correspond to rather precise  
knowledge of the value of 6 prior to the experiment. The two quantities, 
cr2 and v, therefore allow considerable variation in the choice of 
prior distribution within this class of densities. 
We note, for future reference (§7.1), that the prior distribution 
of 6, like x2, tends to normality as v -> oo. To prove this consider 
z = (6 — o-2)l(2o*lv)% with approximately zero mean and unit 
standard deviation. From (2), z has a density whose logarithm 
is 
= - Ml + ^(2/v))-1 - {\v +1) In (1 + *V(2/")) 
omitting terms not involving z. Expansion of this in powers of 
v~% gives — \z2 + 0{v~%) which proves the result. 
30 INFERENCES FOR NORMAL DISTRIBUTIONS [5.3 
Simpler results are obtained by considering vcr2jd9 the 
parameter in terms of which the theorem is couched. From 
the mean and variance of x2 quoted above ${yo-2\6) = v and 
2)\v<T2ld) - 2v so that /(d-1) = o-2 and ^(O-1) = 2<r+jv. 
Q-1 is what we called the precision in §5.1. The mean precision 
is therefore cr~2 (hence the notation) and the coefficient of 
variation of the precision is (now exactly) ^(2/v). 
Likelihood 
Next consider the likelihood (equation (3)). The remarkable 
n 
thing about this is that it only depends on S2 = 2 (*« - /02 and n- 
i=i 
In other words, the scientist can discard all his data provided 
only that he retains the sum of squares about the known mean 
and the size of the sample. The situation is comparable to that 
in §5.1 where only the mean x and n were needed to provide all 
the information about the mean: here S2 and n provide all the 
information about the variance. This is a strong reason for 
evaluating S2 and not some other statistic (see §2.4) such as 
n 
S I**—/*I in order to estimate the variance: but notice the 
i=l 
result assumes the distribution from which the sample is taken 
to be normal (compare the discussion of x and the Central Limit 
Theorem in §5.2). 
Posterior distribution 
The theorem says that the posterior distribution is of the 
same form as the prior distribution but with vQcrl replaced by 
VqvI + S2 and v0 by v0 + n = vlm The interpretation of the 
posterior distribution is therefore the same as for the prior 
distribution with these numerical changes. The result is most 
easily understood by introducing the quantity 
n 
s* = S2/n = j:(Xi-/i)*ln. 
i=l 
The random variables (xi—/i)2 are independent and identically 
distributed with mean <^[(x4-/02] = 0"2> the true value of the 
variance of the sample values. Hence since s2 is a mean of n 
such values it tends to or2, as n -> oo, by the strong law of large 
5.3J NORMAL VARIANCE 31 
numbers (theorem 3.6.3). Consequently s2 is an estimate of cr2 
from the sample. Now prior to sampling the most probable 
value of 6 was o*2, y0/(y0 + 2) and its mean was o"o,;o/0;o — 2), 
v0 > 2, so that cr2,, between these two values, to avoid  
complications with odd 2's, is an estimate of cr2 from prior knowledge. 
The posterior value corresponding to cr2, is (v0cro + ns2)l(vQ + ri)9 
which is a weighted mean of prior knowledge (erg) and sample 
knowledge (s2) with weights v0 and n. The weights are  
appropriate because we saw that large values of v0 correspond to 
rather precise knowledge of 6 before the experiment and large 
values of n correspond naturally to a lot of knowledge from the 
sample. Hence the result for the variance is very similar to that 
for the mean (equation 5.1.8); in both cases evidence from the 
sample is combined with the evidence before sampling, using 
a weighted mean. The posterior density of 6 has mean and 
variance ^ (%) = ^ ^ + ^^ + ^ _ ^ (?) 
®\61 x) = 2(^0 cr2 + S2)2l(v0 + n-2)2(v0 + n- 4) (8) 
from (5) and (6). These expressions are valid provided 
v0 + n-4 > 0. The approximate results are that the mean is 
(^0o'2 + ns2)lv1 = erf, say, and the coefficient of variation is 
V(2/^i). The coefficient of variation is thus reduced by sampling, 
from V(2/^o)> corresponding to an increase in our knowledge 
of 6 due to sampling. As n -> oo the variance of 6 tends to zero 
and the mean behaves like S2jn = s29 tending to cr2. So that 
with increasing sample size we eventually gain almost complete 
knowledge about the true value of 6. Similar results are  
available in terms of the precision c?-1. 
Vague prior information 
In the normal mean situation (§5.1) it was explained that 
special attention is paid to the case where the prior information is 
very imprecise: in the notation of that section, cr0 -> oo. Also, in 
§5.2, it was shown that a wide class of prior distributions could 
lead to results equivalent to large cr0 and that a convenient prior 
distribution would be a uniform distribution of the unknown 
parameter, there the mean. In these circumstances the weighted 
32 INFERENCES FOR NORMAL DISTRIBUTIONS [5.3 
mean of ju,0 and x depends less on ju,0 and more on x and in the 
limit as cr0 -> oo, 0 is N(x9 cr^jri). Closely related results apply in 
the normal variance situation. Very imprecise prior information 
clearly corresponds to v0->0; for example, the coefficient of 
variation of 0~\ V(2/^o)> -* °°- Then the weighted mean depends 
less on the prior value cr2, and more on s29 and in the limit as 
v0 -> 0 we have the simple result that S2/d is x2 with n degrees of 
freedom. We shall not give the equivalent result to theorem 
5.2.1 for the variance situation, but there exists a wide class of 
prior distributions, satisfying conditions similar to those in that 
theorem, which leads to this last result as an approximation. 
A convenient prior distribution is obtained by letting v0 -> 0 in 
(2). That expression may be written, with a slight rearrange- 
ment' as ( v cr*\ lv <rM"o 1 1 
exD' ' ' ' 
26 \\2d ) (&v0-l)\0 
and (%Vq-\)\ times this tends, as v0->09 to 0_1. Hence the 
prior distribution suggested has density proportional to 0_1. 
This is not a usual form of density since, like the uniform  
distribution of the mean, it cannot be standardized to integrate to 
one. But like the uniform distribution it can be treated as 
a conditional density (§5.2). With this prior distribution and the 
likelihood given by (3), the posterior distribution is obviously, 
apart from the constant of proportionality, e^Wfl-Jw-i, which 
is (4) with vQ = 0- Hence the usual form of inference made in 
the situation of the theorem, that is with imprecise prior 
knowledge, is 
(b) the parameter 6 is such that S2/0 is distributed in a %2- 
distribution with n degrees of freedom. 
This statement should be compared with (5.1(6)) to which 
corresponds a parallel statement (5.1(a)) with, however, quite 
a different meaning. It is interesting to note that there is a 
similar parallel statement corresponding to (6). To obtain this 
we recall a remark made in §3.5 and used above that the sum 
of two (and therefore of any number) of independent T- 
variables with the same parameter has also a T-distribution 
with that parameter and index equal to the sum of the indices. 
Also, from example 3.5.1 we know that (Xi-/i)2 is T(J, l/2cr2), 
5.3] NORMAL VARIANCE 33 
so that S2 = Ii(Xi-/i)2 is r(w/2, l/2cr2), and hence, by the 
relationship between the T- and ^-distributions, we have 
(a) the datum S2 is such that S2/cr2 is distributed in a x2- 
distribution with n degrees of freedom. Warnings about  
confusing (a) and (b) similar to those mentioned in §5.1 apply here. 
The conditional density #_1 is related to the uniform  
distribution over the whole real line in the following way. Let <f> = ln#. 
Then the density of <p is constant, since d<f>jdd = d~x (theorem 
3.5.1) and as ln# extends from — oo to +00, the logarithm of the 
variance has a uniform distribution over the whole real line. 
The distribution arises in yet another way. The density of any 
Xi is (27Td)-iQxp{ — (Xi—fi)2l26}. Effect transformations of x{ 
and 0 as follows: zt = ln[(x4-/^)2], <p = ln#. The density 
of zt is proportional to ei(Zi~^exp{-^e(Zi~^}9 a function of 
Zi — (j) only. Similarly, in the case of the normal mean the 
density of xt (without any transformation) is proportional to 
exp {- %(Xi - d)2}, a function of x{ - 6 only. If a random variable 
x has a density p(x \ d), depending on a single parameter 6, 
which is a function only of x — 6 then 6 is said to be a location 
parameter for x (cf. §2.4). It means that as 6 changes the 
density remains unaltered in shape and merely moves along the 
x-axis. Our remarks show that the normal mean and the 
normal log-variance (or equally log-standard deviation) are 
both location parameters. It therefore seems in good agreement 
with our choice of a uniform distribution of the mean in §5.2 to 
use a uniform distribution of log-variance here. Both in the 
case of the normal mean and the normal variance, it is because 
the parameter is transformable to one of location that there 
exist related pairs of statements (a) and (6). If 
p(x\6)=J{x-6) 
an operation on 6 is an operation on x — 6 or equivalently x: 
tnus, ^ »! /•w+i 
f{t-6)dd = j{u)du= f(x-w)dx 
t-i 
0 
w 
so that statements made about 6 (type (b)) correspond to 
statements about the datum (type (a)). 
3 
LSII 
34 INFERENCES FOR NORMAL DISTRIBUTIONS [5.3 
Confidence intervals 
It remains to discuss how confidence interval statements may 
be made about 6 on the basis of the posterior distribution. We 
shall suppose the posterior distribution to be such that S2jd is 
X2 with n degrees of freedom (v0 -> 0), the case of general vQ 
follows similarly. To make confidence interval statements we 
need the distribution function of the ^-distribution for integral 
values of v, the degrees of freedom. This is extensively tabulated; 
see, for example, Lindley and Miller (1961). To simplify the 
exposition we introduce some standard terminology. If F(x) is 
a distribution function, the value xa9 supposed unique, such that 
F(xa) — 1 -a is called the upper 100a % point of the  
distribution: the value xa such that F(xa) = a is called the lower 100a % 
point. If x is a random variable with F(x) as its distribution 
function p(x ^ xa) = p(x > xa) = a. The numerical values for 
the normal distribution function given in equations 2.5.13 can 
be rephrased by saying ' the upper 2\ % point of the  
standardized normal distribution is 1-96', etc. Let xl(v) \xl(v)] be the 
upper [lower] 100a % points of the ^-distribution with v degrees 
of freedom. These are tabulated by Lindley and Millerf for 
a = 0-05, 0-025, 0-01 and 0-005. A confidence interval for 0, 
with confidence coefficient yff, can then be found from the result 
"(S2ld > xl(n)\x) = l-a = /?, 
which gives tt(6 < S2lxl(n)\x) = J39 (9) 
on rephrasing the description of the event being considered. 
Here, in a convention which will be adhered to, fi = 1 — a, a is 
typically small and yff near one. This (semi-infinite) interval may 
be enough for some practical purposes for it says, with large ft, 
that one is fairly certain that 6 is less than S2lxl(n). In the 
example of conductivity measurements it may be enough to 
know that the variance does not exceed that limit: or,  
equivalent^, that the precision is greater than ^{n)jS2. However, in 
most cases a finite interval is wanted and the usual practice is to 
obtain it by removing \<x from the upper and lower ends of the 
t The upper ones only are tabulated for 100a =10 and 1/10 %. 
5.3] NORMAL VARIANCE 35 
^-distribution; that is, an equal amount of probability from 
each tail. The result is 
^\M < &I0 < fy(n)\x) = ft, 
or 7T(S*lxiM < 6 < S*lxlM\*) = ft. (10) 
But this is arbitrary and although it provides a valid confidence 
interval there is no justification for preferring it to any other. 
In order to obtain confidence intervals satisfying the rule given 
in §5.2 that values inside the interval should have higher density 
than those outside, it is first necessary to decide which  
parameter is to be used since, as explained in §5.2, the rule is not 
invariant under a change, say from 0 to d~\ Since ln# has been 
assumed uniformly distributed in the prior distribution, so that 
its density is constant and no value of ln# is more likely than 
any other, it seems natural to use ln# in the posterior  
distribution. Tables of x\n) and x2(n) such that 
<S*lJftn) < 6 < S*lx2(n)\x) = fi, (11) 
and values of ln# in the interval have higher density than those 
outside are given in the Appendix. (The dependence on a has 
been temporarily omitted.) 
Example 
Consider the example of §5.1. Suppose the ten readings 
cited there had been made on a material of conductivity 
known to be 16, with an instrument free from systematic error 
but of unknown variance. The sum of squares about ju, = 16, 
n 
S2 = Z(x4-/02, is equal to 16-70, and n = 10. The upper 
i=i 
and lower 5 % points of the ^-distribution with 10 degrees 
of freedom are XoosOO) = 18-31 and xiJ-osOO) = 3-94. Hence 
we can say that, with confidence coefficient 95 %, 
0 < 16-70/3-94 = 4-24; 
or the precision is most likely greater than 0-236; or the standard 
deviation is most likely below 2-06. The upper and lower 2\ % 
points of the same distribution are 20-48 and 3-25 so that (10) 
becomes 
tt(16-70/20-48 < 6 < 16-70/3-25|x) = 0-95 
or the variance most likely lies between 0-815 and 5-14. The 
3-2 
36 INFERENCES FOR NORMAL DISTRIBUTIONS [5.3 
shortest confidence interval for the log-variance is obtained 
from the tables just referred to in the Appendix: still with 
a = 0-05, x2(10) = 21-73 and x2(10) = 3-52 so that (11) says 
that the variance most likely lies between 0-769 and 4-74. There 
is not much difference between the intervals given by (10) and 
(11). Notice that s2 = S2/n = 1-67, which, being a point  
estimate of 6, is naturally near the centre of these intervals. Notice 
also that had we been in the position contemplated at the 
beginning of this section where the instrument had been claimed 
to have lower variance than the old (with variance unity) then 
the claim would hardly have been substantiated since d = 1 is 
quite a probable value for the variance. We return to this point 
later in discussing significance tests. 
5.4. Interval estimates for the normal mean and variance 
Again the data are a random sample from a normal  
distribution but now both the mean and variance are unknown. The 
general ideas in §5.1 extend without difficulty to the case where 
6 is a finite set of real numbers 8 = (6l9 d2, ..., 6S); here s = 2. 
The only change is that it will be necessary to consider the joint 
density of 61 and d2, both prior and posterior, instead of the 
univariate densities. 
If a random variable, usually in this context denoted by t9 has 
a density proportional to 
(l+t2lv)'^+1) (1) 
for all t, and some v > 0, it is said to have Student's t-distribution 
with v degrees of freedom, or simply a t-distribution. 
Theorem 1. Let x = (xl9 x2, ..., xn) be a random sample of 
size nfrom N(dl9 d2) and the prior distributions ofd1 and ln#2 be 
independent and both uniform over (-co, oo). Then the posterior 
distribution of 61 is such that m(61 — x)js has a t-distribution with 
v = n—l degrees of freedom, where 
n 
s2 = Sfe-Jc)2/^. (2) 
i=i 
The joint prior density of d1 and 62 may, because they are 
independent, be obtained by taking the product of the separate 
densities of d1 and #2, which are constant and proportional to 
5.4] MEAN AND VARIANCE 37 
d}1 respectively. Hence the joint prior density is proportional 
to 0;1. 
The likelihood is 
(27r02)-^exp [ - jt(Xi - #i)2/202] , (3) 
so that the joint posterior density is 
n(6l9 02|x) oc 0a-i<»+*> exp T- |i(x,-ei)2/202l. 
n 
It is convenient to rewrite 2 (*« — #i)2 in an alternative form. 
We have 
i=l i=l 
= 2(x<-*)a + fl(x-0i)a = v& + n{x-0tf 
i=i 
so that 
7r(01? 021 x) oc 02-^+2>exp [- {vs2 + n(x - 0!)2}/202]. (4) 
To obtain the posterior density of 61 it is necessary to integrate 
(4) with respect to 62 (equation 3.2.6). This is easily done using 
theorem 5.3.2 with m — %(n + 2) and A = %{vs2 + n(x — d1)2}. 
The result is ^ | x) ^ ^2 + ^ _ ^)2}_^ 
ocil+nix-dtflvs2}-^. (5) 
This is the density for d±: to obtain that of t = n%{x — 6^js we 
use theorem 3.5.1. The Jacobian of the transformation from 
61 to t is a constant, and hence 
7r(t\x)oc{l + t2lv}-^+1\ 
which is (1), proving the theorem. 
Theorem 2. Under the same conditions as in theorem 1 the 
posterior distribution of vs2\d2 is x2 with v degrees of freedom. 
To obtain the posterior density of 62 it is only necessary to 
integrate (4) again, this time with respect to d1: that is, we have 
to evaluate 
0-h(n+2)e-Vs*l202 | °° exp{-/f(JC-ei)a/2^*i. 
J — CO 
38 INFERENCES FOR NORMAL DISTRIBUTIONS [5.4 
The integral is the usual normal integral proportional to d\. 
Hence ^ | x) ^ e-*#w% q-h-\ (6) 
and a comparison with equation 5.3.2 immediately establishes 
the result. 
A parameter which enters a likelihood but about which it is 
not desired to make inferences is called a nuisance parameter. 
For example, 62 in theorem 1 and 61 in theorem 2 are both 
nuisance parameters. 
Example 
The situation of this section, where both the parameters of 
a normal distribution are unknown, is of very common  
occurrence and its investigation by a statistician writing under the 
pseudonym of 'Student' in 1908 is a milestone in statistical 
theory. It is often reasonable to assume that the data come from 
some normal distribution, usually because of past experience 
with similar data, but that the mean and variance are unknown. 
The theory is then so simple that, regrettably, it is often 
assumed that the distributions are normal without any better 
reason for this assumption than the convenience of the results 
that flow from it. The example of a new measuring instrument 
considered in §5.3 is one where, granted the normality, the 
theory of the present section applies. As previously explained 
any instrument may have bias (systematic error) and random 
error and these would be represented by the mean minus the 
known true value of the conductivity, and the variance, and 
they would typically both be unknown. The analysis of the last 
section applied when the bias was known. 
Student's distribution 
Student's /-distribution is not obviously related to any of the 
distributions previously studied, except the Cauchy distribution 
(3.6.5) which it equals when v = 1. Its density is symmetrical 
about the origin, where it is a maximum, and tends to zero as 
t -> ± oo like H"+1). The mean is therefore zero, provided v > 1, 
otherwise the integral diverges. The missing constant of  
proportionality in (1) is easily found by integration from — oo to 
5.4] MEAN AND VARIANCE 39 
+ oo, or, because of the symmetry, by doubling the integral from 
0 to +oo. The substitution t2/v = jc/(1 -x) with 
dt/dx = iv*x-*(l-jc)-* 
gives 
(1 + t2lv)-*<r+t>dt = v* f1 jc-*(1 -xfr-idx 
'•oo 
2 
0 
%l 
0 
= vi(-i)!(iv-l)!/(^-i)!. 
This last result follows from the Beta-integral 
'•i 
xm{\-x)ndx = mini l(m + n+1)1 (7) 
o 
for ra, w > — 1. (This standard result should be learnt if not 
already known, as it is frequently needed in statistical  
calculations.) We saw (§2.5) that ( — ■£)! = V77"* so that the /-distribution 
has density M/ 1VII , 
[-2-0-1)]! 1 ^ 
\/H(^-l)!(l+W^+1)" 
A similar substitution enables the variance to be found: provided 
v > 2 the result is vl(v — 2)9 otherwise the integral diverges. 
More important than these results is the behaviour of the  
distribution as v -> oo. Since, from a standard result, 
lim (I+ t2 !»)-&+» = e~& 
v—>co 
it follows that the density tends to the standardized normal 
density. Thus, for large v, the distribution is approximately 
normal. This is certainly not true for small v since, as we have 
seen, with v ^ 2 the spread is so great that the variance does 
not exist. The distribution function is extensively tabulated. 
Lindley and Miller (1961) give ta(v)9 the upper 100a % point of 
the distribution on v degrees of freedom, for 100a = 5, 2J, 1, \ 
and other values. Because of the symmetry about the origin the 
lower 100a % point is — ta(v), and, as with the standardized 
normal distribution, p(\t\ > t^{v)) = a. It is worth observing 
that ta(v) is greater than the corresponding 100a % point of the 
standardized normal distribution, which is to be expected since 
the variance of t ( = vfcv — 2)) exceeds the normal value of unity. 
As v -> oo the percentage points of the /-distribution approach 
quite rapidly those of the normal distribution. 
40 INFERENCES FOR NORMAL DISTRIBUTIONS [5.4 
Prior distribution 
Since there are two unknown parameters (that is two  
parameters about which the degrees of belief have to be expressed) 
it is necessary to consider a joint distribution of them (§3.2). We 
have already discussed at length in the earlier sections of this 
chapter the prior distribution of either one when the other is 
known, ^{d^ d29 H)9 and 7r(d2\dl9 H). Thus, if 62 = or2 we took 
n(@i\ °"2> H)to be N(ju>09 a2). But we did not say, because we did 
not need to, whether ju>0 or o% would change with <r2. To do this 
we would have to answer the question: if someone came along 
and convinced you that the variance of each observation was 
not cr2, but r2, say, would this change your beliefs about 6^ 
In many situations it would not and therefore we can take the 
conditional distribution tt{61 | 629 H) to be the same for all 62; 
that is, 61 and 62 are independent. Similar remarks apply, of 
course, to tt{62\ 6l9 H) provided the precision, d2x9 is not affected 
by the true value 61 being measured. We therefore suppose 61 
and d2 to be independent, when their joint distribution can be 
defined by their separate distributions. In the case of imprecise 
prior knowledge we saw that it was a reasonable approximation 
to take 61 and ln#2 to be uniformly distributed over the whole 
real line. This explains the choice of prior distribution in the 
theorems. These choices are only reasonable in the case of vague 
knowledge about independent mean and variance: or, in the 
spirit of theorem 5.2.1, when the densities of 61 and ln#2 are 
appreciably constant over the effective range of the likelihood 
function and not too large elsewhere. 
Likelihood 
The rearrangement of the likelihood function to obtain (4) is 
illuminating, besides being necessary for the integration leading 
to (6). In the case where only the mean was unknown we saw 
(§5.1) that the statistician could discard the sample provided 
that he retained x and n: when only the variance was unknown 
(§5.3), he needed to retain Efo■—/i)2 and n. Equation (4) shows 
that when both mean and variance are unknown and only the 
5.4] MEAN AND VARIANCE 41 
n 
normality is assumed, x, s2 = 2 (Xi — x)2l(n—l) and n need be 
retained. (Notice that s2 was defined differently in §5.3, where 
n 
it denoted T,(Xi—/i)2ln.) Again the statistician has achieved 
i=i 
a substantial reduction of data (at least for all but very small n) 
by replacing n numbers by three. The statistics, x and s2 are 
most easily calculated by first evaluating 2x4 and Sjc? and then 
x = SjCi/w, s2 = [Sjcf — (2iXz)2lri\l(n— 1). 
The latter result is easily verified (cf. theorem 2.4.1). x and s2 
are called the sample mean and sample variance respectively. Of 
course there is some ambiguity about what should be retained: 
2x4- and Sjc?, together with n, would be enough. What is 
required is at least enough for the likelihood to be evaluated: 
x and s2 are perhaps the most convenient pair of values. 
Posterior distribution of the mean 
Now consider theorem 1. The posterior distribution of 0± is 
given by equation (5). The most probable value is x and the 
density falls symmetrically as the difference between 6X and x 
increases, the rate of decrease depending mainly on s2. The 
situation is perhaps best understood by passing to t and  
comparing the result with that of theorem 5.2.1 where the variance 
was known. The posterior distribution of d1 was N(x9 cr2\n), 
when 61 had constant prior density; and this may be expressed 
by saying: 
(i) If the variance, cr2, is known, m{d1 — x)jcr is 7V(0, 1). 
Compare this with the above theorem which says: 
(ii) If the variance is unknown, n^{61 — x)js has a  
/-distribution with v = n — 1 degrees of freedom. 
The parallelism between (i) and (ii) is obvious: cr is replaced by s 
and the normal by the /-distribution. Since cr2 is unknown in 
the second situation it has to be replaced by a statistic and 
s2 = S(jCi — x)2l(n— 1) is a reasonable substitute. We saw in 
§5.3 that 2i(Xi—/i)2ln -+ or2 as n -> oo, but that statistic cannot be 
42 INFERENCES FOR NORMAL DISTRIBUTIONS [5.4 
used here to replace cr2 since pi is unknown, so 2(Xi — x)2/w 
naturally suggests itself. In fact 
2(x4 — x)2 = 2i(Xi—fi)2 — n(ju, — x)2, 
and since x -> /i as n -> oo, by the strong law of large numbers, 
2(x4 — x)2l(n— 1) tends to cr2 as w -> oo. The use of (n- 1)  
instead of « is merely for later convenience (§6.1). 
The knowledge contained in the posterior distribution of 0l9 
when 62 is unknown, would be expected to be less than when 
#2 was known, since more knowledge is available prior to 
sampling in the latter case and s has to replace cr. This is 
reflected in the use of the /-distribution in place of the normal, 
for we have just seen that it has larger tails than the normal 
because the upper percentage points are higher (compare the 
use of the inverse of the variance to measure the precision: here 
the precision is (v — 2)lv < 1). But as n -> oo, so does v, and the 
/-distribution approaches the normal. The lack of prior  
knowledge about the variance has little effect when the sample size 
is large. 
The close relationship between (i) and (ii) enables confidence 
interval statements about d1 to be made in a similar manner to 
those in § 5.2. For example, from (i) (x -1 •96cr/A/«, x + 1 •96cr/A/«) 
is a 95 % confidence interval for 61: the corresponding  
statement here is obtained from the /-distribution. We have with 
or p[~tiM < ni(0i-x)ls < t^Jy)] = /?, 
giving p[x-tia{v)sl^n < 61 < x + t^{v)s^n] = ft. (9) 
With fl = 0-95 this gives a 95 % confidence interval for 61 
which has the same structure as the case of known variance, 
with s for or and t0.025(v) for 1-96. For example, with v = 10, 
WOO) = 2-23; for v = 20, WC20) = 2*°9. Thus if s is near 
cr, as would be expected, the confidence interval is longer when 
the variance is unknown than when it is known. Intervals like 
(9), which are symmetric about the origin are obviously the 
shortest ones and satisfy the rule of §5.2 because the /-density is 
unimodal and symmetric about zero. 
5.4] 
MEAN AND VARIANCE 
43 
Posterior distribution of the variance 
The posterior distribution of 62 in theorem 2 is very similar 
to that in §5.3. We put the results together for comparison: 
(iii) If the mean, jll, is known, Yi{xi—pi)2ld2 is x2 with 
n degrees of freedom (theorem 5.3.1 with v0 = 0). 
(iv) If the mean is unknown, 2(x* — x)2I62 is x2 with (n~ 1) 
degrees of freedom (theorem 2). 
The effect of lack of knowledge of the mean is to replace ju> by x, 
a natural substitution, and to reduce the degrees of freedom of 
X2 by one. The mean and variance of x2 being v and 2v  
respectively, they are both reduced by lack of knowledge on the mean 
and the distribution is more concentrated. The effect on the  
distribution of d2, which is inversely proportional to %2, is just the 
opposite: the distribution is more dispersed. The mean and 
variance of d2 are proportional to vfty — 2) and 2v2\{y — 2)2 {v — 4) 
respectively (equations 5.3.5,5.3.6), values which increase as v  
decreases from n to n — 1. This is the effect of the loss of information 
about /£. Confidence interval statements are made as in § 5.3, with 
the degrees of freedom reduced by one and the sum of squares 
about the sample mean replacing the sum about the true mean. 
Example 
Consider again the numerical example of the conductivity 
measurements (§5.1). Suppose the ten readings are from a 
normal distribution of unknown mean and variance; that is, 
both the systematic and random errors are unknown. The 
values of x and s2 are 16-57 and 1-490. Hence a confidence 
interval for 61 with coefficient 95 % is 16-57 ± 2-26 x (1-490/10)*; 
that is 16-57 ± 0-87, which is larger than the interval 16-57 ± 0-63, 
obtained in §5.2 when cr = 1. A confidence interval for 62 with 
coefficient 95% is (13-41/20-31 <6 < 13-41/2-95). This is 
obtained by inserting the values for %2(9) and #2(9) from the 
appendix, instead of %2(10) and %2(10) used in §5.3, and the 
value 2(Xj — x)2 = 13-41. The result (0-660,4-55) gives lower 
limits than when the mean is known (0-815, 5-14) because part 
of the variability of the data can now be ascribed to variation of 
the mean from the known value, /i = 16, used in §5.3. 
44 INFERENCES FOR NORMAL DISTRIBUTIONS [5.4 
Joint posterior distribution 
The two theorems concern the univariate distributions of 
mean and variance separately. This is what is required in most 
applications, the other parameter usually being a nuisance 
parameter that it is not necessary to consider, but occasionally 
the joint distribution is of some interest. It is given by  
equation (4), but, like most joint distributions, is most easily studied 
by considering the conditional distributions. From (4) and (6) 
TTid^X) = 7T(dl9d2\x)l7T(d2\x) 
oc 02-*exp [-/i(x-0i)a/20aL (10) 
that is, N(x9 d2jn)9 in agreement with the result of §5.1. But 
since the variance in (10) is d2jn this distribution depends on 62 
and we see that, after sampling, 61 and 62 are not independent. 
The larger 62 is, the greater is the spread of the distribution of 
dl9 and the less precise the knowledge of 6V This is sensible since 
each sample value, xi9 has variance d2 and has more scatter 
about ju> the larger 62 is, and so provides less information about 
/£. If, for example, d2 doubles in value, it requires twice as many 
sample values to acquire the same precision (w/#2) about /i. 
From (4) and (5), absorbing into the constant of  
proportionality terms which do not involve d29 we have 
<ea|e1,x) = <e1,ea|x)Me1|x) 
oc 62^^Qx^[-{vs2 + n{x-d^}l2d2}9 (11) 
that is, {vs2 + n(x - #i)2}/#2 is X2 with v + 1 = n degrees of  
freedom. Again this involves 6l9 as it must since 61 and 62 are not 
independent. The numerator of the x2 quantity is least when 
61 = x so that 629 which is this numerator divided by %2, has 
least mean and spread when 61 = x. If x = dx then the sample 
mean has confirmed the value of the mean of the distribution, 
but otherwise x departs from 61 and this may be due to d29 hence 
the uncertainty of 62 increases. Although 61 and 62 are not 
independent their covariance is zero. This easily follows from 
(10), which shows that ${0^6^ x) = x for all 629 and equation 
5.4] MEAN AND VARIANCE 45 
3.1.21. We shall see later (§7.1) that in large samples the joint 
distribution is normal so that the zero covariance will imply 
independence. The explanation is that in large samples the 
posterior density will only be appreciable in a small region 
around 61 = x9 62 = s2, as may be seen by considering the 
posterior means and variances of 61 and 02, and within this 
region the dependence of one on the other is not strong. 
Tabulation of posterior distributions 
It may be helpful to explain why, in the theorems of this 
section and the last, the results have been expressed not as 
posterior distributions of the parameter concerned but as 
posterior distributions of related quantities: thus, t instead of 0±. 
The reason is that the final stage of an inference is an expression 
in numerical terms of a probability, and this means that the 
distribution function involved in the posterior distribution has 
to be tabulated and the required probability obtained from the 
tables; or inversely the value of the distribution obtained for 
a given probability. It is naturally convenient to have firstly, 
as few tables as possible, and secondly, tables with as few  
independent variables as possible. In inferences with normal 
samples it has so far been possible to use only three tables; those 
of the normal, t- and ^-distributions. The normal table involves 
only one variable, the probability, to obtain a percentage point. 
The t- and ^-distributions involve two, the probability and the 
degrees of freedom. This tabulation advantage of the normal 
distribution was explained in connexion with the binomial  
distribution (§2.5). The t- and ^-distributions have similar  
advantages. Consider, for example, 62 with a posterior density given 
by equation (6), dependent on v and s2. Together with the 
probability, this seems to require a table of triple entry to obtain 
a percentage point. But vs2jd2 has the ^-distribution, involving 
only a table of double entry, the probability and v. The other 
variable, s2, has been absorbed into vs2\d2. Similarly the mean 
and standard deviation of 61 can be absorbed into n^(61 — x)/s. 
Of course it is still a matter of convenience whether, for example, 
the distribution of vs2\d2 = x2 or x~2 is tabulated. Historical 
reasons play a part but there is a good reason, as we shall see 
46 INFERENCES FOR NORMAL DISTRIBUTIONS [5.4 
later (§6.1), why s2, equation (2), should be defined by dividing 
by (n -1) instead of n. 
Notice that in the two cases of posterior distributions of 
means, the convenient quantity to consider is of the form: the 
difference between unknown mean and sample mean divided by 
the standard deviation of the sample mean, or an estimate of it 
if unknown. (Cf. (i) and (ii) above remembering that @\x) = or2/n 
from theorem 3.3.3.) This property will be retained in other 
examples to be considered below. Thus confidence limits for the 
unknown mean will always be of the form: the sample mean 
plus or minus a multiple of the (estimated or known) standard 
deviation of that mean; the multiple depending on the normal, 
t-9 or later (§6.3), Behrens's distribution. 
5.5. Sufficiency 
Denote by x any data and by p(x \ 6) the family of densities of 
the data depending on a single parameter 6. Then p(x \ 6)9  
considered as a function of d, is the likelihood of the data. Usually 
x, as in the earlier sections of this chapter, will be a random 
sample from a distribution which depends on 6 and/?(x|#) will 
be obtained as in equation 5.1.1. Let t(x) be any real-valued 
function of x, usually called a statistic (cf. §2.4) and let 
p(t(x) 10) denote the density of t(x), which will typically also 
depsnd on 6. Then we may write 
P(x\d)=p(t(x)\d)p(x\t(x)9d) (l) 
in terms of the conditional density of the data, given t(x). 
Equation (1) is general and assumes only the existence of the 
necessary densities. But suppose the conditional density in (1) 
does not involve d, that is 
p(x\d) = p(t(x)\d) p(x\t(x)); (2) 
then t(x) is said to be a sufficient statistic for the family of 
densities p(x \ d\ or simply, sufficient for 6. The reason for the 
importance of sufficient statistics is the following 
Theorem 1. {The sufficiency principle.) Ift(x) is sufficient for the 
family p(x\d); then, for any prior distribution, the posterior  
distributions given x and given t(x) are the same. 
5.5] SUFFICIENCY 47 
We have n(d | x) ex p(x \ 6) tt{6\ by Bayes's theorem, 
= p(t(x)\d)p(x\t(x))7T(d), by (2), 
oc p(t(x) 10) n(d) oc n(d \ t(x)). 
The essential point of the proof is that since p(x\ t(x)) does not 
involve 6 it may be absorbed into the constant of proportionality. 
It follows that inferences made with t(x) are the same as those 
made with x. The following theorem is important in recognizing 
sufficient statistics. 
Theorem 2. {Neyman's factorization theorem.) A necessary and 
sufficient condition for t(x) to be sufficient for p(x\d) is that 
p(x 16) be of the form 
p(x\d) = f(t(x), 6)g(x) (3) 
for some functions f and g. 
The condition is clearly necessary since (2) is of the form (3) 
with/(f(x), 6) = p(t(x)\ 0) andg(x) = p(x\ t(x)). It only remains 
to prove that (3) implies (2). Integrate or sum both sides of (3) 
over all values of x such that t(x) = t9 say. Then the left-hand 
side will, from the basic property of a density, be the density of 
t(x) for that value t9 and 
p(t\e)=f(t,d)G(t), 
where G(t) is obtained by integrating g(x). This result holds for 
all t, so substituting this expression for /into (3) we have 
p(x\6) = p(t(x)\d)g(x)IG(t(x)). 
But comparing this result with the general result (1) we see that 
g(x)IG(t(x)) must be equal to the conditional probability of x 
given t(x)9 and that it does not depend on 69 which is the defining 
property of sufficiency. 
The definition of sufficiency extends without difficulty to any 
number of parameters and statistics. If ^(xlB) depends on 
8 = (6l9 d2, ..., ds)9 s real parameters, and 
t(x) = (^(x), f2(x), ..., tr(*)) 
is a set of r real functions such that 
/>(x|e)=/>(t(x)|e)/>(x|t(x)), (4) 
48 INFERENCES FOR NORMAL DISTRIBUTIONS [5.5 
then the statistics ^(x), ..., fr(x) are said to be jointly sufficient 
statistics for the family of densities p(x\B). The sufficiency 
principle obviously extends to this case: the joint posterior  
distributions of 6l9 ..., ds are the same given x or given t(x). The 
factorization theorem similarly extends. In both cases it is only 
necessary to write 8 for 6 and t for t in the proofs. 
As explained above, we are often interested in the case where 
x is a random sample from some distribution and then the 
natural question to ask is whether, whatever be the size of 
sample, there exist jointly sufficient statistics, the number, r in 
the above notation, being the same for all sizes, n, of sample. 
The answer is provided by the following 
Theorem 3. Under fairly general conditions, if a family of  
distributions with densities f{x i\ 8) is to have a fixed number of jointly 
sufficient statistics *i(x), ..., fr(x)> whatever be the size of the 
random sample x = (xl9 x2, ..., xn) from a distribution of the 
family, the densities must be of the form 
Kxi\9) = FfXi) (fflexp 
S ufa) fa(9) 
3=1 
(5) 
where F, Wj, £*2, • • •, Uif are functions of xt and G, <fil9 <fi2, • ••> <fir are 
functions of 8. Then 
n 
tfa = 2 ufa) (j = 1, 2, ..., r). (6) 
If f(Xi 18) is of the form (5) then it is easy to see that 
tfa, ..., fr(x), defined by (6), are jointly sufficient because 
n 
/>(x|e) = n/(**|e) 
1=1 
n 
n F(Xi) 
i=i 
G(8)"exp S Xufa)M9)\ 
Li=iy=i J 
which satisfies the multiparameter form of equation (3), with 
n 
g(x) = II F(Xi). It is more difficult to show that the distribu- 
i=i 
tions (5) are the only ones giving sufficient statistics for any size 
5.5] SUFFICIENCY 49 
of sample, and we omit the proof. The result, in that direction, 
will not be needed in the remainder of the book. 
Any distribution which can be written in the form (5) is said 
to belong to the exponential family. 
Equation (2) is obtained from (1) by supposing the conditional 
probability does not depend on 6. Another specialization of (1) 
is to suppose the distribution of t(x) does not depend on d, 
that is p(x 16) = p{t(x)) p(x | t(x), d), (7) 
when t(x) is said to be an ancillary statistic for the family of 
densities p(x\d), or simply, ancillary for 6. The definition 
extends to higher dimensions in the obvious way. 
Example: binomial distribution 
Though we have already met examples in earlier sections in 
connexion with a normal distribution, a computationally simpler 
case is a random sequence of n trials with a constant  
probability 6 of success (§1.3). In other words, xt = 1 (success) or 
0 (failure) with 
f(l\d) = d, f(0\6)= l-d, (8) 
and we have a random sample from the (discrete) density (8). 
To fix ideas suppose that the results of the n trials are, in order, 
x = (1 0 1 1 1 0 0 1 0 0 1 1). 
Then the density for this random sample, or the likelihood, is 
p(x\d) = d\\-df. (9) 
n 
Now consider the statistic t(x) = S xu the sum of the xt% in 
this case, 7, the number of successes. We know the density of 
t(x) from the results of §2.1 to be binomial, 5(12, 0), so that 
^(x)|e) = C72)^7(l-^)5. GO) 
Hence it follows from the general result (1) that 
p(x\t(x)9d) = e?y\ (ii) 
which does not depend on 6. Hence t(x)9 the total number of 
successes, is sufficient for 6. 
4 
LSII 
50 INFERENCES FOR NORMAL DISTRIBUTIONS [5.5 
Equation (11) means that all random samples with 7 successes 
and 5 failures, there are ft2) of them, have equal probability, 
ft2)"1, whatever be the value of 6. To appreciate the importance 
of this lack of dependence on 6 we pass to the sufficiency 
principle, theorem 1. Suppose for definiteness that 6 has a 
prior distribution which is uniform in the interval [0, 1 ]; that is, 
7t(6) = 1 for 0 ^ 0 < 1, the only possible values of 6. Then, 
given the sample, the posterior density is proportional to 
#7(1 — d)5. But, given not the whole sample but only the number 
of successes out of the 12 values, the posterior density is  
proportional to ft2)07(l-0)5, or 6\\-df because ft2) may be 
absorbed into the missing constant of proportionality. Hence 
the two posterior distributions are the same and since they 
express the beliefs about 6 it follows that we believe exactly the 
same whether we are given the complete description of the 
12 values or just the total number of successes. In everyday 
language this total number is enough, or sufficient, to obtain 
the posterior beliefs. In other words, the actual order in which 
the successes and failures occurred is irrelevant, only their total 
number matters. We say that the total number of successes is a 
sufficient statistic for the random sequence of n trials with 
constant probability of success. 
Recognition of a sufficient statistic 
It was possible in this example to demonstrate sufficiency 
n 
because we knew the distribution of t(x) = 2 xit In other cases 
i=i 
it might be difficult to find the distribution of t(x). The  
integration or summation might be awkward; and, in any case, one 
would not like to have to find the distribution of a statistic 
before testing it for sufficiency. This is where Neyman's 
factorization theorem is so valuable: it gives a criterion for 
sufficiency without reference to the distribution of t(x). In the 
example we see that the likelihood (9) can be written in the 
n 
form (3) as the product of a function of t(x) = 2 xt and 6 and 
i=i 
a function of x; indeed, it is already so written with g(x) in (3) 
5.5] 
SUFFICIENCY 
51 
trivially equally to 1. Consequently, t(x) is sufficient and its 
distribution theory is irrelevant. It is also possible to define 
sufficiency by (3) and deduce (1) and hence theorem 1. 
Finally note that the densities (8) are members of the  
exponential family. For we may write 
f(x\ 0) = (1 - 0)exp [xln{0/(l - 0)}] (12) 
for the only possible values of x, 0 and 1. This is of the form (5) 
with F(x) = 1, G{0) = (1-0), Ul(x) = x, ^(0) = In {0/(1-0)} 
and r = 1. Notice that §^(0) is the logarithm of the odds in 
favour of x = 1 against x = 0. 
Examples: normal distribution 
The idea of sufficiency has occurred in the earlier sections of 
this chapter and we now show that the statistics considered 
there satisfy the definition of sufficiency. In §5.1, where the 
family N(69 or2) was considered, the likelihood was  
(equation 5.1.9) p(x| Q) ^ exp {_i(__ d)2 {nj(jr2)l 
where the constant of proportionality depended only on x (and 
cr, but this is fixed throughout the argument). So it is  
immediately of the form (3) with t(x) = x, and the sample mean is 
sufficient. The density is of the exponential form since 
f(x 10) = (27rcr2)-i exp [ - (x - 0)2/2cr2] 
= [(27rcr2)-iexp{-ix2/cr2}] 
x [exp{-i02/cr2}]exp(x0/cr2), (13) 
which is of the form (5) with u^x) = x9 ^x(0) = 0/cr2, r = 1 
and F and G given by the functions in square brackets. 
In §5.3 the family was N(/i9 0) and the likelihood (equation 
5.3.3) was proportional to exp [ - S2/20] d~%n so that 
S2 = 2(x,-/02 
is sufficient. Again the family is of the exponential form since 
f(x\d) = (27r0)-iexp[-(x-/*)2/20], (14) 
4 2 
52 INFERENCES FOR NORMAL DISTRIBUTIONS [5.5 
which is already in the form (5) with u1(x) = (x-/i)29 
fa(0) = -1/26, r = 1, G(d) = (lirey* and F(x) = 1. 
Finally, in §5.4, both parameters of the normal distribution 
were considered and the family was N(dl9 d2). Here, with two 
parameters, the likelihood (equation 5.4.3) can be written 
(cf. equation 5.4.4) 
P(x\el9e2) = (27Td2)-^Qxp[-{vs2+n(x-e1y}i2d2]9 (15) 
showing that s2 and x are jointly sufficient for 61 and d2. Again 
the family is of the exponential form since 
f(x\el9 e2) = [{27Td2)-iexV{-id2ie2}}exp{xdx\d2-x2\2d2\ (16) 
which is of the form (5) with u^x) = x9u2(x) = x29 ^(8) = dx\d29 
^2(6) = -1 /2#2> r == s = 2, (7(8) equal to the expression in 
square brackets and F(x) = 1. Notice the power of the  
factorization theorem here: we have deduced the joint sufficiency of 
s2 and x without knowing their joint distribution. 
Minimal sufficient statistics 
Consider next the problem of uniqueness of a sufficient 
statistic: can there exist more than one sufficient statistic ? Take 
the case of a single statistic; the general case of jointly sufficient 
statistics follows similarly. The key to this is found by thinking 
of the factorization theorem as saying: given the value of a 
sufficient statistic, t(x)9 but not the value of x, the likelihood 
function can be written down, apart from a constant factor 
which does not involve 6. This is not true for a general statistic. 
Suppose that s(x) is another function of the sample values and 
that t(x) is a function of s(x). Then s(x) must also be sufficient 
since given s(x) one can calculate t(x) (this is what is meant by 
saying that t(x) is a function of s(x)) and hence the likelihood, 
so that the latter can be found from s(x). If the situation is 
reversed and s(x) is a function of a sufficient statistic t(x) then 
s(x) is not necessarily sufficient. To show this consider the 
binomial example. With the sample as before let s(x) = 1 or 0 
according as 2x4 is even or odd. This is a function of t(x) = Xxi9 
a sufficient statistic, and yet is clearly not sufficient, as it is not 
enough to know whether the number of successes was odd or 
5.5] SUFFICIENCY 53 
even to be able to write down the likelihood function. On the 
other hand, if s(x) is a one-to-one function of t(x); that is, if 
t(x) is also a function of s(x)—to one value of s corresponds one 
value of t and conversely—then s(x) is certainly sufficient. So 
we can think of a sequence of functions *i(x), t2(x), ..., each one 
of which is a function of the previous member of the sequence: 
if ^(x) is sufficient, so is f*(x) for i < j. Typically there will be 
a last member, ^(x), which is sufficient; no ^(x), / > k, being 
sufficient. Now every time we move along this sequence, from 
ti(x) to ti+1(x), we gain a further reduction of data (cf. §5.1) 
since ti+1(x), being a function of f<(x), is a reduction of ^(x) • it 
can be found from ft-(x) but not necessarily conversely. So 
^(x) is the best sufficient statistic amongst ^(x), ..., ^(x) because 
it achieves the maximum reduction of data. These  
considerations lead us to adopt the definition: a statistic t(x) which is 
sufficient and no function of which, other than a one-to-one 
function, is sufficient is a minimal sufficient statistic. It is 
minimal sufficient statistics that are of interest to us: they are 
unique except for a one-to-one functional change—in the last 
example above x, s2 and 2jc<, Sjc? are both minimal sufficient— 
and represent the maximal reduction of data that is possible. 
In this book, in accord with common practice, we shall omit 
the adjective minimal, since non-minimal sufficient statistics 
are not of interest. The whole sample x is usually a non-minimal 
sufficient statistic. It is beyond the level of this book to prove 
that minimal sufficient statistics exist. 
Equivalence of sufficient statistics and sample 
The above arguments all amount to saying that nothing is lost 
by replacing a sample by a sufficient statistic. We now show that 
this is true in a stronger sense: namely, given the value of 
a sufficient statistic it is possible to produce a sample identical 
in probability structure with the original sample. The method is 
to consider the distribution p{x\t{x)) which is completely 
known: it involves no unknown parameters. Consequently x 
can be produced using tables of random sampling numbers in 
the way discussed in §3.5. For example, in the case of a random 
sequence of trials with 7 successes out of 12, we saw (equa- 
54 INFERENCES FOR NORMAL DISTRIBUTIONS [5.5 
tion (11)) that all the (172) sequences with that property were 
equally likely. So one of them can be chosen at random and 
the resulting sequence will, whatever be the value of 6, have the 
same distribution as the original sample, namely that given by 
equation (9). To take another example, if, in sampling from 
a normal distribution someone had calculated the median, he 
might feel that he had some advantage over someone who had 
lost the sample and had retained only x and s2. But this is not 
so: if the second person wants the median he can produce 
a sample from x and s2 using p(x \ x9 s2) and calculate the 
median for that sample. It will have the same probability  
distribution as the first person's median whatever be the values of 
#! and d2. Of course, the person with the sufficient statistics has 
lost something if the true distribution is not of the family  
considered in defining sufficiency. For the definition of sufficiency is 
relative to a family of distributions. What is sufficient for the 
family N(d, or2) (namely x) is not sufficient for the wider family 
N(#i, 82)- Generally the wider the family the more complicated 
the sufficient statistics. In sampling from (8) the order is  
irrelevant: but had the probability of success in one trial been 
dependent on the result of the previous trial, as with a Markov 
chain (§§4.5, 4.6), then the order would be relevant. 
Number of sufficient statistics 
It is typically true that the number of statistics (r in the above 
notation) is not less than the number of parameters s, and it is 
commonly true that r = s, as in all our examples so far. An 
example with r > s is provided by random samples from a 
normal distribution with known coefficient of variation, v; that 
is, from N(69 v2d2). The density is 
f(x\d) = (27TV2d2)-lQxp[-(x-d)2l2v2d2] 
= (ar^r^eip [-^ I+ £ ±], (17) 
which is of the form (5) but with r = 2 and s = 1: x and s2 are 
jointly sufficient for the single parameter 0. Of course, if the 
distribution is rewritten in terms, not of 8, but of <f>l9 ..., $r 
5.5] SUFFICIENCY 55 
(equation (5)), then necessarily the numbers of statistics and 
parameters are equal. 
Exponential family 
Two comments on theorem 3 are worth making. Suppose 
we are sampling from an exponential family with prior  
distribution tt{6) and consider the way in which the beliefs change with 
the size of sample. We suppose, purely for notational simplicity, 
that r = s = 1 so that 
f(Xi\d) = F(jcOG(«)exp[M(jcO^(e)]. 
Then tt(0| x) oc 7r(0)G(0)"exp [t(x)<t>(6)]9 (18) 
n 
with t(x) = 2 u(xt), after a random sample of size n. As « and 
i=i 
f(x) change, the family of densities of 0 generated by (18) can be 
described by two parameters, n and t(x). This is because the 
functional form of n(d | x) is always known, apart from these 
values. Consequently the posterior distribution always belongs 
to a family dependent on two parameters, one of which, n9 
changes deterministically, and the other, t(x)9 changes randomly 
as sampling proceeds. In sampling from an exponential family 
there is thus a natural family of distributions of 0 to consider; 
namely that given by (18). Furthermore, it is convenient to 
choose tt{6) to be of the form G(d)aeh^d\ for suitable a and b9 
because then the prior distributions fit in easily with the  
likelihood. This is part of the reason for the choice of prior  
distributions in theorems 5.1.1 and 5.3.1. For general r and s9 the joint 
posterior density of the s parameters depends on (r+1) 
n 
parameters, n and the statistics 2 Uj(xt). 
i=i 
The distributions (18) are said to belong to the family which 
is conjugate to the corresponding member of the exponential 
family. The nomenclature is due to Raiffa and Schlaifer (1961). 
One advantage in using the conjugate family is that methods 
based on it essentially reduce to the methods appropriate to 
vague prior knowledge, like most of the methods developed 
in this book. The point has been mentioned in §5.2 and examples 
are available in the alternative proof of theorem 6.6.1 and in 
56 INFERENCES FOR NORMAL DISTRIBUTIONS [5.5 
§7.2. If we have samples from the exponential family (still  
considering r = s = 1) and decide the prior distribution is 
adequately represented by the conjugate distribution (18) with 
n = a, t(x) = b, then the observations are adequately sum- 
n 
marked by the sample size, n, and 2 w(*0> and the posterior 
distribution is of the conjugate family with parameters a + n and 
n 
b+ 2 u(Xi)- This posterior distribution is equivalent to that 
i=i 
which would have been obtained with a sample of size a + n 
n 
yielding sufficient statistic b + 2 u(xt) with prior knowledge 
i=i 
7t(6) (in (18)). If 7t{6) corresponds to vague prior knowledge, we 
may use the methods available for that case. 
Regularity conditions 
A second comment on theorem 3 concerns the first few words 
'under fairly general conditions'. We have no wish to weary the 
reader with the precise conditions, but the main condition 
required for the result to obtain is that the range of the  
distribution does not depend t on d. If the range is a function of 6 then 
the situation can be quite different even in simple cases.  
Consider, for example, a uniform distribution in the interval (0, 6) 
with, as usual, 6 as the unknown parameter. Here 
f(Xi\6) = 6-\ O^Xi^d; 
and is otherwise zero, and 
p(x\6) = 0-*, if 0 ^ Xi<0 for all i; 
and is otherwise zero. Consequently the likelihood is 6~n9 
provided 6 > max xi9 and is otherwise zero. Hence the likeli- 
i 
hood depends only on max xt which is sufficient, and yet the 
i 
density is not of the exponential family. Difficulties of this sort 
usually arise where the range of possible values of the random 
variable depends on the unknown parameter. In the other cases 
studied in this chapter the range is the same for all parameter 
values. 
f Readers familiar with the term may like to know that difficulties arise when 
the distributions are not absolutely continuous with respect to each other. 
5.5] 
SUFFICIENCY 
57 
Ancillary statistics 
Ancillary statistics do not have the same importance as 
sufficient statistics. Their principal use is to say that the  
distribution of *(x) is irrelevant to inferences about 69 because the 
likelihood, p(x\6)9 is proportional to p(x\t(x)9 6). In other 
words, t(x) may be supposed constant. Important examples of 
their use are given in theorems 7.6.2 and 8.1.1. A simple 
example is provided by considering a scientist who wishes to 
take a random sample from a distribution with density f{x\d). 
Suppose that the size of the random sample he is able to take 
depends upon factors such as time at his disposal, money 
available, etc.: factors which are in no way dependent on the 
value of 69 but which may determine a probability density, p{n)9 
for the sample size. Then the density of the sample is given by 
n 
p(x\6)=p(ri)Uf(Xi\0). 
1=1 
It follows that t(x) = n is an ancillary statistic. Consequently 
the scientist can suppose n fixed at the value he actually obtained 
and treat the problem as one of a random sample of fixed size 
from/(;c|#). In particular, tff(x\6) is normal, the methods of 
this chapter will be available. 
Nuisance parameters 
Finally, notice something that has not been defined. If there 
is more than one parameter; say two, 61 and 629 we have not 
defined the notion of statistics sufficient or ancillary for one of 
the parameters, 6l9 say, when 62 is a nuisance parameter. No 
satisfactory general definition is known. For example, it is not 
true that, in sampling from N(6l9 62)9 x is sufficient for 6l9 for 
the posterior distribution of 61 involves s9 so without it the  
distribution could not be found. However, something can be done. 
Suppose (cf. (1)) 
P(x\ 619 d2) = P(t(x)\ el9 62)P(x\ t{x)9e2\ (19) 
so that 61 is not present in the second probability on the right- 
hand side. Then, given 629 t(x) is sufficient for 0±: for, if 62 were 
58 INFERENCES FOR NORMAL DISTRIBUTIONS [5.5 
known the only unknown parameter is d1 and (2) obtains. So 
we can speak of sufficiency for one parameter, given the other. 
In the normal example just cited x is sufficient for 6l9 given 62. 
Similarly, if 
p(x 16l9 62) = p(t(x) 162) p(x I t(x)9 619 d2)9 (20) 
then, given d29 t(x) is ancillary for 0V Hence it is possible to 
speak of ancillary statistics for one parameter given the other. 
If (19) can be specialized still further to 
p(x 1619 62) = p(t(x) 16,) p(x | t(x)9 62)9 (21) 
then, given 629 t(x) is sufficient for 6l9 and, given 6l9 t(x) is 
ancillary for d2. 
Both the original definitions made no mention of prior 
information, but if this is of a particular type then further  
consequences emerge. Suppose, with a likelihood given by (21), 
61 and 62 have independent prior distributions. The posterior 
distribution is then 
7T(6l9 62\X)CX: [p(t(x) I 6,) 77(6,)] [P(X I *(X), 62) 7T(d2)]9 (22) 
and 61 and 62 are still independent. Furthermore, if only the 
distribution of 61 is required t(x) provides all the information: 
if only that of 62 is required, then t(x) may be supposed constant. 
Or, for independent prior distributions, t(x) is sufficient for d1 
and ancillary for 62. But unlike the usual definitions of sufficient 
and ancillary statistics these depend on certain assumptions 
about the prior distribution. 
5.6. Significance tests and the likelihood principle 
Suppose we have some data, x, whose distribution depends 
on several parameters, 6l9 629 ..., 6S9 and that some value of 0l9 
denoted by dl9 is of special interest to us. Let the posterior 
density of dx be used to construct a confidence interval for dx 
with coefficient /? (on data x). If this interval does not contain 
the value d± then the data tend to suggest that the true value of 
d1 departs significantly from the value d1 (because 61 is not  
contained in the interval within which one has confidence that the 
true value lies) at a level measured by a = 1 — ytf. In these 
5.6] SIGNIFICANCE TESTS 59 
circumstances we will say the data are significant at the a level; 
a is the level of significance and the procedure will be called a 
significance test. The statement that 61 = d± is called the null 
hypothesis. Any statement that 61 = Of 4= ~dx is an alternative 
hypothesis. The definitions extend to any subset of the  
parameters 8l9 62, ...,6U with u < s using the joint confidence set of 
#i> •••> @u> and values 6l9 ..., 6U. An alternative interpretation of 
a confidence interval is an interval of those null hypotheses 
which are not significant on the given data at a prescribed 
significance level a = 1 -ft. 
The likelihood principle 
If two sets of data, x and y, have the following properties: 
(i) their distributions depend on the same set of parameters; 
(ii) the likelihoods of these parameters for the two sets are 
the same; 
(iii) the prior densities of the parameters are the same for the 
two sets; 
then any statement made about the parameters using x should 
be the same as those made using y. 
The principle is immediate from Bayes's theorem because the 
posterior distributions from the two sets will be equal. 
Example of a significance test 
Like the closely related concept of a confidence interval a 
significance test is introduced for practical rather than  
mathematical convenience. Consider the situation of §5.1, where the 
data are samples from N(d9 cr2), so that there is only one  
parameter, and consider in particular the numerical illustration 
on the measurement of conductivity of an insulating material, 
with the uniform prior distribution (cr0 -> oo). There may well 
be a standard laid down to ensure that the conductivity is not 
too high: for example that the conductivity does not exceed 
17, otherwise the insulation properties would be unsatisfactory. 
This value of 17 is the value 6 referred to in the definition: the 
null hypothesis. The main interest in the results of the  
experiment lies in the answer to the question: 'Is the conductivity 
below 17?' If it is, the manufacturer or the inspector (or the 
60 INFERENCES FOR NORMAL DISTRIBUTIONS [5.6 
consumer) probably does not mind whether it is 15 or 16. 
Consequently the main feature of the posterior distribution that 
matters is its position relative to the value 17. Let us first  
consider the reasonableness of 17 as a possible value of the  
conductivity. The 95 % confidence interval for the mean is 
16-57 ± 1-96 x(l/V10); that is (15-95, 17-19) so that we can say 
that the true conductivity, granted that the measuring  
instrument is free from bias, most likely lies in this interval. But this 
interval includes the value 17 so it is possible that this material 
does not meet the specification. In the language above, the 
result is not significant at the 5 % level. Had an 80 % confidence 
interval been used the values would have been (16-17, 16-97), 
since <D(-l-28) = 0-10 (cf. equation 2.5.14). This does not 
include the value 17 so that the result is significant at the 20 % 
level. This is not normally regarded as a low enough level of 
significance. The values 5, 1 and 0-1 % are the levels commonly 
used, because they provide substantial amounts of evidence that 
the true value is not dl9 when the result is significant.  
Occasionally the exact significance level is quoted: by this is meant the 
critical level such that levels smaller than it will not give  
significance, and values above it will. In the numerical example we 
require to find a such that 
16-57 + Aa(l/V10) = 17, (1) 
where 20(-Aa) = a: or, in the language of §5.3, Aa is the 
upper \cl point of the standardized normal distribution. From 
(1) the value of Aa is 1-36 and the tables show that 
<D(-l-36) = 0-087, 
so that a = 0-174. The result is just significant at the level, 
17-4 %. This agrees with what we have just found: the result is 
significant at 20 % but not at 5 %. 
Just as there is some arbitrariness about a confidence interval, 
so there must be arbitrariness about a significance test (cf. §5.2). 
Thus, in this example, we could have used a one-sided confidence 
interval from — oo to some value and defined significance with 
respect to it. Indeed this might be considered more appropriate 
since all we want to do is to be reasonably sure that the true 
5.6] SIGNIFICANCE TESTS 61 
value does not exceed 17. At the 5 % level this interval extends 
to 16-57 + 1-64 (1/V10) = 17-09 so we think that the conductivity 
is most likely less than 17-09 which would not normally be 
enough evidence to dismiss the doubts that the conductivity 
might exceed 17. If this test is used the exact significance level 
is the posterior probability that 6 exceeds 17. Since 6 is 
#(16-57, 0-1) this value is 
l-*(!™°^)- 1-0>(1.36) = 0-087 
(compare the calculation above) so that the exact significance 
level is 8-7 %, half what it was with the symmetric shortest 
confidence interval. In this example this is probably the best 
way of answering the question posed above: by quoting the 
relevant posterior probability that the material is unsatisfactory 
and avoiding significance level language completely. There is 
discernible in the literature a tendency for significance tests to be 
replaced by the more informative confidence intervals, and this 
seems a move in the right direction. 
It must be emphasized that the significance level is, even more 
than a confidence interval, an incomplete expression of posterior 
beliefs. It is used because the complete posterior distribution is 
too complicated. A good example of its use will be found in 
§8.3 where many parameters are involved. In the simple cases 
so far discussed there seems no good reason for using significance 
tests. 
Prior knowledge 
The type of significance test developed here is only  
appropriate when the prior knowledge of 6 is vague (in the sense  
discussed in §5.2). In particular, the prior distribution in the 
neighbourhood of the null value 6 must be reasonably smooth 
for the tests to be sensible. In practical terms this means that 
there is no reason for thinking that 6 = 6 any more than any 
other value near 6. This is true in many applications, but there 
are situations where the prior probability that 6 = 6 is  
appreciably higher than that for values near 6. For example, if 6 is the 
parameter in genetics that measures the linkage between two 
62 INFERENCES FOR NORMAL DISTRIBUTIONS [5.6 
genes, the value 6 = 6 = 0 corresponds to no linkage, meaning 
that the genes are on different chromosomes; whereas 6 near 6 
means that there is some linkage, so that the genes are on the 
same chromosome, but relatively well separated on it. In this 
case the prior distribution may be of the mixed discrete and 
continuous type with a discrete concentration of probability 
on 6 = 6 and a continuous distribution elsewhere. The relevant 
quantity to consider here is the posterior probability that 6 = 6. 
Significance tests of this type have been considered by Jeffreys 
(1961) but are not much used. They will not be mentioned 
further in this book, but their introduction provides a further 
opportunity to remind the reader that the methods developed 
in this book are primarily for the situation of vague prior 
knowledge, and have to be adapted to take account of other 
possibilities (see, for example, §7.2). 
Decision theory 
It is also well to notice what a significance test is not. It is not 
a recipe for action in the sense that if a result is significant then 
one can act with reasonable confidence (depending on the level) 
as if 6 was not 6. For example, the manufacturer of insulating 
material may not wish to send to his customers any material 
with 0^17. Since he does not know 6 exactly he may have to 
decide whether or not to send it on the evidence of the data. He 
therefore has to act in one of two ways: to send it to the customer 
or to retain it for further processing, without exact knowledge 
of 6. We often say he has to take one of two decisions. The 
problem of action or decision is not necessarily answered by 
retaining it iff the test is significant. The correct way to decide is 
discussed in decision theory: which is a mathematical theory of 
how to make decisions in the face of uncertainty about the true 
value of parameters. The theory will not be discussed in detail 
in this book but since it is so closely related to inference a brief 
description is not out of place. 
The first element in decision theory is a set of parameters 8 
which describes in some way the material of interest and about 
which the decisions have to be made. In the conductivity 
example there is a single parameter, namely the conductivity, 
5.6] SIGNIFICANCE TESTS 63 
and it is required to decide whether or not it exceeds 17 in value. 
The second element is a set of decisions d which contains all the 
possible decisions that might be taken. In the example there are 
two decisions, to despatch or retain the material. Notice that the 
set of decisions is supposed to contain all the decisions that 
could be taken and not merely some of them: or to put it 
differently, we have, in the theory, to choose amongst a number 
of decisions. Thus it would not be a properly defined decision 
problem in which the only decision was whether to go to the 
cinema, because if the decision were not made (that is, one did 
not go to the cinema) one would have to decide whether to stay 
at home and read, or go to the public-house, or indulge in other 
activities. All the possible decisions, or actions, must be included 
in the set. Our example is probably inadequate in this respect 
since it does not specify just what is going to happen if the 
material is to be rejected. Is it, for instance, to be scrapped 
completely or to be reprocessed ? 
The two elements, the decisions d and the parameters 8, are 
related to one another in the sense that the best decision to take 
will depend on the value of the parameter. In the example it will 
clearly be better to reject if 6 ^ 17 and otherwise accept. We 
now discuss the form the relationship must take. If 8 were 
known then clearly all that is needed is a knowledge of the best 
decision for that value of 8. But decision theory deals with the 
case where 8 is unknown: it can then be shown that a  
relationship of such a form, namely knowing the best d for each 8, 
between d and 8 is inadequate. This can be seen in the example: 
one could only expect to be able to make a sensible decision if 
one knew how serious it was to retain material whose  
conductivity was below 17, and to despatch material to the customer 
whose conductivity was in excess of 17. If the latter was not 
serious the manufacturer might argue that he could ' get away 
with it' and send out material of inadequate insulation. If the 
seriousness were changed, for example by the introduction of 
a law making it an offence to sell material of conductivity in 
excess of 17, he might well alter his decision. 
A relationship between d and 8 stronger than knowledge of 
the best d for each 8 is required. It has been shown (see §1.6) 
64 INFERENCES FOR NORMAL DISTRIBUTIONS [5.6 
that the relationship can always be described in the following 
way. For each pair of values, d and 8, there is a real number 
U(d, 8) called the utility of making decision d when the true 
value is 8. U(d, 8) is called a utility function. It describes, in 
a way to be elaborated on below, the desirability or usefulness of 
the decision d in the particular set of circumstances described 
by 8. In particular, the value of d which maximizes U(d, 8) for 
a fixed 8 is the best decision to take if the situation is known 
to have that value of 8. Generally if U(d, 8) > U{d\ 8) then, 
if 8 obtains, d is preferred to d'. In the example, where 8 = 0 is 
the conductivity, d is the decision to despatch and d' to retain, 
a utility function of the following form might be appropriate, at 
least in the neighbourhood of 6 = 17: 
U(d,0) = a + c(d-d\ 
U(d',d) = a + c'(d-d\ 
where a, c and c' are positive constants. Then, for 6 > 6, 
U(d\ 6) > U(d9 6) so that it is best to retain; and for 6 < 6 the 
opposite inequality obtains and it is best to despatch. Notice that 
if 6 differs from 6 by an amount h, say, in either direction, then 
the two utilities differ by the same amount, namely (c + c')h. 
Consequently, the mistake of retention when 6 = 6 — h is just 
as serious (in the sense of losing utility) as is the mistake of 
despatch when 6 = 6 + h. This would probably not be reasonable 
if there was a law against marketing insulating material of an 
unsatisfactory nature: the mistake of despatching poor material 
might then involve more serious consequences than that of 
retaining good workmanship. 
The scale upon which utility is to be measured has to be 
carefully considered. In particular, consider what is meant by 
saying that one course of action has twice the utility of another. 
Specifically, in a given set of circumstances, defined by 8, let d1 
and d2 be two courses of action with utilities 1 and 2 respectively, 
and let dQ be a course of action with utility zero. Suppose that 
between d0 and d2 the course of action is chosen by the toss of 
a fair coin. If it falls heads then dQ is taken, if it falls tails then d2 
is taken. The expected utility (in the usual sense of expectation) 
is half the utility of d0 plus half the utility of d2, or \. 0 + \. 2 = 1. 
5.6] SIGNIFICANCE TESTS 65 
Hence this random choice between d0 and d2 has the same 
expected utility as dl9 namely 1, and the random choice is  
considered as of the same usefulness as dv In other words, one 
would be indifferent between d1 and the random choice between 
d0 and d2. Utility is measured on a scale in which the certainty 
of 1 is as good as a fifty-fifty chance of 2 or 0. There would be 
nothing inconsistent in saying that the utility of obtaining £1 
was 1 but that of obtaining £2 was 1 J, for an evens chance of £2 
(yielding utility, f) may not be considered as desirable as the 
certainty of £ 1. Utility is not necessarily the same as money. 
The discussion of the utility scale shows how the utility can 
be used to arrive at a decision in a sensible manner when the 
true value of 9 is not completely known. The important quantity 
is the expected utility. If there is some uncertainty about 9 it 
follows, from our earlier considerations, that there exists a 
probability distribution for 9, and hence we need to consider 
the expected utility, where the expectation is with respect to this 
distribution. Let 7r(9) denote our knowledge of 9 expressed in 
the usual way as a probability density. The expected utility of 
a decision d is then, by definition, 
U(d, 9) tt(9) dQ = U(d), (2) 
say. The best decision to take is that which maximizes U(d). 
In the example of a utility function cited above the expected 
utility of despatch is 
U(d) = a+ c(B-d)TT{d)dd = a+c(&-6^ 
and of retention is a + c\d1 — 6), where 61 is the expected value 
of 09 or the mean of the density n(6). U(d) > TJ(d') iff 61 < d. 
Hence the material is despatched iff the expected value of 6 
is below the critical value. This is sensible because, as we 
remarked, mistakes on either side of the critical value are 
equally serious with this utility function. 
It remains to discuss the role of observations in decision 
theory. Suppose that a set of observations, x, is obtained and 
that, as a result, the knowledge of 9 is changed by Bayes's 
theorem to the posterior distribution given x, 7r(9|x). The 
5 
LSII 
66 INFERENCES FOR NORMAL DISTRIBUTIONS [5.6 
decision must now be made according to the new expected 
utility « 
U(d9 8) tt(8 I x) dQ = U(d9 x), (3) 
say; that decision being taken which maximizes this quantity. 
On intuitive grounds one expects the decision made on the 
basis of (3) to be better than that made using (2), for otherwise 
the observations have not been of any value. This feeling is 
expressed mathematically by saying that the expected value of 
the maximum of (3) exceeds the maximum of (2). The expected 
value is 
s 
max U(d, x) n(x) dx, (4) 
d 
where 7r(x) is the distribution of x anticipated before the  
observations were made: that is, in the light of the prior knowledge 
concerning 8. (Compare the discussion on beliefs about the 
sample in §5.1.) Precisely 
7T(X) = p(x | 8)77(8)^8, 
where p(x\Q) is the density of the observation given 8 in the 
usual way. This mathematical result is easily established. 
Inference and decision theory 
The reader is referred to the list of suggestions for further 
reading if he wishes to pursue decision theory beyond this brief 
account. It should be clear that the inference problem discussed 
in the present book is basic to any decision problem, because 
the latter can only be solved when the knowledge of 8 is 
adequately specified. The inference problem is just the problem 
of this specification: the problem of obtaining the posterior 
distribution. Notice that the posterior distribution of 8 that 
results from an inference study is exactly what is required to 
solve any decision problem involving 8. The role of inference is 
to provide a statement of the knowledge of 8 that is adequate 
for making decisions which depend on 8. This it does by 
providing the posterior distribution either in its entirety or in 
some convenient summary form, such as a confidence statement. 
5.6] SIGNIFICANCE TESTS 67 
The person making the inference need not have any particular 
decision problem in mind. The scientist in his laboratory does 
not consider the decisions that may subsequently have to be 
made concerning his discoveries. His task is to describe  
accurately what is known about the parameters in question. 
[The remainder of this section is not necessary for an  
understanding of the rest of the book. It is included so that the 
person who learns his statistics from this book can understand 
the language used by many other scientists and statisticians.] 
Non-Bayesian significance tests 
The concept of a significance test is usually introduced in 
a different way and defined differently. We shall not attempt a 
general exposition but mainly consider the significance test for 
a normal mean when the variance is known, the case just  
discussed numerically. This will serve to bring out the major points 
of difference. Suppose, then, that we have a random sample of 
size n from N(d9 or2) and we wish to test the null hypothesis that 
6 = 6. The usual argument runs as follows. Since x is sufficient 
it is natural to consider it alone, without the rest of the data. 
x is known to have a distribution which is N(ji, o-2/n) if ju, is 
the true, but unknown, value of 6 (cf. §§3.3, 3.5). Suppose the 
true value is the null hypothesis value, d, then x is N(d, cr2jn) 
and so we can say that, with probability 0-95, x will lie within 
about Icrj^n of d. (Notice that this probability is a frequency 
probability, and is not normally thought of as a degree of belief.) 
Hence we should be somewhat surprised, if ju> were 6, to have x 
lie outside this interval. If it does lie outside then one of two 
things must have happened: either (i) an event of small  
probability (0-05) has taken place, or (ii) ju> is not d. If events of small 
probability are disregarded then the only possibility is that /i is 
not d. The data would seem to signify, whenever x differs from d 
by more than 2orj^n9 that ju> is not d, the strength of the  
significance depending on the (small) probability that has been 
ignored. Hence, if \x — d\ > Icrj^jn, it is said that the result is 
significant at the 5 % level. Notice that this interval is exactly 
the same as that obtained using Bayes's theorem and confidence 
intervals to construct the significance test: for the confidence 
5-2 
68 INFERENCES FOR NORMAL DISTRIBUTIONS [5.6 
interval for 0 is x ± Icr^n. This identification comes from the 
parallelism between (a) and (b) in §5.1. 
The general procedure, of which this is an example, is to take 
a statistic t(x)9 which is not necessarily, or even usually,  
sufficient ; to find the probability distribution of t(x) when dx = d1 
and to choose a set of values of t(x) which, when dx = dl9 are 
more probable under some plausible alternative hypotheses than 
under the null hypothesis. If the observed value of t(x) belongs 
to this set then, either d1 = d1 and an improbable event has 
occurred, or dx 4= dv The result is said to be significant at a level 
given by the probability of this set. There exists a great deal of 
literature on the choice of t(x) and the set. Confidence limits 
can be obtained from significance tests using the alternative 
definition of the limits given above. 
There are many criticisms that can be levelled against this 
approach but we mention only two. First, the probability 
quoted in the example of a significance test is a frequency 
probability derived from random sampling from a normal  
distribution: if one was to keep taking samples from the  
distribution the histogram of values of x obtained would tend to 
N(69 cr2jri). But the interpretation of the probability is in terms 
of degree of belief, because the 5 %, or 1 %, is a measure of 
how much belief is attached to the null hypothesis. It is used as 
if 5 % significance meant the posterior probability that 0 is 
near 6 is 0-05. This is not so: the distortion of the meaning is 
quite wrong in general. It may, as with N(69 cr2), be justifiable, 
but this is not always so. 
Significance tests and the likelihood principle 
The second criticism is much more substantial. The use of 
significance tests based on the sampling distribution of a 
statistic t(x) is in direct violation of the likelihood principle. 
The principle is self-evident once it has been admitted that 
degrees of belief obey the axioms of probability; yet its practical 
consequences are far-reaching. Since it is not our purpose to 
provide an account of significance tests based on the sampling 
distribution of a statistic we shall use an example to show how 
the usual significance test violates the likelihood principle. Our 
5.6] SIGNIFICANCE TESTS 69 
example may seem a little curious but this is because it is 
necessary to take a case where the choice of t(x) is unambiguous 
so that there is no doubt about the significance test and  
therefore about the significance level. We shall quote the exact level 
defined above. 
Suppose a random sample of one is taken from B(49 d): or 
equivalently we have a random sequence of four trials with 
constant probability, 6, of success. By the sufficiency argument 
only x, the number of successes is relevant, so that x may 
take one of 5 values: 0, 1, 2, 3, 4. Suppose scientist 1 can 
observe the value of x. Suppose scientist 2 can merely observe 
whether the number of successes is 1, or not: in other words, he 
will observe A if x = 1 and A if x is 0, 2, 3 or 4. Now let them 
observe the same random value and suppose scientist 2 observes 
A and therefore scientist 1 observes x = 1, so that they both 
have the same information. (Of course, had scientist 2 observed 
A, they would not have been similarly placed, but that is 
irrelevant.) Then the likelihood for both scientists is 46(1 —df 
and hence, by the likelihood principle their inferences should be 
the same. Nevertheless, had the above type of significance test 
based on a sampling distribution been used with 6 = \ as the 
null hypothesis and \ < 6 < \ as the alternatives, scientist 1 
would have declared the result significant at 500/16% and 
scientist 2 at 400/16 %. The explanation of the difference is that 
scientist 1 would have included the points 0 and 1 in the set of 
values, whereas scientist 2 would only have included A. When 
0 = \, scientist l's set has probability 1/16 + 4/16 = 5/16, 
whereas scientist 2's set has probability 4/16. Larger differences 
could be obtained with numerically more complicated examples. 
Similar objections can be raised to confidence intervals 
derived from such significance tests and that is why they are not 
used in this book. Nevertheless, in the situations commonly 
studied in statistics the results given here agree with those 
obtained by other methods. The reason is the existence of 
parallel properties of t(x) and 0 similar to those mentioned in 
§5.1 ((a) and (6)). Only in §6.3 will there be a case where the 
results are different. 
70 INFERENCES FOR NORMAL DISTRIBUTIONS [5.6 
Suggestions for further reading 
As already stated in the text the treatment of statistics adopted 
in this book is unorthodox in that the approach is unusual, 
though the results are broadly the same as provided by the 
usual methods. Every reader ought to look at one of the 
orthodox books. He will find much overlap with our  
treatment, though a few topics (like unbiased, minimum variance 
estimation) which loom large in those books receive little 
attention here, and others (like posterior distributions) are 
scarcely considered by the orthodox school. The two approaches 
complement each other and together provide a substantially 
better understanding of statistics than either approach on its 
own. Among the orthodox books at about the level of the 
present one mention can be made of Alexander (1961), Birn- 
baum (1962), Brunk (1960), Hogg and Craig (1959), Hoel (1960), 
Tucker (1962) and Fraser (1958). 
Statistics is essentially a branch of applied mathematics in the 
properf meaning of the word 'applied' and no satisfactory 
understanding of the subject can be obtained without some 
acquaintance with the applications. These have largely been 
omitted from the present work, for reasons explained in the 
preface, so that every reader ought to look at at least one book 
written, not for the mathematician, but for the user of statistics. 
The number of these is legion and cover applications to almost 
every branch of human knowledge. We mention only one which 
seems extremely good, that edited by Davies (1957). 
Little statistical work is possible without a set of tables of the 
more common distributions. A small set, adequate for most of 
the methods described in this book is that by Lindley and 
Miller (1961). Two larger collections are those by Fisher and 
Yates (1963) and Pearson and Hartley (1958). An index to 
statistical tables has recently been issued by Greenwood and 
Hartley (1962). 
The field of decision theory is covered in two books, Schlaifer 
(1959) and Raiffa and Schlaifer (1961), along Bayesian lines, 
f As distinct from its meaning in the subject Applied Mathematics as taught 
in British Universities, which is confined to applications to physics. 
5.6] SUGGESTIONS FOR FURTHER READING 71 
broadly similar to those in the present book. Other treatments 
are given by Chernoff and Moses (1959) at an elementary level, 
by Weiss (1961) at an intermediate level, and by Blackwell and 
Girshick (1954) at an advanced level. 
The most important name in modern statistics is that of 
Fisher and his works can always be read with interest and 
profit. The early book on methods, Fisher (1958) could replace 
Davies's mentioned above. His later views on the foundations 
of statistics are contained in Fisher (1959). 
One great work that everyone ought to read at some time is 
Jeffreys (1961). This book covers the whole field from the 
foundations to the applications and contains trenchant criticisms 
of the orthodox school. The present book owes much to the 
ideas contained therein. 
Exercises 
1. The following are ten independent experimental determinations of 
a physical constant. Find the posterior distribution of the constant and 
give 95 % confidence limits for its value: 
11-5, 11-7, 11-7, 11-9, 120, 121, 12-2, 12-2, 124, 12-6. 
(Camb. N.S.) 
2. The Young's modulus of a fibre determined from extension tests is 
1-2 x 108c.g.s. units, with negligible error. To determine the possible effects 
of fibre anistropy on elastic properties, the effective Young's modulus is 
estimated from bending tests; in this experiment there is appreciable 
random error. Eight tests gave the following values: 
1-5, 1-2, 1-0, 14, 1-3, 1-3, 1-2, 1-7 
x 108c.g.s. units. 
Is there reason to think that the extension and bending tests give 
different average results? (Camb. N.S.) 
3. A new method is suggested for determining the melting-point of metals. 
Seven determinations are carried out with manganese, of which the 
melting-point is known to be 1260°C, and the following values obtained: 
1267, 1262, 1267, 1263, 1258, 1263, 1268 °C. 
Discuss whether these results provide evidence for supposing that the 
new method is biased. (Camb. N.S.) 
4. The value of 0 is known to be positive and all positive values of 0 are 
thought to be equally probable. A random sample of size 16 from N(091) 
has mean 040. Obtain a 95 % confidence interval for 6. 
72 INFERENCES FOR NORMAL DISTRIBUTIONS 
5. A manufacturer is interested in a new method of manufacture of his 
product. He decides that it will cost £10,000 to change over to the new 
method and that if it increases the yield by 0 he will gain 6 times £8000. 
(Thus it will not be worth using it unless 0 > 1-25.) His knowledge of 6 is 
confined to the results of 16 independent determinations of 6 that gave 
a sample mean of 2-00 and an estimate of variance of 2-56. Find the 
probability distribution of his expected profit if he changes over to the 
new method. 
6. For fixed s, the random variable r has expectation s and variance <r\, 
the same for all s. The random variable s has expectation ft and variance o%. 
Find the unconditional expectation and variance of r. 
For each of several rock samples from a small region the strength of the 
remanant magnetism is determined in the laboratory. The laboratory error 
in determining the magnetism of a sample is known to be free from bias 
and have variance of = 0-36. There may, however, be variations between 
samples of unknown variance o%. For the determinations for 10 samples 
given below carry out a significance test to see if there is evidence for 
suspecting this second source of variation, and if so give limits between 
which o% most probably lies. 
11-80, 9-23, 11-73, 11-78, 10-21, 9-65, 10-62, 9-76, 8-81, 9-66. 
(The sum of the values is 103-25, the sum of the squares about the mean is 
11-07825; the units are conventional.) (Camb. N.S.) 
7. A random sample of size n is to be taken from N(6, cr2), a2 known. 
The prior distribution of 0 is N(ji0, cr%). How large must n be to reduce the 
posterior variance of 0 to o%lkl (k > 1). 
8. The following two random samples were taken from distributions with 
different means but common variance. Find the posterior distribution of 
this variance: 9.^ g^ g^ 8.8> 9.4; 
10-6, 10-4, 9-3, 101, 101, 100, 10-7, 9-6. 
(Camb. N.S.) 
9. The following are the lengths in inches of six tubes taken as a random 
sample from the output of a mass production process: 
11-93, 11-95, 1201, 1202, 1203, 1206. 
Estimate the mean and standard deviation of the probability distribution 
of the length of a tube. The tubes are used in batches of ten, being fixed, 
end to end, to form a single large tube; it being a requirement that the 
total length of each large tube shall not exceed 10 ft. 1 in. Find a point 
estimate of the frequency with which a batch will have to be sent back to 
be made up afresh. (Camb. N.S.) 
10. A parameter 0 has a prior distribution which is N(/i0, <r%), and an 
observation x is N(6, a2) with cr2 known. 
An experimenter only retains x if 
|*-/e0| < 2(70: 
EXERCISES 
73 
otherwise he takes an independent observation y which is also N(0, a2) 
and ignores the value of x. Given the value of y and the knowledge that 
\x—/*ol ^ 2ao (but not what the value of x was), obtain the posterior 
distribution of 0. Sketch the form of this distribution when y = [iQ. 
(Wales Maths.) 
11. A random sample of size m from N(0, cr2), with cr2 known and 0 
having a uniform prior distribution, yields a sample mean x. Show that 
the distribution of the mean y from a second independent random sample 
of size n from the same distribution, given the value of x; that is, p(y \x, cr2); 
• 
N(x, cr^m^ + n-1)). 
A scientist using an apparatus of known standard deviation 0-12 takes 
nine independent measurements of the same quantity and obtains a mean 
of 17-653. Obtain limits between which a tenth measurement will lie with 
99 % probability. 
12. It is required to test the hypothesis that /* = fi0, using a random 
sample of 30 observations. To save time in calculation, the variance is 
estimated from the first 10 observations only, although the mean is 
estimated from all 30. Find the appropriate test. (Lond. B.Sc.) 
13. Two scientists have respectively beliefs about 0 which are 
NQii9 of) (i=l, 2). 
On discussing the reasons for their beliefs, they decide that the separate 
pieces of information which led them to their beliefs are independent. 
What should be their common beliefs about 6 if their knowledge is 
pooled? 
14. X\, «^2> • • • 9 ^n 
form a random sample from a rectangular distribution 
over the interval (a —/?, a+/?). Discuss the joint posterior distribution of 
a and /? under reasonable assumptions about their prior distribution. 
What are the sufficient statistics for oc and /?? (Camb. Dip.) 
15. The length of life r of a piece of equipment before it fails for the 
first time is a.n 
where r is known but 0 is not. The prior distribution of 0 is uniform in 
(0, oo). n pieces are independently tested and, after a time T, m of them 
have failed at times tl912, ..., tm and the remaining (n — m) are still working. 
Find the posterior distribution of 0 in terms of the incomplete T-function 
I(x, r) = \Zre-tdt I* re-tdt. 
Obtain 95 % confidence limits for 0 in the case r = 1. 
16. The Pareto distribution has density function 
0Ld/x1+d 
for x > L where 0 is a positive parameter. A random sample of size n is 
available from a Pareto distribution with L = 1. Show there exists a 
74 INFERENCES FOR NORMAL DISTRIBUTIONS 
sufficient statistic, and if the prior distribution of ln# is uniform, show 
that the posterior distribution of 6 is T(n, n\nz), where z is the geometric 
mean of the observations. Hence describe how confidence limits for 0 
can be obtained. 
17. Find sufficient statistics for random samples from the multivariate 
normal distribution (§3.5). 
18. Show that 
A*l?) = ^(*+l)e-' (*>0) 
is, for any 0 > 0, a density function. A random sample of size n is taken 
from this density giving values Irrespective of your prior 
knowledge of 6, what quantities would you calculate from the sample in 
order to obtain the posterior distribution of 0 ? 
19. The set of observations X — (xl9 x2,..., xn) has a probability density 
which is known except for the values of two parameters 0l9 62. Prove that, 
if tx = tx(X) is sufficient for Qx when 02 is known and t2 = t2(X) is sufficient 
for 02 when 0X is known, then T = (tl912) is sufficient for 0 = (Ql9 62). 
(Manch. Dip.) 
20. Prove that ifxl9x29..., xn is a random sample of size n from N(ji9 cr2)9 
then Xi — x is independent of x and hence 2(*<—x)2 of x. 
Use this result to prove an extension of the result of § 5.3 that £(Xj—ft)2/a2 
is x2 with n degrees of freedom; namely that ^(x* — x)2/cr2 is x2 with 
(«— 1) degrees of freedom. 
21. The breakdowns of a machine occur in such a way that the probability 
of a breakdown in the interval (t, t+St) is A(t)St+o(St) independently of 
stoppages prior to t. The machine is observed in the interval (0, T) and the 
time of breakdowns recorded. If A(r) = ae^f obtain a test of the  
hypothesis that fi = 0 (i.e. that the breakdown rate does not change with time). 
(Camb. Dip.) 
22. In a large town, the size n of families, is distributed with probability 
density function 
p(n) = (l-p)pn (0<p< 1), 
and in each family the chance of a male child is p independent of other 
children. Given a series of k families, m of which are known to contain 
rl9 ...9rm boys (with rt > 0, i = 1, ..., m) but whose sizes are unknown, 
whilst the rest contain no boys, obtain the posterior distribution of p. 
(Lond. Dip.) 
23. Bacterial organisms are known to be present in a large volume of 
water but it is not known whether there are many or few. Accordingly it 
is decided to examine samples each of 10 cm3 of water for the presence 
(or absence) of the organism, to go on taking samples until k samples have 
been found containing the organism and then to stop. Find the  
distribution of the number of samples, n9 necessary to achieve k positive results, 
EXERCISES 
75 
and the mean and standard deviation of this distribution. Explain how to 
set up confidence limits for p, the proportion of infected samples, so that 
one may be 100a % certain that/? will lie within these limits. 
(Lond. Dip.) 
24. A Geiger counter has a probability 08u+ 0(8u?) of clicking during the 
time interval u, u + Su, independent of u and all previous clicks. 
If the average interval between consecutive members of a sequence of 
11 clicks is 2-2 sec, show that 6 may be asserted to lie in the range 
0-25 (sec)"1 to 0-71 (sec)"1 with a 90 % level of confidence. (Camb. N.S.) 
25. Certain radioactive substances emit particles at random at an average 
rate A. n intervals between such emissions are observed for each of two 
such substances, the mean intervals being x and y respectively. Explain 
how you could test whether the rates of emission are the same. 
(Leic. Gen.) 
26. Observations are made as follows on a Poisson process in which 
events occur at unknown rate 6 per unit time. In the time period (0, u] it 
is observed that m events occur at instants xx < x2 < ... < xm, whereas 
in the time interval («, 2u] it is observed only that n events occur, the 
instants of occurrence not being recorded. What is the (minimal) sufficient 
statistic for 0 and what is its sampling distribution? (Lond. M.Sc.) 
76 
6 
INFERENCES FOR SEVERAL NORMAL 
DISTRIBUTIONS 
The previous chapter was concerned with the problems of 
inference that arise when a single sample is taken from a normal 
distribution: in this chapter similar problems are considered 
when two or more samples are taken from possibly different 
normal distributions. We shall continue to use uniform prior 
distributions for the mean and the logarithm of the variance, 
thinking of them as approximations to distributions representing 
little prior knowledge of the parameters, as in theorem 5.2.1. 
6.1. Comparison of two means 
Theorem L Let xx = (xll9 x129 ..., xln^) be a random sample of 
size nx from N(dl9 erf) and x2 = (x2l9 x229 ••-, x2n^) be an  
independent random sample of size n2from N{629 cr|), where cr1 andcr2 
are known. Then if the prior distributions ofd1 and d2 are  
independent and both uniform over ( — oo, oo), the posterior distribution of 
8 = d1 — 62 is Nix-L — x^ (711^ + v\\n2\ where xx and x2 are the 
respective means of the two samples. 
The joint prior distribution of 61 and 62 is everywhere  
constant and the likelihood of the two samples is (equation 5.1.9) 
proportional to 
f /^(xi — #i)2 n2(x2 — 02)2~] /1x 
so that (1) is also proportional to the joint posterior distribution 
of 61 and d2 given x1 and x2. This is a product of two factors, 
one involving 61 only, one involving 02 only, and hence (§3.1) 
01 and 62 are independent. Furthermore, they are clearly 
N(xl9 o\\n^ and N{x29 <r\ln2) respectively. (This could be 
deduced directly from the corollary to theorem 5.1.1 with 
cr0->oo.) Since, if 62 is normal so is — d29 it follows from 
theorem 3.5.5 that 8 = d1 — d2 is also normal with mean equal 
6.1] COMPARISON OF TWO MEANS 77 
to the difference of the means and variance equal to the sum of 
the variances. 
Theorem 2. Let xx be a random sample of size n^from N(dl9 §S) 
and x2 be an independent random sample of size n2from N(629 <p). 
Then if the prior distributions ofdl9 62 and ln<p are independent and 
uniform over (— oo, oo), the posterior distribution of vs2j(j) is x2 
with v degrees of freedom, where 
JV? = S? = E (Xij-Xif> *>i = nt-l (i = 1, 2) (2) 
and vs2 = S2 = Sl + S229 v = v± + v2. (3) 
[Note that the variances of the two normal distributions from 
which samples are taken are supposedly known to be equal] 
The joint prior density of 6l9 62 and $ is proportional to ^>~1 
and the likelihoods of the two samples are given by equation 
5.4.3 (with the change of a few suffixes to distinguish the two 
samples). The likelihoods may be rearranged in the form (5.4.4) 
and multiplying these expressions together we see that the joint 
posterior distribution of 0l9 62 and $ is 
7T(dl9 d29 </> | Xl, X2) OC 0-K*i+«.+«>exp [ - {/I^ - dtf 
+ n2(x2-d2y + Sl + Sl}l2<f>]. (4) 
To obtain the posterior distribution of <p it is only necessary to 
integrate (4) with respect to 61 and 62. This is easily done since 
the two integrals are the usual normal ones (compare the passage 
to equation 5.4.6) and the result is 
tt(0 I xl9 x2) oc <r*»W 0-i"-1. (5) 
A comparison with equation 5.3.2 establishes the result. 
Theorem 3. Under the same conditions as in theorem 2 the 
posterior distribution of 8 = d1 — d2 is such that 
t = {fo - x2) - 8)1 s{\ K + 1 ln2}i (6) 
has Student's t-distribution with v degrees of freedom. 
The situations in theorem 3 and in theorem 1 (with cr1 = <r2) 
are the same except that <p9 unknown in theorem 3, is known in 
78 INFERENCES FOR SEVERAL DISTRIBUTIONS [6.1 
theorem 1 to be equal to the common value of a1 and cr2. Hence 
theorem 1, in the notation of this theorem, says that 
n(S\ 0, xl9 x2) is #(*! -x2, ^{l/«i + l/«2}). 
Furthermore, n(<p \ xl9 x2) is known from theorem 2, so  
combining these results we have 
tt(S9 $ | Xl, x2) 
= 7T(8\<f>, Xl9 X2)7T(^)\X19 X2) 
oc 0-W^exp ^J^l+lJ"1 p-C^-i^ + i;^}^] 
= ^-^+3)exp [ - (filv + 1) ^2/2^], (7) 
on substituting the expression (6) for t. The integration with 
respect to (f> is easily carried out using theorem 5.3.2 with the 
result that ^ ^ ^ ^ {1 + /2/^_i(,+1)> (8) 
The Jacobian of the transformation from 8 to t is a constant so 
that a comparison with equation 5.4.1 establishes the result. 
A few definitions will be useful. S\ is called the sum of 
squares, vx the degrees of freedom and s\ = S\\vx the mean 
square, for the first sample; with similar definitions for the 
second sample. S2 is called the within sum of squares and v the 
within degrees of freedom, s2 = S2/v is the mean square for §S. 
These terms will be used again later (§6.5). 
Comparative experiments 
Comparative experiments, in which several samples are  
compared, are much more common than single sample experiments 
in which the comparison is with some standard. For example, 
a scientist wishing to investigate the qualities of a new variety of 
wheat would not merely sow some fields with it and obtain 
yields, the liability to disease and other factors, since he might 
obtain a good yield because it was a good year and freedom 
from disease because it was generally a disease-free year. He 
would sow neighbouring fields, or plots, some with the new 
variety and some with one or more varieties which had been 
used for several years and whose behaviour was well known. 
6.1] COMPARISON OF TWO MEANS 79 
The new variety would be judged by comparison with the older 
ones: he is using what scientists call a control This is a  
comparative experiment with two or more samples. The experiments used 
to illustrate the single sample techniques in chapter 5 were all 
absolute experiments (to measure the conductivity of the 
material (§5.1)) or experiments to compare new material with 
a standard (to compare the precision of a new instrument with 
the standard (§5.3)). In some branches of science standards are 
not easy to obtain and it is necessary to use controls. Even 
where standards are available a comparative experiment may be 
preferable because the error is less: a point to be discussed in 
detail under paired comparisons below. 
Case of known variances 
Theorem 1 is the simplest case of a two-sample experiment 
and is a direct extension of the corresponding result for the 
single-sample experiment (theorem 5.2,1). The variances of the 
two normal distributions are supposed known and the two 
samples are independent. It is clear from the likelihood (1) that 
the two sample means are jointly sufficient for the population 
means. The joint posterior distribution of 61 and 62 is normal 
with means xx and x2 and variances o\\nx and or2/n29 and, since 
they are independent, correlation zero. It is usually, however, 
just the difference 8 = d1 — d2 that is of interest, and theorem 
3.5.5 shows that it is also normally distributed. Consequently 
confidence intervals for it may be constructed in exactly the 
same way as with the single value 0±. Thus with r2 = <r\\nx + <r\\n2, 
the posterior variance of 8, a 95 % confidence interval for 8 is 
(xx — x2) ± 1 -96r. It is quite usual for the value 8 = 8 = 0 to be 
of especial interest because it corresponds to the two means 
being equal. For example, if the first sample is taken from the 
new material and the second from the control, the null  
hypothesis 8 = 0 would say that the mean of the new material was 
the same as that of the control, whereas if it were not zero then 
there would be some difference between them. The result will be 
significant at the 5 % level if | X-± X2 I /r exceeds 1 -96. If it is only 
of interest to know whether the new material is an improvement 
over the control, in the sense of having higher mean, then only 
80 INFERENCES FOR SEVERAL DISTRIBUTIONS [6.1 
values of 8 > 0 are of interest and a one-sided confidence 
interval would be used leading to a different significance test. 
Thus, we would be fairly certain that 8 > (x1 — x2) — 2-33r, using 
a 99 % confidence interval [0(2-33) = 0-99] and the result would 
be judged significant at the 1 % level if this interval does not 
include 5 = 0, that is, if {^-x^lr > 2-33. But a significance 
test is rarely adequate in this situation because if the result is 
significant (that is, you feel fairly sure that the new material 
is better than the control) then you naturally want to know 
how much better. Even if it is not significant the additional 
knowledge of the confidence interval is valuable because it 
provides a warning of what values of 8, apart from zero, are 
reasonable. If the confidence interval is too wide you may feel 
obliged to take additional measurements before being content 
that the new material is not different from the control. We again 
issue the warning that a significance test is not a method of 
making decisions: it only expresses one's degree of belief about 
the null hypothesis. If the scientist has to decide whether or not 
to grow the new variety he should use the methods of decision 
theory, as explained in §5.6. Similarly, if he has prior reason to 
believe that 8 is 0, or is very near zero, this prior information 
should be incorporated into the analysis. For example, if 61 and 
#2 have a joint prior normal distribution which is such that 
8 = d1 — d2 has mean zero and small variance, theorem 6.6.3 
may be used to obtain a posterior normal distribution for 8 that 
incorporates this prior knowledge. 
Case of unknown variance 
Theorem 3 is an extension of theorem 1 to the case where the 
variances are unknown and is similar to the extension of the 
single sample case with variance known (theorem 5.2.1) to the 
case of unknown variance (theorem 5.4.1). In both extensions 
the normal distribution is replaced by the /-distribution but 
otherwise the methods are the same. Theorem 2 is also similar to 
the single sample variance result (theorem 5.4.2) leading again 
to a ^-distribution. But before discussing the simplicity and 
elegance of these extensions it must be emphasized that in both 
theorems it is assumed that the two normal distributions have 
6.1] COMPARISON OF TWO MEANS 81 
the same variance, even though the common value is unknown. 
This severe assumption is often likely to be satisfied in practical 
applications. For example, the same measuring instrument may 
be used in both samples, giving the same precision; or the  
variability in the two samples may be due to common causes, as 
with wheat, where it is unlikely that the two varieties would 
differ in their variability over a field, f The corresponding result 
where the variances are not assumed equal will be given in §6.3, 
and will show that there is not much difference between the two 
situations (variances equal or unequal) as far as the difference 
of means is concerned. The elegance and simplicity of the results 
when the variances are equal, the beautiful extensions to several 
samples (§6.4) and the analysis of variance (§6.5), make the 
assumption a most convenient one. 
We discuss theorem 3 first. In the case of known, equal 
variances, erf = erf = cr2, theorem 1 says that 
{fe-^-^MiK+i/^}* (9) 
has a posterior distribution N(0, 1). The quantity t, equation (6), 
is the same as (9) except that s replaces cr. So we have a  
parallelism between the two situations closely similar to that existing 
in the single sample case, 5.4(i) and (ii). Confidence intervals 
and significance tests for 8 = 0 may be constructed in the same 
way as for known cr, with the substitution of s for cr and 
Student's distribution for the normal. These are the same 
replacements as were needed in passing from 5.4(i) to (ii) and 
need not be discussed again. What do merit attention, however, 
are the values of s and v9 the degrees of freedom for t. There are 
two sources for the posterior knowledge of the variance; namely 
the two samples. In §5.4 we saw that the sum of squares of the 
sample values about their mean divided by the number of  
observations, n, was a reasonable statistic to replace the variance. 
In fact we divided by (n— 1) for a reason which will appear in 
a moment. Similarly, in the two sample case S\lv1 = s\ and 
S\lv2 = s% are both reasonable estimates of the variance and we 
f With wheat, it is usually the coefficient of variation that stays constant; but 
if the means do not differ too much the variances will not either. 
6 
LSll 
82 INFERENCES FOR SEVERAL DISTRIBUTIONS [6.1 
naturally combine them. A suggestion would be to take their 
average %(s\ + si) but the analysis of the theorem shows that the 
weighted average (y1sl + p2sl)IO;i + ^2) = ^2 is the more  
convenient quantity to use because, from equation (4), s29 xx and x2 
are jointly sufficient. So what we do (in terms of the definitions 
above) is to take the sums of squares for the two samples and 
add them together obtaining the within sum of squares; to take 
the degrees of freedom for the two samples and add them 
together obtaining the within degrees of freedom; and divide the 
former by the latter to obtain the mean square to replace the 
variance. This simple procedure of addition of sums of squares 
and degrees of freedom generalizes to more complicated  
situations. (Notice that the sums of squares are always sums of 
squares about the sample means; the last four words are  
understood in speaking of sums of squares. Sums of squares about 
the origin are called uncorrected (§6.5).) 
We can now explain the term 'degrees of freedom'. The sum 
^1 = 2 (Xli~Xl) 
1=1 
is the sum of the squares of nx terms and therefore appears at 
first glance to have nx parts which can vary. But the terms are 
constrained (to use a mechanical term) to add to zero, since 
Z (xu — x) = 0, so that once values are assigned to (n1— 1) of 
i=i 
them, the last is then fixed. To continue the analogy with 
mechanics, it is rather like a mechanical system of nx parts with 
only («! — 1) of them free to vary because of a single constraint, 
and we say, as we would of the mechanical system, that it has 
{n1— 1) degrees of freedom. Similarly, S\ has (n2 — 1) degrees of 
freedom and S2, the sum of nx + n2 terms with two constraints 
has nx + n2 — 2 — (n1—l) + (n2—l) degrees of freedom. Again 
this idea extends to more complicated situations. The  
extension will also explain the use of the adjective 'within'. 
It is also possible to see (at the cost of a little algebra which is 
omitted) why the degrees of freedom were used to divide the sum 
of squares, and not n. Suppose s*2 = 2(x4 — x)2jn had been used 
in §5.4 instead of s2 and t* had been defined as t with s* for s. 
6.1] COMPARISON OF TWO MEANS 83 
Then the posterior distribution of t* would have been pro- 
portionalto (i + ,-/„)-*». (i0) 
If, in the two-sample case, s*2 = (Sl + Sl)l(n1 + n2) had been 
used instead of s2 then the posterior distribution of t*9 defined 
as t with s* for s, would have been proportional to 
{1 +1*2^ + n2)}^^+n^1\ (11) 
Now (10) and (11) are not of the same form, with nt + n2 
replacing n, and new tables would be needed for the situation of 
this section: whereas (8) is the same as expression 5.4.1, and 
the same tables suffice. 
An alternative proof of theorem 3 starts from equation (4). 
In this we can change variables to 8 = 61 - d% and 61 + #2, say, 
and integrate with respect to dx + d% to obtain (7), the joint 
density of S and $. The argument using conditional probabilities 
avoids this integration, or rather utilizes the fact that it has 
already been done in the proof of theorem 1, in effect. 
Theorem 2 is a natural extension of theorem 5.4.2. The 
degrees of freedom still determine which ^distribution is 
appropriate and the quantity which, when divided by the  
variance, has this distribution is still the sum of squares, vs29 here 
the total sum of squares. Inferences about the variance can 
therefore be made using the ^-distribution in exactly the same 
way as for a single sample. 
Paired comparisons 
Whenever a theorem is used one should make sure that the 
conditions of the theorem are reasonably well satisfied. This is 
particularly true of theorems 1 and 3, and we now illustrate 
a possible misuse of them. As we have already explained 
a common use of the result is in experiments that compare a 
control and a new treatment. One way of designing such  
experiments can be illustrated on a method used to examine the effect 
of a paint in preventing corrosion. Pieces of metal had one half 
treated with the paint and the other left in the usual state 
appropriate to the use that the metal was to be put. The pieces 
were placed in a wide range of positions, differing in exposure 
6-2 
84 INFERENCES FOR SEVERAL DISTRIBUTIONS [6.1 
to weather, etc., and at the end of a suitable period of time 
measurements were made of the corrosion, xli9 on the untreated 
part and also, x2i, on the treated part of the ith piece. It is not 
unreasonable to suppose that the xu form a random sample 
from N(6l9 $) where 61 is the average corrosion and <p is a 
measure of the variability under different conditions of  
exposure, etc. Similarly, the x2i form a random sample from 
N(629 §S), assuming the same variability for painted and un- 
painted metal. We wish to investigate 61 — 62, the average  
reduction in corrosion due to painting. At first glance it would appear 
that theorem 3 could be used, but this is not so because the two 
random samples {xl4} and {x2i} are not independent. Since xu 
and x2i refer to the corrosion on two parts of a piece of metal 
subject to identical conditions except for the painting, they are 
likely to be much closer together than, say, xu and x2j (y# i) 
which were in different conditions of exposure. Consequently 
the conditions of the theorem are not satisfied and the likelihood 
would be different because of the correlations between xu and x2i. 
Inferences about d1 - 62 may be made in the following way. 
The random variables zt = xu — x2i have expectations 01 — d2 
and variances <j>l9 say, and might perhaps be assumed normally 
distributed. (If xu and x2i have a bivariate normal distribution 
this will follow from theorem 3.5.5.) If so they would form a 
random sample from a normal distribution, since the separate 
pieces of metal are probably independent, and we would wish 
to make inferences about the mean of this distribution. This is 
a single sample problem and theorem 5.4.1 enables the usual 
confidence limits, or significance test for the mean being zero, to 
be found with the aid of the /-distribution. Notice that this 
analysis does not assume that xu and x2i have the same 
variances. 
Although this method is valid, it is not obvious that it is 
a complete inference about 61 — 62 from the whole of the data. 
The argument amounts to considering only the differences 
between the two values on the same plate and takes no account 
of the separate values. Although it is intuitively obvious that 
no information about 01 — 62 is lost by this procedure the 
argument does need further justification. 
6.1] COMPARISON OF TWO MEANS 85 
This is provided by remarking that we can write, with 
Wi — xu + x2i, omitting reference to the parameters, and the rest 
of the notation obvious, 
p(xl9 x2) = p(z, w) = /?(z)/?(w|z). 
Whilst p(z) depends on 8, /?(w | z) typically will not, and,  
whatever parameters the latter does depend on, they will usually not 
occur in/?(z) and will, prior to the observations, be independent 
of those in /?(z). Consequently as far as inferences about 8 are 
concerned we may confine attention to p(z), absorbing/?(w|z) 
into the constant of proportionality. (Compare the discussion 
on ancillary statistics in §5.5.) If xu and x2i9 and hence zt and 
wi9 have a bivariate normal distribution, wi9 for fixed zi9 will 
depend on a, /?, or2 (equations 3.2.13 and 3.2.14) whilst zt will 
depend on /i± and erf (in the notation of §3.2). /i± is here 8, and 
provided (a, /?, or2) and (jil9 erf) are independent the inference 
using z only will not lose any information. 
Two practical points of interest emerge from the experiment. 
First, the quantity 61 — 629 whose posterior distribution, given 
the observed differences, has been found, is the average  
reduction over the conditions of the experiment, and may not be the 
most sensible thing to consider. For example, suppose the effect 
of the paint is to reduce the corrosion by a fixed percentage 
irrespective of the amount of the corrosion. Then 62 = Xd1 and 
the difference is c91(l —A). But 61 may be different if the  
experiment is done again and is, in any case, irrelevant in assessing the 
effect of the paint. The quantity of interest is A and this may be 
studied by taking logarithms of the readings: the differences of 
these will have an approximate mean value of In A and inferences 
can be made about that, although it must be remembered that 
the logarithms might not have a normal distribution. It is 
important, in applications, to make sure that one is applying the 
theoretical results to the quantity of interest. 
Secondly, one might ask why this experimental design was 
used at all. Why not take a random sample of painted pieces and 
a second, independent, random sample of unpainted pieces? 
The reason is that the precision of the determination of the 
effect, whether measured by the inverse of the variance of the 
86 INFERENCES FOR SEVERAL DISTRIBUTIONS {6.1 
posterior distribution or by some other means, such as  
considering the width of the final confidence interval, depends on the 
true variance of the normal distribution (or distributions) from 
which the samples have been taken. The xu and the x2i will have 
a variance which includes the variation from place to place 
whereas the differences X-±i X2$ will have a variance, which only 
involves variations due to the instrument measuring the  
corrosion and to any minor differences in the two parts of the same 
piece of metal. Consequently the effect of taking the differences 
is to reduce, probably very considerably, the variance, and hence 
increase the precision of the determination of the effect of 
painting. As the readings are taken in pairs the method is 
termed 'paired comparisons'. We shall later (§§6.4, 6.5) see how 
to extend this idea and to analyse the variability in some  
experiments into different parts in order to remove the larger parts and 
permit a more sensitive analysis. 
6.2. Comparison of two variances 
If a random variable, usually in this context denoted by F9 
has a density proportional to 
F*vi-1l(y2 + v1F)&i+v*> (1) 
for F ^ 0, and zero otherwise, where vx and v2 are positive, it is 
said to have Fisher's^ F-distribution with vx and v2 degrees of 
freedom, or simply an F-distribution. We shall often speak of an 
F(vl9 v2) distribution. Notice that the order of reference to the 
degrees of freedom is relevant: F(vl9 v2) is not the same as 
F(v2, j/J. 
Theorem 1. Let x4- = (xil9 xi29 ..., xirli) be a random sample of 
size nt from N(di9 §S4) (/ = 1, 2), with xx and x2 independent; let 
the prior distributions ofdl9 d29 ln^x and ln^2 be independent and 
each uniform over (— oo, oo). Then the posterior distribution of 
(sl/sDIijpJfa) is F(yl9 v2)9 where s\9 s\9 v^ and v2 are as in theorem 
6.1.2. 
Because of the independence, both of the samples and of the 
prior distributions, the posterior distributions of (j)1 and $2 are 
independent J and their separate distributions are given by 
t Some writers refer to it as Snedecor's F-distribution. 
% Compare the argument with which the proof of theorem 6.1.1 began. 
6.2] COMPARISON OF TWO VARIANCES 87 
theorem 5.4.2, that is vts\l$i is x2 with vt degrees of freedom 
(i = 1,2). Hence (equation 5.3.2) 
*($i, 02|*i, x2> <* ^r^'W^^expf-^jf^!-^jI/2^2}. (2) 
Let F = (^i/^i)/(^i/^2); then the joint density of F and §S2 is 
(by theorem 3.5.2, with Jacobian equal to slfa/slfa)  
proportional to 
Fi"!-1fc i(-i+^)-i eXp { - (y2 + v1 F) s|/202}. (3) 
The integration with respect to fa is easily carried out using 
theorem 5.3.2 and the result is 
n(F\xl9 x2) oc Fivi-1l(y2 + v1F)tei+v*> 
as required. 
Corollary. Let the conditions be as in the theorem except that 
the means 6l9 62 are known, equal to fa and fa respectively. 
Then the posterior distribution of (sllsl)l(falfa) is F(nl9 n2), 
where 
m 
rii'sl = S (Xij-/*i)2 (i = 1, 2). (4) 
The posterior distribution of fa is now such that n^l^i is x2 
with ^ degrees of freedom (theorem 5.3.1 with v0 = 0), and the 
result follows exactly as in the proof of the theorem. 
The F-distribution 
With the F-distribution we meet the last of the trio (%2, 
t and F) that has played so important a part in modern 
statistics. It is the most important of the three, and, indeed, the 
other two are special cases of it as we shall see below. The 
missing constant of proportionality in (1) is easily found by 
integration from 0 to 00, which is carried out by substituting 
x = V\F\{yi + V\F), with dF/dx = ^2/K(l— x)2]. Then 
'00 
f F^~\v2 + vx F)-*0^^ dF 
x^~x(\ -x^^dxjv^v^ 
0 
vi^viHfri- !)!(*",-1) !/ft("i+ vj-1]!. 
88 INFERENCES FOR SEVERAL DISTRIBUTIONS [6.2 
This last result follows from the 5-integral (equation 5.4.7). 
Hence the F(yl9 v2) distribution has the density 
[Jfri + ^-l]! vw >2 F^-1 (5) 
(i"i-l)!(i"2-l)! * 2 (vt + VlFp*+*' W 
For vx > 2 the density (5) is zero at F = 0, increases to a 
maximum at F = v2(y1 — 2)/y1(V2 + 2) and then decreases to zero. 
For large vx and v2 the maximum is at about F = 1. The mean 
and variance are easily found from (5) using the same  
substitution as produced the missing constant in (1). The results are 
^2/(^2_ 2) for the mean, provided v2 > 2, and 
2v\iy1 + v2-2)lv1(y2-A)iy2-2f 
for the variance, provided v2 > 4. For large degrees of freedom 
these values are approximately 1 and 2(y1 + v2)jv1 v2. If v1 < 2 
the density has the maximum at F = 0 and decreases to zero as 
F-> 00. In the case v1 = 1 the substitution F = f in (5) gives 
the density of Student's /-distribution with v2 degrees of freedom 
(equation 5.4.8). Since the /-distribution is symmetric it follows 
that tables of the F-distribution with v1 = 1 are equivalent to 
tables of the /-distribution. The reason for this connexion 
between the two distributions will appear later (§6.4). 
It follows from the proof of the theorem that 
is the ratio of two independent quantities which are x2 with 
v1 and v2 degrees of freedom respectively: hence the  
nomenclature for v1 and v2. Now consider what happens as v2 -> 00. 
The size of the second sample increases and, as explained in §5.3, 
the knowledge of $2 becomes more and more precise so that we 
effectively know the true value of <fi2, namely the limit of s%. 
Hence as v2 -> 00 we approach the case where $2 is known, and 
then only the first sample is relevant. So if <j)2 = <r§, s\ tends 
to cr\ and we see that v±F tends to Jvf/^i which we know to be 
X2 with v1 degrees of freedom. Hence v^iy^, 00) is x2 with 
v1 degrees of freedom. Thus the statement above that / and x2 
are both special cases of F is substantiated. 
The upper percentage points of the F-distribution have been 
6.2] COMPARISON OF TWO VARIANCES 89 
extensively tabulated: see, for example, Lindley and Miller 
(1961). They give values Fa(yl9 v2) such that if Fis F(vl9 v^ then 
p(F > Fa(vl9 v2)) = a (6) 
for 100a = 5, 2\9 1 and 0-1, and a wide range of degrees of 
freedom. Notice that it requires a whole page to tabulate the 
values for a single value of a. This is a table of triple entry: three 
values a, v1 and v2 are needed to determine a single value of F. 
With the normal distribution a table of single entry (a) sufficed: 
with t and x2 a table of double entry (a, v) was needed. With 
more complex problems more extensive tables are needed and 
the inference problems cannot be considered solved until they 
have been provided. Approximate devices to avoid these  
difficulties will be discussed later (§7.1). Fortunately, with the 
F-distribution it is not necessary to tabulate the lower percentage 
points as well. The reason is that F_1 = Csi/^D/C^/^i) has, by 
interchanging the first and second samples in the theorem, also 
an F-distribution with the degrees of freedom similarly  
interchanged, namely v2 and v±. Consequently, if F is F(vl9 v2) 
p(F < x) = p^F-1 > x-1) 
and this equals a, from (6), when x'1 = Fa(y29 z^). Thus 
Fa(vl9 v2) = {Fa(v29 Vl)}-\ (7) 
where p(F < Fa(vl9 v2)) = a. We therefore have the rule: to 
find the lower point, interchange the degrees of freedom and 
take the reciprocal of the upper point for those values. Of 
course this result may be established purely algebraically, 
without appeal to the theorem, directly from the density, 
equation (5). 
Comparison of two variances 
The theorem refers to independent samples from normal 
distributions and the prior distributions represent ignorance 
about all four parameters. It is not assumed, as in the previous 
section, that the two variances are equal. Such a situation 
might arise when comparing two measuring instruments for 
precision. They give rise to the two samples, one for each  
instrument, and one would be particularly interested in knowing if fa 
90 INFERENCES FOR SEVERAL DISTRIBUTIONS [6.2 
were equal to fa, so that a possible null hypothesis of interest 
is fa = fa. This hypothesis could be expressed by saying 
0i — 02 = 0 but it is more convenient to use falfa = 1 because 
the posterior distribution of the ratio is easier to handle (for 
tabulation purposes) than that of the difference. It is also  
consistent with the use of the logarithms of the variances as the 
parameters having uniform prior distributions to consider the 
posterior distribution of the difference of their logarithms; that 
is, the logarithms of their ratio. Since fa and fa are known, by 
theorem 5.4.2, to have independent posterior distributions 
related to %2, the only point of substance in the proof is to find 
the distribution of the ratio of independent %2's. The result is 
the F-distribution if the degrees of freedom divide the x2 
variables. Precisely, ViS2Jfa = x2 and F = (Xil^dKxll^X  
(Compare the relation to x2 discussed above.) Notice that s\ is an 
estimate of 04 (§5.3) so that Fis the ratio of the ratio of estimates 
of variance to the ratio of the population variances. For this 
reason it is sometimes called a variance-ratio. 
It is now possible to make confidence interval statements about 
the ratio of fa to fa. For example, from (6), with 
F = (sl/sMfalfa) 
7r(fa/fa < (sl/sDIF^, v2)\xl9 x2) = a. (8) 
Similarly, ^^ > pj^jp^ v^ ^ = ^ (9) 
and hence, from (7), 
*K0i/02 > FJ?*9 vd 0!>i)|xi, Xg) = a. (10) 
From (8) and (10) a confidence limit of finite extent for fa/fa 
with coefficient /? = 1 — a is given by 
(AI4)lFUvi* ^ < <M<t>* < FUV» "i) (*» 
This interval will not be the shortest in the sense of §5.2 but 
tables for the shortest interval have not been calculated. 
The main use for the theorem is to provide a significance test 
for the null hypothesis that fa = fa. Sometimes the alternative 
hypotheses of principal interest are that fa > fa: for example, 
if the first sample is taken using a control instrument, and the 
6.2] COMPARISON OF TWO VARIANCES 91 
second using a new instrument, one may only be interested in 
the latter if it is more accurate. The confidence limit required 
would be that ^i/^2 exceeds some value, so would be obtained 
from (8). The result would be significant if the confidence 
interval did not contain the value 1. That is, if the interval in (8) 
did contain this value. Hence it is significant at the level a if 
sllsi> Fa(vl9v2y (11) 
The final inequality is simply that the ratio of the sample 
variances exceeds the upper 100a % point of the F-distribution 
with vx and v2 degrees of freedom: the first degree of freedom 
refers to the numerator of the sample ratio. This one-sided 
significance test will appear again later (§6.4) in an important 
problem. 
The corollary is not often useful since it is unusual for the 
means to be known but not the variances. There is little need to 
comment on it except to remark that we again have the F- 
distribution, and the only difference is that the degrees of 
freedom are equal to the sample sizes and not one less than 
them. Otherwise confidence intervals and significance tests are 
as before. 
6.3. General comparison of two means 
If t1 and t2 are independent random variables each distributed 
in Student's /-distribution, with vx and v2 degrees of freedom 
respectively, and if & is a constant, representing an angle 
between 0 and 90 degrees, the random variable 
d— t1cosaj — t2sm6) (1) 
will be said to have Behrens's distribution with vx and v2 degrees 
of freedom and angle &. 
Theorem L Let the conditions be the same as those of theorem 
6.2 J; then the posterior distribution of 8 = 61 — d2 is such that 
</={*-(*i- *2)}/C*2i/"i + 4M* (2) 
has Behrens's distribution with vx and v2 degrees of freedom, and 
angle G> given by ^ _ ^^^ (3) 
92 INFERENCES FOR SEVERAL DISTRIBUTIONS [6.3 
Again, because of the independence both of the samples and 
of the prior distributions, the posterior distributions of 61 and 
#2 are independent and their separate distributions are given by 
theorem 5.4.1. That is, n^(6i — xi)lsi = ti9 say, has Student's 
/-distribution with vi degrees of freedom (j = 1, 2). We have 
Oi — Xt = tiSil^nt (i = 1, 2) 
so that (0X - 62) - (x± - x2) = /i(^i/V«i) - t2(s2^n2)9 
or, dividing by (sl/^ + s\jn2)% and using (2) and (3), 
d = ^coso) — /2sin&, 
which, on comparison with (1), proves the theorem. 
Behrens's distribution 
We shall not attempt any detailed study of Behrens's 
distribution. It will suffice to remark that it is symmetrical 
about d = 0, that its moments follow easily from those of 
Student's distribution and that, as vx and v2 both tend to 
infinity it tends to normality. This final result follows because 
the /-distribution tends to normality (§5.4) and the difference of 
normal variables is also normal (theorem 3.5.5). Some  
percentage points of the distribution are to be found in Fisher 
and Yates (1963). These tables give values da(vl9 v2, ft) such 
that 
n(\d\ > dia(vl9 v29 0)| xx, x2) = a (4) 
and because of the symmetry 
n(d > da(yl9 v29 ti) \ xl9 x2) = a. (5) 
Comparison of two means 
Theorem 1 differs from theorem 6.1.3 only that in the latter 
the two variances are assumed equal with a uniform prior  
distribution for the logarithm of their common value, whereas here 
the two variances are supposed independent with each logarithm 
uniformly distributed. In both situations we seek the posterior 
distribution of the differences between the means, d1 — d2 = 8: 
We recall that in using the earlier theorem the two estimates of 
6.3] GENERAL COMPARISON OF TWO MEANS 93 
variance, s\ and s\9 from the two samples were pooled to provide 
a single estimate s2 given by vs2 = v±sl + v2sl with v = v1 + v2. 
The quantity t was then obtained by taking the quantity used 
when the common variance, cr2, was known (equation 6.1.9) and 
replacing cr in it by the estimate s. In the present situation the 
two variances must be separately estimated, by s\ and s%9 and 
the quantity used when the variances are known 
{8 - (xx - x2)}/(crf \nx + <r\\n$ 
(from theorem 6.1.1 where it is shown to be N(09 1)) continues 
to be used, but with s\ and s\ replacing cr\ and o\: this gives (2). 
Unfortunately its distribution is complicated, and furthermore 
depends on three parameters, so that a table of quadruple entry 
is needed. The change of the percentage points with ft, and with 
vx and v2 when these are both large, is small so that rather coarse 
grouping of the arguments is possible in tabulation. Notice that 
the statistic d is still (see §5.4) of the form: the difference 
between 8 (the unknown value) and the difference of sample 
means, divided by the estimated standard deviation of this  
difference. For Q)2(xx — x2) = cr\\nx + cr\jn29 since the samples, and 
hence xx and x29 are independent. Confidence limits for 8 are 
therefore provided in the usual way by taking the sample  
difference plus or minus a multiple, depending on Behrens's  
distribution, of the estimated standard deviation of this  
difference. Thus, from (4), writing da for da(vl9 v2, ft), 
Trfo - x2 - dla{s\\nx + s\ln2f 
^ 8 ^ x1-x2 + dia(slln1 + slln2)i\xl9 x2) = /?, (6) 
where, as usual, a = 1 — /?. A significance test of 8 = 0 at 
level a is obtained by declaring the result to be significant if this 
interval does not contain the origin. 
Relationship to Student's problem 
It is a moot point which of theorems 6.1.3 and the present one 
should be used in practical problems. Fortunately the  
differences are not very great; that is tj^v^v^ is not usually very 
different from da(yl9 v29 ti)9 and the differences between pooling 
the variances or using them separately are usually small, at least 
94 INFERENCES FOR SEVERAL DISTRIBUTIONS [6.3 
if the sample sizes are nearly equal. Another argument that is 
sometimes used is the following. In an obvious abbreviated 
notation the joint posterior distribution of 8 and ^i/^2 maY be 
Wn en n(89 0i/02| x) = tt(8\ falfa, x) ntyjfa\ x). (7) 
Now ndfijfalx) is known (theorem 6.2.1). Suppose that this 
has its maximum value near $J$2 = 1 and that it decreases 
sharply from the maximum: in other words, a confidence 
interval for fa/fa is a small interval containing 1. Then an 
integration of (7) with respect to (pjfa, in order to obtain the 
marginal distribution of 8, will only have an appreciable  
integrand near $il$2 = 1 an^ will therefore be approximately 
7t(8\1,x) which gives Student's distribution (theorem 6.1.3). 
Consequently, one can first investigate the ratio of variances 
and, if this suggests that they are about equal, use Student's 
method based on the assumption that they are. The adequacy 
of the approximation has not been investigated. Often it is held 
enough that the test for ^i/^2 = 1 be not significant for Student's 
method to be used: that is, one merely looks to see if the  
confidence interval contains 1, not how small it is. There seems little 
point in doing this here since Behrens's distribution is  
tabulated, but in more complicated situations the procedure of using 
a preliminary significance test has much to recommend it (see 
§6.5). Undoubtedly the simplicity and elegant extensions 
(§§6.4, 8.3) of the equal variances result make it more attractive 
in practical situations and it is used today far more often than 
Behrens's result. There is, however, one other reason for this 
which we now discuss. 
[The remainder of this section uses the ideas developed at the 
end of §5.6 and, like those remarks, may be omitted.] 
Non-Bayesian significance test 
Behrens's result is the first example in this book of a situation 
where the confidence limits and significance test derived from a 
Bayesian approach differ from those obtained using a  
significance test based on the sampling distribution of a statistic. It 
was also the first important example to arise historically, and 
has been the subject of much discussion. The statistic used in 
6.3] GENERAL COMPARISON OF TWO MEANS 95 
the alternative approach is still d, which we now write d(x), 
where x = (xl9 x2), to indicate that it is as a function of x, and 
not 5, that it interests us. Unfortunately the distribution of 
d(x) under the null hypothesis, that is, when 5 = 0, depends on 
crf/cr|, the ratio of the two population variances. It is not  
therefore possible to find a set of values of d such that the probability 
is a that d(x) belongs to this set, when 5 = 0, irrespective of the 
unknown value of crf/cr|. It is necessary to be able to do this in 
order that the level, a, quoted be correct whatever the value of 
the ratio. Instead the procedure is to declare the result  
significant if \d(x)\ ^ g{s\js$) where g is some function chosen so that 
the probability is constant when 5 = 0. It is not known if such 
a function exists, but an adequate approximation has been given 
by Welch and is tabulated in Pearson and Hartley (1958). The 
resulting test is different from that of this section. To make 
matters more complicated, and more unsatisfactory for the user 
of statistical results, Fisher has derived Behrens's result by an 
argument different from ours, but one which does not find ready 
acceptance by any substantial group of statisticians. In view of 
the remarks in §5.6 about significance tests based on the 
sampling distribution of a statistic, it seems clear that Behrens's 
solution to the problem is correct, granted either the  
appropriateness of the prior distributions, or the acceptance of 
Fisher's argument. 
6.4. Comparison of several means 
In theorem 6.1.3 a significance test, using Student's  
/-distribution, was developed to test the null hypothesis that two means 
of normal distributions were equal when their unknown variances 
were equal. In the present section the significance test is 
generalized to several means. The proof of the theorem is  
somewhat involved and the reader is advised not to try to understand 
all the details at first, only returning to study them when he has 
seen how the result is used. 
Theorem 1. Let xt- = (xil9 xi29..., xin) (i = 1, 2, ..., r) be r  
independent random samples each of size n from N(di9 §S), and 
6l9 d2, ...96r and ln^ have uniform and independent prior distribu- 
96 INFERENCES FOR SEVERAL DISTRIBUTIONS [6.4 
tions over ( — 00,00). Then a significance test at level <x of the null 
hypothesis that all the d{ are equal is provided by declaring the 
data significant if 
w 2 (**.-*..)2K 
exceeds Fa(vl9 v2)9 the upper 100a % point of the F-distribution 
with vx and v2 degrees of freedom {equation 6.2.6). 
The notation in (I) is 
n r 
*i. = 2 xtjln, xmm = 2 XtJr = 2 x^/m, (2) 
and v1 = r— 1, y2 = K^-!)- (3) 
The joint posterior distribution of all the parameters is clearly 
niPi* &2> • ••> 0r> 0lx) 
oc ^H^exp [- {2 a (*,. - 0,)2 + 2 S»/2fl, (4) 
a direct generalization of equation 6.1.4, where 
n 
$i — 2 (*t7"~*t.) • 
jc*. is a new notation for jc* and x denotes (xl9 x2, ..., xr). It is 
convenient to write *S2 for 2 Sf. Now change from the random 
variables 6l9 629 ...90r to 69 Xl9 A2, ..., Ay, where 0* = # + A4- and 
2A,- = 0; leaving ^ unaffected. The point of the change is that 
we are interested in differences between the 0* which can be 
expressed in terms of the A,: the null hypothesis is that 
Ax = A2 = ... = Ar = 0. (5) 
The Jacobian of the transformation is constant (theorem 3.5.2) 
so that, provided 2A,- = 0, 
7t{69 Xl9 A2, ..., Ay, 0|x) 
oc 9H<^+2>exp [ - &n{xim - d - A,)2 + S2}/2# (6) 
6.4] COMPARISON OF SEVERAL MEANS 97 
The summation over i may be written 
nrd2 - 2dnX(xim- A,) + n2(xim- A,)2 
= nr62 — 26nrxmm + n^i(xi—Xi)29 since 2Af = 0, 
= nr(d — x )2 + n2i(xim — A^)2 — nrx2mm 
= nr(6-x )2 + nZi(Xi -x -A,)2, using 2A,. = 0 again. 
(7) 
On substituting this expression into (6) the integration with 
respect to 6 is easily carried out using the normal integral, with 
the result that 
n(Xl9 A2, ..., Ar, 0|x) 
oc ^-i^+Dexp [ - {n S (*,. - *.. - A,)2 + S2}/2# (8) 
i 
Integrating with respect to <p, using theorem 5.3.2, we obtain 
"(K K • •., KI x) oc {n S (x,. - x . - A,)2 + S2H<—D, (9) 
provided 2A, = 0. 
Hence the joint density of the A,- has spherical symmetry 
about the value 
\Xl.~Xm 9 X2m — Xm9 ..., Xr— Xj 
in the (r— l)-dimensional space with 2A,- = 0. Furthermore, 
the density decreases as the distance of X = (Al5 A2, ..., A,) from 
the centre of symmetry increases. Consequently, a confidence 
set for the parameters A, would, according to the rule suggested 
at the end of §5.2, consist of a sphere with centre at the centre of 
symmetry and a radius, A0, so chosen that the probability, 
according to (9), of A lying in the sphere was yff, where fi is 
the confidence coefficient. But this probability is exactly the 
probability that A < A0, where A2 = Z(jtf. —je.. —At-)2.  
Consequently, in order to obtain the relationship between fi and A0 
it is only necessary to find the distribution of A2 and to choose 
A0 so that tt(A2 < Ag|x) = yff. 
In order to find this probability it is necessary to integrate (9) 
over all values such that A2 < A2,. This will be the distribution 
function of A2. Instead let us find the density of A2; that is, let 
us find the probability that A2, < A2 < A2, + 8, where 8 is small; 
the result will be, to order 8, the required density times 8 (cf. 
7 
LSII 
98 INFERENCES FOR SEVERAL DISTRIBUTIONS [6.4 
§2.2). Now A2 satisfies these inequalities provided the values A, 
lie in between two spheres with centres at the centre of  
symmetry and squared radii A2, and A2, + 8, and in this region, called 
an annulus, the joint density of the At. is sensibly constant at a 
value proportional to ^ + ^^ (10) 
(from (9)). Hence the required probability, namely the integral 
of (9) over the annulus, will be equal to the constant value of the 
integrand, (10), times the volume of the annulus. The volume of 
a sphere of radius A0 in (r — 1) dimensions (because SAt- = 0) is 
proportional to Aj-1 = (Ag)^-1*, so that the volume of the 
annulus is proportional to 
(A2 + tf)fr-D - (A2)^-1) oc 5(A2)i<'-3>, 
to order S. Hence we have 
n(A21 x) oc (A2)i<'-37{/*A2 + S2}i<™-D. (11) 
The substitution A9„ ^ K9I 
O = nAl(r~l) = nAK 
S*lr(n-l) S*/v2 
gives 7r(01 x) oc Ofo-1/^ + vi <&fiyi+v*9 
so that the posterior distribution of O is F(vl9 v2) (equation 6.2.1). 
A confidence set for the A's with confidence coefficient 
ft = 1 — a is therefore provided by the set of A's for which 
O ^ Fa(vl9 v2). Hence the null hypothesis that all the A's are 
zero will be judged significant if the point (5) does not belong to 
this set. When (5) obtains O = F, in the notation of (1), so that 
the result is significant if F > Fa(vl9 v2\ as required. 
Theorem 2. If the situation is as described in theorem 1 except 
that (j> is known, equal to cr2: then the equivalent significance test 
is provided by declaring the data significant if 
x2 = *2(**.-*..)2/<r2 (12) 
exceeds %I(^i), the upper 100a % point of the ^-distribution 
with vx degrees of freedom (§5.3). 
6.4] COMPARISON OF SEVERAL MEANS 99 
The proof is similar to that of theorem 1 as far as equation (8) 
which will now read 
n(Xl9 A2, ..., Ar| x) oc exp [-n 2 (x,. - xmm- A,)2/2cr2] 
i 
with 2A; = 0. The argument continues on similar lines since 
this joint density also has spherical symmetry. The only  
difference is that the sensibly constant value over the annulus is 
exp [-/*Ag/2cr2] instead of (10). Hence instead of (11), we have 
tt(A2| x) oc (A2)i<'-3>exp [-/*A2/2cr2]. 
The substitution O = «A2/cr2 gives 
tt(0|x)oc 0*(r-3>e-^, 
so that the posterior distribution of O is x2(vi) (equation 5.3.1). 
The same argument as used in theorem 1 produces the  
significance test. 
Relationship with the t-test 
In this section we discuss the theorem and its proof and 
develop some corollaries from it: the practical significance of 
the result will be discussed in the next section. The first point to 
notice is that, apart from the restriction to equal sizes of sample, 
a restriction that we shall remove below, the theorem is a 
generalization of the significance test of theorem 6.1.3 from two 
to r samples. To see this we first notice that the samples, as in 
the earlier result, are independent and normal with a common, 
but unknown, variance. The significance test in the two-sample 
case was performed by calculating a statistic t and declaring 
the result significant if |f| exceeded t^(y), the upper il00a% 
point of the /-distribution with v degrees of freedom; that is, if 
t2 > t\x(y)* The statistic t, in the notation of 6.1.3, was equal 
to (Xi — x^fclsPlrift and v was 2(n— 1), remembering that 
Wi = n2 = n, say. If we change to the notation of this section, 
Xi is xim and ^2 = s s (^ _Xiyj2{n_ 1} 
i 0 
so that, taking the square of t, 
/2 __ n\Xl.~~X2.) 
E(*„-*«.)s/(ii-1)* 
1,3 
7"2 
100 INFERENCES FOR SEVERAL DISTRIBUTIONS [6.4 
But it is easy to verify that (xlm-x2m)2 = 2 2 (xi.~x..)2 so that 
i 
«2(*<.-x..)2K 
t2= S(^-^.)V2* (B) 
with z^ = 1 and v2 = 2(n-l) = v9 and hence f2 = F in the 
notation of (1). Consequently the statistic used here is the same 
as that in the earlier result. But we saw in §6.2 that if t has a 
/-distribution with v degrees of freedom, t2 has an F(l, v)  
distribution, which is just the distribution used here since vx = 1 and 
v2 = v. Hence for r = 2, the two significance tests are the same. 
The change from \<x in f|a(V) to a in FJ}9 v) arises because in the 
squaring, the \<x in each tail of the /-distribution gives a in the 
single tail of the t2 = F distribution. 
Unequal sizes of samples 
Theorem 1 generalizes without difficulty to the case of 
unequal sample sizes. We state it formally as 
Corollary 1. If the situation is as described in the theorem 
except that the ith sample has size ni9 then 
2/fi(x<.-x..)2/j'i 
F - W*..», *FXv" •> (14) 
provides a significance test of the null hypothesis that all the dt 
are equal; where 
m 
xi. — 2 xijlnt9 x.. — 2 *#/2 wt- (15) 
j = l i,j i 
and v1 = r—l9 v2 = 2(^-1). (16) 
i 
The proof is exactly the same as that of the theorem with a few 
minor algebraic complexities which tend to obscure the salient 
points of the argument, this being the reason for first proving it 
in the equal sample size case. 
Distribution of a single mean 
The practical reasons for the significance test being more 
important than the confidence set must await discussion in the 
6.4] COMPARISON OF SEVERAL MEANS 101 
next section but it is already clear that even the joint distribution 
of the A's (equation (9)) is rather a complicated expression of 
degrees of belief, and it is natural to consider instead the  
function A2, or equivalently O, since the joint density depends only 
on this function. The change to O is made so that the tables 
of the F-distribution are readily available. Although the joint 
distribution is not suitable for practical work it is possible to 
obtain the posterior distribution of any #*. 
Corollary 2. Under the conditions of the theorem the posterior 
distribution, 7r(0t-|x), of dt is such that n%(di — xim)ls has a 
t-distribution with v2 degrees of freedom, where 
s* = yL(xii-xi?jVi9 (17) 
• • 
the denominator of (I). 
Start from equation (4) and integrate with respect to all the 
#'s except di% Each integral is a normal integral and the result 
is obviously 
7T(6i9 <f>\x)oc 0-4^+Mr-DJexp[-{n(Xi-dt)2 + S2}/20] 
= 0-JOvH»exp [ - {n(xim- dt)2 + v2s2}l2<f>]; (18) 
the result follows from this just as theorem 5.4.1 followed from 
equation 5.4.4 by integration with respect to <p. The present 
result is clearly a generalization of that theorem. 
Distribution of the variance 
The analysis has so far concentrated on the distribution of 
the A's. Sometimes the posterior distribution of <j) is of interest. 
Corollary 3. Under the conditions of the theorem the posterior 
distribution of <p, tt{<j> | x), is such that S2l<fi is x2 with v2 degrees 
of freedom. 
This follows immediately from (4) on integrating with respect 
to all the #'s. The result is 
7T(<p I X) OC <f>-:k[n(r-l)+2]e-Sm<f>9 (19) 
and a comparison with equation 5.3.2 establishes the result. 
Inferences about the unknown variance can now be made in 
102 INFERENCES FOR SEVERAL DISTRIBUTIONS [6.4 
the usual way (§ 5.3) with the ^-distribution. For general sample 
sizes the result persists with v2 now defined by (16), S2 being 
unaltered. 
Distribution of linear functions 
A useful extension of corollary 2 is possible using this last 
result. 
Corollary 4. Under the conditions of the theorem, if 
cl9 c2, ..., cr are any constants, not all zero, the posterior  
distribution of 2iCi0i is such that n^yZici{di — xi)ls{^c\)^ has 
Student's t-distribution with v2 degrees of freedom. 
(Corollary 2 is the special case where ct = 1 and all the other 
c's vanish.) The proof is very similar to that of theorem 6.1.3. 
If (j) is known the #4 are independent and normal with means 
xim and variances <pjn, by the corollary to theorem 5.1.1 with 
<x0 -> oo. Hence the posterior distribution of 2c* 0* is normal with 
mean 2c*x^ and variance Y,c\^\n by theorem 3.5.5. Hence we 
know 7r(2c^|0, x) and from corollary 3 we know 7r(^|x), so 
that 7r(IiCidi9 §S|x) follows on multiplication and 7r(2c^|x) on 
integration with respect to $, exactly as in the earlier result. 
In particular, the posterior distribution of 6 = 2#,-/r can be 
found. With unequal sample sizes the /-statistic is 
Xciidi- xim)ls(Xc2Jni)i. 
Significance test for a subset of the means 
From this corollary it is possible to pass to a more general 
result. 
Corollary 5. Under the conditions of the theorem, a  
significance test at level <x of the null hypothesis that some of the dx are 
equal, say 61 = 62 = ... = dt, is provided by declaring the data 
significant if t 
nZ(Xi-x(l)yi(t-l) 
F = -*=* — (20) 
_ t 
exceeds Fa(t— 1, v2); where x® = S x% It- 
6.4] COMPARISON OF SEVERAL MEANS 103 
The joint posterior distribution of 6l9 d29 ...9dt and <p is 
provided by integrating (4) with respect to 6t+l9 ..., #r. The 
result is clearly 
tf(0i, 0* ...,dt9<f>\x) 
OC <fi-iKnr+2)-(r-t)]QXpr _l £ n(x. _ fl.)2 + S2\ 12<p] , (21) 
and the result follows from (21) in the same way that the 
theorem followed from (4). Notice that the numerator of (20) 
contains only those samples whose means are in question, 
whereas the denominator contains contributions from all 
samples, as before. It is the same with the degrees of freedom: 
for the numerator they are reduced from (r-1) to (t-l) but 
they stay the same, at r(n— 1), in the denominator. 
It is worth noting that it follows from the form of the  
likelihood, which is proportional to the expression on the right-hand 
side of (4) multiplied by <p9 and the factorization theorem 5.5.2 
that (xl9 x29 ..., xr, S2) are jointly sufficient for (dl9 d29..., dr9 $). 
All the posterior distributions quoted are in terms of these 
statistics. 
Theorem 2 covers the situation where the variance is known. 
It is possible to see that the test statistic proposed, equation (12), 
is the natural analogue of (1), used when or2 is unknown, because, 
since o*2 does not have to be estimated, the denominator of (1) 
can be replaced by cr2 (see §6.5). This theorem has corollaries 
analogous to those for theorem 1. 
Derivation of the density 
There is one part of the proof that involves a type of argument 
that has not been used before and is therefore worth some  
comment. This is the derivation of the density of A2. On many 
occasions we have had to integrate a joint density in order to 
obtain the density of a single random variable; but in these cases 
the joint density has been expressed already in terms of the single 
random variable required and others. But here we are required 
to obtain 7r(A2|x) by integration of tt(\19 A2, ..., A^x) where the 
latter is not already expressed as a density of A2 and other 
random variables. One method would be to rewrite 
7T(\l9 A2, ..., A,.|x) 
104 INFERENCES FOR SEVERAL DISTRIBUTIONS [6.4 
as a joint density of A2 and (r — 2) other variables, using the 
Jacobian of the transformation to do so (theorem 3.5.2), and 
then integrate out the (r — 2) unwanted variables. This type of 
argument has already been used in the proof to remove 6. The 
geometrical argument, which is commonly useful, avoids the 
necessity of transforming and calculating the Jacobian. Notice 
that we have worked in terms of A2, not A, since the sign of A is 
irrelevant. The posterior density of A is proportional to (11) 
times A, since dA2 oc AdA (cf. equation 3.5.1). 
6.5. Analysis of variance: between and within samples 
In this section the more practical aspects of theorem 6.4.1 and 
its corollaries are discussed. There are r independent, random 
samples, each of size n, from normal distributions with unknown 
means and an unknown common variance. The prior  
distributions are those which represent considerable ignorance as to the 
values of the unknown parameters. It is important to  
remember this assumption about the prior distribution: there are 
practical circumstances where it is not appropriate. This point 
will be mentioned again, particularly in connexion with 
theorem 6.6.3. 
The expression 2 (Xij — xmm)2 is called the total sum of squares, 
2 (xi:j — xim)2 is called the within sum of squares (cf. §6.1) and 
n 2 (**. — x.)2 is called the between sum of squares. We have the 
i 
identity ^ {Xi._xy = s (Xi._Xif+n s (Xi-Xy, (l) 
which follows immediately on writing the left-hand side as 
^ [(Xij — x{) + (Xi . — x. .)]2 and expanding the square. In words, 
(1) says that the total sum of squares is the sum of the between 
and within sums of squares. The total sum of squares has  
associated with it rn — 1 degrees of freedom, called the total degrees 
of freedom; the within sum of squares has r(n— 1) = v2, the 
within degrees of freedom (corollary 3 to theorem 6.4.1); and, to 
preserve the addition, the between sum of squares has (r — 1) = vl9 
the between degrees of freedom, in agreement with the fact that 
6.5] ANALYSIS OF VARIANCE 105 
it is the sum of r squares with one constraint, 2(x<t —*..) = 0. 
The ratio of any sum of squares to its degrees of freedom is 
called a mean square. It is then possible to prepare table 6.5.1. 
Such a table is called an analysis of variance table and one is said 
to have carried out an analysis of variance. The first two columns 
are additive, but the third column is not. The final column gives 
the value of F, defined in the last section, the ratio of the two 
mean squares, needed to perform the significance test. 
Table 6.5.1 
Sum of Degrees of 
squares freedom Mean square F 
Between n X (xL—xj2 vx = (r— 1) Mx MJM2 
i 
Within EOCtf-jeJ1 v% = r(#i-l) s2 = M2 = S2/v2 — 
Total SOctf-jeJ1 (nr-1) — — 
Practical example: basic analysis of variance 
The discussion will be carried out using a numerical example. 
In the preparation of a chemical, four modifications of the 
standard method were considered and the following experiment 
was carried out to see whether the purity would be improved by 
use of any of the modifications. Batches of the chemical were 
prepared from similar material using each of the five methods 
(the standard one and the four modifications), six samples were 
taken from each batch and tested for purity. The results are 
given in table 6.5.2 in suitable units. In this case the measures 
were of the impurities and all were between 1-3 and 1-4: 1-3 was 
therefore subtracted from each reading and the results were then 
multiplied by 1000. Past experience was available to show that 
within a batch produced by the standard method (number 1 in 
the table) the distribution was approximately normal, and the 
same might therefore be supposed for the new batches. The 
changes in method probably did not affect the variability, 
which was due more to the raw material than the method. The 
samples were taken independently (to ensure this the five 
106 INFERENCES FOR SEVERAL DISTRIBUTIONS [6.5 
methods were used in a random order). Hence all the conditions 
on the probability of the observations assumed in theorem 6.4.1 
seem reasonably satisfied. For the moment assume the prior 
distribution used there; we return to this point later. The  
calculations then proceed as follows: 
(1) Calculate the totals for each method (table 6.2) and the 
grand total, 1047. We note that r = 5, n = 6 in the notation 
of §6.4. 
Table 6.5.2 
Method... 1 
43 
41 
54 
57 
48 
63 
Totals 306 
2 
33 
2 
31 
23 
41 
27 
157 
3 
10 
24 
40 
37 
24 
30 
165 
4 
44 
29 
31 
44 
45 
28 
221 
5 
37 
21 
35 
30 
28 
47 
198 
(2) Calculate the uncorrected (§6.1) sum of squares of all the 
readings; S At = 41,493. 
• • 
(3) Calculate the uncorrected sum of squares of the totals for 
each method, divided by the number of samples contributing to 
each total, here n = 6; S (S XijYIn = 38,926 (to the nearest 
i i 
integer). 
(4) Calculate the correction factor defined as the square of the 
grand total divided by the total number of readings, here 
rn = 30; (2 Xi^fjrn = 36,540 (to the nearest integer). 
• • 
(5) Calculate the total sum of squares, (2)-(4); 4953. 
(6) Calculate the between sum of squares, (3) - (4); 2386. 
(7) Construct the analysis of variance table (table 6.3). 
(Notice that the within sum of squares is calculated by  
subtraction.) The upper percentage points of the F-distribution with 
4 and 25 degrees of freedom are, from the tables, 2-76 (at 5 %), 
4-18 (at 1 %) and 6-49 (at 01 %). The test of the null hypothesis 
that none of the modified methods has resulted in a change in 
the purity is therefore significant at 1 % (since F = 5-81 > 4-18) 
but not at 0-1 % (F < 6-49). The result can perhaps better be 
6.5] ANALYSIS OF VARIANCE 107 
expressed by saying that one's degree of belief that the  
differences between the methods are not all zero is higher than 0-99 
but is not as great as 0-999. One is quite strongly convinced that 
there are some differences between the five methods. 
Table 6.5.3 
Between 
Within 
Total 
Sum of 
squares 
2386 
2567 
4953 
Degrees of 
freedom 
4 
25 
29 
Mean 
square 
596-5 
102-7 
F 
5-81 
Tests for individual differences 
Having established grounds for thinking there exist differences, 
one must consider what differences there are, and the most  
important of these to look for is obviously that between the 
standard method and the new ones: but which of the new ones ? 
Corollary 5 of §6.4 enables us to see what evidence there is for 
differences between the four new methods. The new F statistic, 
F4, is calculated by the same methods as were used in forming 
table 6.5.3. The within sum of squares is unaltered (in the  
denominator of (6.4.20)) and the between sum of squares is calculated 
as before, but excluding method 1. In place of (3) above we have 
S (S Xij)2ln = 23,320, and for (4), ( 2 x4i)*l(r-l)n = 22,878. 
Thus the between sum of squares, the difference of these, is 442 
with 3 degrees of freedom, giving a mean square (the numerator 
of (6.4.20)) of 147-3, which is only a little larger than the within 
mean square and the F4 value (which would have to reach 2-99 
with 3 and 25 degrees of freedom for significance at 5 %) is 
certainly not significant. The posterior probability that there are 
no differences between the new methods is quite high. It looks, 
therefore, as if the difference suggested by the first test must be 
due to a difference between the standard method and the new 
ones, which themselves, so far as we can see, are equivalent. 
Corollary 4 of §6.4 enables this point to be investigated.  
Consider 0X - \{62 + 63 + 64 + 65) = 6 say: the difference between the 
standard method and the average performance of the new ones. 
108 INFERENCES FOR SEVERAL DISTRIBUTIONS [6.5 
Since 2cf = 1-25, *i.-i(*2. + *3. + *4. + *5.) = 20-125, v2 = 25 
and s2 = 102-7 (within degrees of freedom and mean square), 
the confidence limits for 6 are 
20-125 ± *ia(25) x (102-7 x 1-25/6)* 
with confidence coefficient 1 -a. At 5 % the /-value is 2-06 and 
the limits are (10-6, 29-7). At 0-1 % with a /-value of 3-72 the 
limits are (2-9, 37-3), which exclude the origin, so that the 
difference is significant at 0-1 %. 
The results of the experiment may be summarized as follows: 
(i) There is no evidence of any differences between the four 
new methods. 
(ii) The average effect of the new methods is to reduce the 
impurity as compared with the standard method. The effect is 
estimated to lie most likely (with 95 % confidence) between a 
reduction of 0-0106 and 0-0297, and almost certainly (99-9 % 
confidence) to lie between 0-0029 and 0-0373: the most probable 
value is 0-0201. 
A better way to express this would be to say that this reduc- 
tion d was such that {5_0.0201}/0-00463 
had a posterior distribution which was Student's t on 25 degrees 
of freedom. 
Form of the F-statistic 
The computational method is merely a convenient way of 
arranging the arithmetic in order to obtain the required F-ratio. 
But since the sufficient statistics (the xia and S2) are found in 
doing this, it is suitable for any inference that one wishes to 
make. Let us first look at this F-statistic. In the language of the 
definitions given above it is the ratio of the between mean square 
to the within mean square. An extension of the discussion in 
§6.1 shows that the within mean square, being the ratio of the 
sum of sums of squares for each sample, SSf, and the sum 
of the separate sample degrees of freedom, is a natural estimate 
to replace the unknown <p; and this, whatever be the values of 
the 6im A similar discussion shows that if the null hypothesis is 
true, so that the X{ are independent and normal with the same 
mean and variance <pjn, then 2(jcie —jc..)2, divided by its degrees 
6.5] ANALYSIS OF VARIANCE 109 
of freedom, (r — 1), is an estimate of <pjn. Hence when the null 
hypothesis is true the between mean square, n^(xit — x^)2l(r — 1), 
is also an estimate of ^, and F is the ratio of two such estimates, 
and is therefore about 1. When the null hypothesis is not true 
the between mean square will be larger since the xim will be more 
scattered because the 6{ are. Consequently F will be typically 
greater than 1. This shows that the test, which says that the null 
hypothesis is probably not true when F is large, is sensible on 
intuitive grounds. 
Notice that the sums of squares are all calculated by  
subtracting a squared sum from an uncorrected sum of squares 
(cf. §5.4). Thus the between sum of squares is 
n2i(Xirt — x)2 = n^x\ — nrx2m 
=za:xij)2in-a:xij)2im9 (2) 
and the total sum of squares is 
ZO^-x..)2 = 2xly-(2x^)2/r«. (3) 
It is useful to notice that whenever a quantity in the form of 
a sum is squared, it is always, after squaring, divided by the 
number of x{j that were added up to give the quantity. Thus, in 
(2), (2 xi:})2 is divided by n because 2 % is the sum of n terms. 
i i 
The within sum of squares could be found by calculating each 
Sf and adding, but it is computationally easier to use (1). 
The calculations proceed in essentially the same manner when 
the sample sizes are unequal. In (2) we have to calculate 
2 {(2 XijYIrii}, in agreement with the sentence after equation (3): 
i j 
but otherwise there is no change in the method. Notice, too, 
that the calculations are much simplified by changing origin and 
scale to reduce the magnitudes and avoid the troubles of 
negative values. A desk calculating machine is essential for 
all but the smallest analyses. 
Extended analysis of variance table 
The additivity of the sums of squares can be extended still 
further. We saw in the last section that the F-test was a  
generalization of the f-test, and a /-test could be put in an F form by 
110 INFERENCES FOR SEVERAL DISTRIBUTIONS [6.5 
considering t2 = F. Consider the use just made of the /-statistic 
in order to assess the difference between the standard method 
and the others. The square of the /-statistic is (from corollary 4 
to theorem 6.4.1) ^^ _ ^ )mSc|) 
(with cx = 1, ct = —\,i> 1) and a test of the null hypothesis 
that XcA = ^-i^ + ^ + ^ + ^s) = 0 = 0 will be significant 
niXciXtyisXXcl) > FJl, v2), 
{4 S *y - S (*« + ^ + *« + %)}2/120 
or ~ W *.)■/*. > F°(l>v^ (4) 
The test criterion is therefore still of the form of one mean 
square (of a single term) divided by the within mean square. 
A little algebra also shows that the numerator of the left-hand 
side of (4) (which is also a sum of squares, since the degree of 
freedom is 1) is the difference between the between sum of 
squares in table 6.5.3 and the between sum of squares calculated 
in order to test the null hypothesis 62 = 63 = #4 = 0h. The 
analysis of variance (table 6.5.3) may therefore be extended. 
The sum of squares between the standard and the rest, 1944, is 
the numerator of the left-hand side of (4), and is therefore easily 
calculated. Consequently we have an easier way of calculating 
the sum of squares between the rest; namely by subtraction from 
the between sum of squares (table 6.5.3). Table 6.5.4 provides 
the two F values which are needed for the separate tests. The 
first one has 1 and 25 degrees of freedom for which the upper 
0-1 % value is 13-88 so that it is significant at this level, agreeing 
with our previous computations of the confidence interval using 
the /-distribution. 
Table 6.5.4 
Between 
Within 
Total 
Sums of 
squares 
r Standard 1944 
{ and the rest 
I The rest 442 
2567 
4953 
Degrees of 
freedom 
1 
3 
25 
29 
Mean 
squares 
1944-0 
147-3 
102-7 
F 
18-93 
1-43 
6.5] ANALYSIS OF VARIANCE 111 
This breakdown of the total sum of squares into additive 
parts is capable of important generalizations, some of which 
will be touched on in later sections (§§8.5, 8.6), but it is mainly 
useful as a computational device. The real problem is to express 
concisely the salient features of the joint posterior distribution 
of the A's (equation 6.4.9), and ideally this should lead at least 
to confidence interval statements. These are usually obtained by 
doing preparatory significance tests to see what differences, if 
any, are important and estimating these. The numerical example 
is typical in this respect: only the difference between the standard 
method and the rest was significant and the posterior density 
of that alone, instead of the joint density, needed to be quoted. 
The variance estimation 
It is sometimes useful to consider the variance c£. In virtue of 
corollary 3 to theorem 6.4.1 inferences about it can be made 
using the ^-distribution in the usual way (§5.3). The within sum 
of squares divided by c£ is x2 with v2 degrees of freedom, so that 
X\25) < 2567c*"1 < x2(25). 
At the 5 % level, the upper and lower points are respectively 
13-51 and 41-66, from the Appendix, so that the 95 % confidence 
limits for c£ are (61-6, 190-0): the mean square being 102-7. 
The limits for the standard deviation, in the original units, are 
therefore (0-0079, 0-0138). Notice that there are usually a large 
number of degrees of freedom available for error so that c£ is 
usually tolerably accurately determined. 
If c£ is known, equal to cr2, then theorem 6.4.2 is applicable. 
There is then no need to use the within sum of squares, which 
only serves to estimate c£, since it is already known. Instead the 
ratio of the between sum of squares (not the between mean 
square) to cr2 provides the necessary test statistic, which is 
referred to the ^2-table with (r — 1) degrees of freedom. One way 
of looking at this is to say that when c£ is known, equal to cr2, we 
have effectively an infinity of degrees of freedom for the within 
mean square and we saw (§6.2) that in these circumstances 
vxF{yl9 oo) was the same as a ^-distribution. The other devices 
112 INFERENCES FOR SEVERAL DISTRIBUTIONS [6.5 
used in this section extend to this case with the infinite degrees 
of freedom replacing v2. 
If cr2 is known it is possible to provide a check on the data by 
using corollary 3 to theorem 6.4.1 to provide a test of the null 
hypothesis that c£ = cr2 in the usual way. 
Prior distributions 
In any application of the analysis of variance it is important 
to remember the assumptions about the probability of the  
observations: independence, normality, and constancy of variance. 
But it is also as well to remember the assumptions about the 
prior knowledge, particularly concerning the means. These are 
supposed independent and uniformly distributed, where the 
latter distribution is an approximation for a distribution of the 
type discussed in §5.2. Whilst any 0 may have this distribution 
it is not always reasonable to suppose them independent. For 
example, in the chemical illustration the impurity measure may 
well fluctuate substantially depending on the raw material. This 
fluctuation was minimized here by making up the five batches 
from similar material. Although one might be very vague about 
the average value of the c9's, or about any one of them, because 
of one's vagueness about the raw material, one might well feel 
that any two of the #'s would be close together, precisely because 
the batches had been made from similar material. In the next 
section we shall show how some allowance can be made for this 
in the case where c£ is known, equal to cr2, say. 
6.6. Combination of observations 
Our topic is again that of several independent samples from 
normal distributions, but we no longer consider differences of 
means. In the first two theorems the means are supposed equal 
and the samples are combined to provide an inference about the 
common value. In the third theorem another sort of  
combination is considered. 
Theorem L Let xt = (xil9 xi2, ..., xin%) (i = 1, 2, ..., r) be r in- 
dependent random samples of sizes ntfrom N(6, erf), where the cr4- 
are known but the common mean, 69 is unknown. Then if 6 has 
6.6\ COMBINATION OF OBSERVATIONS 113 
a uniform prior distribution over ( — 00, 00), the posterior  
distribution of 6 is N(x„9 (2w4)-1) where 
x„ = liWiXiJIiWi and wf1 = &\jni. (1) 
Wi is called the weight of the ith sample, and x the weighted mean 
(§5.1). 
The likelihood of 6 from the ith sample is given, apart from 
the unnecessary constant of proportionality, by equation 5.1.9; 
and hence the likelihood from all r samples is the product of 
these. Thus, if x denotes the set of all xij9 and xit = E*#/w<, 
p(x 16) cc exp [ - i^WiiXi. - Of] 
oc expi-^w^+^WiXid] 
oc exp [-J(2w,) (6- 2w,x,./2w,)a]. (2) 
Since n(d) is constant, 7r(d\x) is also proportional to (2), which 
proves the theorem. (An alternative proof is given below.) 
Theorem 2. Let the conditions be as in theorem 1 except that 
cr\ = (j)T% where the r's are known but <fi is not. Then ifln<p has 
a uniform prior distribution over ( — 00, 00) independent of d9 the 
posterior distribution of 6 is such that 
(XwdHe-xjis (3) 
has Student's t-distribution with v = (2^ — 1) degrees of freedom, 
\Ajh0r0 
x„ = XWiXiJXWi, w^1 = r\lnt (4) 
and 
• • • • • 
(5) 
The likelihood now involves <fi which cannot be absorbed into 
the constant of proportionality as in the proof of theorem 1. 
From the normal density for each xy the likelihood is easily 
seen to be 
p(x10, 0) oc0-io+Dexp[-S (x„-&f\2r\ ft, (6) 
• • 
and hence n(d9 0|x) is proportional to the same expression 
multiplied by <jr\ To simplify this, collect together like terms 
in the exponential in 6 and d2 in order to have a single term 
8 LSII 
114 INFERENCES FOR SEVERAL DISTRIBUTIONS [6.6 
involving 6 (compare the argument leading to equation 5.4.4). 
The expression in square brackets is — 1 \24> times 
The terms not involving 62 clearly reduce to vs2. Hence 
n(0, #|x)oc^-^^exp[-{(S^)(e-Jc..)2 + wa}/2^]. (7) 
The result follows from (7) in the same way that theorem 5.4.1 
followed from equation 5.4.4. The alternative expression for s2 
in (5) follows easily (cf. equation 6.5.1). 
In the following theorem we use vector and matrix notation. 
x denotes a column vector, and x' its transpose, a row vector, 
with elements (xl9 x2, ..., xn). Capital letters denote square 
matrices (cf. §§3.3, 3.5). 
Theorem 5. If the random variables x have a multivariate normal 
distribution with expectations $(x) = A8, where A is known but 
8 = (dl9 62, ..., 6p) is not, and known dispersion matrix C; and if 
the prior distribution ofQ is multivariate normal with expectations 
<f(8) = (ji0 and dispersion matrix C0; then the posterior  
distribution of 8 is also multivariate normal with expectations 
\h = {Co"1 + A'C^A}-1 {C0-Vo + A'C-*} (8) 
and dispersion matrix f^, , k,„ , A. , /rk. 
{C0 + A C LA} \ (9) 
From the expression for the normal multivariate density 
(equation 3.5.17), written in the vector notation, we have, since 
the dispersion matrix is known, 
/?(x|8) oc exp{-i(x-A8)' C^x-AO)} (10) 
and tt(8) cc exp{-K<>-J*a)' CfHP-^}. (H) 
Hence 7r(81 x) is proportional to the product of these two  
exponentials. The resulting single exponential we manipulate in the 
6.6\ COMBINATION OF OBSERVATIONS 115 
same way as in the last theorem, discarding into the omitted 
constant any multiplicative terms not involving 6. Thus, 
7t(6 I x) oc exp {- i[6'(C0-1 + A'C^A) 6 - 26'(Q- Vo + A'C^x)]} 
ex exp {- \ [(6 - ft)' (C0-1 + A'C^A) (6 - ft)]}. 
A comparison of this last result with the normal density, (11), 
establishes the result. 
Combinations of inferences 
In calling this section ' combination of observations' we are 
extending the original meaning of this phrase, which referred to 
the results of the type discussed in §3.3, and made no reference 
to degrees of belief. Our excuse for the extension is that the 
earlier results were (and indeed are) used in an inference sense 
which is justified by the above results. In amplification of this 
point consider a simple typical result obtained in §3.3. Ifx1 and 
x2 are two independent, unbiased determinations of an unknown 
fi9 then ^(x1 + x2) is another unbiased determination.  
Furthermore, !(*! + ^2) is typically a more precise determination than 
either xx or x2 separately. Precisely, if 
9\Xi) = cr% @* [K*x + x2)] = i(crf + of). 
However, there are two things that remain to be discussed. 
What inferences can be made about /i given xx and x2; and is 
^(x1 + x2) the best determination ? If the additional assumption 
of normality of x± and x2 is made we shall see how to answer 
both these questions. The inference part is simple since we have 
already seen how a statement about a normal random variable 
can easily be turned into a statement of belief about its mean. 
(Compare the statements (a) and (b) in §5.1.) Thus, in the 
example, /i, given ^(x1 + x2)9 is normally distributed about 
i(xx + x2) with variance \{cr\ + cr|) provided the prior knowledge 
of 11 is vague. In answer to the second question we now show 
that the ordinary average, \{xx + x2)9 is not necessarily the best 
determination of /*. 
Theorem 1 provides an answer to the question: if several 
independent determinations are made of the same unknown 
with different precisions, then how should they be combined 
into a single determination, and what precision has this ? 
8-2 
116 INFERENCES FOR SEVERAL DISTRIBUTIONS [6.6 
First consider any sample. In the language of §3.3 this forms 
a set of rii determinations of an unknown (now denoted by 6) 
with equal precisions 072. (In §5.1 the precision was defined as 
the inverse of the variance.) The results of §5.1 show that, 
because of the normality, the mean is sufficient and forms a 
determination of precision /li/crf, the w{ of the theorem. So much 
is familiar to us. The new point is to discover how the separate 
determinations from the different samples should be combined. 
The answer is to use the weighted mean (§5.1) with the inverses 
of the variances as weights: that is, the precisions of each 
determination. This is intuitively sensible since the larger the 
variance the smaller the precision or weight. 
Relationship with Bayes's theorem 
The result is very similar to the corollary to theorem 5.1.1, 
where a weighted mean was used to combine prior knowledge 
with the determination. In fact, the present theorem can be 
deduced from the corollary in the following way. Consider the 
samples taken one at a time, and apply the corollary each time 
a new sample is taken. Initially, in the notation of the corollary, 
we let crl -> 00; the posterior distribution after the first sample 
being N(xl9 crf/^). Now proceed to the second sample. The 
prior distribution now appropriate; that is, the distribution 
expressing one's present beliefs about 6, is the posterior  
distribution derived from the first sample, N(xlm9 o\\n^). Use of the 
corollary with /i0 = xlm, cr2, = cr\\nx gives a posterior distribution 
after the second sample which is 
N[(w1x1 + w2x2)l(w1 + w2), (w1 + H'a)"1], 
the precisions of the first sample (now the prior precision) and 
the second sample being added, and the means weighted with 
them. Continuing in this way to r samples we have the general 
result. This second proof is interesting because it shows how 
one's beliefs change in a sequence as the new samples become 
available, and how the posterior distribution after one set of 
observations, or samples, becomes the prior distribution for the 
next set of observations. 
6.6] COMBINATION OF OBSERVATIONS 117 
Case of unknown variances 
Notice that in order to make the best determination, namely 
the weighted mean, it is only necessary to know the ratios of the 
weights, or the precisions, or the variances erf. (Their absolute 
values are needed for the precision of the best determination.) 
Theorem 2 is available when these ratios are known but the 
final precision has to be estimated because the absolute values 
are not known. It is often useful when cr\ is known to be 
constant, but there are other situations of frequent occurrence 
where the relative precisions are known but the absolute values 
are not. An example is where two instruments have been used, 
one of which is known to have twice the precision of the other 
when used on the same material, but where the actual 
precision depends on the material. 
The result is best appreciated by noting that it is an  
extension of theorem 1 in the same way that Student's result 
(theorem 5.4.1) is an extension of theorem 5.2.1: namely the 
variance is estimated by a mean square, and the /-distribution 
replaces the normal. Thus if the result of theorem 1 is written, 
(2w*)* (0-x) is N(0, 1), or, in the notation of theorem 2, 
(%Wi)l(d — x..)/$* is N(0, 1), the comparison is obvious except 
for the apparently rather unusual estimate, s, that replaces <fi. 
The form of s can be easily understood by using analysis of 
variance terminology of the last section. We have seen in §6.1 
that several estimates of variance should be combined by adding 
the sums of squares and adding the degrees of freedom, and 
dividing the former by the latter. In the situation of theorem 2 
each sample provides an estimate of variance, the sum of 
squares for the ith sample being 2 (jc^ —jc<.)2. But this sample 
j 
has variance ^rf, so 2 (*# — **. )2/t? is clearly the sum of squares 
appropriate to $. Summing these over the samples gives the 
within sum of squares, the first term in the first form for s2 in (5). 
But some more information is available about <j). The sample 
means xit have variances ^tJ/t^ = $Wi\ and hence x{w\ has 
variance ^. Taking the sum of squares of the xim about the 
weighted mean xmm with weights wt gives Sh^jc*. —x..)2, the 
118 INFERENCES FOR SEVERAL DISTRIBUTIONS [6.6 
remaining term in the first form for s2 in (5). This latter  
expression is a generalization of the idea of the between sum of squares 
to the case where the means have different variances of known 
ratios. The two expressions may be added to give the second 
form for s2 in (5), and the part in square brackets is therefore the 
total sum of squares. The degrees of freedom are 0^-1) for 
each sample; giving 2(^-1) = 2^-r, for the within sum, and 
(r - 1) for the between sum; giving 2^- - r + (r - 1) = 2^ -1 = v, 
in all. This last result is also intuitively reasonable since the 
S/i, readings have lost one degree of freedom for the single 
unknown 6. 
It would be useful to have an extension of theorem 1 to the 
case where the <r\ are unknown, but no simple result is available. 
The situation is complex because r integrations are needed (to 
remove each of the erf), and the distribution depends on many 
parameters. The reader can compare the difficulties with those 
encountered in Behrens's problem (§6.3). There are two things 
one can do: if the cr\ are not likely to be too unequal then one 
can assume them all equal to an unknown <r2, and use theorem 2; 
alternatively, one can estimate cr\ in the usual way by 
s\ = S(^-x,-.)2/(^-l), 
j 
replace cr\ by s\ and ignore any distributional changes that 
might result. The latter is only likely to be satisfactory if all the 
sample sizes are large so that the error in replacing o\ by s\ is 
small. 
Multivariate problems 
The alternative proof of theorem 1, using the corollary to 
theorem 5.1.1, demonstrates a connexion between the  
combination of observations and the combination of prior information 
and likelihood, and therefore between theorem 1 and theorem 3, 
which is an extension of theorem 5.1.1 from one variable to 
several. (In the univariate case we can take A to be the unit 
matrix.) It says that a multivariate normal prior distribution 
combines with a multivariate normal likelihood to give a  
multivariate normal posterior distribution. If A = I, the unit matrix 
6.6] COMBINATION OF OBSERVATIONS 119 
(necessarily p = ri)9 then the inverses of the dispersion matrices 
add (equation (9)) and the posterior mean is an average of the 
two means with weights equal to these inverses. 
Example 1. We consider two applications of theorem 3. The 
first is to the weighing example of § 3.3. In the discussion of that 
example in §3.3 we were concerned only with the variances of 
certain quantities (equations 3.3.2), suggested as determinations 
of the weights, t We now use theorem 3 to show that these 
determinations are the best possible. It is clear that the xt of 
§3.3 have expectations (equation 3.3.1) 
*(*i) = 01 + 02-08-0* (12) 
etc., and that, granted the normality, they satisfy the conditions 
of the theorem with 
(1 1-1-1 
1-1 1-1 
1-1-1 1 
1111 
and C equal to the unit matrix times cr2. If the prior knowledge 
of the true weights, di9 is vague compared with cr2, then Q"1 may 
be ignored and the posterior distribution of the true weights has 
mean (equation (8)) equal to (A'A)_1A'x. An easy calculation 
shows that A'A is four times the unit matrix so that the mean is 
JA'x, the values given in equation 3.3.2. The dispersion matrix 
of the posterior distribution is (A'A)-1cr2: so that the variances 
are all Jcr2 and the covariances zero. Note that, apart from a 
factor 4, A is an orthogonal matrix (A'A = 41), hence the name, 
orthogonal experiment, used in §3.2. 
Example 2. The second example applies to the analysis of 
variance between and within samples discussed in §§6.4, 6.5. 
Suppose the situation to be that described in the statement 
of theorem 6.4.2 with c£ known, equal to cr2, say.  
Without loss of generality we can suppose n = 1 and denote the 
observations by xi9 since, with n > 1, {xim} is sufficient. Then 
the likelihood is as in theorem 3 with A equal to the unit matrix 
of size rxr and C equal to the unit matrix times cr2. In §6.5 we 
t The reader should be careful to avoid confusing the weights put on the 
balance and the statistical weights of the observations. 
(13) 
120 INFERENCES FOR SEVERAL DISTRIBUTIONS [6.6 
made the point that in the prior distribution the 0* were not 
always independent. Suppose that 8 is N(\k09 C0) with all the 
elements of (x0 equal to /i09 say, and with C0 having the special 
form of a matrix with erg down the leading diagonal and perl 
everywhere else: denote such a matrix by ||o"g,/)crg||. Then the 
di have the same prior means /i0 and variances erg, and the 
correlation between any pair is p. In particular, for any 
unequal /, /, 
4 ®\di-d^ = 2<T\{\-p), (14) 
by theorem 3.3.2. This prior distribution is one in which the 6/s 
are all alike in means and variances, and are equally correlated. 
Some algebra is necessary to obtain the mean of the posterior 
distribution, but this is considerably lightened by using the fact 
(which is easily verified) that if the matrices are of size rxr 
|fl,6||-i = \\[a + (r-2)b]c9-bc\\ 
with c-1 = 
[a + (r-l)b](a-b). (15) 
It immediately follows from this result that 
Cr1 
'0 
[l+O- 
2)p]A,-pA[| 
with A-1 = 
[l+(r-l)p](\-p)< 
Now we are interested (as explained in §6.5) in cases where any 
one di is vaguely known but any two are thought to be close 
together. We therefore consider the case where erg -> oo but 
°o(l—/°) ->t2j saY> which is finite: this will mean that p-> 1. 
From (14) it follows that the difference between any two c9/s has 
finite variance, 2r2, but the variance of any c9, tends to infinity. 
In this limit 
Cr1 
'0 
1 
1 
rr 
2 9 
rT 
and 
Co^ + C"1 
rr 
1 1 
O" 
rr' 
Using (15) again, we obtain 
(Co-HC-1)-1 
\rr2 cr2/ rr 
with t1 = 
cr2 + f< 
err2 
6.6\ COMBINATION OF OBSERVATIONS 121 
Since Q"Vo = 0 and C-1x has typical element x4/cr2, we have 
for a typical element of y.l9 the value 
In words, the expected posterior value of 0* is a weighted mean 
of x- and the mean of the x's, x., with weights equal to the  
precision of each xt and twice the precision of the prior difference 
of any two #/s. The effect is to replace the natural value, xi9 for 
the expectation of 6{ by a value shifted toward the common 
mean. The values of 6{ with x{ near x will be little affected, but 
those with the extreme values of xt will be shifted to a greater 
degree towards xm. This is in satisfactory agreement with 
common-sense ideas for it is usual to feel that the extreme 
values should be a little discounted just because they are extreme 
values. For example, in the numerical illustration of §6.5 the 
second method had the lowest mean and was, on the evidence 
of the experiment, the best; but one might not feel happy at 
saying its expected value was 157/6 = 26-2 because it may just 
have done rather well in this experiment: in another experiment 
method 3 might be lower. 
General inference problems 
Theorem 3 does not easily extend to deal with the case of 
unknown cr2 and r2 in example 2 (evidence on these is available 
from the within and between sums of squares respectively). But 
it does show how prior knowledge should be incorporated into 
the data. This is perhaps a convenient point to make the general 
remark that every inference situation should be judged on its 
own. In this book we give recipes that fit a wide class of 
problems but it should not be thought that these are the only 
cases that arise. The best approach to any inference problem is 
to consider the likelihood and the prior distributions separately 
and then to form their product to obtain the posterior  
distribution. If they fit into the framework of a standard method so 
much the better, but if not, one can always try to put the 
posterior distribution into a convenient form and, if all else 
fails, it can always be calculated numerically in any particular case. 
122 INFERENCES FOR SEVERAL DISTRIBUTIONS [6.6 
Suggestions for further reading 
The suggestions given in chapter 5 are relevant. Two more 
advanced books on mathematical statistics are Kendall and 
Stuart (1958, 1961) of which a third volume is promised, and 
Wilks (1962). Another important work is the older book on 
probability and statistics by Cramer (1946). 
Exercises 
1. The table gives the yields of two varieties A and B of wheat grown in 
strictly comparable conditions at each of twelve Canadian testing stations. 
Obtain the posterior distribution of the difference in average yield of the 
two varieties: 
Station... 1 2 3 4 5 6 7 8 9 10 11 12 
A 25 28 28 20 23 23 26 29 26 26 37 31 
B 17 15 21 18 22 14 25 28 25 19 34 25 
(Camb. N.S.) 
2. The following results relate to the yields of two varieties of tomato 
grown in pairs in nine different positions in a greenhouse: 
 
Position, 
• 
i 
1 
2 
3 
4 
5 
Variety 
x, 
Xi 
1-39 
1-41 
1-07 
1-65 
1-57 
Variety 
Y, 
yt 
1-17 
1-22 
108 
1-42 
1-39 
 
Position, 
i 
6 
7 
8 
9 
Variety 
X, 
Xi 
1-63 
0-88 
114 
1-32 
Variety 
Y, 
yt 
1-22 
0-84 
0-94 
1-25 
(S*«= 12-06,2*= 10-53,E*? = 16-7258,Stf = 12-6123, S*^, = 14-4768.) 
Give 99 % confidence limits for the difference between the average 
yields of X and Y. 
Discuss whether the arrangement of the experiment in pairs has been 
advantageous. (Camb. N.S.) 
3. A new wages structure is introduced throughout a certain industry. 
The following values of output per man-hour are obtained from a number 
of factories just before and just after its introduction: 
Output per man-hour 
<■ 
Old wages 
structure 
122-3 
1410 
1201 
109-6 
1191 
128-0 
132-8 
% 
New wages 
structure 
1190 
131-8 
118-7 
1060 
118-2 
126-6 
124-1 
EXERCISES 
123 
If £ is the average change in output per man-hour over all factories in the 
industry, consequent upon the change in wages structure, calculate, on 
conventional assumptions, 95 % confidence limits for £ in each of the 
following cases: 
Case I. The data refer to seven different factories, corresponding figures 
in the two series coming from the same factory. 
* Case II. The data refer to fourteen different factories. 
Suppose in case II that further values were obtained, making up n  
factories in each column instead of 7. Roughly how large would n have to be 
for one to be reasonably sure of detecting that £ was not zero, if in fact 
£ was equal to —2-0 units? (Manch. Dip.) 
4. Two normal populations of known equal variance a% have means 
0l9 62. It is required to calculate, from random samples of size n taken 
from each population, a 100a % confidence interval for A = 62 — 6lm The 
following procedure is suggested: 100a % confidence intervals are  
calculated separately for 0X and 62 and are, say, (ll9 ux) and (/2, u2). Any value 
in (ll9 «x) is, at this probability level, a possible value for 0l9 and similarly 
for 02. Hence any value in the interval (l2 — ul9 u2 — lj) is a possible value 
for A at confidence level 100a% and is the confidence interval required. 
Explain why this argument is false and compare the answer it gives with 
that obtained from the correct procedure. (Lond. B.Sc.) 
5. It is expected that a certain procedure for estimating the percentage 
amount of a substance X in a sample of material will give a standard error 
approximately equal to 0-12. Two chemists take samples from a given bulk 
of material and each performs five independent determinations of the 
percentage amount of X present in the material. Their readings are as 
follows: First chemist: 243, 242, 2-22, 2-29, 2-06 
Second chemist: 248, 2-72, 243, 240, 2.58 
Do these figures support the assumption that the standard error of a 
determination is 012? Is there any evidence that one chemist's readings 
are biased relative to the other ? (Camb. N.S.) 
6. Two workers carried out an experiment to compare the repeatabilities 
with which they used a certain chemical technique. The first worker made 
eight determinations on the test substance and estimated the standard 
deviation of a single measurement as 0-74; the second worker made fifteen 
determinations on the same substance and obtained a standard deviation 
of 1*28. Compare the repeatabilities of the two workers' measurements. 
(Camb. N.S.) 
7. Two analysts each made six micro-analytical determinations of the 
carbon content of a chemical, giving the values: 
Analyst I: 5909, 5917, 59-27, 59-13, 59-10, 5914 
Analyst II: 5906, 5940, 5900, 59-12, 59-01, 59-25 
Discuss whether these results provide evidence that the two analysts 
differ in their accuracy of determination. (Camb. N.S.) 
124 INFERENCES FOR SEVERAL DISTRIBUTIONS 
8. Two methods, I and II, are available for determining the sulphur 
dioxide content of a soft drink. The variances associated with single 
determinations are cr\ and o% respectively, but it is usual to use several 
independent determinations with one of the two methods. Show that 
if I costs c times as much to carry out as II per determination, then I is 
only to be preferred if 9, 9 
oa/oi > c 
Thirteen determinations were made using method I on one drink and 
thirteen using method II with another drink. The sample variances were 
equal. Show that, on the basis of this information, in order to be able to 
make a fairly confident decision about which method to use, one method 
would have to be at least 3-28 times as costly as the other. 
[Interpret 'fairly confident' as a 95 % level of probability.] 
(Wales Maths.) 
9. A random sample xl9 x2, ..., xm is available from N(6l9 <f>) and a second 
independent random sample yl9 y29 ..., yn from N(62, 2<f>). Obtain, under 
the usual assumptions, the posterior distribution of #i—#2, and of <fi. 
10. Use the result of exercise 5 in chapter 2 to show that tables of the 
F-distribution can be used to obtain tables of the binomial distribution. 
Specifically, in an obvious notation show that if s is B(n9 p) then 
j* > r) = p[FQ0-r+1), 2r) > ^^]. 
11. The following table gives the values of the cephalic index found in 
two random samples of skulls, one consisting of fifteen and the other of 
thirteen individuals: 
Sample I: 74-1, 77-7, 74-4, 74-0, 73-8, 79-3, 75-8, 
82-8, 72-2, 75-2, 78-2, 77-1, 78-4, 76-3, 76-8 
Sample II: 70-8, 74-9, 74-2, 70-4, 69-2, 72-2, 76-8, 
72-4, 77-4, 78-1, 72-8, 74-3, 74-7 
If it is known that the distribution of cephalic indices for a homogeneous 
population is normal, test the following points: 
(a) Is the observed variation in the first sample consistent with the 
hypothesis that the standard deviation in the population from which it has 
been drawn is 3-0? 
(b) Is it probable that the second sample has come from a population in 
which the mean cephalic index is 720? 
(c) Use a more sensitive test for (b) if it is known that the two samples 
are obtained from populations having the same but unknown variance. 
(d) Obtain the 90 % confidence limits for the ratio of the variances of 
the populations from which the two samples are derived. (Leic. Gen.) 
12. The table gives the time in seconds for each of six rats (A, B, ..., F) to 
run a maze successfully in each of four trials (1, 2, 3, 4). Perform an 
EXERCISES 
125 
analysis of variance to determine whether the rats give evidence of differing 
in their ability. 
A 
1 15 
2 10 
3 13 
4 14 
B 
18 
15 
16 
16 
C 
11 
11 
13 
11 
D 
18 
22 
17 
16 
E F 
19 14 
14 16 
16 16 
14 15 
(Camb. N.S.) 
13. Twelve tomato plants are planted in similar pots and are grown under 
similar conditions except that they are divided into three groups A, B, C of 
four pots each, the mixture of soil and fertilizer being the same within each 
group but the groups differing in the type of fertilizer. The yields (in fruit 
weight given to the nearest pound) are tabulated below. Decide by 
carrying out an analysis of variance (or otherwise) whether the evidence 
that the fertilizers are of different value is conclusive or not. 
Groups 
A 
B 
C 
r 
3 
6 
9 
Yield of plants 
3 4 
8 8 
12 13 
N 
6 
10 
14 
(Camb. N.S.) 
14. An experiment was carried out to investigate the efficacies of four 
compounds; two, Pl9 P2, based on one antibiotic and two, Sl9 S2, based on 
another antibiotic. A control C was also used. The compounds were each 
used five times in a suitable arrangement to ensure independence, with the 
following results: 
c 
5-48 
4-42 
4-97 
3-28 
4-50 
Pi 
4-79 
7-32 
5-80 
6-52 
611 
P2 
716 
6-78 
7-05 
9-21 
7-36 
Si 
4-28 
2-92 
3-97 
5-07 
3-72 
S2 
5-74 
6-80 
604 
6-93 
6-50 
Find the posterior distributions of the following quantities: 
(1) the difference between the mean of Px and P2 and the control; 
(2) the difference between the mean of Sx and S2 and the control. 
Is there any evidence that the P compounds differ from the S  
compounds? Or that the two P compounds are more different amongst 
themselves than the two S compounds? 
15. Four independent determinations are made of a physical constant 
using apparatuses of known standard deviations. The means and standard 
deviations of the posterior distributions associated with the four  
determinations are, in the form ft ± a: 
1001010, 1-211008, 1-271011, llliO-06. 
Find the most probable value for the constant, and its standard 
deviation. (Camb. N.S.) 
126 INFERENCES FOR SEVERAL DISTRIBUTIONS 
16. An apparatus for determining g, the acceleration due to gravity, has 
standard deviation ax when used at one place and cr2 at another. It is 
proposed to make TV determinations—some at one place, some at the 
other—to find the difference between g at the two places. Determine the 
optimum allocation of the determinations between the two places in order 
to minimize the posterior variance of the difference. 
If a\ = 0-0625, o| = 0-1250 find how large the observed mean difference 
will have to be before a test of the hypothesis that the real difference is zero 
is significant at the 5 % level. How large must TV be if the real difference 
is 0-25 and we wish to have a 90 % chance of the above test yielding a 
significant result? 
17. In an experiment to determine an unknown value 0 for a substance, 
independent measurements are made on the substance at times tl9t2, ..., tn. 
A measurement made at time t has mean de~Kt9 where k > 0 is a known 
constant, and has known constant coefficient of variation v. Determine the 
posterior distribution of 0 under the assumption that the measurements 
are normally distributed. 
18. In the experiment discussed in detail in § 6.5 a possible prior distribution 
might be one in which the four methods were thought unlikely to differ 
much among themselves but might differ substantially from the control 
(in either direction). On the lines of the example discussed in §6.6 put this 
prior distribution into a suitable multivariate normal form and, assuming 
the common sample variance known, obtain algebraic expressions for the 
means of the posterior distributions for the five methods analogous to those 
of equation 6.6.16. 
19. A random sample xl9 x2, ..., xm from N(0l9 62) has sufficient statistics 
x = YiXilm and Sl= Sfe—x)2. 
It is proposed to take a second independent random sample of size n from 
the same distribution and to calculate from the values yl9y29 ..., >>„ the 
two statistics 
y = Xyjn and S*y = Eto-jD1. 
Show that p(y, 5*|jc, S% = jp(y\x, 0Jp(Si\6Jir(6%\Sftd9t9 
and hence that 
p{Sl\x9 St) = p(S2y\Sl) = jp(Sl\6,)7T(62\Sl)dd2. 
Using the known expressions for p{Sl\d2) (exercise 5.20) and 7t(62\S%) 
(equation 5.3.4) evaluate this integral and show that p(Sl\x9 Si) is such 
(m-\)Sl 
(n-\)Sl 
has an F-distribution on (n— 1) and (m— 1) degrees of freedom. 
EXERCISES 
127 
The posterior distribution of dx based on the combined sample of 
m + n observations will, if m + n is large (so that the /-distribution is almost 
normal) and the little information about 02 provided by the difference 
between x and y is ignored, be approximately normal with mean 
(mx + ny)/(m + n) and variance (Si + Sl)/(m + n - 2) (m + ri). Show how to 
use the above result and tables of the -F-distribution to determine the value 
of n to be taken for the second sample in order to be 90 % certain that the 
95 % confidence interval for 0X obtained by this approximation has width 
less than some pre-assigned number c. 
Obtain also p(y\x, Si). 
20. Two independent Poisson processes are each observed for a unit time. 
m and n incidents are observed respectively and the unknown rates (the 
Poisson means) are 0X and 02 respectively. If 0X and 02 have independent 
uniform prior distributions over the positive real line show that the 
posterior distribution of ^ = 01/(61 + 02) has density 
(W + n+l)! 
mini 
Show that the conditional distribution of m, given m + n, depends only 
on ft. 
A statistician who wishes to make inferences about ft (but not about 
0X and 62 separately) decides to use this last fact to avoid consideration of 
#x and 02. Show that if he assumes ft to have a uniform prior distribution 
in (0, 1) then he will obtain a posterior distribution identical to (*). Does 
this mean that m + n gives no information about ft! 
21. A complex organic compound contains a chemical which decays 
when the compound is removed from living tissue, lip is the proportion 
of the chemical present at the time of removal (t = 0), the proportion 
present at time t \spe~Xt where A (> 0) is known. To determine/? a quantity 
of the compound is removed and independent, unbiased estimates are 
made of the proportions present at times t = rr (r = 0, 1, 2, ..., A0 in 
individual experiments each of which is of duration r, the estimates having 
variances Ke~XrT (i.e. proportional to the proportion present at the  
beginning of each experiment). Determine the best estimate of p and its 
variance when N is large. 
A second method of determination is suggested in which each  
experiment is twice as quick (i.e. r is reduced to \r) but with each variance 
increased by 50%. Show that this second method is only better if 
A < r-1ln4. (Camb. N.S.) 
22. The random variables Yl9 ..., Yn are independently distributed with 
*( Yd = fixi9 where known. 
How would you estimate ft if the errors Y{—fixi are independently 
rectangularly distributed over ( — 0, 6)9 where 6 is unknown? 
(Lond. M.Sc.) 
128 
7 
APPROXIMATE METHODS 
The results obtained for normal distributions in the two previous 
chapters were exact for the specified prior distribution. In 
dealing with samples for distributions other than normal it is 
often necessary to resort to approximations to the posterior 
density even when the prior distribution is exact. In this chapter 
we shall mainly be concerned with binomial, multinomial and 
Poisson distributions, but begin by describing an approximate 
method of wide applicability. 
7.1. The method of maximum likelihood 
Let x be any observation with likelihood function p(x\6) 
depending on a single real parameter d. The value of 69 denoted 
by #(x), or simply 09 for which the likelihood for that  
observation is a maximum is called the maximum likelihood estimate of d. 
Notice that 0 is a function of the sample values only: it is an 
example of what we have previously called a statistic (§5.5). 
The definition generalizes to a likelihood depending on several 
parameters ^(xl^, 629 ..., 6S): the set {0l9 029 •••> &8) f°r which 
the likelihood is a maximum form the set of maximum  
likelihood estimates, 6U of 0t (i = 1,2,..., s). The estimate is  
particularly important in the special case where x = (xl9 x29 ..., xn) 
is a random sample of size n from a distribution with density 
f{Xi\d). Then n 
p(x\0) = IlAxi\0). (I) 
In this case the logarithm of the likelihood can be written as 
a sum: n 
L(x 16) = ln/>(x 10) = 2 ln/(jc, 16). (2) 
i=i 
Important properties of the log-likelihood, L(x\6)9 can be 
deduced from the strong law of large numbers (theorem 3.6.3) 
in the following way. In saying that x is a random sample we 
7.1] THE METHOD OF MAXIMUM LIKELIHOOD 129 
imply that the sample values have the same density f{Xi \ 6) for 
some 6 fixed, for all i. Denote this value of 6 by 60. We refer to 
it as the true value of 6. It is, of course, unknown. Then, for 
each 6, the quantities, ln/(x4|#), are independent random  
variables with a common distribution, depending on 60, and, by the 
strong law, their mean converges strongly to their common 
expectation. By definition this expectation is 
<f0{ln/(x, 16)} = Jln/(x, 16). f(Xi 160) dxi9 (3) 
where the suffix has been added to the expectation sign to 
indicate the true value, 60, of 6. Hence the law says that with 
probability one 
lim {n^Lix\6)} = *0{ln/(jc,10)}. (4) 
n-»oo 
Similarly, provided the derivatives and expectations exist, 
lim {n-1 ^L(x10)180*} = e$rln/(jc,10)180*}. (5) 
n->oo 
Equation (4) may be expressed in words by saying that, for 
large n9 the log-likelihood behaves like a constant times n9 where 
the constant is the expectation in (3). Similar results apply in 
the case of several parameters. 
Theorem L If a random sample of size n is taken from-f(xt\d) 
then, provided the prior density, n(d)9 nowhere vanishes, the 
posterior density of 6 is, for large n, approximately normal with 
mean equal to the maximum likelihood estimate and variance, cr\, 
given by\ 
<T~2 = -d*L(x\0)ldd*. (6) 
It is not possible to give a rigorous proof of this theorem at the 
mathematical level of this book. The following 'proof should 
convince most readers of the reasonableness of the result. 
The posterior density is proportional to 
exp{L(x|0) + ln7r(0)}, 
and since we have seen that L(x \ 6) increases like n, it will  
ultimately, as n -> oo, dwarf lnn(d) which does not change with n. 
Hence the density is, apart from a constant, approximately 
exp{L(x|0)} = exp{L(x|^) + i(^-^)2^x|^)/^2 + i?}, 
A A 
t d2L(x\6)/d62 denotes the second derivative with respect to 6 evaluated at 6. 
9 
LSII 
130 APPROXIMATE METHODS [7.1 
on expanding L(x\6) by Taylor's theorem about 09 where R is 
a remainder term. Since the likelihood, and hence the log- 
likelihood, has a maximum at 0 the first derivative vanishes there. 
Also the second derivative will be negative there and may  
therefore be written — cr~2. Furthermore, since it does not involve 0, 
the first term may be incorporated into the omitted constant of 
proportionality and we are left with 
exp{-K0-#)2K + i?}. (7) 
From the discussions of the normal density in §2.5 it is clear 
that the term exp { - %(d - 0)2/<r*} is negligible if 16 - 0\ > 3<rn; so 
that since cr~2 is, by (5), of order n, this term is only appreciable 
if 6 differs from 6 by something of the order of w-*. In that case 
the remainder term, i?, which may be written 
for some 0l9 is of order n~% times n (by (5)). Hence it is of order 
n~% and is negligible compared with the other term in (7). 
Hence, inserting the constant of proportionality, the posterior 
density is approximately 
(2™-J-*exp{-i(0-#)2K}, (8) 
which establishes the result. Notice that cr2 is, under the  
assumptions made here, of order rr1. 
Theorem 2. If a random sample of size n is taken from 
f(Xi\dl9 02, ..., 08) 
then, provided the joint prior density, 7r(6l9 62, ..., 6S) nowhere 
vanishes, the joint posterior density is, for large n, approximately 
multivariate normal with means equal to the maximum likelihood 
estimates @i and a dispersion matrix whose inverse has typical 
element _ ^^ | ^ ^ ^ ^^ ^ (9) 
This is the extension of theorem 1 to the case of several  
parameters. The proof proceeds as in that case. The important terms 
in the Taylor series expansion of L(x\6l9 62, ..., 6S) are 
\ S (0i - b) (0, - 00) d*L(x 1$l9 029 ..., falM* 86 j9 (10) 
7.1] THE METHOD OF MAXIMUM LIKELIHOOD 131 
and a comparison with the multivariate normal density  
(equation 3.5.17) establishes the result. 
The matrix, whose typical element is given by (9), will be 
called the information matrix. It is the inverse of the dispersion 
matrix of the posterior density (compare the definition of 
precision in §5.1). Similarly (6) is called the information. 
General remarks 
Although known to earlier writers, the method of maximum 
likelihood has only become widely used through the work of 
R. A. Fisher, who obtained its principal properties. The main 
advantages of the method are that it produces a description 
of the posterior distribution which, because it is normal, is 
easy to handle, and which has a particularly simple mean and 
variance. (We shall see below that these are easy to compute.) 
Fisher used the method to provide a point estimate of 6. We 
shall not have much to say in this book about the problem of 
point estimation; by which is usually meant the problem of 
finding a single statistic which is, in some sense, near to the true 
value of a parameter (see §5.2); our reason for not doing so is 
that posterior distributions cannot be adequately described by 
one statistic, f But the problem is much discussed by some 
statisticians and, in large samples, is adequately solved by the 
maximum likelihood estimate, though other approximations are 
available. There is, as we shall see below, a close relationship 
between @2(@) and c\ above: so that 0 and its variance do 
provide, because of the normality, an adequate description, in 
large samples, of the posterior density. 
In the definition of 0 the word 'maximum' is used in the sense 
of ' largest value': that is, L(x \ 6) ^ L(x 10) for all 6. The  
estimate is therefore not necessarily determined by equating the 
first derivative to zero. This latter process will only yield the 
local maxima (or minima). An example where this process is 
inapplicable is provided by the uniform distribution discussed 
below. (In the 'proof it has been assumed that the first  
derivative is zero at the maximum.) 
t If we wished to use a single statistic we could take the mean of the posterior 
distribution. But this would not be helpful without, in addition, at least the 
variance. 
9-2 
132 APPROXIMATE METHODS [7.1 
Notice that the method has little or nothing to recommend it 
in small samples. There are two reasons for this. First, the 
posterior distribution is not necessarily normal. Secondly, the 
prior distribution is relevant in small samples because the  
information provided by it may be comparable with the information 
provided by the sample, and any method based on the likelihood 
alone may be misleading. We have mentioned the diminishing 
importance of the prior distribution as the sample size increases 
in connexion with the normal distribution (§§5.1, 5.2) but the 
point is quite general. Of course, if the prior knowledge is very 
vague (as in theorem 5.2.1), even a small sample may contain 
virtually all the information. Notice that, in the statement of 
the theorems, it has been necessary to assume that the prior 
density nowhere vanishes. If it did vanish near dl9 say, then no 
amount of evidence would ever convince one that 6 was near d1 
(cf. §5.2) and the posterior density would vanish near 61 even if 
0 = 0V (In the proof Xnnid) would be minus infinity, and 
certainly not negligible.) 
Example: normal distribution 
We begin with situations already studied, in order to see how 
the approximation compares with the exact result. In the case 
of the normal mean (§§5.1, 5.2) with known variance the  
likelihood is given by equation 5.1.9. The log-likelihood is therefore 
L(x\d) = C-%(x-d)2(nl<r2\ 
where C is a constant. Differentiating and equating to zero 
gives the result (x — 6) {njcr2) = 0, so that 0 = x. A second 
differentiation gives 
or~2 = -d2L(x\$)ldd2 = (w/cr2), 
so that the posterior density is approximately N(x9 o-2jn), which 
agrees with theorem 5.2.1, and is exact (corollary to theorem 
5.1.1) if the prior distribution of 6 is uniform over the real line. 
If the variance is also unknown (§5.4) then the logarithm of 
the likelihood is, from equation 5.4.3 rearranged in the same 
way as the posterior distribution was rearranged to obtain 5.4.4, 
C-%nlnd2- {vs2 + n(x - d^2}l2d2. 
7.1] THE METHOD OF MAXIMUM LIKELIHOOD 133 
To obtain the maximum likelihood estimates we differentiate 
partially with respect to 61 and 62 and equate to zero. The 
results are 
n(x-6j)l$a = 0, 
8L 
dd1 
8L 
B02 
%nl$2 + {vs2 + n(x- 0^)1201 = 0, 
(11) 
so that 01 = x and 62 = vs2\n = 2(x4 —x)2/w. (12) 
The matrix whose elements are minus the second derivatives of 
the log-likelihood at the maximum (the information matrix of 
(9)) is easily seen to be , a_x ft v 
Co WW* (13) 
with inverse ( f JJJ. (14) 
The posterior distribution of 61 is thus approximately N(@l9 02jn), 
or, from (12), ni{d1-x)jsiyln)^ is approximately iV(0, 1). This 
is in agreement with the exact result (theorem 5.4.1) that 
ni(61 — x)ls has Student's distribution, since this distribution 
tends to normality as n -> oo (§5.4), and v\n-> 1. The  
distribution of 62 is approximately N($29 lB\\rt). This agrees with the 
exact result (theorem 5.4.2) that vs*\d2 is x2 with v degrees of 
freedom, because the mean and variance of 62 are s2 and 2s*jv 
(equations 5.3.5 and 6) in large samples and the distribution of 
d2 tends to normality. This last result was proved in §5.3. 
Finally we note that the covariance between 61 and 62 is zero, 
which is in exact agreement with the result obtained in §5.4. 
Example: binomial distribution 
Consider a random sequence of n trials with constant  
probability 6 of success. If r of the trials result in a success the 
likelihood is (cf. equation 5.5.9) 
0r(l-0)*-*. (15) 
The derivative of the log-likelihood is therefore 
rl6-(n-r)l(l-0), 
134 APPROXIMATE METHODS [7.1 
so that 0 = r/n9 the proportion of successes in the n trials. The 
second derivative is 
-rld*-(n-r)l{\-d)\ 
which gives o\ = r(n — r)/«3. These results agree with the exact 
results of §7.2. 
Example: exponential family 
The method of maximum likelihood is easily applied to any 
member of the exponential family. In the case of a single  
sufficient statistic for a single parameter the density is (equation 
5.5.5) 
f[Xi\d) = F(xi)G(6)eu^^ 
and the log-likelihood is, apart from a constant, 
n 
where g(6) = lnG(^) and t(x) = 2 u(xt). The posterior density 
i=i 
of 6 is therefore approximately normal with mean equal to the 
r00t0f ng'(d) + t(xmd) = 0 (16) 
and variance equal to 
i-ngrw-wm-1 
evaluated at that root. Similar results apply in the case of 
several sufficient statistics and parameters. 
Solution of maximum likelihood equation 
The equation for the maximum of the likelihood, 
dL(x\6)/dd = 0, 
or, in the case of the exponential family, (16) above, may not be 
solvable in terms of elementary functions. However, there is an 
elegant numerical method of solving the equation, in the course 
of which cr2n is also obtained. This is Newton's method of 
solving an equation. A reference to fig. 7.1.1 will explain the 
idea. On a graph of the derivative of the log-likelihood against 
6 a tangent to the graph is drawn at a first approximation, 6a\ 
7.1] THE METHOD OF MAXIMUM LIKELIHOOD 135 
to the value of 0. The tangent intersects the #-axis at a second 
approximation #(2) which is typically nearer to 0 than 6(1) is, and, 
in any case, may be used in place of #(1) to repeat the process, 
obtaining #(3), and so on. The sequence {6(i)} usually converges 
to 6. Algebraically the method may be expressed by expanding 
in a Taylor series 
8L(x\0)ld6 = 0 = dL(x\dV)ld0 + ($-dV)d2L(x\d®)ldd2+... 
Fig. 7.1.1. Newton's method for solution of the maximum likelihood equation. 
and retaining only the first two terms. The root of the equation 
for 0 so obtained is 6(2\ that is 
0©>_0O) = {dL(x\dV)ldd}l{-d2L(x\d®)ldd2}. (17) 
It is not necessary to recalculate the second derivative at each 
approximation: the method will still work with a single value 
retained throughout. A final recalculation may be advisable 
at the termination of the process when 0 has been obtained 
to sufficient accuracy (that is when 0W —fl^-D is negligible) in 
order to obtain a better value for cr\ = {-d2L(x\6^)ldd2}. 
The method is equally convenient when several parameters 
are involved. The Taylor series expansion gives 
dL(x | e&>)/a0, = 2 (0f> - 0f>) { - d2L(x 10N)/00< 80,}, (18) 
136 APPROXIMATE METHODS [7.1 
where 8(1) = (6^\ 6(P, ..., 6f\ a set of linear equations for 
0(2) _ Q(i) ti^ matrix which has to be inverted is the  
information matrix at argument 8(1) instead of at argument 6. At the 
final approximation, 8(r), this has to be inverted in any case in 
order to obtain the dispersion matrix. Thus the method is well 
suited, not only to the evaluation of the means, but also to the 
evaluation of the dispersion matrix, of the posterior  
distribution. Numerical methods for the inversion of matrices are given 
in §8.4. 
Example 
As an illustration of Newton's method consider random 
samples from a T-distribution, or equivalently a ^-distribution, 
with both the index and the parameter unknown. The density 
for a single observation is (equation 2.3.7) 
Ax<\ el9 e2) = —^ e-*^4>-\ (19) 
where we have written 61 for A and 62 for n. The likelihood for 
a random sample of size n is thus 
vrhvi exP " 0i 2 x< + 02 -1) 2 In*, . (2°) 
\U2~-l)1) I 1=1 1=1 J 
This shows that the distribution belongs to the exponential 
family and that x = Zxt-/w and y = 2 InXijn are jointly sufficient 
statistics. Differentiation of the log-likelihood gives 
8 
80^ 
n{-x + 6j6d = 0, 
•\ 
8 „(ln^+>?-^-In(^-l)! 
. ..p^., de 
0. 
(21) 
2 V "w2 
The first of these equations is sufficiently simple to enable $± to 
be eliminated and a single equation, 
ln02-lnx+y-^-ln(02-l)\ = 0, (22) 
au2 
for 02 to be solved by Newton's method. The derivative of the 
left-hand side of (22) is ^-(d^ddDln^- 1)! and tables of the 
derivatives of the logarithm of the factorial function (see, for 
7.1] THE METHOD OF MAXIMUM LIKELIHOOD 137 
example, Davis (1933)) enable the calculations to be carried out. 
It is necessary, however, to start with a first approximation. 
Here this is most easily obtained by using the approximationf 
to d\n(02-\)\jdd2 of InB2-l/2#2, which, on insertion in (22), 
gives a value of d2 equal to {2(lnx-j>)}-1 to use as 0$> in the 
iteration. The approximation is remarkably good for all values 
of d2 except those near zero, so that except in that case, a single 
stage of Newton's procedure should be sufficient. We leave the 
reader to verify that the dispersion matrix is the matrix 
/jjln(*2-l)! *i\ (23) 
\ kx 6J6\J 
with each element divided by w#f2{#2d?2ln(#2- \)\ldd\-1}. 
These results might be of value if one wished to investigate 
whether observed incidents were occurring in a Poisson process 
(§2.3). It might be reasonable to suppose the intervals between 
successive incidents to be independent (for example if the  
incidents were failures of a component which was immediately 
replaced by a new one when it failed, §4.4), with a distribution 
of T-type. The Poisson process is the case 62 = 1 (theorem 2.3.2), 
so one could perform an approximate significance test of the 
null hypothesis that 02 = 1 by remarking that the posterior 
distribution of d2 is approximately normal with mean 02 and 
variance {n{d2\n{02—\)\ldd\ — 021)}-1 = cr2, say. The  
approximation to d2ln(d2-l)\lddl of (l/#2) +(1/2#1), obtained from the 
above approximation to the first derivative by another  
differentiation of Stirling's formula, shows that cr2 is approximately 
2@ljn. The result will therefore be significant at the 5 % level if 
62 exceeds 1 + 2crn. Notice that, in agreement with the general 
result, cr2 is of the order rr\ 
Choice of parameter 
In the method of maximum likelihood there is no distinction 
between the estimation of 6 and the estimation of a function 
of 6, 0(0). We have the obvious relation that $ = <p(@). The 
t This may be obtained by taking logarithms and differentiating both sides of 
Stirling's formula, equation 4.4.15. 
138 APPROXIMATE METHODS [7.1 
variance of <p9 {— d2L(x \ $)ld<j>2}~1, may also be related to the  
variance of 6 in the following way. Write L for the log-likelihood in 
order to simplify the notation. Then 
d<f> " dd df> e<f>2" dd2 \d<f>) +dd d<f>29 
so that, since dLjdd = 0 at #, the second equation gives 
^|x) = (f)2^ix)' (24) 
where the derivative is evaluated at the maximum likelihood 
value. These results may also be obtained from theorem 3.4.1 
since the variances are small, being of order rr\ Thus in 
changing from 6 to <j) the means and variances change in the 
usual approximate way. Since the method does not distinguish 
between 6 and ^, both parameters have an approximately 
normal distribution. At first glance this appears incorrect since 
if 6 is normal then, in general, $ will not be normal. But it 
must be remembered that these results are only limiting ones as 
n->oo and both the distributions of 6 and $ can, and indeed do, 
tend to normality. What will distinguish 6 from ^ in this respect 
will be the rapidity of approach to normality: $ may be normal 
to an adequate approximation for smaller n than is the case 
with 6. It often pays, therefore, to consider whether some  
transform of 6 is likely to be more nearly normal than 6 itself and, 
if so, to work in terms of it. Of course, there is some transform 
of 0 which is exactly normal since any (sufficiently respectable) 
distribution can be transformed into any other (compare the 
argument used in §3.5 for obtaining random samples from any 
distribution), but this transform will involve the exact posterior 
distribution and since the point of the approximation is to 
provide a simple result this is not useful. What is useful is to take 
a simple transformation which results in a more nearly normal 
distribution than is obtained with the untransformed  
parameter. No general results seem available here, but an example 
is provided by the variance 62 of a normal distribution just  
discussed. The distribution of 62, as we saw in §5.3, has a longer 
tail to the right (large d2) than to the left (small #2). This suggests 
considering ln#2 which might remove the effect of the long tail. 
7.1] THE METHOD OF MAXIMUM LIKELIHOOD 139 
Detailed calculations show that the posterior distribution of 
In #2 is more nearly normal than that of #2, though even better 
transformations are available. The approximate mean and  
variance of the distribution of ln#2 may be found either by maximum 
likelihood directly, equation (9), or from the results for d29 
equation (14), combined with equation (24). Other examples 
will occur in later sections. 
Distribution of the maximum likelihood estimate 
We saw in §5.1 that when making inferences that involved 
using the mean of a normal distribution there were two distinct 
results that could be confused (statements (a) and (b) of that 
section). A similar situation obtains here because of the 
approximate normality of the posterior distribution. The two 
statements are: 
(a) the maximum likelihood estimate, 09 is approximately 
normally distributed about 60 with variance the inverse of 
WJ = *o{-PI&\WW*}; (25) 
(b) the parameter 6 is approximately normally distributed 
about 0 with variance c\. 
(6Q is the true, unknown, fixed value of 6 as explained before, 
equation (3).) Statement (b) is the result of theorem 1,  
statement (a) can be proved in essentially the same way as that theorem 
was proved. Statement (a) is a result, in frequency  
probability, about a statistic, 0\ (b) is a result, in degrees of belief, 
about a parameter #. In practice (a) and (b) can be confused, 
as explained in §5.1, without harm. Actually (a) is rarely used 
in the form given, since 60 is unknown and yet occurs in the 
variance (equation (25)). Consequently, (25) is usually replaced 
by IjO). This still differs from cr~2 because of the expectation! 
used in (25) but not in the expression for cr~2. It is interesting 
to note that the use of the expectation makes no difference in 
random samples of fixed size from an exponential family: there 
In(§) = or-2. (See equation (16) and the one immediately 
following.) 
t Those who have read the relevant paragraph in §5.6 will notice that the use 
of the expectation violates the likelihood principle, and is, on this score,  
unsatisfactory. 
140 APPROXIMATE METHODS [7.1 
Exceptional cases 
It is necessary to say a few words about the question of rigour 
in the proofs of the theorems. A complete proof with all the 
necessary details is only possible when certain assumptions are 
made about the likelihood: for example, assumptions about the 
existence and continuity of derivatives and their expectations. 
These assumptions are not always satisfied and the theorem is 
not always true; the most common difficulty arises when the 
range of possible values of xt depends on 6. The difficulty is the 
same as that encountered in discussing sufficiency in §5.5 and 
the same example as was used there suffices to demonstrate the 
point here. If A^d) = 9-, <p < * < *), 
and is otherwise zero; then the likelihood is d~n provided 
6 ^ max Xi = X, say, and is otherwise zero. Hence the 
posterior density is approximately proportional to 6~n and 
clearly this does not tend to normality as n->oo. Indeed, the 
maximum value is at 6 = X, so that 0 = X, at the limit of the 
range of values of 6 with non-zero probability. If the prior 
distribution of 6 is uniform over the positive half of the real 
line, a simple evaluation of the constant shows that 
tt(6\x) = {n-VtXn-W-n (6 > X,n > 1), 
with mean (n -1) Xj(n - 2) and variance (n - 1) X2l(n -3)(n- 2)2 
if n > 3. As n-> oo the variance is approximately X2\n29 whereas 
the theorem, if applicable here, would give a result that is of 
order n~~\ not rr2. The estimation of 6 is much more accurate 
than in the cases covered by the theorem. The practical reason 
for the great accuracy is essentially that any observation, xi9 
immediately implies that 6 < xi has zero posterior probability; 
since, if 6 < xi9 xt has zero probability. This is a much stronger 
result than can usually be obtained. The mathematical reason 
is the discontinuity of the density and its differential with 
respect to 6 at the upper extreme of the range of x. Difficulties 
can also occur with estimates having smaller accuracy than  
suggested by the theorem, when dealing with several parameters. 
Examples of this phenomenon will not arise in this book. 
7.2] RANDOM SEQUENCES OF TRIALS 141 
7.2. Random sequences of trials 
In this section we consider the simple probability situation of 
a random sequence of trials with constant probability, 6, of 
success, and discuss the inferences about 6 that can be made. 
If a random variable, x, has a density 
(a + b + l)\ 
a\b\ 
xa{\-xf, (1) 
for 0 ^ x ^ 1 and a, b > — 1, it is said to have a  
Beta-distribution with parameters a and b. We write it BQ(a, b), the suffix 
distinguishing it from the binomial distribution B(n,p) with 
index n and parameter p (§2.1). 
Theorem 1. If, with a random sequence ofn trials with constant 
probability, 6, of success, the prior distribution of d is B0(a, b), 
then the posterior distribution of 6 is BQ(a + r,b + n — r) where r is 
the number of successes. 
The proof is straightforward: 
tt(6) oc da(l - Of, from (1), (2) 
the likelihood is p(x \ 6) = dr(l - 6)n^9 
so that 7t{6 I x) oc da+r(l - d)b+n~% (3) 
proving the result. (Here x denotes the results of the sequence 
of trials. Since r is sufficient n(d\ x) may be written 7r(d\r).) 
Corollary 1. Under the conditions of the theorem the posterior 
distribution of ,, , , 1X , n x 
* - <c£$) (i4») ™ 
is F[2(a + r+l),2(b + n-r +1)]. 
From (4) dF/dd oc (l - 0)~2 
and also d = a'FI(b' + a'F), 
where a' = a + r+l, b' = b+n — r + l. Substitution in (3), not 
forgetting the derivative (theorem 3.5.1), gives 
tt(F\ x) oc fW-V(6' + a'F)a'+b' 
ocFa'-1l(2b' + 2a'F)a'+b'. 
142 APPROXIMATE METHODS [7.2 
A comparison with the density of the F-distribution (equation 
6.2.1) establishes the result. 
Corollary 2. The posterior distribution of In {6/(1 — 6)} has, for 
large r and (n — r), approximate mean 
ln{r/(n-r)} + ^±i-^, (5) 
1 1 
and variance - + 7 r. (6) 
r (n — r) 
In §6.2 the mean and variance of the F-distribution were 
obtained. From those results and corollary 1, the mean of F9 
given by equation (4), is 
b+n—r+l. 1 1 1 ,_ 
—r = 1+t ^1+ , (7) 
b + n — r b + n — r n — r v ' 
and the variance is approximately n/r(n — r). Since the variance 
of F is small when r and (n — r) are large, the mean and variance 
of u = In F can be found to the same order of approximation as 
in (7) by use of theorem 3.4.1. 
£(u) - In (l +—) -^-T—\ (l +—) 
\ n — r) 2r(n — r)\ n — r) 
-2 
1 n 1/1 
n — r 2r(n — r) 2\n — r r)9 ^' 
and @\u) ^ (l +—)"2-7-^-t ^ - +—. (9) 
\ n — r) r(n — r) r n — r v ' 
Since u = ln{(b + n -r + \)j(a + r +1)} + In{0/(1 - 6)} the  
expression, (6), for the variance immediately follows. That for the 
mean, (5), is easily obtained since 
<f{In [0/(1 - 6)]} ^ £(u) + ln{(a + r + l)/(b + n - r + 1)} 
r a+l b+l 
1/1 l\ * r 
-~ +ln + 
2\n — r r) n — r 
t r a+\ b+\ 
= In + -, 
n—r r n—r 
n 
7.2] RANDOM SEQUENCES OF TRIALS 143 
Beta-distribution 
The family of Beta-distributions is convenient for random 
variables which lie between 0 and 1. An important property of 
the family is that y = 1 - x also has a Beta-distribution, but with 
a and b interchanged, that is B0(b9 a). This is obvious from the 
density. This makes it particularly useful if x is the probability 
of success, for y is then the probability of failure and has a  
distribution of the same class. That the integral of (1) is unity 
follows from the Beta-integral (equation 5.4.7). The same 
integral shows that the mean is (a+ 1 )l(a + b + 2) and the  
variance (from equation 2.4.5) is 
(g+l)(g + 2) (fl+1)2 = (a+l)(b+l) 
(a + b + 2)(a + b + 3) (a + b + 2)2 (a + b + 2)2 (a + b + 3)' K } 
If a and b are positive the density increases from zero at x = 0 
to a maximum at x = afca + b) and falls again to zero at x = 1. 
If -1 < a < 0 or -1 < b < 0 the density increases without 
limit as x approaches 0 or 1, respectively. The case of small a 
and large b corresponds to a distribution in which x is usually 
nearer 0 than 1, because the mode and mean are then both near 
zero. The relation between the Beta- and F-distributions has 
already been seen in deriving equation 6.2.5. 
Change in Beta-distribution with sampling 
Apart from the reasons just given, the family of  
Beta-distributions is the natural one to consider as prior distributions for the 
probability 0 of success. We saw in § 5.5 that any member of the 
exponential family had associated with it a family of conjugate 
prior distributions (cf. equation 5.5.18). The likelihood here 
belongs to the exponential family (equation 5.5.12) and the 
conjugate family is easily seen to be the Beta-family of  
distributions. Theorem 1 merely expresses the result discussed after 
equation 5.5.18 that the posterior distribution also belongs to 
the same family but with different parameters. In fact a and b 
change into a + r and b + (n-r). It is often convenient to  
represent the situation on a diagram (fig. 7.2.1). The axes are of a and 
b. The prior distribution corresponds to the point (a, b) with, say, 
144 APPROXIMATE METHODS [7.2 
integer co-ordinates. Every time a success occurs 1 is added to 
the value of a; every time a failure occurs 1 is added to the value 
of b. In this way the random sequence of trials can be represented 
by a path in the diagram. The figure shows the path  
corresponding to a sequence of trials where the prior distribution has 
a=l9b = 2 and beginning SFFS(S—success: F— failure). Notice 
6 r 
o 
-l 
-l 
a 
0 
Fig. 7.2.1 
that at each stage (a + b) always changes by one, and hence after 
n trials has increased by n, so that the posterior variance of 0 
(equation (10)) is small for large n, corresponding to increased 
knowledge about the value of d. 
Vague prior knowledge 
In any application it is necessary to choose values of a and b 
for the prior distribution. As discussed in connexion with the 
normal distribution, we usually require a prior distribution 
7.2] RANDOM SEQUENCES OF TRIALS 145 
corresponding to a fair amount of ignorance about the  
parameter. The situation where the prior knowledge of 6 is not vague 
is discussed below. Since any observation always increases 
either a or b it corresponds to the greatest possible ignorance to 
take a and b as small as possible. For the prior density to have 
total integral 1 it is necessary and sufficient that both a and b 
exceed - 1. Therefore the usual prior distribution to take is that 
with a = b = — 1. It is true that the resulting density can only 
be defined as a conditional density, but we have already used 
this type of prior density in supposing the mean or log-variance 
of the normal distribution to be uniformly distributed. In fact 
the density just suggested is proportional to {#(1 -#)}-1: this is 
equivalent to saying that $ = In {0/(1 —6)} is uniformly  
distributed over the whole real line, since d$\dd = {6(1 — 0)}'1. Hence 
the convenient parameter to consider is <f>9 the logarithm of the 
odds in favour of the event of success in a trial. Notice that it 
is this parameter which occurs in the natural representation of 
the likelihood as a member of the exponential family  
(equation 5.5.12). With a = b = -1 the posterior distribution is 
B0(r— 1, n — r— 1), the two parameters being one less than the 
number of successes and failures respectively. Bayes himself 
suggested supposing 6 to be uniformly distributed over the 
interval (0, 1) and this distribution has often been suggested by 
later writers; a common argument in its favour being that if we 
are ignorant of 6 then any value of 6 is as likely as any other. 
Unfortunately this argument also applies to 0. The practical 
difference between these two apparently very different densities 
is, however, slight because two observations, one a success and 
one a failure, are sufficient to change one (<fi uniform) into the 
other (6 uniform). This is clearly seen from fig. 7.2.l.f So in 
arguing in favour of one against the other we are only arguing 
over the equivalent of two observations, usually a small fraction 
of the total number, n, of observations on which the posterior 
distribution is based. If the uniform distribution of <p is used 
then the posterior distribution can only be defined as a  
conditional density until both a success and a failure have been 
observed. It is not unreasonable to say that reliable inferences 
t Notice that in the figure the axes pass through a = b = — 1. 
10 
LSII 
146 APPROXIMATE METHODS [7.2 
cannot be made about the ratio of successes to failures until an 
example of each has occurred. 
Use of F-tables 
In order to reduce the number of tables of distribution  
functions that is needed, it is convenient in practice to use the 
relation between the Beta-distribution and the F-distribution and 
use tables of the latter (compare the discussion at the end of 
§5.4). This is the purpose of corollary 1. With a = b = -1 the 
quantity having the F-distribution is 
the ratio of the odds in favour of success to the empirical odds 
in favour of success; the empirical odds being the ratio of 
observed number of successes to number of failures. The degrees 
of freedom are 2r and 2(n — r); twice the numbers of successes 
and failures respectively. Confidence intervals for F are given in 
the usual way (§6.2) with the F-distribution. A confidence  
interval for F of coefficient fl is given by 
F = Fia[2r, 2(/*-r)] < F < Fia[2r9 2(n-r)] = F9 
with a = 1 — /3. In terms of 6 this is equivalent to 
7m77r\ <e < SF,{£i7r\- (12) 
Since F is not directly tabulated it is more convenient to use 
equation 6.2.7 and to write (12) as 
1 <d<, ./ x, *. 03) 
l+(n-r)F'/r \+{n-r)jrF9 
where 
F = Fia[2(n-r)9 2r] = {Fja[2r, 2(n-r)]}-* = F~\ 
The limits given by (13) are wider than people usually expect 
and a numerical example may prove instructive. In 17 trials, 
5 successes (and 12 failures) were observed. The degrees of  
freedom for F are therefore 10 and 24, so that F = 2-64 with 
a = 0-05, and F = 3-37, from the tables. Hence (/*-r)/rF = 0-91 
7.2] RANDOM SEQUENCES OF TRIALS 147 
and (n - r)F'lr = 8-09, so that the 95 % confidence interval for 6 
is, from (13), (0-11, 0-52). The mean value of the posterior  
distribution is r/n, namely 0-29. Notice that the interval is not 
symmetric about the mean, the upper value differing from it by 
0-23, but the lower only by 0-18. The interval is only symmetric 
when r = n — r, the mean then being \. 
Approximate methods 
The Beta- and F-distributions are a little awkward to handle, 
especially when inferences are to be made on the basis of several 
samples, and it is desirable to develop suitable approximations. 
The maximum likelihood method of §7.1 provides one such 
approximation, and we saw in that section that the posterior 
distribution of 6 was approximately N(r/n, r(n - r)jnz). The mean 
and variance agree, in large samples, with the exact results, from 
equation (10) above, of (a + r + 1 )l(a + b + n + 2) and 
(a + r+l)(b + n-r+l)l(a + b + n + 2)2(a + b + n + 3)9 
but the approach to normality is slow unless /• is about \n. 
Thus in the numerical example the posterior distribution is, 
according to the approximation, and with a = b = — 1, 
N(0-29,0-0122), so that the 95 % confidence limits are (0-07,0-51), 
not in too good agreement with the exact result, especially at the 
lower limit. Notice that this interval, unlike the exact one, is 
necessarily symmetrical about the mean. As explained in §7.1 
some parametric functions will have a more nearly normal  
distribution than others and so we look for a better approximation 
using a parameter different from 6. The F-distribution is no 
more nearly normal than is the Beta-distribution, but since 
(§6.2), for all except very small values of the degrees of freedom, 
the density increases rapidly from zero at F = 0 to a maximum 
near F = 1 and then decreases slowly to zero as F-> oo, it looks 
as though In F will be much more nearly normal. The logarithm 
will have a stronger effect on the sharp left-hand tail, between 
F = 0 and F = 1, than on the right-hand tail, above F — 1. 
This transformation was first suggested by Fisher t and In F may 
be shown to have a normal distribution to a much better approxi- 
t Fisher took ilnF, but the \ is more conveniently omitted in this context. 
IO-2 
148 APPROXIMATE METHODS [7.2 
mation than does F itself. The transformation also commends 
itself in the present situation because In {0/(1 -#)}, which differs 
from InF by a constant, is the natural parameter to consider, 
both in the representation of the original distribution in  
exponential form (equation 5.5.12) and as the parameter having a 
uniform prior distribution. 
Corollary 2 gives the mean and variance of the posterior  
distribution of In {6/(1 -#)}, as far as the terms in 1/r and l/(w-r), 
which, in conjunction with the assumption of normality, are 
sufficient to define the posterior distribution. The logarithm is 
the dominant term in the mean, the next term is of smaller order 
and may be ignored for large r and (n — r): in any case it is of 
smaller order than the standard deviation. Notice that the prior 
distribution only enters into this second term, and not into the 
dominant terms of mean and variance. This is another example 
of the fact that for large samples the prior distribution can be 
ignored. Notice that, to the same order (5) may be written 
ln( r + at\), (14) 
[n-r + b + ^j9 v } 
which is more convenient for calculation. Consider this  
approximation in the numerical case just cited with n — r = 12 and 
r = 5, and a = b = — 1. The logarithm of the odds is  
approximately normal with mean —0-939 and standard deviation 
0-532, and hence 95% confidence limits are -1-982 and 
+ 0-104, giving 95% confidence limits for 6 of (0-12, 0-53), 
agreeing very well with the exact values of (0-11, 0-52) obtained 
above using the F-distribution. 
Approximation to the likelihood 
Another method of obtaining a useful approximate result is 
to approximate to the likelihood instead of to the posterior 
distribution directly. In particular, if the likelihood can be 
changed into an approximately normal one, then the inferences 
appropriate to a normal distribution can be made. Now one 
reason why inferences for the binomial distribution are not as 
simple as for the normal is that the parameter of interest affects 
both mean and variance. It is therefore sometimes convenient 
7.2] RANDOM SEQUENCES OF TRIALS 149 
to find a transformation which will remove the parameter from 
the variance. We saw how this could be done in certain cases in 
§3.4. In particular for the binomial distribution (example 3 of 
that section), if r is B(n, p) then sin~y(r/n) has approximate 
variance 1 /4/t, not dependent on p, about a mean of sin~\/p. 
Furthermore, it may be shown that the distribution of the 
inverse-sine is more nearly normal than is that of r itself.  
Consequently, it is now possible to make inferences using the  
approximate normality of sin_y(r/«). The transformation is tabulated 
in Lindley and Miller (1961) and in the numerical example 
above, sin~y(5/17) = 0-573 (in radians) with a standard  
deviation of 1/^/68 = 0-121. Hence 95% confidence limits for the 
inverse sine are approximately 0-573 +1-96x0-121, that is 
(0-336, 0-810). Applying the inverse transformation the limits 
for 6 are (0-11, 0-52) in agreement with the exact values. 
The inverse-sine transformation is particularly valuable 
because it enables all the inference methods appropriate to the 
normal distribution to be used with the binomial. For example, 
suppose we have k independent sets of random sequences each 
of n trials, with constant probabilities of success 0i9 and giving 
ri successes (j = 1, 2, ..., k). Then xi = sin-1^(rjn) will be  
approximately normal with means sin~\/^ and common variance 
l/4«. Consequently the hypothesis that the 0/s are all equal can 
be tested by the analysis of variance method of §6.4, using 
theorem 2 of that section, since the variance is known. The test 
statistic will be 4«2(x^ — x)2, which may be approximately  
compared with a ^-distribution with (k— 1) degrees of freedom. 
A disadvantage of the inverse-sine transformation is that the 
transform is not a useful parameter to consider. Thus, if we 
carry out an analysis of variance as described in §8.5 on the 
inverse-sines and learn that an interaction is probably zero, that 
result is not usually of much value to the experimenter to whom 
the inverse-sine is not a physically meaningful quantity. 
The reader who has studied the final part of §5.6 may object 
to the above argument since it uses the distribution of r to 
derive the transformation and not just the likelihood. This may 
be avoided, however, by replacing the argument of example 3 
of §3.4, by the comparable argument of §7.1 in connexion with 
150 APPROXIMATE METHODS [7.2 
maximum likelihood. Equation (24) of that section replaces 
equation 3.4.2 and the purpose of the transformation is to find 
a parameter about which the information is constant. This 
appears to have been Fisher's intention in producing the  
transformation originally. 
Several samples 
The true worth of any of these approximations is only  
appreciated when considering several samples. Suppose, for example, 
that we have two independent random sequences of /% trials, 
with constant probability, 0i9 of success, yielding ri successes 
(i = 1,2). Then we may wish to inquire about differences 
between 91 and 62 and, in particular, to make a significance test 
of the hypothesis that 61 = 62. This hypothesis may be  
conveniently rephrased in terms of the logarithms of odds by 
investigating fa - fa = ln^^l - S2)I62(1 - 6-J} and, in particular, 
testing whether fa —fa = 0. The posterior distributions of fa 
and fa will be independent and both approximately normal, 
so that the posterior distribution of fa —fa is approximately 
normal with mean 
1 / ^1 \ 1 / r2 \_1 f ^1(^2 — ^2) \ /14TN 
11 Vh - rj n \n2 -rj " n Ifo - rx) r2) 
1111 
and variance — -\ 1 1 . (16) 
ri Wi-fi r2 n2-r2 
(In (15) the second term of (5) has been ignored, or equivalently 
we have supposed a = b = — J.) Hence the null hypothesis 
that 61 = 629 or equivalently fa = fa, will be judged significant 
at the 5% level if (15) exceeds 1-96 times the standard deviation 
obtained from (16): that is, if 
[-6£&)T 
(I+_L.+I+_L_) 
> (1-96)8 = 3-84. (17) 
This method may be extended to cover numerous other  
situations but we shall not pursue it any further here since another 
method, using a different approximation, is available and is 
more commonly used. This will be studied in §7.6. 
7.2J RANDOM SEQUENCES OF TRIALS 151 
Inference with appreciable prior knowledge 
We now consider the analysis suitable when the prior  
knowledge of 6 is comparable in its importance to the knowledge to 
be obtained from the likelihood function. The situation we have 
in mind is exemplified by a problem concerning a defect in a 
mass-produced model of a car. The parameter in question is the 
proportion of cars of this particular model exhibiting the defect 
by the time they are one year old. After the model had been in 
production about a year the manufacturer realized, from  
complaints received, that this defect was of reasonably common 
occurrence. He naturally wished to find out how common it 
was and proposed to carry out a survey. For our purposes it 
will suffice to suppose that the survey consists in taking a random 
sample of size n of one-year-old cars and observing the number, 
r, with the defect. The likelihood will then be that considered in 
this section. However, there is already, before the sample is 
taken, some knowledge of 6. It is difficult to be precise about 
this because some customers will not notice the defect, others 
will not complain even if they do notice it, and not all complaints 
will reach the group in the manufacturing company conducting 
the investigation. Nevertheless, some information is available 
and should be incorporated into the analysis. 
The question is: how should the prior knowledge be obtained 
and expressed ? It will be most convenient if it is expressed in 
terms of the Beta-distribution, should this be possible. In order 
to do this it is necessary to obtain the values of a and b: when 
this has been done theorem 1 may be applied. In the example 
the company felt that the defect most probably occurred in about 
15 % of their cars, and that it must certainly occur in about 5 % 
in order to explain the complaints already received. They also 
felt it to be most unlikely that more than 1 in 3 of the cars would 
exhibit the defect. This information can be turned into a  
probability density by supposing that the upper and lower limits 
quoted correspond roughly to the upper and lower 5 % points 
of a Beta-distribution. From tables of the percentage points of 
this distribution (for example those in Pearson and Hartley 
(1958), Table 16) the values a = 2, b = 14 provide upper and 
152 APPROXIMATE METHODS [7.2 
lower 5% points of 6 at 0-050 and 0-326 respectively, f The 
mean value of d is then (a+l)/(a + b + 2) = 3/18 = 0-167 and 
the mode (the most likely value) is al(a + b) = 2/16 = 0-125. 
These three figures agree tolerably well with those suggested 
above and the distribution B0(2, 14) might therefore be used to 
express the prior knowledge. Notice that the extent of the prior 
knowledge assumed is equivalent to observing the defect in 
about 3 cars out of a randomly chosen 18 when initially vague 
about 6: that is, with a = b = -1 (cf. §§5.2, 5.5). This  
comment suggests that if the survey is to consider a random sample 
of the order of a hundred or more cars then the prior knowledge 
is small compared with that to be obtained from the survey. 
Thus, if 100 cars are inspected and 32 have the defect the 
posterior distribution is B0(349 82) which is not very different 
from B0(31, 67) which would have been obtained without the 
prior knowledge. Approximations in terms of the log-odds can, 
of course, still be used. 
Methods of this type are always available whenever the  
likelihood belongs to the exponential family. Consequently when 
dealing with this family, it is no real restriction to confine 
attention to the prior distribution corresponding to substantial 
ignorance of 6 provided, in any special case, the actual prior 
knowledge is capable of being represented by a distribution of 
the conjugate family. (Cf. the discussion in §5.5.) 
Binomial distribution 
The methods of this section are often said to be appropriate to 
the binomial distribution. This is certainly true, for if a fixed 
number, n, of trials is observed to result in r successes, the 
distribution of r is B(n9 p), where p is the probability of success, 
and the methods of this section can be used. But it is not  
advisable to associate the methods only with the binomial  
distribution; that is, only with fixed n. The methods do not assume 
n fixed. They are valid, for example, when r is fixed and the 
experimenter continues sampling until some random number, 
n9 of trials yield r successes: or again, if n was the random 
f Notice that the tables referred to use a— 1 and b—\ for what we here call 
a and b. 
7.2] RANDOM SEQUENCES OF TRIALS 153 
number that the experimenter had time to investigate (§5.5). 
This is an example of the use of the likelihood principle (§5.6) 
and readers who have read the latter part of that section will 
appreciate that this is another example of the irrelevance of the 
sample space to the inference being made. 
7.3. The Poisson distribution 
In this section we discuss exact methods of making inferences 
with the Poisson distribution before proceeding to general 
approximations in §7.4. 
Theorem 1. If fa, r2, ..., rn) is a random sample of size nfrom 
a Poisson distribution P(0)9 and if the prior distribution of 6 is 
r(s9 m); then the posterior distribution of 6 is T(s + nr, m + ri), 
n 
where r = n~x S *V 
From the definition of the density of the T-distribution 
(equation 2.3.7) the prior density is 
n(d) oc e-^d8'1 (1) 
for 6 ^ 0, and otherwise zero. The likelihood is 
p(x\ d) = e-ned*-i 7 n fa!) oc e~n0d™; (2) 
/ i=i 
where x = fa, r2, ..., rn). This shows that r is sufficient. Hence, 
multiplying (1) by (2), we have 
7t{6 I x) oc e-<m+n) 0d8+n*-\ (3) 
and a further comparison with the density of the T-distribution 
establishes the result. 
Corollary. The posterior density of 2(m + n)d is x2 with 
2(s + nr) degrees of freedom. 
This follows immediately from the relationship between the 
r- and ^-distributions (§5.3) that if y is T(n, A) then 2Ay is x2 
with In degrees of freedom. 
Theorem 2. If two independent random samples of sizes n± andn2 
are available from Poisson distributions P(d^) andP{62)9 and if the 
154 APPROXIMATE METHODS [7.3 
prior distributions of di are independent and T(si9 m^ (/ = 1, 2); 
then the posterior distribution of 
(OJ0J {(mi + nx) 02 + 7*2 r2)/(m2 + n2) (s1 + n± rx)} 
is F with [2(s1 + n± f x), 2{s2 + n2 f2)\ degrees of freedom, where rt is 
the mean of the ith sample. 
From the corollary the posterior densities of 2(mi + n{) 6i are 
X2 on 2(st + Hi r^ degrees of freedom (i = 1,2); and because of 
the independence, both of the prior distributions and the samples, 
they will be independent. Now in §6.2 we remarked that if %f 
were independent and had ^-distributions with vi degrees of 
freedom (/ = 1, 2) then (Xil^Kxll^) had an F-distribution with 
(vi9 v%) degrees of freedom. (The proof was given in the course 
of proving theorem 6.2.1.) Applying this result here we immedi- 
aCy a 2(/w1 + yi1)g1/2(j1 + yi1r1) ^ 
2(m2 + n2)d2l2(s2 + n2r2) 
has the F-distribution with P^ + Uxfi), 2(s2 + n2r2)] degrees of 
freedom, as required. 
Exact methods 
The Poisson distribution is a member of the exponential 
family (§5.5), as is immediately seen by writing the Poisson 
density in the form , . . 
w)e'6enine (5) 
and comparing it with equation 5.5.5 with the r of that equation 
equal to 1. The conjugate prior distribution which fits naturally 
with it, in the way described in §5.5, is easily seen to be the 
r-distribution with density (of 0) proportional to e-m6ei8~1)]I10. 
Theorem 1 expresses the known general form of the change in 
this distribution (§5.5) with observations. The parameter m 
changes deterministically by the addition of 1 for each sample 
taken from the Poisson distribution: the parameter s changes 
randomly by the addition of the sample value, r^ The known 
connexion between the T- and ^-distributions enables tables of 
the latter (depending on only one parameter, the degrees of 
freedom) to be used for inferences. 
7.3] THE POISSON DISTRIBUTION 155 
Vague prior knowledge 
Since both parameters of the T-distribution increase with any 
observation, the greatest possible ignorance is reflected in a prior 
distribution with these parameters as small as possible. For 
convergence of the T-distribution it is necessary that both  
parameters be positive (§2.3); hence the usual prior distribution to 
take to represent considerable ignorance about 6 is that with 
both parameters zero. The prior density, (1), is then proportional 
to 6-1. Consequently, in order to represent vague knowledge, we 
assume ln# to have a uniform prior distribution over the whole 
real line. This also agrees with the representation of the Poisson 
distribution as a member of the exponential family, equation (5), 
where the natural parameter is <p = ln#, With this prior  
distribution the posterior density of 2nd is x2 with 2nr degrees of 
freedom. Notice that, since the sum of independent Poisson 
variables is itself a Poisson variable (§3.5), the sufficient statistic 
n 
2 ri has a Poisson distribution with parameter nd; so that infer- 
ences from a sample of size n from P{6) are equivalent to 
inferences from a single sample from P(nd)m 
Two samples 
Theorem 2 gives an exact method of making inferences about 
the ratio of two Poisson means. The ratio is again the natural 
quantity to consider (rather than, say, the difference) because it 
is a function of the difference of the logarithms, which are, as 
we have just seen, the natural parameters for each distribution 
separately. Using the prior distribution of ignorance with 
st = nii = 0 the posterior distribution of (d1lri)l(dzlr2) is F with 
(2n1rl9 2n2r2) degrees of freedom. 
Consider a numerical illustration. Suppose two independent 
Poisson processes had been observed for \ and 1 hour  
respectively, and gave 12 and 30 incidents respectively over the 
periods. (Notice that since Sr^ is sufficient no additional  
information would be obtained by considering the numbers of  
incidents in, say, separate 5-minute periods.) Then 12 is the value of 
a Poisson variable with mean \dx and 30 is the value of a 
156 APPROXIMATE METHODS [7.3 
Poisson variable with mean 62, where 61 and 62 are the rates of 
occurrence per hour of incidents in the two processes. It follows 
that the posterior distribution of (i#i/12)/(02/3O) is F with 24 
and 60 degrees of freedom (/^ = n2 = 1). The upper and lower 
2\ % points of the F-distribution with those degrees of freedom 
are 1-88 and 0-48 respectively, so that a 95 % confidence interval 
for 6J62 is 
11 (ft) 0-48 < 6J6, < (§*) 1-88, 
that is 0-38 < 6X\62 < 1-50. 
Approximations 
The logarithmic transformation of F can be used, if the 
F-tables are not available, with the approximate normality and 
mean and variance obtained as in the previous section. 
The logarithmic transformation can be useful when handling 
several Poisson distributions, though the method of §7.4  
(especially equation 7.4.16) is also available. We remarked in §3.4 that 
if a random variable was T(n, A) then its logarithm had an 
approximate standard deviation n~?9 independent of A. Also 
the logarithm is more nearly normally distributed than is the 
original T-variable. Consequently the use of In 6 in the present 
case gives an approximately normal posterior distribution in the 
same way that the log-odds, in the binomial situation, had an 
approximately normal distribution (corollary 2 to theorem 7.2.1). 
Indeed, since the F-distribution is the ratio of two independent 
T-distributions (§6.2) the earlier result follows from this remark. 
Another approximate method for the Poisson distribution is 
to transform the distribution to one more nearly normal by 
arranging that the variance (or the information) is  
approximately constant, whatever be the mean. In §3.4, example 2, we 
saw that this could be done by taking the square root of the 
Poisson variable, with approximately constant variance of 1/4. 
As with the inverse-sine transformation for the binomial, the 
analysis of variance techniques are then available. For example, 
the equality of 61 and 62 in the above numerical example could 
be tested by this method: ^12 is approximately Ma/Q^i), i), or 
^/24 is approximately N(J(d^)9 i); and ^30 is approximately 
7.3] 
THE POISSON DISTRIBUTION 
157 
N(^(62),1); so that by the methods of §6.1 the posterior  
distribution of V^i~V^2 is approximately N(*j24-^30, J+i). This 
gives confidence limits for ^61 — ^jd2 and a test of significance of 
the hypothesis 61 = 62 in the usual way. However, the square 
root of 0 is not a useful parameter to consider so that other 
techniques are usually to be preferred. 
7.4. Goodness-of-fit tests 
In this section we discuss an important approximation, of 
wide application, to the posterior distribution when, in each of 
a random sequence of trials, one of a number of exclusive and 
exhaustive events of constant probability is observed to occur. 
The events will be denoted by Al9 A2, ..., Ak and their prob- 
k 
abilities by 0l9 029 ..., 6k, so that 2 6i = 1. The case, k = 2, 
studied in §7.2, is a special case. 
Theorem 7. If, in a random sequence of n trials, the exclusive 
and exhaustive events At have constant probabilities of success di 
and occur ri times (2r^ -— n); then the posterior distribution of 
^{r.-nd^lnd, (1) 
i=i 
is, for large n, approximately x1 with (A:— 1) degrees of freedom. 
Suppose that the prior distribution of the #'s is uniform over 
the region 6i ^ 0, 2^ = 1. Then the posterior distribution of 
the d\ 7r(8|r), where 6 = (dl9 02, ..., dk) and r = (rl9 r2, ..., rk), 
is proportional to the likelihood; that is, to 
0p0p...0j?. (2) 
k 
Hence In 77(6 |r) = C+ S^ln^, (3) 
1=1 
where C denotes a constant (that is, a quantity which does not 
depend on 8) and will not necessarily be the same constant 
throughout the argument. Now (3) only obtains if 2^ = 1, so 
that the value of 8 for which (3) is a maximum can be found by 
differentiating Sr^ln^-AZ^ partially with respect to each 0i 
and equating to zero, where A is a Lagrange undetermined 
158 APPROXIMATE METHODS [7.4 
multiplier. The result is obviously that (3) has its maximum at 
0i - $i = rjn which, because of the uniform prior distribution, 
is also the maximum likelihood estimate of 0im Let us therefore 
write St « 6i — riln, with 25^ = 0, and obtain, the Jacobian 
being unity, ln7r(8|r) = c+ 2r.in(r./„ + (j,). (4) 
We saw in §7.1 (after equation 7.1.7) that the posterior  
distribution was only appreciable in an interval about the maximum 
likelihood estimate of width of order n~%. Hence 8i may be 
taken to be of this order and (4) may be expanded in a power 
series in the <Ts, terms higher than the third degree being ignored. 
Then In tt(8 | r) = C + Sr, In (rjn) + ^ In (1 + n *,//•<) 
= C-iZ/r^/r, + i2W*?/ri, (5) 
the terms in S{ vanishing since JiSi = 0. Since St is of order rr% 
and ri is of order n9 the first term after the constant is of order 
one and the next of order n~%. For the moment, then, ignore 
the last term and write 
A, = nSJJin) with 2VMA, = 0. 
The Jacobian may be absorbed into the constant and approxi- 
mately ln7r(X|r) = C-iSAJ. (6) 
Hence the joint posterior density of the A's is constant over 
spheres with centres at the origin, in the space of (k— 1)  
dimensions formed from the A>dimensional space of the A's  
constrained by li^J(ri)Xi « 0, a plane passing through the origin. 
Hence we may argue that confidence sets must be based on 
ZAf, exactly as in the proof of theorem 6.4.2, which was there 
shown to have a ^-distribution with (k— 1) degrees of freedom, 
(k— 1) being the dimension of the spheres. 
In terms of the #'s the result just established is that the 
posterior distribution of 2(r* - «^)2/r^ is approximately x2 with 
(k~ 1) degrees of freedom. This differs from the result required 
7.4] GOODNESS-OF-FIT TESTS 159 
in that Yi replaces ndi in the denominator. In order to show 
that this replacement does not afiect the result we have to show 
(a) that (5) may be written C—iSw2^/^, plus terms of the 
same order as the final term in (5), 
(b) that the Jacobian of the transformation from 6i (or 
equivalents St) to ^ = ^.^^ (7) 
instead of to A^ = (ndi - r^)jr\ 
introduces a term of higher order than those retained in (6). 
If {a) and (b) are established we shall be able to obtain 
In77-((jlIr) = C—i^/4 instead of (6) and the x2 result will be as 
before. 
Proof of (a). We have 
ignoring terms of order n8*. Equation (5) can therefore be 
written ln7r(8|r) = C-iSwWJ/^-iS^^/rJ. (8) 
Hence to order n§\ it is immaterial whether Yi or ndi is in the 
denominator. In fact with ndi the next term in (8) is only half 
the magnitude that it is with ri in (5): so the approximation is 
better with ndi% 
Proof of (b). For the purpose of this proof onl>, omit the 
suffixes. We easily obtain from (7) that 
dju> _ nd+r 
dd " 2r$di 
and hence, with a little algebra, 
ln-T- = ln^- + |ln 1+ -1 !~ M -■— 
dpi n 
= ln^, (9) 
n r 
In 
C + n^SJr,. 
160 APPROXIMATE METHODS [7.4 
omitting terms of order rr1. The logarithm of the Jacobian of 
the transformation from the #'s (or equally the <Ts) to the /i's is 
obtained from the sum of k terms like (9) and gives 
dQ 
d[L 
The summation term is of order n~% and therefore of the order 
of terms neglected. The sign of this term is variable and it is not 
a simple matter to see if the approximation is better with /it 
instead of A^. (This is discussed again below.) 
These results, (a) and (b)9 show that jjl may replace X in (6) 
and the proof is complete except for the remark that the effect of 
any other reasonable prior distribution besides the uniform one 
would have negligible effect as n increases. 
Corollary. An approximate significance test (valid as n->co) 
of the hypothesis that di = pi (i = 1,2,..., k) for assigned values 
of pi9 is obtained at level <x by declaring the data significant when 
X^Sfo-HA)2/^ (10) 
exceeds xl(k— 1), the upper 100 a % point of the ^-distribution 
with (k— 1) degrees of freedom (§5.3). 
The result is significant if the point (j>l9p2, • ••,/>&) lies outside 
the confidence set which, in terms of the A's, is a sphere, centre 
the origin, and radius determined by the distribution of (1) 
(cf. theorem 6.4.1). This proves the result. 
The quantity (10) is called Pearson's chi-squared statistic. The 
numbers, ri9 are called observed values and are often denoted by 
Ot: the numbers, npi9 are called expected values and are often 
denoted by Ei9 (The reason for the latter term is that if 0i = pi9 
that is if the null hypothesis is true, then S(r^) = npt.) The 
statistic may then be written 
tf = i(Oi-EtflEi. (11) 
Pearson's test 
The test of the corollary was one of the first significance tests 
to be developed. Pearson developed it in 1900 in a different way, 
but our approach, using Bayesian methods, gives essentially the 
7.4] GOODNESS-OF-FIT TESTS 161 
same results as he obtained and subsequent writers have 
used. It is usually described as a goodness-of-fit test, for the 
following reason. Suppose one is interested in a particular 
null hypothesis for the probabilities of the events Ai9 namely 
that di=pi(i= 1,2, ...,&), and wishes to see how well 
the observations agree with it. The null hypothesis may be 
thought of as a model to describe the sampling behaviour of the 
observations and one wishes to know how good the model is in 
fitting the data: hence the terminology. The criterion, (10), may 
be justified on the following intuitive grounds: if di = pt we 
should expect Ei occurrences of the event Ai9 whereas we 
actually obtained 0{. The discrepancy, for a single event Ai9 is 
conveniently measured by (Oi - E^f9 just as the spread of a  
distribution is measured by squares of departures from expectation. 
But it would not be sensible to add these squares to provide an 
overall measure of discrepancy because, even if the null  
hypothesis is true, some will clearly be larger than others due to the 
fact that the E^s will normally differ. In fact, since Oi has, for 
fixed n, a binomial distribution, the variance of Ot is E^n — E^)jn9 
and therefore (Oi - E^f will be of this order. Additionally the 
O/s are correlated and allowance for this shows that the order 
is more accurately measured by E€. (This point will be elaborated 
below.) Hence a sensible criterion is 2(0^ - Eif\Ei9 agreeing 
with the rigorous argument based on the posterior distribution. 
Note that if n is fixed the r{ will have a multinomial distribution 
(§3.1). The analysis does not, however, assume n fixed. 
Example 
An example of the use of Pearson's statistic is provided 
by the following genetical situation. If two recessive genes 
giving rise to phenotypes A and B are not linked, the  
proportions of the phenotypes AB9 AB9 AB9 AB (AB means 
exhibiting both phenotypes, etc., A denotes not-^4) should be in 
the ratios 1:3:3:9. Hence if we take a random sample of n  
individuals and observe the numbers of each of the four phenotypes, 
they will have, on the null hypothesis of no linkage, a  
multinomial distribution of index n and four classes with probabilities 
1/16, 3/16, 3/16 and 9/16. The hypothesis may be tested by 
ii 
LSII 
162 APPROXIMATE METHODS [7.4 
Pearson's method. A numerical example with n = 96 gave 
6, 13, 16 and 61 in the four classes. These are the observed 
numbers, C^. The expected numbers, Ei9 are 96/16 = 6, 18, 18 
and 54 and x2 is 
0 + 52/18 + 22/18 + 72/54 = 2-52, 
which is well within the confidence set for x2 with 3 degrees of 
freedom, the upper 5 % value being 7-81. Consequently there is 
no reason to doubt the null hypothesis on the evidence of these 
data. 
Extension to any distribution 
The range of applicability of Pearson's statistic is enormously 
increased by noticing that any case of random sampling can be 
converted to the multinomial situation by grouping (§2.4). To 
show this, suppose that each sample yields a single random 
variable, x (the extension to more than one variable is  
straightforward) and that the range of variation of x is divided into 
groups: for example, let at < x < ai+1 be such a group for two 
fixed numbers a^ and ai+1, and let A{ denote the event that x falls 
in the group. Then if there are k exclusive and exhaustive groups 
with corresponding exclusive and exhaustive events Al9A29..., Ak, 
the distribution of the number of occurrences, ri9 of the event 
Ai (i = 1, 2, ..., k) will (because of the random sampling) be 
multinomial with index n and parameters 
Pi = P(A) = p(x)dx, (12) 
where p{x) is the density of x. Hence, if it is desired to test the 
hypothesis that the density of x is of some particular form, p{x\ 
this can be done, at the loss of some information, by testing, by 
Pearson's method, the hypothesis that the grouped results have 
probabilities given by (12). This equation is in the form for 
continuous distributions but the idea is equally applicable to 
discrete distributions which may already be thought of as 
grouped, or may be put into larger groups with 
p(At) = 2 <ls> 
7.4} GOODNESS-OF-FIT TESTS 163 
where qs is the density for the discrete variable. As an example 
consider the case where the null hypothesis is that the random 
variable is uniformly distributed in the interval (0, 1). If this 
interval is divided into k equal intervals each of length k~x the 
probability associated with each of them under the null  
hypothesis is k~\ so that the expected numbers in a sample of size n 
are n/k. 
Relevance of prior knowledge 
It might be thought that this method could be used in a  
situation illustrated by the following example, and it is important to 
understand why it would not be appropriate. In §5.6, using 
results developed in §5.1, we discussed the case of a random 
sample of size n from a normal distribution of known variance 
<r2, and developed a test of the null hypothesis that the mean, 6, 
was a specified value, say zero. This was done by declaring the 
result significant at 5 % if |jc| exceeded l-%(r[<Jn. The same null 
hypothesis could be tested by grouping the observations in the 
way just described and using Pearson's statistic with the density 
of N(0, a-2); the probabilities in (12) being obtained from tables 
of the normal distribution function. The differencef between 
these two tests for the same null hypothesis lies in the form of the 
prior distribution. In the first test, using normal theory, the 
prior information consisted of three parts: (i) knowledge that 
the distribution was normal, (ii) knowledge of the variance of 
the distribution, (iii) ignorance of the mean of the distribution. 
In the second test, using the %2-statistic, there was considerable 
prior ignorance of the values of the /?'s, contrasting markedly 
with the precise knowledge contained in (i) and (ii). The second 
test would therefore not be appropriate if the knowledge of 
normality and variance was available. Pearson's is a test which 
is appropriate when the prior knowledge is much vaguer than in 
the tests described in the last two chapters; though notice that it 
does assume some considerable knowledge, namely that the 
trials are random with constant probabilities. There are tests 
designed for situations where even this prior knowledge is not 
f There is also a difference due to the grouping used in one method. But this 
is slight if the grouping interval is small. (See Sheppard's corrections in §7.5.) 
II-2 
164 APPROXIMATE METHODS [7.4 
available, but they will not be discussed in this book. They 
include tests for randomness. 
Confidence sets 
Notice that the theorem, as distinct from the corollary, does 
give a confidence set for the 6{. This is not often useful in 
practice, at least when k is not small, because of its complexity. 
The test is preferred for much the same reasons as the F-test was 
in §6.4. When k = 2, the binomial situation, the result does 
provide approximate confidence intervals for the single  
parameter 6 = 61 = 1 - #2- If the result is expressed in the binomial 
notation of §7.2, that is rx = r, r2 = n — r, it says that 
Oi-ftfli)2 (r2-nd2)2 _ /}_ 1 \ 
nd1 + nd2 " ^ n } \n6 + n(l-6)J 
= <rzl*r (13) 
has approximately a ^-distribution with one degree of freedom. 
Thus a 95 % confidence interval for 6 is given by (13) being less 
than A = 3-84, the upper 5 % point of the ^-distribution with 
one degree of freedom. This yields a quadratic inequality for 6 
(n2 + An)d2-(2rn + An)d + r2 < 0. (14) 
Clearly the roots of the quadratic are real and lie between 
0 and 1, and the coefficient of 02 is positive; so that the confidence 
set for 6 is the interval between the two roots formed by equating 
the quadratic to zero. Although this result is only a large 
sample one, so that the exact form of the prior distribution is 
irrelevant, it was obtained with a uniform prior distribution 
for 6. If the distribution of ignorance suggested in § 7.2 had been 
used, with density proportional to {#(1 -#)}-1 the effect would 
have been to have replaced (3) by C+ 2(r^- l)ln^. Hence in 
order to compare (14) with the exact result and approximations 
of §7.2, one should be subtracted from the values of r and (n — r) 
before using (13). In the numerical case of §7.2 with n = 17, 
r = 5, the reduction ton = 15, r = 4 will give, for the quadratic 
(225 + 15A)02-(12O + 15A)0 + 16 = 0 
(A = 3-84), with roots 0 = 0-11, 0-52. The values agree with the 
exact ones obtained in §7.2. 
7.4] GOODNESS-OF-FIT TESTS 165 
Tests for Poisson means 
The test can be used in a rather unexpected context. Suppose 
that we have k independent random variables rl9 r29 ..., rk 
with Poisson distributions of parameters 6l9 629 ..., 0k. (Notice 
that, as explained in §7.3, several random samples from the 
same Poisson distribution are equivalent to a single random 
sample from a Poisson distribution, so that each ri may be 
obtained from several samples from one distribution.) The 
joint density of the ri is then 
r~ k ~~i k 
p(rlt r„ ..., rk\db 62, ..., 6k) = exp - S 0, n WlrS- 
L i=l J i=l 
This can be written as the product of the density of n = Sr^ and 
the joint conditional density of the ri9 given n. But n9 being the 
sum of independent Poisson variables, will have a Poisson  
distribution of parameter 0 = H6i (§3.5), so that 
pin\6l9d29...9dk) = e-dd"lnl. 
Now we may write 
(e-00n\ l n\ /6\ri) 
p(rl9 r29 ..., rk\dl9 629 ..., dk) = \—j-j (nfrl) n (~gJ J' (15) 
the expression in the first set of braces being p(n \ 6l9 629 ..., dk); 
that in the second set is therefore p(rl9 r29 ..., rk \ n: 6l9 629 ..., dk). 
If we write ^ = djd9 so that ^ ^ 0 and 2^ = 1 the latter  
distribution is easily seen to be multinomial with index n and 
parameters ^. So (15) may be written 
PiirM {&}) = P(n\6)p{{r%)\n, {</>,}) 
and a comparison with equation 5.5.21 shows that if 6 is known, 
or has a prior distribution independent of the ^'s, n is an ancillary 
statistic and may be supposed fixed for the purposes of inferences 
about the ^'s. When n is fixed we return to the multinomial 
situation. If, therefore, we wish to test the null hypothesis that 
the #'s are all equal, or, more generally, are in assigned ratios, 
and we are prepared to make the prior judgement of the  
independence of d and the ^'s, we can perform the test by expressing 
the null hypothesis in terms of the ^'s and use only the likelihood 
of the 0's, and hence Pearson's statistic. If we require to test 
166 APPROXIMATE METHODS [7.4 
that all the #'s are equal, or equivalently that all the ^'s are k~\ 
the statistic is sfo-w/fc)2 S(r€-r)« 
n\k = ~~T~' (16) 
k 
where f = w/& = k~x £ **> the average of the r's, and may be 
i=i 
compared with %«(&- 1). The statistic (16) is often known as a 
coefficient (or index) of dispersion. Its form is interesting: if it 
were divided by (k — 1) the numerator would be the usual mean 
square estimate of the variance of the ri9 assuming them to have 
come from a common distribution (the null hypothesis), and the 
denominator would be the usual estimate of the mean. But for 
a Poisson distribution the mean and variance are equal so that 
the ratio should be about one, or (16) should be about (k— 1). 
The statistic is effectively a comparison of the variance and the 
mean. When k = 2, the test is an approximation to the exact 
method based on theorem 7.3.2. 
There are two points of interest about the method just used. 
First, it shows that the multinomial distribution of random  
variables ri9 implying that the ri are correlated (see §3.1), can be 
thought of as being obtained by taking independent ri9 each with 
a Poisson distribution, and investigating their distribution  
conditional on Sr^ being fixed. The substitution of correlated ri by 
independent ri is often convenient. For example, in the intuitive 
discussion of Pearson's statistic above we could think of the Ot 
as Poisson with means Eiy and therefore also variances Et: 
hence weighting with the inverses of the variances in the usual 
way we obtain Pearson's criterion. Although a sum of & squares 
it has only (k — 1) degrees of freedom because of the constraint 
on 2^. 
Small samples 
We conclude by making a few remarks about the proof of 
theorem 1. The argument used in §7.2 to explain the use of the 
prior distribution n(6) oc d-\\ — d)-1 in the case of k = 2 can 
easily be extended to the case of general k to justify the prior 
distribution with density 
<el9e29...9ek)cc^uie)j~\ 
7.4] GOODNESS-OF-FIT TESTS 167 
The effect of using this instead of the uniform prior distribution 
would be to replace rt everywhere, from (3) onwards, by (r* -1). 
Hence, in applying the result it would probably be better to 
deduct one from each observed value (and k from n) before 
using it: but this is not general practice. This was done in the 
numerical application to the binomial case k = 2, above, with 
successful results. Actually, although the theorem is only a 
limiting result as «->oo, the exact distribution of Pearson's 
statistic is very nearly x2 for quite small samples. The general 
recommendation, based on considerable investigations of  
particular cases, is that the statistic and test may safely be used 
provided each expected value is not less than two. In these 
investigations the comparison has been made with the sampling 
distribution of %2, that is, for fixed di and varying ri9 rather than 
with the posterior distribution, but similar conclusions probably 
apply to the latter. 
The basis of the approximation is the observation that the 
logarithm of the posterior density has a maximum at 8 and that 
the density falls away fairly sharply from this maximum. The 
logarithmic density can therefore be replaced by a second-degree 
surface about this maximum value: this corresponds to the 
retention of only the second-degree (in 8t) terms in (5). This is 
essentially the same type of approximation that was used with 
the maximum likelihood method (§7.1). It leads naturally to 
the quantities A^ and the statistic 
X'2 = S(0,~^)2A (17) 
to replace (11). This statistic is often called ' chi-dash-squared\ 
The use of x'2 would be similar to saying that di has  
approximately a normal posterior distribution with mean rjn and 
variance r Jn2; so that x'2 is the sum of squares of k standard 
normal variables with the single constraint 2^ = 1. But this is 
not too good an approximation as we saw in the case k = 2 in 
§7.2, where we took 6 to be approximately normal on the basis 
of maximum likelihood theory: the normal approximation gave 
symmetric confidence limits, whereas the true ones are  
asymmetric. Some of this asymmetry can be introduced by replacing 
168 APPROXIMATE METHODS [7.4 
Oi in the denominator of (17) by Ei9 and we saw above, in one 
numerical example, that this introduces the required degree of 
asymmetry. In general, the improvement in the approximation 
obtained by using Ei is reflected in the smaller size of the highest- 
order term neglected in the expansion of the logarithm of the 
posterior density. The term of order n~% in (8) is only half the 
magnitude of the corresponding term in (5), and the Jacobian 
of the transformation introduces a term of order «~*, equation 
(9), which, on the average is zero, since <f(<^ 10t) = 0. 
One great limitation of Pearson's statistic as developed in this 
section is that it is only available for testing a completely  
specified hypothesis, namely that all the 0/s are equal to assigned 
values p%. There are many situations where one wishes to test a 
hypothesis which only partially specifies the #/s. For example, 
we have described how to test the hypothesis that a random 
sample comes from a normal distribution of specified mean and 
variance, but we cannot, using the methods of this section, test 
the less specific hypothesis that it comes from some normal 
distribution. It is to the type of problems of which this is an 
example that we turn in the next section. 
7.5. Goodness-of-fit tests (continued) 
The notation is the same as that of the previous section. 
Theorem L If in each of a random sequence of n trials, the 
exclusive and exhaustive events Ai(i = 1, 2, ..., k) have constant 
probabilities of success dt and occur ri times (Srf = n); then an 
approximate significance test {valid as « -> oo) of the hypothesis 
that I (< k) functions, functionally independent of each other and 
of %di9 are all zero, 
0i(0i> d2> •••> °u) = 02(#i> d2, •••> Ok) = ••• = &(0i, 02, ••>, dk) 
= 0, (1) 
is obtained at level a. by declaring the data significant when 
X2 = S (r« - nB^lnS, (2) 
1 = 1 
exceeds xKfy the upper 100a% point of the ^-distribution 
7.5] GOODNESS-OF-FIT TESTS 169 
with I degrees of freedom, where Bi is the maximum likelihood 
estimate of di assuming that (1) obtains. 
We shall use the asymptotic result of theorem 7.4.1. That 
result was based on the remark that the posterior distribution 
had a maximum at 6i = rjn and that 8i = di-riln would be 
small, in fact of order «"*, so that only a small region in the 
neighbourhood of the maximum need be considered. In that 
region it will be assumed that the functions ^ of (1) are  
sufficiently smooth for them to be replaced by linear functions of the 
0's: that is, we suppose that the constraints ^ = 0 (j = 1,2,..., /) 
can be approximately written 
k 
S Oijdj = ct (3) 
3 = 1 
for i = 1, 2, ..., /. These may alternatively be written 
k 
S a'a^i = c'i9 (4) 
3 = 1 
k 
where a^ = %V(o)/w and c\ = ct— 2 a^r^n, and, as in §7.4, 
3 = 1 
^i = nfiiljri = Kd^rjn)!^. (5) 
Now it is well known (see, for example, most books on 
linear algebra) that 'an orthogonal k x k matrix with elements 
bti can be found such that the restrictions imposed on the A's by 
(4) are equivalent to the restrictions 
k 
2 byXj = di9 (6) 
3 = 1 
for i = 1,2,...,/. (For this to be so it is necessary that the ^'s, 
and hence the left-hand sides of (3), be independent functions of 
the #'s.) Define new parameters ^ by the equations 
k 
fi = S M;> (7) 
3 = 1 
for all i, so that the constraints (6) are ^ = dt for i < /. Then 
the hypothesis we are interested in testing is that i/rt = dt for 
i < /, and the parameters ^ for i > I are not of interest to us. 
In accord with the general principles of inference we have to 
170 APPROXIMATE METHODS [7.5 
find the posterior distribution of the ^'s involved in the  
hypothesis ; that is, ^ for i < /. 
Now from equation 7.4.6 
lnw(X|r) = C-\ SAJ (8) 
to the order of approximation being used. It immediately 
follows that to the same order 
ln7r(i|>|r) = C-\ SjfrJ (9) 
£ i=l 
since the Jacobian is unity. This result follows because the 
matrix of elements b^ is orthogonal. Consequently, integrating 
the posterior probability density with respect to ^+1, ^+2,..., tyk 
ln^i, f» -.., ^|r) = C-- S fl (10) 
A=i 
Also since the fts are independent of 2^, so are the i/rt9s for 
i < / and hence there is no linear relation between the i/r/s 
in (10). It follows as in theorem 6.4.2 that the distribution of 
i 
2 fti is X2with / degrees of freedom and hence that a significance 
i=l 
I 
test is obtained by declaring the data significant if 2 d\ exceeds 
i=i 
i 
^(/). It remains to calculate 2 ^f- 
i=i 
Since, in deriving (8), and hence (9), the prior distribution of 
the #'s was supposed uniform, the right-hand side of (9) is equally 
the logarithm of the likelihood of the ^'s, the ^'s being linear 
functions of the #'s. If (1) obtains this logarithmic likelihood is 
1 l 1 k 
with maximum obtained by putting ^ri — 0 for i > /, giving 
1 l 
A=l 
1 l 1 k 
Consequently ~ 2 d\ is equal to the value of ^ 2 AJ at the 
^i=i ^i=i 
maximum likelihood values of the #'s when (1) obtains. This 
7.5] GOODNESS-OF-FIT TESTS 171 
proves the result with rt instead of nBi in the denominator of (2). 
We may argue, as in the proof of theorem 7.4.1, that the  
replacement will not affect the result as n -> oo. The theorem is therefore 
proved. 
The quantity (2) may be written in Pearson's form 
2(0,-^)2/^, (11) 
provided the E{ are suitably interpreted. Ei is the expected 
number of occurrences of Ai if the maximum likelihood value 
under the null hypothesis were the true value. 
Special case: I = k — 1 
The corollary to theorem 7.4.1 is a special case of the above. 
In that corollary the null hypothesis being tested was completely 
specified so that the number of functions in (1) must be / = k — 1, 
which, together with 2^ = 1, completely determine the #'s. 
Since the #'s are completely determined at values pi it follows 
that 6i = Pi and (2) reduces to (7.4.10), the degrees of freedom 
being / = k— 1. 
Test for a binomial distribution 
In the last section we saw how to test the hypothesis that a 
random sample comes from a completely specified distribution; 
for example, binomial with index t s and parameter p, where 
s and p are given. The new result enables a test to be made of the 
hypothesis that a random sample comes from a family of  
distributions; for example, binomial with index s9 the parameter 
being unspecified. To do this suppose that we have n  
observations, all integer and between 0 and s, of independent random 
variables from a fixed, but unknown distribution which may be 
a binomial distribution with index s. Let the number of 
observations equal to / be r{ (0 < i < s\ so that Sr^ = n. Then 
the r's have a multinomial distribution, with index s +1 (since 
the original random variables are supposed independent and 
from a fixed distribution) and if the further assumption is made 
t We use s, instead of the usual n, for the index in order not to confuse it with 
the value n of the theorem. 
172 APPROXIMATE METHODS [7.5 
that the distribution is binomial, the parameters of the  
multinomial distribution will be, for 0 ^ i ^ s, 
ei = Qe%i-ey-i, (12) 
where 6 is the unknown binomial parameter. If 6 were specified, 
say equal to p, the 0i would be known by putting 0 = p in (12); 
the Ei would be ndi and Pearson's statistic could be used with 
s degrees of freedom. But if 0 is unknown it has to be estimated 
by maximum likelihood. The logarithm of the likelihood is, 
apart from a constant, Sr^ln^ (equation 7.4.3) or, by (12), 
s s 
S fr<ln0 + 2Cy-O^ln(l-0). (13) 
The maximum is easily seen (cf. §7.1) to be given by 
8 = £ irjns. (14) 
Consequently if 6, in (12), is put equal to 6 the resulting values 9i 
may be used to compute expected values nBi and Pearson's 
statistic compared with %2. 
Degrees of freedom 
It only remains to determine the degrees of freedom.  
According to the theorem the number of degrees of freedom is equal to 
the number of constraints on the di that the null hypothesis (12) 
imposes. The latter number is most easily determined by 
remarking that the number of #/s (the number of groups) is 
s +1, the k of the theorem, and hence the values of s ( = k— 1) 
constraints (apart from 20* = 1) are needed to specify the 
OfS completely: but the binomial hypothesis leaves only one 
value, 69 unspecified so that it must implicitly specify (s—l) 
constraints. Hence there are (s-l) constraints in (1): that is 
/ = s — 1 and the degrees of freedom are therefore s—l. 
This method of obtaining the degrees of freedom is always 
available and is often useful. In essence, one considers the 
number of constraints, additional to those in (1), that would 
have to be specified in order to determine all the #/s. If this 
number is m, then l+m = k—\ and the degrees of freedom are 
7.5] GOODNESS-OF-FIT TESTS 173 
/ = (k— l)-ra. There is a loss of one degree of freedom for 
each unspecified constraint in the null hypothesis. Each such 
constraint corresponds to a single unspecified parameter in the 
null hypothesis (here 6) and the rule is that there is a loss of one 
degree of freedom for each parameter that has to be estimated 
by maximum likelihood. Rather roughly, the ability to vary 
each of these parameters enables one to make the £^ agree more 
closely with the Oi9 so that the statistic is reduced, and this is 
reflected in a loss of degrees of freedom—the mean value of x2 
being equal to the degrees of freedom (§5.3). The result on the 
reduction in the degrees of freedom is due to Fisher; Pearson 
having been under the mistaken impression that no correction 
for estimation was necessary. 
Test for Poisson distribution 
Similar ideas may be applied to any discrete distribution. 
Another example would be the Poisson distribution; but there 
the number of possible values would be infinite, namely all non- 
negative integers, and some grouping would be necessary, as 
explained in §7.4. Usually it will be enough to group all  
observations not less than some value c, and suppose them all equal 
to c, the value of c being chosen so that the expectation is not 
below about two. This introduces a slight error into the  
maximum likelihood estimation, but this is not likely to be serious. 
It is important to notice the difference between this type of 
test and the test for Poisson distributions in §7.4. The latter 
was for the differences between Poisson means; that is, the  
distributions were assumed, because of prior knowledge, to be 
Poisson and only their means were compared. The present test 
supposes it known only that the variables come from some 
distribution and asks if that distribution could be Poisson. 
There is obviously less prior knowledge available here and 
consequently a larger sample is needed to produce a useful test. 
Continuous distributions 
The same ideas may be applied to fitting a continuous  
distribution : for example, we might ask if the variables come from 
an unspecified normal distribution. The observations would be 
174 APPROXIMATE METHODS [7.5 
divided into k convenient groups and the numbers rt in the 
different groups counted. It only remains to find the maximum 
likelihood estimates of the 6i when the null hypothesis is true. 
If the null hypothesis distribution depends on parameters 
al9 a2, ..., as and has density/?(*I ai> a2> •••> as)> the #/s are given 
by (cf. equation 7.4.12) 
/•Ot + i 
ei = P(x\ocl9 a2, ..., ocs)dx9 (15) 
J (H 
and the logarithm of the likelihood is as usual. The maximum 
likelihood estimation presents some difficulties because the 
equations to be solved are 
S rtd^WJta, = 0 (j= 1, 2, ..., s) (16) 
i=i 
and the partial derivatives can be awkward. It is usual to 
approximate to (15) by assuming the density approximately 
constant in the interval when, if & is some point in the interval, 
ei = P(£i I <*i> <*2> • • •, ocs) (ai+1 - a%). (17) 
The maximum likelihood equations are then 
k 
2 ndlnpi^loc^ cc29 ..., a«)/ax, = 0 (J = 1, 2, ..., s). (18) 
i=i 
If each observation x* (t = 1, 2, ..., n) is replaced by the value 
ii of the group to which it belongs (there then being r{ x9s which 
yield a value ^), equations (18) are the ordinary maximum  
likelihood equations for the a's when the sample values are the £'s 
and the density is p(x\al9 ot2,..., as). These may be easier to 
solve than (16). Thus with the normal case where there are two 
parameters, the mean and the variance, this gives the usual 
estimates (equation 7.LI2—where the present a's are there #'s) 
in terms of the grouped values, ^. These estimates can then be 
inserted in (17), or (15), to obtain the Bi% The procedure may be 
summarized as follows: (i) replace the observed values xt by the 
grouped values &; (ii) estimate the parameters in the density by 
maximum likelihood applied to the £i9 (18); (iii) estimate the 6€ 
from (15) or (17); (iv) use these estimates in the x2 statistic (2), 
7.5] GOODNESS-OF-FIT TESTS 175 
subtracting a degree of freedom for each parameter estimated. 
Thus with the normal distribution the degrees of freedom will 
be(ifc-l)-2 = ifc-3. 
Sheppard's corrections 
The replacement of (16) by (18) can lead to systematic errors. 
They can be reduced by a device which we illustrate for the case 
where there is a single unspecified parameter in the family of 
distributions, which will be denoted in the usual way by d, and 
where the grouping intervals are equal (except for the end ones 
which will have to be semi-infinite). Equation (15) may be 
written ,{i+hh 
p(x\d)dx (19) 
0< 
24 p 
for a convenient origin. An expansion in Taylor series gives an 
improvement on (17), namely 
hs 
Oi = hp + ^p", 
where the arguments of/? are ih (formerly £^) and d, a remainder 
term has been ignored, and dashes denote differentiation with 
respect to x. Consequently 
T h2 v"l 
ln^ = lnAp + ln I+54 = ^hp + 
to the same order, and 
We have to solve (16) with ax = 6. This is easily seen to be 
equivalent to summing the right-hand side of (20) over the 
observations at their grouped values and equating to zero. (The 
equation is the same as (18) with the addition of the term in h2.) 
This equation can be solved by Newton's method (§7.1) with 
a first approximation given by #(1), the root of (18). The second 
approximation is seen, from (7.1.17), to be #(2) = #(1) + A, where 
■-UsMj) I k^4 (21) 
Here the xt have been replaced by their grouped values and the 
176 APPROXIMATE METHODS [7.5 
other argument of/? is #(1). One iteration is usually sufficient. 
The 0/s can then be obtained from (19) with 6 = 0(2). Generally 
#(2) will be nearer the true maximum likelihood estimate than #a). 
The method extends without difficulty to several parameters. 
A correction related to A (equation (21)) was obtained by 
a different method by Sheppard and it is sometimes known as 
Sheppard's correction. 
Prior knowledge 
This method of testing goodness of fit of distributions has the 
disadvantage that the observations have to be grouped in an 
arbitrary way. Against this it has the advantage that a  
reasonable prior distribution for the grouped distribution is easy to 
specify. For example, it would be difficult to specify a  
parametric family, and an associated prior distribution of the  
parameters, appropriate for investigating the null hypothesis that a 
distribution was normal, unless some definite prior knowledge 
was available about possible alternatives to the normal  
distribution. In the absence of such prior knowledge the %2-test is a 
useful device. 
The main application of theorem 1 will be described in the 
next section. 
7.6. Contingency tables 
Theorem 1. If, in each of a random sequence of n trials, the 
exclusive and exhaustive events Al9 A2, ..., As(s > 1) occur with 
unknown constant probabilities, dim, of success; and if in the same 
sequence, the exclusive and exhaustive events Bl9 B29 ..., Bt (t> 1) 
also occur with unknown constant probabilities, 0mj9 of success; 
then, if 6tj is the probability that both At and Bj occur in a trial, 
an approximate significance test of the null hypothesis that for 
allUJ, 
On = 0t6.i (1) 
for unspecified di% and dmj is obtained at level oc by declaring the 
result significant if 
i sfa-^y") (2) 
7.6] CONTINGENCY TABLES 177 
exceeds xl(v)> where v = (s — 1) (t — 1). In (2), r^ is the number 
of times that both Ai andBj occur in the n trials, so that 2 r%j = n, 
and\ t 
r%. — S rij9 r.j — 2 f#> (3) 
j=i i=i 
$0 f/*af ffese are respectively the number of times that Ai9 and the 
number of times that Bj9 occur in the n trials. 
The situation here described is essentially the same as that of 
theorem 7.5.1 with a random sequence of n trials and k = st 
exclusive and exhaustive events, namely the simultaneous  
occurrence of Ai and Bj9 the events AiBj in the notation of § 1.2. The 
null hypothesis (1) puts some constraints on the probabilities of 
these events and says that the events Ai and Bj are independent, 
that is 
p{AiB0) = p(Ax)p(B0)9 
without specifying the individual probabilities of the A9s and B's. 
Theorem 7.5.1 can therefore be applied. We have only to  
determine (a) the maximum likelihood estimates of 6^ under the null 
hypothesis, (1), and (b) the number, /, of constraints involved 
in(l). 
(a) The logarithm of the likelihood is, if (1) obtains, 
C+E/yM^.fl.,) = C+S/Y.ln^. + S^.lntf.,.. 
Remembering that S ^. = S ^.i = 1 we see easily (compare 
• • 
the proof of theorem 7.4.1) that the maximum likelihood  
estimates of 0i% and dmj are respectively rimjn and rmjjn. Hence 
*0ii = rt.rjn. (4) 
(b) As in the applications in the last section it is easiest to 
calculate / by finding the number of parameters unspecified in 
the null hypothesis. Here it is obviously the number of  
functionally independent 0i% and 6mj9 that is, (s—l) + (t— 1). Hence 
/= (st-l)-(s-l)-(t-l) = st-s-t+l = (s-l)(t-l) = v. 
f Notice that the dot suffix is here used slightly differently from previously 
(§6.4). Thus rim denotes the sum over the suffix j, not the mean. 
12 
LSII 
178 APPROXIMATE METHODS [7.6 
Inserting the values of Bti given by (4) into equation 7.5.2 
gives (2). This, with the value of /just obtained, establishes the 
result. 
Theorem 2. Consider s > 1 independent random sequences of 
trials, with rim trials in the ith sequence (i = 1,2, ...,s) and 
Srt-. = n. Suppose that in each of these trials the exclusive and 
exhaustive events Bl9 B2,..., Bt occur with constant probabilities 
t 
<f>ij(j = 1,2,..., 0 in the ith sequence, so that 2^=1 for all i. 
Then an approximate significance test of the null hypothesis that 
faj does not depend on i: that is, that for all /, j9 
<Pij = 4>i (5) 
for unspecified <pj9 is obtained by the test of theorem 1. In (2) 
r^ is the number of times Bj occurs in the ith sequences of ri% 
t 
trials, so that rt = 2 rij9 the number of trials in the ith sequence; 
and rmj is as defined in (3), the total number of times Bj occurs 
in all trials. 
Consider the situation described in theorem 1. The set of all 
rij9 written {rti}9 referring to the events AtBJ9 has probability 
density 8 t 
xww«n nqy, (6) 
and similarly the set of rim9 written {rim}9 referring to the events 
Ai9 has probability density 
Xfo.m^}) <* n a?: = n n e&. o) 
i=l i=l j = l 
Hence, dividing (6) by (7), 
P({r<i}\{r<.}> {°S« & ft (OdOtJv. (8) 
i=i j=i 
t 
Let $i} = dijjdim9 so that S i>a = 1 f°r all i, and change from a 
i=i 
parametric description in terms of {0ti} to one in terms of {0im} 
and {<pi:}}. Then what has just been proved may be written 
PdrM.}, {&*}) = P({^}m.})P({r'iM{^l W). 
A comparison of this equation with equation 5.5.21 shows that, 
given {fa}, KJ is sufficient for {dt}; and, what is more relevant, 
7.6] CONTINGENCY TABLES 179 
given {9im}9 {rim} is ancillary for {^}. By the argument 
leading to equation 5.5.22, if these two sets of parameters have 
independent prior distributions, inferences about {^} may be 
made with {rim} either fixed or random. But the right-hand side 
of (8) is the likelihood in the situation of the present theorem, 
(6) is the likelihood in theorem 1, and in that theorem inferences 
were made about {<t>i0)\ namely that 0# = dmj9 or fa does not 
depend on j. This is equivalent to (5). Hence the same test may 
be used here as in theorem 1, with such a prior distribution. But 
the test is only asymptotic and the exact form of the prior 
distribution is almost irrelevant. 
Variable margins 
The situation described in theorem 1 arises when, in each trial, 
the outcome is classified in two ways into exclusive and exhaustive 
categories and we wish to know whether the classifications are 
related to each other, or, as we sometimes say, are contingent on 
each other. The data may be written in the form of an s x t 
contingency table, with s rows corresponding to the  
classification into the A classes, and t columns corresponding to the iTs. 
The rim and rmj are often referred to as the margins of the table 
(compare the definition of a marginal distribution in §3.1). For 
example, if we take a random sample of n school-children of a 
given age-group from a city, and classify them according to 
height and social class, where the range of heights has been 
divided into s convenient groups, and there are t social classes. 
The test would be applicable if we wanted to know whether the 
childrens' heights are influenced by the social class that they 
come from. If they are not influenced, then an event A{9 having 
a certain height, would be independent of an event Bj9 belonging 
to a certain social class, and the null hypothesis would be true. 
The r^ are the observed numbers, Oij9 say, of children of a given 
height and social class. The expected numbers of children of that 
height and social class, Eij9 say, are given by 
En = r<.r.jl* (9) 
and the test statistic (2) may be written 
S(0,,-^.)2/^r (10) 
12-2 
180 APPROXIMATE METHODS [7.6 
The choice of expected numbers can be justified intuitively in 
the following way. We do not know, and the null hypothesis 
does not say, what the probability, 0im9 of Ai9 is; but, ignoring 
the other classification into the iTs, our best estimate of it is 
about rimln9 the observed proportion of A^s. If the null  
hypothesis is true (and remember the nd^ = Etj are calculated on 
this assumption) then the proportion of A/s should be the same, 
irrespective of the classification into the B9s. So if we consider 
the rmj members of class Bj the expected proportion of At9s 
amongst them would still be r^. /n, and so the expected number 
in both Ai and Bj is rir^\n = E€i agreeing with (9). The formula 
for the expected number is easily remembered by noting that Etj 
corresponding to Otj = rtj is the total (rim) for the row  
concerned, times the total (rmj) for the column concerned, divided 
by the grand total, t 
Example 
In §7.4 we discussed a genetical example involving two genes. 
The individuals may be thought of as being classified in two 
ways, A or A, B or B, so that s = t = 2 and we have a 2 x 2 
contingency table: _ 
B B 
6 
16 
13 
61 
22 74 96 
Thus rn = 6, rlu = 19, rml = 22, n = 96, etc. The expected 
number in the class AB is 19 x 22/96 = 4-35, the other classes 
being found similarly, and (10) is equal to 1-02. The upper 5 % 
point of x2 on one degree of freedom is 3-84; since the statistic 
is less than this there is no reason to think that the frequency of 
^4's (against Z's) depends on B. Notice the difference between 
this test and that of §7.4. The earlier test investigated whether 
0U = 1/16, d12 = 3/16, 621 = 3/16, 622 = 9/16, in the notation 
of this section. The present test investigates whether d^ = 6im6tj 
t In an extension of our notation n = r... 
7.6] CONTINGENCY TABLES 181 
for all /, j, and a little calculation shows that this is equivalent 
to #n/#i2 = #2i/#22- The question asked here is less specific than 
that of §7.4. 
The computational problem of dealing with a contingency 
table of general size may be reduced in the case of a 2 x 2 table 
by noting that (2) may, in that special situation, be written 
y li **22 ~ ^12^21) n 
'*l.'\l/2.'\2 
This can be proved by elementary algebra. 
One margin fixed 
The situation considered in theorem 2 is rather different. To 
change the language a little: in the first theorem we have a single 
sample of n individuals, each classified in two ways; in the 
second theorem we have s independent samples in which each 
individual is classified in only one way. However, the second 
situation may be related to the first by regarding the s samples 
as another classification, namely classifying the individual 
according to the number of the sample to which he belongs. 
The first stage in the proof of theorem 2 is to show that in the 
first situation with n individuals, the conditional probability 
distribution of the individuals with regard to the second (the B) 
classification given the first (the A) classification is the same as 
in the second situation, with the ^-classification being into the 
different samples. Or, more mathematically, the probability 
distribution of the entries in the table (the rid) given one set of 
margins (rim) is the same as the distribution of s samples of 
sizes rim. Furthermore the distribution of the margin does not 
depend on the parameters of interest. The situation therefore is 
exactly that described in equation 5.5.21 with 62 there being the 
present ^'s. Consequently inferences about the ^'s may be made 
in the same way whether the margins are fixed or random. If 
they are random theorem 1 shows us how to make the  
significance test. Therefore the same test can be used in the situation 
of theorem 2 with the margin fixed. There is another way of 
looking at the same situation. As an example of an ancillary 
statistic in §5.5 we took the case of a random sample of size n, 
182 APPROXIMATE METHODS [7.6 
where n may have been chosen randomly according to a  
distribution which did not depend on the parameter of interest. We 
saw that n was ancillary and could therefore be supposed fixed. 
Here we have s samples of different sizes, possibly random, 
ri.? r2.9 • ••> ^.; these sizes having a distribution which does not 
depend on the parameters, <pij9 of interest. Exactly as before 
they may be supposed fixed. 
The null hypothesis tested in theorem 2 is that the probabilities 
of the events Bj are the same for all samples: that is, the  
classification by the B's is not contingent on the sample (or equiva- 
lently the classification by the A9s). It would arise, for example, 
if one took random samples of school-children of a given age- 
group in a city from each of the social classes, and asked if the 
distribution of height varied with the social class. The  
computations would be exactly the same as in the former example where 
the numbers in the social classes were also random. 
Binomial distributions 
The situation of theorem 2 with s = t = 2 is the same as 
that considered in §7.2 for the comparison of the probabilities 
of success in two random sequences of trials. The %2-test 
provides a much more widely used alternative to that using 
equation 7.2.17 based on the logarithmic transformation of the 
odds. The earlier test has the advantage that it can be extended 
to give confidence intervals for the ratio of odds in the two 
sequences. It is not common practice to quote confidence 
limits in dealing with contingency tables for much the same 
reasons as were discussed in §6.5 in connexion with the analysis 
of variance. But, where possible, this should certainly be done; 
or better still, the posterior distribution of the relevant  
parameter, or parameters, quoted. 
The situation of theorem 2 with t = 2 and general s is the 
comparison of several probabilities of success in several random 
sequences of trials. The distribution of the ra will be B(rim9 <f>n) 
and the null-hypothesis is that 0a = $ for all j, with $  
unspecified. It is easily verified that if rim = n9 say,f the same for all /, 
t We use n because it is the index of the binomial distribution. The n of the 
theorems is now sn. 
7.6] CONTINGENCY TABLES 183 
so that the test is one for the equality of the parameters of 
binomial distributions of the same index, the test statistic (2) 
reduces to s 
S(r,-r)2 
i=i= =-, (11) 
r n — r 
n 
n n 
s 
where we have written rn = ri9 ri2 = n — ri and f = 2 rils- The 
form of this statistic is interesting and should be compared with 
that of equation 7.4.16. It is (s — 1) times the usual estimate of 
the variance of the ri9 divided by an estimate based on the 
assumption that the distributions are binomial with fixed 0, 
namely @\r?) = n<p(l-<p) with <p replaced by r\n. The statistic 
(11) is also known as a coefficient (or index) of dispersion. It 
can be referred to x2 with (s — 1) degrees of freedom. The test 
should not be confused with that of §7.5 for testing the goodness 
of fit of the binomial distribution: here the distributions are 
assumed binomial and the investigation concerns their means. 
Example 
It is sometimes as instructive to see when a result does not 
apply as to see when it does. We now describe a problem 
where a comparison of binomial probabilities seems  
appropriate but, in fact, is not. The papers of 200 candidates for an 
examination were marked separately by the examiners of two 
examining boards and were thereby classified by them as pass or 
fail. The results are shown below: 
Board B 
, * , 
Pass Fail Total 
^ A A (Pass 136 2 138 
Board A {Fafl 16 46 62 
Total 152 48 200 
The percentage failure for board A (31 %) is higher than for 
board B (24 %) and one might contemplate comparing these by 
the method just described for the difference of two binomial 
proportions, or by the method of §7.2. But that would be quite 
false because the two samples are not independent: the same 
candidates are involved in both samples of 200. Again the mere 
look of the table may suggest carrying out a %2-test (theorem 1) 
184 APPROXIMATE METHODS [7.6 
on a 2 x 2 contingency table, but this would ask whether the 
proportion passing with board B was the same for those who 
failed with board A as for those who passed with board A, 
a question of no interest. What one wishes to know is whether 
one board has a higher failure rate than the other—the question 
that would have been answered by the first test had it been 
valid. To perform a test we note that the 182 candidates who 
were treated similarly by the two boards (both failed or both 
passed) can provide no information about the differences 
between the boards: so our interest must centre on the 18 other 
candidates. We can see this also by remarking that in the table 
as arranged the likelihood is, assuming the candidates to be 
a random sample, 
[flu+022)rii+r22i l(0i,+e21y^ 
= {^ii+r22(l _ 1/riyi2+r2i} {^11(1 - i/r2)r™} {^2(1 - ^3)r21}, (12) 
where ^ = dn + 6229 i/r2 = 0U/(0U + d22) and i/r3 = d12l(d12 + 621). 
We are only interested in i/rz which is the probability, given that 
the candidate has been treated differently by the two boards, 
that he has been failed by B and passed by A. By the principle 
of ancillary statistics we can therefore consider only r12 and r2l9 
provided our prior judgment is that ^r1 and i/r2 are independent 
of ^3, which is reasonable. From the last set of braces in (12), 
the likelihood is seen to be of the form appropriate to r12 + r21 
independent trials with constant probability i/rz. If the boards 
are acting similarly i/rz = \ and it is this hypothesis that we 
must test. The exact method is that of §7.2. From equation 
7.2.13 a confidence interval for i/rz is 
(1+8F)-1 < i/r3 < (1+8F-1)-1. 
With 4 and 32 degrees of freedom and a = 0-05, F = 3-22 (using 
\ol = 0-025) and P = 8-45 so that 0-015 < ^3 < 0-29 with 
probability 0-95. The value \ is outside the interval and there 
is evidence that board A is more severe than board B. 
7.6] SUGGESTIONS FOR FURTHER READING 185 
Suggestions for further reading 
The suggestions given in chapter 5 are adequate. 
Exercises 
1. An individual taken from a certain very large biological population is 
of type A with probability i(l + F) and of type B with probability i(l - ^0- 
Give the probability that a random sample of n such individuals will consist 
of a of type A and b of type B. Find F, the maximum likelihood estimate 
of F. Show that the expectation of F is F. Calculate the information 
about F. (Camb. N.S.) 
2. A random sample is drawn from a population with density function 
M°) = ]4^ee-dx (0^*< 1). 
Show that the mean of the sample is a sufficient statistic for the parameter 
6 and verify that the maximum likelihood estimate is a function of the 
sample mean. Let this estimate be denoted by Qx. 
Suppose that the only information available about each sample member 
concerns whether or not it is greater than a half. Derive the maximum 
likelihood estimate in this case and compare its posterior variance with 
that of 0lm (Wales Dip.) 
3. A continuous random variable x, defined in the range 0 ^ x < ^n, has 
distribution function proportional to (l — e-a8inx) where a > 0. Find the 
density function of x. 
Given a random sample of n observations from this distribution, derive 
the maximum likelihood equation for a, the estimate of a. Indicate very 
briefly how this equation can be solved numerically. 
Also prove that the posterior variance of a is 
4*2sinh2ja -. G v 
«(4sinh2ia-a2)' (i^ic. uen.j 
4. Calculate the maximum likelihood estimate, 6, of 69 the parameter of 
the Cauchy distribution 
1 1 
dF = - r—t ^ dx (-oo < x < oo), 
given the sample of 7 observations, 
3-2, 20, 2-3, 10-4, 1-9, 0-4, 2-6. 
Describe how you could have used this calculation to determine  
approximate confidence limits for 6, if the sample had been a large one. 
(Manch. Dip.) 
5. Independent observations are made on a Poisson variable of unknown 
mean 0. It is known only that of n observations, n0 have the value 0, 
nx have the value 1, and the remaining n — n^ — nx observations have values 
186 
APPROXIMATE METHODS 
greater than one. Obtain the equation satisfied by the maximum likelihood 
estimate and suggest how the equation could be solved numerically. 
(Lond. B.Sc.) 
6. The random variables Xl9 ..., Xm, Xm+l9 ..., Xm+n are independently, 
normally distributed with unknown mean 0 and unit variance. After 
Xl9 ..., Xm have been observed, it is decided to record only the signs of 
Xm+1,..., Xn. Obtain the equation satisfied by the maximum likelihood 
estimate 6, and calculate an expression for the asymptotic posterior 
variance of 0. (Lond. M.Sc.) 
7. In an experiment on the time taken by mice to respond to a drug, the 
log dose Xi received by mouse i is controlled exactly, and the log response 
time ti is measured individually for all mice responding before log time T. 
The results of the experiment consist of (n — r) pairs (xi9 tt) with tt < T, 
and r pairs in which x{ is known exactly, but ti is 'censored', and is known 
only to be greater than T. It is known that u is normally distributed with 
known uniform variance a2, about the regression line 
^(U) = oi+fixi9 
where a and fi are unknown. Verify that the following iterative procedure 
will converge to the maximum likelihood estimates of a and /?. 
(i) Using preliminary estimates ax and bl9 calculate y for each 'censored' 
observation from the formula 
y4 = ax + bxXi + avy V /' 
where v(u) is the ratio of the ordinate to the right-hand tail of the  
distribution N(Q9 1) at the value u. 
(ii) For non-censored observations, take yt = tt. 
From the n pairs (xi9 yi) calculate the usual least squares estimates of 
regression parameters.t Take these as a2, b2, and repeat until stable values 
are reached. (Aberdeen Dip.) 
8. Serum from each of a random sample of n individuals is mixed with 
a certain chemical compound and observed for a time T, in order to 
record the time at which a certain colour change occurs. It is observed 
that r individuals respond at times tl912, ..., tr, and that the remaining 
(n — r) have shown no response at the end of the period T. The situation is 
thought to be describable by a probability density function oce-at (0 ^ i) 
for a fraction fi of the population (0 < fi ^ 1) and complete immunity to 
the reaction in the remaining fraction (l—fi). 
Obtain equations for a, /?, the maximum likelihood estimates of a, /?. 
If the data make clear that a is substantially greater than l/T, indicate how 
you would solve the equations. By consideration of asymptotic variances 
of the posterior distribution, show that, if a is known and clT is small, 
taking a sample of 2n instead of n may be much less effective for improving 
the precision of the estimation of fi than the alternative of observing for 
time 2T instead of for T. (Aberdeen Dip.) 
t Defined in equation 8.1.21. 
EXERCISES 
187 
9. A new grocery product is introduced at time t = 0. Observations are 
made for a period T on a random sample of n households. It is observed 
that m of these buy the product for the first time at times tl9 ...,tm and 
that in the remaining n — m households the product has not been bought 
by the end of the period T. 
It is suggested that a fraction 1 — 0 of the population are not susceptible 
and will never buy the product and that for the susceptible fraction the 
time of first buying has an exponential probability density function oce~at. 
Show that the likelihood of the observations is proportional to 
(0a)™exp[-mod] (1 - 0 + 0exp[-ocT])n~m9 
where 1 = Zti/m. Hence obtain the "equations satisfied by the maximum 
likelihood estimates $, d of the unknown parameters 6, a and show that 
in particular 
7 = 1 1 
T dT exp[a71-r 
Indicate how you would determine d numerically. (Lond. B.Sc.) 
10. A household consists of two persons, either of whom is susceptible to 
a certain infectious disease. After one person has contracted the infection 
a period of time U elapses until symptoms appear, when he is immediately 
isolated, and during this period it is possible for him to transmit the 
infection to the other person. Suppose that U is a random variable with 
probability density fie~Pu (0 < u < oo) and that for given U the probability 
that the other person has not been infected by a time t after the beginning 
of the period is e-at (0 < t < U). What is the probability of both persons 
being infected, given that one person has contracted the infection from 
some external source, assuming that the possibility of the second person 
contracting the infection from an external source may be neglected? 
Out of n households in which infection has occurred there are r in which 
only one person is infected and for the remaining s, the intervals between 
the occurrence of the symptoms for the two persons are tl912, ..., t3. 
Assuming that the probabilities for these households are all mutually 
independent, write down the likelihood function of the data, and show 
that the maximum likelihood estimates of a and fi are 
d = s2/rT, p = s/T9 where T= f U. 
Determine also the asymptotic posterior variances of a and /?. 
(Camb. Dip.) 
11. A survey is carried out to investigate the incidence of a disease. The 
number of cases occurring in a fixed period of time in households of 
size k is recorded for those households having at least one case. Suppose 
that the disease is not infectious and that one attack confers immunity. 
Let/? = \—q be the probability that any individual develops the disease 
188 
APPROXIMATE METHODS 
during the given period independently of other individuals. Show that the 
probability distribution of the number of cases per household will be 
P(r\p) = (*) prqk-r(l -q*)-1 (r = 1, 2, ..., k). 
Derive the equation for the maximum likelihood estimate, p, of p and 
show that, iip is small, 
where r is the average number of cases per household. Fit this distribution 
to the following data for A: = 3, and, if your results justify it, estimate the 
number of households with no cases. 
Distribution of number of cases per household 
(k =3) 
Frequency 
1 
390 28 
Total 
425 
(Wales Dip.) 
12. A new type of electric light bulb has a length of life, whose probability 
density function is of the form 
f(x) = ^e-xix (X>0). 
Forty bulbs were given a life test and failures occurred after the following 
times (in hours): 
196, 327, 405, 537, 541, 660, 671, 710, 
786, 940, 954, 1004, 1004, 1006, 1202, 1459, 
1474, 1484, 1602, 1662, 1666, 1711, 1784, 1796, 
1799. 
The tests were discontinued after 1800 hours, the remaining bulbs not 
then having failed. 
Determine the maximum likelihood estimate of A, and obtain an 
approximate 95 % confidence interval for this parameter. Hence, or 
otherwise, determine whether these results justify the manufacturer's 
claim that the average life of his bulbs is 2000 hours. (Leic. Gen.) 
13. The random variables Y, Z have the form 
Y = X+ U, Z= X+V, 
where X, U, V, are independently normally distributed with zero means 
and variances 0,1,1. Given n independent pairs (yl9 zx), ..., (yn, zn), 
obtain the maximum likelihood estimate 0. What is the posterior  
distribution of 6 ? (Lond. M.Sc.) 
EXERCISES 
189 
14. A random sequence of n trials with initially unknown constant 
probability of success gives r successes. What is the probability that the 
(«+l)st trial will be successful? Consider in particular the cases r — 0 
and r = n. 
15. A random sample of size n from P(6) gave values rl9 r2, ..., rn, and the 
prior knowledge of 0 was small. Determine the probability that a further 
random sample of size m from P(26) will have mean s. 
16. A random sample from B(2Q, 0) gave the value r = 7. Determine 
50, 95 and 99 % confidence limits for 0 by the following methods and 
compare their values: 
(i) the exact method of theorem 7.2.1 and its first corollary: 
(ii) the approximation of the second corollary to that theorem; 
(iii) the inverse-sine transformation (§ 7.2); 
(iv) x2, with and without the finite correction for small prior knowledge. 
17. You are going to play a pachinko machine at a fair and can either 
win or lose a penny on each play. If 0 is the probability of winning, you 
know that 6 ^ \ since otherwise the fair owner would not continue to use 
the machine (though there is always the slight chance that it has gone 
wrong, for him, and he has not noticed it). On the other hand, 0 cannot 
be too much less than \ since then people would not play pachinko. 
Suggest a possible prior distribution. You play it 20 times and win on 
7 of them. Find 95 % confidence limits for 0 and compare with the 
corresponding limits in the previous exercise. 
18. Initially, at time t = 0, a Poisson process is set to produce on the 
average one incident per unit time. It is observed for a time rand incidents 
occur at tl912, ..., tn. It is then learnt from other information that at time T 
the Poisson process is working at a rate of two incidents per unit time. If 
it is believed that the process doubled its rate at some instant of time, 6, 
between 0 and T and that 0 is equally likely to be anywhere in (0, 71), 
discuss the form of the posterior distribution of 6 given the observed 
pattern of incidents in (0, T). 
19. The table shows the number of motor vehicles passing a specified 
point between 2.00 p.m. and 2.15 p.m. on 6 days in 2 successive weeks. 
Mon. Tues. Wed. Thurs. Fri. Sat. 
Week 1 50 65 52 63 84 102 
Week 2 56 49 60 45 112 90 
The flow of vehicles on any day is believed to conform to a Poisson 
distribution, in such a way that the mean number of vehicles per 15 minutes 
is A on Monday to Thursday, ft on Friday and Saturday. Indicate how you 
would test deviations from either Poisson distribution. Obtain 95 %  
confidence limits for A and fi. Test the hypothesis 2A = /i. (Aberdeen Dip.) 
190 
APPROXIMATE METHODS 
20. The table shows the numbers of births (rounded to nearest 100) and 
the numbers of sets of triplets born in Norway between 1911 and 1940. 
Period 
1911-15 
1916-20 
1921-25 
1926-30 
1931-35 
1936-40 
Total 
births 
308,100 
319,800 
303,400 
253,000 
222,900 
227,700 
Set of 
triplets 
52 
52 
40 
30 
24 
20 
Apply X2 to test the significance of differences between periods in the 
frequencies of triplets, explaining and taking advantage of any convenient 
approximations. Discuss the relation of your test to a test of homogeneity 
of Poisson distributions. Indicate briefly how any linear trend in the  
proportion of triplet births might be examined. (Aberdeen Dip.) 
21. Cakes of a standard size are made from dough with currants randomly 
distributed throughout. A quarter is cut from a cake and found to contain 
25 currants. Find a 95 % confidence interval for the mean number of 
currants per cake. (Camb. N.S.) 
22. Past observations of a bacterial culture have shown that on the 
average 2-J % of the cells are undergoing division at any instant. After 
treatment of the culture 200 cells are counted and only 1 is found to be 
dividing. Is this evidence that the treatment has decreased the ability of 
the cells to divide? (Camb. N.S.) 
23. The following table shows the result of recording the telephone calls 
handled between 1 p.m. and 2 p.m. on each of 100 days, e.g. on 36 days 
no calls were made. Show that this distribution is consistent with calls 
arriving independently and at random. Obtain (on this assumption) a 
99 % confidence limit for the probability that if the operator is absent for 
10 minutes no harm will be done. 
Calls 0 1 2 3 4 or more 
Days 36 35 22 7 0 
(Camb. N.S.) 
24. Bacterial populations in a fluid suspension are being measured by 
a technique in which a given quantity of the fluid is placed on a microscope 
slide and the number of colonies on the slide is counted. If the laboratory 
technique is correct, replicate counts from the same culture should follow 
a Poisson distribution. Six slides prepared from a certain culture give the 
counts: 105, 92, 113, 90, 97, 102. Discuss whether there is any evidence 
here of faulty laboratory technique. (Camb. N.S.) 
25. In an investigation of the incidence of death due to a particular cause 
it is required to find whether death is more likely at some times of the day 
EXERCISES 
191 
bserved 
315 
101 
108 
32 
Theoretical 
312-75 
104-25 
104-25 
34-75 
than at others. In 96 cases the times of death are distributed as follows in 
3-hourly intervals beginning at the stated times. 
 
Midnight 3 a.m. 6a.m. 9a.m. Noon 3p.m. 6p.m. 9p.m. Total 
19 15 8 8 12 14 9 11 96 
Do these data provide adequate evidence for stating that the chance of 
death is not constant? 
Indicate, without doing the calculations, what test you would have used 
had it been suggested from information unconnected with these data, that 
the chance of death is highest between midnight and 6 a.m. Why is the 
qualification in italics important? (Camb. N.S.) 
26. In some classical experiments on pea-breeding Mendel obtained the 
following frequencies for different kinds of seed in crosses with round 
yellow seeds and wrinkled green seeds: 
Ol 
Round and yellow 
Wrinkled and yellow 
Round and green 
Wrinkled and green 
556 55600 
The column headed 'Theoretical' gives the numbers that would be 
expected on the Mendelian theory of inheritance, which predicts that the 
frequencies should be in proportions 9, 3, 3,1. Can the difference between 
the observed and theoretical figures be ascribed to chance fluctuations ? 
(Camb. N.S.) 
27. In an experiment to investigate whether certain micro-organisms 
tended to move in groups or not, 20 micro-organisms were placed in the 
field of view A of a microscope and were free to move in a plane of which 
A was a part. After a lapse of time the number of micro-organisms 
remaining within A was counted. The experiment was repeated six times 
with the following results: 
7, 12, 4, 3, 16, 17. 
Perform a significance test designed to test the hypothesis that the 
micro-organisms move independently of each other, explaining carefully 
the appropriateness of your analysis to the practical problem described. 
(Camb. N.S.) 
28. The independent random variables Xlt..., Xn follow Poisson  
distributions with means /il9..., fin. The hypotheses Hl9 H2i Hz are defined as 
follows: fTj :/*!,..., /*„ are arbitrary; 
H2: fa = ot + ifi (i = 1, ..., n); 
H*\ fii = y, 
where a, /?, y are unknown. Develop tests for: 
(i) the null hypothesis H2 with Hx as alternative; 
(ii) the null hypothesis H* with H2 as alternative. 
(Lond. M.Sc.) 
192 
APPROXIMATE METHODS 
29. An observer records the arrival of vehicles at a road bridge during 
a period of just over an hour. In this period 54 vehicles reach the bridge, 
the intervals of time between their consecutive instants of arrival (in 
seconds) being given in numerical order in the table below. 
It is postulated that the 53 intervals t between consecutive arrivals are 
distributed independently according to the exponential distribution 
(or, equivalently, that the values of 2f/A are independently distributed as x2 
with two degrees of freedom). Test this hypothesis. 
Observed values of t 
2 11 29 44 77 148 
4 
6 
7 
9 
9 
9 
10 
10 
15 
17 
18 
19 
19 
28 
29 
29 
31 
33 
34 
35 
37 
38 
38 
43 
47 
50 
51 
53 
61 
68 
73 
74 
97 
114 
114 
116 
121 
124 
135 
146 
158 
163 
165 
180 
203 
340 
393 
(Manch. Dip.) 
30. A suspension of particles in water is thoroughly shaken. 1 c.c. of the 
suspension is removed and the number of particles in it is counted. The 
remainder of the suspension is diluted to half concentration by adding an 
equal volume of water, and after being thoroughly shaken another c.c. is 
removed from the suspension and a count of particles is made. This 
dilution process is repeated until four such counts in all have been made. 
The following counts were obtained by two different experimenters A, B, 
using similar material. 
Dilution 1 i i £ 
A 18 11 9 2 
B 49 9 5 2 
Deduce what you can about the adequacy of the experimental technique 
and the consistency of the experimenters. (Camb. N.S.) 
31. The following table gives the results of 7776 sequences of throws of 
an ordinary die. For s = 1, 2, 3, na is the number of sequences in which 
a 6 was first thrown at the sth throw, and «4 is the number of sequences in 
which a 6 was not thrown in any of the first 3 throws. Investigate if there 
is evidence of the die being biased. Suggest a simpler experimental  
procedure to test for bias. 
s 1 2 3 4 Total 
na 1216 1130 935 4495 7776 
(Camb. N.S.) 
32. Two fuses included in the same apparatus are assumed to have 
independent probabilities/? and q of being blown during each repetition of 
EXERCISES 
193 
a certain experiment. Show that the probabilities of neither, one only, or 
both being blown during any repetition are given respectively by 
1-p-q+pq, P + q~2pq, pq. 
In 154 repetitions, one only is blown 93 times, and both are blown 
36 times. Assuming that/? = 0*60, q = 0-50, test the assumption that the 
actions of the fuses are independent. What kind of departure from 
independence is in fact indicated? (Camb. N.S.) 
33. A sequence is available of n +1 geological strata of four different types 
in all, denoted by a, b, c and d. The null hypothesis assumes that the  
occurrence of the four types occur randomly (with possibly different probabilities 
Pa, Pb, Pc and pd), except that two consecutive strata of the same type 
cannot be separated and thus do not appear in the sequence. Show that 
the sequence is a Markov chain on the null hypothesis, and write down its 
transition probability matrix. What are the maximum likelihood estimates 
of pa, pb, pc and pan. It is suspected that the actual sequence may show 
some non-randomness, for example, some tendency to a repeated pattern 
of succession of states. How would you construct a x2 test to investigate 
this? Give some justification for your answer, indicating the formula you 
would use to calculate x2 and the number of degrees of freedom you would 
allocate to it. (Lond. Dip.) 
34. The table given below, containing part of data collected by Parkes 
from herd-book records of Duroc-Jersey Pigs, shows the distribution of 
sex in litters of 4, 5, 6 and 7 pigs. 
Examine whether these data are consistent with the hypothesis that the 
number of males within a litter of given size is a binomial variable, the sex 
ratio being independent of litter size. If you are not altogether satisfied 
with this hypothesis, in what direction or directions does it seem to fail ? 
No. of 
males 
in litter 
0 
1 
2 
3 
4 
5 
6 
7 
Totals 
4 
1 
14 
23 
14 
1 
— 
— 
— 
53 
Size of litter 
5 
22 
20 
41 
35 
14 
4 
— 
— 
116 
6 
3 
16 
53 
78 
53 
18 
— 
— 
221 
7 
— 
21 
63 
117 
104 
46 
21 
2 
374 
(Lond. B.Sc.) 
35. In connexion with a certain experiment an instrument was devised 
which, in each consecutive time period of ^th of a second, either gave an 
impulse or did not give an impulse. It was hoped that the probability, p, 
of the impulse occurring would be the same for every time period and 
independent of when the previous impulses occurred. 
13 
LSII 
194 
APPROXIMATE METHODS 
To test the instrument it was switched on and a note was made of the 
number of periods between the start of the test and the first impulse and 
thereafter of the number of periods between consecutive impulses. The 
test was continued until 208 impulses had been noted and the data were 
then grouped into the number of impulses which occurred in the first 
period after the previous one, the number in the 2nd period after the 
previous one (i.e. after one blank period) and so on. These grouped data 
are given in the table. 
Period after 
previous 
impulse 
(or start) 
1st 
2nd 
3rd 
4th 
5th 
6th 
No. of 
impulses 
81 
44 
24 
22 
13 
9 
Period after 
previous 
impulse 
(or start) 
7th 
8th 
9th 
10th 
Total 
No.c 
impul: 
4 
3 
5 
3 
208 
Fit the appropriate probability distribution, based on the above  
assumptions, to these data and test for goodness of fit. 
The makers of the instrument claim that the probability of an impulse 
occurring during any period is 0-35. Test whether the data are consistent 
with this claim. (Lond. B.Sc.) 
36. If trees are distributed at random in a wood the distance r from a tree 
to its nearest neighbour is such that the probability that it exceeds a is 
exp [ — 7raa/4/A2], where /* is the mean distance. 
The following data (from E. C. Pielou, /. Ecol. 48, 575, 1960) give the 
frequencies of the nearest neighbour distances for 158 specimens of Pinus 
ponderosa. Test whether the trees may be regarded as randomly distributed 
and comment on your results. 
Distance to 
nearest 
neighbour 
(in cm) 
0-50 
51-100 
101-150 
151-200 
201-250 
251-300 
Frequency 
13 
17 
34 
21 
9 
17 
Distance to 
nearest 
neighbour 
(in cm) 
301-350 
351-400 
401-450 
451-500 
501 — 
Total 
Frequency 
8 
5 
3 
7 
14 
158 
(Leic. Stat.) 
37. Antirrhinum flowers are either white, or various shades of red. Five 
seeds, taken from a seed-pod of one plant, were sown, and the number of 
seeds which produced plants with white flowers was noted. The process 
was repeated with five seeds from each of 100 seed-pods, and the following 
EXERCISES 
195 
table shows the number of seed-pods in which k seeds were produced with 
white flowers (k = 0, 1, ..., 5). 
No. of plants with 
white flowers 
No. of seed pods 
0 
20 
1 
44 
2 
25 
3 
7 
4 
3 
5 
1 
Total 
100 
Test the hypotheses (a) that the probability of obtaining a white flower 
is constant for all 100 seed-pods, and (b) that the probability of obtaining 
a white flower for a seed chosen at random is 1/4. (Lond. Psychol.) 
38. Birds of the same species, kept in separate cages, were observed at 
specified times and it was found that each bird had a probability of \ of 
being on its perch when observed. Nine of these birds were then put 
together into an aviary with a long perch. At each of the next 100 times 
of observation the number of birds on the perch was counted and the 
following results were found: 
No. of birds 12 3 4 5 
Frequency with which this 2 6 18 22 52 
number was observed 
Show that there is some evidence for a change in perching habits. 
(Camb. N.S.) 
39. Each engineering apprentice entering a certain large firm is given 
a written test on his basic technical knowledge, his work being graded 
A, B, C, or D (in descending order of merit). At the end of his third year 
with the firm his section head gives him a rating based on his current 
performance; the four possible classifications being 'excellent', 'very 
good', 'average', and 'needs to improve'. The results for 492 apprentices 
are given in the following two-way table. Analyse the data, and state 
clearly what conclusions you draw regarding the usefulness, or otherwise, 
of the written test as an indicator of an entrant's subsequent performance 
in the firm. 
Section head's 
assessment 
Excellent 
Very good 
Average 
Needs to improve 
Written test result 
A 
26 
33 
47 
7 
B 
29 
43 
71 
12 
C 
21 
35 
72 
11 
N 
D 
11 
20 
45 
9 
(Manch. Dip.) 
40. The following data (from H. E. Wheeler, Amen J. Bot. 46, 361, 
1959) give the numbers of fertile and infertile perithecia from cultures of 
Glomerella grown in different media: 
Medium 
Oatmeal 
Cornmeal 
Potato dextrose 
Synthetic 
Fertile 
179 
184 
161 
176 
Infertile 
56 
19 
39 
26 
Total 
235 
203 
200 
202 
13-2 
196 
APPROXIMATE METHODS 
Determine whether there is a significant difference between these media 
in respect to fertility of the perithecia. If there is a significant difference 
obtain an approximate 95 % confidence interval for the proportion of 
fertile perithecia given by the best medium. Otherwise obtain a similar 
interval for the pooled results. (Leic. Stat.) 
41. Each of n individuals is classified as A or not-^4 (A) and also as B or B. 
The probabilities are: 
p(AB) = 6ll9 p(AB) = 012, p(AB) = 621, p(AB) = 622, 
with 2 0^ = 1; and the individuals are independent. Show that the joint 
distribution of m, the number classified as A; r, the number classified as B; 
and a, the number classified as both A and B (that is, AB) is 
&¥)* 6^^m~r Mm, r, a\ 
where N(m, r, a) is some function of m, r and a, but not of the du. Hence 
show that the probability distribution of a, given r and m depends only on 
#11022/012 021. 
Suggest a method of testing that the classifications into A and B are 
independent: that isp(AB) = p(A)p(B). (Wales Maths.) 
42. Test whether there is any evidence for supposing an association 
between the dominance of hand and of eye in the following contingency 
table (i.e. of the 60 people, 14 were left-handed and left-eyed): 
Left-eyed Right-eyed Totals 
Left-handed 14 6 20 
Right-handed 19 21 40 
Totals 33 27 60 
How would you test if the association was positive (i.e. left-handedness 
went with left-eyedness) ? (Camb. N.S.) 
43. One hundred plants were classified with respect to the two contrasts 
large L versus small /, white W versus coloured h\ the numbers in the four 
resulting classes being as shown in the table: 
w 
w 
Totals 
L 
40 
20 
60 
/ 
15 
25 
40 
Totals 
55 
45 
100 
Investigate the following questions: 
(a) Is there reason to believe that the four classes are not being produced 
in equal numbers? 
(b) Is the apparent association between L and Wstatistically significant? 
(Camb. N.S.) 
EXERCISES 
197 
44. k samples Si (i = 1, 2, ..., k) of an insect population are taken at 
k different sites in a certain habitat. Si consists of nt insects of which at are 
black and &*(=«* — #*) are brown. When the samples are combined the 
total number of specimens is n of which a are black and b(=n — a) are 
brown. It is desired to test whether the observed proportions of the two 
kinds of insect differ significantly from site to site. Calculate a x2  
appropriate to test this hypothesis. State the number of degrees of freedom, and 
prove that the x2 can be put into any one of the following forms: 
2 _ |. (nai-nia)2 _ |, (baj-abj)2 
i=i abrii i=i abrii 
ab\i=irii n) 
= J*M| (ai-bd2 (a-b)2\ 
4ab\i=i rii n V 
(Camb. N.S.) 
45. A group of individuals is classified in two different ways as shown in 
the following table: 
Y 
Not-y 
X 
a 
c 
a+c 
Not-Z 
b 
d 
b+d 
a+b 
c+d 
n 
Establish the formula 
X2 = n(ad-bc)2/(a + c) (b + d) (a + b) (c + d) 
for testing the independence of the two classifications. 
An entomologist collects 1000 specimens of Insecta corixida, 500 from 
each of two lakes, and finds that 400 specimens from one lake and 375 
from the other have long wings. Is he justified in reporting a significant 
difference in the proportions of long-winged corixids in the two lakes ? 
(Camb. N.S.) 
46. A sample of 175 school-children was classified in two ways, the result 
being given in the table. P denotes that the child was particularly able and 
not-P that it was not so. Is the apparent association between ability and 
family prosperity large enough to be regarded as statistically significant? 
Very well clothed 
Well clothed 
Poorly clothed 
P 
25 
15 
5 
Not-P 
40 
60 
30 
(Camb. N.S.) 
47. In a routine eyesight examination of 8-year-old Glasgow  
schoolchildren in 1955 the children were divided into two categories, those who 
wore spectacles (A), and those who did not (B). As a result of the test, 
visual acuity was classed as good, fair or bad. The children wearing 
spectacles were tested with and without them. 
198 
APPROXIMATE METHODS 
The following results are given: 
Visual acuity of Glasgow school-children (1955) 
Category 
Good 
Fair 
Bad 
Total 
A9* 
vith 
spectacles 
Boys 
157 
322 
62 
541 
Girls 
175 
289 
50 
514 
A, without 
spectacles 
Boys 
90 
232 
219 
541 
Girls 
81 
222 
211 
514 
B 
T 
Boys 
5908 
1873 
576 
8357 
Girls 
5630 
2010 
612 
8252 
What conclusions can be drawn from these data, regarding (a) sex 
differences in eyesight, (b) the value of wearing spectacles ? 
The figures for 8-year-old boys and girls for the years 1953 and 1954 
were not kept separately for the sexes. They are shown as follows: 
Visual acuity of Glasgow school-children (1953, 1954) 
Category 
Good 
Fair 
Bad 
Total 
A, 
with 
spectacles 
1953 
282 
454 
84 
820 
1954 
328 
555 
78 
961 
A, without 
spectacles 
1953 
152 
378 
290 
820 
1954 
173 
443 
345 
961 
1953 
8,743 
3,212 
1,015 
12,970 
B 
1954 
10,511 
3,565 
1,141 
15,217 
Are there, in your opinion, any signs of changes in the 8-year-old 
population of visual acuity with time ? Do you think it possible that your 
conclusions might be vitiated by the pooling of the frequencies for both 
sexes in 1953 and 1954? (Lond. B.Sc.) 
48. A radioactive sample emits particles randomly at a rate which decays 
with time, the rate being Xe~Kt after time t. The first n particles emitted are 
observed at successive^ times tl912,..., tn. Set up equations for the maximum- 
likelihood estimates A and k. and show that k satisfies the equation 
ktn = i-ki 
?*'«-! 
where t = - 2 tt. 
n i 
Find a simple approximate expression for k when tjt is a little greater 
than 2. (Camb. Dip.) 
49. A man selected at random from a population has probabilities 
(1 — 0)2,20(1 — 6), 62 of belonging to the categories AA, AB, BB respectively. 
The laws of inheritance are such that the probabilities of the six possible 
pairs of brothers, instead of being the terms of a trinomial expansion, are: 
Brothers 
AA,AA 
AA,AB 
AA,BB 
AB}AB 
No. of cases 
ia_ dy (2-ey nx 
6(1 -Of (2-6) n2 
\6\\-6y n3 
6(1-6) (1 + 6-62) n* 
EXERCISES 
199 
Brothers 
AB,BB 
BB,BB 
Total 
(92(1 - (9)2 
i(92(l+<9)2 
1 
No. of cases 
N 
A random sample of TV pairs of brothers is collected and is found to 
consist of nl9 n2, ... n6 instances of the six types. Obtain an equation for 
the maximum likelihood estimate of the parameter 6. Show that the 
posterior distribution of 0 has approximate variance 
g(l+g)(2+g) 
N(6 + 5g+4g*Y 
where g = 6(1 — ff). 
An alternative and arithmetically simpler estimation procedure would 
be to score 0 for each AA individual, 1 for each AB, and 2 for each BB, and 
then to equate the total score of the whole sample to its expectation. Find 
the posterior variance of 0 using this method and compare with the 
maximum likelihood value. (Camb. Dip.) 
50. Bacteria in suspension form into clumps, the probability that a 
random clump will contain n bacteria is 0n_1(l — 0) (n = 1,2,...). When 
subject to harmful radiation there is a probability XSt that a bacteria will 
be killed in any interval of length 8t irrespective of the age of the bacteria 
in the same or different clumps. A clump is not killed until all the bacteria 
in it have been killed. Prove that the probability that a random clump 
will be alive after being exposed to radiation for a time t is 
e-xt/(l-d + 6e-xt). 
In order to estimate the strength, A, of the radiation a unit volume of the 
unradiated suspension is allowed to grow and the number nx of live clumps 
counted. The remainder of the suspension is irradiated for a time t and 
then a unit volume is allowed to grow free of radiation and the number rx 
of live clumps counted. It may be assumed that both before and after 
radiation the clumps are distributed randomly throughout the suspension. 
The experiment is repeated with new suspensions s times in all giving 
counts (nl9n2, ...,««) and (rl9 r2, ...,r8). Show that if 6 is known the 
maximum likelihood estimate of A is 
„ (N-6R\ 
s s 
where N = 2 n{ and R = 2^. (Camb. Dip.) 
51. There are two ways in which an item of equipment may fail. If 
failure has not occurred at time t, there is a chance A^f+o(St) of failure of 
type I and a chance X^dt+oidt) of failure of type II in (f, t+dt). A number 
n of items is placed on test and it is observed that rx of them fail from 
cause I at times t'l9 ..., t'ri9 that r2 of them fail from cause II at times 
fi, ..., t"2 and that when the test is stopped at time T the remaining 
200 
APPROXIMATE METHODS 
(w — rx — r2) items have not failed. (As soon as an item fails, it takes no 
further part in the test, and the separate items are independent.) Obtain 
maximum likelihood estimates of Ax and A2 and explain the intuitive 
justification of your formulae. Derive a test of the null hypothesis Ax = A2. 
(Camb. Dip.) 
52. In an experiment to measure the resistance of a crystal, independent 
pairs of observations (xi9 y{) (/= 1, 2, ...,«) of current x and voltage y 
are obtained. These are subject to errors (Xi9 Yi), so that 
Xi = hi + Xi> y% = Vi + ■*■if 
where (&, rj^ are the true values of current and voltage on the ith occasion, 
and rji = oc^i9 a being the resistance of the crystal. On the assumption that 
the errors are independently and normally distributed with zero means 
and variances &*(Xi) = o\, ®\ Y{) = o\ = Ao^, where A is known, show 
that a, the maximum likelihood estimate of a, is a solution of the 
& &xyi &>\A&xx byy) ^^xy = ^> 
where Sxy = Itx^i/ii, Sxx = 2#J//i, Syy = 2y?//i. 
Show also that if SgJ/« tends to a limit as n -± oo, then a converges in 
probability to a. 
Show that the method of maximum likelihood gives unsatisfactory 
results when A is not assumed known. (Camb. Dip.) 
53. Let Zl9 Z2, ..., Zm be independent observations which are identically 
distributed with distribution function 1 — exp (— x&), where fi is an unknown 
parameter. Obtain an estimate of fi useful as m -> oo and hence the 
approximate posterior distribution of /?. If the calculation of your  
estimator involves iteration, explain how a first approximation to this is to 
be found. How would you test the hypothesis j3 = 1 against the two-sided 
alternative fi 4= 1 ? (Camb. Dip.) 
54. In an investigation into the toxicity of a certain drug an experiment 
using k groups of animals is performed. Each of the rii animals in the ith 
group is given a dose xi9 and the number of resulting deaths r{ is recorded 
(i = 1, 2, ..., k). The probability of death for an animal in the ith group is 
assumed to be 
PA0*-, p) = i+e-iU+px49 
the probabilities for different animals being mutually independent. Show 
that the maximum likelihood estimates a, y? of a, j3 satisfy the equations 
2>z- = XriiPiid, ft), 
^r^i = XriiXiPiid, ft), 
and indicate how these estimates may be determined by successive 
approximation. 
EXERCISES 
201 
Show also that the asymptotic variances and covariances of the posterior 
distribution of a and fi are given by 
<*»(«) = (1/Emv) + (*/SJ, ®\P) = USXX, 
n*j)= -*/sxx, 
where wt = mPtf, h [1 -P<(&9 fi)]9 
X = TtWiXi/XWi, Sxx = ZWtiXi — x)2. 
Hence obtain a large sample test of the hypothesis that — a/fi, the mean of 
the tolerance distribution associated with P*(a, fi), has a specified value /*0, 
and derive a 95 % large sample confidence interval for — a/fi. 
(Camb. Dip.) 
55. Show that the probability of needing exactly k independent trials to 
obtain a given number n of successes, when each trial can result in success 
or failure and the chance of success is p for all trials, is 
where q = 1 —p. Writing k = n + s9 show that the mean and variance of s 
are nq/p and nq/p2. 
Two such sequences of trials were carried out in which the required 
numbers of successes were nx and n2, and the numbers of trials needed 
were nx+sx and n2 + s2. Two estimates were proposed for the mean number 
of failures per success; the first was the total number of failures divided by 
the total number of successes, and the second was the average of the 
numbers of failures per success in the two sequences. Which estimate 
would you prefer ? State your reasons carefully. (Camb. N.S.) 
56. A subject answers n multiple-choice questions, each having k possible 
answers, and obtains R correct answers. Suppose that the subject really 
knows the answers to v of the questions, where v is an unknown parameter, 
and that his answers to the remaining n — v questions are pure guesses, 
each having a probability 1 /k of being correct. Write down the probability 
distribution of R, indicating its relation to the binomial distribution. 
If 0 is the probability that he knows the correct answer and has prior 
distribution B0(a, b), and if the questions are answered independently 
discuss the posterior distribution of 0 given R. (Lond. B.Sc.) 
57. An estimate is required of the number of individuals in a large 
population of size N, who possess a certain attribute A. Two methods are 
proposed. Method I would select a random sample of size aTV and  
interview each member of the sample to determine whether he possesses A or 
not. Method II would send out the postal question 'Do you possess AT 
to all TV individuals and then, in order to avoid any possible bias due to 
202 
APPROXIMATE METHODS 
misunderstanding of the question, etc., would interview a random sample 
of size ocNy of those Ny individuals who replied 'Yes' and a random 
sample of size ocNn of those Nn individuals who replied 'No'. 
Assuming that, if method II were used, all TV individuals would reply to 
the question, suggest estimates for the two methods in terms of the results 
of the interviews. If py(=l— qy) is the proportion of the yes-replying 
individuals who actually have A, andpn(= 1 — qn) is the proportion of the 
no-replying individuals who actually have A, show that the variances of 
the estimates for methods I and II are 
(Nypy + Nnpn) (Nyqy + Nnqn) ^ NyPyqy + Nnpnqn 
Not a 9 
respectively. 
Show that method II has the smaller variance. Discuss the relevance of 
these sample variances to the posterior distribution of the number in the 
population possessing A. (Wales Dip.) 
203 
8 
LEAST SQUARES 
The method of least squares is a method of investigating the 
dependence of a random variable on other quantities. Closely 
associated with the method is the technique of analysis of  
variance. We begin with a special case and later turn to more 
general theory. The reader is advised to re-read the section on 
the multivariate normal distribution in §3.5 before reading §8.3. 
8.1. Linear homoscedastic normal regression 
We are interested in the dependence of one random variable y 
on another random variable x. We saw in §3.2 that this could 
most easily be expressed in terms of the distribution of x and the 
conditional distribution of y9 given x. These distributions will 
depend on parameters; suppose the dependence is such that 
p(x9y\d,<f>)=p(x\<P)p(y\x9d); (1) 
that is, the parameters in the two distributions are distinct. 
Then this is effectively equation 5.5.21 again with x = (x9 y) 
and t(x) = x. As explained in §5.5, and illustrated in proving 
theorem 7.6.2, if 6 and <j) have independent prior distributions, 
inferences about the conditional distribution may be made by 
supposing x to be fixed. We shall make these two assumptions 
((1) and the prior independence of 6 and fi) throughout this 
section, and use more general forms of them in later sections. 
Consequently all our results will be stated for fixed x. Provided 
the assumptions obtain, they are equally valid for random x. 
Linear homoscedastic normal regression has already been 
defined (equations 3.2.13 and 14). 
Theorem 1. Ifx = (xl9 x2, ..., xn) is a set of real numbers, and if 
for fixed x, the random variables yl9y29 ••">yn are independent 
normal random variables with 
S{yt\x) = <x+/3(Xi-x) (2) 
204 LEAST SQUARES [8.1 
and ®2(yi\x) = <f> (3) 
(i = 1, 2, ..., ri); then if the prior distributions of a, /? andln<f> are 
independent and uniform: 
(i) the posterior distribution of 
<Jl-b)l{S*ISxx(n-2)}i (4) 
is a t-distribution with (n — 2) degrees of freedom; 
(ii) the posterior distribution of 
(«-a)l{S*ln(n-2)}l (5) 
is a t-distribution with (n — 2) degrees of freedom; 
(iii) the posterior distribution of 
S2lf (6) 
is x2 with (n — 2) degrees of freedom. 
The notation used in (4), (5) and (6) is 
(7) 
Szx = Z(x{-x)2, Sxy = ^{xi-x)(yi-y\ 
Syv = ^(ji-yf, 
S — Syy — SXylOxx (O) 
and a = y, b = SxyISxx. (9) 
The likelihood of the y's, given the x's, is 
p(y | x, a, /?, 4>) oc 0-*»exp [ - £{* - a - /?(** - x»w] (10) 
and hence the joint posterior distribution of a, /? and <i> is 
*(«, A 01 x, y) oc 0-i<»+2>exp T - i{y, - a -/?(** - x)}2/2?T] . 
(11) 
The sum of squares in the exponential may be written as 
= Svv+n(y-*f+l3*Sxx-2pSxy 
~ Syy~SxylSxx+n(y~(X) + $xx(P ~~ SxyISxx) 
= S2+n(a-a)2 + Sxx(/?-6)2, (12) 
8.1] LINEAR REGRESSION 205 
in the notation of (8) and (9). Hence 
n(a9 /?, <f> | x, y) ex 0-^+2>exp [ - {S2 + n(oc - a)2 
+sxx(f}-b)2}m (i3) 
Integration with respect to a gives 
tt(/?, </>\x, y) ex 0-K»+»exp [~{S2 + Sxx(/1-b)2}l2<f>], (14) 
and then with respect to $ gives (using theorem 5.3.2) 
tt(/?|x, y) ex {S2 + Sxx(f}-b)2}-^-». (15) 
If f, defined by (4), replaces /?; then, the Jacobian being constant, 
tt(*|x, y) oc {1 +/2/(w-2)}-i^-1>. (16) 
A comparison with the density of the /-distribution, equation 
5.4.1, establishes (i). 
(ii) follows similarly, integrating first with respect to /? and 
then with respect to 0. 
To prove (iii) we integrate (14) with respect to /?, which gives 
7r(^|x,y)oc^-^e-i^, (17) 
and a comparison with equation 5.3.2 establishes the result. 
Discussion of the assumptions 
The basic assumptions, (1) and the prior independence of the 
separate parameters of the marginal and conditional  
distributions, are often satisfied in practice. In the language of §5.5 we 
may say that x is ancillary for 6 for fixed $. As in the examples 
of sample size in §5.5 and the margin of a contingency table in 
§7.6, it does not matter how the x values were obtained. 
One application of the results of the present section is to the 
situation where a random sample of n items is taken and two 
quantities, x and>>, are measured on each item: for example, the 
heights and weights of individuals. Another application is to the 
situation where experiments are carried out independently at 
several different values xl9 x2, ..., xn of one factor and  
measurements yl9 y2..., yn are made on another factor: for example, a 
scientist may control the temperature, xi9 of each experiment 
and measure the pressure, y{. The x value, usually called the 
206 LEAST SQUARES [8.1 
independent variable, is different in the two cases: in the former 
it is a random variable, in the latter it is completely under the 
control of the experimenter. Nevertheless, the same analysis of 
the dependence of y on x applies in both cases. The y variable is 
usually called the dependent variable. The reader should be 
careful not to confuse the use of the word ' independent' here 
with its use in the phrase ' one random variable is independent 
of another'. 
The form of the conditional distribution assumed in theorem 1 
is that discussed in §3.2. The distributions are normal and  
therefore can be completely described by their means and variances. 
The means are supposed linear in the independent variable and 
the variances are constant (homoscedastic). (Notice that we have 
written the regression in (2) in a form which is slightly different 
from equation 3.2.13, the reason for which will appear below: 
essentially, a of 3.2.13 has been rewritten a—fix.) It is important 
that the assumptions of linearity and homoscedasticity should 
be remembered when making any application of the theorem. 
Too often regression lines are fitted by the simple methods of 
this section without regard to this point. An example of  
alternative methods of investigating the dependence of y on x without 
invoking them will be discussed below. Particular attention 
attaches to the case /? = 0, when the two variables are  
independent and there is said to be no regression effect. A test of 
this null hypothesis will be developed below (table 8.1). 
The prior distributions of oc and <p are as usual: <p is a variance 
and a = n~l^LS{yi | x); that is, the expectation of y averaged over 
the values of x used in the experiment. We have chosen /?, which 
is the only parameter that expresses any dependence of y on x, to 
be uniformly distributed: this is to be interpreted in the sense of 
§5.2, merely meaning that the prior knowledge of/? is so diffuse 
that the prior density is sensibly constant over the effective range 
of the likelihood function. It has been supposed independent of 
a and $ because it will typically be reasonable to suppose that 
knowledge of the expectation and/or the variance of y (that is, 
of a and/or <p) will not alter one's knowledge of the dependence 
of y on x (that is, of /?). 
8.1] 
LINEAR REGRESSION 
207 
Known variance 
Under these assumptions the joint posterior distribution of 
a, J3 and c£ is given by (11), or more conveniently by (13). It is 
instructive to consider first the case where $ is known, equal 
to cr2, say. Then, from (13) and (17), 
"(*, P\ x, y, cr2) ex exp [- {n(<x-af + SXX(P-6)2}/2cr2]. (18) 
It is clear from (18) that a and /? are now independent and have 
posterior distributions as follows: 
(i) a is N(a, cr*ln\ (19) 
(ii) ft is N(b, cr*jSxx). (20) 
The posterior independence of a and /? is the reason for writing 
the regression in the form (2). (See also §8.6(c) below.) The 
posterior expectations of a and /?, a and b, are of importance. 
The former is simply y, agreeing with our remark that a is the 
expectation of y averaged over the values of x. a has the usual 
variance cr2jn\ though notice that cr2 here is ^2Q\-| x), not @>2(y^). 
The expectation of /? is b = SxyISxx and is usually called the 
sample regression coefficient; its variance is cr2ISxx. The  
variance increases with cr2 but decreases with Sxx. In other words, 
the greater the spread of values of the independent variable the 
greater is the precision in the determination of /?, which is in 
good agreement with intuitive ideas. 
Least squares 
The values a and b may be obtained in the following  
illuminating way. Suppose the pairs (xi9 y^) plotted as points on a 
diagram with x as horizontal and y as vertical axes. Then the 
problem of estimating a and /? can be thought of as finding a 
line in the diagram which passes as close as possible to all 
these points. Since it is the dependence of y on x expressed 
through the conditional variation of y for fixed x that interests 
us, it is natural to measure the closeness of fit of the line by the 
distances in a vertical direction of the points from the line. It is 
easier to work with the squares of the distances rather than their 
208 LEAST SQUARES [8.1 
absolute values, so a possible way of estimating a and JS is to 
find the values of them which minimize 
n 
S \yt-*-/Kxt-x)?. (21) 
i=l 
This is the principle of least squares: the principle that says that 
the best estimates are those which make least the sum of squares 
of differences between observed values, yi9 and their  
expectations, a+P(xt -x). Since (21) can be written in the form (12) it 
is obvious that a and b are the values which make (21) a  
minimum. They are called the least squares estimates of a and /?. The 
regression line y = a + b(x — x) is called the line of best fit. 
Notice, that from (10), a and b are also the maximum likelihood 
estimates of a and /?. We shall have more to say about the 
principle of least squares in §8.3. 
Unknown variance 
Now consider what happens when <t2 is unknown. In the case 
of p the JV(0, 1) variable, (P-b)l(cr2ISxxf9 used to make  
statements of posterior belief, such as confidence intervals or  
significance tests, is no longer available, and it is natural to expect, as 
in §5.4, that cr2 would be replaced by an estimate of variance and 
the normal distribution by the /-distribution. This is exactly 
what happens here, and it is easy to see why if one compares 
equation (14) above for the joint density of /? and <f>9 with 
equation 5.4.4, where the /-distribution was first derived. The 
form of the two equations is the same and the estimate of $ used 
here is S2j{n — 2). Consequently the quantity having a  
/-distribution is (P - b)l{S2jSxx(n - 2)}* agreeing with (4). Similar remarks 
apply to a. The form of S2 and the degrees of freedom, (n - 2), 
associated with it, fit in naturally with the principle of least 
squares. The minimum value of (21) is easily seen from (12) to 
be S2. Hence S2 is the sum of squares of deviations from the line 
of best fit. Furthermore, since a and P have been estimated, 
only (n — 2) of these deviations need be given when the remaining 
two can be found. Hence S2 has only (n — 2) degrees of  
freedom. (Notice that if n were equal to 2 the line would pass 
through all the points and S2 would be zero with zero degrees of 
8.1] LINEAR REGRESSION 209 
freedom.) According to the principle of least squares the  
estimate of variance is obtained by dividing the minimum of the 
sum of squares (called the residual sum of squares) by the number 
of observations less the number of parameters, apart from ^, 
estimated, here 2. Notice that the maximum likelihood estimate 
of (j) is S2ln. 
In view of the close connexion between equations (14) and 
5.4.4 it is clear that the posterior distribution of <p will be related 
to that of x2 in the usual way. This is expressed in part (iii) of 
theorem 1. As usual, the degrees of freedom will be the same as 
those of t, namely (n — 2). 
Significance tests 
Confidence intervals and significance tests for any of the  
parameters will follow in the same way as they did in chapter 5. For 
example, a 95 % confidence intervalf for /? is (cf. equation 5.4.9), 
with e = 0-025, 
b - te(n - 2) S/{Sxx(n - 2)}* < fi < b + te(n - 2) SI{Sxx(n - 2)}*. (22) 
The significance test for /? = 0; that is, for independence of x 
and y, can be put into an interesting form. The test criterion is 
(4), with /? = 0, and since the /-distribution is symmetrical we 
can square it and refer 
b SXylSa 
'xyi ^xx 
S*ISxx(n-2) [Syy-SlJS^K.n-2) 
(23) 
to values of t%n-2). (The alternative form follows from (8) 
and (9).) Now _ 
Syy — [^yy~~^xylSxx] + [SxyISxx]. (24) 
The left-hand side is the total sum of squares for the y9s. The 
first bracket on the right is the residual sum of squares and also, 
when divided by (n-2), the denominator of (23). The second 
bracket is the numerator of (23) and will be called the sum of 
squares due to regression. Furthermore, since (§6.2) t2 is equal 
to F with 1 and (n - 2) degrees of freedom, we can write the 
calculations in the form of an analysis of variance table (§6.5) 
t To avoid confusion between the average expectation of yt and the significance 
level, e, instead of a, has been used for the latter. 
14 
LSII 
210 LEAST SQUARES [8.1 
and use an F-test (table 8.1.1). The total variation in y has been 
split into two parts, one part of which, the residual, is unaffected 
by the regression. (24) should be compared with (3.2.23). These 
can be compared, by taking their ratio and using an F-test, to 
see whether the latter is large compared with the former. If it is, 
we can conclude that a significant regression effect exists. That 
is to say, we believe that y is influenced by x, or that JS #= 0. 
This is a test of the null hypothesis that the two variables are 
independent, granted the many assumptions of the theorem. 
Table 8.1.1 
Due to 
regression 
Residual 
Total 
Sum of squares 
Sxy/SvX ^ & Sxx 
&yy *"" &xy/&xx ^ m 
&yy 
Degrees 
of 
freedom 
1 
n-2 
n-1 
Mean square 
b Sxx 
SVOi-2) 
— 
F 
t?/{SVSxx(n-2)} 
— 
Sufficient statistics 
The sufficient statistics in this problem are four in number; 
namely, S2, a, b and Sxx. This is immediate from (10), (12) 
and the factorization theorem (theorem 5.5.2). Thus four 
statistics are needed to provide complete information about 
three parameters. The calculation of the sufficient statistics is 
most easily done by first finding the sums and sums of squares 
and products, Xxi9 Zy^; Zxf, Zx^., Zyf and then 
x, y, Sxx = Zxf-(Z^)2/>z, 
Svv similarly and ^ = ^.^(s^ 
From these 5a, a, b and Sxx are immediately obtained. The  
computation is easy using a desk machine but some people object to 
it and argue that as good a fit of the line can be obtained by eye 
using a transparent ruler. In skilled hands this is true, but what 
such people forget is that the method here described not only 
fits the line, that is, provides a and b9 but also provides an idea 
of how much in error the line can be; for example, by enabling 
confidence limits to be obtained. This cannot be done readily 
when fitting by eye. 
8.1] 
LINEAR REGRESSION 
211 
Posterior distribution of the conditional expectation 
It is sometimes necessary to test hypotheses or make  
confidence statements about quantities other than a, J3 or $ separately. 
For example, we may require to give limits for the expected value 
of y for a given value x0 of the independent variable, or to test 
the hypothesis that the regression line passes through a given 
point (xQ9 y0). These may easily be accomplished by using a 
device that was adopted in proving theorem 6.1.3: namely, to 
consider the required posterior distribution for fixed $ and then 
to average over the known posterior distribution of $  
(equation (6)). In the examples quoted we require the posterior  
distribution of ^(y\x0) = a+fi(x0-x). Now, given <p = cr2, we 
saw in equations (18), (19) and (20) that a and /? were  
independent normal variables with means and variances there specified. 
It follows (theorem 3.5.5) that ^(y\x0) is 
N[a + b(x0 - x), <r2{n-1 + (x0 - xYISxx}]. 
Hence, by exactly the same argument as was used in proving 
theorem 6.1.3, the posterior distribution of <e(y\x0) is such that 
£(y\xo)-a-b(x0-x) (2S) 
[S2{n-i + (x0-xflSxx}l(n-2)]i K } 
has a /-distribution with (n — 2) degrees of freedom. Confidence 
limits for ${y \ xQ) may be obtained in the usual way, and the null 
hypothesis that the line passes through (x09y0), that is, that 
&(y\xo) = Jo? maY be tested by putting &(y\ x0) = y0 in (25) and 
referring the resulting statistic to the /-distribution. Notice that 
the confidence limits derived from (25) will have width  
proportional to the denominator of (25) and will therefore be wider the 
more xQ deviates from x. It is more difficult to be precise about 
the line at points distant from x than it is near x; which is 
intuitively reasonable. 
Posterior distribution of the line 
It is possible to make confidence statements about the line 
and not merely its value at a fixed value, x0. To do this it is 
necessary to find the joint distribution of a and /?. This can be 
14-2 
212 LEAST SQUARES [8.1 
done by integrating (13) with respect to <f>. The result will clearly 
be a joint density which is constant for values of a and /? for 
which n(oc - a)2 + Sxx(fi - b)2 is constant. To find joint confidence 
sets for a and JS it is therefore only necessary to find the  
distribution of n(oc — a)2 + SXX(J3 — b)2. Now a comparison of equation (13) 
with equation 6.4.8, and the argument that led from the latter 
to the result concerning the F-distribution, will here show that 
SW^2) (26) 
has an F-distribution with 2 and (n-2) degrees of freedom. 
This is a special case of a general result to be established later 
(theorem 8.3.1). The hypothesis that the line has equation 
y = a0+/?0(x-x) may be tested by putting a = a0, /? = J30 in 
(26) and referring the statistic to the F-distribution. 
Prediction 
Another problem that often arises in regression theory is that 
of predicting the value of the dependent variable for a given 
value, x0, of the independent variable: a scientist who has  
experimented at values xl9 x29 ...9xn may wish to estimate what would 
happen if he were to experiment at xQ; or if the independent 
variable is time he may wish to predict what will happen at some 
point in the future. To answer this problem we need to have 
the density of y given a value x0, the data x, y, and the prior 
knowledge: that is n{y | x, y, x0)9 where reference to the prior 
knowledge H is, as usual, omitted. To find this, consider first 
7r(y\x9 y, x0, a,/?, <p). 
This is N(<x+fi(xQ - x), 4>). Also 
n(<x I x, y, x0, /?, ^) is N(a, <}>\n) 
(equation (19)). Hence, for fixed x, y, x0, /? and <fi; y and a 
have a joint normal density (§3.2, in particular equation 3.2.18) 
ir(y | x, y, x0, /?, 0) is N[a + /J(x0 -x),$ + <t>\n\. 
Applying the same argument again to the joint density of y and /? 
for fixed x, y, xQ and <j) we have, since 
n(fi\ x, y, x0, 0) is N(b, <f>ISxx) 
8.1] LINEAR REGRESSION 213 
(equation (20)) that 7r(y | x, y, x0, $) is 
N[a + b(x0 -x)9<f> + <f>/n + (x0 - x)2 <f>ISxx]. 
Hence 
= J 7r(y\x, y, x09 ^) tt(^| x, y,x0)# 
»f**"«p[-(,^';^S.^)/y]* 
from equation (17). The integration performed in the usual way 
(compare the passage from (14) to (15)) shows that 
y-a-b(x0-x) (2 . 
has a ^-distribution with (n — 2) degrees of freedom. Hence  
confidence limits f for y may be found in the usual way. A  
comparison of (25) with (27) shows that the limits for y in (27) are 
wider than those for ${y \ xQ) in (25) since there is an additional 
term, 1, in the multiplier of the estimate, S2l(n — 2)9 of residual 
variance in the denominator of (27). This is reasonable since y 
equals $(y \ xQ) plus an additional term which is iV(0, $) (cf. 
equation 3.2.22): indeed we could have derived (27) by using 
this remark for fixed ^, and then integrating as usual with 
respect to $. Again these limits increase in width as xQ departs 
from x. Remember, too, that these methods are based on the 
assumption of linear regression which may not hold into the 
future even when it holds over the original range of x-values. 
An alternative method 
We conclude this section by giving an example of how the 
dependence of one random variable on an independent variable 
(random or not) can be investigated without making the  
assumptions of linear homoscedastic regression. The method described 
is by no means the only possible one. Suppose, for convenience 
t Notice that y is not a parameter, so that the definition of a confidence 
interval given in §5.2 is being extended from the posterior distribution of a  
parameter to the posterior distribution of any random variable, here a future 
observation. 
214 LEAST SQUARES [8.1 
in explanation, that the x-values are chosen by the experimenter. 
Then the range of x-values may be divided into groups and the 
numbers in the groups, rim9 say (the reason for the notation will 
be apparent in a moment), are fixed numbers chosen by the 
experimenter. Suppose the range of y similarly divided into 
groups and let rtj be the numbers of observations in both the 
/th-group of x-values and the y'th-group of j-values. Then 
Sty = ty, and Sty = rmj 
is a random variable which is the number in the /th-group of 
y-values. Let ^>tj be the probabilty of an observation which 
belongs to the /th-group of x-values belonging to the /th-group 
of j-values. Then if y is independent of x9 (f>i:} does not depend on 
/ and the null hypothesis of independence may be tested by the 
methods of theorem 7.6.2. If x were a random variable theorem 
7.6.1 would be needed, but the resulting test will be the same. 
The group sizes will have to be chosen so that the expected 
numbers are sufficiently large for the limiting ^-distribution to 
be used as an approximation. In the case of the dependent  
variable this choice, which will have to be made after the results are 
available, will influence the randomness of the r j. The precise 
effect of this is not properly understood. Fisher and others 
claim that the results of §7.6 are still valid even if the rtj9 as well 
as the rit, are fixed, but it is not clear from Bayesian arguments 
that this is so. Nevertheless, the effect of fixing the rmj is likely 
to be small and the test can probably be safely applied. Notice 
that this test makes no assumptions about linear homoscedastic 
regression of y on x. 
8.2. Correlation coefficient 
The topic of this section has nothing to do with least squares 
but is included here because of its close relationship to that of the 
previous section. The results are not used elsewhere in the book. 
Theorem L If (x, y) = (xl9 y±; x29 y2; ~>'9Xn9yn) is a random 
sample from a bivariate normal density with ${x?) *= 6l9 
S(y^) = 629 ^(Xi) = (j)l9 @2(yi) = ^2 and correlation coefficient 
p; and if the prior distributions ofdl9 d29 ln^ and ln^2 are uniform 
8.2] CORRELATION COEFFICIENT 215 
and independent, and independent of the prior distribution of p 
which nowhere vanishes; then the posterior distribution of p is 
such that Cj = tanh~V> is approximately (n -> oo) normal with 
mean: tanh-1r, variance: n"1, (1) 
where r = Sxyl^(SxxSyy). 
If 7T(p) is the prior distribution of p then, in the usual way, the 
joint posterior density of 0l9 6Z, <f>l9 <f>2 and p is proportional to 
(cf. 3.2.17) 
(^i^2)iw+1(l-P2)iw L 2(1 -p2) A { <j>x 
)] • (2) 
2p(xi-d1)(yi-62) + (yi-d2r 
44 $ 
In the usual way we may write 
SOi-fli)2 = Z(Xi-xf + n{x-dtf = S„ + n(x-61) 
in the notation of equation 8.1.7, etc., and (2) becomes 
"(ft) cxp f * iSxx 2f>Sxv I Svv^ 
(^^*»+1(l-p")1* L 2(1-p2) I & 44 & 
n \{x-6tf 2p(x - dx) (y - d2)+ (y - 6^ 
2(1-p2) I fa 44 ' & JJ' (3) 
The integration with respect to 0X and 02 may now be carried out 
using the fact that the normal bivariate density necessarily has 
integral one. Consequently 
If we substitute for <p2 a multiple of 0X, the integration with 
respect to (j)x can be easily carried out. It is convenient, because 
of the form of the term in braces, to put ^2 — ^ViC^/S^), 
when, remembering the Jacobian, we have 
**» *>p] x'y) K tiro^w*-*0* \-~W^F1 
s°ra f XV>J Svv f2 
216 LEAST SQUARES [8.2 
and the integration with respect to (j)x gives (theorem 5.3.2) 
n(p) (i-/>2r-i 
nty, PI x, y) oc 
^n(l -^fcn-l) (1 - 2/>r/^ + ^"2) 
-2\n-l 
i/r(i/r — 2pr + i/r-1) 
-l\n-l 9 
(5) 
where r = Sxyl*J(SxxSyy). 
The integral with respect to i/r is not expressible in elementary 
functions so we attempt to find an approximation to it. The 
natural thing to do is to attempt a substitution for i/r - 2pr + ^_1 
and a convenient choice is to change from i/r to £ defined by 
i/r-lpr + i/r-1 = 2 j=^. (6) 
The factor 2(1 — pr) is convenient because i/r — lpr + i/r-1 ranges 
between 2(1-pr), when i/r = 1, and infinity: (l-£)-1 leads to 
the power series expansion below. It is tedious but  
straightforward to verify that 
1 di/r ["/ 1— pr\2 .~]-%(l—pr) 
[hm-'i 
fdg L\ 1-iJ J (l-£)2' 
and we finally obtain 
(1 -pr)*-* 
o 
(7) 
It is easy to show (see below) that \pr\ < 1 so that the expression 
in square brackets may be expanded in a power series in £, 
when each term in the integrand may be integrated using the 
beta-integral (equation 5.4.7). The sth term in the integrated 
series is equal to the first term times a quantity of order /Hs-1), 
so that an approximation for large n may be obtained by 
retaining only the first term. Therefore, approximately, 
</>lx,y)oc ^_^y_s . (8) 
Now make the substitutions 
p = tanhcD, r = tanhz, (9) 
8.2] CORRELATION COEFFICIENT 217 
so thatt tt(& I x, y) oc —-r V *, »> (10) 
1 v ' J; cosh^cosh^-^-z)' v 
where 7r(cD) is now the prior density of fi. If n is large a further 
approximation may be made and the term 
7r(cD) cosh^ (cD - z)/cosh^ ft 
replaced by a constant, since it does not change with n. Then 
n((o\ x, y) oc cosh~n(cD —z). 
Finally, put &' = (& — z) rfi and expand the hyperbolic cosine, 
giving approximately 
n(<o'\x,y)<x[l+-2— J =e-&* (11) 
whence the result follows. 
Improvements to the approximation which involve choice of 
the form of n(p) will be given below. 
Use of correlation methods 
The regression methods discussed in the last section for 
investigating the association between two random variables are 
more useful than the results of this section, but the fact that they 
are not symmetrical in the two variables is sometimes an  
inconvenience. For example, in anthropometry, if two measurements 
such as nose length and arm length are being considered, there 
is no more reason to consider one regression than the other. 
Similarly, in education there is no reason to consider the  
influence of English marks on arithmetic marks rather than the 
other way around. The correlation coefficient (equation 3.1.9) is 
a measure of association which treats the two random variables 
symmetrically and is often used in these, and similar, situations. 
Nevertheless, it is a coefficient that requires care in its use. For 
the bivariate normal distribution zero correlation means  
independence of the two variables (§3.2) and the dependence 
increases with the modulus of p, so that there p is satisfactory 
as a measure of dependence; but in other cases this may be far 
from the case. An example is given in §3.1. Also p is a much 
t Readers may like to be reminded of the elementary result 
1 —tanh;ttanh>> = cosh(*—>>)/cosh*cosh>>. 
218 LEAST SQUARES [8.2 
more difficult quantity to handle than the regression coefficients. 
Finally, notice that with the bivariate normal distribution the 
variance (equation 3.2.23) of one variable, say y, can be written 
as the sum of two parts, that due to x and the residual variance. 
The former is only a proportion p2 of the total and, due to the 
square occurring here, p has to be very near one for this to 
amount to an appreciable proportion of the total variation. 
Consequently p tends to over-emphasize the association: with 
p = 0-70, p2 = 0-49 and only about half the variation in one 
variable can be ascribed to the other. Unlike regression methods, 
correlation techniques only apply when both variables are 
random. 
Derivation of the posterior distribution 
The prior distributions of the means and variances in theorem 1 
are as usual. The prior distribution of p has been chosen to be 
independent of these since it seems typically unlikely that, as it is 
dimensionless, knowledge of it would be influenced by  
knowledge of the separate variations of x and y. The first stage of the 
proof, leading to equation (4), serves to eliminate the means: 
a comparison of (4) with (2) shows that, in the usual way, lack 
of knowledge of the means effectively reduces the sample size by 
one. The quantity r appearing in (5) is called the sample  
correlation coefficient and its definition in terms of the sample is 
exactly the same as that of p in terms of the density (equation 
3.1.9). It follows therefore since \p\ ^ 1 that also |r| ^ 1. 
Equation (7) shows that the posterior distribution of p depends 
on the sample only through r. It is sometimes said that r is 
sufficient for p but this does not agree with the definition of 
sufficiency in §5.5. It is only for certain forms of prior  
distributions of the other parameters that the posterior distribution of p 
involves only r, and sufficiency is a concept which does not 
involve the prior distribution. 
Approximations and prior distributions 
In order to proceed beyond (7) it is simplest to introduce 
approximations. An alternative way of looking at the  
approximation given in the proof is to take logarithms of the posterior 
8.2] CORRELATION COEFFICIENT 219 
density, as we did in §7.1. It is then easy to see that the terms 
(apart from tt(p)) outside the integral provide the dominant 
quantities, so that we obtain (8). From the logarithm of (8) we 
can evaluate its first and second derivatives and so obtain the 
normal approximation, again as in §7.1. 
The form of (8) suggests prior densities of the former (1 -~/>2)c, 
for some c, as being convenient, and the following considerations 
suggest the value of c. If the knowledge of p is very vague a 
sample of size 1 will not provide any information, indeed r is 
then undefined. A sample of size 2 has necessarily r = ± 1, so 
only provides information about the sign of p. Consequently, if 
our prior knowledge of p is slight, we should not expect (8) to 
converge until n - 3. For this to happen calculation shows that 
we must have -2 < c < -1. If we confine ourselves to integers 
we must have c — — 1 and 
7T(p) OC (1 -p*)~\ (12) 
This argument will be supported by a further consideration 
below. 
It is possible to show that the posterior distribution in the 
form (8) tends to normality as n ~> oo but the limit is approached 
very slowly owing to the extreme skewness of (8). Fisher  
suggested the transformation (9) to avoid this. The transformation 
has some interesting properties. First, the large-sample form 
of (8) is, with 7r(p) given by (12), say, 
ln7r(p\x9y) = C+n[iln(l-p*)-]n(l-pr)]9 
where C is a constant. This clearly has a maximum value at 
p = r and the second derivative at the maximum is 
{$ln^lx'yl=r=-(T^- W 
In the sense of maximum likelihood (13) is the negative of the 
information about p (equation 7.1.6) and depends heavily on r. 
Let us therefore find a transformation of p that has constant 
(that is, independent of r) information (compare the  
transformation for the binomial and Poisson distributions in §§7.2, 
220 LEAST SQUARES [8.2 
7.3). If &(p) is such a function equation 7.1.24 says that it must 
satisfy, since the information is the inverse of the variance, 
(I)2 = * ->">-*• (14> 
where a is a constant. A function satisfying this is 
&(p) = iln j^ = tanh-V, (15) 
the constant information being n. It is convenient to make a 
similar transformation of r, when the result (10) follows. A 
second property of the transformation is that if the prior  
distribution of p is (12) then fi is uniformly distributed over the whole 
real line. If inferences are to be expressed in terms of & it is not 
unreasonable to assume cD so distributed before the results are 
available (compare the discussion in connexion with the variance 
in §5.3). 
Further approximations 
If one works in terms of ti with a uniform prior distribution it 
is often worthwhile to consider more accurate approximations 
to (10) than is provided by (11). We have 
ln7r(cD|x, y) = C - ^ln cosh & — (w —§) In cosh (<3 —z), 
where C is a constant. The maximum of this density occurs 
where 
p. 
zln7r(&\x, y) = -JtanhcD-(«-|)tanh(cD-z) = 0. 
dco 
The root of this is approximately cD = z (from (11)) so write 
a) = z + e and retain only the terms of order e: we have 
— J{tanhz + esech2z} -(«-§) e = 0, 
so that e = — tanhz/{(2« - 3) + sech2z} 
or, to order rr\ e = —r\2n. (16) 
The second derivative is 
— \ sech2 ti — (n — §) sech2 (cD — z) 
8.2] CORRELATION COEFFICIENT 221 
which, at the maximum ti = z — r/2n, is approximately 
-0*-§)-isech2z. (17) 
Hence a slight improvement on the result of the theorem is the 
Corollary. The posterior distribution of & = tanh~V is 
approximately normal with 
mean: tanh_1r —r/2«, variance: [« —§ + ^(1 — r2)]-1. (18) 
The change in the variance between (1) and (18) is not  
appreciable but the correction to the mean of — rjln can be of  
importance if it is desired to combine estimates of p from different 
sources by means of theorem 6.6.1. 
Several samples 
The exact posterior distribution of p with 7r(p) = \  
(equation (7)) has been tabulated (David, 1954), but the  
approximations are often more useful. The main merit of the approximate 
results is that they enable the usual normal methods developed 
in chapters 5 and 6 to be used when giving confidence limits or 
comparing several correlation coefficients. For example: if rx 
and r2 are the sample correlation coefficients obtained from 
independent samples of sizes nx and n2 respectively, the  
hypothesis that p1 = p2 (in an obvious notation) may be investigated 
by comparing the difference {z1 — r1l2n-^) — {z2 — r2l2n2) with its 
standard deviation [n^ + ni1]* in the usual way. 
The approximation (18) differs a little from that usually given, 
namely tanh_1r — rj2{n — 1) for the mean and (n — 3)_1 for the 
variance, but the differences are trivial, or, more precisely, of 
smaller order than the terms retained. 
8.3. Linear hypothesis 
In this and the following sections of the present chapter we 
consider independent (except for § 8.6(d)) normal random  
variables (xl9 x2, ..., xn) with a common unknown variance $. A 
statement that the expectations of these random variables are 
known linear functions of unknown parameters is called a linear 
222 LEAST SQUARES [8.3 
hypothesis. The column vector of the x{ will be denoted by x and 
the linear hypothesis will be written 
'(x) = A6, (1) 
where 8 is a column vector of s unknown parameters (0l9 d29...9 6S) 
and A is a known matrix of elements aiS (i = 1,2, ..., n; 
j = 1, 2, ..., s). A is often called the design matrix. We suppose 
s < n. For reasons which will appear below we consider the 
sum of squares of the differences between the random variables 
and their expectations (cf. 8.1.21); that is, 
n / s \2 
(x-A6)' (x-A6) = £ U- S aM . (2) 
The least value of (2) for all real 8 but fixed x and A is called the 
residual sum of squares and will be denoted by S2. We are 
interested in considering whether 
dr+1 = dr+2 = ... =d8 = 0 (0 < r < s), 
and the least value of (2) when these #'s are put equal to zero 
will not be less than S2. The difference between this least value 
and S2 will therefore be non-negative and is called the reduction 
in sum of squares due to 6r+l9 0r+29 ...,ds (allowing for dl9 d29...9 dr) 
and will be denoted by 52. The words in brackets are often 
omitted when it is clear which parameters have been included iix 
the restricted minimization. Throughout this chapter the square 
(s x s) matrix A'A will be assumed to be non-singular, t 
Theorem 1. If the random variables x satisfy the linear  
hypothesis (1), with a non-singular matrix A'A, and if the parameters 
6l9 d29 ,.., 6S and \&<j> have independent prior distributions which 
are uniform over the real line; then a significance test at level ct of 
the hypothesis that 
6r+1 = 6r+2 = ... = 6S = 0 (0 < r < s) (3) 
is obtained by declaring the data significant if 
[S?l(s-r)]l[S*l(n-s)] (4) 
exceeds Fa(vl9 v2) {equation 6.2.6); with v1 = s — r9v2 = n — s. 
t The results can be extended to singular A'A, but the extensions will not be 
required in this book. The only situation with a singular matrix that will be 
studied is that of §8.5 with K = 1, which will be dealt with by a special argument. 
8.3] LINEAR HYPOTHESIS 223 
The joint posterior distribution of 8 and <j> is clearly 
*(e, 0|x) ex 0-*-i«p [-^{(x-A8)' (x-A6)}] . (5) 
As on other occasions (for example, in the proof of theorem 
6.6.2) the expression in braces can be rearranged by collecting 
together the linear and quadratic terms in 8 and completing the 
square. In this way the expression can be rewritten 
8'A'A8--2e'A'x + x'x = (8~-6)'A'A(e-6) + x'x-6'A'A6, (6) 
where 8'A'A6 = 8'A'x: that is, since A'A is non-singular, 
6 = (A'A^A'x, (7) 
a function of the data not involving the parameters.  
Furthermore, it is clear, by writing A(8 - 6) = z, that the first term in (6) 
is the sum of squares of the z's and is therefore non-negative. 
Since it is zero when 8 = 6 the remaining terms in (6), which do 
not involve 0, must be the smallest value that (6) can take and 
are therefore equal to the residual sum of squares, 52. 
Consequently 
tt(8, </> | x) oc $H"-*exp [ --L {(8 - 8)' A'A(8 - 6) + S2}]. (8) 
A comparison of (8) with equation 3.5.17 shows that, for 
fixed ^, the #'s have a multivariate normal density. Now this 
density may alternatively be written in terms of the regressions, 
equation 3.5.10. Consider first the distribution of 6a9 then the 
distribution of 6s_l9 conditional on 0S9 and so on down to the 
distribution of 61 conditional on 629 #3, ..., 9S- (Notice we are 
dealing with the #'s in the order which is the reverse of that of 
the jc's in equation 3.5.10: that is, d1 is equivalent to xni etc.) 
In this way we may write 
tt(8, <f> | x) cc 0-i"-1 exp I" - -L {c^ - aj* + c2(d2 - a2)2 +... 
+ ^s-as)2 + S2}], (9) 
where the c's are constants and a^ is a linear function of 
Oi+i, 0i+2, • • •, 6S (in particular, a8 is a constant, the means in 
+ ...+C 
224 LEAST SQUARES [8.3 
equation 3.5.10 being zero). Furthermore, cl1^ is the variance 
of 6i for fixed 0i+l9 di+29 ...96s and hence q > 0. Since 61 occurs 
only in the first term in braces, we may integrate with respect to 
it, and then with respect to 029 and so on up to 6r finally obtaining 
^r+i, #r+2,.-., ^ ^|x)cc ^-i^-^exp ^^{cr+1(^r+1-ar+1)2 
,(0.-«J»+ $■}]. (10) 
A further integration with respect to <p9 using theorem 5.3.2, 
gives 
^(rr+l* &r+29 •*•> ^slX) °C \cr+l\yr+l ~~ ar+l) + ••• 
+c8(d8-asy+sTi(n-r >• (11) 
The result now follows in the same way that theorem 6.4.1 
followed from equation 6.4.9. Denote the expression in braces 
in (11) by 5^(8)+ 52. The posterior density, (11), is constant 
where S2(8) is constant. In the (s-r)-dimensional space of 
6r+1, 6r+2, ..., 0S the surfaces S2(8) = c are ellipsoids, since the 
Ci are positive, and the density decreases as c increases; that is, 
as the distance from the common centre of the ellipsoids 
increases. Hence a confidence set is an ellipsoid and the same 
argument as used in proving theorem 6.4.1 shows that the 
posterior distribution of 
O = [S?(B)l(s-r)]l[S2l(n-s)] 
is F(vl9 v2). The degrees of freedom, vl9 is the dimension of the 
#'s, namely (s — r), and a simple calculation following the lines 
of the derivation of equation 6.4.11 shows that v2 = n ~ s- 
The confidence set leads to a result which is significant if the 
null value 6r+1 = 6r+2 = ... = 6S = 0 does not belong to the 
confidence set. The confidence set is O < Fa(vl9 v2) so that the 
result is significant if 
Smi(s-r) 
S2l(n-s) 
> F«(vi, "a)- 
To complete the proof of the theorem it remains only to prove 
that 5^(0) is the reduction in sum of squares due to 6r+l9 6r+29 ...96s. 
8.3] LINEAR HYPOTHESIS 225 
This is easily done by returning to the sum of squares, (2), in 
the form of the expression in braces in (9). When 
this is c^dx - <x[y +... + cr(6r - <)2 + S2(0) + 52, 
where a^ is the value of oct when 6r+1 = dr+2 = ... = ds — 0 
(i < r). Since the c's are positive this has a minimum, when 
0. = aJ (i < r), of Sr2(0) + 52. The equations di = c^ (/ < r) are 
linear equations in 0l9 d2, ...96r and will always have a solution. 
Hence S2(0) is the reduction, as required, and the theorem is 
proved. 
Corollary 1. The posterior distribution of S2l<f> is x2 with 
(n — s) degrees of freedom. 
This follows from (9) on integrating with respect to all the #'s. 
We obtain ^ | x) ^ ^-£(n-*)-iexp [_ 52/2^]. 
A comparison with equation 5.3.2 establishes the result. 
Corollary 2. The posterior distribution of a linear function, 
s 
2 gify, of the parameters is such that 
g'(e-6)/[S2g'Cg/(H-*)]* (12) 
has a ^distribution with v = n — s degrees of freedom; where g' 
is the row vector (gl9 g29 ..., gs) and C is the matrix (A'A)-1. 
For fixed ^, the #'s are multivariate normal, equation (8), 
with means 8 and dispersion matrix (A'A)-1^, so that 
i=l 
is normal (a generalization to s variables of theorem 3.5.5) with 
mean g'6 and variance ^g'Cg (theorem 3.3.2). Hence, by 
corollary 1, 
oc sH^-^exp [- S2/20]$Hexp [-i{g'(6 - 6)}2/^g'Cg]. 
The usual integration with respect to <p (cf. equation 5.4.5) 
establishes the result. 
The quantities 6i (equation (7)) are called the least squares 
estimates of the 0i% They are the means of the posterior distribu- 
15 LSII 
226 LEAST SQUARES [8.3 
tion of the #'s. Similarly, g'6 is the least squares estimate of g'8. 
The equations (A'A)S = A'x, whose solution is (7), are called 
the least squares equations. It is often useful to find the variance 
of linear functions, g'6, of the 0t. 
^2(g'6|6, <f>) = ^2(g'CA'x|8, </>) 
= g'CA'ACg^, 
since the x's are independent with common variance (corollary 
to theorem 3.3.2). Finally, since C = (A'A)-1, 
SP(g'% 16, cj>) = g'Cg^. (13) 
This provides a means of calculating the denominator in (12). 
Corollary 3. Under the same conditions as in theorem 1 a 
significance test at level a of the hypothesis that 
S6«fy = 0 (i = r + l,r + 2, ...,s), (14) 
i=i 
where B, a (s — r) x s matrix with elements bij9 has rank (s — r), is 
obtained by declaring the data significant if 
[T2rl(s-r)]l[S*l(n-s)] (15) 
exceeds Fa(yl9 v2) with v1 = s — r9 v2 = n — s. Here T? is the 
reduction in sum of squares due to 
S6..0. (/ = r + l,r + 2, ...,*); 
*=i 
that is, T? + S2 is the minimum of (2) when (14) obtains. 
Since B has full rank we can find a non-singular sxs matrix 
B0 whose last (s — r) rows agree with the rows of B. Denote the 
elements of B0 by bij9 i now running from 1 to s. Now change 
the parameters to ty = B06. Then «?(x) = A8 = AB^"1^, which 
is a linear hypothesis in the ^'s, and the hypothesis to be tested, 
(14), is frr+1 = ^r+2 = ... = irs = 0. The result follows from 
the theorem. 
Linear hypothesis 
The theorem is one of the most important single results in 
statistics. We have met several special cases before and had we 
8.3] LINEAR HYPOTHESIS 227 
stated it earlier could have deduced them from it. However, we 
preferred to treat simple situations before passing to the general 
case. The linear hypothesis means that the random variables 
being observed are, apart from independent random variation 
of variance §S, known linear functions of unknown quantities. 
Since so many situations have a linear structure, or  
approximately so, we should expect to find the result widely applicable. 
In its general form, (14), the hypothesis to be tested is also 
linear. The test is carried out by minimizing the sum of squares, 
first allowing the parameters to range freely, obtaining the  
minimum 52, and then restricting them according to the hypothesis, 
obtaining the minimum S2 + S2 (or S2 + T2 in the general case in 
corollary 3). It is important to observe, a point we will enlarge 
on below, that it is not necessary to consider the structure of A 
or C = (A'A)-1: only the minima are required. Let us first see 
how cases already considered fit into the linear hypothesis 
concept. 
Examples: 
(i) Normal means. Take s = 2 and write n = n1 + n2. Let 
A be a matrix whose first column is n± l's followed by n2 O's 
and whose second column is nx O's followed by n2 l's. That is, 
g(x%) = d1 (1 < i < /ij), £(Xi) = 0a (% < i < ri), (16) 
and we have two independent samples of sizes nx and n2 from 
N(dl9 $) and N(629 <p) respectively. Consider a test of the  
hypothesis that 61 = 62, that the means are equal (theorem 6.1.3). 
The sum of squares, (2), is 
i=l i=n1+l 
with unrestricted minimum, equal to the residual, of 
S = 2j \Xi — ^i) + 2j \Xi ~~ X2) , 
where xl9 x2 are the means of the two samples. If 61 = d2 we 
have to minimize with respect to the common value 0 
n 
2 (Xi - 8)\ 
15-2 
228 LEAST SQUARES [8.3 
n 
with the result 2 (X- — *)2> 
i=i 
where x is the mean of the combined sample of n observations. 
It is easy to verify that the difference between these two minima is 
(x1-x2y(n^1 + n^1)-\ (17) 
Hence (with s = 2, r = 1), the F-statistic, (15), is 
(x.-x,)2 (n^ + n^rmS2l(n-2)]9 (18) 
with 1 and (n — 2) degrees of freedom. This is the same as the 
significance test derived from equation 6.1.6: the relationship 
between the two equations is easily established by putting 5 = 0, 
s2 = S2l(n-2) in equation 6.1.6 and remembering that F = t2 
when the value of vx = 1 (§6.2). Furthermore, the present 
corollary 1 is equivalent in this special case to theorem 6.1.2 and 
corollary 2, with g'8 = d1-d29 to theorem 6.1.3. We leave it to 
the reader to verify that theorem 6.4.1, for testing the difference 
between several normal means, is also a special case of the 
present theorem 1: indeed, we have used the method of proof of 
the earlier theorem to prove the new one. 
(ii) Weighing. Another special case is the weighing example 
of §3.3. In §6.6, equations (12) and (13), the situation was 
expressed as a linear hypothesis with design matrix given in 
the latter equation. However, there $ was supposed known, 
equal to a2. If <p is unknown, since n = s = 4, there are no 
degrees of freedom for error (n — s = 0) and no inferences are 
possible. But if the whole experiment were repeated, so that 
n = 8, still with s = 4, then the results of this section could be 
applied. Corollary 1 gives the posterior distribution of the 
precision (§ 5.1) and corollary 2 gives the posterior distribution of 
any weight or of any difference of weights. Corollary 3 could be 
used, for example, to test the hypothesis that all the weights were 
equal (61 = 62 = 63 = #4). 
(iii) Linear regression. As a final special case consider linear 
homoscedastic normal regression and the results obtained in 
§8.1. Replace the x's of the present section by y's and (dl9d2) 
by (a, /?). Then if the /th row of A is 
(1, **-*), (19) 
8.3] LINEAR HYPOTHESIS 229 
the linear hypothesis is that given by equation 8.1.2. The matrix 
A'Ais --<m; **%■)-(; £)• <»> 
The least squares estimates are 6 = CA'x, which here give 
&\ (n-i OW Z* \ = ( ? \ = (a 
fi) \0 SitJKZyfa-x)) \SJSJ \b 
agreeing with the least squares estimates of §8.1. Alternatively, 
we can consider the sums of squares. Either by minimization, 
or by inserting the least squares estimates, the residual is 
Sfo - a - b(xt - x))2 = Syy - S2JSXX9 as before. If /? = 0, the 
restricted minimum is Syy and the difference between these two 
minima is S2JSXX = b2Sxx. The F-test of our present theorem is 
therefore the same as that of table 1 in §8.1. The present 
corollaries similarly provide results equivalent to those of 
theorem 8.1.1. 
Prior distribution 
New applications of the theorem are postponed until later 
sections: here we make some comments on the assumptions, the 
proof and the conclusions. The form of the prior distribution is 
important. It is equivalent to saying that our knowledge of each 
parameter is vague in comparison with that to be obtained from 
the observations, which may often be reasonable, but also that 
these parameters are independent; an assumption which is often 
unreasonable. We may know, for example, that all the #'s have 
about the same value, a case that was discussed in §6.6 in  
connexion with between and within analyses. (It will be shown 
below that this is a special case of the results of this section.) 
Alternatively, we may know that a few (most likely one or two) 
of the #'s may differ from the rest, which are approximately 
equal. No simple methods are known which are available for 
treating this situation when $ is unknown. The methods of 
theorem 6.6.3, based on the multivariate normal distribution, 
are available if <j) is known to be equal to <r2. The form which 
has here been assumed for the prior distribution should be 
remembered in all applications. With this form of prior  
distribution the results obtained in this book agree with standard 
(21) 
230 LEAST SQUARES [8.3 
methods based on the considerations of the sample space only 
(§5.6) when the #'s are assumed constants—the so-called 'fixed 
effects' model (§8.5). 
Design matrix 
The design matrix is so-called because it describes the  
experimental method without reference either to the unknown values 
(the c9's) that the experiment is designed to investigate, or to the 
observations (the x's) that will result from any performance of it. 
For example, we saw in §6.6 that the design matrix for the 
weighing example was given by equation (13) there: had each 
object been weighed separately the design matrix would have 
been the unit matrix, I, a diagonal matrix with l's in the diagonal. 
Hence these two matrices distinguish the two possible ways of 
carrying out the experiment. A is typically under our control 
and there is a large literature on the design of experiments which, 
mathematically, reduces to the choice of A. 
Discussion of the proof 
The first stage of the proof consists in rewriting the posterior 
density in the form of equation (8). Notice that to do this it is 
necessary that A'A be non-singular: otherwise 6 is undefined. 
When this is so the distribution of the c9's, for fixed c£, equal to 
cr2, say, is multivariate normal with means equal to the least 
squares estimates, 6, and non-singular dispersion matrix 
(A'A)_1cr2. This is a special case of theorem 6.6.3. That theorem 
was concerned with a linear hypothesis and it is only necessary 
to take the matrix, there denoted by C, to be Icr2 and the matrix 
C0 to be a multiple of the unit matrix (the multiple being  
supposed to tend to infinity, so as to obtain the uniform prior 
distribution) to obtain the present situation, but with c£ known 
equal to cr2. 
If (j) = cr2, then we obtain from (10), 
7T(dr+1, dr+2, ..., 0a|x) cc exp I -^r2| S c^-Of)2 
and this result replaces (11). In the proof just given we extended 
the proof of theorem 6.4.1 to obtain an F-distribution. If c£ = cr2 
)• 
8.3] LINEAR HYPOTHESIS 231 
we can similarly extend the proof of theorem 6.4.2 to obtain a 
^-distribution. As a consequence of this, a significance test of 
the hypothesis that 6r+1 = 6r+2 = ... = 6S is provided by  
referring Sljcr2 to the ^-distribution with v — s — r degrees of  
freedom. This differs from the test of the present section in that 
S2l(n — s) is used as an estimate of cr2 and the F-distribution 
replaces %2. 
The second stage in the proof consists of rewriting the  
multivariate normal distribution in its regression form. This was the 
way the distribution was introduced in §3.5. The reason for 
writing it this way is that the variables, here the #'s, are  
introduced successively and may be integrated in the reverse order. 
Here we begin with 6S9 then introduce 6s_l9 depending on dS9 and 
so on: whence 61 only appears in the final stage where we may 
integrate with respect to it. This expression in regression form 
will form the basis of the method of numerical calculation to be 
described in the next section. With the integration of the nuisance 
parameters 6l9 629 ..., 6r9 $ carried out we are left with (11) and 
from there we proceed in the way discussed in detail in §6.4. 
Distribution of a linear form 
Corollary 2 provides the distribution of any linear function of 
the #'s, in particular of any 6it (The joint distribution is given 
by (11) but is usually too complicated to be used.) As in previous 
situations, the relevant distribution is a /-distribution. The 
variance is estimated by the residual sum of squares divided 
by (n — s)9 and the mean by the least squares estimate. The 
denominator in (12) can be troublesome because of the term 
g'Cg. A simple way of calculating this term is provided by 
equation (13), which says that g'Cg^ is the variance of the 
least squares estimate, g'6, of g'8. Since 8 is a linear function 
of the x's this variance may be found by the methods of §3.3. 
An example is given in §8.5. The result being used here is an 
extension of that discussed in §5.1 where two distinct  
statements (a) and (b) are easily confused: the posterior variance of 
g'8 is the same as the sampling variance of its estimate g'6. 
In many situations the corollary is more useful than the 
theorem. The latter gives a test of a rather complicated hypo- 
232 LEAST SQUARES [8.3 
thesis and not a complete statement of a posterior distribution. 
The former provides a posterior distribution for a linear  
function of the parameters. Complete specification for one function 
is often more relevant than a partial specification for many. It is 
true that the theorem can easily be adapted to yield a posterior 
distribution but the result is too complicated for appreciation. 
It is commonly sensible to use the theorem in order to eliminate 
unnecessary parameters, and then to use the corollary to  
investigate those that contribute most to the variation of the  
dependent variable. 
General linear constraints 
Corollary 3 extends the test of the theorem to general linear 
constraints amongst the parameters. Notice that B must have 
full rank, or alternatively that the (s — r) constraints in (14) must 
be linearly independent. If this were not so then some of the 
constraints would be implied by the others and the effective 
number would be less than (s — r), thereby influencing the degrees 
of freedom in the F-test. 
Analysis of variance 
The F-test can be written as an analysis of variance test. 
Table 8.3.1 is a generalization of table 8.1.1. The total sum of 
squares refers to the minimum of the sum of squares of  
differences between the x's and their expectations when these latter 
are expressed in terms of the nuisance parameters dl9d2> •••> @r 
only. The total can be broken down into two parts: the  
reduction due to introducing the additional parameters 6r+l9 #r+2,..., dS9 
and a residual which is the corresponding minimum when all 
parameters are used. If the null hypothesis that the extra  
parameters are all zero is correct the two corresponding mean 
squares should be of comparable magnitude since one is  
breaking down quite arbitrarily a total which is entirely due to random 
error: on the other hand, if the null hypothesis is false the 
former should be substantially greater than the residual. The 
F-statistic is, apart from a multiplier involving the degrees of 
freedom, the ratio of these two parts and the data are significant 
if F is some amount in excess of 1; the exact amount depending 
8.3] LINEAR HYPOTHESIS 233 
on the F-distribution. The degrees of freedom, n — s, for the 
residual are obvious from corollary 1 and equally the degrees of 
freedom for the total are n — r. The remaining degrees of freedom 
for the reduction are defined to have the same additivity as the 
sum of squares and the F-statistic is, as before, a ratio of mean 
squares. 
Table 8.3.1 
Sums of Degrees of Mean 
squares freedom squares F 
Reduction due to 2 [S /(s-r)] 
(allowing for 
Residual (fitting S2 n-s S2/(n-s) — 
6lf62i ...,0S) 
Total 5"+5" n-r — — 
(foT0l90%9 ...96r) 
Breakdown of sum of squares 
The sum of squares can be broken down still further, but the 
interpretation requires considerable care. Let r < t < s and let 
the three groups of parameters 
(rij ^2> •••> "rj9 (yr+19 ^r+29 •••? "th (yt+l* "t+2> •••> "s) 
be referred to as the first, second and third groups, respectively. 
Let 5f3 be the minimum of the sum of squares when the  
expectations are assumed to contain only the first and third groups of 
parameters, the second group being zero. Define other sums of 
squares similarly: thus SI is now the total sum of squares for 
0l9 d2,...9dr and 5f23 is the residual. In table 8.3.1 we have written 
$i = *Sri23 + 0S'i"~'Sri23)« 
We may further write 
Si = *$i23 + 0$i2 — ^123) + Wi— S12). (22) 
The three terms on the right-hand side are, respectively, the 
residual, allowing for all three groups, the reduction due to the 
third group allowing for the first two, and the reduction due to 
the second group allowing for the first. Equally we can write 
$i — ^123 + (*^i3"~ £123) + 0$i — ^13)? (23) 
234 LEAST SQUARES [8.3 
where the roles of the second and third groups have been  
interchanged. In both (22) and (23) the ratio of the mean squares of 
the second term on the right-hand side to that of the first term 
gives a valid F-test: in (22) of the null hypothesis that the  
parameters in the third group all vanish, in (23) of the null hypothesis 
that those of the second group all vanish. This follows from the 
theorem. The remaining terms on the right-hand sides do not 
provide tests, under the assumptions of the theorem, since, for 
example in (22), in general S2 — S22 will not be equal to S2Z — 5f23- 
These are both reductions due to the second group but the 
former does not take account of the possible existence of the 
third group. However, it can happen that 
Si- 
S2 
12 
C2 _ C2 
^13 °123> 
(24) 
and consequently also 
SI-SI 
13 
s2 
12 
*^123> 
(25) 
and the two decompositions, (22) and (23), are equivalent. 
Equation (25) says that the reduction in sum of squares due to 
the third group is the same irrespective of whether only the first 
group, or both the first and second groups, have been allowed for. 
Equation (24) is the same with the second and third groups  
interchanged. In this situation we often refer to the reduction due to 
the second or third groups without saying what has been allowed 
for. In these special circumstances table 8.3.1 can be extended to 
the form shown in table 8.3.2. From this table, in which the sums 
of squares and degrees of freedom are additive, it is possible to 
derive valid F-tests for the hypotheses that the second and third 
Reduction due to 
or+1, ...,ot 
Reduction due to 
Residual 
Total (for 0l9...,er) 
Table 8.3.2 
Sums of 
squares 
C2 c2 
O13 — O123 
o2 o2 
O12 —0 123 
*Si23 
Si 
Degrees of 
freedom 
t-r 
s-t 
n—s 
n — r 
Mean squares 
(SU-SlMs-t) 
SlpJin-s) = s2 
F 
Mean 
squares 
divided 
bys2 
8.3] LINEAR HYPOTHESIS 235 
groups of parameters separately vanish. But it must be  
remembered that this is only sensible if (24), and therefore (25), obtain. 
A sufficient, but not necessary, condition for (24) is easily 
obtained from the expression in (8) for the sum of squares. 
Suppose that the matrixf B = A'A can be partitioned into sub- 
matrices corresponding to the three groups of parameters in the 
following way: 
r / Bn | 0 | 0 
t-r I 0 j B22 | 0 | = B. (26) 
s-t \ 0 | 0 j B33 
r t—r s—t 
(The letters at the side and bottom indicate the sizes of the sub- 
matrices: thus Bn has r rows and r columns.) In terms of the 
submatrices B is diagonal and therefore in (8) the three groups 
of parameters appear separately and, for given <j>, they are 
independent. Consequently, minimization with respect to the 
parameters of one group does not affect those of another and 
(24) holds. An example of this is the weighing example of 
§§3.3, 6.6, where we saw (equation 6.6.13) that B = A'A was 
diagonal so that each parameter may be investigated separately, 
irrespective of the others. A design matrix A which is such that 
A'A has the form (26) is said to be orthogonal with respect to the 
three groups of parameters (and generally for any number of 
groups). Other things being equal, orthogonal experiments are 
to be preferred. 
Although, in general, S\ — S\2 does not provide a test under 
the assumptions of the theorem, because it ignores the third 
group of parameters, it can under different assumptions. If 
»$i2 — ^123 does not yield a significant result, so that the  
parameters of the third group are possibly all zero, one might wish 
to test whether those of the second group are also zero under 
the assumption that those of the third group are. Clearly S\ — S\2 
then does provide such a test in comparison with S\2 (not 
5f23). This test is particularly useful where there is a natural order 
in which the groups should be introduced—first, second, third— 
f B should not be confused with the matrix occurring in corollary 3. 
236 LEAST SQUARES [8.3 
and examples will be found in §§8.6 (a) and (c). Compare, also, 
this argument with that suggested in §6.3 for deciding whether 
to use Behrens's test or a f-test. 
Least squares estimates 
It follows from (8) that the statistics 6 and S2 are jointly 
sufficient for 8 and ^. The least square estimates and the residual 
are therefore important quantities to calculate in any analysis of 
a linear hypothesis. The calculation can be done in two ways: 
by finding (A'A)-1 = C and hence 6 from (7), and then 
S2 = x'x-6'A'A6, from (6), (27) 
= x'x-x'ACA'x, from (7) (28) 
(from which it appears that x'x and A'x are also jointly  
sufficient); or alternatively by actual minimization of the sum of 
squares, the minimum being 52, the values of 8 at the minimum 
being 6. The former method is to be preferred in the case of 
a general A and we discuss it in the next section: the calculation 
depending on the inversion of A'A. The latter method is more 
convenient when the structure of A is particularly simple, as it 
often is in well-designed (for example, orthogonal) experiments. 
Examples are given in §§8.5, 8.6. 
8.4. Computational methods 
In this section we consider how the computations necessary to 
perform the tests of the previous section may be arranged when 
the matrix A'A is general and it is therefore not possible to take 
advantage of any special structure that it might have. The 
methods are designed for use with desk calculating machines. 
If f' and g' are two row-vectors, or simply rows, 
*' = (/l>/2> --->fn), g' = (gl,#2, ...,gW), 
each of n elements; then by the product of these two rows we 
n 
mean the scalar product S/Jg*. The product of a row and 
8.4] COMPUTATIONAL METHODS 237 
column or two columns is defined similarly. If all the elements 
of f and g are known, except for gn, and we require the product 
n 
to equal c, we speak of 'making-up9 g so that E/Jg* = c. 
The stages in the computation are as follows. They should be 
studied in connexion with the numerical example given below: 
(1) The quantities x'x, u = A'x and B = A'A are calculated. 
(2) The theory was based on rewriting (8.3.8) in the form 
(8.3.9) and the same device is used in the computations. Let 
A'A be written TT, where r is an s x s upper triangular matrix : 
that is, a matrix with all elements below the leading diagonal 
zero; y^ = 0 for i > J Then 
(e - Sy A'A(e - 8) = (e-Sy rT(e-S) = \% 
where \ = T(8 — 8) and, because of the triangular form of T, 
(ij inyolves only dj9 0J+l9 ..., d8. It easily follows that, in the 
notation of equation 8.3.9, £J = ^(fy-a;)2. The fact that we 
know such a transformation is possible and unique, establishes 
the existence and uniqueness of I1. 
The equations for T, TT = B, may be written: 
(jth row of I") x (jth column of T) = bi:j 
or (ith column of T) x (Jth column of T) = bijt 
If the calculations are carried out in the order i = 1,7= 1,2,...,,?; 
i = 2,7 = 2, 3, ..., s and so on, on each occasion one element in 
the product of the two columns will be unknown and may be 
found by' making-up': this follows because of the zero elements 
in T. (If i = 7 two equal elements are unknown and the 
'making-up' involves a square root.) 
It is advisable, as always, to have a check on the calculations. 
This is here conveniently provided by using the relation, which 
easily follows from (1), that 
(ith column of T) x (column of row sums of V) 
= (ith element of row sums of B) 
after each row of r has been found. 
(i) 
238 LEAST SQUARES [8.4 
(3) The vector to, of elements o)i9 satisfying 
Fto = u (2) 
is computed. This is done by thinking of (2) as 
(ith column of T) x (the column to) = (*th element of u). (3) 
Since the first column of T contains only one non-zero element, 
<<>! may be found; and then o)29 &>3, ... . A check is provided by 
(column of row sums of T) x (the column to) 
= (sum of elements of u). 
(4) The reduction in sum of squares due to 6r+l9 6r+29 ..., #3 is 
s s 
S wf • the residual sum of squares is S2 = x'x — 2 *>?• The 
i=r+l i=l 
latter result may be proved by remarking that since 
TTS = u (from equation 8.3.7), 
rS = to, (4) 
from (2). Hence, from equation 8.3.6, 
S2 = x'x-6'A'A6 = x'x-STTS 
= x'x—to'to, (5) 
as required. The former result follows by remarking that just as 
3 
x'x — S w| 
i=i 
is the minimum after including all the #'s, so 
x'x — 2 to2 
i=i 
r 
is the minimum if only 0l9 629 ..., 6r are included and the 
remainder put equal to zero. This is obvious if the computing 
scheme with this reduced number of O's is considered: it will 
mean merely considering only the first r rows and columns of 
8.4] COMPUTATIONAL METHODS 239 
the matrices. Hence the minimum is reduced still further by 
s 
S w? if #r+i> #r+2> • • •, #s are included. 
i=r+l 
(5) The statistic (8.3.4) is then 
T £ <o2il(s-r)]l[S*l(n-s)]. 
Li=r+1 J/ 
(6) The least squares estimates, 6, are found from (4), which 
may be thought of as 
(fth row of T) x (the column 6) = (ot. (6) 
Since the last row of T contains only one non-zero element, 0S 
may be found first, then 0s_l9 $s_2, .... A check is provided by 
s 
(row of column sums of T) x (the column S) = 2 &v 
(7) Since A'A = TT 
(A'A)-1 = r-1^')-1 
and (A'A)"1!' = r-1. 
This may be thought of as 
(ith row of B"1) x (Jfh row of T) = 7^', 
where yij is the typical element of T-1. Now we know that since 
T is upper triangular so also is r_1. Hence yij = 0 for / > j\ 
Furthermore, yu = y^1. Thus all elements of T_1 on the leading 
diagonal and below it are known and consequently with 
and so on, we can find the elements of B_1, known to be  
symmetric, successively. A check is provided by 
(fth row of B"1) x (column of row sums of B) = 1. 
All the relevant quantities are now available. 
Arrangement of the calculations 
The following example illustrates the procedure and gives the 
most convenient arrangement of the calculations on the paper. 
240 LEAST SQUARES [8.4 
We consider first the purely numerical part, and leave the 
statistical considerations until later. 
+ 5-5140 
B 
+ 5-0958 
+ 6-6257 
-1-5150 
-0-8510 
+ 5-9980 
+ 2-3482 
+ 2-1701 
+ 1-3843 
-0-6452 
+0-3967 
+ 2-3290 
-l-4446\ 
-1-1755 \ 
+ 1-7614 I 
+ 7-3071/ 
-0-6152' 
+ 0-1153 
+ 0-5662 
+ 2-5680; 
row sums 
+ 3-2579 
+ 1-8963 
+ 2-8952 
+ 2-5680 
column 
sums + 2-3482 
yi1 +0-4259 
(6979-32' 
7583-66 
3488-51 
6262-42/ 
sum 24313-91 
+ 3-5544 +2-0805 +2-6343 
+0-7224 +0-4294 +0-3894 
c«) 
2972-20' 
818-96 
2181-75 
V2632-86/ 
sum 8605-77 
9 
1437-52' 
309-18 
687-53 
1025-26- 
+0-6873 
B 
-i 
-0-5112 +0-0918 
+ 0-5370 -0-0523 
+ 0-1933 
+ 0-0315' 
-0-0021 
-0-0369 
+ 0-1516, 
row sums 
ofB 
+ 7-6502 
+ 9-6950 
+ 5-3934 
+ 6-4484 
Notes. (1) B is symmetric, so only the leading diagonal and 
terms above it need be written out. Nevertheless, in forming the 
row sums, for checking, the omitted elements should not be 
forgotten. These row sums are most conveniently placed at the 
end of the computations next to B_1, since that is where they are 
needed in checking stage (7) of the calculation. 
(2) The omitted elements of r are, of course, zero. Both row 
sums (in stage (3)) and column sums (in stage (6)) are needed 
for checking. 
(3) Notice that aside from the elements required for checking 
and the final quantities required (the wf's, 6 and B_1) only the 
8.4J COMPUTATIONAL METHODS 241 
matrix T, the vector u and the inverses y^1 = yu have to be 
recorded. Since the main source of error with desk machines 
lies in the transfer from paper to machine and vice versa the 
opportunities for error are reduced by this procedure. 
(4) All operations consist of row or column multiplications 
with' making-up'. The use of only one basic operation makes for 
simplicity. 
(5) Experience shows that this method is of high accuracy. 
It is usually enough to take one more significant figure in the 
calculations than will be needed in the final answer to allow for 
rounding-off errors. 
Multiple regression 
The case of general A most usually arises in a generalization 
of the situation of § 8.1, Consider s variables (y, xl9 x2, ..., xs_i): 
y is a random variable whose distribution depends on the x's in 
a way to be described. The x's may be random or not; if 
random, then they are ancillary (see the discussion in §8.1). If 
the x's take values xil9 xi29 ..., xit s_x (i = 1,2,..., n) the random 
variable y9 with value yi9 is normally distributed with mean 
s-l 
*0>l xn> xi2, • • •> xit s_i) = a + 2 foxy - xtj) (7) 
and @2(y\xil9 xi2, ...,xit s_x) = 0, (8) 
n 
where, as usual, xmj = 2 Xyln. 
In these circumstances y is said to have a (linear homoscedastic) 
multiple regression on the x's. The words in brackets are usually 
omitted. If the yt are, for these values of the x's, independent, 
we have a random sample from the multiple regression, fa is 
called the multiple regression coefficient of y on xjt The notation 
for it is an abbreviation since it also depends on the other  
variables included in the regression and, in general, would change if 
any of them were excluded or others introduced. The coefficient 
measures the change in the dependent variable caused by a unit 
change in xj9 the others being held constant. 
16 LSII 
242 LEAST SQUARES [8.4 
The situation is clearly a linear hypothesis: in the notation 
of§83: x' = (^2,...,A), (9) 
e' = (a,A, ...,/U) (io) 
and 
1 Xu Xml X12 Xm2 ... xl,s-l x.s-l 
1 ^21 ""-*'. 1 *22 ~x.2 ••• X2,s-l~X.s-l 
»•• # # * ••• ••• ••• 
(11) 
(compare the linear regression example in §8.3 especially  
equation 8.3.19). Typically the xtj cannot be conveniently chosen, 
because the x's are random or are otherwise not under control, 
and the design matrix has a general form, apart from the first 
column. The test of theorem 8.3.1 is a test that 
Pr = Pr+1 = ... == Ps-1 ~ "' 
that is, a test of the hypothesis that the random variable y does 
not depend on the variables xr, xr+l9 ..., xs^v 
The fact that the first column of A consists entirely of l's can 
be exploited. We easily see that 
n 0 0 ... 0 
A'A = J ° bl1 bl2 - *m-i |, (12) 
• •• ••# • • # * • • ••• 
0 ^l,s-l ^2,s-l •*• ^s-l,s-l 
n 
where by = 2 (xw-x.i) (*#-*.,), (13) 
the sum of squares (/ = j) and products (j 4= 7) of the x's about 
their means. Because of the zeros in A'A; in equation 8.3.8 the 
term in dx (here 0X = a) is separate from the rest and we may 
perform an integration with respect to it to obtain (in the 
multiple regression notation) 
tK/^Aj, ...,&-i, 0|x) 
oc 0-i<"-D-iexp [- {(P- $)'B(p- p) + S2}/20], (14) 
which is the same form as before with B the matrix whose 
typical element is bij9 equation (13). It is also clear from 
8.4J COMPUTATIONAL METHODS 243 
n 
equation 8.3.7 that & = y — 2 yjn, and hence, from equation 
8.3.6, that the residual sum of squares is 
S2 = x'x-6'A'Ae = Z^-/*y2--P'B(a 
= s(y<-y)f-NI. (15) 
Hence the form of (14) is the same as that of (8.3.8) with 
n reduced by one and the observed variables replaced by  
deviations from their means. We have the usual phenomenon that the 
presence of an unknown mean reduces the degrees of freedom 
by 1. The calculations can be carried through with /?i,/?2> • • •> /?s-i 
only, and deviations from the means. 
Numerical example 
The numerical example concerned a situation of the multiple 
regression form, where a sample of thirty small farms of 
similar character was studied over a period of three years. The 
variable y was income of a farm, and it was desired to see how 
this was affected by xl9 the size of the farm; x29 the standardized 
production and by two indices, x3 and x4, of yield and feeding. 
The matrix B of the numerical example gives the sums of 
squares and products, equation (13), of the x variables usually 
called the independent variables. These have been given here in 
suitable units to make the four diagonal entries of B of the same 
order. Such changes of scale are desirable in order to simplify 
the calculations. The vector u is the vector of sums of products 
of y with the jc's : namely the /th element is 
n n 
Ui = 2 yk(xki-xmi) = 2 (yk-y) (xM-xmi). (16) 
&=i &=i 
The total sum of squares of the /s is S(y,.-J)2 = 40, 572, 526. 
Hence the residual sum of squares is this less 
I>f = 21, 196,653, 
i=i 
giving S2 = 19, 375, 873 on (29-4) = 25 degrees of freedom. 
As an example of the test of theorem 8.3.1 consider a test of 
16-2 
244 LEAST SQUARES [8.4 
whether /?3 = /?4 = 0; that is, of whether the income depends 
on the two indices. This is provided by referring 
K"§+"!)/2]/[S2/25] (17) 
to the F-distribution with 2 and 25 degrees of freedom. The 
numerical value of (17) is 7-54 and the upper 1 % value of Fis 
5-57. The result is therefore significant at 1 %: or, in Bayesian 
terms, we are at least 99 % confident that the yield does depend 
on the indices. The significance level is greater than 0-1 %, 
however, since at that level the F value is 9-22. 
A test of dependence on any one of the x's is provided by 
corollary 2 of the last section. For example, a test of the 
dependence on x2, the standardized production, is obtained by 
referring 3Q9# {g/ ^ x 0.5370/25jj = 0-48 
to the /-distribution with 25 degrees of freedom. (^2 = 309*18 
and 0-5370 is the appropriate element in B-1 = C.) The 5 % 
value for / is 2-06 so that the result is not significant. There is, 
therefore, fair evidence to suggest that the income is not affected 
by the standardized production, after allowance for the other 
factors. 
Consequences of non-orthogonality 
The only hypotheses that it is possible to test using the  
computational lay-out suggested above are that 
Ar+i = Ar+2 = ... = A = 0 for r = 0, 1, 2, 3 
(by the theorem) or 
p. = 0 for i = 1,2,3,4 
(by corollary 2). It is not immediately possible, for example, 
to test the hypothesis that /?x = /?2 = 0. To do this it would be 
necessary to rearrange the matrix B and the vector u so that the 
last two rows and columns referred to xx and x2\ when a test 
would be obtained by using what would then be w|+w|,  
corresponding to the last two parameters introduced. Before  
beginning the computations it is therefore important to consider 
which hypotheses are the most important to test and to order the 
8.4J COMPUTATIONAL METHODS 245 
rows and columns accordingly. Notice that it is not permissible 
to arrange the calculations in the form of an analysis of  
variance table (table 8.3.2) in order to obtain a test, for example, of 
P\ — 02 = 0- If the calculations are so arranged all one could 
obtain would be a test that /?x = /?2 = 0 assuming /?3 = /?4 = 0, 
since the reduction due to /?x and /?2 in the computations, as 
arranged above, has not allowed for the presence of /?3 and /?4. 
This test might have been useful had the test of /?3 = /?4 = 0 
not been significant. 
The argument used above in describing stage (2) of the 
calculations shows when the analysis of variance table is useful 
in providing significance tests. Consider the notation as in 
table 8.3.2. Suppose that r has the form 
t-r 
s-t 
xii 
0 
o 
r 
r 
x12 
r 
x22 
o I 
t-r 
r13 
0 
r 
x33 
s-t 
(18) 
(cf. 8.3.26): that is r23 = 0. If the sum of squares is written in 
the form 2£f+ S2, where S2 is the residual and \ = r(8-fi), 
we see from (18) that £2, for r<j^t, has no terms in 6t+l9 ..., 
ds. That is, it involves only terms in the second group, just as 
£2 fory > t has only terms in the third group. This equally applies 
to c£di — oii) = £i (equation 8.3.9). Consequently the reduction 
t 
in the sum of squares due to the second group is 2 ty(0 — a;)2 
j=r+l 
irrespective of the values in the third group. (Here a J is the 
value of otj when the #'s in the second group are zero.)  
Consequently this is the genuine reduction due to the second group, 
$i ~ >$i2 = ^13" >$i23» and the tests for the two groups are both 
valid. 
Joint distribution of regression coefficients 
If (j) is known, equal to cr2, the matrix B-1cr2 is the dispersion 
matrix of the posterior distribution of the regression coefficients 
(from 8.3.8). The diagonal elements are the variances needed in 
the /-tests for the individual coefficients. It is also useful 
to calculate the correlations from the off-diagonal terms, the 
246 LEAST SQUARES [8.4 
covariances, so that one can see how far the distribution of one 
coefficient is affected by another. The correlations here are 
— -0-8415 +0-2519 +0-0976 
— — -0-1623 -0-0074 
— — — -0-2155 
The only noteworthy one is the negative correlation between xx 
and x2, the larger farms having the smaller standardized  
production. The result above, that x2 probably had no effect, may have 
arisen because of this correlation. The effect investigated by the 
test is the effect on y of varying x29 keeping xx constant, and it is 
difficult to estimate this accurately with such a high correlation 
between xx and x2. 
It is possible to proceed in much the same way that we did in 
§8.1 and obtain confidence limits for quantities like the expected 
value of y when xt = xf> (i = 1, 2, ..., s— 1). Confidence limits 
for the individual /?'s may similarly be obtained. Joint confidence 
sets for the four /?'s (or the three significant ones, /?2 not being 
significant) are complicated in form and it is perhaps better to 
look at the form (19) which gives a picture of these sets had 
<j> been known. 
8.5. Two-way classification 
In this section we discuss an application of the general theory 
of §8.3 when the design matrix is particularly simple and the full 
computational technique of §8.4 is not needed. Observations, 
xijk9 normal and independently distributed with common  
unknown variance <fi, are said to form a two-way classification if 
'(**) = *<* (1) 
for i = 1, 2, ..., I;j = 1, 2, ...,/andfc = 1, 2, ..., K, where/,/ 
and K all exceed one. All the observations for a fixed value of i 
are said to be at the ith level of the first factor: all those for a fixed 
value of j are at theyth level of the second factor. The K  
observations at both the ith level of the first, and the yth level of the 
(19) 
8.5] TWO-WAY CLASSIFICATION 247 
second factor, are replications, identically distributed. Using the 
usual' dot' notation for an average we may write 
-O'ij + Oi + Oj + O., (2) 
where the terms with primes correspond to the three terms in 
brackets in the previous line. Necessarily we have 
2*i. = 2^ = 2^ = 2^ = 0, (3) 
i j i j 
and, provided these relations obtain, the correspondence between 
the #'s and the #"s plus dm is one to one. 0'im is the main effect of 
the first factor at the /th level: if d\ = 0, all /, then the first 
factor has no main effect. Similarly, for the second factor. 6^ is 
the interaction of the two factors at the /th and /th levels: if 
Oy = 0, all i and j, then there is no interaction of the two 
factors. 
Theorem 1. In a two-way classification, as just defined, the 
analysis of variance table {table 8 J.1) provides, in an obvious way, 
significance tests of the null hypotheses (i) no main effect of the 
first factor, (ii) no main effect of the second factor, and (iii) no 
interaction of the two factors. 
Sums of squares 
Main effect of first factor 
S} = JK^(xu-x.J2 
• 
Main effect of second factor 
1* 
J 
Interaction 
^(Xij. — xi.. — xJ. + x..) 
i 1 
Residual 
S = 2j {Xijjc — Xij j 
i, j, k 
Total S (xm-x..f 
i, j, k 
Table 8.5.1 
Degrees of 
freedom 
I-\ 
J-\ 
(/-1)</-1) 
IJ(K-1) 
IJK-1 
Mean squares 
*J = sj/(/-i) 
*J = 5J/(/-l) 
sh=sijia-iw-i) 
s2 = syu(K-1) 
— 
F 
s?/s2 
sj/s2 
sh/sz 
— 
— 
248 LEAST SQUARES [8.5 
The proof is based on the following breakdown of the total 
sum of squares, suggested by equation (3): 
2 (xijk~@ij) : 
i>j>k 
- 2 [(xijk~~xij) + (xij. — xi..~x.j. + x...~® 
i,J,k 
+ (*..-*...-0{.) + (*.y.-*...-0'.y) 
+ (*..~02 
- 2 (^fc — ^.) 
+^S(%.-^..-x.:?.+x...-^)2 
+/*E(*..-x...-<?;.)■ 
• 
+/^s(x.,.-x...-<?:,)« 
+nK(x„-ey, (4) 
in which all the product terms vanish. 
Consider first a test of the null hypothesis that the first factor 
has no main effect: that is, 0'im = 0 for all *. This is a linear  
hypothesis and the result of corollary 3 to theorem 8.3.1 may be 
applied. To do this the total sum of squares has first to be  
unrestrictedly minimized with respect to the 0's; this may be 
replaced by a minimization with respect to the #"s and 6 
provided the relations (3) obtain. The values 
6'< 
%o 
Xjj Xj X /f ~r X , 
X* X , 
• J • • • . * 
X 
...9 
(5) 
6'< 
$[ 
6 
satisfy (3) and with these values (4) reduces simply to its first 
term. Hence the residual sum of squares is S2 = 2 (xw ~~ xij.)2- 
The degrees of freedom for S2 are IJK-IJ = IJ(K-1). If 
0'ia = 0 for all i, and we minimize with this restriction the values 
6'ii9 0'mj and 6am still obviously provide the least values and the 
minimum, with this restriction, is S2 +JK 2 (xium — xmm)2. Hence 
the reduction in sum of squares due to d'lm9 d'2m9 ..., d'Im (allowing 
for the other d"s and 0 ) is 
Sj=JKX(xt.-x„y. (6) 
8.5] TWO-WAY CLASSIFICATION 249 
The null hypothesis is 0lm = 62 = ... = 6Z which imposes 
(/— 1) linear relations amongst the #'s, and hence 
F= [SW-l)]l[S2IU(K-l)] (7) 
with (I—I) and IJ(K— 1) degrees of freedom provides the 
required significance test. Similar remarks apply to testing that 
there is no main effect of the second factor. 
Consider next a test of the null hypothesis that the interaction 
is zero: 6^ = 0 for all i and j. Minimizing with this restriction 
the values 6'im9 0'mj and 0 m still provide the least values and the 
reduction in sum of squares due to all the dy is 
Sh = K2 (**.-*..-*,. + * ..)2. (8) 
The degrees of freedom need a little care. In terms of the #'s the 
null hypothesis is that 
0<,-0<-0.i + 0.. = 0 (9) 
for all i and j. For any fixed j this means that 
dij~6i. = G2j~G2. = ... = Oij-Oi.i (10) 
that is, (/— 1) independent relations amongst the #'s. Consider 
these for ally except y = /: that is, (/— 1) (/-1) relations in all. 
Then the same relation, (10), for j = J can be deduced from 
them, since from (3), 
^(Gij-OJ+ 6^-6^ = 0. 
Hence (9) implies that the matrix B of corollary 3 to theorem 
8.3.1 has rank (/— 1) (/— 1) and these are therefore the degrees 
of freedom to be associated with 5|j. The F-test proceeds in 
the usual way and the three tests described in the table have all 
been obtained. Because of the breakdown of the sum of squares, 
(4), the design is orthogonal with respect to the four sets of  
parameters, {Oy}, {d'i}, {d'mj} and 6 as explained in §8.3, and the sums 
of squares do not need to specify the other parameters allowed 
for. The total in table 8.5.1 refers to that allowing for 0 , which is 
not usually of interest. The sums of squares are additive, again 
by (4). 
250 LEAST SQUARES [8.5 
Relation with one-way classifications 
The situation considered in this section is a generalization of 
that of §§6.4, 6.5. The between and within analysis there  
discussed can be put into a linear hypothesis form. In the notation 
of those sections ^(x^) = 6i9 the notation only differing from 
that of this section in the use of double suffixes for the  
observations : i and j here, for the single suffix i there. The null  
hypothesis to be tested is that all the #'s are equal. The observations 
may be thought of as being classified in one way according to 
the sample to which they belong. The observations in this 
section are classified in two ways according to the two factors. 
A practical example of such a situation is provided by the  
preparation of a drug, in which two factors may be of importance 
in determining the quality; the amount of glucose used and the 
temperature of the reaction. If K preparations of the drug are 
made at each of / levels of the amount of glucose and / levels of 
temperature, the // different combinations produce in all UK 
observations on the quality of the drug which might satisfy the 
linear hypothesis, equation (1). If we wished to test the null 
hypothesis that all the #'s were equal, the methods of §§6.4, 6.5 
could be used. There are // samples each of size K But we are 
usually more interested in testing hypotheses specific to one of 
the factors; for example, that the temperature does not affect 
the drug. We consider how this can be done. 
Main effects and interactions 
The meanings to be attached to the terms 'main effect' and 
'interaction' require careful consideration. To take the main 
effect first: from the definition di m = 2 OalJ* Q%. is the average 
i 
expectation at the fth level of the first factor, the average being 
taken over the / levels of the other factor. d\ m is the deviation of 
this average from the average over all levels of both factors, 
9 = S OijlH* Consequently the main effect of the first factor 
is only defined with reference to the second. For example, if one 
of the levels of the second factor were to be omitted, as might 
happen if a calamity overtook the readings at that level, then the 
8.5] TWO-WAY CLASSIFICATION 251 
definition of the main effect of the first factor might change. 
Thus, if there is no main effect, 0'ia = 0 for all j, or dt does not 
depend on i, and, we ought, in full, to say' the first factor does 
not influence the results (when averaged over the particular 
levels of the other factor used in the experiment)'. The words in 
brackets are often omitted. 
There are circumstances in which the averaging effect may be 
irrelevant and to discuss this consider the interaction. The 
meaning to be attached to saying that the interaction is zero 
(one null hypothesis tested in the analysis of variance) can most 
easily be understood from equation (10). 0y —#i. is the  
difference between the expectation at the 7th level of the second factor 
and the average over the second factor when the first factor is at 
level 1: briefly, the effect of the second factor at level j when 
i = l. Equation (10) says that this is the same for all i and is 
consequently equal to the main effect 0^ — 6 = d'jt Thus the 
interaction being zero means that this is true for all i and j. The 
two factors can be interchanged, and consequently the  
significance test investigates whether the effect of one factor depends 
upon the other or not. If it does the two factors are said to 
interact. If there is no interaction then the main effect of one 
factor is the same as the effect at all levels of the other factor 
included in the experiment, and may therefore be quoted  
without reference to the averaging over it. Consequently it is only 
when the interaction is zero that it is satisfactory to talk of a 
main effect. Even then it only refers to the levels used in the 
experiment. If there is an interaction then it is probably better 
not to use a main effect but to think in terms of the effects 
0tj - 0i at the different levels of the other factor. Thus, if in our 
example, the temperature only influences the drug at high 
glucose concentrations, so that an interaction is present; then 
rather than quote a main effect of temperature (that is, averaged 
over the glucose levels) it would be better to quote the  
temperature effects separately at high and low levels of glucose (the 
latter being zero). Notice that to say that a main effect is zero 
does not mean that the factor has no effect, for it may interact 
with the other factor in opposite directions at different levels of 
the other factor, whilst keeping the average zero. To say that 
252 LEAST SQUARES [8.5 
the first factor had no effect would be to say that dy = 6 %i for 
all i, j: that is, 0'ia = Oy = 0, or that both main effect and 
interaction are zero. 
Additivity 
An interaction being zero also means (equation (2)) that 
*(*«) = O'i. + Vi + O.* (11) 
so that the effects are often described as being additive: the 
expectation is the total of the overall mean plus the two main 
effects. Notice that whether or not the interaction is zero 
depends upon the random variable being considered. For 
example, suppose , x 
0a = "tfii (12) 
for suitable a's and /?'s. Then (11) is not true and there is, in 
general, an interaction. But 
In 6^ = lna^ + ln/?,., (13) 
so that if the logarithms are used the effects are additive and the 
interaction vanishes. 
Orthogonality 
Another reason for discussing main effects and interactions is 
the breakdown of the sum of squares into the five separate sums 
in equation (4). The first of these refers only to the random  
variation and is unaffected by the #'s; the second to the interaction, 
the third and fourth to the main effects of the two factors 
separately, and the last to the deviation from the mean, 0 %m. The 
last is not usually of interest because experiments of this form 
are usually comparative ones: that is, one wishes to compare 
one level of a factor with another level, rather than to consider 
the value of any 6ijt Because of this breakdown the reduction 
due to one set of 0"s is unaffected by the values of the other #"s 
and the design is orthogonal with respect to the three groups of 
parameters (cf. §8.3). Consequently the analysis of variance 
table is useful and the only difficulty in proving theorem 1 lies in 
deriving the appropriate F-tests. In table 8.5.1 the reduction in 
8.5] TWO-WAY CLASSIFICATION 253 
sum of squares due to d'l9 d'2, ..., 6'T has simply been referred to 
as that of the main effect of the first factor, and the others 
similarly. 
Numerical calculations 
The arithmetical calculation of table 8.5.1 follows the same 
method as used for table 6.1 in §6.5. The calculation of the sum 
of squares for the main effect of the first factor is performed by 
thinking of the factor as dividing the observations into / groups 
with JK readings in each, when the quantity required is simply 
the between groups sum of squares, found as in (6) of §6.5. 
Similarly, for the other factor. Equally the observations may 
be thought of as being made up of // groups with K readings in 
each. The between sum of squares with this grouping will  
combine the two main effects and the interaction, so, by the additivity 
of the sums, the interaction sum of squares is obtained by  
subtracting from this 'between' sum the two previous 'between' 
sums for the main effects. The residual is most easily obtained 
by subtraction from the total, although it could be found by 
adding the // separate sums of squares E (**# — *# .)2 from 
k 
each of the // samples. 
Breakdowns of the sum of squares 
The resulting table is capable of a simple and informative 
interpretation: like all analysis of variance tables it provides a 
breakdown of the total variation present in the readings, as 
measured by the total sum of squares, into components with 
separate meanings. Here it has four components, one of which, 
the residual, is unaffected by the #'s and provides, in effect, our 
knowledge of <p (corollary 1 to theorem 8.3.1). The other three 
refer to the main effects and interaction as already explained. 
The separation of these components can be appreciated by 
remarking, for example, that a change in one main effect will 
not alter the sum of squares for the other, nor for the interaction. 
Thus if all the readings with i = 1 were increased by the same 
amount the only sum of the four in table 8.5.1 to be affected would 
be the main effect of the first factor. 
254 LEAST SQUARES [8.5 
One should not stick slavishly to the orthogonal breakdown of 
table 8.5.1 and its associated tests. There are other possibilities: 
for example, one could test the null hypothesis that the first 
factor had no effect by adding the first main effect and the  
interaction sums of squares, similarly adding the degrees of freedom 
together, and using the F-test derived from these. Alternatively, 
since the method is an extension of that of §6.5, it is possible, as 
in that section, to break up the sum of squares into further 
orthogonal parts, if this seems physically appropriate. For 
example, if one of the levels of temperature is the standard 
temperature, then the main effect for temperature may have its 
sum broken into two parts; one comparing the standard against 
the average for the remaining levels, and one amongst those 
levels. The interaction may be similarly subdivided. 
Confidence limits 
Corollary 2 to theorem 8.3.1 enables confidence limits to be 
assigned to any linear function of the #'s. Suppose, in line with 
the suggestion in the last paragraph, we wished to consider the 
value of a = 0X — 2 ^ 1(1—1)> the difference between the first 
i>l 
level and the average for the other levels of the first main effect. 
Since dtj has posterior mean 0tj = xii%9 cc will have mean 
a = *i..- 2 *<../(/-1). 
i>l 
The posterior variance of a (the quantity g'Cg^ of the corollary) 
is most easily found from equation 8.3.13. Since the xijk have 
variance ^ and are independent 
^2(& I {0<,}, 4>) = WK+ <f>IJK(I-1) = fflJK(I-1). (14) 
Consequently (a - &)l[s2IIJK(I-1)]* (15) 
has a ^-distribution with v = IJ(K-1) degrees of freedom. 
Random effects 
There are situations which superficially look like a two-way 
classification of the type here considered but require a different 
treatment. Suppose that the levels of one or both of the factors 
8.5] TWO-WAY CLASSIFICATION 255 
are obtained by random sampling from a population of such 
levels. For example, we may be interested in the effect of  
different human operators on the quality of the drug, so that one 
factor would correspond to operators, and the levels of that 
factor to the different people, chosen at random from the  
population. In that situation we would wish to make inferences about 
the population of operators, and not merely about those 
operators who took part in the experiment, so that a different 
analysis is required. The model we are studying, under the name 
linear hypothesis, is & fixed effects model (the 6tj have no random 
variation): the other is called a random effects model. If one 
factor is fixed and the other random we have a mixed model 
Only the fixed effects model will be discussed in this book. The 
others are considered, from a practical viewpoint by O. L. 
Davies (1957, chapter 6) and the theory is given by H. Scheffe 
(1959, part II): though neither practice nor theory are in the 
same definitive form as is that of the fixed effects model based on 
the linear hypothesis. 
No replication 
The case K = 1, so far excluded, is interesting and of common 
occurrence. The observations may be denoted by xtj instead of 
xijv Since each is about a possibly different mean 6^ there is 
no information about the random error <j). This is reflected in 
table 8.5.1 where the degrees of freedom for the residual are zero. 
However, if some prior knowledge of the d's is available  
inferences are possible. A common form for this prior knowledge to 
take is for it to be known that the interaction is zero. Then (11) 
obtains and the two main effects are additive. This equation is a 
linear hypothesis in certain parameters {0J}, {d[0) and 0 mm which 
themselves satisfy two linear constraints, from equation (3), 
S 01. = S 0'.i = 0. (16) 
Because of these constraints it is not of the usual linear  
hypothesis form but may be put into the form in two fewer  
parameters by eliminating, say d'lt and 0[l9 using (16). The tests could 
then be obtained in this modified system by the appropriate 
256 LEAST SQUARES [8.5 
minimizations. But it is not necessary to do this because it is 
clear that the procedure just mentioned is equivalent to  
minimizing the sum of squares in the original system of {6^}, {O[0) 
and 6 subject to the constraints (16) and any additional ones 
needed in connexion with the hypothesis being tested. 
Corresponding to (4) we have 
+mxi.-x.-d'i?+i?l{x.j-x.-d[))*+ij{x.-dy, 
i j 
(17) 
and the same type of argument that justified table 8.5.1 
produces the analysis of variance table 8.5.2. The first two rows 
are exactly as in table 8.5.1 with K = 1. The third row only differs 
from the corresponding row of table 8.5.1 in that, instead of  
referring to an interaction, now known to be zero, it is the residual. 
The F-tests now compare the main effects mean squares with 
this new residual. Essentially then the case K = 1 is only  
distinguished from K > 1 by the fact that the interaction is used as 
a residual in default of the usual residual: which procedure is 
only valid if the true interaction, {##}, is known to be zero. 
Table 8.5.2 
Sums of squares 
Main effect of first factor 
i 
Main effect of second factor 
• 
Residual 
S — 2 \Xij—Xi t—x j-\- x t) 
Total Xixv-x.f 
——. r— 
Degrees of 
freedom 
7-1 
/-l 
(7-1) (/-l) 
7/-1 
Mean squares F 
*?= 5|/(7-l) ,J/5" 
sj = SJKJ-1) sjls* 
s2 = S2l(I-l)(J-l) — 
— — 
Another related situation that often arises is a two-way  
classification in which the numbers of observations at each  
combination of factor levels are not all equal: that is, k runs from 1 to 
8.5] TWO-WAY CLASSIFICATION 257 
Ky in (1). The simple orthogonal breakdown given in  
equation (4) is no longer possible and it is necessary to resort to a 
fairly complicated minimization of the sums of squares using 
theorem 8.3.1. We shall not enter into details: these will be 
found in Scheffe (1959). 
8.6. Further applications of linear hypothesis theory 
(a) Comparison of regression lines 
Theorem l.Ifx = (xi:j; i = 1, 2, ..., n$\ j = 1, 2, ..., m) is a set 
of real numbers, and if for fixed x, the random variables ytj {with 
the same ranges of suffixes) are independent normal random  
variables with /?/ I \ Of \ /1\ 
<%dx) = <Xj+fa(xn-xJ (1) 
and ^20^-|x) = <f>; (2) 
then, if the prior distributions of{oCj}9 {/?;} andlvKJ) are independent 
and uniform, a significance test at level a of the hypothesis that 
fil = A = .- = fim (3) 
is obtained by declaring the data significant if 
[5j {SxyjISxxj} ~~ {S SXyj\ /S ^xxjllV71 ~~ V 
~ 2SJ/E(«,-2j " 
0 0 
_ m 
exceeds Fa(yl9 v2) with vx = m— 1, v2 = S (fy —2). 
77*e notation used in (4) fa (c/. theorem 8.1.1) 
i i 
Syyj = 2 (jij'-y.j) 9 
i 
(4) 
(5) 
Oy — Syyj — SxyjlSxxj. (6) 
The conditions of the theorem ensure that we are dealing with 
a linear hypothesis and the general theorem 8.3.1 may be 
applied, t The #'s of that theorem are here the olj and jSj9 2m in 
all. The null hypothesis (equation (3)), places (m — 1) restrictions 
t Notice that the variables denoted by y here correspond to those denoted 
by x in the general result. 
17 lsii 
258 LEAST SQUARES [8,6 
on these. Hence, in the notation of theorem 8.3.1, corollary 3, 
n — 2/fy, s — 2m, r — m +1, and the degrees of freedom for F 
are as stated. It remains to calculate the minima of the sums of 
squares. 
First, notice that the regression lines (1) have been written in 
a slightly different form from that used in §8.1 (equation 8.1.2). 
The overall mean xtt has been used and not xmj (x in (8.1.2)). 
The reason for this is that we wish (see theorem 2 below) to test 
a hypothesis concerning the fy in the form (1). To recover the 
form of §8.1, and hence to be able to use the results of that 
section, we have only to write (1) as 
= n'j+Pfay-xJ, say. 
To calculate the residual we have to minimize 
j=l 1=1 
We may minimize for each j separately and the result, from 
equation 8.1.8, is 
m 
S = 2 5^ = 2 Syyj — 2 {SxyjISxxj}* (') 
y*=i 1 o 
In order to find the minimum when the hypothesis (3) obtains 
we have to minimize 
S S [y«-*,-flx«-x..)]», (8) 
j = l i = l 
where JS is the common value of the /?,-. Differentiating (8) with 
respect to a;- and /?, and denoting the values at which the 
minimum is attained by af and b9 we have 
ll\yij-ai-b(xii-xj] = 0 (9) 
1 = 1 
m ?ij 
and 2 2 \yti-at-b(xi}-xj](xti-xj = 0. (10) 
3=11=1 
From (9) a, = y j-b(xj-xj, (11) 
so that, inserting these values into (10), 
j=l1=1 
8.6J LINEAR HYPOTHESIS THEORY 259 
m I 771 
whence b = S Sxyjf S 5^ (12) 
since 2 (*#-*.*) = 2 Oty-J\*) = 0. 
i=i t=i 
Replacing a,- in (8) by aj given by (11), the minimum of the 
sum of squares, constrained by (3), is 
= 1 i=l 
= 2 ^yitf "" 26 2 *^xj/i "^" ^ S *^xxy 
i J i 
= S5VW-{S5,^}2/SS'^, from (12). (13) 
i i i 
The reduction in the sum of squares due to different /?'s (allowing 
for the oij and a common /?) is the difference between (13) and 
Sfi = 2 {SxitjlSzxj}~~\2j Sxyj) /S S'xxi* (14) 
and the F-ratio is as given in (4), so proving the theorem. 
Theorem 2. Under the same conditions as theorem 1, except that 
the fi9s are known to be all equal, and their common value fi has 
a uniform prior distribution independent of {ay} and In <f>, a  
significance test at level a of the hypothesis that 
ai = a2 = ••• = am (15) 
is obtained by declaring the data significant if 
[{Syy ~ SXylSXX} ~ [2 Syyj ~ \ElSggyj} /S^iRltf JJ /(/W ~ 1) 
3 j 0 0 
exceeds FJyl9 p2) with vx = m-1, *>2 = Sty-#*—1« 
77*e notation used in (16) & 
5a» =* S C%~~*..) > 
Sxy = S (*#-*..) Cft/-"J\.X 
S^ = S0^-j>..)2- 
(16) 
(17) 
17-2 
260 LEAST SQUARES [8.6 
This is another linear hypothesis. In the notation of theorem 
8.3.1, corollary 3, n = 2/^-, s = ra + 1, r = 2 and the degrees of 
freedom for F are as stated. 
The residual, the unrestricted minimum, is the minimum of (8) 
given in (13). The residual, with the constraint (15), is the 
residual after fitting a common regression line to all the data, 
which, from equation 8.1.8, is 
^yy ^xyl^xx* \*®) 
The reduction in sum of squares due to different a's (allowing 
for a common value for /?) is the difference between (18) and 
13), namely 
*^a = [Syy ~~ ^xyl^xxl ~~ 12 Syyj ~~ \S ^xyj) /S ^xxjh (19) 
0 3 0 
and the F-ratio is as given in (16) 
Test for slopes 
The results of this subsection are a straightforward  
generalization of those of §8.1 from one regression line to several. It was 
pointed out in §8.3 that the topic of §8.1 was a particular case 
of the linear hypothesis: here the general theory is applied to 
give the required results. In the formulation of theorem 1 there 
are m sets of data, to each of which the usual linear homo- 
scedastic regression situation applies. Furthermore, the  
variance, ^, about the regression line is the same for each set: this 
important assumption is similar to that used in the between and 
within analysis of §6.5. In theorem 1 a test is developed of the 
null hypothesis that the m regression lines have the same slope. 
This is often useful when it is desired to test whether the  
relationship between two variables is the same under m different sets of 
conditions, without making assumptions about the equality of 
the means of the variables under the different conditions: in 
particular, without assuming the ordinates, aj9 to be equal. 
Notice, however, that a significant result when this test is applied 
does not necessarily mean that the relationship between x and y 
changes with the conditions. For example, suppose that the 
regression of y on x is not linear but that it is almost linear over 
each of two non-overlapping ranges of x-values. If two sets of 
8.6] LINEAR HYPOTHESIS THEORY 261 
data have x-values in the different ranges then the regression 
coefficients will differ. To avoid such troubles the x-values 
should be about the same in each set: but the test is still  
satisfactory even if they are not; it is the interpretation of it that 
requires attention. 
Test for ordinates 
If the slopes of the regression lines are unequal, the difference 
between the ordinates changes with the value of x. But in the 
special case of equal slopes, the differences in ordinates are the 
same for all x-values. It is then of interest to know if the 
ordinates differ. Theorem 2 enables this equality to be tested. 
Notice that both tests involve the sums of squares and products 
within each set Sxxj9 Sxyj and Syyj (equations (5)), and the latter 
also involves the total sums of squares and products Sxx, Sxy 
and Syy (equations (17)) obtained by pooling all the data. From 
the fundamental relationship expressed in equation 6.5.1, here 
generalized to the case of unequal sample sizes, the total may be 
expressed as the sum of the within and the between: thus, 
Sxx = S Sxxj + 2 n£cmi - xy (20) 
(with a similar expression with y in place of x) and 
S^ = S SaM + ZnAxj-xJ (y.j-y..). (21) 
0 i 
Analysis of variance 
Both tests may be incorporated into an analysis of variance 
table, but since the two reductions, S\ and S% are not  
orthogonal some care must be exercised, as explained in §8.3. 
Table 8.6.1 shows the arrangement. The total sum of squares is 
that about the common line—the values of which we do not wish 
to test. This may be reduced, first by fitting different ordinates 
and then by fitting different slopes. The first breaks the total up 
into Si for the ordinates and SP + S). The second breaks the 
latter up into S2 (equation (7)) and S|. The title ' between slopes 
sum of squares', S% is an abbreviation for the reduction due to 
different slopes (allowing for different a's and /?). The title 
'between ordinates sum of squares', S%, is an abbreviation for 
262 LEAST SQUARES [8.6 
the reduction due to different ordinates (allowing for a common 
line). This is not equal to the reduction due to different ordinates 
(allowing for different /?'s and a); as explained above this is not 
usually of interest. What one usually does is to test for the slopes 
(theorem 1) first. If this is not significant one can test for the 
ordinates, acting as if the slopes were the same. The point being 
that there is a natural ordering of the parameters here: the a's 
naturally occurring first in the reduction, before the /?'s. 
Table 8.6.1 
Between 
ordinates 
Between 
slopes 
Residual 
Ordinates 
residual 
Total 
Sums of 
squares 
s! 
Sp 
s2 
52+5| 
Degrees of 
freedom 
m—l 
m—l 
2(^-2) 
2fy—m—1 
Svy—Szy/Sxx E/fy—2 
■s2 
Mean squares 
sl = S*J(m-l) 
s$ = S$/(m-l) 
s* = S2/X(n3—2) 
= (S2+Sj)/(2^-™- 
— 
-i) 
F 
4P 
4/j* 
— 
— 
— 
(b) Analysis ofco variance 
Theorem 3. Let i?(x, x) denote the residual sum of squares from 
the linear hypothesis {in the notation of §5.5) 
<f(x) = A8 or £(x^ = S a^O,. (22) 
0 
From equation 8.3.28 
i?(x, x) = x'x-x'ACA'A^A'x. (23) 
Then if the linear hypothesis 
<f(x) = A8+/?z or ^(^) = 2^-^+M (24) 
is considered, the residual sum of squares is 
i?(x, x) - i?(x, z)2/i?(z, z). (25) 
{Here /? is an additional parameter and z is a known vector linearly 
independent of the columns of A.) 
The expression to be minimized is 
(x-A6-/te)'(x-Ae-jfe). 
8.6] LINEAR HYPOTHESIS THEORY 263 
Let us minimize it in two stages, first with respect to 8 for 
fixed /?, and then with respect to /?. If y = x-/?z the first stage 
gives 
i?(y,y) = y'y-y'A(A'A)-iA'y 
= (x - /?z)' (x - /?z) - (x - /?z)' A(A'A)-* A'(x - /?z), 
which is a quadratic in /?, 
i?(x, x)-2/?i?(x, z)+l&R(z, z), 
with minimum at b = i?(x, z)/i?(z, z) and minimum value given 
by (25). 
C/ie of analysis of co variance 
This theorem describes how the residual sum of squares is 
reduced by the introduction of a single additional parameter, /?, 
and, as such, gives an alternative test for this single parameter /? 
which is identical with that obtained in corollary 2 to theorem 
8.3.1. The corollary is the most convenient form when this test is 
required: the present form is more suitable in a connexion now 
to be described. 
The term 'analysis of co variance' is a rather loose one. 
'Analysis of variance' most commonly refers to the case of a 
design matrix A having specially simple properties (usually 
orthogonality with respect to sets of parameters) such as was 
discussed in §8.5. The matrix A is truly designed, that is the 
elements are known before experimentation, unlike the elements 
in the design matrix A of the multiple regression situation of 
§8.4 which, depending on the x's (in the notation of that section), 
are not systematic. Analysis of co variance usually refers to the 
model given in (24), where A is truly designed and z is  
(irrelevantly) random. Some, or all, of the 0's are of interest, /? is not. 
The situation arises when one is investigating the dependence 
of x on certain factors, expressed in the 0's, but x is also known 
to be influenced by another quantity z. Thus, in an experiment 
to investigate the effect of type of wool and type of machine on 
the breakage rate in spinning textile yarn, a two-way  
classification as in §8.5 might be used with these two factors. However, 
the breakage rate is obviously affected by the weight per unit 
264 LEAST SQUARES [8.6 
length of yarn, a variable which it is difficult to control. An 
analysis of covariance might therefore be used, based on the 
model in (24), with the weight as the 'z' variable. 
Regression reduction 
The calculations necessary for an analysis of covariance 
follow directly from those for the corresponding analysis of 
variance, in which the z-variable is omitted, by using theorem 3. 
Suppose the d's are divided into two groups, called the first and 
secondhand it is desired to test the hypothesis that those in the 
second group are all zero. Then in the analysis of variance 
SI — 5f2 must be compared with 5f2 (the notation is as in §8.3). 
In the analysis of covariance S\p — S\2/? must be compared with 
Sf2/?> where S\2p is the residual fitting both groups and /?, and 
S\p similarly. But both *S?2/? and S\p are easily obtained from 
S\2 and S\ respectively by use of theorem 3. 
The quantity i?(x, x) is the residual for the x's with /? = 0. 
i?(z, z) is the same expression with z replacing x. i?(x, z) is the 
same expression with every squared x replaced by a product of 
an x with its corresponding z. Consequently, if we calculate the 
analysis of variance for x and add the similar analysis for z and 
for the product of x and z, we shall have all the terms necessary 
for (25). The operation of subtracting i?(x, z)2/i?(z, z) from the 
original residual to obtain the new one will be called the 
regression reduction. Both Sf2/? and S\p can be obtained from 
S\2 and S\ respectively by regression reduction. Table 8.6.2 shows 
how the calculations may be laid out for the two-way  
classification in order to test the interaction. The notation corresponds 
to that of table 8.5.1 with additions to make clear which 
variable is being used: only the sums of squares and products 
are included. Arrows show the order in which the terms are 
calculated. Thus Sf j(x) denotes K2 (xiJu — ximm-xmjm + x...)2 and 
so Sij(x9 z) denotes 
The interaction effects and the residuals are added together to 
give the minima of the sums of squares with zero interaction, 
8.6] LINEAR HYPOTHESIS THEORY 265 
Sjj(x) + S2(x), etc. Both minima suffer a regression reduction: 
thus S} = S2(x) - {S2(x, z)}2/S2(z) 
and 
$jfi + S} = Sh(x) + S2(x) - {$/& z) + S2(x, z)}2/{5| j(z) + 52(z)}. 
Then SjJJ3 is obtained by subtraction. The test for the interaction 
proceeds as before except that the residual S} has one less 
degree of freedom than 52(x) since an additional parameter, /?, 
has been incorporated. The F-statistic is therefore 
{ShA1-1) V-1)} divided by S}I[IJ{K-1) -1]. 
Exactly similar methods enable the two main effects to be 
tested. The sums of squares and products will be added to the 
residual, will undergo a regression reduction, and have S} 
subtracted from them. 
Table 8.6.2 
Interaction 
Residual 
Interaction 
4- residual 
■ 
Sums of squares and products 
f... 
X 
S,2,(x) 
sfAx) 
+ S\x) 
x and z 
SfAx, z) 
S\x, z) 
5|y(x, z) 
+ £2(x, z) 
z 
sfAz) -> 
+ 52(z) 
Covariance 
C2 
t 
Notice that the test of theorem 2 may be regarded as an 
analysis of covariance, where the corresponding analysis of  
variance is the simple between and within analysis of §6.5. To see 
this we note that the linear hypothesis is 
?= a'j+fixv, say, 
and without /? it is ^(ytj | x) = afjt 
There are m samples of sizes nl9 n2,..., nm, with means ol] ; and the 
null hypothesis is that all the a] are equal. We leave the reader 
to verify that the F-test based on (16) results from the analysis 
of covariance. 
266 LEAST SQUARES 18.6 
(c) Polynomial regression 
There are many situations, where the relationship between 
two variables is being considered, in which the regression 
${y\x) is not linear and the results of §8.1 are inadequate 
because of curvature in the regression. Indeed, one of the oldest 
of statistical problems is that of 'fitting a curve to data'; that is, 
to a set of (x, j>)-points. In modern language we consider a set 
x = (xl9 x29 ..., xn) and suppose that for fixed x the random 
variables yl9 y^ ..., yn are independent normal variables with 
#(yi\x) = ctQ+(x,1xi + a2xl + ...+ockx% (26) 
and ^{yt | x) = <f>. (27) 
The assumptions are those of homoscedastic (equation (27)) 
normal regression of polynomial form. If the ai and In <j> have 
independent uniform prior distributions, then the model is in 
the standard linear hypothesis form with 8' = (a0, al9 ..., ak) 
and a design matrix with rth row (1, xi9 x\9 ..., xf). In view of 
the general form of the design matrix the general computational 
methods of §8.4 have to be used; and we perform a multiple 
linear regression on the quantities x9 x29 ..., x**. (In the notation 
of equation 8.4.7, x^ = x\.) It was pointed out in that section 
that the order in which the variables were introduced was 
important because it affected the significance tests that could 
easily be performed. In polynomial regression there is a natural 
order, namely that of increasing powers of x. The parameters 
will then be introduced in the order a0, al9 ..., otk and it is 
possible to test that as+1 = as+2 = ... = ak = 0 for any s. This 
provides a test that, given the polynomial is at most of degree k9 
it is of degree at most s. Usual practice is to take s = k -1; 
that is, to test whether ak = 0. If this is not significant then k is 
reduced by one and one tests whether a^ = 0, finishing up 
with the polynomial of least degree not exceeding k. The tests 
are easily carried out since the reduction in sum of squares due 
to including ocS9 allowing for the notation of 
the computational scheme of §8.4, simply wf (stage (4) of the 
computation). When a test is significant then the polynomial of 
8.6] LINEAR HYPOTHESIS THEORY 267 
that degree must be used, and the estimates of the coefficients at 
have, for fixed 0, a dispersion matrix given by the inverse 
matrix B_1 (in the notation again of §8.4) of the order  
corresponding to the degree of the polynomial. 
A difficulty here is knowing what value of k to use to start. 
Even if the tests of a89 ocs+l9 ...,otk are all insignificant there is 
always the possibility that the inclusion of a term of higher 
degree than k would result in an even better 'fit': or, in modern 
language, would reduce the residual sum of squares appreciably. 
The key to this problem lies in the form of this residual. In any 
of these tests o)2s is compared with the residual, which is the sum 
of squares of deviations about the curve of best * fit' of degree s. 
Therefore, rather loosely, the tests compare whether the  
introduction of x8 results in an appreciable reduction compared with 
what remains, but there is no way of judging whether what 
remains is adequate. The situation is most satisfactorily resolved 
if the value of <j> is known, equal to or2. In this case corollary 1 
to theorem 8.3.1 may be used, in conjunction with the method 
of §5.3, to test the adequacy of the 'fit'. For, if the correct 
degree of polynomial has been used a test of the null hypothesis 
that $ = <t2 should be nonsignificant. Consequently, when 
<fi = a-2, the powers of x may be introduced in ascending order 
until the residual sum of squares is compatible with the known 
amount of variation, <r2. It would be pointless to attempt to 
improve the 'fit' so that the residual was less than the known 
random error. This method may be modified if an independent 
estimate of <j) is available such that c/ji, for appropriate c, has a 
^-distribution. (Such an estimate results from most normal 
distribution situations as we have repeatedly seen.) The x2-test 
from §5.3, used above, may then be replaced by the F-test of 
§6.2, to compare the two estimates of variance: one from the 
residual and the other independent one. 
Orthogonal polynomials 
One unpleasant feature of the polynomial regression, in the 
form of equation (26), is that if the degree of the polynomial is 
changed, say from s to s — 1, then all the estimates of the  
parameters will, in general, be changed. This is because the design is 
268 LEAST SQUARES [8.6 
not orthogonal and just as a test for at will be influenced by how 
many other a's have been allowed for, so will the posterior  
distribution of 0Li be similarly changed. A way out of this difficulty 
is to rewrite equation (26) in the form 
'(KI x) = PoP&ti + fi1P1{x%) + /«**) +... + PMxJ, (28) 
where Ps(x) is a known polynomial of degree s in x, to be 
determined. Equation (26) is the special case Ps(x) = x8. The 
design matrix, A, now has typical row (P0(Xi)9 Pi(jc*), ..., Pk{x^)) 
n 
and the elements of A'A are 2 Pi(xu) Pj(xu)- Suppose that the 
u = l 
polynomials are chosen so that 
n 
£ P,(*J P/xJ = 0 (i* j). (29) 
U = l 
Then A'A is a diagonal matrix and (see equation 8.3.26) the 
design is orthogonal with respect to each of the parameters 
separately. Consequently if the degree of the polynomial (28) 
is increased from k to k +1 none of the fi's in (28) is altered. 
Such polynomials are called orthogonal polynomials. 
It remains to show that orthogonal polynomials exist. P0(x) 
may be put equal to 1. Then P^x) must satisfy (29) with i = 0, 
7=1. If Pi(x) = ax + b, for some a and b, this gives 
n 
Z(axu + b) = 0 
u = l 
which is satisfied, for example, by 
n 
0=1, b = — 2 xjn = x. 
u=l 
Hence, if k = 1, (28) becomes 
*(y<\*) = flo+fli(Xi-x), 
exactly the form used in §8.1. Indeed the term —fix was  
introduced into equation 8.1.2 so that the estimates of a and fi should 
be orthogonal and the posterior distributions of a and fi be  
independent, given (j). Suppose P0(x), ..., Ps_i(x) have been found: 
we show how Ps(x) can be found. Since Ps(x) is a polynomial of 
degree x8 it may be written 
Psix) = a8x* + 0s-iPs_i(x) + as_2Ps_2(x) + ...+ a0P0(x), (30) 
8.6J LINEAR HYPOTHESIS THEORY 269 
n 
and it has to satisfy the equations 2 Ps(xu) Pj(xv) = 0 f°r a^ 
u=l 
j < s. Let as = 1. Since orthogonal polynomials exist up to 
degree s — 1, substituting the expression (30) into these equations, 
we have 
n n n 
w=l u=l u=l 
The coefficient of a* is necessarily non-zero and 
n In 
Clj = 2-1 Xu*AXu) I 2-i *j\xu) • 
/ w=l 
Hence fl; is determined and Ps(x) is found from (30). Hence, 
by the usual inductive argument, the existence of orthogonal 
polynomials is established. Notice that the polynomials depend 
on the values xl9 x2, ..., xn. For the special case where 
xs = x1 + (s-l)h 
for some h, so that the values are equally spaced at distance h 
apart, the polynomials have been tabulated by Fisher and Yates 
(1963) and Pearson and Hartley (1958). 
(d) Weighted least squares 
We have already mentioned the close connexion between the 
linear hypothesis theory and the results on combination of 
observations in §6.6. It is possible to extend significantly all the 
linear hypothesis theory to the case of correlated observations, 
as in §6.6. Suppose all the conditions stated at the beginning of 
§8.3 for a linear hypothesis are retained except that instead of 
the jc's being independent and normal they have a multivariate 
normal distribution with dispersion matrix \<j>, where V is known 
but $ is not. The case already studied is V = I, the unit matrix. 
Then we know (§3.5) that it is possible to find a linear  
transformation of the x's, say y = Tx such that the y's are  
independent and normal with a common variance. Then 
<f (y) = <f (Tx) = Ttf (x) = TA6 
and the /s obey the original conditions on a linear hypothesis 
with design matrix TA instead of A. Consequently the whole 
270 LEAST SQUARES [8.6 
theory can be carried through with A replaced by TA. Notice, 
that unfortunately the orthogonality of any design based on A 
will be thereby destroyed. Since x = T-1y the dispersion matrix 
of the jc's will be T^Cr-1)'^ = V0, so that this establishes the 
relationship between the new and old design matrices. 
The sum of squares to be minimized (with or without  
constraints) is 
(y-TA6)'(y-TAe) - (x-A6)' TT(x-Ae) 
= (x-Aeyv-Xx-Ae). (31) 
Hence the term weighted least squares, since the terms are 
weighted with the elements of V-1 (compare the weighting in 
theorem 6.6.1). 
Suggestions for further reading 
A valuable text on least squares is that of Plackett (1960) and 
a related one on the analysis of variance is by Scheffe (1959). 
The important and interesting topic of the design of  
experiments was founded in its modern form by Fisher (1960). More 
modern works in this field are those of Cochran and Cox (1957) 
and Kempthorne (1952); the latter being the more mathematical. 
An excellent introductory, non-mathematical account is given 
by Cox (1958). 
Sampling methods are discussed in the book by Cochran 
(1963). 
Exercises 
1. The Macaroni penguin lays clutches of two eggs which are markedly 
different in size. The following are the weights, in grammes, of the eggs in 
eleven clutches. Fit a regression line of weight of the larger egg on the 
smaller egg and test whether the slope differs significantly from unity. 
mailer 
egg. 
X 
79 
93 
100 
105 
101 
96 
Larger 
egg, 
y 
133 
143 
164 
171 
165 
159 
Smaller 
egg, 
X 
96 
109 
70 
71 
87 
Larger 
egg, 
y 
162 
170 
127 
133 
148 
EXERCISES 271 
(The following values of the sums, and sums of squares and products, of 
the above readings, after 100 has been subtracted from the values of y9 may 
be used in your answer: Sx = 1007, Ej> = 575, Sx2 = 93,939, Xy2 = 32,647, 
Xxy = 54,681.) 
A further clutch gives weights 75 and 115 g. Test the suspicion that this 
clutch was not that of a Macaroni penguin. (Camb. N.S.) 
2. Obtain a 95 % confidence interval for the regression coefficient of y on x 
from the following data: 
x -2-1012 
y -2-1 -0-9 0 11 1-9 
Obtain a similar interval for the value of y to be expected when x = 4. 
(Camb. N.S.) 
3. Single wool fibres are measured for fibre diameter x\ and their 
breaking load y is then determined. The following table shows pairs of 
measurements x, y made on a sample of 29 fibres. Use these data to 
examine how breaking load depends on diameter, giving posterior standard 
deviations for the parameters of any equation you fit to the observations: 
Fibre 
diameter, 
X 
24 
41 
20 
38 
12 
13 
33 
18 
15 
30 
Breaking 
load, 
y 
3-2 
18-3 
1-2 
10-5 
0-6 
11 
7-8 
1-1 
2-1 
5-8 
Fibre 
diameter, 
X 
28 
38 
40 
22 
42 
10 
11 
32 
41 
42 
Breaking 
load, 
y 
9-5 
7-8 
19-0 
2-0 
9-0 
0-8 
0-7 
5-8 
17-0 
11-8 
Fibre 
diameter, 
X 
38 
35 
14 
19 
31 
29 
28 
21 
24 
Breaking 
load, 
y 
12-0 
15-0 
0-6 
3-1 
12-0 
40 
7-1 
50 
5-2 
(Lond. B.Sc.) 
4. The sample correlation between height of plant and total leaf area was 
obtained for each of six sites. The results were: 
No. of plants 22 61 7 35 49 21 
Correlation +0-612 +0-703 +0-421 +0-688 +0-650 +0-592 
Find an approximate 95 % confidence interval for the correlation. 
5. Three groups of children were each given two psychological tests. The 
numbers of children and the sample correlations between the two test 
scores in each group were as follows: 
No. of children 51 42 67 
Correlation +0-532 +0-477 +0-581 
Is there any evidence that the association between the two tests differs in 
the three groups ? 
272 LEAST SQUARES 
6. The mean yield &(y) of a chemical process is known to be a quadratic 
function of the temperature T. Observations of the yield are made with 
an error having a normal distribution of zero mean and constant variance 
a2. An experiment is performed consisting of m observations at T = T0 
and n observations at each of T = T0-l and T0+ 1. Writing the quadratic 
relation in the form 
£(y) = ot + /3(T-T0) + y(T-T0)\ 
show that the least squares estimates of /?, y are given by 
y?=iCPi-7-i), 
7 = iCPi-2jP0+;P-i), 
where y_l9 y0, yx are the averages of the observations at T0— 1, T0, T0+ 1, 
respectively. 
Show that, if y < 0, £(y) is maximum when T = Tm = T0-lfi/y and that 
when cr2 is small, the variance of the estimate T0 — ift/y of 7m is approxi- 
8y2 L« 72 vi ml J 
For a fixed large number of observations N = 2n + m, show that if 
Tm— T0 is known to be large the choice of m which minimizes the latter 
variance is approximately iN. (Wales Dip.) 
7. In an agricultural experiment, k treatments are arranged in a A: by A: 
Latin square and y^ is the yield from the plot in the ith row andyth column, 
having treatment T{ij)\ x„ is a measure of the fertility of this plot obtained 
in the previous year. It is assumed that 
ya = /i + n + Cj + t{ij)+fan + yx% + %, 
where the {e^} are normally and independently distributed variables with 
mean zero and variance a2. Show how you would test whether y = 0. 
[A Latin square is an arrangement such that each treatment occurs once 
in each row and once in each column.] (Camb. Dip.) 
8. In an experiment there are a control C and t treatments Tl9..., Tt. The 
treatments are characterized by parameters 6l9 ...,6t to be thought of as 
measuring the difference between the treatments and the control. For 
each observation an individual is tested under two conditions and the 
difference in response taken as the observation for analysis. Observations 
are subject to uncorrelated normal random errors of zero mean and 
constant variance cr2. 
There are k observations comparing 7\ with C, i.e. having expectation 
#x; k observations comparing T2 with C, i.e. having expectation 02, and so 
on for r3, ..., Tt. Then there are / observations comparing T2 with 7\ and 
having expectation 02 — Ql9 and so on for every possible comparison of 
treatments, i.e. there are for every i > j, I observations comparing T{ with 
Tj and having expectation 0+ — di% 
EXERCISES 
273 
Obtain the least squares estimate 6t and prove that 
(Lond. M.Sc.) 
9. The random variables Yll9 ..., Ynn are independently normally  
distributed with constant unknown variance cr2 and with 
*(r„) = {i-Hn+iyia+V-Hn+iyifi + Xar, 
where a, /?, y are unknown parameters and xu are given constants. Set 
out the calculations for (i) obtaining confidence intervals for y, (ii) testing 
the hypothesis a = j3 = 0. (Lond. M.Sc.) 
10. In order to estimate two parameters 0 and <j) it is possible to make 
observations of three types: (i) the first type have expectation 0 + <fi; 
(ii) the second type have expectation 20-\-<j)\ (iii) the third type have 
expectation 0 + 2^. All observations are subject to uncorrelated normal 
errors of mean zero and constant variance cr2. 
If n observations of type 1, m observations of type 2 and m observations 
of type 3 are made, obtain the least squares estimates of 0 and <f> and prove 
that, given the data, 
(Lond. B.Sc.) 
11. The set of normal variables (yl9 y2, ..., yn) has 
*(y*) = 0xi9 V(yi9 yk) = vjk9 ®2(y3) = vjj9 
the quantities xj9 vjk being known. Obtain the posterior distribution of 6 
under the usual assumptions. 
Suppose n 
yt = 0 + Vi-Vi-i, 
where the ijj are independent normal random variables with mean zero 
and variance cr2. By solving the system of equations 
n 
2 !>,*&. = 1 
k=l 
in £*, for this case, obtain the posterior distribution of 0. (Camb. Dip.) 
12. The following experiment, due to Ogilvie et al.9 was done to determine 
if there is a relation between a person's ability to perceive detail at low 
levels of brightness, and his absolute sensitivity to light. For a given size 
of target, which is measured on the visual acuity scale (1/angle subtended 
at eye) and denoted xl9 a threshold brightness (denoted y) for seeing this 
target was determined [xx was restricted to be either 0-033 or 0-057]. In 
addition, the absolute light threshold (denoted x2) was determined for 
each subject. 
It is known that y depends on xl9 and this dependence can be taken to 
be linear. Use multiple regression to test whether y is significantly 
dependent on x2 (for the purposes of the test, assume that any such 
18 lsii 
274 
LEAST SQUARES 
dependence would be linear), i.e. use the model in which the expected 
value of y is n , _ x n , 
a + Pi\X\ - *i) + Aa(*2 - Xzh 
where xx and x% denote the averages of xx and x2, respectively. 
Examine graphically (without carrying out a test of significance), 
whether the dependence of y on x2 is in fact linear. 
Subject 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
04) 
3-73 
3-75 
3-86 
3-88 
3-89 
3-90 
3-92 
3-93 
3-98 
3-99 
4-04 
4-07 
4-07 
(» 
4-72 
4-60 
4-86 
4-74 
4-42 
4-46 
4-93 
4-96 
4-73 
4-63 
5-20 
5-31 
506 
(C) 
5-17 
4-80 
5-63 
5-35 
4-73 
4-92 
5-15 
5-42 
5-14 
5-92 
5-51 
5-89 
5-44 
Subject 
14 
15 
16 
17 
18 
19 
20 
21 
22 
23 
24 
25 
(A) 
4-09 
4-10 
4-17 
4-20 
4-22 
4-23 
4-24 
4-29 
4-29 
4-31 
4-32 
4-42 
(B) 
5-21 
4-97 
4-82 
5-03 
5-44 
4-64 
4-81 
4-58 
4-98 
4-94 
4-90 
5-10 
(C) 
6-74 
5-41 
5-22 
5-49 
6-71 
5-14 
5-37 
5-11 
5-40 
5-71 
5-41 
5-51 
Column (A) gives values of x2. Columns (B) and (C) give values of y for 
xx = 0-033 and xx = 0-057, respectively. (Lond. Psychol.) 
13. In an experiment into the laboratory germination of Hypericum 
perforatum (by E. W. Tisdale et al, Ecology, 40, 54, 1959), 9 batches of 
seeds were buried in siltloam soil at depths of \, 1 and 3 in. (3 batches at 
each depth). One batch at each depth was left buried for 1 year, one for 
2 years and one for 3 years. At the end of these times the seeds were 
recovered and tested for germination. The experiment was repeated three 
times. 
The percentages germinating were as follows (each figure being the 
average of 3 replications): 
No. of 
years 
buried 
1 
2 
3 
i 
20-6 
30-6 
9-6 
Depth (in.) 
1 
27-3 
42-0 
45-0 
N 
3 
25-2 
62-0 
52-0 
Test for differences in percentages germinating between the different 
depths and the different lengths of time for which the seeds were buried. 
Test also for quadratic dependence of percentage germinating on the 
length of time buried. 
State what additional information could have been obtained had the 
individual percentages for the replicates been available, and comment on 
the statistical advantage, if any, which might have been gained from 
burying seeds at a depth of If in. rather than of 1 in. (Leic. Stat.) 
EXERCISES 
275 
14. In an experiment on the effect of radiation on the synthesis of  
deoxyribonucleic acid (DNA) four rats were partially hepatectomized and 
irradiated. The DNA contents of samples of cells were determined by three 
different methods; the table gives the mean contents per cell for the four 
rats by each of the three methods. 
Rat 
Method 
1 
2 
3 
Total 
1 
217 
206 
231 
654 
2 
283 
269 
298 
850 
3 
239 
226 
256 
721 
N 
4 
262 
274 
252 
788 
Total 
1001 
975 
1037 
Investigate whether the rats differed in DNA content, and whether 
values obtained by the three methods could be treated as comparable. 
(Camb. N.S.) 
15. In a poison assay doses xl9 ..., xn of poison are given to groups of 
k mice and the numbers dying in each group are rl9 ..., rn\ r{ may be 
assumed to have a binomial distribution with parameter pi9 where 
f(pd = <*+fi(xt-x)9 
where f(u) is a known function, x is the mean of and a and fi are 
constants. Set up the maximum likelihood equations for the estimation of 
a and /?, and show that if a0, fi0 are approximate solutions of these  
equations the process of obtaining better solutions is equivalent to a certain 
least squares estimation problem. (Camb. N.S.) 
16. In an experiment on the effect of cultivation on the absorption by bean 
plants of radioactive contamination deposited on the soil surface, three 
systems of cultivation were used. Twelve plots were arranged in four 
blocks, each block containing one plot of each system. The values of the 
radioactivity per unit weight of pod for the twelve plots, and the totals for 
blocks and systems were: 
Block 
System of 
cultivation 
A 
B 
C 
Total 
i 
1 
14 
8 
9 
31 
2 
10 
7 
5 
22 
3 
19 
12 
9 
40 
^ 
4 
17 
12 
10 
39 
Total 
60 
39 
33 
The sums of squares of the plot values, the block totals and the system 
totals are respectively 1634, 4566 and 6210. Determine what evidences 
there are that the system used affects the amount of radioactivity taken up 
by the plant, and the arrangement of the plots in blocks increases the  
precision of the experiment. (Camb. N.S.) 
17. An agricultural experiment to compare five different strains of cereal, 
A, B, C, D and E, was carried out using five randomized blocks, each 
containing five plots. The resulting yields per plot (lb) are shown below. 
Analyse these data and discuss what conclusions regarding differences 
in yields can be legitimately drawn from them. 
18-2 
276 
LEAST SQUARES 
Strain 
E 
46-5 
41-3 
48-8 
55-7 
45-3 
(Lond. B.Sc.) 
18. In an agricultural trial a variety of wheat was grown for 3 years in 
succession on 4 plots in each of 3 areas, and the yield per plot was measured 
each year. Different randomly selected plots in each area were used each 
year. The yields per plot (kg) are given in the table below: 
Area 1 
14-20 
14-44 
17-46 
16-80 
14-98 
15-90 
15-80 
17-58 
14-14 
12-14 
11-86 
15-24 
Area 2 
13-60 
16-28 
17-22 
15-40 
1702 
14-36 
13-06 
13-10 
1200 
14-74 
14-50 
12-86 
Area 3 
16-96 
16-10 
16-62 
14-30 
12-16 
14-34 
16-84 
13-46 
17-88 
18-98 
16-22 
15-12 
Analyse these data and report on your conclusions. (Lond. B.Sc.) 
19. The table shows three sets of observed values y{j of a variable y 
corresponding to fixed values x, of a related variable x (j = 1, 2, ..., 6; 
i = 1, 2, 3). The standard linear regression model may be assumed, viz. 
that the y^ belong independently to normal distributions with means 
di + fiiX, and common variance cr2. Show that al9 a2, az may reasonably be 
taken to have a common value, a, and calculate 98 % confidence limits 
for a. 
xf 
2*6 
3-6 
4-4 
5-3 
5-7 
6*0 
yu 
16-5 
21-5 
25-3 
27-7 
25*8 
29-0 
y%i 
120 
15-2 
12-4 
14-4 
17-1 
19-1 
y* 
28-2 
33-7 
40-6 
43-2 
46-9 
47-8 
(Manch. Dip.) 
20. A random variable y is normally distributed with mean a 4- bx and 
known variance cr2. At each of n values of x a value of y is observed giving 
« pairs of observations, (xl9 >>i), fe, j>2), ..., (*«, >>«). Obtain the least 
squares estimates of a and b and find the posterior standard deviation of b. 
Suppose that observations are made on two different days, nx on the 
first day and n2 on the second. There are practical reasons for supposing 
that conditions are not constant from day to day but vary in such a way 
Block 
1 
2 
3 
4 
5 
r 
A 
36-5 
48-4 
50-9 
60-9 
46-3 
B 
47-1 
43-0 
52-4 
65-5 
50-0 
C 
53-5 
580 
660 
67-1 
58-0 
D 
37-1 
40-9 
47-9 
56-1 
44-5 
Year 1 
Year 2 
Year 3 
EXERCISES 
277 
as to affect a but not b. Show that the least squares estimate of the 
common b derived from the two samples is 
where b± and £2 are the least squares estimates of b from the first and 
second samples, respectively, and w± and w2 are the corresponding sums 
of squares of the x's about their means. Obtain the posterior standard 
deviation of b. 
Hence show that this posterior standard deviation is not less than that 
which would have been obtained if the combined samples had been 
treated as if a had remained constant. Comment briefly on the practical 
implications of this result giving special consideration to the case when the 
standard deviations are equal. (Wales Dip.) 
21. The (n x 1) vector random variable x is such that &(x) = A9 and 
^2(x) = <r2V, where A is a known (nxp) matrix of rank p < n, 9 is a 
(p x 1) vector of parameters and V is a non-singular matrix. 
Show that the method of least squares gives 
§ = (A,V-1A)-1A,V-1x, 
as an estimate of 9. 
Let u be a (n x 1) vector each of whose elements is unity and let I be the 
unit matrix. If V = (1 — p)I+puu' and A = u, so that 9 is a scalar, show 
that V"1 = 1/(1 -/>) -puu'/U -/>) [1 + (/i - l)p}. 
Hence show that, in this case, 
§ = u'x/«, 
is the least squares estimate of 9 and determine the variance of this 
estimate. (Leic. Gen.) 
22. The distance between the shoulders of the larger left valve and the 
lengths of specimens oiBairdia oklahomaensis from two different geological 
levels (from R. H. Shaver, /. Paleontology, 34, 656, 1950) are given in the 
following table: 
Level 1 Level 2 
l 
Distance 
between 
shoulders 
0*) 
631 
606 
682 
480 
606 
556 
429 
454 
* 
Length 
<A0 
1167 
1222 
1278 
1045 
1151 
1172 
970 
1166 
t 
Distance 
between 
shoulders 
O) 
682 
631 
631 
707 
631 
682 
707 
656 
682 
656 
672 
■ ■' \ 
Length 
00 
1257 
1227 
1237 
1368 
1227 
1262 
1313 
1283 
1298 
1283 
1278 
18-3 
278 
LEAST SQUARES 
Test for any differences between the two levels in respect of the  
regression of distance on length, and obtain 95 % confidence intervals for each 
of the regression coefficients. (Leic. Stat.) 
23. F. C. Steward and J. A. Harrison (Ann. Bot., N.S., 3,1939) considered 
an experiment on the absorption and accumulation of salts by living plant 
cells. Their data pertain to the rate of uptake of rubidium (Rb) and 
bromide (Br) ions by potato slices after immersion in a solution of 
rubidium bromide for various numbers of hours. The uptake was measured 
in the number of milligramme equivalents per 1000 g of water in the 
potato tissue. 
Mg. equivalents per 1000 g 
Time of 
• • 
i ty\ m f*t*o i /"vn 
llllillCl MU11 
(hours) 
21-7 
46-0 
67-0 
90-2 
95-5 
320-4 
of water in the tissue 
Rb Br 
7-2 0-7 
11-4 6-4 
14-2 9-9 
19-1 12-8 
20-0 15-8 
71-9 45-6 
Total 
On the assumption that the rates of uptake of both kinds of ions are 
linear with respect to time, determine the two regression equations giving 
the rates of change, and test the hypothesis that the two rates are, in fact, 
equal. Also, determine the mean uptake of the Rb and Br ions and test 
their equality. 
Give a diagrammatic representation of the data and of the two 
regression lines. (Leic. Gen.) 
24. An experimenter takes measurements, y, of a property of a liquid 
while it is being heated. He takes the measurements at minute intervals 
for 15 min beginning 1 min after starting the heating apparatus. He 
repeats the experiment with a different liquid and obtains 15  
measurements at minute intervals as before. The 30 measurements of y are given 
in the table below: 
Time 
(min) 
1 
2 
3 
4 
5 
6 
7 
8 
1st 
experiment, 
y 
1-51 
3-80 
4-39 
1-97 
3-34 
3-39 
4-86 
3-81 
2nd 
experiment, 
y 
1-35 
1-86 
3-16 
0-63 
0-69 
300 
4-53 
2-38 
Time 
(min) 
9 
10 
11 
12 
13 
14 
15 
1st 
experiment, 
y 
4-53 
3-00 
3-83 
4-80 
2-54 
5-24 
6-11 
2nd 
experiment, 
y 
3-84 
4-99 
4-62 
4-35 
5-93 
7-16 
5-82 
Before carrying out the experiments he had anticipated from theoretical 
considerations that: 
(1) the slope of the regression lines of y on time would be greater than 
zero for both liquids; 
EXERCISES 
279 
(2) the slopes of the two regression lines would differ from one another; 
(3) the difference between the slopes would be 0-25. 
Examine whether the data are consistent with these hypotheses. 
The experimenter wishes to estimate the values of y at the start of the 
experiments (time 0) for both liquids and also the time at which the values 
of y are the same for both liquids. 
Derive these estimates attaching standard deviations to the first two. 
(Lond. B.Sc.) 
25. The nx\ vector of observations Y has expectation a9, where a is an 
nxp matrix of known constants of rank p < n, and 9 is a p x 1 vector of 
unknown parameters. The components of Y are uncorrelated and have 
equal variance cr2. The residual sum of squares is denoted Y'rY. Prove 
that 
r2 = r, ra = 0. 
Suppose now that Y has expectation 
a9 + b& 
where b is an n x 1 column linearly independent of the columns of a and 
<fi is an unknown scalar. Prove that the least squares estimate of 0 is 
brY 
brb 
and has variance (^/(b'rb). Show how to estimate cr2. (Lond. M.Sc.) 
26. In each of two laboratories the quadratic regression of a variable y on 
a variable x is estimated from eleven observations of y, one at each of the 
values 0,1, 2,..., 10 for x. The fitted regressions are 
y = 2n+011x + 012x2 in the first laboratory, 
y = 2-52 + 0-69x+0*13x2 in the second laboratory. 
The residual mean square in the first laboratory is 0-24 and in the second 
laboratory, 0*29. Assuming that the regression of y on x is, in fact, 
quadratic, and that variation of y about the regression curve is normal, 
estimate the probability that, if one further measurement is carried out in 
each laboratory with x = 5, the value of y observed in the first laboratory 
will be less than the value observed in the second laboratory. 
(Lond. Dip.) 
27. To investigate the effect of two quantitative factors A, B on the yield 
of an industrial process an experiment consisting of 13 independent runs 
was performed using the following pairs of levels (xl9 x2) the last being 
used five times: 
*! -1 -1 1 1 -42 42 0 0 0 
x2 -1 1-1 1 0 0 -V2 42 0 
It is assumed that the effect of level xx of A and level x2 of B can be  
represented by a second-degree polynomial 
<fi(xl9 x2) = Po+fi1x1+fi2x2 + P^xi+fi12x1x2+fi22x 
2 
2-j 
280 LEAST SQUARES 
and that variations in yield from run to run are distributed independently 
with variance cr2. Show that the least squares estimates of the coefficients 
in <$> are given by 
10A = 2Stt-(E^+Ztt*W. 
8A = E**„ (/= 1,2), 
I6OA1 = 23Sjv*?, + 3Sjvj&-16Stt, 
I6OA2 = 3S^x^ + 23S^.xL—16S^, 
3X2h 
where yj9 xlh x2i are respectively the yield and the levels of A, B for run /. 
Show also that the least squares estimate of <fi(xl9 x2) for any given  
combination of levels (xl9 x%) has variance 
^2(i—Ap' + i^V4), where p2 = *J + *J. 
Indicate briefly how you would use the results of this experiment in 
recommending values of xx and x2 for a future experiment. (Camb. Dip.) 
28. A surveyor measures angles whose true values are A and 11 and then 
makes a determination of the angle A + /^. His measurements are x, y and z, 
and have no bias, but are subject to independent random errors of zero 
mean and variance cr2. Apply the method of least squares to derive 
estimates A, /i, and show that the variance of A is fcr2. 
Suppose now that all measurements are possibly subject to an unknown 
constant bias /?. Show how to estimate fi from x, y and z, and, assuming cr2 
known and the errors normally distributed, give a significance test of the 
hypothesis fi = 0. (Lond. B.Sc.) 
29. To determine a law of cooling, observations of temperature T are 
made at seven equally spaced instants t, which are taken to be 
t = -3, -2, -1,0,1,2,3. 
It is supposed that the temperature readings T are equal to a quadratic 
function f(i) of t, plus a random observation error which is normally 
distributed with zero mean and known variance cr2, the errors of all 
readings being independent. Taking the form 
f(t) = a + bt+c(t2-4)t 
find the estimates of a, b and c which minimize the sum of squares of the 
differences T—f(i). Prove that the posterior variances of a, b and c are 
(72 a* 0* 
T* 28 84' 
respectively. 
Indicate the corresponding results when/(Y) is expressed in the alternative 
f°rm At) = a' + b't+c't\ (Camb. N.S.) 
EXERCISES 
281 
30. The random variables Yl9..., Yn have covariance matrix or2v, where v 
is a known positive definite n x n matrix, 
*(¥<) = 0xi9 
where 
*^i> • • • t *^w arc known constants and 0 is an unknown (scalar) 
parameter. Derive from first principles the least squares estimate, 6, of 6 
and obtain the variance of 6. Compare this variance with that of the 
'ordinary' least squares estimate Sxt- Yi/2iXif when v is the matrix with 1 
in the diagonal elements and p in all off-diagonal elements. 
(Lond. M.Sc). 
APPENDIX 
Twonsided tests for the x2-distribution 
5% 1% 57% 
r \ t \ ( ^ 
V 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
22 
23 
24 
25 
26 
27 
28 
29 
30 
31 
32 
33 
34 
35 
36 
37 
38 
39 
40 
x2 
X2 
0-0231593 7-8168 
0-084727 
0-29624 
0-60700 
0-98923 
1 -4250 
1-9026 
2-4139 
2-9532 
3-5162 
4-0995 
4-7005 
5-3171 
5-9477 
6-5908 
7-2453 
7-9100 
8-5842 
9-2670 
9-9579 
10-656 
11-361 
12-073 
12-791 
13-514 
14-243 
14-977 
15-716 
16-459 
17-206 
17-958 
18-713 
19-472 
20-235 
21-001 
21-771 
22-543 
23-319 
24-097 
24-879 
9-5303 
11-191 
12-802 
14-369 
15-897 
17-392 
18-860 
20-305 
21-729 
23-135 
24-525 
25-900 
27-263 
28-614 
29-955 
31-285 
32-607 
33-921 
35-227 
36-525 
37-818 
39-103 
40-383 
41-658 
42-927 
44-192 
45-451 
46-707 
47-958 
49-205 
50-448 
51-688 
52-924 
54-157 
55-386 
56-613 
57-836 
59-057 
60-275 
x2 
0-0313422 
0-017469 
0-101048 
0-26396 
0-49623 
0-78565 
1-1221 
1 -4978 
1-9069 
2-3444 
2-8069 
3-2912 
3-7949 
4-3161 
4-8530 
5-4041 
5-9683 
6-5444 
7-1316 
7-7289 
8-3358 
8-9515 
9-5755 
10-2073 
10-846 
11-492 
12-145 
12-803 
13-468 
14-138 
14-813 
15-494 
16-179 
16-869 
17-563 
18-261 
18-964 
19-670 
20-380 
21-094 
x2 
11-345 
13-285 
15-127 
16-901 
18-621 
20-296 
21-931 
23-533 
25-106 
26-653 
28-178 
29-683 
31-170 
32-641 
34-097 
35-540 
36-971 
38-390 
39-798 
41-197 
42-586 
43-967 
45-340 
46-706 
48-064 
49-416 
50-761 
52-100 
53-434 
54-762 
56-085 
57-403 
58-716 
60-025 
61-330 
62-630 
63-927 
65-219 
66-508 
67-793 
K2 
0-0M4026 
0-0218055 
0-022097 
0-083097 
0-19336 
0-35203 
0-55491 
0-79722 
1-0745 
1-3827 
1-7185 
2-0791 
2-4620 
2-8651 
3-2865 
3-7248 
4-1786 
4-6468 
5-1281 
5-6218 
6-1269 
6-6428 
7-1688 
7-7043 
8-2487 
8-8016 
9-3625 
9-9310 
10-507 
11-089 
11-678 
12-274 
12-875 
13-482 
14-094 
14-712 
15-335 
15-963 
16-595 
17-232 
x2 
16-266 
18-468 
20-524 
22-486 
24-378 
26-214 
28004 
29-754 
31-469 
33-154 
34-812 
36-446 
38-058 
39-650 
41-225 
42-783 
44-325 
45-854 
47-370 
48-874 
50-366 
51-848 
53-320 
54-782 
56-236 
57-682 
59-119 
60-549 
61-972 
63-388 
64-798 
66-202 
67-599 
68-991 
70-378 
71-759 
73-136 
74-507 
75-874 
77-236 
APPENDIX 
283 
Two-sided tests for the x2-distribution (cont.) 
5 % 1 % 0-1 % 
r \ i \ t \ 
V 
41 
42 
43 
44 
45 
46 
47 
48 
49 
50 
51 
52 
53 
54 
55 
56 
57 
58 
59 
60 
61 
62 
63 
64 
65 
66 
67 
68 
69 
70 
71 
72 
73 
74 
75 
76 
77 
78 
79 
80 
81 
82 
83 
84 
85 
i 
25-663 
26-449 
27-238 
28-029 
28-823 
29-619 
30-417 
31-218 
32-020 
32-824 
33-630 
34-439 
35-248 
36060 
36-873 
37-689 
38-505 
39-323 
40-143 
40-965 
41-787 
42-612 
43-437 
44-264 
45-092 
45-922 
46-753 
47-585 
48-418 
49-253 
50-089 
50-926 
51-764 
52-603 
53-443 
54-284 
55-126 
55-969 
56-814 
57-659 
58-505 
59-352 
60-200 
61-049 
61-899 
x2 
61-490 
62-703 
63-913 
65-121 
66-327 
67-530 
68-731 
69-931 
71-128 
72-323 
73-516 
74-708 
75-897 
77-085 
78-271 
79-456 
80-639 
81-820 
83-000 
84-178 
85-355 
86-531 
87-705 
88-878 
90-049 
91-219 
92-388 
93-555 
94-722 
95-887 
97-051 
98-214 
99-376 
100-536 
101-696 
102-85 
104-01 
105-17 
106-32 
107-48 
108-63 
109-79 
110-94 
112-09 
113-24 
x2 
21-811 
22-531 
23-255 
23-982 
24-712 
25-445 
26-181 
26-919 
27-660 
28-404 
29-150 
29-898 
30-649 
31-403 
32-158 
32-916 
33-675 
34-437 
35-201 
35-967 
36-735 
37-504 
38-276 
39-049 
39-824 
40-600 
41-379 
42-159 
42-940 
43-723 
44-508 
45-294 
46-081 
46-870 
47-661 
48-452 
49-245 
50-040 
50-836 
51-633 
52-431 
53-230 
54-031 
54-833 
55-636 
x2 
69-075 
70-354 
71-629 
72-901 
74-170 
75-437 
76-700 
77-961 
79-220 
80-475 
81-729 
82-979 
84-228 
85-474 
86-718 
87-960 
89-200 
90-437 
91-673 
92-907 
94-139 
95-369 
96-597 
97-823 
99-048 
100-271 
101-492 
102-71 
103-93 
105-15 
106-36 
107-58 
108-79 
11000 
111-21 
112-42 
113-62 
114-83 
116-03 
117-23 
118-44 
119-64 
120-84 
122-03 
123-23 
x2 
17-873 
18-518 
19-168 
19-821 
20-478 
21-139 
21-803 
22-471 
23-142 
23-816 
24-494 
25-174 
25-858 
26-544 
27-233 
27-925 
28-620 
29-317 
30-016 
30-719 
31-423 
32-130 
32-839 
33-551 
34-264 
34-980 
35-698 
36-418 
37-140 
37-864 
38-590 
39-317 
40-047 
40-778 
41-511 
42-246 
42-983 
43-721 
44-461 
45-203 
45-946 
46-690 
47-436 
48-184 
48-933 
X2 
78-595 
79-948 
81-298 
82-645 
83-987 
85-326 
86-661 
87-992 
89-321 
90-646 
91-968 
93-287 
94-603 
95-916 
97-227 
98-535 
99-840 
101-142 
102-442 
103-74 
105-03 
106-33 
107-62 
108-91 
110-19 
111-48 
112-76 
114-04 
115-32 
116-59 
117-87 
119-14 
120-41 
121-68 
122-94 
124-21 
125-47 
126-73 
127-99 
129-25 
130-51 
131-76 
133-02 
134-27 
135-52 
284 
APPENDIX 
Two-sided tests for the x2-distribution (cont.) 
5% 1% 
t \ i \ 
V 
86 
87 
88 
89 
90 
91 
92 
93 
94 
95 
96 
97 
98 
99 
100 
_*2 
62-750 
63-601 
64-454 
65-307 
66-161 
67-016 
67-871 
68-728 
69-585 
70-443 
71-302 
72-161 
73-021 
73-882 
74-744 
x2 
114-39 
115-54 
116-68 
117-83 
118-98 
120-12 
121-26 
122-41 
123-55 
124-69 
125-83 
126-97 
128-11 
129-25 
130-39 
x2 
56-440 
57-245 
58-052 
58-859 
59-668 
60-477 
61-288 
62100 
62-912 
63-726 
64-540 
65-356 
66-172 
66-990 
67-808 
x2 
124-43 
125-62 
126-81 
128-01 
129-20 
130-39 
131-58 
132-76 
133-95 
135-14 
136-32 
137-51 
138-69 
139-87 
141-05 
The values of x2 and x2 satisfy the equations 
Cx* C°° 
Jo J x* 
and (x2)lve-i? = &)*ve-& 
for a = 0-05, 001 and 0-001. 
Taken, with permission, from Lindley, D. V., East, D. A. and Hamilton, 
P. A. 'Tables for making inferences about the variance of a normal 
distribution.' Biometrika, 47, 433-8. 
0-1 % 
x2 
49-684 
50-436 
51-189 
51-944 
52-700 
53-457 
54-216 
54-976 
55-738 
56-500 
57-264 
58-029 
58-795 
59-562 
60-331 
x2 
136-77 
138-02 
139-26 
140-51 
141-75 
142-99 
144-23 
145-47 
146-71 
147-95 
149-19 
150-42 
151-66 
152-89 
154-12 
285 
BIBLIOGRAPHY 
Alexander, H. W. (1961). Elements of Mathematical Statistics. New 
York: John Wiley and Sons Inc. 
Birnbaum, Z. W. (1962). Introduction to Probability and Mathematical 
Statistics. New York: Harper and Bros. 
Blackwell, D. and Girshick, M. A. (1954). Theory of Games and 
Statistical Decisions. New York: John Wiley and Sons Inc. 
Brunk, H. D. (1960). An Introduction to Mathematical Statistics. Boston: 
Ginn and Co. Ltd. 
Chernoff, H. and Moses, L. E. (1959). Elementary Decision Theory. New 
York: John Wiley and Sons Inc. 
Cochran, W. G. (1963). Sampling Techniques. New York: John Wiley 
and Sons Inc. 
Cochran, W. G. and Cox, G. M. (1957). Experimental Designs. New 
York: John Wiley and Sons Inc. 
Cox, D. R. (1958). Planning of Experiments. New York: John Wiley and 
Sons Inc. 
Cramer, H. (1946). Mathematical Methods of Statistics. Princeton 
University Press. 
David, F. N. (1954). Tables of the Ordinates and Probability Integral of the 
Distribution of the Correlation Coefficient in Small Samples. Cambridge 
University Press. 
Da vies, O. L. (editor) (1957). Statistical Methods in Research and  
Production, 3rd edition. Edinburgh: Oliver and Boyd. 
Davis, H. T. (1933). Tables of the Higher Mathematical Functions, volume i. 
Bloomington: Principia Press. 
Fisher, R. A. (1958). Statistical Methods for Research Workers, 13th 
edition. Edinburgh: Oliver and Boyd. 
Fisher, R. A. (1959). Statistical Methods and Scientific Inference, 2nd 
edition. Edinburgh: Oliver and Boyd. 
Fisher, R. A. (1960). The Design of Experiments, 7th edition. Edinburgh: 
Oliver and Boyd. 
Fisher, R. A. and Yates, F. (1963). Statistical Tables for Biological, 
Agricultural and Medical Research, 6th edition. Edinburgh: Oliver and 
Boyd. 
Fraser, D. A. S. (1958). Statistics: an Introduction. New York: John 
Wiley and Sons Inc. 
Greenwood, J. A. and Hartley, H. O. (1962). Guide to Tables in  
Mathematical Statistics. Princeton University Press. 
Hoel, P. G. (1960). Introduction to Mathematical Statistics, 2nd edition. 
New York: John Wiley and Sons Inc. 
Hogg, R. V. and Craig, A. T. (1959). Introduction to Mathematical 
Statistics. New York: Macmillan. 
286 
BIBLIOGRAPHY 
Jeffreys, H. (1961). Theory of Probability, 3rd edition. Oxford: 
Clarendon Press. 
Kempthorne, O. (1952). The Design and Analysis of Experiments. New 
York: John Wiley and Sons Inc. 
Kendall, M. G. and Stuart, A. (1958, 1961). The Advanced Theory of 
Statistics. Two volumes. London: Griffin and Co. 
Lindley, D. V. and Miller, J. C. P. (1961). Cambridge Elementary 
Statistical Tables. Cambridge University Press. 
Pearson, E. S. and Hartley, H. O. (1958). Biometrika Tables for 
Statisticians, volume i. Cambridge University Press. 
Plackett, R. L. (1960). Principles of Regression Analysis. Oxford: 
Clarendon Press. 
Raiffa, H. and Schlaifer, R. (1961). Applied Statistical Decision Theory. 
Boston: Harvard University Graduate School of Business  
Administration. 
Scheffe, H. (1959). The Analysis of Variance. New York: John Wiley and 
Sons Inc. 
Schlaifer, R. (1959). Probability and Statistics for Business Decisions. 
New York: McGraw-Hill Book Co. Inc. 
Tucker, H. G. (1962). An Introduction to Probability and Mathematical 
Statistics. New York: Academic Press. 
Weiss, L. (1961). Statistical Decision Theory. New York: McGraw-Hill 
Book Co. Inc. 
Wilks, S. S. (1962). Mathematical Statistics. New York: John Wiley and 
Sons Inc. 
287 
SUBJECT INDEX 
absolutely continuous, 56 
additivity (of effects), 252 
alternative hypothesis, 59 
analysis of covariance, 262-5 
analysis of variance, 232-6, 245 
between and within samples, 104-12, 
119-21,228,250,253,265 
orthogonality, 234-6, 245 
regression, 210, 261-2 
tables, 105-7, 110, 210, 233-4, 247, 
256, 262, 265 
two-way classification, 246-57 
ancillary statistic, 49, 57-8 
contingency tables, 181-2 
regression, 203, 205 
angle (Behrens's distribution), 91 
Bayesian, 15 
Bayes's theorem, 1-16, 47, 65-6, 116 
Behrens's distribution, 91-2 
Beta-distribution, 141, 143-4 
^-distribution, 146 
Beta-integral, 39 
between degrees of freedom, 104 (see 
also analysis of variance) 
between sums of squares 104, 261 (see 
also analysis of variance) 
bias, 27 
binomial distribution, 6 
^-distribution, 124 
goodness-of-fit, 171 
inferences, 141-53, 164, 182-3 
maximum likelihood, 133-4 
sufficiency, 49-50 
bivariate normal distribution, 214-21 
Cauchy distribution, 22, 38 (see also 
^-distribution) 
central limit theorem, 21, 28 
coefficient of dispersion, 166, 183 
combination of observations, 112-21 
comparative experiments, 78-9 
computation (see under numerical 
methods) 
conditional density, 19 
confidence coefficient, 15 
confidence interval, 15, 22-6 
analysis of variance, 108, 254 
binomial parameter, 146-50,164,184 
correlation coefficient, 221 
non-uniqueness, 24-5 
non-Bayesian, 68 
normal mean, 42 
normal means, 80-1, 93 
normal variance, 34-6, 43, 111 
normal variances, 90-1 
observation, 213 
Poisson parameter, 155-7 
regression, 209, 213, 246 
shortest, 25, 35, 282-4 
significance tests, 58-61 
confidence level, 15, 23 
confidence set, 25 
linear hypotheses, 224 
multinomial parameters, 164 
normal means, 97 
significance tests, 59 
conjugate family, 20-1, 55-6 
binomial distribution, 145 
Poisson distribution, 154 
contingency tables, 176-84, 213-14 
control, 79, 107-8 
correction factor, 106 
correlation coefficient, 214-21, 246 
Datum distribution, 9 
confusion with posterior  
distribution, 9, 32-3, 67-8, 115, 231: 
maximum likelihood, 139 
decision theory, 62-7 
dependent variable, 206 
degrees of freedom, 81-3 
analysis of covariance, 265 
Behrens's distribution, 91 
between and within samples, 78, 96, 
103-4, 110, 118 
binomial parameter, 141, 146 
contingency table, 177 
F-distribution, 86, 88 
goodness-of-fit, 157, 168, 172-3 
interaction, 249 
linear hypothesis, 222, 224, 226, 
233 
Poisson parameter, 153 
regression, 204, 208-9, 257, 259 
^-distribution, 36 
two-way classification, 247-9 
^-distribution, 26 
2S8 
SUBJECT INDEX 
design of experiments, 230, 270 
design matrix, 222, 230, 235, 242, 263 
effect, 247, 250-2 
estimation, 6, 23-4, 115 
contingency table, 180 
interval, 15 (see also confidence 
interval) 
least squares, 208 (see also least 
squares) 
maximum likelihood, 128 (see also 
maximum likelihood) 
point, 23, 131 
variance, 31, 41, 81-3 
expected utility, 65 
expected values, 160, 179 
exponential family, 5, 20-1, 49, 55-6 
binomial distribution, 51 
maximum likelihood, 134 
normal distribution, 51-2 
Poisson distribution, 154 
F-distribution, 86-9 
beta distribution, 143, 146 
binomial distribution, 124 
logarithmic transformation, 142, 
147-8 
odds, 141 
percentage points, 89 
Poisson parameters, 153-6 
F-statistic, 108-9 
F-test 
analysis of covariance, 262-5 
between and within samples, 105, 
107, 109-11 
linear hypotheses, 222-36, 239, 244 
normal means, 95-100,102-3 
normal variances, 86-7, 89-91 
regression, 257-62 
Mest, 99-100 
two-way classification, 247-9, 256 
factor, 246 
factorial function (and derivatives), 
136-7 
factorization theorem, 47, 50-1 
fair bet, 7 
family of distributions, 1, 5 (see also 
exponential family) 
fit of curve, 266 
fixed effects model, 230, 254-5 
gamma (I1-) distribution 
logarithmic transformation, 156 
maximum likelihood, 136-7 
Poisson parameter, 153-5 
sum, 32 
^-distribution, 28 
goodness-of-fit, 157-76 
grouped data, 162, 173-6, 214 
Homoscedastic, 206, 266 
independence (test), 177 
independent variable, 206 
index of dispersion, 166, 183 
inference, 1, 4, 66-7, 121 
information, 131-2 
binomial parameter, 150 
correlation coefficient, 219-20 
Poisson parameter, 156 
sample, 19 
information matrix, 131-3 
interaction, 247, 249-52 
interval estimate, 15 (see also  
confidence interval) 
inverse hyperbolic tangent  
transformation, 216, 219-20 
inverse sine transformation, 148-50 
joint (see under nouns) 
large samples, 17-18, 21-2, 118, 132- 
3, 138-9, 148, 167 
Latin square, 272 
least squares, 207-8, 222, 269-70 
estimates, 208,225,236,239 
equations, 226 
likelihood, 2, 4-6, 16, 21, 128 (see also 
maximum likelihood and  
sufficient statistic) 
likelihood principle, 59, 68-9, 139, 
149 
line of best fit, 208 
linear function, 102, 107-8, 225-6, 
231-2, 254 
linear hypothesis, 221-2, 226-9, 241 
location parameter, 33 
log-likelihood, 128-30, 157, 167 
log-odds, 142, 147-8 
logarithmic transformation, 85, 252 
F-distribution, 142, 147-8 
gamma (I1-) distribution, 156 
make-up, 237, 241 
main effect, 247, 250-2 
margins (contingency table), 179-82 
Markov chain, 54 
maximum likelihood estimate, 24,128— 
40 
binomial distribution, 133-4 
SUBJECT INDEX 
289 
maximum likelihood estimate (cont.) 
exponential family, 134 
gamma (r~) distribution, 136-7 
goodness-of-fit, 170-1 
least squares, 208 
normal distribution, 132-3 
sampling distribution, 139 
maximum likelihood equation, 134- 
7 
mean (see under normal distribution) 
mean square, 78, 105 
minimal sufficient statistic, 52-3 
mistakes, 13 
mixed model, 254-5 
multinomial distribution, 161, 181 
Poisson distribution, 165-6 
multiple regression, 241-6 
on powers, 266-9 
multiple regression coefficient, 241, 
245-6 
multiplication law of probabilities, 4 
multivariate normal distribution (see 
also bivariate) 
likelihood, 114-15,118-21 
posterior distribution of means, 223, 
225, 230-1, 245-6 
regression transformation, 223, 231, 
237 
sufficient statistics, 74 
Newton's method, 134-6 
non-Bayesian methods, 25-6, 67-70, 
94-5, 230 (see also datum  
distribution) 
non-orthogonal, 244, 267 
normal distribution (see also bivariate 
and multivariate) 
exponential family, 51-2 
maximum likelihood, 132-3 
mean, 1-23 
mean and variance, 36-46 
means, 76-86, 91-104, 227-8 
sufficient statistics, 51-2 
variance, 26-36, 101, 111-12 
variances, 86-91 
nuisance parameter, 38, 57-8, 232 
null hypothesis, 59, 67, 232 
numerical methods 
between and within samples, 106-7 
linear hypothesis, 236-46 
maximum likelihood, 134-6 
two-way classification, 253 
observed values, 160, 179 
odds, 7, 141, 146 
ordinates (regression), 259-61 
orthogonal design, 119, 234-5, 245, 
270 
two-way classification, 252-3 
orthogonal polynomials, 267-9 
paired comparisons, 83-6 
parameter, 1, 5 (see also nuisance 
parameter) 
Pareto distribution, 73 
percentage points, 34 
F-distribution, 88-9 
^-distribution, 39 
^-distribution, 34-5 
point estimate (see under estimation) 
Poisson distribution, 153-7 
goodness-of-fit, 173 
means, 165-6 
multinomial distribution, 166 
Poisson process, 137 
polynomial regression, 266-9 
posterior distribution, 2, 21-2,121 (see 
also datum distribution, and large 
samples) 
approximation by maximum  
likelihood, 129-32, 138 
binomial distribution, 141-8 
conjugate distribution, 55-6 
correlation coefficient, 214-15, 220- 
1 
decision theory, 65 
goodness-of-fit, 157-60 
linear function, 102, 107-8, 225-6, 
231-2, 254 
linear hypothesis, 223-6, 230 
multivariate normal, 114-15 
normal mean, 2-3, 7-11, 13-16, 
36-7, 41-2, 101 
normal mean and variance, 44-5, 
133 
normal means, 76-83, 91-3, 95-9, 
103, 112-14, 119-21 
normal variances, 86-7, 89-91 
Poisson distribution, 153-4 
prior to next sample, 116 
regression, 204-5, 207, 245-6: 
conditional expectation, 211 line, 
211-12 
sufficiency, 46-7 
tabulation, 45-6, 83 
precision, 8, 10, 24, 30-1, 116-18 
prediction, 11-12, 66 
regression, 212-13 
principle of precise measurement, 
12 
290 
SUBJECT INDEX 
prior distribution, 2, 7, 16-17, 21, 132 
{see also large samples) 
between and within samples, 112, 
119-21 
binomial distribution, 141 
conjugate distribution, 55-6 
correlation coefficient, 218-20 
decision theory, 65 
goodness-of-fit, 163, 166-7, 176 
linear hypothesis, 229 
maximum likelihood, 132 
normal mean, 2-3, 7 
normal mean and variance, 40 
normal variance, 26-33 
Poisson distribution, 153 
posterior of last sample, 116 
regression, 205-6 
sample information, 20-1, 151-3 
significance test, 61-2 
uniform, 18-19, 33, 145, 155, 
220 
vague prior, 9, 13-18, 31-3, 76, 132, 
144-6,155,219-20 
probability distribution (derivation), 
97-8, 103-4 
probable error, 23 
product (vectors), 236 
random effects model, 254-5 
random sample, 1-4, 10 
random sampling numbers, 53 
random sequence of trials, 141-53, 
157, 168, 176-8 
range, 56 
reduction of data, 10 
reduction in sums of squares, 222-5, 
232-6, 238, 245 
analysis of covariance, 262-5 
regression, 258-60 
regression, 203 
analysis of covariance, 265 
coefficient, 207, 241, 245-6 
linear, 203-14, 217, 228-9, 257- 
62 
multiple, 241-6 
polynomial, 266-9 
reduction, 264-5 
replication, 247, 255 
residual sum of squares, 222, 233-4, 
238, 243-5 
analysis of covariance, 262-3 
regression, 209, 258, 260-2 
two-way classification, 247-8, 253, 
256 
robustness, 13 
sample beliefs, 11-12, 66 
regression, 211-13 
sample correlation coefficient, 218 
sample information, 19-20 
sample mean, 41 
sample regression coefficient, 207,245- 
6 
sample variance, 41 
sampling, 4, 270 
Sheppard's correction, 163, 175-6 
significance level, 59 
exact, 60 
significance tests, 58-62, 111, 163 {see 
also F-, t- and x2-tests) 
non-Bayesian, 67-70, 94-5 
square root transformation, 156 
standard deviation, 8 
standard error, 23 
statistic, 46 
Stirling's formula, 137 
sufficiency, 10, 30, 41, 46-58 
sufficiency principle, 46 
sufficient statistic, 23, 46-58, 67 
between and within samples, 103 
binomial distribution, 49-51, 141 
correlation coefficient, 218 
linear hypothesis, 236 
maximum likelihood, 134 
normal distribution, 51-2 
Poisson distribution, 153 
regression, 210 
sum of squares, 78, 81-2 {see also 
mean square, reduction in sum of 
squares, residual, between, total 
and within sum of squares) 
uncorrected, 82 
sum of squares due to regression, 
209 
^-distribution, 36-9 
Behrens's distribution, 93-4 
F-distribution, 88 
normal distribution, 41-2 
Mest 
F-test, 99-100 
linear functions, 102, 107-8, 225-6, 
231-2, 254 
normal mean, 42 
normal means, 79-86, 99-101 
regression, 203-5, 208-10 
tests of hypotheses, 24 {see also  
significance tests) 
total degrees of freedom, 104 
total sum of squares, 104, 232-6, 
261 
SUBJECT INDEX 
291 
transformation to constant information 
binomial distribution, 149-50 
correlation coefficient, 219-20 
Poisson distribution, 156 
triangular matrix, 237 
two-way classification, 246-57 
uncorrected sum of squares, 106 
uniform distribution {see also prior 
distribution) 
goodness-of-fit, 163 
maximum likelihood, 140 
sufficiency, 56 
utility, 64 
utility function, 64 
vague prior knowledge, 9, 13-18, 76 
binomial distribution, 31-3 
maximum likelihood, 132 
normal variance, 31-3 
Poisson distribution, 155 
variance {see also normal distribution) 
assumption of equality, 80-2, 92, 99, 
105, 112,118,227;homoscedastic, 
206, 266 
ratio, 90 
weighing example, 119, 228 
weight, 113 
weighted least squares, 269-70 
weighted mean, 8, 31,113 
within degrees of freedom, 78,104 {see 
also analysis of variance) 
within sum of squares, 78, 82, 104, 
117-18, 261 {see also analysis of 
variance) 
^-distribution, 26-30 
F-distribution, 88 
gamma (T-) distribution, 28 
percentage points, 34, 282-4 
^'-statistic, 160 
X'2-statistic, 167 
X2-test 
binomial distribution, 164, 182-3 
contingency tables, 176, 178 
goodness-of-fit, 160, 168 
linear hypothesis, 225 
normal variance,26-38,43,101, 111— 
12 
Poisson distribution, 153,155,165-6 
regression, 204, 209 
292 
INDEX OF NOTATIONS 
(much of the notation is defined in Part I) 
A (design matrix), 222 
B0(a,b), 141 
C = (A'A)"1, 225 
£x(Vi, V2), Fa(vl5 
H,\ 
L(x\6), 128 
Pi-), 1 
s\ 41 
S2, 78, 96, 204, 
SJ, 222 
v2), 
222 
c c c 904 
'-'xx* '-'xj/j ^yy, **n 
&ij, 234 
/, 36 
'«("), 42 
x, 41 
x, 1 
_a> *^a> *^ 
a (probability), 
a (regression), : 
/? (probability), 
34 
203, 
15, 
89 
241 
34 
Pi, 241 
T, 237 
0,6 
6, 222 
0, 128 
K, 60 
j>, 26, 36, 86 
n(x), 11 
"(0), 1 
n(d\x), 2 
*n\ 129 
X2, 26, 160, 168, 171 
X'2, 167 
X», X», 35, 284 
X%v), j&v), 34 
<3 (Behrens), 91 
<3 (correlation), 215 
'Dot' notation, 96, 177 
Vector notation, 114, 222 
/? (regression), 203 
